journal of artificial intelligence research                  

submitted        published      

grounded semantic composition for visual scenes
peter gorniak
deb roy

pgorniak media mit edu
dkroy media mit edu

mit media laboratory
   ames st  cambridge  ma       usa

abstract
we present a visually grounded language understanding model based on a study of how
people verbally describe objects in scenes  the emphasis of the model is on the combination
of individual word meanings to produce meanings for complex referring expressions  the
model has been implemented  and it is able to understand a broad range of spatial referring
expressions  we describe our implementation of word level visually grounded semantics and
their embedding in a compositional parsing framework  the implemented system selects
the correct referents in response to natural language expressions for a large percentage of
test cases  in an analysis of the systems successes and failures we reveal how visual context
influences the semantics of utterances and propose future extensions to the model that take
such context into account 

   introduction
we introduce a visually grounded language understanding model based on a study of how
people describe objects in visual scenes of the kind shown in figure    we designed the study
to elicit descriptions that would naturally occur in a joint reference setting and that are easy
to produce and understand by a human listener  a typical referring expression for figure  
might be  the far back purple cone thats behind a row of green ones  speakers construct
expressions to guide listeners attention to intended objects  such referring expressions
succeed in communication because speakers and listeners find similar features of the visual
scene to be salient  and share an understanding of how language is grounded in terms of
these features  this work is a step towards our longer term goals to develop a conversational
robot  hsiao  mavridis    roy        that can fluidly connect language to perception and
action 
to study the use of descriptive spatial language in a task similar to one our robots
perform  we collected several hundred referring expressions based on scenes similar to figure
   we analysed the descriptions by cataloguing the visual features that they referred to
within a scene  and the range of linguistic devices  words or grammatical patterns  that
they used to refer to those features  the combination of a visual feature and corresponding
linguistic device is referred to as a descriptive strategy  the example sentence above contains
several descriptive strategies that make use of colour  spatial relations  and spatial grouping 
these descriptive strategies are used in composition by the speaker to make reference to a
unique object 
we propose a set of computational mechanisms that correspond to the most commonly
used descriptive strategies from our study  the resulting model has been implemented as a
set of visual feature extraction algorithms  a lexicon that is grounded in terms of these visual
c
    
ai access foundation  all rights reserved 

figorniak   roy

figure    a sample scene used to elicit visually grounded referring expressions  if this figure
has been reproduced in black and white  the light cones are green in colour  the
dark cones are purple 

features  a robust parser to capture the syntax of spoken utterances  and a compositional
engine driven by the parser that combines visual groundings of lexical units  to evaluate
the system  we collected a set of spoken utterances from three speakers  the verbatim transcriptions of the speech  complete with speech repairs and various other ungrammaticalities
common in spoken language  were fed into the model  the model was able to correctly
understand the visual referents of     of the expressions  chance performance
assuming
p  
that a random object is selected on each of a sessions    trials was      i     i        
the system was able to resolve a range of linguistic phenomena that made use of relatively
complex compositions of spatial semantics  we provide a detailed analysis of the sources
of failure in this evaluation  based on which we propose a number of improvements that
are required to achieve human level performance  in designing our framework we build on
prior work in human reference resolution and integration of semantics during parsing  the
main contribution of this work lies in using visual features based on a study of human visual
and linguistic reference as the grounded semantic core of a natural language understanding
system 
while our previous work on visually grounded language has centred on machine learning
approaches  roy  gorniak  mukherjee    juster        roy   pentland        roy        
we chose not to apply machine learning to the problem of compositional grounded semantics
in this investigation  rather  we endeavoured to provide a framework that can process the
types of descriptive strategies and compositionality that were found in our study with human
participants  in future work  we will investigate how machine learning methods can be used
to acquire parts of this framework from experience  leading to more robust and accurate
performance 
   

figrounded semantic composition for visual scenes

    grounded semantic composition
we use the term grounded semantic composition to highlight that both the semantics of
individual words and the word composition process itself are visually grounded  in our
model  each lexical entrys meaning is grounded through an association to a visual model 
for example  green is associated with a probability distribution function defined over a
colour space  we propose processes that combine the visual models of words  governed by
rules of syntax 
given our goal of understanding and modelling grounded semantic composition  several
questions arise 
 what are the visual features that people use to describe objects in scenes such as
figure   
 how do these features connect to language 
 how do features and their descriptions combine to produce whole utterances and
meanings 
 are word meanings independent of the visual scene they describe 
 is the meaning of the whole utterance based only on the meanings of its parts 
 is composition of meanings a purely incremental process 
we assumed easy answers to some of these questions as a place to start our modelling
effort  our current implementation assumes that the meaning of a whole utterance is
fully derived from the meanings of its parts  performs composition incrementally  without
backtracking   and does not let the visual context influence the interpretation of word
meanings  despite these assumptions  the system handles relatively sophisticated semantic
composition  when evaluated on test data  the system correctly understood and chose
appropriate referents for expressions such as  the purple one behind the two green ones
and the left green cone in front of the back purple one 
after analysing our systems performance on human participants utterances  we found
that 
 word meanings can be strongly dependent on the visual scene they describe  for
instance  we found four distinct visual interpretations for the word middle that are
linguistically indistinguishable  but instead depend on different visual contexts to be
understood 
 the meaning of an utterance may sometimes depend on more than the meanings of
its parts  its meaning may also depend on the visual context in which the utterance
occurs  which can modify how parts compose  for example  some objects that are
referred to as frontmost left would be referred to neither as left or frontmost
in isolation  nor are they the result of a multiplicative joined estimation of the two 
 composition of meanings in this task is not a purely incremental process  in some
cases we found it necessary to backtrack and reinterpret parts of the utterance when
   

figorniak   roy

no good referents can be found at a later processing stage  or when ambiguities cannot
be resolved with the current interpretation  due to a strictly feed forward model of
language understanding  our current implementation fails on such cases 
these results are similar to those reported in prior studies  brown schmidt  campana 
  tanenhaus        griffin   bock        pechmann         although our model does
not currently address these issues of context dependent interpretation and backtracking  we
believe that the framework and its approach to grounded compositional semantics provide
useful steps towards understanding spatial language  the system performs well in understanding spatial descriptions  and can be applied to various tasks in natural language and
speech based human machine interfaces 
this paper begins by highlighting several strands of related previous work  in section   
we introduce the visual description task that serves as the basis for this study and model 
section   presents our framework for grounded compositional semantics  section   describes
the resulting computational model  an example of the whole system at work is given in
section    we discuss the results of applying our system to human data from the spatial
description task in section    together with an analysis of the systems successes and failures 
this leads to suggestions for future work in section    followed by a summary in section   
    related work
winograds shrdlu is a well known system that could understand and generate natural
language referring to objects and actions in a simple blocks world  winograd         like
our system it performs semantic interpretation during parsing by attaching short procedures
to lexical units  see also miller   johnson laird         however  shrdlu had access to a
clean symbolic representation of the scene  whereas the system discussed here works with a
synthetic vision system and reasons over geometric and other visual measures  furthermore 
we intend our system to robustly understand the many ways in which human participants
verbally describe objects in complex visual scenes to each other  whereas shrdlu was
restricted to sentences it could parse completely and translate correctly into its formalism 
shrdlu is based on a formal approach to semantics in which the problem of meaning
is addressed through logical and set theoretic formalisms  partee provides an overview of
this approach and to the problems of context based meanings and meaning compositionality
from this perspective  partee         our work reflects many of the ideas from this work 
such as viewing adjectives as functions  pustejovskys theory of the generative lexicon
 gl  in particular takes seriously noun phrase semantics and semantic compositionality
 pustejovsky         our approach to lexical semantic composition was originally inspired
by pustejovskys qualia structures  however  these formal approaches operate in a symbolic
domain and leave the details of non linguistic influences on meaning unspecified  whereas
we take the computational modelling of these influences as our primary concern 
research concerning human production of referring expressions has lead to studies related to the one described here  but without computational counterparts  brown schmidt
e a   for example  engage participants in a free form dialogue  as opposed to the one sided
descriptions in our task  producing referential descriptions to solve a spatial arrangement
problem  brown schmidt et al          due to the use of more complicated scenes and
complete dialogues  they find that their participants often engage in agreement behaviours
   

figrounded semantic composition for visual scenes

and use discourse and visual context to disambiguate underspecified referring expressions
more often than in our study  similar tasks have been used in other studies of dialogue
and referring expressions  pechmann        griffin   bock         we intentionally eliminated dialogue and used a simpler visual scene and task to elicit spatial descriptions  as
opposed to description by object attributes   and to be able to computationally model the
strategies our participants employ  formal theories of vagueness support our findings in the
expressions produced by our participants  kyburg   morreau        barker        
word meanings have been approached by several researchers as a problem of associating
visual representations  often with complex internal structure  to word forms  models have
been suggested for visual representations underlying colour  lammens        and spatial
relations  regier        regier   carlson         models for verbs include grounding their
semantics in the perception of actions  siskind         and grounding in terms of motor
control programs  bailey        narayanan         object shape is clearly important when
connecting language to the world  but remains a challenging problem in computational
models of language grounding  in previous work  we have used histograms of local geometric
features which we found sufficient for grounding names of basic objects  dogs  shoes  cars 
etc    roy   pentland         this representation captures characteristics of the overall
outline form of an object that is invariant to in plane rotations and changes of scale  landau
and jackendoff provide a detailed analysis of additional visual shape features that play a
role in language  landau   jackendoff         for example  they suggest the importance of
extracting the geometric axes of objects in order to ground words such as end  as in end
of the stick  shi and malik propose an approach to performing visual grouping on images
 shi   malik         their work draws from findings of gestalt psychology that provide
many insights into visual grouping behaviour  wertheimer        desolneux  moisan   
morel         engbers e a  give an overview and formalization of the grouping problem
in general and various approaches to its solution  engbers   smeulders         in parallel
with the work presented in this paper  we also been studying visual grouping and will fold
the results into the systen described here  dhande        
our model of incremental semantic interpretation during parsing follows a tradition of
employing constraint satisfaction algorithms to incorporate semantic information starting
with shrdlu and continued in other systems  haddock         most prior systems use
a declaratively stated set of semantic facts that is disconnected from perception  closely
related to our work in this area is schulers         who integrates determination of referents
to the parsing process by augmenting a grammar with logical expressions  much like we
augment a grammar with grounded composition rules  see section       our emphasis 
however  is on a system that can actively ground word and utterance meanings through its
own sensory system  even though the system described here senses a synthetic scene  it
makes continuous measurements during the parsing process and we are now integrating it
into an active vision system  hsiao et al          schulers system requires a human specified
clean logical encoding of the world state  which ignores the noisy  complex and difficultto maintain process linking language to a sensed world  we consider this process  which
we call the grounding process  one of the most important aspects of situated human like
language understanding 
sam  brown  buntschuh    wilpon        and ubiquitous talker  nagao   rekimoto 
      are language understanding systems that map language to objects in visual scenes 
   

figorniak   roy

similar to shdrlu  the underlying representation of visual scenes is symbolic and loses
much of the subtle visual information that our work  and the work cited above  focus
on  both sam and ubiquitous talker incorporate a vision system  phrase parser and
understanding system  the systems translate visually perceived objects into a symbolic
knowledge base and map utterances into plans that operate on the knowledge base  in
contrast  we are primarily concerned with understanding language referring to the objects
and their relations as they appear visually 
we have previously proposed methods for visually grounded language learning  roy  
pentland         understanding  roy et al          and generation  roy         however  the
treatment of semantic composition in these efforts was relatively primitive  for a phrase  the
visual models of each word in the phrase were individually evaluated and multiplied  this
method only works for phrases with conjunctive modifiers  and even in those cases  as we
discuss later  ordering of modifiers sometimes needs to be taken into account  i e   leftmost
front does not always refer to what front leftmost does   while this simple approach
worked in the constrained domains that we have addressed in the past  it does not scale to
the present task  for example  the describer system  roy        encodes spatial locations
in absolute terms within the frame of reference of a visual scene  as a result  describer
makes mistakes that humans would not make  its grounding of the word highest  as an
example  is defined by a probability distribution centred at a specific height in the scene  so
that the object closest to that height is the best example of highest  not accounting for
the fact that there may be objects at greater height  depending on the relative sizes and
shapes of objects   in addition  describers only interpretation of a phrase like the highest
green rectangle is to find an object that is both close to the center of the probability
distributions for highest and for green  not accounting for the fact that for a human
listener the highest green rectangle need not be high on the screen at all  but only higher
than the other green rectangles   a word such as highest requires a visual binding that
includes some notion of rank ordering  such a move  however  requires a rethinking of how
to achieve semantic composition  which is addressed in the approach here 

   a spatial description task
we designed a task that requires people to describe objects in computer generated scenes
containing up to    objects with random positions on a virtual surface  the objects all had
identical shapes and sizes  and were either green or purple in colour  each object had a
    chance of being green  otherwise it was purple  we refer to this task as the bishop
task  and to the resulting language understanding model and implemented system simply
as bishop 
    motivation for task design
in our previous work  we have investigated how speakers describe objects with distinctive
attributes like colour  shape and size in a constraint speaking task and in scenes with a
constant number of objects  roy         speakers in such a task are rarely compelled to
use spatial relations and never use groups of objects  because in most cases objects can
be distinguished by listing their properties  in designing the bishop task  our goal was
to naturally lead speakers to make reference to spatial aspects of the scene  therefore 
   

figrounded semantic composition for visual scenes

we drastically increased the number of objects in the scene and decreased the number of
distinctive object attributes  we also let the number of objects vary throughout the trials to
cover both scenes cluttered with objects and scenes with only a few objects in our analysis 
in a variation of the task  we ran experiments in which the system chose objects at
random for the speaker to describer  rather than allowing the describer to make the choice 
we found that this made the task difficult and highly unnatural for the speaker as there
were often few visually salient arrangements that the randomly chosen objects took part
in  as a result  listeners make many more errors in resolving the reference in this variation
of the task       error when the speaker chose the object versus     when the system
chose   there are limits to the accuracy of pure linguistic reference which we appeared to
be reaching in the random selection version of the task  speakers seemed to have a much
harder time finding visually salient landmarks  leading to long and less natural descriptions 
for example in the centre there are a bunch of green cones  four of them  um  actually there
are more than four  but  ah  theres one thats in the centre pretty much of the pile of them
up to the its at the top  ahm  how can you say this    or the seventh cone from the right
side  followed by the listener counting cones by pointing at the screen   to avoid collecting
such unnatural data  we decided not to use the random selection version of the task 
another possible variant of the task would be to let the system choose objects in some
non random manner based on the systems analysis of which objects would be more natural to describe  however  this approach would clearly bias the data towards objects that
matched preexisting models of the system 
since we are interested in how people described objects spatially as well as which visual
features they found salient  we decided to let the listener pick objects that he or she felt
were concisely yet not trivially describable  we acknowledge that this task design eases
the difficulty of the understanding task  when speakers could not find an interesting object
that was easy to describe in other ways  they resorted to simpler choices like the leftmost
one  yet  the utterances elicited through this task are relatively complex  see appendix
a for a complete listing  and and provided serious challenges from an automatic language
understanding perspective 
scenes were rendered in  d instead of using an equivalent  d scene in anticipation of
the transition of the understanding system to a camera driven vision system  the use
of  d rendering introduces occlusion  shadows  and other sources of ambiguity that must
eventually be addressed if we are to transition to a real vision system  however  we note that
the scene did not include any interesting  d spatial relations or other features  and that we
do not claim that the description task and thus the system presented here would generalize
directly to a true  d setting  furthermore  we did not use any  d information about the
visual scene  so our system interprets all spatial relations in  d  this errs on the  d side
of the ambiguity inherent in a word like leftmost in reference to one of our scenes  the
interpretation can differ due to perspective effects  the leftmost object when interpreting
the scene as  d might not be the leftmost when interpreting it as  d  we believe the
task and the types of visually grounded descriptions it produced were challenging for a
computational system to understand  as we hope to show in the remainder of this paper 
finally  we should note that our goal was not to design a task to study collaboration 
dialogue and agreement  which is the goal of other experiments and analyses  carletta  
mellish        eugenio  jordan  thomason    moore         we use a speaker listener dyad
   

figorniak   roy

to ensure that the descriptions produced are understandable to a human listener  but we
purposefully did not allow listeners to speak  their only feedback channel to speakers was
the successful or unsuccessful selection of the described object  while this does introduce
a minimal form of dialogue  the low error rate of listeners leads us to believe that negative
reinforcement was negligible for all speakers and that the task should not be viewed as
an exercise in collaboration  we cannot rule out that listeners adopted strategies used by
their partners when it was their turn to speak  however  the relative similarity of strategies
between pairs shows that this phenomenon does not make the data unrepresentative  and
even produces the types of shortenings and vagueness that we would expect to see in an
extended description task when speaking to a machine 
    data collection
participants in the study ranged in age from    to    years  and included both native and
non native english speakers  pairs of participants were seated with their backs to each
other  each looking at a computer screen which displayed identical scenes such as that in
figure    in each pair  one participant served as describer  and the other as listener  the
describer wore a microphone that was used to record his or her speech  the describer used
a mouse to select an object from the scene  and then verbally described the selected object
to the listener  the listener was not allowed to communicate verbally or otherwise at all 
except through object selections  the listeners task was to select the same object on their
own computer display based on the verbal description  if the selected objects matched  they
disappeared from the scene and the describer would select and describe another object  if
they did not match  the describer would re attempt the description until understood by
the listener  using a describer listener dyad ensured that speech data resembled natural
communicative dialogue  participants were told they were free to select any object in the
scene and describe it in any way they thought would be clear  they were also told not to
make the task trivial by  for example  always selecting the leftmost object and describing
it as leftmost  the scene contained    objects at the beginning of each session  and a
session ended when no objects remained  at which point the describer and listener switched
roles and completed a second session  some participants fulfilled a role multiple times   we
found that listeners in the study made extremely few mistakes in interpreting descriptions 
and seemed to generally find the task easy to perform 
initially  we collected     spoken object descriptions from   participants  the raw
audio was segmented using our speech segmentation algorithm based on pause structure
 yoshida         along with the utterances  the corresponding scene layout and target
object identity were recorded together with the times at which objects were selected  this
    utterance corpus is referred to as the development data set  we manually transcribed
each spoken utterance verbatim  retaining all speech errors  false starts and various other
ungrammaticalities   rather than working with grammatically controlled language  our
interest was to model language as it occurs in conversational settings since our longer term
goal is to transplant the results of this work into conversational robots where language
will be in spoken form  off topic speech events  laughter  questions about the task  other
remarks  and filled pauses  were marked as such  they do not appear in any results we
report  
   

figrounded semantic composition for visual scenes

we developed a simple algorithm to pair utterances and selections based on their time
stamps  this algorithm works backwards in time from the point at which the correct
object was removed from the scene  it collects all on topic utterances that occurred before
this removal event and after the previous removal event and that are not more than  
seconds apart  it fuses them into a single utterance  and sends the scene description  the
complete utterance and the identity of the removed object to the understanding system 
the utterance fusing is necessary because participants often paused in their descriptions 
at the same time  pauses beyond a certain length usually indicated that the utterances
before the pause contained errors or that a rephrase occurred  this pairing algorithm is
obviously of a heuristic nature  and we mark instances where it makes mistakes  wrongly
leaving out utterances or attributing utterances to the wrong selection event  in the analysis
of our data below  when we report numbers of utterances in data sets in this paper  they
correspond to how many utterance selection pairs our pairing algorithm produces  this
means that due to errors by this algorithm the numbers of utterances we report are not
divisible by     the actual number of objects selected in each session 
the development corpus was analysed to catalogue the range of common referring strategies  see section       this analysis served as a basis for developing a visually grounded
language understanding system designed to replace the human listener in the task described
above  once this implementation yielded acceptable results on the development corpus  we
collected another     spoken descriptions from three additional participants to evaluate generalization and coverage of our approach  we used exactly the same equipment  instructions
and collection protocol as in collecting the development data to collect the test data  the
average length of utterances in both the development and the test set was between   and
  words  the discussion and analysis in the following sections focuses on the development
set  in section   we discuss performance on the test set 
    descriptive strategies for achieving joint reference
as we noted earlier  we call a combination of a visual feature measured on the current scene
 or  in the case of anaphora  on the previous scene  together with its linguistic realization
a descriptive strategy  in this section  we catalogue the most common strategies that describers used to communicate to listeners  this analysis is based strictly on the development
data set  we discuss how our implemented system handles these categories in section   
we distinguish three subsets of our development data 
 a set containing those utterance selection pairs that contain errors  an error can be
due to a repair or mistake on the human speakers part  a segmentation mistake by
our speech segmenter  or an error by our utterance selection pairing algorithm 
 a set that contains those utterance selection pairs that employ descriptive strategies
other than those we cover in our computational understanding system  we cover those
in sections       to        
 the set of utterance selection pairs in the development data that are not a member
of either subset described above  we refer to this subset as the clean set 
   

figorniak   roy

note that the first two subsets are not mutually exclusive  in the following sections 
we report two percentages for each descriptive strategy  the first is the percentage of
utterance selection pairs that employ a specific descriptive strategy relative to all the utterance selection pairs in the development data set  the second is the percentage of utterance selection pairs relative to the clean set of utterance selection pairs  as described above 
all examples given in this paper are actual utterances and scenes from our development
and test sets 
      colour
almost every utterance employs colour to pick out objects  while designing the task  we
intentionally trivialized the problem of colour reference  objects come in only two distinct
colours  green and purple  unsurprisingly  all participants used the terms green and
purple to refer to these colours  in previous work we have addressed the problems of
learning visually grounded models of colour words  roy   pentland        roy        
here  our focus is semantic compositionality of terms  so we chose to simplify the colour
naming problem  figure   shows one of the few instances where only colour is used to pick
out a referent  most of the examples in subsequent sections will be of colour composed with
other descriptive strategies 
syntactically  colours are expressed through adjectives  as mentioned  green and purple  that always directly precede the nouns they modify  that is  nobody ever said the
green left one in the data  but rather adjectives would commonly occur in the order the
left green one 
in our data  green and purple can also sometimes take the roles of nouns  or at least
be left dangling in a noun phrase with an ellipse like the leftmost purple  although this
form of dangling modifier might seem unlikely  it does occur in spoken utterances in our
task  as the only objects in bishop are cones  participants had no trouble understanding
such ellipsis  which occur in    of the data 
participants used colour to identify one or more objects in     of the data  and     of
the clean data 

the purple cone
figure    example utterance using only colour

   

figrounded semantic composition for visual scenes

      spatial regions and extrema
the second most common descriptive strategy is to refer to spatial extremes within groups
of objects and to spatial regions in the scene  the left example in figure   uses two spatial
terms to pick out its referent  closest and in the front  both of which leverage spatial
extrema to direct the listeners attention  in this example  selection of the spatial extremum
appears to operate relative to only the green objects  i e  the speaker seems to first attend
to the set of green cones  then choose amongst them  alternatively  closest and in the
front could pick several objects of any colour  and the colour specification could then filter
these spatial extrema to determine a final referent  in this case the two interpretations yield
the same referent  but there are cases in the corpus in which the second alternative  spatial
selection followed by colour selection  yields no referents at all 

the green one thats closest to us in the
front

the purple one on the left side

figure    example utterances specifying objects by referring to spatial extrema
the right example in figure   shows that phrases not explicitly indicating spatial extrema are still sometimes intended to be interpreted as referring to extrema  if the listener
was to interpret on the left side as referring to the left side of the scene  the phrase would
be ambiguous since there are four purple cones on the left side of the scene  on the other
hand  the phrase is unambiguous if interpreted as picking out an extremum  figure   shows
an instance where on the right hand side actually refers to a region on the board  the
first example in that figure shows the phrase on the right hand side combined with an
extremum term  lowest  note that the referent is not the right extremum  in the second
example in figure    the referent is not the bottommost green object  and   arguably 
if taking the scene as existing in  d   neither is it the leftmost  regions on the board
seem to play a role in both cases  often the local context of the region may play a stronger
role than the global one  as the referent in the second example in figure   can be found
by attending to the front left area of the scene  then selecting the left bottom example
amongst the candidates in this area  along the same lines  words like middle are largely
used to describe a region on the board  not a position relative to other cones 
being rather ubiquitous in the data  spatial extrema and spatial regions are often used
in combination with other descriptive strategies like grouping  but are most frequently
combined with other extrema and region specifications  as opposed to when combined with
colour adjectives  multiple spatial specifications tend to be interpreted in left to right order 
that is  selecting a group of objects matching the first term  then amongst those choosing
objects that match the second term  the examples in figure   could be understood as
   

figorniak   roy

the lowest purple on the right hand side

the green cone at the left bottom

figure    example utterances specifying regions

the purple one in the front left corner
figure    extrema in sequence
simply ignoring the order of spatial specifications and instead finding a conjoined best fit  i e 
the best example of both bottommost and leftmost  however  figure   demonstrates
that this is not generally the case  this scene contains two objects that are best fits to
an unordered interpretation of front left  yet the human participant confidently picks the
front object  possible conclusions are that extrema need to be be interpreted in sequence 
or that participants are demonstrating a bias preferring front back features over left right
ones  in our implementation  we choose to sequence spatial extrema in the order they occur
in the input 
participants used single spatial extrema to identify one or more objects in     of the
data  and in     of the clean data  they used spatial region specifications in     of the
data  also     of the clean data   and combined multiple extrema or regions in         
of the clean data  
      grouping
to provide landmarks for spatial relations and to specify sets of objects to select from  participants used language to describe groups of objects  figure   shows two examples of such
grouping constructs  the first using an unnumbered group of cones  the green cones   the
second using a count to specify the group  three   the function of the group is different
in the two examples  in the left scene the participant specifies the group as a landmark
to serve in a spatial relation  see section         whereas in the right scene the participant
first specifies a group containing the target object  then utters another description to select
within that group  note that grouping alone never yields an individual reference  so participants compose grouping constructs with further referential tactics  predominantly extrema
and spatial relations  in all cases 
   

figrounded semantic composition for visual scenes

the purple cone in the middle to the left
of the green cones

theres three on the left side  the one in
the furthest back

figure    example utterances using grouping

participants used grouping to identify objects in     of the data and     of the clean
data  they selected objects within described groups in      of the data     of the clean
data  and specified groups by number of objects  two  three       in      of the data
 also      of the clean data  
      spatial relations
as already mentioned in section        participants sometimes used spatial relations between
objects or groups of objects  examples of such relations are expressed through prepositions
like below or behind as well as phrases like to the left of or in front of  we already
saw an example of a spatial relation involving a group of objects in figure    and figure  
further shows two examples that involve spatial relations between individual objects  the
first example is one of the few examples of pure spatial relations between two individual
objects referenced only by colour  the second example is a more typical one where the
spatial relation is combined with another strategy  here an extremum  as well as two speech
errors by the describer  

the green cone below the green cone

theres a purple cone thats its all the
way on the left hand side but its its
below another purple

figure    example utterances specifying spatial relations

participants used spatial relations in    of the data     of the clean data  
   

figorniak   roy

      anaphora
in a number of cases participants used anaphoric references to the previous object removed
during the description task  figure   shows a sequence of two scenes and corresponding
utterances in which the second utterance refers back to the object selected in the first 

the closest purple one on the far left
side

the green one right behind that one

figure    example sequence of an anaphoric utterance

participants employed spatial relations in    of the data     of the clean data  
      other
in addition to the phenomena listed in the preceding sections  participants used a small
number of other description strategies  some that occurred more than once but that we
have not yet addressed in our computational model are selection by distance  lexicalised as
close to or next to   selection by neighbourhood  the green one surrounded by purple
ones   selection by symmetry  the one opposite that one   and selection by something
akin to local connectivity  the lone one   there are also additional types of groupings  for
example grouping by linearity  the row of green ones  the three purple on a diagonal 
and picking out objects within a group by number  the second one from the left in the row
of five purple ones  that we do not cover here  each of these strategies occurs less often
in the data than anaphora does  it occurs in    of utterances  see the previous section  
we annotated     of our data as containing descriptive strategies other than the ones
covered in the preceding sections  however  these other devices are often combined with
the phenomena covered here  we marked     of our data as containing errors  errors
come in the form of repairs by the speaker  as faulty utterance segmentation by our speech
segmenter  or through the misaligning of utterances with scenes by our system 
there are also a few instances of participants composing semantic phenomena in ways
that we do not handle  there were two instances of combining spatial relations  the one
below and to the right  and a few instances of specifying groups by spatial extrema and
regions  the group of the purple ones on the left   we did not count these as other in
our evaluation  but rather we counted them as errors  the reported success rate is correspondingly lower 
   

figrounded semantic composition for visual scenes

    summary
the preceding sections catalogue strategies participants employed in describing objects  a
computational system that understands utterances using these strategies must fulfill the
following requirements 
 the system must have access to the visual scene and be able to compute visual
features like those used by human speakers  natural groupings  inter object distances 
orderings and spatial relations
 it must have a robust language parsing mechanism that discovers grammatical patterns associated with descriptive strategies
 feeding into the parsing mechanism must be a visually grounded lexicon  each entry
in this lexicon must carry information as to which descriptive strategies it takes part
in  and how these descriptive strategies combine with others
 the semantic interpretation and composition machinery must be embedded into the
parsing process
 the system must be able to interpret the results of parsing an utterance and make a
best guess as to which object the whole utterance describes
we go on to describe our systems understanding framework  consisting of the visual
system  grounded lexical entries and the parser in section    in section   we discuss the
modules we implemented to understand human descriptive strategies 

   the understanding framework
in this section we describe the components of our bishop understanding system in detail 
with emphasis on how they fit together to work as a visually grounded understanding
system  we cover in turn bishops vision system  its parser and lexicon and give a short
overview of how our implementation of descriptive strategies fits into the framework 
    synthetic vision
instead of relying on the information we use to render the scenes in bishop  which includes
 d object locations and the viewing angle  we implemented a simple synthetic vision algorithm  this algorithm produces a map attributing each pixel of the rendered image to one
of the objects or the background  in addition  we use the full colour information for each
pixel drawn in the rendered scene  our goal is to loosely simulate the view of a camera
pointed at a scene of real world objects  the situation our robots find themselves in  we
have in the past successfully migrated models from synthetic vision  roy        to computer
vision  roy et al         and plan on a similar route to deploy bishop  obviously  many of
the hard problems of object detection as well as lighting and noise robustness do not need
to be solved in the synthetic case  but we hope that the transfer back to a robots camera
will be made easier by working from a  d image  we chose to work in a virtual world for
this project so that we could freely change colour  number  size  shape and arrangement of
   

figorniak   roy

objects to elicit interesting verbal behaviours in our participants  without running into the
limitations of object detection algorithms or field of view problems 
given an input image in which regions corresponding to objects have been segmented 
the features produced by the vision system are 
average rgb colour the average of the red  green and blue components of all the pixels
attributed to an object
centre of mass the average of the x and y pixel positions of an object
distance the euclidean distance between pairs of objects centres of mass
groups the groups of objects in the scene as determined by finding all sets of objects
that contain more than one object  and in which each object is less than a threshold
distance from another object in the group  distances are measured between centres of
mass 
pairs the same as groups  but filtered to produce only groups of two objects
triplets the same as groups  but filtered to produce only groups of three objects
convex hull the set of pixels forming the smallest convex region enclosing a set of objects
attentional vector sum  avs  the avs is a spatial relation measure between to objects  at extreme parameter settings it measures one of two angles  that formed
by the centres  the other formed by the closest points of two objects  we use a
parameter setting          in between these two extremes  which produces an intermediate angle depending on the objects shape  the resulting direction is measured relative to a set of reference angles  in our system the four cartesian vectors
                                regier   carlson        
    knowledge representation
objects are represented as integer ids in our system  for each id or set of ids the vision
system can compute the visual features described in section     based on the corresponding
set of pixels in the image  the distinguishing id together with the visual features represents
the systems total knowledge of the objects present in the scene  the system can further
instantiate new objects in the vision system from the convex hull of groups of other objects 
the system also remembers the id of the object removed last  and can ask the vision
system to perform a feature computation on the visual scene as it was before the object
was removed 
groups of objects have their own integer ids so they can be treated as objects themselves
 all visual features are available for them   their ids are stored together with a list of their
constituent objects ids  so that groups can be broken apart when necessary 
finally  as visible in the lexicon file in appendix b  each lexical item is stored with a set
of associated parameters  these parameters specify grammatical type  compositional arity
and reference behaviour  what the word sense can be taken as referring to on its own  a
single object  a group of objects or no objects   furthermore  the lexical item is associated
   

figrounded semantic composition for visual scenes

with a semantic composer  see sections     and    which store their own sets of parameters 
such as those specifying a gaussian together with its applicable dimensions in the case of
probabilistic composers 
    lexical entries and concepts
conceptually  we treat lexical entries like classes in an object oriented programming language  when instantiated  they maintain an internal state that can be as simple as a tag
identifying the dimension along which to perform an ordering  or as complex as multidimensional probability distributions  each entry also has a function interface that specifies how
it performs semantic composition  currently  the interface definition consists of the number
and arrangement of arguments the entry is willing to accept  whereas type mismatches are
handled during composition rather than being enforced through the interface  finally  each
entry can contain a semantic composer that encapsulates the actual function to combine
this entry with other constituents during a parse  these composers are described in depth
in section    the lexicon used for bishop contains many lexical entries attaching different
semantic composers to the same word  for example  left can be either a spatial relation
or an extremum  the grammatical structure detected by the parser  see the next section 
determines which compositions are attempted in a given utterance 
during composition  structures representing the objects that a constituent references
are passed between lexical entries  we refer to these structures as concepts  each entry
accepts zero or more concepts  and produces zero or more concepts as the result of the
composition operation  a concept lists the entities in the world that are possible referents
of the constituent it is associated with  together with real numbers representing their ranking
due to the last composition operation  a composer can also mark a concept as referring to
a previous visual scene  to allow for anaphoric reference  see section       it also contains
flags specifying whether the referent should be a group of objects or a single object  cones
vs  cone   and whether it should uniquely pick out a single object or is ambiguous in
nature  the vs  a   these flags are used in the post processing stage to determine
possible ambiguities and conflicts 
our lexicon  based on the development corpus  contains    words     adj  adjectives     cadj  colour adjectives  green  purple      n  nouns     rel  relative
pronouns  that  which     vpres  present tense verbs  is     relvpres  relative pronoun present tense verb combinations  thats  its     art  the     spec
 adjective specifiers  right  as in right above   just     p  prepositions     specific
prepositions  pof  pat  and two versions of pin   the complete lexicon specification is
reproduced in appendix b 
    parsing
while in previous work we have used markov models to parse and generate utterances
 roy         we here employ to context free grammars  these grammars naturally let us
specify local compositional constraints and iterative structures  specifically  they allow us to
naturally perform grounded semantic composition whenever a grammar rule is syntactically
complete  producing partial understanding fragments at every node of the parse tree  the
parse structure of an utterance thus dictates which compositions are attempted  we use a
   

figorniak   roy

bottom up chart parser to guide the interpretation of phrases  allen         such a parser
has the advantage that it employs a dynamic programming strategy to efficiently reuse
already computed subtrees of the parse  furthermore  it produces all sub components of a
parse and thus produces a useable result without the need to parse to a specific symbol 
by using a dynamic programming approach we are assuming that meanings of parts can
be assembled into meanings of wholes  we are not strictly committed to this assumption
and in the future will consider backtracking strategies as necessary  also note that due
to the fact that our framework often produces functions to be applied at later stages of
interpretation  see section    we avoid some possible overcommitting decisions  excluding
the correct referent at an early stage of understanding  
bishop performs only a partial parse  a parse that is not required to cover a whole
utterance  but simply takes the longest referring parsed segments to be the best guess 
unknown words do not stop the parse process  rather  all constituents that would otherwise
end before the unknown word are taken to include the unknown word  in essence making
unknown words invisible to the parser and the understanding process  in this way we recover
essentially all grammatical chunks and relations that are important to understanding in our
restricted task  for an overview of related partial parsing techniques  see the work of abney
       
the grammar used for the partial chart parser is shown in figure    together with
the grammar rules the table shows the argument structures associated with each rule  in
the given grammar there is only one argument structure per rule  but there can be any
number of argument structures  in the design of our grammar we were constrained by
the compositions that must occur when a rule is applied  this can especially be seen for
prepositional phrases  which must occur in a rule with the noun phrase they modify  the
chart parser incrementally builds up rule fragments in a left to right fashion during a parse 
when a rule is syntactically complete  it checks whether the composers of the constituents
in the tail of the rule can accept the number of arguments specified in the rule  as shown
in the last column of table     if so  it calls the semantic composer associated with the
constituent with the concepts yielded by its arguments to produce a concept for the head
of the rule  if the compose operation fails for any reason  the constituent cannot accept
the arguments or the compose operation does not yield a new concept  the rule does not
succeed and does not produce a new constituent  if there are several argument structures
 not the case in the final grammar shown here  or if a compose operation yields several
alternative concepts  several instances of the head constituent are created  each with its
own concept 
we provide an example chart produced by this grammar in figure    in section    as
part of an example of the whole understanding process  the composition actions associated
with each lexical item  and thus with each rule completion using this grammar  are listed
in appendix b 
    post parse filtering
once a parse has been performed  a post parse filtering algorithm picks out the best interpretation of the utterance  first  this algorithm extracts the longest constituents from
the chart that are marked as referring to objects  assuming that parsing more of the utter   

figrounded semantic composition for visual scenes

adj
np
np
np
np
np
np
np
np
np
np
np
np
np
np
np
p
p
p





















t 
adj
adj
cadj
n
art
np
np
np
np
np
np
np
np
np
np
np
spec
p
pof

t 
adj
np
n

t 

np
p
p
relvpres
p
rel
rel
rel
relvpres
rel
relvpres
rel
p
p

np
art
p
n
vpres
p
vpres
p
vpres
adj
cadj

t 

t 

t 

t 

n
art
pof
np
np
p
np
adj

pof
n
np

np
pof

np

np

arg structure
t   t   
t   t   
t   t   
t    
t   t   
t   t    t   
t   t    t   
t   t    t   
t   t    t   
t   t    t   
t   t    t   
t   t    t   
t   t    t   
t   t   
t   t   
t   t   
t   t   
t    
t    

table    grammar used in bishop
ance implies better understanding  the filtering process then checks these candidates for
consistency along the following criteria 
 all candidates must either refer to a group or to a single object
 if the candidates are marked as referring to an unambiguously specified single object 
they must unambiguously pick a referent
 the referent in the specified single object case must be the same across all candidates
 if the candidates are marked as selecting a group  each must select the same group
if any of these consistency checks fail  the filtering algorithm can provide exact information as to what type of inconsistency occurred  within group ambiguity  contradicting
constituents  no object fulfilling the description   which constituents were involved in the
inconsistency and which objects  if any  are referenced by each candidate constituent  in
the future  we plan to use this information to resolve inconsistencies through active dialogue 
currently  we enforce a best single object choice after the post processing stage  if the
filtering yields a single object  nothing needs to be done  if the filtering yields a group
of objects  we choose the best matching object  note that in this case we ignore the fact
of whether the resulting concept is marked as referring to a group or a single object   if
several inconsistent groups of referents remain after filtering  we randomly pick one object
from the groups 
   

figorniak   roy

   semantic composition
in this section we revisit the list of descriptive strategies from section     and explain
how we computationally capture each strategy and its composition with other parts of the
utterance  most of the composers presented follow the same composition schema  they take
one or more concepts as arguments and yield another concept that references a possibly
different set of objects  concepts reference objects with real numbered values indicating
the strength of the reference  composers may introduce new objects  even ones that do
not exist in the scene as such  and they may introduce new types of objects  e g  groups
of objects referenced as if they were one object   to perform compositions  each concept
provides functionality to produce a single referent  or a group of referents  the single
object produced is simply the one having maximum reference strength  whereas a group is
produced by using a reference strength threshold below which objects are not considered as
possible referents of this concept  the threshold is relative to the minimum and maximum
reference strength in the concept  most composers first convert an incoming concept to the
objects it references  and subsequently perform computations on these objects 
furthermore  composers can mark concepts as not referring  as referring to a single
object or as referring to a group of objects  this is independent of the actual number of
objects yielded by the concept  and can be used to identify misinterpretations and ambiguities  we currently use these flags to delay composition with arguments that do not refer
to objects  for example  this constraint prevents the left green to cause any composition when green is considered to be an adjective  for such cases  new chaining semantic
composers are created that delay the application of a whole chain of compositions until a
referring word is encountered  these chaining composers internally maintain a queue of
composers  if during the argument for a composition operation does not refer to an object 
both the composer producing the argument and the composer accepting it are pushed onto
the queue  when the first referring argument is encountered  the whole queue of composers
is executed starting with the new argument and proceeding backwards in the order the
composers were encountered 
we plan to exploit these features further in a more co operative setting than the one
described here  where the system can engage in clarifying dialogue with the user  we explain
in section     how we converged on a single object reference in the task under discussion
here  and what the other alternatives would be 
    colour   probabilistic attribute composers
as mentioned in section      we chose not to exploit the information used to render the
scene  and therefore must recover colour information from the final rendered image  this
is not a hard problem because bishop only presents virtual objects in two colours  the
renderer does produce colour variations in objects due to different angles and distances from
light sources and the camera  the colour average for the  d projection of each object also
varies due to occlusion by other objects  in the interest of making our framework more easily
transferable to a noisier vision system  we worked within a probabilistic framework  we
separately collected a set of labelled instances of green and purple cones  and estimated
a three dimensional gaussian distribution from the average red  green and blue values of
each pixel belonging to the example cones 
   

figrounded semantic composition for visual scenes

when asked to compose with a given concept  this type of probabilistic attribute composer assigns each object referenced by the source concept the probability density function
evaluated at the measured average colour of the object 
    spatial extrema and spatial regions   ordering composers
to determine spatial regions and extrema  an ordering composer orders objects along a
specified feature dimension  e g  x coordinate relative to a group  and picks referents at an
extreme end of the ordering  to do so  it assigns an exponential weight function to objects
according to
 i   v 
for picking minimal objects  where i is the objects position in the sequence  v is its value
along the feature dimension specified  normalized to range between   and   for the objects
under consideration  the maximal case is weighted similarly  but using the reverse ordering
subtracting the fraction in the exponent from   
  imax i   v 
where imax is the number of objects being considered  for our reported results         
this formula lets referent weights fall off exponentially both with their position in the ordering and their distance from the extreme object  in that way extreme objects are isolated
except for cases in which many referents cluster around an extremum  making picking out
a single referent difficult  we attach this type of composer to words like leftmost and
top 
the ordering composer can also order objects according to their absolute position  corresponding more closely to spatial regions rather than spatial extrema relative to a group 
the reference strength formula for this version is
d

     dmax  
where d is the euclidean distance from a reference point  and dmax the maximum such
distance amongst the objects under consideration 
this version of the composer is attached to words like middle  it has the effect that
reference weights are relative to absolute position on the screen  an object close to the centre of the board achieves a greater reference weight for the word middle  independently
of the position of other objects of its kind  ordering composers work across any number
of dimensions by simply ordering objects by their euclidean distance  using the same exponential falloff function as in the other cases  the ordering composer for middle  for
example  computes distance from the board centre to the centres of mass of objects  and
thus prefers those that are centred on the screen 
    grouping composers
for non numbered grouping  e g   when the describer says group or cones   the grouping
composer searches the scene for groups of objects that are all within a maximum distance
threshold from another group member  this threshold is currently set by hand based on
a small number of random scenes in which the designers identified isolated groups and
   

figorniak   roy

adjusted the threshold to correctly find them but not others  it only considers objects
that are referenced by the concept it is passed as an argument  for numbered groups
 two  three   the composer applies the additional constraint that the groups have to
contain the correct number of objects  reference strengths for the concept are determined
by the average distance of objects within the group  we acknowledge that this approach
to grouping is simplistic and we are currently investigating more powerful visual grouping
algorithms that take topological features into consideration  in spite of our simple approach 
we can demonstrate some instances of successfully understanding references to groups in
bishop 
the output of a grouping composer may be thought of as a group of groups  to understand the motivation for this  consider the utterance  the one to the left of the group of
purple ones  in this expression  the phrase group of purple ones will activate a grouping
composer that will find clusters of purple cones  for each cluster  the composer computes
the convex hull  the minimal elastic band that encompasses all the objects  and creates
a new composite object that has the convex hull as its shape  when further composition
takes place to understand the entire utterance  each composite group serves as a potential
landmark relative to left 
however  concepts can be marked so that their behaviour changes to split apart concepts
refering to groups  for example  the composer attached to of sets this flag on concepts
passing through it  note that of is only involved in composition for grammar rules of the
type np  np p np  but not for those performing spatial compositions for phrases like
to the left of  therefore  the phrase the frontmost one of the three green ones will pick
the front object within the best group of three green objects 
    spatial relations   spatial composers
the spatial semantic composer employs a version of the attentional vector sum  avs 
suggested by regier and carlson         the avs is a measure of spatial relation meant
to approximate human judgements corresponding to words like above and to the left
of in  d scenes of objects  it computes an interpolation between the angle between the
centres of masses of the objects and the angle between the two closest points of the objects 
in addition to a value depending on height relative to the top of the landmark object 
despite our participants talking about  d projections of  d scenes we found that the avs
distinguishes the spatial relations used in our data rather well when simply applied to the
 d projections  the participants often used spatial descriptors such as below  suggesting
that they sometimes conceptualized the scenes as  d  in a  d setting we would expect to
see consistent use of semantic patterns like in front of instead of below 
given two concepts as arguments  the spatial semantic composer converts both into sets
of objects  treating one set as providing possible landmarks  the other as providing possible
targets  the composer then calculates the avs for each possible combination of landmarks
and targets  the reference vector used for the avs is specified in the lexical entry containing
the composer  e g         for behind  finally  the spatial composer divides the result by
the euclidean distance between the objects centres of mass  to account for the fact that
participants exclusively used nearby objects to select through spatial relations 
   

figrounded semantic composition for visual scenes

    anaphoric composers
triggered by words like that  as in to the left of that one  or previous  an anaphoric
composer produces a concept that refers to a single object  namely the last object removed
from the scene during the session  this object specially marks the concept as referring not
to the current  but the previous visual scene  and any further calculations with this concept
are performed in that visual context 
for example  when the parser calls upon the anaphoric composer attached to the lexical entry for that to provide an interpretation of that one  this composer marks the
produced concept as referring back to the previous visual scene  and sets the previously selected object as the only possible referent  now consider another composer  say the spatial
composer attached to left in the one to the left of that one  when it asks for spatial
relation features between the referents of the one and that one  these spatial relation
features  see section      are computed on the previous visual scene with the object that
was removed due to the previous utterance as the only possible landmark of the spatial
relation 

   example  understanding a description

example scene

the purple one

one on the left

the purple one on the left

figure    example  the purple one on the left
to illustrate the operation of the overall system  in this section we step through some
examples of how bishop works in detail  consider the scene in the top left of figure    and
the output of the chart parser for the utterance  the purple one on the left in figure    
starting at the top left of the parse output  the parser finds the in the lexicon as an art
 article  with a selecting composer that takes one argument  it finds two lexical entries
for purple  one marked as a cadj  colour adjective   and one as an n  noun   each
of them have the same composer  a probabilistic attribute composer marked as p    but
the adjective expects one argument whereas the noun expects none  given that the noun
expects no arguments and that the grammar contains a rule of the form np n  an np
 noun phrase  is instantiated and the probabilistic composer is applied to the default set
of objects yielded by n  which consists of all objects visible  this composer call is marked
   

figorniak   roy

p n  in the chart  after composition  the np contains a subset of only the purple objects
 figure    top right   at this point the parser applies np  art np  which produces the
np spanning the first two words and again contains only the purple objects  but is marked
as unambiguously referring to an object  s np  marks the application of this selecting
composer called s 
the
art the

purple

one

on

the

left

cadj purple
n purple
np p n 
np s np 
n one
np one
np p n 
np s np 
p on
art the
n left
adj left
n left
np left
np left
np s np 
np s np 
np o x min np 
np o x min np 
np o x min np 

figure     sample parse of a referring noun phrase

the parser goes on to produce a similar np covering the first three words by combining
the purple cadj with one and the result with the  the on p  preposition  is left
dangling for the moment as it needs a constituent that follows it  it contains a modifying
semantic composer that simply bridges the p  applying the first argument to the second 
after another the  left has several lexical entries  in its adj and one of its n forms it
contains an ordering semantic composer that takes a single argument  whereas its second n
form contains a spatial semantic composer that takes two arguments to determine a target
and a landmark object  at this point the parser can combine the and left into two
possible nps  one containing the ordering and the other the spatial composer  the first of
these nps in turn fulfills the need of the on p for a second argument according to np 
np p np  performing its ordering compose first on one  for one on the left   selecting all
the objects on the left  figure    bottom left   the application of the ordering composer is
   

figrounded semantic composition for visual scenes

denoted as o x min np  in the chart  indicating that this is an ordering composer ordering
along the x axis and selecting the minimum along this axis  on combining with purple
one  the same composer selects all the purple objects on the left  figure    bottom right  
finally on the purple one  it produces the same set of objects as purple one  but marks
the concept as unambiguously picking out a single object  note that the parser attempts
to use the second interpretation of left  the one containing a spatial composer  but fails
because this composer expects two arguments that are not provided by the grammatical
structure of the sentence 

   results and discussion
in this section we first discuss our systems overall performance on the collected data  followed by a detailed discussion of the performance of our implemented descriptive strategies 
    overall performance
for evaluation purposes  we hand annotated the data  marking which descriptive strategies
occurred in each example  most examples use several reference strategies  in table  
we present overall accuracy results  indicating for which percentage of different groups of
examples our system picked the same referent as the person describing the object  the first
line in the table shows performance relative to the total set of utterances collected  the
second one shows the percentage of utterances our system understood correctly excluding
those marked as using a descriptive strategy that was not listed in section    and thus not
expected to be understood by bishop  some such examples are given in section        the
final line in table   shows the percentage of utterances for which our system picked the
correct referent relative to the clean development and testing sets  leaving out utterances
marked as other as well as those marked as containing some kind of error  as we defined
earlier  this could be a speech error that was still understood by the human listener  or due to
an error by the algorithm that pairs utterances with selection events  additionally  relying
on automatic speech segmentation sometimes merged utterances into one that should have
been separate utterances  this mistakenly attributes the combination of two descriptions to
one object selection and leaves another object selection without a corresponding utterance 
note  however  that due to our loose parsing strategy and the frequent redundancies in
speakers utterances our system was able to handle a good number of utterances marked as
either other or error 
utterance set
all
all except other
all except other and errors  clean 

accuracy   development
     
     
     

accuracy   testing
     
     
     

table    overall results

using unconstrained speech primarily made writing a covering yet precise grammar difficult  this difficulty together with the loose parsing strategy made our system occasionally
attempt compositions that are not supported by the grammatical structure of the utterance 
   

figorniak   roy

this overeager parsing strategy also produces a number of correct guesses that would not
be found by a tighter grammar  and we found during development that the tradeoff often
favoured looser parsing in terms of number of correct responses produced  constructing
the grammar is an obvious area to be addressed with a machine learning approach in the
future  using a speech segmenter together with an utterance reassembler produced very
few errors because we used the successful selection event as a strong guideline for deciding
which speech segments were part of a description  errors of this type occur in less that   
of the data 
bishop performance

   

random guess mean
random guess mean    std dev

  
  

average accuracy

  
  
  
  
  
  
  
 

 

 

 

 

development all

development clean

test all

test clean

figure     results for the development and test corpora

figure    graphs the results for each corpus and for a simulation of a uniform random
selection strategy  each bar shows the mean performance on a data set  with error bars delimiting one standard deviation  the figure shows results from left to right for the complete
development corpus  the clean development corpus  the complete test corpus and the clean
test corpus  our system understands the vast majority of targeted utterances and performs
significantly better than the random baseline  given the unconstrained nature of the input
and the complexity of the descriptive strategies described in section     we consider this an
important achievement 
table   provides more detail for the various descriptive strategies and lists the percentage of correctly identified referents for utterances employing spatial extrema and regions 
   

figrounded semantic composition for visual scenes

combinations of more than one spatial extremum  grouping constructs  spatial relations
and anaphora  note again that these categories are not mutually exclusive  we do not list
separate results for the utterances employing colour terms because colour terms are not a
source of errors  due to the synthetic nature of the vision system  
utterance set
spatial extrema
combined spatial extrema
grouping
spatial relations
anaphora

accuracy   development
               
             
            
            
          

accuracy   test
             
             
            
            
           

table    detailed results
not surprisingly  bishop makes more mistakes when errors are present or strategies
other than those we implemented occur  however  bishop achieves good coverage even
in those cases  this is often a result of overspecification on the part of the describer 
this tendency towards redundancy shows even in simple cases  for example in the use of
purple even though only purple cones are left in the scene  it translates furthermore
into specifications relative to groups and other objects when a simple leftmost would
suffice  overspecification in human referring expressions is a well known phenomenon often
attributed to the incremental nature of speech production  speakers may be listing visually
salient characteristics such as colour before determining whether colour is a distinguishing
feature of the intended referent  pechmann        
the worst performance  that of the grouping composers  can be attributed both to
the fact that the visual grouping strategy is too simplistic for the task at hand  and that
this phenomenon is often combined in rather complex ways with other strategies  these
combinations also account for a number of mistakes amongst the other composer that
perform much better when combined with strategies other than grouping  we cover the
shortcomings of the grouping composers in more detail in section       
mistakes amongst the descriptive strategies we cover have several causes 
overcommitment undercommitment some errors are due to the fact that the interpretation is implemented as a filtering process without backtracking  each semantic
composer must produce a set of objects with attached reference strengths  and the
next composer works from this set of objects in a strictly feedforward manner  during
composition this strategy fails when the target object is left out at one stage  e g  in
the leftmost one in the front  leftmost selects the leftmost objects  not including
the obvious example of front that is not a good example of leftmost   it also
fails when too many target objects are included  e g  a poor example of leftmost
is included in the set that turns out to be an ideal example of front   estimating
the group membership thresholds from the data will certainly decrease occurrence of
these errors  but the real solution lies in a backtracking strategy combined with composers that are sensitive to the visual scenery beyond their immediate function  such
sensitive composers might take into account facts about the isolated nature of certain
   

figorniak   roy

candidates as well as the global distribution of cones across the board  we discuss
specific cases in which global and local visual context influence the interpretations of
words in section     
insufficient grammar for some cases that contain many prepositional phrases  e g  the
leftmost one in the group of purple ones on the right and to the bottom  our grammar
was not specific enough to produce unambiguous answers  the grammar might attach
on the right to the object rather than to the group of objects  not taking into account
the biases in parsing that human listeners showed 
flawed composers some of the composers we have implemented are not sufficient to
understand all facets of the corresponding human descriptive strategies  we will
mention these problems in the following section 
    performance of composers
we will now go reconsider each descriptive strategy and discuss the successes and failures
of our composers that were designed to deal with each 
      colour
due to the simple nature of colour naming in the bishop task  the probabilistic composers
responsible for selecting objects based on colour made no errors 
      spatial regions and extrema
our ordering composers correctly identify      of the cases in which a participant uses only
colour and a single spatial extremum in his or her description  we conclude that participants
follow a process that yields the same result as ordering objects along a spatial dimension
and picking the extreme candidate  participants also favour this descriptive strategy  using
it with colour alone in     of the clean data  figure   provides examples of this type that
our system handles without problems 
description by spatial region occurs alone in only    of the clean data  and together
with other strategies in     of the clean data  almost all the examples of this strategy
occurring alone use words like middle or centre  the left image in figure    exemplifies
the use of middle that our ordering semantic composer models  the object referred to is
the one closest to the centre of the board  we do not model the fact that human speakers
only use this version of the descriptive strategy if there is an obvious single candidate object 
the right image in figure    shows a different interpretation of middle  the object in the
middle of a group of objects  note that the group of objects is linguistically not mentioned 
also note that within the group there are two candidate centre objects  and that the one in
the front is preferred  our composer only picks the correct object for this use of middle
if the target object also happens to be the one closest to the centre of the board 
figure    shows another use of the word middle  this strategy seems related to the
last one  picking the object in the middle of a group   however here the scene happens to be
divided into two groups of objects with a single object in between them  even though the
object is in the back and not the closest one to the centre of the board  due to the visual
   

figrounded semantic composition for visual scenes

the green one in the middle

the purple cone in the middle

figure     types of middles  
context participants understand it to be the object in the middle  our composer fails in
this case 

the purple one in the middle
figure     types of middles  
figure    shows a sequence of two scenes that followed each other during a data collection
session  the first scene and utterance are a clear example of an extremum combined with a
region specification  and our ordering composers easily pick out the correct object  in the
next scene  the listener identified the leftmost object to be the one right in the middle 
despite the scenes similarity to the right image in figure     where the middle object
was in the middle of the group  we suspect that the use of middle in the scene before
biases the understanding of middle as being relative to the board in this case  providing
an example where not only visual  but also historical context influence the meanings of
words   note that right in the utterance right in the middle is interpreted to have
spec grammatical type by bishop  and does not have a spatial role  see the grammar in
table    

the green one in the middle front

the purple one right in the middle

figure     types of middles  
   

figorniak   roy

in summary  we can catalogue a number of different meanings for the word middle
in our data that are linguistically indistinguishable  but depend on visual and historical
context to be correctly understood  more generally  it is impossible to distinguish regionbased uses from the various extrema based uses of words based on the utterance alone in
our data  we made the decision to treat middle to signify regions and left  top  etc 
to signify extrema  but our examples of middle show that the selection of which meaning
of these words to use depends on far subtler criteria such as global and local visual context 
existence of an unambiguous candidate and past use of descriptive strategies 
participants composed one or more spatial region or extrema references in     of the
clean data  our ordering composers correctly interpret     of these cases  for example
those in figure   in section        the mistakes our composers make are usually due to
overcommitment and faulty ordering  figure    shows an example that could be interpreted
as either problem  we indicate both the correct example and the object our system selects  
we should note that this example comes from a non native english speaker who often
used to where native speakers would use in  our system selects the purple object
closest to the back of the board instead of the indicated correct solution  this could be
interpreted as overcommitment  because the composer for back does not include the
target object  leaving the composer for left the wrong set of objects to choose from  a
better explanation perhaps is that the ordering of the composers should be reversed in this
case  so that the composer for back should take the objects selected by left as input 
however  this ordering violates the far more common left to right ordering of region and
extrema strategies in our data  which we selected to implement in our system  the question
thus becomes what causes the difference in ordering in cases like the one in figure     once
again  we suspect that visual context plays a role  perhaps it is clear to the listener here
that the double spatial specification would be an overspecification for the object our system
selects  it is simply the purple one in the back   in response  the listener may seek an
object that needs the full utterance  such as the true target  however  this analysis is
hard to combine with the very common trend towards overspecification on the part of the
speaker  leaving us with the need to run a more focused study of these phenomena to pin
down the factors that play a role in their interpretation 

purple cone to the back on the left side
figure     misinterpreted utterance using composed extrema

   

figrounded semantic composition for visual scenes

      grouping
our composers implementing the grouping strategies used by participants are the most simplistic of all composers we implemented  compared to the depth of the actual phenomenon
of visual grouping  the left scene in figure    shows an example our grouping composer
handles without a problem  the group of two cones is isolated from all other cones in
the example  and thus is easily found by our distance thresholding algorithm  in contrast 
the right scene depicts an example that would require much greater sophistication to find
the correct group  the target group of three cones is not visually isolated in this scene 
requiring further criteria like colinearity to even make it a candidate  furthermore  there
is a second colinear group of three cones that would easily be the best example of a row
of three purple cones in the absence of the target group  it is only the target groups
alignment with the vertical axis that let it stand out more as a row and make it the
most likely interpretation  our algorithm currently fails to include such grouping hints 
and thus fails to pick the correct answer in this scene  note that such hints are not always linguistically marked as they are here  through row   but often colinearity is silently
assumed as holding for groups  making our simple grouping operator fail  a rich source
of models for possible human grouping strategies like co linearity comes from research in
gestalt grouping  wertheimer        

the cone on the right in the pair of
cones

the purple cone on at the front of the
row of three purple cones

figure     easy and hard visual grouping

      spatial relations

the green cone behind the purple cone
figure     successful spatial relation understanding
the avs measure divided by distance between objects corresponds very well to human
spatial relation judgements in this task  all the errors that occur in utterances that contain
   

figorniak   roy

spatial relations are due to the possible landmarks or targets not being correctly identified
 grouping or region composers might fail to provide the correct referents   our spatial
relation composer picks the correct referent in all those cases where landmarks and targets
are the correct ones  for example in figure     also see the next section for a further correct
example of spatial relations  obviously  there are types of spatial relations such as relations
based purely on distance and combined relations  to the left and behind  that we decided
not to cover in this implementation  but that occur in the data and should be covered in
future efforts 
      anaphora

the cone at the right front

the cone behind that on the left

figure     successful anaphora understanding
our solution to the use of anaphora in the bishop task performs perfectly in replicating
reference back to a single object in the clean data  this reference is usually combined with
a spatial relation in our data  as in figure     due to the equally good performance of our
spatial relation composer  we cover all cases of anaphora in the development data  however 
there are more complex variants of anaphora that we do not currently cover  for example
reference back to groups of objects such as in the sequence in figure     which follows the
right example in figure    

the next cone in the row

the last cone in the row

figure     group based anaphora

   future directions
given the analysis of bishops performance  there are several areas of future improvements that may be explored  the descriptive strategies we classified as other should be
understood by the computational system 
   

figrounded semantic composition for visual scenes

distance a simple implementation to understand this strategy has the grammatical behaviour of our spatial relation composers  but uses an inverted distance measure to
score target objects 
symmetry selection by symmetry only occurred for symmetry across the horizontal centre
of the board in our data  we thus propose to mirror the landmark object across the
horizontal centre  and scoring possible targets by their inverted distance to this mirror
image 
numbered grouping we limited ourselves to groups of two and three objects  but the
same algorithm should work for higher numbers 
in group numbering a descriptive strategy like the second in the row can be understood with a slight modification to our ordering composer that can put the peak of
its exponential distribution not only at the ends and at the middle of the sequences 
but rather at arbitrary points 
connectivity a simple way to understand the lonely cone could measure distance to
the closest objects within the group of possible referents  an better solution might
construct a local connectivity graph and look for topologically isolated objects 
furthermore  we have already mentioned several areas of possible improvement to the
existing system due to faulty assumptions 
individual composers every one of our semantic composers attempts to solve a separate
hard problem  some of which  e g  grouping and spatial relations  have seen long
lines of work dedicated to more sophisticated solutions than ours  the individual
problems were not the emphasis of this paper  we believe that improvements in their
implementation will improve our system as a whole  if not as much as the following
possible techniques 
backtracking the lack of backtracking in bishop should be addressed  if a parse does
not produce a single referent  backtracking would provide an opportunity to revise
or loosen the decisions made at various stages of interpretation until a referent is
produced 
visual context semantics backtracking only solves problems in which the system knows
that it has either failed to obtain an answer  or knows that the answer it produced
is an unlikely one  however  there are numerous examples in the data where one
interpretation of an utterance produces a perfectly likely answer according to our
measurements  for example middle finds an object at the exact centre of the screen 
in many scenes this interpretation produces the correct answer  and a measurement
relative to the other objects would produce a wrong one  however  we observe that
participants only interpret middle in this way if there is no obvious structure to the
rest of the scene  if by chance the scene is divided into a group of objects on the left
and a group of objects on the right  middle will reliably refer to any isolated object
in between these to groups  even if another object is closer to the actual centre of
the screen  a future system should take into account local and global visual context
during composition to account for these human selection strategies 
   

figorniak   roy

lexical entries we have made the assumption that lexical entries are word like entities
that contain encapsulated semantic information  even in our relatively constrained
task  this is a somewhat faulty assumption  for example  the ambiguity in a word like
of is only resolved by careful design of our grammar  see section       but it may be
useful to treat entire phrases such as to the left of as single lexical entries  perhaps
with its own grammar to replace left with other spatial markers  jackendoff has
proposed absorbing most rules of syntax into the lexicon  see jackendoff        
dialogue by constructing the parse charts we obtain a rich set of partial and full syntactic
and semantic fragments offering explanations for parts of the utterance  at present 
we largely ignore this information rich resource when selecting the best referent  a
more successful approach might entail backtracking and revision as described above 
but also engage in clarification dialogue with the human speaker  the system could
use the fragments it knows about to check the validity of its interpretation  is this
the group of green ones you mean   or could simply disambiguate directly  which
of these two do you mean   followed by an explanation of its confusion in terms of
the semantic fragments it formed 
manual construction of a visually grounded lexicon as presented here will be limited in
accuracy due to various structural and parametric decisions that had to be manually approximated  machine learning algorithms may be used to learn many of the parameter settings
that were set by hand in this work  including on line learning to adapt parameters during
verbal interaction  although some thresholds and probability distribution functions may
be estimated from data using relatively straightforward methods  other learning problems
are far more challenging  for example  learning new types of composers and appropriate
corresponding grammatical constructs poses a difficult challenge for the future  minimally 
we plan to automate the creation of new versions of old composers  e g  applied to different
dimensions and attributes   moving beyond this  it is not clear how  for example  the set
handling functionality used to determine groups of referents can expand automatically and
in useful ways  it will also be interesting to study how people learn to understand novel
descriptive strategies 
we are also continuing work in applying our results from grounded language systems to
multimodal interface design  gorniak   roy         we recently demonstrated an application of the bishop system as described in this paper to the problem of referent resolution in
the graphical user interface for a  d modelling application  blender  blender foundation  
       using a bishop blender hybrid  users can select sets of objects and correct wrong
mouse selections with voice commands such as select the door behind that one or show
me all the windows 

   summary
we have presented a model of visually grounded language understanding  at the heart of
the model is a set of lexical items  each grounded in terms of visual features and grouping
properties when applied to objects in a scene  a robust parsing algorithm finds chunks
of syntactically coherent words from an input utterance  to determine the semantics of
phrases  the parser activates semantic composers that combine words to determine their
   

figrounded semantic composition for visual scenes

joint reference  the robust parser is able to process grammatically ill formed transcripts of
natural spoken utterances  in evaluations  the system selected correct objects in response
to utterances for       of the development set data  and for       of the test set data  on
clean data sets with various speech and processing errors held out  performance was higher
yet  we suggested several avenues for improving performance of the system including better
methods for spatial grouping  semantically guided backtracking during sentence processing 
the use of machine learning to replace hand construction of models  and the use of interactive
dialogue to resolve ambiguities  in the near future  we plan to transplant bishop into an
interactive conversational robot  hsiao et al          vastly improving the robots ability to
comprehend spatial language in situated spoken dialogue 

acknowledgments
thanks to ripley  newt and jones 

appendix a  utterances in the test data set
the following are the     utterances we collected as our test data set  they are presented
in the correct order and as seen by the understanding system  this means that they include
errors due to faulty speech segmentation as well as due to the algorithm that stitches oversegmented utterances back together 
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the

green cone in the middle
purple cone behind it
purple cone all the way to the left
purple cone in the corner on the right
green cone in the front
green cone in the back next to the purple cone
purple cone in the middle front
purple cone in the middle
frontmost purple cone
green cone in the corner
most obstructed green cone
purple cone hidden in the back
purple cone on the right in the rear
green cone in the front
solitary green cone
purple cone on at the front of the row of three purple cones
next cone in the row
last cone in the row
cone on the right in the pair of cones
other cone
cone closest to the middle in the front
cone in the right set of cones furthest to the left
cone at the right front
   

figorniak   roy

the cone behind that on the left
the frontmost left cone
the backmost left cone
the solitary cone
the cone on in the middle on the right
the front cone the cone
the frontmost green cone
the green cone in the front to the right of the purple cone
the green cone at the back of the row of four
the cone the green cone behind the purple cone
the purple cone behind the row of three green cones
the frontmost green cone on the right
the green cone in the corner in the back
the green cone in the back
the purple cone in the back to the right
the green cone at the front left
purple cone behind it
the purple cone behind that one
the green cone behind the purple cone
the green cone in between two purple cone
the purple cone in the front
the purple cone touching the green cone
the green cone in the front
purple cone at the left
the green cone in the back left
the purple cone in the middle in front of the other two purple cones
the purple cone to the left of the four green cones
the purple cone to the left of that leftmost
green cone
frontmost green cone
rear cone
rightmost cone
rearmost cone
the left green cone
the purple cone the green cone
the green cone
the furthestmost green cone in the exact middle
the frontmost green cone
the rightmost green cone in the clump of four green cones to the right
the green cone in front of the two purple cones near the left
the green cone between the two purple cones in the back middle
the frontmost purple cone
the leftmost of the two purple cones on the right i mean on the left 
sorry so that was the leftmost of the two purple cones on the left side
the green cone on the left halfway back
   

figrounded semantic composition for visual scenes

the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the
the

frontmost green cone in front of a purple cone
most middle purple cone
green cone on the most left to the most left
green cone in the middle in front of the other green cone
only green cone on the left
furthestmost purple cone on the left
furthest green cone
leftmost green cone
leftmost purple cone
middle green cone
green cone between the two other green cones
frontmost purple cone
backmost purple cone
green cone between the two purple cones nearest to the front
leftmost purple cone
green cone in the front
green cone
frontmost of the two back purple cones
rightmost purple cone
leftmost purple cone
purple cone in the front
last purple cone
frontmost purple cone in the clump of five purple cones
on the right
backmost green cone
the backmost purple cone
the green cone directly in front of the purple cone
the purple cone behind the green cone on the left
the green cone behind the purple cone on the left
the leftmost of the two left back corner green cones
the rightmost purple cone
the middle cone behind the frontmost purple cone
the green cone on the left front corner
the purple cone on the right back corner
the third green cone in the line of green cones near the middle
the green cone between the two purple cones near the back
the green cone in the back left
the only green cone in the back
the green cone behind the frontmost green cone
the frontmost green cone
the only green cone
the last of the line of four purple cones
the centre purple cone in the three cones on the left
the purple cone between the two purple cones
the middle purple cone
   

figorniak   roy

the
the
the
the
the
the
the
the
the
the

leftmost purple cone
middle purple cone
front left purple cone
front right purple cone
second of the four purple cones
middle purple cone
purple cone on the left
last purple cone
green one in the middle all the way at the back
purple one its all the way in the middle but a little but
to the left and all the way in the back
the green one in the middle in the front thats in front of another
green one
the purple one in the middle thats behind the green one
on the on the right the purple one at the very front of the line of
purple ones
on the left the green one in between two purple ones in a line
and on the left the purple one in the middle of a row of i mean in the middle
of a line of three purple ones
the green one on the left thats hidden by a purple one
on the left the purple one thats all the way in the corner and its separate
in the middle towards the right theres a line of purple ones and then
theres a kink in the line and the one thats right where the lines turns
the purple one all the way on the right in the front
the purple one in the front in the middle
the green one in the middle
the purple in the front all the way on the right
and the rightmost green one
the leftmost green one
and then the last green one the last green one
the frontmost purple one on the right
the purple one in the back towards the left thats next to two other
purple ones
the purple one in the back towards the right thats not part of a pair
the purple one in the front of a group on the right
the purple one in the middle thats in front of a group one the right
the purple one on the left all the way in the back
the purple one on the left thats behind another purple one
the purple one on the left thats in the front the purple one
on the left thats by itself
the purple one on the right thats hidden by two other purple ones
the purple one all the way in the back corner on the right
the purple one thats in front on the right and the last one
and the last one
the purple on in the front on the right
   

figrounded semantic composition for visual scenes

the
the
the
the
the
the

only purple one all the way on the right
green one on the right thats in the middle of a bunch
green one all the way on the left thats almost totally obscured
last purple one on the left that in a crooked line of purple ones
first purple one on the left thats in a crooked line
purple one all the way one the all the way in the back towards
the left thats behind a green and a purple one all the way in
the back
the purple one towards the back thats pretty much in the back but
thats in front of a green and purple one
the purple one in the middle in the back
the purple one on the left thats furthest to the back
the green one in the middle thats furthest to the front
the purple one towards the in the middle but towards the left
thats closest
in the middle the purple one that stands out thats closest
of the purple ones in the middle and towards the right the one
at the corner
the purple one thats closest to the middle
all the way on the right the green one in the middle of a line of three
green ones
and then all the way on the right the closest green one
all the way on the right the only close green one
the green one all the way in the right corner in the back
the purple one thats towards the back and the left corner
the purple one in the front left corner
the purple one near the middle thats not with another purple one
the purple one thats in front of another purple one
and the only purple one on the right
the only purple one on the left
the green one in the middle thats just behind another green one
and the closest green one in the middle the green one thats closest to
the middle
the green one all the way in the back towards the right
the only close green one the only one left
the only one left

appendix b
the following specifies the complete lexicon used in bishop in xml format  the initial
comment explains the attributes of lexical entries 
see online appendix file lexicon xml 

   

figorniak   roy

references
abney  s          part of speech tagging and partial parsing  in corpus based methods in
language and speech  chap     pp          kluwer academic press  dordrecht 
allen  j          natural language understanding  chap     the benjamin cummings
publishing company  inc  redwood city  ca  usa 
bailey  d          when push comes to shove  a computational model of the role of motor
control in the acquisition of action verbs  ph d  thesis  computer science division 
eecs department  university of california at berkeley 
barker  c          the dynamics of vagueness  linguistics and philosophy          
blender foundation         blender  d graphics creation suite  http   www blender d org 
brown  m   buntschuh  b     wilpon  j          sam  a perceptive spoken languageunderstanding robot  ieee transactions on systems  man and cybernetics         
         
brown schmidt  s   campana  e     tanenhaus  m  k          reference resolution in the
wild  in proceedings of the cognitive science society 
carletta  j     mellish  c          risk taking and recovery in task oriented dialogue 
journal of pragmatics            
desolneux  a   moisan  l     morel  j          a grouping principle and four applications 
ieee transactions on pattern analysis and machine intelligence                  
dhande  s          a computational model to connect gestalt perception and natural
language  masters thesis  massachusetts institure of technology 
engbers  e     smeulders  a          design considerations for generic grouping in vision 
ieee transactions on pattern analysis and machine intelligence                  
eugenio  b  d   jordan  p  w   thomason  r  h     moore  j  d          the agreement
process  an empirical investigation of human human computer mediated collaborative
dialogues  international journal of human computer studies                   
gorniak  p     roy  d          augmenting user interfaces with adaptive speech commands 
in proceedings of the international conference for multimodal interfaces 
griffin  z     bock  k          what the eyes say about speaking  psychological science 
           
haddock  n          computational models of incremental semantic interpretation  language and cognitive processes            
hsiao  k   mavridis  n     roy  d          coupling perception and simulation  steps
towards conversational robotics  in proceedings of the ieee rsj international conference on intelligent robots and systems  iros  
jackendoff  r          whats in the lexicon   in noteboom  s   weerman  f     wijnen
 eds    storage and computation in the language faculty  chap     kluwer academic
press 
   

figrounded semantic composition for visual scenes

kyburg  a     morreau  m          fitting words  vague words in context  linguistics and
philosophy             
lammens  j  m          a computational model of color perception and color naming  ph d 
thesis  state university of new york 
landau  b     jackendoff  r          what and where in spatial language and spatial
cognition  behavioural and brain sciences                 
miller  g     johnson laird  p          language and perception  harvard university press 
nagao  k     rekimoto  j          ubiquitous talker  spoken language interaction with
real world objects  in proceeding of the international joint conference on artificial
intelligence 
narayanan  s          karma  knowledge based action representations for metaphor and
aspect  ph d  thesis  univesity of california  berkeley 
partee  b  h          lexical semantics and compositionality  in gleitman  l  r     liberman  m   eds    an invitation to cognitive science  language  vol     chap      pp 
        mit press  cambridge  ma 
pechmann  t          incremental speech production and referential overspecification  linguistics            
pustejovsky  j          the generative lexicon  mit press  cambridge  ma  usa 
regier  t          the human semantic potential  mit press 
regier  t     carlson  l          grounding spatial language in perception  an empirical and computational investigation  journal of experimental psychology  general 
                
roy  d          learning visually grounded words and syntax for a scene description task 
computer speech and language         
roy  d   gorniak  p  j   mukherjee  n     juster  j          a trainable spoken language
understanding system  in proceedings of the international conference of spoken language processing 
roy  d     pentland  a          learning words from sights and sounds  a computational
model  cognitive science                 
schuler  w          using model theoretic semantic interpretation to guide statistical parsing and word recognition in a spoken language interface  in proceedings of the association for computational linguistics 
shi  j     malik  j          normalized cuts and image segmentation  ieee transactions
on pattern analysis and machine intelligence                 
siskind  j  m          grounding the lexical semantics of verbs in visual perception using
force dynamics and event logic  journal of artificial intelligence research           
wertheimer  m          laws of organization in perceptual forms  in a source book of
gestalt psychology  pp        routledge  new york 
winograd  t          procedures as a representation for data in a computer program for
understanding natural language  ph d  thesis  massachusetts institute of technology 
   

figorniak   roy

yoshida  n          utterance segmenation for spontaneous speech recognition  masters
thesis  massachusetts institute of technology 

   

fi