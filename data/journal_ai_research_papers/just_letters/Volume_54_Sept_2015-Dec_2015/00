journal artificial intelligence research               

submitted        published      

knowledge based textual inference via
parse tree transformations
roy bar haim

barhair gmail com

ido dagan

dagan cs biu ac il

computer science department  bar ilan university
ramat gan        israel

jonathan berant

yonatan cs stanford edu

computer science department  stanford university

abstract
textual inference important component many applications understanding
natural language  classical approaches textual inference rely logical representations
meaning  may regarded external natural language itself  however 
practical applications usually adopt shallower lexical lexical syntactic representations 
correspond closely language structure  many cases  approaches lack principled meaning representation inference framework  describe inference formalism
operates directly language based structures  particularly syntactic parse trees  new
trees generated applying inference rules  provide unified representation
varying types inferences  use manual automatic methods generate rules 
cover generic linguistic structures well specific lexical based inferences 
present novel packed data structure corresponding inference algorithm allows
efficient implementation formalism  proved correctness new algorithm
established efficiency analytically empirically  utility approach
illustrated two tasks  unsupervised relation extraction large corpus 
recognizing textual entailment  rte  benchmarks 

   introduction
textual inference natural language processing  nlp  concerned deriving target
meanings texts  textual entailment framework  dagan  roth  sammons   
zanzotto         reduced inferring textual statement  the hypothesis h 
source text  t   traditional approaches formal semantics perform inferences
logical forms derived text  contrast  practical nlp applications avoid
complexities logical interpretation  instead  operate shallower representations
parse trees  possibly supplemented limited semantic information named
entities  semantic roles  forth  clearly demonstrated recent pascal
recognizing textual entailment  rte  challenges  dagan  glickman    magnini      b 
bar haim  dagan  dolan  ferro  giampiccolo  magnini    szpektor        giampiccolo 
magnini  dagan    dolan        giampiccolo  trang dang  magnini  dagan    dolan 
      bentivogli  dagan  dang  giampiccolo    magnini        bentivogli  clark  dagan 
c
    
ai access foundation  rights reserved 

fibar haim  dagan   berant

dang    giampiccolo         popular framework evaluating application independent
semantic inference  
inference representations commonly made applying transformations
substitutions tree graph representing text  transformations based
available knowledge paraphrases  lexical relations synonyms hyponyms 
syntactic variations   de salvo braz  girju  punyakanok  roth    sammons 
      haghighi  ng    manning        kouylekov   magnini        harmeling        
transformations may generally viewed inference rules  available semantic knowledge bases composed manually  either experts  example wordnet
 fellbaum         large community contributors  wikipedia based
dbpedia resource  lehmann et al          knowledge bases learned automatically distributional pattern based methods  using aligned monolingual
bilingual parallel texts  lin   pantel        shinyama  sekine  sudo    grishman 
      szpektor  tanev  dagan    coppola        chklovski   pantel        bhagat  
ravichandran        ganitkevitch  van durme    callison burch         overall  applied
knowledge based inference prominent line research gained much interest  recent examples include series workshops knowledge reasoning answering
questions  saint dizier   mehta melkar        evaluation knowledge resources
recent recognizing textual entailment challenges  bentivogli et al         
many applied systems use semantic knowledge inference rules 
use typically limited  application specific  somewhat heuristic  formalizing
practices important textual inference research  analogous role well formalized
models parsing machine translation  take step direction introducing
generic inference formalism parse trees  formalism uses inference rules capture
wide variety inference knowledge simple uniform manner  specifies small
set operations suffice broadly utilize knowledge 
formalism  applying inference rule clear  intuitive interpretation generating new sentence parse  a consequent   semantically entailed source sentence 
inferred consequent may subject rule applications  on  rule applications may independent other  modifying disjoint parts source tree 
may specify mutually exclusive alternatives  e g   different synonyms source
word   deriving hypothesis text analogous proof search logic 
propositions parse trees deduction steps correspond rule applications 
nave implementation formalism would generate consequent explicitly
separate tree  however  discuss section    implementation raises
severe efficiency issues  since number consequents may grow exponentially
number possible rule applications  previous work proposed partial solutions
problem  cf  section     work present novel data structure  termed compact
forest  packed representation entailed consequents  corresponding inference
algorithm  prove new algorithm valid implementation formalism 
establish efficiency analytically  showing typical exponential to linear reduction 
empirically  showing improvement orders magnitude  together  formalism
   see  instance  listing techniques per submission provided organizers first
three challenges  dagan et al       b  bar haim et al         giampiccolo et al         

 

fiknowledge based textual inference via parse tree transformations

novel efficient inference algorithm open way large scale rule application within
well formalized framework 
based formalism inference algorithm  built inference engine
incorporates variety semantic syntactic knowledge bases  cf  section    
evaluated inference engine following tasks 
   unsupervised relation extraction large corpus  setting allows evaluation
knowledge based inferences real world distribution texts 
   recognizing textual entailment  rte   cope complex rte examples  complemented knowledge based inference engine machine learningbased entailment classifier  provides necessary approximate matching capabilities 
inference engine shown substantial contribution tasks  illustrating
utility approach 
bar haim  dagan  greental  shnarch        bar haim  berant  dagan
       described earlier versions inference framework algorithm efficient implementation  respectively  current article includes major enhancements
contributions  formalism presented detail  including
examples pseudo code algorithms  present several extensions formalism  including treatment co reference  traces long range dependencies  enhanced
modeling polarity  efficient inference algorithm presented detail 
including pseudo code  addition  provide complete proofs theorems 
establish correctness algorithm  finally  article contains extended analysis
inference component rte system  terms applicability  coverage 
correctness rule applications 

   background
section  provide background textual entailment  survey various
approaches applied task recognizing textual entailment  rte   particular 
focus use semantic knowledge within current rte systems 
    textual entailment
many semantic applications need identify meaning expressed by 
inferred from  various language expressions  example  question answering systems
need verify retrieved passage text entails selected answer  given question
john lennons widow   text yoko ono unveiled bronze statue late
husband  john lennon  complete official renaming englands liverpool airport
liverpool john lennon airport  entails expected answer yoko ono john lennons
widow     similarly  information extraction systems need validate given text
indeed entails semantic relation expected hold extracted slot fillers
 e g   x works    information retrieval queries alzheimers drug treatment 
   example taken rte   dataset  bar haim et al         
   one topics trec   ir benchmark  voorhees   harman        

 

fibar haim  dagan   berant

rephrased propositions  e g   alzheimers disease treated using drugs  
expected entailed relevant documents  selecting sentences
included summary  multi document summarization systems verify
meaning candidate sentence entailed sentences already summary 
avoid redundancy 
observation led dagan glickman propose unifying framework modeling
language variability  termed textual entailment  te   dagan   glickman         dagan
et al       b  define te follows 
say entails h if  typically  human reading would infer h
likely true  somewhat informal definition based  and assumes  common human understanding language well common background knowledge 
dagan et al         discuss te definition relation classical semantic
entailment linguistics literature  recognizing textual entailment challenges  rte  
held annually since       dagan et al       b  bar haim et al        
giampiccolo et al               bentivogli et al                formed growing research
community around task 
holy grail te research development entailment engines  used
generic modules within different semantic applications  similar current use
syntactic parsers morphological analyzers  since textual entailment defined
relation surface texts  bound particular semantic representation 
allows black box view entailment engine  input output interface
independent internal implementation  may employ different types
semantic representations inference methods 
    determining entailment
consider following  t h  pair   

h

oddest thing uae           million
people living country uae citizens 
population united arab emirates   million 

understanding h involves several inference steps  first  infer
reduced relative clause   million people living country proposition 
      million people live country 
next  observe country refers uae  rewrite    
      million people live uae 
knowing uae acronym united arab emirates  obtain 
      million people live united arab emirates 
   taken rte  test set  dagan et al       b  

 

fiknowledge based textual inference via parse tree transformations

finally paraphrase obtain h 
    population united arab emirates   million 
general  textual inference involves diverse linguistic world knowledge  including
knowledge relevant syntactic phenomena  e g   relative clauses   paraphrasing  x people
live population x    lexical knowledge  uae united arab emirates  
on  may require co reference resolution  example  substituting country uae  may think types knowledge representing inference
rules define derivation new entailed propositions consequents  work
introduce formal inference framework based inference rule application  current
discussion  however  informal notion inference rules would suffice 
example illustrates derivation h sequence inference
rule applications  procedure generally known forward chaining  finding sequence
rule applications would get us h  or close possible  thus search
problem  defined space possible rule application chains 
ideally  would base entailment engine solely trusted knowledge based
inferences  practice  however  available knowledge incomplete  full derivation h
often feasible  therefore  requiring strict knowledge based proofs likely
yield limited recall  alternatively  may back heuristic approximate
entailment classification 
next two sections survey two complementary inference types  knowledgebased inference  focus research  approximate entailment matching
classification 
    knowledge based inference
section  describe common resources inference rules         
use textual entailment systems         
      semantic knowledge resources
lexical knowledge lexical semantic relations words phrases play important role textual inference  prominent lexical resource wordnet  fellbaum 
       manually composed wide coverage lexical semantic database  following wordnet relations typically used inference  synonyms  buy purchase   antonyms  win
lose   hypernyms hyponyms  is a relations  violin musical instrument   meronyms
 part of relations  provence france  derivations meeting meet 
many researchers aimed deriving lexical relations automatically  using diverse methods sources  much automatically extracted knowledge complementary
wordnet  however  typically less accurate  snow  jurafsky  ng      a  presented
method automatically expanding wordnet new synsets  achieving high precision 
lins thesaurus  lin        based distributional similarity  recently  several works
aimed extract lexical semantic knowledge wikipedia  using metadata  well
textual definitions  kazama   torisawa        ponzetto   strube        shnarch  barak 
  dagan        lehmann et al         others   recent empirical study
 

fibar haim  dagan   berant

inferential utility common lexical resources  see work mirkin  dagan  shnarch
       
paraphrases lexical syntactic inference rules rules typically represent
entailment equivalence predicates  including correct mapping
arguments  e g   acquisition x x purchase    much work dedicated
unsupervised learning relations comparable corpora  barzilay   mckeown        barzilay   lee        pang  knight    marcu         querying web
 ravichandran   hovy        szpektor et al          local corpus  lin   pantel 
      glickman   dagan        bhagat   ravichandran        szpektor   dagan       
yates   etzioni         particular  textual entailment systems widely used
dirt resource lin pantel  common idea underlying algorithms 
predicates sharing argument instantiations likely semantically related 
nomlex plus  meyers  reeves  macleod  szekeley  zielinska    young        lexicon containing mostly nominalizations verbs  allowed argument structures  e g  
xs acquisition y ys acquisition x etc    argument mapped wordnet  amwn 
 szpektor   dagan        resource inference rules verbal nominal predicates  including argument mapping  based wordnet nomlex plus 
verified statistically intersection unary dirt algorithm  szpektor  
dagan        
syntactic transformations textual entailment often involves inference generic
syntactic phenomena passive active transformations  appositions  conjunctions  etc  
illustrated following examples 
john smiled laughed john laughed  conjunction 
neighbor  john  came john neighbor  apposition 
paper im reading interesting im reading paper  relative clause  
syntactic transformations addressed extent de salvo braz et al 
       romano  kouylekov  szpektor  dagan  lavelli         describe novel
syntactic rule base entailment  based survey relevant linguistic literature  well
extensive data analysis  sections         
      use semantic knowledge textual entailment systems
following description common knowledge sources textual inference  discuss
use knowledge textual entailment systems 
textual entailment systems usually represent h trees graphs  based
syntactic parse  predicate argument structure  various semantic relations  entailment
determined measuring well h matched  or embedded   t  estimating
distance h  commonly defined cost transforming h 
next section  briefly cover various methods proposed approximate
matching heuristic transformations graphs trees  role semantic knowledge
general scheme bridge gaps h stem language
variability  example  applying lexical semantic rule purchase buy allows
matching word buy appearing h word purchase appearing t 
 

fiknowledge based textual inference via parse tree transformations

rte systems restrict type allowed inference rules search space 
systems based lexical  word based phrase based  matching h  haghighi et al  
      maccartney  galley    manning        heuristic transformation h
 kouylekov   magnini        harmeling        typically apply lexical rules  without
variables   sides rule matched directly h 
hickl        derived given  t  h  pair small set consequents terms
discourse commitments  commitments generated several different tools
techniques  based syntax  conjunctions  appositions  relative clauses  etc    co reference 
predicate argument structure  extraction certain relations  paraphrase acquisition
web  pairs commitments derived h fed next stages
rte system lexical alignment entailment classification  prior commitment
generation  several linguistic preprocessing modules applied text  including
syntactic dependency parsing  semantic dependency parsing  named entity recognition 
co reference resolution  hickl employed probabilistic finite state transducer  fst  based
extraction framework commitment generation  extraction rules modeled
series weighted regular expressions  commitments textual form fed
back system  additional commitments generated 
de salvo braz et al         first incorporate syntactic semantic inference
rules comprehensive entailment system  system  inference rules applied
hybrid syntactic semantic structures called concept graphs  left hand side  lhs 
rule matched concept graph  graph augmented instantiation
right hand side  rhs  rule  several iterations rule application 
system attempts embed hypothesis augmented graph  types semantic
knowledge  verb normalization lexical substitutions  applied either
rule application  at preprocessing time  rule application  part hypothesis
subsumption  embedding  
several entailment systems based logical inference  bos markert             
represented h drs structures used discourse representation theory  kamp  
reyle         translated first order logic  background knowledge
 bk  encoded axioms  comprised lexical relations wordnet  geographical
knowledge  small set manually composed axioms encoding generic knowledge 
bos markert used logic theorem prover find proof entails h  alone
together background knowledge bk   h inconsistent
 implying non entailment  background knowledge  logic prover
complemented model builder aimed find counter examples  e g   model
h holds   logical inference system suffered low coverage  due limited
background knowledge available  able find proofs small fraction
rte  dataset  therefore  rte system bos markert combined logical inference
shallow approximate matching method  based mainly word overlap 
lccs logic based entailment system  tatu   moldovan        one top performers rte  rte   tatu  iles  slavick  novischi    moldovan        tatu  
moldovan         based proprietary tools deriving rich semantic representations  extensive knowledge engineering  syntactic parses h
transformed logic forms  moldovan   rus         representation enriched
variety relations extracted semantic parser  well named entities
 

fibar haim  dagan   berant

temporal relations  inference knowledge included on demand axioms based extended
wordnet lexical chains  wordnet glosses  nlp rewrite rules  additional knowledge
types included several hundreds world knowledge axioms  temporal axioms  semantic composition axioms  e g   encoding transitivity kinship relation   based
rich semantic representation extensive set axioms  theorem prover aimed
prove refutation entails h  proof failed  h repeatedly simplified
proof found  reducing proof score simplification 
    approximate entailment classification
semantic knowledge always incomplete  therefore  cases  knowledge based inference must complemented approximate  heuristic methods determining entailment  rte systems employ limited amount semantic knowledge 
focus methods approximate entailment classification  common architecture
rte systems  hickl  bensley  williams  roberts  rink    shi        snow  vanderwende 
  menezes      b  maccartney  grenager  de marneffe  cer    manning        comprises
following stages 
   linguistic processing  includes syntactic  and possibly semantic  parsing  namedentity recognition  co reference resolution  etc  often  h represented trees
graphs  nodes correspond words edges represent relations
words 
   alignment  find best mapping h nodes nodes  taking account
node edge matching 
   entailment classification  based alignment found  set features extracted
passed classifier determining entailment  features measure
alignment quality  try detect cues false entailment  example 
node h negated aligned node negated  may indicate false
entailment 
alternative approach aims transform text hypothesis  rather
aligning them  kouylekov magnini        applied tree edit distance algorithm
textual entailment  edit operation  node insertion deletion substitution  assigned
cost  algorithm aims find minimum cost sequence operations transform
h  mehdad magnini      b  proposed method estimating cost
edit operation based particle swarm optimization  wang manning       
presented probabilistic tree edit approach models edit operations using structured
latent variables  tree edits represented state transitions finite state machine
 fsm   model parameterized conditional random field  crf   harmeling
       developed probabilistic transformation based approach  defined fixed set
operations  including syntactic transformations  wordnet based substitutions 
heuristic transformations adding removing verb noun  probability
transformation estimated development set  similarly  heilman smith
       classify entailment based sequence edits transforming h  employ
generic edit operations greedy search heuristic  guided cost function
measures remaining distance h using tree kernel 
 

fiknowledge based textual inference via parse tree transformations

zanzotto  pennacchiotti  moschitti        aimed classify given  t  h  pair
analogy similar pairs training set  method based finding intra pair
alignment  i e   h  capturing transformation h  interpair alignment  capturing analogy new pair  t  h  previously seen
pair  t    h     cross pair similarity kernel computed  based tree kernel similarity
applied aligned texts aligned hypotheses  another cross pair similarity kernel
proposed wang neumann         extracted tree skeletons h 
consisting left right spines  defined unlexicalized paths starting root 
found sections h spines differ compared sections across pairs
using subsequence kernel 

   research goal
goal textual entailment research develop entailment engines used
generic inference components within various text understanding applications  logic based
entailment systems provide formalized expressive framework textual inference 
however  deriving logic representations text complex task  available tools
match accuracy robustness current syntactic parsers  which often basis
semantic parsing   furthermore  interpretation logic forms often unnecessary 
many common inferences modeled shallower representations 
follows textual entailment systems  and text understanding applications
general  operate lexical syntactic representations  possibly supplemented
partial semantic annotation  however  unlike logic based approaches  systems
lack clear  unified formalism knowledge representation inference  instead
employ multiple representations inference mechanisms  notable exception
natural logic framework maccartney manning         rather different
focus current work  discuss section   
work  develop well formalized entailment approach lexical syntactic
level  formalism models wide variety inference rules composition  based
unified representation small set inference operations  moreover  present
efficient implementation formalism using novel data structure algorithm
allow compact representation proof search space 
see contribution work practical theoretical  practical
 or engineering  perspective  formalism may simplify development entailment
systems  number representations inference mechanisms need dealt
minimal  furthermore  efficient implementation may allow entailment engines
explore much larger search spaces  theoretical perspective  concise  formal modeling
leads better insight phenomenon investigation  particular 
formal model entailment engine makes possible apply formal methods investigating properties  enabled us prove correctness efficient implementation
formalism  cf  appendix a   next present inference formalism 
 

fibar haim  dagan   berant

rule
type
syntactic

sources

examples

manually composed

lexical

learned unsupervised algorithms  dirt  tease  
derived automatically integrating information wordnet
nomlex  verified using corpus
statistics  amwn 
wordnet  wikipedia

passive active  apposition  relative
clause  conjunctions
xs wife  x married

syntactic

lexical

x bought sold x

x maker x produces
steal take  albanianalbania
janis joplinsinger
amazonsouth america

table    representing diverse knowledge types inference rules

   inference formalism parse trees
previous sections highlighted need principled  well formalized approach
textual inference lexical syntactic level  section  propose step towards
filling gap  defining formalism textual inference parse based representations  semantic knowledge required inference represented inference rules 
encode parse tree transformations  rule application generates new consequent sentence  represented parse tree  source tree  figure  b shows sample inference
rule  representing passive to active transformation 
knowledge representation usage perspective  inference rules provide simple
unifying formalism representing applying broad range inference knowledge 
examples breadth illustrated table    knowledge acquisition
perspective  representing inference rules lexical syntactic level allows easy incorporation rules learned unsupervised methods  important scaling inference
systems  interpretation stipulated semantic representations  often difficult
inherently supervised semantic task learning  circumvented altogether 
historical machine translation perspective  approach similar transfer based translation  contrasted semantic interpretation interlingua  overall research goal
explore reach inference approach  identify scope
semantic interpretation may needed 
given syntactically parsed source text set inference rules  formalism
defines set consequents derivable text using rules  consequent
obtained sequence rule applications  generating intermediate parse
tree  similar proof process logic  addition  new consequents may inferred based
co reference relations identified traces  formalism includes annotation rules
add features existing trees  according formalism  text entails hypothesis
h h consequent t 
rest section  define illustrate formalism components 
sentence representation  section       inference rules application  sections    
      inference based co reference relations traces  section       annotation
  

fiknowledge based textual inference via parse tree transformations

input  source tree   rule e   l r
output  set derived trees
set matches l

f
l subtree matched l according match f
   r instantiation
r copy r
variable v r
instantiate v f  v 
aligned pair nodes ul l ur r
daughter ul
  l
copy subtree rooted ur r  dependency relation
   derived tree generation
substitution rule
copy l  and descendants nodes  replaced r
else    introduction rule
dr
add

algorithm    applying rule tree
rules  section       components form inference process specifies set
inferable consequents given text set rules  section       section     extends
hypothesis definition  allowing h template rather proposition  finally 
section     discusses limitations possible extensions formalism 
    sentence representation
assume sentences represented form parse trees  work  focus
dependency tree representation  often preferred directly capture predicateargument relations  two dependency trees shown figure  a  nodes represent words
hold set features values  features include word lemma
part of speech  additional features may added inference process 
edges annotated dependency relations 
    inference rules
entailment  or inference  rule l r primarily composed two templates  lefthand side  lhs  l right hand side  rhs  r  templates dependency subtrees 
may contain pos tagged variables  matching lemma  figure   shows passiveto active transformation rule  illustrates application 
rule application procedure given algorithm    rule application generates set
derived trees  consequents  source tree steps described below 
  

fibar haim  dagan   berant

root




rain verb

expletive

r

wha



 

adj


r

mary noun
mod

see verb

obj

q


mod

bysubj



verb

prep

 

yesterday noun

pcompn


little adj

john noun

source  rained little mary seen john yesterday 

root




rain verb

r

expletive

wha



 

adj



subj

r

john noun

see verb
obj

mod

 

mary noun yesterday noun
mod



little adj
derived  rained john saw little mary yesterday 

 a  passive to active tree transformation


v verb
obj

l

u

n  noun

v verb

bysubj


subj

obj



 

u

 

verb

prep

n  noun

n  noun

pcompn

r



n  noun
 b  passive active substitution rule 
figure    application inference rule  pos relation labels based minipar
 lin         n    n   v variables  whose instances l r implicitly aligned 
by subj dependency relation indicates passive sentence 

  

fiknowledge based textual inference via parse tree transformations

root

root









v  verb v  verb
l



wha

r

adj




v  verb
figure    temporal clausal modifier extraction  introduction rule 

      l matching
first  matches l source tree sought  l matched exists
one to one node mapping function f l s  that 
   node u l  f  u  features feature values u  variables
match lemma value f  u  
   edge u v l  edge f  u  f  v  s  dependency
relation 
matching fails  rule applicable s  example  variable v matched
verb see  n   matched mary n   matched john  matching succeeds 
following performed match found 
      r instantiation
copy r generated variables instantiated according matching node
l  addition  rule may specify alignments  defined partial function l nodes
r nodes  alignment indicates modifier source node
part rule structure  subtree rooted copied modifier
target node  addition explicitly defining alignments  variable l implicitly
aligned counterpart r  example  alignment v nodes implies
yesterday  modifying see  copied generated sentence  similarly
little  modifying mary  copied n   
      derived tree generation
let r instantiated r  along descendants copied l alignment 
l subtree matched l  formalism two methods generating
derived tree d  substitution introduction  specified rule type  substitution
rules specify modification subtree s  leaving rest unchanged  thus 
formed copying replacing l  and descendants ls nodes  r 
case passive rule  well lexical rules buy purchase 
contrast  introduction rules used make inferences subtree s 
parts ignored affect d  typical example inferring proposition
embedded relative clause s  case  derived tree simply taken
  

fibar haim  dagan   berant

root


root




buy verb
subj



purchase verb

obj

subj

obj

v

 

v

 

john noun

books noun

john noun

books noun

john bought books 

l

buy verb

john purchased books 



purchase verb

r

figure    application lexical substitution rule  dotted arc represents explicit
alignment 

r  figure   presents rule  enables deriving propositions embedded
within temporal modifiers  note derived tree depend main clause 
applying rule right part figure  a yields proposition john saw little
mary yesterday 
    examples rule application
section illustrate rule representation application additional
examples 
      lexical substitution rule explicit alignment
figure   shows derivation consequent john purchased books sentence
john bought books using lexical substitution rule buy purchase  example
illustrates role explicit alignment  since buy purchase variables 
implicitly aligned  however  need aligned explicitly  otherwise daughters
buy would copied purchase 
      lexical syntactic introduction rule
figure   illustrates application lexical syntactic rule  derives sentence
husband died knew late husband  defined introduction rule  since
resulting tree derived based solely phrase late husband  ignoring
rest source tree  example illustrates leaf variable l  variable
leaf node  may become non leaf r vice versa  alignment
instances variable n  matched husband   allows copying modifier   recall
alignments defined implicitly formalism   note correctness
rule application may depend context applied  instance 
rule example correct late meaning longer alive given
context  discuss context sensitivity rule application section     
  

fiknowledge based textual inference via parse tree transformations

root

root







know verb
subj

die verb

obj

subj

v

 

noun

husband noun
gen




husband noun

mod

v

 

noun

late adj

gen



noun

knew late husband 

husband died 

root


l



n noun

die verb



subj

mod

late adj



r

n noun

figure    application lexical syntactic introduction rule

    co reference trace based inference
aside primary inference mechanism rule application  formalism allows
inference based co reference relations long distance dependencies  view coreference equivalence relation complete subtrees  either within tree
different trees  linked co reference chain  practice  relations
obtained external co reference resolution tool  part text pre processing 
co reference substitution operation similar application substitution rule 
given pair co referring subtrees  t  t    derived tree generated copying
tree containing t    replacing t  t    operation symmetrically
applicable t     example  given sentences  my brother  musician   he  plays
drums  infer brother plays drums 
long distance dependencies another type useful relation inference  illustrated following examples 
    relative clause  boyi  i saw ti   went home 
  saw boy  
    control verbs  johni managed  ti open door  
  john opened door  
   view co referring expressions substitutional found seminal paper van
deemter kibble         noun phrases shown non substitutable evidence
co referring 

  

fibar haim  dagan   berant

    verbal conjunction   johni sang   ti danced  
  john danced  
parsers including minipar  use current work  recognize annotate
long distance dependencies  instance  minipar generates node representing
trace  ti examples   holds pointer antecedent  e g   johni      
shown examples  inference sentences may involve resolving long  distance
dependencies  traces substituted antecedent  thus  generalize
co reference substitution operate trace antecedent pairs  well  mechanism
works together inference rule application  instance  substituting trace
antecedent     obtain john managed  john opened door  
apply introduction rule n managed extract embedded clause john
opened door 
    polarity annotation rules
addition inference rules  formalism implementation includes mechanism
adding semantic features parse tree nodes  however  many cases natural
way define semantic features classes  hence  often difficult agree right
set semantic annotations  a common example definition word senses  
approach  aim keep semantic annotation minimum  sticking lexicalsyntactic representation  widely agreed schemes exist 
consequently  semantic annotation employ predicate polarity  feature
marks truth predicate  may take one following values  positive    
negative    unknown     examples polarity annotation shown below 
    john called    mary 
    john hasnt called   mary yet 
    john forgot call   mary 
    john might called    mary 
    john wanted call    mary 
sentences         entail john didnt call mary  hence negative annotation
call  contrast  truth john called mary cannot determined         
therefore predicate call marked unknown  general  polarity predicates
may affected existence modals  negation  conditionals  certain verbs  etc 
technically  annotation rules right hand side r  rather node l
may contain annotation features  l matched tree  annotations contains
copied matched nodes  figure   shows example annotation rule application 
predicates assumed positive polarity default  polarity rules used
mark negative unknown polarity  one rule applies predicate
 as sentence john forgot call mary   may applied order 
following simple calculus employed combine current polarity new polarity 
  

fiknowledge based textual inference via parse tree transformations

root


v  
l





listen  
subj

verb

verb





v

 

verb

john noun

verb

neg

neg



adj



adj
john listening    

 a  annotation rule

 b  annotated sentence

figure    application annotation rule  a   marking predicate listen negative
polarity  b 

current polarity
 

 
     

new polarity



 

result

 
 
 

annotation rules used detecting polarity mismatches text hypothesis  incompatible polarity would block hypothesis matched text 
case approximate entailment classification  polarity mismatches detected
annotation rules used features classifier  discuss section     
addition  existence polarity annotation features may prevent inappropriate inference
rule applications  blocking l matching  discuss section     
    inference process
let set dependency trees representing text  along co reference
trace information  let h dependency tree representing hypothesis  let r
collection inference rules  including inference polarity rules   based
previously defined components inference framework  next give procedural
definition set trees inferable using r  denoted i t  r   inference
process comprises following steps 
   initialize i t  r   
   apply matching polarity rules r trees i t  r   cf  section      
   replace trace nodes copy antecedent subtree  cf  section      
   add i t  r  trees derivable co reference substitution  cf  section      
  

fibar haim  dagan   berant

   apply matching inference rules r trees i t  r   cf  section      
add derived trees i t  r   repeat step iteratively newly added
trees  new trees added 
steps     performed h well   h inferable using r h i t  r  
since i t  r  may infinite large  practical implementation process must
limit search space  example restricting number iterations applied
rules iteration 
inference rule applied  polarity annotation propagated source
tree derived tree follows  first  nodes copied retain original
polarity  second  node gets polarity aligned node s 
    template hypotheses
many applications useful allow hypothesis h template rather
proposition  is  contain variables  variables case existentially quantified  entails h exists proposition h    obtained h variable instantiation 
entails h    variable x instantiated  replaced  subtree sx   x
modifiers h  i e   x leaf   become modifiers sx root  obtained
variable instantiations may stand answers sought questions slots filled relation extraction  example  applying framework question answering setting 
question killed kennedy  may transformed hypothesis x killed kennedy 
successful proof h sentence assassination kennedy oswald shook
nation would instantiate x oswald  providing sought answer 
    limitations possible extensions
conclude section discussing limitations presented inference formalism 
well possible extensions address limitations  first  inference rules match
single subtree  therefore less expressive logic axioms used bos
markert        tatu moldovan         may combine several predicates
originating text representation well background knowledge 
allows logic based systems make inferences combine multiple pieces information 
instance  text says person x lives city   background knowledge
tells us city country z  infer x lives country z  using
rule person x  location y  location z  live x y  in y z  live x z   
schoenmackers  etzioni  weld  davis        describe system acquires rules
 first order horn clauses  web text  allowing rules match multiple subtrees
t  well information background knowledge  seems plausible future extension
formalism 
another limitation formalism lack context disambiguation  word sense
mismatch potential cause incorrect rule applications  example  rule hit
score applied correctly          
   step   applied h since hypothesis typically short  simple sentence usually
include co referring nps  moreover  presented formalism h single tree  applying co referencebased inference would resulted additional trees inferred h  thus would required
extending formalism accordingly 

  

fiknowledge based textual inference via parse tree transformations

    team hit home run  team scored home run 
     car hit tree    car scored tree 
several works past years addressed problem context dependent rule application  dagan  glickman  gliozzo  marmorshtein    strapparava      a  pantel  bhagat 
coppola  chklovski    hovy        connor   roth        szpektor  dagan  bar haim   
goldberger        dinu   lapata        ritter  mausam    etzioni        berant  dagan 
  goldberger        melamud  berant  dagan  goldberger    szpektor         szpektor
et al         proposed comprehensive framework modeling context matching  termed
contextual preferences  cp   given text t  hypothesis h  possibly template hypothesis  inference rule r bridging h  objects annotated
two context components   a  global  topical  context   b  preferences constraints instantiation objects variables  for r template h   cp requires
h r matched t  h matched r    context component
matched counterpart  szpektor et al  proposed concrete implementations
components  example  could model global context
r sets content words  compute semantic relatedness
two sets  using methods latent semantic analysis  lsa   deerwester  dumais 
furnas  landauer    harshman         explicit semantic analysis  esa   gabrilovich
  markovitch         would expect semantic relatedness  score 
 team  home run  much higher  score   car  tree   would
permit inference          
rte systems  including system rte experiments  described
section      lexicalized rules bridge h directly  rules lhs
rhs matched h  respectively  since rte benchmarks h tend
semantic context  setting alleviates context matching problems
extent  however  analysis  presented later work  subsection         shows
context matching remains issue even setting  expected become even
important chaining lexicalized rules attempted  adding contextual preferences
formalism important direction future work 
validity rule application depends monotonicity properties application site  instance  hypernym rule poodle dog applicable upward
monotone contexts  monotonicity may affected presence quantifiers  negation  certain verbs implicatives counterfactives  nairn  condoravdi   
karttunen         common textual entailment systems  assume upward monotonicity anywhere  assumption usually holds true  cases may lead
incorrect inferences  following examples show correct applications rule
upward monotone contexts              incorrect applications downward monotone
contexts                  
     bought poodle  bought dog 
     didnt buy poodle   didnt buy dog
     poodles smart    dogs smart 
   context matching  textual entailment  directional relation 

  

fibar haim  dagan   berant

     failed avoid buying poodle failed avoid buying dog 
     fail avoid buying poodle   fail avoid buying dog 
maccartney manning        address monotonicity well semantic relations
exclusion  natural logic framework based syntactic representation 
discuss work detail section   
finally  since polarity annotation rules applied locally  may fail complex
cases  computing polarity buying sentences            polarity
information need propagated along syntactic structure sentence 
truthteller system  lotan  stern    dagan         computes predicate polarity  truth
value  combination annotation rules global polarity propagation algorithm 
extending previous work nairn et al         maccartney manning        
    summary
section  presented well formalized approach textual inference parsebased representations  core paper  framework  semantic knowledge
represented uniformly inference rules specifying tree transformations  provided
detailed definitions representation rules well inference mechanisms
apply them  formalism models inferences based co reference relations
traces  addition  includes annotation rules used detect contexts affecting
polarity predicates  next section present efficient implementation
formalism 

   compact forest scalable inference
according formalism  rule application generates new sentence parse  a consequent   semantically entailed source sentence  inferred consequent may
subject rule applications  on  straightforward implementation
formalism would generate consequent separate tree  unfortunately  nave
approach raises severe efficiency issues  since number consequents may grow exponentially number rule applications  consider  example  sentence children
fond candies  following rules  childrenkids  candiessweets  x
fond yx likes y  number derivable sentences  including source sentence 
would     the power set size   rule either applied not  independently 
found exponential explosion leads poor scalability nave implementation
approach practice 
intuitively  would rule application add entailed part rule
 e g   kids  packed sentence representation  yet  still want resulting structure
represent set entailed sentences  rather mixture sentence fragments
unclear semantics  discussed section    previous work proposed partial solutions
problem 
section  introduce novel data structure  termed compact forest  corresponding inference algorithm  efficiently generate represent consequents
preserving identity individual one  data structure allows compact representation large set inferred trees  rule application generates explicitly
  

fiknowledge based textual inference via parse tree transformations

nodes rules right hand side  rest consequent tree shared source
sentence  reduces number redundant rule applications  explained later
section  show representation based primarily disjunction edges 
extension dependency edges specify set alternative edges multiple trees 
since follow well defined inference formalism  able prove inference
operations formalism equivalently applied compact forest  compare
inference cost compact forests explicit consequent generation theoretically 
illustrating exponential to linear complexity ratio  empirically  showing improvement
orders magnitude  empirical results reported section      
    compact forest data structure
compact forest f represents set dependency trees  figure  d shows example
compact forest containing trees sentences little mary seen john yesterday
john saw little mary yesterday  first define general data structure
directed graphs  narrow definition case trees 
compact directed graph  cdg  pair g    v  e  v set nodes e
set disjunction edges  d edges   let set dependency relations  d edge
triple  sd   reld   td    sd td disjoint sets source nodes target
nodes  reld   sd function specifying dependency relation corresponds
source node  graphically  d edges shown point nodes  incoming edges
source nodes outgoing edges target nodes  instance  let bottommost
d edge figure    sd    of  like   td    candy  sweet   rel of     pcomp n 
rel like    obj  
d edge represents  si sd   set alternative directed edges   si   tj     tj
td    labeled relation given reld  si    edges 
termed embedded edge  e edge   would correspond different graph represented g 
obj

obj

pcompn

previous example  e edges likecandy  likesweet  ofcandy
pcompn
ofsweet  the definition implies source nodes sd set
alternative target nodes td    d edge called outgoing d edge node v v sd
incoming d edge v v td   compact directed acyclic graph  cdag 
cdg contains cycles e edges 
dag g rooted node v v cdag g embedded g derived
follows  initialize g v alone  then  expand v choosing exactly one target
node td outgoing d edge v  adding corresponding e edge
 v  t  g  expansion process repeated recursively new node added g 
set choices results different dag v root  figure  d 
may choose connect root either left see  resulting source passive
sentence  right see  resulting derived active sentence 
compact forest f cdag single root r  i e   r incoming d edges 
embedded dags rooted r trees  set trees  termed embedded
trees  denoted  f  comprise set trees represented f 
figure   shows another example compact forest efficiently representing    sentences resulting three independently applied rules presented beginning
section 
  

fibar haim  dagan   berant

root

root





see

v

by subj obj





mary

pcomp n

john

see

mod



by subj obj

yesterday



mod

pcomp n

little

mod



yesterday

little

 b  variable instantiation

root

root





see
obj



mod

john

 a  right hand side generation

by subj

mary

see

see

see



by subj

mod mod

see
mod

mod

obj

obj

subj


pcomp n

john

mary



yesterday



mod



yesterday
pcomp n

little

john

 c  alignment sharing

mary
mod

little

 d  dual leaf variable sharing

figure    step by step construction compact forest containing source sentence little mary seen john yesterday sentence john saw little mary
yesterday derived via application passive rule figure  b  parts
speech omitted 

    inference process
next describe algorithm implementing inference process described section    
compact forest  henceforth  compact inference   illustrated figures  b  the
passive to active rule    
  

fiknowledge based textual inference via parse tree transformations

root



pred

fond



mod subj

subj
obj



child

kid
pcomp n

candy

sweet

figure    compact forest representing    sentences derivable sentence children fond candies using following three rules  childrenkids  candiessweets 
x fond yx likes y 

      forest initialization
f initialized set dependency trees representing text sentences 
roots connected forest root target nodes single d edge  dependency
edges transformed trivially d edges single source target  annotation
rules applied stage initial f  figure  a  without node labeled v
incoming edge  corresponds initial forest  containing single sentence
example  
      inference rule application
inference rule application comprises steps described below  summarized
algorithm   
l matching first find matches rules lhs l forest f  line    
sake brevity  omitted technical details l matching implementation
pseudocode algorithm    following high level description matching
procedure  focusing key algorithmic points 
l matched f exists embedded tree f l matched
t  section      denote l subtree l matched  line    
  

fibar haim  dagan   berant

input  compact forest f   inference rule e   l r
output  modified f  denoted f      f        f  d  set trees derived
applying e subset ls matches trees  f 
   set matches l f
   match f
  
l subtree f l matched according f
  
  
  
  
  
  
   
   
   
   
   

   right hand side generation
sr copy r excluding dual leaf variable nodes
add sr f
sl l excluding dual leaf variable nodes
rr root sr  
rl root l 
e substitution rule
incoming d edge rl    set sr alternative sl
else    introduction rule
outgoing d edge root f     set sr alternative trees  f 
add rr td

   
   
   
   
   

   variable instantiation
variable x held node xr sr    rs variables excluding dual leaves
x leaf l
xl f  x     node sl matched x
 xr  lemma  xr  polarity   xl  lemma  xl  polarity 

   
   
   
   
   
   
   

else    x leaf l matched whole target node set
 xr  lemma  xr  polarity   n lemma  n polarity  node n f  x 
n  f  x   n     n
generate substitution rule n n  n n  aligned  apply xr
x r instantiation n 
u sl u aligned xr
add alignment u x r

   
   
   
   
   
   

   alignment sharing
aligned pair nodes nl sl nr sr
nr  polarity nl  polarity
outgoing d edge nl whose e edges part sl
add nr sd
reld  nr   reld  nl  

   
   
   
   

   dual leaf variable sharing
dual leaf variable x matched node v l
incoming d edge v
p parent node x sr

   
   
   
   
   

   go p alternatives p generated variable instantiation
p set target nodes ps incoming d edge
p  p
add p  sd
reld  p    relation x p

algorithm    applying inference rule compact forest

  

fiknowledge based textual inference via parse tree transformations

subtree may shared multiple trees represented f  case rule
applied simultaneously trees  section      match example
 v  n    n     see  mary  john   definition allow l scattered
multiple embedded trees  matches constructed incrementally  aiming add ls nodes
one one partial matches constructed far  verifying candidate node
f node content corresponding edge labels match  verified
match contain one e edge d edge  nodes f
indexed using hash table enable fast lookup 
target nodes d edge specify alternatives position tree 
parts of speech expected substitutable  assume target nodes
d edge part of speech  polarity  consequently  variables
leaves l may match certain target node d edge mapped whole
set target nodes td rather single node  yields compact representation
multiple matches  prevents redundant rule applications  instance  given compact
representation  children kids  fond  candies sweets   cf  figure     rule x
fond yx likes matched applied once  rather four times  for
combination matching x   
right hand side generation given inference rule l r  define dual leaf
variable variable leaf l r  example  n   n  
dual leaf variables passive to active rule figure  b  variables
node r  and hence root leaf   variables additional
alignments  other implicit alignment occurrences l r 
considered dual leaves  explained below  instantiations dual leaf variables
shared source target trees 
right hand side generation step  template sr  line     consisting r
excluding dual leaf variables  generated inserted f  line     example 
sr includes node v passive rules rhs  similarly  define sl l
excluding dual leaf variables  line    
case substitution rule  as example   sr set alternative sl
adding sr root td   incoming d edge sl root  line      case
introduction rule  set alternative trees forest adding
sr root target node set forest roots outgoing d edge  line      figure  a
illustrates results step example  sr gray node labeled
variable v   becomes additional target node d edge entering original
 left  see 
variable instantiation variable sr  i e   non dual leaf  instantiated  lines
       according match l  as section       example  v instantiated
see  figure  b  lines         specified above  variable sr leaf l  which
case example  matched set nodes 
instantiated sr  lines         decomposed sequence simpler
operations  first  sr instantiated representative set  line     
apply ad hoc lexical substitution rules creating new node additional node
   case current implementation  based coarse tag set minipar 

  

fibar haim  dagan   berant

set  line         nodes  addition usual alignment source nodes
sl  lines         share daughters sr  due alignment n
n    defined line     
alignment sharing modifiers aligned nodes shared  rather copied  follows 
given node nl sl aligned node nr sr   outgoing d edge nl
part l  share nl nr adding nr sd setting
reld  nr     reld  nl    lines         example  figure  c   aligned nodes nl
nr left right see nodes  respectively  shared modifier yesterday 
dependency relation mod copied right see node  copy polarity annotation
nl nr  line     
note point instantiation variables dual leaves cannot
shared typically different modifiers two sides rule  yet 
modifiers  part rule  shared alignment operation
 recall common variables always considered aligned   dual leaf variables 
hand  might shared  described next  since rule doesnt specify modifiers
them 
dual leaf variable sharing final step  lines        performed similarly
alignment sharing  suppose dual leaf variable x matched node v l whose
incoming d edge d  simply add parent p x sr sd set reld  p 
relation p x  in r   since v shared  modifiers become shared
well  implicitly implementing alignment operation  subtrees little mary john
shared way variables n   n    figure  d   ad hoc substitution rules
applied p variable instantiation phase  generated nodes serve alternative
parents x  thus sharing procedure applied p repeated them 
applying rule example added single node linked four d edges 
compared duplicating whole tree explicit inference 
      co reference substitution
section     defined co reference substitution  inference operation allows replacing subtree t  co referring subtree t    operation implemented generating
on the fly substitution rule t  t  applying t    implementation 
initial compact forest annotated co reference relations obtained external
co reference resolution tool  substitutions performed prior rule applications 
substitutions t  pronoun ignored  usually useful 
    correctness
section  present two theorems proving inference process presented
valid implementation inference formalism  provide full proofs appendix a 
theorem    argue applying rule compact forest results compact
forest  since begin valid compact forest created initialization step  follows
induction sequence rule applications result inference process
compact forest  fact embedded dags generated inference
process indeed trees trivial  since nodes generally many incoming e edges
  

fiknowledge based textual inference via parse tree transformations

many nodes  however  show pair parent nodes cannot part
embedded dag  example  figure    node candy incoming
e edge node node   however  nodes
part embedded dag  d edge emanating root
forces us choose node node be  thus  see reason
correctness local  two incoming e edges leaf node candies cannot
embedded dag rule applied root tree  turn
theorem proof scheme 
theorem   applying rule compact forest results compact forest 
proof scheme prove applying rule compact forest creates cycle
embedded dag tree  cycle non tree dag already existed
prior rule application  contradicts assumption original structure
compact forest  crucial observation proof directed path
node u node v passes sr   u v outside sr  
analogous path u v passes sl instead 
next theorem main result  argue inference process compact
forest complete sound  is  generates exactly set consequents derivable
text according inference formalism 
theorem   given rule base r set initial trees   tree represented
compact forest derivable inference process consequent according
inference formalism 
proof scheme first show completeness induction number explicit rule
applications  let tn   tree derived tree tn using rule rn according
inference formalism  inductive assumption determines tn embedded
derivable compact forest f  easy verify applying rn f yield compact
forest f   tn   embedded 
next  show soundness induction number rule applications
compact forest  let tn   tree represented derived compact forest fn    tn  
 f n       fn   derived compact forest fn   using rule rn   inductive
assertion states trees  f n   consequents according formalism 
hence  tn   already  f n   consequent   otherwise  shown
exists tree tn  f n   applying rn tn yield tn   according
formalism  tn consequent according inductive assertion therefore
tn   consequent well 
two theorems guarantee compact inference process valid  is 
yields compact forest represents exactly set consequents derivable given
text given rule set 
  

fibar haim  dagan   berant

    complexity
section  explain compact inference exponentially reduces time space
complexity typical scenarios 
consider set rule matches tree independent matched left handsides  excluding dual leaf variables  overlap   application
chained order  example  three rule matches presented figure  
independent 
let us consider explicit inference first  assume start single tree k
independent rules matched  applying k rules yield  k trees  since subset
rules might applied   therefore  time space complexity applying k
independent rule matches   k    applying rules newly derived consequents
behaves similar manner 
next  examine compact inference  applying rule using compact inference adds
right hand side rule shares existing d edges  since size
right hand side number outgoing d edges per node practically bounded
low constants  applying k rules tree yields linear increase size forest 
thus  resulting size o  t     k   see figure   
time complexity rule application composed matching rule forest
applying matched rule  applying matched rule linear size  matching
rule size r forest f takes o  f r   time even performing exhaustive
search matches forest  since r tends quite small bounded
low constant    already gives polynomial time complexity  furthermore  matches
constructed incrementally  step aim extend partial matches found 
due typical low connectivity forest  well various constraints imposed
rule  lemma  pos  dependency relation   number candidates extending
matches step     f   candidates retrieved efficiently using
proper indexing  thus  matching procedure fast practice  illustrated
empirical evaluation described section     
    related work packed representations
packed representations various nlp tasks share common principles  underlie
compact forest  factoring common substructures representing choice local
disjunctions  applying general scheme individual problems typically requires specific representations algorithms  depending type alternatives
represented specified operations creating them  create alternatives rule
application  newly derived subtree set alternative existing subtrees 
alternatives specified locally using d edges 
packed chart representations parse forests introduced classical parsing algorithms cyk earley  jurafsky   martin         extended later
work various purposes  maxwell iii   kaplan        kay         alternatives
parse chart stem syntactic ambiguities  specified locally possible decompositions phrase sub phrases 
   rte system  average rule lhs size found   nodes  maximal size  
nodes  experimental setting described section        applied rte  test set 

  

fiknowledge based textual inference via parse tree transformations

packed representations utilized transfer based machine translation 
emele dorna        translated packed source language representation packed target
language representation avoiding unnecessary unpacking transfer  unlike
rule application  work transfer rules preserve ambiguity stemming source
language  rather generating new alternatives  mi et al         applied statistical
machine translation source language parse forest  rather   best parse 
transfer rules tree to string  contrary tree to tree rules  chaining
attempted  rules applied single top down pass source forest   thus 
representation algorithms quite different ours 

   incorporated knowledge bases
section  describe various knowledge bases used inference engine 
first describe novel rule base addressing generic linguistic structures  rule base
composed manually  based formalism  includes inference rules  section     
polarity annotation rules  section       addition  derived inference rules
several large scale semantic resources  section       overall  variety illustrates
suitability formalism representing diverse types inference knowledge 
    inference rules generic linguistic phenomena
rules capture inferences associated common syntactic structures 
summarized table    rules three major functions 
   simplification canonization source tree  categories     table    
   extracting embedded propositions  categories          
   inferring propositions non propositional subtrees source tree  category    
inference rules merely extract subtree source tree without changing
structure  such relative clause rule  useful exact inference aims generate
hypothesis  used evaluation inferences  cf  section       however  currently implemented approximate classification features focused matching
substructures hypothesis forest  as described section       hence
take advantage extractions  therefore  rules excluded rest
experiments  reported sections        
rules categories     depend solely syntactic structure closed class words 
referred generic rules  contrast  verb complement extraction rules  category
   considered lexicalized rules  since specific certain verbs  replace forced
advised example  entailment would hold  extracted parc
polarity lexicon  nairn et al         list verbs allow inference appearing
positive polarity contexts  generated inference rules verbs  list
complemented reporting verbs  say announce  since information
news domain  rules applied experiments  cf  section     
often given reported speech  speaker usually considered reliable 
sidestep issue polarity propagation applying rules main
clause  implemented including tree root node rule lhs 
  

fibar haim  dagan   berant

 
 

category
conjunctions

 

clausal extraction
connectives
relative
clauses

 

 

appositives

 

determiner
canonization

 

passive

 

genitive
modifier

 

verb complement clause
extraction

example  source
helenas experienced
played long time
tour 
celebrations muted
many iranians observed
shiite mourning month 
assailants fired six bullets car  carried
vladimir skobtsov 
frank robinson  onetime manager indians  distinction
nl 
plaintiffs filed lawsuit last year u s  district
court miami 
approached
investment banker 
malaysias crude palm oil
output estimated
risen six percent 
yadav forced resign 

example  derived
helena played long
time tour 
many iranians observed
shiite mourning month 
car carried vladimir
skobtsov 
frank robinson onetime manager indians 

plaintiffs filed lawsuit last year u s  district
court miami 
investment banker approached us 
crude palm oil output malaysia estimated
risen six percent 
yadav resigned 

table    inference rules generic linguistic structures

embedded clause extracted  becomes main clause derived tree  rules
extract embedded clauses  polarity verb detected applying
annotation rules  described next  verb annotated negative unknown
polarity  matching complement extraction rules fails  example  last sentence
table   yadav forced resign  forced would annotated negative
polarity  consequently matching corresponding complement extraction rule
would fail  yadav resigned would entailed  hence  annotation rules may block
erroneous inference rule applications  polarity important correct application
rules  case rule types  passive to active transformation 
therefore checked polarity matching rule application exact inference
experiment  section       verb complement extraction rules used  leave
analysis polarity dependence rules future work 
    polarity annotation rules
use annotation rules mark negative unknown polarity predicates  cf  section       table   summarizes polarity inducing contexts address  inference rules  annotation rules comprise generic rules  categories      lexicalized
  

fiknowledge based textual inference via parse tree transformations

 
 

category
explicit negation

 
 
 

implied negation
modal auxiliaries
overt conditionals

 
 
 

verb complements
adjectives
adverbs

example
weve never seen   actual costs come
down 
one stayed   last lecture 
could eat    whale now 
venus wins    game  meet    sarena
finals 
pretend know   calculus 
impossible survived   fall 
probably danced    night 

table    polarity annotation rules

rules  categories       verb complement embedded clause negative unknown
polarity  extracted  however  polarity annotated  category    compare
category   table     list verbs imply negative unknown polarity
clausal complements taken parc lexicon  well verbnet  kipper 
      
    lexical lexical syntactic rules
addition manually composed generic rules  system integrates inference knowledge variety large scale semantic resources  introduced section      information derived resources represented uniformly inference rules
formalism  examples rules shown table    following resources
used 
wordnet  extracted wordnet  fellbaum        lexical rules based synonym  hyponym  a word entailed hyponym  e g   dog animal    instance
hyponym    derivation relations 
wikipedia  used lexical rulebase shnarch et al          extracted rules
janis joplin singer wikipedia based metadata  e g  
links redirects  text definitions  using patterns x    
dirt  dirt algorithm  lin   pantel        learns corpus inference rules
binary predicates  example  x fond yx likes y  used
version learns canonical rule forms  szpektor   dagan        
argument mapped wordnet  amwn   resource inference rules predicates  covering verbal nominal forms  szpektor   dagan         includ    according wordnet glossary  instance proper noun refers particular  unique
referent  as distinguished nouns refer classes   specific form hyponym 
example  ganges instance river 
    addition extraction methods described shnarch et al          employed two additional
methods  first  extraction entailments among terms redirected page  second 
generalization rules rhs common lhs head  different modifiers  instance 
rules ferrari f    car ferrari ascari car generalized ferrari car  

  

fibar haim  dagan   berant

ing argument mapping  based wordnet nomlex plus  meyers
et al          verified statistically intersection unary dirt algorithm  szpektor   dagan         amwn rules defined unary templates 
example  kill xx die
automatically extracted inference rules lack two attributes defined formalism  rule type  substitution introduction  explicit alignments  beyond alignments
rs variables l counterparts  defined default   attributes added automatically using following heuristics 
   roots l r part of speech  substitution rule
 e g   x buy sold x    otherwise  e g   ys acquisition x
sold x    introduction rule 
   roots l r assumed aligned 
note application rules   e g   wordnet derivations
rules learned dirt   result valid parse tree  rules
used aiming exact derivation h t  however  may useful
inference engine used together approximate matching component 
rte system  approximate matcher  described section      employs features
coverage words subtrees h f  therefore benefit
inferences  rules preferably applied last step inference
process  avoid cascading errors 

   evaluation
section  present empirical evaluation entailment system whole 
well evaluation individual components  evaluate quality systems
output  in terms accuracy  precision  recall  computational efficiency  in terms
running time space  using various application settings 
first evaluate knowledge based inference engine  section      describe
experiment engine aims prove simple template hypotheses  representing
binary predicates  texts sampled large corpus  next  section     evaluate
efficiency engine implementation using compact forest data structure 
evaluate complete entailment system  including approximate entailment classifier
 section       finally  sections        provide in depth analysis performance
inference component rte data 
    proof system evaluation
experiment  evaluate inference engine finding strict proofs  is 
inference process must derive precisely target hypothesis  or instantiation
it  case template hypotheses  contain variables defined section      
thus  evaluate precision text hypothesis pairs complete proof
chain found  using available rules  note pascal rte datasets
suitable purpose  rather small datasets include many text hypothesis pairs
  

fiknowledge based textual inference via parse tree transformations

available inference rules would suffice deriving complete proofs  furthermore 
since focus research applied textual inference  inference engine
evaluated nlp application setting texts represent realistic distribution
linguistic phenomena  manually composed benchmarks fracas test suite
 cooper et al          contains synthetic examples specific semantic phenomena 
clearly suitable evaluation 
alternative  chose relation extraction  re  setting  complete
proofs achieved large number corpus sentences  setting  system
needs identify pairs arguments sentences target semantic relation  e g   x buy
  
      system configuration
experiment  first reported bar haim et al          used earlier
version engine rule bases  engine experiment make use
compact forest  rather generates consequent explicitly  polarity annotations
propagated source derived trees  instead  polarity annotation rules
applied original text t  inferred consequent  prior application
inference rule  following rule bases used experiment 
generic linguistic rules used generic rule base presented section    including inference polarity annotation rules  early version include
lexicalized polarity rules derived verbnet parc lexicon  category  
table    
lexical syntactic rules nominalization rules  inference rules xs acquisition
x acquired capture relations verbs nominalizations 
rules derived automatically  ron        nomlex  hand coded database
english nominalizations  macleod  grishman  meyers  barrett    reeves        
wordnet 
automatically learned rules  used dirt paraphrase collection  well
output tease  szpektor et al          another unsupervised algorithm learning
lexical syntactic rules  tease acquires entailment relations web given
input template identifying characteristic variable instantiations shared
templates  algorithms provide ranked list output templates given input
template  learned rules linguistic paraphrases   e g   x confirm x
approve    others capture world knowledge   e g   x buy x   
algorithms learn entailment direction rule  reduces accuracy
applied given direction  system  considered top    bi directional
rules learned template 
generic default rules rules used define default behavior  situations
case by case rules available  used one default rule allows removal
modifiers nodes  ideally  rule would replaced future work
specific rules removing modifiers 
  

fibar haim  dagan   berant

      evaluation process
use sample test template hypotheses correspond typical relations 
x approve y  identify large test corpus  sentences instantiation
test hypothesis proved  example  sentence budget approved
parliament found prove instantiated hypothesis parliament approve budget
 via passive to active inference rule   finally  sample candidate sentenceshypothesis pairs judged manually true entailment  repeated process compare
different system configurations 
since publicly available sample output tease much smaller
resources   randomly selected resource   transitive verbs may correspond
typical predicates     formed test templates adding subject object varisubj

able nodes  example  verb accuse constructed template xnoun
obj

accuse verb ynoun  
test template h identify sentences corpus template
proved system  efficiently find proof chains generate h corpus
sentences combine forward backward  breadth first  searches available
rules  first  use backward search lexical syntactic rules  starting rules
whose right hand side identical test template  process backward chaining
dirt tease nominalization rules generates set templates ti  
proving  deriving  h  example  hypothesis x approve may generate
template x confirm y  backward application dirt tease rule 
generate template confirmation x  nominalization rule 
since templates ti generated lexical syntactic rules  modify open class
lexical items  may considered lexical expansions h 
next  specific ti generate search engine query composed open class
words ti   query fetches candidate sentences corpus  ti might
proven using generic linguistic rules  recall rules modify openclass words   end  use forward search applies generic rules  starting
candidate sentence trying derive ti sequence rule applications 
successful  variables ti instantiated  cf  section       consequently  know
variable instantiations  h proven  since derives ti turn
derives h  
performed search sentences prove test template
reuters rcv  corpus  cd    applying minipar parsing  random sampling 
obtained    sentences prove  according tested system configuration 
  test templates  yielding total     pairs sentence  instantiated hypothesis  four tested configurations  described       pairs overall  
pairs split entailment judgment two human annotators  graduate students
bar ilan nlp group   annotators achieved  sample     shared exam    output tease dirt  well many knowledge resources  available rte
knowledge resources page 
http   aclweb org aclwiki index php title rte knowledge resources
    verbs approach  approve  consult  lead  observe  play  seek  sign  strike 

  

fiknowledge based textual inference via parse tree transformations

 
 
 
 
 

configuration
baseline  embed h anywhere s 
proof  embed h root s 
proof   generic
proof   generic   lexical syntactic

precision
     
     
     
     

yield
     
     
     
      

table    proof system evaluation

ples  agreement level      kappa value       corresponding substantial
agreement  
      results
tested four configurations proof system 
   baseline  baseline configuration follows prominent approach graph based
entailment systems  system tries embed given hypothesis anywhere
candidate sentence tree s  negative unknown polarity  detected
annotation rules  may block embedding 
   proof  configuration h strictly generated candidate sentence s  inference rule available default rule removing modifiers
 polarity annotation rules active baseline   configuration equivalent
embedding h root h matched root s  since modifiers
part match removed default rule  however 
h embedded elsewhere extracted  opposed baseline
configuration 
   proof   generic  proof  plus generic linguistic rules 
   proof   generic   lexical syntactic  previous configuration  plus
lexical syntactic rules 
system configuration measure precision  percentage examples judged
correct  entailing   average extrapolated yield  expected number
truly entailing sentences corpus would proven system 
extrapolated yield specific template calculated number sample sentences
judged entailing  multiplied sampling proportion  average calculated
test templates  note that  similar ir evaluations  possible compute
true recall setting since total number entailing sentences corpus
known  recall equal yield divided total   however  straightforward
measure relative recall differences among different configurations based yield  thus 
using two measures estimated large corpus possible conduct robust
comparison different configurations  reliably estimate impact different
rule types  analysis possible rte datasets  rather small 
hand picked examples represent actual distribution linguistic phenomena 
  

fibar haim  dagan   berant

results reported table    first  comparing results proof
results baseline  observe requirement matching h root  i e  
main clause s   rather allowing matched anywhere s  improves
precision considerably baseline  by         reducing yield nearly     
proof configuration avoids errors resulting improper extraction embedded
clauses 
remarkably  using generic inference rules  system able gain back lost
yield proof surpass yield baseline configuration  addition 
obtain higher precision baseline  a      difference   statistically
significant p        level  using z test proportions  demonstrates
principled proof approach appears superior heuristic baseline embedding
approach  exemplifies contribution generic rule base  overall  generic rules
used     proofs 
adding lexical syntactic rules increased yield factor six  shows
importance acquiring lexical syntactic variability patterns  however  precision
dirt tease currently quite low  causing overall low precision  manual filtering
rules learned systems currently required obtain reasonable precision 
error analysis revealed third configuration proof   generic rules 
significant     errors due parsing errors  notably incorrect dependency
relation assignment  incorrect pos assignment  incorrect argument selection  incorrect analysis complex verbs  e g   play text vs  play hypothesis  ungrammatical sentence fragments  another     errors represent conditionals  negation 
modality phenomena  could handled additional rules  making use elaborate syntactic information verb tense  remaining 
rather small     errors represent truly ambiguous sentences would require
considerable world knowledge successful analysis 
    compact forest efficiency evaluation
next  evaluate efficiency compact inference  cf  section    setting recognizing textual entailment  using rte   rte   datasets  giampiccolo et al        
       datasets consist  text  hypothesis  pairs  need classified
entailing non entailing  first experiment  using generic inference rule set  shows
compact inference outperforms explicit inference  efficiency wise  orders magnitude  section         second experiment shows compact inference scales well
full blown rte setting several large scale rule bases  hundreds rules
applied per text  section        
      compact vs  explicit inference
compare explicit compact inference randomly sampled     pairs rte  
development set  parsed text pair using minipar  lin         avoid
memory overflow explicit inference  applied sentences subset
generic inference rules described section      fair comparison  aimed make
explicit inference implementation reasonably efficient  example preventing multiple
generations tree different permutations rule applications 
  

fiknowledge based textual inference via parse tree transformations

time  msec 
rule applications
node count
edge endpoints

compact
  
  
  
   

explicit
      
   
     
      

ratio
   
  
  
  

table    compact vs  explicit inference  using generic rules  results averaged per
text hypothesis pair 

configurations perform rule application iteratively  new matches found 
iteration  first find rule matches apply matching rules  compare run
time  number rule applications  overall generated size nodes edges 
edge size represented sum endpoints    regular edge   sd      td  
d edge  
results summarized table    expected  results show compact
inference orders magnitude efficient explicit inference  avoid memory
overflow  inference terminated reaching         nodes  three     test
texts reached limit explicit inference  maximal node count compact
inference      number rule applications reduced due sharing
common subtrees compact forest  single rule application operates
simultaneously large number embedded trees  results suggest scaling
larger rule bases longer inference chains would feasible compact inference 
prohibitive explicit inference 
      application rte system
goal second experiment test compact inference scales well broad
inference rule bases  experiment used bar ilan rte system  bar haim et al  
       system operates two primary stages 
inference  inference rules first applied initial compact forest f  aiming bring
closer hypothesis h  experiment  use knowledge bases
described section    overall  rule bases contain millions rules 
current system implemented simple search strategy  spirit
 de salvo braz et al          first  applied three exhaustive iterations generic
rules  since rules low fan out  few possible right hand sides given
left hand side   affordable apply chain freely  iteration
first find rule matches  apply matched rules  avoid repeated
identical rule applications  mark newly added nodes iteration 
next iteration consider matches containing new nodes  perform single
iteration lexical lexical syntactic rules  applying l
part matched f r part matched h  investigation
effective search heuristics representation left future research 
classification  following inference  set features extracted resulting f
h fed svm classifier  determines entailment  describe
  

fibar haim  dagan   berant

rule applications
node count
edge endpoints

rte  dev
avg  max 
  
   
  
   
         

rte 
avg  max 
  
   
  
   
         

table    application compact inference rte   dev  rte   datasets  using
rule types

classification stage detail next section  discusses performance
rte system 
table   provides statistics rule applications using rule bases  rte  
development set rte   dataset     overall  primary result compact
forest indeed accommodates well extensive rule applications large scale rule bases 
resulting forest size kept small  even maximal cases causing memory
overflow explicit inference 
    complete rte system evaluation
previous sections  evaluated knowledge based inference engine  the proof system  respect quality output  precision  recall  well computational
efficiency  time  space   evaluate complete rte system  combines
inference engine approximate classification module 
classification setting features quite typical rte literature  features broadly categorized two subsets   a  lexical features solely depend
lexical items f h   b  lexical syntactic features take account
syntactic structures dependency relations f h  brief description
features  complete description appears rte system report  bar haim et al  
      
lexical features  coverage features check words h present  covered  f 
assume high degree lexical coverage correlates entailment 
features measure proportion uncovered content words  verbs  nouns  adjectives
adverbs  named entities numbers  polarity mismatch features detect cases
nouns verbs h matched f incompatible polarity 
features assumed indicate non entailment 
edge coverage features  say edge h matched f edge
f matching relation  source node target node  say edge h
loosely matched path f matching source node matching
target node  based definitions extract two features  proportion h
edges matched loosely matched f   
    running time included since dedicated rule fetching  rather slow
available implementation resources  elapsed time seconds per  t  h  pair 
    look subset edges labeled relevant dependency relations 

  

fiknowledge based textual inference via parse tree transformations

predicate argument features  f entails h  predicates h matched
f along arguments  predicates include verbs  except verb be 
subject complements copular sentences  example  smart joseph smart 
arguments daughters predicate node h    four features computed
f  h pair  categorize every predicate h match f one
four possible categories 
   complete match   matching predicate exists f matching arguments
dependency relations 
   partial match   matching predicate exists f matching arguments
dependency relations 
   opposite match   matching predicate exists f matching arguments
incorrect dependency relations 
   match   matching predicate f matching arguments 
predicate categorized complete match category 
finally  compute four features f  h pair  proportion predicates
h complete match f  three binary features  checking
predicate h categorized partial match opposite match no match  since
subject object arguments crucial textual entailment  compute four
similar features subset predicates arguments  ignoring
arguments  
global lexical syntactic feature  feature measures well subtrees h
covered f  weighted according proximity root h  feature
somewhat similar dependency tree kernel collins duffy        
measures similarity two dependency trees counting common
subtrees  however  measure several distinct properties makes suitable
needs   a  directional measure  estimating coverage h f 
vice versa  b  operates compact forest tree  rather pair
trees   c  takes account distance root h  assuming nodes
closer root important 
system trained rte   development set  tested rte 
rte   test sets  no development set released rte     co reference substitution
disabled due insufficient accuracy co reference resolution tool used 
first report overall performance  provide analysis inference module 
focus work 
accuracies obtained experiment shown table    under inference
column   results rte   quite competitive  compared          teams
   participated rte   scored higher      three systems
scored          results rte  rank            teams
scoring higher     overall  results show system well situated
state art rte task 
table   provides detailed view systems performance  precision  recall 
f  results given entailing non entailing pairs  well overall accuracy 
    dependent preposition clause take complement preposition head
clause respectively dependent 

  

fibar haim  dagan   berant

table shows results per task  ie  ir  qa sum   overall  system tends
predict entailment often non entailment  recall entailing pairs much
higher recall non entailing pairs  precision non entailing pairs
much higher entailing pairs  performance varies considerably among different tasks 
rte  accuracy results qa ir considerably higher average results
achieved rte  submissions  reported organizers  giampiccolo et al        
            respectively   ie sum  results bit average
             rte  results better ir sum  seem easier
tasks rte   giampiccolo et al           
    usage contribution knowledge bases
evaluate accuracy gain knowledge based inference  ran system
inference module disabled  entailment classification applied directly initial
parse tree text  results shown inference column table   
comparing results full system accuracy  inference   see applying
inference module resulted higher accuracy test sets  contribution
prominent rte   dataset  results illustrate typical contribution current
knowledge sources current rte systems  contribution likely increase
current near future research  topics extending improving knowledge
resources  applying semantically suitable contexts  improved classification
features  broader search strategies 
tables      illustrate usage contribution individual rule bases  table  
shows distribution rule applications various rule bases  table    presents
ablation study showing marginal accuracy gain rule base  results show
rule bases applicable large portion pairs  contributes
overall accuracy  note results highly dependent search
strategy  instance  chaining lexical rules expected increase number lexical
rule applications  reduce accuracy  provide detailed analysis rule
applications system next section 
    manual analysis
conclude evaluation two manual analyses inference component within
rte system  first analysis  subsection        assesses applicability inference
framework rte task well actual coverage current system 
categorizes cases formalism falls short   subsection        assess
correctness applied rules  analyze various causes incorrect applications 
analyses done one authors randomly sampled subsets rte  
test set 

    according rte  organizers  ie task appeared difficult task  sum
ir seemed easier tasks  however  report average accuracy per task 

  

fiknowledge based textual inference via parse tree transformations

test set
rte 
rte 

accuracy
inference inference
     
     
     
     


     
     

lexical
overlap
     
     

best rte
result
     
     

table    inference contribution rte performance  system trained rte  development set    indicates statistically significant difference  at level p         using
mcnemars test   best results achieved rte  rte  challenges  hickl  
bensley        bensley   hickl         well lexical overlap baseline results  mehdad
  magnini      a   given reference  mehdad magnini tested eight
configurations lexical overlap baselines  chose one performs best average
rte    test sets 

rte 

rte 

task
ie
ir
qa
sum

ie
ir
qa
sum


non entailing pairs
precision recall
f 
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           

entailing pairs
precision recall
f 
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           

accuracy
     
     
     
     
     
     
     
     
     
     

table    rte results breakdown task pair type

rule base
wordnet
amwn
wikipedia
dirt
generic
polarity

rte  dev
rules app
   
   
   
   
   
   
   
   
        
   
   

rte 
rules app
   
   
   
   
   
   
   
   
        
   
   

table    average number rule applications per  t  h  pair  rule base  app counts
rule application  rules ignores multiple matches rule
iteration 

      applicability coverage
analysis assesses ability inference framework derive complete proofs
rte  t h  pairs idealized setting perfect knowledge bases co reference
resolution available  provides upper bound coverage inference
  

fibar haim  dagan   berant

rule base
wordnet
amwn
wikipedia
dirt
generic
polarity

accuracy  rte  
    
    
    
    
    
    

table     contribution various rule bases  results show accuracy loss rte    obtained
removing rule base  ablation tests  

engine  similar analysis previously done bar haim  szpektor  glickman
       subset rte   dataset  however  go  a  assess
actual coverage required inferences implemented rte system   b  present
classification uncovered cases different categories 
carried analysis follows     positive  entailing  pairs randomly
sampled rte   test set  pair aimed manually derive proof
comprising inference steps expressible formalism  similar example
section      complete proof could derived  pair classified inferable 
otherwise  classified one following categories 
discourse references  complete proof requires incorporating pieces information
discourse  including event co reference bridging  mirkin et al          nominal co reference substitution included  covered formalism 
instance  text titanics sinking hitting iceberg april    
            year      explicitly specified time titanics sinking 
relation derived discourse order infer hypothesis
titanic sank      
non decomposable  inference cannot reasonably decomposed sequence
local rewrites  case  example  text black plague lasted
four years killed one third population europe  approximately   
million people hypothesis black plague swept europe 
other  cases fall categories 
distribution categories shown table     found    
pairs could proven formalism given appropriate inference rules co reference
information  demonstrates utility approach  results somewhat
higher     reported bar haim et al          may attributed
fact rte  considered difficult dataset  entailment systems consistently
perform better rte  
remaining     pairs  analysis highlights significance discourse
references  occur       pairs  previous analysis discourse references
textual entailment applied rte   search task  text sentences
interpreted context full discourse  mirkin et al          analysis shows
  

fiknowledge based textual inference via parse tree transformations

category
inferable
non decomposable
discourse references


count
  
  
  
 

 
     
     
     
    

table     applicability inference framework rte task     randomly selected
entailing pairs rte   test set analyzed 

significance discourse references even short  self contained texts  rte  composed  mirkin et al  show framework  similar methods based
tree transformations  extended utilize discourse references  several works
last years targeted implied predicate argument relationships  notable
semeval      task linking events participants discourse
 ruppenhofer  sporleder  morante  baker    palmer         particular  stern dagan
       recently showed identifying relations improves performance
rte system  finally  entailment       pairs could established
sequence local rewrites  thus cases likely require deeper methods semantic
analysis inference 
manually derived proofs    inferable pairs included total    rule applications  average      rule applications per pair    maximal number rules per
pair       rules         applied system      proofs
inferable pairs fully derived rte system  partial proofs derived
additional     pairs  remaining     pairs  system apply
rules manual proof  results demonstrate utility inference
mechanisms rule bases system  hand suggest still
much room improvement coverage existing rule bases 
      correctness applied rules
next assess correctness rules applied inference engine  focus
four lexical lexical syntactic rule bases described section      wordnet  wikipedia 
dirt  argument mapped wordnet  amwn   except wordnet  rule bases
generated automatically  therefore accuracy issue accuracy
manually composed generic inference rules polarity annotation rules  furthermore  lexicalized rules often context sensitive  additional potential source
incorrect rule applications 
evaluation randomly sampled    pairs rte   test set  analyzed
lexical lexical syntactic rule applications performed system pairs 
total     rule applications  define two levels rule application correctness 

    previously mentioned  rte system apply rules merely extract subtree
given source tree  accordingly  rules ignored analysis well 

  

fibar haim  dagan   berant

propositional  derived tree resulting rule application grammatical
entailed source tree  level correctness assumed
formalism 
referential  case propositional correctness hold  turn weaker criterion referential correctness  following notion lexical reference  glickman 
shnarch    dagan        shnarch et al          extend case
template based rules variables  let rule e   l r inference rule matched
source tree s  let l r instantiations l r respectively  according
variable matching l s  say referential correctness holds l generates reference possible meaning r  examples rules found
analyzed sample are  popepapal  turkishturkey fishermenfishing 
rule applications result valid entailed tree  still useful
context rte system applies approximate matching  as previously
discussed end section    
incorrect rule applications classified one following categories 
   bad rule  rule a priori incorrect  e g   walesyear   
   bad context  rule incorrect context source sentence  example 
wordnet rule strikecreate corresponds rare sense strike defined
produce ignition blow  as strike fire flint stone  
   bad match  rule applied due incorrect matching left hand side 
resulting incorrect parse source tree 
results summarized table     overall        rule applications correct 
interestingly  referential         propositional         rule applications  unsurprisingly  accurate knowledge resource manually composed
wordnet        correct applications   followed amwn         wikipedia
        rule bases  derived automatically human generated resources 
least accurate resource dirt          makes use human knowledge engineering  rather learned automatically based corpus statistics  accuracy dirt
considerably lower accuracy resources  substantially decreasing
overall accuracy well  errors dirt wikipedia due bad rules 
overall dominant cause incorrect applications  wordnet
amwn a priori rule quality high errors due bad context  wikipedia rules suffer bad context  explained fact
left hand side often unambiguous named entity  madrid  antelope valley
freeway  microsoft office   analysis highlights need improving accuracy
automatically generated rule bases  whose quality still far human generated resources  analysis shows context sensitivity lexicalized rules still issue
even rules applied conservatively experiment  no chaining  l
r matched f h   addressed future research 
  

fiknowledge based textual inference via parse tree transformations

  rule applications
propositional
referential
correct
bad rule
bad context
bad matching
incorrect

dirt
     
     
    
     
     
    
     
     

amwn
    
     
     
     
    
     
    
     

wikipedia
     
     
     
     
     
    
    
     

wordnet
     
     
     
     
    
     
    
     


      
     
     
     
     
     
    
     

table     analysis lexical lexical syntactic rule applications

   discussion  comparison related approaches
section  compare work several closely related inference methods 
described section       
discourse commitments derived hickl        quite similar kind consequents generate applying syntactic  lexical syntactic  co reference substitution rules  however  work differs hickls several respects  first foremost 
hickls work fully describe knowledge representation inference framework 
main focus work  hickl briefly mentions commitments
generated using probabilistic fst based extraction framework  explanations examples given paper  second  framework allows unified modeling
variety inference types addressed various tools components hickls
system  fst  relation extraction  paraphrase acquisition  etc    addition  system
operates lexical syntactic representations  rely semantic parsing  finally  consequents generated formalism packed efficient data structure 
whereas hickls commitments generated explicitly discuss commitment
generation efficiency  noted  however  explicit generation commitments restricts search space  may simplify approximate matching  e g   finding
alignment h given consequent vs  aligning h whole compact forest  
de salvo braz et al         presented semantic inference framework augments
text representation right hand side applied rule  respect
similar ours  however  work  rule application semantics
resulting augmented structure fully specified  particular  distinction
individual consequents lost augmented graph  contrast  compact
inference fully formalized proved equivalent expressive  well defined
formalism operating individual trees  inferred consequent recovered
compact forest 
maccartney manning        proposed model natural language inference which 
similar framework  operates directly parse based representations  work extends previous work natural logic  valencia         focused semantic containment monotonicity  incorporating semantic exclusion implicativity  model
inference h sequence atomic edits  thought generating
intermediate premise  calculus computes semantic relation source
  

fibar haim  dagan   berant

derived premise propagating semantic relation local edit upward
parse tree according properties intermediate nodes  example 
correctly infer first year students arrived students arrived  
every first year student arrived every student arrived   composition semantic relations along inference chain yields semantic relation holding
h  contribution complementary ours  approaches  inference
h modeled sequence atomic steps  rule applications edits   focus
framework representation application diverse types transformations
needed textual inference  well efficient representation possible inference chains 
application inference rule assumed always generate entailed consequent 
polarity rules may used detect situations assumption hold
block rule application  comparison  formalism maccartney manning assumes
rather simple edit operations  focused precise predication semantic relation
h given sequence edits transform h  thus  combining
two complementary approaches natural direction future research 

   conclusion
subject work representation use semantic knowledge textual
inference lexical syntactic level  defined novel inference framework parse
trees  represents diverse semantic knowledge inference rules  proof process
aims transform source text target hypothesis sequence rule
applications  generating intermediate parse tree  complementary contribution
work novel data structure associated rule application algorithm 
proved valid implementation inference formalism  illustrated inference
efficiency analytically empirically 
approach several advantageous properties  first  ability represent
apply wide variety inferences combine rule chaining makes framework expressive previous rte architectures  second  expressive
power obtained well formalized compact framework  based unified knowledge
representation inference mechanisms  finally  shown rte experiments 
compact forest data structure allows approach scale well practical settings
involve large rule bases hundreds rule applications per text hypothesis pair 
demonstrated utility approach two different semantic tasks  experiments unsupervised relation extraction showed exact proofs outperform
heuristic common practice hypothesis embedding  achieved competitive
results rte benchmarks  adding simple approximate matching module
inference engine  contribution semantic knowledge illustrated tasks 
limitations possible extensions formalism discussed section     
manual analysis inference engines performance relation extraction rte
tasks suggested promising directions future research  discussed subsections
           two additional major areas research approximate matching
heuristics proof search strategy  stern dagan        stern  stern  dagan 
felner        extended work address two aspects  respectively 
  

fiknowledge based textual inference via parse tree transformations

acknowledgments
article based doctoral dissertation first author  completed
guidance second author bar ilan university  bar haim        
work partially supported israel science foundation grants                 
ist programme european community pascal network excellence ist              pascal   network excellence european community
fp  ict                israel internet association  isoc il   grant      
fbk irst bar ilan university collaboration  third author grateful azrieli
foundation award azrieli fellowship  authors wish thank cleo condoravdi making polarity lexicon developed parc available research 
grateful eyal shnarch help implementing experimental setup described
section      thank iddo greental collaboration developing generic rule
base  finally  would thank dan roth  idan szpektor  yonatan aumann  marco
pennacchiotti  marc dymetman anonymous reviewers valuable feedback
work 

appendix a  compact forest complete proofs
section  provide complete proofs correctness compact inference
algorithm presented section    start definitions 
definition let l r rule matched applied compact forest f  section      let l subtree represented tree  f   l matched  recall
sl defined l excluding nodes matched dual leaf variables  similarly sr
defined copy r without dual leaf variables generated inserted
f part rule application  roots sl sr denoted rl rr respectively 
say node sr tied node s  sl   set source node one
outgoing d edges s    due alignment sharing dual leaf variable sharing 
graph operations performed applying rule l r compact forest f
summarized follows 
   adding subtree sr f 
   setting rr target node d edge f 
   setting nodes sr tied nodes sl source nodes d edges f 
according rules variable sharing dual leaf variable sharing  recall
d edges part sl  
first  show simple property cdgs generated inference process 
lemma   every node cdg generated inference process one incoming d edge 
  

fibar haim  dagan   berant

proof construction  initial forest node one incoming d edge 
rule application adds subtree sr   whose nodes one incoming d edge 
last  root rr   initially incoming edges  set target single
d edge rule application  the incoming d edge rl    therefore  lemma follows
induction number rule applications 
using following theorem show inference process generates compact
forest 
theorem   applying rule compact forest results compact forest 
proof let f   cdg generated applying rule l r compact forest f 
show f   compact forest  is  cdag single root r
embedded dags rooted r trees  first  show f   cdag   i e  
contain cycle e edges  
assume contradiction f   contains simple cycle e edges c  applying
rule l r add e edges nodes f  therefore  c must pass
rr   root sr contain e edge  p  rr    since sr tree  c must leave sr
e edge  u  v   u sr v
  sr    cycle written p rr    
u v     p  notice path v p fully contained f since cycle
c simple entering sr possible rr  
l r must substitution rule  otherwise p would root f 
impossible  since root incoming d edges  therefore  rr rl
single incoming d edge  e edge  p  rl   exists f  addition  u added
source node d edge f since tied u  sl   source node d 
therefore  path rl     u  v exists f  finally  know path v
p fully contained f  therefore construct cycle p rl     u  v     p
f  contradiction assumption f compact forest 
shown f   cdag  next  define generalization embedded dags 
help us show embedded dags f   rooted r trees 
definition embedded partial dag g    v  e  cdag g rooted node v v
similar embedded dag generated using following process 
   initialize g v alone
   repeat number iterations 
 a  choose node v
 b  choose outgoing d edge already chosen previous
iteration  d edges chosen   halt 
 c  choose target node td add e edge  s  t d g 
show embedded partial dags f   rooted node trees  since
embedded dag embedded partial dag  proves embedded dags
f   rooted r trees  assume contradiction applying l r
  

fiknowledge based textual inference via parse tree transformations

embedded partial dag   rooted node n tree  assume n
sr   otherwise  extend   adding path p rr     n  p
node outside sr source node incoming edge rr  
since   tree  two simple paths p  p  n reach
node z two different e edges  z cannot sr   since two paths meet
subtree sr   must first meet root rr entering incoming d edge  however  could
construct f two paths  selecting rl instead rr   contradiction
assumption f compact forest  clearly  either p  p  must pass
new subtree sr   otherwise two paths already existed f 
first handle case where  without loss generality  p  passes sr
p  not  p  passes sr contains e edge  p  rr    since z
  sr  
contains e edge  u  v  u sr v
  sr   p  written
n     p rr     u v     z  paths n p v z
f  way enter sr rr p  simple 
incrementally construct f following embedded partial dag   first  construct
p  section p  n p     next  expand p e edge  p  rl  
instead  p  rr    would expand rl reach z possible 
previously explained  u tied node u  sl therefore e edge  u    v  exists
f  therefore  path p   sl   rl  u    v  z  however 
guaranteed whole p   added   try expand incrementally
p     step adding next e edge path  succeed  embedded
graph f two paths z  contradiction  fail  due e edge
 z     t  p   cannot add  thus  z   must already p    node
two distinct paths embedded graph   contradiction  path constructed
indeed different p  since contains e edge  p  rl   cannot part p    since
p  contains disjoint edge  p  rr   
remaining case  p  p  pass sr reach node z
  sr   p 
written n     u  v      z p  n     u  v      z 
u    u  sr   v    v 
  sr   assume first e edges  u    v     u    v   
originate d edge d  u     u    otherwise  u    v     u    v    could
embedded partial dag  u   u  tied nodes u     u   sl  
show u      u     assume contradiction u     u     u    u  tied u 
u  due alignment sharing dual leaf variable sharing  u  cannot tied u 
u  due alignment sharing since alignment function nodes sl nodes
sr   cannot tied due dual leaf variable sharing  since variable appears
r  finally  u  tied u   without loss generality  due dual leaf
variable sharing  d edge part l  therefore  u  include
aligned modifier  thus u  tied u  due alignment 
construct embedded graph rooted rl f  sl part
match l f  construct embedded graph rooted rl path
node sl   particular paths u   u     since u      u     u  
u   source nodes d  part sl   expand two paths
e edges  u     v     u     v    get embedded graph gn tree 
contradiction 
  

fibar haim  dagan   berant

suppose e edges  u    v     u    v    originate different d edges d 
d  respectively  u  u  tied u   u     therefore  v     v  construct
following embedded graph rooted rl   previous case  expand
paths sl rl u   u     next  add e edges  u     v    d   u     v   
d    recall d  d  sl therefore used expansion  try
expand embedded graph include paths v  v  z  succeed 
two paths leading z  fail two paths tn meeting
node z     explained above  last  v    v    v  v node f two
incoming d edges  contradicting lemma   
case introduction rule quite similar simpler  p  passes sr
p  not  n must root compact forest  the node path
rr    however  case n single outgoing d edge  therefore outgoing
e edges disjoint  i e  cannot part embedded dag   thus  p  must
pass rr   contradiction  p  p  pass sr   proof identical
case substitution rule 
shown f   cdag whose embedded dags rooted r trees  f  
single root new nodes added applying l r incoming
edge  hence  f   compact forest 
corollary   inference process generates compact forest 
proof easy verify initialization generates compact forest  since applying
rule compact forest results compact forest  inference process generates
compact forest induction number rule applications 
theorem   given rule base r set initial trees   tree represented
compact forest derivable inference process consequent according
inference formalism 
proof    first show completeness induction number rule applications n 
n     one initial trees represented initial compact forest 
let tn   tree derived formalism applying sequence n     rules  show
tn   represented derivable compact forest  tn   derived applying
rule l r tree tn   according inductive assumption  tn represented
compact forest f derivable inference process  therefore  rule l r
matched applied f  assume l r substitution rule since case
introduction rule similar  tn   almost identical tn except contains subtree
r instead l instantiated variables aligned modifiers  easy verify
application l r f resulting f     f   contain embedded tree
almost identical tn   except root sr   rr   chosen instead root
sl   rl   rest sr chosen appropriate instantiated variables
modifiers  therefore  tn     contained f   required  guaranteed
tree according corollary   
   next  prove soundness induction number rule applications
forest  initialization  initial trees consequents  let fn   compact
forest derived n     rule applications  corollary   guarantees fn   indeed
  

fiknowledge based textual inference via parse tree transformations

compact forest   given tree tn   represented fn     show tn   consequent
formalism 
tn   already represented compact forest n rule applications 
according assumption induction consequent formalism  not 
tn   new embedded tree created application rule l r  therefore 
tn   contains entire subtree sr   incrementally construct embedded tree tn
represented fn tn   result applying l r tn  
substitution rule  first construct part tn   include
subtree rooted rr   introduction rule  take path forests root
rl   next  construct sl rl instead sr rr   possible since
according corollary   embedded graphs trees  therefore nodes sl
already tn   look set e edges  s  t  tn   sr
  sr  
let  s  z  edge originating d edge sz subtree rooted z
tn     notice sz already part fn   tied s  sl therefore s  source
node d  expand tn include edge  s    z  sz s  already used
d edge tn   guaranteed part sl  only d edges
part sl shared   finally  complete construction tn arbitrarily
expanding unused outgoing d edge tn nodes  obtain complete embedded
tree 
constructed embedded tree tn fn   therefore  according inductive
assumption  tn consequent formalism  tn contains sl instantiation
dual leaf variables  therefore  matched l rule l r applied 
easy verify application rule tn yield tn     required  thus  tn  
consequent formalism 

sake simplicity  proofs ignored case one leaf
variables l match multiple target nodes l appear r non leaves  described
section      case matched target nodes inserted sr alternatives  with
proper sharing modifiers   consequently  sr becomes compact forest containing
multiple trees  similarly  sl compact forest  whose represented trees correspond
possible choices matching leaf variables  mapping nodes matched
leaf variables sl nodes generated sr defines one to one
mapping trees sl sr  
proofs easily adapted handle case  follows  first  proof
lemma   need change  theorem    proof rule application create
cycles still holds underlying graph sr dag rather tree  prove
embedded partial dag   tree  observe exactly one trees embedded
sr part     thus  consider tree sr corresponding tree
sl   ignoring rest sr sl   proceed original proof  similarly 
prove completeness theorem    refer tree represented sl   part
tn   corresponding tree sr   prove soundness  consider subtrees
sr corresponding tree sl  
  

fibar haim  dagan   berant

references
bar haim  r          semantic inference lexical syntactic level  ph d  thesis 
department computer science  bar ilan university  ramat gan  israel 
bar haim  r   berant  j     dagan  i          compact forest scalable inference
entailment paraphrase rules  proceedings emnlp 
bar haim  r   berant  j   dagan  i   greental  i   mirkin  s   shnarch  e     szpektor  i 
        efficient semantic deduction approximate matching compact parse
forests  proceedings tac      workshop 
bar haim  r   dagan  i   dolan  b   ferro  l   giampiccolo  d   magnini  b     szpektor 
i          second pascal recognising textual entailment challenge 
second pascal challenges workshop recognizing textual entailment 
bar haim  r   dagan  i   greental  i     shnarch  e          semantic inference
lexical syntactic level  proceedings aaai 
bar haim  r   szpektor  i     glickman  o          definition analysis intermediate
entailment levels  proceedings acl workshop empirical modeling
semantic equivalence entailment 
barzilay  r     lee  l          learning paraphrase  unsupervised approach using
multiple sequence alignment  proceedings hlt naacl 
barzilay  r     mckeown  k  r          extracting paraphrases parallel corpus 
proceedings acl 
bensley  j     hickl  a          workshop  application lccs groundhog system
rte    proceedings tac      workshop 
bentivogli  l   clark  p   dagan  i   dang  h  t     giampiccolo  d          sixth
pascal recognizing textual entailment challenge  proceedings tac     
workshop 
bentivogli  l   dagan  i   dang  h  t   giampiccolo  d     magnini  b          fifth
pascal recognizing textual entailment challenge  proceedings tac     
workshop 
berant  j   dagan  i     goldberger  j          global learning typed entailment rules 
proceedings acl 
bhagat  r     ravichandran  d          large scale acquisition paraphrases learning
surface patterns  proceedings acl     hlt 
bos  j     markert  k          recognising textual entailment logical inference techniques  proceedings emnlp 
bos  j     markert  k          logical inference helps determining textual entailment
 and doesnt   proceedings second pascal recognising textual
entailment challenge 
chklovski  t     pantel  p          verbocean  mining web fine grained semantic
verb relations  proceedings emnlp 
  

fiknowledge based textual inference via parse tree transformations

collins  m     duffy  n          convolution kernels natural language  advances
neural information processing systems    
connor  m     roth  d          context sensitive paraphrasing single unsupervised
classifier  ecml 
cooper  r   crouch  r   van eijck  j   fox  c   van genabith  j   jaspars  j   kamp  h  
pinkal  m   milward  d   poesio  m   pulman  s   briscoe  t   maier  h     konrad  k 
        using framework  tech  rep   fracas  framework computational
semantics 
dagan  i     glickman  o          probabilistic textual entailment  generic applied modeling language variability  pascal workshop text understanding mining 
dagan  i   glickman  o   gliozzo  a   marmorshtein  e     strapparava  c       a   direct
word sense matching lexical substitution  proceedings coling acl 
dagan  i   glickman  o     magnini  b       b   pascal recognising textual entailment challenge  quinonero candela  j   dagan  i   magnini  b     dalche buc  f 
 eds    machine learning challenges  lecture notes computer science  vol       
pp          springer 
dagan  i   roth  d   sammons  m     zanzotto  f  m          recognizing textual entailment  models applications  synthesis lectures human language technologies 
morgan   claypool publishers 
de salvo braz  r   girju  r   punyakanok  v   roth  d     sammons  m          inference
model semantic entailment natural language   proceedings aaai 
deerwester  s   dumais  s  t   furnas  g  w   landauer  t  k     harshman  r         
indexing latent semantic analysis  journal american society information
science                 
dinu  g     lapata  m          topic models meaning similarity context  proceedings coling       posters 
emele  m  c     dorna  m          ambiguity preserving machine translation using packed
representations  proceedings coling acl 
fellbaum  c   ed            wordnet  electronic lexical database  language  speech
communication  mit press 
gabrilovich  e     markovitch  s          computing semantic relatedness using wikipediabased explicit semantic analysis  proceedings ijcai 
ganitkevitch  j   van durme  b     callison burch  c          ppdb  paraphrase
database  proceedings hlt naacl 
giampiccolo  d   magnini  b   dagan  i     dolan  b          third pascal recognizing textual entailment challenge  proceedings acl pascal workshop
textual entailment paraphrasing 
giampiccolo  d   trang dang  h   magnini  b   dagan  i     dolan  b          fourth
pascal recognizing textual entailment challenge  proceedings tac     
workshop 
  

fibar haim  dagan   berant

glickman  o     dagan  i          identifying lexical paraphrases single corpus 
case study verbs  proceedings ranlp 
glickman  o   shnarch  e     dagan  i          lexical reference  semantic matching
subtask  proceedings emnlp 
haghighi  a  d   ng  a  y     manning  c  d          robust textual inference via graph
matching  proceedings emnlp 
harmeling  s          inferring textual entailment probabilistically sound calculus 
natural language engineering                 
heilman  m     smith  n  a          tree edit models recognizing textual entailments 
paraphrases  answers questions  proceedings hlt naacl 
hickl  a          using discourse commitments recognize textual entailment  proceedings coling 
hickl  a     bensley  j          discourse commitment based framework recognizing textual entailment  proceedings acl pascal workshop textual
entailment paraphrasing 
hickl  a   bensley  j   williams  j   roberts  k   rink  b     shi  y          recognizing textual entailment lccs groundhog system  second pascal
challenges workshop recognizing textual entailment 
jurafsky  d     martin  j  h          speech language processing  introduction
natural language processing  computational linguistics speech recognition
 second edition   prentice hall 
kamp  h     reyle  u          discourse logic  introduction modeltheoretic
semantics natural language  formal logic discourse representation theory 
kluwer academic publishers  dordrecht 
kay  m          chart generation  proceedings acl 
kazama  j     torisawa  k          exploiting wikipedia external knowledge named
entity recognition  proceedings emnlp conll 
kipper  k          verbnet  broad coverage  comprehensive verb lexicon  ph d  thesis 
university pennsylvania 
kouylekov  m     magnini  b          tree edit distance textual entailment  proceedings ranlp 
lehmann  j   bizer  c   kobilarov  g   auer  s   becker  c   cyganiak  r     hellmann 
s          dbpedia   crystallization point web data  journal web
semantics 
lin  d          dependency based evaluation minipar  proceedings workshop
evaluation parsing systems lrec 
lin  d     pantel  p          discovery inference rules question answering  natural
language engineering                
lotan  a   stern  a     dagan  i          truthteller  annotating predicate truth 
proceedings hlt naacl 
  

fiknowledge based textual inference via parse tree transformations

maccartney  b   galley  m     manning  c  d          phrase based alignment model
natural language inference  proceedings emnlp 
maccartney  b   grenager  t   de marneffe  m  c   cer  d     manning  c  d         
learning recognize features valid textual entailments  proceedings hltnaacl 
maccartney  b     manning  c  d          extended model natural logic  proceedings iwcs   
macleod  c   grishman  r   meyers  a   barrett  l     reeves  r          nomlex  lexicon
nominalizations  proceedings euralex   
maxwell iii  j  t     kaplan  r  m          method disjunctive constraint satisfaction  tomita  m   ed    current issues parsing technology  kluwer academic
publishers 
mehdad  y     magnini  b       a   word overlap baseline recognizing textual
entailment task  unpublished manuscript 
mehdad  y     magnini  b       b   optimizing textual entailment recognition using particle swarm optimization  proceedings      workshop applied textual
inference 
melamud  o   berant  j   dagan  i   goldberger  j     szpektor  i          two level
model context sensitive inference rules  proceedings acl 
meyers  a   reeves  r   macleod  c   szekeley  r   zielinska  v     young  b         
cross breeding dictionaries  proceedings lrec 
mi  h   huang  l     liu  q          forest based translation  proceedings acl    
hlt 
mirkin  s   dagan  i     pado  s          assessing role discourse references
entailment inference  proceedings acl 
mirkin  s   dagan  i     shnarch  e          evaluating inferential utility lexicalsemantic resources  proceedings eacl 
moldovan  d  i     rus  v          logic form transformation wordnet applicability question answering  proceedings acl 
nairn  r   condoravdi  c     karttunen  l          computing relative polarity textual
inference  proceedings international workshop inference computational
semantics  icos    
pang  b   knight  k     marcu  d          syntax based alignment multiple translations 
extracting paraphrases generating new sentences  proceedings hlt naacl 
pantel  p   bhagat  r   coppola  b   chklovski  t     hovy  e          isp  learning
inferential selectional preferences  proceedings hlt naacl 
ponzetto  s  p     strube  m          deriving large scale taxonomy wikipedia 
proceedings aaai 
ravichandran  d     hovy  e          learning surface text patterns question answering system  proceedings acl 
  

fibar haim  dagan   berant

ritter  a   mausam    etzioni  o          latent dirichlet allocation method selectional
preferences  proceedings acl 
romano  l   kouylekov  m   szpektor  i   dagan  i     lavelli  a          investigating
generic paraphrase based approach relation extraction  proceedings eacl 
ron  t          generating entailment rules based online lexical resources  masters
thesis  computer science department  bar ilan university 
ruppenhofer  j   sporleder  c   morante  r   baker  c     palmer  m          semeval     task     linking events participants discourse  proceedings
workshop semantic evaluations  recent achievements future directions
 sew       
saint dizier  p     mehta melkar  r   eds            proceedings joint workshop fam lbr kraq    learning reading applications intelligent
question answering 
schoenmackers  s   etzioni  o   weld  d  s     davis  j          learning first order horn
clauses web text  proceedings emnlp 
shinyama  y   sekine  s   sudo  k     grishman  r          automatic paraphrase acquisition news articles  proceedings hlt 
shnarch  e   barak  l     dagan  i          extracting lexical reference rules
wikipedia  proceedings acl ijcnlp 
snow  r   jurafsky  d     ng  a  y       a   semantic taxonomy induction heterogenous evidence  proceedings coling acl 
snow  r   vanderwende  l     menezes  a       b   effectively using syntax recognizing
false entailment  proceedings hlt naacl 
stern  a     dagan  i          confidence model syntactically motivated entailment
proofs  proceedings ranlp 
stern  a     dagan  i          recognizing implied predicate argument relationships
textual inference  proceedings acl 
stern  a   stern  r   dagan  i     felner  a          efficient search transformation based
inference  proceedings acl 
szpektor  i     dagan  i          learning canonical forms entailment rules  proceedings
ranlp 
szpektor  i     dagan  i          learning entailment rules unary templates  proceedings coling 
szpektor  i     dagan  i          augmenting wordnet based inference argument
mapping  proceedings acl ijcnlp workshop applied textual inference
 textinfer  
szpektor  i   dagan  i   bar haim  r     goldberger  j          contextual preferences 
proceedings acl     hlt 
szpektor  i   tanev  h   dagan  i     coppola  b          scaling web based acquisition
entailment patterns  proceedings emnlp 
  

fiknowledge based textual inference via parse tree transformations

tatu  m   iles  b   slavick  j   novischi  a     moldovan  d          cogex second recognizing textual entailment challenge  second pascal challenges
workshop recognizing textual entailment 
tatu  m     moldovan  d          logic based semantic approach recognizing textual
entailment  proceedings coling acl 
tatu  m     moldovan  d          cogex rte   proceedings acl pascal
workshop textual entailment paraphrasing 
valencia  v  s          studies natural logic categorial grammar  ph d  thesis 
university amsterdam 
van deemter  k     kibble  r          coreferring  coreference muc related
annotation schemes  computational linguistics                 
voorhees  e  m     harman  d          overview sixth text retrieval conference
 trec     proceedings trec 
wang  m     manning  c          probabilistic tree edit models structured latent
variables textual entailment question answering  proceedings coling 
wang  r     neumann  g          recognizing textual entailment using subsequence
kernel method  proceedings aaai 
yates  a     etzioni  o          unsupervised methods determining object relation
synonyms web  journal artificial intelligence research  jair              
zanzotto  f  m   pennacchiotti  m     moschitti  a          machine learning approach
textual entailment recognition  natural language engineering                 

  


