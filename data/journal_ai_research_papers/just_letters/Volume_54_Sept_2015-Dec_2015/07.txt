journal of artificial intelligence research                 

submitted        published      

weighted regret based likelihood  a new approach to
describing uncertainty
joseph y  halpern

halpern cs cornell edu

computer science department
cornell university
ithaca  ny        usa

abstract
recently  halpern and leung suggested representing uncertainty by a set of weighted
probability measures  and suggested a way of making decisions based on this representation
of uncertainty  maximizing weighted regret  their paper does not answer an apparently
simpler question  what it means  according to this representation of uncertainty  for an
event e to be more likely than an event e     in this paper  a notion of comparative
likelihood when uncertainty is represented by a set of weighted probability measures is
defined  it generalizes the ordering defined by probability  and by lower probability  in
a natural way  a generalization of upper probability can also be defined  a complete
axiomatic characterization of this notion of regret based likelihood is given 

   introduction
recently  samantha leung and i  halpern   leung        suggested representing uncertainty by a set of weighted probability measures  and suggested a way of making decisions
based on this representation of uncertainty  maximizing weighted regret  however  we did
not answer an apparently simpler question  given this representation of uncertainty  what
does it mean for an event e to be more likely than an event e     this is what i do in this
paper  to explain the issues  i start by reviewing the halpern leung approach 
it has frequently been observed that there are many situations where an agents uncertainty is not adequately described by a single probability measure  specifically  a single
measure may not be adequate for representing an agents ignorance  for example  there
seems to be a big difference between a coin known to be fair and a coin whose bias an agent
does not know  yet if the agent were to use a single measure to represent her uncertainty 
in both of these cases it would seem that the measure that assigns heads probability    
would be used 
one approach that has been suggested for representing ignorance is to use a set p of
probability measures  this idea is an old one  apparently going back to the work of boole
       ch        and ostrogradsky         some authors  e g   campos   moral       
couso  moral    walley        gilboa   schmeidler        levi        walley        have
additionally required the set p to be convex  so that if   and   are in p  then so is
a    b    where a  b         and a   b       this approach has the benefit of representing
uncertainty in general  not by a single number  but by a range of numbers  this allows us
to distinguish the certainty that a coin is fair  in which case the uncertainty of heads is
represented by a single number       from knowing only that the probability of heads could
be anywhere between  say      and     
c
    
ai access foundation and morgan kaufmann publishers  all rights reserved 

fihalpern

but this approach also has its problems  for example  consider an agent who believes
that a coin may have a slight bias  thus  although it is unlikely to be completely fair  it
is close to fair  how should we represent this with a set of probability measures  suppose
that the agent is quite sure that the bias is between     and      we could  of course  take
p to consist of all the measures that give heads probability between     and      but how
does the agent know that the possible biases are exactly between     and      does she
not consider        possible for some small   and even if she is confident that the bias is
between     and      this representation cannot take into account the possibility that she
views biases closer to     as more likely than biases further from     
there is also a second well known concern  learning  suppose that the agent initially
considers possible all the measures that gives heads probability between     and      she
then starts tossing the coin  and sees that  of the first    tosses     are heads  it seems that
the agent should then consider a bias of greater than     more likely than a bias of less than
     but if we use the standard approach to updating with sets of probability measures
 halpern         and condition each of the measures on the observation  since the coin
tosses are viewed as independent  the agent will continue to believe that the probability of
the next coin toss is between     and      the observation has no impact as far as learning
to predict better  the set p stays the same  no matter what observation is made 
there is a well known solution to these problems  putting a measure of uncertainty on
these probability measures in p  this idea too has a long history  one special case is to put
a second order probability on these probability measures  see  good        for discussion
of this approach and further references  for example  an agent can express the fact that
the bias of a coin is more likely to be close to     than far from      in addition  the
problem of learning can be dealt with by straightforward conditioning  but this approach
leads to other problems  essentially  it seems that the ambiguity that an agent might feel
about the outcome of the coin toss seems to have disappeared  for example  suppose that
the agent has no idea what the bias is  the obvious second order probability to use is the
uniform probability on possible biases  while we cannot talk about the probability that the
coin is heads  there is a set of probabilities  after all  not a single probability   the expected
probability of heads is      why should an agent that has no idea of the bias of the coin
know or believe that the expected probability of heads is      of course  if one had to use
a single probability measure to describe uncertainty  symmetry considerations dictate that
it should be the one that ascribes equal likelihood to heads and tails  similarly  if one had
to put a single second order probability on the set of possible biases  uniform probability
seems like the most obvious choice  moreover  if our interest is in making decisions  then
maximizing the expected utility using the expected probability again does not take the
agents ignorance into account  kyburg        and pearl        have even argued that
there is no need for a second order probability on probabilities  whatever can be done with
a second order probability can already be done with a basic probability 
nevertheless  when it comes to decision making  it does seem useful to use an approach
that represents ambiguity  while still maintaining some of the features of having a secondorder probability on probabilities  this idea goes back to at least gardenfors and sahlin
              walley        suggested putting a possibility measure  dubois   prade       
zadeh        on probability measures  this was also essentially done by cattaneo        
chateauneuf and faro         and de cooman         all of these authors and others  such
   

fiweighted regret based likelihood

as klibanoff et al          maccheroni et al          and nau         proposed approaches
to decision making using their representations of uncertainty 
leung and i similarly suggested putting weights on each probability measure in p  since
we assumed that the weights are normalized so that the supremum of the weights is    these
weights can also be viewed as a possibility measure  if the set p is finite  we can also
normalize so as to view the weights as being second order probabilities  as with secondorder probabilities  the weights can vary over time  as more information is acquired  for
example  we can start with a state of complete ignorance  modeled by assuming that all
probability measures have weight     and update the weights after making an observation
ob  we take the weight of a measure pr to be the relative likelihood of ob if pr were the
true measure   see section   for details   with this approach  called likelihood updating by
halpern and leung         if there is a true underlying measure generating the data  over
time  the weight of the true measure approaches    while the weight of all other measures
approaches    thus  this approach allows learning in a natural way  if  for example  the
actual bias of the coin was     in the example above  no matter what the initial weights  as
long as     had positive weight  then its weight would almost surely converge to   as more
observations were made  while the weight of all other measures would approach    this 
of course  is exactly what would happen if we had a second order probability on p  the
weights can also be used to represent the fact that some probabilities in the set p are more
likely than others 
like essentially all others who considered a representation of uncertainty based on a set
of probability with weights  leung and i also suggested a way of using this representation
to make decisions  however  our approach was different than those suggested earlier  we
based our approach on regret  a standard approach to decision making that was introduced
 independently  by niehans        and savage         if uncertainty is represented by
a set p of probability measures  then regret works as follows  for each act a and each
measure pr  p  we can compute the expected regret of a with respect to pr  this is
the difference between the expected utility of a and the expected utility of the act that
gives the highest expected utility with respect to pr  we can then associate with an act
a its worst case expected regret of a  over all measures pr  p  and compare acts with
respect to their worst case expected regret  with weights in the picture  we modify the
procedure by multiplying the expected regret associated with measure pr by the weight of
pr  and compare acts according to their worst case weighted expected regret  this approach
to making decisions is very different from the others mentioned above that incorporate a
likelihood on probabilities  moreover  using the weights in the way means that we cannot
simply replace a set of weighted probability measures by a single probability measure  the
objections of kyburg        and pearl        do not apply 
leung and i  halpern   leung        show that this approach seems to do reasonable
things in a number of examples of interest  and provide an axiomatization of decision making
with this approach  since sets of weighted probabilities are certainly intended to be a way
of representing uncertainty  it seems natural to ask whether they can be used to represent
relative likelihood in a direct way  surprisingly  this is something largely not considered in
earlier papers using sets of weighted probabilities  since their focus was on decision making
 although the work of nau discussed in section   is an exception  
   

fihalpern

representing relative likelihood is straightforward if uncertainty is represented by a
single probability measure  e is more likely than e   exactly if the probability of e is greater
than the probability of e     when using sets of probability measures  various approaches
have been considered in the literature  the most common takes e to be more likely than
e   if the lower probability of e is greater than the lower probability of e     where the lower
probability of e is its worst case probability  taken over the measures in p  see section    
we could also compare e and e   with respect to their upper probabilities  the best case
probability with respect to the measures in p   another possibility is to take e to be
more likely than e   if pr e   pr e     for all measures pr  p  this gives a partial order
on likelihood   but what should we do if uncertainty is represented by a set of weighted
probability measures 
in this paper  i define a notion of relative likelihood when uncertainty is represented
by a set of weighted probability measures that generalizes the ordering defined by lower
probability in a natural way  i also define a generalization of upper probability  we can then
associate with an event e two numbers that are analogues of lower and upper probability  if
uncertainty is represented by a single measure  then these two numbers coincide  in general 
they do not  the interval can be thought of as representing the degree of ambiguity in
the likelihood of e  indeed  in the special case when all the weights are    the numbers
are essentially just the lower and upper probability  technically  they are   minus the lower
and upper probability  respectively   interestingly  the approach to assigning likelihood is
based on the approach to decision making  essentially  what i am doing is the analogue of
defining probability in terms of expected utility  rather than the other way around  the
approach can be viewed as generalizing both probability and lower probability  while at
the same time allowing a natural approach to updating 
why we should be interested in such a representation  if all that we ever did with probability was to use it to make decisions  then arguably this wouldnt be of much interest  my
work with leung already shows how sets of weighted probabilities can be used in decisionmaking  the results of this paper add nothing further to that question  however  we often
talk about the likelihood of events quite independent of their use in decision making  there
are clearly many examples in physics  the issue arises in ai applications as well  a typical
explanation of why we did a rather than b is that we thought some event e was more
likely than f   and computations of expectation  which clearly involve a representation of
uncertainty  arise in many ai applications  thus  having an analogue of probability seems
important and useful in its own right 
the rest of this paper is organized as follows  after reviewing the relevant material
from  halpern   leung        in section    i define regret based likelihood in section   
and compare it to lower probability  i provide an axiomatic characterization of regret based
likelihood in section    and show how it relates to the axiomatic characterization of lower
probability  i conclude in section   

   there is a long tradition of considering partially ordered notions of likelihood  see  halpern        and
the references therein  and the work of walley        

   

fiweighted regret based likelihood

   weighted expected regret  a review
consider the standard setup in decision theory  we have a state space s and an outcome
space o  an act is a function from s to o  it describes an outcome for each state  suppose
that we have a utility function u on outcomes and a set p   of weighted probability measures 
that is  p   consists of pairs  pr  pr    where pr is a weight in        and pr is a probability
on s  let p    pr     pr     p       for each pr  p there is assumed to be
exactly one   denoted pr   such that  pr     p     it is further assumed that weights
have been normalized so that there is at least one measure pr  p such that pr     
finally  p   is assumed to be weakly closed  so that if  prn   n    pr  for n                  
 prn   n     pr  pr    and pr      then  pr  pr    p      i discuss below why i require
p   to be just weakly closed  rather than closed  
the assumption that at least one probability measure has a weight of   is convenient
for comparison to other approaches  see below  however  making this assumption has no
impact on the results of this paper  as long as we restrict to sets where the weight is bounded 
all the results hold without change  this assumption is  of course  incompatible with the
weights being probabilities  note that the assumption that the weights are probabilities
runs into difficulties if we have an infinite number of measures in p  for example  if p
includes all measures on heads from     to      as discussed in the introduction  using a
uniform probability  we would be forced to assign each individual probability measure a
weight of    which would not work well for our later definitions 
where are the weights in p   coming from  in general  they can be viewed as subjective 
just like the probability measures  however  as leung and i  halpern   leung       
observed  there is an important special case where the weights can be given a natural
interpretation  suppose that  as in the case of the biased coin in the introduction  we make
observations in a situation where the probability of making a given observation is determined
by some objective source  then we can start by giving all probability measures a weight of   
given an observation ob  e g   sequence of coin tosses in the example in the introduction  
we can compute pr ob  for each measure pr  p  we can then update the weight of pr
to be pr ob   suppr  p pr   ob   thus  the more likely the observation is according to pr 
the higher the updated weight of pr relative to other probability measures in p    the
denominator is just a normalization to ensure that some measure has weight     with this
approach to updating  if there is a true underlying measure generating the data  then as an
agent makes more observations  almost surely  the weight of the true measure approaches
   while the weight of all other measures approaches     in addition  this approach gives
an agent a natural way of determining weights for each probability measure in p  while 
in general  this means that the agent may need to carry around a lot of information  not
   the idea of putting a possibility on probabilities in p that is determined by likelihood also appears in the
work of moral         although he does not consider a general approach to dealing with sets of weighted
probability measures 
   the almost surely is due to the fact that  with probability approaching    as more and more observations are made  it is possible that an agent will make misleading observations that are not representative
of the true measure  this also depends on the set of possible observations being rich enough to allow
the agent to ultimately discover the true measure generating the observations  for example  an agent will
never learn the distributions of outcomes of a die she never gets to observe the die when it lands   or   
since learning is not a focus of this paper  i do not make this notion of rich enough precise here 

   

fihalpern

only a possibly infinite set of probabilities  but a weight associated with each one   if the
set p has a reasonable parametric representation  then the weight can often be evaluated
in terms of the parameters  so should admit a compact representation  see example      
the weight associated with a probability pr can be viewed as an upper bound on an
agents confidence that pr actually describes the situation  that is why an agent who has
no idea of what is going on is modeled as starting by placing weight   on all probability
measures  i believe that having the weights will allow agents to express nuances that they
consider important  and that such weights will not be hard to elicit  whether this is the
case is really an empirical question  one which i believe deserves further exploration  but is
beyond the scope of this paper 
i now review the definition of weighted regret  and introduce the notion of absolute
 weighted  regret  i start with regret  the regret of an act a in a state s  s is the
difference between the utility of the best act at state s and the utility of a at s  typically 
the act a is not compared to all acts  but to the acts in a set m   called a menu  thus  the
regret of a in state s relative to menu m   denoted reg m  a  s   is supa  m u a   s    u a s    
there are typically some constraints put on m to ensure that supa  m u a   s   is finitethis
is certainly the case if m is finite  or the convex closure of a finite set of acts  or if there is a
best possible outcome in the outcome space o  the latter assumption holds in this paper 
so i assume throughout that supa  m u a   s   is finite 
for simplicity  i assume that the state space s is finite  given a probability measure
pr on s  the expected regret of an act a with respect to pr relative to menu m is just
p
m
reg m
ss reg  a  s  pr s   the  expected  regret of a with respect to p and a menu
pr  a   
m is just the worst case regret  that is 
m
reg m
p  a    sup reg pr  a  
prp

similarly  the weighted  expected  regret of a with respect to p   and a menu m is just the
worst case weighted regret  that is 
m
wr m
p    a    sup pr reg pr  a  
prp

thus  regret is just a special case of weighted regret  where all weights are   
note that  as far weighted regret goes  it does not hurt to augment a set p   of weighted
probability measures by adding pairs of the form  pr     for pr 
  p  but if we start with a set
 
p of unweighted probability measures  the set p     pr       pr  p   pr       pr 
  p  is
not closed in general  although it is weakly closed  there may well be a sequence prn  pr 
where prn 
  p for all n  but pr  p  but then we would have  prn       p   converging to
 
 pr     
  p   this is exactly why i required only weak closedness  note for future reference
that  since p   is assumed to be weakly closed  if wr m
p    a       then there is some element
m
 
m
 pr  pr    p such that wr p    a    pr reg pr  a  
weighted regret induces an obvious preference order on acts  act a is at least as good
 
 
m
m
as a  with respect to p   and m   written a reg
p    m a   if wr p    a   wr p    a    as usual  i
   recall that if x is a set of real numbers  sup x  the supremum of x  is the smallest real numbers that
is greater than or equal to all the elements of x  if x is finite  then the sup is the same as the max 
but if x is  say  the interval         then sup x      similarly  inf x is the largest real number that is
less than or equal to all the elements in x 

   

fiweighted regret based likelihood

reg
reg
 
 
 
write a reg
p    m a if a p    m a but it is not the case that a p    m a  the standard notion
of regret is the special case of weighted regret where all weights are    i sometimes write
 
 
a reg
p m a to denote the unweighted case  i e   where all the weights in p are    
in this setting  using weighted regret gives an approach that allows an agent to transition
smoothly from regret to expected utility  it is well known that regret generalizes expected
m  
utility in the sense that if p is a singleton  pr   then wr m
p  a   wr p  a   iff eupr  a  
 
eupr  a    where eupr  a  denotes the expected utility of act a with respect to probability
pr   this follows from the observation that  given a menu m   there is a constant cm such
that  for all acts a  m   wr m
 pr   a    cm  eupr  a    in particular  this means that if p
is a singleton  regret is menu independent   if we start with all the weights being    then 
as observed above  the weighted regret is just the standard notion of regret  as the agent
makes observations  if there is a measure pr generating the uncertainty  the weights will
get closer and closer to a situation where pr gets weight    with the weights of all other
measures dropping off quickly to    so the ordering of acts will converge to the ordering
given by expected utility with respect to pr 
there is another approach with some similar properties  which again starts with uncertainty being represented by a set p of  unweighted  probability measures  define wc p  a   
inf prp eupr  a   thus wc p  a  is the worst case expected utility of a  taken over all pr  p 
then define a mm
a  if wc p  a   wc p  a     this is the maxmin expected utility rule  quite
p
often used in economics  gilboa   schmeidler         there are difficulties in getting a
weighted version of maxmin expected utility  halpern   leung         discussed further in
section     however  epstein and schneider        propose another approach that can be
combined with maxmin expected utility  they fix a parameter           and update p
after an observation ob by retaining only those measures pr such that pr ob     for any
choice of       we again end up converging almost surely to a single measure  so again
this approach converges almost surely to expected utility 
i conclude this section with a discussion of menu dependence  maxmin expected utility
is not menu dependent  the preference ordering on acts induced by regret can be  as the
following example illustrates 

example      take the outcome space to be         and the utility function to be the
identity  so that u        and u         as usual  if e  s   e denotes the indicator
function on e  where  for each state s  s  we have  e  s      if s  e  and  e  s     
if s 
  e  let s    s    s    s    s     e     s     e     s     e     s    s     m      e     e    
m      e     e     e     and p    pr    pr     where pr   s      pr   s      pr   s          
 
pr   s           and pr   s           a straightforward calculation shows that reg m
pr    e     
m 
m 
m 
m 
m 
   reg pr    e           reg pr    e           reg pr    e         reg pr    e           reg pr    e     
m 
m 
m 
 
     reg m
pr    e         and reg pr    e           thus        reg p   e      reg p   e          
m 
 
while     reg m
p   e      reg p   e           the preference on  e  and  e  depends on
whether we consider the menu m  or the menu m   
suppose that there is an outcome o  o that gives the maximum utility  that is 
 u o  for all o  o  if o is the constant act that gives outcomes o in all states 
then o is clearly the best act in all states  if there is such a best act  an absolute 
menu independent notion of weighted expected regret can be defined by always comparing

u o  

   

fihalpern

to o   that is  define
reg s  a    u o    u a s   
p
reg pr  a    ss  u o    u a s   pr s    u o    eupr  a  
p
reg p  a    supprp ss  u o    u a s   pr s    u o    inf prp  eupr  a  
p
wr p    a    supprp pr ss  u o    u a s   pr s    supprp pr  u o    eupr  a   
if there is a best act  then i write a p   a  if wr p    a   wr p    a     similarly in the
unweighted case  i write a p a  if wr p  a   wr p  a    
conceptually  we can think of the agent as always being aware of the best outcome o  
and comparing his actual utility with a to u o    equivalently  the absolute notion of regret
is equivalent to a menu based notion with respect to a menu m that includes o  since if
the menu includes o   it is the best act in every state   as we shall see  in our setting  we
can always reduce menu dependent regret to this absolute  menu independent notion  since
there is in fact a best act   s  

   relative ordering of events using weighted regret
in this section  i consider how a notion of comparative likelihood can be defined using sets
of weighted probability measures 
as in example      take the outcome space to be         the utility function to be the
identity  and consider indicator functions  it is easy to see that eupr   e     pr e   so that
with this setup  we can recover probability from expected utility  thus  if uncertainty is
represented by a single probability measure pr and we make decisions by preferring those
acts that maximize expected utility  then we have  e   e   iff pr e   pr e     
consider what happens if we apply this approach to maxmin expected utility  now we
have that  e mm
 e   iff inf prp pr e   inf prp pr e      in the literature  inf prp pr e  
p
denoted p  e   is called the lower probability of e  and is a standard approach to describing likelihood  the dual upper probability  supprp pr e   is denoted p   e   an easy
calculation shows that
p   e       p  e  
where  as usual  e denotes the complement of e  the interval  p  e   p   e   can be
thought of as describing the uncertainty of e  the larger the interval  the greater the ambiguity 
what happens if we apply this approach to regret  first consider unweighted regret 
if we restrict to acts of the form  e   then the best act is clearly  s   which is just the
constant function    thus  we can  and do  use the absolute notion of regret here  and
for the remainder of this paper  we then get that  e reg
p  e   iff supprp     pr e   
 
 
supprp     pr e    iff supprp pr e   supprp pr e    that is 
 



 e reg
p  e   iff p  e   p  e   

   

fiweighted regret based likelihood

moreover  easy manipulation shows that supprp     pr e        inf prp pr e      
p  e   it follows that
 e reg
p  e  
iff     p  e        p  e     
iff p  e   p  e    
iff  e mm
 e    
p
that is  both regret and maxmin expected utility put the same ordering on events 
   e   the  weighted  regret based
the extension to weighted regret is immediate  let preg
likelihood of e  be defined by taking
 
preg
 e    sup pr pr e  
prp

if p   is unweighted  so that all the weights are    i write preg  e  to denote supprp pr e  
note that preg  e       p  e   so
preg  e   preg  e     iff p  e   p  e     
that is  the ordering induced by preg is the opposite of that induced by p   so  for example 
preg        and preg  s       smaller sets have larger regret based likelihood  however  since
an act with smaller regret is viewed as better  the ordering on acts of the form  e induced
by regret is the same as that induced by maxmin expected utility 
regret based likelihood provides a way of associating a number with each event  just
as probability and lower probability do  moreover  just as lower probability gives a lower
   e  as giving an upper bound on the uncertainty 
bound on uncertainty  we can think of preg
 it is an upper bound rather than a lower bound because larger regret means less likely 
just as smaller lower probability does   the naive corresponding lower bound is given by
inf prp pr pr e   this lower bound is not terribly interesting  if there are probability
measures pr   p such that pr  is close to    then this lower bound will be close to   
independent of the agents actual feeling about the likelihood of e  a more reasonable
 
lower bound is given by the expression p  
reg  e       preg  e   recall that the analogous
expression relates upper probability and lower probability   the intuition for this choice
is the following  if nature were conspiring against us  she would try to prove us wrong
by making pr pr e  as large as possiblethat is  make the weighted probability of being
wrong as large as possible  on the other hand  if nature were conspiring with us  she would
try to make pr pr e  as large as possible  or  equivalently  make    pr pr e  as small
as possible  note that this is different from making pr pr e  as large as possible  unless
pr     for all pr  p  an easy calculation shows that
   e       sup
   preg
prp pr pr e 
  inf prp     pr pr e   

this motivates the definition of p  
reg  
the following lemma clarifies the relationship between these expressions  and shows that
 
 p  
reg  e   preg  e   really does give an interval of ambiguity 
   e   p    e  
lemma      inf prp pr pr e      preg
reg

   

fihalpern

proof  clearly
inf pr pr e    inf pr     pr e   

prp

prp

since  as observed above 
 
   preg
 e    inf     pr pr e   
prp

and for all pr  p  we have
   pr pr e   pr     pr e   
   e  
it follows that inf prp pr pr e      preg
since  by assumption  there is a probability measure pr   p such that pr       it
follows that
   e       sup
   preg
prp pr pr e 
    pr   e 
  pr   e 
 supprp pr pr e 
   e  
 preg

in general  equality does not hold in lemma      as shown by the following example  the
example also illustrates how the ambiguity interval can decrease with weighted regret  if
the weights are updated as leung and i  halpern   leung        suggested 
example      suppose that the state space consists of  h  t   for heads and tails   let pr
be the measure that puts probability  on h  let p       pr                     that is 
we initially consider all the measures that put probability between     and     on heads  we
toss the coin and observe it lands heads  intuitively  we should now consider it more likely
that the probability of heads is greater than      indeed  applying likelihood updating  we
get the set p       pr                       the probability measures that give h higher
probability get higher weight  in particular  the weight of pr    is still    but the weight of
pr    is only       the weight of pr is the likelihood of observing heads according to pr  
which is just   normalized by the likelihood of observing heads according to the measure
that gives heads the highest probability  namely       if the coin is tossed again and this
time tails is observed  we update further to get p       pr                         
before going on  it is worth noting here how the simple parametric form of p   leads to
simple parametric forms for p   and p    
 
 
 
an easy calculation shows that  p  
  reg  h   p  reg  h                  p   regret  h   p  reg  h    
 
 
            and  p   reg  h   p  reg  h                     in more detail  since pr  h     and
pr  t         so we have the following 
 
 p  reg
 h    sup                      

 p    reg  h    inf                         
 
 p  reg
 h    sup                    taking the derivative shows that         
 
is maximized when         so p  reg
 h        

   

fiweighted regret based likelihood

 p    reg  h    inf                       now          is minimized  when      
is maximized  for               this happens when         so p    reg  h        
 
 p  reg
 h    sup                   taking the derivative shows that      
is maximized when         in which case it is       

 p    reg  h    inf                        now              is minimized when
        is maximized  for               this happens when         so p    reg  h   
      
it is also easy to see that inf pr        pr  t    inf                           so
 
 
inf       pr  t       p  reg
 t    p  reg
 h  

prp 

thus  for p     we get strict inequalities for the expressions in lemma     
 
the width of the interval  p  
reg  e   preg  e   can be viewed as a measure of the ambiguity
the agent feels about e  just as the interval  p  e   p   e    indeed  if all the weights are   
   e  and p   e       p    e 
the two intervals have the same width  since p  e       preg
reg
in this case 
however  weighted regret has a significant advantage over upper and lower probability 
if the true bias of the coin is  say      then if the set pk  represents the uncertainty after
 
k steps  as k increases  almost surely   p  
k reg  h   pk reg  h   will be a smaller and smaller
interval containing               more generally  using likelihood updated combined with
weighted regret provides a natural way to model the reduction of ambiguity via learning 
it is worth at this point comparing the approach to representing likelihood taken here
to the work of nau         nau starts with a preference order on lotteries  functions from
some finite state space s to the reals  satisfying certain axioms  and derives from that what
he calls confidence weighted  lower and upper  probabilities  roughly speaking  rather than
just associating with each event its lower and upper probability  nau can associate with
 of probabilities
each event e  confidence c          and probability p         the set pc p
that give event e lower probability p with confidence at least c  if c   c  then pc   p  pc p
 every probability measures that gives e lower probability p with the higher confidence
c  will also give it lower probability p with confidence c  but the converse may not hold  
similarly  we can consider the probability measures that give e upper probability p with
confidence c  with a set p of unweighted probabilities  an agents uncertainty regarding an
event e can be characterized by a single interval  p  e   p   e    in naus framework  an
agents uncertainty regarding e can be characterized by a family of intervals  pc  e   p c  e   
indexed by the confidence c  where pc  e  is the largest p such that e has lower probability
with confidence c  and p c  e  is defined similarly  clearly these intervals are nested  if
 
c    c  then  pc   e   p c  e   contains  pc  e   p c  e    thus  naus approach provides a
more fine grained representation of uncertainty than the single intervals  p  e   p   e   or
 
 p  
reg  e   preg  e    to some extent  this distinction is due to the fact that naus preference
order on lotteries is only a partial order  the preference order induced by max min expected
    and p   all put a
utility regret is total  however  note that even though p   p    preg
reg

    and p   together 
total order on events  when considering both p and p or both preg
reg

   

fihalpern

we can also obtain a partial order on events  in particular  these approaches can express
ambiguity 
one benefit of the regret based approach is that it provides a natural way of updating 
nau does not consider updating  it would be interesting to see if an analogue of likelihood
updating could be defined axiomatically in naus framework  perhaps in the spirit of the
characterization that leung and i  halpern   leung        gave for likelihood updating in
the context of regret 
one concern with the use of regret has been the dependence of regret on the menu 
naus approach  and other approaches to decision making that are not based on regret  do
not require a menu  while there is evidence from the psychology literature suggesting that
people are quite sensitive to menus  it is also worth noting that when dealing with likelihood 
there is a sense in which we can work with the absolute notion of weighted regret without
loss of generality  if we restrict to indicator functions  then a preference relative to a menu
can always be reduced to an absolute preference  given a menu m consisting of indicator
functions  let em    e    e  m    that is  em is the union of the events for which the
corresponding indicator function is in m   the following property shows that  when restrict
to indicator functions  regret satisfies satisfies an axiom similar in spirit to naus       
cancellation axiom 
proposition      if m is a menu consisting of indicator functions  and  e     e   m  
reg
then  e  reg
p    m  e  iff  e     e m p    e     e m  
proof  let m   be any menu consisting of indicator functions that includes  e     e m  
reg
 e     e m   and  s   recall that  e     e m reg
p    e     e m iff  e     e m m    p    e     e m  
the absolute notion of regret is equivalent to the menu based notion  as long as the menu
includes the best act  which in this case is  s   it clearly suffices to show that  for all states
s  s and all acts  e  m  
 

reg m   e   s    reg m   e    e m   s  
this is straightforward  there are two cases  depending on whether s  em  
if s  em   then  by definition  there is some act  e    m such that s  e     so
supam u a s     u     clearly supam   u a s     u     since  s  m     moreover 
 e m  s       so   e    e m   s     e  s   thus  for s  em  
reg m   e   s    supam u a s    u  e  s  
  supam   u a s    u   e    e m   s  
 
  reg m   e    e m   s  
for s 
  e m   we have a s      for all a  m and  e  s       so supam u a s    u  e  s    
   on the other hand  supam   u a s     u     and u   e    e m   s     u     so again
 
supam   u a s    u   e    e m   s        thus  we again have reg m   e   s    reg m   e  
 e m   s  

   

fiweighted regret based likelihood

   characterizing weighted regret based likelihood
the goal of this section is to characterize weighted regret based likelihood axiomatically 
in order to do so  it is helpful to review the characterizations of probability and lower
probability  for ease of exposition in this discussion  i assume that the sample space is
finite and all sets are measurable 
a probability measure on a finite set s maps subsets of s to        in a way that satisfies
the following three properties 
pr   pr s      
pr   pr        
pr   pr e  e       pr e    pr e     if e  e      
these three properties characterize probability in the sense that any function f    s        
that satisfies these properties is a probability measure 
lower probabilities satisfy analogues of these properties 
lp   p  s      
lp   p        
lp     p  e  e      p  e    p  e     if e  e      
however  these properties do not characterize lower probability  there are functions that
satisfy lp   lp   and lp   that are not the lower probability corresponding to some set
of probability measures   see  halpern   pucella        proposition      for an example
showing that analogous properties do not characterize p    the same example also shows
that they do not characterize p   
various characterizations of p  and p    have been proposed in the literature  anger  
lembcke        giles        huber              lorentz        williams        wolf        
all similar in spirit  i discuss one due to anger and lembcke        here  since it makes
the contrast between lower probability and regret particularly clear  the characterization
is based on the notion of set cover  a set e is said to be covered n times by a multiset
m if every element of e appears at least n times in m   it is important to note here that
m is a multiset  not a set  its elements are not necessarily distinct   of course  a set is a
special case of a multiset   let t denote multiset union  thus  if m  and m  are multisets 
then m  t m  consists of all the elements in m  or m    which appear with multiplicity that
is the sum of the multiplicities in m  and m    for example  using the           notation to
denote a multiset  then             t                                    
if e  s  then an  n  k  cover of  e  s  is a multiset m that covers s k times and
covers e n   k times  multiset m is an n cover of e if m covers e n times  for example  if
s              then                      is a        cover of       s   a        cover of          s  
and a   cover of     
we will be interested in whether a multiset of the form e   t       t e m is an  n  k  cover
of  e  s   this is perhaps best thought of in terms of indicator functions  e   t       t e m
   this property actually follows from the other two  using the observation that pr s      pr s    pr   
i include it here to ease the comparison to other approaches 

   

fihalpern

is an  n  k  cover of  e  s  if and only if  e          em  n e   k s   the use of equalities and inequalities involving sums of indicator functions in axiomatic characterizations of
uncertainty has a long history  for example  they were used by scott        to characterize
qualitative probability  set covers are just a special case of such inequalities  typically  such
axioms make it possible to apply results from linear programming to prove characterization
results  as we shall see  that will be the case here too 
consider the following property 
lp   for all integers m  n  k and all subsets e            em of s  if e  t       t em is an  n  k p
 
cover of  e  s   then k   np  e   m
i   p  ei   
there is an analogous property for upper probability  where  is replaced by   it is easy
to see that lp  implies lp    since e t e   is a        cover of  e  e     s    it follows
by a straightforward induction from lp   that if e            em are pairwise disjoint  then
p  e          em    p  e           p  e     lp  generalizes this property to allow for sets
that are not necessarily disjoint  the soundness of lp  for lower probability follows using
the same techniques as given below for the soundness of the property reg   as anger
and lembcke        show  lp  is just the property that is needed to characterize lower
probability 
theorem       anger   lembcke        if f    s          then there exists a set p of
probability measures with f   p if and only if f satisfies lp   lp   and lp  
moving to regret based likelihood  clearly we have
   s      
reg   preg
         
reg   preg

the whole space s has the least regret  the empty set has the greatest regret  again  we see
that regret based likelihood inverts the standard ordering of probability  larger regret based
likelihood corresponds to probability 
in the unweighted case  since preg  e    p   e   reg   reg   and the following analogue of lp   appropriately modified for p    clearly characterize preg  
reg     for all integers m  n  k and all subsets e            em of s  if e   t       t e m is an
p
 n  k  cover of  e  s   then k   npreg  e   m
i   preg  ei   
note that complements of sets  e             e m   e  are used here  since regret is minimized if
the probability of the complement is maximized  this need to work with the complement
makes the statement of the properties  and the proofs of the theorems  slightly less elegant 
but seems necessary 
it is not hard to see that reg   does not hold for weighted regret based likelihood 
for example  suppose that s    a  b  c  and p       pr           pr           pr         where 
identifying the probability pr with the tuple  pr a   pr b   pr c    we have
 pr                  
   note that lp  implies lp   using the fact that  t  is a       cover of    s  

   

fiweighted regret based likelihood

 pr                  
 pr                    
    a  b     p     b  c          while p     b          since  a  b  t  b  c  is a
then preg
reg
reg
      cover of   b    a  b  c    reg   would require that
 
 
 
preg
  a  b     preg
  b  c        preg
  b   

which is clearly not the case 
we must thus weaken reg   to capture weighted regret based likelihood  it turns out
that the appropriate weakening is the following 
reg   for all integers m  n and all subsets e            em of s  if e   t       t e m is an n cover
   e   pm p    e   
of e  then npreg
i
i   reg
although reg  is weaker than reg     it still has some nontrivial consequences  for
  is anti monotonic  if e  e     then e is a   cover
example  it follows from reg  that preg
 
   e   p    e      since e t e   is trivially a   cover of
of e   so by reg   we must have preg
reg
   e    p    e      p    e  e      reg  also implies reg  
e  e     it also follows that preg
reg
reg
since     s  is an n cover of itself for all n 
i can now state the representation theorem  it says that a representation of uncertainty
satisfies reg   reg   and reg  iff it is the weighted regret based likelihood determined
by some set p     the set p   is not unique  but it can be taken to be maximal  in the
sense that if weighted regret based likelihood with respect to some other set  p      gives
the same representation  then for all pairs  pr        p        there exists     such that
 pr     p     this  unique  maximal set p   can be viewed as the canonical representation
of uncertainty 
theorem      if f    s          then there exists a weakly closed set p   of weighted
  if and only if f satisfies reg   reg   and reg  
probability measures with f   preg
moreover  p   can be taken to be maximal 
proof  clearly  given a weakly closed set p   of weighted probability measures  the function
  satisfies reg  and reg   to see that it satisfies reg   suppose that e t       t e is
preg
 
m
   e       then reg  trivially holds  if p    e       then since p  
an n cover of e  if preg
reg
   e     pr e  
is weakly closed  there must be some probability pr  p such that preg
pr
since e   t     te m is an n cover of e  it is easy to see that pr e        pr e m     n pr e  
   e   by construction 
so pr pr e            pr pr e m     npr pr e   but pr pr e    preg
p
m
   e    i              n  thus  np    e  
 
and pr pr e i    preg
i
reg
i   preg  ei   
s
for the opposite direction  suppose that f             satisfies reg   reg   and
reg   let p    s   the set of all probability measures on s  and for pr  p  define
pr   sup     pr e   f  e  for all e  s  
note that  for all pr  p  we have   pr e   f  e  for all e  s  since f  e           and
  pr     f         it follows that pr         for all pr  p  let p       pr  pr    
   

fihalpern

pr   s    it is easy to see that p   is weakly closed  moreover  if we can show that p  
     it is immediate that p   is maximal among all sets of weighted
represents f  i e   f   preg
probability measures that represent f   thus  it suffices to show that there exists pr   s 
such that     pr      since this is one of the conditions on sets of weighted measures  and
   e  for all e  s 
    f  e    preg
the proof of this result makes critical use of the following variant of farkas lemma
 farkas         see also schrijver        pg      from linear programming  where a is a
matrix  b is a column vector  and x is a column vector of distinct variables 
lemma      if ax  b is unsatisfiable  then there exists a row vector  such that
      
   a    
   b     
intuitively   is a witness of the fact that ax  b is unsatisfiable  this is because if there
were a vector x satisfying ax  b  then      a x    ax   b      a contradiction 
to prove the first claim  suppose that s    s            sn    i now construct a set of linear
equations in the variables x            xn such that a solution to the equations guarantees the
existence of a probability measure pr   s  such that pr      intuitively  we want
xi to be pr si    since we must have pr e   f  e  for all e  s   for each e  s  we
p
have the inequality  i si e 
xi  f  e   note that since f         the equation when
 
e    is x         xn     in addition  we require that xi    for i              n   and that
x      xn      it suffices to require that x      xn     since  as i observed earlier  the
equation corresponding to e    already says x         xn     to apply farkas lemma
all the inequalities need to involve   so this collection of inequalities must be rewritten as 
  i si e 
xi  f  e   for all e  s
 
xi     for i              n
x         xn    
p

this system of inequalities can be expressed in the form ax  b  note that a is a matrix all
of whose entries are either       or    and  in the first  n    rows  the lines corresponding
to equations for each e  s   all the entries are either   or    while in the final n    
rows  all the entries are either   or   
a solution of this system of inequalities provides the desired pr  but if this systems has
no solution  then by farkas lemma  there exists a nonnegative vector  such that a    
and b      since all the entries of a are either       or    it follows from standard
observations  cf   fagin  halpern    megiddo        lemma      we can take  to a vector
of all whose entries are rational   since we can multiply each term in  by the product
   i use  to denote strict subset 
   there is a slight subtlety here since  also has to satisfy b      and b may involve irrational numbers
 since f  e  may be irrational for some sets e   however  if there is a nonnegative  that satisfies a    
and b      then there is a nonnegative  that satisfies a     and b       where b  consists only of
rational entries and b   b  thus  there is a vector  with rational entries such that a     and b      
so b     

   

fiweighted regret based likelihood

of the denominators of the entries of   we can assume without loss of generality that the
entries of  are natural numbers 
since a has  n   n rows   is a vector of the form               n  n    let a            a n  n
be the rows of a  each of these is a vector of length n   since a      that means that
  a          n  n a n  n      suppose for now that  n            n  n    the coefficients
for the rows corresponding to the inequalities xi    for i              n   are all    as i show
below  this assumption can be made without loss of generality 
with this assumption  we can rewrite the equations as   a          n   a n      n  n a n  n  
if e            e n   are the subsets of s that correspond to the equations for a            a n    
respectively  this equation says that   copies of e       copies of e             and  n   copies
of e  n   form a  n  n  cover of s   recall that a n  n is a row of all  s  so a n  n corresponds to s   thus  by reg     f  e            n   f  e n       n  n f       n  n  
but farkas lemma requires that b      where  by construction  bi   f  ei   for i  
            n     bi     for i    n            n   n     and b n  n      thus  we must have
   f  e        n   f  e n         n  n   clearly  this gives a contradiction  thus  we
can conclude  as desired  that the equations are solvable  and that there exists a probability
measure pr such that pr     
n
n
it remains to show that we can assume without loss of generality that                  n  
are all    note that since      they must all be nonnegative  i prove by induction on
 n         n  n   that if there is a vector     such that a     and b      then
there is such a vector with  n         n  n       
so suppose that there is a solution  with  n         n  n        suppose without
loss of generality that  n      recall that a n corresponds to the inequality x     
choose j               n     such that j     and s  
  ej   there must be such a j  for
otherwise we would not have a      let j   be such that ej     ej   s     define a vector
   j  j      n   
   such that    n    n     j    j     j      j      and i    i if i 
 
 
 
it is easy to check that  a     and that  n         n  n      n         n  n    
it remains to show that    b      since ej  ej     we must have f  ej    f  ej      so
   b   b   f  ej    f  ej      b      this completes the inductive step of the argument 
   e  for
now we must show the second required property holds  namely  that f  e    preg
all e  s  by construction  pr pr e   f  e  for all e  s  so it suffices to show that there
is some pr  p such that pr pr e    f  e   for this  it suffices to show that there exists a
measure pr such that pr e       and for each e    s  we have f  e  pr e      f  e      since
then pr   f  e   so pr pr e    f  e   as desired 
to show that such a measure exists  we again construct a set of linear inequalities
much as above  and apply farkas lemma  using the same notation as above  suppose for
simplicity that e    s            sm    where m  n   now the required inequalities just involve
the variables x            xm  
 

  i s ee     xi  f  e     f  e   for all e    s such that e  e    
i
xi     for i              m
x         xm    
p

again  the requirement that x         xm    follows from the equation for e 
if this system of inequalities is satisfiable  then we have the required probability measure 
so suppose that it is not satisfiable  again  writing this system of equations as ax  b  by
   

fihalpern

farkas lemma  there exists a nonnegative vector  such that a     and b      we now
proceed much as before  again  we can assume that  is a vector of natural numbers  if
m
m
we assume for now that                  m    the coefficients for the rows corresponding to
the inequalities xi    for i              n   are all    then the fact that a     means that
we have  m  m cover of e  we get a contradiction to reg  in an almost identical way to
above  this completes the argument 
as i said earlier  the set p   guaranteed to exist by theorem     is not unique  although
it is canonical  in the sense of being the unique maximal set of weighted probability measures
that represents f   we might wonder if we can actually get uniqueness by imposing a few
extra requirements  particularly since leung and i were able to do so in our representation
theorem  the answer seems to be no  to explain why  it is helpful to review some material
from  halpern   leung        
define a sub probability measure p on s to be like a probability measure  i e   a function
mapping measurable subsets of s to        such that p t  t       p t     p t     for disjoint
sets t and t      without the requirement that p s       we can identify a weighted
probability distribution  pr    with the sub probability measure  pr  conversely  given a
sub probability measure p  there is a unique pair    pr  such that p    pr  we simply
take    p s  and pr   p   thus  in the sequel  i identify a set of sub probability
measures with a set of weighted probability measures 
a set b of sub probability measures is downward closed if  whenever p  b and q  p 
then q  b 
one advantage of considering sub probability measures is that while it is not clear what
it would mean for a set of weighted probabilities to be convex  indeed  it is not obvious what
should count as a convex combination of  pr    and  pr          it is quite clear what counts
as a convex combination of sub probability measures  moreover  a convex combination of
sub probability measures is itself a sub probability measure 
call a set of subprobability measures regular if it is convex  downward closed  closed 
and contains at least one proper probability measure   the latter requirement corresponds
to having pr     for some pr  p      leung and i provide a set of axioms for preference
orders  and show that a family of preference orders m indexed by menus satisfies these
axioms iff there is a unique regular set of weighted probability measures p   such that  for
m
all a m b iff wr m
p    a   wr p    b   thus  we might hope that we can get uniqueness by
imposing a regularity requirement  it is easy to see that the canonical maximal set p  
constructed in the proof of theorem     is regular  which lends some credence to this hope 
unfortunately  as the following example shows  regularity does not suffice for uniqueness 
example      let s    s    s     and let f be defined on  s by taking f   s           and
f   s          and f  s      and f          a sub probability measure p on s can be
identified with the pair  p s     p s      which makes it easy to think about sub probability
measures on s geometrically  a set of sub probability measures is just a region in ir 
contained in the triangle bounded by the lines x      y      and y      x  a set p  
of subprobability measures is downward closed if  whenever it contains a point  x  y   it
contains all  x    y     in the rectangle defined by the points          x          y   and  x  y  
with this intuition  let p   be the set of subprobabilities in the quadrilateral bounded
by x      y      y      x  and y        the region marked by vertical lines in figure     it
   

fiweighted regret based likelihood

is not hard to show that p   is the maximal set of weighted probabilities representing f   it
 
is clearly regular  since it contains the subprobability         it follows that p  reg
  s         
 
 
it is also easy to see that  since           p  and p s         for all p  p    we have that
 
p  reg
  s           
but now let p   consist of all sub probabilities in the triangle bounded by x      y     
 
and y    x
   the region marked by horizontal lines in figure     clearly p  is a strict
 
subset of of p    but it is clear from the figure that it is also regular  moreover  since it
contains the points           and         it also represents f   indeed  it easily follows from
the geometry of the situation that there are uncountably many regular sets of weighted
probabilities representing f   for all z            the regular set bounded by the lines x     
y      y        and the line from  z       to        
y
 

           

 
 

 

 
 

 

x

figure    regular sets of weighted probability measures that represent f  

intuitively  the problem here is that a function on s does not contain enough information
to uniquely determine a regular set of weighted probability measures  it is not clear whether
there are natural further conditions that can be imposed that we lead to uniqueness  it
seems that the closest that we can come to uniqueness is to consider the maximal set 

   conclusion
i have defined an approach for associating with an event e a numerical representation of
its likelihood when uncertainty is represented by a set of weighted probability measures 
the representation consists of a pair of a numbers  which can be thought of as upper and
lower bounds on the uncertainty  the difference between these numbers can be viewed
as a measure of ambiguity  the two numbers coincide when uncertainty is represented
by a single probability  moreover  if each probability measure gets weight    then the
two numbers can essentially be viewed as the lower and upper probabilities of e  more
precisely     p  e  and    p   e    thus  the approach can be viewed as a generalization
of lower and upper probability to the case of weighted probability measures  with regretbased likelihood corresponding to upper probability  the definitions show that there is
   

fihalpern

an interesting connection between regret based approaches and minimization maximization
approaches when it comes to defining likelihood  this connection breaks down when it comes
to more general utility calculations  halpern   leung        
the main technical result of the paper is a complete characterization of the likelihood
in the case where the state space is finite  the notion of likelihood can easily be extended
to the case of an infinite state space  of course  an integral has to be used instead of a sum
to calculate expected utility   i conjecture that the characterization theorem will still hold
with essentially no change  although i have not checked details carefully 
of course  it would be useful to get a better understanding of this numerical representation  to see if it really captures an agents feelings about both the ambiguity and the risk
associated with an event  and to understand its technical properties  i leave this to future
work 

acknowledgments
i thank samantha leung  the reviewers of ecsqaru  and the jair referees for many useful
comments on the paper  the work was supported in part by nsf grants iis          iis         and ccf          by afosr grants fa                fa                and
fa                and by aro grant w   nf           

references
anger  b     lembcke  j          infinitely subadditive capacities as upper envelopes of
measures  zeitschrift fur wahrscheinlichkeitstheorie und verwandte gebiete         
    
boole  g          an investigation into the laws of thought on which are founded the
mathematical theories of logic and probabilities  macmillan  london 
campos  l  m  d     moral  s          independence concepts for sets of probabilities 
in proc  eleventh conference on uncertainty in artificial intelligence  uai      pp 
       
cattaneo  m  e  g  v          statistical decisions based directly on the likeihood function 
ph d  thesis  eth 
chateauneuf  a     faro  j          ambiguity through confidence functions  journal of
mathematical economics               
couso  i   moral  s     walley  p          examples of independence for imprecise probabilities  in proc  first international symposium on imprecise probabilities and their
applications  isipta     
de cooman  g          a behavioral model for vague probability assessments  fuzzy sets
and systems                  
dubois  d     prade  h          possibility measures  qualitative and quantitative aspects 
in gabbay  d  m     smets  p   eds    quantified representation of uncertainty and
   

fiweighted regret based likelihood

imprecision  vol    of handbook of defeasible reasoning and uncertainty management
systems  pp          kluwer  dordrecht  netherlands 
epstein  l     schneider  m          learning under ambiguity  review of economic
studies                   
fagin  r   halpern  j  y     megiddo  n          a logic for reasoning about probabilities 
information and computation                  
farkas  j          theorie der enfachen ungleichungen  j  reine und angewandte math  
         
gardenfors  p     sahlin  n          unreliable probabilities  risk taking  and decision
making  synthese             
gardenfors  p     sahlin  n          decision making with unreliable probabilities  british
journal of mathematical and statistical psychology             
gilboa  i     schmeidler  d          maxmin expected utility with a non unique prior 
journal of mathematical economics             
gilboa  i     schmeidler  d          updating ambiguous beliefs  journal of economic
theory           
giles  r          foundations for a theory of possibility  in gupta  m  m     sanchez  e 
 eds    fuzzy information and decision processes  pp          north holland 
good  i  j          some history of the hierarchical bayesian methodology  in bernardo 
j  m   degroot  m  h   lindley  d     smith  a   eds    bayesian statistic i  pp 
        university press  valencia 
halpern  j  y          defining relative likelihood in partially ordered preferential structures  journal of a i  research         
halpern  j  y          reasoning about uncertainty  mit press  cambridge  mass 
halpern  j  y     leung  s          weighted sets of probabilities and minimax weighted
expected regret  new approaches for representing uncertainty and making decisions  in
proc  twenty ninth conference on uncertainty in artificial intelligence  uai       
pp          to appear  theory and decision 
halpern  j  y     pucella  r          a logic for reasoning about upper probabilities 
journal of a i  research           
huber  p  j          kapazitaten statt wahrscheinlichkeiten  gedanken zur grundlegung
der statistik  jahresbericht der deutschen mathematiker vereinigung           
huber  p  j          robust statistics  wiley  new york 
klibanoff  p   marinacci  m     mukerji  s          a smooth model of decision making
under ambiguity  econometrica                   
   

fihalpern

kyburg  jr   h  e          higher order probabilities and intervals  international journal
of approximate reasoning            
levi  i          imprecision and uncertainty in probability judgment  philosophy of science 
           
lorentz  g  g          multiply subadditive functions  canadian journal of mathematics 
              
maccheroni  f   marinacci  m     rustichini  a          ambiguity aversion  robustness 
and the variational representation of preferences  econometrica                   
moral  s          calculating uncertainty intervals from conditional convex sets of probabilities  in proc  eighth conference on uncertainty in artificial intelligence  uai
     pp         
nau  r  f          indeterminate probabilities on finite sets  annals of statistics         
         
niehans  j          zur preisbildung bei ungewissen erwartungen  schweizerische zeitschrift
fur volkswirtschaft und statistik                 
ostrogradsky  m  v          extrait dun memoire sur la probabilite des erreurs des tribuneaux  memoires dacademie st  petersbourg  series       xixxxv 
pearl  j          do we need higher order probabilities and  if so  what do they mean   in
proc  third workshop on uncertainty in artificial intelligence  uai      pp       
savage  l  j          the theory of statistical decision  journal of the american statistical
association           
schrijver  a          theory of linear and integer programming  wiley  new york 
scott  d          measurement structures and linear inequalities  journal of mathematical
psychology            
walley  p          statistical reasoning with imprecise probabilities  vol     of monographs
on statistics and applied probability  chapman and hall  london 
walley  p          statistical inferences based on a second order possibility distribution 
international journal of general systems                 
williams  p  m          indeterminate probabilities  in przelecki  m   szaniawski  k    
wojcicki  r   eds    formal methods in the methodology of empirical sciences  pp 
        reidel  dordrecht  netherlands 
wolf  g          obere und untere wahrscheinlichkeiten  ph d  thesis  eth  zurich 
zadeh  l  a          fuzzy sets as a basis for a theory of possibility  fuzzy sets and systems 
       

   

fi