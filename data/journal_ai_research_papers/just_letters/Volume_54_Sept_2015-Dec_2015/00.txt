journal of artificial intelligence research               

submitted        published      

knowledge based textual inference via
parse tree transformations
roy bar haim

barhair gmail com

ido dagan

dagan cs biu ac il

computer science department  bar ilan university
ramat gan        israel

jonathan berant

yonatan cs stanford edu

computer science department  stanford university

abstract
textual inference is an important component in many applications for understanding
natural language  classical approaches to textual inference rely on logical representations
for meaning  which may be regarded as external to the natural language itself  however 
practical applications usually adopt shallower lexical or lexical syntactic representations 
which correspond closely to language structure  in many cases  such approaches lack a principled meaning representation and inference framework  we describe an inference formalism
that operates directly on language based structures  particularly syntactic parse trees  new
trees are generated by applying inference rules  which provide a unified representation for
varying types of inferences  we use manual and automatic methods to generate these rules 
which cover generic linguistic structures as well as specific lexical based inferences  we also
present a novel packed data structure and a corresponding inference algorithm that allows
efficient implementation of this formalism  we proved the correctness of the new algorithm
and established its efficiency analytically and empirically  the utility of our approach was
illustrated on two tasks  unsupervised relation extraction from a large corpus  and the
recognizing textual entailment  rte  benchmarks 

   introduction
textual inference in natural language processing  nlp  is concerned with deriving target
meanings from texts  in the textual entailment framework  dagan  roth  sammons   
zanzotto         this is reduced to inferring a textual statement  the hypothesis h  from
a source text  t   traditional approaches in formal semantics perform such inferences over
logical forms derived from the text  by contrast  most practical nlp applications avoid the
complexities of logical interpretation  instead  they operate over shallower representations
such as parse trees  possibly supplemented with limited semantic information about named
entities  semantic roles  and so forth  this was clearly demonstrated in the recent pascal
recognizing textual entailment  rte  challenges  dagan  glickman    magnini      b 
bar haim  dagan  dolan  ferro  giampiccolo  magnini    szpektor        giampiccolo 
magnini  dagan    dolan        giampiccolo  trang dang  magnini  dagan    dolan 
      bentivogli  dagan  dang  giampiccolo    magnini        bentivogli  clark  dagan 
c
    
ai access foundation  all rights reserved 

fibar haim  dagan   berant

dang    giampiccolo         a popular framework for evaluating application independent
semantic inference  
inference over such representations is commonly made by applying transformations or
substitutions to the tree or graph representing the text  these transformations are based on
available knowledge about paraphrases  lexical relations such as synonyms and hyponyms 
syntactic variations  and more  de salvo braz  girju  punyakanok  roth    sammons 
      haghighi  ng    manning        kouylekov   magnini        harmeling        
such transformations may be generally viewed as inference rules  some of the available semantic knowledge bases were composed manually  either by experts  for example wordnet
 fellbaum         or by a large community of contributors  such as the wikipedia based
dbpedia resource  lehmann et al          other knowledge bases were learned automatically through distributional and pattern based methods  or by using aligned monolingual
or bilingual parallel texts  lin   pantel        shinyama  sekine  sudo    grishman 
      szpektor  tanev  dagan    coppola        chklovski   pantel        bhagat  
ravichandran        ganitkevitch  van durme    callison burch         overall  applied
knowledge based inference is a prominent line of research that has gained much interest  recent examples include the series of workshops on knowledge and reasoning for answering
questions  saint dizier   mehta melkar        and the evaluation of knowledge resources
in the recent recognizing textual entailment challenges  bentivogli et al         
while many applied systems use semantic knowledge through such inference rules  their
use is typically limited  application specific  and somewhat heuristic  formalizing these
practices is important for textual inference research  analogous to the role of well formalized
models in parsing and machine translation  we take a step in this direction by introducing
a generic inference formalism over parse trees  our formalism uses inference rules to capture
a wide variety of inference knowledge in a simple and uniform manner  and specifies a small
set of operations that suffice to broadly utilize such knowledge 
in our formalism  applying an inference rule has a clear  intuitive interpretation of generating a new sentence parse  a consequent   semantically entailed by the source sentence 
the inferred consequent may be subject to further rule applications  and so on  rule applications may be independent of each other  modifying disjoint parts of the source tree  or
may specify mutually exclusive alternatives  e g   different synonyms for the same source
word   deriving the hypothesis from the text is analogous to proof search in logic  where
the propositions are parse trees and deduction steps correspond to rule applications 
a nave implementation of the formalism would generate each consequent explicitly as
a separate tree  however  as we discuss further in section    such implementation raises
severe efficiency issues  since the number of consequents may grow exponentially in the
number of possible rule applications  previous work proposed only partial solutions to this
problem  cf  section     in this work we present a novel data structure  termed compact
forest  for packed representation of entailed consequents  and a corresponding inference
algorithm  we prove that the new algorithm is a valid implementation of the formalism 
and establish its efficiency both analytically  showing typical exponential to linear reduction 
and empirically  showing improvement by orders of magnitude  together  our formalism and
   see  for instance  the listing of techniques per submission that was provided by the organizers of the first
three challenges  dagan et al       b  bar haim et al         giampiccolo et al         

 

fiknowledge based textual inference via parse tree transformations

its novel efficient inference algorithm open the way for large scale rule application within a
well formalized framework 
based on our formalism and inference algorithm  we built an inference engine that
incorporates a variety of semantic and syntactic knowledge bases  cf  section     we
evaluated the inference engine on the following tasks 
   unsupervised relation extraction from a large corpus  this setting allows evaluation
of knowledge based inferences over a real world distribution of texts 
   recognizing textual entailment  rte   to cope with the more complex rte examples  we complemented our knowledge based inference engine with a machine learningbased entailment classifier  which provides necessary approximate matching capabilities 
the inference engine was shown to have a substantial contribution to both tasks  illustrating
the utility of our approach 
bar haim  dagan  greental  and shnarch        and bar haim  berant  and dagan
       described earlier versions of the inference framework and the algorithm for its efficient implementation  respectively  the current article includes major enhancements to
both of these contributions  the formalism is presented in more detail  including further
examples and pseudo code for its algorithms  we present several extensions to the formalism  including treatment of co reference  traces and long range dependencies  and enhanced
modeling of polarity  the efficient inference algorithm is also presented in more detail 
including its pseudo code  in addition  we provide complete proofs for the theorems  which
establish the correctness of this algorithm  finally  the article contains an extended analysis
of the inference component in our rte system  in terms of applicability  coverage  and the
correctness of rule applications 

   background
in this section  we provide background on textual entailment  we then survey the various
approaches applied to the task of recognizing textual entailment  rte   in particular  we
focus on the use of semantic knowledge within current rte systems 
    textual entailment
many semantic applications need to identify that the same meaning is expressed by  or can
be inferred from  various language expressions  for example  question answering systems
need to verify that the retrieved passage text entails the selected answer  given the question
who is john lennons widow   the text yoko ono unveiled a bronze statue of her late
husband  john lennon  to complete the official renaming of englands liverpool airport as
liverpool john lennon airport  entails the expected answer yoko ono is john lennons
widow     similarly  information extraction systems need to validate that the given text
indeed entails the semantic relation that is expected to hold between the extracted slot fillers
 e g   x works for y    information retrieval queries such as alzheimers drug treatment 
   the example is taken from the rte   dataset  bar haim et al         
   this was one of the topics in the trec   ir benchmark  voorhees   harman        

 

fibar haim  dagan   berant

can be rephrased as propositions  e g   alzheimers disease is treated using drugs   which
are expected to be entailed from relevant documents  when selecting sentences to be
included in the summary  multi document summarization systems should verify that the
meaning of the candidate sentence is not entailed by sentences already in the summary  to
avoid redundancy 
this observation led dagan and glickman to propose a unifying framework for modeling
language variability  termed textual entailment  te   dagan   glickman         dagan
et al       b  define te as follows 
we say that t entails h if  typically  a human reading t would infer that h is most
likely true  this somewhat informal definition is based on  and assumes  common human understanding of language as well as common background knowledge 
dagan et al         further discuss te definition and its relation to classical semantic
entailment in linguistics literature  the recognizing textual entailment challenges  rte  
which have been held annually since       dagan et al       b  bar haim et al        
giampiccolo et al               bentivogli et al                have formed a growing research
community around this task 
the holy grail of te research is the development of entailment engines  to be used
as generic modules within different semantic applications  similar to the current use of
syntactic parsers and morphological analyzers  since textual entailment is defined as a
relation between surface texts  it is not bound to a particular semantic representation 
this allows a black box view of the entailment engine  where the input output interface
is independent from the internal implementation  which may employ different types of
semantic representations and inference methods 
    determining entailment
consider the following  t h  pair   
t
h

the oddest thing about the uae is that only         of the   million
people living in the country are uae citizens 
the population of the united arab emirates is   million 

understanding that t  h involves several inference steps  first  we infer from the
reduced relative clause in   million people living in the country the proposition 
      million people live in the country 
next  we observe that the country refers to the uae  so we can rewrite     as
      million people live in the uae 
knowing that uae is an acronym for united arab emirates  we further obtain 
      million people live in the united arab emirates 
   taken from the rte  test set  dagan et al       b  

 

fiknowledge based textual inference via parse tree transformations

we finally paraphrase this to obtain h 
    the population of the united arab emirates is   million 
in general  textual inference involves diverse linguistic and world knowledge  including
knowledge of relevant syntactic phenomena  e g   relative clauses   paraphrasing  x people
live in y  the population of y is x    lexical knowledge  uae  united arab emirates  
and so on  it may also require co reference resolution  for example  substituting the country with uae  we may think of all these types of knowledge as representing inference
rules that define derivation of new entailed propositions or consequents  in this work we
introduce a formal inference framework based on inference rule application  for the current
discussion  however  an informal notion of inference rules would suffice 
the above example illustrates the derivation of h from t through a sequence of inference
rule applications  a procedure generally known as forward chaining  finding the sequence
of rule applications that would get us from t to h  or as close as possible  is thus a search
problem  defined over the space of all possible rule application chains 
ideally  we would like to base our entailment engine solely on trusted knowledge based
inferences  in practice  however  available knowledge is incomplete  and full derivation of h
from t is often not feasible  therefore  requiring strict knowledge based proofs is likely
to yield limited recall  alternatively  we may back off to a more heuristic approximate
entailment classification 
the next two sections survey these two complementary inference types  knowledgebased inference  which is the focus of this research  and approximate entailment matching
and classification 
    knowledge based inference
in this section  we describe some of the common resources for inference rules          and
their use in textual entailment systems         
      semantic knowledge resources
lexical knowledge lexical semantic relations between words or phrases play an important role in textual inference  the most prominent lexical resource is wordnet  fellbaum 
       a manually composed wide coverage lexical semantic database  the following wordnet relations are typically used for inference  synonyms  buy  purchase   antonyms  win
 lose   hypernyms hyponyms  is a relations  violin musical instrument   meronyms
 part of relations  provence france  and derivations such as meeting meet 
many researchers aimed at deriving lexical relations automatically  using diverse methods and sources  much of this automatically extracted knowledge is complementary to
wordnet  however  it is typically less accurate  snow  jurafsky  and ng      a  presented
a method for automatically expanding wordnet with new synsets  achieving high precision 
lins thesaurus  lin        is based on distributional similarity  recently  several works
aimed to extract lexical semantic knowledge from wikipedia  using its metadata  as well as
textual definitions  kazama   torisawa        ponzetto   strube        shnarch  barak 
  dagan        lehmann et al         and others   for a recent empirical study on the
 

fibar haim  dagan   berant

inferential utility of common lexical resources  see the work of mirkin  dagan  and shnarch
       
paraphrases and lexical syntactic inference rules these rules typically represent
entailment or equivalence between predicates  including the correct mapping between their
arguments  e g   acquisition of y by x  x purchase y    much work has been dedicated
to unsupervised learning of such relations from comparable corpora  barzilay   mckeown        barzilay   lee        pang  knight    marcu         by querying the web
 ravichandran   hovy        szpektor et al          or from a local corpus  lin   pantel 
      glickman   dagan        bhagat   ravichandran        szpektor   dagan       
yates   etzioni         in particular  textual entailment systems have widely used the
dirt resource of lin and pantel  the common idea underlying these algorithms  is that
predicates sharing the same argument instantiations are likely to be semantically related 
nomlex plus  meyers  reeves  macleod  szekeley  zielinska    young        is a lexicon containing mostly nominalizations of verbs  with allowed argument structures  e g  
xs acquisition of y ys acquisition by x etc    argument mapped wordnet  amwn 
 szpektor   dagan        is a resource for inference rules between verbal and nominal predicates  including their argument mapping  it is based on wordnet and nomlex plus  and
was verified statistically through intersection with the unary dirt algorithm  szpektor  
dagan        
syntactic transformations textual entailment often involves inference over generic
syntactic phenomena such as passive active transformations  appositions  conjunctions  etc  
as illustrated in the following examples 
 john smiled and laughed  john laughed  conjunction 
 my neighbor  john  came in  john is my neighbor  apposition 
 the paper that im reading is interesting  im reading a paper  relative clause  
syntactic transformations have been addressed to some extent by de salvo braz et al 
       and romano  kouylekov  szpektor  dagan  and lavelli         we describe a novel
syntactic rule base for entailment  based on a survey of relevant linguistic literature  as well
as on extensive data analysis  sections         
      the use of semantic knowledge in textual entailment systems
following our description of common knowledge sources for textual inference  we now discuss
the use of such knowledge in textual entailment systems 
textual entailment systems usually represent t and h as trees or graphs  based on their
syntactic parse  predicate argument structure  and various semantic relations  entailment
is then determined by measuring how well h is matched  or embedded   in t  or by estimating
the distance between t and h  commonly defined as the cost of transforming t into h  in
the next section  we briefly cover various methods that have been proposed for approximate
matching and heuristic transformations of graphs and trees  the role of semantic knowledge
in this general scheme is to bridge the gaps between t and h that stem from language
variability  for example  applying the lexical semantic rule purchase buy to t allows the
matching of the word buy appearing in h with the word purchase appearing in t 
 

fiknowledge based textual inference via parse tree transformations

most rte systems restrict both the type of allowed inference rules and the search space 
systems based on lexical  word based or phrase based  matching of h in t  haghighi et al  
      maccartney  galley    manning        or on heuristic transformation of t into h
 kouylekov   magnini        harmeling        typically apply only lexical rules  without
variables   where both sides of the rule are matched directly in t and h 
hickl        derived from a given  t  h  pair a small set of consequents that he terms
discourse commitments  the commitments were generated by several different tools and
techniques  based on syntax  conjunctions  appositions  relative clauses  etc    co reference 
predicate argument structure  the extraction of certain relations  and paraphrase acquisition
from the web  pairs of commitments derived from t and h were fed into the next stages
of the rte system  lexical alignment and entailment classification  prior to commitment
generation  several linguistic preprocessing modules were applied to the text  including
syntactic dependency parsing  semantic dependency parsing  named entity recognition  and
co reference resolution  hickl employed a probabilistic finite state transducer  fst  based
extraction framework for commitment generation  and extraction rules were modeled as a
series of weighted regular expressions  the commitments in their textual form were fed
back into the system  until no additional commitments were generated 
de salvo braz et al         were the first to incorporate syntactic and semantic inference
rules in a comprehensive entailment system  in their system  inference rules are applied over
hybrid syntactic semantic structures called concept graphs  when the left hand side  lhs 
of a rule is matched in the concept graph  the graph is augmented with an instantiation
of the right hand side  rhs  of the rule  after several iterations of rule application  their
system attempts to embed the hypothesis in the augmented graph  other types of semantic
knowledge  such as verb normalization and lexical substitutions  are applied either before
rule application  at preprocessing time  or after rule application  as part of hypothesis
subsumption  embedding  
several entailment systems are based on logical inference  bos and markert             
represented t and h as drs structures used in discourse representation theory  kamp  
reyle         which were then translated into first order logic  background knowledge
 bk  was encoded as axioms  and comprised lexical relations from wordnet  geographical
knowledge  and a small set of manually composed axioms encoding generic knowledge 
bos and markert used a logic theorem prover to find a proof that t entails h  alone or
together with the background knowledge bk   or that h and t are inconsistent with each
other  implying non entailment  or with the background knowledge  the logic prover was
complemented by a model builder that aimed to find counter examples  e g   a model where
t  h holds   the logical inference system suffered from low coverage  due to the limited
background knowledge available  and was able to find proofs only for a small fraction of the
rte  dataset  therefore  the rte system of bos and markert combined logical inference
with a shallow approximate matching method  based mainly on word overlap 
lccs logic based entailment system  tatu   moldovan        was one of the top performers in rte  and rte   tatu  iles  slavick  novischi    moldovan        tatu  
moldovan         it was based on proprietary tools for deriving rich semantic representations  and on extensive knowledge engineering  the syntactic parses of t and h were
transformed into logic forms  moldovan   rus         and this representation was enriched
with a variety of relations extracted by a semantic parser  as well as named entities and
 

fibar haim  dagan   berant

temporal relations  inference knowledge included on demand axioms based on extended
wordnet lexical chains  wordnet glosses  and nlp rewrite rules  additional knowledge
types included several hundreds of world knowledge axioms  temporal axioms  and semantic composition axioms  e g   encoding the transitivity of the kinship relation   based on
the rich semantic representation and the extensive set of axioms  a theorem prover aimed
to prove by refutation that t entails h  if the proof failed  h was repeatedly simplified until
a proof was found  reducing the proof score with each simplification 
    approximate entailment classification
semantic knowledge is always incomplete  therefore  in most cases  knowledge based inference must be complemented with approximate  heuristic methods for determining entailment  most rte systems employ only a limited amount of semantic knowledge  and
focus on methods for approximate entailment classification  a common architecture for
rte systems  hickl  bensley  williams  roberts  rink    shi        snow  vanderwende 
  menezes      b  maccartney  grenager  de marneffe  cer    manning        comprises
the following stages 
   linguistic processing  includes syntactic  and possibly semantic  parsing  namedentity recognition  co reference resolution  etc  often  t and h are represented as trees
or graphs  where nodes correspond to words and edges represent relations between
words 
   alignment  find the best mapping from h nodes to t nodes  taking into account both
node and edge matching 
   entailment classification  based on the alignment found  a set of features is extracted
and passed to a classifier for determining entailment  these features measure the
alignment quality  and also try to detect cues for false entailment  for example  if a
node in h is negated but its aligned node in t is not negated  it may indicate false
entailment 
an alternative approach aims to transform the text into the hypothesis  rather than
aligning them  kouylekov and magnini        applied a tree edit distance algorithm for
textual entailment  each edit operation  node insertion deletion substitution  is assigned a
cost  the algorithm aims to find the minimum cost sequence of operations that transform
t into h  mehdad and magnini      b  proposed a method for estimating the cost of
each edit operation based on particle swarm optimization  wang and manning       
presented a probabilistic tree edit approach that models edit operations using structured
latent variables  tree edits are represented as state transitions in a finite state machine
 fsm   and the model is parameterized as a conditional random field  crf   harmeling
       developed a probabilistic transformation based approach  he defined a fixed set of
operations  including syntactic transformations  wordnet based substitutions  and more
heuristic transformations such as adding removing a verb or a noun  the probability of
each transformation was estimated from the development set  similarly  heilman and smith
       classify entailment based on the sequence of edits transforming t to h  they employ
more generic edit operations and a greedy search heuristic  which is guided by a cost function
that measures the remaining distance from h using a tree kernel 
 

fiknowledge based textual inference via parse tree transformations

zanzotto  pennacchiotti  and moschitti        aimed to classify a given  t  h  pair by
analogy to similar pairs in the training set  their method is based on finding intra pair
alignment  i e   between t and h  for capturing the transformation from t to h  and interpair alignment  capturing the analogy between the new pair  t  h  and a previously seen
pair  t    h     a cross pair similarity kernel is then computed  based on tree kernel similarity
applied to the aligned texts and the aligned hypotheses  another cross pair similarity kernel
was proposed by wang and neumann         they extracted tree skeletons from t and h 
consisting of left and right spines  defined as unlexicalized paths starting at the root  they
then found sections where t and h spines differ and compared these sections across pairs
using a subsequence kernel 

   research goal
the goal of textual entailment research is to develop entailment engines that can be used as
generic inference components within various text understanding applications  logic based
entailment systems provide a formalized and expressive framework for textual inference 
however  deriving logic representations from text is a complex task  and available tools do
not match the accuracy and robustness of current syntactic parsers  which is often the basis
for semantic parsing   furthermore  interpretation into logic forms is often unnecessary  as
many of the common inferences can be modeled with shallower representations 
it follows that most textual entailment systems  and text understanding applications
in general  operate over lexical syntactic representations  possibly supplemented with some
partial semantic annotation  however  unlike logic based approaches  most of these systems
lack a clear  unified formalism for knowledge representation and inference  instead they
employ multiple representations and inference mechanisms  a notable exception is the
natural logic framework of maccartney and manning         which has a rather different
focus than the current work  we discuss this further in section   
in this work  we develop a well formalized entailment approach for the lexical syntactic
level  our formalism models a wide variety of inference rules and their composition  based
on a unified representation and a small set of inference operations  moreover  we present
an efficient implementation of this formalism using a novel data structure and algorithm
that allow compact representation of the proof search space 
we see the contribution of this work as both practical and theoretical  from a practical
 or engineering  perspective  our formalism may simplify the development of entailment
systems  as the number of representations and inference mechanisms that need to be dealt
with is minimal  furthermore  our efficient implementation may allow entailment engines to
explore much larger search spaces  from a theoretical perspective  concise  formal modeling
leads to better insight into the phenomenon under investigation  in particular  having a
formal model of an entailment engine makes it possible to apply formal methods for investigating its properties  this enabled us to prove the correctness of the efficient implementation
of our formalism  cf  appendix a   we next present our inference formalism 
 

fibar haim  dagan   berant

rule
type
syntactic

sources

examples

manually composed

lexical

learned with unsupervised algorithms  dirt  tease   and
derived automatically by integrating information from wordnet and
nomlex  verified using corpus
statistics  amwn 
wordnet  wikipedia

passive active  apposition  relative
clause  conjunctions
xs wife  y  x is married to y

syntactic

lexical

x bought y  y was sold to x

x is a maker of y  x produces y
steal take  albanianalbania
janis joplinsinger
amazonsouth america

table    representing diverse knowledge types as inference rules

   an inference formalism over parse trees
the previous sections highlighted the need for a more principled  well formalized approach
for textual inference at the lexical syntactic level  in this section  we propose a step towards
filling this gap  by defining a formalism for textual inference over parse based representations  all semantic knowledge required for inference is represented as inference rules  which
encode parse tree transformations  each rule application generates a new consequent sentence  represented as a parse tree  from a source tree  figure  b shows a sample inference
rule  representing a passive to active transformation 
from a knowledge representation and usage perspective  inference rules provide a simple
unifying formalism for representing and applying a very broad range of inference knowledge 
some examples of this breadth are illustrated in table    from a knowledge acquisition
perspective  representing inference rules at the lexical syntactic level allows easy incorporation of rules learned by unsupervised methods  which is important for scaling inference
systems  interpretation into stipulated semantic representations  which is often difficult and
is inherently a supervised semantic task for learning  is circumvented altogether  from a
historical machine translation perspective  our approach is similar to transfer based translation  as contrasted with semantic interpretation into interlingua  our overall research goal
is to explore the reach of such an inference approach  and to identify the scope in which
semantic interpretation may not be needed 
given a syntactically parsed source text and a set of inference rules  our formalism
defines the set of consequents derivable from the text using the rules  each consequent is
obtained through a sequence of rule applications  each generating an intermediate parse
tree  similar to a proof process in logic  in addition  new consequents may be inferred based
on co reference relations and identified traces  our formalism also includes annotation rules
that add features to existing trees  according to the formalism  a text t entails a hypothesis
h if h is a consequent of t 
in the rest of this section  we define and illustrate each of the formalism components 
sentence representation  section       inference rules and their application  sections    
      inference based on co reference relations and traces  section       and annotation
  

fiknowledge based textual inference via parse tree transformations

input  a source tree s   a rule e   l  r
output  a set d of derived trees
m  the set of all matches of l in s
d
for each f  m do
l  the subtree matched by l in s according to match f
   r instantiation
r  a copy of r
for each variable v  r do
instantiate v with f  v 
for each aligned pair of nodes ul  l and ur  r do
for each daughter m of ul such that m 
  l do
copy the subtree of s rooted in m under ur in r  with the same dependency relation
   derived tree generation
if substitution rule then
d  s copy with l  and the descendants of its nodes  replaced by r
else    introduction rule
dr
add d to d

algorithm    applying a rule to a tree
rules  section       these components form an inference process that specifies the set of
inferable consequents for a given text and a set of rules  section       section     extends
the hypothesis definition  allowing h to be a template rather than a proposition  finally 
section     discusses limitations and possible extensions of our formalism 
    sentence representation
we assume that sentences are represented by some form of parse trees  in this work  we focus
on dependency tree representation  which is often preferred to directly capture predicateargument relations  two dependency trees are shown in figure  a  nodes represent words
and hold a set of features and their values  these features include the word lemma and
part of speech  and additional features that may be added during the inference process 
edges are annotated with dependency relations 
    inference rules
an entailment  or inference  rule l  r is primarily composed of two templates  lefthand side  lhs  l and right hand side  rhs  r  templates are dependency subtrees 
which may contain pos tagged variables  matching any lemma  figure   shows a passiveto active transformation rule  and illustrates its application 
the rule application procedure is given in algorithm    rule application generates a set
d of derived trees  consequents  from a source tree s through the steps described below 
  

fibar haim  dagan   berant

root


i

rain verb

expletive

r

wha

it other

 

when adj
i

r

mary noun
mod

see verb

obj

q


mod

 bysubj

be

be verb

by prep

 

yesterday noun

 pcompn


little adj

john noun

source  it rained when little mary was seen by john yesterday 

root
i



rain verb

r

expletive

wha

it other

 

when adj


i
subj

r

john noun

see verb
 obj

mod

 

mary noun yesterday noun
mod



little adj
derived  it rained when john saw little mary yesterday 

 a  passive to active tree transformation


v verb
obj

l

u

n  noun

v verb

bysubj
be

subj

obj



 

u

 

be verb

by prep

n  noun

n  noun

pcompn

r



n  noun
 b  passive to active substitution rule 
figure    application of an inference rule  pos and relation labels are based on minipar
 lin         n    n   and v are variables  whose instances in l and r are implicitly aligned 
the by subj dependency relation indicates a passive sentence 

  

fiknowledge based textual inference via parse tree transformations

root

root



i

i



v  verb  v  verb
l



wha

r

when adj
i



v  verb
figure    temporal clausal modifier extraction  introduction rule 

      l matching
first  matches of l in the source tree s are sought  l is matched in s if there exists a
one to one node mapping function f from l to s  such that 
   for each node u in l  f  u  has the same features and feature values as u  variables
match any lemma value in f  u  
   for each edge u  v in l  there is an edge f  u   f  v  in s  with the same dependency
relation 
if matching fails  the rule is not applicable to s  in our example  the variable v is matched
in the verb see  n   is matched in mary and n   is matched in john  if matching succeeds 
then the following is performed for each match found 
      r instantiation
a copy of r is generated and its variables are instantiated according to their matching node
in l  in addition  a rule may specify alignments  defined as a partial function from l nodes
to r nodes  an alignment indicates that for each modifier m of the source node that is not
part of the rule structure  the subtree rooted at m should also be copied as a modifier of the
target node  in addition to explicitly defining alignments  each variable in l is implicitly
aligned to its counterpart in r  in our example  the alignment between the v nodes implies
that yesterday  modifying see  should be copied to the generated sentence  and similarly
little  modifying mary  is copied for n   
      derived tree generation
let r be the instantiated r  along with its descendants copied from l through alignment 
and l be the subtree matched by l  the formalism has two methods for generating the
derived tree d  substitution and introduction  as specified by the rule type  substitution
rules specify modification of a subtree of s  leaving the rest of s unchanged  thus  d is
formed by copying s while replacing l  and the descendants of ls nodes  with r  this is
the case for the passive rule  as well as for lexical rules such as buy  purchase  by
contrast  introduction rules are used to make inferences from a subtree of s  while the other
parts of s are ignored and do not affect d  a typical example is inferring a proposition
embedded as a relative clause in s  in this case  the derived tree d is simply taken to be
  

fibar haim  dagan   berant

root
i

root
i



buy verb
subj



purchase verb

obj

subj

obj

v

 

v

 

john noun

books noun

john noun

books noun

john bought books 

l

buy verb

john purchased books 



purchase verb

r

figure    application of a lexical substitution rule  the dotted arc represents explicit
alignment 

r  figure   presents such a rule  which enables deriving propositions that are embedded
within temporal modifiers  note that the derived tree does not depend on the main clause 
applying this rule to the right part of figure  a yields the proposition john saw little
mary yesterday 
    further examples for rule application
in this section we further illustrate rule representation and application through additional
examples 
      lexical substitution rule with explicit alignment
figure   shows the derivation of the consequent john purchased books from the sentence
john bought books using the lexical substitution rule buy  purchase  this example
illustrates the role of explicit alignment  since buy and purchase are not variables  they are
not implicitly aligned  however  they need to be aligned explicitly  otherwise the daughters
of buy would not be copied under purchase 
      lexical syntactic introduction rule
figure   illustrates the application of a lexical syntactic rule  which derives the sentence
her husband died from i knew her late husband  it is defined as introduction rule  since
the resulting tree is derived based solely on the phrase her late husband  while ignoring
the rest of the source tree  this example illustrates that a leaf variable in l  variable
at a leaf node  may become a non leaf in r and vice versa  the alignment between the
instances of variable n  matched in husband   allows copying of its modifier  her  recall that
such alignments are defined implicitly by the formalism   we note here that the correctness
of rule application may depend on the context in which it is applied  for instance  the
rule in our example is correct only if late has the meaning of no longer alive in the given
context  we discuss context sensitivity of rule application in section     
  

fiknowledge based textual inference via parse tree transformations

root

root

i

i



know verb
subj

die verb

obj

subj

v

 

i noun

husband noun
gen




husband noun

mod

v

 

her noun

late adj

gen



her noun

i knew her late husband 

her husband died 

root
i

l



n noun

 die verb



subj

mod

late adj



r

n noun

figure    application of a lexical syntactic introduction rule

    co reference and trace based inference
aside from the primary inference mechanism of rule application  our formalism also allows
inference based on co reference relations and long distance dependencies  we view coreference as an equivalence relation between complete subtrees  either within the same tree
or in different trees  which are linked by a co reference chain  in practice  such relations are
obtained from an external co reference resolution tool  as part of the text pre processing 
the co reference substitution operation is similar to the application of a substitution rule 
given a pair of co referring subtrees  t  and t    the derived tree is generated by copying
the tree containing t    while replacing t  with t    the same operation is symmetrically
applicable for t     for example  given the sentences  my brother  is a musician   he  plays
the drums  we can infer that my brother plays the drums 
long distance dependencies are another type of useful relation for inference  as illustrated by the following examples 
    relative clause  the boyi whom  i saw ti   went home 
  i saw the boy  
    control verbs  johni managed to  ti open the door  
  john opened the door  
   the view of co referring expressions as substitutional can also be found in the seminal paper of van
deemter and kibble         where noun phrases are shown to be non substitutable as evidence that
they are not co referring 

  

fibar haim  dagan   berant

    verbal conjunction   johni sang  and  ti danced  
  john danced  
some parsers including minipar  which we use in the current work  recognize and annotate
such long distance dependencies  for instance  minipar generates a node representing the
trace  ti in the examples   which holds a pointer to its antecedent  e g   johni in       as
shown in these examples  inference from such sentences may involve resolving long  distance
dependencies  where traces are substituted with their antecedent  thus  we can generalize
co reference substitution to operate over trace antecedent pairs  as well  this mechanism
works together with inference rule application  for instance  after substituting the trace
with its antecedent in     we obtain john managed to  john opened the door   we then
apply the introduction rule n managed to s  s to extract the embedded clause john
opened the door 
    polarity annotation rules
in addition to inference rules  our formalism implementation includes a mechanism for
adding semantic features to parse tree nodes  however  in many cases there is no natural
way to define semantic features or classes  hence  it is often difficult to agree on the right
set of semantic annotations  a common example is the definition of word senses   with
our approach  we aim to keep semantic annotation to a minimum  while sticking to lexicalsyntactic representation  for which widely agreed schemes do exist 
consequently  the only semantic annotation we employ is predicate polarity  this feature
marks the truth of a predicate  and may take one of the following values  positive    
negative    or unknown     some examples of polarity annotation are shown below 
    john called    mary 
    john hasnt called   mary yet 
    john forgot to call   mary 
    john might have called    mary 
    john wanted to call    mary 
sentences     and     both entail john didnt call mary  hence the negative annotation of
call  by contrast  the truth of john called mary cannot be determined from     and     
therefore the predicate call is marked as unknown  in general  the polarity of predicates
may be affected by the existence of modals  negation  conditionals  certain verbs  etc 
technically  annotation rules do not have a right hand side r  but rather each node of l
may contain annotation features  if l is matched in a tree  then the annotations it contains
are copied to the matched nodes  figure   shows an example of annotation rule application 
predicates are assumed to have positive polarity by default  the polarity rules are used
to mark negative or unknown polarity  if more than one rule applies to the same predicate
 as with the sentence john forgot not to call mary   they may be applied in any order 
and the following simple calculus is employed to combine current polarity with new polarity 
  

fiknowledge based textual inference via parse tree transformations

root
i

v  
l

be



listen  
subj

verb

verb

be



v

 

be verb

john noun

be verb

neg

neg



not adj



not adj
john is not listening    

 a  annotation rule

 b  annotated sentence

figure    application of the annotation rule  a   marking the predicate listen with negative
polarity  b 

current polarity
 

 
      

new polarity



 

result

 
 
 

annotation rules are used for detecting polarity mismatches between the text and the hypothesis  incompatible polarity would block the hypothesis from being matched in the text 
in the case of approximate entailment classification  polarity mismatches detected by the
annotation rules are used as features for the classifier  as we discuss further in section      in
addition  the existence of polarity annotation features may prevent inappropriate inference
rule applications  by blocking their l matching  we discuss this further in section     
    the inference process
let t be a set of dependency trees representing the text  along with co reference and
trace information  let h be the dependency tree representing the hypothesis  and let r
be a collection of inference rules  including both inference and polarity rules   based on
the previously defined components of our inference framework  we next give a procedural
definition for the set of trees inferable from t using r  denoted i t  r   the inference
process comprises the following steps 
   initialize i t  r  with t  
   apply all matching polarity rules in r to each of the trees in i t  r   cf  section      
   replace all the trace nodes with a copy of their antecedent subtree  cf  section      
   add to i t  r  all the trees derivable by co reference substitution  cf  section      
  

fibar haim  dagan   berant

   apply all matching inference rules in r to the trees in i t  r   cf  section       and
add the derived trees to i t  r   repeat this step iteratively for the newly added
trees  until no new trees are added 
steps   and   are performed for h as well   h is inferable from t using r if h  i t  r  
since i t  r  may be infinite or very large  practical implementation of this process must
limit the search space  for example by restricting the number of iterations and the applied
rules at each iteration 
when an inference rule is applied  polarity annotation is propagated from the source
tree s to the derived tree d as follows  first  nodes copied from s to d retain their original
polarity  second  a node in d gets the polarity of its aligned node in s 
    template hypotheses
for many applications it is useful to allow the hypothesis h to be a template rather than a
proposition  that is  to contain variables  the variables in this case are existentially quantified  t entails h if there exists a proposition h    obtained from h by variable instantiation 
so that t entails h    each variable x is instantiated  replaced  with a subtree sx   if x
has modifiers in h  i e   x is not a leaf   they become modifiers of sx s root  the obtained
variable instantiations may stand for answers sought in questions or slots to be filled in relation extraction  for example  applying this framework in a question answering setting  the
question who killed kennedy  may be transformed into the hypothesis x killed kennedy 
a successful proof of h from the sentence the assassination of kennedy by oswald shook
the nation would instantiate x with oswald  providing the sought answer 
    limitations and possible extensions
we conclude this section by discussing some limitations of the presented inference formalism 
as well as possible extensions to address these limitations  first  our inference rules match
only a single subtree  and therefore are less expressive than the logic axioms used by bos
and markert        and tatu and moldovan         which may combine several predicates
originating from the text representation as well as from the background knowledge  this
allows logic based systems to make inferences that combine multiple pieces of information 
for instance  if the text says that a person x lives in a city y   and the background knowledge
tells us that the city y is in country z  we can infer that x lives in country z  using a
rule such as person x   location y   location z   live x y   in y z   live x z   
schoenmackers  etzioni  weld  and davis        describe a system that acquires such rules
 first order horn clauses  from web text  allowing our rules to match multiple subtrees in
t  as well as information in the background knowledge  seems a plausible future extension
to our formalism 
another limitation of the formalism is the lack of context disambiguation  word sense
mismatch is a potential cause for incorrect rule applications  for example  the rule hit 
score is applied correctly in     but not in      
   step   is not applied to h since the hypothesis is typically a short  simple sentence that usually does not
include co referring nps  moreover  in the presented formalism h is a single tree  applying co referencebased inference would have resulted in additional trees inferred from h  and thus would have required
extending the formalism accordingly 

  

fiknowledge based textual inference via parse tree transformations

    the team hit a home run   the team scored a home run 
     the car hit a tree    the car scored a tree 
several works over the past years addressed the problem of context dependent rule application  dagan  glickman  gliozzo  marmorshtein    strapparava      a  pantel  bhagat 
coppola  chklovski    hovy        connor   roth        szpektor  dagan  bar haim   
goldberger        dinu   lapata        ritter  mausam    etzioni        berant  dagan 
  goldberger        melamud  berant  dagan  goldberger    szpektor         szpektor
et al         proposed a comprehensive framework for modeling context matching  termed
contextual preferences  cp   given a text t  a hypothesis h  possibly a template hypothesis  and an inference rule r bridging between t and h  each of these objects is annotated
with two context components   a  global  topical  context  and  b  preferences and constraints on the instantiation of the objects variables  for r and template h   cp requires
that h and r are matched in t  and h is matched in r    where each context component
is matched to its counterpart  szpektor et al  also proposed concrete implementations for
each of these components  in the above example  we could model the global context of t and
r as the sets of their content words  and compute the semantic relatedness between these
two sets  using methods such as latent semantic analysis  lsa   deerwester  dumais 
furnas  landauer    harshman         or explicit semantic analysis  esa   gabrilovich
  markovitch         we would expect that the semantic relatedness between  score  and
 team  home run  will be much higher than between  score  and  car  tree   which would
permit inference in     but not in      
in most rte systems  including our system in the rte experiments  described in
section      lexicalized rules bridge between t and h directly  so that the rules lhs and
rhs are matched in t and h  respectively  since in the rte benchmarks t and h tend to
have the same semantic context  this setting alleviates context matching problems to some
extent  however  our analysis  presented later in this work  subsection         shows that
context matching remains an issue even in this setting  and is expected to become even more
important when chaining of lexicalized rules is attempted  adding contextual preferences
to our formalism is an important direction for future work 
the validity of rule application also depends on the monotonicity properties of its application site  for instance  the hypernym rule poodle  dog is applicable only in upward
monotone contexts  monotonicity may be affected by the presence of quantifiers  negation  and certain verbs such as implicatives and counterfactives  nairn  condoravdi   
karttunen         as common with textual entailment systems  we assume upward monotonicity anywhere  while this assumption usually holds true  in some cases it may lead to
incorrect inferences  the following examples show correct applications of the above rule in
upward monotone contexts              and incorrect applications in downward monotone
contexts                  
     she bought a poodle   she bought a dog 
     she didnt buy a poodle   she didnt buy a dog
     poodles are smart    dogs are smart 
   context matching  like textual entailment  is a directional relation 

  

fibar haim  dagan   berant

     she failed to avoid buying a poodle  she failed to avoid buying a dog 
     she did not fail to avoid buying a poodle   she did not fail to avoid buying a dog 
maccartney and manning        address monotonicity as well as other semantic relations
such as exclusion  in a natural logic framework based on syntactic representation  we
discuss their work in more detail in section   
finally  since our polarity annotation rules are applied locally  they may fail in complex
cases  such as computing the polarity of buying in sentences      and       in which polarity
information need to be propagated along the syntactic structure of the sentence  the
truthteller system  lotan  stern    dagan         computes predicate polarity  truth
value  by a combination of annotation rules and a global polarity propagation algorithm 
extending previous work by nairn et al         and maccartney and manning        
    summary
in this section  we presented a well formalized approach for textual inference over parsebased representations  which is the core of this paper  in our framework  semantic knowledge
is represented uniformly as inference rules specifying tree transformations  we provided
detailed definitions for the representation of these rules as well as the inference mechanisms
that apply them  our formalism also models inferences based on co reference relations and
traces  in addition  it includes annotation rules that are used to detect contexts affecting
the polarity of predicates  in the next section we present an efficient implementation of this
formalism 

   a compact forest for scalable inference
according to our formalism  each rule application generates a new sentence parse  a consequent   semantically entailed by the source sentence  each inferred consequent may be
subject to further rule applications  and so on  a straightforward implementation of this
formalism would generate each consequent as a separate tree  unfortunately  this nave
approach raises severe efficiency issues  since the number of consequents may grow exponentially in the number of rule applications  consider  for example  the sentence children
are fond of candies  and the following rules  childrenkids  candiessweets  and x is
fond of yx likes y  the number of derivable sentences  including the source sentence 
would be     the power set size   as each rule can either be applied or not  independently  we
found that this exponential explosion leads to poor scalability of the nave implementation
approach in practice 
intuitively  we would like for each rule application to add just the entailed part of the rule
 e g   kids  to a packed sentence representation  yet  we still want the resulting structure
to represent a set of entailed sentences  rather than a mixture of sentence fragments with
unclear semantics  as discussed in section    previous work proposed only partial solutions
to this problem 
in this section  we introduce a novel data structure  termed compact forest  and a corresponding inference algorithm  which efficiently generate and represent all consequents while
preserving the identity of each individual one  this data structure allows compact representation of a large set of inferred trees  each rule application generates explicitly only the
  

fiknowledge based textual inference via parse tree transformations

nodes of the rules right hand side  the rest of the consequent tree is shared with the source
sentence  which also reduces the number of redundant rule applications  as explained later
in this section  we show that this representation is based primarily on disjunction edges 
an extension of dependency edges that specify a set of alternative edges of multiple trees 
since we follow a well defined inference formalism  we are able to prove that all inference
operations in our formalism are equivalently applied over the compact forest  we compare
inference cost over compact forests to explicit consequent generation both theoretically 
illustrating an exponential to linear complexity ratio  and empirically  showing improvement
by orders of magnitude  empirical results are reported in section      
    the compact forest data structure
a compact forest f represents a set of dependency trees  figure  d shows an example of a
compact forest containing trees for the sentences little mary was seen by john yesterday
and john saw little mary yesterday  we first define a more general data structure for
directed graphs  and then narrow the definition to the case of trees 
a compact directed graph  cdg  is a pair g    v  e  where v is a set of nodes and e
is a set of disjunction edges  d edges   let d be a set of dependency relations  a d edge
d is a triple  sd   reld   td    where sd and td are disjoint sets of source nodes and target
nodes  reld   sd  d is a function specifying the dependency relation that corresponds to
each source node  graphically  d edges are shown as point nodes  with incoming edges from
source nodes and outgoing edges to target nodes  for instance  let d be the bottommost
d edge in figure    then sd    of  like   td    candy  sweet   rel of     pcomp n  and
rel like    obj  
a d edge represents  for each si  sd   a set of alternative directed edges   si   tj     tj 
td    all of which are labeled with the same relation given by reld  si    each of these edges 
termed embedded edge  e edge   would correspond to a different graph represented in g 
obj

obj

pcompn

in the previous example  the e edges are likecandy  likesweet  ofcandy and
pcompn
ofsweet  the definition implies that all source nodes in sd have the same set of
alternative target nodes td    the d edge d is called an outgoing d edge of a node v if v  sd
and an incoming d edge of v if v  td   a compact directed acyclic graph  cdag  is a
cdg that contains no cycles of e edges 
a dag g rooted in a node v  v of a cdag g is embedded in g if it can be derived
as follows  we initialize g with v alone  then  we expand v by choosing exactly one target
node t  td from each outgoing d edge d of v  and adding t and the corresponding e edge
 v  t  to g  this expansion process is repeated recursively for each new node added to g 
each such set of choices results in a different dag with v as its only root  in figure  d 
we may choose to connect the root either to the left see  resulting in the source passive
sentence  or to the right see  resulting in the derived active sentence 
a compact forest f is a cdag with a single root r  i e   r has no incoming d edges 
where all the embedded dags rooted in r are trees  this set of trees  termed embedded
trees  and denoted t  f  comprise the set of trees represented by f 
figure   shows another example of a compact forest efficiently representing the    sentences resulting from the three independently applied rules presented at the beginning of
this section 
  

fibar haim  dagan   berant

root

root

i

i

see

v

by subj obj

by

be

mary

pcomp n

john

see

mod

be

by subj obj

yesterday

by

mod

pcomp n

little

mod

be

yesterday

little

 b  variable instantiation

root

root

i

i

see
obj

be

mod

john

 a  right hand side generation

by subj

mary

see

see

see

be

by subj be

mod mod

see
mod

mod

obj

obj

subj

by
pcomp n

john

mary

be

yesterday

by

mod

be

yesterday
pcomp n

little

john

 c  alignment sharing

mary
mod

little

 d  dual leaf variable sharing

figure    step by step construction of the compact forest containing both the source sentence little mary was seen by john yesterday and the sentence john saw little mary
yesterday derived from it via the application of the passive rule of figure  b  parts of
speech are omitted 

    the inference process
we next describe the algorithm implementing the inference process described in section    
over the compact forest  henceforth  compact inference   illustrated by figures  b  the
passive to active rule  and   
  

fiknowledge based textual inference via parse tree transformations

root
i

be
pred

fond

like

mod subj

subj
obj

of

child

kid
pcomp n

candy

sweet

figure    a compact forest representing the    sentences derivable from the sentence children are fond of candies using the following three rules  childrenkids  candiessweets 
and x is fond of yx likes y 

      forest initialization
f is initialized with the set of dependency trees representing the text sentences  with their
roots connected under the forest root as the target nodes of a single d edge  dependency
edges are transformed trivially to d edges with a single source and target  annotation
rules are applied at this stage to the initial f  figure  a  without the node labeled v
and its incoming edge  corresponds to the initial forest  containing a single sentence in our
example  
      inference rule application
inference rule application comprises the steps described below  which are summarized in
algorithm   
l matching we first find all the matches of the rules lhs l in the forest f  line     for
the sake of brevity  we omitted the technical details of the l matching implementation from
the pseudocode of algorithm    the following is a high level description of the matching
procedure  focusing on the key algorithmic points 
l is matched in f if there exists an embedded tree t in f such that l is matched in
t  as in section      we denote by l the subtree of t in which l was matched  line    
  

fibar haim  dagan   berant

input  a compact forest f   an inference rule e   l  r
output  a modified f  denoted f     such that t  f       t  f   d  where d is the set of trees derived by
applying e for any subset of ls matches in each of the trees in t  f 
   m  the set of all matches of l in f
   for each match f  m do
  
l  the subtree of f in which l is matched according to f
  
  
  
  
  
  
   
   
   
   
   

   right hand side generation
sr  copy of r excluding dual leaf variable nodes
add sr to f
sl  l excluding dual leaf variable nodes
rr  root sr  
rl  root l 
if e is a substitution rule then
d  the incoming d edge of rl    will set sr as an alternative to sl
else    introduction rule
d  the outgoing d edge of root f     will set sr as an alternative to other trees in t  f 
add rr to td

   
   
   
   
   

   variable instantiation
for each variable x held in node xr  sr do    rs variables excluding dual leaves
if x is not a leaf in l then
xl  f  x     the node in sl matched by x
 xr  lemma  xr  polarity    xl  lemma  xl  polarity 

   
   
   
   
   
   
   

else    x is a leaf in l so it is matched in the whole target node set
 xr  lemma  xr  polarity    n lemma  n polarity  for some node n  f  x 
for each n   f  x   n     n do
generate a substitution rule n  n  where n and n  are aligned  and apply it to xr
x r  the instantiation of n 
for each u  sl such that u is aligned to xr do
add alignment from u to x r

   
   
   
   
   
   

   alignment sharing
for each aligned pair of nodes nl  sl and nr  sr do
nr  polarity  nl  polarity
for each outgoing d edge d of nl whose e edges are not part of sl do
add nr to sd
reld  nr    reld  nl  

   
   
   
   

   dual leaf variable sharing
for each dual leaf variable x matched in a node v  l do
d  the incoming d edge of v
p  parent node of x in sr

   
   
   
   
   

   go over p and alternatives for p generated during variable instantiation
p  set of target nodes of ps incoming d edge
for each p   p do
add p  to sd
reld  p     the relation between x and p

algorithm    applying an inference rule to a compact forest

  

fiknowledge based textual inference via parse tree transformations

this subtree may be shared by multiple trees represented in f  in which case the rule is
applied simultaneously to all of these trees  as in section      the match in our example
is  v  n    n     see  mary  john   this definition does not allow l to be scattered over
multiple embedded trees  matches are constructed incrementally  aiming to add ls nodes
one by one to the partial matches constructed so far  while verifying for each candidate node
in f that both node content and the corresponding edge labels match  it is also verified
that the match does not contain more than one e edge from each d edge  the nodes in f
are indexed using a hash table to enable fast lookup 
as the target nodes of a d edge specify alternatives for the same position in the tree  their
parts of speech are expected to be substitutable  we further assume that all target nodes of
the same d edge have the same part of speech  and polarity  consequently  variables that
are leaves in l and may match a certain target node of a d edge d are mapped to the whole
set of target nodes td rather than to a single node  this yields a compact representation of
multiple matches  and prevents redundant rule applications  for instance  given a compact
representation of  children kids  are fond of  candies sweets   cf  figure     the rule x
is fond of yx likes y will be matched and applied only once  rather than four times  for
each combination of matching x and y   
right hand side generation given an inference rule l  r  we define a dual leaf
variable as a variable that is a leaf of both l and r  in our example  both n   and n  
are dual leaf variables in the passive to active rule of figure  b  variables that are the
only node in r  and hence are both the root and a leaf   and variables with additional
alignments  other than the implicit alignment between their occurrences in l and r  are
not considered dual leaves  as explained below  the instantiations of dual leaf variables are
shared between the source and the target trees 
in the right hand side generation step  a template sr  line     consisting of r while
excluding dual leaf variables  is generated and inserted into f  line     in our example 
sr only includes the node v out of the passive rules rhs  similarly  we define sl as l
excluding dual leaf variables  line    
in the case of a substitution rule  as in our example   sr is set as an alternative to sl
by adding sr s root to td   where d is the incoming d edge of sl s root  line      in case
of an introduction rule  it is set as an alternative to the other trees in the forest by adding
sr s root to the target node set of the forest roots outgoing d edge  line      figure  a
illustrates the results of this step for our example  sr is the gray node labeled with the
variable v   and it becomes an additional target node of the d edge entering the original
 left  see 
variable instantiation each variable in sr  i e   a non dual leaf  is instantiated  lines
       according to its match in l  as in section       in our example  v is instantiated
with see  figure  b  lines         as specified above  if a variable in sr is a leaf in l  which
is not the case in our example  then it is matched in a set of nodes  and each of them
should be instantiated in sr  lines         this is decomposed into a sequence of simpler
operations  first  sr is instantiated with a representative from the set  line      we then
apply ad hoc lexical substitution rules for creating a new node for each additional node in
   this is the case in our current implementation  which is based on the coarse tag set of minipar 

  

fibar haim  dagan   berant

the set  line         these nodes  in addition to the usual alignment with their source nodes
in sl  lines         share the same daughters in sr  due to the alignment between n and
n    defined in line     
alignment sharing modifiers of aligned nodes are shared  rather than copied  as follows 
given a node nl in sl aligned to a node nr in sr   and an outgoing d edge d of nl
which is not part of l  we share d between nl and nr by adding nr to sd and setting
reld  nr     reld  nl    lines         in our example  figure  c   the aligned nodes nl and
nr are the left and right see nodes  respectively  and the shared modifier is yesterday  the
dependency relation mod is copied for the right see node  we also copy polarity annotation
from nl to nr  line     
we note at this point that the instantiation of variables that are not dual leaves cannot
be shared because they typically have different modifiers at the two sides of the rule  yet 
their modifiers  which are not part of the rule  are shared through the alignment operation
 recall that common variables are always considered aligned   dual leaf variables  on the
other hand  might be shared  as described next  since the rule doesnt specify any modifiers
for them 
dual leaf variable sharing this final step  lines        is performed similarly to
alignment sharing  suppose that a dual leaf variable x is matched in a node v in l whose
incoming d edge is d  then we simply add the parent p of x in sr to sd and set reld  p  to
the relation between p and x  in r   since v itself is shared  its modifiers become shared as
well  implicitly implementing the alignment operation  the subtrees little mary and john
are shared this way for variables n   and n    figure  d   if ad hoc substitution rules were
applied to p at the variable instantiation phase  the generated nodes serve as alternative
parents of x  thus the sharing procedure applied to p should be repeated for each of them 
applying the rule in our example added only a single node and linked it to four d edges 
compared to duplicating the whole tree in explicit inference 
      co reference substitution
in section     we defined co reference substitution  an inference operation that allows replacing a subtree t  with a co referring subtree t    this operation is implemented by generating
on the fly a substitution rule t   t  and applying it to t    in our implementation  the
initial compact forest is annotated with co reference relations obtained from an external
co reference resolution tool  and all substitutions are performed prior to rule applications 
substitutions where t  is a pronoun are ignored  as they are usually not useful 
    correctness
in this section  we present two theorems proving that the inference process presented is a
valid implementation of the inference formalism  we provide the full proofs in appendix a 
in theorem    we argue that applying a rule to a compact forest results in a compact
forest  since we begin with a valid compact forest created by the initialization step  it follows
by induction that for any sequence of rule applications the result of the inference process
is a compact forest  the fact that the embedded dags generated during the inference
process are indeed trees is not trivial  since nodes generally have many incoming e edges
  

fiknowledge based textual inference via parse tree transformations

from many nodes  however  we show that any pair of these parent nodes cannot be part
of the same embedded dag  for example  in figure    the node candy has an incoming
e edge from both the node like and the node of   however  the nodes like and of  are
not part of the same embedded dag  this is because the d edge emanating from the root
forces us to choose between the node like and the node be  thus  we see that the reason
for correctness is not local  the two incoming e edges into the leaf node candies cannot be
in the same embedded dag because of a rule applied at the root of the tree  we now turn
to the theorem and its proof scheme 
theorem   applying a rule to a compact forest results in a compact forest 
proof scheme we prove that if applying a rule to a compact forest creates a cycle or an
embedded dag that is not a tree  then such a cycle or a non tree dag already existed
prior to rule application  this contradicts the assumption that the original structure is a
compact forest  a crucial observation for this proof is that for any directed path from a
node u to a node v that passes through sr   where u and v are outside sr   there is also an
analogous path from u to v that passes through sl instead 
the next theorem is the main result  we argue that the inference process over a compact
forest is complete and sound  that is  it generates exactly the set of consequents derivable
from a text according to the inference formalism 
theorem   given a rule base r and a set of initial trees t   a tree t is represented by a
compact forest derivable from t by the inference process  t is a consequent of t according
to the inference formalism 
proof scheme we first show completeness by induction on the number of explicit rule
applications  let tn   be a tree derived from a tree tn using the rule rn according to the
inference formalism  the inductive assumption determines that tn is embedded in some
derivable compact forest f  it is easy to verify that applying rn on f will yield a compact
forest f   in which tn   is embedded 
next  we show soundness by induction on the number of rule applications over the
compact forest  let tn   be a tree represented in some derived compact forest fn    tn   
t  f n       fn   was derived from the compact forest fn   using the rule rn   the inductive
assertion states that all the trees in t  f n   are consequents of t according to the formalism 
hence  if tn   is already in t  f n   then it is a consequent of t   otherwise  it can be shown
that there exists a tree tn  t  f n   such that applying rn to tn will yield tn   according to
the formalism  tn is a consequent of t according to the inductive assertion and therefore
tn   is a consequent of t as well 
these two theorems guarantee that the compact inference process is valid  that is  it
yields a compact forest that represents exactly the set of consequents derivable from a given
text by a given rule set 
  

fibar haim  dagan   berant

    complexity
in this section  we explain why compact inference exponentially reduces the time and space
complexity in typical scenarios 
we consider a set of rule matches in a tree t independent if their matched left handsides  excluding dual leaf variables  do not overlap in t   and their application over t can
be chained in any order  for example  the three rule matches presented in figure   are
independent 
let us consider explicit inference first  assume we start with a single tree t with k
independent rules matched  applying k rules will yield  k trees  since any subset of the
rules might be applied to t   therefore  the time and space complexity of applying k
independent rule matches is   k    applying more rules on the newly derived consequents
behaves in a similar manner 
next  we examine compact inference  applying a rule using compact inference adds
the right hand side of the rule and shares with it existing d edges  since the size of the
right hand side and the number of outgoing d edges per node are practically bounded by
low constants  applying k rules on a tree t yields a linear increase in the size of the forest 
thus  the resulting size is o  t     k   as we can see from figure   
the time complexity of rule application is composed of matching the rule in the forest
and applying the matched rule  applying a matched rule is linear in its size  matching
a rule of size r in a forest f takes o  f r   time even when performing an exhaustive
search for matches in the forest  since r tends to be quite small and can be bounded by
a low constant    this already gives polynomial time complexity  furthermore  matches are
constructed incrementally  where at each step we aim to extend the partial matches found 
due to the typical low connectivity of the forest  as well as the various constraints imposed
by the rule  lemma  pos  and dependency relation   the number of candidates for extending
the matches at each step     f   and these candidates can be retrieved efficiently using
proper indexing  thus  the matching procedure is very fast in practice  as illustrated in the
empirical evaluation described in section     
    related work on packed representations
packed representations in various nlp tasks share common principles  which also underlie
our compact forest  factoring out common substructures and representing choice as local
disjunctions  applying this general scheme to individual problems typically requires specific representations and algorithms  depending on the type of alternatives that should be
represented and the specified operations for creating them  we create alternatives by rule
application  where a newly derived subtree is set as an alternative to existing subtrees 
alternatives are specified locally using d edges 
packed chart representations for parse forests were introduced in classical parsing algorithms such as cyk and earley  jurafsky   martin         and were extended in later
work for various purposes  maxwell iii   kaplan        kay         alternatives in the
parse chart stem from syntactic ambiguities  and are specified locally as the possible decompositions of each phrase into its sub phrases 
   in our rte system  the average rule lhs size was found to be   nodes  and the maximal size was  
nodes  for the experimental setting described in section        applied to the rte  test set 

  

fiknowledge based textual inference via parse tree transformations

packed representations have also been utilized in transfer based machine translation 
emele and dorna        translated packed source language representation to packed target
language representation while avoiding unnecessary unpacking during transfer  unlike our
rule application  in their work transfer rules preserve ambiguity stemming from source
language  rather than generating new alternatives  mi et al         applied statistical
machine translation to a source language parse forest  rather than to the   best parse 
their transfer rules are tree to string  contrary to our tree to tree rules  and chaining is not
attempted  rules are applied in a single top down pass over the source forest   thus  their
representation and algorithms are quite different from ours 

   incorporated knowledge bases
in this section  we describe the various knowledge bases used by our inference engine  we
first describe a novel rule base addressing generic linguistic structures  this rule base was
composed manually  based on our formalism  and includes both inference rules  section     
and polarity annotation rules  section       in addition  we derived inference rules from
several large scale semantic resources  section       overall  this variety illustrates the
suitability of our formalism for representing diverse types of inference knowledge 
    inference rules for generic linguistic phenomena
these rules capture inferences associated with common syntactic structures  which are
summarized in table    the rules have three major functions 
   simplification and canonization of the source tree  categories   and   in table    
   extracting embedded propositions  categories          
   inferring propositions from non propositional subtrees of the source tree  category    
inference rules that merely extract a subtree out of the source tree without changing its
structure  such as the relative clause rule  are useful for exact inference that aims to generate
the hypothesis  and were used in the evaluation of such inferences  cf  section       however  the currently implemented approximate classification features are focused on matching
substructures of the hypothesis in the forest  as described in section       hence they do
not take advantage of such extractions  therefore  these rules were excluded from the rest
of the experiments  reported in sections        
the rules in categories     depend solely on syntactic structure and closed class words 
and are referred to as generic rules  by contrast  verb complement extraction rules  category
   are considered lexicalized rules  since they are specific to certain verbs  if we replace forced
with advised in the example  the entailment would not hold  we extracted from the parc
polarity lexicon  nairn et al         a list of verbs that allow such inference when appearing
in positive polarity contexts  and generated inference rules for these verbs  the list was
complemented with a few reporting verbs  such as say and announce  since information in
the news domain  in which these rules were applied in our experiments  cf  section      is
often given in reported speech  while the speaker is usually considered reliable 
we sidestep the issue of polarity propagation by applying these rules only at the main
clause  which is implemented by including the tree root node in the rule lhs  when the
  

fibar haim  dagan   berant

 
 

category
conjunctions

 

clausal extraction from
connectives
relative
clauses

 

 

appositives

 

determiner
canonization

 

passive

 

genitive
modifier

 

verb complement clause
extraction

example  source
helenas very experienced
and has played a long time
on the tour 
but celebrations were muted
as many iranians observed a
shiite mourning month 
the assailants fired six bullets at the car  which carried
vladimir skobtsov 
frank robinson  a onetime manager of the indians  has the distinction for
the nl 
the plaintiffs filed their lawsuit last year in u s  district
court in miami 
we have been approached
by the investment banker 
malaysias crude palm oil
output is estimated to have
risen by up to six percent 
yadav was forced to resign 

example  derived
 helena has played a long
time on the tour 
 many iranians observed a
shiite mourning month 
 the car carried vladimir
skobtsov 
 frank robinson is a onetime manager of the indians 

 the plaintiffs filed a lawsuit last year in u s  district
court in miami 
 the investment banker approached us 
 the crude palm oil output of malaysia is estimated
to have risen by up to six percent 
 yadav resigned 

table    inference rules for generic linguistic structures

embedded clause is extracted  it becomes the main clause in the derived tree  and these rules
can then extract its own embedded clauses  the polarity of the verb is detected by applying
annotation rules  as described next  if the verb was annotated with negative or unknown
polarity  matching of complement extraction rules fails  for example  if the last sentence in
table   was yadav was not forced to resign  then forced would be annotated with negative
polarity  and consequently the matching of the corresponding complement extraction rule
would fail  and yadav resigned would not be entailed  hence  annotation rules may block
erroneous inference rule applications  while polarity is important for correct application of
such rules  this is not the case for other rule types  such as passive to active transformation 
we therefore checked polarity matching for rule application only in the exact inference
experiment  section       where the verb complement extraction rules were used  we leave
further analysis of polarity dependence of our rules to future work 
    polarity annotation rules
we use annotation rules to mark negative and unknown polarity of predicates  cf  section       table   summarizes the polarity inducing contexts that we address  like inference rules  annotation rules also comprise generic rules  categories      and lexicalized
  

fiknowledge based textual inference via parse tree transformations

 
 

category
explicit negation

 
 
 

implied negation
modal auxiliaries
overt conditionals

 
 
 

verb complements
adjectives
adverbs

example
what weve never seen   is actual costs come
down 
no one stayed   for the last lecture 
i could eat    a whale now 
if venus wins    this game  she will meet    sarena
in the finals 
i pretend that i know   calculus 
it is impossible that he survived   such a fall 
she probably danced    all night 

table    polarity annotation rules

rules  categories       when a verb complement embedded clause has negative or unknown
polarity  it is not extracted  however  its polarity is annotated  category    compare with
category   in table     this list of verbs that imply negative unknown polarity for their
clausal complements was taken from the parc lexicon  as well as from verbnet  kipper 
      
    lexical and lexical syntactic rules
in addition to the manually composed generic rules  the system integrates inference knowledge from a variety of large scale semantic resources  introduced in section      the information derived from these resources is represented uniformly as inference rules in our
formalism  some examples for such rules were shown in table    the following resources
were used 
wordnet  we extracted from wordnet  fellbaum        lexical rules based on the synonym  hyponym  a word is entailed by its hyponym  e g   dog animal    instance
hyponym    and derivation relations 
wikipedia  we used the lexical rulebase of shnarch et al          who extracted rules
such as janis joplin  singer from wikipedia based on both its metadata  e g  
links and redirects  and text definitions  using patterns such as x is a y    
dirt  the dirt algorithm  lin   pantel        learns from a corpus inference rules
between binary predicates  for example  x is fond of yx likes y  we used a
version that learns canonical rule forms  szpektor   dagan        
argument mapped wordnet  amwn   a resource for inference rules between predicates  covering both verbal and nominal forms  szpektor   dagan         includ    according to the wordnet glossary  an instance is a proper noun that refers to a particular  unique
referent  as distinguished from nouns that refer to classes   this is a specific form of hyponym  for
example  ganges is an instance of river 
    in addition to the extraction methods described by shnarch et al          we employed two additional
methods  first  extraction of entailments among terms that are redirected to the same page  second 
generalization of rules with the same rhs and common lhs head  but different modifiers  for instance 
the rules ferrari f     car  and ferrari ascari car  are generalized into ferrari car  

  

fibar haim  dagan   berant

ing their argument mapping  it is based on wordnet and nomlex plus  meyers
et al          verified statistically through intersection with the unary dirt algorithm  szpektor   dagan         amwn rules are defined between unary templates 
for example  kill xx die
these automatically extracted inference rules lack two attributes defined in our formalism  rule type  substitution introduction  and explicit alignments  beyond alignments
between rs variables and their l counterparts  which are defined by default   these attributes are added automatically using the following heuristics 
   if the roots of l and r have the same part of speech  then it is a substitution rule
 e g   x buy y  y was sold to x    otherwise  e g   ys acquisition by x  y was
sold to x    it is an introduction rule 
   the roots of l and r are assumed to be aligned 
note that application of some of the above rules   e g   wordnet derivations and some
of the rules learned by dirt   does not result in a valid parse tree  these rules should
not be used when aiming for exact derivation of h from t  however  they may be useful
when the inference engine is used together with an approximate matching component  as
in our rte system  our approximate matcher  described in section      employs features
such as the coverage of words and subtrees in h by f  and therefore can benefit from such
inferences  these rules should preferably be applied only as the last step of the inference
process  to avoid cascading errors 

   evaluation
in this section  we present an empirical evaluation of our entailment system as a whole  as
well as evaluation of its individual components  we evaluate both the quality of the systems
output  in terms of accuracy  precision  and recall  and its computational efficiency  in terms
of running time and space  using various application settings 
we first evaluate the knowledge based inference engine  in section      we describe an
experiment in which the engine aims to prove simple template hypotheses  representing
binary predicates  from texts sampled from a large corpus  next  in section     we evaluate
the efficiency of our engine implementation using the compact forest data structure  we then
evaluate the complete entailment system  including the approximate entailment classifier
 section       finally  in sections        we provide an in depth analysis of the performance
of the inference component on rte data 
    proof system evaluation
in this experiment  we evaluate the inference engine on finding strict proofs  that is 
the inference process must derive precisely the target hypothesis  or an instantiation of
it  in the case of template hypotheses  which contain variables as defined in section      
thus  we should evaluate its precision over text hypothesis pairs for which a complete proof
chain is found  using the available rules  we note that the pascal rte datasets are not
suitable for this purpose  these rather small datasets include many text hypothesis pairs for
  

fiknowledge based textual inference via parse tree transformations

which available inference rules would not suffice for deriving complete proofs  furthermore 
since the focus of this research is applied textual inference  the inference engine should
be evaluated in an nlp application setting where the texts represent realistic distribution
of linguistic phenomena  manually composed benchmarks such as the fracas test suite
 cooper et al          which contains synthetic examples for specific semantic phenomena 
are clearly not suitable for such an evaluation 
as an alternative  we chose a relation extraction  re  setting  for which complete
proofs can be achieved for a large number of corpus sentences  in this setting  the system
needs to identify pairs of arguments in sentences for a target semantic relation  e g   x buy
y   
      system configuration
in this experiment  which was first reported by bar haim et al          we used an earlier
version of the engine and the rule bases  the engine in this experiment does not make use
of the compact forest  but rather generates each consequent explicitly  polarity annotations
are not propagated from source to derived trees  instead  polarity annotation rules are
applied to the original text t  and to each inferred consequent  prior to application of any
inference rule  the following rule bases were used in this experiment 
generic linguistic rules we used the generic rule base presented in section    including both inference and the polarity annotation rules  this early version did not include the
lexicalized polarity rules derived from verbnet and from the parc lexicon  category   in
table    
lexical syntactic rules nominalization rules  inference rules such as xs acquisition
of y  x acquired y  capture the relations between verbs and their nominalizations 
these rules were derived automatically  ron        from nomlex  a hand coded database
of english nominalizations  macleod  grishman  meyers  barrett    reeves         and
from wordnet 
automatically learned rules  we used the dirt paraphrase collection  as well the
output of tease  szpektor et al          another unsupervised algorithm for learning
lexical syntactic rules  tease acquires entailment relations from the web for a given
input template i by identifying characteristic variable instantiations shared by i and other
templates  both algorithms provide a ranked list of output templates for a given input
template  some of the learned rules are linguistic paraphrases   e g   x confirm y  x
approve y    while others capture world knowledge   e g   x buy y  x own y    these
algorithms do not learn the entailment direction of the rule  which reduces their accuracy
when applied in any given direction  for each system  we considered the top    bi directional
rules learned for each template 
generic default rules these rules are used to define default behavior  in situations
where no case by case rules are available  we used one default rule that allows removal
of any modifiers from nodes  ideally  this rule would be replaced in future work by more
specific rules for removing modifiers 
  

fibar haim  dagan   berant

      evaluation process
we use a sample of test template hypotheses that correspond to typical re relations  such as
x approve y  we then identify in a large test corpus  sentences from which an instantiation
of the test hypothesis is proved  for example  the sentence the budget was approved by
the parliament is found to prove the instantiated hypothesis parliament approve budget
 via the passive to active inference rule   finally  a sample of such candidate sentenceshypothesis pairs is judged manually for true entailment  we repeated the process to compare
different system configurations 
since the publicly available sample output of tease is much smaller than the other
resources   we randomly selected from this resource   transitive verbs that may correspond
to typical re predicates     we formed test templates by adding subject and object varisubj

able nodes  for example  for the verb accuse we constructed the template xnoun 
obj

accuse verb  ynoun  
for each test template h we identify sentences in the corpus from which the template
can be proved by our system  to efficiently find proof chains that generate h from corpus
sentences we combine forward and backward  breadth first  searches over the available
rules  first  we use a backward search over the lexical syntactic rules  starting with rules
whose right hand side is identical to the test template  the process of backward chaining
the dirt tease and nominalization rules generates a set of templates ti   all of them
proving  deriving  h  for example  for the hypothesis x approve y we may generate
the template x confirm y  through backward application of a dirt tease rule  and
then further generate the template confirmation of y by x  through a nominalization rule 
since the templates ti are generated by lexical syntactic rules  which modify open class
lexical items  they may be considered lexical expansions of h 
next  for each specific ti we generate a search engine query composed of the open class
words in ti   this query fetches candidate sentences from the corpus  from which ti might
be proven using the generic linguistic rules  recall that these rules do not modify openclass words   to that end  we use a forward search that applies the generic rules  starting
from a candidate sentence s and trying to derive ti by a sequence of rule applications  if
successful  the variables in ti are instantiated  cf  section       consequently  we know that
under these variable instantiations  h can be proven from s  since s derives ti which in turn
derives h  
we performed the above search for sentences that prove each test template over the
reuters rcv  corpus  cd    applying minipar for parsing  through random sampling 
we obtained    sentences that prove  according to the tested system configuration  each of
the   test templates  yielding a total of     pairs of a sentence  and an instantiated hypothesis  for each of the four tested configurations  described below       pairs overall   these
pairs were split for entailment judgment between two human annotators  graduate students
at the bar ilan nlp group   the annotators achieved  on a sample of     shared exam    the output of tease and dirt  as well as many other knowledge resources  is available from the rte
knowledge resources page 
http   aclweb org aclwiki index php title rte knowledge resources
    the verbs are approach  approve  consult  lead  observe  play  seek  sign  strike 

  

fiknowledge based textual inference via parse tree transformations

 
 
 
 
 

configuration
baseline  embed h anywhere in s 
proof  embed h at the root of s 
proof   generic
proof   generic   lexical syntactic

precision
     
     
     
     

yield
     
     
     
      

table    proof system evaluation

ples  an agreement level of      and a kappa value of       corresponding to substantial
agreement  
      results
we tested four configurations of the proof system 
   baseline  the baseline configuration follows the prominent approach in graph based
entailment systems  the system tries to embed the given hypothesis anywhere in the
candidate sentence tree s  while only negative or unknown polarity  detected by the
annotation rules  may block embedding 
   proof  in this configuration h has to be strictly generated from the candidate sentence s  the only inference rule available is the default rule for removing modifiers
 polarity annotation rules are active as in baseline   this configuration is equivalent
to embedding h in s with the root of h matched at the root of s  since modifiers that
are not part of the match can be removed from s by the default rule  however  if
h is embedded elsewhere in s it will not be extracted  as opposed to the baseline
configuration 
   proof   generic  as proof  plus generic linguistic rules 
   proof   generic   lexical syntactic  as the previous configuration  plus
lexical syntactic rules 
for each system configuration we measure precision  the percentage of examples judged
as correct  entailing   and average extrapolated yield  which is the expected number of
truly entailing sentences in the corpus that would be proven as such by the system  the
extrapolated yield for a specific template is calculated as the number of sample sentences
judged as entailing  multiplied by the sampling proportion  the average is calculated over
all test templates  we note that  similar to ir evaluations  it is not possible to compute
true recall in our setting since the total number of entailing sentences in the corpus is not
known  recall is equal to the yield divided by this total   however  it is straightforward to
measure relative recall differences among different configurations based on the yield  thus 
using these two measures estimated from a large corpus it is possible to conduct robust
comparison between different configurations  and reliably estimate the impact of different
rule types  such analysis is not possible with the rte datasets  which are rather small  and
their hand picked examples do not represent the actual distribution of linguistic phenomena 
  

fibar haim  dagan   berant

the results are reported in table    first  comparing the results for proof with the
results for baseline  we observe that the requirement for matching h at the root of s  i e  
at the main clause of s   rather than allowing it to be matched anywhere in s  improves the
precision considerably over the baseline  by         while reducing the yield by nearly     
the proof configuration avoids errors resulting from improper extraction of embedded
clauses 
remarkably  using the generic inference rules  our system is able to gain back the lost
yield in proof and further surpass the yield of the baseline configuration  in addition 
we obtain a higher precision than the baseline  a      difference   which is statistically
significant at a p        level  using z test for proportions  this demonstrates that our
principled proof approach appears to be superior to the more heuristic baseline embedding
approach  and exemplifies the contribution of our generic rule base  overall  generic rules
were used in     of the proofs 
adding the lexical syntactic rules increased the yield by a factor of six  this shows
the importance of acquiring lexical syntactic variability patterns  however  the precision of
dirt and tease is currently quite low  causing overall low precision  manual filtering of
rules learned by these systems is currently required to obtain reasonable precision 
error analysis revealed that for the third configuration proof   generic rules  a
significant     of the errors are due to parsing errors  most notably incorrect dependency
relation assignment  incorrect pos assignment  incorrect argument selection  incorrect analysis of complex verbs  e g   play down in the text vs  play in the hypothesis  and ungrammatical sentence fragments  another     of the errors represent conditionals  negation 
and modality phenomena  most of which could be handled by additional rules  some making use of more elaborate syntactic information such as verb tense  the remaining  and
rather small     of the errors represent truly ambiguous sentences which would require
considerable world knowledge for successful analysis 
    compact forest efficiency evaluation
next  we evaluate the efficiency of compact inference  cf  section    in the setting of recognizing textual entailment  using the rte   and rte   datasets  giampiccolo et al        
       these datasets consist of  text  hypothesis  pairs  which need to be classified as
entailing non entailing  our first experiment  using the generic inference rule set  shows
that compact inference outperforms explicit inference  efficiency wise  by orders of magnitude  section         the second experiment shows that compact inference scales well to
a full blown rte setting with several large scale rule bases  where up to hundreds of rules
are applied per text  section        
      compact vs  explicit inference
to compare explicit and compact inference we randomly sampled     pairs from the rte  
development set  and parsed the text in each pair using minipar  lin         to avoid
memory overflow for explicit inference  we applied to these sentences only the subset of
generic inference rules described in section      for a fair comparison  we aimed to make the
explicit inference implementation reasonably efficient  for example by preventing multiple
generations of the same tree by different permutations of the same rule applications  both
  

fiknowledge based textual inference via parse tree transformations

time  msec 
rule applications
node count
edge endpoints

compact
  
  
  
   

explicit
      
   
     
      

ratio
   
  
  
  

table    compact vs  explicit inference  using generic rules  results are averaged per
text hypothesis pair 

configurations perform rule application iteratively  until no new matches are found  in each
iteration  we first find all rule matches and then apply all matching rules  we compare run
time  number of rule applications  and the overall generated size of nodes and edges  where
edge size is represented by the sum of its endpoints    for a regular edge   sd      td   for a
d edge  
the results are summarized in table    as expected  the results show that compact
inference is by orders of magnitude more efficient than explicit inference  to avoid memory
overflow  inference was terminated after reaching         nodes  three out of the     test
texts reached that limit with explicit inference  while the maximal node count for compact
inference was only      the number of rule applications is reduced due to the sharing
of common subtrees in the compact forest  by which a single rule application operates
simultaneously over a large number of embedded trees  the results suggest that scaling to
larger rule bases and longer inference chains would be feasible for compact inference  but
prohibitive for explicit inference 
      application to an rte system
the goal of the second experiment was to test if compact inference scales well for broad
inference rule bases  in this experiment we used the bar ilan rte system  bar haim et al  
       the system operates in two primary stages 
inference  inference rules are first applied to the initial compact forest f  aiming to bring
it closer to the hypothesis h  in this experiment  we use all the knowledge bases
described in section    overall  these rule bases contain millions of rules 
in the current system we implemented a simple search strategy  in the spirit of
 de salvo braz et al          first  we applied three exhaustive iterations of generic
rules  since these rules have low fan out  few possible right hand sides for a given
left hand side   it is affordable to apply and chain them more freely  at each iteration
we first find all rule matches  and then apply all matched rules  to avoid repeated
identical rule applications  we mark newly added nodes at each iteration  and in the
next iteration consider only matches containing new nodes  we then perform a single
iteration of all other lexical and lexical syntactic rules  applying them only if their l
part was matched in f and their r part was matched in h  further investigation of
effective search heuristics over our representation is left for future research 
classification  following inference  a set of features is extracted from the resulting f and
from h and fed into an svm classifier  which determines entailment  we describe the
  

fibar haim  dagan   berant

rule applications
node count
edge endpoints

rte  dev
avg  max 
  
   
  
   
         

rte 
avg  max 
  
   
  
   
         

table    application of compact inference to the rte   dev  and rte   datasets  using
all rule types

classification stage in more detail in the next section  which discusses the performance
of our rte system 
table   provides statistics on rule applications using all rule bases  over the rte  
development set and the rte   dataset     overall  the primary result is that the compact
forest indeed accommodates well extensive rule applications from large scale rule bases  the
resulting forest size is kept small  even in the maximal cases which were causing memory
overflow for explicit inference 
    complete rte system evaluation
in the previous sections  we evaluated our knowledge based inference engine  the proof system  with respect to the quality of its output  precision  recall  as well as its computational
efficiency  time  space   we now evaluate the complete rte system  which combines the
inference engine with an approximate classification module 
the classification setting and its features are quite typical for the rte literature  features can be broadly categorized into two subsets   a  lexical features that solely depend on
the lexical items in f and h  and  b  lexical syntactic features that also take into account
the syntactic structures and dependency relations in f and h  below is a brief description
of the features  a complete description appears in our rte system report  bar haim et al  
      
lexical features  coverage features check if the words in h are present  covered  in f 
we assume that a high degree of lexical coverage correlates with entailment  these
features measure the proportion of uncovered content words  verbs  nouns  adjectives
and adverbs  named entities and numbers  polarity mismatch features detect cases
where nouns or verbs in h are only matched in f with incompatible polarity  these
features are assumed to indicate non entailment 
edge coverage features  we say that an edge in h is matched in f if there is an edge
in f with matching relation  source node and target node  we say an edge in h is
loosely matched if there is some path in f from a matching source node to a matching
target node  based on these definitions we extract two features  the proportion of h
edges matched loosely matched in f   
    running time is not included since most of it was dedicated to rule fetching  which was rather slow for
our available implementation of some resources  the elapsed time was a few seconds per  t  h  pair 
    we only look at a subset of the edges labeled with relevant dependency relations 

  

fiknowledge based textual inference via parse tree transformations

predicate argument features  if f entails h  then the predicates in h should be matched
in f along with their arguments  predicates include verbs  except for the verb be 
or subject complements in copular sentences  for example  smart in joseph is smart 
arguments are the daughters of the predicate node in h    four features are computed
for each f  h pair  we categorize every predicate in h that has a match in f to one
or more of four possible categories 
   complete match   a matching predicate exists in f with matching arguments and
dependency relations 
   partial match   a matching predicate exists in f with some matching arguments
and dependency relations 
   opposite match   a matching predicate exists in f with some matching arguments
but incorrect dependency relations 
   no match   no matching predicate in f has any matching arguments 
if a predicate is categorized as a complete match it will not be in any other category 
finally  we compute the four features for the f  h pair  the proportion of predicates
in h that have a complete match in f  and three binary features  checking if there
is any predicate in h categorized as a partial match opposite match no match  since
the subject and object arguments are crucial for textual entailment  we compute four
similar features only for the subset of predicates that have these arguments  ignoring
other arguments  
a global lexical syntactic feature  this feature measures how well the subtrees in h
are covered by f  weighted according to the proximity to the root of h  this feature
is somewhat similar to the dependency tree kernel of collins and duffy         which
measures the similarity between two dependency trees by counting their common
subtrees  however  our measure has several distinct properties which makes it suitable
for our needs   a  its a directional measure  estimating the coverage of h by f  but
not vice versa  b  it operates on a compact forest and a tree  rather than on a pair of
trees   c  it takes into account the distance from the root of h  assuming that nodes
closer to the root are more important 
the system was trained on the rte   development set  and was tested on the rte  and
rte   test sets  no development set was released for rte     co reference substitution was
disabled due to the insufficient accuracy of the co reference resolution tool we used  we
first report its overall performance  and then provide some analysis of the inference module 
which is our focus in this work 
the accuracies obtained in this experiment are shown in table    under the inference
column   the results on rte   are quite competitive  compared to our        only   teams
out of the    who participated in rte   scored higher than      and three more systems
scored between     and      the results for rte  rank      out of     with only   teams
scoring higher by more than     overall  these results show that our system is well situated
in the state of the art for the rte task 
table   provides a more detailed view of our systems performance  precision  recall  and
f  results are given for both entailing and non entailing pairs  as well as the overall accuracy 
    when the dependent is a preposition or a clause we take the complement of the preposition or the head
of the clause respectively as the dependent 

  

fibar haim  dagan   berant

the table also shows the results per task  ie  ir  qa and sum   overall  our system tends
to predict entailment more often than non entailment  the recall for entailing pairs is much
higher than the recall for non entailing pairs  while the precision for non entailing pairs is
much higher than for entailing pairs  performance varies considerably among different tasks 
our rte  accuracy results for qa and ir are considerably higher than the average results
achieved by rte  submissions  as reported by the organizers  giampiccolo et al        
      and       respectively   while for ie and sum  our results are a bit above the average
      and        our rte  results are better for ir and sum  which seem to be the easier
tasks in rte   giampiccolo et al           
    usage and contribution of knowledge bases
to evaluate the accuracy gain from knowledge based inference  we ran the system with the
inference module disabled  so that entailment classification is applied directly to the initial
parse tree of the text  the results are shown under the no inference column of table   
comparing these results to the full system accuracy  inference   we see that applying the
inference module resulted in higher accuracy on both test sets  the contribution was more
prominent for the rte   dataset  these results illustrate a typical contribution of current
knowledge sources for current rte systems  this contribution is likely to increase with
current and near future research  on topics such as extending and improving knowledge
resources  applying them only in semantically suitable contexts  improved classification
features  and broader search strategies 
tables   and    illustrate the usage and contribution of individual rule bases  table  
shows the distribution of rule applications over the various rule bases  table    presents
ablation study showing the marginal accuracy gain for each rule base  these results show
that each of the rule bases is applicable for a large portion of the pairs  and contributes
to the overall accuracy  we note that the results are highly dependent on the search
strategy  for instance  chaining of lexical rules is expected to increase the number of lexical
rule applications  but reduce their accuracy  we provide a more detailed analysis of rule
applications in our system in the next section 
    manual analysis
we conclude the evaluation with two manual analyses of the inference component within the
rte system  the first analysis  subsection        assesses the applicability of our inference
framework to the rte task as well as the actual coverage of the current system  it also
categorizes the cases in which our formalism falls short  we then  subsection        assess
the correctness of the applied rules  and analyze the various causes for incorrect applications 
the analyses were done by one of the authors on randomly sampled subsets of the rte  
test set 

    according to the rte  organizers  the ie task appeared to be the most difficult task  while sum and
ir seemed to be the easier tasks  however  they did not report the average accuracy per task 

  

fiknowledge based textual inference via parse tree transformations

test set
rte 
rte 

accuracy
no inference inference
     
     
     
     


     
     

lexical
overlap
     
     

best rte
result
     
     

table    inference contribution to rte performance  the system was trained on the rte  development set    indicates statistically significant difference  at level p         using
mcnemars test   the best results achieved in the rte  and rte  challenges  hickl  
bensley        bensley   hickl         as well as lexical overlap baseline results  mehdad
  magnini      a   are also given for reference  mehdad and magnini have tested eight
configurations of lexical overlap baselines  and chose the one that performs best on average
over the rte    test sets 

rte 

rte 

task
ie
ir
qa
sum
all
ie
ir
qa
sum
all

non entailing pairs
precision recall
f 
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           

entailing pairs
precision recall
f 
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           

accuracy
     
     
     
     
     
     
     
     
     
     

table    rte results breakdown by task and pair type

rule base
wordnet
amwn
wikipedia
dirt
generic
polarity

rte  dev
rules app
   
   
   
   
   
   
   
   
        
   
   

rte 
rules app
   
   
   
   
   
   
   
   
        
   
   

table    average number of rule applications per  t  h  pair  for each rule base  app counts
each rule application  while rules ignores multiple matches of the same rule in the same
iteration 

      applicability and coverage
this analysis assesses the ability of the inference framework to derive complete proofs for
rte  t h  pairs in an idealized setting where perfect knowledge bases and co reference
resolution are available  this provides an upper bound to the coverage of our inference
  

fibar haim  dagan   berant

rule base
wordnet
amwn
wikipedia
dirt
generic
polarity

accuracy  rte  
    
    
    
    
    
    

table     contribution of various rule bases  results show accuracy loss on rte    obtained
when removing each rule base  ablation tests  

engine  a similar analysis was previously done by bar haim  szpektor  and glickman
       on a subset of the rte   dataset  however  here we go further and  a  assess the
actual coverage of the required inferences by the implemented rte system  and  b  present
classification of uncovered cases into different categories 
we carried out the analysis as follows     positive  entailing  pairs were randomly
sampled from the rte   test set  for each pair we aimed to manually derive a proof
comprising inference steps that are expressible in our formalism  similar to the example
in section      if a complete proof could be derived  the pair was classified as inferable 
otherwise  it was classified into one of the following categories 
discourse references  complete proof requires incorporating pieces of information from
the discourse  including event co reference and bridging  mirkin et al          nominal co reference substitution was not included  as it is covered in our formalism 
for instance  in the text the titanics sinking after hitting an iceberg on april    
            the year      is not explicitly specified as the time of the titanics sinking 
and this relation should be derived from the discourse in order to infer the hypothesis
the titanic sank in      
non decomposable  the inference cannot be reasonably decomposed to a sequence of
local rewrites  this is the case  for example  with the text the black plague lasted
four years and killed about one third of the population of europe  or approximately   
million people and the hypothesis black plague swept europe 
other  a few other cases that did not fall into the above categories 
the distribution of these categories is shown in table     we found that     of the
pairs could be proven by our formalism given appropriate inference rules and co reference
information  which demonstrates the utility of our approach  the results are somewhat
higher than the     reported by bar haim et al          which may be attributed to the
fact that rte  is considered a more difficult dataset  and entailment systems consistently
perform better on rte  
out of the remaining     pairs  our analysis highlights the significance of discourse
references  which occur in       of the pairs  while previous analysis of discourse references
in textual entailment was applied to the rte   search task  where the text sentences are
interpreted in the context of their full discourse  mirkin et al          our analysis shows
  

fiknowledge based textual inference via parse tree transformations

category
inferable
non decomposable
discourse references
other

count
  
  
  
 

 
     
     
     
    

table     applicability of our inference framework to the rte task     randomly selected
entailing pairs from the rte   test set were analyzed 

the significance of discourse references even for short  self contained texts  of which rte  is composed  mirkin et al  show how our framework  and similar methods based on
tree transformations  can be extended to utilize discourse references  several works over
the last few years targeted implied predicate argument relationships  the most notable of
which is the semeval      task linking events and their participants in discourse
 ruppenhofer  sporleder  morante  baker    palmer         in particular  stern and dagan
       recently showed that identifying such relations improves the performance of their
rte system  finally  the entailment in       of the pairs could not be established by a
sequence of local rewrites  thus these cases are likely to require deeper methods for semantic
analysis and inference 
the manually derived proofs for the    inferable pairs included a total of    rule applications  an average of      rule applications per pair    the maximal number of rules per
pair was       of these rules         were applied in our system      of the proofs for
the inferable pairs were fully derived by our rte system  partial proofs were derived for
additional     of the pairs  for the remaining     of the pairs  our system did not apply
any of the rules in the manual proof  the results demonstrate the utility of the inference
mechanisms and rule bases in our system  but on the other hand suggest that there is still
much room for improvement in the coverage of the existing rule bases 
      correctness of applied rules
we next assess the correctness of rules applied by the inference engine  we focus on the
four lexical and lexical syntactic rule bases described in section      wordnet  wikipedia 
dirt  and argument mapped wordnet  amwn   except for wordnet  these rule bases
were generated automatically  therefore their accuracy is more of an issue than the accuracy
of the manually composed generic inference rules and polarity annotation rules  furthermore  lexicalized rules are often context sensitive  which is an additional potential source
of incorrect rule applications 
for this evaluation we randomly sampled    pairs from the rte   test set  and analyzed
all lexical and lexical syntactic rule applications performed by the system for these pairs  a
total of     rule applications  we define two levels of rule application correctness 

    as previously mentioned  the rte system does not apply rules that merely extract a subtree from a
given source tree  accordingly  such rules were ignored in this analysis as well 

  

fibar haim  dagan   berant

propositional  the derived tree resulting from the rule application is both grammatical
and entailed from the source tree  this is the level of correctness assumed by our
formalism 
referential  in case the propositional correctness does not hold  we turn to the weaker criterion of referential correctness  following the notion of lexical reference  glickman 
shnarch    dagan        shnarch et al          which we extend here to the case of
template based rules with variables  let rule e   l  r be an inference rule matched
in a source tree s  let l and r be the instantiations of l and r respectively  according
to the variable matching of l in s  we say that referential correctness holds if l generates a reference in s to a possible meaning of r  some examples for such rules found
in our analyzed sample are  popepapal  turkishturkey and fishermenfishing 
while these rule applications do not result in a valid entailed tree  they are still useful
in the context of an rte system that applies approximate matching  as previously
discussed at the end of section    
incorrect rule applications were classified into one of the following categories 
   bad rule  the rule is a priori incorrect  e g   walesyear   
   bad context  the rule is incorrect in the context of the source sentence  for example 
the wordnet rule strikecreate corresponds to the rare sense of strike defined as
produce by ignition or a blow  as in strike fire from the flint stone  
   bad match  the rule was applied due to incorrect matching of the left hand side 
resulting from incorrect parse of the source tree 
the results are summarized in table     overall        of the rule applications are correct 
interestingly  there are more referential         than propositional         rule applications  unsurprisingly  the most accurate knowledge resource is the manually composed
wordnet        correct applications   followed by the amwn         and wikipedia
        rule bases  which were derived automatically from human generated resources  the
least accurate resource is dirt          which makes no use of human knowledge engineering  but rather was learned automatically based on corpus statistics  the accuracy of dirt
is considerably lower than the accuracy of the other resources  substantially decreasing the
overall accuracy as well  most of the errors for dirt and wikipedia are due to bad rules 
this is also the overall dominant cause for incorrect applications  while for wordnet and
amwn the a priori rule quality is very high and most of the errors are due to bad context  wikipedia rules did not suffer from bad context  which can be explained by the fact
that their left hand side was often an unambiguous named entity  madrid  antelope valley
freeway  microsoft office   the analysis highlights the need for improving the accuracy
of automatically generated rule bases  whose quality is still far below human generated resources  the analysis also shows that context sensitivity of lexicalized rules is still an issue
even when these rules are applied conservatively as in our experiment  no chaining  both l
and r were matched in f and h   this should be addressed in future research 
  

fiknowledge based textual inference via parse tree transformations

  out of rule applications
propositional
referential
correct
bad rule
bad context
bad matching
incorrect

dirt
     
     
    
     
     
    
     
     

amwn
    
     
     
     
    
     
    
     

wikipedia
     
     
     
     
     
    
    
     

wordnet
     
     
     
     
    
     
    
     

all
      
     
     
     
     
     
    
     

table     analysis of lexical and lexical syntactic rule applications

   discussion  comparison to related approaches
in this section  we compare our work to several closely related inference methods  most of
which were described in section       
the discourse commitments derived by hickl        are quite similar to the kind of consequents we generate by applying our syntactic  lexical syntactic  and co reference substitution rules  however  our work differs from hickls in several respects  first and foremost 
hickls work does not fully describe a knowledge representation and inference framework 
which is the main focus of our work  hickl briefly mentions that the commitments were
generated using a probabilistic fst based extraction framework  but no further explanations or examples are given in the paper  second  our framework allows unified modeling of
a variety of inference types that are addressed by various tools and components in hickls
system  fst  relation extraction  paraphrase acquisition  etc    in addition  our system
operates over lexical syntactic representations  and does not rely on semantic parsing  finally  the consequents generated in our formalism are packed in an efficient data structure 
whereas hickls commitments are generated explicitly and he does not discuss commitment
generation efficiency  it should be noted  however  that while explicit generation of commitments restricts the search space  it may simplify approximate matching  e g   finding
alignment between h and a given consequent vs  aligning h with the whole compact forest  
de salvo braz et al         presented a semantic inference framework that augments
the text representation with only the right hand side of an applied rule  and in this respect
is similar to ours  however  in their work  both rule application and the semantics of
the resulting augmented structure were not fully specified  in particular  the distinction
between individual consequents was lost in the augmented graph  by contrast  our compact
inference is fully formalized and is proved to be equivalent to an expressive  well defined
formalism operating over individual trees  where each inferred consequent can be recovered
from the compact forest 
maccartney and manning        proposed a model of natural language inference which 
similar to our framework  operates directly on parse based representations  their work extends previous work on natural logic  valencia         which focused on semantic containment and monotonicity  by incorporating semantic exclusion and implicativity  they model
the inference of h from t as a sequence of atomic edits  each can be thought of as generating
an intermediate premise  their calculus computes the semantic relation between the source
  

fibar haim  dagan   berant

and the derived premise by propagating the semantic relation from the local edit upward
through the parse tree according to the properties of intermediate nodes  for example  it
can correctly infer that some first year students arrived  some students arrived   but
every first year student arrived  every student arrived   the composition of these semantic relations along the inference chain yields the semantic relation holding between t
and h  their contribution is complementary to ours  in both approaches  the inference of
h from t is modeled as a sequence of atomic steps  rule applications or edits   the focus
of our framework is the representation and application of diverse types of transformations
needed for textual inference  as well as efficient representation of possible inference chains 
application of an inference rule is assumed to always generate an entailed consequent  and
polarity rules may be used to detect situations where this assumption does not hold and
block rule application  by comparison  the formalism of maccartney and manning assumes
rather simple edit operations  and is focused on precise predication of the semantic relation
between t and h for a given sequence of edits that transform t into h  thus  combining
these two complementary approaches is a natural direction for future research 

   conclusion
the subject of this work was the representation and use of semantic knowledge for textual
inference at the lexical syntactic level  we defined a novel inference framework over parse
trees  which represents diverse semantic knowledge as inference rules  the proof process
aims to transform the source text into the target hypothesis through a sequence of rule
applications  each generating an intermediate parse tree  a complementary contribution of
this work is a novel data structure and an associated rule application algorithm  which are
proved to be a valid implementation of the inference formalism  we illustrated inference
efficiency both analytically and empirically 
our approach has several advantageous properties  first  the ability to represent and
apply a wide variety of inferences and combine them through rule chaining makes our framework more expressive than most of the previous rte architectures  second  this expressive
power is obtained by a well formalized and compact framework  based on unified knowledge
representation and inference mechanisms  finally  as shown by our rte experiments  the
compact forest data structure allows our approach to scale well to practical settings that
involve very large rule bases and hundreds of rule applications per text hypothesis pair 
we demonstrated the utility of our approach in two different semantic tasks  experiments with unsupervised relation extraction showed that our exact proofs outperform the
more heuristic common practice of hypothesis embedding  we also achieved competitive
results on rte benchmarks  by adding a simple approximate matching module to our
inference engine  the contribution of semantic knowledge was illustrated on both tasks 
limitations and possible extensions for our formalism were discussed in section     
manual analysis of the inference engines performance on the relation extraction and rte
tasks further suggested promising directions for future research  as discussed in subsections
      and      two additional major areas for further research are the approximate matching
heuristics and the proof search strategy  stern and dagan        and stern  stern  dagan 
and felner        extended our work to address these two aspects  respectively 
  

fiknowledge based textual inference via parse tree transformations

acknowledgments
this article is based on the doctoral dissertation of the first author  which was completed
under the guidance of the second author at bar ilan university  bar haim         this
work was partially supported by israel science foundation grants         and         
the ist programme of the european community under the pascal network of excellence ist              the pascal   network of excellence of the european community
fp  ict                the israel internet association  isoc il   grant       and the
fbk irst bar ilan university collaboration  the third author is grateful to the azrieli
foundation for the award of an azrieli fellowship  the authors wish to thank cleo condoravdi for making the polarity lexicon developed at parc available for this research  we are
grateful to eyal shnarch for his help in implementing the experimental setup described in
section      we also thank iddo greental for his collaboration on developing the generic rule
base  finally  we would like to thank dan roth  idan szpektor  yonatan aumann  marco
pennacchiotti  marc dymetman and the anonymous reviewers for their valuable feedback
on this work 

appendix a  compact forest complete proofs
in this section  we provide complete proofs for the correctness of the compact inference
algorithm presented in section    we start with a few definitions 
definition let l  r be a rule matched and applied to a compact forest f  as in section      let l be a subtree of some represented tree t  t  f   in which l is matched  recall
that sl was defined as l excluding nodes matched by dual leaf variables  and similarly sr
was defined as a copy of r without its dual leaf variables that is generated and inserted into
f as part of rule application  the roots of sl and sr are denoted rl and rr respectively 
we say that a node s  sr is tied to a node s   sl   if s is set as a source node for one of
the outgoing d edges of s    due to alignment sharing or dual leaf variable sharing 
the graph operations performed when applying a rule l  r to a compact forest f
can be summarized as follows 
   adding the subtree sr to f 
   setting rr as a target node of a d edge in f 
   setting nodes in sr that are tied to nodes in sl as source nodes for d edges in f 
according to the rules of variable sharing and dual leaf variable sharing  recall that
these d edges are not part of sl  
first  we show a simple property of cdgs generated by the inference process 
lemma   every node in a cdg generated by the inference process has at most one incoming d edge 
  

fibar haim  dagan   berant

proof by construction  in the initial forest each node has at most one incoming d edge 
each rule application adds a subtree sr   whose nodes have at most one incoming d edge 
last  the root rr   which initially has no incoming edges  is set as a target for a single
d edge during rule application  the incoming d edge of rl    therefore  the lemma follows
by induction on the number of rule applications 
using the following theorem we show that the inference process generates a compact
forest 
theorem   applying a rule to a compact forest results in a compact forest 
proof let f   be the cdg generated by applying the rule l  r to a compact forest f 
we show that f   is a compact forest  that is  a cdag with a single root r where all the
embedded dags rooted in r are trees  first  we show that f   is a cdag   i e   it does not
contain a cycle of e edges  
assume by contradiction that f   contains a simple cycle of e edges c  applying the
rule l  r did not add any e edges between nodes in f  therefore  c must pass through
rr   the root of sr and contain an e edge  p  rr    since sr is a tree  c must also leave sr
through an e edge  u  v   u  sr and v 
  sr    the cycle can be written as p  rr      
u  v       p  notice that the path from v to p is fully contained in f since the cycle
c is simple and entering sr is possible only through rr  
l  r must be a substitution rule  otherwise p would be the root of f  this is
impossible  since the root has no incoming d edges  therefore  rr and rl have the same
single incoming d edge  and the e edge  p  rl   exists in f  in addition  u was added as a
source node of a d edge d in f since it is tied to some u   sl   which is also a source node of d 
therefore  the path rl       u   v exists in f  finally  we know that the path from v to
p is fully contained in f  therefore we can construct a cycle p  rl       u   v       p
in f  in contradiction to our assumption that f is a compact forest 
we have shown that f   is a cdag  next  we define a generalization for embedded dags 
which will help us show that all embedded dags in f   rooted in r are trees 
definition an embedded partial dag g    v  e  in a cdag g rooted in a node v  v is
similar to an embedded dag and is generated using the following process 
   initialize g with v alone
   repeat any number of iterations 
 a  choose a node s  v
 b  choose an outgoing d edge d of s that was not already chosen by s in a previous
iteration  if all d edges have been chosen   halt 
 c  choose a target node t  td and add the e edge  s  t d to g 
we now show that all embedded partial dags in f   rooted in any node are trees  since
an embedded dag is also an embedded partial dag  this proves that all embedded dags
in f   rooted in r are trees  assume by contradiction that after applying l  r there is an
  

fiknowledge based textual inference via parse tree transformations

embedded partial dag t   rooted at a node n that is not a tree  we can assume that n is
not in sr   otherwise  we can extend t   by adding a path p  rr       n  where p is a
node outside sr that is a source node of the incoming edge of rr  
since t   is not a tree  there are two simple paths p  and p  from n that reach some
node z from two different e edges  z cannot be in sr   since any two paths that meet in the
subtree sr   must first meet in its root rr entering its incoming d edge  however  we could
then construct in f the same two paths  only selecting rl instead of rr   in contradiction
with the assumption that f is a compact forest  clearly  either p  or p  must pass through
the new subtree sr   otherwise the two paths already existed in f 
we first handle the case where  without loss of generality  p  passes through sr and
p  does not  p  passes through sr and contains an e edge  p  rr    since z 
  sr   it
also contains an e edge  u  v  such that u  sr and v 
  sr   so p  can be written as
n       p  rr       u  v       z  the paths from n to p and from v to z
are in f  because the only way to enter sr is through rr and p  is simple  we can now
incrementally construct in f the following embedded partial dag t   first  we construct
p  and the section in p  from n to p as in t     next  we expand p with the e edge  p  rl  
instead of  p  rr    we would like to expand t from rl and reach z if possible 
as previously explained  u is tied to a node u   sl and therefore the e edge  u    v  exists
in f  therefore  there is a path p   in sl   from rl through  u    v  to z  however  it is not
guaranteed that the whole p   can be added to t   we try to expand t incrementally with
p     at each step adding the next e edge in the path  if we succeed  then t is an embedded
graph in f with two paths to z  a contradiction  if we fail  this can only be due to an e edge
 z     t   p   we cannot add  thus  z   must already be in p    and is a node for which there
are two distinct paths in the embedded graph t   a contradiction  the path constructed is
indeed different from p  since it contains the e edge  p  rl   that cannot be part of p    since
p  contains the disjoint edge  p  rr   
in the remaining case  both p  and p  pass through sr and reach a node z 
  sr   p 
can be written as n       u   v        z and p  as n       u   v        z 
where u    u   sr   and v    v  
  sr   assume first that the e edges  u    v    and  u    v   
originate from the same d edge d  then u     u    otherwise  u    v    and  u    v    could not
be both in the same embedded partial dag  u   u  are tied to the nodes u     u    sl  
we show that u      u     assume by contradiction that u     u     u    u  is tied to u 
and u  due to alignment sharing or dual leaf variable sharing  u  cannot be tied to both u 
and u  due to alignment sharing since alignment is a function from nodes in sl to nodes in
sr   it cannot be tied to both due to dual leaf variable sharing  since any variable appears
only once in r  finally  if u  is tied to u   without loss of generality  due to dual leaf
variable sharing  then the d edge d is part of l  therefore  u  will not include d as an
aligned modifier  and thus u  will not be tied to u  due to alignment 
we can now construct an embedded graph t rooted at rl in f  because sl is part of
the match of l in f  we can construct an embedded graph rooted at rl with a path to
any node in sl   in particular with paths to u   and u     since u      u     and both u   and
u   are source nodes of d  which is not part of sl   we can expand these two paths with the
e edges  u     v    and  u     v    of d and get an embedded graph in gn that is not a tree  a
contradiction 
  

fibar haim  dagan   berant

suppose that the e edges  u    v    and  u    v    originate from different d edges d  and
d  respectively  u  and u  are tied to u   and u     therefore  if v     v  we can construct
the following embedded graph t rooted at rl   as in the previous case  we can expand the
paths in sl from rl to u   and u     next  we add the e edges  u     v    of d  and  u     v    of
d    recall that d  and d  are not in sl and can therefore be used for expansion  we try
to expand this embedded graph to include the paths from v  and v  to z  if we succeed 
we have two paths in t leading to z  if we fail we have two paths in tn meeting at some
other node z     as explained above  last  if v    v    v  then v is a node in f that has two
incoming d edges  contradicting lemma   
the case of an introduction rule is quite similar but simpler  if p  passes through sr
and p  does not  then n must be the root of the compact forest  the only node with a path
to rr    however  in this case n has a single outgoing d edge  and therefore all its outgoing
e edges are disjoint  i e  cannot be part of the same embedded dag   thus  p  must also
pass through rr   a contradiction  if both p  and p  pass through sr   the proof is identical
to the case of a substitution rule 
we have shown that f   is a cdag whose embedded dags rooted in r are trees  f  
also has a single root because all new nodes added when applying l  r have an incoming
edge  hence  f   is a compact forest 
corollary   the inference process generates a compact forest 
proof it is easy to verify that initialization generates a compact forest  since applying
a rule to a compact forest results in a compact forest  the inference process generates a
compact forest by induction on the number of rule applications 
theorem   given a rule base r and a set of initial trees t   a tree t is represented by a
compact forest derivable from t by the inference process  t is a consequent of t according
to the inference formalism 
proof    we first show completeness by induction on the number of rule applications n 
if n     then t is one of the initial trees and is represented by the initial compact forest 
let tn   be a tree derived in the formalism by applying a sequence of n     rules  we show
that tn   is represented in a derivable compact forest  tn   was derived by applying the
rule l  r to the tree tn   according to the inductive assumption  tn is represented in a
compact forest f derivable by the inference process  therefore  the rule l  r can be
matched and applied in f  we assume l  r is a substitution rule since the case of an
introduction rule is similar  tn   is almost identical to tn except it contains the subtree
r instead of l with instantiated variables and aligned modifiers  it is easy to verify that
after application of l  r on f resulting in f     f   will contain an embedded tree t that is
almost identical to tn   except that the root of sr   rr   will be chosen instead of the root of
sl   rl   and the rest of sr can also be chosen with the appropriate instantiated variables
and modifiers  therefore  tn     t is contained in f   as required  t is guaranteed to be a
tree according to corollary   
   next  we prove soundness by induction on the number of rule applications in the
forest  at initialization  all of the initial trees are consequents  let fn   be a compact
forest derived by n     rule applications  corollary   guarantees that fn   is indeed a
  

fiknowledge based textual inference via parse tree transformations

compact forest   given a tree tn   represented by fn     we show that tn   is a consequent
in the formalism 
if tn   was already represented by the compact forest after n rule applications  then
according to the assumption of the induction it is a consequent in the formalism  if not 
then tn   is a new embedded tree created after the application of the rule l  r  therefore 
tn   contains the entire subtree sr   we now incrementally construct an embedded tree tn
represented by fn such that tn   is the result of applying l  r to tn  
for a substitution rule  we first construct the part of tn   that does not include the
subtree rooted at rr   for an introduction rule  we take any path from the forests root to
rl   next  we construct sl through rl instead of sr through rr   this is possible since
according to corollary   all embedded graphs are trees  therefore the nodes of sl are not
already in tn   we then look at the set of e edges  s  t   tn   such that s  sr and t 
  sr  
let  s  z  be such an edge originating from a d edge d and sz be the subtree rooted at z in
tn     notice that sz was already part of fn   s is tied to s   sl and therefore s  is a source
node of d  we can expand tn to include the edge  s    z  and sz if s  is not already used
with the d edge d in tn   this is guaranteed because d is not part of sl  only d edges that
are not part of sl are shared   finally  we complete the construction of tn by arbitrarily
expanding any unused outgoing d edge of tn s nodes  until we obtain a complete embedded
tree 
we constructed an embedded tree tn in fn   therefore  according to the inductive
assumption  tn is a consequent in the formalism  tn contains sl and an instantiation of the
dual leaf variables  therefore  it is matched by l and the rule l  r can be applied  it is
easy to verify that an application of the rule on tn will yield tn     as required  thus  tn  
is also a consequent in the formalism 

for the sake of simplicity  the above proofs ignored the case where one or more leaf
variables in l that match multiple target nodes in l appear in r as non leaves  as described
in section      in this case the matched target nodes are inserted to sr as alternatives  with
proper sharing of their modifiers   consequently  sr becomes a compact forest containing
multiple trees  similarly  sl is a compact forest  whose represented trees correspond to the
possible choices of matching the leaf variables  the mapping between the nodes matched
by the leaf variables in sl and the nodes generated for them in sr defines a one to one
mapping between the trees in sl and sr  
the above proofs can be easily adapted to handle this case  as follows  first  the proof of
lemma   need not change  in theorem    the proof that a rule application does not create
cycles still holds if the underlying graph of sr is a dag rather than a tree  to prove that
each embedded partial dag t   is a tree  we observe that exactly one of the trees embedded
in sr is part of t     thus  we can consider only that tree in sr and its corresponding tree
in sl   while ignoring the rest of sr and sl   and proceed with the original proof  similarly 
to prove completeness in theorem    we refer to the tree represented in sl   which is part of
tn   and the corresponding tree in sr   to prove soundness  we consider each of the subtrees
in sr and their corresponding tree in sl  
  

fibar haim  dagan   berant

references
bar haim  r          semantic inference at the lexical syntactic level  ph d  thesis 
department of computer science  bar ilan university  ramat gan  israel 
bar haim  r   berant  j     dagan  i          a compact forest for scalable inference over
entailment and paraphrase rules  in proceedings of emnlp 
bar haim  r   berant  j   dagan  i   greental  i   mirkin  s   shnarch  e     szpektor  i 
        efficient semantic deduction and approximate matching over compact parse
forests  in proceedings of the tac      workshop 
bar haim  r   dagan  i   dolan  b   ferro  l   giampiccolo  d   magnini  b     szpektor 
i          the second pascal recognising textual entailment challenge  in the
second pascal challenges workshop on recognizing textual entailment 
bar haim  r   dagan  i   greental  i     shnarch  e          semantic inference at the
lexical syntactic level  in proceedings of aaai 
bar haim  r   szpektor  i     glickman  o          definition and analysis of intermediate
entailment levels  in proceedings of the acl workshop on empirical modeling of
semantic equivalence and entailment 
barzilay  r     lee  l          learning to paraphrase  an unsupervised approach using
multiple sequence alignment  in proceedings of hlt naacl 
barzilay  r     mckeown  k  r          extracting paraphrases from a parallel corpus  in
proceedings of acl 
bensley  j     hickl  a          workshop  application of lccs groundhog system
for rte    in proceedings of the tac      workshop 
bentivogli  l   clark  p   dagan  i   dang  h  t     giampiccolo  d          the sixth
pascal recognizing textual entailment challenge  in proceedings of the tac     
workshop 
bentivogli  l   dagan  i   dang  h  t   giampiccolo  d     magnini  b          the fifth
pascal recognizing textual entailment challenge  in proceedings of the tac     
workshop 
berant  j   dagan  i     goldberger  j          global learning of typed entailment rules 
in proceedings of acl 
bhagat  r     ravichandran  d          large scale acquisition of paraphrases for learning
surface patterns  in proceedings of acl     hlt 
bos  j     markert  k          recognising textual entailment with logical inference techniques  in proceedings of emnlp 
bos  j     markert  k          when logical inference helps determining textual entailment
 and when it doesnt   in proceedings of the second pascal recognising textual
entailment challenge 
chklovski  t     pantel  p          verbocean  mining the web for fine grained semantic
verb relations  in proceedings of emnlp 
  

fiknowledge based textual inference via parse tree transformations

collins  m     duffy  n          convolution kernels for natural language  in advances in
neural information processing systems    
connor  m     roth  d          context sensitive paraphrasing with a single unsupervised
classifier  in ecml 
cooper  r   crouch  r   van eijck  j   fox  c   van genabith  j   jaspars  j   kamp  h  
pinkal  m   milward  d   poesio  m   pulman  s   briscoe  t   maier  h     konrad  k 
        using the framework  tech  rep   fracas  a framework for computational
semantics 
dagan  i     glickman  o          probabilistic textual entailment  generic applied modeling of language variability  pascal workshop on text understanding and mining 
dagan  i   glickman  o   gliozzo  a   marmorshtein  e     strapparava  c       a   direct
word sense matching for lexical substitution  in proceedings of coling acl 
dagan  i   glickman  o     magnini  b       b   the pascal recognising textual entailment challenge  in quinonero candela  j   dagan  i   magnini  b     dalche buc  f 
 eds    machine learning challenges  lecture notes in computer science  vol       
pp          springer 
dagan  i   roth  d   sammons  m     zanzotto  f  m          recognizing textual entailment  models and applications  synthesis lectures on human language technologies 
morgan   claypool publishers 
de salvo braz  r   girju  r   punyakanok  v   roth  d     sammons  m          an inference
model for semantic entailment in natural language   in proceedings of aaai 
deerwester  s   dumais  s  t   furnas  g  w   landauer  t  k     harshman  r         
indexing by latent semantic analysis  journal of the american society of information
science                 
dinu  g     lapata  m          topic models for meaning similarity in context  in proceedings of coling       posters 
emele  m  c     dorna  m          ambiguity preserving machine translation using packed
representations  in proceedings of coling acl 
fellbaum  c   ed            wordnet  an electronic lexical database  language  speech
and communication  mit press 
gabrilovich  e     markovitch  s          computing semantic relatedness using wikipediabased explicit semantic analysis  in proceedings of ijcai 
ganitkevitch  j   van durme  b     callison burch  c          ppdb  the paraphrase
database  in proceedings of hlt naacl 
giampiccolo  d   magnini  b   dagan  i     dolan  b          the third pascal recognizing textual entailment challenge  in proceedings of the acl pascal workshop
on textual entailment and paraphrasing 
giampiccolo  d   trang dang  h   magnini  b   dagan  i     dolan  b          the fourth
pascal recognizing textual entailment challenge  in proceedings of the tac     
workshop 
  

fibar haim  dagan   berant

glickman  o     dagan  i          identifying lexical paraphrases from a single corpus  a
case study for verbs  in proceedings of ranlp 
glickman  o   shnarch  e     dagan  i          lexical reference  a semantic matching
subtask  in proceedings of emnlp 
haghighi  a  d   ng  a  y     manning  c  d          robust textual inference via graph
matching  in proceedings of emnlp 
harmeling  s          inferring textual entailment with a probabilistically sound calculus 
natural language engineering                 
heilman  m     smith  n  a          tree edit models for recognizing textual entailments 
paraphrases  and answers to questions  in proceedings of hlt naacl 
hickl  a          using discourse commitments to recognize textual entailment  in proceedings of coling 
hickl  a     bensley  j          a discourse commitment based framework for recognizing textual entailment  in proceedings of the acl pascal workshop on textual
entailment and paraphrasing 
hickl  a   bensley  j   williams  j   roberts  k   rink  b     shi  y          recognizing textual entailment with lccs groundhog system  in the second pascal
challenges workshop on recognizing textual entailment 
jurafsky  d     martin  j  h          speech and language processing  an introduction
to natural language processing  computational linguistics and speech recognition
 second edition   prentice hall 
kamp  h     reyle  u          from discourse to logic  introduction to modeltheoretic
semantics of natural language  formal logic and discourse representation theory 
kluwer academic publishers  dordrecht 
kay  m          chart generation  in proceedings of acl 
kazama  j     torisawa  k          exploiting wikipedia as external knowledge for named
entity recognition  in proceedings of emnlp conll 
kipper  k          verbnet  a broad coverage  comprehensive verb lexicon  ph d  thesis 
university of pennsylvania 
kouylekov  m     magnini  b          tree edit distance for textual entailment  in proceedings of ranlp 
lehmann  j   bizer  c   kobilarov  g   auer  s   becker  c   cyganiak  r     hellmann 
s          dbpedia   a crystallization point for the web of data  journal of web
semantics 
lin  d          dependency based evaluation of minipar  in proceedings of the workshop
on evaluation of parsing systems at lrec 
lin  d     pantel  p          discovery of inference rules for question answering  natural
language engineering                
lotan  a   stern  a     dagan  i          truthteller  annotating predicate truth  in
proceedings of hlt naacl 
  

fiknowledge based textual inference via parse tree transformations

maccartney  b   galley  m     manning  c  d          a phrase based alignment model
for natural language inference  in proceedings of emnlp 
maccartney  b   grenager  t   de marneffe  m  c   cer  d     manning  c  d         
learning to recognize features of valid textual entailments  in proceedings of hltnaacl 
maccartney  b     manning  c  d          an extended model of natural logic  in proceedings of iwcs   
macleod  c   grishman  r   meyers  a   barrett  l     reeves  r          nomlex  a lexicon
of nominalizations  in proceedings of euralex   
maxwell iii  j  t     kaplan  r  m          a method for disjunctive constraint satisfaction  in tomita  m   ed    current issues in parsing technology  kluwer academic
publishers 
mehdad  y     magnini  b       a   a word overlap baseline for the recognizing textual
entailment task  unpublished manuscript 
mehdad  y     magnini  b       b   optimizing textual entailment recognition using particle swarm optimization  in proceedings of the      workshop on applied textual
inference 
melamud  o   berant  j   dagan  i   goldberger  j     szpektor  i          a two level
model for context sensitive inference rules  in proceedings of acl 
meyers  a   reeves  r   macleod  c   szekeley  r   zielinska  v     young  b          the
cross breeding of dictionaries  in proceedings of lrec 
mi  h   huang  l     liu  q          forest based translation  in proceedings of acl    
hlt 
mirkin  s   dagan  i     pado  s          assessing the role of discourse references in
entailment inference  in proceedings of acl 
mirkin  s   dagan  i     shnarch  e          evaluating the inferential utility of lexicalsemantic resources  in proceedings of eacl 
moldovan  d  i     rus  v          logic form transformation of wordnet and its applicability to question answering  in proceedings of acl 
nairn  r   condoravdi  c     karttunen  l          computing relative polarity for textual
inference  in proceedings of international workshop on inference in computational
semantics  icos    
pang  b   knight  k     marcu  d          syntax based alignment of multiple translations 
extracting paraphrases and generating new sentences  in proceedings of hlt naacl 
pantel  p   bhagat  r   coppola  b   chklovski  t     hovy  e          isp  learning
inferential selectional preferences  in proceedings of hlt naacl 
ponzetto  s  p     strube  m          deriving a large scale taxonomy from wikipedia  in
proceedings of aaai 
ravichandran  d     hovy  e          learning surface text patterns for a question answering system  in proceedings of acl 
  

fibar haim  dagan   berant

ritter  a   mausam    etzioni  o          a latent dirichlet allocation method for selectional
preferences  in proceedings of acl 
romano  l   kouylekov  m   szpektor  i   dagan  i     lavelli  a          investigating a
generic paraphrase based approach for relation extraction  in proceedings of eacl 
ron  t          generating entailment rules based on online lexical resources  masters
thesis  computer science department  bar ilan university 
ruppenhofer  j   sporleder  c   morante  r   baker  c     palmer  m          semeval     task     linking events and their participants in discourse  in proceedings of
the workshop on semantic evaluations  recent achievements and future directions
 sew       
saint dizier  p     mehta melkar  r   eds            proceedings of the joint workshop fam lbr kraq    learning by reading and its applications in intelligent
question answering 
schoenmackers  s   etzioni  o   weld  d  s     davis  j          learning first order horn
clauses from web text  in proceedings of emnlp 
shinyama  y   sekine  s   sudo  k     grishman  r          automatic paraphrase acquisition from news articles  in proceedings of hlt 
shnarch  e   barak  l     dagan  i          extracting lexical reference rules from
wikipedia  in proceedings of acl ijcnlp 
snow  r   jurafsky  d     ng  a  y       a   semantic taxonomy induction from heterogenous evidence  in proceedings of coling acl 
snow  r   vanderwende  l     menezes  a       b   effectively using syntax for recognizing
false entailment  in proceedings of hlt naacl 
stern  a     dagan  i          a confidence model for syntactically motivated entailment
proofs  in proceedings of ranlp 
stern  a     dagan  i          recognizing implied predicate argument relationships in
textual inference  in proceedings of acl 
stern  a   stern  r   dagan  i     felner  a          efficient search for transformation based
inference  in proceedings of acl 
szpektor  i     dagan  i          learning canonical forms of entailment rules  in proceedings
of ranlp 
szpektor  i     dagan  i          learning entailment rules for unary templates  in proceedings of coling 
szpektor  i     dagan  i          augmenting wordnet based inference with argument
mapping  in proceedings of acl ijcnlp workshop on applied textual inference
 textinfer  
szpektor  i   dagan  i   bar haim  r     goldberger  j          contextual preferences  in
proceedings of acl     hlt 
szpektor  i   tanev  h   dagan  i     coppola  b          scaling web based acquisition of
entailment patterns  in proceedings of emnlp 
  

fiknowledge based textual inference via parse tree transformations

tatu  m   iles  b   slavick  j   novischi  a     moldovan  d          cogex at the second recognizing textual entailment challenge  in the second pascal challenges
workshop on recognizing textual entailment 
tatu  m     moldovan  d          a logic based semantic approach to recognizing textual
entailment  in proceedings of coling acl 
tatu  m     moldovan  d          cogex at rte   in proceedings of the acl pascal
workshop on textual entailment and paraphrasing 
valencia  v  s          studies on natural logic and categorial grammar  ph d  thesis 
university of amsterdam 
van deemter  k     kibble  r          on coreferring  coreference in muc and related
annotation schemes  computational linguistics                 
voorhees  e  m     harman  d          overview of the sixth text retrieval conference
 trec     in proceedings of trec 
wang  m     manning  c          probabilistic tree edit models with structured latent
variables for textual entailment and question answering  in proceedings of coling 
wang  r     neumann  g          recognizing textual entailment using a subsequence
kernel method  in proceedings of aaai 
yates  a     etzioni  o          unsupervised methods for determining object and relation
synonyms on the web  journal of artificial intelligence research  jair              
zanzotto  f  m   pennacchiotti  m     moschitti  a          a machine learning approach
to textual entailment recognition  natural language engineering                 

  

fi