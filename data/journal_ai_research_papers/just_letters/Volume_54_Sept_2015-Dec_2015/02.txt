journal of artificial intelligence research                 

submitted        published      

word vs  class based word sense disambiguation
ruben izquierdo

ruben   izquierdobevia   vu   nl

vu university of amsterdam
amsterdam  the netherlands

armando suarez

armando   dlsi   ua   es

university of alicante
alicante  spain

german rigau

german   rigau   ehu   es

university of the basque country
san sebastian  spain

abstract
as empirically demonstrated by the word sense disambiguation  wsd  tasks of the last senseval semeval exercises  assigning the appropriate meaning to words in context has resisted all
attempts to be successfully addressed  many authors argue that one possible reason could be the
use of inappropriate sets of word meanings  in particular  wordnet has been used as a de facto
standard repository of word meanings in most of these tasks  thus  instead of using the word
senses defined in wordnet  some approaches have derived semantic classes representing groups
of word senses  however  the meanings represented by wordnet have been only used for wsd
at a very fine grained sense level or at a very coarse grained semantic class level  also called supersenses   we suspect that an appropriate level of abstraction could be on between both levels 
the contributions of this paper are manifold  first  we propose a simple method to automatically
derive semantic classes at intermediate levels of abstraction covering all nominal and verbal wordnet meanings  second  we empirically demonstrate that our automatically derived semantic classes
outperform classical approaches based on word senses and more coarse grained sense groupings 
third  we also demonstrate that our supervised wsd system benefits from using these new semantic classes as additional semantic features while reducing the amount of training examples 
finally  we also demonstrate the robustness of our supervised semantic class based wsd system
when tested on out of domain corpus 

   introduction
word sense disambiguation  wsd  is an intermediate natural language processing  nlp  task
that consists in assigning the correct lexical interpretation to ambiguous words depending on the surrounding context  agirre   edmonds        navigli         one of the most successful approaches
in the last years is the supervised learning from examples  in which machine learning classification
models are induced from semantically annotated corpora  marquez  escudero  martnez    rigau 
       quite often  machine learning systems have obtained better results than the knowledge based
ones  as shown by experimental work and international evaluation exercises such as senseval or semeval    nevertheless  lately some weakly supervised or knowledgebased approaches are reaching
a performance close to the supervised techniques on some specific tasks  in all these tasks  the
   all the information about these competitions can be found at http   www senseval org 
c
    
ai access foundation  all rights reserved 

fii zquierdo   s u arez   r igau

corpora are usually manually annotated by experts with word senses taken from a particular lexical
semantic resource  most commonly wordnet  fellbaum        
however  wordnet has been widely criticized for being a sense repository that often provides
too finegrained sense distinctions for higher level applications like machine translation  mt  or
question   answering  aq   in fact  wsd at this low level of semantic granularity has resisted
all attempts of inferring robust broad coverage models  it seems that many wordsense distinctions
are too subtle to be captured by automatic systems with the current small volumes of wordsense
annotated examples  using wordnet as a sense repository  the organizers of the english all words
task at senseval   reported an inter annotation agreement of        snyder   palmer         interestingly  this result is difficult to outperform by state of the art sense based wsd systems 
moreover  supervised sensebased approaches are too biased towards the most frequent sense or
the predominant sense on the training data  therefore  the performance of supervised sensebased
systems is strongly punished when applied to domain specific texts where the sense distribution differs considerably with respect to the sense distribution in the training corpora  escudero  marquez 
  rigau         
in this paper we try to overcome these problems by facing the task of wsd from a semantic
class point of view instead of the traditional word sense based approach  a semantic class can be
seen as an abstract concept that groups subconcepts and word senses sharing some semantic properties or features  examples of semantic classes are vehicle  food or animal  our hypothesis is
that using an appropriate set of semantic classes instead of word senses could help wsd in several
aspects 
 a higher level of abstraction could ease the integration of wsd systems into other higher
level nlp applications such as machine translation or question   answering
 grouping together semantically coherent sets of training examples could also increase the
robustness of supervised wsd systems
 the socalled bottleneck acquisition problem could also be alleviated
these points will be further explained along the paper  following this hypothesis we propose to
create classifiers based on semantic classes instead of word sense experts  one semantic classifier
will be trained for each semantic class and the final system will assign the proper semantic class to
each ambiguous word  instead of the sense as in traditional approaches   for example  using our
automatically derived semantic classes  that will be introduced later   the three senses of church in
wordnet     are subsumed by the semantic classes r eligious o rganization  b uilding and
r eligious c eremony  also note that these semantic classes still discriminate among the three
different senses of the word church  for instance  if we assign the semantic class b uilding to
an occurrence of church in a context  we still know that it refers to its second sense  additionally 
the semantic class b uilding now covers more than six times more training examples than those
covered by the second sense of church 
an example of text from senseval  automatically annotated with semantic classes can be seen
in figure    it shows the automatic annotations by our classbased classifiers with different semantic classes  blc stands for basic level concepts   izquierdo  suarez    rigau         ss
   we will use the following format throughout this paper to refer to a particular sense  wordnum
pos   where pos is the
part of speech  n for nouns  v for verbs  a for adjectives and r for adverbs  and num stands for the sense number 

  

fiw ord vs   c lass  based w ord s ense d isambiguation

for supersenses  ciaramita   johnson         wnd for wordnet domains  magnini   cavaglia 
      l  bentivogli   pianta        and sumo for suggested upper merged ontology  niles  
pease         incorrect assignments are marked in italics  the correct tags are included between
brackets next to the automatic ones  obviously  these semantic resources relate senses at different
level of abstraction using diverse semantic criteria and properties that could be of interest for subsequent semantic processing  moreover  their combination could improve the overall results since
they offer different semantic perspectives of the text 
id
 
 
 
 
 
 
 

word
an
ancient
stone
church
amid
the
fields

blc

ss

wnd

sumo

artifact n
building n

noun artifact
noun artifact

building
building

mineral
building

geographic area n
 physical object n  

noun location
 noun object 

factotum  geography 

landarea

 
  
  

 
the
sound

property n

noun attribute

factotum  acoustics 

radiatingsound
 soundattribute 

  
  

of
bells

device n

noun artifact

musicalinstrument

  
  
  
  
  

cascading
from
its
tower
calling

move v

verb motion

factotum  acoustics 
factotum

construction n
designate v
 request v  

noun artifact
factotum
verb stative
factotum
 verb communication 

building
communication
 requesting 

  
  

the
faithful

 sogroup n
cial group n  

noun group

person  religion 

group

  
  

to
evensong

time of day n
 writing n  

noun communication

religion

timeinterval
 text 

motion

table    example of the automatic annotation of a text with several semantic class labels
the main goal of our research is to investigate the performance of alternative semantic classes
derived from wordnet on supervised wsd  first  we propose a system to automatically extract sets
of semantically coherent groupings from nominal and verbal senses from wordnet  the system
allows to generate arbitrary sets of semantic classes at distinct levels of abstraction  second  we
also analyze its impact with respect to alternative semantic classes when performing classbased
wsd  our empirical results show that our automatically generated classes performs better than
those created manually  wndomains  sumo  supersenses  etc   while capturing more precise
information  third  we also demonstrate that our supervised wsd system benefits from using
these new semantic classes as additional semantic features while reducing the amount of training
  

fii zquierdo   s u arez   r igau

examples  finally  we show that our supervised class based system can be adapted to a particular
domain  traditional word sense based systems are also included only for comparison purposes 
summarizing  our research empirically investigates 
 the performance of alternative semantic groupings when used in a supervised class based
wsd system
 the impact of class based semantic features in our supervised wsd framework
 the required amount of training examples needed by a class based wsd in order to obtain
competitive results
 the relative performance of the class based wsd systems with respect wsd based on word
experts
 the robustness of our class based wsd system on specific domains
moreover  when tested on out of domain dataset  our supervised class based wsd system obtains slightly better results than a state of the art word sense based wsd system  the itmakessense
system presented by zhong and ng        
after this introduction  we present the work directly related with our research on supervised
wsd based on semantic classes  then  section   presents the sense groupings and semantic classes
used in this study  section   explains our method to automatically derive semantic classes from
wordnet at different levels of abstraction  moreover an analysis of different semantic groupings is
included  section    presents the system that we have developed to perform supervised class based
wsd  the performance of this system is shown in section    where the system is tested on several
wsd datasets provided by international evaluations  a comparison with other participants on these
competitions is introduced in sections   and    some experiments with our system applied to a
specific domain are analyzed in section    finally  some conclusions and future work are presented
in section    

   related work
the field of wsd is very broad  there have been a large amount of publications about wsd over the
last    years  this section only revises some relevant wsd approaches dealing with the appropriate
sets of meanings a word should have 
some research has been focused on deriving different word sense groupings to overcome the
finegrained distinctions of wordnet  hearst   schutze        peters  peters    vossen       
mihalcea   moldovan        agirre   de lacalle        navigli        snow  s   d     a         
that is  they provide methods for grouping senses of the same word  thus producing coarser word
sense groupings  for example  for the word church having three senses in wordnet      the sense
grouping presented by snow et al         only produces a unique grouping  that is  according to
this approach church is monosemous 
in the ontonotes project  hovy  marcus  palmer  ramshaw    weischedel         the different
meanings of a word are considered as a kind of tree  ranging from coarse concepts on the root to
finegrained meanings on the leaves  the merging was increased from fine to coarse grained until
obtaining an inter annotator agreement of around      this coarse grained repository was used
  

fiw ord vs   c lass  based w ord s ense d isambiguation

in the wsd lexical sample task of semeval       pradhan  dligach    palmer         where the
systems scored up to       fscore  note that this merging was created for each word following a
manual and very costly process 
similarly to the previous approach  another task was organized within semeval      which
consisted in the traditional wsd all word task using another coarsegrained sense repository derived
from wordnet  navigli  litkowski    hargraves         in this case all the wordnet synsets were
automatically linked to the oxford dictionary of english  ode  using a graph algorithm  all the
meanings of a word linked to the same ode entry were merged into a coarse sense  the systems
achieving the top scores followed supervised approaches taking advantage of different corpora for
the training  reaching a top fscore of        
both of the previous cases are aimed at solving the granularity problem of the word sense
definitions in wordnet  however  both approaches are still word experts  one classifier is trained
for each word   obviously  decreasing the average polysemy of a word by using coarsersenses
makes easier the classification choice  as a result  the performance of these systems increase at the
cost of reducing its discriminative power 
conversely  instead of word experts  our approach creates semantic class experts  each of these
semantic classifiers can exploit diverse information extracted from all the meanings from different
words that belong to that class 
wikipedia  wikipedia        has been also recently used to overcome some problems of the supervised learning methods  excessively finegrained definition of meanings  lack of annotated data
and strong domain dependence of the existing annotated corpora  in this way  wikipedia provides
a new source of annotated data  very large and constantly in expansion  mihalcea        gangemi 
nuzzolese  presutti  draicchio  musetti    ciancarini        
in contrast  some research has been focused on using predefined sets of sense groupings for
learning classbased classifiers for wsd  segond  schiller  greffenstette    chanod        ciaramita   johnson        villarejo  marquez    rigau        curran        ciaramita   altun 
      izquierdo  suarez    rigau         that is  grouping senses of different words into the same
explicit and comprehensive semantic class  also the work presented by mihalcea  csomai  and
ciaramita        makes use of three different sets of semantic classes  wordnet classes and two
named entity annotated corpora  to train sequential classifiers  the classifiers are trained using
basic features  collocations and semantic features  and they reach a performance around     and
the   th position in the semeval      allwords task 
the semantic classes of wordnet  also called supersenses  have been widely used in different
works  for instance  paa and reichartz      a  apply conditional random fields to model the
sequential context of words and their relation to supersenses  they also extend the model to include
the potential supersenses of each word into the training data  an f  score of       is reported  both
nouns and verbs  when only potential labels are used  no training data at all  which is just    worse
than when using the training data with right labels  although interesting  they only evaluate the
system applying a   fold cross validation on semcor 

   semantic classes and levels of abstraction
the meanings represented by wordnet have been only used for wsd at a very fine grained sense
level or at a very coarse grained semantic class level  also called supersenses   we suspect that an
appropriate level of abstraction could be found on between both levels  in this section we propose a
  

fii zquierdo   s u arez   r igau

simple method to automatically derive semantic classes at intermediate levels of abstraction covering all nominal and verbal wordnet meanings  first  we introduce wordnet  the semantic resource
and sense repository used by most wsd systems  also note that all semantic classes used in our
work are also linked to wordnet 
wordnet  fellbaum        is an online lexical database of english which contains concepts
represented by synsets  which are sets of synonyms of content words  nouns  verbs  adjectives and
adverbs   one synset groups together several senses of different words which are synonyms 
in wordnet different types of lexical and semantic relations interlink different synsets  creating
in this way a very large structured lexical and semantic network  the most important relation
encoded in wordnet is the subclass relation  for nouns is called hyponymy relation and for verbs
troponymy relation   table   shows some basic figures of different wordnet versions including the
total number of words  polysemous words  synsets  senses  all the possible senses for all the words 
and average polysemy 
version
wn    
wn    
wn      
wn    
wn    
wn    

words
       
       
       
       
       
       

polysemous
      
      
      
      
      
      

synsets
      
       
       
       
       
       

senses
       
       
       
       
       
       

avg  polysemy
    
    
    
    
    
    

table    statistics of wordnet versions 

    supersenses
supersenses is the name of the wordnet lexicographer files within the framework of wsd    more
in detail  wordnet synsets are organized into forty five supersenses  based on syntactic categories
 nouns  verbs  adjectives and adverbs  and logical groupings such as person  phenomenon 
feeling   location   etc  there are    basic categories for nouns     for verbs    for adjectives
and   for adverbs  in some cases  different senses of a word can be grouped at a high level under
the same supersense  reducing the polysemy of the word  this is often the case of very similar
senses of a word  having just a few classes for adjectives and adverbs  supersense taggers have
been usually developed only for nouns and verbs   tsvetkov  schneider  hovy  bhatia  faruqui   
dyer        presents a very interesting study on tagging adjectives with supersenses acquired from
germanet  hamp  feldweg  et al         
    wordnet domains
wordnet domains   wnd   magnini   cavaglia        l  bentivogli   pianta        is a hierarchy of     domains which have been used to label semi automatically all wordnet synsets  this
set of labels is organized into a taxonomy following the dewey decimal classification system   
   more information of these supersenses can be found at http   wordnet princeton edu wordnet 
man lexnames  wn html 
   http   wndomains itc it
   http   www oclc org dewey

  

fiw ord vs   c lass  based w ord s ense d isambiguation

for building wnd  many labels were assigned to high levels of the wordnet hierarchy and were
automatically inherited across the hypernym and troponym hierarchy  thus  the semi automatic
method  used to develop this resource was not free of errors and inconsistencies  castillo  real   
rigau        gonzalez  rigau    castillo        
information brought by domain labels is complementary to what is already in wordnet  wnd
present some characteristics that can be interesting for wsd  first of all  a domain label may contain
senses from different wordnet subhierarchies  derived from different supersenses   for instance 
the domain religion contains senses such as priest  deriving from noun   person and church 
deriving from noun   artifact  second  a domain label may also include synsets of different
syntactic categories  for instance  the domain religion also contains the verb pray or the adjective
holy 
furthermore  a single wnd label can subsume different senses of the same word  reducing in
this way its polysemy  for instance  the first and third senses of church in wordnet     have the
domain label religion 
    sumo concepts
sumo   niles   pease        was created as part of the ieee standard upper ontology working
group  their goal was to develop a standard upper ontology to promote data interoperability  information search and retrieval  automated inference  and natural language processing  s umo consists
of a set of concepts  relations  and axioms that formalize an upper ontology  for these experiments 
we used the complete wordnet     mapping with       s umo labels  niles   pease         in this
case  the three noun senses of church in wordnet     are classified as r eligious o rganization 
b uilding and r eligious c eremony according to the sumo ontology 
    example of semantic classes
as an example  table   presents the three senses and glosses of the word church in wordnet     
sense
 

 
 

wordnet    
gloss
 
christian churchn a group of christians  any group professing
christian doctrine or belief  church is a biblical term for assembly
church n church building n
for public  especially christian  worship  the
church was empty
church service n church n
a service conducted in a church  dont be late
for church
word senses
church n
christianity n

table    glosses and examples for the senses of churchn
in table   we show the classes assigned to each sense according to the semantic resources introduced previously  for instance  considering wordnet domains  it can be observed that the senses
number    group of christians  and    service conducted in a church  belong to the same domain
   it was based on several cycles of manual checking over automatically labeled data 
   http   www ontologyportal org

  

fii zquierdo   s u arez   r igau

religion   on the contrary  supersenses and sumo represent the three senses of church using
different semantic classes  also note that the resulting assignment of semantic classes identifies
each word sense individually 

sense
 
 
 

supersense
noun   group
noun   artifact
noun   act

semantic class
wnd
sumo
r eligion r eligious o rganization
b uildings
b uilding
r eligion
r eligious c eremony

table    semantic classes for the noun churchn
    levels of abstraction
basic level concepts  rosch         hereinafter blc  are the result of a compromise between two
conflicting principles of characterization  general vs  specific  
 represent as many concepts as possible
 represent as many features as possible
as a result of this conflicting characterization  blc typically should occur in the middle levels
of the semantic hierarchies 
the notion of base concepts  hereinafter bc  was introduced in eurowordnet  vossen        
bc are supposed to be the most important concepts in several language specific wordnets  this
importance can be measured in terms of two main criteria 
 a high position in the semantic hierarchy
 having many relations to other concepts
in eurowordnet a set of       concepts were selected and called common base concepts 
common bc are concepts that act as bc in at least two languages  only local wordnets for english 
dutch and spanish were used to select this set of common bc  in later initiatives  similar sets have
been derived to harmonize the construction of multilingual wordnets 
considering both definitions  in the next section we present a method to automatically generate
different sets of basic level concepts from wordnet at different levels of abstraction 

   automatic selection of basic level concepts
several approaches have been developed trying to alleviate the fine granularity problem of wordnet
senses by obtaining word sense groupings  hearst   schutze        peters et al         mihalcea
  moldovan        agirre   de lacalle        navigli        snow et al         bhagwani  satapathy    karnick         in most cases the approach consists on grouping different senses of
the same word  resulting in a decrease of the polysemy  obviously  when the polysemy is reduced
the wsd task as a classification problem becomes easier  and a system using these coarse senses
obtain better results than other systems using word senses  other works have used predefined sets
of semantic classes  mainly supersenses  segond et al         ciaramita   johnson        curran 
  

fiw ord vs   c lass  based w ord s ense d isambiguation

      villarejo et al         ciaramita   altun        picca  gliozzo    ciaramita        paa  
reichartz      b  tsvetkov et al         
in this section  we describe a simple method to automatically create different sets of basic level
concepts from wordnet  the method exploits the nominal and verbal structure of wordnet  the
basic idea is that synsets in wordnet having a high number of relations are important  and they could
be candidates to be a blc  to capture the relevance of a synset in wordnet we have considered two
options 
   all  the total number of relations encoded in wordnet for the synset
   hypo  the total number of the hyponymy relations of the synset
our method follows a bottomup approach exploiting the hypernymy chains of wordnet  for
each synset  the process starts visiting the synsets in the hyperonymy chain and selecting  and
stopping the walk for this synset  as its blc the ancestor having the first local maximum considering
the total number of relations  either all or hypo     for synsets having more than one hyperonym  the
method chooses the one with the higher number of relations to continue the process  this process
ends with a preliminary set of candidate synsets selected as potential blc 
additionally  each synset selected as a potential blc candidate must subsume  or represent  at
least a certain number of descendant synsets  thus  the minimum number of synsets a blc must
subsume is another parameter of the algorithm  and it is represented by the symbol   candidate
blcs that do not reach this threshold are discarded  and their subsumed synsets are reassigned to
other blc candidate appearing in higher levels of abstraction 
algorithm   presents the pseudocode of the algorithm  the parameters of the algorithm are 
a wordnet resource  the type of relations considered  all or hypo   and the minimum number
of concepts that must be subsumed by each blc     the algorithm has two phases  the first
one selects the candidate blc  following the bottomup approach  the second phase discards the
candidate blc that do not satisfy the  threshold 
figure   shows a schema to illustrate the selection process  each node represents a synset  and
the edges represent the hyperonymy relations  for instance  a is the hyperonym of d  and d is the
hyperonym of f   the number under each synset indicates its number of hyponymy relations 
the schema illustrates the selection process of blc candidates for synset j using criterion hypo 
the process starts checking the hyperonym of j  which is f  f has two hyperonyms  b and d  the
next synset visited in the hyperonymy chain of j is d since it has a higher number of hyponymy
relations  three   again the algorithm compares the number of relations of the hyperonym synset  d
with three relations   with those from the previous synset  f with two   as the number is increasing
the process continues  now  the next node to visit is a  as the number of relations of a is two and
the number for d is three  the process stops and the synset selected as blc candidate for j is d 
table   shows a real example of the selection process for the noun church in wordnet      the
hyperonym chain and the number of relations encoded in wordnet  all criterion  are shown for each
synset  the local maximum in the chain is marked in bold 
   the algorithm starts by checking the first hyperonym of the synset  not the synset itself 

  

fii zquierdo   s u arez   r igau

figure    example of blc selection
 rel 
  
  
  
  
  
 
 rel 
  
  
  
  
  
  
  
 rel 
  
  
 
  
 
 

synset
group   grouping  
social group  
organisation   organization  
establishment   institution  
faith   religion  
christianity   church   christian church  
synset
entity   something  
object   physical object  
artifact   artefact  
construction   structure  
building   edifice  
place of worship       
church   church building  
synset
act   human action   human activity  
activity  
ceremony  
religious ceremony   religious ritual  
service   religious service   divine service  
church   church service  

table    blc selection for the noun church in wordnet    

  

fiw ord vs   c lass  based w ord s ense d isambiguation

algorithm   blc extraction
require  wordnet  wn    typeofrelation  t   threshold   
blccandidates   
for each  synset s  w n   do
cur    s
 obtaining the hypernym chains for the current synset cur 
h    hypernyms w n  cur 
new    synsetw ithm orerelations w n  h  t  
 iterating while the number of relations is increased 
while n umof rels w n  t  cur    n umof rels w n  t  new  do
cur    new
h    hypernyms w n  cur 
new    synsetw ithm orerelations w n  h  t  
end while store cur as a candidate blc 
blccandidates    blccandidates   cur 
end for
 filtering out the blc candidates 
blcf inal   
for each  blc  blccandidates  do
if    n umberof descendants w n  blc  then
blcf inal    blcf inal   blc 
end if
end for
return blcf inal

figure    example of blc selection for the sense   of church
  

fii zquierdo   s u arez   r igau

in figure   we can see a diagram showing a partial view of the selection process of a candidate
blc for the sense number   of the noun church  the synset dotted is the synset that is being
processed  church n    the synsets in bold are those that are visited by the algorithm  and the one
in gray  building n   is the one selected as blc for church n   the process stops checking the synset
for structure n as the number of relations is     which is lower than the number of relations of the
previous synset     relations for edifice n   
obviously  combining different values for the  threshold  for example           or     and the
criterion considered by the algorithm  all or hypo   the process ends up in different sets of blc
extracted automatically from any wordnet version 
furthermore  instead of the number of relations we can consider the frequency of the synsets
in a corpus as a measure of its importance  synset frequency can be calculated as the sum of the
frequencies of the word senses contained in the synset  which can be obtained from semcor  miller 
leacock  tengi    bunker         or wordnet 
to sum up  the algorithm has two main parameters  the  parameter  representing the minimum number of synsets that each blc must represent  and the criterion used for characterizing the
relevance of the synsets  the values for both parameters can be 
  parameter  any integer value greater or equal to  
 synset relevance parameter  the value considered to measure the importance of the synset 
four possibilities 
 number of relations of the synset
 all  all relations encoded for the synset
 hypo  only hyponymy relations
 frequency of the synset
 freqwn  frequency obtained using wordnet
 freqsc  frequency obtained using semcor
an implementation of this algorithm and the different sets of blc used in this paper for several
wordnet versions are freely available   
    analysis of basic level concepts
we have selected wordnet     to generate several sets of blc  combining the four types of synset
relevance criteria and values           and    for   these values have been selected since they
represent different levels of abstraction  ranging from       no filtering  to        each blc
must subsume at least    synsets   table   shows  for combinations of  and synset relevance
parameters  the number of concepts that each set of blc contains  and the average depth on the
wordnet hierarchy of each group  in gray we highlight the two sets of blc  blc    and blc   
with all relations parameter  that we use through all the experiments described in this paper 
as expected  increasing the  threshold has a direct effect on the number of blc and on its
average depth in the wordnet hierarchy  in particular  both values are decreased  indicating that
when the  threshold is increased  the concepts selected are more abstract and general  for instance 
   http   adimen si ehu es web blc

  

fiw ord vs   c lass  based w ord s ense d isambiguation

 threshold

synset relevance

 

  

  

  

all
hypo
freqsc
freqwn
all
hypo
freqsc
freqwn
all
hypo
freqsc
freqwn
all
hypo
freqsc
freqwn

  blc
nouns verbs
           
           
            
            
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
  
   
  
   

depth
nouns verbs
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

table    automatic base level concepts for wn   

using  all  in the nominal part of wordnet  the number of concepts selected range from       with
no filtering        to              however  on average  its depth reduction is not so acute since
it varies from      to       this fact shows the robustness of our method for selecting synsets from
an intermediate level of abstraction 
also as expected  the verbal part of wordnet behave differently  in this case  since the verbal
hierarchies are less deep  the average depth of the synsets selected ranges from      to only     
using all relations  and from      to      using hypo relations 
in general  when using the frequency criteria  we can observe a similar behavior than when
using the relation criteria  however  now the effect of the threshold is more dramatic  specially for
nouns  again  as expected  verbs behave differently than nouns  the number of blc  for both
semcor and wordnet frequencies  reaches a plateau of around      in fact  this number is very
close to the verbal top beginners of wordnet 
summing up  we have devised a simple automatic procedure for deriving different sets of blc
representing at a different level of abstraction the whole set of nominal and verbal synsets of wordnet  in the following section we show and explain the supervised framework developed for wsd
in order to exploit the semantic classes described in this section and the previous one 

   supervised class based wsd
we follow a supervised machine learning approach to develop a set of semantic class based wsd
classifiers  our systems use an implementation of a support vector machine algorithm to train the
classifiers  one per semantic class  on semantic annotated corpora for acquiring both positive and
negative examples of each class  these classifiers are built on the basis of a set of features defined
for representing these examples  being class based  the training data must be collected and treated
in a pretty different way than in the usual word based approach 
  

fii zquierdo   s u arez   r igau

first  word based and class based approaches selects the training examples very differently  in
the word based approach  only instances of the same word can be used as training examples  figure
  shows the distribution of training examples used to generate a word sense classifier for the noun
house  following the binary definition of svm  one classifier is generated for each word sense  for
each of these classifiers  only occurrences of the word sense associated with the classifier can be
used as positive examples  while the rest of word sense occurrences are used as negative examples 
classifier for house

classifier
for sense  

    house n      

classifier
for sense  

    house n     

    house n      

classifier
for sense  

    house n      

    house n      

figure    distribution of examples using a word based approach
in a class based approach  we can use all the examples from all the words that belong to a
particular semantic class  figure   shows the distribution of examples in the class based approach 
in this case  one classifier is created for each semantic class  all occurrences of words belonging
to the semantic class associated with the classifier can be used as positive examples  while the rest
of occurrences of word senses associated with a different semantic class are selected as negative
examples 
obviously  in the class based approach the number of examples for training is increased  table
  shows an example for sense church n   following a word based approach only    examples can
be found in semcor for church n   conversely      positive training examples can be used when
building a classifier for the semantic class building  edifice 
we think that this approach has several advantages  first  semantic classes reduce the average
polysemy degree of words  some word senses might be grouped together within the same semantic
class   moreover  the acquisition bottleneck problem in supervised machine learning algorithms is
attenuated because of the increase in the number of training examples  however  we are mixing
in one classifier examples from very different words  for instance  for the building class we are
grouping together examples from hotel  hospital or church  which could introduce noise in the
learning process when grouping unrelated word senses 
    the learning algorithm  svm
support vector machines  svm  have been proven to be robust and very competitive in many nlp
tasks  and in wsd in particular  marquez et al          in our experiments  we used svm light
  

fiw ord vs   c lass  based w ord s ense d isambiguation

classifier for animal

classifier for building

   hospital  
 building 

   house  
 building 

   dog   
 animal 

   cat   
 animal 

   star  
 person 

figure    distribution of examples using a class based approach

church n

classifier
 word based approach 

building  edifice
 class approach 

examples
church n
church n
building n
hotel n
hospital n
barn n
       

  of positive examples
  
  
  
  
  
  
      
    examples

table    number of examples in semcor  word vs  class based approaches

  

fii zquierdo   s u arez   r igau

implementation  joachims         svm are used to induce a hyperplane that separates the positive
from the negative examples with a maximum margin  it means that the hyperplane is located in an
intermediate position between positive and negative examples  trying to keep the maximum distance
to the closest positive example  and to the closest negative example  in some cases  it is not possible
to get a hyperplane that divides the space linearly  or it is better to allow some errors to obtain a more
efficient hyperplane  this is known as soft margin svm  and requires the estimation of a parameter
 c   that represents the trade off allowed between training errors and the margin  we have set this
value to       which has been demonstrated as a good value for svm in wsd tasks 
when classifying an example  we obtain the value of the output function for each svm classifier
corresponding to each semantic class for the word example  and our system simply selects the class
having the greatest value 
    corpora
three semantic annotated corpora have been used for training and testing  semcor for training  and
senseval   and senseval   english all words tasks  for testing 
semcor  miller et al         is a subset of the brown corpus plus the novel the red badge
of courage  and it has been developed by the same group that created wordnet  it contains    
texts and around         running words  and more than         are also lemmatized and sensetagged according to princeton wordnet      the sense annotations from semcor have been also
automatically ported to other wordnet versions    
senseval     english all words corpus  hereinafter se    palmer  fellbaum  cotton  delfs   
dang        consists of       words of text from three wall street journal  wsj  articles representing different domains from the penn treebank ii  the sense inventory used for tagging was
wordnet     
senseval     english all words corpus  hereinafter se    snyder   palmer         is made up
of       words  extracted from two wsj articles and one excerpt from the brown corpus  sense
repository of wordnet       was used to tag       words with their proper senses 
we also considered alternative evaluation datasets  for instance  the semeval      coarse
grained task corpus     however  this dataset has been discarded because this corpus is annotated
with a particular set of word sense clusters  additionally  it does not provide a clear and simple way
to compare orthogonal sets of clusterings  although there have been more recent senseval semeval
tasks about wsd  we think that for the purpose of this evaluation  different level of abstraction in
wsd   senseval   and senseval   are still the datasets that best fit to our purposes  more recent
semeval competitions have been designed to address specific topics  such as multilinguality or joint
wsd and named entity recognition  however  we have also make some additional experiments
on domain adaptation with the dataset provided by semeval    task    all words word sense
disambiguation on a specific domain  wsd domain     agirre  lopez de lacalle  fellbaum 
hsieh  tesconi  monachini  vossen    segers        
   
   
   
   
   

http   web eecs umich edu mihalcea downloads html semcor
http   www sle sharp co uk senseval 
http   www senseval org senseval 
indeed we participated in this task with a preliminary version of our system
http   semeval  fbk eu semeval  php location tasks t  

  

fiw ord vs   c lass  based w ord s ense d isambiguation

    feature types
following previous contributions in supervised wsd  we have selected a set of basic features to
represent the training and testing examples  we also include additional features based on semantic
classes 
 basic features
 word forms and lemmas in a window of    words around the target word 
 pos  the concatenation of the preceding following three and five pos tags 
 bigrams and trigrams formed by lemmas and word forms in a window of   words
around the target word  we use all tokens regardless their pos to build bi trigrams  we
replace the target word by a character x in these features to increase its generalization 
 semantic features
 most frequent semantic class for the target word  calculated over semcor 
 monosemous semantic class of monosemous words in a window of size five words
around the target word 
basic features are those widely used in the literature  as the work presented by yarowsky        
these features are pieces of information that occur in the context of the target word  local features
including bigrams and trigrams  including the target word  of lemmas  word forms or partof
speech labels  pos   in addition  wordforms or lemmas in some larger window around the target
word are considered as features representing the topic of the discourse 
the set of features is extended with semantic information  several types of semantic classes
have been considered to create these features  in particular  two different sets of blc  blc   and
blc        supersenses  wordnet domains  wnd  and sumo 
in order to increase the generalization capabilities of the class based classifiers we filter out
irrelevant features  we measure the relevance of a feature   f for a class c in terms of the frequency
of f  for each class c  and for each feature f of that class  we calculate the frequency of the feature
within the class  the number of times that it occurs in examples of the class   and we also obtain
the total frequency of the feature for all the classes  we get the relative frequency by dividing
both values  classfreq   totalfreq  and if the result is lower than a certain threshold t  the feature is
removed from the feature list of the class c     in this way  we make sure that the features selected
for a class are more frequently related with that class than with others  we set this threshold t to
      obtained empirically with very preliminary versions of the classifiers when applying a crossvalidation setting on semcor 
    we have selected these set since they represent different levels of abstraction  as said in section       and    refer to
the threshold of minimum number of synsets that a possible blc must subsume to be considered as a proper blc 
these sets of blc were built using all criterion 
    that is  the value of the feature  for example a feature type can be word form  and a feature of that type can be
houses 
    depending on the experiment  around     of the original features are removed by this filter 

  

fii zquierdo   s u arez   r igau

   semantic classbased wsd experiments
in this section we present the performance of our semantic class based wsd system in the all
words wsd senseval    se   and senseval   se   datasets  we want to analyze the behavior
of our class based wsd system when working at different levels of abstraction  as we have said
before  the level of abstraction is defined by the semantic class used to build the classifiers 
an experiment is defined by two different parameters each one involving a particular set of
semantic classes 
   target class  the semantic classes used to train the classifiers  determining the abstraction
level of the system   in this case  we tested  word sense     blc    blc    wordnet domains  wnd   sumo and supersenses  ss  
   semantic features class  the semantic classes used for building the semantic features  in
this case  we tested  blc    blc    wnd  sumo and supersenses  ss  
the target class is the type of classes that the classifier assigns to a given ambiguous word  for
instance  the target class for the traditional word expert classifiers are word senses  the semantic
feature class is the one used for building the semantic features  which is independent of the target
class  for instance  we can use wordnet domains to extract monosemous words from the context
of the target word and use the wnd labels of these words as semantic features for building the
classifier 
combining different semantic classes for target and features  we generated the set of experiments described in the next sections  in that way  we can evaluate independently the impact of
selecting one semantic class or another as target class or as semantic feature class 
test
se 
se 

pos
n
v
n
v

sense
    
    
    
     

blc  
    
    
    
    

blc  
    
    
    
    

sumo
    
    
    
    

ss
    
    
    
    

wnd
    
    
    
    

table    average polysemy on se  and se 
table   shows the average polysemy  ap  measured on se  and se  with respect to the different semantic classes used in our evaluation as target classes  as expected  every corpus behaves
differently and the average polysemy for verbs is higher than for nouns  also as we could assume
in advance  relevant reductions on the polysemy degree are obtained when increasing the level of
abstraction  this fact is more acute also for verbs  note the large reduction of polysemy for verbs
when using supersenses and also wnd  also note that a priori se  seems to be more difficult to
disambiguate than se   independently of its abstraction level 
    baselines
as baselines of these evaluations we define the most frequent classes  mfc  of each word calculated
over semcor  ties between classes on a specific word are solved obtaining the global frequency in
    we included a word based evaluation for comparison purposes only since the current system have been designed for
class based evaluation 

   

fiw ord vs   c lass  based w ord s ense d isambiguation

semcor of each of these tied classes  and selecting the most frequent class over the whole training
corpus  when semcor has no occurrences of a particular word  that is  we are not able to calculate
the most frequent class of the word   we compute the global frequency for each of its possible
semantic classes  obtained from wordnet  over semcor  and we select the most frequent one  table
  shows the baseline for each semantic class over both testing corpora 
class
sense
blc  
blc  
sumo
supersense
wnd

pos
n
v
n
v
n
v
n
v
n
v
n
v

se 
mfc ap
          
          
          
          
          
          
          
          
          
          
          
          

se 
mfc
ap
     
    
           
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    

table    most frequent class baselines and average polysemy  ap  on se  and se 
as expected  the performances of the mfc baselines are very high  in particular  those corresponding to nouns  ranging from     to       while nominal baselines seem to perform similarly
in both se  and se   verbal baselines appear to be consistently much lower for se  than for se  
in se   verbal baselines range from     to     while in se  verbal baselines range from     to
     the results of wnd are very high due to its low polysemy degree for both nouns and verbs 
obviously  when increasing the level of abstraction  from senses to wnd  the results also increase 
    results of our basic system
in this section we present the performance of our supervised semantic classbased wsd system 
table    shows the results of the system when trained varying the target classes and using only
the basic feature set  their values correspond to the f  measures  harmonic mean of recall and
precision  when training our systems on semcor and testing on se  and se  test sets  the results
that improve the baselines are shown in italics  additionally  those results showing a statistically
significant positive difference when compared with its corresponding baseline using mcnemars
test are marked in bold 
interestingly  only the basic system at a word sense level outperforms the baselines in se  and
se  for both nouns and verbs  in addition  our systems obtain in some cases significantly better
results for verbs  also interesting is that on verbs at a word sense level the baselines and results are
very different  while at a class level the differences on both datasets are much smaller 
as expected  the results of the systems increase when augmenting the level of abstraction  from
senses to wnd   and in most cases  the baseline results are reached or outperformed  this is even
more relevant if we consider that the baseline results are already quite high  however  at a very high
level of abstraction  supersenses or wnd  our basic systems seem to be unable to outperform the
baselines 
   

fii zquierdo   s u arez   r igau

class
sense
blc  
blc  
sumo
supersense
wnd

pos
n
v
n
v
n
v
n
v
n
v
n
v

se 
     
     
     
     
     
     
     
     
     
     
     
     

se 
     
     
     
     
     
     
     
     
     
     
     
     

table     results of the basic system trained on semcor with a basic set of features and evaluated
against se  and se 

in general  the results obtained by blc   are not so different to those of blc    for instance 
if we consider the number of classes within blc        classes   blc        classes  and supersense     classes   blc classifiers obtain high performance rates while maintaining much higher
expressive power than supersenses  they are able to classify among much larger number of classes  
in fact  using supersenses     classes for nouns and verbs  we obtain a very accurate semantic tagger with a performance close to      even more interesting  we could use blc   for tagging
nouns      semantic classes and f  around      and supersenses for verbs     semantic classes
and f  around      
    results exploiting the semantic features
one of our main goals is to prove that simple semantic features added to the training process are
capable of producing significant improvements against the basic systems  the results of the experiments considering also the different types of semantic features are presented in tables    and    
respectively for nouns and verbs 
in both tables  the column labeled as class refers to what we have called the target class  and
the column labeled as sf indicates the type of semantic features included to represent the examples
within our machine learning approach 
again  the values in the tables correspond to the f  measures  harmonic mean of recall and
precision  when training our systems on semcor and testing on se  and se  test sets  the results
improving the baselines appear in italics  additionally  those results showing a statistically significant positive difference when compared with its corresponding baseline using the mcnemars test
are marked in bold 
regarding nouns  see table      a very different behavior is observed for se  and se   adding
semantic features mainly improves the results on se   while for se  none of the systems present
a significant improvement over the baselines  for se  such improvement is obtained when using
several types of semantic features  in particular  when using wnd features on se    the use of
semantic class based features seems to improve the systems using as target classes intermediate
levels of abstraction  specially blc   and blc     interestingly  in se  only blc   and blc  
   

fiw ord vs   c lass  based w ord s ense d isambiguation

class

sf
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd

sense

blc  

blc  

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

class

sumo

ss

wnd

sf
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table     results for nouns using the extended system
seem to provide some improvements over the baselines in some of the target classes  for instance 
blc    blc   and ss   although not significant 
regarding verbs  see table      also a very different behavior is observed for se  and se   in
this case  we can observe almost the opposite effect than for nouns  on se  most of the semantic
class features improve the results obtained by the baselines  while for se  only some of the systems
present a significant improvement over the baselines  for se  such improvement is obtained when
using several types of semantic features  however  in this case we also obtain significantly better
results for several semantic features on se   the use of semantic class based features seems to
benefit lower levels of abstraction  specially word sense  blc    blc   and also sumo  
in general  the results show that using semantic features in addition to the basic features helps to
reach a better performance for the class based wsd systems  additionally  it also seems that using
these semantic features we are able to obtain very competitive classifiers at a sense level 
    learning curves
we also investigate the behavior of the class based wsd system with respect the number of training
examples  although the same experiments have been carried out for nouns and verbs  we only
include the results for nouns since in both cases  the trend is very similar 
in these experiment  the semcor files have been randomly selected and added to the training
corpus in order to generate subsets of               etc  of the training corpus     then  we train
    each portion contains also the same files than the previous portion  for example  all files in the     portion are also
contained in the     portion 

   

fii zquierdo   s u arez   r igau

class

sense

blc  

blc  

sf
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

class

sumo

ss

wnd

sf
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table     results for verbs using the extended system
the system on each of the training portions and we test the system on se  and se   finally  we also
compare the resulting system with the baseline computed over the same training portion 
figures   and   present the learning curves over se  and se   respectively  in this case  we
selected a blc   class based wsd system using wordnet domains as semantic features    
surprisingly  in se  the system only improves the f  measure around    while increasing the
training corpus from     to      of semcor  in se   the system again only improves the f 
measure around    while increasing the training corpus from     to      of semcor  that is 
most of the knowledge required for the class based wsd system seems to be already present on a
small part of semcor 
figures   and   present the learning curves over se  and se   respectively  of a class based
wsd system based on supersenses using as semantic features those built with wordnet domains 
in se  the system just improves the f  measure around    while increasing the training corpus
from     to      of semcor  in se   the system again only improves the f  measure around   
while increasing the training corpus from     to      of semcor  that is  with only     of the
whole corpus  the class based wsd system reaches a f  close to the performance using all corpus 
in se  ans se   when using blc    figures   and    or supersenses  figures   and    as
semantic classes for wsd  the behavior of the system is similar to the mfc baseline  this is very
interesting since the mfc obtains very high results due to the way it is defined  the mfc over the
total corpus is assigned if there are no occurrences of the word in the training corpus  without this
definition  there would be a large number of words in the test set with no occurrences when using
    as shown in previous experiments  this combination obtains a very good performance 

   

fiw ord vs   c lass  based w ord s ense d isambiguation

  

system sv 
mfc sv 

  

  

  

  
f 
  

  

  

  

  
 

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

      

  corpus

figure    learning curve of blc   classifier on se 

  

system sv 
mfc sv 

  

  

  

f 

  

  

  

  

  
 

  

  

  

  

  

  

  

  

     
  corpus

  

  

  

  

  

  

  

figure    learning curve of blc   classifier on se 

   

      

fii zquierdo   s u arez   r igau

  

system sv 
mfc sv 

  

  

  

f 

  

  

  

  

  
 

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

      

  corpus

figure    learning curve of supersense classifier on se 

  

system sv 
mfc sv 

  

  

f 

  

  

  

  
 

  

  

  

  

  

  

  

  

     
  corpus

  

  

  

  

  

  

  

      

figure    learning curve of supersense classifier on se 

   

fiw ord vs   c lass  based w ord s ense d isambiguation

small training portions  in these cases  the recall of the baselines  and in turn f   would be much
lower 
this evaluation seems to indicate that the class based approach to wsd reduces considerably
the required amount of training examples 

   comparison with senseval systems  sense level
the main goal of the experiments included in this section is to verify whether the abstraction level
of our class based systems maintains its discriminative power when evaluated at a sense level  additionally  we compare our results against the results of the top participant systems in se  and se 
which provided the best senselevel outputs  thus  our class based systems have been adapted following a simple protocol  the output based on semantic classes is converted to sense identifiers 
instead of the semantic class produced by our systems for a particular instance  we select the first
sense of the word according to the wordnet sense ranking belonging to the predicted semantic class 
so  first we obtain the semantic class by means of our classifiers  then we obtain the restricted set of
senses for the word that match the semantic class obtained  and then we choose the most frequent
sense from that restricted subset 
the results of the first experiment on se  data are shown in table     all our systems have the
prefix svm  while the suffix denotes the type of semantic class used to generate the classifier    
in all cases in these experiments  wnd has been selected as target semantic class to generate the
semantic features  two baselines marked in italics have been also included  the first sense in wordnet  base wordnet  and the most frequent sense in semcor  base semcor   in fact  the developers
of wordnet ranked their word senses using semcor and other sense annotated corpora  thus  the
frequencies and ranks appearing in semcor and in wordnet are similar  but not equal  we also
include the results of our system when working at a word level  svm sense  
in both cases  for nouns and verbs  our systems outperform the most frequent baselines  the
most frequent sense for a word  according to the wordnet sense ranking is very competitive in
wsd tasks  and it is extremely hard to improve upon even slightly  mccarthy  koeling  weeds 
  carroll         as expected  the behavior of the different semantic features produces slightly
different results  however  independently of the semantic features used  in se  at a sense level  the
class based systems rank at the third position 
table    shows the same experiment but using se  dataset  in this case  our class based systems
clearly outperform the baselines  achieving the best results for nouns and the second place for verbs 
interestingly  for nouns  the best system at the se  did not achieve the semcor baseline  also recall
that se  seems to be more difficult than se  
it is worth to mention that our class based systems use the same features for both nouns and
verbs  for instance  we do not take profit of complex feature sets encoding syntactic information
that seems to be important for verbs 
these experiments show that class based classifiers seem to be quite competitive when evaluated at a word sense level  they perform over the most frequent sense according to wordnet and
semcor  and achieve the higher position for nouns and the second for verbs in se   and the third
position for nouns and verbs in se   obviously  this indicates that class based wsd maintains a
very high discriminative power at a word sense level 
    for instance  svm blc   stands for the experiment that creates classifier considering blc   semantic classes 

   

fii zquierdo   s u arez   r igau

class  sense on se 
nouns
verbs
system
f 
system
smuam
      smuaw
ave antwerp
      ave antwerp
svm semblc         svm semsumo
svm semblc         svm sense
svm semsumo       svm semwnd
svm semwnd
      svm semblc  
svm sense
      svm semss
svm semss
      svm semblc  
base wordnet
      lia sinequa
base semcor
      base semcor
lia sinequa
      base wordnet

f 
     
     
     
     
     
     
     
     
     
     
     

table     class to sense results on se   class to word sense transformation 

class  sense on se 
nouns
system
svm semwnd
svm semblc  
svm semsumo
svm sense
svm semblc  
svm semss
base semcor
gambl aw
base wordnet
kuaw
untaw
meaning allwords
lccaw

f 
     
     
     
     
     
     
     
     
     
     
     
     
     

verbs
system
gambl aw
svm semsumo
svm semwnd
svm semss
svm sense
svm semblc  
svm semblc  
untaw
meaning allwords
kuaw
r d 
base semcor
base wordnet

f 
     
     
     
     
     
     
     
     
     
     
     
     
     

table     class to sense results on se   class to word sense transformation 

   

fiw ord vs   c lass  based w ord s ense d isambiguation

   comparison with senseval systems  class level
the experiments presented in this section explore the performance of the word based classifiers
participating at se  and se  when are evaluated at a class level  to perform this kind of evaluation 
the word sense output of the participant systems have been mapped to its corresponding semantic
classes  our class based systems are not modified  obviously  we expect different performances
of the systems depending on the semantic class level  considering the results presented in tables
   and     in order to perform the comparison  we have selected the experiments that use wnd to
build the semantic features     thus  our system results using the different target semantic classes
are all represented by svm semwnd 
table    presents ordered by f  measure the results of the best performing systems on se  data
when evaluated at a different levels of abstraction  as previously  in italics we include the most
frequent senses according to wordnet base wordnet and semcor base semcor 
on se   independently of the abstraction level and pos  our system  svm semwnd  scores
at the first positions of the ranking  in one case our system reaches the best position  and twice
the second one  the baselines are outperformed in all experiments  except for nouns using wnd 
where basesemcor is very high 
table    presents ordered by f  measure the results of the best performing systems on se  data
when evaluated at a different levels of abstraction  in italics we include the most frequent senses
according to wordnet base wordnet and semcor base semcor  our systems are again represented
by svm semwnd 
on se   we can see that our system performs better than the baselines in most cases  except for
the semcorbased baseline on nouns  which obtains a very high result  in particular  our system
obtains very good results on verbs  reaching the first or second best positions in all cases  and
outperforming both baselines in all cases 
to sum up  our classbased approach outperforms most senseval participants  both se  and
se    at sense level and at semantic class level  this suggests that the good performance of the
semantic classifiers are not only due to the polysemy reduction  actually  it confirms that our
classbased semantic classifiers are learning from the semantic class training examples at different
abstraction levels 

   out of domain evaluation
in this section we describe our system at the semeval   allwords word sense disambiguation
on a specific domain task  izquierdo  suarez    rigau         the aim of this evaluation is to
show how robust our semantic class approach is when tested on a specific domain  different to the
domain of the training material 
traditionally  senseval competitions have been focused on general domain texts  thus  domain
specific texts present fresh challenges for wsd  for example  specific domains reduce the possible meaning of a word in a given context  moreover  the distribution of word senses on the data
examples changes when compared to general domains  these problems affect both supervised and
knowledgebased systems  in fact  supervised word based wsd systems are very sensitive to the
corpora used for training and testing the system  escudero et al         
    remind that the semantic features are the most frequent class of the target word  and the semantic class of monosemous words in the context around target word 

   

fii zquierdo   s u arez   r igau

nouns

verbs
f 
system
sense  blc  
smuaw
      smuaw
svm semwnd       svm semwnd
ave antwerp
      lia sinequa
base semcor
      ave antwerp
base wordnet
      base semcor
lia sinequa
      base wordnet
sense  blc  
smuaw
      smuaw
svm semwnd       svm semwnd
ave antwerp
      lia sinequa
base semcor
      ave antwerp
base wordnet
      base semcor
lia sinequa
      base wordnet
sense  sumo
smuaw
      smuaw
svm semwnd       lia sinequa
base semcor
      ave antwerp
ave antwerp
      svm semwnd
lia sinequa
      base semcor
base wordnet
      base wordnet
sense  supersense
svm semwnd       smuaw
smuaw
      lia sinequa
ave antwerp
      svm semwnd
base semcor
      ave antwerp
lia sinequa
      base wordnet
base wordnet
     
base semcor
sense  wnd
smuaw
      smuaw
base semcor
      svm semwnd
svm semwnd       base semcor
ave antwerp
      lia sinequa
base wordnet
      base wordnet
lia sinequa
      ave antwerp
system

f 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table     results for sense to blc    blc    sumo  supersense and wnd semantic classes on
se 

   

fiw ord vs   c lass  based w ord s ense d isambiguation

nouns

verbs
f 
system
sense  blc  
base semcor
      gambl aw
gambl aw
      svm semwnd
kuaw
      kuaw
lccaw
      r d 
untaw
      untaw
svm semwnd
      meaning allwords
base wordnet
     
base semcor
meaning allwords       base wordnet
sense  blc  
base semcor
      gambl aw
gambl aw
      svm semwnd
kuaw
      kuaw
svm semwnd
      r d 
lccaw
      untaw
untaw
      meaning allwords
base wordnet
      base semcor
r d 
      base wordnet
sense  sumo
base semcor
      gambl aw
kuaw
      svm semwnd
lccaw
      untaw
svm semwnd
      kuaw
untaw
      meaning allwords
gambl aw
      upv eaw 
base wordnet
     
base semcor
meaning allwords       base wordnet
sense  supersense
base semcor
      svm semwnd
kuaw
      gambl aw
svm semwnd
      base semcor
untaw
      base wordnet
gambl aw
      meaning allwords
upv eaw 
      meaning simple
upv eaw
      kuaw
base wordnet
      upv eaw 
sense  wnd
base semcor
      svm semwnd
svm semwnd
      base semcor
untaw
      untaw
kuaw
      gambl aw
gambl aw
      base wordnet
base wordnet
     
r d 
lccaw
      meaning simple
meaning allwords       kuaw
system

f 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table     results for sense to blc    blc    sumo  supersense and wnd semantic classes on
se 

   

fii zquierdo   s u arez   r igau

therefore  the main challenge is how to develop specific domain wsd systems or how to adapt
a general system to a particular domain  following this research line  a task was proposed within the
semeval  competition  allwords word sense disambiguation on a specific domain  agirre
et al          the restricted domain selected for this task was the environmental domain  the test
corpora consist of three texts compiled by the european center for nature conservation    ecnc 
and world wildlife forum    wwf   the task was proposed in several languages  chinese  dutch 
english an italian  although our participation was limited to english  more in detail  there were
a total of       noun tokens and     verb tokens to be tagged  moreover  a set of background
documents related with the environmental domain were provided  these texts were no sense tagged 
they were just plain text  and they were also provided by ecnc and wwf  they could be used by
the systems to help to the adaptation to the specific domain  for english  there were a total of    
background documents  containing           words 
we do not apply any kind of specific domain adaptation technique to our supervised classbased
system  in order to adapt our supervised system to the environmental domain we just increase automatically the training data with new training examples from the domain  to acquire these examples 
we use the     background documents of the environmental domain provided by the organizers  we
use treetagger  schmid        to preprocess the documents  performing postagging and lemmatization  since the background documents are not semantically annotated  and our supervised system
needs labeled data  we have selected only the monosemous instances occurring in the documents
according to our blc   semantic classes     note that this approach can only be exploited by classbased wsd systems  in this way  we have obtained automatically a large set of examples annotated
with blc    this semantic class was selected because it provided very good results in previous
experiments  in order to analyze how the same approach and system would work with other level of
abstraction  we performed the same evaluation a posteriori using blc    wordnet domains and
supersenses besides to blc    which was the official participation in semeval    nevertheless 
this section will be focused on blc   
regarding blc    table    presents the total number of training examples extracted from semcor  sc  and from the background documents  bg   as expected  by this method a large number
of monosemous examples can be obtained for nouns and verbs  although  verbs are much less productive than nouns  however  all these background examples correspond to a reduced set of      
monosemous words 
sc
bg
total

nouns
      
       
       

verbs
      
      
      

n v
       
       
       

table     number of training examples for blc  
table    lists the ten most frequent monosemous nouns and verbs occurring in the background
documents  remember that all these examples are monosemous according to blc   semantic
classes 
    http   www ecnc org
    http   wwf org
    blc    see section    stands for basic level concepts obtained with all relations criterion and a minimum threshold
of subconcepts subsumed equal to    

   

fiw ord vs   c lass  based w ord s ense d isambiguation

 
 
 
 
 
 
 
 
 
  

nouns
lemma
biodiversity
habitat
specie
climate
european
ecosystem
river
grassland
datum
directive

  ex 
     
     
     
     
     
     
     
     
     
     

verbs
lemma   ex 
monitor
   
achieve
   
target
   
select
   
enable
   
seem
   
pine
   
evaluate    
explore
   
believe
   

table     most frequent monosemous words in the background documents
sc
bg
total

nouns
      
       
       

verbs
      
     
      

n v
       
       
       

table     number of training examples for word senses
our approach applies the same semantic class architecture shown in the previous sections  but
using examples extracted from the background documents  in this case  the semantic class used to
extract the examples and generate the classifiers is blc       we select a simple feature set widely
used in many wsd systems  in particular  we use a window of five tokens around the target word
to extract word forms  lemmas  bigrams and trigrams of word forms and lemmas  trigrams of pos
tags  and also the most frequent blc   semantic class of the target word in the training corpus 
to analyze the contribution of the monosemous examples in the performance of the system three
experiments we have defined 
 blc  sc  only training examples extracted from semcor
 blc  bg  only monosemous examples extracted from the background data
 blc  scbg  training examples extracted from semcor and monosemous background data
the first run  blc  sc  aims to show the behavior of a supervised system trained on a general
corpus  and tested in a specific domain  the second one  blc  bg  analyzes the contribution of
the monosemous examples extracted from the background data  finally  the third run  blc  scbg  studies the robustness of the approach when combining the training examples from semcor
and the automatic ones obtained from the background documents 
table    summarizes ordered by recall the official results of the participants in the english
wsd domain specific task of semeval   in this table  type refers to the approach followed by
the corresponding system  weakly supervised  ws   supervised  s  or kb  knowledge based 
unsupervised   we only participate with the system using blc   as semantic class  the blc  
sc bg scbg runs   the wordbased classifiers  labeled sensebg  sense sc and sensescbg 
    in this case we use the set of blcs from wordnet     because also this version of wn was the one used in the
annotation 

   

fii zquierdo   s u arez   r igau

have been included after the evaluation campaign  finally  as we mentioned in the introduction 
we have also included the performance of itmakessense system  which is one of the best performing wsd systems  on this same task for comparison purposes  it is the row on the table called
itmakessense in italics  
rank
 
 
 
 
 
 
 
 
 
  
  
   
  
   
  

system id
cfilt 
cfilt 
iiith  d   ppr   
iiith  d   ppr   
blc  scbg
itmakessense
blc  sc
most frequent sense
cfilt 
treematch
treematch 
sensescbg
sensesc
   
blc  bg
   
random baseline
sensebg

type
ws
ws
ws
ws
s
s
s
kb
kb
kb
s
s
   
s
   
s

p
     
     
     
     
     
     
     
     
     
     
     
     
     
   
     
   
     
     

r
     
     
     
     
     
     
     
     
     
     
     
     
     
   
     
   
     
     

table     precision and recall of semeval  participants  itmakessense results are included for
comparison purpose only
in general  the results reported by semeval for this task were quite low  the best system only
achieved a precision of        and the most frequent baseline reached a precision of        this
fact shows that the domain adaptation of wsd systems is a very difficult task 
analyzing the results of our three runs at semeval  our worst result is obtained by the system
using only the monosemous background examples  blc  bg   this system ranks   rd   with a
precision and recall of              for nouns and       for verbs   the system using only semcor
 blc  sc  ranks  th with precision and recall of              for nouns and       for verbs  
this is also the performance of the first sense baseline  as expected  the best result of our three
runs is obtained when combining the examples from semcor and the background  blc  scbg  
this supervised system obtains the  th position with a precision and recall of              for
nouns        for verbs  which is slightly above the baseline  actually  this version of the system
obtains slightly better results than the best performing supervised system  itmakessense   also note
that we could include automatically monosemous examples from the background test thanks to the
class based nature of the wsd system 
moreover  our system is the only one completely supervised participating in the task  the organizers calculated the recall with a confidence interval of     using bootstrap re sampling procedure
 noreen         this method of estimation might be more strict than other pairwise methods  it
reveals that the differences between the four first systems and our system  blc  scbg  are not
    in the table it appears in the   th position due to we have included the wordbased classifier results 

   

fiw ord vs   c lass  based w ord s ense d isambiguation

statistically significant  as can be seen in figure    there is overlapping between the recall confidence interval of the four first systems and our system  ranking the  th   which proves that the
differences are not statistically significant    

figure    recall confidence intervals 
possibly  the reason of low performance of the bcl  bg system is the high correlation between the features of the target word and its semantic class  in this case  these features correspond
to the monosemous word while later are evaluated over polysemous words  with all kind of features  however  it also seems that class based systems are robust enough to incorporate large sets
of monosemous examples from a domain text  in fact  to our knowledge  this is the first time that
a supervised wsd algorithm has been successfully adapted to a specific domain  furthermore 
our system trained only on semcor also achieves a good performance  reaching the most frequent
baseline  showing the robustness of class based wsd approaches to domain variations 
comparing to wordbased classifiers  it seems that our blc   classes contribute in two main
aspects  first  using the same set of features  the classbased classifiers obtain better results than
wordbased ones  the classifiers built with blc   are more robust and domain adaptable than
wordbased approaches  second  the experiment that uses only examples extracted from background data considering word senses  sense bg  obtain an accuracy very close to zero  while the
same experiment but using blc   semantic classes  blc  bg  reaches an accuracy of       
this fact indicates that blcs are useful to extract good training examples from unlabeled data  as
mentioned previously  in order to obtain a better insight  after the evaluation campaign we performed
the same evaluation with our system using other semantic classes which represent different levels
of abstractions  blc    wordnet domains and supersenses  table    shows the precision  p 
and recall  r    of our evaluation considering different training datasets  semcor only  background
documents only and both semcor and background documents  sc  bg and sc bg respectively 
and different semantic classes 
as can be seen in table     blc   leads to a better performance when using the three different
corpora for training  bg  sc and scbg   when training only with monosemous examples extracted
from the background documents  blc   obtains the best result  which may indicate that its level
of abstraction is more adequate than any other  including wnd or ss  which are sets much smaller
and with a much lower polysemy  the same effect can be drawn from the results when training
with semcor and the monosemous examples from the background  scbg   the best results are
obtained with blc    and together with supersenses are the only two semantic classes that seem
    this figure has been taken directly from the overview paper of the task 
    these figures have been obtained using the official scorer script and the official gold key  without any modification 

   

fii zquierdo   s u arez   r igau

system id
blc  scbg
itmakessense
blc  sc
most frequent sense
wndsc
sensescbg
sensesc
ss scbg
blc  scbg
blc  sc
sssc
wnscbg
blc  bg
wndbg
ssbg
blc  bg
random baseline

type
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
 

p
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

r
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table     results of our experiments according to different semantic classes
to benefit from the background monosemous examples  these results seem to confirm the potential
capabilities of blc   to provide an adequate level of abstraction to perform class based wsd 
finally  we have proved that our system performs at the same level of one state of the art sys  
tem   the itmakessense system  zhong   ng         considering that the set of features of our
system is quite simple  and that we do not apply any machine learning optimization nor feature
engineering  our results show that the use of semantic classes provides a very robust behavior on
specific domains  reaching state of the art results 

    concluding remarks
word sense disambiguation is a difficult task as empirically has been demonstrated by all senseval semeval exercises  one reason of such difficulties could be the use of inappropriate sets of
word meanings  while wordnet is the de facto standard repository of meanings  several attempts
have been made grouping its senses in order to achieve higher levels of accuracy  moreover  this
approach tries to ease the hard task of creating large enough sets of annotated data per domain and
language to train supervised systems  a possible solution would be to use for manual annotation semantic class labels instead of fine grained word senses  schneider  mohit  oflazer    smith       
schneider  mohit  dyer  oflazer    smith        
several attempts have been made to obtain word sense groupings to alleviate the problem of the
too fine granularity of word senses  most widely using wordnet senses  in most cases the approach
consists in grouping different senses of the same word  resulting in a decrease of the polysemy 
while reducing its discriminative capacity  other works use predefined sets of semantic classes to
be integrated directly in a wsd system  mainly supersenses 
    this has been tested offline  as the itmakessense system did not participate in the task  we downloaded the last
version of the software from http   www comp nus edu sg nlp software html 

   

fiw ord vs   c lass  based w ord s ense d isambiguation

in this work we describe a simple method to automatically select basic level concepts from
wordnet  based on very simple structural properties of wordnet  our method automatically selects
different sets of blc representing different levels of abstraction 
the aim of this work is to explore on several allwords wsd tasks the performance of different
levels of abstraction provided by basic level concepts  wordnet domains  sumo and supersense
labels  furthermore  our study empirically demonstrates that 
a  these word sense groupings cluster senses into a coherent level of abstraction in order to
perform supervised classbased wsd while not harming its performance 
b  these semantic classes can be successfully used as semantic features to boost the performance
of these classifiers 
c  the classbased approach to wsd reduces dramatically the required amount of training examples to obtain competitive classifiers 
d  the classbased approach obtains competitive performances compared with word based systems 
e  the classbased approach outperforms wordbased systems when evaluated at class level 
f  the robustness of our class based wsd system when performing out of domain evaluation 
g  our system reaches results comparable to a state of the art system  itmakessense  when
tested on a specific domain 
in general  classbased disambiguation of nouns and verbs achieves better results than most of
the wordbased systems presented in both senseval  and senseval   we also showed that the classbased approach reduces considerably the required amount of training examples  in order to prove
that such type of disambiguation is possible and accurate we have ranked the class based systems
together with the senseval  and senseval  official results  in order to establish a fair comparison
we mapped when necessary word senses to semantic classes and viceversa 
some experiments have been designed to use our classbased classifiers to perform wordsense
disambiguation  it has been shown that a very simple approach of selecting the first sense in wordnet that corresponds to the class selected by the classifiers performs as well as the top systems at
senseval  and senseval  
additional experiments have been carried out to compare the wordbased systems to perform
classbased disambiguation  in this case we translated the official system outputs to its corresponding semantic classes 
different experiments have been performed using different levels of abstraction  ranging from
supersenses  a very small set  to sumo  which has over       labels linked to wordnet    senses  
wordnet domains  with     labels   or basic level concepts  with an arbitrary number of classes
depending on the abstraction level selected  
with some expected differences between senseval  and senseval  results  most of the class
based systems outperform the baselines both for nouns and verbs  specially for nouns  class based
systems outperforms most of the senseval  and senseval  systems  in general  the results obtained
by svm semblc   are not very different to the results of svm semblc    thus  we can select
   

fii zquierdo   s u arez   r igau

a medium level of abstraction  without having a significant decrease of the performance  considering the number of classes  blc classifiers obtain high performance rates while maintaining much
higher expressiveness than supersenses  however  using supersenses     classes  we can obtain
a very accurate semantic tagger with performances around      even better  we can use blc  
for tagging nouns      semantic classes and f  over      and supersenses for verbs     semantic
classes and f  around      
our systems at semeval  all words word sense disambiguation on a specific domain task
proved that simple features exploiting blc can perform as well as more sophisticated methods 
comparing with wordbased classifiers  we see that our blc   classes contribute in two main
aspects  the classbased classifiers obtain better results than wordbased ones and semantic classes
contribute effectively to those results  this fact indicates that  in particular  blc   are useful to
extract monosemous training examples from unlabeled domain data 
our next goal is to exploit the inconsistencies of the different labeling provided by the different
class based classifiers in order to obtain a more robust and accurate class based wsd system  the
main idea is to study why several classifiers  each one based on a different degree of abstraction  e g 
blc    blc    wordnet domains  etc   label a concrete context or example with incompatible
tags  in this manner  we would be able to predict when to apply the best classifier depending on the
context 

acknowledgements
this work has been partially supported by the newsreader project    ict               the spanish project skater    tin           c       

references
agirre  e     de lacalle  o  l          clustering wordnet word senses  in proceedings of
ranlp    borovets  bulgaria 
agirre  e     edmonds  p          word sense disambiguation  algorithms and applications 
springer 
agirre  e   lopez de lacalle  o   fellbaum  c   hsieh  s  k   tesconi  m   monachini  m   vossen 
p     segers  r          semeval      task     all words word sense disambiguation on a
specific domain  in proceedings of the  th international workshop on semantic evaluation 
pp        uppsala  sweden  association for computational linguistics 
bhagwani  s   satapathy  s     karnick  h          merging word senses  in proceedings of workshop on graph based methods for natural language processing  textgraphs     pp       
castillo  m   real  f     rigau  g          automatic assignment of domain labels to wordnet  in
proceeding of the  nd international wordnet conference  pp       
ciaramita  m     altun  y          broad coverage sense disambiguation and information extraction with a supersense sequence tagger  in proceedings of the conference on empirical methods in natural language processing  emnlp     pp          sydney  australia  acl 
    http   www newsreader project eu
    http   nlp lsi upc edu skater

   

fiw ord vs   c lass  based w ord s ense d isambiguation

ciaramita  m     johnson  m          supersense tagging of unknown nouns in wordnet 
in proceedings of the conference on empirical methods in natural language processing
 emnlp     pp          acl 
curran  j          supersense tagging of unknown nouns using semantic similarity  in proceedings
of the   rd annual meeting on association for computational linguistics  acl     pp    
    acl 
escudero  g   marquez  l     rigau   g          an empirical study of the domain dependence
of supervised word sense disambiguation systems  in proceedings of the joint sigdat
conference on empirical methods in natural language processing and very large corpora 
emnlp vlc  hong kong  china 
fellbaum  c   ed            wordnet  an electronic lexical database  the mit press 
gangemi  a   nuzzolese  a  g   presutti  v   draicchio  f   musetti  a     ciancarini  p         
automatic typing of dbpedia entities  in proceedings of the   th international conference on
the semantic web   volume part i  iswc    pp        berlin  heidelberg  springer verlag 
gonzalez  a   rigau  g     castillo  m          a graph based method to improve wordnet domains 
in computational linguistics and intelligent text processing  pp        springer 
hamp  b   feldweg  h   et al          germanet a lexical semantic net for german  in proceedings of
acl workshop automatic information extraction and building of lexical semantic resources
for nlp applications  pp       citeseer 
hearst  m     schutze  h          customizing a lexicon to better suit a computational task  in
proceedingns of the acl siglex workshop on lexical acquisition  stuttgart  germany 
hovy  e   marcus  m   palmer  m   ramshaw  l     weischedel  r          ontonotes  the   
in proceedings of the human language technology conference of the naacl  companion
volume  short papers  naacl short     pp        stroudsburg  pa  usa  association for
computational linguistics 
izquierdo  r   suarez  a     rigau  g          exploring the automatic selection of basic level concepts  in et al   g  a   ed    international conference recent advances in natural language
processing  pp          borovets  bulgaria 
izquierdo  r   suarez  a     rigau  g          an empirical study on class based word sense disambiguation  in proceedings of the   th conference of the european chapter of the association
for computational linguistics  eacl     pp          stroudsburg  pa  usa  association
for computational linguistics 
izquierdo  r   suarez  a     rigau  g          gplsi ixa  using semantic classes to acquire monosemous training examples from domain texts  in proceedings of the  th international workshop
on semantic evaluation  pp          association for computational linguistics 
joachims  t          text categorization with support vector machines  learning with many relevant
features  in nedellec  c     rouveirol  c   eds    proceedings of ecml       th european
conference on machine learning  no        pp          chemnitz  de  springer verlag 
heidelberg  de 
l  bentivogli  p  forner  b  m     pianta  e          revising wordnet domains hierarchy  semantics  coverage  and balancing  in coling      workshop on multilingual linguistic
resources  geneva  switzerland 
   

fii zquierdo   s u arez   r igau

magnini  b     cavaglia  g          integrating subject field codes into wordnet  in proceedings of
lrec  athens  greece 
marquez  l   escudero  g   martnez  d     rigau  g          supervised corpus based methods
for wsd  in e  agirre and p  edmonds  eds   word sense disambiguation  algorithms and
applications   vol     of text  speech and language technology  springer 
mccarthy  d   koeling  r   weeds  j     carroll  j          finding predominant word senses in
untagged text  in in   nd annual meeting of the association for computational linguistics 
barcelona  spain 
mihalcea  r          using wikipedia for automatic word sense disambiguation  in proceedings of
naacl hlt      
mihalcea  r   csomai  a     ciaramita  m          unt yahoo  supersenselearner  combining
senselearner with supersense and other coarse semantic features  in proceedings of the  th
international workshop on semantic evaluations  semeval     pp          stroudsburg 
pa  usa  association for computational linguistics 
mihalcea  r     moldovan  d          automatic generation of coarse grained wordnet  in proceding of the naacl workshop on wordnet and other lexical resources  applications  extensions and customizations  pittsburg  usa 
miller  g   leacock  c   tengi  r     bunker  r          a semantic concordance  in proceedings
of the arpa workshop on human language technology 
navigli  r          meaningful clustering of senses helps boost word sense disambiguation performance  in acl     proceedings of the   st international conference on computational
linguistics and the   th annual meeting of the association for computational linguistics 
pp          morristown  nj  usa  association for computational linguistics 
navigli  r          word sense disambiguation  a survey  acm computing surveys             
navigli  r   litkowski  k     hargraves  o          semeval      task     coarse grained english
all words task  in proceedings of the fourth international workshop on semantic evaluations  semeval        pp        prague  czech republic  association for computational
linguistics 
niles  i     pease  a          towards a standard upper ontology  in proceedings of the  nd
international conference on formal ontology in information systems  fois        pp    
    chris welty and barry smith  eds 
niles  i     pease  a          linking lexicons and ontologies  mapping wordnet to the suggested
upper merged ontology  in arabnia  h  r   ed    proc  of the ieee int  conf  on inf  and
knowledge engin   ike        vol     pp          csrea press 
noreen  e          computer intensive methods for testing hypotheses  an introduction  a wiley
interscience publication  wiley 
paa  g     reichartz  f       a   exploiting semantic constraints for estimating supersenses with
crfs   in sdm  pp          siam 
paa  g     reichartz  f       b   exploiting semantic constraints for estimating supersenses with
crfs   in sdm  pp          siam 
   

fiw ord vs   c lass  based w ord s ense d isambiguation

palmer  m   fellbaum  c   cotton  s   delfs  l     dang  h  t          english tasks  all words
and verb lexical sample  in proceedings of the senseval   workshop  in conjunction with
acl     eacl      toulouse  france 
peters  w   peters  i     vossen  p          automatic sense clustering in eurowordnet  in first international conference on language resources and evaluation  lrec     granada  spain 
picca  d   gliozzo  a  m     ciaramita  m          supersense tagger for italian   in lrec  citeseer 
pradhan  s   dligach  e  l  d     palmer  m          semeval      task     english lexical sample 
srl and all words  in semeval     proceedings of the  th international workshop on semantic
evaluations  pp        morristown  nj  usa  association for computational linguistics 
rosch  e          human categorisation  studies in cross cultural psychology  i         
schmid  h          probabilistic part of speech tagging using decision trees  in proceedings of
the international conference on new methods in language processing  pp       
schneider  n   mohit  b   dyer  c   oflazer  k     smith  n  a          supersense tagging for
arabic  the mt in the middle attack   in hlt naacl  pp          citeseer 
schneider  n   mohit  b   oflazer  k     smith  n  a          coarse lexical semantic annotation
with supersenses  an arabic case study  in proceedings of the   th annual meeting of the
association for computational linguistics  short papers volume    pp          association
for computational linguistics 
segond  f   schiller  a   greffenstette  g     chanod  j          an experiment in semantic tagging
using hidden markov model tagging  in acl workshop on automatic information extraction
and building of lexical semantic resources for nlp applications  pp        acl  new
brunswick  new jersey 
snow  r   s   p   d   j     a   n          learning to merge word senses  in proceedings of joint
conference on empirical methods in natural language processing and computational natural language learning  emnlp conll   pp           
snyder  b     palmer  m          the english all words task  in mihalcea  r     edmonds  p 
 eds    senseval    third international workshop on the evaluation of systems for the semantic analysis of text  pp        barcelona  spain  association for computational linguistics 
tsvetkov  y   schneider  n   hovy  d   bhatia  a   faruqui  m     dyer  c          augmenting
english adjective senses with supersenses  in proc  of lrec  pp           
villarejo  l   marquez  l     rigau  g          exploring the construction of semantic class classifiers for wsd  in proceedings of the   th annual meeting of sociedad espaola para el
procesamiento del lenguaje natural sepln    pp          granada  spain  issn          
vossen  p   ed            eurowordnet  a multilingual database with lexical semantic networks
  kluwer academic publishers  
wikipedia         wikipedia  the free encyclopedia  https   en wikipedia org    online 
accessed    august       
yarowsky  d          decision lists for lexical ambiguity resolution  application to accent restoration in spanish and french  in proceedings of the   nd annual meeting of the association for
computational linguistics  acl    
   

fii zquierdo   s u arez   r igau

zhong  z     ng  h  t          it makes sense  a wide coverage word sense disambiguation system
for free text  in proceedings of the acl      system demonstrations  acldemos     pp 
      stroudsburg  pa  usa  association for computational linguistics 

   

fi