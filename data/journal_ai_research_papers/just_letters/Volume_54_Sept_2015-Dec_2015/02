journal artificial intelligence research                 

submitted        published      

word vs  class based word sense disambiguation
ruben izquierdo

ruben   izquierdobevia   vu   nl

vu university amsterdam
amsterdam  netherlands

armando suarez

armando   dlsi   ua   es

university alicante
alicante  spain

german rigau

german   rigau   ehu   es

university basque country
san sebastian  spain

abstract
empirically demonstrated word sense disambiguation  wsd  tasks last senseval semeval exercises  assigning appropriate meaning words context resisted
attempts successfully addressed  many authors argue one possible reason could
use inappropriate sets word meanings  particular  wordnet used de facto
standard repository word meanings tasks  thus  instead using word
senses defined wordnet  approaches derived semantic classes representing groups
word senses  however  meanings represented wordnet used wsd
fine grained sense level coarse grained semantic class level  also called supersenses   suspect appropriate level abstraction could levels 
contributions paper manifold  first  propose simple method automatically
derive semantic classes intermediate levels abstraction covering nominal verbal wordnet meanings  second  empirically demonstrate automatically derived semantic classes
outperform classical approaches based word senses coarse grained sense groupings 
third  demonstrate supervised wsd system benefits using new semantic classes additional semantic features reducing amount training examples 
finally  demonstrate robustness supervised semantic class based wsd system
tested domain corpus 

   introduction
word sense disambiguation  wsd  intermediate natural language processing  nlp  task
consists assigning correct lexical interpretation ambiguous words depending surrounding context  agirre   edmonds        navigli         one successful approaches
last years supervised learning examples  machine learning classification
models induced semantically annotated corpora  marquez  escudero  martnez    rigau 
       quite often  machine learning systems obtained better results knowledge based
ones  shown experimental work international evaluation exercises senseval semeval    nevertheless  lately weakly supervised knowledgebased approaches reaching
performance close supervised techniques specific tasks  tasks 
   information competitions found http   www senseval org 
c
    
ai access foundation  rights reserved 

fii zquierdo   u arez   r igau

corpora usually manually annotated experts word senses taken particular lexical
semantic resource  commonly wordnet  fellbaum        
however  wordnet widely criticized sense repository often provides
finegrained sense distinctions higher level applications machine translation  mt 
question   answering  aq   fact  wsd low level semantic granularity resisted
attempts inferring robust broad coverage models  seems many wordsense distinctions
subtle captured automatic systems current small volumes wordsense
annotated examples  using wordnet sense repository  organizers english all words
task senseval   reported inter annotation agreement        snyder   palmer         interestingly  result difficult outperform state of the art sense based wsd systems 
moreover  supervised sensebased approaches biased towards frequent sense
predominant sense training data  therefore  performance supervised sensebased
systems strongly punished applied domain specific texts sense distribution differs considerably respect sense distribution training corpora  escudero  marquez 
  rigau         
paper try overcome problems facing task wsd semantic
class point view instead traditional word sense based approach  semantic class
seen abstract concept groups subconcepts word senses sharing semantic properties features  examples semantic classes vehicle  food animal  hypothesis
using appropriate set semantic classes instead word senses could help wsd several
aspects 
higher level abstraction could ease integration wsd systems higher
level nlp applications machine translation question   answering
grouping together semantically coherent sets training examples could increase
robustness supervised wsd systems
socalled bottleneck acquisition problem could alleviated
points explained along paper  following hypothesis propose
create classifiers based semantic classes instead word sense experts  one semantic classifier
trained semantic class final system assign proper semantic class
ambiguous word  instead sense traditional approaches   example  using
automatically derived semantic classes  that introduced later   three senses church
wordnet     subsumed semantic classes r eligious rganization  b uilding
r eligious c eremony  note semantic classes still discriminate among three
different senses word church  instance  assign semantic class b uilding
occurrence church context  still know refers second sense  additionally 
semantic class b uilding covers six times training examples
covered second sense church 
example text senseval  automatically annotated semantic classes seen
figure    shows automatic annotations classbased classifiers different semantic classes  blc stands basic level concepts   izquierdo  suarez    rigau         ss
   use following format throughout paper refer particular sense  wordnum
pos   pos
part of speech  n nouns  v verbs  adjectives r adverbs  num stands sense number 

  

fiw ord vs   c lass  based w ord ense isambiguation

supersenses  ciaramita   johnson         wnd wordnet domains  magnini   cavaglia 
      l  bentivogli   pianta        sumo suggested upper merged ontology  niles  
pease         incorrect assignments marked italics  correct tags included
brackets next automatic ones  obviously  semantic resources relate senses different
level abstraction using diverse semantic criteria properties could interest subsequent semantic processing  moreover  combination could improve overall results since
offer different semantic perspectives text 
id
 
 
 
 
 
 
 

word

ancient
stone
church
amid

fields

blc

ss

wnd

sumo

artifact n
building n

noun artifact
noun artifact

building
building

mineral
building

geographic area n
 physical object n  

noun location
 noun object 

factotum  geography 

landarea

 
  
  

 

sound

property n

noun attribute

factotum  acoustics 

radiatingsound
 soundattribute 

  
  


bells

device n

noun artifact

musicalinstrument

  
  
  
  
  

cascading


tower
calling

move v

verb motion

factotum  acoustics 
factotum

construction n
designate v
 request v  

noun artifact
factotum
verb stative
factotum
 verb communication 

building
communication
 requesting 

  
  


faithful

 sogroup n
cial group n  

noun group

person  religion 

group

  
  


evensong

time day n
 writing n  

noun communication

religion

timeinterval
 text 

motion

table    example automatic annotation text several semantic class labels
main goal research investigate performance alternative semantic classes
derived wordnet supervised wsd  first  propose system automatically extract sets
semantically coherent groupings nominal verbal senses wordnet  system
allows generate arbitrary sets semantic classes distinct levels abstraction  second 
analyze impact respect alternative semantic classes performing classbased
wsd  empirical results show automatically generated classes performs better
created manually  wndomains  sumo  supersenses  etc   capturing precise
information  third  demonstrate supervised wsd system benefits using
new semantic classes additional semantic features reducing amount training
  

fii zquierdo   u arez   r igau

examples  finally  show supervised class based system adapted particular
domain  traditional word sense based systems included comparison purposes 
summarizing  research empirically investigates 
performance alternative semantic groupings used supervised class based
wsd system
impact class based semantic features supervised wsd framework
required amount training examples needed class based wsd order obtain
competitive results
relative performance class based wsd systems respect wsd based word
experts
robustness class based wsd system specific domains
moreover  tested domain dataset  supervised class based wsd system obtains slightly better results state of the art word sense based wsd system  itmakessense
system presented zhong ng        
introduction  present work directly related research supervised
wsd based semantic classes  then  section   presents sense groupings semantic classes
used study  section   explains method automatically derive semantic classes
wordnet different levels abstraction  moreover analysis different semantic groupings
included  section    presents system developed perform supervised class based
wsd  performance system shown section    system tested several
wsd datasets provided international evaluations  comparison participants
competitions introduced sections      experiments system applied
specific domain analyzed section    finally  conclusions future work presented
section    

   related work
field wsd broad  large amount publications wsd
last    years  section revises relevant wsd approaches dealing appropriate
sets meanings word have 
research focused deriving different word sense groupings overcome
finegrained distinctions wordnet  hearst   schutze        peters  peters    vossen       
mihalcea   moldovan        agirre   de lacalle        navigli        snow  s   d     a         
is  provide methods grouping senses word  thus producing coarser word
sense groupings  example  word church three senses wordnet      sense
grouping presented snow et al         produces unique grouping  is  according
approach church monosemous 
ontonotes project  hovy  marcus  palmer  ramshaw    weischedel         different
meanings word considered kind tree  ranging coarse concepts root
finegrained meanings leaves  merging increased fine coarse grained
obtaining inter annotator agreement around      coarse grained repository used
  

fiw ord vs   c lass  based w ord ense isambiguation

wsd lexical sample task semeval       pradhan  dligach    palmer        
systems scored       fscore  note merging created word following
manual costly process 
similarly previous approach  another task organized within semeval     
consisted traditional wsd word task using another coarsegrained sense repository derived
wordnet  navigli  litkowski    hargraves         case wordnet synsets
automatically linked oxford dictionary english  ode  using graph algorithm 
meanings word linked ode entry merged coarse sense  systems
achieving top scores followed supervised approaches taking advantage different corpora
training  reaching top fscore        
previous cases aimed solving granularity problem word sense
definitions wordnet  however  approaches still word experts  one classifier trained
word   obviously  decreasing average polysemy word using coarsersenses
makes easier classification choice  result  performance systems increase
cost reducing discriminative power 
conversely  instead word experts  approach creates semantic class experts 
semantic classifiers exploit diverse information extracted meanings different
words belong class 
wikipedia  wikipedia        recently used overcome problems supervised learning methods  excessively finegrained definition meanings  lack annotated data
strong domain dependence existing annotated corpora  way  wikipedia provides
new source annotated data  large constantly expansion  mihalcea        gangemi 
nuzzolese  presutti  draicchio  musetti    ciancarini        
contrast  research focused using predefined sets sense groupings
learning classbased classifiers wsd  segond  schiller  greffenstette    chanod        ciaramita   johnson        villarejo  marquez    rigau        curran        ciaramita   altun 
      izquierdo  suarez    rigau         is  grouping senses different words
explicit comprehensive semantic class  work presented mihalcea  csomai 
ciaramita        makes use three different sets semantic classes  wordnet classes two
named entity annotated corpora  train sequential classifiers  classifiers trained using
basic features  collocations semantic features  reach performance around    
  th position semeval      allwords task 
semantic classes wordnet  also called supersenses  widely used different
works  instance  paa reichartz      a  apply conditional random fields model
sequential context words relation supersenses  extend model include
potential supersenses word training data  f  score       reported  both
nouns verbs  potential labels used  no training data all     worse
using training data right labels  although interesting  evaluate
system applying   fold cross validation semcor 

   semantic classes levels abstraction
meanings represented wordnet used wsd fine grained sense
level coarse grained semantic class level  also called supersenses   suspect
appropriate level abstraction could found levels  section propose
  

fii zquierdo   u arez   r igau

simple method automatically derive semantic classes intermediate levels abstraction covering nominal verbal wordnet meanings  first  introduce wordnet  semantic resource
sense repository used wsd systems  note semantic classes used
work linked wordnet 
wordnet  fellbaum        online lexical database english contains concepts
represented synsets  sets synonyms content words  nouns  verbs  adjectives
adverbs   one synset groups together several senses different words synonyms 
wordnet different types lexical semantic relations interlink different synsets  creating
way large structured lexical semantic network  important relation
encoded wordnet subclass relation  for nouns called hyponymy relation verbs
troponymy relation   table   shows basic figures different wordnet versions including
total number words  polysemous words  synsets  senses  all possible senses words 
average polysemy 
version
wn    
wn    
wn      
wn    
wn    
wn    

words
       
       
       
       
       
       

polysemous
      
      
      
      
      
      

synsets
      
       
       
       
       
       

senses
       
       
       
       
       
       

avg  polysemy
    
    
    
    
    
    

table    statistics wordnet versions 

    supersenses
supersenses name wordnet lexicographer files within framework wsd   
detail  wordnet synsets organized forty five supersenses  based syntactic categories
 nouns  verbs  adjectives adverbs  logical groupings person  phenomenon 
feeling   location   etc     basic categories nouns     verbs    adjectives
  adverbs  cases  different senses word grouped high level
supersense  reducing polysemy word  often case similar
senses word  classes adjectives adverbs  supersense taggers
usually developed nouns verbs   tsvetkov  schneider  hovy  bhatia  faruqui   
dyer        presents interesting study tagging adjectives supersenses acquired
germanet  hamp  feldweg  et al         
    wordnet domains
wordnet domains   wnd   magnini   cavaglia        l  bentivogli   pianta        hierarchy     domains used label semi automatically wordnet synsets 
set labels organized taxonomy following dewey decimal classification system   
   information supersenses found http   wordnet princeton edu wordnet 
man lexnames  wn html 
   http   wndomains itc it
   http   www oclc org dewey

  

fiw ord vs   c lass  based w ord ense isambiguation

building wnd  many labels assigned high levels wordnet hierarchy
automatically inherited across hypernym troponym hierarchy  thus  semi automatic
method  used develop resource free errors inconsistencies  castillo  real   
rigau        gonzalez  rigau    castillo        
information brought domain labels complementary already wordnet  wnd
present characteristics interesting wsd  first all  domain label may contain
senses different wordnet subhierarchies  derived different supersenses   instance 
domain religion contains senses priest  deriving noun   person church 
deriving noun   artifact  second  domain label may include synsets different
syntactic categories  instance  domain religion contains verb pray adjective
holy 
furthermore  single wnd label subsume different senses word  reducing
way polysemy  instance  first third senses church wordnet    
domain label religion 
    sumo concepts
sumo   niles   pease        created part ieee standard upper ontology working
group  goal develop standard upper ontology promote data interoperability  information search retrieval  automated inference  natural language processing  umo consists
set concepts  relations  axioms formalize upper ontology  experiments 
used complete wordnet     mapping       umo labels  niles   pease        
case  three noun senses church wordnet     classified r eligious rganization 
b uilding r eligious c eremony according sumo ontology 
    example semantic classes
example  table   presents three senses glosses word church wordnet     
sense
 

 
 

wordnet    
gloss
 
christian churchn group christians  group professing
christian doctrine belief  church biblical term assembly
church n church building n
public  especially christian  worship 
church empty
church service n church n
service conducted church  dont late
church
word senses
church n
christianity n

table    glosses examples senses churchn
table   show classes assigned sense according semantic resources introduced previously  instance  considering wordnet domains  observed senses
number    group christians     service conducted church  belong domain
   based several cycles manual checking automatically labeled data 
   http   www ontologyportal org

  

fii zquierdo   u arez   r igau

religion   contrary  supersenses sumo represent three senses church using
different semantic classes  note resulting assignment semantic classes identifies
word sense individually 

sense
 
 
 

supersense
noun   group
noun   artifact
noun   act

semantic class
wnd
sumo
r eligion r eligious rganization
b uildings
b uilding
r eligion
r eligious c eremony

table    semantic classes noun churchn
    levels abstraction
basic level concepts  rosch         hereinafter blc  result compromise two
conflicting principles characterization  general vs  specific  
represent many concepts possible
represent many features possible
result conflicting characterization  blc typically occur middle levels
semantic hierarchies 
notion base concepts  hereinafter bc  introduced eurowordnet  vossen        
bc supposed important concepts several language specific wordnets 
importance measured terms two main criteria 
high position semantic hierarchy
many relations concepts
eurowordnet set       concepts selected called common base concepts 
common bc concepts act bc least two languages  local wordnets english 
dutch spanish used select set common bc  later initiatives  similar sets
derived harmonize construction multilingual wordnets 
considering definitions  next section present method automatically generate
different sets basic level concepts wordnet different levels abstraction 

   automatic selection basic level concepts
several approaches developed trying alleviate fine granularity problem wordnet
senses obtaining word sense groupings  hearst   schutze        peters et al         mihalcea
  moldovan        agirre   de lacalle        navigli        snow et al         bhagwani  satapathy    karnick         cases approach consists grouping different senses
word  resulting decrease polysemy  obviously  polysemy reduced
wsd task classification problem becomes easier  system using coarse senses
obtain better results systems using word senses  works used predefined sets
semantic classes  mainly supersenses  segond et al         ciaramita   johnson        curran 
  

fiw ord vs   c lass  based w ord ense isambiguation

      villarejo et al         ciaramita   altun        picca  gliozzo    ciaramita        paa  
reichartz      b  tsvetkov et al         
section  describe simple method automatically create different sets basic level
concepts wordnet  method exploits nominal verbal structure wordnet 
basic idea synsets wordnet high number relations important  could
candidates blc  capture relevance synset wordnet considered two
options 
   all  total number relations encoded wordnet synset
   hypo  total number hyponymy relations synset
method follows bottomup approach exploiting hypernymy chains wordnet 
synset  process starts visiting synsets hyperonymy chain selecting  and
stopping walk synset  blc ancestor first local maximum considering
total number relations  either hypo     synsets one hyperonym 
method chooses one higher number relations continue process  process
ends preliminary set candidate synsets selected potential blc 
additionally  synset selected potential blc candidate must subsume  or represent 
least certain number descendant synsets  thus  minimum number synsets blc must
subsume another parameter algorithm  represented symbol   candidate
blcs reach threshold discarded  subsumed synsets reassigned
blc candidate appearing higher levels abstraction 
algorithm   presents pseudocode algorithm  parameters algorithm are 
wordnet resource  type relations considered  all hypo   minimum number
concepts must subsumed blc     algorithm two phases  first
one selects candidate blc  following bottomup approach  second phase discards
candidate blc satisfy threshold 
figure   shows schema illustrate selection process  node represents synset 
edges represent hyperonymy relations  for instance  hyperonym d 
hyperonym f   number synset indicates number hyponymy relations 
schema illustrates selection process blc candidates synset j using criterion hypo 
process starts checking hyperonym j  f  f two hyperonyms  b d 
next synset visited hyperonymy chain j since higher number hyponymy
relations  three   algorithm compares number relations hyperonym synset  d
three relations   previous synset  f two   number increasing
process continues  now  next node visit a  number relations two
number three  process stops synset selected blc candidate j d 
table   shows real example selection process noun church wordnet     
hyperonym chain number relations encoded wordnet  all criterion  shown
synset  local maximum chain marked bold 
   algorithm starts checking first hyperonym synset  synset itself 

  

fii zquierdo   u arez   r igau

figure    example blc selection
 rel 
  
  
  
  
  
 
 rel 
  
  
  
  
  
  
  
 rel 
  
  
 
  
 
 

synset
group   grouping  
social group  
organisation   organization  
establishment   institution  
faith   religion  
christianity   church   christian church  
synset
entity   something  
object   physical object  
artifact   artefact  
construction   structure  
building   edifice  
place worship       
church   church building  
synset
act   human action   human activity  
activity  
ceremony  
religious ceremony   religious ritual  
service   religious service   divine service  
church   church service  

table    blc selection noun church wordnet    

  

fiw ord vs   c lass  based w ord ense isambiguation

algorithm   blc extraction
require  wordnet  wn    typeofrelation  t   threshold   
blccandidates  
 synset w n  
cur   
 obtaining hypernym chains current synset cur 
h    hypernyms w n  cur 
new    synsetw ithm orerelations w n  h   
 iterating number relations increased 
n umof rels w n  t  cur    n umof rels w n  t  new 
cur    new
h    hypernyms w n  cur 
new    synsetw ithm orerelations w n  h   
end while store cur candidate blc 
blccandidates    blccandidates  cur 
end
 filtering blc candidates 
blcf inal  
 blc blccandidates 
  n umberof descendants w n  blc 
blcf inal    blcf inal  blc 
end
end
return blcf inal

figure    example blc selection sense   church
  

fii zquierdo   u arez   r igau

figure   see diagram showing partial view selection process candidate
blc sense number   noun church  synset dotted synset
processed  church n    synsets bold visited algorithm  one
gray  building n   one selected blc church n   process stops checking synset
structure n number relations     lower number relations
previous synset     relations edifice n   
obviously  combining different values threshold  for example              
criterion considered algorithm  all hypo   process ends different sets blc
extracted automatically wordnet version 
furthermore  instead number relations consider frequency synsets
corpus measure importance  synset frequency calculated sum
frequencies word senses contained synset  obtained semcor  miller 
leacock  tengi    bunker         wordnet 
sum up  algorithm two main parameters  parameter  representing minimum number synsets blc must represent  criterion used characterizing
relevance synsets  values parameters be 
parameter  integer value greater equal  
synset relevance parameter  value considered measure importance synset 
four possibilities 
number relations synset
all  relations encoded synset
hypo  hyponymy relations
frequency synset
freqwn  frequency obtained using wordnet
freqsc  frequency obtained using semcor
implementation algorithm different sets blc used paper several
wordnet versions freely available   
    analysis basic level concepts
selected wordnet     generate several sets blc  combining four types synset
relevance criteria values                values selected since
represent different levels abstraction  ranging      no filtering        each blc
must subsume least    synsets   table   shows  combinations synset relevance
parameters  number concepts set blc contains  average depth
wordnet hierarchy group  gray highlight two sets blc  blc    blc   
relations parameter  use experiments described paper 
expected  increasing threshold direct effect number blc
average depth wordnet hierarchy  particular  values decreased  indicating
threshold increased  concepts selected abstract general  instance 
   http   adimen si ehu es web blc

  

fiw ord vs   c lass  based w ord ense isambiguation

threshold

synset relevance

 

  

  

  


hypo
freqsc
freqwn

hypo
freqsc
freqwn

hypo
freqsc
freqwn

hypo
freqsc
freqwn

  blc
nouns verbs
           
           
            
            
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
  
   
  
   

depth
nouns verbs
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

table    automatic base level concepts wn   

using  all  nominal part wordnet  number concepts selected range      
filtering                     however  average  depth reduction acute since
varies            fact shows robustness method selecting synsets
intermediate level abstraction 
expected  verbal part wordnet behave differently  case  since verbal
hierarchies less deep  average depth synsets selected ranges          
using relations            using hypo relations 
general  using frequency criteria  observe similar behavior
using relation criteria  however  effect threshold dramatic  specially
nouns  again  expected  verbs behave differently nouns  number blc  for
semcor wordnet frequencies  reaches plateau around      fact  number
close verbal top beginners wordnet 
summing up  devised simple automatic procedure deriving different sets blc
representing different level abstraction whole set nominal verbal synsets wordnet  following section show explain supervised framework developed wsd
order exploit semantic classes described section previous one 

   supervised class based wsd
follow supervised machine learning approach develop set semantic class based wsd
classifiers  systems use implementation support vector machine algorithm train
classifiers  one per semantic class  semantic annotated corpora acquiring positive
negative examples class  classifiers built basis set features defined
representing examples  class based  training data must collected treated
pretty different way usual word based approach 
  

fii zquierdo   u arez   r igau

first  word based class based approaches selects training examples differently 
word based approach  instances word used training examples  figure
  shows distribution training examples used generate word sense classifier noun
house  following binary definition svm  one classifier generated word sense 
classifiers  occurrences word sense associated classifier
used positive examples  rest word sense occurrences used negative examples 
classifier house

classifier
sense  

    house n      

classifier
sense  

    house n     

    house n      

classifier
sense  

    house n      

    house n      

figure    distribution examples using word based approach
class based approach  use examples words belong
particular semantic class  figure   shows distribution examples class based approach 
case  one classifier created semantic class  occurrences words belonging
semantic class associated classifier used positive examples  rest
occurrences word senses associated different semantic class selected negative
examples 
obviously  class based approach number examples training increased  table
  shows example sense church n   following word based approach    examples
found semcor church n   conversely      positive training examples used
building classifier semantic class building  edifice 
think approach several advantages  first  semantic classes reduce average
polysemy degree words  some word senses might grouped together within semantic
class   moreover  acquisition bottleneck problem supervised machine learning algorithms
attenuated increase number training examples  however  mixing
one classifier examples different words  instance  building class
grouping together examples hotel  hospital church  could introduce noise
learning process grouping unrelated word senses 
    learning algorithm  svm
support vector machines  svm  proven robust competitive many nlp
tasks  wsd particular  marquez et al          experiments  used svm light
  

fiw ord vs   c lass  based w ord ense isambiguation

classifier animal

classifier building

   hospital  
 building 

   house  
 building 

   dog   
 animal 

   cat   
 animal 

   star  
 person 

figure    distribution examples using class based approach

church n

classifier
 word based approach 

building  edifice
 class approach 

examples
church n
church n
building n
hotel n
hospital n
barn n
       

  positive examples
  
  
  
  
  
  
      
    examples

table    number examples semcor  word vs  class based approaches

  

fii zquierdo   u arez   r igau

implementation  joachims         svm used induce hyperplane separates positive
negative examples maximum margin  means hyperplane located
intermediate position positive negative examples  trying keep maximum distance
closest positive example  closest negative example  cases  possible
get hyperplane divides space linearly  better allow errors obtain
efficient hyperplane  known soft margin svm  requires estimation parameter
 c   represents trade off allowed training errors margin  set
value       demonstrated good value svm wsd tasks 
classifying example  obtain value output function svm classifier
corresponding semantic class word example  system simply selects class
greatest value 
    corpora
three semantic annotated corpora used training testing  semcor training 
senseval   senseval   english all words tasks  testing 
semcor  miller et al         subset brown corpus plus novel red badge
courage  developed group created wordnet  contains    
texts around         running words          lemmatized sensetagged according princeton wordnet      sense annotations semcor
automatically ported wordnet versions    
senseval     english all words corpus  hereinafter se    palmer  fellbaum  cotton  delfs   
dang        consists       words text three wall street journal  wsj  articles representing different domains penn treebank ii  sense inventory used tagging
wordnet     
senseval     english all words corpus  hereinafter se    snyder   palmer         made
      words  extracted two wsj articles one excerpt brown corpus  sense
repository wordnet       used tag       words proper senses 
considered alternative evaluation datasets  instance  semeval      coarse
grained task corpus     however  dataset discarded corpus annotated
particular set word sense clusters  additionally  provide clear simple way
compare orthogonal sets clusterings  although recent senseval semeval
tasks wsd  think purpose evaluation  different level abstraction
wsd   senseval   senseval   still datasets best fit purposes  recent
semeval competitions designed address specific topics  multilinguality joint
wsd named entity recognition  however  make additional experiments
domain adaptation dataset provided semeval    task    all words word sense
disambiguation specific domain  wsd domain     agirre  lopez de lacalle  fellbaum 
hsieh  tesconi  monachini  vossen    segers        
   
   
   
   
   

http   web eecs umich edu mihalcea downloads html semcor
http   www sle sharp co uk senseval 
http   www senseval org senseval 
indeed participated task preliminary version system
http   semeval  fbk eu semeval  php location tasks t  

  

fiw ord vs   c lass  based w ord ense isambiguation

    feature types
following previous contributions supervised wsd  selected set basic features
represent training testing examples  include additional features based semantic
classes 
basic features
word forms lemmas window    words around target word 
pos  concatenation preceding following three five pos tags 
bigrams trigrams formed lemmas word forms window   words
around target word  use tokens regardless pos build bi trigrams 
replace target word character x features increase generalization 
semantic features
frequent semantic class target word  calculated semcor 
monosemous semantic class monosemous words window size five words
around target word 
basic features widely used literature  work presented yarowsky        
features pieces information occur context target word  local features
including bigrams trigrams  including target word  lemmas  word forms partof
speech labels  pos   addition  wordforms lemmas larger window around target
word considered features representing topic discourse 
set features extended semantic information  several types semantic classes
considered create features  particular  two different sets blc  blc  
blc        supersenses  wordnet domains  wnd  sumo 
order increase generalization capabilities class based classifiers filter
irrelevant features  measure relevance feature   f class c terms frequency
f  class c  feature f class  calculate frequency feature
within class  the number times occurs examples class   obtain
total frequency feature classes  get relative frequency dividing
values  classfreq   totalfreq  result lower certain threshold t  feature
removed feature list class c     way  make sure features selected
class frequently related class others  set threshold
      obtained empirically preliminary versions classifiers applying crossvalidation setting semcor 
    selected set since represent different levels abstraction  said section          refer
threshold minimum number synsets possible blc must subsume considered proper blc 
sets blc built using criterion 
    is  value feature  example feature type word form  feature type
houses 
    depending experiment  around     original features removed filter 

  

fii zquierdo   u arez   r igau

   semantic classbased wsd experiments
section present performance semantic class based wsd system
words wsd senseval    se   senseval   se   datasets  want analyze behavior
class based wsd system working different levels abstraction  said
before  level abstraction defined semantic class used build classifiers 
experiment defined two different parameters one involving particular set
semantic classes 
   target class  semantic classes used train classifiers  determining abstraction
level system   case  tested  word sense     blc    blc    wordnet domains  wnd   sumo supersenses  ss  
   semantic features class  semantic classes used building semantic features 
case  tested  blc    blc    wnd  sumo supersenses  ss  
target class type classes classifier assigns given ambiguous word 
instance  target class traditional word expert classifiers word senses  semantic
feature class one used building semantic features  independent target
class  instance  use wordnet domains extract monosemous words context
target word use wnd labels words semantic features building
classifier 
combining different semantic classes target features  generated set experiments described next sections  way  evaluate independently impact
selecting one semantic class another target class semantic feature class 
test
se 
se 

pos
n
v
n
v

sense
    
    
    
     

blc  
    
    
    
    

blc  
    
    
    
    

sumo
    
    
    
    

ss
    
    
    
    

wnd
    
    
    
    

table    average polysemy se  se 
table   shows average polysemy  ap  measured se  se  respect different semantic classes used evaluation target classes  expected  every corpus behaves
differently average polysemy verbs higher nouns  could assume
advance  relevant reductions polysemy degree obtained increasing level
abstraction  fact acute verbs  note large reduction polysemy verbs
using supersenses wnd  note priori se  seems difficult
disambiguate se   independently abstraction level 
    baselines
baselines evaluations define frequent classes  mfc  word calculated
semcor  ties classes specific word solved obtaining global frequency
    included word based evaluation comparison purposes since current system designed
class based evaluation 

   

fiw ord vs   c lass  based w ord ense isambiguation

semcor tied classes  selecting frequent class whole training
corpus  semcor occurrences particular word  that is  able calculate
frequent class word   compute global frequency possible
semantic classes  obtained wordnet  semcor  select frequent one  table
  shows baseline semantic class testing corpora 
class
sense
blc  
blc  
sumo
supersense
wnd

pos
n
v
n
v
n
v
n
v
n
v
n
v

se 
mfc ap
          
          
          
          
          
          
          
          
          
          
          
          

se 
mfc
ap
     
    
           
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    
     
    

table    frequent class baselines average polysemy  ap  se  se 
expected  performances mfc baselines high  particular  corresponding nouns  ranging           nominal baselines seem perform similarly
se  se   verbal baselines appear consistently much lower se  se  
se   verbal baselines range         se  verbal baselines range    
     results wnd high due low polysemy degree nouns verbs 
obviously  increasing level abstraction  from senses wnd  results increase 
    results basic system
section present performance supervised semantic classbased wsd system 
table    shows results system trained varying target classes using
basic feature set  values correspond f  measures  harmonic mean recall
precision  training systems semcor testing se  se  test sets  results
improve baselines shown italics  additionally  results showing statistically
significant positive difference compared corresponding baseline using mcnemars
test marked bold 
interestingly  basic system word sense level outperforms baselines se 
se  nouns verbs  addition  systems obtain cases significantly better
results verbs  interesting verbs word sense level baselines results
different  class level differences datasets much smaller 
expected  results systems increase augmenting level abstraction  from
senses wnd   cases  baseline results reached outperformed  even
relevant consider baseline results already quite high  however  high
level abstraction  supersenses wnd  basic systems seem unable outperform
baselines 
   

fii zquierdo   u arez   r igau

class
sense
blc  
blc  
sumo
supersense
wnd

pos
n
v
n
v
n
v
n
v
n
v
n
v

se 
     
     
     
     
     
     
     
     
     
     
     
     

se 
     
     
     
     
     
     
     
     
     
     
     
     

table     results basic system trained semcor basic set features evaluated
se  se 

general  results obtained blc   different blc    instance 
consider number classes within blc        classes   blc        classes  supersense     classes   blc classifiers obtain high performance rates maintaining much higher
expressive power supersenses  they able classify among much larger number classes  
fact  using supersenses     classes nouns verbs  obtain accurate semantic tagger performance close      even interesting  could use blc   tagging
nouns      semantic classes f  around      supersenses verbs     semantic classes
f  around      
    results exploiting semantic features
one main goals prove simple semantic features added training process
capable producing significant improvements basic systems  results experiments considering different types semantic features presented tables       
respectively nouns verbs 
tables  column labeled class refers called target class 
column labeled sf indicates type semantic features included represent examples
within machine learning approach 
again  values tables correspond f  measures  harmonic mean recall
precision  training systems semcor testing se  se  test sets  results
improving baselines appear italics  additionally  results showing statistically significant positive difference compared corresponding baseline using mcnemars test
marked bold 
regarding nouns  see table      different behavior observed se  se   adding
semantic features mainly improves results se   se  none systems present
significant improvement baselines  se  improvement obtained using
several types semantic features  in particular  using wnd features se    use
semantic class based features seems improve systems using target classes intermediate
levels abstraction  specially blc   blc     interestingly  se  blc   blc  
   

fiw ord vs   c lass  based w ord ense isambiguation

class

sf
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd

sense

blc  

blc  

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

class

sumo

ss

wnd

sf
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table     results nouns using extended system
seem provide improvements baselines target classes  for instance 
blc    blc   ss   although significant 
regarding verbs  see table      different behavior observed se  se  
case  observe almost opposite effect nouns  se  semantic
class features improve results obtained baselines  se  systems
present significant improvement baselines  se  improvement obtained
using several types semantic features  however  case obtain significantly better
results several semantic features se   use semantic class based features seems
benefit lower levels abstraction  specially word sense  blc    blc   sumo  
general  results show using semantic features addition basic features helps
reach better performance class based wsd systems  additionally  seems using
semantic features able obtain competitive classifiers sense level 
    learning curves
investigate behavior class based wsd system respect number training
examples  although experiments carried nouns verbs 
include results nouns since cases  trend similar 
experiment  semcor files randomly selected added training
corpus order generate subsets               etc  training corpus     then  train
    portion contains files previous portion  example  files     portion
contained     portion 

   

fii zquierdo   u arez   r igau

class

sense

blc  

blc  

sf
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

class

sumo

ss

wnd

sf
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd
baseline
basicfeat
blc  
blc  
sumo
ss
wnd

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

se 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table     results verbs using extended system
system training portions test system se  se   finally 
compare resulting system baseline computed training portion 
figures     present learning curves se  se   respectively  case 
selected blc   class based wsd system using wordnet domains semantic features    
surprisingly  se  system improves f  measure around    increasing
training corpus          semcor  se   system improves f 
measure around    increasing training corpus          semcor  is 
knowledge required class based wsd system seems already present
small part semcor 
figures     present learning curves se  se   respectively  class based
wsd system based supersenses using semantic features built wordnet domains 
se  system improves f  measure around    increasing training corpus
         semcor  se   system improves f  measure around   
increasing training corpus          semcor  is     
whole corpus  class based wsd system reaches f  close performance using corpus 
se  ans se   using blc    figures      supersenses  figures     
semantic classes wsd  behavior system similar mfc baseline 
interesting since mfc obtains high results due way defined  mfc
total corpus assigned occurrences word training corpus  without
definition  would large number words test set occurrences using
    shown previous experiments  combination obtains good performance 

   

fiw ord vs   c lass  based w ord ense isambiguation

  

system sv 
mfc sv 

  

  

  

  
f 
  

  

  

  

  
 

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

      

  corpus

figure    learning curve blc   classifier se 

  

system sv 
mfc sv 

  

  

  

f 

  

  

  

  

  
 

  

  

  

  

  

  

  

  

     
  corpus

  

  

  

  

  

  

  

figure    learning curve blc   classifier se 

   

      

fii zquierdo   u arez   r igau

  

system sv 
mfc sv 

  

  

  

f 

  

  

  

  

  
 

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

      

  corpus

figure    learning curve supersense classifier se 

  

system sv 
mfc sv 

  

  

f 

  

  

  

  
 

  

  

  

  

  

  

  

  

     
  corpus

  

  

  

  

  

  

  

      

figure    learning curve supersense classifier se 

   

fiw ord vs   c lass  based w ord ense isambiguation

small training portions  cases  recall baselines  and turn f   would much
lower 
evaluation seems indicate class based approach wsd reduces considerably
required amount training examples 

   comparison senseval systems  sense level
main goal experiments included section verify whether abstraction level
class based systems maintains discriminative power evaluated sense level  additionally  compare results results top participant systems se  se 
provided best senselevel outputs  thus  class based systems adapted following simple protocol  output based semantic classes converted sense identifiers 
instead semantic class produced systems particular instance  select first
sense word according wordnet sense ranking belonging predicted semantic class 
so  first obtain semantic class means classifiers  obtain restricted set
senses word match semantic class obtained  choose frequent
sense restricted subset 
results first experiment se  data shown table     systems
prefix svm  suffix denotes type semantic class used generate classifier    
cases experiments  wnd selected target semantic class generate
semantic features  two baselines marked italics included  first sense wordnet  base wordnet  frequent sense semcor  base semcor   fact  developers
wordnet ranked word senses using semcor sense annotated corpora  thus 
frequencies ranks appearing semcor wordnet similar  equal 
include results system working word level  svm sense  
cases  nouns verbs  systems outperform frequent baselines 
frequent sense word  according wordnet sense ranking competitive
wsd tasks  extremely hard improve upon even slightly  mccarthy  koeling  weeds 
  carroll         expected  behavior different semantic features produces slightly
different results  however  independently semantic features used  se  sense level 
class based systems rank third position 
table    shows experiment using se  dataset  case  class based systems
clearly outperform baselines  achieving best results nouns second place verbs 
interestingly  nouns  best system se  achieve semcor baseline  recall
se  seems difficult se  
worth mention class based systems use features nouns
verbs  instance  take profit complex feature sets encoding syntactic information
seems important verbs 
experiments show class based classifiers seem quite competitive evaluated word sense level  perform frequent sense according wordnet
semcor  achieve higher position nouns second verbs se   third
position nouns verbs se   obviously  indicates class based wsd maintains
high discriminative power word sense level 
    instance  svm blc   stands experiment creates classifier considering blc   semantic classes 

   

fii zquierdo   u arez   r igau

class sense se 
nouns
verbs
system
f 
system
smuam
      smuaw
ave antwerp
      ave antwerp
svm semblc         svm semsumo
svm semblc         svm sense
svm semsumo       svm semwnd
svm semwnd
      svm semblc  
svm sense
      svm semss
svm semss
      svm semblc  
base wordnet
      lia sinequa
base semcor
      base semcor
lia sinequa
      base wordnet

f 
     
     
     
     
     
     
     
     
     
     
     

table     class sense results se   class word sense transformation 

class sense se 
nouns
system
svm semwnd
svm semblc  
svm semsumo
svm sense
svm semblc  
svm semss
base semcor
gambl aw
base wordnet
kuaw
untaw
meaning allwords
lccaw

f 
     
     
     
     
     
     
     
     
     
     
     
     
     

verbs
system
gambl aw
svm semsumo
svm semwnd
svm semss
svm sense
svm semblc  
svm semblc  
untaw
meaning allwords
kuaw
r d 
base semcor
base wordnet

f 
     
     
     
     
     
     
     
     
     
     
     
     
     

table     class sense results se   class word sense transformation 

   

fiw ord vs   c lass  based w ord ense isambiguation

   comparison senseval systems  class level
experiments presented section explore performance word based classifiers
participating se  se  evaluated class level  perform kind evaluation 
word sense output participant systems mapped corresponding semantic
classes  class based systems modified  obviously  expect different performances
systems depending semantic class level  considering results presented tables
       order perform comparison  selected experiments use wnd
build semantic features     thus  system results using different target semantic classes
represented svm semwnd 
table    presents ordered f  measure results best performing systems se  data
evaluated different levels abstraction  previously  italics include
frequent senses according wordnet base wordnet semcor base semcor 
se   independently abstraction level pos  system  svm semwnd  scores
first positions ranking  one case system reaches best position  twice
second one  baselines outperformed experiments  except nouns using wnd 
basesemcor high 
table    presents ordered f  measure results best performing systems se  data
evaluated different levels abstraction  italics include frequent senses
according wordnet base wordnet semcor base semcor  systems represented
svm semwnd 
se   see system performs better baselines cases  except
semcorbased baseline nouns  obtains high result  particular  system
obtains good results verbs  reaching first second best positions cases 
outperforming baselines cases 
sum up  classbased approach outperforms senseval participants  both se 
se    sense level semantic class level  suggests good performance
semantic classifiers due polysemy reduction  actually  confirms
classbased semantic classifiers learning semantic class training examples different
abstraction levels 

   domain evaluation
section describe system semeval   allwords word sense disambiguation
specific domain task  izquierdo  suarez    rigau         aim evaluation
show robust semantic class approach tested specific domain  different
domain training material 
traditionally  senseval competitions focused general domain texts  thus  domain
specific texts present fresh challenges wsd  example  specific domains reduce possible meaning word given context  moreover  distribution word senses data
examples changes compared general domains  problems affect supervised
knowledgebased systems  fact  supervised word based wsd systems sensitive
corpora used training testing system  escudero et al         
    remind semantic features frequent class target word  semantic class monosemous words context around target word 

   

fii zquierdo   u arez   r igau

nouns

verbs
f 
system
sense blc  
smuaw
      smuaw
svm semwnd       svm semwnd
ave antwerp
      lia sinequa
base semcor
      ave antwerp
base wordnet
      base semcor
lia sinequa
      base wordnet
sense blc  
smuaw
      smuaw
svm semwnd       svm semwnd
ave antwerp
      lia sinequa
base semcor
      ave antwerp
base wordnet
      base semcor
lia sinequa
      base wordnet
sense sumo
smuaw
      smuaw
svm semwnd       lia sinequa
base semcor
      ave antwerp
ave antwerp
      svm semwnd
lia sinequa
      base semcor
base wordnet
      base wordnet
sense supersense
svm semwnd       smuaw
smuaw
      lia sinequa
ave antwerp
      svm semwnd
base semcor
      ave antwerp
lia sinequa
      base wordnet
base wordnet
     
base semcor
sense wnd
smuaw
      smuaw
base semcor
      svm semwnd
svm semwnd       base semcor
ave antwerp
      lia sinequa
base wordnet
      base wordnet
lia sinequa
      ave antwerp
system

f 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table     results sense blc    blc    sumo  supersense wnd semantic classes
se 

   

fiw ord vs   c lass  based w ord ense isambiguation

nouns

verbs
f 
system
sense blc  
base semcor
      gambl aw
gambl aw
      svm semwnd
kuaw
      kuaw
lccaw
      r d 
untaw
      untaw
svm semwnd
      meaning allwords
base wordnet
     
base semcor
meaning allwords       base wordnet
sense blc  
base semcor
      gambl aw
gambl aw
      svm semwnd
kuaw
      kuaw
svm semwnd
      r d 
lccaw
      untaw
untaw
      meaning allwords
base wordnet
      base semcor
r d 
      base wordnet
sense sumo
base semcor
      gambl aw
kuaw
      svm semwnd
lccaw
      untaw
svm semwnd
      kuaw
untaw
      meaning allwords
gambl aw
      upv eaw 
base wordnet
     
base semcor
meaning allwords       base wordnet
sense supersense
base semcor
      svm semwnd
kuaw
      gambl aw
svm semwnd
      base semcor
untaw
      base wordnet
gambl aw
      meaning allwords
upv eaw 
      meaning simple
upv eaw
      kuaw
base wordnet
      upv eaw 
sense wnd
base semcor
      svm semwnd
svm semwnd
      base semcor
untaw
      untaw
kuaw
      gambl aw
gambl aw
      base wordnet
base wordnet
     
r d 
lccaw
      meaning simple
meaning allwords       kuaw
system

f 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table     results sense blc    blc    sumo  supersense wnd semantic classes
se 

   

fii zquierdo   u arez   r igau

therefore  main challenge develop specific domain wsd systems adapt
general system particular domain  following research line  task proposed within
semeval  competition  allwords word sense disambiguation specific domain  agirre
et al          restricted domain selected task environmental domain  test
corpora consist three texts compiled european center nature conservation    ecnc 
world wildlife forum    wwf   task proposed several languages  chinese  dutch 
english italian  although participation limited english  detail 
total       noun tokens     verb tokens tagged  moreover  set background
documents related environmental domain provided  texts sense tagged 
plain text  provided ecnc wwf  could used
systems help adaptation specific domain  english  total    
background documents  containing           words 
apply kind specific domain adaptation technique supervised classbased
system  order adapt supervised system environmental domain increase automatically training data new training examples domain  acquire examples 
use     background documents environmental domain provided organizers 
use treetagger  schmid        preprocess documents  performing postagging lemmatization  since background documents semantically annotated  supervised system
needs labeled data  selected monosemous instances occurring documents
according blc   semantic classes     note approach exploited classbased wsd systems  way  obtained automatically large set examples annotated
blc    semantic class selected provided good results previous
experiments  order analyze approach system would work level
abstraction  performed evaluation posteriori using blc    wordnet domains
supersenses besides blc    official participation semeval    nevertheless 
section focused blc   
regarding blc    table    presents total number training examples extracted semcor  sc  background documents  bg   expected  method large number
monosemous examples obtained nouns verbs  although  verbs much less productive nouns  however  background examples correspond reduced set      
monosemous words 
sc
bg
total

nouns
      
       
       

verbs
      
      
      

n v
       
       
       

table     number training examples blc  
table    lists ten frequent monosemous nouns verbs occurring background
documents  remember examples monosemous according blc   semantic
classes 
    http   www ecnc org
    http   wwf org
    blc    see section    stands basic level concepts obtained relations criterion minimum threshold
subconcepts subsumed equal    

   

fiw ord vs   c lass  based w ord ense isambiguation

 
 
 
 
 
 
 
 
 
  

nouns
lemma
biodiversity
habitat
specie
climate
european
ecosystem
river
grassland
datum
directive

  ex 
     
     
     
     
     
     
     
     
     
     

verbs
lemma   ex 
monitor
   
achieve
   
target
   
select
   
enable
   
seem
   
pine
   
evaluate    
explore
   
believe
   

table     frequent monosemous words background documents
sc
bg
total

nouns
      
       
       

verbs
      
     
      

n v
       
       
       

table     number training examples word senses
approach applies semantic class architecture shown previous sections 
using examples extracted background documents  case  semantic class used
extract examples generate classifiers blc       select simple feature set widely
used many wsd systems  particular  use window five tokens around target word
extract word forms  lemmas  bigrams trigrams word forms lemmas  trigrams pos
tags  frequent blc   semantic class target word training corpus 
analyze contribution monosemous examples performance system three
experiments defined 
blc  sc  training examples extracted semcor
blc  bg  monosemous examples extracted background data
blc  scbg  training examples extracted semcor monosemous background data
first run  blc  sc  aims show behavior supervised system trained general
corpus  tested specific domain  second one  blc  bg  analyzes contribution
monosemous examples extracted background data  finally  third run  blc  scbg  studies robustness approach combining training examples semcor
automatic ones obtained background documents 
table    summarizes ordered recall official results participants english
wsd domain specific task semeval   table  type refers approach followed
corresponding system  weakly supervised  ws   supervised  s  kb  knowledge based 
unsupervised   participate system using blc   semantic class  the blc  
sc bg scbg runs   wordbased classifiers  labeled sensebg  sense sc sensescbg 
    case use set blcs wordnet     version wn one used
annotation 

   

fii zquierdo   u arez   r igau

included evaluation campaign  finally  mentioned introduction 
included performance itmakessense system  one best performing wsd systems  task comparison purposes  it row table called
itmakessense italics  
rank
 
 
 
 
 
 
 
 
 
  
  
   
  
   
  

system id
cfilt 
cfilt 
iiith  d   ppr   
iiith  d   ppr   
blc  scbg
itmakessense
blc  sc
frequent sense
cfilt 
treematch
treematch 
sensescbg
sensesc
   
blc  bg
   
random baseline
sensebg

type
ws
ws
ws
ws



kb
kb
kb


   

   


p
     
     
     
     
     
     
     
     
     
     
     
     
     
   
     
   
     
     

r
     
     
     
     
     
     
     
     
     
     
     
     
     
   
     
   
     
     

table     precision recall semeval  participants  itmakessense results included
comparison purpose
general  results reported semeval task quite low  best system
achieved precision        frequent baseline reached precision       
fact shows domain adaptation wsd systems difficult task 
analyzing results three runs semeval  worst result obtained system
using monosemous background examples  blc  bg   system ranks   rd  
precision recall              nouns       verbs   system using semcor
 blc  sc  ranks  th precision recall              nouns       verbs  
performance first sense baseline  expected  best result three
runs obtained combining examples semcor background  blc  scbg  
supervised system obtains  th position precision recall             
nouns        verbs  slightly baseline  actually  version system
obtains slightly better results best performing supervised system  itmakessense   note
could include automatically monosemous examples background test thanks
class based nature wsd system 
moreover  system one completely supervised participating task  organizers calculated recall confidence interval     using bootstrap re sampling procedure
 noreen         method estimation might strict pairwise methods 
reveals differences four first systems system  blc  scbg 
    table appears   th position due included wordbased classifier results 

   

fiw ord vs   c lass  based w ord ense isambiguation

statistically significant  seen figure    overlapping recall confidence interval four first systems system  ranking  th   proves
differences statistically significant    

figure    recall confidence intervals 
possibly  reason low performance bcl  bg system high correlation features target word semantic class  case  features correspond
monosemous word later evaluated polysemous words  kind features  however  seems class based systems robust enough incorporate large sets
monosemous examples domain text  fact  knowledge  first time
supervised wsd algorithm successfully adapted specific domain  furthermore 
system trained semcor achieves good performance  reaching frequent
baseline  showing robustness class based wsd approaches domain variations 
comparing wordbased classifiers  seems blc   classes contribute two main
aspects  first  using set features  classbased classifiers obtain better results
wordbased ones  classifiers built blc   robust domain adaptable
wordbased approaches  second  experiment uses examples extracted background data considering word senses  sense bg  obtain accuracy close zero 
experiment using blc   semantic classes  blc  bg  reaches accuracy       
fact indicates blcs useful extract good training examples unlabeled data 
mentioned previously  order obtain better insight  evaluation campaign performed
evaluation system using semantic classes represent different levels
abstractions  blc    wordnet domains supersenses  table    shows precision  p 
recall  r    evaluation considering different training datasets  semcor only  background
documents semcor background documents  sc  bg sc bg respectively 
different semantic classes 
seen table     blc   leads better performance using three different
corpora training  bg  sc scbg   training monosemous examples extracted
background documents  blc   obtains best result  may indicate level
abstraction adequate other  including wnd ss  sets much smaller
much lower polysemy  effect drawn results training
semcor monosemous examples background  scbg   best results
obtained blc    together supersenses two semantic classes seem
    figure taken directly overview paper task 
    figures obtained using official scorer script official gold key  without modification 

   

fii zquierdo   u arez   r igau

system id
blc  scbg
itmakessense
blc  sc
frequent sense
wndsc
sensescbg
sensesc
ss scbg
blc  scbg
blc  sc
sssc
wnscbg
blc  bg
wndbg
ssbg
blc  bg
random baseline

type
















 

p
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

r
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table     results experiments according different semantic classes
benefit background monosemous examples  results seem confirm potential
capabilities blc   provide adequate level abstraction perform class based wsd 
finally  proved system performs level one state of the art sys  
tem   itmakessense system  zhong   ng         considering set features
system quite simple  apply machine learning optimization feature
engineering  results show use semantic classes provides robust behavior
specific domains  reaching state of the art results 

    concluding remarks
word sense disambiguation difficult task empirically demonstrated senseval semeval exercises  one reason difficulties could use inappropriate sets
word meanings  wordnet de facto standard repository meanings  several attempts
made grouping senses order achieve higher levels accuracy  moreover 
approach tries ease hard task creating large enough sets annotated data per domain
language train supervised systems  possible solution would use manual annotation semantic class labels instead fine grained word senses  schneider  mohit  oflazer    smith       
schneider  mohit  dyer  oflazer    smith        
several attempts made obtain word sense groupings alleviate problem
fine granularity word senses  widely using wordnet senses  cases approach
consists grouping different senses word  resulting decrease polysemy 
reducing discriminative capacity  works use predefined sets semantic classes
integrated directly wsd system  mainly supersenses 
    tested offline  itmakessense system participate task  downloaded last
version software http   www comp nus edu sg nlp software html 

   

fiw ord vs   c lass  based w ord ense isambiguation

work describe simple method automatically select basic level concepts
wordnet  based simple structural properties wordnet  method automatically selects
different sets blc representing different levels abstraction 
aim work explore several allwords wsd tasks performance different
levels abstraction provided basic level concepts  wordnet domains  sumo supersense
labels  furthermore  study empirically demonstrates that 
a  word sense groupings cluster senses coherent level abstraction order
perform supervised classbased wsd harming performance 
b  semantic classes successfully used semantic features boost performance
classifiers 
c  classbased approach wsd reduces dramatically required amount training examples obtain competitive classifiers 
d  classbased approach obtains competitive performances compared word based systems 
e  classbased approach outperforms wordbased systems evaluated class level 
f  robustness class based wsd system performing domain evaluation 
g  system reaches results comparable state of the art system  itmakessense 
tested specific domain 
general  classbased disambiguation nouns verbs achieves better results
wordbased systems presented senseval  senseval   showed classbased approach reduces considerably required amount training examples  order prove
type disambiguation possible accurate ranked class based systems
together senseval  senseval  official results  order establish fair comparison
mapped necessary word senses semantic classes viceversa 
experiments designed use classbased classifiers perform wordsense
disambiguation  shown simple approach selecting first sense wordnet corresponds class selected classifiers performs well top systems
senseval  senseval  
additional experiments carried compare wordbased systems perform
classbased disambiguation  case translated official system outputs corresponding semantic classes 
different experiments performed using different levels abstraction  ranging
supersenses  a small set  sumo  which       labels linked wordnet    senses  
wordnet domains  with     labels   basic level concepts  with arbitrary number classes
depending abstraction level selected  
expected differences senseval  senseval  results  class
based systems outperform baselines nouns verbs  specially nouns  class based
systems outperforms senseval  senseval  systems  general  results obtained
svm semblc   different results svm semblc    thus  select
   

fii zquierdo   u arez   r igau

medium level abstraction  without significant decrease performance  considering number classes  blc classifiers obtain high performance rates maintaining much
higher expressiveness supersenses  however  using supersenses     classes  obtain
accurate semantic tagger performances around      even better  use blc  
tagging nouns      semantic classes f       supersenses verbs     semantic
classes f  around      
systems semeval  all words word sense disambiguation specific domain task
proved simple features exploiting blc perform well sophisticated methods 
comparing wordbased classifiers  see blc   classes contribute two main
aspects  classbased classifiers obtain better results wordbased ones semantic classes
contribute effectively results  fact indicates that  particular  blc   useful
extract monosemous training examples unlabeled domain data 
next goal exploit inconsistencies different labeling provided different
class based classifiers order obtain robust accurate class based wsd system 
main idea study several classifiers  one based different degree abstraction  e g 
blc    blc    wordnet domains  etc   label concrete context example incompatible
tags  manner  would able predict apply best classifier depending
context 

acknowledgements
work partially supported newsreader project    ict               spanish project skater    tin           c       

references
agirre  e     de lacalle  o  l          clustering wordnet word senses  proceedings
ranlp    borovets  bulgaria 
agirre  e     edmonds  p          word sense disambiguation  algorithms applications 
springer 
agirre  e   lopez de lacalle  o   fellbaum  c   hsieh  s  k   tesconi  m   monachini  m   vossen 
p     segers  r          semeval      task     all words word sense disambiguation
specific domain  proceedings  th international workshop semantic evaluation 
pp        uppsala  sweden  association computational linguistics 
bhagwani  s   satapathy  s     karnick  h          merging word senses  proceedings workshop graph based methods natural language processing  textgraphs     pp       
castillo  m   real  f     rigau  g          automatic assignment domain labels wordnet 
proceeding  nd international wordnet conference  pp       
ciaramita  m     altun  y          broad coverage sense disambiguation information extraction supersense sequence tagger  proceedings conference empirical methods natural language processing  emnlp     pp          sydney  australia  acl 
    http   www newsreader project eu
    http   nlp lsi upc edu skater

   

fiw ord vs   c lass  based w ord ense isambiguation

ciaramita  m     johnson  m          supersense tagging unknown nouns wordnet 
proceedings conference empirical methods natural language processing
 emnlp     pp          acl 
curran  j          supersense tagging unknown nouns using semantic similarity  proceedings
  rd annual meeting association computational linguistics  acl     pp    
    acl 
escudero  g   marquez  l     rigau   g          empirical study domain dependence
supervised word sense disambiguation systems  proceedings joint sigdat
conference empirical methods natural language processing large corpora 
emnlp vlc  hong kong  china 
fellbaum  c   ed            wordnet  electronic lexical database  mit press 
gangemi  a   nuzzolese  a  g   presutti  v   draicchio  f   musetti  a     ciancarini  p         
automatic typing dbpedia entities  proceedings   th international conference
semantic web   volume part i  iswc    pp        berlin  heidelberg  springer verlag 
gonzalez  a   rigau  g     castillo  m          graph based method improve wordnet domains 
computational linguistics intelligent text processing  pp        springer 
hamp  b   feldweg  h   et al          germanet a lexical semantic net german  proceedings
acl workshop automatic information extraction building lexical semantic resources
nlp applications  pp       citeseer 
hearst  m     schutze  h          customizing lexicon better suit computational task 
proceedingns acl siglex workshop lexical acquisition  stuttgart  germany 
hovy  e   marcus  m   palmer  m   ramshaw  l     weischedel  r          ontonotes    
proceedings human language technology conference naacl  companion
volume  short papers  naacl short     pp        stroudsburg  pa  usa  association
computational linguistics 
izquierdo  r   suarez  a     rigau  g          exploring automatic selection basic level concepts  et al   g  a   ed    international conference recent advances natural language
processing  pp          borovets  bulgaria 
izquierdo  r   suarez  a     rigau  g          empirical study class based word sense disambiguation  proceedings   th conference european chapter association
computational linguistics  eacl     pp          stroudsburg  pa  usa  association
computational linguistics 
izquierdo  r   suarez  a     rigau  g          gplsi ixa  using semantic classes acquire monosemous training examples domain texts  proceedings  th international workshop
semantic evaluation  pp          association computational linguistics 
joachims  t          text categorization support vector machines  learning many relevant
features  nedellec  c     rouveirol  c   eds    proceedings ecml       th european
conference machine learning  no        pp          chemnitz  de  springer verlag 
heidelberg  de 
l  bentivogli  p  forner  b  m     pianta  e          revising wordnet domains hierarchy  semantics  coverage  balancing  coling      workshop multilingual linguistic
resources  geneva  switzerland 
   

fii zquierdo   u arez   r igau

magnini  b     cavaglia  g          integrating subject field codes wordnet  proceedings
lrec  athens  greece 
marquez  l   escudero  g   martnez  d     rigau  g          supervised corpus based methods
wsd  e  agirre p  edmonds  eds   word sense disambiguation  algorithms
applications   vol     text  speech language technology  springer 
mccarthy  d   koeling  r   weeds  j     carroll  j          finding predominant word senses
untagged text    nd annual meeting association computational linguistics 
barcelona  spain 
mihalcea  r          using wikipedia automatic word sense disambiguation  proceedings
naacl hlt      
mihalcea  r   csomai  a     ciaramita  m          unt yahoo  supersenselearner  combining
senselearner supersense coarse semantic features  proceedings  th
international workshop semantic evaluations  semeval     pp          stroudsburg 
pa  usa  association computational linguistics 
mihalcea  r     moldovan  d          automatic generation coarse grained wordnet  proceding naacl workshop wordnet lexical resources  applications  extensions customizations  pittsburg  usa 
miller  g   leacock  c   tengi  r     bunker  r          semantic concordance  proceedings
arpa workshop human language technology 
navigli  r          meaningful clustering senses helps boost word sense disambiguation performance  acl     proceedings   st international conference computational
linguistics   th annual meeting association computational linguistics 
pp          morristown  nj  usa  association computational linguistics 
navigli  r          word sense disambiguation  survey  acm computing surveys             
navigli  r   litkowski  k     hargraves  o          semeval      task     coarse grained english
all words task  proceedings fourth international workshop semantic evaluations  semeval        pp        prague  czech republic  association computational
linguistics 
niles  i     pease  a          towards standard upper ontology  proceedings  nd
international conference formal ontology information systems  fois        pp    
    chris welty barry smith  eds 
niles  i     pease  a          linking lexicons ontologies  mapping wordnet suggested
upper merged ontology  arabnia  h  r   ed    proc  ieee int  conf  inf 
knowledge engin   ike        vol     pp          csrea press 
noreen  e          computer intensive methods testing hypotheses  introduction  wiley
interscience publication  wiley 
paa  g     reichartz  f       a   exploiting semantic constraints estimating supersenses
crfs   sdm  pp          siam 
paa  g     reichartz  f       b   exploiting semantic constraints estimating supersenses
crfs   sdm  pp          siam 
   

fiw ord vs   c lass  based w ord ense isambiguation

palmer  m   fellbaum  c   cotton  s   delfs  l     dang  h  t          english tasks  all words
verb lexical sample  proceedings senseval   workshop  conjunction
acl     eacl      toulouse  france 
peters  w   peters  i     vossen  p          automatic sense clustering eurowordnet  first international conference language resources evaluation  lrec     granada  spain 
picca  d   gliozzo  a  m     ciaramita  m          supersense tagger italian   lrec  citeseer 
pradhan  s   dligach  e  l  d     palmer  m          semeval      task     english lexical sample 
srl words  semeval     proceedings  th international workshop semantic
evaluations  pp        morristown  nj  usa  association computational linguistics 
rosch  e          human categorisation  studies cross cultural psychology  i         
schmid  h          probabilistic part of speech tagging using decision trees  proceedings
international conference new methods language processing  pp       
schneider  n   mohit  b   dyer  c   oflazer  k     smith  n  a          supersense tagging
arabic  mt in the middle attack   hlt naacl  pp          citeseer 
schneider  n   mohit  b   oflazer  k     smith  n  a          coarse lexical semantic annotation
supersenses  arabic case study  proceedings   th annual meeting
association computational linguistics  short papers volume    pp          association
computational linguistics 
segond  f   schiller  a   greffenstette  g     chanod  j          experiment semantic tagging
using hidden markov model tagging  acl workshop automatic information extraction
building lexical semantic resources nlp applications  pp        acl  new
brunswick  new jersey 
snow  r   s   p   d   j     a   n          learning merge word senses  proceedings joint
conference empirical methods natural language processing computational natural language learning  emnlp conll   pp           
snyder  b     palmer  m          english all words task  mihalcea  r     edmonds  p 
 eds    senseval    third international workshop evaluation systems semantic analysis text  pp        barcelona  spain  association computational linguistics 
tsvetkov  y   schneider  n   hovy  d   bhatia  a   faruqui  m     dyer  c          augmenting
english adjective senses supersenses  proc  lrec  pp           
villarejo  l   marquez  l     rigau  g          exploring construction semantic class classifiers wsd  proceedings   th annual meeting sociedad espaola para el
procesamiento del lenguaje natural sepln    pp          granada  spain  issn          
vossen  p   ed            eurowordnet  multilingual database lexical semantic networks
  kluwer academic publishers  
wikipedia         wikipedia  free encyclopedia  https   en wikipedia org    online 
accessed    august       
yarowsky  d          decision lists lexical ambiguity resolution  application accent restoration spanish french  proceedings   nd annual meeting association
computational linguistics  acl    
   

fii zquierdo   u arez   r igau

zhong  z     ng  h  t          makes sense  wide coverage word sense disambiguation system
free text  proceedings acl      system demonstrations  acldemos     pp 
      stroudsburg  pa  usa  association computational linguistics 

   


