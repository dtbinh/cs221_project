journal of artificial intelligence research                 

submitted       published      

continuing plan quality optimisation
fazlul hasan siddiqui
patrik haslum

fazlul siddiqui anu edu au
patrik haslum anu edu au

the australian national university  
nicta optimisation research group
canberra  australia

abstract
finding high quality plans for large planning problems is hard  although some current
anytime planners are often able to improve plans quickly  they tend to reach a limit at
which the plans produced are still very far from the best possible  but these planners fail
to find any further improvement  even when given several hours of runtime 
we present an approach to continuing plan quality optimisation at larger time scales 
and its implementation in a system called bdpo   key to this approach is a decomposition
into subproblems of improving parts of the current best plan  the decomposition is based
on block deordering  a form of plan deordering which identifies hierarchical plan structure 
bdpo  can be seen as an application of the large neighbourhood search  lns  local search
strategy to planning  where the neighbourhood of a plan is defined by replacing one or more
subplans with improved subplans  on line learning is also used to adapt the strategy for
selecting subplans and subplanners over the course of plan optimisation 
even starting from the best plans found by other means  bdpo  is able to continue
improving plan quality  often producing better plans than other anytime planners when
all are given enough runtime  the best results  however  are achieved by a combination of
different techniques working together 

   introduction
the classical ai planning problem involves representing models of the world  initial and
goal states  and available actions in some formal modelling language  and reasoning about
the preconditions and effects of the actions  given a planning problem  a planning system
 or planner  for short  generates a sequence of actions  whose application transforms the
world from the initial state to a desired goal state  thus  planning makes an intelligent
system autonomous through the construction of plans of action to achieve its goals 
a key concern in automated planning is producing high quality plans  planners using
optimal or bounded suboptimal  heuristic  search methods offer guarantees on plan quality 
but are unable to solve large problems  fast planners  using greedy heuristic search or other
techniques  on the other hand  can solve large problems but often find poor quality plans 
the gap between the capabilities of these two kinds of planners means that producing high
quality plans for large problems is still a challenge  an example of this gap is shown in
figure    we seek to address this gap by proposing a new approach to continuing plan
improvement  that is able to tackle large problems and works at varying time scales 
anytime search tries to strike a balance between optimal  or bounded suboptimal  and
greedy heuristic search methods  anytime search algorithms do so by finding an initial
solution  possibly of poor quality  quickly and then continuing to search for better solutions
c
    
ai access foundation  all rights reserved 

fi  
 

  

plan cost

  

siddiqui   haslum

  
          
 
 
    
 

  

   

   

problem  sorted 

figure    illustration of the plan quality gap  the dashed line represents the best  lowestcost  plan for     problems from genome edit distance  ged  domain  haslum       
found by different non optimal planners  including anytime planners  the solid line represents the corresponding highest known lower bound  the difference between these two is
the optimality gap  the   points represent plans found by optimal planners  while the
vertical bars show the optimality gap obtained by a problem specific algorithm  grimm  

the more time they are given  anytime search algorithms such as  for example  rwa 
 richter  thayer    ruml        or aees  thayer  benton    helmert      b  have been
successfully used in anytime planners  however  these planners are often not effective at
making use of increasing runtime beyond the first few minutes  xie  valenzano    muller
       define the unproductive time of a planner as the amount of time remaining when
it finds its best plan  out of the total time given  they show that in four ipc      domains
 barman  elevators  parcprinter  and woodworking   the unproductive time of the lama
planner  which uses rwa    given    minutes per problem  is more than     
we have observed similar results  as shown in figure    the figure shows the average
ipc quality score as a function of time for several anytime planners and plan optimisation
methods  including the lama planner   a full description of the experiment setup  and
results for even more anytime planners  is presented in section    from page       lama
finds a first solution quickly  for       of the problems it solves  within a maximum of  
hours cpu time per problem   the first plan is found in less than    minutes  the quality of
lamas plans improve rapidly early on  but the later trend is one of flattening out  i e  
decreasing increase   the drop at the beginning is due to the figure showing the average
plan quality over solved problems  as initial  low quality  plans for more problems are found
the average drops  before increasing again as better plans are found   between   and  
hours cpu time  lama improves the plans for       of solved problems  yet for a further
      of problems better plans exist  and are found by other methods  in the same time
interval  lamas average plan quality score increases by only       while an increase of
   

ficontinuing plan quality optimisation

    
    

average quality score  relative ipc quality score   coverage 

    
    
    
    




































































































   
    
    
    
    
    
    


    









































bdpo  on pngs on base plans
bdpo  on base plans
pngs on base plans
ibcs on base plans
bss on base plans
lama from scratch
ibacop  from scratch








    



 

   

 

   

 

   

 

   

 

   

 

   

 

   

 

    

time  hours 

figure    average ipc quality score as a function of time per problem  on a set of    
large scale planning problems  the quality score of a plan is cref c  where c is the cost
of the plan and cref the reference cost  least cost of all plans for the problem   hence
a higher score represents better plan quality  anytime planners  lama  ibacop   start
from scratch  while post processing  pngs  bdpo  and bounded cost search  ibcs  beamstack search  methods start from a set of base plans  their curves are delayed by   hour to
account for the maximum time given to generating each base plan  the experiment setup
and results for additional planners are described in section      page      

   

fisiddiqui   haslum

figure    general framework of bdpo 

at least       is possible  memory limited branch and bound algorithms  like beam stack
search  zhou   hansen        may run indefinitely  but find improvements very slowly 
the increase in average plan quality made by bss over the entire time depicted in figure
  is only      
plan optimisation approaches based on post processing start with a valid plan and seek
to improve it  figure   shows results for plan neighbourhood graph search  nakhost
  muller         pngs searches for shortcuts in a subgraph of the state space of the
problem  constructed around the current plan   the pngs implementation used in this
experiment also applies nakhosts and mullers action elimination technique   applying
pngs results in substantial plan quality improvements quickly        of improved plans
are found in less than    minutes  but then stops  as it runs out of memory 
in summary  this experiment shows that current anytime plan optimisation methods
become unproductive as runtime increases  or suffer from a very slow rate of plan quality
improvement 
we present a post processing approach to plan optimisation  and its implementation in
a system called bdpo    the source code for bdpo  is provided as an on line appendix
to this article   as a post processor  bdpo  does not work on its own  it depends on other
methods providing an initial plan  in the experiment  the set of input plans  referred to as
base plans  are the best plans found by lama after   hour  or the plan found by ibacop 
in the      ipc  what figure   shows is that switching to our approach after some time can
overcome the limitation of current anytime planning techniques  and continue to improve
plan quality as allotted time increases  the best result  as shown  is obtained by chaining
several techniques together  applying first pngs on the base plans  and then bdpo  on
the best result produced by pngs  this result could not be achieved by previous anytime
planning approaches alone 
bdpo  uses large neighborhood search  lns   a local search technique  the local
search explores a neighbourhood around the current solution plan for a better quality valid
plan  in lns  the neighbourhood of a solution defined by destroy and repair methods 
which together replace a part of the current solution  while keeping the rest of it unchanged 
in bdpo   the destroy step selects a subsequence of some linearisation of a deordering of
the current plan  we call this a window  and the repair step applies a bounded cost
planner to the subproblem of finding a better replacement for this subplan  this focus
on solving smaller subproblems makes local search  and lns in particular  scale better
to large problems  the size and structure of the neighborhood  however  plays a crucial
   

ficontinuing plan quality optimisation

role in the performance of local search  hoffmann         in our setting  the neighbourhood is determined by the strategies used to select windows and subplanners  the destroy
methods used in lns algorithms often contain an element of randomness  and the local
search may accept moves to lower quality solutions  ropke   pisinger        schrimpf 
schneider  stamm wilbrandt    dueck         in contrast  we explore the neighbourhood
systematically  examining candidate windows generated and ordered by several heuristics 
and accept only moves to strictly better plans  we also introduce into lns the idea of
delayed restarting  meaning that we search for and combine multiple local improvements
before restarting the next iteration from the new best plan  we have found that delayed
restarts allow better exploration of subplans from different parts of the current plan  and
helps avoid local minima that otherwise occur when the system attempts to re optimise the
same part of the plan in successive iterations 
the bdpo  framework  shown in figure    broadly consists of four components  plan
decomposition  lns  i e   the repeated destroy and repair steps   windowing  and on line
adaptation  the first step  decomposition  uses deordering to produce a partially ordered
plan  deordering enables the windowing strategies to find subplans that are easier to
improve on  leading to much better anytime performance  we use block deordering  siddiqui
  haslum         which simultaneously decomposes a given plan into coherent subplans 
called blocks  and relaxes ordering constraints between blocks  block deordering removes
some of the inherent limitations of existing  step wise deordering techniques  and is able
to deorder sequential plans in some cases where no step wise deordering is possible  the
windowing component is a collection of strategies for extracting windows from the block
deordered plan  and ranking policies which order the windows so that the system attempts
to optimise more promising windows first 
bdpo  extends our earlier system  bdpo  siddiqui   haslum      b   mainly by using
a variety of alternatives for each task  where bdpo used a single windowing strategy  with
no ranking  and a single subplanner  bdpo  uses portfolios of window generation and
ranking strategies and several subplanners  this improves the capability and robustness of
the system  since no single alternative  windowing strategy  subplanner  etc   dominates
all others across all problems  furthermore  we take advantage of the fact that the system
solves many subproblems over the course of the local search to learn on line which are the
best alternatives for the current problem  in particular  we use the ucb  multi armed
bandit learning policy  auer  cesa bianchi    fischer        for subplanner selection  and
a sequential portfolio of window ranking policies 
the remainder of this article is structured as follows  section   describes block deordering  the theory of block deordering presented here is slightly different from our earlier
account  siddiqui   haslum         allowing for more deordering in some cases and better contrasting it with traditional partially ordered plan semantics  section   presents an
overview of the bdpo  system and main empirical results  while sections   and   give
more details of the windowing and on line adaptation components  respectively  including
empirical analysis of their impact on the performance of the system as a whole  section  
reviews related work  and section   presents conclusions and outlines ideas for future work 
   

fisiddiqui   haslum

   plan decomposition
our approach to continuing plan quality improvement is based on optimising a plan by
parts  one at a time  every subplan that we consider for local optimisation is a subsequence
of some linearisation of a partially ordered plan  therefore  a key step is removing unnecessary ordering constraints from the  typically sequential  input plan  this process is called
plan deordering  the importance of deordering is demonstrated by one of our experiments
 presented in section      page       in which we apply bdpo  to input plans that are
already of high quality  the total plan quality improvement  measured by the increase in
the average ipc plan quality score  achieved by bdpo  without any deordering is      
less than that achieved with bdpo  using our plan deordering technique 
the standard notion of a valid partially ordered plan requires all unordered steps in the
plan to be non interfering  i e   for two subsequences of the plan to be unordered  every
interleaving of steps from the two must form a valid execution   this limits the amount of
deordering that can be done  in some cases to the extent that no deordering of a sequential
plan is possible   an example of this situation is shown in figure   on page       to
remedy this  we have introduced block deordering  siddiqui   haslum         which creates
a hierarchical decomposition of the plan into non interleaving blocks and deorders these
blocks  this makes it possible to deorder plans further  including in some cases where
conventional  step wise  deordering is not possible   again  an example can be found in
figure   on page       in this section  we present a new  and slightly different account of the
theory and practice of block deordering  first  it relaxes a restriction on block deordered
plans  thereby allowing more deordering of some plans  second  it contrasts the semantics
of block decomposed partially ordered plans with the traditional partially ordered plan
semantics in a clearer way 
sections        describe necessary background  while sections        introduce block
decomposed partially ordered plans and the block deordering algorithm 
    the planning problem  sequential plan and its validity
we consider the standard strips representation of classical planning problems with action
costs  a planning problem is a tuple    hm  a  c  i  gi  where m is a set of atoms
 alternatively called fluents or propositions   a is a set of actions  c   a  r   is a cost
function on actions  which assigns to each action a non negative cost  i  m is the initial
state  and g  m is the goal 
an action a is characterised by a triple hpre a   add a   del a i  where pre a   add a  
and del a  are the preconditions  add and delete effects of a respectively  we also say that
action a is a consumer of an atom m if m  pre a   a producer of m if m  add a   and
a deleter of m if m  del a   an action a is applicable in a state s if pre a   s  and
if applied in s  results in the state apply a  s     s   del a    add a   a sequence of
actions    hai   ai          aj i is applicable in a state si if     pre ak    sk for all i  k  j 
and     si     apply ai   si    si     apply ai     si      and so on  the resulting state is
apply   si     si j    
a valid sequential plan  also totally ordered plan  seq   ha         an i for a planning
problem  is a sequence of actions that is applicable in i and such that g  apply seq   i  
the actions of seq must be executed in the specified order 
   

ficontinuing plan quality optimisation

    the partially ordered plan and its validity
plans can be partially ordered  in which case actions can be unordered with respect to
each other  a partially ordered plan  p o  plan  is a tuple  pop   hs  i  where s is a
set of steps  each of which is labelled by an action from a  and  represents a strict  i e  
irreflexive  partial order over s  the unordered steps in pop can be executed in any order 
  denotes the transitive closure of   an element hsi   sj i   also si  sj   is a basic
ordering constraint iff it is not transitively implied by other constraints in   for a plan
step s  we use pre s   add s  and del s  to denote the preconditions  add and delete effects
of the action associated with s  we also use the terms producer  consumer  deleter  and
cost for plan steps  referring to their associated actions  we include in s two more steps 
si and sg   si is ordered before all other steps  consumes nothing and produces the initially
true atoms  while sg is ordered after all other steps  consumes the goal atoms and produces
nothing 
a linearisation of pop is a total ordering of the steps in s that respects   a p o  plan
pop is valid  for a planning problem   iff every linearisation of pop is a valid sequential
plan  for    in other words  a p o  plan can be viewed as a compact representation of a
set of totally ordered plans  namely its linearisations 
every basic ordering constraint  si  sj   in pop has a set of associated reasons  denoted
by re si  sj    these reasons explain why the ordering is necessary for the plan to be
valid  if re si  sj   is non empty  then some step precondition may be unsatisfied before
its execution in some linearisations of pop that violate si  sj   the reasons are of three
types 
pc m   producerconsumer of atom m   the first step  si   produces m which is a precondition of the second step  sj   thus  if the order is changed  and sj executed before si  
the precondition of sj may not have been established when it is required 
cd m   consumerdeleter of m   the second step  sj deletes m  which is a precondition of
si   thus  if the order is changed  m may be deleted before it is required 
dp m   deleterproducer of m   the first step  si deletes m  which is produced by the
second step  sj   if the order is changed  the add effect of the producer step may be
undone by the deleter  causing a later step to fail  it is  however  not necessary to
order a producer and deleter if no step that may occur after the producer in the plan
depends on the added atom 
note that an ordering constraint can have several associated reasons  including several
reasons of the same type but referring to different atoms  the producerconsumer relation
pc m   re si  sj   is usually called a causal link from si to sj for m  mcallester  
rosenblitt         and denoted by a triple hsi   m  sj i  a causal link hsi   m  sj i is threatened
if there is any deleter of m that may be ordered between the last producer of m before sj
and sj   since this implies a possibility of m being false when required for the execution of
sj   the formal definition is as follows 
definition    let pop   hs  i be a p o  plan  and hsp   m  sc i be a causal link in pop  
hsp   m  sc i is threatened if there is a step sd that deletes m such that neither     sc   sd
nor     s p   m  add s p    sd   s p   sc is true 
   

fisiddiqui   haslum

as mentioned above  a p o  plan  pop   hs  i for a planning problem  is valid iff
every linearisation of pop is a valid sequential plan for   however  the following theorem
gives an alternative  equivalent  condition for p o  plan validity 
theorem    e g   nebel   backstrom         a p o  plan is valid iff every step precondition
can be supported by a causal link such that there is no threat to that causal link 
this condition is the same as chapmans        modal truth criterion  that
sc  s  m  pre sc    
sp  s    pc m   re sp  sc  
st   m  del st    sc   sd  s p   m  add s p    sd   s p   sc



 

    deordering
the process of deordering converts a sequential plan into a p o  plan by removing ordering
constraints between steps  such that the steps of the plan can be successfully executed in
any order consistent with the partial order and still achieve the goal  backstrom         we
will refer to this as step wise deordering  to distinguish it from the block decomposition and
deordering that we introduce later in this section  since current state space search planners
can produce sequential plans very efficiently  deordering plays an important role in efficient
generation of p o  plans 
let pop   hs  i be a valid p o  plan  a  step wise  deordering of pop is a valid plan
 
 
pop
  hs    i such that           that is  pop
is the result of removing some basic
ordering constraints without invalidating the plan  a sequential plan seq   ha         an i can
be represented as a p o  plan with one step si  s for each action ai in seq and an ordering
si  sj whenever i   j  so that no two steps in s which are unordered  thus  deordering a
sequential plan is no different from  further  deordering a p o  plan 
computing a  step wise  deordering with a minimum number of ordering constraints is
np hard  backstrom         but there are several non optimal algorithms  e g   pednault 
      veloso  perez    carbonell        regnier   fade         we have used a variant of
the explanation based generalisation algorithm by kambhampati and kedar         the
algorithm works in two phases  in the first phase it constructs a validation structure  which
has exactly one causal link hsp   m  sc i for each precondition m of each step sc   sp is chosen
as the earliest producer of m preceding sc in the input plan  with no intervening threatening
step  i e   that deletes m  between sp and sc    the algorithm by veloso  perez and carbonell
is similar  but selects the latest producer instead   in the second phase  the algorithm builds
a partial ordering  keeping only those orderings in the original plan which either correspond
to causal links in the validation structure or that are required to prevent a threatening step
from becoming unordered w r t  the steps in such a causal link 
kambhampati and kedars deordering algorithm  due to its greedy strategy  does not
guarantee optimality  an example where it fails to transform a totally ordered plan to
a least constrained plan is shown in figure    however  a recent study found that the
algorithm did produce optimal step wise plan deorderings of all plans on which it was
tested  muise  mcilraith    beck        
however  our motivation for plan deordering is to find a deordering that is adequate for
generating useful candidate subplans for local optimisation  more important than achieving
   

ficontinuing plan quality optimisation

figure    an example on which kambhampati and kedars        algorithm fails to find
the least constrained plan   derived from figure    in backstroms      article on plan
deordering   figure  a  is a sequential input plan   b  is the plan produced by the algorithm
after choosing the earliest producer  for the validation structure  of the preconditions p and
q of d  and  c  is the minimally ordered version of  a   for simplicity  the goal atoms
produced by the steps a  b  c and d are not shown in the figure 

an optimal step wise deordering is overcoming the inherent limitation of step wise deordering  which only allows plan steps to be unordered when they are non interfering  block
deordering  described in the next two sections  can remove further orderings from input
plans by forming blocks  which helps generate a decomposed plan that is more suitable for
extracting subplans for local optimisation 
    block decomposition
in a conventional p o  plan  whenever two subplans are unordered every interleaving of steps
from the two forms a valid execution  this limits deordering to cases where individual steps
are non interfering  to remove this restriction  we have proposed block decomposed partial
ordering  which restricts the interleaving of steps by dividing the plan steps into blocks  such
that the steps in a block must not be interleaved with steps not in the block  however 
steps within a block can still be partially ordered  this is illustrated with an example in
figure    the figure shows the difference in linearisations between a p o  plan and a block
decomposed p o  plan  b  a  c  d is a valid linearisation in the standard partial ordering but
not in the block decomposed p o  plan  the formal definition of a block is as follows 
definition    let pop   hs  i be a p o  plan  a block w r t    is a subset b  s of steps
such that for any two steps s  s   b  there exists no step s     s  b  such that s   s     s   
a decomposition of a plan into blocks can be recursive  i e   a block can be wholly
contained in another  however  blocks cannot be partially overlapping  two blocks are
ordered bi  bj if there exist steps si  bi and sj  bj such that si  sj and neither block is
contained in the other  i e   bi   bj and bj   bi   
definition    let pop   hs  i be a p o  plan  a set b of subsets of s is a block
decomposition of pop iff     each b  b is a block w r t   and     for every bi   bj  b 
either bi  bj   bj  bi   or bi and bj are disjoint  a block decomposed plan is denoted by
bdp   hs  b  i 
   

fisiddiqui   haslum

figure    a normal p o  plan  left  represents the set of all sequential plans that are linearisations of the plan steps  in this example ha  b  c  di  hb  a  c  di  hb  c  a  di  and hb  c  d  ai 
a block decomposed p o  plan  shown on the right with dashed outlines for blocks  allows
unordered blocks to be executed in any order  but not steps from different blocks to be
interleaved  thus  only ha  b  c  di  hb  c  a  di  and hb  c  d  ai are possible linearisations of
this plan 

the semantics of a block decomposed plan is defined by restricting its linearisations  for
which it must be valid  to those that respect the block decomposition  i e   that do not
interleave steps from disjoint blocks  if bi  bj   all steps in bi must precede all steps in bj
in any linearisation of the block decomposed plan 
definition    let bdp   hs  b  i be a block decomposed p o  plan for planning problem
  a linearisation of bdp is a total order lin on s such that     lin and     every
b  b is a block w r t  lin   bdp is valid iff every linearisation of bdp is a plan for  
blocks behave much like  non sequential  macro steps  having preconditions  add and
delete effects that can be a subset of the union of those of their constituent steps  this
enables blocks to encapsulate some plan effects and preconditions  reducing interference
and thus allowing more deordering  the following definition captures those preconditions
and effects that are visible from outside the block  i e   those that give rise to dependencies
or interference with other parts of the plan  these are what we need to consider when
deciding if two blocks can be unordered   note that a responsible step is a step in a block
that causes it to produce  consume or threaten an atom  
definition    let bdp   hs  b  i be a block decomposed p o  plan  and b  b be a block 
the block semantics are defined as 
 b adds m iff b does not have precondition m  and there is a responsible step s  b with
m  add s   such that for all s   b  if s  deletes m then s   s 
 b has precondition m iff there is a responsible step s  b with m  pre s   and there
is no step s   b such that there is a causal link hs    m  si without an active threat 
 b deletes m iff there is a responsible step s  b with m  del s   and there is no step
s   b with s  s   that adds m 
note that if a block consumes a proposition  it cannot also produce the same proposition 
the reason for this is that taking the black box view of block execution  the proposition
simply persists  it is true before execution of the block begins and remains true after it has
finished  if the steps within a block are totally ordered  the preconditions and effects of a
block according to definition   are nearly the same as the cumulative preconditions and
   

ficontinuing plan quality optimisation

effects of an action sequence defined by haslum and jonsson         the only difference
being that a consumer block cannot also be a producer of the same proposition 
a conventional p o  plan  to be valid  must not contain any threat to a causal link  in
contrast  a block decomposed p o  plan allows a threat to a causal link to exist in the plan 
as long as the causal link is protected from that threat by the block structure  a causal
link is protected from a threat iff either  i  the causal link is contained by a block that does
not contain the threat  or  ii  the threat is contained by a block that does not contain the
causal link and does not delete the threatened atom  i e   encapsulates the delete effect   a
threat to a causal link is active if the link is not protected from it  otherwise inactive  the
formal definition is as follows 
definition    let bdp   hs  b  i be a block decomposed p o  plan  and st  s be a threat
to a causal link hsp   m  sc i in bdp   hsp   m  sc i is protected from st  s iff there exist a block
b  b such that either of the following is true      sp   sc  b  st 
  b  or     st  b  sp   sc 
  b 
and m 
  del b  
an example of how the block decomposition protects a causal link can be seen in figure
  i  on page     
the following theorem provides an alternative criterion for the validity of a block decomposed p o  plan  in analogy with the condition for a conventional p o  plan given in
the theorem cited above  the only difference is that a block decomposed p o  plan allows
threats to causal links  as long as those threats are inactive  let bdp   hs  b  i be a block
decomposed p o  plan  analogously with chapmans modal truth criterion  this condition
can be stated as follows 
sc  s  m  pre sc  
sp  s    m  add sp  
st  s    m  del st    st    sp  sc    st  hsp   m  sc i is protected from st    
theorem    a block decomposed p o  plan is valid iff every step precondition is supported
by a causal link that has no active threat 
proof  let bdp   hs  b  i be a block decomposed p o  plan of a planning problem   let
us first prove the if part  i e   that if every step precondition is supported by a causal
link that has no active threat then every linearisation of bdp is a valid plan for   let
seq   h        sc        i be an arbitrary linearisation of bdp with a total order seq on s  where
m  pre sc    then  according to the validity criteria for a sequential plan  we have to show
that m must be satisfied before the execution of sc in seq   since every step precondition
is supported by a causal link in bdp that has no active threat  m must be supported by a
causal link hsp   m  sc i that has no active threat  moreover  since seq then sp seq sc  
let st be a threat to hsp   m  sc i in bdp   clearly  sp seq st seq sc is the only possibility
that may cause m to be unsatisfied before the execution of sc   since hsp   m  sc i has no active
threat  hsp   m  sc i is protected from st   and therefore  according to definition    either    
sp   sc  b and st 
  b  or     st  b  sp   sc 
  b  and m 
  del b   must hold  if     is true 
then sp seq st seq sc can not occur in any valid linearisation of bdp   since it interleaves
steps sp   sc  b with st 
  b  and thus b is not a block w r t  seq   in the second case  since
   

fisiddiqui   haslum

m
  del b  then there must be a producer of m  s p  b  such that st seq s p   moreover 
since sp   sc 
  b  sp seq st seq sc can only be true if sp seq st seq s p seq sc   this also
makes m true before the execution of sc in seq  
let us now prove the only if part  i e   that if bdp is valid then every step precondition
is supported by a causal link that has no active threat  let sc  s  m  pre sc    and
seq   h        sc        i be a linearisation of bdp with a total order seq on s  we consider two
possible situations      there is no producer s  from which a causal link hs    m  sc i in bdp
can be constructed  or     there is at least one such producer that can construct the causal
link with sc for the atom m but that causal link has an active threat in bdp   we will show
that none of the above situations can happen as long as bdp is valid  according to situation
     there is no s  in seq as well such that s  seq sc   this causes m to be unsatisfied before
the execution of sc in seq   i e   seq become invalid  consequently  bdp become invalid
 since one of its linearisation is invalid   which contradicts with our assumption  therefore 
there must exist at least one producer s  that can construct a causal link hs    m  sc i in bdp  
now  for situation      assume sp is the last producer of m before the execution of sc in
seq   i e   s p  s   sp   m  add s p     s p seq sp  sc seq s p    let sp be the producer
in the causal link hsp   m  sc i in bdp  which is possible  since sp is not ordered after sc in
bdp    assume hsp   m  sc i has an active threat st in bdp   since hsp   m  sc i has an active
threat st  i e   hsp   m  sc i is not protected from st    then neither  i  sp   sc  b  st 
  b  nor
 ii  st  b  sp   sc 
  b  and m 
  del b   is true  therefore  sp seq st seq sc is a possible
linearisation of bdp   moreover  since there is no more producer of m in between sp and sc  
m must be unsatisfied before the execution of sc   i e   seq becomes invalid  consequently 
bdp is invalid since one of its linearisations is invalid  therefore  hsp   m  sc i must not have
any active threat 
    block deordering
block deordering  siddiqui   haslum        is the process of removing orderings between
plan steps by adding blocks to a block decomposed p o  plan  it may also add to the plan
some new ordering constraints  but those are transitively implied by the other ordering
constraints  block deordering can often remove ordering constraints where step wise deordering can not  this is because the no interleaving restriction among the blocks affords
us a simplified  black box  view of blocks that localises some interactions  in which only
the preconditions and effects of executing the block as a whole are important  thus  it allows further deordering by being able to ignore some dependencies and effects that matter
only internally within the block  in addition to providing more linearisations  by improving
deordering  the blocks formed by block deordering often correspond to coherent  more selfcontained subplans  and form the basis for the windowing strategies  described in detail
in section    that we use to generate candidate subplans for local optimisation 
this subsection presents the conditions under which adding blocks to a block decomposition allows the removal of basic ordering constraints  the complete block deordering
algorithm is presented in the next subsection 
as a simple example of block deordering  figure   i  shows a sequential plan for a small
logistics problem  this plan can not be deordered into a conventional p o  plan  because
each plan step has a reason to be ordered after the previous  block deordering  however 
   

ficontinuing plan quality optimisation

figure    a sequential plan and a block deordering of this plan with two unordered blocks
b  and b   ordering constraints are labelled with their reasons  producerconsumer  pc  
i e   causal link  deleterproducer  dp   and consumerdeleter  cd   note that no ordering
constraint in the sequential plan can be removed without invalidating it  thus  step wise
deordering of this plan is not possible 

is able to break the ordering s   s  by removing the only reason pc at p  a  based on
the formation of two blocks b  and b  as shown in figure   ii   neither of the two blocks
delete or add the atom at p  a  although it is a precondition of both   this removes
the interference between them  and allows the two blocks to be executed in any order but
without any interleaving  therefore  the possible linearisations of the block decomposed
p o  plan are only hs   s   s   s i and hs   s   s   s i  note that if b  is ordered before b  
then b  can be optimised by removing step s  
besides the necessary orderings between a pair of steps in a plan due to reasons pc 
cd  and dp  stated in section       a valid block decomposed p o  plan must maintain one
more type of necessary ordering  called threat protection ordering  if removing an ordering
sx   sy causes a block containing both steps to have delete effect  which it did not have
with this ordering  and that delete effect causes a causal link outside the block to become
unprotected  not satisfying either of the two conditions of definition     then sx   sy is a
threat protection ordering  which may not be removed  a threat protection ordering can be
introduced during the block deordering process  and once introduced can not be removed 
this is demonstrated in figure    where removing this kind of ordering leads to an invalid
block decomposed p o  plan  the threat protection ordering is defined formally as follows 
definition    let bdp   hs  b  i be a block decomposed p o  plan  and hsp   m  sc i be a
causal link that is protected from st  s in bdp   let b  b  st   s   b  sp   sc 
  b  m  add s    
m
  del b   and st   s    st   s  is a threat protection ordering if breaking this ordering
causes m  del b  and that causes hsp   m  sc i to become unprotected from st  
   

fisiddiqui   haslum

figure    two block decompositions of a plan containing five steps  s   s   s   s   and s  
in decomposition  i   there are three  transitively reduced  necessary orderings  s   s  
s   s   and s   s   where re s   s      dp m   dp n    re s   s      pc m   
and re s   s      pc n    this decomposition is valid since every step precondition is
satisfied by a causal link without active threats  the threat from s  to causal link hs   n 
s i is inactive  since the link is protected by block bx    s   s   s   which contains s  but
does not delete m  and is disjoint from the causal link  by forming two blocks  by    s  
and bz    s   s   it would be possible to remove s   s   as shown in  ii   since hs   m  s i
is then protected from s  by bz   however  in this decomposition the delete effect of block bx
becomes del bx      m  n   and the block therefore no longer protects hs   n  s i  therefore 
this decomposition and deordering is invalid  the ordering s   s  is a threat protection
ordering  which must not be broken  note that in  i  s  has no consumers of its produced
atom n  yet acts as a white knight for hs   n  s i to protect n from the deleter s  

the notion of threat protection ordering was missing from our earlier block deordering
procedure  siddiqui   haslum         which relied  implicitly  on the stronger restriction
that the delete effects of a block do not change due to subsequent deordering inside the block 
explicitly checking only the necessary threat protection orderings allows more deordering
inside created blocks to take place 
to remove a basic ordering  si  sj   from a block decomposed p o  plan bdp   hs  b  i 
we create two blocks  bi and bj   where si  bi   sj  bj   and bi  bj     note that one of the
two blocks can consist of a single step  both blocks must be consistent with the existing
decomposition  i e   b   bi   bj   must still be a valid block decomposition  in the sense of
definition    in the remainder of this subsection  we define four rules which state conditions
on blocks bi and bj that allow different reasons for the ordering si  sj to be eliminated 
since the ordering si  sj can exist for several reasons  including several reasons of the
same type  referring to different atoms   it is only if blocks bi and bj can be found that
allow us to remove every reason in re si  sj   that the ordering between the steps can be
removed 
rule    let bdp   hs  b  i be a valid block decomposed p o  plan  si  sj be a basic
ordering whose removal does not cause any threat protection ordering to be removed  and
pc m   re si  sj    let bi be a block  where si  bi   sj 
  bi   and s   bi   si  s    pc m 
can be removed from re si  sj   if m  pre bi   and sp 
  bi such that sp can establish a
causal link to bi and to sj  
   

ficontinuing plan quality optimisation

figure    formation of a block  s p  and addition of a causal link hr m qi in  ii  in order
to remove the reason pc m  behind the basic ordering constraint p  q from  i   different
situations   iii and iv   where a threat  t  may be active to hr m qi 

as an explanation of rule    if pc m   re si  sj    then bi must not produce m  since
si produces m and is not followed by a deleter of m within bi  because si  sj is a basic
ordering and sj 
  bi   the only way for this to happen is if bi consumes m  since the plan is
valid  there must be some producer  sp 
  bi   that necessarily precedes the step  in bi   that
 
consumes m  note that sp  sj   then adding the causal link pc m  to re sp  sj    i e  
adding hsp   sj i to  if not already present  allows pc m  to be removed from re si  sj   
theorem    deordering according to rule   preserves plan validity 
proof  let bdp   hs  b  i be a valid block decomposed p o  plan  therefore  according to
theorem    every step precondition of bdp is supported by a causal link that has no active
threat  let p  q be a basic ordering constraint  where p  q  s   bp   bq  b be the blocks
that meet the conditions for removing pc m   re p  q   and bp   bq are not ordered for
any other ordering constraints  we will show that removing pc m  from re p  q  results
 
in a new plan  bdp
  hs  b       i  that meets the condition of theorem    and therefore
remains valid 
assume pc m   re p  q  is removed  and the precondition of q is now supplied by the
step r based on the newly established causal link hr m qi after deordering and formulating
 
bp    s     p   bq    q  in bdp
  as shown in figure    ii   we have to show that hr m qi
 
 
has no active threat in bdp   and therefore  bdp
is valid  assume  there is an active threat 
 
 
t  to hr m qi in bdp   then  of course  t  r and q   t  we will examine every other
situation  where t can be an active threat to hr m qi 
situation      assume s   t  as shown in figure    iii   since t is not an active threat
to hr m si in bdp   then according to theorem    either t is contained by a block that does
   

fisiddiqui   haslum

figure    formation of blocks for removing the reason cd m  behind the basic ordering
p  q 

not delete the threatened atom and does not contain hr m si  or hr m si is contained by a
 
block b     r  s       that does not contain t  for the first case  it also holds true in bdp
  and
 
therefore  t can not be an active threat to hr m qi  for the second case  b can not partially
overlap with bp    s     p   therefore  either bp  b  or b   bp   if bp  b    bp must contain r 
which can not happen according to the pc removing criteria  i e   r 
  bp must hold  stated
in rule    if b   bp   then b  must contain at least r  s  and p  because b  can not partially
overlap with bp    s     p   since t is also not an active threat to hp m qi in bdp   hp m qi
must be contained by some block b      p  q       that does not contain t  now  since b  and
b   can not partially overlap  b  or b    whichever is bigger  must contain at least r  s  p  and
q  for which b  or b    whichever is bigger  protects hr m qi from t 
situation      assume t   p  also shown in figure    iii   since t is also not an active
threat to hp m qi in bdp   like before  we can show that either t is contained by a block that
encapsulates the threatened atom  i e   does not delete m  and does not contain hp m qi 
or hp m qi is contained by a block b     r  s  p  q       that does not contain t  in both cases 
hr m qi is protected from t 
situation      assume s   t   p shown in figure    iv   this is only possible if t  bp  
since t can not interleave with the steps in bp if t 
  bp   therefore  t  bp   which causes
hr m qi to be protected from t  this is because bp does not contain hr m qi and does not
delete m  since m  add p  and t   p  
therefore  we can conclude that t can never be an active threat to hr m qi under any
situation 
rule    let bdp   hs  b  i be a valid block decomposed p o  plan  si  sj a basic ordering
whose removal does not cause any threat protection ordering to be removed  and cd m  
   

ficontinuing plan quality optimisation

re si  sj    let bi and bj be two blocks  where si  bi   sj  bj   and bi  bj     then
cd m  can be removed from re si  sj   if bi does not consume m 
theorem    deordering according to rule   preserves plan validity 
proof  let bdp   hs  b  i be a valid block decomposed p o  plan  and p  q be a basic
ordering constraint  where p  q  s with cd m   re p  q   in order to meet the
condition of rule    let us assume bp is a block that includes r and p such that hr m pi is a
causal link and every other consumer of m in bp  if they exist  is ordered after r in bdp  as
shown in figure    i    therefore it meets the condition that bp must not consume m  also 
assume bq is a block that contains  q  and bp   bq are not ordered for any other ordering
constraints  therefore  cd m   re p  q  as well as p  q are removed  which results a
 
 
new plan bdp
  hs  b       i  we will show that bdp
is valid according to theorem   
since bdp is valid  there is no active threat to any causal link in bdp according to
theorem    but due to the deordering of p  q  the deleter q becomes a new threat only to
 
  however  hr m pi is contained by bp that does not contain
the causal link hr m pi in bdp
q  and therefore  according to definition    hr m pi is protected from q  i e   q becomes an
 
remains valid 
inactive threat  as a result  bdp
rule    let bdp   hs  b  i be a valid block decomposed p o  plan  si  sj a basic ordering
whose removal does not cause any threat protection ordering to be removed  and cd m  
re si  sj    let bi and bj be two blocks  where si  bi   sj  bj   and bi  bj     then
cd m  can be removed from re si  sj   if bj does not delete m 
theorem    deordering according to rule   preserves plan validity 
proof  let bdp   hs  b  i be a valid block decomposed p o  plan  and p  q be a basic
ordering constraint  where p  q  s with cd m   re p  q   in order to meet the condition
of rule    let us assume bq is a block that includes q and s such that dp m   re q  s 
and every other deleter of m in bq  if they exist  is ordered before s in bdp  as shown in
figure    ii    therefore it meets the condition that bq must not delete m  also  assume bp
is a block that contains  p   and bp   bq are not ordered for any other ordering constraints 
therefore  cd m   re p  q  as well as p  q is removed  which results a new plan
 
 
is valid according to theorem   
  hs  b       i  we will show that bdp
bdp
since bdp is valid  there is no active threat to any causal link in bdp according to
theorem    but due to the deordering of p  q  the deleter q becomes a new threat only to
 
the causal link hr m pi in bdp
  however  q is contained by bq that does not contain hr m pi 
and does not delete m  therefore  according to definition    hr m pi is protected from q  i e  
 
q becomes an inactive threat  as a result  bdp
satisfies the condition of theorem   and
therefore remains valid 
rule    let bdp   hs  b  i be a valid block decomposed p o  plan  si  sj a basic ordering
whose removal does not cause any threat protection ordering to be removed  and dp m  
re si  sj    let bj be a block  where sj  bj but si 
  bj   then dp m  can be removed from
re si  sj   if bj includes every step s  such that pc m   re sj  s    
theorem    deordering according to rule   preserves plan validity 
   

fisiddiqui   haslum

figure     formation of blocks for removing the reason dp m  behind the basic ordering
p  q 

proof  let bdp   hs  b  i be a valid block decomposed p o  plan  and let p  q be a basic
ordering constraint  where p  q  s   let bq be a block that includes all the steps  r and
s such that hq m ri  hq m si are the causal links in bdp  as shown in figure    ii    hence 
it meets the condition of rule    also  assume bp is a block that contains  p  and bp   bq
are not ordered for any other ordering constraints  as a result  dp m   re p  q  as well
 
 
  hs  b       i  we will show that bdp
as p  q is removed  which results a new plan bdp
satisfies the condition of theorem   and therefore remains valid 
since bdp is valid  there is no active threat to any causal link in bdp according to
theorem    but due to the deordering of p  q  the deleter p becomes a new threat to the
 
  however  those causal links are contained by
only causal links hq m ri and hq m si in bdp
bq that does not contain p  and therefore  according to definition    are protected from p 
 
i e   p becomes an inactive threat  as a result  bdp
remains valid 

even when  by applying the four rules above  we can find blocks bi and bj that remove
all reasons for an ordering si  sj   thus permitting the ordering to be removed  it is not
guaranteed that the two blocks bi and bj will be unordered  they may be ordered because
bi contains some step other than si that is ordered before some step in bj  whether sj or
another   even if they are not  if there is a block b  b that contains bi  or bj but not both  
and b is still ordered before bj  resp  after bi   due to some constraint in   other than
hsi   sj i  then blocks bi and bj will still be ordered  in the sense that bi will appear before bj
in any linearisation consistent with the block decomposition 
   

ficontinuing plan quality optimisation

    block deordering algorithm
the previous subsection described four conditions  rules     under which adding blocks
to a decomposition allows reasons for ordering constraints  and thus ultimately the ordering
constraints themselves  to be removed while preserving plan validity  next  we describe the
algorithm that uses these rules to perform block deordering  i e   to convert a sequential
plan seq into a block decomposed p o  plan bdp  
the algorithm is divided into two phases  first  we apply a step wise deordering procedure to convert seq into a p o  plan pop    s       we have used kambhampati and
kedars        algorithm for this  because it is simple and has been shown to produce very
good results  muise et al          even though it has no optimality guarantee 
after the step wise plan deordering  we extend ordering to blocks  two blocks are ordered
bi  bj if there exist steps si  bi and sj  bj such that si  sj and neither block is contained
in the other  i e   bi   bj and bj   bi    in this case  all steps in bi must precede all steps in bj
in any linearisation of the block decomposed plan  we also extend the reasons for ordering
 pc  cd and dp  to ordering constraints between blocks  with the set of propositions
produced  consumed and deleted by a block given by definition    recall that a responsible
step is a step in a block that causes it to produce  consume or delete a proposition  for
example  if b produces p  there must be a step s  b that produces p  such that no step in
the block not ordered before s deletes p  we say step s is responsible for b producing p 
the next phase is block deordering  which converts the p o  plan pop    s    into a
block decomposed p o  plan bdp    s  b       this is done by a greedy procedure  which
examines each basic ordering constraint bi  bj in turn and attempts to create blocks that
are consistent with the decomposition built so far and that will allow this ordering to be
removed  the core of this algorithm is the resolve procedure  algorithm     it takes as
input two blocks  bi and bj   that are ordered  one or both blocks may consist of a single step  
and tries to break the ordering by extending them to larger blocks  b i and b j   the procedure
examines each reason for the ordering constraint and extends one of the blocks to remove
that reason  following the rules given in the previous subsection  after this  the sets of
propositions produced  consumed and deleted by the new blocks  b i and b j   are recomputed
 following definition    and any new reasons for the ordering constraint that have arisen
because of steps that have been included are added to re b i  b j    this is repeated
until either no reason for the ordering remains  in which case the new blocks returned by
the procedure can safely be unordered  or some reason cannot be removed  in which case
deordering is not possible  signalled by returning null   the function intermediate bi   bj  
returns the set of steps ordered between bi and bj   i e    s   bi   s   bj    where algorithm
  refers to a nearest step s  preceding or following another step s  it means a step with a
smallest number of basic ordering constraints between s  and s 
if we applied the resolve procedure to each basic ordering constraint we would obtain
a collection of blocks with which we can break some orderings  but this collection is not
necessarily a valid decomposition  since some of the blocks may have partial overlap  to
find a valid decomposition  we use a greedy procedure  we repeatedly examine each basic
ordering constraint bi  bj and call resolve to find two extended blocks b i  bi and b j  bj
that allow the ordering to be removed  in each iteration  constraints are checked in order
from the beginning of the plan  a block  once added into bdp   will not be removed to
   

fisiddiqui   haslum

algorithm   resolve ordering constraints between a pair of blocks 
   procedure resolve bi   bj  
  
initialise b i   bi   b j   bj  
  
while re b i  b j       do
  
for each r  re b i  b j   do
  
if r   pc p  then
   try rule  
  
find a responsible step s  b i and a nearest s    b i that consumes
p such that s    s 
  
if such s  exists then
  
set b i   b i   s     intermediate s    b i   
  
else return null
   
else if r   dp p  then
   try rule  
   
find a responsible step s  b j and all s    b j such that
each hs  p  s  i is a causal link 
   
if such s  exists then
   
set b j   b j   s     intermediate b j   s    
   
else return null
   
else if r   cd p  then
   try rule  
   
find a responsible step s  b j and a nearest s    b j that produces p 
such that s   s   
   
if such s  exists then
   
set b j   b j   s     intermediate b j   s    
   
else
   try rule  
   
find a responsible step s  b i and a nearest s    b i that produces
p  such that s    s 
   
if such s  exists then
   
set b i   b i   s     intermediate s    b i   
   
else return null 
   
recompute re b i  b j   
   

return  b i   b j   

accommodate another block that partially overlaps with the existing block throughout the
procedure  even if the later  rejected  block could produce more deordering than the one
created earlier  since the choice of deordering to apply is greedy  the result is not guaranteed
to be optimal  if b i or b j cannot be added to the decomposition  because one or both of
them partially overlaps with an existing block   we consider all blocks ordered immediately
after bi   and check if all these orderings can be broken simultaneously  using the union of the
blocks returned by resolve for each ordering constraint   symmetrically  we also check the
set of blocks immediately before bj   though this is only very rarely useful   as an additional
   

ficontinuing plan quality optimisation

heuristic  we discard the two blocks if there is a basic ordering constraint between a step
that is internal to one of the blocks  i e   that has both preceding and following steps within
the block  and a step outside the block 
if the ordering can be removed  the inner loop exits and the ordering relation is updated
with any new constraints between b i and blocks ordered after bj and between b j and blocks
ordered before bi   this is done by checking for the three reasons  pc  cd and dp  based
on the sets of propositions produced  consumed and deleted by b i and b j   the inner loop
is then restarted  with ordering constraints that previously could not be broken checked
again  this is done because removing ordering constraints can make possible the resolution
of other constraints  since removal of orderings can change the set of steps intermediate
between two steps 
the main loop repeats until no further deordering consistent with the current decomposition is found  each iteration runs in polynomial time  but we do not know of an upper
bound on the number of iterations  note  however  that our procedure is anytime  in the
sense that if interrupted before running to completion  the result at the end of the last completed iteration is still a block deordering of the plan  in bdpo   we use a time limit of  
minutes for the whole deordering procedure  however  for almost every problem considered
in our experiments  described in section       block deordering finishes in a few seconds
 except for a few problems in the visitall domain  for which it takes a couple of minutes  
in summary  deordering makes the structure of a plan explicit  showing us which parts
are necessarily sequential  because of dependency or interference  and which are independent and non interfering  block deordering improves on this by creating an on the fly
hierarchical decomposition of the plan  encapsulating some dependencies and interferences
within each block  considering blocks  instead of primitive actions  as the units of partial
ordering thus enables deordering plans to a greater extent  including in cases where no deordering is possible using the standard  step wise  partial order plan notion  the impact of
block decomposition on the anytime performance of our plan quality optimisation system
is discussed in section     

   system overview
bdpo  is a post processing based plan quality optimisation system  starting with an initial
plan  it seeks to optimise parts of the plan  i e  subplans  replacing them with lower cost
subplans  we refer to the subplans that are candidates for replacement as windows  when
a better plan has been found and certain conditions are met  it starts over from the new
plan  this can be viewed as a local search  using the large neighborhood search  lns 
strategy  in which the neighborhood of a plan is defined as the set of plans that can be
reached by replacing a window with a new subplan  the local search is plain hill climbing 
each move is to a strictly better neighbouring plan  as in other lns algorithms  searching
for a better plan in the neighbourhood is done by formulating local optimisation problems 
which are solved using bounded cost subplanners 
block deordering  described in the previous section  helps identify candidate windows
by providing a large set of possible plan linearisations  the block decomposition is also used
by some of our windowing strategies  each window is a subsequence of some linearisation
of the block deordered input plan  however  we represent a window in a slightly different
   

fisiddiqui   haslum

way  by a partitioning of the blocks into the part to be replaced  w   and those ordered
before  p  and after  q  that part 
definition    let bdp    s  b    be a block decomposed p o  plan  a window in bdp
is a partitioning of b into sets p  w  q  such that bdp has a linearisation consistent with
 bp  bw  bq   bp  p  bw  w  bq  q  
each window defines a subproblem  which is the problem of finding a plan that can
fill the gap left by removing the steps in w from a linearisation of bdp consistent with the
window  this problem is formally defined as follows 
definition    let bdp    s  b    be a block decomposed p o  plan for planning problem
  hp  w  qi a window in bdp   and s            s p    s p              s p   w    s p   w              sn a linearisation of bdp consistent with that window  the subproblem corresponding to hp  w  qi 
sub   has the same atoms and actions as   the initial state of sub   isub   is the result of
progressing the initial state of  through s            s p   i e   applying s            s p  in i   and the
goal of sub   gsub   is the result of regressing the goal of  through sn           s p   w     
theorem    let bdp    s  b    be a block decomposed p o  plan for planning problem   hp  w  qi a window in bdp   sub the subproblem corresponding to the window 
and s            s p    s p              s p   w    s p   w              sn the linearisation that sub is constructed
 
 
    s            s  be a plan for 
from  let w
sub   then s            s p    s            sk   s p   w              sn
 
k
is a valid sequential plan for  
proof  the proof is straightforward  the subsequence s            s p  is applicable in the initial
state of   i  and  by construction of sub   results in the initial state of sub   isub   hence
s            s p    s             s k is applicable in i  and  again by construction of sub   results in a state
sg that satisfies the goal of sub   gsub   since gsub is the result of regressing the goal of  
g  through s p   w              sn in reverse  it follows that this subsequence is applicable in sg  
and applying it results in a state satisfying g   for the relevant properties of regression 
see  for example  ghallab  nau    traverso        section        
the subproblem corresponding to a window hp  w  qi always has a solution  in the form
of a linearisation of the steps in w  to improve plan quality  however  the replacement
subplan must have a cost that is strictly lower than the cost of w  c w   this amounts to
solving bounded cost subproblems  the subplanners we have used for this in bdpo  are
described in section      we return to the question of when and how multiple windows
within the same plan can be simultaneously replaced in section     
algorithm   describes how bdpo  performs one step of the local search  by exploring
the neighbourhood of the current plan  the first step is to block deorder the current plan
 line     next  optimisation using a bounded cost subplanner is tried systematically on
candidate windows  lines       until a restart condition is met  line      until no more
local improvements are possible  or until a time limit is reached  a point of difference
with other lns algorithms is that we have used delayed restart  meaning that exploration
of the neighbourhood can continue after a better plan has been found  this helps avoid
local minima  by driving exploration to different parts of the current plan  the restart
conditions  and the impact they have on the local search  are described in section     
   

ficontinuing plan quality optimisation

algorithm   the neighbourhood exploration procedure in bpdo  
   procedure bdpo  in   tlimit   banditpolicy  rankpolicy  optsubprob 
  
initialize  telapsed      last   in   triallimit     n       windowdb   
  
bdp   blockdeorder in  
  
while telapse   tlimit and last is not locally optimal do
  
if more windows needed then
  
extractmorewindows bdp   windowdb  optsubprob 
  
  
  
   
   
   
   
   
   
   
   
   
   
   

p   selectplanner banditpolicy 
w   selectwindow p  rankpolicy  triallimit  windowdb 
if w   null and no more windows to extract then triallimit p      
if w   null then continue
wnew   searchresult   optimisewindow p  w 
updatewindowdb p  w  wnew   optsubprob  searchresult  windowdb 
if c wnew     c w  then
new   merge bdp   windowdb 
if c new     c last   then last   new
updatebanditpolicy p  w  wnew   searchresult  banditpolicy 
updaterankpolicy p  searchresult  rankpolicy 
if c last     c in   and restart condition is true then
return bdpo   last   tlimt  telapsed   banditpolicy  rankpolicy  optsubprob 
return last

a key design goal of the procedure is to avoid unproductive time  meaning spending
too much time in one step or trying to optimise one window while other options that could
lead to an improvement are left waiting  therefore  all steps are done incrementally  with
a time limit on any step that could take an unbounded time 
a database  windowdb  stores each unique window extracted from the block deordered
plan  and records its status  how many times optimisation of this window has been tried
with each subplanner and the result   and structural summary information about the window  the window database is populated incrementally  lines      by applying different
windowing strategies with a limit on the time spent and the number of windows added 
the limits we have used are     seconds and    windows  respectively  this balances time
between window extraction and optimisation  to prevent the procedure spending unproductive time  the windowing strategies are described in section    we also compute a lower
bound on the cost of any replacement plan for the window  using the admissible lm cut
heuristic  helmert   domshlak         a window is proven optimal if the current subplan cost equals this bound  or a previous attempt to optimise the window exhausted the
bounded cost search space  already optimal windows are  of course  excluded from further
optimisation  more windows are added to the database when the number of windows eligible to be selected for optimisation by any one subplanner  defined in the next paragraph 
drops below a threshold  we have used     of the current window database size as the
threshold 
   

fisiddiqui   haslum

the subplanner to use is selected using the ucb  multi armed bandit policy  auer et al  
       which learns over repeated trials to select more often the subplanner that succeeds
more often in finding improvements  the next window to try is chosen  among the eligible
ones in the database  according to a ranking policy  windows eligible for optimisation by
the chosen subplanner are those that     are not already proven optimal      have not been
tried with the chosen subplanner up to its current trial limit  and     do not overlap with
any improved window already found  the ranking policy is a heuristic aimed at selecting
windows more likely to be improved by the chosen subplanner  we use several ranking
policies and switch from one to the next when the subplanner fails to find an improvement
for a number of consecutive tries  since this indicates the current ranking policy may not be
recommending the right windows for the current problem  the threshold we have used for
switching the ranking policy is      this is     of the maximum number of windows added to
the window database in each call to extractmorewindows   the ranking policies are
described in section      the subplanner is given a time limit  which is increased each time
it is retried on the same window  we have used a limit of    seconds  increasing by another
   seconds for each retry  a limit on the number of times it can be retried on the same
window is kept for each subplanner  initially set to    the limit is increased only when the
subplanner has been tried on every window in the database  excluding windows that have
already been proven optimal or that overlap with windows for which a better replacement
has been found  and no strategy can generate more new windows  line     if a lower cost
replacement subplan for the window is found  this together with all improvements already
found in the current neighbourhood are fed into the merge procedure  which tries to
combine several replacements to achieve a greater overall plan cost reduction  the merge
procedure is described in section     
when the procedure restarts with a new best plan  the learned bandit policy for subplanner selection and the current ranking policy  for each subplanner  are carried over to
the next iteration  we also keep a database of the subproblems  defined by their initial
state and goal  whose plan cost has been proven optimal  to avoid trying fruitlessly to optimise them further  the window database  which contains only information specific to the
current input plan  is reset 
the remainder of this section is organised as follows  the next two sections describe the
settings that we have used for our experiments and an overview of main results  respectively 
we then describe the subplanners used in bdpo   section       the restart conditions
 section      and the merge procedure  section       section     discusses the impact of
block deordering on the performance of the system  the windowing strategies and ranking
policies are described in section    while more details of the on line adaptation methods
used are presented in section   
    experiment setup
before presenting the overview of results  we outline below the three different experimental
setups that we have used  for experiment setup   and   we used     large scale instances
from    ipc domains  the selection of domains and instances is described below  for
experiment    we included additional medium sized instances for a total of     instances
from the same    domains  we used all domains from the sequential satisficing track of
   

ficontinuing plan quality optimisation

the             and      ipc  except for the cybersec  cavediving and citycar domains 
 the cybersec domain is too slow for our system to parse  the other two have conditional
effects  which our implementation does not handle   we also used the alarm processing for
power networks  appn  domain  haslum   grastien         the plans used as input to
bdpo  are the plan produced by ibacop   cenamor  de la rosa    fernandez        in
the      ipc for the problems from that competition  and the best plan found by lama
 richter   westphal        ipc      version  in   hour cpu time for all other problems 
we refer to these as the base plans  for experiments   and    we selected from each domain
the    last instances for which a base plan exists   in some domains less than    instances
are solved by lama ibacop   which is why the total is     rather than       for domains
that appeared in more than one competition  we used instances only from the ipc      set 
all experiments were run on   core     ghz amd cpus with  m l  cache  with an  
gb memory limit for every system  when comparing the anytime performance of bdpo 
and other systems that require an input plan  we count the time to generate each base plan
as   hour cpu time  this is the maximum time allocated to generating each base plan 
most of them were found much more quickly 
in our first experiment  we did not use the bdpo  system  instead  we ran each of two
subplanners  pngs and ibcs  for up to    seconds on every subproblem corresponding to
a window extracted  by our six windowing strategies  from all base plans  excluding only
subproblems for which the window was proven optimal by the lower bound obtained from
the admissible lm cut heuristic  helmert   domshlak         this experiment provided
information to inform the design of the combined window extraction procedure  the window
ranking policies  and other aspects of the system  we do not present its results here  but
will refer to it later when we discuss these system components in detail 
in experiment    we compare bdpo  and eight other anytime planners and plan optimisation systems  lama  richter   westphal        ipc      version   aees  implemented
in the fast downward code base  cf  thayer et al       b   ibcs  as described in section
      beam stack search  bss   zhou   hansen         pngs  including action elimination  nakhost   muller         ibacop   cenamor et al          lpg  gerevini  
serina         and arvand  nakhost   muller         bdpo  uses pngs and ibcs as
subplanners  and is configured as described above  aees uses lm cut  helmert   domshlak        as its admissible heuristic  and the ff heuristic  with and without action costs
for its inadmissible estimates  bss uses the lm cut heuristic  our implementation of bss
does not use divide and conquer solution reconstruction  and was run with a beam width
of      the other systems are described further in section   
each system was run for up to   hours cpu time per problem  bdpo  and pngs
both use the base plans as input  and ibcs and beam stack search both use the base
plan cost as the initial cost bound  as mentioned above  we allocated   hour cpu time
for generating each base plan  therefore  when comparing these systems with planners
starting from scratch  lama  aees  ibacop   lpg and arvand   we add a   hour start
up delay to their runtime  beam stack search is much slower than the other planners used
in the experiment  therefore  we ran it for up to    hours cpu time  and in reporting its
results we divide its runtime by    in other words  the results shown are for a hypothetical
implementation of beam stack search that does the same amount of search  but faster by
a constant factor of   
   

fisiddiqui   haslum

experiment   uses the same setup as experiment    except that the input to bdpo  is
the best plan found by running pngs for up to   hour cpu time  with an   gb memory
limit  on the base plans   as mentioned previously  in the vast majority of cases pngs runs
out of memory in much less time than that  but in a few cases it does run up to the   hour
limit   we use this setup primarily to run different configurations of bdpo  to analyse
the impact of different designs  e g   the planner selection and window ranking policies 
immediate vs  delayed restart  and so on  in a setting where input plans are already of good
quality  when comparing the anytime result of bdpo  in this experiment to the other
systems  we add   hours to its runtime 
    overview of results
figure    shows a headline result  in the form of the average plan quality achieved by
bdpo  and other systems as time per problem increases  the ipc quality score of a plan
is calculated as cref  c  where c is the cost of the plan and cref is the cost of the best plan
for the problem instance found over all runs of all systems used in our experiments  thus 
a higher score reflects a lower cost plan  the results in figure    are from experiment  
and    described in the previous section  it is the same as shown in figure    on page      
but including results for all the compared anytime planning systems  none of the planners
starting from scratch find a solution to all     problems  lama solves     problems 
ibacop       arvand      aees    and lpg     for these planners  the average quality
score shown in figure    is the average over only those problems for which they have  at
that time  found at least one plan   as previously mentioned  this is also the reason why the
average quality sometimes falls  when a first plan  of low quality  for a previously unsolved
problem is found  the average can decrease   in other words  this metric is unaffected by
the differences in coverage  likewise  none of the post processing or bounded cost search
methods improve on all base plans  bdpo  finds a plan of lower cost than the base plan
for     problems  pngs for      ibcs for    and beam stack search for     for these
systems  the average quality shown in figure    is taken over all     problems  using the
base plan quality score for those problems that a system has not improved on 
the majority of the compared systems show a trend similar to that of lama  i e  
improving quickly early on but then flattening out and ultimately stagnating  the reasons
vary  memory is a limiting factor for some algorithms  notably pngs  which exhausts the
  gb available memory before reaching the   hour cpu time limit for       of problems 
and lama  which does the same for     of problems  aees runs out of memory on just
over     of problems  on the other hand  planners that use limited memory algorithms 
such as beam stack search  lpg and arvand  both of which use local search   never run
out of memory and thus could conceivably run indefinitely  however  the rate at which they
find plan quality improvements is small  from   to   hours  the average quality produced by
lpg and arvand increases by        and         respectively   the latter excludes three
problems that were solved by arvand for the first time between   and   hours  including
those brings the average down  making the increase less than         the increase in average
quality achieved by bdpo   starting from the high quality plans generated by pngs from
the base plans  over the same time interval is        
   

ficontinuing plan quality optimisation

    
    
    


average quality score  relative ipc quality score   coverage 

   













































































































































    
    



    





    
   
    
    
    
    
   
    
    
    



    



bdpo  on pngs on base plans
bdpo  on base plans
pngs on base plans
ibcs on base plans
bss on base plans
lama from scratch
aees from scratch
ibacop  from scratch
arvand from scratch
lpg from scratch

   
    



    
    










































































 

   

 

   

 

   

 

   

 

   

 

   



   



 

   



 

    





time  hours 

figure     average ipc quality score as a function of time per problem  on a set of    
large scale planning problems  the quality score of a plan is cref c  where c is the cost of the
plan and cref is the least cost of all plans for the problem  hence a higher score represents
better plan quality  the lama  aees  lpg  arvand and ibacop  planners start from
the scratch  whereas the post processing  pngs  bdpo   and bounded cost search  ibcs 
beam stack search  methods start from a set of base plans  their curves are delayed by  
hour  which is the maximum time allocated to generating each base plan  the experimental
setup is described in detail in section     

   

fisiddiqui   haslum

bdpo 
bdpo 
on pngs
           
appn
  
     
  
barman
      
  
childsnack
      
  
elevators
     
     
floortile
  
     
ged
  
  
hiking
  
     
maintenance    
   
nomystery
   
       
   
openstacks
parcprinter
   
      
  
  
     
parking
scanalyzer
     
     
sokoban
   
   
tetris
     
     
thoughtful
     
     
tidybot
  
  
transport
     
     
visitall
     
     
woodworking      
     
overall
               
domains

lama

aees

arvand

lpg

ibcs

bss

pngs

ibacop 

                                             
  
  
  
           

 

  
  

  
  

     
     

     
     

  

  

  
  
  

  

     

  

  

  

  

     

  

     

  

  

     
     

     

  
     

  

  

  

  

     
  
  
  
     
  

     

  

  

  

     
     

 

 

 

 

 

 

 

             

     
   

 

table    for each plan improvement method  the percentage of instances where it found a
plan of cost matching the best plan      found a plan strictly better than any other method
     and found a plan that is known to be optimal  i e   matched by the highest lower bound
     the percentage is of the same instances in each domain shown in figure      zeros are
omitted to improve readability   bdpo  on pngs is the result of bdpo  in experiment
   the other results are from experiment    see section      

we draw two main conclusions  first  bdpo  achieves the aim of continuing quality
improvement even as the time limit grows  in fact  it continues to find better plans  though
at a decreasing rate  even beyond the   hour time limit used in this experiment  second  the
combination of pngs and bdpo  achieves a better result than either does alone  partly
this is because they work well on different sets of problems and the figure is showing an
average  but bdpo  sometimes produces a better result when started with the best plan
found by pngs also in domains where bdpo  already outperforms pngs when both start
from the same base plans  e g   elevators and transport   however  we have also seen the
opposite in some domains  e g   floortile and hiking   where starting bdpo  with a worse
input plan often yields a better final plan  this can be seen in figure     which provides
a more detailed view  it shows for each problem the cost of the best plan found by each
system at the   hour total time limit  scaled to the interval between the base plan cost
and the highest known lower bound  hlb  on any plan for the problem   lower bounds
were obtained by a variety of methods  including several optimal planners  cf  haslum 
          of the     problems are excluded from figure     in   cases  the base plan cost
already matches the lower bound  so no improvement is possible  for another    problems 
no method improves on the base plans within the stipulated time   the pegsol domain
does not appear in the graph  because all base plans but one are optimal  and no method
improves on the cost of the last one  
   

ficontinuing plan quality optimisation

base plans















 

























best cost achieved  normalised 



































nomystery

maintenance

ged

floortile

barman

hiking



elevators



childsnack



appn

hlb







lama from scratch
aees from scratch
arvand from scratch
lpg from scratch
ibacop  from scratch
pngs on base plans
ibcs on base plans
bss on base plans
bdpo  on base plans
bdpo  on pngs on base plans

base plans









 


























best cost achieved  normalised 





 
 


























hlb



lama from scratch
aees from scratch
arvand from scratch
lpg from scratch
ibacop  from scratch
pngs on base plans
ibcs on base plans
bss on base plans
bdpo  on base plans
bdpo  on pngs on base plans
woodworking

visitall

transport

tidybot

thoughtful

tetris

sokoban

scanalyzer

parking

parcprinter

openstacks

 



figure     best plan cost  normalised to the interval between the cost of the base plan
and the corresponding highest known lower bound  achieved by different anytime plan
optimisation methods in experiment    and by bdpo  in experiments        see section
     

   

fisiddiqui   haslum

table   provides a different summary of the information in figure     showing for each
domain and system the percentage of instances for which it found a plan with a cost    
matching the best plan for that instance      strictly better than any other method  and
    matching the lower bound  i e   known to be optimal  in aggregate  the combination
of bdpo  after pngs over the base plans achieves the best result on all three measures 
however  in   domains  ged  hiking  openstacks  parking  and tidybot   lama finds
more plans that are strictly better than any other method  we tried using lama as
one of the subplanners in bdpo   but this does not lead to better results overall  in some
domains  such as openstacks and ged  the smallest improvable subplan is often the whole 
or almost the whole  plan  and lama finds an improvement of the plan only after searching
for a longer time  although bdpo  increases the time limit given to subplanners with each
retry  the average time limit  across all local optimisation attempts in this experiment  is
only       seconds  thus  our strategy of searching for quick improvements of plan parts
does not work well in these domains 
    subplanners used for window optimisation
the subplanners used by bdpo  are used to find a plan for the window subproblem  as
stated in definition    with a cost less than the cost of the current window  c w   we have
considered three subplanners 
    iterated bounded cost search  ibcs   using a greedy search with an admissible heuristic for pruning 
    plan neighbourhood graph search  pngs   including the action elimination technique  nakhost   muller        
    restarting weighted a   richter et al          as implemented in the lama planner 
however  in the experimental setups described in the previous section  bdpo  uses only
two subplanners  ibcs and pngs  there are two reasons for choosing these two  first 
they show good complementarity across domains  for example  ibcs is significantly better than pngs in the appn  barman  floortile  hiking  maintenance  parking  sokoban 
thoughtful and woodworking domains  while pngs is better in the elevators  scanalyzer 
tetris  transport and visitall domains  second  the learning policy that we use for subplanner selection learns faster with a smaller number of options  therefore  adding a third
subplanner will only improve the overall performance of bpdo   given a limited time per
problem  if that subplanner complements the other two well  i e   it performs well on a
significant fraction of instances where both the other two do not  on the set of benchmark
problems used in our experiment  this was not the case   a different set of benchmarks
could of course yield a different outcome   an experiment comparing the effectiveness of all
three subplanners  individually as well as the combination of ibcs and pngs under the
learning policy  in bdpo  is presented in section     on page     
to solve the bounded cost problem  ibcs uses a greedy best first search guided by the
unit cost ff heuristic  pruning states that cannot lead to a plan within the cost bound
using the f value based on the admissible lm cut heuristic  helmert   domshlak        
it is implemented in the fast downward planner  the search is complete  if there is no plan
   

ficontinuing plan quality optimisation

within the cost bound  it will prove this by exhausting the search space  given sufficient
time and memory  the bounded cost search can return any plan that is within the cost
bound  to get the best subplan possible within the given time limit  we iterate it  whenever
a plan is found  as long as time remains  the search is restarted with the bound set to be
strictly less than the cost of the new plan 
pngs  nakhost   muller        is a plan improvement technique  it searches a subgraph of the state space around the input plan  limited by a bound on the number of states 
for a lower cost plan  if no better plan is found the exploration limit is increased  usually
doubled   this continues until the time or memory limit is reached  like with ibcs  we
iterate pngs to get the best subplan possible within the given time limit  if it improves
the current subplan  the process is repeated around the new best plan 
lama  richter   westphal        finds a first solution using greedy best first search 
it then switches to rwa   richter et al         to search for better quality solutions 
    restart
the restart condition determines a trade off between exploring the neighbourhood of the
current solution and continuing the local search into different parts of the solution space 
the most obvious choice  and the one used in other lns algorithms  is to restart with the
new best solution as soon as one is found  we call this immediate restart  however  we have
found that continuing to explore the neighbourhood of the current plan even after a better
plan has been found  and merging together several subplan improvements  as described in
section     below  often produces better results  we call this delayed restart 
setting the right conditions for when to make a delayed restart is critical to the success
of this approach  we have used a disjunction of two conditions  first  if the union of
improved windows found in the neighbourhood covers     of the steps in the input plan 
recall that when we continue the exploration loop  algorithm    after an improvement has
been found  windows that overlap with any already improved window are excluded from
further optimisation  this drives the procedure to search for improvements to different
parts of the current plan  and helps avoid a certain myopic behaviour that can occur
with immediate restarts  when restarting with the new best plan  we get a new block
decomposition and a new set of windows  this can lead to attempting to re optimise the
same part of the plan that was just improved  even over several restarts  which may lead to a
local optimum that is time consuming to escape  the second condition is that    consecutive
subplanner calls have failed to find any further improvement  the threshold of    is three
times the threshold for switching the ranking policy  cf  description of algorithm   at the
beginning of this section   this means that after    attempts we have tried to optimise
the    most promising windows  among the remaining eligible ones  recommended by all
ranking policies  without success  this suggests there are no more improvable windows
to be found  or that none of our ranking policies are good in the current neighbourhood 
making a restart at this point allows the exploration to return to parts of the plan that
intersect already improved windows  thus increasing the set of eligible windows 
the average plan quality  as a function of time per problem  achieved by bdpo  using
immediate restart and delayed restart based on the conditions above is shown by the top
two lines in figure     page       in this experiment  both configurations were run using
   

fisiddiqui   haslum

algorithm   merge improved windows
   procedure merge bdp   windowdb 
  
initialise bdp   bdp
  
w   improved windows from windowdb sorted by cost reduction  c w   c wnew   
  
while w     do
  
 hp  w  qi  wnew     pop window with highest c w   c wnew   from w
  
bdp   replaceifpossible bdp   hp  w  qi  wnew  
  
w   removeconflictingwindows w  bdp  
  

return bdp

the same setup as experiment    described in section     on page      as can be seen 
delayed restart yields better results overall  compared to bdpo  with immediate restart 
it achieves a total improvement that is     higher  however  we found immediate restart to
work better for a few instances  especially in the visitall and woodworking domains  where
bdpo  with immediate restart found a better final plan for nearly     of the instances 
the average number of iterations  i e   steps in the lns  done by bdpo  using the
delayed restart condition is      per problems across all the domains considered in the
experiment  the highest average in a single domain is      in thoughtful solitaire  with
immediate restart the average over all domains increases to       in other words  both
configurations of bdpo  spend significant time exploring the neighbourhood of each plan 
the anytime performance curve in figure    shows that the additional time spent in each
neighbourhood when using delayed restarts pays off 
    merging improved windows
delayed restarting would not have any benefit without the ability to simultaneously replace
several improved windows in the current plan  the improved windows are always nonoverlapping  because once a better subplan for a window is found  windows that overlap
with it are no longer considered for optimisation  but their corresponding subproblems may
have been generated from different linearisations of the block deordered plan  because of
this  the replacement subplans may have additional preconditions or delete effects that the
replaced windows did not  or lack some of their add effects  thus  there may not be a
linearisation that permits two or more windows to be simultaneously replaced 
the merge procedure shown in algorithm   is a greedy procedure  it maintains at all
times a valid block deordered plan  bdp    meaning that each precondition of each block
is supported by a causal link with no active threat   recall that a block in this context
can be block that consists of a single step   initially  this is the input plan  bdp    for
which causal links  and other ordering constraints  are computed by block deordering  the
procedure gets the improved windows  w   from the window database  and tries to replace
them in the current plan bdp in order of their contribution to decreasing plan cost  i e  
the cost of the replaced window  c w   minus the cost of the new subplan  c wnew    
the first replacement always succeeds  since  by construction of the subproblem  there
is a linearisation of the input plan in which wnew is valid  cf  theorem     subsequent
replacements may fail  in which case merge proceeds to the next improved window in w  
   

ficontinuing plan quality optimisation

since replacing a window with a different subplan may impose new ordering constraints 
any remaining improved windows that conflict with partial order of the current plan are
removed from w  
the replaceifpossible function takes the current plan  bdp    and returns an updated plan  which becomes the current plan   or the same plan if the replacement is not
possible  the replacement subplan  wnew   is made into a single block whose steps are totally ordered  the preconditions and effects of this block  and those of the replaced window
 w   are computed according to definition    page       for any atom in pre wnew   that
is also in w  the existing causal link is kept  likewise  causal links from an effect in add w 
that are are also in add wnew   are kept  these links are unthreatened and consistent with
the order  since the plan is valid before the replacement  for each additional precondition
of the new subplan  m  pre wnew     pre wi    and for each causal link hbp   m  bc i in bdp
where the producer is in the replaced window  bp  w   the consumer is not  bc   w   and
the atom of the link is not produced by the replacement subplan  m   add wnew     a new
causal link must be found  given a consumer  bc   and an atom it requires  m  pre bc    
the procedure tries the following two ways of creating an unthreatened causal link 
 c   if there is a block b    bc with m  add b     and for every threatening block  i e  
b   with m  del b       either b    b  or bc  b   can be added to the existing plan ordering
without contradiction  then b  is chosen  and the ordering constraints necessary to resolve
the threats  if any  are added 
 c   otherwise  if there is a block b  with m  add b    that is unordered w r t  bc   and for
every threatening block either b    b  or bc  b   can be enforced  then b  is chosen  and the
causal link  implying the new ordering b   bc   and threat resolution ordering constraints
 if any  are added to the plan 
the two are tried in order  c  first and c  only if c  fails  if neither rule can find the
required causal link  the replacement fails  wnew may also threaten some existing causal
links in bdp that w did not  for each threatened link  hbp   m  bc i  the procedure tries to
resolve the threat in three ways 
 t   if the consumer bc was ordered before w in the linearisation of the corresponding
subproblem  bc  p   and bc  wnew is consistent  the threat is removed by adding this
ordering 
 t   if the producer bp was ordered after w in the linearisation of the corresponding subproblem  bp  q   and wnew  bp is consistent  the threat is removed by adding this ordering 
 t   if a new  unthreatened causal link supplying m to bc can be found by one of the two
rules c  or c  above  the threatened link is replaced with the new causal link 
the rules are tried in order  and if none of them can resolve the threat  the replacement
fails 
some non basic ordering constraints between blocks not in w may disappear when w
is replaced with wnew   likewise  some ordering constraints between w and the rest of the
plan may become unnecessary  because wnew may not delete every atom that w deletes and
may not have all preconditions of w  and thus can be removed  this may make pairs of
blocks b  b  in the plan that were ordered before the replacement unordered  and thus create
new threats  all such new threats are checked by replaceifpossible  and if found are
resolved by restoring the ordering constraint that was lost 
   

fisiddiqui   haslum

lemma    if the current plan bdp is valid  and wnew solves the subproblem corresponding
to window hp  w  qi  the plan returned by replaceifpossible is valid 
proof  the procedure ensures that every precondition of every step is supported by a causal
link with no active threat  such a link either existed in the plan before replacement  and
any new threats to it created by the replacement are resolved by ordering constraints   or
was added by the procedure  thus  if the replacement succeeds  the resulting plan is valid
according to theorem    if the replacement fails  the plan returned is the current plan 
bdp   unchanged  which is valid by assumption 
theorem    if the input plan  bdp is valid  then so is the plan returned by merge 
proof  immediate from lemma   by induction on the sequence of accepted replacements 

    the impact of plan decomposition
the neighbourhood explored in each step of the lns in bdpo  is defined by substituting
improved subplans into the current plan  each subplan considered for local optimisation is
a subsequence of some linearisation of the block deordering of the current plan  obviously 
we can also restrict windows to be consecutive subsequences of the totally ordered input
plan  in fact  similar approaches to plan optimisation have adopted this restriction  ratner
  pohl        estrem   krebsbach        balyo  bartak    surynek         in this section 
we address the question of how much the block deordering contributes to the performance
of bdpo  
in the preliminary experiment  setup    as described in section     on page      we
observed that more than     of the subproblems for which an improved subplan was found
correspond to a non consecutive part of the sequential input plan  however  this in itself does not prove that optimising only the     of subplans that can be found without
deordering would not lead to an equally good end result 
therefore  we conducted another experiment  using the same setup as experiment    described in section       in this experiment  we ran bdpo  separately with different degrees
of plan decomposition      with block deordering  as in the default bdpo  configuration 
the one used in experiments   and   presented in section     on page           with standard  i e   step wise  plan deordering only  in this configuration  we used kambhampati
and kedars        algorithm  described in section      for plan deordering      without
any deordering  i e   passing the totally ordered input plan directly to the lns process 
in addition  each of these configurations was run once with immediate restarting and once
with delayed restarting  as described in section     
figure    shows the average ipc plan quality score as a function of time per problem
achieved by each of these configurations of bdpo   it shows a simple and clear picture 
with immediate restart  lns applied to block deordered plans outperforms lns applied
to step wise deordered plans  which in turn outperforms its use on totally ordered plans 
the total improvement  as measured by the increase in the average ipc plan quality score 
achieved by bdpo  without deordering is       less than what is achieved by the best
configuration  we can also see that deordering is an enabler for delayed restarting  with
block and step wise deordering  delayed restarting further boosts the performance of lns
   

ficontinuing plan quality optimisation

     












     








     







    



     










     






















     









     



    

 

   

 

   

 

   

 

     

 



   



 



     

bdpo  with delayed restart over block deordered plans
bdpo  with immediate restart over block deordered plans
bdpo  with delayed restart over standard partially ordered plans
bdpo  with immediate restart over standard partially ordered plans
bdpo  with delayed restart over totally ordered plans
bdpo  with immediate restart over totally ordered plans
   



 




   

average quality score  relative ipc quality score   coverage 



time  hours 

figure     average ipc quality score as a function of time per problem for bdpo  applied
to the totally ordered input plan  the standard  step wise  deordering of the plan  and the
block deordering of the plan  for each plan type  the system was run in two configurations  once with delayed restarting and once with immediate restarting  cf  section     on
page       this experiment was run with setup    as described in section     on page     
the time shown here is only the runtime of bdpo   i e   without the   hour delay for
generating the input plans  as shown in figure      note also that the y axis is truncated 
all curves start from the average quality of the input plans  which is       

   

fisiddiqui   haslum

plan optimisation by     and        respectively  while on totally ordered plans it has no
significant effect 
deordering increases the number of linearisations and therefore enables many more
distinct candidate windows to be created  however  recall that bdpo s neighbourhood
exploration procedure  algorithm    interleaves incremental window generation with optimisation attempts  many of the windows that could be generated from the current plan may
never be generated before a restart occurs  thus  the average number of windows generated
in each iteration does not reflect the difference in performance   with block deordering  the
average number of windows generated is         of which        remain after filtering  while
on the totally ordered plans it is        and        after filtering  both are using immediate
restart   but deordering helps the windowing strategies generate windows that are more
easily optimised  recall that neighbourhood exploration will retry the same subplanner on
the same window  with a higher time limit  only after all windows have been tried by that
subplanner  the average number of optimisation attempts  using either subplanner  on each
window selected for optimisation at least once  is around     when either block deordering
or standard deordering is used on the input plan  without any deordering  however  the
average number of attempts is higher  and very high in a few domains  leaving out the
highest    of neighbourhoods encountered  the average is slightly more than    in more
than     of the plan neighbourhoods the average number of attempts is over    and in a
few cases more than     in other words  generating windows from a totally ordered plan
causes the procedure to spend  on average  more time on each window before an improving
plan is found 
on the other hand  as noted in section      in some domains subplanners need more
runtime to find better plans for improvable windows  and the bdpo  configuration without
deordering does find a better plan than the default configuration for    of the     problems 
in the current bdpo  system  the subplanner time limit is increased only when a window
is retried  a procedure that either attempts candidate windows more likely to be improved
 for example  as indicated by the window ranking policies described in section      more
frequently  or varies the amount of time given to optimise each window may perform better 
the optimal amount of deordering to do on each plan may well be different from problem to problem  but averaged across the set of benchmark problems  more deordering is
unarguably better than none 

   windowing strategies and ranking of windows
a window is a subplan of some linearisation of the block deordered plan  extracted in order
to attempt local optimisation  this section describes the strategies we use to generate and
rank windows  and an experimental evaluation of their impact on our systems performance 
recall from definition    page      that a window is represented by a triple hp  w  qi 
where w is the set of blocks to be replaced  and p and q are the sets of blocks ordered
before and after w  respectively  in the linearisation  a block decomposed p o  plan can
have many linearisations  producing many possible windows  typically far too many to
attempt to optimise them all  a windowing heuristic is a procedure that extracts a reduced
set of windows  hopefully including the most promising ones  in a systematic way  we
   

ficontinuing plan quality optimisation

figure     a block deordered plan and its transformation into extended blocks  blocks b 
and b  are merged into a single block  as are blocks b  and b  

windowing heuristics
rule based
cyclic thread
causal followers

generated
basic
ext 
   
  
  
  
  
  

not filtered out
basic
ext 
  
  
  
  
  
  

improved
basic
ext 
  
 
    
  
    
 

impr  gen 
basic
ext 
    
    
    
    
    
    

table    the total number  in thousands  of windows that were generated  not filtered out 
and finally improved  using different windowing heuristics over different block types  basic
and extended   the number of possible windows over the sequential input plans  before
even considering deordering  is over      million  the rightmost pair of columns shows the
rate of success  meaning the fraction of improved windows out of the generated windows 
the numbers are from the results of experiment    described in section     on page      
present three windowing heuristics  called the rule based  cyclic thread  and causal followers
heuristics  each of them is described in detail in the following subsections 
each heuristic is applied over two types of block  basic and extended  one at a time 
basic blocks are the blocks generated by a block deordering   for the purpose of windowing 
any step that is not included in a block created by block deordering is considered to be
a block on its own   extended blocks are created by merging basic blocks in the block
deordered plan that form complete non branching subsequences  if block bi is the only
immediate predecessor of block bj   and bj the only immediate successor of bi   they are
merged into one extended block  algorithm   shows the procedure for extended block
formation   ip b  denotes the set bs immediate predecessors  while is b  is bs immediate
successors  
algorithm   computing extended blocks 
   bext  bbasic
   while bi   bj  bext   ip bj      bi    is bi      bj   do
  
bext  bext   bi  bj      bi   bj  
the process is further illustrated by an example in figure     note that blocks b 
and b  are not merged into one extended block  this is because although b  is the only
immediate successor of b   b  is not the only immediate predecessor of b   extended blocks
are useful because they allow some windowing heuristics to capture larger windows  our
experiment results show that windows of different sizes are more useful in different domains 
   

fisiddiqui   haslum

algorithm   extract more candidate windows
   global array strategy       stores the state of each windowing strategy   
   procedure extractmorewindows bdp   windowdb  optsubprob 
  
w  
  
tlimit   initial time limit tincrement
  
while telapsed   tlimit or  w     nwindowslimit do
  
i   nextwindowingstrategy  
  
if i   null then break    all windowing strategies are exhausted   
  
w   strategy i  getwindows bdp   windowdb  optsubprob 
nwindowslimit   w    tlimit  telapsed  
  
if telapsed  tlimit and w    then tlimit    tincrement
  

windowdb insert w  

for example  larger windows are more likely to be improved in the pegsol  openstacks and
parcprinter domains  while optimising smaller windows is better in the elevators  transport 
scanalyzer and woodworking domains 
a windowing strategy is a windowing heuristic applied to a block type  thus  we use
a total of six different strategies  each of these strategies contributes some improvable
windows that are not generated by any of the other strategies  cf  section      and in
particular table   on page       thus  all of them are  in this sense  useful  on the other
hand  the size of the set of windows that each generates and the fraction of improvable
windows in this set varies between the strategies  and in that sense some are more useful
than others 
table   shows results from our first experiment  in which we systematically tried two
subplanners  pngs and ibcs  on every window generated  and not filtered out  by any
windowing strategy over     input plans  the table shows the total number  in thousands 
of windows that were generated  that remain after filtering  and that were finally improved
by at least one of the two subplanners  in this experiment  windows were filtered out only
if the window cost matched the lower bound given by the admissible lm cut heuristic
 helmert   domshlak         the experiment setup is further described in section      on
page       the first observation is that all strategies are very selective  the number of
windows that could potentially be generated  even without considering deordering  i e   only
by taking all subsequences of the totally ordered input plans  is over      million  thus 
even the most prolific strategy generates less than a tenth of the possible windows  second 
we used the rate of success  meaning the fraction of windows generated that were improved
by any of the subplanners used in the experiment  to order the strategies  the order is as
follows 
   rule based heuristic over extended blocks 
   cyclic thread heuristic over basic blocks 
   cyclic thread heuristic over extended blocks 
   causal followers heuristic over basic blocks 
   rule based heuristic over basic blocks 
   causal followers heuristic over extended blocks 
   

ficontinuing plan quality optimisation

the neighbourhood exploration procedure  algorithm   on page      adds windows to
the database incrementally  by calling the extractmorewindows procedure shown in
algorithm    this procedure selects the next strategy to try  cycling through them in
the order above  and asks this strategy to generate a specified number of windows  in
a limited time  each strategy keeps its own state  what part of the heuristic has been
applied and up to what part of the plan   so that the next time it is queried it can resume
generating new windows  when all windows that are possible under a given strategy have
been generated  we say the strategy is exhausted  the windowing strategies discard    
windows that are known to be optimal  either because their cost matches the lower bound
given by the admissible lm cut heuristic  helmert   domshlak         or because they
are in the stored set of optimally solved subproblems  and     windows that overlap with an
already improved window  these windows are not eligible for optimisation  cf  section    
so generating them is redundant  if the selected strategy finishes without generating enough
windows and time remains  the next not yet exhausted strategy in the order is queried  and
so on  until either  w     nwindowslimit or time is up  if no windows are generated  and
some strategies are still not exhausted  the time limit is increased 
    rule based windowing heuristic
our first version of bdpo  siddiqui   haslum      b  used a single windowing strategy 
based on applying a fixed set of rules over extended blocks  because this strategy complements the new windowing heuristics well  we have kept it in bdpo  
each rule when applied to a block b in a block deordered plan bdp selects a set of
blocks from to go into the replaced part  w  based on their relation to b  to ensure that
the window is consistent with the block deordering  i e   has a consistent linearisation  as
stated in definition   on page       any blocks that are constrained to be ordered between
blocks in the window must also be included  we call these the intermediate blocks  formally
defined as follows 
definition     let bdp   hs  b  i be a block decomposed p o  plan  the intermediate
blocks of b  b are ib b     b    b    b    b   b   b  b     
let b be a block in bdp   and let un b  be the set of blocks that are not ordered w r t  b 
ip b  the immediate predecessors of b  and is b  its immediate successors  the rules used
by the windowing heuristic are 
   w    b  
   w    b   ip b  
   w    b   is b  
   w    b   un b  
   w    b   un b   ip b  
   w    b   un b   is b  
   w    b   un b   ip b   is b  
   w    b   un b   ip  b   un b   
   w    b   un b   is  b   un b   
   

fisiddiqui   haslum

figure     window formation by applying the  st rule of the rule based windowing heuristic
over block b   i e   w   b    p  un b    the unordered block b  is placed in its
predecessor set  note that this window can be optimised by removing s  because this step
has no causal link to its successors 

    w    b   un b   ip  b   un b    is  b   un b   
given the blocks selected by one of the rules above  the partitioning of blocks into hp  w  qi
is made by setting w   w   ib  w    and assigning to p any block that is ordered before
or unordered with w  and to q any block ordered after w  figure    shows an example of
rule based windowing  where the  st rule is applied to block b   applied to all blocks  the
above rules can produce duplicates  of course  only unique windows are kept 
the first rules  which include fewer blocks  generally produce smaller windows  while
the later rules tend to produce larger window  though there is no exact relation  since the
number of actions in a block varies   the heuristic applies all rules to each block in the
block deordered plan bdp in turn  rules are applied in the order                       i e  
starting with the first  the last  the second  the second last  and so on  the blocks are
ordered by size  descending   with ties broken by their order in the input plan  in opposite
direction for extended blocks  
recall that extractmorewindows repeatedly asks each windowing strategy to generate a limited number of windows  the ordering of blocks and rules described above helps
to ensure that the heuristic generates a varied set of windows  including both small and
large  covering different parts of the current plan  each time it is queried 
    the cyclic thread windowing heuristic
to discover new windowing heuristics  we noted some key changes in the decomposed plan
structure that frequently occur when a plan is improved  one significant observation is
that if multiple steps of an input plan have the same add effects  then those steps together
with the steps necessarily ordered in between them form a subplan that can often be im   

ficontinuing plan quality optimisation

proved  we call this cyclic behavior  in one experiment  we found that cycles of this type
are either removed from the plan or replaced with different cycles in more than     of
the improvements across most domains  the definition of cyclic behavior is based on an
individual atom  intuitively  an atom has cyclic behavior if it has multiple producers  as
defined below  
definition     let bdp   hs  b  i be a block decomposed p o  plan  and pm  s be the
set of producers of an atom m  i e   spm m  add s   m has cyclic behavior iff  pm       
note that pm contains the init step si iff m  i  however  since a window never contains
the initial step si   candidate windows are formed from extended producers instead  a step
s 
   si   sg   is an extended producer of atom m iff s produces m  or s consumes m and
there is no s     si that produces m and ordered before s in the block deordered plan  the
formal definition is as follows 
definition     let bdp   hs  b  i be a block decomposed p o  plan  a step s  s is an
extended producer of an atom m iff s 
   si   sg   and 
   m  add s  or
   m  pre s  and ks si if m  add k  then s   k 
in order to form candidate windows with respect to an atom m having cyclic behavior 
we first extract all the blocks that contain at least one extended producer of an atom m 
a cyclic thread  cf  definition     is then formed by taking a linearisation of those blocks 
consistent with the input plan 
definition     let bdp   hs  b  i be a block deordering of a sequential plan seq   and
bx   by  b are two blocks such that bx  by     let hbx   by i be a linearisation of  bx   by   
hbx   by i is consistent with seq if at least one step in bx appears before a step in by in seq  
the way we linearise the blocks so that it is consistent with the input plan is clarified
by the following example  assume bx    sa   sc   and by    sb   sd   are two blocks that we
have to linearise  and that the orderings of their constituent steps in the input plan is
sa in sb in sc in sd   the linearisation starts with the block that contains the first
element of in   i e   bx in this case  since it contains sa    in is then updated to in  bx   and
the linearisation continues in the same fashion until in is empty  the resulting linearisation
of the example blocks will be hbx   by i  if multiple  nested  blocks contain the first element
of in   the innermost one is picked  the formal definitions of thread and cyclic thread are
as follows 
definition     let bdp   hs  b  i be a block deordering of a sequential plan seq   epm  s
be the set of extended producers of an atom m  and bm  b be the set of blocks  where
each element of bm contains at least one element of epm   the thread of m  tm   is the
linearisation of blocks in bm such that the linearisation is consistent with seq   the thread
is called cyclic iff m has cyclic behavior 
for example  in the plan shown in figure    i   atom  at t  a  has cyclic behaviour 
since it holds in the initial state and is added by step s   its extended producers are s   s 
and s   so the cyclic thread is t at t  a    hb   b i 
   

fisiddiqui   haslum

finally  candidate windows are formed by taking a consecutive subsequence of blocks
 and intermediate blocks  as necessary  from a cyclic thread  like in rule based windowing 
blocks that are unordered with respect to the window are assigned to the set of blocks that
will precede the window 
definition     let tm   b         bk be a cyclic thread of an atom m  the cyclic thread based
windows over the cyclic thread tm are wl m    b  ib b    b   bi        bi l is a consecutive
subsequence of tm    while the unordered blocks are always placed in its predecessor set 
also like the rule based windowing heuristic  the cyclic thread heuristic generates windows in an order that aims to ensure it returns a varied set of windows each time it is
called  it first identifies all cyclic threads in the block deordered plan and then generates
a stream of candidate windows from one cyclic thread after another  as mentioned  each
candidate window is formed by taking a consecutive subsequence of blocks  and the intermediate blocks as required to form a consistent window  from the cyclic thread  given a
thread of  tm   blocks  subsequences are generated according to the following order of sizes 
    tm        tm                tm      in other words  the subsequence lengths are ordered as the
smallest  the biggest  the second smallest  the second biggest  and so on  for each size in
this order  all windows are generated moving from the beginning to the end of the thread 
    causal followers windowing heuristic
the third strategy that we have use to obtain a broader range of potentially improvable
windows is similar to the cyclic thread heuristic in that it creates windows that are subsequences of a linearisation of blocks connected by a particular atom  but different in that
these connections are via causal links 
definition     let bdp   hs  b  i be a block decomposed p o  plan  and c be the set of
causal links in   the causal followers of an atom m for a producer p  s are cfhm pi  
 p  sj        sk   hp  m  sj i       hp  m  sk i   c      si   sg    the causal followers of m  for all
producers   cfm   is the sequence hcfhm p  i        cfhm pn i i  where p         pn is a linearisation
of all the producers of m 
in other words  the causal followers of an atom m is a list of sets of steps  in each
set of steps  one is the producer s and the others are consumers sj of m  and s has a
causal link to every sj for m  i e   pc m   re s  sj    for example  the atom  at t  b 
in the block deordered plan in figure    i  appears in two causal links  both with the
same producer  hs    at t  b   s i and hs    at t  b   s i  thus  the causal followers are
cf at t  b    h s   s   s  i 
from the block deordered plan we extract the sequence of sets of blocks corresponding
to the causal follower steps  according to the definition below  for example  the sequence
of causal follower blocks of cf at t  b  in the plan in figure    i  is cfb at t  b    h b  i 
since all steps in cf at t  b  are contained in block b  
definition     let bdp   hs  b  i be a block decomposed p o  plan  and cfhm pi be
the causal followers of an atom m with respect to a producer p  s  the causal follower
blocks with respect to a producer p  s of an atom m  cfbhm pi   is the set of blocks 
where each block contains at least one element of cfhm pi   the causal follower blocks of m
   

ficontinuing plan quality optimisation

exclusive
all

basic block
      
      

ext  block
     
      

rule based
      
      

cyclic thread
     
      

causal followers
      
      

table    percentage of improvable windows found using the two block types and three
windowing heuristics  out of the total number of improvable windows found using all blocks
types and windowing heuristics  the first row gives the percentage of improvable windows
found by one block type but not the other  or by one windowing heuristic but not the
others   while the second row gives the percentage of all improvable windows found by one
block type  or windowing heuristic   the results are from the first experiment  described
in section     
 for all producers   cfbm   is the sequence hcfbhm p  i        cfbhm pn i i  where p         pn is a
linearisation of all the producers of m in bdp  
candidate windows are formed by taking consecutive subsequences of the sequence of
causal follower blocks  with intermediate blocks  as necessary   the formal definition is
given below  like in the other windowing heuristics  blocks that are unordered with respect
to the window are assigned to the set of blocks that will precede the window 
definition     let bdp   hs  b  i be a block decomposed p o  plan  and cfbm  
hcfbhm p  i        cfbhm pn i i be the causal follower blocks of m  the causal followers based
windows over cfbm are wl m    b  ib b    b   cfbhm pi i       cfbhm pi l i is a
consecutive subsequence of cfbm of length l   while the unordered blocks are always placed
to its predecessor list 
the order in which windows are generated by the causal followers heuristic is based
on the same principle as in the cyclic thread heuristic  it generates a stream of candidate
windows from the causal follower blocks cfbm associated with each atom m in turn  these
windows are consecutive subsequences of sets of blocks from cfbm   of lengths chosen
according to the pattern    l     l           l     where l is the length of cfbm  
    the impact of windowing heuristics
no one single windowing heuristic or block type  nor any combination of them  is guaranteed
to find all improvable windows  the first row of table   shows the percentage of improvable
windows found using one block type but not the other  or by one windowing heuristic but
not the others   out of the total number of improvable windows found using all blocks types
and windowing heuristics   the results are from the first experiment  described in section
      it shows that every windowing heuristic and block type contributes some improvable
windows that are not found by other strategies  for example        of improvable windows
are found only by the rule based windowing heuristic  using both basic and extended blocks  
on the other hand         of all improvable windows are not found by this heuristic  each
of the windowing heuristics has its strengths and limitations  the rule based heuristic 
for example  can only generate windows that contain sequences of extended blocks up to
a fixed length  while the cyclic thread and causal followers heuristics only make windows
from blocks connected by a single atom 
   

fisiddiqui   haslum

     

     

     




     














































     




















     


































     



















     





     



 

   

 

   

 

   

 

 



     

   




 

     

bdpo   combined windowing heuristics 
bdpo   random windowing 
bdpo   rulebased windowing only 
bdpo   causal followers windowing only 
bdpo   cyclic thread windowing only 
   



 



   

average quality score  relative ipc quality score   coverage 

     

time  hours 

figure     average ipc quality score as a function of time for separate runs of bdpo  using
each of the three windowing heuristics alone  all three heuristics combined  and random
window generation  each run is done using the same setup as experiment    described in
section      on page       the x axis here shows only the runtime of bdpo   i e   without
the   hour delay for generating the input plans  as shown in figure      note also that the
y axis is truncated  the average quality of the input plans is       

   

ficontinuing plan quality optimisation

figure    shows the impact of different windowing heuristics on the anytime performance
of bdpo   as measured by the average ipc plan quality score achieved as a function of timeper problem  in this experiment  we ran bdpo  with each of the three windowing heuristics
alone  and with all three combined in a sequential portfolio  as described at the beginning
of this section   the combined portfolio of windowing heuristic is the same configuration
of bdpo  that is presented in the experimental results in section      page       we also
compare these with a non heuristic  random windowing strategy  in which each window
is formed by taking a random subsequence of blocks from a random linearisation of the
block deordered plan  subsequences are chosen so that the distribution of window sizes
 measured by the number of actions in the window  is roughly the same as that produced
by the combined heuristics  the experiment uses setup    described in section     on page
      i e   the input plans to bdpo  are already of high quality   their average ipc plan
quality score is        
as predicted by the data in table    using any of the three windowing heuristics on
its own results in a much worse system performance  since each fails to find a substantial
fraction of improvable windows  in fact  random window generation is better than any
of the heuristics on their own  however  the combined portfolio of heuristics outperforms
random windowing by a good margin  the total quality improvement achieved with the
random windowing strategy is       less than that of the best bdpo  configuration  this
demonstrates that the heuristics capture information that is useful to guide the selection of
windows 
    possible extensions to the windowing strategies
since a window is formed by partitioning plan steps into three disjoint sets of blocks  the
number of possible windows is exponential  the challenge for a good windowing heuristic is
to extract a reduced set that contains windows more likely to be improved  every windowing
strategy has some limitations  hence  there is always a scope for developing new windowing
heuristics or extending the existing ones  one such extension is discussed in this section 
the combination of strategies we use may miss some improvable windows  for example 
a long sequence of blocks that do not form part of a cyclic thread or causal followers sequence
with respect to a single atom will not be captured by these heuristics  an example of this is
shown in figure     where three candidate windows  w   w  and w   found by the causal
followers windowing heuristic are not improvable separately  in this situation  forming a
window as the union of separate windows  found by one or several strategies  can overcome
the limitations of those strategies  in the example  the union of w  and w  is improvable 
this type of composite windows could be formed in the later stages of the plan improvement
process  after all the individual windowing heuristics have been exhausted  however  the
number of composite windows that can be created from a large set of candidate windows is
combinatorial and thus optimising all of them will take a long time 
    window ranking
although the windowing strategies generate only a fraction of all possible windows  the
number of candidate windows is still often large  cf  table     in order to speed up the
   

fisiddiqui   haslum

figure     three candidate windows  w   w   and w   found by the causal followers
windowing heuristic for atoms  at t  b    in p  t    and  in p  t   respectively  none of
them are improvable  however  the composite window formed by merging w  and w  is
improvable by substituting the delivery of package p   from location b to c  provided by
truck t  with truck t   this is because the atom  at t  c  is not required by any of its
successors  i e   the goal in this example  

plan improvement process  it is helpful to order windows so that those more likely to be
improved are optimised first  this is the role of window ranking 
ranking windows is made difficult by the fact that the properties of improvable windows
vary from one to another  and a lot from domain to domain  for example  as mentioned at
the beginning of this section  larger windows are more likely to be improved in the pegsol 
openstacks and parcprinter domains  while smaller windows are better for the elevators 
transport  scanalyzer  and woodworking domains  in the sokoban domain  on the other
hand  medium sized windows are better  moreover  an improvable window may not be
improved by a particular subplanner within the given time bound  we have noted that in
some domains  e g   pegsol or scanalyzer  subplanners require  on average  more time to
find a lower cost plan 
we have developed a set of window ranking policies by examining structural properties
of the generated candidate windows generated and the results of our first experiment  cf 
section      in which we ran two subplanners  ibcs and pngs  on each generated window
with a    second time limit  excluding only windows whose cost is already shown to be
optimal by the admissible lm cut heuristic  helmert   domshlak         investigating
the properties of improved and unimproved windows  we identified four metrics that work
relatively well across domains 
   

ficontinuing plan quality optimisation

    
random ranking
outgoing causal links per length  min to max 
incoming causal links per length  min to max 
pairwise ordering disagreement  min to max 
gap between cost   admissible heuristic  max to min 

fraction of improvable windows out of the selected windows



    

   

    

    

    

    






























   

   

   

   

   

   

   

   

   

   

   

   

   

  

  

  

   

number of selected  top ranked  windows

figure     fraction of improvable windows  across all domains  out of the selected top
windows from the ranked orders generated by each of the ranking policies  see text  

    the total number of causal links whose producers reside in a window and whose consumers are outside the window  divided by the length of the window  the lower the
value the higher the rank  we call this property outgoing causal links per length 
    the total number of causal links whose consumers reside in a window and whose producers are outside the window  divided by the length of the window  the lower the
value the higher the rank  we call this property incoming causal links per length 
    the gap between the cost of a window and the lower bound on the cost of any plan for
the corresponding subproblem given by the admissible heuristic  the higher the value
the higher the rank 
    the number of pairwise ordering  of steps  disagreements between a window hp  w  qi
and the sequential input plan  the lower the value the higher the rank  to calculate
this we first take the linearisation of hp  w  qi that is used to generate the corresponding
subproblem  then  for every pair of plan steps  if the ordering between them in the
linearisation is not the same as in the input plan we call this a pairwise ordering
disagreement  the lower the total number of such disagreements is for a window  the
higher its rank  in other words  if the ordering of steps in a window is very different
from the input plan then it is less likely to be improved 

   

fisiddiqui   haslum

    
fraction of improvable windows out of the selected windows

random ranking
outgoing causal links per length  min to max 
incoming causal links per length  min to max 
pairwise ordering disagreement  min to max 
gap between cost   admissible heuristic  max to min 



    
   
    
    
    
    
   
    
    
    
    












   

















    




    
    
    

   

   

   

   

   

   

   

   

   

   

   

   

   

  

  

  

   

number of selected  top ranked  windows

figure     fraction of improvable windows in the parking domain  out of the selected top
windows from the ranked orders generated by each of the ranking policies  see text  

we can infer from the first two ranking policies that the more disconnected a window is
from other blocks in the decomposed plan the more likely it is to be improved  figure   
compares these ranking policies with the performance of a random ordering of the windows 
on average across all domains  all four ranking policies are good at picking out improvable
windows  for example  if we take the top    windows from the order generated by the
incoming causal links per length policy  nearly     of those windows are improvable  by
at least one subplanner   while the top    windows from the random order contain only
    improvable windows  the random ranking in figure    is the best result out of three
separate random rankings for each of the values on the x axis  as expected  it exhibits
roughly the same ratio of improvable windows over all ranges  from    to       nearly    
of the selected windows  across all domains  are improvable  however  the performance of
individual ranking policies varies by domain  and for each policy we find some domain in
which it is not good  for example  figure    shows ranking results for instances of the
parking domain only  here  the outgoing causal links per length policy does not work
well  considering the top    windows in the ranked order  it is even worse than random 
however  the other ranking policies are quite beneficial in this domain 
bdpo  uses the first three ranking policies in a sequential portfolio  as explained in
section     for each subplanner  bdpo  uses a current ranking policy to select the next
   

ficontinuing plan quality optimisation

     

average quality score  relative ipc quality score   coverage 

     
     
     



     



     












     





     








     





     





     




     





     




     





     


     




     

bdpo   rankbased 
bdpo   randomranked 
 

   

 

   

 

   

 

   

 

   

 

   

 



time  hours 

figure     average ipc plan quality score as a function of time in two separate runs  with
and without window ranking  in the second case  the order of the candidate windows is
randomised  each run is done using experimental setup    as described in section     on
page      the time shown here is the runtime of bdpo  only  excluding the   hour delay
for generating the input plans  as shown in figure      also  the y axis is truncated  all
curves start from the average quality score of the input plans  which is       

window for the chosen subplanner  from those eligible for optimisation by that subplanner  
if no improvement is found by that subplanner in a certain number of attempts      in our
current configuration   the system switches a different ranking policy  to produce a different
ordering of the candidate windows for that subplanner 
the use of window ranking has a beneficial effect on the anytime performance of the
plan improvement process  as shown in figure     we achieve higher quality scores  and in
particular  achieve them faster  when using window ranking compared to random ranking 
in this experiment  we ran bdpo  once with the portfolio of ranking policies  as described
   

fisiddiqui   haslum

above  and once with windows chosen for optimisation in a random order  the experiment
used the same setup as experiment    described in section     on page      
we tried many alternative methods of combining the ordered lists generated by different
ranking policies  in order to achieve a ranking with more stable performance across domains 
the problem of combining rankings  often called rank aggregation  has been studied in many
disciplines  such as social choice theory  sports and competitions  machine learning  information retrieval  database middleware  and so on  rank aggregation techniques range from
quite simple  based on rank average or number of pairwise wins  to complex procedures
that in themselves require solving an optimisation problem  we tried five simple but popular rank aggregation techniques  namely bordas        method  kemenys        optimal
ordering  copelands        majority graph  mc   dwork  kumar  naor    sivakumar 
       and multivariate spearmans rho  bedo   ong         the result of those experiments  however  is that rank aggregation does not produce better  or more stable  window
rankings  especially in cases where one individual policy is relatively bad  hence our choice
of using the ranking policies in a cyclic portfolio instead 

   on line adaptation
the lns approach to optimisation by repeatedly solving local subproblems gives us the
opportunity for adapting the process on line to the current problem  we have noted that
different subplanners  windowing strategies  and ranking policies work better in different
domains  for example  figure    shows the fraction of local improvements found by each
of three subplanners in different domains  as can be seen  the ibcs subplanner is more
productive  compared to pngs and lama  in the appn  barman  maintenance  parking 
sokoban  and woodworking domains  pngs  on the other hand  is better in the scanalyzer
and visitall domains  and lama in the elevators and openstacks domains  therefore  if we
can learn over the course of the local search the relative success rate of different subplanners
on the current problem  the system will perform better  in a similar fashion  window
generation strategies and ranking policies may also be adapted to the current problem  so
that the system is more likely to select subplans for optimisation that are improvable 
we use an on line machine learning technique  the multi armed bandit  mab  model 
to be specific  to select the subplanner for each local optimisation attempt  this technique  and its impact on the anytime performance of bpo  is described in the following
subsections 
for window selection  on line adaptation is limited to switching between alternative
ranking policies  the window selected for optimisation by a subplanner is the top one
in the order given by the current ranking policy for that subplanner  cf  section      
as long as improvements are found among these windows  we can consider the current
policy to be useful  when a subplanner reaches a certain number of attempts with no
improvements found  we switch to using the next policy for that subplanner  the number
of windows in each neighbourhood that are optimised is typically small compared to the
number of candidate windows generated  on average across all problems in experiment  
 cf  section     on page      optimisation by at least one subplanner is tried on       of
generated windows  because of this  adapting the ranking policy has more influence over
   

ficontinuing plan quality optimisation

figure     the percentage of improved windows found by each of the subplanners  pngs 
ibcs  and lama   out of the total number of improved windows found by all the subplanners  in this experiment  bdpo  was run three times  each time with one subplanner  the
setup was the same as for experiment    described in section     on page      

which windows are tried than adapting the windowing strategies  the effect of adaptive
window ranking on the anytime performance of bdpo  is shown in figure     page      
    bandit learning
the multi armed bandit  mab  model is a popular machine learning formulation for dealing
with the exploration versus exploitation dilemma  in a mab problem  an algorithm is
presented with a sequence of trials  in each round  the algorithm chooses one from a set of
alternatives  often called arms  based on the past history  and receives a reward for this
choice  the goal is to maximise the total reward over time  a bandit learning algorithm
balances exploiting the arms with the highest observed average reward with exploring poorly
understood arms to discover if they can yield better reward 
mab has found numerous applications in diverse fields  e g   control  economics  statistics  and learning theory  after the influential paper by robbins         many policies have
been proposed for the mab problem under different assumptions  for example  with independent  auer et al         or dependent arms  pandey  chakrabarti    agarwal        
exponentially or infinitely many arms  wang  audibert    munos         finite or infinite
time horizon  jones   gittins         with or without contextual information  slivkins 
       and so on 
we cast the problem of selecting the subplanner for each local optimisation attempt as a
multi armed bandit problem  the goal of this is to maximise the total number of improved
windows over time  we use a learning algorithm based on the optimistic exploration strategy  which chooses an arm in the most favorable environments that has a high probability of
being the best  given what has been observed so far  this strategy is often called optimism
in the face of uncertainty  at each trial t  and for each arm k  the strategy is to use past
observations and a probabilistic argument to define high probability confidence intervals
for the expected reward k   the most favorable environment for arm k is thus the upper
   

fisiddiqui   haslum

confidence bound  ucb  on k   a simple policy based on this strategy is to play the arm
having the highest ucb 
a number of algorithms have been developed for optimistic exploration of bandit arms 
such as ucb   ucb  and ucb  normal by auer et al          ucb v by audibert 
munos and szepesvari         and kl ucb by garivier and cappe         we use the
ucb  algorithm for planner selection  the ucb  algorithm
selects at each trial t the arm
q
  ln t
with highest upper confidence bound bk t   
bk t  
nk   the sum of an exploitation term
and an exploration term  respectively  
bk t is the empirical mean of the rewards received
from arm k up
to
trial
t 
and
n
is
the
number
of times arm k has been tried so far  the
k
q
  ln t
second term 
nk   is a confidence interval for the average reward  within which the true
expected reward falls with almost certain probability  hence  bk t is a upper confidence
bound  the ucb  algorithm can achieve logarithmic regret uniformly over the number of
trials and without any preliminary knowledge about the reward distributions  auer et al  
      
applied to subplanner selection in bdpo   the algorithm works as follows  first  we
select each subplanner p once  to initialise the average reward 
bp   after each optimisation
attempt  we give a reward of   to the chosen subplanner if it found an improvement and a
reward of   otherwise  we could use some other scheme for assigning rewards rather than
simply   and    for example  making the reward proportional to the amount of improvement
 or time taken to find it   however  we have observed that assigning varying rewards
to subplanners makes the bandit learning system more complicated  and does not help
in achieving better overall result  next  we select for each
q attempt a subplanner p that
maximises the upper confidence bound of p  bp t   
bp    nlnp t   as explained above  here 
np is the number of times p has been tried so far  and t is the total number of optimisation
attempts  by all subplanners  done so far  we can see that bp t grows with t but shrinks
when t and np increase uniformly  this ensures that each alternative is tried infinitely often
but still balances exploration and exploitation  in other words  the more we try p  the
smaller the size of the confidence interval and the closer up gets to its mean value 
bp   but

p cannot be tried once up becomes smaller than p   where p is the planner with the best
average reward 
    the impact of bandit learning
the response of the bandit policy for subplanner selection is shown in figure     the figure
shows the fraction of the total number of optimisation attempts that one subplanner  ibcs 
was selected  and the fraction of the total number of window improvements found by that
subplanner  since bdpo  in this experiment uses only two subplanners  ibcs and pngs 
the corresponding fraction for pngs is    y  as an example  in the third problem  from
the left  in the appn domain       of window improvements are found by ibcs  and the
bandit policy selects this subplanner for     of the total number optimisation attempts 
pngs is chosen for the other      but finds no improvement  we can see that the bandit
policy selects the more promising subplanner more often across the problems  however 
the bandit policy is somewhat conservative  because it ensures that we do not rule out
any subplanners that fare poorly early on  moreover  as the current plan is improved it
   

ficontinuing plan quality optimisation

 
improvement ratio
exploitation ratio



   
exploitation and improvement ratio by ibcs



   
   




   
   





 

   









 

             


  
 
 
 

 



 
  








 


  












   



   



 
 





 



   
   

woodworking

visitall

transport

thoughtful

tetris

sokoban

parking

scanalyzer

parcprinter

nomystery

maintenance

hiking

ged

floortile

elevators

childsnack

barman

appn

 

figure     the response of the bandit policy to subplanner success rates  the exploitation
ratio is the fraction of the total number of optimisation attempts for which the ibcs
subplanner was chosen  out of the total number of attempts by both subplanners  the
improvement ratio is the fraction of the total number of improved windows found by ibcs 
out of total number of improved windows found by both subplanners  since ibcs and
pngs are the only two subplanners used in this experiment  the corresponding ratios for
pngs are the opposite  i e      y   the experiment was run with the same setup as
experiment    described in section     on page     

becomes harder to find further improvements  within the given time bound   so the average
reward for both subplanners decreases  this forces the bandit policy to switch between the
subplanners more often 
figure    shows the impact of combining the subplanners using the ucb  bandit policy 
compared to simply alternating between subplanners or using each subplanner alone  on the
anytime performance of bdpo   in this experiment we ran bdpo  once with each of ibcs 
pngs and lama as the only subplanner  once combining two of them  ibcs and pngs 
using a simple alternation policy  which selects each of the two in turn  and once combining
the two using the bandit policy  each run was done with experiment setup    as described
in section     on page       i e   with input plans of a high quality   the ipc plan quality
score of each plan is calculated as before  see page       the average score of the input
plans is         as expected  combining the ibcs and pngs subplanners in some fashion
leads to more quality improvement across the entire time scale than achieved by running
bdpo  with any individual subplanner  the figure also shows that combining multiple
subplanners using the bandit policy is a better strategy than simply alternating between
   

fisiddiqui   haslum

     

average quality score  relative ipc quality score   coverage 

     




















     






















     















     






     







     




     


     




     



















































     

bdpo   pngs ibcs  bandit 
bdpo   pngs ibcs  alternating 
bdpo   pngs only 
bdpo   ibcs only 
bdpo   lama only 









     


 

   

 

   

 

   

 

   

 

   

 

   

 

     

time  hours 

figure     average ipc quality score as a function of time per problem in five different
runs of bdpo   using only each one of the three subplanners  using two of them  ibcs
and pngs  combined with the ucb  bandit policy  and without  using simple alternation
instead   this experiment was run with setup   as described in section      on page      
note that the y axis is truncated  all curves start at the average quality of the input plans 
which        the time shown here is the runtime of bdpo  only  excluding the   hour
delay for generating the input plans shown in figure     

   

ficontinuing plan quality optimisation

them  the total quality improvement achieved by bdpo  using the alternation policy is
     less than that by bdpo  using the bandit policy 

   related work
we survey four areas of related work  anytime search algorithms and post processing approaches  which have in common with our approach the aim of continuing plan quality
improvement  uses of local search in planning  and finally  uses of algorithm portfolios in
planning 
    anytime search
large state space search problems  of the kind that frequently arise in planning problems 
often cannot be solved optimally because optimal search algorithms exhaust memory before
finding a solution  anytime search algorithms try to deal with such problems by finding a
first solution quickly  possibly using a greedy or suboptimal heuristic search  then continue
 or restart  searching for a better quality solution  anytime algorithms are attractive because they allow users to stop computation at any time  i e   after a good enough solution
has been found  or after too long a wait  this contrasts with algorithms that require the
user to decide in advance on a deadline  a suboptimality bound  or some other parameter
that fixes the trade off between time and solution qualty 
bounded suboptimal search is the problem of finding a solution with cost less than or
equal to a user specified factor w of optimal  weighted a   wa   search  pohl       
and explicit estimation search  ees   thayer   ruml        are two algorithms of this
kind that have been most used in planning  iteratively applying any bounded suboptimal
search algorithm with a lower value of w whenever a new best solution is found provides an
anytime improvement of plan quality  restarting wa   richter et al         does this  using
a schedule of decreasing weights  rwa  is used by the lama planner  richter   westphal 
      lama finds the first plan using a greedy best first search  bonet   geffner        
it also uses several search enhancements  like preferred operators and deferred evaluation
 richter   helmert         ees conducts a bounded suboptimal best first search restricted
to expanding nodes that may lead to a solution with a cost no more than a given factor w
times optimal  among the open nodes in this set  it expands the one estimated to have the
fewest remaining actions between it and a goal  it uses both an admissible heuristic for plan
cost and more informative but inadmissible estimates to guide the search  aees  thayer
et al       b  is an anytime version of ees  to achieve an anytime behavior  aees lowers
the value of w whenever a new best solution is found 
the bounded cost search  stern  puzis    felner        problem  of which the subproblems solved in our approach are an example  requires finding a solution with a cost less
than or equal to a user specified cost bound c  the aim of a bounded cost search algorithm is to find such a solution as quickly as possible  iteratively applying any bounded cost
search algorithm with a bound less than the cost of the best solution found so far provides
anytime quality improvement  this is what the ibcs algorithm  used as one of the subplanners in bdpo   does  the bees and beeps algorithms  thayer  stern  felner   
ruml      a  adapt ees to the setting of bounded cost search  these algorithms expand
   

fisiddiqui   haslum

the best open node among those whose inadmissible cost estimate is at most c  falling back
to to expanding the node with the best admissible estimate only if the set is empty 
branch and bound algorithms explore the search space in some systematic fashion  using
an admissible heuristic  lower bound on cost  to prune nodes that cannot lead to a solution
better than the best found so far  branch and bound can be implemented with a linearmemory  depth first search strategy as well as on top of other strategies  in the experiment
reported in section      page      we used beam stack search  bss   zhou   hansen 
      as a bounded cost search algorithm by providing as an initial upper bound the cost of
the base plan for each problem  bss combines backtracking branch and bound with beam
search  which behaves like a breadth first search but limits the size of the open list in each
layer by a user specified parameter  known as the beam width  when forced to backtrack 
bss reconstructs nodes pruned from the open list so that the search is complete  the beam
width parameter can be used to control the memory consumption of bss so that it never
exceeds available memory  for planning problems  however  whose state spaces are often
dense in transpositions and where accurate admissible heuristics are expensive to compute 
repeatedly reconstructing paths to unexplored nodes becomes time consuming 
anytime search planners aim to provide continuing improvement of plan quality given
more time  and often succeed in doing that in the early stages of the search  however  as we
have observed in the results of our experiments  these algorithms often stagnate  reaching
a point where they do not find any better plans even after several hours of cpu time   cf 
figure    on page     and section     on page       for example  in our experiment lama
and aees found better plans for only      and       respectively  of the total number of
problems between   hours and   hours cpu time  while bdpo  found better plans for
      of the problems during the same time interval  memory is one limiting factor  but
not the only one  for almost half the problems  aees ran for a full   hours cpu time
without running out of memory  yet found very few improved plans  bss found plans with
a cost less than the initial upper bound  the cost of the base plans  for only    out of    
problems even after    hours cpu time per problem 
    local search
local search explores a space by searching only a small neighbourhood of a current element
in the search space for one that is  in some way  better  then moving to the neighbour and
repeating the process  compared to systematic search algorithms  the advantage of local
search is that it needs much less memory  therefore  local search algorithms are widely
used to solve hard optimisation problems  however  local search algorithms cannot offer
any guarantees of global optimality  or bounded suboptimality  in planning  local search has
been used mainly to find plans quickly  and rarely to improve plan quality  though some of
the post processing methods discussed in the next section can be viewed as local searches 
ff  hoffmann   nebel        is a forward chaining heuristic state space search planner 
the heuristic used by ff estimates the distance from a state to the nearest goal state  ff
uses a local search strategy  called enforced hill climbing  that in each state uses a breadthfirst search to find a neighbour state  which may be several steps away from the current
state  with a strictly better heuristic value  i e   that is believed to be closer to the goal 
it then commits to that state and starts a new search for a neighbour with a better yet
   

ficontinuing plan quality optimisation

heuristic value  if the local search fails  due to getting trapped in a dead end  ff falls back
on a complete best first search algorithm  the rw ls planning algorithm  xie  nakhost 
  muller        is similar to ffs hill climbing approach  but uses a combination of greedy
best first search and exploration by random walks to find a better next state in each local
search step  nakhost and muller        developed a planning system  called arvand  that
uses random walk based local exploration in conjunction with the ff search heuristic  they
showed that arvand outperforms ff on hard problems in many domains  the execution
of arvand consists of a series of search episodes  each episode starts with a set of random
walks from the initial state  the endpoint of each random walk is evaluated using the
heuristic function to choose the next state  the search episode then continues with a set
of random walks from this state  this process repeats until either the goal is reached 
or enough transitions are made without heuristic progress  in which case the process is
restarted  the ipc      and      versions of arvand apply post processing to improve the
quality of each generated plan  the post processing techniques are action elimination and
plan neighborhood graph search  nakhost   muller         they are discussed in the next
subsection  because arvands search is randomised  the system can continue generating
alternative plans  which are then optmised  indefinitely  storing at all times the best plan
generated so far  this provides a certain anytime capability  it is in this manner that it
was used in the experiment reported in section     on page     
the lpg planner  gerevini   serina        is based on local search in the space of
action graphs  which represent partial plans  the neighbourhood is defined by operators
that modify an action graph  such as inserting or removing actions  the function that
evaluates nodes in the neighbourhood combines terms that estimate both how far an action
graph is from becoming a valid plan  termed search cost  and the expected quality of
the plan it may become  the choice of neighbour to move to also involves an element of
randomness  lpg also performs a continuing search for better plans  in this  it is similar to
the anytime search algorithms discussed in the last subsection  whenever it finds a plan 
the local search restarts with a partial plan obtained by removing some randomly selected
actions from the current plan  a numerical constraint forcing the cost of the next plan to
be lower is also added  this provides some guidance towards a better quality next plan 
there is a close relationship between local search approaches to planning and plan repair
or adaptation methods  garrido  guzman    onaindia         the lpg planner originated
as a method of plan repair  gerevini   serina         and iterative repair methods can also
be used for plan generation  chien  knight  stechert  sherwood    rabideau        
a key difference between our use of local search and its previous uses in planning is that
we carry out a local search only in the space of valid plans  this permits the neighbourhood
evaluation to focus exclusively on plan quality  searching a space of partial plans  represented by states  as done in ff  or incomplete  invalid  plans  as done in lpg  requires
neighbourhood evaluation to consider how close an element is to becoming a valid plan  and
balancing that with quality 
the large neighbourhood search  lns  strategy formulates the problem of finding a
good neighbor as an optimisation problem  rather than simply enumerating and evaluating
neighbours  this allows a much larger neighbourhood to be considered  lns has been used
very successfully to solve hard combinatorial optimisation problems like vehicle routing with
time windows  shaw        and scheduling  godard  laborie    nuijten         theoretical
   

fisiddiqui   haslum

and experimental studies have shown that the increased neighborhood size may improve the
effectiveness  quality of solutions  of local search algorithms  ahuja  goodstein  mukherjee 
orlin    sharma         if the neighbourhood of the current solution is too small then it
is difficult to escape from local minima  in this case  additional meta heuristic techniques 
such as simulated annealing or tabu search  may be needed to escape the local minimum 
in lns  the size of the neighborhood itself may be sufficient to allow the search process to
avoid or escape local minima 
in the lns literature  the neighborhood of a solution is usually defined as the set of
solutions that can be reached by applying a destroy heuristic and a repair method 
the destroy heuristic selects a part of the current solution to be removed  unassigned  
and the repair method rebuilds the destroyed part  keeping the rest of the current solution
fixed  the destroy heuristic often includes an element of randomness  enabling the search
to explore modifications to different parts of the current solution  the role of the destroy
heuristic in our system is played by the windowing strategies  which select candidate windows  subplans  for re optimisation  we explore these windows systematically  some lns
algorithms  e g   ropke   pisinger        schrimpf et al         allow the local search to
move to a neighbouring solution with a lower quality  e g   using simulated annealing   we
consider only strictly improving moves  however  in difference to previous lns algorithms 
we do not immediately move to a better plan and restart neighbourhood exploration after a
local improvement has been found  instead  we use delayed restarting  which allows a better
solution to be found in one local search step by destroying and repairing multiple parts of
the current plan  experimentally  we found that delayed restarting produces better quality
plans  and produces them faster  than immediate restarts  cf  section     on page      
    plan post processing
by a post processing method  we mean one that takes a valid plan as input and attempts to
improve it  by making some modifications  this is also related to plan repair and adaptation
 chien et al         fox  gerevini  long    serina        garrido et al          but with
the key difference that plan repair or adaptation starts from a plan that is not valid for
the current situation and focuses on making it work  the discrepancy between the current
state or goals and those the plan was originally built for provide guidance to where repairs
are needed  in contrast  post processing for plan optimisation may require modifications
anywhere in the current plan 
nakhost and muller        proposed two post processing techniques  action elimination  ae  and plan neighborhood graph search  pngs   action elimination identifies and
removes some unnecessary actions from the given plan  pngs constructs a plan neighborhood graph  which is a subgraph of the state space of the problem  built around the
path through the state space induced by the current plan by expanding a limited number of
states from each state on that path  it then searches for the least cost plan in this subgraph 
if this finds a plan better than the current  the process is repeated around the new best
plan  otherwise  the exploration limit is increased  until a time or memory limit is exceeded 
furcys        iterative tunneling search with a   itsa   is similar to pngs  itsa 
explores an area  called a tunnel  of the state space using a  search  restricted to a fixed
distance from the current plan  these methods can be seen as creating a neighborhood
   

ficontinuing plan quality optimisation

that includes only small deviations from the current plan  but anywhere along the plan  in
contrast  bdpo  focuses on one section of the decomposed plan at a time  often grouping
together different parts of the input plan  but puts no restriction on how much that section
changes  hence  it creates a different neighbourhood  our experiments show that the best
results are obtained by exploring both neighbourhoods  for example  pngs often finds
plan improvements quickly  but running it for an additional   hours improves its average
ipc plan quality score  over that of the best plans it finds in the first hour  only by       
running instead bdpo   using pngs as the only subplanner and taking the best plans
found by pngs in   hour as input  improves the average plan quality score by    in  
hours 
ratner and pohl        used local optimisation for shortening solutions to sequential
search problems  to select the subpath to optimise  they used a sliding window of a predefined size dmax over consecutive segments of the current path  estrem and krebsbach
       instead used a form of windowing heuristic  they select for local optimisation pairs
of states on the current path that maximise an estimate of redundancy  based on the ratio
between the estimated distances between the two states  given by a state space heuristic 
and the cost of the current path  balyo  bartak and surynek        used a sliding window
approach to minimise parallel plan length  that is  makespan  assuming all actions have
unit duration   rather than take segments of a single path in the state space  we use block
deordering of the input plan to create candidate windows for local optimisation  as shown
by the experimental results  this is very important for the success of bdpo   the total
improvement of average plan quality achieved without deordering was       less than that
achieved by bdpo  using block deordering of input plans  cf  section     on page      
the planning by rewriting approach  ambite   knoblock        also uses local modifications of partially ordered plans to improve their quality  plan modifications are defined by
domain specific rewrite rules  which have to be provided by the domain designer or learned
from many examples of both good and bad plans  hence  this technique can be effective for
solving many problem instances from the same domain  using a planner to solve subproblems may be more time consuming than applying pre defined rules  but makes the process
automatic  however  if we consider solving many problems from the same domain it may be
possible to reduce average planning time by learning  generalised  rules from the subplan
improvements we discover and using these where applicable to avoid invoking a subplanner 
    portfolio planning and automatic parameter tuning
a portfolio planning system runs several subplanners in sequence  or in parallel  with short
timeouts  in the hope that at least one of the component planners will find a solution in
the time allotted to it  portfolio planning systems are motivated by the observations that
no single planner dominates all others in all domains  and that if a planner does not solve
a planning task quickly  often it does not solve it at all  therefore  many of todays most
successful planners run a sequential portfolio of planners  coles  coles  olaya  celorrio 
linares lopez  sanner    yoon        
gerevini  saetti and vallati        introduced the pbp planner  which learns a portfolio
over a given set of planners for a specific domain  as well as domain specific macro actions 
fast downward stone soup  fdss  helmert  roger  seipp  karpas  hoffmann  keyder 
   

fisiddiqui   haslum

nissim  richter    westphal        uses a fixed portfolio  computed to optimise performance
on a large sample of training domains  for all domains  ibacop   cenamor et al        
dynamically configures a portfolio using a predictive model of planner success 
another recent trend is the use of automatic algorithm configuration tools  like the
paramils framework  hutter  hoos  leyton brown    stutzle         to enhance planner
performance on a specific domain  paramils does a local search in the space of configurations  using a suite of training problems to evaluate performance under different parameter
settings  the combinatorial explosion caused by many parameters with many different values is managed by varying one parameter at a time  paramils has been used to configure
the lpg planner  vallati  fawcett  gerevini  hoos    saetti        and the fast downward planner  fawcett  helmert  hoos  karpas  roger    seipp         the pbp  portfolio
planner  gerevini  saetti    vallati         successor to pbp  includes a version of lpg
customised to the domain with paramils in the learned portfolio 
bdpo   of course  uses a portfolio of subplanners  and  as we have shown  selecting the
right subplanner for the current problem is important  cf  section     much more important 
however  is the focus on subproblems that our approach brings  comparing figures     page
     and     page       it is clear that using even a single subplanner within bdpo  is
more effective than using any of the subplanners on its own  the multiple window ranking
policies used in bdpo   cf  section      can also be viewed as a simple sequential portfolio 
compared to previous portfolio planners  the iterated use of subplanners  windowing strategies and other components in our approach offers a possibility to learn the best portfolio
or configuration on line  that is  rather than spend time on configuring the system using
training problems  we can learn from the experience of solving several subproblems  while
actually working on optimising the current plan 
finally  although we have not explored it in great depth  our results suggest that combining different anytime search and post processing methods  in what is effectively a kind of
sequential portfolio  such as running bdpo  on the result of running pngs on the result
of lama or ibacop   as in the results of experiment    shown in figure   on page      
often achieves better quality final plans than investing all available time into any single
method 

   conclusions and future work
plan quality optimisation  particularly for large problems  is a central concern in automated
planning  anytime planning  which aims to deliver a continuing stream of better plans
given more time  is an attractive idea  offering the flexibility to stop the process at any
point  such as when the best plan found is good enough or the wait for the next plan
becomes too long  we have presented an approach to anytime plan improvement  and its
realisation in the bdpo  system  this approach is based on the large neighbourhood local
search strategy  shaw         using windowing heuristics to select candidate windows from a
block deordering of the current plan  for local optimisation using off the shelf bounded cost
planning techniques 
experiments demonstrate that bdpo  achieves continuing plan quality improvement
even at large time scales  several hours cpu time   when other anytime planners stagnate 
key to achieving this is our focus on optimising subproblems  corresponding to windows 
   

ficontinuing plan quality optimisation

as mentioned in section      extending the windowing heuristics and improving the on line
learning of effective window rankings is one way to improve the approach  also  complementing the window ranking  which estimates how promising a window is  with an
estimate of how difficult windows are to optimise  and using this to inform the time allocated to subplanners  which is currently uniform for all windows  may contribute to better
performance  the best result  however  is achieved by chaining several techniques together
 for example  applying bdpo  to the best plan found by pngs applied to the best plan
found by lama or ibacop    this result cannot be achieved by any of the previous anytime planning approaches alone  thus  another area of future work is to examine in greater
depth what is the best way to combine different plan improvement methods  and how this
can be learned on line while optimising a plan  for example  we have conducted a study
of the optimal time to switch from base plan generation  using lama  to post processing
using pngs or bdpo  as a function of the total runtime  siddiqui   haslum      a  
as we have demonstrated experimentally  the block deordering step is essential for the
good performance of bdpo   cf  section     on page       block deordering creates a
decomposition of the plan into non interleaving blocks while removing ordering constraints
between blocks  this lifts a limitation of conventional  step wise  deordering  which
requires all unordered steps in the plan to be non interfering  as we have shown  a validity
condition for block decomposed partially ordered plans can be stated that is almost the
same as chapmans        modal truth criterion  but allowing threats to a causal link to
remain unordered as long as the link is protected by the block structure  theorem   on
page       therefore  block deordering can yield less order constrained plans  including in
some cases where no conventional deordering is possible 
the plan structure uncovered by block decomposition can also have other uses  recently it was used in the planner independent macro generation system bloma  chrpa  
siddiqui        to find longer macros that capture compound activities in order to improve
planners coverage and efficiency  in some domains  e g   barman  childsnack  scanalyzer 
parcprinter  gripper  woodworking  etc    block deordering often identifies structurally similar subplans  which also have symmetric improvement patterns  this could potentially be
exploited in learning plan rewrite rules  ambite  knoblock    minton         the structure
of block deordered plans  which often comprises a nested  hierarchical decomposition into
meaningful subplans  is reminiscent of hierarchical task network  htn  representations 
hence  block deordering technique could potentially be applied to generating  or helping to
generate  htn structures in a domain independent way  reducing the knowledge engineering
effort  recent work by scala and torasso        extends deordering to plans for planning
domains with numeric state variables  identifying numeric dependencies that capture the
additional reasons for necessary orderings  defining the conditions on blocks sufficient to
encapsulate these dependencies would allow block deordering also of numeric plans  there
may be a synergy between block deordering and numeric planning  since numeric dependencies often involve groups of plan steps  rather than a single producerconsumer pair 
acknowledgment
this work was partially supported by the australian research council discovery project
dp          robust ai planning for hybrid systems  nicta is funded by the aus   

fisiddiqui   haslum

tralian government through the department of communications and the australian research council through the ict centre of excellence program 

references
ahuja  r  k   goodstein  j   mukherjee  a   orlin  j  b     sharma  d          a very
large scale neighborhood search algorithm for the combined through fleet assignment
model  informs journal on computing                 
ambite  j  l     knoblock  c  a          planning by rewriting  journal of artificial
intelligence research  jair                  
ambite  j  l   knoblock  c  a     minton  s          learning plan rewriting rules  in
proc  of the  th international conference on artificial intelligence planning systems 
aips       breckenridge  co  usa  april              pp       aaai press 
audibert  j  y   munos  r     szepesvari  c          explorationexploitation tradeoff using
variance estimates in multi armed bandits  theoretical computer science           
         
auer  p   cesa bianchi  n     fischer  p          finite time analysis of the multiarmed
bandit problem  machine learning                   
backstrom  c          computational aspects of reordering plans  journal of artificial
intelligence research  jair            
balyo  t   bartak  r     surynek  p          on improving plan quality via local enhancements  in proc  of the  th international symposium on combinatorial search  socs
      niagara falls  canada  july              aaai press 
bedo  j     ong  c  s          multivariate spearmans rho for aggregating ranks using
copulas  corr  abs           
bonet  b     geffner  h          planning as heuristic search  artificial intelligence                
cenamor  i   de la rosa  t     fernandez  f          ibacop and ibacop  planners  in
proc  of the  th international planning competition  ipc       deterministic part 
pp       
chapman  d          planning for conjunctive goals  artificial intelligence                 
chien  s   knight  r   stechert  a   sherwood  r     rabideau  g          using iterative
repair to improve the responsiveness of planning and scheduling  in proc  of the
 th international conference on artificial intelligence planning systems  aips      
breckenridge  co  usa  april              pp          aaai press 
chrpa  l     siddiqui  f  h          exploiting block deordering for improving planners efficiency  in proc  of the   th international joint conference on artificial intelligence 
ijcai       buenos aires  argentina  july              pp            aaai press 
coles  a  j   coles  a   olaya  a  g   celorrio  s  j   linares lopez  c   sanner  s     yoon 
s          a survey of the seventh international planning competition  ai magazine 
             
   

ficontinuing plan quality optimisation

copeland  a  h          a reasonable social welfare function  in university of michigan
seminar on applications of mathematics to the social sciences 
de borda  j  c          memory on election ballot  history of the royal academy of
sciences  paris         
dwork  c   kumar  r   naor  m     sivakumar  d          rank aggregation methods for
the web  in proc  of the   th international conference on world wide web  www
      hong kong  may            pp          new york  ny  usa  acm 
estrem  s  j     krebsbach  k  d          airs  anytime iterative refinement of a solution  in proc  of the   th international florida artificial intelligence research society
conference  marco island  florida  may             
fawcett  c   helmert  m   hoos  h   karpas  e   roger  g     seipp  j          fd autotune 
domain specific configuration using fast downward  in proc  of the      icaps
workshop on planning and learning  pal       freiburg  germany  june       
      pp        aaai press 
fox  m   gerevini  a   long  d     serina  i          plan stability  replanning versus plan
repair  in proc  of the   th international conference on automated planning and
scheduling  icaps       cumbria  uk  june              pp          aaai press 
furcy  d          itsa   iterative tunneling search with a   in proc  of the      aaai
workshop on heuristic search  memory based heuristics and their applications 
july             boston  massachusetts  pp        aaai press 
garivier  a     cappe  o          the kl ucb algorithm for bounded stochastic bandits
and beyond  corr  abs           
garrido  a   guzman  c     onaindia  e          anytime plan adaptation for continuous
planning  in proc  of the joint   th workshop of the uk special interest group on
planning and scheduling and  th italian workshop on planning and scheduling  pp 
     
gerevini  a   saetti  a     vallati  m          an automatically configurable portfolio based
planner with macro actions  pbp  in proc  of the   th international conference on
automated planning and scheduling  icaps       thessaloniki  greece  september
             pp          aaai press 
gerevini  a   saetti  a     vallati  m          pbp   automatic configuration of a portfoliobased multi planner  in  th international planning competition  ipc        learning
track  http   www plg inf uc m es ipc     learning 
gerevini  a     serina  i          lpg  a planner based on local search for planning graphs
with action costs  in proc  of the  th international conference on artificial intelligence
planning and scheduling  aips       april              toulouse  france  pp     
     aaai press 
gerevini  a  e     serina  i          fast plan adaptation through planning graphs  local
and systematic search techniques  in proc  of the  th international conference on
artificial intelligence planning systems  aips       breckenridge  co  usa  april
             pp          aaai press 
   

fisiddiqui   haslum

ghallab  m   nau  d  s     traverso  p          automated planning  theory   practice 
morgan kaufmann publishers inc   san francisco  ca  usa 
godard  d   laborie  p     nuijten  w          randomized large neighborhood search for
cumulative scheduling  in proc  of the   th international conference on automated
planning and scheduling  icaps       monterey  california  usa  june           
pp        aaai press 
haslum  p          computing genome edit distances using domain independent planning 
in proc  of the      icaps workshop on scheduling and planning applications 
spark       freiburg  germany  june              aaai press 
haslum  p          incremental lower bounds for additive cost planning problems  in proc 
of the   nd international conference on automated planning and scheduling  icaps
      atibaia  sao paulo  brazil  june              pp        aaai press 
haslum  p     grastien  a          diagnosis as planning  two case studies  in proc  of
the      icaps workshop on scheduling and planning applications  spark      
freiburg  germany  june              aaai press 
haslum  p     jonsson  p          planning with reduced operator sets  in proc  of the
 th international conference on artificial intelligence planning systems  aips      
breckenridge  co  usa  april              pp          aaai press 
helmert  m   roger  g   seipp  j   karpas  e   hoffmann  j   keyder  e   nissim  r   richter 
s     westphal  m          fast downward stone soup  planner abstract   in
proc  of the  th international planning competition  ipc       deterministic part 
http   www plg inf uc m es ipc     deterministic 
helmert  m     domshlak  c          landmarks  critical paths and abstractions  whats
the difference anyway   in proc  of the   th international conference on automated
planning and scheduling  icaps       thessaloniki  greece  september             
pp          aaai press 
hoffmann  j          local search topology in planning benchmarks  an empirical analysis 
in proc  of the   th international joint conference on artificial intelligence  ijcai
      seattle  washington  usa  august             pp          san francisco  ca 
usa  morgan kaufmann publishers inc 
hoffmann  j     nebel  b          the ff planning system  fast plan generation through
heuristic search  journal of artificial intelligence research  jair              
hutter  f   hoos  h  h   leyton brown  k     stutzle  t          paramils  an automatic
algorithm configuration framework  journal of artificial intelligence research  jair  
               
jones  d  m     gittins  j          a dynamic allocation index for the sequential design of
experiments  university of cambridge  department of engineering 
kambhampati  s     kedar  s          a unified framework for explanation based generalization of partially ordered and partially instantiated plans  artificial intelligence 
             
   

ficontinuing plan quality optimisation

mcallester  d     rosenblitt  d          systematic nonlinear planning  in proc  of the  th
national conference on artificial intelligence  aaai       anaheim  ca  usa  july
             volume     pp          aaai press   the mit press 
muise  c  j   mcilraith  s  a     beck  j  c          optimally relaxing partial order plans
with maxsat  in proc  of the   nd international conference on automated planning
and scheduling  icaps       atibaia  sao paulo  brazil  june              pp     
     aaai press 
nakhost  h     muller  m          monte carlo exploration for deterministic planning 
in proc  of the   st international joint conference on artificial intelligence  ijcai
      pasadena  california  usa  july              vol     pp           
nakhost  h     muller  m          action elimination and plan neighborhood graph search 
two algorithms for plan improvement  in proc  of the   th international conference
on automated planning and scheduling  icaps       toronto  canada  may       
      pp          aaai press 
nebel  b     backstrom  c          on the computational complexity of temporal projection  planning  and plan validation  artificial intelligence                 
pandey  s   chakrabarti  d     agarwal  d          multi armed bandit problems with
dependent arms  in proc  of the   th international conference on machine learning 
icml       corvallis  oregon  usa  june              vol       pp          acm 
pednault  e  p  d          formulating multiagent  dynamic world problems in the classical
planning framework  reasoning about actions and plans       
pohl  i          heuristic search viewed as path finding in a graph  artificial intelligence 
              
ratner  d     pohl  i          joint and lpa   combination of approximation and search  in
proc  of the  th national conference on artificial intelligence  aaai       philadelphia  pa  august              volume    science   pp          morgan kaufmann 
regnier  p     fade  b          complete determination of parallel actions and temporal
optimization in linear plans of action  in proc  of the european workshop on planning 
ewsp       sankt augustin  frg  march              vol      of lecture notes in
computer science  pp          springer 
richter  s     helmert  m          preferred operators and deferred evaluation in satisficing
planning  in proc  of the   th international conference on automated planning and
scheduling  icaps       thessaloniki  greece  september              pp         
aaai press 
richter  s   thayer  j  t     ruml  w          the joy of forgetting  faster anytime search
via restarting  in proc  of the   th international conference on automated planning
and scheduling  icaps       toronto  canada  may              pp          aaai
press 
richter  s     westphal  m          the lama planner  guiding cost based anytime
planning with landmarks  journal of artificial intelligence research  jair          
    
   

fisiddiqui   haslum

robbins  h          some aspects of the sequential design of experiments  in herbert
robbins selected papers  vol      pp          springer 
ropke  s     pisinger  d          an adaptive large neighborhood search heuristic for
the pickup and delivery problem with time windows  transportation science         
       
scala  e     torasso  p          deordering and numeric macro actions for plan repair 
in proc  of the   th international joint conference on artificial intelligence  ijcai
      buenos aires  argentina  july              pp            aaai press 
schrimpf  g   schneider  j   stamm wilbrandt  h     dueck  g          record breaking
optimization results using the ruin and recreate principle  journal of computational
physics                  
shaw  p          using constraint programming and local search methods to solve vehicle
routing problems  in proc  of the  th international conference on principles and
practice of constraint programming  cp         pisa  italy  october             
vol       of lecture notes in computer science  pp          springer 
siddiqui  f  h     haslum  p          block structured plan deordering  in proc  of the   th
australasian joint conference on advances in artificial intelligence  ai       sydney 
australia  december            vol       of lecture notes in computer science  pp 
        berlin  heidelberg  springer 
siddiqui  f  h     haslum  p       a   local search in the space of valid plans  in proc 
of the      icaps workshop on evolutionary techniques in planning and scheduling  evops       rome  italy  june              pp        http   icaps   icapsconference org wp content uploads         evops   proceedings pdf 
siddiqui  f  h     haslum  p       b   plan quality optimisation via block decomposition 
in proc  of the   rd international joint conference on artificial intelligence  ijcai
      beijing  china  august            pp            aaai press 
slivkins  a          contextual bandits with similarity information  journal of machine
learning research                   
stern  r  t   puzis  r     felner  a          potential search  a bounded cost search
algorithm  in proc  of the   st international conference on automated planning and
scheduling  icaps       freiburg  germany june              pp          aaai
press 
thayer  j   stern  r   felner  a     ruml  w       a   faster bounded cost search using
inadmissible heuristics  in proc  of the   nd international conference on automated
planning and scheduling  icaps       atibaia  sao paulo  brazil  june             
pp          aaai press 
thayer  j  t   benton  j     helmert  m       b   better parameter free anytime search
by minimizing time between solutions  in proc  of the  th international symposium
on combinatorial search  socs       niagara falls  canada  july              pp 
        aaai press 
   

ficontinuing plan quality optimisation

thayer  j  t     ruml  w          bounded suboptimal search  a direct approach using
inadmissible estimates  in proc  of the   nd international joint conference on artificial intelligence  ijcai       barcelona  catalonia  spain  july              pp 
        aaai press 
vallati  m   fawcett  c   gerevini  a   hoos  h     saetti  a          parlpg  generating domain specific planners through automatic parameter configuration in lpg  in
proc  of the  th international planning competition  ipc       deterministic part 
http   www plg inf uc m es ipc     deterministic 
veloso  m  m   perez  a     carbonell  j  g          nonlinear planning with parallel
resource allocation  in proc  of the darpa workshop on innovative approaches to
planning  scheduling and control  san diego  california  november            pp 
        morgan kaufmann 
wang  y   audibert  j     munos  r          algorithms for infinitely many armed bandits 
in proc  of the   nd annual conference on neural information processing systems 
nips       vancouver  british columbia  canada  december             pp      
      curran associates  inc 
xie  f   nakhost  h     muller  m          planning via random walk driven local search  in
proc  of the   nd international conference on automated planning and scheduling 
icaps       atibaia  sao paulo  brazil  june              pp          aaai press 
xie  f   valenzano  r  a     muller  m          better time constrained search via randomization and postprocessing  in proc  of the   rd international conference on
automated planning and scheduling  icaps       rome  italy  june             
pp          aaai press 
young  h  p     levenglick  a          a consistent extension of condorcets election principle  siam journal on applied mathematics                 
zhou  r     hansen  e  a          beam stack search  integrating backtracking with beam
search  in proc  of the   th international conference on automated planning and
scheduling  icaps       monterey  california  usa  june             pp       
aaai press 

   

fi