journal artificial intelligence research               

submitted          published        

properties bethe free energies message passing
gaussian models
botond cseke
tom heskes

b cseke science ru nl
t heskes science ru nl

institute computing information sciences
faculty science  radboud university nijmegen
heyendaalseweg           aj  netherlands

abstract
address problem computing approximate marginals gaussian probabilistic
models using mean field fractional bethe approximations  define gaussian fractional bethe free energy terms moment parameters approximate
marginals  derive lower upper bound fractional bethe free energy
establish necessary condition lower bound bounded below  turns
condition identical pairwise normalizability condition  known
sufficient condition convergence message passing algorithm  show
stable fixed points gaussian message passing algorithm local minima
gaussian bethe free energy  counterexample  disprove conjecture stating
unboundedness free energy implies divergence message passing
algorithm 

   introduction
one major tasks probabilistic inference calculating marginal posterior probabilities set variables given observations  case gaussian models 
computational complexity computing marginals might scale cubically number
variables  models discrete variables often leads intractable computations 
computations made faster tractable using approximate inference methods
mean field approximation  e g   jaakkola        bethe type approximation  e g  
yedidia  freeman    weiss         methods developed discrete probabilistic
graphical models  applicable gaussian models well  however 
important differences behavior discrete gaussian cases  example 
discrete models error function bethe approximationcalled bethe free
energyis bounded  heskes        watanabe   fukumizu         gaussian
models might always case  welling   teh        
understanding properties bethe free energy gaussian models might
help understand properties energy function conditional gaussian
models  conditional gaussian hybrid graphical models  switching kalman filters
 zoeter   heskes         combine discrete gaussian variables  approximate
inference models carried expectation propagation  e g   minka       
      viewed generalization bethe approximation 
marginal consistency constraints approximate marginals replaced expectation
constraints  heskes  opper  wiegerinck  winther    zoeter         order understand
c
    
ai access foundation  rights reserved 

ficseke   heskes

properties bethe free energy hybrid models  good understanding two
special cases discrete gaussian models needed  properties bethe
free energy discrete models studied extensively last decade well
understood  yedidia et al         heskes        wainwright  jaakkola    willsky       
watanabe   fukumizu         properties gaussian bethe free energy
studied much less 
message passing algorithm well established method finding stationary
points bethe free energy  yedidia et al         heskes         works locally
updating approximate marginals successfully applied discrete  e g  
murphy  weiss    jordan        wainwright et al         gaussian models  e g   weiss
  freeman        rusmevichientong   roy        malioutov  johnson    willsky       
johnson  bickson    dolev        nishiyama   watanabe        bickson         gaussian
message passing simplest case free energy based message passing algorithm
models continuous variables  therefore  important understand behavior 
gaussian message passing many practical applications distributed averaging
 moallemi   roy         peer to peer rating  linear detection  svm regression  bickson 
      generally problems involve solving large sparse linear systems
approximating marginal variances large sparse gaussian systems typically encountered distributed computing settings  applications reader referred
work bickson        references therein 
finding sufficient conditions convergence message passing gaussian models
successfully addressed many authors  using computation tree approach 
weiss freeman        proved message passing converges whenever precision
matrixinverse covarianceof probability distribution diagonally dominant   
help analogy message passing walksum analysis   malioutov et al  
      derived stronger condition pairwise normalizability    different approach
taken welling teh         directly minimized bethe free energy regard
parameters approximate marginals  conjecturing gaussian message passing
converges free energy bounded below  experiments showed
message passing direct minimization either converge solution
fail converge  adopt similar approach  is  instead analyzing properties
gaussian message passing algorithm using approaches weiss freeman
malioutov et al   choose study properties gaussian bethe free energy
stationary points  help us draw conclusions existence local
minima  possible stable fixed points message passing converge 
paper structured follows  section   introduce gaussian markov random
fields message passing algorithm  section   define gaussian fractional
bethe free energies parameterized moment parameters approximate marginals
derive boundedness conditions them  two sections based authors
earlier work  cseke   heskes         section   analyze stability properties
gaussian message passing algorithm and  using similar line argument watanabe
p
   matrix diagonally dominant  aii     j  i  aij   i 
   following work malioutov et al          call gaussian distribution pairwise normalizable

q
factorized product normalizable pair factors  is  p x            xn     ij ij  xi   xj  
ij normalizable 

 

fibethe free energies message passing gaussian models

fukumizu         show stable fixed points indeed local minima bethe
free energy  conclude paper experiments sections     supporting
results implications 

   approximating marginals gaussian models
probability density gaussian random vector x rn defined terms canonical
parameters h q


 

p x  exp h x x qx  
   
 
q positive definite matrix  expectation covariance v x
given   q  h v   q  respectively  many real world applications
matrix q sparse typically low density  is  number non zero
elements q scales number variables n 
probability density defined terms undirected probabilistic
graphical model commonly known gaussian markov random field  gmrf   since
interactions variables p pairwise  associate variables xi
nodes v v               n  undirected graph g    v  e   edges e e v v
graph stand non zero off diagonal elements q  use j proxy
 i  j  e  using notation introduced above  density p     written
product

p x 
ij  xi   xj  
   
ij

gaussian functions ij  xi   xj    also called potentials  associated edges e    i  j 
graph  h q given define potentials
j
j


ij  xi   xj     exp  ij
hi xi   ij
hj xj ij
qii x i    ij
qjj x j    qij xi xj    

p
p
j


ij ij    
ji ij     partitioning h q corresponding
factors  practice  however  factors ij might given problem hand
j computed summing parameters computing
h q well ij
ij
partitioning respectively  without loss generality  use qii      since
results paper easily re formulated general qs rescaling
variables  e g   malioutov et al         
numerical calculation marginals  done solving linear system
  q  h performing sparse cholesky factorization llt   q followed solving takahashi equations  takahashi  fagan    chin         alternative option
calculate marginal means approximate marginal variances run gaussian message passing algorithm probabilistic graphical model associated
representation      gaussian message passing algorithm gaussian variant
message passing algorithm  pearl         dynamical programming algorithm
introduced compute marginal densities discrete probabilistic models pairwise interactions tree structured graphs g  however  turned running loops
graphs cycles  yields good approximations marginal distributions  murphy
et al          weiss freeman        showed gaussian message passing
 

ficseke   heskes

figure    illustration incoming outgoing messages adjacent nodes j 

algorithm converging  computes exact mean parameters m  thus
used solving linear systems  e g   bickson         message passing works updating
passing directed messages along edges graph g  which  case algorithm
converges  used compute  approximate  marginal probability distributions 
gaussian discrete algorithms functional form exception
summation  discrete case  integration operators  gaussian case   message
ij  xi   updated according
z

new
ij  xi     dxj ij  xi   xj  
jk  xj    
   
kj i

   j   j i  denotes index set variables connected xi g  step
current approximations qij  xi  j   p xi   xj   computed according
qij  xi   xj   ij  xi   xj  


li j

il  xi  



jk  xj    

   

kj i

update steps     iterated convergence  corresponding qij  xi   xj  s
yield final approximation p xi   xj  s  common use damping  is 
  new  x           practice  helps
replace new
ij  xi   ij  xi  
ij
dampen possible periodic paths      keeps properties fixed points
unchanged  figure   illustrates incoming outgoing messages nodes associated
variables xi xj   quite significant difference discrete gaussian
message passing replacement sum operator integral operator 
finite sums always exist  integral     become infinite  problem
remedied technically canonical parameterization  see section    keeps
algorithm running  lead non normalizable approximate marginals qij   thus
 possible  break down algorithm 
message passing introduced pearl        heuristic algorithm  in discrete
models   however  yedidia et al         showed viewed algorithm
 

fibethe free energies message passing gaussian models

finding stationary points so called bethe free energy  error function measuring
difference p specific family distributions detailed next
section  shown heskes        later different way watanabe
fukumizu        stable fixed points  loopy  message passing algorithm local
minima corresponding bethe free energy  paper show holds
gaussian models well 
interest properties gaussian bethe free energy corresponding gaussian message passing algorithm motivated mainly implications
general models inference algorithms non gaussian models expectation propagation  respectively  reason  compare speed method
accuracy approximation mentioned exact linear algebraic methods 
mentioned introduction  approach take similar welling
teh         is  study properties gaussian bethe free energy  parameterized
terms moment parameters approximate marginals  following
introduce mean field bethe approximation gaussian models  readers familiar
subject continue section   
    gaussian bethe free energy
popular method approximate marginals approximating p distribution q
form makes marginals easy identify  example  factorizes treelike form  common quantity measure difference two probability
distributions kullback leibler divergence  q    p   often used characterize
quality approximation formulate computation approximate marginals
optimization problem


z
q x 

q  x    argmin dx q x  log
 
   
p x 
qf
here  f set distributions mentioned form  since symmetric 
kullback leibler divergence distance   q    p    proper q p 
 q    p      p   q  convex q p 
family f densities possessing form
q makes marginals easy identify
family distributions factorize q x    k qk  xk    words  problem    
approximate p distribution independent variables  approximation q
thisqtype called mean field approximation  e g   jaakkola         defining fmf   qk     
  qk    p  writing right hand side     detail  one gets
z
fmf   qk     

dx



qk  xk   log p x   

k

xz

dxk qk  xk   log qk  xk   

k

using parameterization qk  xk     n  xk  mk   vk       m            mn  t v    v            vn  t  
reduces
 
 x
 x
fmf  m  v    ht   mt qm  
qkk vk
log vk     cmf  
 
 
 
k

 

k

ficseke   heskes

q
cmf irrelevant constant  although   k qk    p  might convex
 q            qn    one easily check fmf convex variables v
minimum obtained   q  h vk     qkk   since

 
 

 
q kk   qkk qtk  k q k  k
q k k
 
one easily see mean field approximation underestimates variances  mean
field approximation computes solution means exact  variances
computed interactions variables  namely  matrix
q diagonal  thus giving poor estimates variances 
order improve estimates variances  one choose approximating distributions q able capture dependencies variables p 
verified distribution dependencies form tree graph written
form
p xi   xj  
p x   
p xk   
p xi  p xj  
ij

k

j run edges  i  j  tree k nodes            n 
although cases undirected graph generated non zero elements q
tree  based tree intuition one construct q one two variable
marginals
qij  xi   xj  
q x 
qk  xk  
   
qi  xi  qj  xj  
ij

k

andr constrain functions qij qk marginally
consistent normalize   
r
is  dxj qij  xi   xj     qi  xi   j dxk qk  xk       k  approximation
form     together constraints qij qk called bethe approximation 
let us denote family functions fb   choosing qij  xi   xj     qi  xi  qj  xj   one
easily check fmf fb   thus fb non empty  assuming approximate
marginals correct q normalizes   substituting          get
approximation kullbackleibler divergence     called bethe free energy 
due factorization p  write bethe free energy
xz
fb   qij   qk     
dxi j qij  xi j   log ij  xi j  
   
ij

 

xz
ij



xz
qij  xi j  
dxi j qij  xi j   log
 
dxk qk  xk   log qk  xk   
qi  xi  qj  xj  
k

one define free energy bethe approximation
z
xz
dx q  x  log q  x 
dxi j q  xi j   log q  xi j  
ij

 

x
k

 

z
   nk  

dxk q  xk   log q  xk  

fibethe free energies message passing gaussian models

entropy  e g   yedidia et al         substitute marginals functions qij
qrk normalize one connected marginal consistency constraints
dxj qij  xi   xj     qi  xi   
stationary conditions lagrangian corresponding fractional bethe
free energy     marginal consistency normalization constraints  one derive
iterative algorithm     corresponding lagrange multipliers
consistency constraints  yedidia et al          similarly  approximate marginals
computed according      shown one to one correspondence
stationary points bethe free energy     fixed points
message passing algorithm      later  section   link stable fixed points    
local minima     
    fractional free energies message passing algorithm
mentioned introduction  case gaussian models message passing algorithm
always converge  reason appears approximate marginals
may get indefinite negative definite covariance matrices  welling teh        pointed
due unboundedness bethe free energy 
since fmf convex bounded bethe free energy might unbounded 
seems plausible analyze fractional bethe free energy
xz
f   qij   qk     
dxi j qij  xi j   log ij  xi j  
   
ij


xz
x   z
qij  xi j  
 
dxi j qij  xi j   log
 
dxk qk  xk   log qk  xk   
ij
qi  xi  qj  xj  
ij

k

introduced wiegerinck heskes         here  denotes set positive reals  ij   
showed fractional bethe free energy interpolates mean field
bethe approximation  is  ij     get bethe free energy 
case ij tend    mutual information variables xi xj highly
penalized  therefore      enforces solutions close mean field solution  showed
fractional message passing algorithm derived     interpreted pearls
message passing algorithm difference instead computing local marginals
pearls algorithmone computes local ij marginals   local ij marginals
correspond true local marginals ij     local mean field approximations
ij      resulting algorithm called fractional message passing algorithm
message updates defined
z


new
 x
 
 
dxj ij  xi   xj  
jk  xj   ji  xj     
   
ij
kj i

approximate marginals computed according


qij  xi   xj   ij  xi   xj  
il  xi   ij  xi   
jk  xj   ji  xj     
li j

    

kj i



q
   define marginals distribution p argmin qk   p k qk   divergence
k
r

r
r
 p    q    dxp x  q x     dxp x         dxq x         e g   minka        

 

ficseke   heskes

power expectation propagation minka        approximate inference method
uses local approximations divergences  case gaussian models power
expectation propagationwith fully factorized approximating distributionleads
message passing algorithm one derived     appropriate constraints 
starting idea creating upper bound log partition function p
q exponential distributions  wainwright et al         derived form    
ij chosen bound convex  qij   qk   
message passing works well practice  however  ways find local
minima fractional free energies direct minimization w r t  parameterization approximate marginals qij qk  welling   teh         latter method
slower likely converge  following analyze bethe free energy
expressed terms moment parameters approximate marginals qij   later
section   analyze stability conditions fractional message passing algorithm
expressing conditions term moment parameters approximate
marginals  show stable fixed points fractional gaussian message passing
local minima fractional bethe free energy 

   bounds gaussian bethe free energy
section analyze parametric form      show fractional gaussian bethe free energy non increasing function   letting ij tend infinity  obtain lower bound free energies  turns condition
lower bound bounded pairwise normalizability
condition work malioutov et al         
mentioned section    without loss generality  work unit diagonal q  define r matrix zeros diagonal q     r 
identity matrix   r  matrix formed absolute values
rs elements  use moment parameterization qij  xi j     n  xi j  mij   vij  
  v   v   v j    v   v  
qk  xk     n  xk  mk   vk    mij    miij   mjij  t vij    vij
ij
ji ij
ij
ji
  mi v v   v k j k  embed
using mi

ij
ik
ik
r ij
r
marginalization   dxj qij  xi   xj     qi  xi   j  normalization   dxj qj  xj       
constraints parameterization  slight abuse notation matrix formed
diagonal elements vk off diagonal elements vij denoted v  we take vij    
j   vector means    m            mn  t vector variances
v    v            vn  t   substituting qij qk     one gets
 
 
f  m  v     ht   mt qm   tr qt v  
 
   
 
x
v
 
 
 x
ij

log  

log  vk     c 
 
ij
vi vj
 
ij

    

k

c irrelevant constant  note variables v independent  hence
minimizations f  m  v   regard v carried independently 
 

fibethe free energies message passing gaussian models

property    f  m  v   convex bounded  m   vij  i  j   stationary point

  q  h
vij



p
      ij rij    vi vj  
  sign rij  
 
 ij  rij  

    

proof  q positive definite definition  therefore  quadratic term convex
bounded  variables v independent minimum regard
achieved   q  h  one check second order derivative
f  m  v   regard vij non negative first order derivative one
  v v   since variables v independent  one conclude
solution vi vj vij
j
ij
f  m  v   convex vij   independence v   follows f
convex  m   vij  i  j   

    thus
since vij constrained covariance matrices  vi vj   vij
first logarithmic term      negative  consequence 

f   m  v   f   m  v  



         

    taken element element  observation leads following
property 
property    ij     f non increasing function  
f define constrained function
using property   substituting vij

 
 x
fc  m  v    ht   mt qm  
vk
 
 
k
q

 x  
      ij rij    vi vj  

 

ij ij
 
p
      ij rij    vi vj  
  x  

log  
 
ij
  ij rij    vi vj
n i j 

 x

log vk     c c  
 

    

k

c c irrelevant constant  property    follows choosing ij    
function      non increasing function   makes sense take
verify whether get lower bound      
lemma    v                following inequalities hold 


fmf  m  v  fc   m  v  fb m   vij
   v


fb m   vij
   v fc   m  v       

 
      fmf  m  v 
v  r  v
 
moreover  tight  is 


lim f m   vij
     v   fmf  m  v 
 

 

ficseke   heskes





 

v  r  v 
lim f m   vij
     v   fmf  m  v 
 
proof  since bethe free energy specific case fractional bethe free energy
     v  follow property    now  show
     inequalities fb  m   vij
upper lower bounds tight  function      x         behaves    x 
neighborhood    therefore 


v     
log   ijvi vj
    
vij
 

lim
lim vij
      

lim
 
    
 
 

vi vj  


showing fmf  m  v  tight upper bound 
tends infinity 
p
      rij    vi vj  

   rij   vi vj
lim

 

 
log
lim


 
p
      rij    vi vj  
    
  rij    vi vj

yielding tight lower bound


 

v  r  v 
lim f m   vij
     v   fmf  m  v 

 



let max   r   largest eigenvalue  r   analyzing boundedness lower
bound  arrive following theorem 
theorem    fractional bethe free energy      corresponding connected
gaussian model  following statements hold
    max   r        f bounded     
    max   r        f unbounded     
p p  
    max   r        f bounded
ij  n 
ij

proof  since f interaction parameters v term
depending bounded due positive definiteness q  simply
neglect term analyzing boundedness f   let us write detail
lower bound fractional bethe free energies form

 
v  r  v  
 

 
 
   
q ht  
v  i  r   v  t log v    const 
 
 
 

fmf  m  v 

    

statement      condition max   r       implies  r  positive definite  now 
  

fibethe free energies message passing gaussian models







log x  x    thus    v  i  r   v  t log  v     v  i  r   v  t v   n 
latter bounded follows      bounded
well  according lemma    boundedness      implies fractional bethe free
energies bounded below 
statement      assumed gaussian network connected undirected  according perron frobenius theory non negative matrices  e g   horn   johnson 
        r  simple maximal eigenvalue max   r   elements eigenvector umax corresponding positive  let us take fractional bethe free energy

analyze behavior v   tumax   large values
       ij rij     uimax ujmax    t          ij  rij  uimax ujmax t    therefore  sum second
third term      simplifies    max   r   t  term dominates
logarithmic ones   result  limit independent choice ij
tends whenever max   r       
statement      max   r        direction quadratic term

dominate v   tumax   therefore  analyze p
behavior loga 
rithmic terms        large ts behave   ij ij
 n  log t  
c
reason  boundedness f thus f depends condition
statement     

shown malioutov et al         condition max   r       equivalent
condition pairwise normalizability  therefore  pairwise normalizability
sufficient condition message passing algorithm converge  necessary
condition fractional gaussian bethe free energies bounded  using lemma   
show suitably chosen     always exists constrained
fractional free energy fc possesses local minimum        property a 
section appendix  
example case models adjacency matrix  non zero entries r  corresponding kregular graph  equal interaction weights rij   r  maximal eigenvalue  r  max   r     kr eigenvector corresponding eigenvalue   
 we define   vector elements equal     model symmetric
verifying stationary point conditions  turns choice r
exists local minimum  lies direction    one show
model pairwise normalizable  kr       critical r
p fractional
bethe free energy possesses local minimum rc  k           k  
valid r critical
p fractional bethe free energies possesses local
minimum c  k  r       k        kr      results illustrated figure   
 note  regular graphs  valid models pairwise normalizable possess
unique global minimum  

kregular graphs  convexity fractional bethe free energy terms
 qij   qk   requires k  much stronger condition c  k  r   thus  choose
sufficiently large bethe free energy guaranteed unique global
minimum  minimum unbounded 

   kregular graph graph nodes connected k nodes 

  

ficseke   heskes

 

  

 

 

  

                
  d           
     
   vtuwff fi
   

 

                   

                   

 

  

     



  

 

  



  

 

                
  d           
           
    

  

                   


  

 

  

 





  



  

 

  


 

  

  



 



  

  



  

 

  


 

  



  

figure    visualizing critical parameters symmetric k regular gaussian model rij   r 
plots
left panel correspond constrained fractional bethe free energies fc

v       node  regular gaussian model r       kr      varying
 

plots right panel correspond constrained bethe free energies f c v    
  node  regular gaussian model varying r  here  rvalid supremum
rs model valid  is  q positive definite 

example disproves conjecture welling teh         is  even
bethe free energy bounded below  possess finite local minimum
message passing minimization algorithms converge 

   message passing algorithm gaussian models
section  turn attention towards properties message passing algorithm gaussian models  following similar line argument watanabe fukumizu
       show stable fixed points message passing algorithm correspond local
minima bethe free energy  use moment parameterization introduced
previous sections  way proceed following      make linear expansion
message passing iteration fixed point      express linear expansion terms
moment parameters corresponding fixed point finally     connect properties latter properties hessian bethe free energy using
matrix determinant lemma 
form equation     implies messages ij  xi   univariate gaussian
functions  thus express terms two scalar  canonical  parameters ij
ij log ij  xi     ij x i      ij xi   ijj   ij irrelevant constants 
expressed terms ij ij   damped message passing algorithm     translates
  

fibethe free energies message passing gaussian models


j
hj  
ij


new
ij

 

    ij  

p

jk       ji




kj i

p
ij hi rij

j

ij
 
jk       ji

    

kj i



new
ij

 

 
x


j
 
  rij
ij
 
jk       ji
    ij   ij



kj i

    
  j   h r parameters section      r   q
ij

ij
ij
ij
ij
ij
assumption qii      approximate marginals qij      might normalizable 
message passing iteration           stays well defined unless zero
denominator rhs  rarely happens practice  however 
common message passing converges intermediate steps
approximate marginals qij normalizable  often remedied choosing
appropriate damping parameter  
iteration      ij independent ij iteration      ij
linear ij   interesting see h     neither constrained bethe
free energy      message passing algorithm      depend sign rij  
relevant compute meanswhen h     and signs correlations
      result  marginal variances computed either minimizing bethe free
energy running message passing algorithm depend  r   similarly
constrained fractional free energy fc  

    stability gaussian message passing algorithm
following analyze stability message passing iteration fixed points 
is  stationary points lagrangian corresponding constrained minimization gaussian bethe free energy  reiterate use g    v  e  denote
graph corresponding q  namely  v               n  e     i  j    qij        vector r e    corresponding set messages  ij  ij   composed concatenation
ij ij followed ji  ij  ji  blocks follow lexicographic order w r t 
ij   j  vector consists variables ij follows similar structure  
j
define r  h  r e  rij   rji   rij   hij   hj ij   ij
  define
 e   e  matrix

  j   k

  kl   ji
mij kl   

  otherwise
encodes weighted edge adjacency corresponding g   number nonzero elements m    scales roughly nnzeros  q    n  nnzeros  q  denotes
number non zeros q  since parallel message update given equations     
     rewritten terms two matrix vector multiplications element element
operations vectors  computational complexity update scales roughly
nnzeros  q    n 
  

ficseke   heskes

notation  local linearization update equations          
written
  new   new  
           i      
    






h m  
 
diag r  m   m   diag r   m     m  

 


 
 

 
diag   r     m   
m  
 

    

operations vectors element element  stability fixed point
      depends union spectra

j         diag r    m      m  


j         diag   r       m      m   
important point stability properties depend r
independent h 
goal connect stability properties message passing algorithm
properties bethe free energy  therefore  express stability properties terms
moment parameters approximate marginals  leads normalizable approximate marginals qij  xi   xj    use      identify local covariance
parameters vij defined section    without enforcing marginal matching
  v   correspondence given
constraints vij
ik
 


vij
vij

vij
j
vij


 

  

 

 

 

j
vij
vij

vj v 
vij
ij
ij
p
 
il       ij
ij

vij

vij

 
    
rij

li j
j
ij

rij

 

p

jk       ji



 

kj i
  v j r
approximate local covariances vij fully determined vij
ij
ij
form       leaves us  e  moment parameters computed
  v   v j  v   
message passing algorithm  let v r e  defined vij   vij
ji
ij
ij
v j v      v
vij   vij
ij
ij computed according       checked
ij
mapping v continuous bijective  implies canonical
moment parameter transformation      written y v      m    since
m   singular   k graph g k regularsee property a 
section appendix detailsfor rest cases  continuous 
bijective mapping moment parameters v canonical parameters
lead normalizable approximate marginals 
  v v
fixed point       moment matching  is  vij

ik
k  j i  therefore express stability properties terms moment parameters

  

fibethe free energies message passing gaussian models

v    vi           vn    using p
     defining diagonal matrix r e  e 
diagonal elements dij ij   vi   get

  v 
v
  
v
ij

j
    diag q
m  


vi vj


dj    v   d  

    


 





j    v   d

 

 

 

diag

vij    vi   vj   
vi vj

 
m   

    


let  a  denote
spectrum matrix a  since dj      j  

  j      j    sufficient analyze spectral properties right hand
sides equations           
message passing algorithm asymptotically stable  v  
max    j    v        j    v          

    

   denotes spectral radius  interesting see although functional
forms free energies message passing algorithms different gaussian
discrete case  stability conditions similar forms  allow us use
results watanabe fukumizu         next section  show
implications condition properties hessian free energy 
    stable fixed points local minima
hessian h f   bethe free energy      depends moment parameters
vi   vj vij   note now  vij unconstrained parameters    e       n 
  e       n  matrix form


q


 
h f   v    

 

diag
h  

   

f
  vij


h

f
vij vi ij i

 

  f
vij vi ij i

h



  f
vi vj i j




 


use v denote collection parameters vi                n vij   j 
since block corresponding partial differentials w r t  vij diagonal positive
elements  hessian positive definite v schur complement corresponding
  

ficseke   heskes

partial differentials w r t  vi positive definite v   latter given
x   f   f  
  f
v

hii  f   v    
vi vi
vij vi
vij
ij


   
  x c ij
 
 
 
 
 
  vi 

 

c
ij
ij

 
 
 
f
f   f   f
v
hij  f   v    

vi vj
vij vi vij vj   vij
      c ij
 
 
  vi vj   c ij

use notation cij   vij   vi vj  
now  would connect condition      positive definiteness
matrix h v  f   v    following show stable fixed points  v   gaussian
message passing algorithm  satisfying       correspond local minima gaussian free
energy f v vij    vi   vj   
according watanabe fukumizu         arbitrary vector w r e  one



det i e    diag  w  m     det     a w 
   wij wji   
    
ij


aii  w   

x
ij

wij wji
  wij wji



aij  w   

wij
 
  wij wji

    

proof application matrix determinant lemma reproduction
found section appendix  equation      expresses determinant
 e  e  matrix determinant nn matrix 

let c r e  cij  v     vij   vi vj   substituting w   c v          find


det   diag c v    m     f  v   det  h f   v     
    
f  v   positive function defined
f  v      n  e   q  


k

vk 


 
 
vi vj vij
ij

 
vi vj   vij

 
vij
 
vi vj

 
 

v corresponding normalizable approximate marginals  now  adapting theorem
watanabe fukumizu       

following theorem 
theorem   diag c v    m   c   r  hessian  gaussian 
bethe free energy h f   positive definite
v 
 
 
proof  assumption
diag c v   m   c   r  implies
det   diag c v    m         choosing vij  t    tvij         find
 
c v t      t  c v      therefore  det   diag c v  t   m               
  

fibethe free energies message passing gaussian models

implies det  h f   v  t                since h f   v           
eigenvalues h f   v  t   change continuously w r t          results
h f   v          v   thus satisfying condition theorem 



fixed point       stable
max  j    v       j    v          
 

 
implies diag c v    m   c   r  leads following property 

property    stable fixed points       damped gaussian message passing algorithm      local minima gaussian bethe free energy fc      v     
shows boundedness f existence local minima case
unbounded f plays significant role convergence gaussian message passing  illustrate section    fractional message passing algorithm converges
converges set messages corresponds local minimum fractional free energy  implies mean parameters local approximate
marginals exact  see property    section     note observations section  
property a  appendix together property   imply always
range values fractional free energy possesses local minimum
fractional message passing converge 
    damping fractional parameters
local stability condition      independent damping parameter   therefore 
alter local stability properties  makes iteration slower numerically stable  is  dampen possible periodic trajectories message
passing algorithm 
fractional parameter characterizes inference process seen
example previous sections  choosing smaller create local minima 
particular case h      somewhat similar property message passing
updates well  let r e  set messages lead normalizable approximate
marginals  set characterized model parameters  r     reiterate
v j continuous bijective
elements v local variances vij
ij
 e 

mapping v r  given y v      m    unless   k g
k regular  allows us study q
stability properties terms moment parameters
  v j   
v    let c v       vij    vij
ij

v j   vector local correlations  using
vij
ij ij

gershgorins theorem  horn   johnson        c v     c v     find
eigenvalue   diag c v    m     diag c v      m  


   max   c v      nj             
i j

h      updates   rhs equation depends
  c v      see equations            lim   c v          thus  small
 

values help achieve convergence  however  h      term   c v   
dominating effects decreasing towards zero ambiguous 
  

ficseke   heskes

   experiments
implemented direct minimization fractional message passing analyzed
behavior different values max   r    reasons simplicity  set ij
equal  results small scale model summarized figure    note
good correspondence behavior fractional bethe free energies
direction eigenvalue corresponding max   r   convergence newton
method  newton method started different initial points  experienced
max   r       setting initial value v    t  u max   algorithm
converge high values t  explained top plots figure   
high values t  initial point might convergence region local
minimum  fractional message passing algorithm used two types initialization 
 
    max   r       set ij normalizable setting ij
    n  
 rij  ujmax  max uimax  malioutov et al              max   r      used ij

is  symmetric partitioning diagonal elements  set initial messages
approximate marginals normalizable first step iteration 
experienced behavior similar described welling teh       
standard message passing  namely  fractional message passing direct minimization either
converge fail converge  experiments combination theorem  
show max   r        standard message passing best converges local
minimum bethe free energy  standard message passing fails converge  one
decrease search stationary pointpreferably local minimumof
corresponding fractional free energy 
seen results right panels figure    model
longer pairwise normalizable  local minimum unbounded global minimum
viewed natural continuation  bounded  global minimum pairwise
normalizable models  explains quality approximation local
minimum models pairwise normalizable still comparable
global minimum models pairwise normalizable 

   conclusions


seen  fmf fmf    v  r  v provide tight upper lower bounds
gaussian fractional bethe free energies  turns pairwise normalizability
sufficient condition message passing algorithm converge 
necessary condition gaussian fractional bethe free energies bounded
below 
model pairwise normalizable  lower bound bounded  direct
minimization message passing converging  experiments converged
minimum  suggests pairwise normalizable case  fractional bethe
free energies possess unique global minimum 
model pairwise normalizable  none fractional bethe free energies
bounded below  however  always range values
fractional free energy possesses local minimum direct minimization
fractional message passing converge  thus  decreasing towards zero  one gets
  

fibethe free energies message passing gaussian models

 

  

 

 

  

          
  d           
           
           

 

  

 

  

 

 

  

  

 

 

  

  



  

 





 

  

  

 

 

  
 

  

 

  

  

function value convergence

function value convergence

 
 
 
 
 
 
 

 

  

 

  
 

 

 

  

  

 

 

  

  


 
 
 
 
 
 
 
 
 

 

  

error variances convergence

newton method
message passing
 

  

 

  

 

  



 

 

 

 

  

 

error variances convergence

 

          
  d           
           
           

 

  

 

  

 

  

  


 

newton method
message passing
 

  

 

  

 

 

  

 

  


  

 

  

 

  

  


 

figure    top panels show constrained
fractional bethe free energies gaussian model

  variables direction v   tumax   umax eigenvector corresponding max   r   max   r          top left  max   r          top right  
thick lines functions fmf  dashed   fb  dashed dotted  lower bound


fmf    v  r  v  continuous   thin lines constrained  fractional free
energies fc               center panels show final function values
convergence newton method  bottom panels
show        error approximation single node standard deviations   v  missing values indicate
non convergence 
  

ficseke   heskes

closer mean field energy finite local minimum appear  property a 
appendix   experienced suitable range s s initial values
fractional gaussian message passing made converge 
mentioned section      ij correspond using local ij divergences applying power expectation propagation fully factorized approximating distribution 
seeger        reports expectation propagation converge  applying power
expectation propagation     helps achieve convergence  case problem
addressed paper behavior explained observation small
make finite local minima likely occur thus prevents covariance matrices
becoming indefinite even non positive definite  although common reason
using     ep numerical robustness  implies finding saddle point
 fractional ep free energy  might interesting investigate whether
reason convergence likely case gaussian fractional message passing 
wainwright et al         propose convexify bethe free energy discrete models
choosing ij sufficiently large fractional bethe free energy unique
global minimum  strategy appears fail gaussian models  convexification makes
possibly useful finite local minima disappear  leaving unbounded global minimum  case general hybrid models  use convexification still
unclear 
example section   disproves conjecture work welling teh        
even bethe free energy bounded below  possess finite local
minimum message passing minimization algorithms converge 
shown stable fixed points gaussian fractional message passing
algorithms local minima fractional bethe free energy  although existence
local minimum guarantee convergence message passing algorithm 
practice experienced existence local minimum implies convergence 
based results  hypothesize pairwise normalizability hold 
gaussian bethe free energy gaussian message passing algorithm       
two types behavior 
    gaussian bethe free energy possesses unique finite local minimum
optimization methods converge starting from  say  mean field solution
vi     qii   gaussian message passing corresponding unique stable fixed
point  converge suitable starting point sufficient damping 
    finite local minimum exists  thus  optimization message
passing algorithm diverge 
using fractional free energy fractional message passing varying  
one switch behaviors  computing critical c   r   general  r 
remains open question  believe properties free energies k regular
symmetric models  section     critical values easily computed  give good
insight properties free energies general gaussian models 
  

fibethe free energies message passing gaussian models

acknowledgments
would thank jason k  johnson sharing ideas properties
message passing algorithm k regular models  would thank anonymous
reviewers valuable comments earlier versions manuscript  research
reported paper supported vici grant             netherlands
organization scientific research  nwo  

appendix a  properties proofs
lemma a    watanabe   fukumizu        graph g    v  e   edge adjacency
matrix m    defined section       arbitrary vector w r e    one


det i e    diag  w  m     det i v       a w 
   wij wji   
ij


aii  w   

x
ij

wij wji
  wij wji

aij  w   



wij
 
  wij wji

proof  reproduce proof somewhat simplified form  let us define uij    etj  
vij    eti ek k th unit vector rn



sij ij sij ji
   
 
 
sji ij sji ji
   
m     u v s  let us define w r e  e  diagonal matrix
wij ij   wij   using matrix determinant lemma reads

det   w u v

  det   w   w u v



  det   w u v  i   w s   det  i   w s 


  det   v  i   w s   w u det  i   w s  
 ij  ji  block  i   w s   w


 
 
wij
wij
 
 
  wji wji wji

 
wji


 

 
  wji wji



wij
wji wij

wij wji
wji



thus  define v  i   w s   w u
x wij wji
wij
ai i  
ai j  
 
  wij wji
  wij wji
ij

completes proof matrix determinant lemma      section     

  



ficseke   heskes

property a   matrix m     u v singular k regular graphs
  k 
p
proof  let x r e   pm  x  yij   kj xjk xji   let us fix j 
yij     means kj xjk   xji i  hold graph
k regular    k xij equal xij     pair indices ij 

property a   suitably chosen      exists constrained
fractional free energy fc possesses local minimum        

proof  let us define vm
f   argminv fm f  v 


um
f    v   fm f  v  fm f  vm f         

form fm f implies always choose um
f proper subset
n

n
positive quadrant r   words  um f r    due properties fm f
 continuous convex  unique finite global minimum attained finite value  





domain um
f closed  bounded  convex vm f um f   um f   is  vm f

n
c

interior um f   since fm f f  v  continuous r    set um f closed
bounded lim fc  v    fm f  v   pointwise convergence  v rn    follows fc
 


c
converges uniformly um
f    this  together monotonicity f w r t   
implies exists fm f  vm f     fc  vm f     fm f  vm f      
  let us fix   known that  since u
  v um
f
f closed bounded
     

c
c
f continuous  f attains extrema um f   since fm f  v    fm f  vm
f
  
c

c

v um f f  v    fm f  v  v um f follows f  v    fm f  vm
f


c

  
v um f   chosen fm f  vm f     f  vm f     fm f  vm
f
latter two conditions imply one extrema local minimum
 
interior um

f

references
bickson  d          gaussian belief propagation  theory application  ph d  thesis 
hebrew university jerusalem 
cseke  b     heskes  t          bounds bethe free energy gaussian networks 
mcallester  d  a     myllymaki  p   eds    uai       proceedings   th
conference uncertainty artificial intelligence  pp         auai press 
heskes  t          stable fixed points loopy belief propagation minima bethe
free energy  becker  s   thrun  s     obermayer  k   eds    advances neural
information processing systems     pp          cambridge  ma  mit press 
heskes  t   opper  m   wiegerinck  w   winther  o     zoeter  o          approximate
inference techniques expectation constraints  journal statistical mechanics 
theory experiment        p      
heskes  t          uniqueness loopy belief propagation fixed points  neural
computation               
horn  r  a     johnson  c          matrix analysis  cambridge university press  cambridge  uk 
  

fibethe free energies message passing gaussian models

jaakkola  t          tutorial variational approximation methods  opper  m     saad 
d   eds    advanced mean field methods  theory practice  pp          cambridge 
ma  mit press 
johnson  j  k   bickson  d     dolev  d          fixing convergence gaussian belief
propagation  corr  abs           
malioutov  d   johnson  j     willsky  a          walk sums belief propagation
gaussian graphical models  journal machine learning research              
minka  t  p          power ep  tech  rep   microsoft research ltd   cambridge  uk 
msr tr          
minka  t  p          divergence measures message passing  tech  rep  msr tr          microsoft research ltd   cambridge  uk 
moallemi  c     roy  b  v          consensus propagation  weiss  y   scholkopf  b    
platt  j   eds    advances neural information processing systems     pp         
mit press  cambridge  ma 
murphy  k   weiss  y     jordan  m  i          loopy belief propagation approximate
inference  empirical study  proceedings fifteenth conference uncertainty artificial intelligence  vol     pp          san francisco  usa  morgan
kaufman 
nishiyama  y     watanabe  s          accuracy loopy belief propagation gaussian
models  neural networks                  
pearl  j          probabilistic reasoning intelligent systems  networks plausible inference  morgan kaufman publishers  san mateo  ca 
rusmevichientong  p     roy  b  v          analysis belief propagation turbo
decoding graph gaussian densities  ieee transactions information theory 
           
seeger  m  w          bayesian inference optimal design sparse linear model 
journal machine learning research            
takahashi  k   fagan  j     chin  m  s          formation sparse impedance matrix
application short circuit study  proceedings  th pica conference 
wainwright  m   jaakkola  t     willsky  a          tree reweighted belief propagation
algorithms approximate ml estimation via pseudo moment matching  bishop 
c     frey  b   eds    proceedings ninth international workshop artificial
intelligence statistics  society artificial intelligence statistics 
watanabe  y     fukumizu  k          graph zeta function bethe free energy
loopy belief propagation  bengio  y   schuurmans  d   lafferty  j   williams  c 
k  i     culotta  a   eds    advances neural information processing systems    
pp            mit press 
weiss  y     freeman  w  t          correctness belief propagation gaussian graphical
models arbitrary topology  neural computation                    
  

ficseke   heskes

welling  m     teh  y  w          belief optimization binary networks  stable alternative loopy belief propagation  breese  j  s     koller  d   eds    proceedings
  th conference uncertainty artificial intelligence  pp          morgan
kaufmann publishers 
wiegerinck  w     heskes  t          fractional belief propagation  becker  s   thrun 
s     obermayer  k   eds    advances neural information processing systems    
pp          cambridge  ma  mit press 
yedidia  j  s   freeman  w  t     weiss  y          generalized belief propagation 
advances neural information processing systems     pp          cambridge  ma 
mit press 
zoeter  o     heskes  t          change point problems linear dynamical systems 
journal machine learning research              

  


