journal of artificial intelligence research                  

submitted        published      

research note
policy invariance under reward transformations for
general sum stochastic games
xiaosong lu
howard m  schwartz

luxiaos   sce   carleton   ca
schwartz   sce   carleton   ca

department of systems and computer engineering
carleton university
     colonel by drive  ottawa  on k s  b  canada

sidney n  givigi jr 

s idney g ivigi   rmc   ca

department of electrical and computer engineering
royal military college of canada
   general crerar cres  kingston  on k k  b  canada

abstract
we extend the potential based shaping method from markov decision processes to multi player
general sum stochastic games  we prove that the nash equilibria in a stochastic game remains
unchanged after potential based shaping is applied to the environment  the property of policy
invariance provides a possible way of speeding convergence when learning to play a stochastic
game 

   introduction
in reinforcement learning  one may suffer from the temporal credit assignment problem  sutton  
barto        where a reward is received after a sequence of actions  the delayed reward will lead
to difficulty in distributing credit or punishment to each action from a long sequence of actions and
this will cause the algorithm to learn slowly  an example of this problem can be found in some
episodic tasks such as a soccer game where the player is only given credit or punishment after a
goal is scored  if the number of states in the soccer game is large  it will take a long time for a
player to learn its equilibrium policy 
reward shaping is a technique to improve the learning performance of a reinforcement learner
by introducing shaping rewards to the environment  gullapalli   barto        mataric        
when the state space is large  the delayed reward will slow down the learning dramatically  to
speed up the learning  the learner may apply shaping rewards to the environment as a supplement
to the delayed reward  in this way  a reinforcement learning algorithm can improve its learning
performance by combining a  good  shaping reward function with the original delayed reward 
the applications of reward shaping can be found in the literature  gullapalli   barto       
dorigo   colombetti        mataric        randlv   alstrm         gullapalli and barto       
demonstrated the application of shaping to a key press task where a robot was trained to press keys
on a keyboard  dorigo and colombetti        applied shaping policies for a robot to perform a
predefined animate like behavior  mataric        presented an intermediate reinforcement function
for a group of mobile robots to learn a foraging task  randlv and alstrm        combined reinforcement learning with shaping to make an agent learn to drive a bicycle to a goal  the theoretical
c
    
ai access foundation  all rights reserved 

fil u   s chwartz     g ivigi

analysis of reward shaping can be found in the literature  ng  harada    russell        wiewiora 
      asmuth  littman    zinkov         ng et al         presented a potential based shaping
reward that can guarantee the policy invariance for a single agent in a markov decision process
 mdp   ng et al  proved that the optimal policy keeps unchanged after adding the potential based
shaping reward to an mdp environment  following ng et al   wiewiora        showed that the effects of potential based shaping can be achieved by a particular initialization of q values for agents
using q learning  asmuth et al         applied the potential based shaping reward to a model based
reinforcement learning approach 
the above articles focus on applications of reward shaping to a single agent in an mdp  for the
applications of reward shaping in general sum games  babes  munoz de cote  and littman       
introduced a social shaping reward for players to learn their equilibrium policies in the iterated
prisoners dilemma game  but there is no theoretical proof of policy invariance under the reward
transformation  in our research  we prove that the nash equilibria under the potential based shaping
reward transformation  ng et al         will also be the nash equilibria for the original game under
the framework of general sum stochastic games  note that the similar work of devlin and kudenko
       was published while this article was under review  but devlin and kudenko only proved
sufficiency based on a proof technique introduced by asmuth et al          while we prove both
sufficiency and necessity using a different proof technique in this article 

   framework of stochastic games
stochastic games were first introduced by shapley         in a stochastic game  players choose the
joint action and move from one state to another state based on the joint action they choose  in this
section  under the framework of stochastic games  we introduce markov decision processes  matrix
games and stochastic games respectively 
    markov decision processes
a markov decision process is a tuple  s  a  t     r  where s is the state space  a is the action space 
t   s  a  s         is the transition function           is the discount factor and r   s  a  s  r
is the reward function  the transition function denotes a probability distribution over next states
given the current state and action  the reward function denotes the received reward at the next state
given the current action and the current state  a markov decision process has the following markov
property  the players next state and reward only depend on the players current state and action 
a players policy    s  a is defined as a probability distribution over the players actions given
a state  an optimal policy   will maximize the players discounted future reward  for any mdp 
there exists a deterministic optimal policy for the player  bertsekas        
starting in the current state s and following the optimal policy thereafter  we can get the optimal
state value function as the expected sum of discounted rewards  sutton   barto       
 
 
v   s    e


t

  j rk  j    sk   s   

   

j  

where k is the current time step  rk  j   is the received immediate reward at the time step k   j     
         is a discount factor  and t is a final time step  in      we have t   if the task is an
infinite horizon task such that the task will run over infinite period  if the task is episodic  t is
   

fip olicy i nvariance

under

r eward t ransformations

defined as the terminal time when each episode is terminated at the time step t   then we call the
state where each episode ends as the terminal state st   in a terminal state  the state value function is
always zero such that v  st       for all st  s  given the current state s and action a  and following
the optimal policy thereafter  we can define an optimal action value function  sutton   barto       
h
i


   
q  s  a     t  s  a  s   r s  a  s     v   s  
s s

where t  s  a  s     pr  sk     s  sk   s  ak   a  is the probability of the next state being sk     s
given the current state sk   s and action ak   a at time step k  and r s  a  s     e rk    sk   s  ak   a 
sk     s   is the expected immediate reward received at state s given the current state s and action
a  in a terminal state  the action value function is always zero such that q st   a      for all st  s 
    matrix games
a matrix game is a tuple  n  a            an   r            rn   where n is the number of players  ai  i              n 
is the action set for the player i and ri   a       an  r is the payoff function for the player i 
a matrix game is a game involving multiple players and a single state  each player i i              n 
selects an action from its action set ai and receives a payoff  the player is payoff function ri is
determined by all players joint action from joint action space a       an   for a two player matrix
game  we can set up a matrix with each element containing a payoff for each joint action pair  then
the payoff function ri for player i i         becomes a matrix  if the two players in the game are
fully competitive  we will have a two player zero sum matrix game with r    r   
in a matrix game  each player tries to maximize its own payoff based on the players strategy  a
players strategy in a matrix game is a probability distribution over the players action set  to evaluate a players strategy  we introduce the following concept of nash equilibrium  a nash equilibrium
in a matrix game is a collection of all players policies           n   such that
vi           i        n    vi           i        n    i  i   i           n

   

where vi    is the expected payoff for player i given all players current strategies and i is any
strategy of player i from the strategy space i   in other words  a nash equilibrium is a collection
of strategies for all players such that no player can do better by changing its own strategy given that
other players continue playing their nash equilibrium policies  basar   olsder         we define
qi  a            an   as the received payoff of the player i given players joint action a            an   and i  ai  
 i              n  as the probability of player i choosing action a    then the nash equilibrium defined
in     becomes





qi  a            an     a       i  ai      n  an   

a       an a  an

qi  a            an     a       i  ai      n  an    i  i   i           n

   

a       an a  an

where i  ai   is the probability of player i choosing action ai under the player is nash equilibrium
strategy i  
a two player matrix game is called a zero sum game if the two players are fully competitive 
in this way  we have r    r    a zero sum game has a unique nash equilibrium in the sense
of the expected payoff  it means that  although each player may have multiple nash equilibrium
   

fil u   s chwartz     g ivigi

strategies in a zero sum game  the value of the expected payoff vi under these nash equilibrium
strategies will be the same  if the players in the game are not fully competitive or the summation
of the players payoffs is not zero  the game is called a general sum game  in a general sum game 
the nash equilibrium is no longer unique and the game might have multiple nash equilibria  unlike
the deterministic optimal policy for a single player in an mdp  the equilibrium strategies in a multiplayer matrix game may be stochastic 
    stochastic games
a markov decision process contains a single player and multiple states while a matrix game contains
multiple players and a single state  for a game with more than one player and multiple states 
we define a stochastic game  or markov game  as the combination of markov decision processes
and matrix games  a stochastic game is a tuple  n  s  a            an   t     r            rn   where n is the
number of the players  t   s  a       an  s         is the transition function  ai  i              n 
is the action set for the player i           is the discount factor and ri   s  a       an  s  r
is the reward function for player i  the transition function in a stochastic game is a probability
distribution over next states given the current state and joint action of the players  the reward
function ri  s  a            an   s   denotes the reward received by player i in state s after taking joint
action  a            an   in state s  similar to markov decision processes  stochastic games also have the
markov property  that is  the players next state and reward only depend on the current state and all
the players current actions 
to solve a stochastic game  we need to find a policy i   s  ai that can maximize player is
discounted future reward with a discount factor    similar to matrix games  the players policy in
a stochastic game is probabilistic  an example is the soccer game introduced by littman  littman 
      where an agent on the offensive side must use a probabilistic policy to pass an unknown
defender  in the literature  a solution to a stochastic game can be described as nash equilibrium
strategies in a set of associated state specific matrix games  bowling        littman         in
these state specific matrix games  we define the action value function qi  s  a            an   as the expected reward for player i when all the players take joint action a            an in state s and follow the
nash equilibrium policies thereafter  if the value of qi  s  a            an   is known for all the states 
we can find player is nash equilibrium policy by solving the associated state specific matrix game
 bowling         therefore  for each state s  we have a matrix game and we can find the nash
equilibrium strategies in this matrix game  then the nash equilibrium policies for the game are the
collection of nash equilibrium strategies in each state specific matrix game for all the states 
    multi player general sum stochastic games
for a multi player general sum stochastic game  we want to find the nash equilibria in the game if
we know the reward function and transition function in the game  a nash equilibrium in a stochastic
game can be described as a tuple of n policies              n   such that for all s  s and i           n 
vi  s              i           n    vi  s              i           n   for all i  i

   

where i is the set of policies available to player i and vi  s              n   is the expected sum of
discounted rewards for player i given the current state and all the players equilibrium policies  to
simplify notation  we use vi  s  to represent vi  s           n   as the state value function under nash
equilibrium policies  we can also define the action value function q  s  a         an   as the expected
   

fip olicy i nvariance

under

r eward t ransformations

sum of discounted rewards for player i given the current state and the current joint action of all the
players  and following the nash equilibrium policies thereafter  then we can get



vi  s   

qi  s  a         an     s  a       n  s  an   

   

a     an a  an

qi  s  a            an    

 t  s  a            an   s  

s s




ri  s  a            an   s     vi  s    

   

where i  s  ai    pd ai   is a probability distribution over action ai under player is nash equilibrium policy  t  s  a            an   s     pr  sk     s  sk   s  a            an   is the probability of the next state
being s given the current state s and joint action  a            an    and ri  s  a            an   s   is the expected
immediate reward received in state s given the current state s and joint action  a            an    based
on     and      the nash equilibrium in     can be rewritten as



qi  s  a            an     s  a       i  s  ai      n  s  an   

a       an a  an



qi  s  a            an     s  a       i  s  ai      n  s  an   

   

a       an a  an

   potential based shaping in general sum stochastic games
ng et al         presented a reward shaping method to deal with the credit assignment problem
by adding a potential based shaping reward to the environment  the combination of the shaping
reward with the original reward may improve the learning performance of a reinforcement learning
algorithm and speed up the convergence to the optimal policy  the theoretical studies on potentialbased shaping methods that appear in the published literature consider the case of a single agent in
an mdp  ng et al         wiewiora        asmuth et al          in our research  we extend the
potential based shaping method from markov decision processes to multi player stochastic games 
we prove that the nash equilibria under the potential based shaping reward transformation will be
the nash equilibria for the original game under the framework of general sum stochastic games 
we define a potential based shaping reward fi  s  s   for player i as
fi  s  s      i  s    i  s  

   

where    s  r is a real valued shaping function and  st       for any terminal state st   we
define a multi player stochastic game as a tuple m    s  a            an   t     r            rn   where s is a set
of states  a            an are players action sets  t is the transition function   is the discount factor  and
ri  s  a            an   s   i              n  is the reward function for player i  after adding the shaping reward
function fi  s  s   to the reward function ri  s  a            an   s    we define a transformed multi player
stochastic game as a tuple m     s  a            an   t     r            rn   where ri  i              n  is the new
reward function given by ri  s  a            an   s     fi  s  s     ri  s  a            an   s    inspired by ng et al 
      s proof of policy invariance in an mdp  we prove the policy invariance in a multi player
general sum stochastic game as follows 
theorem    given an n player discounted stochastic game m    s  a            an   t     r            rn    we
define a transformed n player discounted stochastic game m     s  a            an   t     r    f            rn  
fn   where fi  s  s is a shaping reward function for player i  we call fi a potential based shaping
function if fi has the form of      then  the potential based shaping function fi is a necessary and
sufficient condition to guarantee the nash equilibrium policy invariance such that
   

fil u   s chwartz     g ivigi

  sufficiency  if fi  i              n  is a potential based shaping function  then every nash equilibrium policy in m  will also be a nash equilibrium policy in m  and vice versa  
  necessity  if fi  i              n  is not a potential based shaping function  then there may exist
a transition function t and reward function r such that the nash equilibrium policy in m 
will not be the nash equilibrium policy in m 
proof   proof of sufficiency 
based on      a nash equilibrium in the stochastic game m can be represented as a set of policies
such that for all i              n  s  s and mi  






qmi  s  a            an  m
 s  a       m
 s  ai      m
 s  an   
 
i
n

a       an a  an





 s  a       mi  s  ai      m
 s  an   
qmi  s  a            an  m
 
n

    

a       an a  an

we subtract i  s  on both sides of      and get



a       an a  an




qmi  s  a            an  m
 s  a       m
 s  ai      m
 s  an    i  s  
 
i
n





qmi  s  a            an  m
 s  a       mi  s  ai      m
 s  an    i  s  
 
n

    

a       an a  an
  s  a         s  a         s  a        we can get
since a       an a  an m
 
i
n
mi
mn
 






 s  a       m
 s  ai      m
 s  an   
 qmi  s  a            an    i  s  m
 
i
n

a       an a  an





 qmi  s  a            an    i  s  m
 s  a       mi  s  ai      m
 s  an   
 
n

    

a       an a  an

we define
qmi  s  a            an     qmi  s  a            an    i  s  

    

then we can get



a       an a  an






qmi  s  a            an  m
 s  a       m
 s  ai      m
 s  an   
 
i
n

a       an a  an



 s  a       mi  s  ai      m
 s  an   
qmi  s  a            an  m
 
n

    

we now use some algebraic manipulations to rewrite the action value function under the nash equilibrium in     for player i in the stochastic game m as

qmi  s  a            an    i  s     t  s  a            an   s   rmi  s  a            an   s     vm i  s  
s s


  i  s     i  s    i  s  

    

since s s t  s  a            an   s        the above equation becomes
qmi  s  a            an    i  s   

 t  s  a            an   s  

s s



rmi  s  a            an   s  


  i  s    i  s    vm i  s     i  s    
   

    

fip olicy i nvariance

under

r eward t ransformations

according to      we can rewrite the above equation as
qmi  s  a            an    i  s   



 

a       an a  an

 
 



a       an a  an

 t  s  a            an   s  

s s



rmi  s  a            an   s      i  s    i  s 



 

 

 s
 
a
 



 s
 
a
 

qmi  s   a            an  m



 s
 
i
 
m
n
 
i

 t  s  a            an   s  



rmi  s  a            an   s      i  s    i  s 

s s
 
   
   

 s   a       m
 s   an    
qmi  s   a            an    i  s   m
 
i

    

based on the definitions of fi  s  s   in     and qmi  s  a            an   in       the above equation becomes
qmi  s  a            an    
 



a       an a  an

 t  s  a            an   s  

s s



rmi  s  a            an   s     fi s  s  




qmi  s   a            an   m
 s   a       m
 s   an    
 
i

    

since equations      and      have the same form as equations          we can conclude that
qmi  s  a            an   is the action value function under the nash equilibrium for player i in the stochastic game m    therefore  we can obtain
qmi  s  a            an     qm  s  a            an     qmi  s  a            an    i  s  
i

    

if the state s is the terminal state st   then we have qmi  st   a            an     qmi  st   a            an   
i  st               based on      and qmi  s  a            an     qm  s  a            an    we can find that
i
the nash equilibrium in m is also the nash equilibrium in m    then the state value function under
the nash equilibrium in the stochastic game m  can be given as
vm   s    vm i  s   i  s  
i

    

 proof of necessity 
if fi  i              n  is not a potential based shaping function  we will have fi  s  s       i  s    i  s  
similar to ng et al        s proof of necessity  we define    fi  s  s      i  s    i  s    then we
can build a stochastic game m by giving the following transition function t and player  s reward
function rm    
t  s    a     a            an   s        
t  s    a     a            an   s        
t  s    a            an   s        
t  s    a            an   s        

rm   s    a            an   s       
 
rm   s    a            an   s        
rm   s    a            an   s        
rm   s    a            an   s        
   

    

fil u   s chwartz     g ivigi

a  
s 

s 

a  
s 
figure    possible states of the stochastic model in the proof of necessity

where ai  i              n  represents any possible action ai  ai from player i  and a   and a   represent
player  s action   and action   respectively  equation t  s    a     a            an   s        in      denotes
that  given the current state s    player  s action a   will lead to the next state s  no matter what
joint action the other players take  based on the above transition function and reward function  we
can get the game model including states  s    s    s    shown in figure    we now define    si    
f   si   s    i             based on                      and       we can obtain player  s action value
function at state s  in m and m 

 
 
qm   s    a                 

qm   s    a              


qm  s    a               f   s    s       f   s    s      
 
 
qm  s    a               f   s    s       f   s    s    
 

then the nash equilibrium policy for player   at state s  is


 s    a     
m
 

  
a  if      


a  


  m
  s    a     
 

otherwise

  
a  if      


a  

 

    

otherwise

therefore  in the above case  the nash equilibrium policy for player   at state s  in m is not the
nash equilibrium policy in m   

the above analysis shows that the potential based shaping reward with the form of fi  s  s    
 i  s    i  s  guarantees the nash equilibrium policy invariance  now the question becomes
how to select a shaping function i  s  to improve the learning performance of the learner  ng
et al         showed that i  s    vm i  s  is a good candidate for improving the players learning
   

fip olicy i nvariance

under

r eward t ransformations

performance in an mdp  we substitute i  s    vm i  s  into      and get
qmi  s  a            an     qm  s  a            an  
i

 
 

 t  s  a            an   s  

s s





rmi  s  a            an   s     fi  s  s  



rmi  s  a            an   s     fi  s  s  



 

 
qm  s   a            an   m

 s
 
a
 



 s
 
a
 
 
m
n
 
i
i

a       an a  an

 

 t  s  a            an   s  

s s


    vm i  s    i  s   


   t  s  a            an   s   rmi  s  a            an   s     fi  s  s    

    

s s

equation      shows that the action value function qm  s  a            an   in state s can be easily obtained
i
by checking the immediate reward rmi  s  a            an   s     fi  s  s   that player i received in state s  
however  in practical applications  we will not have all the information of the environment such as
t  s  a            an   s   and ri  s  a            an   s    this means that we cannot find a shaping function i  s 
such that i  s    vm i  s  without knowing the model of the environment  therefore  the goal for
designing a shaping function is to find a i  s  as a good approximation to vm i  s  

   conclusion
a potential based shaping method can be used to deal with the temporal credit assignment problem
and speed up the learning process in mdps  in this article  we extend the potential based shaping
method to general sum stochastic games  we prove that the proposed potential based shaping reward applied to a general sum stochastic game will not change the original nash equilibrium of the
game  the analysis result in this article has the potential to improve the learning performance of the
players in a stochastic game 

references
asmuth  j   littman  m  l     zinkov  r          potential based shaping in model based reinforcement learning  in proceedings of the   rd aaai conference on artificial intelligence 
pp         
babes  m   munoz de cote  e     littman  m  l          social reward shaping in the prisoners
dilemma  in proceedings of the  th international joint conference on autonomous agents
and multiagent systems  aamas        pp           
basar  t     olsder  g  j          dynamic noncooperative game theory  siam series in classics
in applied mathematics  nd  london  u k 
bertsekas  d  p          dynamic programming  deterministic and stochastic models  prenticehall  englewood cliffs  nj 
bowling  m          multiagent learning in the presence of agents with limitations  ph d  thesis 
school of computer science  carnegie mellon university  pittsburgh  pa 
   

fil u   s chwartz     g ivigi

devlin  s     kudenko  d          theoretical considerations of potential based reward shaping for
multi agent systems   in proceedings of the   th international conference on autonomous
agents and multiagent systems  aamas   taipei  taiwan 
dorigo  m     colombetti  m          robot shaping  developing autonomous agents through
learning  artificial intelligence             
gullapalli  v     barto  a          shaping as a method for accelerating reinforcement learning  in
proceedings of the      ieee international symposium on intelligent control  pp          
littman  m  l          markov games as a framework for multi agent reinforcement learning  in
proceedings of the   th international conference on machine learning  pp         
mataric  m  j          reward functions for accelerated learning  in proceedings of the   th
international conference on machine learning 
ng  a  y   harada  d     russell  s          policy invariance under reward transformations  theory
and application to reward shaping  in proceedings of the   th international conference on
machine learning  pp         
randlv  j     alstrm  p          learning to drive a bicycle using reinforcement learning and
shaping  in proceedings of the   th international conference on machine learning 
shapley  l  s          stochastic games  in proceedings of the national academy of sciences 
vol      pp           
sutton  r  s     barto  a  g          reinforcement learning  an introduction  the mit press 
cambridge  massachusetts 
wiewiora  e          potential based shaping and q value initialization are equivalent  journal of
artificial intelligence research             

   

fi