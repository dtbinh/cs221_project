journal artificial intelligence research                  

submitted        published      

controlling complexity part of speech induction
joo v  graa

joao   graca   l   f  inesc   id   pt

l  f inesc id
lisboa  portugal

kuzman ganchev

kuzman   google   com

google inc 
new york  ny  usa

lusa coheur

luisa   coheur   l   f  inesc   id   pt

 

l f inesc id
lisboa  portugal

fernando pereira

pereira   google   com

google inc 
mountain view  ca  usa

ben taskar

taskar   cis   upenn   edu

computer   information science
university pennsylvania

abstract
consider problem fully unsupervised learning grammatical  part of speech  categories unlabeled text  standard maximum likelihood hidden markov model
task performs poorly  weak inductive bias large model capacity  address
problem refining model modifying learning objective control capacity via parametric non parametric constraints  approach enforces word category association sparsity 
adds morphological orthographic features  eliminates hard to estimate parameters rare
words  develop efficient learning algorithm much computationally intensive standard training  provide open source implementation algorithm 
experiments five diverse languages  bulgarian  danish  english  portuguese  spanish  achieve
significant improvements compared previous methods task 

   introduction
part of speech  pos  categories elementary building blocks syntactic analysis text
play important role many natural language processing tasks  machine translation
information extraction  english handful languages fortunate enough
comprehensive pos annotated corpora penn treebank  marcus  marcinkiewicz 
  santorini         worlds languages extremely limited linguistic resources 
unrealistic expect annotation efforts catch explosion unlabeled electronic
text anytime soon  lack supervised data likely persist near future
investment required accurate linguistic annotation  took two years annotate       sentences
syntactic parse trees chinese treebank  hwa  resnik  weinberg  cabezas    kolak 
      four seven years annotate        sentences across range languages  abeill 
      

c
    
ai access foundation  rights reserved 

fig raa   g anchev  c oheur   p ereira     taskar

supervised learning taggers pos annotated training text well studied task 
several methods achieving near human tagging accuracy  ratnaparkhi        toutanova  klein 
manning    singer        shen  satta    joshi         however  pos induction one
access labeled corpus difficult task much room improvement 
recent literature  pos induction used refer two different tasks  first one 
addition raw text  given dictionary containing possible tags word
goal disambiguate tags particular word occurrence  merialdo         second
task  given raw text  dictionary provided  goal cluster words
grammatical behavior  work  target latter  challenging  unsupervised pos
induction task 
recent work task typically relies distributional morphological features  since words
grammatical function tend occur similar contexts common morphology  brown  desouza  mercer  pietra    lai        schtze        clark         however 
statistical regularities enough overcome several challenges  first  algorithm
decide many clusters use broad syntactic categories  for instance  whether distinguish
plural singular nouns   second  category size distribution tends uneven  example  vast majority word types open class  nouns  verbs  adjectives   even among
open class categories  many nouns adjectives  runs contrary learning
biases commonly used statistical models  common failure models clump several
rare categories together split common categories 
individual word types  third challenge arises ambiguity grammatical role
word sense  many words take different pos tags different occurrences  depending
context occurrence  the word run either verb noun   approaches assume
 for computational statistical simplicity  word one tag  aggregating
local contexts distributional clustering  schtze         one tag per word
assumption clearly wrong  across many languages annotated corpora 
methods perform competitively methods assign different tags word
different contexts  lamar  maron  johnson    bienenstock         partly due typical
statistical dominance one tags word  especially corpus includes single genre 
news  reason less restrictive models encode useful bias
words typically take small number tags 
approaches make one tag per word assumption take form hidden
markov model  hmm  hidden states represent word classes observations
word sequences  brown et al         johnson         unfortunately  standard hmms trained
maximize likelihood perform poorly  since learned hidden classes align well true
pos tags  besides potential model estimation errors due non convex optimization involved
training  pernicious problem  typical maxima likelihood align well
maxima pos tag accuracy  smith   eisner        graa  ganchev  pereira    taskar        
suggesting serious mismatch model data 
work  significantly reduce modeling mismatch combining three ideas 
standard hmm treats words atomic units  without using orthographic morphological information  information critical generalization many languages  clark 
       address problem  reparameterize standard hmm replacing multinomial emission distributions maximum entropy models  similar work berg   

fic ontrolling c omplexity part   s peech nduction

kirkpatrick  bouchard ct  denero    klein       graa         allows use
orthographic morphological features emission model  moreover  standard
hmm model large number parameters  number tags times number
word types  presents extremely rich model space capable fitting irrelevant correlations data  address problem dramatically reduce number parameters
model discarding features small support corpus  is  involving
rare words word parts 
hmm model allows high level ambiguity tags word  result 
maximizing marginal likelihood  common words typically tend associated every
tag non trivial probability  johnson         however  natural property pos
categories across many languages annotation standards word small
number allowed tags  address problem use posterior regularization  pr 
framework  graa  ganchev    taskar        ganchev  graa  gillenwater    taskar       
constrain ambiguity word tag associations via sparsity inducing penalty
model posteriors  graa et al         
show proposed extensions improves standard hmm performance 
moreover  gains nearly additive  improvements significant across different
metrics previously proposed task  instance    many metric  method attains
      average improvement regular hmm  compare proposed method
eleven previously proposed approaches  languages english metrics except     
method achieves best published results  furthermore  method appears stable
across different testing scenarios always shows competitive results  finally  show
induced tags used improve performance supervised pos tagging system
limited labeled data scenario  open source software pos induction evaluation
available http   code google com p pr toolkit  
paper organized follows  section   describes basic hmm pos induction
maximum entropy extension  section   describes standard em sparsity inducing estimation
method  section   presents comprehensive survey previous fully unsupervised pos induction
methods  section   provide detailed experimental evaluation method  finally 
section    summarize results suggest ideas future work 

   models
model experiments based first order hmm  denote sequence
words sentence boldface x sequence hidden states correspond partof speech tags boldface y  sentence length l  thus l hidden state variables
yi             j     l j number possible pos tags  l observation variables
xi             v      l  v number word types  simplify notation  assume
every tag sequence prefixed conventional start tag y    start  allowing us write
p y   y    initial state probability hmm 
probability sentence x along particular hidden state sequence given by 
p x  y   

l


pt  yi   yi   po  xi   yi   

i  

   

   

fig raa   g anchev  c oheur   p ereira     taskar

po  xi   yi   probability observing word xi given state yi  emission
probability   pt  yi   yi    probability state yi   given previous hidden
state yi   transition probability  
    multinomial emission model
standard hmms use multinomial emission transition probabilities  is  generic word
xi tag yi   observation probability po  xi   yi   transition probability pt  yi   yi   
multinomial distributions  experiments refer model simply hmm  model
large number parameters large number word types  see table    
common convention follow lowercase words well map words occurring
corpus special token unk 
    maximum entropy emission model
work  use simple modification hmm model discussed previous section 
represent conditional probability distributions maximum entropy  log linear  models  specifically  emission probability expressed as 
exp  f  x  y  
 
x  exp  f  x   y  

po  x y    p

   

f  x  y  feature function  x ranges word types  model parameters 
refer model hmm me  addition word identity  features include orthographyand morphology inspired cues presence capitalization  digits  common suffixes 
feature sets described section    idea replacing multinomial models hmm
maximum entropy models new applied different domains  chen 
       well pos induction  berg kirkpatrick et al         graa         key advantage
representation allows much tighter control expressiveness
model  many languages helpful exclude word identity features rare words order
constrain model force generalization across words similar features  unlike mapping
rare words unk token multinomial setting  maxent model still captures
information word features  moreover  reduce number
parameters even using lowercase word identities still keeping case information
using case feature  table   shows number features used different corpora  note
reduced feature set order magnitude fewer parameters multinomial model 

   learning
section   describe experiments comparing hmm model model three
learning scenarios  maximum likelihood training using em algorithm  dempster  laird    rubin        hmm hmm me  gradient based likelihood optimization hmm me
model  pr sparsity constraints  graa et al         hmm hmm me 
section describes three learning algorithms 
following  denote whole corpus  list sentences  x    x    x            xn  
corresponding tag sequences    y    y            yn   

   

fic ontrolling c omplexity part   s peech nduction

    maximum likelihood em
standard hmm training seeks model parameters maximize log likelihood observed
data 
x
p  x  y 
   
log likelihood  l     log


x whole corpus  since model assumes independence sentences  
log

x

p  x  y   

n
x

log

x

   

yn

n  



p  xn   yn   

use corpus notation consistency section      latent variables
y  log likelihood function hmm model convex model parameters 
model fitted using em algorithm  em maximizes l   via block coordinate ascent lower
bound f  q    using auxiliary distribution latent variables q y   neal   hinton        
jensens inequality  define lower bound f  q    as 
l     log

x

q y 



p  x  y  x
p  x  y 

q y  log
  f  q    
q y 
q y 

   



rewrite f  q    as 
f  q     

x

q y  log p  x p  y x  



x

q y  log q y 

   



q y 
q y  log
p  y x 

   

  l   kl q y   p  y x   

   

  l  

x


using interpretation  view em performing coordinate ascent f  q     starting
initial parameter estimate     algorithm iterates two block coordinate ascent steps
convergence criterion reached 
e   q t     arg max f  q      arg min kl q y  k pt  y   x  
q

   

q

  t     arg max f  q t         arg max eqt    log p  x  y  


    



e step corresponds maximizing eq    respect q m step corresponds
maximizing eq    respect   em algorithm guaranteed converge local
maximum l   mild conditions  neal   hinton         hmm pos tagger 
e step computes posteriors pt  y x  latent variables  pos tags  given observed
variables  words  current parameters sentence  accomplished forwardbackward algorithm hmms  em algorithm together forward backward algorithm
hmms usually referred baumwelch algorithm  baum  petrie  soules    weiss 
      
step uses q t    qnt   posteriors given sentence  fill values tags
estimate parameters t     since hmm model locally normalized features used
   

fig raa   g anchev  c oheur   p ereira     taskar

depend tag word identities particular position occur 
optimization decouples following way 
eqt    log p  x  y    
 

n
x

eqnt    log

n  
ln
n x
x

ln


n
pt  yin   yi 
 po  xni   yin   

    

i  
n
eqnt   log pt  yin   yi 
    eqnt   log po  xni   yin  



    

n   i  

multinomial emission model  optimization particularly easy simply involves
normalizing  expected  counts parameter  maximum entropy emission model parameterized equation    closed form solution need solve unconstrained
optimization problem  possible hidden tag value solve two problems  estimate emission probabilities po  x y  estimate transition probabilities pt  y    y  
gradient one given


eqt    log p  x  y  
  eqt   f  x  y  ep  x   y   f  x    y    
    

similar gradient supervised models  except expectation
q t    y  instead observed y  optimization done using l bfgs wolfes rule
line search  nocedal   wright        
    maximum likelihood direct gradient
likelihood traditionally optimized em  berg kirkpatrick et al         find
hmm maximum entropy emission model  higher likelihood better accuracy
achieved gradient based likelihood optimization method  use l bfgs
experiments  derivative likelihood is 
l  



 

 
x
log p  x   
p  x   
p  x  y 

p  x 
p  x 

x  
x p  x  y 

 
p  x  y   
log p  x  y 
p  x 
p  x 


x

 
p  y x  log p  x  y  

 

    
    
    



exactly derivative m step  equation    apply chain
rule take derivative log p  x   equation    apply chain rule reverse
direction  biggest difference em procedure direct gradient em
fix counts e step optimize model using counts  directly
optimizing likelihood need recompute counts parameter setting 
expensive  appendix gives detailed discussion methods 
    controlling tag ambiguity pr
one problem unsupervised hmm pos tagging maximum likelihood objective may
encourage tag distributions allow many different tags word given context 
   

fic ontrolling c omplexity part   s peech nduction

  

 

supervised
hmm
hmm me
hmm sp
hmm me sp

 
l l

 
l l

  

supervised
hmm
hmm me
hmm sp
hmm me sp

 
 

 
 
 

 

 
 

                                       

 

                                        

rank word l l

rank word l l

figure    ambiguity measure          word type two corpora supervised
model  em training  hmm  hmm me   train ambiguity penalty
described section      hmm sp  hmm me sp   left en  right pt 

find actual text linguist designed tags  tags designed informative
words grammatical role  following paragraphs describe measure tag ambiguity
proposed graa et al         attempt control  easier understand measure
hard tag assignments  start thene extend discussion distributions
tags 
consider word stock  intuitively  would occurrences stock
tagged small subset possible tags  noun verb  case   hard assignment
tags entire corpus  y  could count many different tags used occurrences
word stock 
instead single tagging corpus  distribution q y  assignments 
need generalize ambiguity measure  instead asking particular tag ever used
word stock  would ask maximum probability particular tag
used word stock  instead counting number tags  would sum
probabilities 
motivation  figure   shows distribution tag ambiguity across words two corpora 
see figure    train using em procedure described section     
hmm models grossly overestimates tag ambiguity almost words  however
models trained using pr penalize tag ambiguity  models  hmm sp 
hmm me sp  achieve tag ambiguity closer truth 
formally  graa et al         define measure terms constraint features  x  y  
constraint feature wvj  x  y  takes value   j th occurrence word type w x assigned
tag v tag assignment y  consequently  probability j th occurrence word w
tag v label distribution q y  eq  wvj  x  y    ambiguity measurement
word type w becomes 
x
ambiguity penalty word type w 
max eq y   wvj  x  y    
    
v

j

sum maxima called       mixed norm  brevity use norm notation   eq  w         computational reasons  add penalty term based ambiguity model distribution p  y x   instead introduce auxiliary distribution q y 
   

fig raa   g anchev  c oheur   p ereira     taskar

must close p must low ambiguity  modified objective becomes
x
  eq  w  x  y        
max l   kl q y   p  y x  
 q

    

w

graa et al         optimize objective using algorithm similar em  added complexity implementing algorithm lies computing kullback leibler projection
modified e step  however  computation involves choosing distribution exponentially
many objects  label assignments   luckily  graa et al         show dual formulation
e step manageable  given by 
 
x
x
max log
p  y x  exp   x  y  
s  t 
wvj
    
 

j



vector dual parameters wvj   one wvj   projected distribution
given by  q y  p  y x  exp    x  y    note p given hmm  q
sentence expressed
q yn  




n
pt  yin   yi 
 qo  xni   yin   

    

i  

qo  xi  yi     po  xi  yi   exp xi yi j   act modified  unnormalized  emission probabilities 
objective equation    negative sum log probabilities sentences
q plus constant  compute running forward backward corpus  similar
e step normal em  gradient objective computed using forwardbackward algorithm  note objective eq     concave respect
optimized using variety methods  perform dual optimization projected gradient 
using fast simplex projection algorithm described bertsekas  homer  logan  patek
        experiments found taking projected gradient steps enough 
performing optimization convergence helps results 

   related work
pos tags place words classes share commonalities  classes of  words
cooccur with  therefore  natural ask whether word clustering methods based word
context distributions might able recover word classification inherent pos tag set 
several influential methods  notably mutual information clustering  brown et al         
used cluster words according immediately contiguous words distributed 
although methods explicitly designed pos induction  resulting clusters capture syntactic information  see martin  liermann    ney        different method
similar objective   clark        refined distributional clustering approach adding
morphological word frequency information  obtain clusters closely resemble pos
tags 
forms distributional clustering go beyond immediate neighbors word represent whole vector coocurrences target word within text window  compare
vectors using suitable metric  cosine similarity  however  wider range similarities problems capturing local regularities  instance  adjective noun might
   

fic ontrolling c omplexity part   s peech nduction

look similar noun tends used noun noun compounds  similarly  two adjectives
different semantics selectional preferences might used different contexts  moreover 
problem aggravated data sparsity  example  infrequent adjectives modify different nouns tend completely disjoint context vectors  but even frequent words
might completely different context vectors  since articles used disjoint right
contexts   alleviate problems  schtze        used frequency cutoffs  singular value decomposition co occurrence matrices  approximate co clustering two stages svd 
clusters first stage used instead individual words provide vector representations second stage clustering 
lamar  maron johnson        recently revised two stage svd model schtze
       achieve close state of the art performance  revisions relatively small 
touch several important aspects model  singular vectors scaled singular values
preserve geometry original space  latent descriptors normalized unit length 
cluster centroids computed weighted average constituent vectors based word
frequency  rare common words treated differently centroids initialized
deterministic manner 
final class approaches include work paper uses sequence model 
hmm  represent probabilistic dependencies consecutive tags 
approaches  observation corresponds particular word hidden state corresponds
cluster  however  noted clark        johnson         using maximum likelihood
training models achieve good results  maximum likelihood training tends result
ambiguous distributions common words  contradiction rather sparse wordtag distribution  several approaches proposed mitigate problem  freitag       
clusters frequent words using distributional approach co clustering  cluster
remaining  infrequent  words  author trains second order hmm emission probabilities frequent words fixed clusters found earlier emission probabilities
remaining words uniform 
several studies propose using bayesian inference improper dirichlet prior favor
sparse model parameters hence indirectly reduce tag ambiguity  johnson        gao   johnson        goldwater   griffiths         refined moon  erk  baldridge
       representing explicitly different ambiguity patterns function content words 
lee  haghighi  barzilay        take direct approach reducing tag ambiguity explicitly modeling set possible tags word type  model first generates tag
dictionary assigns mass one tag word type reflect lexicon sparsity  dictionary used constrain dirichlet prior emission probabilities drawn
support word tag pairs dictionary  token level hmm using
emission parameters transition parameters draw symmetric dirichlet prior used
tagging entire corpus  authors show improvements using morphological features
creating dictionary  system achieves state of art results several languages 
noted common issue sparsity inducing approaches sparsity
imposed parameter level  probability word given tag  desired sparsity
posterior level  probability tag given word  graa et al         use pr framework
penalize ambiguous posteriors distributions words given tokens  achieves better results
bayesian sparsifying dirichlet priors 

   

fig raa   g anchev  c oheur   p ereira     taskar

recently  berg kirkpatrick et al         graa        proposed replacing multinomial distributions hmm maximum entropy  me  distributions  allows use features capture morphological information  achieve promising results  berg kirkpatrick
et al         find optimizing likelihood l bfgs rather em leads substantial improvements  show case beyond english 
note briefly pos induction methods rely prior tag dictionary indicating
word type pos tags have  pos induction task then  word token
corpus  disambiguate possible pos tags  described merialdo         unfortunately  availability large manually constructed tag dictionary unrealistic much
later work tries reduce required dictionary size different ways  generalizing
small dictionary handful entries  smith   eisner        haghighi   klein       
toutanova   johnson        goldwater   griffiths         however  although approach greatly
simplifies problem words one tag and  furthermore  cluster tag mappings predetermined  thus removing extra level ambiguity accuracy methods
still significantly behind supervised methods  address remaining ambiguity imposing
additional sparsity  ravi knight        minimize number possible tag tag transitions
hmm via integer program  finally  snyder  naseem  eisenstein  barzilay        jointly
train pos induction system parallel corpora several languages  exploiting fact
different languages present different ambiguities 

   experiments
section present encouraging results validating proposed method six different testing
scenarios according different metrics  highlights are 
maximum entropy emission model markov transition model trained ambiguity penalty improves regular hmm cases average improvement
       according   many metric  
compared broad range recent pos induction systems  method produces
best results languages except english  furthermore  method seems less sensitive
particular test conditions previous methods 
induced clusters useful features training supervised pos taggers  improving test
accuracy much clusters learned competing methods 
    corpora
experiments test several pos induction methods five languages help manually pos tagged corpora languages  table   summarizes characteristics test corpora 
wall street journal portion penn treebank  marcus et al          we consider
   tag version smith   eisner        en       tag version  en      bosque subset
portuguese floresta sinta c tica treebank  afonso  bick  haber    santos         pt  
bulgarian bultreebank  simov et al          bg   with    coarse tags   spanish corpus cast lb treebank  civit   mart         es   danish dependency treebank
 ddt   kromann  matthias t          dk  

   

fic ontrolling c omplexity part   s peech nduction

en
pt
bg
es
dk

 
sentences
     
    
     
    
    

 
types
     
     
     
     
     

 
lunk
      
      
      
      
      

 
tokens
       
      
      
     
     

 
tags
       
  
  
  
  

 
avg       
           
    
    
    
    

 
total      
               
      
      
     
     

 
 w  
     
     
     
     
     

 
 w  
    
    
    
   
   

table    corpus statistics  third column shows percentage word types lower casing
eliminating word types occurring once  sixth seventh columns show
information word ambiguity corpus average totality  corresponding penalty equation      eighth ninth columns show number
parameters different feature sets  described section     

    experimental setup
compare work two kinds methods  induce single cluster word
type  type level tagging   allow different tags different occurrences word type
 token level tagging   type level tagging  use two standard baselines  b rown c lark 
described brown et al          clark           following headden  mcclosky  charniak         trained c lark system      hidden states letter hmm
ran    iterations  b rown system run according instructions accompanying
code  ran recently proposed ldc system  lamar  maron    bienenstock          
configuration described paper ptb   ptb    ptb   configuration
corpora  noted carry experiments svd 
system  lamar  maron johnson         since svd  superseded ldc according
authors 
token level tagging  experimented feature rich hmm presented bergkirkpatrick et al          trained using em training  bk em  direct gradient  bk dg  
using configuration provided authors    report results type level hmm
 tlhmm   lee et al         applicable  since able run system  moreover 
compared systems implementation various hmm based approaches 
hmm multinomial emission probabilities  section       hmm maximumentropy emission probabilities  section      trained em  hmm me   trained direct gradient  hmm me dg   trained using pr ambiguity penalty  described section    
 hmm sp multinomial emissions  hmm me sp maximum entropy emissions  
addition  compared multinomial hmm sparsifying dirichlet prior parameters  hmm vb  trained using variational bayes  johnson        
following standard practice  multinomial hmms use morphological information  lowercase corpora replace unique words special unknown token 
improves multinomial hmm results decreasing number parameters eliminating
  
  
  
  

implementation  http   www cs berkeley edu  pliang software brown cluster     zip
implementation  http   www cs rhul ac uk home alexc pos  tar gz
implementation provided lamar  maron bienenstock        
implementation provided berg kirkpatrick et al         

   

fig raa   g anchev  c oheur   p ereira     taskar

rare words  mostly nouns   since maximum entropy emission models access morphological features  preprocessing steps improve performance perform
case 
start em  randomly initialize implementations hmm based models
posteriors  obtained running e step hmm model set random
parameters  close uniform random uniform jitter       means random
seed  initialization identical models 
em variational bayes training  train model     iterations  since found
typically models tend converge iteration      hmm vb model fix
transition prior        test emission prior equal            corresponding
best values reported johnson        
pr training  initialize    em iterations run     iterations pr  following graa et al          used results worked best english  en     graa et al  
       regularizing words occur least    times        use configuration scnenarios  setting specifically tuned test languages 
might optimal every language  setting parameters unsupervised manner
difficult task address  graa       discusses experiments different
values parameters  
obtain hard assignments using posterior decoding  position pick label
highest posterior probability  since showed small consistent improvements viterbi
decoding  experiments required random initialization parameters report
average   random seeds 
experiments run using number true tags number clusters  results
obtained test set portion corpus  evaluate systems using four common metrics
pos induction    many mapping      mapping  haghighi   klein         variation information  vi   meila         validity measure  v   rosenberg   hirschberg         metrics
described detail appendix b 
    hmm me sp performance
section compares gains using feature rich representation ambiguity penalty  described section      experiments show feature rich representation always improves performance  ambiguity penalty always improves
performance  then  see improvements two methods combine additively 
suggesting address independent aspects pos induction 
use two different feature sets  large feature set berg kirkpatrick et al         
reduced feature set described graa         apply count based feature selection identity suffix features  specifically  add identity features words
occurring least    times suffix features words occurring least    times  add
punctuation feature  follows  refer large feature set feature set   reduced
feature set    total number features model language given table   
results experiments summarized table   
table   shows results    training methods across six corpora four evaluation metrics 
resulting     experimental conditions  simplify discussion  focus   many metric
   transition prior significantly affect results  report results different values 

   

fic ontrolling c omplexity part   s peech nduction

 
 
 
 
 
 
 
 
 
  

 
 
 
 
 
 
 
 
 
  

hmm
hmm sp
hmm me  prior  
hmm me  prior   
hmm me  prior  
hmm me  prior   
hmm me sp  prior  
hmm me sp  prior   
hmm me sp  prior  
hmm me sp  prior   

en  
    
    
    
    
    
    
    
    
    
    

en  
    
    
    
    
    
    
    
    
    
    

  many
pt bg
         
         
         
         
         
         
         
         
         
         

dk
    
    
    
    
    
    
    
    
    
    

   
es ptb   ptb   pt bg dk es
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        

hmm
hmm sp
hmm me  prior  
hmm me  prior   
hmm me  prior  
hmm me  prior   
hmm me sp  prior  
hmm me sp  prior   
hmm me sp  prior  
hmm me sp  prior   

en  
    
    
    
    
    
    
    
    
    
    

en  
    
    
    
    
    
    
    
    
    
    

vi
pt bg
         
         
         
         
         
         
         
         
         
         

dk
    
    
    
    
    
    
    
    
    
    

v
es ptb   ptb   pt bg dk es
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        

table    results different hmms  hmm hmm sp hmms multinomial emission
functions trained using em pr sparsity constraints  respectively  hmm me
hmm me spare hmms maximum entropy emission model trained using em
pr sparsity constraints  feature rich models  superscript   represents large
feature set  superscript   represents reduced feature set  prior      refers
regularization strength emission model  table entries results averaged
  runs  bold indicates best system overall 

 top left tab table     observe conclusions hold three evaluation
metrics also  table   conclude following 
adding penalty high word tag ambiguity improves performance multinomial
hmm  multinomial hmm trained em  line   table    always worse
multinomial hmm trained pr ambiguity penalty       average  line  
table    
feature rich maximum entropy hmms  lines     table    almost always perform better
multinomial hmm  true feature sets regularization strengths
used  average increase       exceptions possibly due suboptimal regularization 
adding penalty high word tag ambiguity maximum entropy hmm improves performance  almost cases  comparing lines     lines      table    sparsity
constraints improve performance  average improvement        combined system al   

fig raa   g anchev  c oheur   p ereira     taskar

always outperforms multinomial hmm trained using ambiguity penalty
average improvement       every corpus best performance achieved
model ambiguity penalty maximum entropy emission probabilities 
every language except english    tags particular feature configuration  reducing feature set excluding rare features improves performance average       lines
    better lines     table    
regularizing maximum entropy model important many features
word tag ambiguity penalty  lines     table  
maximum entropy hmm many features  see tight parameter prior
almost always out performs looser prior  contrast  looking lines      table   see ambiguity penalty fewer features looser prior
almost always better tighter parameter prior  observed graa        
encouraging see improvements using feature rich model additive
effects penalizing tag ambiguity  especially surprising since optimize strength tag ambiguity penalty maximum entropy emission hmm  rather
used value reported graa et al         work multinomial emission hmm  experiments reported graa        show tuning parameter improve performance 
nevertheless  methods regularize objective different ways interaction
accounted for  would interesting use l  regularization models  instead
l   regularization together feature count cutoff  way model could learn features discard  instead requiring predefined parameter depends particular corpus
characteristics 
reported berg kirkpatrick et al          way objective optimized
big impact overall results  however  due non convex objective function
unclear optimization method works better why  briefly analyze question
appendix leave open question future work 
    error analysis
figure   shows distribution true tags clusters hmm model  left 
hmm me sp model  right  en   corpus  bar represents cluster  labeled tag
assigned performing   many mapping  colors represent number words
corresponding true tag  reduce clutter  true tags never used label cluster
grouped others 
observe models split common tags nouns several hidden states 
splitting accounts many errors models  using   states nouns instead
   hmm me sp able use states adjectives  another improvement comes
better grouping prepositions  example grouped punctuation hmm
hmm me sp correctly mapped prepositions  although correct
behavior  actually hurts  since tagset special tag occurrences word
incorrectly assigned  resulting loss      accuracy  contrast  hmm state
mapped tag word comprises one fifth state  common
error made hmm me sp include word second noun induced tag
figure    right   induced tag contains mostly capitalized nouns pronouns  often
   

fiprep
det

n
adj

rpunc
pos

v
inpunc

conj


epunc
others

prep
det

n
adj

rpunc
pos

v
inpunc

conj


conj

endpunc

v

inpunc

v

v

adj

rpunc

adj

n

adj

n

n

n

n

det

prep



endpunc

conj

v

inpunc

v

pos

n

adj

n

n

n

n

n

n

det

prep

c ontrolling c omplexity part   s peech nduction

epunc
others

figure    induced tags hmm model  left   hmm me sp model  right 
en   corpus  column represents hidden state  labeled   many
mapping  unused true tags grouped cluster named others 

     

     

     

     

     

     

     

     

    

    

 

 
art
n
adj

prop
v fin
prp

punc
num
adv

v pcp
v inf
conj c

pron pers
sumothers

art
n
adj

prop
v fin
prp

punc
num
adv

v pcp
v inf
conj c

pron pers
sumothers

figure    induced tags hmm model  left   hmm me sp model  right 
pt corpus  column represents hidden state  labeled   many mapping 
unused true tags grouped cluster named others 

precede nouns induced tags  suspect capitalization feature cause
error 
better performance feature based models portuguese relative english may due
ability features better represent richer morphology portuguese  figure   shows
induced clusters portuguese  hmm me sp model improves hmm tags
except adjectives  models trouble distinguishing nouns adjectives  reduced
accuracy adjectives hmm me sp explained mapping single cluster containing
adjectives adjectives hmm model nouns hmm me sp model 
removing noun adjective distinction  suggested zhao marcus         would increase
performance models     another qualitative difference observed
hmm me sp model used single induced cluster proper nouns rather spreading
across different clusters 

   

fig raa   g anchev  c oheur   p ereira     taskar

    state of the art comparison
compare best pos induction system  based settings line    table    
recent systems  results summarized table    previously done table   
focus discussion   many evaluation metric  results qualitatively
vi v metrics      metric shows variance across languages 
  many
pt bg
         
         
         
         
         
         
         
         
         
         
    
              

dk
    
    
    
    
    
    
    
    
    
    
    
    

   
es ptb   ptb   pt bg dk es
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
    
         
         
                        

vi
pt bg
         
         
         
         
         
         
         
         
         
         
         

dk
    
    
    
    
    
    
    
    
    
    
    

v
es ptb   ptb   pt bg dk es
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        

 
 
 
 
 
 
 
 
 
  
  
  

en  
b rown
    
c lark 
    
c lark  
    
ldc
    
hmm
    
hmm vb   
    
hmm vb     
    
hmm sp
    
bk em
    
bk dg
    
tlhmm
    
hmm me sp  prior        

en  
    
    
    
    
    
    
    
    
    
    

 
 
 
 
 
 
 
 
 
  
  

en  
b rown
    
c lark 
    
c lark  
    
ldc
    
hmm
    
hmm vb   
    
hmm vb     
    
hmm sp
    
bk em
    
bk dg
    
hmm me sp  prior        

en  
    
    
    
    
    
    
    
    
    
    
    

table    comparing hmm me sp  several pos induction systems  results
models random initialization run  systems                       represent
average   runs  see section     details discussion 

lines     table   show clustering algorithms based information gain various metrics  b rown wins     times  in scenarios fewer clusters  c lark system  despite
fact c lark uses morphology  comparing lines     table   line    see
ldc system particularly strong en   achieves state of the art results  behaves
worse b rown system every corpus 
hmms multinomial emissions  lines     table     maximum likelihood training  hmm  parameter sparsity  hmm vb  perform worse adding ambiguity penalty
 hmm sp   holds evaluation metrics  exception      confirms previous results graa et al          comparing models lines     lines      see

   

fic ontrolling c omplexity part   s peech nduction

best hmm  hmm sp  performs comparably best clustering  b rown   one
model winning   languages remaining   
feature rich hmms  bk em bk dg  perform well  achieving results
better hmm sp     tests  even though optimize objective  achieve
different results different corpora  explore training procedure detail appendix a  comparing implementation berg kirkpatrick et al          brevity 
table   contains results implementation berg kirkpatrick et al         
implementation produces comparable  quite identical results 
lines       table   display two methods attempt control tag ambiguity
feature rich representation capture morphological information  results tlhmm
taken lee et al          report results en   bg corpora  also 
able rerun experiments tlhmm  able compute
information theoretic metrics  consequently  comparison tlhmm slightly less complete
methods  tlhmm hmm me sp perform competitively better
systems  surprising since ability model morphological
regularity penalizing high ambiguity  comparing tlhmm hmm me sp  see
hmm me sp performs better   many metric  contrast  tlhmm performs better
     one possible explanation underlying model tlhmm bayesian hmm
sparsifying dirichlet priors  noted graa et al          models trained way tend
cluster distribution closely resemble true pos distribution  some clusters lots
words words  favors     metric  a description particularity
    metric discussed appendix b  
summarize  non english languages metrics except      hmm me sp
system performs better systems  english  bk dg wins    tag corpus 
ldc wins    tag corpus  hmm me sp system fairly robust  performing well
corpora best several them  allow us conclude tuned
particular corpus evaluation metric 
performance hmm me sp tightly related performance underlying
hmm me system  appendix present discussion performance different optimization methods hmm me  compare hmm me implementation bk em
bk dg show significant differences performance  however 
clear results one better  performs better given situation 
mentioned clark         morphological information particularly useful rare words 
table   compares different models accuracy words according frequency  compare clustering models based information gain without morphological information
 b rown c lark   distributional information based model  ldc   feature rich hmm
tag ambiguity control  hmm me sp   expected see systems using morphology
better rare words  moreover systems improve almost categories except
common words  words occurring    times   comparing hmm me sp c lark 
see even condition c lark overall works better  en     still performs worse
rare words hmm me sp 

   

fig raa   g anchev  c oheur   p ereira     taskar

 
 
  
  
    

b rown
     
     
     
     
     

c lark
     
     
     
     
     

 
 
  
  
    

b rown
     
     
     
     
     

c lark
     
     
     
     
     

 
 
  
  
    

b rown
     
     
     
     
     

c lark
     
     
     
     
     

en  
ldc
     
     
     
     
     
pt
ldc
     
     
     
     
     
es
ldc
     
     
     
     
     

hmm me sp 
     
     
     
     
     

b rown
     
     
     
     
     

c lark
     
     
     
     
     

hmm me sp 
     
     
     
     
     

b rown
     
     
     
     
     

c lark
     
     
     
     
     

hmm me sp 
     
     
     
     
     

b rown
     
     
     
     
     

c lark
     
     
     
     
     

en  
ldc
     
     
     
     
     
bg
ldc
     
     
     
     
     
dk
ldc
     
     
     
     
     

hmm me sp 
     
     
     
     
     
hmm me sp 
     
     
     
     
     
hmm me sp 
     
     
     
     
     

table      many accuracy word frequency different corpora 
    using clusters
comparison different pos induction methods  experiment simple
semisupervised scheme use learned clusters features supervised pos tagger 
basic supervised model features hmm me model  except use
word identities suffixes regardless frequency  trained supervised model using
averaged perceptron number iterations chosen follows  split training set    
development     training pick number iterations optimize accuracy
development set  finally  trained full training set using iterations report results
    sentence test set 
augmented standard features learned hidden state current token 
unsupervised method  b rown c lark ldc  hmm me sp   figure   shows average
accuracy supervised model varied type unsupervised features  average
taken    random samples training set training set size  see figure  
using sem supervised features models improves performance even
    labeled sentences  moreover  see hmm me sp either performs well better
models 

   

fic ontrolling c omplexity part   s peech nduction

ldc
brown
hmm me sp
clark

                   
  training samples

ldc
brown
hmm me sp
clark

  
 
 
 
 
 

                   
  training samples

es

ldc
brown
hmm me sp
clark

                   
  training samples

  
 
 
 
 
 

improvement 

en  

  
 
 
 
 
 

improvement 

bg

improvement 

  
 
 
 
 
 

                   
  training samples

  
 
 
 
 
 

improvement 

ldc
brown
hmm me sp
clark

improvement 

en  

improvement 

  
 
 
 
 
 

pt

ldc
brown
hmm me sp
clark

                   
  training samples

dk

ldc
brown
hmm me sp
clark

                   
  training samples

figure    error reduction using induced clusters features semi supervised model
function labeled data size  top left  en    top middle  en    top right  pt 
bottom left  bg  bottom middle  es  bottom right  dk 

   conclusion
work investigated task fully unsupervised pos induction five different languages 
identified proposed solutions three major problems simple hidden markov model
used extensively task  i  treating words atomically  ignoring orthographic
morphological information addressed replacing multinomial word distributions
small maximum entropy models  ii  excessive number parameters allows models
fit irrelevant correlations adressed discarding parameters small support
corpus  iii  training regime  maximum likelihood  allows high word ambiguity
addressed training using pr framework word ambiguity penalty  show
solutions improve model performance improvements additive  comparing
regular hmm achieve impressive improvement       average 
compared system main competing systems show approach
performs better every language except english  moreover  approach performs well across
languages learning conditions  even hyperparameters tuned conditions 
induced clusters used features semi supervised pos tagger trained small
amount supervised data  show significant improvements  moreover  clusters induced
system always perform well better clusters produced systems 

   

fig raa   g anchev  c oheur   p ereira     taskar

acknowledgments
joo v  graa supported fellowship fundao para cincia e tecnologia  sfrh 
bd               fct project cmu pt humach           fct  inesc id multiannual funding  piddac program funds  kuzman ganchev partially supported
nsf itr eia          ben taskar partially supported darpa cssg      award
onr      young investigator award  lusa coheur partially supported fct  inesc id
multiannual funding  piddac program funds 

appendix a  unsupervised optimization
berg kirkpatrick et al         describe feature rich hmm show training model
using direct gradient rather em lead better results  however  report results
en   corpus  table   compares implementation training regimes  bk em 
bk dg  different languages  comparing two training regimes  see
clear winner  bk em wins   cases  bg en   dk  loses three 
clear predict method suitable  follow discussion
  authors propose difference arises algorithm starts fine tune
weights rare features relative trains weights common features short
suffixes  case direct gradient training  start optimization  weights common
features change rapidly weight gradient proportional feature frequency 
training progresses  weight transferred rarer features  contrast  em training 
optimization done completion m step  even first iterations em
counts mostly random  rarer features get lot weight mass  prevents
model generalizing  optimization terminates local maximum closer starting
point  allow em use common features longer tried small experiments
initially permissive stopping criteria m step  em iterations
permissive stopping criteria  require stricter stopping criteria  tended improve em 
find principled method setting schedule convergence criteria m step 
furthermore  small experiments explain direct gradient better em
languages worse others 
related study  salakhutdinov et al         compares convergence rate em direct
gradient training  identifies conditions em achieves newton like behavior 
achieves first order convergence  conditions based amount missing information 
case approximated number hidden states  potentially  difference
lead different local maxima  mainly due non local nature line search procedure gradient based methods  fact  looking results  dg training seems work better
corpora higher number hidden states  en    es  work worse corpora
fewer hidden states  bg en    
table   compare implementation hmm me model implementation
berg kirkpatrick et al          using conditions  regularization parameter  feature set 
convergence criteria  initialization  observe significant differences results  communication
code comparison revealed small implementation differences  use bias feature
not  random seed  parameters initialized differently theirs 
   http   www cs berkeley edu  tberg gradvsem main html

   

fic ontrolling c omplexity part   s peech nduction

en  
bk em     
bk dg     
hmm me     

  many
en   pt
bg dk es
                        
                        
                        

en  
    
    
    

   
en   pt
bg dk
es
                        
                        
                        

en  
bk em     
bk dg     
hmm me     

vi
en   pt
bg dk es
                        
                        
                        

en  
    
    
    

v
en   pt
bg dk
es
                        
                        
                        

table    em vs direct gradient berg kirkpatrick et al         implementation compared
implementaion em hmm maximum entropy emission probabilities 
rows starting bk berkeley implementation  rows starting
implementation 

different implementations optimization algorithm  different number iterations 
corpora differences result better performance implementation 
corpora implementation gets better results  leave details well better
understanding differences optimization procedure future work  since
main focus present paper 

appendix b  evaluation metrics
compare performance different models one needs evaluate quality induced
clusters  several evaluation metrics clustering proposed previous work  metrics
use evaluate divided two types  reichart   rappoport         mapping based
information theoretic  mapping based metrics require post processing step map cluster
pos tag evaluate accuracy supervised pos tagging  information theoretic  it 
metrics compare induced clusters directly true pos tags 
  many mapping     mapping  haghighi   klein        two widely used mapping
metrics    many mapping  hidden state mapped tag cooccurs
most  means several hidden states mapped tag  tags might
used all      mapping greedily assigns hidden state single tag  case
number tags hidden states same  give     correspondence  major
drawback latter mapping fails express information hidden states 
typically  unsupervised models prefer explain frequent tags several hidden states 
combine rare tags  example pt corpus   tags occur
corpus  grouping together subdividing nouns still provides lot information
true tag assignments  however  would captured     mapping  metric
tends favor systems produce exponential distribution size induced cluster
independent clusters true quality  correlate well information theoretic
metrics  graa et al          nevertheless    many mapping drawbacks  since
distinguish clusters based frequent tag  so  cluster split almost evenly

   

fig raa   g anchev  c oheur   p ereira     taskar

nouns adjectives  cluster number nouns  mixture
words different tags gives   many accuracy 
information theoretic measures use evaluation variation information  vi 
 meila        validity measure  v   rosenberg   hirschberg         based
entropy conditional entropy tags induced clusters  vi desirable geometric properties metric convexly additive  meila         however  range vi values
dataset dependent  vi lies       log n   n number pos tags  allow
comparison across datasets different n   validity measure  v  entropy based
measure always lies range         satisfy geometric properties
vi  reported give high score large number clusters exist  even
low quality  reichart   rappoport         information theoretic measures
proposed better handle different numbers clusters  instance nvi  reichart   rappoport 
       however  work testing conditions corpora number clusters problem exist  christodoulopoulos  goldwater  steedman       
present extensive comparison evaluation metrics  related work maron  lamar 
bienenstock        present another empirical study metrics conclude vi metric
produce results contradict true quality induced clustering  giving high
scores simple baseline systems  instance assigning label words 
point several problems     metric explained previously  since
metric comparison focus work compare methods using four metrics
described section 

references
abeill  a          treebanks  building using parsed corpora  springer 
afonso  s   bick  e   haber  r     santos  d          floresta sinta c tica  treebank portuguese  proc  lrec  pp           
baum  l   petrie  t   soules  g     weiss  n          maximization technique occurring
statistical analysis probabilistic functions markov chains  annals mathematical
statistics                
berg kirkpatrick  t   bouchard ct  a   denero  j     klein  d          painless unsupervised
learning features  proc  naacl 
bertsekas  d   homer  m   logan  d     patek  s          nonlinear programming  athena scientific 
brown  p  f   desouza  p  v   mercer  r  l   pietra  v  j  d     lai  j  c          class based n gram
models natural language  computational linguistics             
chen  s          conditional joint models grapheme to phoneme conversion  proc 
ecsct 
christodoulopoulos  c   goldwater  s     steedman  m          two decades unsupervised pos
induction  far come   proc  emnlp  cambridge  ma 
civit  m     mart  m          building cast lb  spanish treebank  research language  
computation               

   

fic ontrolling c omplexity part   s peech nduction

clark  a          combining distributional morphological information part speech induction  proc  eacl 
dempster  a   laird  n     rubin  d          maximum likelihood incomplete data via
em algorithm  journal royal statistical society  series b  methodological         
freitag  d          toward unsupervised whole corpus tagging  proc  coling  association
computational linguistics 
ganchev  k   graa  j   gillenwater  j     taskar  b          posterior regularization structured
latent variable models  journal machine learning research               
gao  j     johnson  m          comparison bayesian estimators unsupervised hidden
markov model pos taggers  proc  emnlp  pp          honolulu  hawaii  acl 
goldwater  s     griffiths  t          fully bayesian approach unsupervised part of speech
tagging  proc  acl  vol      p      
graa  j   ganchev  k   pereira  f     taskar  b          parameter vs  posterior sparisty latent
variable models  proc  nips 
graa  j   ganchev  k     taskar  b          expectation maximization posterior constraints 
proc  nips  mit press 
graa  j  a  d  a  v          posterior regularization framework  learning tractable models
intractable constraints  ph d  thesis  universidade tcnica de lisboa  instituto superior
tcnico 
haghighi  a     klein  d          prototype driven learning sequence models  proc  htlnaacl  acl 
headden  iii  w  p   mcclosky  d     charniak  e          evaluating unsupervised part of speech
tagging grammar induction  proc  coling  pp         
hwa  r   resnik  p   weinberg  a   cabezas  c     kolak  o          bootstrapping parsers via
syntactic projection across parallel texts  special issue journal natural language
engineering parallel texts                
johnson  m          doesnt em find good hmm pos taggers  proc  emnlp conll 
kromann  matthias t          danish dependency treebank underlying linguistic
theory  second workshop treebanks linguistic theories  tlt   pp          vxj 
sweden 
lamar  m   maron  y     bienenstock  e          latent descriptor clustering unsupervised pos
induction  proceedings      conference empirical methods natural language
processing  pp          cambridge  ma  association computational linguistics 
lamar  m   maron  y   johnson  m     bienenstock  e          svd clustering unsupervised pos tagging  proceedings acl      conference  short papers  pp         
uppsala  sweden  association computational linguistics 
lee  y  k   haghighi  a     barzilay  r          simple type level unsupervised pos tagging 
proceedings      conference empirical methods natural language processing 
pp          cambridge  ma  association computational linguistics 

   

fig raa   g anchev  c oheur   p ereira     taskar

marcus  m   marcinkiewicz  m     santorini  b          building large annotated corpus
english  penn treebank  computational linguistics                
maron  y   lamar  m     bienenstock  e          evaluation criteria unsupervised pos induction  tech  rep   indiana university 
martin  s   liermann  j     ney  h          algorithms bigram trigram word clustering 
speech communication  pp           
meila  m          comparing clusteringsan information based distance  j  multivar  anal         
       
merialdo  b          tagging english text probabilistic model  computational linguistics 
              
moon  t   erk  k     baldridge  j          crouching dirichlet  hidden markov model  unsupervised pos tagging context local tag generation  proc  emnlp  cambridge  ma 
neal  r  m     hinton  g  e          new view em algorithm justifies incremental 
sparse variants  jordan  m  i   ed    learning graphical models  pp         
kluwer 
nocedal  j     wright  s  j          numerical optimization  springer 
ratnaparkhi  a          maximum entropy model part of speech tagging  proc  emnlp 
acl 
ravi  s     knight  k          minimized models unsupervised part of speech tagging 
proc  acl 
reichart  r     rappoport  a          nvi clustering evaluation measure  proc  conll 
rosenberg  a     hirschberg  j          v measure  conditional entropy based external cluster
evaluation measure  emnlp conll  pp         
salakhutdinov  r   roweis  s     ghahramani  z          optimization em expectationconjugate gradient  proc  icml  vol     
schtze  h          distributional part of speech tagging  proc  eacl  pp         
shen  l   satta  g     joshi  a          guided learning bidirectional sequence classification 
proc  acl  prague  czech republic 
simov  k   osenova  p   slavcheva  m   kolkovska  s   balabanova  e   doikoff  d   ivanova  k  
simov  a   simov  e     kouylekov  m          building linguistically interpreted corpus
bulgarian  bultreebank  proc  lrec 
smith  n     eisner  j          contrastive estimation  training log linear models unlabeled
data  proc  acl  acl 
snyder  b   naseem  t   eisenstein  j     barzilay  r          unsupervised multilingual learning
pos tagging  proceedings conference empirical methods natural language
processing  pp            association computational linguistics 
toutanova  k     johnson  m          bayesian lda based model semi supervised part ofspeech tagging  proc  nips     

   

fic ontrolling c omplexity part   s peech nduction

toutanova  k   klein  d   manning  c     singer  y          feature rich part of speech tagging
cyclic dependency network  proc  hlt naacl 
zhao  q     marcus  m          simple unsupervised learner pos disambiguation rules given
minimal lexicon  proc  emnlp 

   


