journal artificial intelligence research                  

submitted        published      

research note
policy invariance reward transformations
general sum stochastic games
xiaosong lu
howard m  schwartz

luxiaos   sce   carleton   ca
schwartz   sce   carleton   ca

department systems computer engineering
carleton university
     colonel drive  ottawa  k s  b  canada

sidney n  givigi jr 

idney g ivigi   rmc   ca

department electrical computer engineering
royal military college canada
   general crerar cres  kingston  k k  b  canada

abstract
extend potential based shaping method markov decision processes multi player
general sum stochastic games  prove nash equilibria stochastic game remains
unchanged potential based shaping applied environment  property policy
invariance provides possible way speeding convergence learning play stochastic
game 

   introduction
reinforcement learning  one may suffer temporal credit assignment problem  sutton  
barto        reward received sequence actions  delayed reward lead
difficulty distributing credit punishment action long sequence actions
cause algorithm learn slowly  example problem found
episodic tasks soccer game player given credit punishment
goal scored  number states soccer game large  take long time
player learn equilibrium policy 
reward shaping technique improve learning performance reinforcement learner
introducing shaping rewards environment  gullapalli   barto        mataric        
state space large  delayed reward slow learning dramatically 
speed learning  learner may apply shaping rewards environment supplement
delayed reward  way  reinforcement learning algorithm improve learning
performance combining  good  shaping reward function original delayed reward 
applications reward shaping found literature  gullapalli   barto       
dorigo   colombetti        mataric        randlv   alstrm         gullapalli barto       
demonstrated application shaping key press task robot trained press keys
keyboard  dorigo colombetti        applied shaping policies robot perform
predefined animate like behavior  mataric        presented intermediate reinforcement function
group mobile robots learn foraging task  randlv alstrm        combined reinforcement learning shaping make agent learn drive bicycle goal  theoretical
c
    
ai access foundation  rights reserved 

fil u   chwartz     g ivigi

analysis reward shaping found literature  ng  harada    russell        wiewiora 
      asmuth  littman    zinkov         ng et al         presented potential based shaping
reward guarantee policy invariance single agent markov decision process
 mdp   ng et al  proved optimal policy keeps unchanged adding potential based
shaping reward mdp environment  following ng et al   wiewiora        showed effects potential based shaping achieved particular initialization q values agents
using q learning  asmuth et al         applied potential based shaping reward model based
reinforcement learning approach 
articles focus applications reward shaping single agent mdp 
applications reward shaping general sum games  babes  munoz de cote  littman       
introduced social shaping reward players learn equilibrium policies iterated
prisoners dilemma game  theoretical proof policy invariance reward
transformation  research  prove nash equilibria potential based shaping
reward transformation  ng et al         nash equilibria original game
framework general sum stochastic games  note similar work devlin kudenko
       published article review  devlin kudenko proved
sufficiency based proof technique introduced asmuth et al          prove
sufficiency necessity using different proof technique article 

   framework stochastic games
stochastic games first introduced shapley         stochastic game  players choose
joint action move one state another state based joint action choose 
section  framework stochastic games  introduce markov decision processes  matrix
games stochastic games respectively 
    markov decision processes
markov decision process tuple  s  a  t    r  state space  action space 
         transition function         discount factor r   r
reward function  transition function denotes probability distribution next states
given current state action  reward function denotes received reward next state
given current action current state  markov decision process following markov
property  players next state reward depend players current state action 
players policy   defined probability distribution players actions given
state  optimal policy maximize players discounted future reward  mdp 
exists deterministic optimal policy player  bertsekas        
starting current state following optimal policy thereafter  get optimal
state value function expected sum discounted rewards  sutton   barto       
 
 
v  s    e




j rk  j    sk   s 

   

j  

k current time step  rk  j   received immediate reward time step k   j     
       discount factor  final time step       task
infinite horizon task task run infinite period  task episodic 
   

fip olicy nvariance



r eward ransformations

defined terminal time episode terminated time step   call
state episode ends terminal state st   terminal state  state value function
always zero v  st       st s  given current state action a  following
optimal policy thereafter  define optimal action value function  sutton   barto       
h



   
q  s  a     s  a    r s  a      v  s  


 s  a      pr  sk      sk   s  ak   a  probability next state sk    
given current state sk   action ak   time step k  r s  a      e rk    sk   s  ak   a 
sk       expected immediate reward received state given current state action
a  terminal state  action value function always zero q st   a      st s 
    matrix games
matrix game tuple  n  a              r            rn   n number players  ai  i              n 
action set player ri   a  r payoff function player i 
matrix game game involving multiple players single state  player i i              n 
selects action action set ai receives payoff  player payoff function ri
determined players joint action joint action space a    two player matrix
game  set matrix element containing payoff joint action pair 
payoff function ri player i i         becomes matrix  two players game
fully competitive  two player zero sum matrix game r    r   
matrix game  player tries maximize payoff based players strategy 
players strategy matrix game probability distribution players action set  evaluate players strategy  introduce following concept nash equilibrium  nash equilibrium
matrix game collection players policies        n  
vi            n   vi            n             n

   

vi    expected payoff player given players current strategies
strategy player strategy space   words  nash equilibrium collection
strategies players player better changing strategy given
players continue playing nash equilibrium policies  basar   olsder         define
qi  a              received payoff player given players joint action a               ai  
 i              n  probability player choosing action a    nash equilibrium defined
    becomes





qi  a                a     ai   n  an  

a       an a 

qi  a                a     ai   n  an             n

   

a       an a 

 ai   probability player choosing action ai player nash equilibrium
strategy  
two player matrix game called zero sum game two players fully competitive 
way  r    r    zero sum game unique nash equilibrium sense
expected payoff  means that  although player may multiple nash equilibrium
   

fil u   chwartz     g ivigi

strategies zero sum game  value expected payoff vi nash equilibrium
strategies same  players game fully competitive summation
players payoffs zero  game called general sum game  general sum game 
nash equilibrium longer unique game might multiple nash equilibria  unlike
deterministic optimal policy single player mdp  equilibrium strategies multiplayer matrix game may stochastic 
    stochastic games
markov decision process contains single player multiple states matrix game contains
multiple players single state  game one player multiple states 
define stochastic game  or markov game  combination markov decision processes
matrix games  stochastic game tuple  n  s  a              t    r            rn   n
number players    a         transition function  ai  i              n 
action set player i         discount factor ri   a  r
reward function player i  transition function stochastic game probability
distribution next states given current state joint action players  reward
function ri  s  a                denotes reward received player state taking joint
action  a              state s  similar markov decision processes  stochastic games
markov property  is  players next state reward depend current state
players current actions 
solve stochastic game  need find policy   ai maximize player
discounted future reward discount factor   similar matrix games  players policy
stochastic game probabilistic  example soccer game introduced littman  littman 
      agent offensive side must use probabilistic policy pass unknown
defender  literature  solution stochastic game described nash equilibrium
strategies set associated state specific matrix games  bowling        littman        
state specific matrix games  define action value function qi  s  a              expected reward player players take joint action a            state follow
nash equilibrium policies thereafter  value qi  s  a              known states 
find player nash equilibrium policy solving associated state specific matrix game
 bowling         therefore  state s  matrix game find nash
equilibrium strategies matrix game  nash equilibrium policies game
collection nash equilibrium strategies state specific matrix game states 
    multi player general sum stochastic games
multi player general sum stochastic game  want find nash equilibria game
know reward function transition function game  nash equilibrium stochastic
game described tuple n policies              n          n 
vi  s                        n   vi  s                        n  

   

set policies available player vi  s              n   expected sum
discounted rewards player given current state players equilibrium policies 
simplify notation  use vi  s  represent vi  s        n   state value function nash
equilibrium policies  define action value function q  s  a        expected
   

fip olicy nvariance



r eward ransformations

sum discounted rewards player given current state current joint action
players  following nash equilibrium policies thereafter  get



vi  s   

qi  s  a          s  a    n  s    

   

a     an a 

qi  s  a               

 s  a               






ri  s  a                  vi  s    

   

 s  ai   pd ai   probability distribution action ai player nash equilibrium policy   s  a                  pr  sk      sk   s  a              probability next state
given current state joint action  a               ri  s  a                expected
immediate reward received state given current state joint action  a               based
         nash equilibrium     rewritten



qi  s  a                s  a     s  ai   n  s   

a       an a 



qi  s  a                s  a     s  ai   n  s    

   

a       an a 

   potential based shaping general sum stochastic games
ng et al         presented reward shaping method deal credit assignment problem
adding potential based shaping reward environment  combination shaping
reward original reward may improve learning performance reinforcement learning
algorithm speed convergence optimal policy  theoretical studies potentialbased shaping methods appear published literature consider case single agent
mdp  ng et al         wiewiora        asmuth et al          research  extend
potential based shaping method markov decision processes multi player stochastic games 
prove nash equilibria potential based shaping reward transformation
nash equilibria original game framework general sum stochastic games 
define potential based shaping reward  s    player
 s       s    s  

   

  r real valued shaping function  st       terminal state st  
define multi player stochastic game tuple    s  a              t    r            rn   set
states  a            players action sets  transition function  discount factor 
ri  s  a                i              n  reward function player i  adding shaping reward
function  s    reward function ri  s  a                 define transformed multi player
stochastic game tuple    s  a              t    r            rn   ri  i              n  new
reward function given ri  s  a                   s      ri  s  a                 inspired ng et al 
      s proof policy invariance mdp  prove policy invariance multi player
general sum stochastic game follows 
theorem    given n player discounted stochastic game    s  a              t    r            rn   
define transformed n player discounted stochastic game    s  a              t    r    f            rn  
fn   shaping reward function player i  call potential based shaping
function form      then  potential based shaping function necessary
sufficient condition guarantee nash equilibrium policy invariance
   

fil u   chwartz     g ivigi

 sufficiency   i              n  potential based shaping function  every nash equilibrium policy nash equilibrium policy  and vice versa  
 necessity   i              n  potential based shaping function  may exist
transition function reward function r nash equilibrium policy
nash equilibrium policy m 
proof   proof sufficiency 
based      nash equilibrium stochastic game represented set policies
             n  mi






qmi  s  a             m
 s  a   
 s  ai  
 s   
 

n

a       an a 





 s  a    mi  s  ai  
 s    
qmi  s  a             m
 
n

    

a       an a 

subtract  s  sides      get



a       an a 




qmi  s  a             m
 s  a   
 s  ai  
 s     s 
 

n





qmi  s  a             m
 s  a    mi  s  ai  
 s     s  
 
n

    

a       an a 
 s     s     s         get
since a       an a 
 

n
mi
mn
 






 s  a   
 s  ai  
 s   
 qmi  s  a               s  m
 

n

a       an a 





 qmi  s  a               s  m
 s  a    mi  s  ai  
 s    
 
n

    

a       an a 

define
qmi  s  a                qmi  s  a               s  

    

get



a       an a 






qmi  s  a             m
 s  a   
 s  ai  
 s   
 

n

a       an a 



 s  a    mi  s  ai  
 s    
qmi  s  a             m
 
n

    

use algebraic manipulations rewrite action value function nash equilibrium     player stochastic game

qmi  s  a               s     s  a                rmi  s  a                  vm  s  



   s    s    s  

    

since  s  a                     equation becomes
qmi  s  a               s   

 s  a               





rmi  s  a               


   s    s    vm  s    s    
   

    

fip olicy nvariance



r eward ransformations

according      rewrite equation
qmi  s  a               s   



 

a       an a 

 
 



a       an a 

 s  a               





rmi  s  a                   s    s 







 s
 

 



 s
 

 

qmi  s   a             m



 s
 

 

n
 


 s  a               



rmi  s  a                   s    s 






 s   a   
 s      
qmi  s   a               s  
 


    

based definitions  s        qmi  s  a                    equation becomes
qmi  s  a               
 



a       an a 

 s  a               





rmi  s  a                  fi s   




qmi  s   a             
 s   a   
 s      
 


    

since equations           form equations          conclude
qmi  s  a              action value function nash equilibrium player stochastic game   therefore  obtain
qmi  s  a                qm  s  a                qmi  s  a               s  


    

state terminal state st   qmi  st   a                qmi  st   a             
 st              based      qmi  s  a                qm  s  a               find

nash equilibrium nash equilibrium   state value function
nash equilibrium stochastic game given
vm  s    vm  s   s  


    

 proof necessity 
 i              n  potential based shaping function   s        s    s  
similar ng et al        s proof necessity  define    s       s    s   
build stochastic game giving following transition function player  s reward
function rm    
 s    a     a              s        
 s    a     a              s        
 s    a              s        
 s    a              s        

rm   s    a              s       
 
rm   s    a              s        
rm   s    a              s        
rm   s    a              s        
   

    

fil u   chwartz     g ivigi

a  
s 

s 

a  
s 
figure    possible states stochastic model proof necessity

ai  i              n  represents possible action ai ai player i  a   a   represent
player  s action   action   respectively  equation  s    a     a              s             denotes
that  given current state s    player  s action a   lead next state s  matter
joint action players take  based transition function reward function 
get game model including states  s    s    s    shown figure    define    si    
f   si   s    i             based                            obtain player  s action value
function state s 

 
 
qm   s    a                 

qm   s    a              


qm  s    a               f   s    s      f   s    s     
 
 
qm  s    a               f   s    s      f   s    s    
 

nash equilibrium policy player   state s 


 s    a     

 

 
a      


a  


 
 s    a     
 

otherwise

 
a      


a  

 

    

otherwise

therefore  case  nash equilibrium policy player   state s 
nash equilibrium policy  

analysis shows potential based shaping reward form  s     
 s    s  guarantees nash equilibrium policy invariance  question becomes
select shaping function  s  improve learning performance learner  ng
et al         showed  s    vm  s  good candidate improving players learning
   

fip olicy nvariance



r eward ransformations

performance mdp  substitute  s    vm  s       get
qmi  s  a                qm  s  a             


 
 

 s  a               







rmi  s  a                   s   



rmi  s  a                   s   






qm  s   a             

 s
 

 



 s
 

 
 

n
 



a       an a 

 

 s  a               




   vm  s    s   


   s  a                rmi  s  a                   s     

    



equation      shows action value function qm  s  a              state easily obtained

checking immediate reward rmi  s  a                   s    player received state  
however  practical applications  information environment
 s  a                ri  s  a                 means cannot find shaping function  s 
 s    vm  s  without knowing model environment  therefore  goal
designing shaping function find  s  good approximation vm  s  

   conclusion
potential based shaping method used deal temporal credit assignment problem
speed learning process mdps  article  extend potential based shaping
method general sum stochastic games  prove proposed potential based shaping reward applied general sum stochastic game change original nash equilibrium
game  analysis result article potential improve learning performance
players stochastic game 

references
asmuth  j   littman  m  l     zinkov  r          potential based shaping model based reinforcement learning  proceedings   rd aaai conference artificial intelligence 
pp         
babes  m   munoz de cote  e     littman  m  l          social reward shaping prisoners
dilemma  proceedings  th international joint conference autonomous agents
multiagent systems  aamas        pp           
basar  t     olsder  g  j          dynamic noncooperative game theory  siam series classics
applied mathematics  nd  london  u k 
bertsekas  d  p          dynamic programming  deterministic stochastic models  prenticehall  englewood cliffs  nj 
bowling  m          multiagent learning presence agents limitations  ph d  thesis 
school computer science  carnegie mellon university  pittsburgh  pa 
   

fil u   chwartz     g ivigi

devlin  s     kudenko  d          theoretical considerations potential based reward shaping
multi agent systems   proceedings   th international conference autonomous
agents multiagent systems  aamas   taipei  taiwan 
dorigo  m     colombetti  m          robot shaping  developing autonomous agents
learning  artificial intelligence             
gullapalli  v     barto  a          shaping method accelerating reinforcement learning 
proceedings      ieee international symposium intelligent control  pp          
littman  m  l          markov games framework multi agent reinforcement learning 
proceedings   th international conference machine learning  pp         
mataric  m  j          reward functions accelerated learning  proceedings   th
international conference machine learning 
ng  a  y   harada  d     russell  s          policy invariance reward transformations  theory
application reward shaping  proceedings   th international conference
machine learning  pp         
randlv  j     alstrm  p          learning drive bicycle using reinforcement learning
shaping  proceedings   th international conference machine learning 
shapley  l  s          stochastic games  proceedings national academy sciences 
vol      pp           
sutton  r  s     barto  a  g          reinforcement learning  introduction  mit press 
cambridge  massachusetts 
wiewiora  e          potential based shaping q value initialization equivalent  journal
artificial intelligence research             

   


