journal artificial intelligence research                  

submitted        published      

efficient multi start strategies local search algorithms
andras gyorgy

gya szit bme hu

machine learning research group
computer automation research institute
hungarian academy sciences
     budapest  hungary

levente kocsis

kocsis sztaki hu

data mining web search research group  informatics laboratory
computer automation research institute
hungarian academy sciences
     budapest  hungary

abstract
local search algorithms applied optimization problems often suffer getting
trapped local optimum  common solution deficiency restart
algorithm progress observed  alternatively  one start multiple instances
local search algorithm  allocate computational resources  in particular  processing
time  instances depending behavior  hence  multi start strategy
decide  dynamically  allocate additional resources particular instance
start new instances  paper propose multi start strategies motivated
works multi armed bandit problems lipschitz optimization unknown
constant  strategies continuously estimate potential performance algorithm
instance supposing convergence rate local search algorithm unknown
constant  every phase allocate resources instances could converge
optimum particular range constant  asymptotic bounds given
performance strategies  particular  prove quadratic increase
number times target function evaluated needed achieve performance
local search algorithm started attraction region optimum  experiments
provided using spsa  simultaneous perturbation stochastic approximation  kmeans local search algorithms  results indicate proposed strategies work
well practice  and  cases studied  need logarithmically evaluations
target function opposed theoretically suggested quadratic increase 

   introduction
local search algorithms applied optimization problems often suffer getting trapped
local optimum  moreover  local search algorithms guaranteed converge
global optimum conditions  such simulated annealing simultaneous
perturbation stochastic approximation  spsa  see  e g   spall  hill    stark         usually
converge slow pace conditions satisfied  hand 
algorithms employed aggressive settings  much faster convergence local
optima achievable  guarantee find global optimum  common soluc
    
ai access foundation  rights reserved 

figyorgy   kocsis

tion escape local optimum restart algorithm progress observed
 see e g   mart  moreno vega    duarte        zabinsky  bulger    khompatraporn       
references therein  
alternatively  one start multiple instances local search algorithm  allocate
computational resources  particular  processing time  instances depending
behavior  instances started time  number instances may grow
time depending allocation strategy   see  e g   chapter    battiti  brunato   
mascia       references therein   type problems computational cost
usually measured total number steps made search algorithm instances 
often reflects situation evaluation target function optimized
expensive  costs related determine algorithms use next negligible
compared former  e g   clearly case task tune parameters
system whose performance tested via lengthy experiments  see  e g   bartzbeielstein        hutter  hoos  leyton brown    stutzle         paper address
problem dynamically starting several instances local search algorithms
allocating resources instances based  potential  performance 
knowledge  solutions problem either based heuristics
assumption local optima search algorithms converge
extreme value distribution  see section   below   paper  propose new multi start
strategies mild conditions target function  attractive theoretical
practical properties  supposing convergence rate local search algorithms
unknown constant  strategies continuously estimate potential performance
algorithm instance every phase allocate resources instances could
converge optimum particular range constant  selection mechanism
analogous direct algorithm  jones  perttunen    stuckman        finkel  
kelley        horn        optimizing lipschitz functions unknown constant 
preference given rectangles may contain global optimum  optimum
within rectangle estimated optimistic way  estimate depends
size rectangle  strategies use function describing convergence rate
local search algorithms similar way size rectangles used
direct algorithm 
since proposed multi start strategies potential performance local search
algorithm continuously estimated currently best value target function
returned algorithm  method restricted work local search algorithms
return best known value target function step  case 
example  certain meta learning problems  goal find good parameter
setting learning algorithm  search space parameter space learning
algorithm  one step local search methods means running learning algorithm
completely possibly large data set  hand  local search algorithm
sort gradient search optimizing error function training data 
value target function usually available case batch learning
 potentially cheap computations   gradient estimated
samples 
rest paper organized follows  section   summarizes related research 
problem defined formally section    new multi start local search strategies
   

fiefficient multi start strategies local search algorithms

paper described analyzed section    section     deal selection
mechanism among fixed number instances local search algorithm  while 
addition  simple schedules starting new instances considered section     
natural extensions case finitely many local search algorithm instances 
section concludes discussion results section      simulation results
real synthetic data provided section    conclusions future work
described section   

   related work
problem allocating resources among several instances search algorithms
comfortably handled generalized version maximum k armed bandit problem 
original version problem consists several rounds  round one
chooses one k arms  receives reward depending choice  goal
maximizing highest reward received several rounds  model easily used
problem considering local search algorithm instance arm  pulling
arm means taking one additional step corresponding algorithm  is  evaluating
target function point suggested algorithm  reward received
value target function sampled point  generic algorithm standard
maximum k armed bandit problem  reward assumed independent
identical distribution  provided adam         so called reservation price
instance introduced  gives maximum amount resources worth spend
instance  instance achieves reservation price  useless select again 
computation reservation price depends model algorithm
learnt specific constraints 
consider scenario several instances some  possibly randomized local
search algorithms run goal maximizing expected performance  instance run terminates  scenario natural assume
values returned instances  usually local optima target function  independent  furthermore  since good search algorithms follow  usually heuristic 
procedures yield substantially better results pure random guessing  cicirello
smith              suggested rewards  evaluated target function values 
search instances may viewed maximum many random variables  if instances run sufficiently long time   hence may modeled extreme value
distributions  several algorithms based assumption  hence developed
maximum k armed bandit problem returns following generalized extreme value
distributions  cicirello smith apply  somewhat heuristic  methods use
extreme value distribution assumption decision point meta learning algorithm 
streeter smith      a  use model obtain upper confidence bounds
performance estimate type algorithms used try algorithms
best expected result  latter theoretically justified example natural
strategy probe algorithm instances while  estimate future performance
based results trial phase  use promising algorithm
time remaining  streeter smith      b  proposed distribution free approach
   

figyorgy   kocsis

combines multi armed bandit exploration strategy heuristic selection among
available arms 
standard maximum k armed bandit problem rewards round
assumed independent  clearly case situation
algorithm instances run parallel reward evaluating target function
point improvement upon current maximum  since samples chosen local
search algorithm usually depend previous samples  nevertheless  ideas lessons
learnt maximum k armed bandit problems used case  well 
example  algorithm threshold ascent streeter smith      b  gives reasonably
good solutions case  principle probing instances using
promising time remaining carries situation easily 
algorithms  first exploration exploitation phase  referred
sequel explore and exploit algorithms  class algorithms  simple rules
suggested beck freuder        predict future performance algorithm 
carchrae beck        employ bayesian prediction 
another related problem find fast algorithms among several ones solve
problem  precisely  several algorithm instances available produce
correct answer certain question run sufficiently long time  time needed
algorithm instance find answer assumed random quantity
independent identical distributions instances  goal combine
given algorithms minimize expected running time answer found 
distribution running time known  optimal non adaptive time allocation strategy 
perform sequence runs certain cut off time depends distribution
 luby  sinclair    zuckerman         distribution unknown  particular running
time sequence chosen results expected total running time
logarithmic factor larger optimum achievable distribution known  note
strategy among provide schedule increases number
algorithm instances  set up specialized problem  goal find
 optimal approximation optimum running time number steps
needed given search algorithm achieve approximation  note
case running time algorithm instance providing  suboptimal solution
defined infinity  results luby et al  remain valid  optimal solution
found positive probability  problem  kautz  horvitz  ruan 
gomes  selman        proposed allocation strategy based updating dynamically
belief run time distribution  concerning latter  hoos stutzle       
found empirically run time distributions approximately exponential certain  nphard  problems  ribeiro  rosseti  vallejos        dealt comparison
different run time distributions 
finally  set time allocation strategies available optimization problem solved several times  one use standard multi armed bandit framework
done gagliolo schmidhuber                    
running several instances algorithm several algorithms parallel selecting
among algorithms intensively studied  example  area meta   non adaptive time allocation strategy running time algorithm instance fixed advance 
is  measured performance algorithm instances effect schedule 

   

fiefficient multi start strategies local search algorithms

learning  vilalta   drissi        automatic algorithm configuration  hutter et al         
underlying problem similar cases  automatic algorithm configuration
usually refers tuning search algorithms  meta learning used subset
problems  tuning machine learning algorithms  the latter often allows specific use
data   main problem allocate time slices particular algorithms
aim maximizing best result returned  allocation may depend intermediate
performance algorithms  automatic algorithm configuration metalearning systems use various heuristics explore space algorithms parameters
 see  e g   hutter et al         
finally  important note that  although multi start local search strategies solve
global optimization problems  concentrate maximizing performance given
underlying family local optimization methods  since choice latter major
effect achievable performance  compare results vast literature
global optimization 

   preliminaries
assume wish maximize real valued function f d dimensional unit hypercube
      d   is  goal find maximizer x       d f  x     f
f   max f  x 
x     d

denotes maximum f       d   simplicity  assume f continuous
      d    continuity f implies existence x   and  particular  f bounded 
therefore  without loss generality  assume f non negative 
form f known explicitly  search algorithms usually evaluate f several
locations return estimate x f based observations 
obvious trade off number samples used  i e   number points
target function f evaluated  quality estimate  performance
search strategy may measured accuracy achieves estimating f
constraint number samples used 
given local search algorithm a  general strategy finding good approximation
optimum x run several instances initialized different starting points
approximate f maximum f value observed  concentrate local search
algorithms defined formally sequence possibly randomized sampling functions
sn         dn       d   n                evaluates f locations x    x          xi    
si  x            xi      starting point x    s  chosen uniformly random
      d   n observations returns estimate x maximum f   respectively 

bn   argmax f  xk  
bn   
x

f  x
 kn 

ties argmax function may broken arbitrarily  is  samples xk
bn chosen them  avoid ambiguity
achieve maximum  x
   results easily extended  arbitrary valued  bounded piecewise continuous functions
finitely many continuous components 

   

figyorgy   kocsis

simplify notation  following  unless stated explicitly otherwise  adopt
convention use argmax denote maximizing sample smallest index 
results remain valid choice break ties 
simplicity  consider starting single local search algorithm different
random points  although results work extended allow varying
parameters  including situation running different local search algorithms 
parameter would choose actually employed search algorithm   well allow
dependence among initializations  that is  starting point parameters
local search instance may depend information previously obtained target
function  
clear starting points sampled uniformly       d algorithm
bn   converges
evaluated starting point strategy consistent  is  f  x
maximum f probability   number instances tends infinity  in
worst case perform random search known converge maximum almost
surely   hand  algorithm favorable properties possible
design multi start strategies still keep random search based consistency 
provide much faster convergence optimum terms number evaluations
f 
bn   bounded non decreasing  converges  no matter
since sequence f  x
random effects occur search   next lemma  proved appendix a  shows
that  high probability  convergence cannot arbitrarily slow 
bn     f    p  e      
lemma   f       d   let e denote event limn f  x
bn   f
then          event e e   p  e     f  x
uniformly almost everywhere e   words  exists non negative  nonincreasing function g  n  limn g  n     



bt     f    
b   f  x
bn   g  n  nfi lim f  x
p lim f  x
   




certain cases  g  n    o en    shown nesterov         gradient based 
optimization convex functions  gerencser vago        noise free spsa
convex functions  kieffer        k means clustering  or lloyds algorithm  one
dimension log concave densities  results pertain simple situation
one local optimum global one  many results
extended general situations  observed exponential rate convergence
experiments functions many local maxima 
convergence property local search algorithms guaranteed lemma  
exploited next section derive efficient multi start search strategies 

   multi start search strategies
standard multi start search strategies run instance seems converge
location hope beat currently best approximation f  
   practice usually assume local search algorithms converge local optima  f may
assumed local optimum 

   

fiefficient multi start strategies local search algorithms

alternative way using multiple instances local search algorithms run algorithms
parallel  round decide algorithms take extra step  approach
may based estimating potential performance local search algorithm based
lemma    note g known  obvious way would run instance
possible performances become separated high probability sense
margin performance actually best second best algorithm
large actually best algorithm guaranteed best  long run 
high probability  could pick best instance run given
computational budget exhausted  this would simple adaptation explore andexploit idea choosing best algorithm based trial phase beck   freuder 
      carchrae   beck        
practice  however  g usually known  certain problem classes local
search algorithms may known belong function class  example  g may
known  multiplicative  constant factor  here  example  constant may
depend certain characteristics f   maximum local steepness   even
latter case  best instance still cannot selected high probability matter
large margin  as g may arbitrarily large   however  using ideas general
methodology lipschitz optimization unknown constant  jones et al         
get around problem estimate  certain optimistic way  potential
performance algorithm instance  round step promising
ones 
main idea resulting strategy summarized follows  assume
k instances algorithm a  denoted a            ak   let xi n                k denote
location f evaluated ai nth time take step  xi  
starting point ai   estimate location maximum algorithm ai n
samples  steps 
bi n   argmax f  xi t  
x
 tn

bi n   
maximum value function estimated fi n   f  x
i  let   limn fi n denote limiting estimate maximum f
provided ai   let g defined lemma   largest values 
f   max  
i       k

since f best achievable estimate maximum f given actual algorithms
a            ak   g gives high probability convergence rate algorithms provide
best estimate maximum long run  note assumption deals
limiting estimate usually local maximum separately  is  assumption
made algorithms whose limiting estimates less f    then  ai evaluates f
ni r points end rth round ai converges best achievable estimate
f   lemma   have  probability least    
fi ni r g  ni r   


fi ni r   g  ni r  
   

   

figyorgy   kocsis

optimistic estimate f   ai suboptimal sense limn fi n   f
estimate still optimistic rate convergence slower g  
pessimistic rate convergence slower g   latter desirable sense
negatively biased estimate expected performance algorithm
want use  we waste samples suboptimal choices  
practice g usually known exactly  estimate     often cannot constructed  hand  g known constant factor construct
family estimates scales  let g denote normalized version g
g         g  n  g  n  constant n  construct family estimates
fi ni r   cg  ni r  

   

c ranges positive reals  reasonable choose  round 
algorithms take another step provide largest estimate values c
 typically  algorithm gives largest estimate c   c interval
containing c algorithm provides largest estimate c i  
way get around fact know real scaling factor g  
certainly use algorithms provide largest value     c   g     g     
and  discussed later  waste many samples algorithms
maximize     values c  using optimistic estimate     similar  spirit 
optimistic estimates standard upper confidence bound type solution
multi armed bandit problem  auer  cesa bianchi    fischer        well known
search algorithm  hart  nilsson    raphael        
however  exact  local  convergence rate known  even constant factor 
many local search algorithms  even is  corresponding bounds usually
meaningful asymptotic regime  often practical interest  therefore 
give freedom design algorithm  going use estimate
form
fi ni r   ch ni r  
   
where  similarly requirements g   h positive  monotone decreasing function
limn h n       assume  without loss generality  h        
actual form h based theoretical analysis resulting algorithms
heuristic considerations  essentially use h functions converge zero
exponentially fast  agreement exponentially fast local convergence rates
examples given lemma    optimal choice h  given  example  g  
known  left future work 
    constant number instances
idea translated algorithm metamax k  shown figure   
consider case fixed number instances  goal perform
 almost  well best  in hindsight   using minimum number
br
evaluations f   note slight abuse notation metamax k  algorithm x
fr denote estimates algorithm r rounds  and r steps samples  
first part step  a  metamax sweep positive c select local
search algorithms maximize estimate      easy see  ai maximizes
   

fiefficient multi start strategies local search algorithms

metamax k   multi start strategy k algorithm
instances 
parameters  k     positive  monotone decreasing function h
limn h n      
initialization               k  take step algorithm ai
once  let ni       fi     f  xi     
round r              
 a               k select algorithm ai exists c    
fi ni r    ch ni r      fj nj r    ch nj r   

   

j              k  ni r    fi ni r        nj r    fj nj r    
several values selected step number
ni r  keep one selected uniformly random 
 b  step selected ai   update variables  is  set ni r  
ni r      ai selected  ni r   ni r  otherwise 
bi n
selected ai evaluate f  xi ni r   compute new estimates x
i r
fi ni r  
 c  let ir   argmaxi       k fi ni r denote index algorithm
currently largest estimate f   estimate location
br   x
bir  n
maximum x
value fr   fir  nir  r  
ir  r
figure    metamax k  algorithm 
    particular c   u closed interval containing u ai
maximizes     c i  therefore  round  strategy metamax k  selects
local search algorithms ai corresponding point  h ni r     fi ni r   
corner upper convex hull set
pr     h nj r     fj nj r      j              k       max fj nj r     
 jk

   

selection mechanism illustrated figure   
avoid confusion  note random selection step  a  metamax k  implies
algorithms exactly state  is   ni r    fi ni r       nj r    fj nj r   
i  j  one algorithm selected uniformly random  this pathological situation
may arise  e g   beginning algorithm local search algorithms give
estimate f range step numbers   apart case one
least used algorithms provides currently best estimate  happens surely
first round usually happen later  and includes previous pathological case  
guaranteed round use least two algorithms  one largest
   

figyorgy   kocsis

   

   

f x 

   

   

   

   

   

 
   

   

   

   

h n 

   

   

   

   

figure    selecting algorithm instances metamax  points represent algorithm
instances  algorithms lie corners upper convex hull
 drawn blue lines  selected 

estimate fi ni r    fr   for small values c   one smallest step number
nj r   for large values c   thus  usually half total number function
calls f used optimal local search algorithm  observation gives practical lower bound  which valid apart pathological situation mentioned above 
proportion function calls f made optimal local search algorithms  surprisingly 
theorem   shows lower bound achieved algorithm asymptotically 
randomization step  a  precludes using multiple instances
step number introduced speed algorithm certain pathological cases 
example  a  converges correct estimate  algorithms a            ak
produce estimate round  independently samples  inferior
estimates a    use randomization  half calls compute f
made a    without randomization would drop   k
round would use algorithm  furthermore  could take step algorithms
lie convex hull  similar pathological examples constructed
beneficial use algorithms corners  hand  almost never
happens practice three algorithms lie line  algorithms typically
never fall non corner points convex hull 
remainder section analyze performance metamax k 
algorithm  proposition   shows algorithm consistent sense performance asymptotically achieves best algorithm instance number rounds
increases  understand algorithm better  lemma   provides general sufficient condition algorithm instance advanced given round  while  based
result  lemma   provides conditions ensure suboptimal algorithm instances
used round stepped many times  i e   evaluated f
many points  before  lemma   gives upper bound number algorithm
   

fiefficient multi start strategies local search algorithms

instances used round  results lemmas used show theorems  
  remark   optimal algorithm instances used  asymptotically  least
minimum frequency that  turn  yields asymptotic rate convergence
metamax k  algorithm 
following proposition shows metamax k  algorithm consistent
sense 
proposition   metamax k  algorithm consistent sense fr f
r 

n
f lim fi n  
f lim fr   min
r

i       k

n

proof proof follows trivially fact algorithm selected infinitely
often  is  limr ni r     see latter  show every k rounds
number steps taken least used algorithm  is  mini       k ni r   guaranteed
increase one  is  k   
min ni kk k 

i       k

   

described above  round select exactly one algorithms made
least number steps  thus  k algorithms  minimum step number
per algorithm increase k rounds  completes proof 
 
metamax k  algorithm efficient suboptimal algorithms step
often  next lemma provides sufficient conditions algorithm used
given round 
lemma   algorithm instance aj used round r     metamax k 
algorithm  algorithms ai ak fi ni r   fj nj r   fk nk r either
ni r nj r


h nj r  
h nj r  


fj nj r fi ni r  
   
  fk nk r
h nk r  
h nk r  
proof round algorithms corners convex hull pr   used 
easy see algorithm aj used round r algorithms ai
ak fi ni r   fj nj r   fk nk r either ni r nj r
fi ni r fk nk r
fi ni r fj nj r

 
h nj r   h ni r  
h nk r   h ni r  

   

finish proof show     implies latter  indeed      equivalent
h nk r   fi ni r fj nj r   h nj r   fi ni r fk nk r     h ni r   fk nk r fj nj r   
last term right hand side inequality negative assumptions 
inequality satisfied
h nk r   fi ni r fj nj r   h nj r   fi ni r fk nk r  
   

figyorgy   kocsis

equivalent     

 

lemma provides conditions using algorithm instances certain
round depend actual performance instances  next result gives similar
conditions  however  based best estimates  usually local optima  achievable
algorithms  let   limr fi ni r asymptotic estimate algorithm ai f  
let f   max ik denote best estimate achievable using algorithms a            ak  
let             k  set optimal algorithms converge best estimate
f  for algorithms   let  o  denote cardinality  i e   number
optimal algorithm instances   note random variable depends actual
realizations possibly randomized search sequences algorithms  next lemma
shows j   o  aj used round r used often far 
lemma   let

  f max fj
j o

denote margin estimates best second best algorithms 
    u   random index r u      j   o  aj
used metamax k  round r       r u 
  



f
j
min ni r
 
nj r h  h
 
    
i       k
f u
furthermore  let          o  let g i denote convergence rate algorithm
ai guaranteed lemma    let g  n    maxio g i  n  n  p  r u 
     where g   generalized inverse g    suboptimal
kg   u  f            fk


algorithm aj   j   o  used r   kg   u  probability least    o  given
 
limiting estimates f            fk
proof let o  since limn fi n     f assumption     implies
 u 
limr ni r     almost surely finite random index ri    
 u 

r   ri f fi n
u
i r
f fr u 

    

using lemma   easily derive high probability upper bound r u    since
r   kg   u    r       implies ni r g   u   lemma   yields



p fi ni r u r   r   f    

follows probability least    o  fi ni r u 
   o    thus 
implies r u  chosen p  r u    r  f            fk
prove lemma  enough show      
clearly        algorithm pick estimate one best algorithms
r u  rounds  let ak algorithm least number steps taken end
   

fiefficient multi start strategies local search algorithms

round r  is  nk r   mini ni r   fk nk r fj nj r aj used round r     
moreover  since aj   o  fj nj r fj   fr case aj used round r    
nj r nir  r  recall nir  r number evaluations f initiated actually
best algorithm ir    therefore  lemma   implies aj used r   r  
 
fj nj r fk nk r
h nj r   h nk r    
 
fr fk n
k r

clearly satisfied
h nj r   h min ni r    


fj
f u

 

    

    u     since     
 

fj
fj nj r fk nk r
fj nj r
 
 
 
fr fk nk r
fr
f u

applying inverse h sides      proves       and  hence  lemma 

 

result provides individual condition suboptimal algorithm
used round  hand  one optimal algorithms
stepped sufficiently many times  give cumulative upper bound number
suboptimal algorithms used round 
lemma   assume h decreases asymptotically least exponentially fast  is 
exist         n     h n   
h n    n   n   assume r large
enough ni r   n i  let r     maxi fi n

  fr
i r

fi ni r
fr

    

r
ln
ln     algorithms stepped round r      x denotes smallest integer
least large x 

proof let i    i            im denote indices algorithms chosen round r    
fi   r   fi   r     fim  r   fr   lemma   implies ni   r   ni   r     nim  r
fr fik  r    fr fik   r  
k                 repeated application inequality implies
fr r fr fim   r    fr fim   r       m   fr fi   r   fr m 
yields

ln r
ln
assumed     algorithms chosen round r      fact finishes
proof 
 
m  

   

figyorgy   kocsis

based lemmas      next theorem shows local search algorithm
converges fast enough  exponentially problem dependent rate  faster exponential  half function calls evaluate f correspond optimal algorithm
instances 
theorem   assume performance algorithms ai                k
same  is   o    k  suppose
 
 
fj
h n     
lim sup
 
    
  min  
j o
h n 
n
f
asymptotically least half function
respond optimal algorithm  is 
p
ni r
 
p lim inf pio

k
r
 
i   ni r

calls evaluate f metamax k  cor 



f            fk
    


furthermore              constant r       
 p
  
k
n
i   i r
f fr g
       o 

    

    

simultaneously r   r     
probability least    o  given f            fk
   
g defined lemma    threshold r
    depends     h  g   
 
g defined lemma   

proof show suboptimal aj chosen large enough r nj r   mink nk r  
lemma    sufficient prove that  large enough r 
  
fj
 
    
min nk r     h
h min nk r    
k
k
f u
    u    recall r larger r u    almost surely finite
random index lemma    
minimum      taken finite set  follows exists small
enough positive u  
 
 
fj
h n     
lim sup
 
    
min  
j o
h n 
n
f u
clearly implies      limr mink nk r        fact finishes proof
      first part theorem 
next prove       let nu     threshold      holds n nu  
furthermore  lemma   union bound      holds local search algorithm
ai g i place g simultaneously probability least    o  
   

fiefficient multi start strategies local search algorithms

    slight modification lemma   imply      holds simultaneously
 
r   k max g   u   nu     r probability least    o  given f            fk
since
round two algorithms used  r   r   c
p
n
i r
c r
pio
   c kr
high probability  since latter bounded       
k
n
i r
i  
pk
p
ni r
c r  k     io ni r i  
r   r      r  r  k   
  

l p highmprobability  algorithm ai   used least
k
i  

ni r
 o     

rounds  implying statement theorem via lemma   

 

remark   proof theorem   based lemma    proof based lemma  
possible  since setting   minj o    fj  f   lemma       implies that 
large enough r  r   round approximately length   lemma 
may happen although decay rate h exponential  quite fast
enough satisfy       optimal scenarios theorem hold 
case turns number algorithms converging local maximum
plays key role determining usage frequency optimal algorithms 
theorem   assume estimates provided algorithms a            ak converge
n     distinct limit points  k     o  algorithms converge f   k    k            kn  
algorithms converge suboptimal limit points  respectively  suppose furthermore
h decreases asymptotically least exponentially fast  is          
lim supn h n   
h n     
p lim inf
r

p

io ni r
pk
i   ni r

 


kmax

f           fk     

k k    kmax  

kmax   max in   ki  
furthermore  using definitions lemma              
constant threshold r   
  
 
pk
k
n
max
i   i r
f fr cg
 k k    kmax     o 
simultaneously r   r     
probability least    o  given f            fk
r    depends     f            sfk convergence rate algorithms   
given  fix random trajectories algorithms 
proof suppose f            fk
single suboptimal algorithm statement trivial kmax   k k   kmax    
    round least two algorithms used  one
suboptimal one  assume least two suboptimal algorithms 
assume aj ak converge suboptimal local maxima  strictly less f   
r large enough  optimal algorithm ai better suboptimal ones 

   instead convergence rate algorithms  r    may defined dependent g  

   

figyorgy   kocsis

is  ai converges f fi ni r   fj nj r   fk nk r   assume  without loss generality 
fj nj r fk nk r   nj r nk r clearly aj chosen round r      assume
nj r   nk r   since fj nj r fk nk r convergent sequences  as r    large enough
r have  j k     
 

fj nj r fk nk r
  j k tj k


fi n fk n
i r

    

k r

tj k   ln j k   ln positive integer  note aj ak converge
point  is  limr  fj nj r fk nk r        second term left hand side
     converges    j k chosen   implying tj k      rearranging
inequality one obtains
   tj k  fi ni r   tj k fk nk r   fj nj r  

    

nj nk tj k   conditions h fact nj r nk r tend
infinity r  recall      imply that  large enough r  h nj r   h nk r     tj k   since
fi ni r fk nk r large enough r       obtain


h nj r  
h nj r  
tj k
tj k

fk nk r  
fi ni r  
fj nj r       fi ni r   fk nk r  
h nk r  
h nk r  
thus  lemma    r large enough  aj cannot used round r     nj r nk r tj k  
since nj r nk r tend infinity  follows that  large enough r 
 nj r nk r   tj k

    

two suboptimal algorithms aj ak   note fact implies
set suboptimal algorithms converging point  eventually one
used round  since corresponding thresholds tj k      
clearly      implies nj r nk r grow linearly r  since differences bounded       limr nj r  nk r      therefore  suboptimal algorithm
aj   limn nj r  r   kmax  this maximal rate using elements
largest group suboptimal algorithms converging local optimum   finally 
optimal algorithm used round r  large enough r 
p
p
io ni r
io ni r
p
lim inf pk
  lim inf p
r
r
n
 
i r
n
i o ni r
io
i r
i  
kmax
r
lim
 
 
r
r r    k k   
k k    kmax
kmax
used fact a  a   b  increasing function a  b      since
 
inequality holds realizations trajectories a            ak   given f            fk
first statement theorem follows 
second statement follows similarly      theorem    since exact value
r    particular interest  derivation omitted 
 

   

fiefficient multi start strategies local search algorithms

remark   main message theorem somewhat surprising observation
suboptimal algorithms slowed large group suboptimal algorithms
converging local optimum  rate suboptimal algorithms used bounded
size largest group 
    unbounded number instances
clear local search algorithms consistent  i e   achieve
global optimum f    then  despite favorable properties  metamax k  strategy
inconsistent  too  however  increase number algorithms infinity
get consistency random search  still keeping reasonably fast convergence
rate metamax k  
clearly  one needs balance exploration exploitation  is 
control often introduce new algorithm  one solution let metamax
algorithm solve problem  metamax   algorithm  given figure    extension metamax k  able run infinitely many local search algorithm
instances  major issue new local search algorithms started
time time  this ensures algorithm converge global maximum f
since performs random search   implemented modifying step  a 
metamax k  algorithm new  randomly initialized local search algorithm introduced round  randomly selecting one algorithm uniformly infinitely
many possible algorithms used far   obviously  skip initialization
step metamax k  start algorithm   samples  better control
length round  i e   exploration   round r allow use different
function h  denoted hr  may depend value measured round r  this
suppressed notation   before  assume hr          hr  n  monotone decreasing n  limn hr  n      r  typically make hr  dependent
pkr 
total number steps  i e   function calls evaluate f   tr    i  
ni r  made
algorithms round r  kr  number algorithm instances used
round r  note kr    r   r  start exactly one new algorithm
round 
desired that  although number local search algorithms grows infinity 
number times best local search algorithm advanced metamax  
algorithm approaches infinity reasonably fast  somewhat relaxing random initialization
condition  may imagine situation local search algorithms initialized
clever  deterministic way  first steps find better
value initial guesses  algorithms optimal  this may viewed result
clever initialization   may provide  example  identical estimates
            first three steps  easy see algorithm stepped
exactly twice  thus convergence optimum  which would found third
step  achieved  although random initialization search algorithms guarantees
consistency metamax    see proposition    below   robust behavior even
pathological cases preferred 
achieved slight modification algorithm  round local search
algorithm overtakes currently best algorithm  is  ir    ir    algorithm air
   

figyorgy   kocsis

metamax    multi start strategy infinitely many
algorithm instances 
parameters   hr    set positive  monotone decreasing functions
limn hr  n      
round r              
 a  initialize algorithm ar setting nr r       fr       
 b               r select algorithm ai exists c    
fi ni r    chr   ni r      fj nj r    chr   nj r   
j              r  ni r    fi ni r        nj r    fj nj r    
several values selected step number
ni r  keep one selected uniformly random 
 c  step selected ai   update variables  is  set ni r  
ni r      ai selected  ni r   ni r  otherwise 
bi n
selected ai evaluate f  xi ni r   compute new estimates x
i r
fi ni r  

 d  let ir   argmaxi       r fi ni r denote index algorithm
currently largest estimate f   estimate location
br   x
bir  n
maximum x
value fr   fir  nir  r  
ir  r
figure    metamax   algorithm 

stepped several times used times air     resulting algorithm 
called metamax  given figure    note algorithms metamax  
metamax conceptually differ one place  step  c  extended step  c 
new algorithm  result  technical modification appears step  d   and 
simplify presentation metamax algorithm  slight  insignificant modification
introduced step  b   see discussion below 
modification metamax really significant practical examples
studied  see section     number steps taken algorithm overtakes
currently best algorithm grows quickly metamax   algorithm  since
metamax overtake usually introduces short rounds  close minimum length
two many cases  leading algorithm becomes used one  goal
modification step  b  synchronize choice optimal algorithms
steps  b   c   equally good solution would choose  case tie step
   way achieve actually best algorithm dominates others terms accuracy
number calls made algorithms compute target function  type
dominance used hutter et al         slightly different context 

   

fiefficient multi start strategies local search algorithms

metamax  multi start strategy infinitely many
algorithm instances 
parameters   hr    set positive  monotone decreasing functions
limn hr  n      
round r              
 a  initialize algorithm ar setting nr r       fr       
 b               r select algorithm ai exists c    
fi ni r    chr   ni r      fj nj r    chr   nj r   
j              r  ni r    fi ni r        nj r    fj nj r    
several values selected step number
ni r  keep one smallest index 
 c  step selected ai   update variables  is  set ni r  
ni r      ai selected  ni r   ni r  otherwise 
bi n
selected ai evaluate f  xi ni r   compute new estimates x
i r

fi ni r  

 c  let ir   argmaxi       r fi ni r denote index algorithm
currently largest estimate f  in case ir unique  choose
one smallest number steps ni r    ir    ir    step
algorithm air  nir   r nir  r      times set nir  r   nir   r     
br   x
bir  n
 d  estimate location maximum x

ir  r
value fr   fir  nir  r  
figure    metamax algorithm 

 c   algorithm used current round  note that  result
modifications  currently best algorithm  with index ir   taken steps 
extra number steps taken step  c  indeed positive  important consequence
modifications that  round r  number steps taken local search
algorithm air   best end round  r  r  see theorem   
below  
rest section devoted theoretical analysis metamax  
metamax  following lines analysis provided metamax k   first  proposition     shown algorithm consistent  is  solution found
algorithm actually converges f   lemma     a counterpart lemma    shows
suboptimal algorithms make finitely many steps  lemma    gives upper
bound length round  main theoretical results section apply
   

figyorgy   kocsis

metamax algorithm  theorem    gives lower bound number steps taken
actually best algorithm end given round  while  consequence  theorem    shows rate convergence algorithm function total number
steps  i e   function calls evaluate f   used algorithm instances  turns
quadratically steps needed generic local search algorithm instance
converges optimum 
since metamax   metamax strategies perform random search  the
number algorithms tends infinity length round finite   algorithms
consistent 
proposition    strategies metamax   metamax consistent 
is 
lim fr   f
r

almost surely 
proof clearly  event fr converge f written
n

 
n
 


 
lim fr    f  
fr   f   n

r

    

n   r   r r

continuity f implies that  n  x chosen uniformly       d
qn p
  p f  x    f   n       thus  round r  p fr   f   n     qn  r  



r   p fr   f   n  finite  therefore  borel cantelli lemma  see  e g   ash  
doleans dade        implies
 
n
 

 

  
p
fr   f   n
r   r r

n  this  together      finishes proof 
p



lim fr    f

r








x

n  

p

n
 
 

r   r r

fr   f   n




 

    
 

reminder section assume local search algorithms achieve
almost optimal value eventually converge optimum 
assumption    let f r denote set local maxima f   let   f
supff  f f f  assume     algorithm ai fi n   f
n  limn fi n   f  
local search algorithms converge local optima  which reasonable assumption
practice   assumption usually satisfied  situation
hold pathological case f infinitely many local maxima set
maxima dense global maximum 
   

fiefficient multi start strategies local search algorithms

assumption    prove  similarly lemma    suboptimal algorithm selected limited number times increases h 
r   particular 
hr   h r large enough  suboptimal algorithm chosen finitely many
times 
lemma    suppose assumption     let q   p f  x    f     x uniformly
distributed       d   then  metamax   metamax algorithms 
suboptimal algorithm aj started round r    used round r     probability
least      q r  



 
nj r hr
 
 f
addition  hr  n  non decreasing function r n 
lim sup
r

h 
r 

n
j r



 f

 

almost surely 

    

particular  hr constant function  is  hr   h  r  limr nj r  
almost surely 
remark    note second
part lemma coulddrop
monotonicity
 
 


assumption hr replace hr   f max r r  hr  f      
proof consider algorithm aj used round r      first note probability
least      q r   fr   f     furthermore  newly introduced algorithm  ar  
used yet  nr   r     fr          thus  lemma    aj used


h nj r  


  fr    hr  nj r     
fj nj r fr  
hr    
since equivalent
nj r

h 
r

fj nj r
 
fr

h 
r

fj nj r
 
fr

 





h 
r



 

 


 f



fr fj nj r

fr  f  
 f      f  
 
 
    

 

f   
 f
fr
fr
first statement proof follows 
b denote first round optimal
prove second part  let r


algorithm ai fi ni rb   f     suboptimal algorithm aj   first part
b
lemma implies that  r   r 










 
 
b
b
      max r  hr 
  
nj r max r  max
h
 r r  r
 f
 f
   

figyorgy   kocsis

equality holds since h 
r  n  non decreasing r  thus




b
nj r
r
 
lim sup max
     




lim sup



h 

r h 
r
h 
r   f
r   f
r   f




b
 
r
     



max



h 
h 
 

 

 f

    

 f

b finite       finite
used h 
non decreasing r  since r
r
probability   
 
simple modification lemma   implies    optimal sample point
found limited number suboptimal algorithms chosen round 
lemma    consider algorithms metamax   metamax  suppose assumption   
holds  assume f fr      r      anyround r   r  hr  n    rn


    r     n   

ln  f
ln   r  

algorithms chosen

estimates fj f  

proof proof follows lemma   taking account suboptimal algorithm
aj satisfies fj f least one optimal algorithm chosen round
r   r  similarly       r defined lemma   bounded r      f   

l
 f
ln
ln   r  

number suboptimal algorithms used round r bounded ln   
 
ln   
r 
r 
 
finally derive convergence rate algorithm metamax  first bound
number steps taken currently best algorithm  terms number
rounds total number steps taken local search algorithms 
theorem    consider metamax algorithm  end round r number
steps taken currently best algorithm r  r  is 
r nir  r    r 

    

furthermore  number calls nir  r evaluate f currently
best algorithm air
pr
bounded function total number times tr   i   ni r target function f
evaluated local search instances

 tr      
nir  r
 
    
 
proof first statement lemma simple  since round actually
best algorithm takes one step overtaking  one two steps
   

fiefficient multi start strategies local search algorithms

overtaking  indeed  round r    overtaking  is  ir   ir   
nir  r   nir  r       otherwise  ir    ir    nir  r   nir   r      since
  nir   r nir   r    
  nir  r nir   r   
situations  since first round clearly algorithm used takes   step 
is  ni               follows 
prove second part  notice round r  nir   r      algorithms
stepped step  c  algorithm used taken steps
currently best one  also  step  c  extra samples used overtaking 
case overtaking  air advanced step  c   well air   
nir   r      extra steps taken air   therefore 
tr tr     nir   r      
thus  since overtaking happens round    obtain
tr    

r
x

  nis   s       

s  

then      
tr      

r
x
s  

        r      r          nir  r      nir  r   

yields      

 

note proof used crude estimate length usual round  without
overtaking  relative to  example  lemma     this  however  affects result
constant factor long able bound number rounds number
extra steps taken overtaking happens  since effect overtakings
introduces quadratic dependence proof       experimental results section  
show  see figure     number algorithm instances  which turn number r
rounds  usual growth rate  tr   ln tr    which  taken account  may sharpen
bound often best algorithm chosen 
assumption     random search component metamax implies eventually optimal algorithm best  point convergence
rate optimal local search algorithms determine performance search 
number steps taken best local search algorithm bounded theorem    
theorem    suppose assumption    holds  almost surely finite random
index r rounds r   r  estimate fr metamax algorithm
total number steps tr taken local search algorithms end round r
satisfies


 t
 
 

 
r

f fr g
 
probability least     g defined lemma   global maximum f  
   

figyorgy   kocsis

remark     i  value r bounded high probability using properties
uniform random search actual problem  would yield similar bounds
theorems     metamax k  algorithm   ii  note exploration exploitation
trade off metamax algorithm  value r potentially decreased introduce
new algorithms often  nir  r reduced time   iii  theorems      
imply that  asymptotically  metamax algorithm needs quadratically function
evaluations local search algorithm
psthat ensured converge optimum 
particular  f form f  x   
i    x isi  x  si form partition
      d   isi denotes indicator function si   belong nicely behaving
function class local search algorithm started si converges maximum
si  e g   f piecewise concave function exponential convergence rate
spsa algorithm  used sufficiently small step size   preserve
performance local search algorithm original function class price
asymptotically quadratic increase number function calls evaluate f  i e  
total number steps taken local search algorithm instances  
    discussion results
sense theoretical results presented previous sections weak 
consistency result metamax k  algorithm follows easily fact
local search algorithm used infinitely many times  consistency metamax  
metamax follows consistency random search  performance bounds
provided disadvantage asymptotic sense hold
possibly large number rounds  a weakness bounds minimum number
rounds obtained properties uniform random search sampling particular
problem  neglecting attractive properties algorithms   fact  quite easy
construct scheduling strategies consistent asymptotically arbitrarily
large fraction function evaluations  even almost all  used optimal local search
algorithms  explore and exploit algorithms achieve goals number
function evaluations used known ahead use arbitrarily small fraction
evaluations target function f exploration  compare performance
algorithms explore and exploit algorithms section    particular  match
performance guarantees metamax family  use algorithms spend half
time exploration half exploitation  exploration part
uniform allocation strategy used finite number local search algorithms 
schedule luby et al         used infinitely many local search algorithms 
although theoretical guarantees proved paper metamax family hold
explore and exploit algorithms  experiments metamax family seems
behave superior compared algorithms  expected 
theoretical results give sufficient guidance chose parameter
h hr  the time varying version h considered metamax k  algorithm
simplicity ease presentation   results require sufficiently fast
exponential decay h  problem dependent cannot determined advance 
sufficiently fast decay rate would ensure  example  metamax k  algorithm
could always use stronger results theorem   would never deal
   

fiefficient multi start strategies local search algorithms

case bound theorem   holds  one may easily choose h
function decreases super exponentially  would make asymptotic bounds work 
however  would slow exploration  in extreme case hr  n     excluded
conditions  exploration would performed  algorithms would use
actually best local search algorithm   practice always found
appropriate chose hr decay exponentially  furthermore  found even
effective gradually decrease decay rate enhance exploration time elapses  the
rationale behind approach assumption good algorithms
less converged while  may greater potential exploration
improve estimates   finally  connection g h
investigated 
keeping limitations theoretical results mind  still believe
theoretical analyses given provide important insight algorithms may guide
potential user practical applications  especially since properties metamax
family proved asymptotic regime  e g   rounds quite short 
usually observed practice  well  furthermore  think possible
improve analysis bound thresholds results become valid
reasonable values  would require different approach and  therefore  left
future work 

   experiments
variants metamax algorithm tested synthetic real examples  since
negligible difference performance metamax   metamax  
following present results metamax k  metamax  first demonstrate performance algorithm optimizing synthetic function  using spsa
local search algorithm   next behavior algorithm tested standard data
sets  show metamax applied tuning parameters machine learning
algorithms  classification task solved neural network  parameters
training algorithm  back propagation  fine tuned metamax combined spsa 
metamax applied boost performance k means clustering  end
section  compare results experiments theoretical bounds obtained
section     
experiments  accordance simplifying assumptions introduced
section    main difference individual runs particular local search
algorithm starting point  obviously  general diversification techniques exist 
example  parameters local search algorithm could vary instance
instance  including running instances different local search algorithms  parameter would select actually employed search algorithm   initialization  starting
point parametrization  new instance could depend results delivered
   example  relative difference average error emetamax   metamax  
emetamax metamax optimizing parameters multi layer perceptron learning letter
data set  see section     especially figure    right details        standard
deviation       averaged
     experiments   relative difference defined


fiemetamax   emetamax   max emetamax     emetamax   

   

figyorgy   kocsis

existing instances  although metamax strategies could applied
general scenarios  behavior better studied simpler scenario  hence 
experiments correspond setup 
    optimizing parameters spsa
section compare two versions metamax algorithm six multi start
strategies  including three constant three variable number algorithm
instances  strategies run fixed time steps  is  target function
evaluated times  together local search instances  note several reference
strategies use parameter  
used spsa  simultaneous perturbation stochastic approximation  spall       
base local search algorithm cases  spsa local search algorithm sampling
function uses gradient descent stochastic approximation derivative 
actual location xt    xt             xt d    spsa estimates lth partial derivative f
f  xt   bt   f  xt bt  
 
ft l  xt l    
 t bt l
bt l i i d  bernoulli random variables components vector
bt   uses sampling function st  xt     xt   ft  xt   choose next point
sampled  is 
xt   l   xt l   ft l  xt l  
l               t scalar parameters  
implementation algorithm followed guidelines provided
 spall         gain sequence   a  a          perturbation size  
  t                               values vary
different experiments  chosen heuristically based experience similar
problems  this cause problem here  goal experiments
provide fast solutions global optimization problems hand demonstrate
behavior multi start algorithms compared   addition two evaluations
required perturbed points  evaluate function current point xt  
starting point chosen randomly  function evaluated first point 
six reference algorithms metamax k  metamax algorithms compared following 
unif  algorithm selects constant number instances spsa uniformly 
implementation instance   mod k selected time t  k denotes
number instances 
thrasc  threshold ascent algorithm streeter smith      b   algorithm begins selecting fixed number instances once  phase
time step thrasc selects best estimates produced far algorithm
instances ai                k previous time steps  ai counts
many estimates produced ai   denoting latter value si t   time
algorithm selects instance index   argmaxi u  si t  ni t   ni t    ni t
   

fiefficient multi start strategies local search algorithms

number times ith instance selected time t 
u    n     

 

p
 n    
n

  ln  t k    parameters algorithm  experiments
best value appeared      set       note threshold
ascent developed maximum k armed bandit problem  nevertheless 
provides sufficiently good performance setup test experiments 
rand  random search algorithm  seen running sequence spsa
algorithms instance used exactly one step  evaluation
random starting point spsa algorithm 
luby  algorithm based work luby et al          method runs several
instances spsa sequentially other  ith instance run ti steps 
ti defined

 k   
   k  
ti  
ti k      
 k     k  
definition produces schedule first  k   algorithm instances
one run  k  steps  two  k  steps  four  k  steps  on 
ee unif  algorithm instance explore and exploit algorithms  first
   steps unif algorithm used exploration  and  subsequently  exploration
phase  spsa instance achieved highest value exploration phase
selected 
ee luby  algorithm similar ee unif  except luby used exploration 
versions metamax algorithm tested  motivated fact spsa
known converge global optimum exponentially fast f satisfies restrictive
conditions  gerencser   vago         chose hr  n  decays exponentially fast 
control exploration far suboptimal algorithm instances  allowed hr  n 
time varying function  is  changes tr   total number function calls
evaluate f  or equally  total number steps taken  algorithms far  thus 
round r     used


hr  n    en 

tr

    

 note used time varying version hr case metamax k 
latter easily extended situation  omitted simplify
presentation  
algorithms fixed number local search instances  metamax k   unif 
ee unif  thrasc   number instances k set     simulations 
choice provided reasonably good performance problems analyzed 
multi start algorithms tested using two versions synthetic function 
tuning parameters learning algorithm two standard data sets 
   

figyorgy   kocsis

synthetic function slightly modified  version griewank function  griewank 
      



 xl x     x l
cos
f  x   
   
l
l  
l  
x    x            xd   xl constrained interval         show
results   dimensional    dimensional cases 
parameters spsa                dimensional case 
               dimensional case  performance search algorithms
measured error defined difference maximum value
function  in case    best result obtained search algorithm given
number steps  results multi start strategies two    dimensional test functions shown figure    error curve averaged       
runs  strategy run         steps  or iterations   one may observe
cases two versions metamax algorithm converge fastest  thrasc
better unif  luby seems fairly competitive two  two exploreand exploit type algorithms  ee unif ee luby  similar performance  dimensional function  clearly better non exploiting base algorithms 
   dimensional function behavior somewhat pathological sense low
values performances best among algorithms  increasing  
error actually increases respective base algorithms achieve smaller errors
values   random search seems option   dimensional function 
similar results obtained dimensions       pathological behavior
explore and exploit algorithms start appear gradually starting   dimensional
function  pronounced   dimensions onwards  limited experimental data
obtained higher dimensions      averaged hundred runs  shows
superiority metamax preserved high dimensional problems well 
reason pathological behavior explore and exploit strategies  i e  
error curves monotone decreasing number iterations  illustrated
follows  assume two spsa instances  one converging global optimum
another one converging suboptimal local optimum  assume first
steps optimal algorithm gives better result  suboptimal algorithm takes
reaches local maximum  algorithms run even further  optimal
algorithm beats suboptimal one  exploration stopped first last
regime  explore and exploit algorithm choose first  optimal local search instance 
whose performance may get quite close global optimum exploitation phase
 even stopped first regime   exploration stopped middle regime 
suboptimal search instance selected exploitation  whose performance may
even get close global optimum  scenario  error exploitation
phase  i e  end  lower small  increases higher values   decrease
error increasing assured optimal instance converges
exploration phase past suboptimal local optima  results selecting optimal
local search instance exploitation  scenario error decrease fast
   modification made order significant differences values function
global maximum local maxima 

   

fi  

  

 

 

   

   

average error

average error

efficient multi start strategies local search algorithms

    

     

      

     

      
rand
luby
unif
thrasc
ee luby
ee unif
metamax   
metamax

 e   

 e   

    

 

rand
luby
unif
thrasc
ee luby
ee unif
metamax   
metamax

 e   

  

   

    

     

 e   

      

 

  

iteration

   

    

     

      

iteration

figure    average error multi start strategies   dimensional  left 
   dimensional  right  modified griewank function      confidence intervals
shown color corresponding curves  note
intervals small 
 

   

    

    

average error

average error

   

     

      
rand
luby
unif
thrasc
ee luby
ee unif
metamax   
metamax

 e   

 e   

 

     

      

rand
luby
unif
thrasc
ee luby
ee unif
metamax   
metamax

 e   

  

   

    

     

      

iteration

 e   

 

  

   

    

     

      

iteration

figure    average error multi start strategies tuning parameters multilayer perceptron vehicle data set  left  letter data set  right  
    confidence intervals shown color corresponding
curves 

initially  increase may decrease till converges   
quite similar observe figure    right  pathological behavior
becomes transparent many local search algorithms  length
exploitation phase scales number local search instances length
exploration instance kept fixed  analyzing experimental data shows
complex versions scenario outlined occurred simulations
main cause observed pathological behavior  the non monotonicity error
curves  
tuning parameters learning algorithm  used two standard data
sets uci machine learning repository  asuncion   newman         vehicle
   

figyorgy   kocsis

letter  multilayer perceptron learning algorithm weka  witten   frank       
 here back propagation algorithm used training phase   two parameters
tuned  learning rate momentum  range         size
hidden layer multilayer perceptron set    number epochs
     parameters spsa algorithm             
   dimensional griewank function  as previous experiment  parameters
chosen based experience   rate correctly classified items test set
vehicle using multilayer perceptron varying values two parameters shown
figure    highest rate           similarly  classification rate letter
shown figure    highest rate        
error rates optimized multilayer perceptron data sets vehicle
letter shown figure    parameters learning algorithm tuned
multi start strategies above  error cases difference
best classification rate obtained                   respectively 
best classification rate obtained multi start strategies given number steps 
results shown averaged       runs  observe metamax algorithm
 with increasing number algorithm instances  converged fastest average  three
strategies fixed number algorithm instances nearly identical results  luby
 and explore and exploit variant  slightly worse these  random search
slowest  although performed nearly badly synthetic functions 
reason random search relatively better performance  relative
used spsa  could twofold   i  large parts error surface offer fairly small error 
 ii  error surface less smooth  therefore spsa less successful using
gradient information  explore and exploit variants performed well vehicle data
set initially  performance worsened larger values  compared metamax 
algorithms extent   this  coupled observation figure   
right would suggest explore and exploit variants competitive small values
  despite asymptotic guarantees 
summary  metamax algorithm  with increasing number algorithm instances  provided far best performance tests  usually requiring significantly
fewer steps find optimum algorithms  e g   letter data set
metamax algorithm found global optimum runs         time steps 
conclude metamax converged faster multi start strategies investigated four test cases  notable advantage difficult surfaces  at least
gradient based optimization viewpoint  induced classification tasks 
    k means clustering
section consider problem partitioning set d dimensional real vectors
xj rd   j              n clusters  cluster si represented center  or
reconstruction  point ci rd                n   cost function minimized sum
distances
pn x pci   data points corresponding centers  is  want
minimize i   xsi  x  ci    two necessary conditions optimality  see  e g  
linde  buzo    gray        gersho   gray                      n  
si    x    x  ci    x  cj   j              n  
   

    

fiefficient multi start strategies local search algorithms

figure    classification rate vehicle data set  rates plotted subtracting
         thus global optima scattered black spots
corresponding value equal       

figure    classification rate letter data set  rates plotted subtracting        thus global optima scattered black spots
corresponding value equal       

 with ties broken arbitrarily 
ci   argmin
crd

x

xsi

   

 x  c  

    

figyorgy   kocsis

p

x

 

usual choice squared euclidean distance  case ci   xs
 si     according necessary conditions  k means algorithm  or generalized lloyd
algorithm  see  e g   linde et al         gersho   gray        alternates partitioning data set according      centers fixed  recomputing
centers      partitioning kept fixed  easily seen cost
 or error  cannot increase steps  hence algorithm converges
local minimum cost function  practice  algorithm stops  or
insufficient  decrease cost function  however  k means algorithm often trapped
local optimum  whose value influenced initial set centers  spsa 
restarting k means different initialization may result finding global optimum 
consider two initialization techniques  first  termed k means  chooses centers
uniformly random data points  second  k means    arthur   vassilvitskii 
      chooses initial center uniformly random data set  chooses
centers data points probability proportional distance
data point closest center already selected 

k means algorithm usually terminates relatively small number steps 
thus multi start strategies bounded number instances would run active local
search algorithms  therefore appear particularly attractive  however 
natural domain consider strategy starts new instance  previous
finished  strategy referred subsequently serial  mentioned
considerations  test metamax  variant algorithms applicable
unbounded number instances   experiments spsa  used hr      
note theoretical results indicate k means may converge exponential
rate  in particular  kieffer       showed rate convergence exponential
random variables log concave densities   dimension provided logarithm
density piecewise affine  
two multi start strategies  serial metamax tested data set cloud
uci machine learning repository  asuncion   newman         data set
employed arthur vassilvitskii        well  number clusters set
ten  performance multi start strategies defined difference
smallest cost function obtained strategy given number steps smallest
cost seen experiments              results averaged       runs
plotted figure    initialization methods metamax strategy converges
faster serial strategy  note data set  k means  
clever initialization procedure yields faster convergence standard k means
uniform initialization  consistent results presented arthur
vassilvitskii        

   extension clustering random variables well known straightforward  omitted
paper consider clustering finite data sets 
   note metamax algorithm practical modification local search
algorithm terminated chosen anymore  clearly improves performance
algorithm chosen anymore improvement observed 

   

fiefficient multi start strategies local search algorithms

      

     
    
   

average error

average error

     

    

   

  
 
   
    
     

  

      
 

serial kmeans
metamax kmeans

 

  

   

    

     

      

iteration

 e   

serial kmeans  
metamax kmeans  

 

  

   

    

     

      

iteration

figure    average error multi start strategies k means  left  kmeans    right       confidence intervals shown color
corresponding curves 

    practical considerations
experiments metamax algorithm presented above  observed
number algorithm instances r  shown figure     grows rate  tr   ln tr    recall
tr total number function calls evaluate f   total number steps 
algorithm instances end round r   hand  derivation

theoretical bounds  see theorem    theorem     used bound r   tr   
contrast quadratic penalty suggested theorem     plugging  tr   ln tr  
estimate r theorem would find logarithmic factor calls
evaluate f  total number steps  needed achieve performance search
algorithm started attraction region optimum 
finally  perhaps main practical question concerning metamax family multistart algorithms decide use them  rule thumb  say
sufficiently large performance difference average run
local search algorithm best one  clearly  single local search produces
acceptable result worth effort run several instances local search 
especially complicated schedule  many real problems often case
relatively easy get close optimum  may acceptable
applications  approaching optimum greater precision hard  latter
importance  metamax algorithm variants may useful  last  one may
wonder computational costs algorithms  discussed before 
consider case evaluation target function expensive  clearly
case griewank function  used demonstrate basic properties
algorithm  holds many optimization problems practice  including
experiments considered paper  problems function evaluation
indeed expensive  and depends available data   overhead introduced
metamax algorithms depends number rounds  metamax k 
algorithm find upper convex hull set k points round 
worst case take long o k     calculations  practice usually
   

figyorgy   kocsis

number algorithm instances   ln t  t

   

griewank  d
griewank   d
vehicle
letter
k means
k means  
min
max

   
   
   
 
   
   
   
   

 

  

   

    

     

      

iteration

figure     number algorithm instances  r  metamax  average number
instances shown six benchmarks  griewank function       dimensional   parameter tuning multilayer perceptron  on vehicle
letter data set   clustering k means k means   
maximum minimum number instances runs benchmarks
shown  one notice larger values tr       tr   ln tr r
    tr   ln tr  

much cheaper  upper convex hull determined point corresponds
actually best estimate point corresponds least used algorithm 
requires o k  computations  even less  special ordering tricks
introduced  since target function f evaluated least twice round  average
o k     computational overhead needed evaluation f worst
case  practically reduced o k   even less  similar considerations hold
metamax   metamax algorithms  resulting average o r    worst case
overhead call f  in r rounds   closer o r  even less practice 
examples considered  apart case griewank function   amount
overhead negligible relative computational resources needed evaluate
f single point 

   conclusions
paper provided multi start strategies local search algorithms  strategies
continuously estimate potential performance algorithm instance optimistic
way  supposing convergence rate local search algorithms unknown
constant  every phase resources allocated instances could converge
optimum particular range constant  three versions algorithm
presented  one able follow performance best fixed number
local search algorithm instances  two that  gradually increasing number
local search algorithms  achieve global consistency  theoretical analysis asymptotic
   

fiefficient multi start strategies local search algorithms

behavior algorithms given  specifically  mild conditions
function maximized  e g   set values local maxima dense
global maximum   best algorithm  metamax  preserves performance local
search algorithm original function class quadratic increase
number times target function needs evaluated  asymptotically   simulations
demonstrate algorithms work quite well practice 
theoretical bound suggests target function evaluated
quadratic factor times achieve performance search algorithm started
attraction region optimum  experiments found logarithmic
penalty  clear whether difference result slightly conservative
 asymptotic  analysis choice experimental settings  also  finite sample
analysis algorithm interest  experiments indicate metamax
algorithm provides good performance even relatively small number steps taken
local search algorithms  sense provides speed up compared
approaches even number times target function evaluated  i e   total
number steps taken algorithms together  relatively small  finally 
future work needed clarify connection convergence rate optimal
algorithms  g   function hr used exploration 

acknowledgments
authors would thank anonymous referees numerous insightful
constructive comments  research supported part mobile innovation
center hungary  national development agency hungary research
technological innovation fund  ktia otka cnk         pascal  network
excellence  ec grant no           parts paper presented ecml     
 kocsis   gyorgy        

appendix a  proof lemma  
bn    since un   almost everywhere e  egoroffs
fix        let un   f f  x
theorem  see  e g  ash   doleans dade        implies event e e
  p  e     un   uniformly almost everywhere e   second part
lemma follows definition uniform convergence 
 

references
adam  k          learning searching best alternative  journal economic
theory              
arthur  d     vassilvitskii  s          k means    advantages careful seeding 
proceedings   th annual acm siam symposium discrete algorithms  pp 
         
ash  r  b     doleans dade  c  a          probability   measure theory  academic press 
   

figyorgy   kocsis

asuncion  a     newman  d  j          uci machine learning repository 
auer  p   cesa bianchi  n     fischer  p          finite time analysis multiarmed
bandit problem  machine learning                   
bartz beielstein  t          experimental research evolutionary computation
new experimentalism  natural computing series  springer  new york 
battiti  r   brunato  m     mascia  f          reactive search intelligent optimization 
vol     operations research computer science interfaces  springer verlag 
beck  c  j     freuder  e  c          simple rules low knowledge algorithm selection 
regin  j  c     rueher  m   eds    cpaior  lecture notes computer science
      pp        springer 
carchrae  t     beck  j  c          low knowledge algorithm control  proceedings
nineteenth national conference artificial intelligence  aaai   pp       
cicirello  v  a     smith  s  f          heuristic selection stochastic search optimization 
modeling solution quality extreme value theory  proceedings   th
international conference principles practice constraint programming  pp 
        springer 
cicirello  v  a     smith  s  f          max k armed bandit  new model exploration
applied search heuristic selection  proceedings twentieth national
conference artificial intelligence  pp           
finkel  d  e     kelley  c  t          convergence analysis direct algorithm  tech 
rep  crsc tr       ncsu mathematics department 
gagliolo  m     schmidhuber  j          learning dynamic algorithm portfolios  annals
mathematics artificial intelligence                   ai math      special
issue 
gagliolo  m     schmidhuber  j          learning restart strategies  veloso  m  m   ed   
ijcai      twentieth international joint conference artificial intelligence 
vol     pp          aaai press 
gagliolo  m     schmidhuber  j          algorithm selection bandit problem
unbounded losses  blum  c     battiti  r   eds    learning intelligent optimization  vol       lecture notes computer science  pp        springer
berlin heidelberg 
gerencser  l     vago  z          mathematics noise free spsa  proceedings
ieee conference decision control  pp           
gersho  a     gray  r  m          vector quantization signal compression  kluwer 
boston 
griewank  a  o          generalized descent global optimization  journal optimization theory applications           
hart  p   nilsson  n     raphael  b          formal basis heuristic determination
minimum cost paths  systems science cybernetics  ieee transactions on 
               
   

fiefficient multi start strategies local search algorithms

hoos  h  h     stutzle  t          towards characterisation behaviour stochastic
local search algorithms sat  artificial intelligence              
horn  m          optimal algorithms global optimization case unknown lipschitz
constant  journal complexity               
hutter  f   hoos  h  h   leyton brown  k     stutzle  t          paramils  automatic
algorithm configuration framework  journal artificial intelligence research         
       
jones  d  r   perttunen  c  d     stuckman  b  e          lipschitzian optimization without
lipschitz constant  journal optimization theory applications             
    
kautz  h   horvitz  e   ruan  y   gomes  c     selman  b          dynamic restart policies  proceedings eighteenth national conference artificial intelligence
 aaai   pp         
kieffer  j  c          exponential rate convergence lloyds method i  ieee trans 
inform  theory  it            
kocsis  l     gyorgy  a          efficient multi start strategies local search algorithms 
buntine  w   grobelnik  m   mladenic  d     shawe taylor  j   eds    machine
learning knowledge discovery databases  vol       lecture notes computer science  pp          springer berlin heidelberg 
linde  y   buzo  a     gray  r  m          algorithm vector quantizer design  ieee
transactions communications  com          
luby  m   sinclair  a     zuckerman  d          optimal speedup las vegas algorithms 
information processing letters             
mart  r   moreno vega  j     duarte  a          advanced multi start methods  gendreau  m     potvin  j  y   eds    handbook metaheuristics   nd edition    edition  
springer 
nesterov  y          introductory lectures convex optimization  basic course 
kluwer academic publishers 
ribeiro  c   rosseti  i     vallejos  r          use run time distributions evaluate
compare stochastic local search algorithms  stutzle  t   birattari  m     hoos 
h   eds    engineering stochastic local search algorithms  designing  implementing
analyzing effective heuristic s  vol       lecture notes computer science 
pp        springer berlin heidelberg 
spall  j   hill  s     stark  d          theoretical framework comparing several stochastic
optimization approaches  calafiore  g     dabbene  f   eds    probabilistic
randomized methods design uncertainty  chap     pp         springerverlag  london 
spall  j  c          multivariate stochastic approximation using simultaneous perturbation
gradient approximation  ieee transactions automatic control             
spall  j  c          implementation simultaneous perturbation algorithm stochastic optimization  ieee transactions aerospace electronic systems             
   

figyorgy   kocsis

streeter  m  j     smith  s  f       a   asymptotically optimal algorithm max
k armed bandit problem  proceedings  twenty first national conference
artificial intelligence eighteenth innovative applications artificial intelligence conference  pp         
streeter  m  j     smith  s  f       b   simple distribution free approach max
k armed bandit problem  principles practice constraint programming cp         th international conference  cp       nantes  france  september       
      proceedings  pp         
vilalta  r     drissi  y          perspective view survey meta learning  artificial
intelligence review               
witten  i  h     frank  e          data mining  practical machine learning tools
techniques   nd edition   morgan kaufmann  san francisco 
zabinsky  z  b   bulger  d     khompatraporn  c          stopping restarting strategy
stochastic sequential search global optimization  j  global optimization 
               

   


