journal of artificial intelligence research                  

submitted        published      

controlling complexity in part of speech induction
joo v  graa

joao   graca   l   f  inesc   id   pt

l  f inesc id
lisboa  portugal

kuzman ganchev

kuzman   google   com

google inc 
new york  ny  usa

lusa coheur

luisa   coheur   l   f  inesc   id   pt

 

l f inesc id
lisboa  portugal

fernando pereira

pereira   google   com

google inc 
mountain view  ca  usa

ben taskar

taskar   cis   upenn   edu

computer   information science
university of pennsylvania

abstract
we consider the problem of fully unsupervised learning of grammatical  part of speech  categories from unlabeled text  the standard maximum likelihood hidden markov model for this
task performs poorly  because of its weak inductive bias and large model capacity  we address this
problem by refining the model and modifying the learning objective to control its capacity via parametric and non parametric constraints  our approach enforces word category association sparsity 
adds morphological and orthographic features  and eliminates hard to estimate parameters for rare
words  we develop an efficient learning algorithm that is not much more computationally intensive than standard training  we also provide an open source implementation of the algorithm  our
experiments on five diverse languages  bulgarian  danish  english  portuguese  spanish  achieve
significant improvements compared with previous methods for the same task 

   introduction
part of speech  pos  categories are elementary building blocks for the syntactic analysis of text
that play an important role in many natural language processing tasks  from machine translation to
information extraction  while english and a handful of other languages are fortunate enough to
have comprehensive pos annotated corpora such as the penn treebank  marcus  marcinkiewicz 
  santorini         most of the worlds languages have extremely limited linguistic resources  it
is unrealistic to expect annotation efforts to catch up with the explosion of unlabeled electronic
text anytime soon  this lack of supervised data will likely persist in the near future because of the
investment required for accurate linguistic annotation  it took two years to annotate       sentences
with syntactic parse trees for the chinese treebank  hwa  resnik  weinberg  cabezas    kolak 
      and four to seven years to annotate        sentences across a range of languages  abeill 
      

c
    
ai access foundation  all rights reserved 

fig raa   g anchev  c oheur   p ereira     taskar

supervised learning of taggers from pos annotated training text is a well studied task  with
several methods achieving near human tagging accuracy  ratnaparkhi        toutanova  klein 
manning    singer        shen  satta    joshi         however  pos induction  where one does
not have access to a labeled corpus  is a more difficult task with much room for improvement  in
recent literature  pos induction has been used to refer to two different tasks  for the first one  in
addition to raw text  we are given a dictionary containing the possible tags for each word and the
goal is to disambiguate the tags of a particular word occurrence  merialdo         for the second
task  we are given raw text  but no dictionary is provided  the goal is to cluster words that have the
same grammatical behavior  in this work  we target this latter  more challenging  unsupervised pos
induction task 
recent work on this task typically relies on distributional or morphological features  since words
with the same grammatical function tend to occur in similar contexts and to have common morphology  brown  desouza  mercer  pietra    lai        schtze        clark         however  those
statistical regularities are not enough to overcome several challenges  first  the algorithm has to
decide how many clusters to use for broad syntactic categories  for instance  whether to distinguish
between plural and singular nouns   second  category size distribution tends to be uneven  for example  the vast majority of the word types are open class  nouns  verbs  adjectives   and even among
open class categories  there are many more nouns than adjectives  this runs contrary to the learning
biases in commonly used statistical models  a common failure of those models is to clump several
rare categories together and split common categories 
for individual word types  a third challenge arises from ambiguity in grammatical role and
word sense  many words can take on different pos tags in different occurrences  depending on
the context of occurrence  the word run can be either a verb or a noun   some approaches assume
 for computational and statistical simplicity  that each word can only have one tag  aggregating
all local contexts through distributional clustering  schtze         while this one tag per word
assumption is clearly wrong  across many languages for which we have annotated corpora  such
methods perform competitively with methods that can assign different tags to the same word in
different contexts  lamar  maron  johnson    bienenstock         this is partly due to the typical
statistical dominance of one of the tags for a word  especially if the corpus includes a single genre 
such as news  the other reason is that less restrictive models do not encode the useful bias that most
words typically take on a very small number of tags 
most approaches that do not make the one tag per word assumption take the form of a hidden
markov model  hmm  where the hidden states represent word classes and the observations are
word sequences  brown et al         johnson         unfortunately  standard hmms trained to
maximize likelihood perform poorly  since the learned hidden classes do not align well with true
pos tags  besides the potential model estimation errors due to non convex optimization involved
in training  there is a more pernicious problem  typical maxima of likelihood do not align well with
maxima of pos tag accuracy  smith   eisner        graa  ganchev  pereira    taskar        
suggesting serious mismatch between model and data 
in this work  we significantly reduce that modeling mismatch by combining three ideas 
 the standard hmm treats words as atomic units  without using orthographic and morphological information  that information is critical to generalization in many languages  clark 
       to address this problem  we reparameterize the standard hmm by replacing the multinomial emission distributions by maximum entropy models  similar to the work of berg   

fic ontrolling c omplexity in part  of  s peech i nduction

kirkpatrick  bouchard ct  denero    klein       and graa         this allows the use
of orthographic and morphological features in the emission model  moreover  the standard
hmm model has a very large number of parameters  the number of tags times the number of
word types  this presents an extremely rich model space capable of fitting irrelevant correlations in the data  to address this problem we dramatically reduce the number of parameters
of the model by discarding features with small support in the corpus  that is  those involving
rare words or word parts 
 the hmm model allows a high level of ambiguity for the tags of each word  as a result  when
maximizing the marginal likelihood  common words typically tend to be associated with every
tag with some non trivial probability  johnson         however  a natural property of pos
categories across many languages and annotation standards is that each word only has a small
number of allowed tags  to address this problem we use the posterior regularization  pr 
framework  graa  ganchev    taskar        ganchev  graa  gillenwater    taskar       
to constrain the ambiguity of word tag associations via a sparsity inducing penalty on the
model posteriors  graa et al         
we show that each of the proposed extensions improves the standard hmm performance  and
moreover  that the gains are nearly additive  the improvements are significant across different
metrics previously proposed for this task  for instance  for the   many metric  our method attains
a       average improvement over the regular hmm  we also compare the proposed method with
eleven previously proposed approaches  for all languages but english and all metrics except     
our method achieves the best published results  furthermore  our method appears the most stable
across different testing scenarios and always shows competitive results  finally  we show how
the induced tags can be used to improve the performance of a supervised pos tagging system in
a limited labeled data scenario  our open source software for pos induction and evaluation is
available at http   code google com p pr toolkit  
this paper is organized as follows  section   describes the basic hmm for pos induction and its
maximum entropy extension  section   describes standard em and our sparsity inducing estimation
method  section   presents a comprehensive survey of previous fully unsupervised pos induction
methods  in section   we provide a detailed experimental evaluation of our method  finally  in
section    we summarize our results and suggest ideas for future work 

   models
the model for all our experiments is based on a first order hmm  we denote the sequence of
words in a sentence as boldface x and the sequence of hidden states which correspond to partof speech tags as boldface y  for a sentence of length l  we have thus l hidden state variables
yi              j      i  l where j is the number of possible pos tags  and l observation variables
xi              v       i  l  where v is the number of word types  to simplify notation  we assume
that every tag sequence is prefixed with the conventional start tag y    start  allowing us to write
as p y   y    the initial state probability of the hmm 
the probability of a sentence x along with a particular hidden state sequence y is given by 
p x  y   

l
y

pt  yi   yi   po  xi   yi   

i  

   

   

fig raa   g anchev  c oheur   p ereira     taskar

where po  xi   yi   is the probability of observing word xi given that we are in state yi  emission
probability   and pt  yi   yi    is the probability of being in state yi   given that the previous hidden
state was yi   transition probability  
    multinomial emission model
standard hmms use multinomial emission and transition probabilities  that is  for a generic word
xi and tag yi   the observation probability po  xi   yi   and the transition probability pt  yi   yi    are
multinomial distributions  in the experiments we refer to this model simply as the hmm  this model
has a very large number of parameters because of the large number of word types  see table     a
common convention we follow is to lowercase words as well as to map words occurring only once
in the corpus to a special token unk 
    maximum entropy emission model
in this work  we use a simple modification of the hmm model discussed in the previous section 
we represent conditional probability distributions as maximum entropy  log linear  models  specifically  the emission probability is expressed as 
exp   f  x  y  
 
x  exp   f  x   y  

po  x y    p

   

where f  x  y  is a feature function  x ranges over all word types  and  are the model parameters  we
will refer to this model as hmm me  in addition to word identity  features include orthographyand morphology inspired cues such as presence of capitalization  digits  and common suffixes  the
feature sets are described in section    the idea of replacing the multinomial models of an hmm
by maximum entropy models is not new and has been applied before in different domains  chen 
       as well as in pos induction  berg kirkpatrick et al         graa         a key advantage
of this representation is that it allows for a much tighter control over the expressiveness of the
model  for many languages it is helpful to exclude word identity features for rare words in order
to constrain the model and force generalization across words with similar features  unlike mapping
all rare words to the unk token in the multinomial setting  the maxent model still captures some
information about the word through the other features  moreover  we can reduce the number of
parameters even further by using lowercase word identities while still keeping the case information
by using a case feature  table   shows the number of features we used for different corpora  note
that the reduced feature set has an order of magnitude fewer parameters than the multinomial model 

   learning
in section   we describe experiments comparing the hmm model to the me model under three
learning scenarios  maximum likelihood training using the em algorithm  dempster  laird    rubin        for both hmm and hmm me  gradient based likelihood optimization for the hmm me
model  and pr with sparsity constraints  graa et al         for both hmm and hmm me  this
section describes all three learning algorithms 
in the following  we denote the whole corpus  a list of sentences  by x    x    x            xn   and
the corresponding tag sequences by y    y    y            yn   

   

fic ontrolling c omplexity in part  of  s peech i nduction

    maximum likelihood with em
standard hmm training seeks model parameters  that maximize the log likelihood of the observed
data 
x
p  x  y 
   
log likelihood  l     log
y

where x is the whole corpus  since the model assumes independence between sentences  
log

x

p  x  y   

n
x

log

x

   

yn

n  

y

p  xn   yn   

but we use the corpus notation for consistency with section      because of the latent variables
y  the log likelihood function for the hmm model is not convex in the model parameters  and the
model is fitted using the em algorithm  em maximizes l   via block coordinate ascent on a lower
bound f  q    using an auxiliary distribution over the latent variables q y   neal   hinton        
by jensens inequality  we define a lower bound f  q    as 
l     log

x

q y 

y

p  x  y  x
p  x  y 

q y  log
  f  q    
q y 
q y 

   

y

we can rewrite f  q    as 
f  q     

x

q y  log p  x p  y x   

y

x

q y  log q y 

   

y

q y 
q y  log
p  y x 

   

  l    kl q y   p  y x   

   

  l   

x
y

using this interpretation  we can view em as performing coordinate ascent on f  q     starting
from an initial parameter estimate     the algorithm iterates two block coordinate ascent steps until
a convergence criterion is reached 
e   q t     arg max f  q  t     arg min kl q y  k pt  y   x  
q

   

q

m   t     arg max f  q t         arg max eqt    log p  x  y  


    



the e step corresponds to maximizing eq    with respect to q and the m step corresponds
to maximizing eq    with respect to   the em algorithm is guaranteed to converge to a local
maximum of l   under mild conditions  neal   hinton         for an hmm pos tagger  the
e step computes the posteriors pt  y x  over the latent variables  pos tags  given the observed
variables  words  and current parameters t for each sentence  this is accomplished by the forwardbackward algorithm for hmms  the em algorithm together with the forward backward algorithm
for hmms is usually referred to as the baumwelch algorithm  baum  petrie  soules    weiss 
      
the m step uses q t    qnt   are the posteriors for a given sentence  to fill in the values of tags
y and estimate parameters t     since the hmm model is locally normalized and the features used
   

fig raa   g anchev  c oheur   p ereira     taskar

only depend on the tag and word identities and not on the particular position where they occur  the
optimization decouples in the following way 
eqt    log p  x  y    
 

n
x

eqnt    log

n  
ln 
n x
x

ln
y

n
pt  yin   yi 
 po  xni   yin   

    

i  
n
eqnt   log pt  yin   yi 
    eqnt   log po  xni   yin  



    

n   i  

for the multinomial emission model  this optimization is particularly easy and simply involves
normalizing  expected  counts for each parameter  for the maximum entropy emission model parameterized as in equation    there is no closed form solution so we need to solve an unconstrained
optimization problem  for each possible hidden tag value y we have to solve two problems  estimate the emission probabilities po  x y  and estimate the transition probabilities pt  y    y   where the
gradient for each one of those is given by


eqt    log p  x  y  
  eqt   f  x  y   ep  x   y   f  x    y    
    

which is similar to the gradient in supervised me models  except for the expectation over all y
under q t    y  instead of observed y  the optimization is done using l bfgs with wolfes rule
line search  nocedal   wright        
    maximum likelihood with direct gradient
while likelihood is traditionally optimized with em  berg kirkpatrick et al         find that for
the hmm with the maximum entropy emission model  higher likelihood and better accuracy can
be achieved by with a gradient based likelihood optimization method  they use l bfgs in their
experiments  the derivative of the likelihood is 
l  



 

 
 x
log p  x   
p  x   
p  x  y 

p  x  
p  x  
y
x  
x p  x  y  

 
p  x  y   
log p  x  y 
p  x  
p  x  
y
y
x

 
p  y x  log p  x  y  

 

    
    
    

y

which is exactly the same as the derivative of the m step  here in equation    we apply the chain
rule to take the derivative of log p  x   while in equation    we apply the chain rule in the reverse
direction  the biggest difference between the em procedure and direct gradient is that for em
we fix the counts on the e step and optimize the me model using those counts  when directly
optimizing the likelihood we need to recompute the counts for each parameter setting  which can be
expensive  appendix a gives a more detailed discussion of both methods 
    controlling tag ambiguity with pr
one problem with unsupervised hmm pos tagging is that the maximum likelihood objective may
encourage tag distributions that allow many different tags for a word in a given context  we do not
   

fic ontrolling c omplexity in part  of  s peech i nduction

  

 

supervised
hmm
hmm me
hmm sp
hmm me sp

 
l l

 
l l

  

supervised
hmm
hmm me
hmm sp
hmm me sp

 
 

 
 
 

 

 
 

                                       

 

                                        

rank of word by l l

rank of word by l l

figure    our ambiguity measure          for each word type in two corpora for a supervised
model  em training  hmm  hmm me   and when we train with an ambiguity penalty
as described in section      hmm sp  hmm me sp   left en  right pt 

find that in actual text with linguist designed tags  because tags are designed to be informative about
the words grammatical role  in the following paragraphs we describe a measure of tag ambiguity
proposed by graa et al         that we will attempt to control  it is easier to understand this measure
with hard tag assignments  so we start with that and thene extend the discussion to distributions over
tags 
consider a word such as stock  intuitively  we would like all occurrences of stock to be
tagged with a small subset of all possible tags  noun and verb  in this case   for a hard assignment of
tags to the entire corpus  y  we could count how many different tags are used in y for occurrences
of the word stock 
if instead of a single tagging of the corpus  we have a distribution q y  over assignments  we
need to generalize this ambiguity measure  instead of asking was a particular tag ever used for
the word stock  we would ask what is the maximum probability with which a particular tag was
used for the word stock  then instead of counting the number of tags  we would sum these
probabilities 
as motivation  figure   shows the distribution of tag ambiguity across words for two corpora 
as we see from figure    when we train using the em procedure described in section      the
hmm and me models grossly overestimates the tag ambiguity of almost all words  however
when the same models are trained using pr to penalize the tag ambiguity  both models  hmm sp 
hmm me sp  achieve a tag ambiguity closer to the truth 
more formally  graa et al         define this measure in terms of constraint features  x  y  
constraint feature wvj  x  y  takes on value   if the j th occurrence of word type w in x is assigned
to tag v in the tag assignment y  consequently  the probability that the j th occurrence of word w
has tag v under the label distribution q y  is eq  wvj  x  y    the ambiguity measurement for
word type w becomes 
x
ambiguity penalty for word type w 
max eq y   wvj  x  y    
    
v

j

this sum of maxima is also called the       mixed norm  for brevity we use the norm notation   eq  w         for computational reasons  we do not add a penalty term based on the ambiguity of the model distribution p  y x   but instead introduce an auxiliary distribution q y  which
   

fig raa   g anchev  c oheur   p ereira     taskar

must be close to p but also must have low ambiguity  our modified objective becomes
x
  eq  w  x  y        
max l    kl q y   p  y x    
 q

    

w

graa et al         optimize this objective using an algorithm very similar to em  the added complexity of implementing their algorithm lies only in computing the kullback leibler projection in
a modified e step  however  this computation involves choosing a distribution over exponentially
many objects  label assignments   luckily  graa et al         show that the dual formulation for
the e step is more manageable  this is given by 
 
x
x
max  log
p  y x  exp    x  y  
s  t 
wvj  
    
 

j

y

where  is the vector of dual parameters wvj   one for each wvj   the projected distribution is then
given by  q y   p  y x  exp     x  y    note that when p is given by an hmm  q for
each sentence can be expressed as
q yn   

i
y

n
pt  yin   yi 
 qo  xni   yin   

    

i  

where qo  xi  yi     po  xi  yi   exp xi yi j   act as modified  unnormalized  emission probabilities 
the objective of equation    is just the negative sum of the log probabilities of all the sentences
under q plus a constant  we can compute this by running forward backward on the corpus  similar
to the e step in normal em  the gradient of the objective is also computed using the forwardbackward algorithm  note that the objective in eq     is concave with respect to  and can be
optimized using a variety of methods  we perform the dual optimization by projected gradient 
using the fast simplex projection algorithm for  described by bertsekas  homer  logan  and patek
        in our experiments we found that taking a few projected gradient steps was not enough  and
performing the optimization until convergence helps the results 

   related work
pos tags place words into classes that share some commonalities as to what other  classes of  words
they cooccur with  therefore  it is natural to ask whether word clustering methods based on word
context distributions might be able to recover the word classification inherent in a pos tag set 
several influential methods  most notably mutual information clustering  brown et al          have
been used to cluster words according to how their immediately contiguous words are distributed 
although those methods were not explicitly designed for pos induction  the resulting clusters capture some syntactic information  see also martin  liermann    ney        for a different method
with a similar objective   clark        refined the distributional clustering approach by adding
morphological and word frequency information  to obtain clusters that more closely resemble pos
tags 
other forms of distributional clustering go beyond the immediate neighbors of a word to represent a whole vector of coocurrences with the target word within a text window  and compare those
vectors using some suitable metric  such as cosine similarity  however  these wider range similarities have problems in capturing more local regularities  for instance  an adjective and a noun might
   

fic ontrolling c omplexity in part  of  s peech i nduction

look similar if the noun tends to be used in noun noun compounds  similarly  two adjectives with
different semantics or selectional preferences might be used with different contexts  moreover  this
problem is aggravated with data sparsity  as an example  infrequent adjectives that modify different nouns tend to have completely disjoint context vectors  but even frequent words like a and
an might have completely different context vectors  since these articles are used in disjoint right
contexts   to alleviate these problems  schtze        used frequency cutoffs  singular value decomposition of co occurrence matrices  and approximate co clustering through two stages of svd 
with the clusters from the first stage used instead of individual words to provide vector representations for the second stage clustering 
lamar  maron and johnson        have recently revised the two stage svd model of schtze
       and achieve close to state of the art performance  the revisions are relatively small  but
touch several important aspects of the model  singular vectors are scaled by their singular values
to preserve the geometry of the original space  latent descriptors are normalized to unit length  and
cluster centroids are computed as a weighted average of their constituent vectors based on the word
frequency  so that rare and common words are treated differently and centroids are initialized in a
deterministic manner 
a final class of approaches  which include the work in this paper  uses a sequence model 
such as an hmm  to represent the probabilistic dependencies between consecutive tags  in these
approaches  each observation corresponds to a particular word and each hidden state corresponds
to a cluster  however  as noted by clark        and johnson         using maximum likelihood
training for such models does not achieve good results  maximum likelihood training tends to result
in very ambiguous distributions for common words  in contradiction with the rather sparse wordtag distribution  several approaches have been proposed to mitigate this problem  freitag       
clusters the most frequent words using a distributional approach and co clustering  to cluster the
remaining  infrequent  words  the author trains a second order hmm where the emission probabilities for the frequent words are fixed to the clusters found earlier and emission probabilities for the
remaining words are uniform 
several studies propose using bayesian inference with an improper dirichlet prior to favor
sparse model parameters and hence indirectly reduce tag ambiguity  johnson        gao   johnson        goldwater   griffiths         this was further refined by moon  erk  and baldridge
       by representing explicitly the different ambiguity patterns of function and content words 
lee  haghighi  and barzilay        take a more direct approach to reducing tag ambiguity by explicitly modeling the set of possible tags for each word type  their model first generates a tag
dictionary that assigns mass to only one tag for each word type to reflect lexicon sparsity  this dictionary is then used to constrain a dirichlet prior from which the emission probabilities are drawn
by only having support for word tag pairs in the dictionary  then a token level hmm using those
emission parameters and transition parameters draw from a symmetric dirichlet prior are used for
tagging the entire corpus  the authors also show improvements by using morphological features
when creating the dictionary  their system achieves state of art results for several languages  it
should be noted that a common issue with the above sparsity inducing approaches is that sparsity
is imposed at the parameter level  the probability of word given tag  while the desired sparsity is
at the posterior level  the probability of tag given word  graa et al         use the pr framework
to penalize ambiguous posteriors distributions of words given tokens  which achieves better results
than the bayesian sparsifying dirichlet priors 

   

fig raa   g anchev  c oheur   p ereira     taskar

most recently  berg kirkpatrick et al         and graa        proposed replacing the multinomial distributions of the hmm by maximum entropy  me  distributions  this allows the use of features to capture morphological information  and achieve very promising results  berg kirkpatrick
et al         also find that optimizing the likelihood with l bfgs rather than em leads to substantial improvements  which we show not to be the case beyond english 
we also note briefly pos induction methods that rely on a prior tag dictionary indicating for
each word type what pos tags it can have  the pos induction task is then  for each word token in
the corpus  to disambiguate between the possible pos tags  as described by merialdo         unfortunately  the availability of a large manually constructed tag dictionary is unrealistic and much
of the later work tries to reduce the required dictionary size in different ways  by generalizing from
a small dictionary with only a handful of entries  smith   eisner        haghighi   klein       
toutanova   johnson        goldwater   griffiths         however  although this approach greatly
simplifies the problem  most words can only have one tag and  furthermore  the cluster tag mappings are predetermined  thus removing an extra level of ambiguity  the accuracy of such methods
is still significantly behind supervised methods  to address the remaining ambiguity by imposing
additional sparsity  ravi and knight        minimize the number of possible tag tag transitions in
the hmm via a integer program  finally  snyder  naseem  eisenstein  and barzilay        jointly
train a pos induction system over parallel corpora in several languages  exploiting the fact that
different languages present different ambiguities 

   experiments
in this section we present encouraging results validating the proposed method in six different testing
scenarios according to different metrics  the highlights are 
 a maximum entropy emission model with a markov transition model trained with the ambiguity penalty improves over the regular hmm in all cases with an average improvement of
       according to the   many metric  
 when compared against a broad range of recent pos induction systems  our method produces
the best results for all languages except english  furthermore  the method seems less sensitive
to particular test conditions than previous methods 
 the induced clusters are useful features in training supervised pos taggers  improving test
accuracy as much or more than the clusters learned by competing methods 
    corpora
in our experiments we test several pos induction methods on five languages with the help of manually pos tagged corpora for those languages  table   summarizes characteristics of the test corpora 
the wall street journal portion of the penn treebank  marcus et al          we consider both the
   tag version of smith   eisner        en    and the    tag version  en      the bosque subset
of the portuguese floresta sinta c tica treebank  afonso  bick  haber    santos         pt   the
bulgarian bultreebank  simov et al          bg   with only the    coarse tags   the spanish corpus from the cast lb treebank  civit   mart         es   and the danish dependency treebank
 ddt   kromann  matthias t          dk  

   

fic ontrolling c omplexity in part  of  s peech i nduction

en
pt
bg
es
dk

 
sentences
     
    
     
    
    

 
types
     
     
     
     
     

 
lunk
      
      
      
      
      

 
tokens
       
      
      
     
     

 
tags
       
  
  
  
  

 
avg       
           
    
    
    
    

 
total      
               
      
      
     
     

 
 w  
     
     
     
     
     

 
 w  
    
    
    
   
   

table    corpus statistics  the third column shows the percentage of word types after lower casing
and eliminating word types occurring only once  the sixth and seventh columns show
information about the word ambiguity in each corpus on average and in totality  corresponding to the penalty in equation      the eighth and ninth columns show the number
of parameters for the different feature sets  as described in section     

    experimental setup
we compare our work with two kinds of methods  those that induce a single cluster for each word
type  type level tagging   and those that allow different tags on different occurrences of a word type
 token level tagging   for type level tagging  we use two standard baselines  b rown and c lark 
as described by brown et al          and clark           following headden  mcclosky  and charniak         we trained the c lark system with both   and    hidden states for the letter hmm and
ran it for    iterations  the b rown system was run according with the instructions accompanying
the code  we also ran the recently proposed ldc system  lamar  maron    bienenstock          
with the configuration described in their paper for ptb   and ptb    and the ptb   configuration
for the other corpora  it should be noted that we did not carry out our experiments with the svd 
system  lamar  maron and johnson         since svd  is superseded by ldc according to its
authors 
for token level tagging  we experimented with the feature rich hmm as presented by bergkirkpatrick et al          trained both using em training  bk em  and direct gradient  bk dg  
using the configuration provided by the authors    we report results from the type level hmm
 tlhmm   lee et al         when applicable  since we were not able to run that system  moreover 
we compared those systems against our own implementation of various hmm based approaches 
the hmm with a multinomial emission probabilities  section       the hmm with maximumentropy emission probabilities  section      trained with em  hmm me   trained by direct gradient  hmm me dg   and trained using pr with the ambiguity penalty  as described in section    
 hmm sp for multinomial emissions  and hmm me sp for maximum entropy emissions   in
addition  we also compared to a multinomial hmm with a sparsifying dirichlet prior on the parameters  hmm vb  trained using variational bayes  johnson        
following standard practice  for the multinomial hmms that do not use morphological information  we lowercase the corpora and replace unique words by a special unknown token  as this
improves the multinomial hmm results by decreasing the number of parameters and eliminating
  
  
  
  

implementation  http   www cs berkeley edu  pliang software brown cluster     zip
implementation  http   www cs rhul ac uk home alexc pos  tar gz
implementation provided by lamar  maron and bienenstock        
implementation provided by berg kirkpatrick et al         

   

fig raa   g anchev  c oheur   p ereira     taskar

very rare words  mostly nouns   since the maximum entropy emission models have access to morphological features  these preprocessing steps do not improve performance and we do not perform
them in that case 
at the start of em  we randomly initialize all of our implementations of hmm based models
from the same posteriors  obtained by running the e step of the hmm model with a set of random
parameters  close to uniform with random uniform jitter of       this means that for each random
seed  the initialization is identical for all models 
for em and variational bayes training  we train the model for     iterations  since we found
that typically most models tend to converge by iteration      for the hmm vb model we fix the
transition prior   to       and test an emission prior  equal to     and        corresponding to
the best values reported by johnson        
for pr training  we initialize with    em iterations and then run for     iterations of pr  following graa et al          we used the results that worked best for english  en     graa et al  
       regularizing only words that occur at least    times  with        and use the same configuration for all the other scnenarios  this setting was not specifically tuned for the test languages  and
might not be optimal for every language  setting such parameters in an unsupervised manner is a
difficult task and we do not address it here  graa       discusses more experiments with different
values of those parameters  
we obtain hard assignments using posterior decoding  where for each position we pick the label
with highest posterior probability  since this showed small but consistent improvements over viterbi
decoding  for all experiments that required random initialization of the parameters we report the
average of   random seeds 
all experiments were run using the number of true tags as the number of clusters  with results
obtained in the test set portion of each corpus  we evaluate all systems using four common metrics
for pos induction    many mapping      mapping  haghighi   klein         variation of information  vi   meila         and validity measure  v   rosenberg   hirschberg         these metrics
are described in detail in appendix b 
    hmm me sp performance
this section compares the gains from using a feature rich representation with those from the ambiguity penalty  as described in section      experiments show that having a feature rich representation always improves performance  and that having an ambiguity penalty also always improves
performance  then  we will see that the improvements from the two methods combine additively 
suggesting that they address independent aspects of pos induction 
we use two different feature sets  the large feature set is that of berg kirkpatrick et al         
while the reduced feature set was described by graa         we apply count based feature selection to both the identity and suffix features  specifically  we only add identity features for words
occurring at least    times and suffix features for words occurring at least    times  we also add a
punctuation feature  in what follows  we refer to the large feature set as feature set   and the reduced
feature set as    the total number of features for each model and language is given in table    the
results of these experiments are summarized in table   
table   shows the results for    training methods across six corpora and four evaluation metrics 
resulting in     experimental conditions  to simplify the discussion  we focus on the   many metric
   the transition prior does not significantly affect the results  and we do not report results with different values 

   

fic ontrolling c omplexity in part  of  s peech i nduction

 
 
 
 
 
 
 
 
 
  

 
 
 
 
 
 
 
 
 
  

hmm
hmm sp
hmm me  prior  
hmm me  prior   
hmm me  prior  
hmm me  prior   
hmm me sp  prior  
hmm me sp  prior   
hmm me sp  prior  
hmm me sp  prior   

en  
    
    
    
    
    
    
    
    
    
    

en  
    
    
    
    
    
    
    
    
    
    

  many
pt bg
         
         
         
         
         
         
         
         
         
         

dk
    
    
    
    
    
    
    
    
    
    

   
es ptb   ptb   pt bg dk es
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        

hmm
hmm sp
hmm me  prior  
hmm me  prior   
hmm me  prior  
hmm me  prior   
hmm me sp  prior  
hmm me sp  prior   
hmm me sp  prior  
hmm me sp  prior   

en  
    
    
    
    
    
    
    
    
    
    

en  
    
    
    
    
    
    
    
    
    
    

vi
pt bg
         
         
         
         
         
         
         
         
         
         

dk
    
    
    
    
    
    
    
    
    
    

v
es ptb   ptb   pt bg dk es
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        

table    results for different hmms  hmm and hmm sp are hmms with multinomial emission
functions trained using em and pr with sparsity constraints  respectively  hmm me and
hmm me spare hmms with a maximum entropy emission model trained using em and
pr with sparsity constraints  for the feature rich models  superscript   represents the large
feature set  and superscript   represents the reduced feature set  prior   and    refers to
the regularization strength for the me emission model  table entries are results averaged
over   runs  bold indicates best system overall 

 top left tab of table     and just observe that the conclusions hold for the other three evaluation
metrics also  from table   we can conclude the following 
 adding a penalty for high word tag ambiguity improves the performance of the multinomial
hmm  the multinomial hmm trained with em  line   in table    is always worse than the
multinomial hmm trained with pr and an ambiguity penalty  by      on average  line   in
table    
 the feature rich maximum entropy hmms  lines     in table    almost always perform better
than the multinomial hmm  this is true for both feature sets and both regularization strengths
used  with an average increase of       the exceptions are possibly due to suboptimal regularization 
 adding a penalty for high word tag ambiguity to the maximum entropy hmm improves performance  in almost all cases  comparing lines     to lines      in table    the sparsity
constraints improve performance  average improvement of        the combined system al   

fig raa   g anchev  c oheur   p ereira     taskar

most always outperforms the multinomial hmm trained using the ambiguity penalty with
an average improvement of       for every corpus the best performance is achieved by the
model with an ambiguity penalty and maximum entropy emission probabilities 
 for every language except english with    tags and a particular feature configuration  reducing the feature set by excluding rare features improves performance on average by       lines
    are better than lines     in table    
 regularizing the maximum entropy model is more important when there are many features
and when we do not have a word tag ambiguity penalty  lines     of table   have the
maximum entropy hmm with many features  and we see that having a tight parameter prior
almost always out performs having a looser prior  by contrast  looking at lines      of table   we see that when we have an ambiguity penalty and fewer features a looser prior is
almost always better than a tighter parameter prior  this was observed also by graa        
it is very encouraging to see that the improvements of using a feature rich model are additive
with the effects of penalizing tag ambiguity  this is especially surprising since we did not optimize the strength of the tag ambiguity penalty for the maximum entropy emission hmm  but rather
used a value reported by graa et al         to work for the multinomial emission hmm  experiments reported by graa        show that tuning this parameter can further improve performance 
nevertheless  both methods regularize the objective in different ways and their interaction should
be accounted for  it would be interesting to use l  regularization on the me models  instead of
l   regularization together with a feature count cutoff  this way the model could learn which features to discard  instead of requiring a predefined parameter that depends on the particular corpus
characteristics 
as reported by berg kirkpatrick et al          the way in which the objective is optimized can
have a big impact on the overall results  however  due to the non convex objective function it
is unclear which optimization method works better and why  we briefly analyze this question in
appendix a and leave it as an open question for future work 
    error analysis
figure   shows the distribution of true tags and clusters for both the hmm model  left  and the
hmm me sp model  right  on the en   corpus  each bar represents a cluster  labeled by the tag
assigned to it after performing the   many mapping  the colors represent the number of words with
the corresponding true tag  to reduce clutter  true tags that were never used to label a cluster are
grouped into others 
we observe that both models split common tags such as nouns into several hidden states  this
splitting accounts for many of the errors in both models  by using   states for nouns instead of
   hmm me sp is able to use more states for adjectives  another improvement comes from a
better grouping of prepositions  for example to is grouped with punctuation by the hmm while
for hmm me sp it is correctly mapped to prepositions  although this should be the correct
behavior  it actually hurts  since the tagset has a special tag to and all occurrences of the word
to are incorrectly assigned  resulting in the loss of      accuracy  in contrast  hmm has a state
mapped to the tag to but the word to comprises only one fifth of that state  the most common
error made by hmm me sp is to include the word the with the second noun induced tag in
figure    right   this induced tag contains mostly capitalized nouns and pronouns  which often
   

fiprep
det

n
adj

rpunc
pos

v
inpunc

conj
to

epunc
others

prep
det

n
adj

rpunc
pos

v
inpunc

conj
to

conj

endpunc

v

inpunc

v

v

adj

rpunc

adj

n

adj

n

n

n

n

det

prep

to

endpunc

conj

v

inpunc

v

pos

n

adj

n

n

n

n

n

n

det

prep

c ontrolling c omplexity in part  of  s peech i nduction

epunc
others

figure    induced tags by the hmm model  left   and by the hmm me sp model  right  on
the en   corpus  each column represents a hidden state  and is labeled by its   many
mapping  unused true tags are grouped into the cluster named others 

     

     

     

     

     

     

     

     

    

    

 

 
art
n
adj

prop
v fin
prp

punc
num
adv

v pcp
v inf
conj c

pron pers
sumothers

art
n
adj

prop
v fin
prp

punc
num
adv

v pcp
v inf
conj c

pron pers
sumothers

figure    induced tags by the hmm model  left   and by the hmm me sp model  right  on the
pt corpus  each column represents a hidden state  and is labeled by its   many mapping 
unused true tags are grouped into the cluster named others 

precede nouns of other induced tags  we suspect that the capitalization feature is the cause of this
error 
the better performance of feature based models on portuguese relative to english may be due
to the ability of features to better represent the richer morphology of portuguese  figure   shows
the induced clusters for portuguese  the hmm me sp model improves over hmm for all tags
except for adjectives  both models have trouble distinguishing nouns from adjectives  the reduced
accuracy for adjectives for hmm me sp is explained by the mapping of a single cluster containing
most of the adjectives to adjectives by the hmm model and to nouns in the hmm me sp model 
removing the noun adjective distinction  as suggested by zhao and marcus         would increase
performance of both models by about     another qualitative difference we observed was that the
hmm me sp model used a single induced cluster for proper nouns rather than spreading them
across different clusters 

   

fig raa   g anchev  c oheur   p ereira     taskar

    state of the art comparison
we now compare our best pos induction system  based on the settings in line    of table     to
other recent systems  results are summarized in table    as we have previously done with table   
we focus the discussion on the   many evaluation metric  as results are qualitatively the same for
the vi and v metrics  while the     metric shows more variance across languages 
  many
pt bg
         
         
         
         
         
         
         
         
         
         
    
              

dk
    
    
    
    
    
    
    
    
    
    
    
    

   
es ptb   ptb   pt bg dk es
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
    
         
         
                        

vi
pt bg
         
         
         
         
         
         
         
         
         
         
         

dk
    
    
    
    
    
    
    
    
    
    
    

v
es ptb   ptb   pt bg dk es
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        
         
                        

 
 
 
 
 
 
 
 
 
  
  
  

en  
b rown
    
c lark 
    
c lark  
    
ldc
    
hmm
    
hmm vb   
    
hmm vb     
    
hmm sp
    
bk em
    
bk dg
    
tlhmm
    
hmm me sp  prior        

en  
    
    
    
    
    
    
    
    
    
    

 
 
 
 
 
 
 
 
 
  
  

en  
b rown
    
c lark 
    
c lark  
    
ldc
    
hmm
    
hmm vb   
    
hmm vb     
    
hmm sp
    
bk em
    
bk dg
    
hmm me sp  prior        

en  
    
    
    
    
    
    
    
    
    
    
    

table    comparing hmm me sp  with several other pos induction systems  all results from
models with random initialization that we run  systems                       represent an
average over   runs  see section     for details and discussion 

lines     in table   show clustering algorithms based on the information gain on various metrics  b rown wins     times  in the scenarios with fewer clusters  over the c lark system  despite
the fact that c lark uses morphology  comparing lines     of table   to line    we see that the
ldc system is particularly strong for en   where it achieves state of the art results  but behaves
worse than the b rown system for every other corpus 
for hmms with multinomial emissions  lines     of table     both maximum likelihood training  hmm  and parameter sparsity  hmm vb  perform worse than adding an ambiguity penalty
 hmm sp   this holds for other evaluation metrics  with the exception of      this confirms previous results by graa et al          comparing the models in lines     to those in lines      we see

   

fic ontrolling c omplexity in part  of  s peech i nduction

that the best hmm  hmm sp  performs comparably with the best clustering  b rown   with one
model winning for   languages and the other for the remaining   
the feature rich hmms  bk em and bk dg  perform very well  achieving results that are
better than hmm sp for   of   tests  even though both optimize the same objective  they achieve
different results on different corpora  we explore the training procedure in more detail in appendix a  comparing also our implementation to that of berg kirkpatrick et al          for brevity 
table   contains only the results from the implementation of berg kirkpatrick et al          our
implementation produces comparable  but not quite identical results 
lines       of table   display the two methods that attempt to control tag ambiguity and have
a feature rich representation to capture morphological information  the results for tlhmm are
taken from lee et al          so we do not report results for the en   and bg corpora  also 
because we were not able to rerun the experiments for tlhmm  we were not able to compute the
information theoretic metrics  consequently  the comparison for tlhmm is slightly less complete
than for the other methods  both tlhmm and hmm me sp perform competitively or better
than the other systems  this is not surprising since they have the ability to model morphological
regularity while also penalizing high ambiguity  comparing tlhmm with hmm me sp  we see
that hmm me sp performs better on the   many metric  in contrast  tlhmm performs better
on      one possible explanation is that the underlying model in tlhmm is a bayesian hmm with
sparsifying dirichlet priors  as noted by graa et al          models trained in this way tend to have
a cluster distribution that more closely resemble the true pos distribution  some clusters with lots
of words and some with few words  which favors the     metric  a description of the particularity
of the     metric is discussed in appendix b  
to summarize  for all non english languages and all metrics except      the hmm me sp
system performs better than all the other systems  for english  bk dg wins for the    tag corpus 
while ldc wins for the    tag corpus  the hmm me sp system is fairly robust  performing well
on all corpora and best on several of them  which allow us to conclude that it is not tuned to any
particular corpus or evaluation metric 
the performance of hmm me sp is tightly related to the performance of the underlying
hmm me system  in appendix a we present a discussion about the performance of different optimization methods for hmm me  we compare our hmm me implementation to that of bk em
and bk dg and show that there are some significant differences in performance  however  its not
clear by the results which one is better  and why it performs better in a given situation 
as mentioned by clark         morphological information is particularly useful for rare words 
table   compares different models accuracy for words according to their frequency  we compare clustering models based on information gain with and without morphological information
 b rown c lark   a distributional information based model  ldc   and the feature rich hmm
with tag ambiguity control  hmm me sp   as expected we see that systems using morphology
do better on rare words  moreover these systems improve over almost all categories except very
common words  words occurring more than    times   comparing hmm me sp against c lark 
we see that even for the condition where c lark overall works better  en     it still performs worse
for rare words than hmm me sp 

   

fig raa   g anchev  c oheur   p ereira     taskar

 
 
   
   
    

b rown
     
     
     
     
     

c lark
     
     
     
     
     

 
 
   
   
    

b rown
     
     
     
     
     

c lark
     
     
     
     
     

 
 
   
   
    

b rown
     
     
     
     
     

c lark
     
     
     
     
     

en  
ldc
     
     
     
     
     
pt
ldc
     
     
     
     
     
es
ldc
     
     
     
     
     

hmm me sp 
     
     
     
     
     

b rown
     
     
     
     
     

c lark
     
     
     
     
     

hmm me sp 
     
     
     
     
     

b rown
     
     
     
     
     

c lark
     
     
     
     
     

hmm me sp 
     
     
     
     
     

b rown
     
     
     
     
     

c lark
     
     
     
     
     

en  
ldc
     
     
     
     
     
bg
ldc
     
     
     
     
     
dk
ldc
     
     
     
     
     

hmm me sp 
     
     
     
     
     
hmm me sp 
     
     
     
     
     
hmm me sp 
     
     
     
     
     

table      many accuracy by word frequency for different corpora 
    using the clusters
as a further comparison of the different pos induction methods  we experiment with a simple
semisupervised scheme where we use the learned clusters as features in a supervised pos tagger 
the basic supervised model has the same features as the hmm me model  except that we use
all word identities and suffixes regardless of frequency  we trained the supervised model using
averaged perceptron for a number of iterations chosen as follows  split the training set into     for
development and     for training and pick the number of iterations  to optimize accuracy on the
development set  finally  trained on the full training set using  iterations and report results on a
    sentence test set 
we augmented the standard features with the learned hidden state for the current token  for
each unsupervised method  b rown c lark ldc  hmm me sp   figure   shows the average
accuracy of the supervised model as we varied the type of unsupervised features  the average is
taken over    random samples for the training set at each training set size  we can see from figure  
that using sem supervised features from any of the models improves performance even if we have
    labeled sentences  moreover  we see that hmm me sp either performs as well or better than
the other models 

   

fic ontrolling c omplexity in part  of  s peech i nduction

ldc
brown
hmm me sp
clark

                   
  training samples

ldc
brown
hmm me sp
clark

  
 
 
 
 
 

                   
  training samples

es

ldc
brown
hmm me sp
clark

                   
  training samples

  
 
 
 
 
 

improvement 

en  

  
 
 
 
 
 

improvement 

bg

improvement 

  
 
 
 
 
 

                   
  training samples

  
 
 
 
 
 

improvement 

ldc
brown
hmm me sp
clark

improvement 

en  

improvement 

  
 
 
 
 
 

pt

ldc
brown
hmm me sp
clark

                   
  training samples

dk

ldc
brown
hmm me sp
clark

                   
  training samples

figure    error reduction from using the induced clusters as features on a semi supervised model
as a function of labeled data size  top left  en    top middle  en    top right  pt 
bottom left  bg  bottom middle  es  bottom right  dk 

   conclusion
in this work we investigated the task of fully unsupervised pos induction in five different languages 
we identified and proposed solutions for three major problems of the simple hidden markov model
that has been used extensively for this task  i  treating words atomically  ignoring orthographic
and morphological information  which we addressed by replacing multinomial word distributions
by small maximum entropy models  ii  an excessive number of parameters that allows models to
fit irrelevant correlations  which we adressed by discarding parameters with small support in the
corpus  iii  a training regime  maximum likelihood  that allows very high word ambiguity  which
we addressed by training using the pr framework with a word ambiguity penalty  we show that all
these solutions improve the model performance and that the improvements are additive  comparing
against the regular hmm we achieve an impressive improvement of       on average 
we also compared our system against the main competing systems and show that our approach
performs better in every language except english  moreover  our approach performs well across
languages and learning conditions  even when hyperparameters are not tuned to the conditions 
when the induced clusters are used as features in a semi supervised pos tagger trained with a small
amount of supervised data  we show significant improvements  moreover  the clusters induced by
our system always perform as well as or better than the clusters produced by other systems 

   

fig raa   g anchev  c oheur   p ereira     taskar

acknowledgments
joo v  graa was supported by a fellowship from fundao para a cincia e tecnologia  sfrh 
bd               and by fct project cmu pt humach           and by fct  inesc id multiannual funding  through the piddac program funds  kuzman ganchev was partially supported by
nsf itr eia          ben taskar was partially supported by the darpa cssg      award and
the onr      young investigator award  lusa coheur was partially supported by fct  inesc id
multiannual funding  through the piddac program funds 

appendix a  unsupervised optimization
berg kirkpatrick et al         describe the feature rich hmm and show that training this model
using direct gradient rather than em can lead to better results  however  they only report results
for the en   corpus  table   compares their implementation of both training regimes  bk em 
bk dg  on the different languages  comparing the two training regimes  we see that there is no
clear winner  bk em wins in   cases  bg en   dk  and loses on the other three 
it is also not clear how to predict which method is more suitable  in a follow up discussion
  the authors propose that the difference arises from when each algorithm starts to fine tune the
weights of rare features relative to when it trains the weights of common features such as short
suffixes  in the case of direct gradient training  at the start of optimization  the weights of common
features change more rapidly because weight gradient is proportional to feature frequency  as
training progresses  more weight is transferred to the rarer features  in contrast  for em training 
the optimization is done to completion on each m step  so even in the first iterations of em where
the counts are mostly random  the rarer features get a lot of the weight mass  this prevents the
model from generalizing  and optimization terminates at a local maximum closer to the starting
point  to allow em to use common features for longer we tried some small experiments where
we initially had very permissive stopping criteria for the m step  after a few em iterations with
permissive stopping criteria  we require stricter stopping criteria  this tended to improve em  but
we did not find a principled method of setting a schedule for the convergence criteria on the m step 
furthermore  these small experiments do not explain why direct gradient is only better than em for
some languages while being worse on others 
a related study  salakhutdinov et al         compares the convergence rate of em and direct
gradient training  and identifies conditions when em achieves newton like behavior  and when it
achieves first order convergence  the conditions are based on the amount of missing information 
which in this case can be approximated by the number of hidden states  potentially  this difference
can also lead to different local maxima  mainly due to the non local nature of the line search procedure of gradient based methods  in fact  looking at the results  dg training seems to work better on
the corpora that have a higher number of hidden states  en    es  and work worse on corpora with
fewer hidden states  bg en    
also in table   we compare our implementation of the hmm me model to the implementation
of berg kirkpatrick et al          using the same conditions  regularization parameter  feature set 
convergence criteria  initialization  and observe significant differences in results  communication
and code comparison revealed small implementation differences  we use a bias feature while they
do not  for the same random seed  our parameters are initialized differently than theirs  we have
   http   www cs berkeley edu  tberg gradvsem main html

   

fic ontrolling c omplexity in part  of  s peech i nduction

en  
bk em     
bk dg     
hmm me     

  many
en   pt
bg dk es
                        
                        
                        

en  
    
    
    

   
en   pt
bg dk
es
                        
                        
                        

en  
bk em     
bk dg     
hmm me     

vi
en   pt
bg dk es
                        
                        
                        

en  
    
    
    

v
en   pt
bg dk
es
                        
                        
                        

table    em vs direct gradient from berg kirkpatrick et al         implementation compared with
our implementaion of em of the hmm with maximum entropy emission probabilities 
the rows starting with bk are for the berkeley implementation  while the rows starting
with me are for our implementation 

different implementations of the optimization algorithm  and a different number of iterations  for
some corpora these differences result in better performance for their implementation  while for
other corpora our implementation gets better results  we leave these details as well as a better
understanding of the differences between each optimization procedure as future work  since this is
not the main focus of the present paper 

appendix b  evaluation metrics
to compare the performance of the different models one needs to evaluate the quality of the induced
clusters  several evaluation metrics for clustering have been proposed in previous work  the metrics
we use to evaluate can be divided into two types  reichart   rappoport         mapping based and
information theoretic  mapping based metrics require a post processing step to map each cluster
to a pos tag and then evaluate accuracy as for supervised pos tagging  information theoretic  it 
metrics compare the induced clusters directly with the true pos tags 
  many mapping and     mapping  haghighi   klein        are two widely used mapping
metrics  in the   many mapping  each hidden state is mapped to the tag with which it cooccurs the
most  this means that several hidden states can be mapped to the same tag  and some tags might not
be used at all  the     mapping greedily assigns each hidden state to a single tag  in the case where
the number of tags and hidden states is the same  this will give a     correspondence  a major
drawback of the latter mapping is that it fails to express all the information of the hidden states 
typically  unsupervised models prefer to explain very frequent tags with several hidden states  and
combine some very rare tags  for example the pt corpus has   tags that occur only once in the
corpus  grouping these together but subdividing nouns still provides a lot of information about
the true tag assignments  however  this would not be captured by the     mapping  this metric
tends to favor systems that produce an exponential distribution on the size of each induced cluster
independent of the clusters true quality  and it does not correlate well with the information theoretic
metrics  graa et al          nevertheless  the   many mapping also has drawbacks  since it can
only distinguish clusters based on their most frequent tag  so  having a cluster split almost evenly

   

fig raa   g anchev  c oheur   p ereira     taskar

between nouns and adjectives  or having a cluster with the same number of nouns  but a mixture of
words with different tags gives the same   many accuracy 
the information theoretic measures we use for evaluation are variation of information  vi 
 meila        and validity measure  v   rosenberg   hirschberg         both are based on the
entropy and conditional entropy of the tags and induced clusters  vi has desirable geometric properties  it is a metric and is convexly additive  meila         however  the range of vi values is
dataset dependent  vi lies in       log n   where n is the number of pos tags  which does not allow
a comparison across datasets with different n   the validity measure  v  is also an entropy based
measure and always lies in the range         but does not satisfy the same geometric properties as
vi  it has been reported to give a high score when a large number of clusters exist  even if these
are of low quality  reichart   rappoport         other information theoretic measures have been
proposed that better handle different numbers of clusters  for instance nvi  reichart   rappoport 
       however  in this work all testing conditions will be on the same corpora with the same number of clusters so that problem does not exist  christodoulopoulos  goldwater  and steedman       
present an extensive comparison between evaluation metrics  in related work maron  lamar  and
bienenstock        present another empirical study about metrics and conclude that the vi metric
can produce results that contradict the true quality of the induced clustering  by giving very high
scores to very simple baseline systems  for instance assigning the same label to all words  they
also point out several problems with the     metric some of which we explained previously  since
metric comparison is not the focus of this work we will compare all methods using the four metrics
described in this section 

references
abeill  a          treebanks  building and using parsed corpora  springer 
afonso  s   bick  e   haber  r     santos  d          floresta sinta c tica  a treebank for portuguese  in proc  lrec  pp           
baum  l   petrie  t   soules  g     weiss  n          a maximization technique occurring in the
statistical analysis of probabilistic functions of markov chains  the annals of mathematical
statistics                
berg kirkpatrick  t   bouchard ct  a   denero  j     klein  d          painless unsupervised
learning with features  in proc  naacl 
bertsekas  d   homer  m   logan  d     patek  s          nonlinear programming  athena scientific 
brown  p  f   desouza  p  v   mercer  r  l   pietra  v  j  d     lai  j  c          class based n gram
models of natural language  computational linguistics             
chen  s          conditional and joint models for grapheme to phoneme conversion  in proc 
ecsct 
christodoulopoulos  c   goldwater  s     steedman  m          two decades of unsupervised pos
induction  how far have we come   in proc  emnlp  cambridge  ma 
civit  m     mart  m          building cast lb  a spanish treebank  research on language  
computation               

   

fic ontrolling c omplexity in part  of  s peech i nduction

clark  a          combining distributional and morphological information for part of speech induction  in proc  eacl 
dempster  a   laird  n     rubin  d          maximum likelihood from incomplete data via the
em algorithm  journal of the royal statistical society  series b  methodological         
freitag  d          toward unsupervised whole corpus tagging  in proc  coling  association for
computational linguistics 
ganchev  k   graa  j   gillenwater  j     taskar  b          posterior regularization for structured
latent variable models  journal of machine learning research               
gao  j     johnson  m          a comparison of bayesian estimators for unsupervised hidden
markov model pos taggers  in in proc  emnlp  pp          honolulu  hawaii  acl 
goldwater  s     griffiths  t          a fully bayesian approach to unsupervised part of speech
tagging  in in proc  acl  vol      p      
graa  j   ganchev  k   pereira  f     taskar  b          parameter vs  posterior sparisty in latent
variable models  in proc  nips 
graa  j   ganchev  k     taskar  b          expectation maximization and posterior constraints 
in in proc  nips  mit press 
graa  j  a  d  a  v          posterior regularization framework  learning tractable models with
intractable constraints  ph d  thesis  universidade tcnica de lisboa  instituto superior
tcnico 
haghighi  a     klein  d          prototype driven learning for sequence models  in proc  htlnaacl  acl 
headden  iii  w  p   mcclosky  d     charniak  e          evaluating unsupervised part of speech
tagging for grammar induction  in proc  coling  pp         
hwa  r   resnik  p   weinberg  a   cabezas  c     kolak  o          bootstrapping parsers via
syntactic projection across parallel texts  special issue of the journal of natural language
engineering on parallel texts                
johnson  m          why doesnt em find good hmm pos taggers  in in proc  emnlp conll 
kromann  matthias t          the danish dependency treebank and the underlying linguistic
theory  in second workshop on treebanks and linguistic theories  tlt   pp          vxj 
sweden 
lamar  m   maron  y     bienenstock  e          latent descriptor clustering for unsupervised pos
induction  in proceedings of the      conference on empirical methods in natural language
processing  pp          cambridge  ma  association for computational linguistics 
lamar  m   maron  y   johnson  m     bienenstock  e          svd and clustering for unsupervised pos tagging  in proceedings of the acl      conference  short papers  pp         
uppsala  sweden  association for computational linguistics 
lee  y  k   haghighi  a     barzilay  r          simple type level unsupervised pos tagging  in
proceedings of the      conference on empirical methods in natural language processing 
pp          cambridge  ma  association for computational linguistics 

   

fig raa   g anchev  c oheur   p ereira     taskar

marcus  m   marcinkiewicz  m     santorini  b          building a large annotated corpus of
english  the penn treebank  computational linguistics                
maron  y   lamar  m     bienenstock  e          evaluation criteria for unsupervised pos induction  tech  rep   indiana university 
martin  s   liermann  j     ney  h          algorithms for bigram and trigram word clustering  in
speech communication  pp           
meila  m          comparing clusteringsan information based distance  j  multivar  anal         
       
merialdo  b          tagging english text with a probabilistic model  computational linguistics 
              
moon  t   erk  k     baldridge  j          crouching dirichlet  hidden markov model  unsupervised pos tagging with context local tag generation  in proc  emnlp  cambridge  ma 
neal  r  m     hinton  g  e          a new view of the em algorithm that justifies incremental 
sparse and other variants  in jordan  m  i   ed    learning in graphical models  pp         
kluwer 
nocedal  j     wright  s  j          numerical optimization  springer 
ratnaparkhi  a          a maximum entropy model for part of speech tagging  in proc  emnlp 
acl 
ravi  s     knight  k          minimized models for unsupervised part of speech tagging  in in
proc  acl 
reichart  r     rappoport  a          the nvi clustering evaluation measure  in proc  conll 
rosenberg  a     hirschberg  j          v measure  a conditional entropy based external cluster
evaluation measure  in emnlp conll  pp         
salakhutdinov  r   roweis  s     ghahramani  z          optimization with em and expectationconjugate gradient  in proc  icml  vol     
schtze  h          distributional part of speech tagging  in proc  eacl  pp         
shen  l   satta  g     joshi  a          guided learning for bidirectional sequence classification  in
proc  acl  prague  czech republic 
simov  k   osenova  p   slavcheva  m   kolkovska  s   balabanova  e   doikoff  d   ivanova  k  
simov  a   simov  e     kouylekov  m          building a linguistically interpreted corpus
of bulgarian  the bultreebank  in proc  lrec 
smith  n     eisner  j          contrastive estimation  training log linear models on unlabeled
data  in proc  acl  acl 
snyder  b   naseem  t   eisenstein  j     barzilay  r          unsupervised multilingual learning for
pos tagging  in proceedings of the conference on empirical methods in natural language
processing  pp            association for computational linguistics 
toutanova  k     johnson  m          a bayesian lda based model for semi supervised part ofspeech tagging  in proc  nips     

   

fic ontrolling c omplexity in part  of  s peech i nduction

toutanova  k   klein  d   manning  c     singer  y          feature rich part of speech tagging
with a cyclic dependency network  in in proc  hlt naacl 
zhao  q     marcus  m          a simple unsupervised learner for pos disambiguation rules given
only a minimal lexicon  in proc  emnlp 

   

fi