journal of artificial intelligence research                

submitted        published      

value of information lattice  exploiting probabilistic
independence for effective feature subset acquisition
mustafa bilgic

mbilgic iit edu

illinois institute of technology
chicago  il       usa

lise getoor

getoor cs umd edu

university of maryland
college park  md       usa

abstract
we address the cost sensitive feature acquisition problem  where misclassifying an instance is costly but the expected misclassification cost can be reduced by acquiring the
values of the missing features  because acquiring the features is costly as well  the objective is to acquire the right set of features so that the sum of the feature acquisition
cost and misclassification cost is minimized  we describe the value of information lattice
 voila   an optimal and efficient feature subset acquisition framework  unlike the common
practice  which is to acquire features greedily  voila can reason with subsets of features 
voila efficiently searches the space of possible feature subsets by discovering and exploiting
conditional independence properties between the features and it reuses probabilistic inference computations to further speed up the process  through empirical evaluation on five
medical datasets  we show that the greedy strategy is often reluctant to acquire features 
as it cannot forecast the benefit of acquiring multiple features in combination 

   introduction
we often need to make decisions and take appropriate actions in a complex and uncertain
world  an important subset of decisions can be formulated as a classification problem 
where an instance is described by a set of features and one of finite categorical options is
chosen based on these features  examples include medical diagnosis where the patients are
described by lab tests and a diagnosis is to be made about the disease state of the patient 
and spam detection where an email is described by its content and the email client needs
to decide whether or not the email is spam 
much research has been done on how to learn effective and efficient classifiers assuming
that the features describing the entities are fully given  even though this complete data
assumption might hold on a few domains  in practice features that describe the entities
often have missing values  in certain domains such as medical diagnosis where a decision is
made based on a number of features that include laboratory test results  the missing feature
values can be acquired at a cost by performing the related tests  in such cases  we need
to decide which tests to perform in which order  the answer to this question  of course 
depends on how important it is to get the correct classification decision  put alternatively 
the cost of an incorrect classification  e g   a misdiagnosis  determines how much we are
willing to spend on expensive tests  thus  we need to devise a feature acquisition policy
that can determine which tests to perform in which order and when to stop and make the
c
    
ai access foundation  all rights reserved 

fibilgic   getoor

final classification decision so that the total incurred cost  the feature acquisition cost and
the expected misclassification cost  is minimized 
devising the optimal policy in general requires considering all possible permutations of
the features and their expected values  to provide some intuition  some features might be
useful only if acquired together  and the cost and benefit of acquiring some features can
depend on which other features have been acquired and what their values turned out to
be  because devising the optimal policy is intractable in general  previous work has been
greedy  gaag   wessels        yang  ling  chai    pan         has approximated value
of information calculations  heckerman  horvitz    middleton         and has developed
heuristic feature scoring techniques  nunez        turney        
the greedy approach  however  has at least two major limitations  first  because it
considers each feature in isolation  it cannot accurately forecast the value of acquiring
multiple features together  causing it to produce sub optimal policies  second  the greedy
strategy assumes that features can be acquired sequentially and the value of a feature can
be observed before acquiring the next one  this assumption  however  is often not very
practical  for example  doctors typically order batches of measurements simultaneously
such as blood count  cholesterol level  etc   and then possibly order another batch after the
results arrive  these two limitations of the greedy approach make it necessary to reason
with sets of features 
reasoning with sets of features  on the other hand  poses serious tractability challenges 
first of all  the number of subsets is exponential in the size of the feature set  second 
judging the value of acquiring a set of features requires taking an expectation over the
possible values of the features in the set  which is also exponential in the number of the
features  the good news  however  is that we do not need to consider all possible subsets of
features in practice  certain features can render other features useless  while some features
are useful only if acquired together  for example  an x ray might render a skin test
useless for diagnosing tuberculosis  similarly  a chest pain alone might not be useful for
differentiating between a cold and a heart disease  it becomes useful only if it is combined
with other features  such as a blood test 
in this article  we describe a data structure that discovers and exploits these types of
constraints  features that render other features useless and features that are useful only
if acquired together  from the underlying probability distribution  we propose value of
information lattice  voila  that reduces the space of all possible subsets by exploiting the
constraints between the features  additionally  voila makes it possible to share value of
information calculations between different feature sets to reduce the computation time 
this article builds upon our earlier work  bilgic   getoor         our contributions in
this article include 
 we introduce two additional techniques for sharing computations between different
subsets of features  these new techniques are based on information caching and
utilizing paths in the underlying bayesian network 
 we experiment with asymmetric misclassification costs in addition to the symmetric
costs  the asymmetric setup reflects a more realistic case and provides new insights 
 in addition to the feature acquisition costs defined by turney         we generate
and experiment with synthetic feature costs  the synthetic feature costs capture
  

fivalue of information lattice

more complex feature acquisition costs and allows for leeway for various acquisition
strategies to differ 
the remainder of this article is organized as follows  we describe the notation and
problem formulation in section    we describe how we can reduce the search space and
share computations using voila in section    we show experimental results in section   
discuss related work in section    and discuss future work in section    we then conclude
in section   

   notation and problem formulation
our main task is to classify a given instance that has missing feature values and incur
the minimum acquisition and misclassification cost  let the instance be described by a set of
features x    x    x            xn   and let y be the random variable representing its class  we
assume that the joint probability distribution p  y  x  is given and concern ourselves with
feature acquisition during only inference  note that the conditional distribution p  y  x  is
not appropriate  as most features are assumed to be unobserved initially   for the purpose
of this article  we assume that we are given a bayesian network  but any joint probabilistic
model that allows us to efficiently answer conditional independence queries can be used 
in the notation  a bold face letter represents a set of random variables and non bold face
letter represents a single random variable  for example x represents the set of features 
whereas xi  x represents a feature in x and y represents the class variable  additionally 
a capital letter represents a random variable  and a lowercase letter represents a particular
value of that variable  this applies to both individual variables and sets of variables  for
example  y represents a variable  and y represents a particular value that y can take 
in addition to the probabilistic model  we are also given the cost models that specify
feature acquisition costs and misclassification costs  formally  we assume that we have a
feature acquisition cost function that given a subset of features  s  and the set of features
whose values are known  evidence  e  returns a non negative real number c s   e   we
also assume that we have a misclassification cost model that returns the misclassification
cost cij incurred when y is assigned yi when the correct assignment is yj   with these cost
functions  we can model non static feature acquisition costs  that is  the cost of acquiring
the feature xi can depend on what has been acquired so far and what their values are
 e  as well what is acquired in conjunction with this feature  s    xi     moreover  the
misclassification cost model does not assume symmetric costs  different kids of errors  false
positives or negatives  can have different costs 
figure   shows a simple example configuration with two features  x  and x    and the
class variable y   in this simple example  the joint distribution p  x  y   is represented as
a table  the feature costs are simple independent costs for x  and x    and the misclassification cost is symmetric where both types of misclassifications cost the same and correct
classification does not cost anything 
a diagnostic policy  is a decision tree where each node represents a feature and the
branches from the nodes represent the possible values of the features  each path of the
policy  ps    represents an ordered sequence feature values s  we will often use ps to
represent an ordered version of s  typically  the order of the features in the set will be
important for computing the feature costs  as the cost of a feature can depend on the values
  

fibilgic   getoor

figure    example configuration with two features x  and x  and class variable y   the
table from left to right represent  the joint probability distribution p  x    x    y   
the feature costs  and the misclassification costs 

of previously acquired features  the order of the features will be irrelevant for computing
the probability p  s   an example conditional policy using the example configuration of
figure   is given in figure   
each policy  has two types of costs  the feature acquisition cost and a misclassification
cost  these costs are defined in terms of the costs associated with following the paths in
the policy  we first describe how to compute the feature acquisition cost of a path and then
describe how to compute the associated expected misclassification cost  finally  we show
how to compute the expected total cost of a policy using the total costs associated with
each path 
in the most naive version  the feature cost of a path ps is the sum of the costs of the
features that appear on the path  however  in practice  the cost of a feature can depend on
which features have been acquired so far and the observed values of the acquired features 
for example  performing the treadmill test  asking the patient to run a treadmill and
measure his heart beat  etc   can be riskier if we had ordered a cholesterol test and its
result turned out to be high  putting the patient in high risk for heart disease  to account
for these types of costs  the order of the features in ps matters  and the total feature cost
of a path is the summation of the individual feature costs conditioned on the values of the
features that precede the features in consideration 
f c ps    

n
x

c ps  j    ps      j  

j  

where ps  j  represents the j th feature in ps and ps      j  represents feature values   through
j in ps  
when we reach the end of a path  we need to make a classification decision  in this
case  we simply utilize the bayesian decision theory and choose the decision with minimum
risk  i e   misclassification cost   we find such a decision by using the probabilistic model to
  

fivalue of information lattice

figure    an example conditional policy with features x    x  and class variable y   each
non leaf node represents a feature acquisition  with probability distribution of the
possible values  and the cost of the feature  each path  e g   x    t  x    t   has
an acquisition cost and expected misclassification cost  the policy overall has an
expected total cost etc  which is the sum of total costs of each path  weighted
by the probability of following that path 

compute the probability distribution p  y   ps   and choose the value of y that leads to the
minimum expected cost  note that the order of the features values do not matter in this
case  that is p  y   ps     p  y   s   the expected misclassification of a path  em c ps    is
  

fibilgic   getoor

computed as follows 
em c ps     em c s    min
yi

x

p  y   yj   s   cij

   

yj

the total cost that we incur by following a path of a policy is simply the sum of the feature
and the expected misclassification costs of that path 
t c ps     f c ps     em c ps  
finally  we compute the expected total cost of a policy  using the total costs of the
individual paths ps    each path ps   has a probability of occurrence in real world 
such probability can be easily computed by the generative probability model that we assumed  it is simply p  s   the expected total cost of a policy is then the sum of the total
cost of each path  t c ps    weighted by the probability of following that path  p  s  
et c    

x

p  s t c ps  

   

ps 

the objective of feature acquisition during inference is  given the joint probabilistic
model and the cost models for acquisition and misclassification  find the policy that has the
minimum expected total cost  however  building the optimal decision tree is known to be
np complete  hyafil   rivest         thus  most research have been greedy choosing the
best feature that reduces the misclassification costs the most and has the lowest cost  e g  
gaag   wessels        dittmer   jensen        or have developed heuristic feature scoring
techniques  e g   nunez        tan        
in the greedy strategy  each path of the policy is extended with the feature that reduces
the misclassification cost the most and that has the lowest cost  more specifically  the path
ps is replaced with new paths psx    psx            psxni where x i   x i           xni are the values
i
i
that xi can take and xi is the feature that has the highest benefit  we define the benefit of
a feature xi given a path ps as the reduction in the total cost of the path when the path
is expanded with the possible values of xi   more formally 
benef it xi   ps     t c ps   

n
x

p  xji   s t c psxj  
i

j  

  f c ps     em c s  

n
x



p  xji   s  f c psxj     em c s  xji  
i

j  


  f c ps    

n
x


p  xji   s f c psxj     em c s  
i

j  

n
x

p  xji   s em c s  xji  

j  

  c xi   s    em c s  

p  xji   s em c s  xji  

j  

  f c ps     f c ps     c xi   s     em c s  
n
x

n
x

p  xji   s em c s  xji  

j  

  

fivalue of information lattice

note that  the last two terms are equivalent to the definition of expected value of information  evi   howard        
ev i xi   s    em c s  

n
x

p  xji   s em c s  xji  

   

j  

substituting evi  the definition of benefit becomes very intuitive 
benef it xi   ps     benef it xi   s    ev i xi   s   c xi   s 

   

with this definition  the greedy strategy iteratively finds the feature that has the highest
positive benefit  value cost difference   acquires it  and stops acquisition when there are no
more features with a positive benefit value 
we also note that it is straightforward to define evi and benefit for a set s  of features
just like we did for a single feature  the only difference is that the expectation needs to be
taken over the joint assignments  s    to the features in the set s   
ev i s    s    em c s  

x

p  s    s em c s  s   

   

s 

and 
benef it s    s    ev i s    s   c s    s 

   

there are a few problems with the greedy strategy as we have mentioned
p earlier  first 
it is short sighted  there exist sets s  x such that benef it s   
benef it xi   
xi s

this is easier to see  for example  for the xor function  y   x  xor x    where x  and x 
alone are not useful but they are determinative together  due to this relationship  a greedy
policy is not guaranteed to be optimal  moreover  the greedy policy can prematurely stop
acquisition because no single feature seems to provide positive benefit 
the second problem with the greedy strategy is that we often need to acquire a set of
features simultaneously  for example  a doctor orders a set of lab tests when s he sends the
patient to a lab  such as blood count  cholesterol level  etc  rather than ordering a single test 
waiting for its result and ordering the next one  however  the traditional greedy strategy
cannot handle reasoning with sets of features naturally 
we would like to be able to reason with sets of features for these two reasons  our
objective in this article is  given an existing potentially empty set of already observed
features e and their observed values e  find the set that has the highest benefit 
l x   e    argmax benef it s   e 

   

sx e

there are two problems with this formulation  first  the number of subsets of x   e is
exponential in the size of x   e  and second  for each set s  we need to take an expectation
over the joint assignments to all features in the set  we address these two problems using
a data structure that we describe next 
  

fibilgic   getoor

   value of information lattice  voila 
voila makes reasoning with sets of features tractable by reducing the space of possible
sets and allowing sharing of evi computations between different sets  in this section  we
will first explain how we can reduce the space and then explain techniques for computation
sharing 
    reducing the space of possible sets
in most domains  there are often complex interactions between the features and the
class label  contrary to the naive bayes assumption  features are often not conditionally
independent given the class label  some features are useless once some other features are
already acquired  for example a chest x ray is typically more determinative than a skin
test for tuberculosis  similarly  some features are useless alone unless they are accompanied
with other features  for example  a chest pain alone might be due to a variety of sicknesses 
if it is accompanied with high cholesterol  it could indicate a heart disease  whereas if it is
combined with fever  a cold might be more probable  these types of interactions between
the features allow us to reduce the space of candidate feature sets 
as we have mentioned in the problem formulation  we have assumed that we already have
a joint probabilistic model over the features and the class variable  p  y  x   we will find
these two types of feature interactions by asking probabilistic independence queries using
p  y  x   specifically  we assume that we are given a bayesian network that represents
p  y  x   the bayesian network will allow us to find these types of interactions through
standard d separation algorithms 
definition   a set s  x   e is irreducible with respect to evidence e if xi  s  xi is
not conditionally independent of y given e and s    xi   
given a bayesian network over x and y   it is straightforward to check irreducibility through
d separation  pearl        
proposition   let s  be a maximal irreducible subset of s with respect to e  then  ev i s  
e    ev i s    e  
proof  let s     s   s    if s  is a maximal irreducible set  s   e d separates y and s    
otherwise  we could make s  larger by including the non d separated element s  from s   in
s    thus  we have p  y   e  s    p  y   e  s    s       p  y   e  s     substitution in equations
  and   yields the desired property 
note that under the assumption that c s    e   c s   e  for any s   s  it suffices to
consider only the irreducible sets to find the optimal solution to the objective function in
equation      voila is a data structure that contains only the irreducible feature subsets
of x  with respect to a particular set of evidence e  we next define voila formally 
definition   a voila v is a directed acyclic graph in which there is a node corresponding
to each possible irreducible set of features  and there is a directed edge from a feature set s
to each node that corresponds to a direct  maximal  subset of s  other subset relationships
in the lattice are then defined through the directed paths in v 
  

fivalue of information lattice

 a 

 b 

figure     a  a simple bayesian network illustrating dependencies between attributes and
the class variable   b  the voila corresponding to the network 

figure   a  shows a simple bayesian network and its corresponding voila  with respect
to the empty evidence set  is shown in figure   b   notice that the voila contains only the
irreducible subsets given the bayesian network  for instance  the voila does not contain
sets that include both x  and x  because x  d separates x  from y   we also observe that
the number of irreducible subsets is   in contrast to         possible subsets  moreover 
note that the largest subset size is now   in contrast to    having smaller feature sets sizes
has a dramatic effect on the value of information calculations  in fact  these savings can
make solving the objective function optimally  equation      feasible in practice 
    sharing evi calculations
finding the set s that has the highest benefit  equation    requires computing ev i s 
 equation     however  computing ev i s  requires taking an expectation over all possible
values of the features in s  moreover  searching for the best set among all the irreducible sets
requires us to compute evi for all irreducible sets  to make such computations tractable in
practice  voila allows computation sharing between its nodes  in this article  we describe
three possible ways of sharing computations between the nodes of voila 
  

fibilgic   getoor

      subset relationships
voila exploits the subset relationships between different feature sets in order to avoid
computing evi for some nodes  first of all  if there is a directed path from node s  to s 
in voila  then s   s  and thus ev i s    e   ev i s    e     now assume that there is
a directed path from si to sj and ev i si   e    ev i sj   e   then  all of the nodes on
this path will also have the same evi  thus we do not need to do the computation for those
subsets  an algorithm that makes use of this observation is given in algorithm   
algorithm    efficient evi computation using voila 
input  voila v and current evidence e
output  voila updated with correct evi values
  for all root node s  s
 
value  ev i s   e   ub s   value  lb s   value
 
ub descendants s    value
 
 
 
 
 
 
  

for all leaf node s  s
value  ev i s   e   ub s   value  lb s   value
lb ancestors s    value
for all node s where lb s     ub s 
value  ev i s   e   ub s   value  lb s   value
lb ancestors s    value
ub descendants s    value

it is important to point out that all nodes of voila are irreducible sets  unless there are
totally useless features that do not change p  y   when observed  then we should not have
any two distinct nodes where the evi values are exactly equal  however  this statement is
true only if we do not have any context specific independencies  independencies that hold
only under certain assignments to the variables  in the underlying bayesian network  in our
description and implementation  we used standard d separation at the variable level  one
can imagine going one step further and define the irreducible sets through both the variable
level d separation and context specific independencies 
in order to share computations between different nodes of the lattice  we keep lower
and upper bounds on the evi of a node  the lower bound is determined by the values of
the descendants of the node whereas the upper bound is determined by the values of its
ancestors  first  we initialize these bounds by computing the value of the information at
the boundary of the lattice  i e   the root node s  and the leaf node s   lines         then 
we loop over the nodes whose upper bounds and lower bounds are not equal  line      
computing their values and updating the bounds at their ancestors and descendants  the
algorithm terminates when the upper bounds and lower bounds for all the nodes become
tight  the order in which to choose the nodes in line   so that the number of sets for which
a value is calculated is minimum is still an open question  a possible heuristic is to perform
   a superset has always a higher or equivalent evi  equation      than its subset 
   we do not need to compute evi for all root nodes  it suffices to compute it for the node that corresponds
to the markov blanket of y   this will be explained in more detail in the next section 

  

fivalue of information lattice

a binary search and choose a middle node on a path between two nodes for which the values
have already been calculated 
      information pathways at the underlying bayesian network
the second mechanism that voila uses to share evi computations is through the edges
in the underlying bayesian network  we specifically make use of the following fact 
proposition   for all s  and s    if s  d separates y from s  with respect to e  then
ev i s    e   ev i s    e  
proof  consider s     s  s    because of the subset relationship  we know that ev i s    
e   ev i s    e  and ev i s     e   ev i s    e  
ev i s     e    em c y   e  

x

p  s     e em c y   e  s    

s  

  em c y   e  

xx
s 

  em c y   e  

xx
s 

  em c y   e  

x

p  s    s    e em c y   e  s    s   

s 

p  s    s    e em c y   e  s   

s 

p  s    e em c y   e  s   

s 

  ev i s    e 
 ev i s    e 
the third line follows from the second by the fact that s  d separates y from s  and thus
p  y   s    s      p  y   s    
corollary  the markov blanket of y    i e   y s parents  y s children  and y s childrens
other parents   is the set that has the highest evi in our search space  as it d separates all
of the remaining variables from y   using this corollary  we do not need to compute the evi
for all root nodes in algorithm    we can compute evi for the root node that corresponds
to the markov blanket of y and it serves as the upper bound for the evi of the remaining
root nodes 
these relationships can very well be exploited like we exploited the subset relationships
above  instead of just using the subset relationships  we can use both subset and independence relationships  one simple way to make use of algorithm   without modification
is to add edges between any s  and s  where the independence property holds  an example s  and s  according to our toy network in figure   a  would be s     x    and
s     x     thus  we can add a directed edge from x  to x  in our voila in figure   b 
and algorithm   will work just fine 
      incremental inference
the third and the last mechanism that voila uses for computation sharing is through
caching of probabilities at its nodes  for each candidate set s  v  we need to compute
ev i s   e  which requires computing p  s   e  and em c y   s  e   if we cache the
  

fibilgic   getoor

conditional probabilities at each node of v  then to compute
p  s   e   we find one of its
p
supersets si   s   xi   and then compute p  s   e    xi p  s  xi   xi   e  
computing em c y   s  e  requires computing p  y   s  e   to perform this computation efficiently  we cache the state of the junction tree at each node of the voila  then  we
find a subset  sj   such that s   sj   xj    we compute p  y   s  e  by integrating the
extra evidence to the junction tree at node sj that is used to compute p  y   sj   e  
    constructing voila
efficient construction of voila is not a straightforward task  the brute force approach
would be to enumerate all possible subsets of x   e and for each subset check whether it is
irreducible  however  this brute force approach is clearly impractical  because the number
of nodes in voila is expected to be much fewer than the number of possible subsets of x e 
if we can be smart about which sets we consider for inclusion in v  we can construct it
more efficiently  that is  instead of generating all possible candidates and checking whether
they are irreducible or not  we try to generate only irreducible sets  we first introduce the
notion of a dependency constraint and then explain how we can use dependency constraints
to efficiently construct voila 
definition   a dependency constraint for a feature xi  s with respect to s and e is the
constraint on s  e that ensures a dependency between xi and y exists 
for instance  in our running example  a dependency constraint for x  is x    in other
words  in order for x  to be relevant  x  should not be included in s  e  similarly  the
dependency constraint for x  is x    meaning that x  must be included in se  specifically 
a dependency constraint for a feature xi requires that all xj on the path from y to xi
not to be included in s  e if xj is not part of a v structure  if xj is part of a v structure 
then either xj or one of its descendants must be included in s  e  we refer to these latter
constraints as positivity constraints   the algorithm that uses these ideas to compute the
dependency constraints for each feature is given in algorithm   
algorithm    dependency constraint computation for xi  
input  xi   y
output  dependency constraint for xi   denoted dc xi  
  dc xi    false
  for each undirected path pj between xi and y
 
dcj  xi    true
 
for each xk on the path pj
 
if xk does not a cause a v structure then
 
dcj  xi    dcj  xi    xk
 
else
 
dcj  xi    dcj  xi     xk  descendants xk   
 

dc xi    dc xi    dcj  xi  

  

fivalue of information lattice

these dependency constraints can be used to check whether a set is irreducible or
potentially irreducible  intuitively  a set is potentially irreducible if it is not irreducible but
it is possible to make the set irreducible by adding more features into it  more formally 
definition   a set s  x   e is potentially irreducible with respect to evidence e if 
s is not irreducible but there exists a non empty set of features s   x    e  s  such that
s  s  is irreducible 
potential irreducibility is possible due to the non monotonic nature of d separation  that is 
a feature that is d separated from y can become dependent if we consider it in combination
with other features  for example  in our running example   x    is not irreducible  as x 
is d separated from y   whereas  x    x    is irreducible 
we use the dependency constraints to check whether a set is irreducible or potentially
irreducible  because a set s is irreducible only if a dependency between all of its elements
and y exists  the dependency constraint for the set s is the conjunction of the dependency
constraints of its members  the irreducibility of s can be checked by setting the elements
of s and e to true and setting the remaining elements of x to false and evaluating the
sets dependency constraint  in our running example  the dependency constraint for the set
 x    x    is x   x    assuming e     when we set the members of  x    x    to true  and
set the remaining features  x  and x    to false  x   x  then evaluates to false and thus
this set is not irreducible  this makes sense because given no evidence  x  is independent
of y   so while  x    is a useful feature set to consider for acquisition   x    x    is not 
checking for potential irreducibility is very similar  set the elements of s and e to true
like we did above  then  set the positivity constraints of the members of s to true  finally 
set everything else to false  using the same example above  to check whether  x    x    is
potentially irreducible  set x    true  x    true  also set x    true because it is the
positivity constraint for x    set the remaining features  that is x    to false  evaluating the
constraint x   x  yields to true  showing that  x    x    is potentially irreducible  while
it was not irreducible  
given the definitions of irreducibility and potential irreducibility and the mechanisms to
check for these properties through the notion of dependency constraints  we next describe
the algorithm to construct voila 
voila construction proceeds in a bottom up fashion  beginning with the lowest level 
which initially contains only the empty set and constructs new irreducible feature sets by
adding one feature at a time into the voila structure  algorithm   gives the details of
the algorithm  the algorithm keeps track of the irreducible feature sets is  and the set
of potentially irreducible feature sets ps  when we are done processing feature xij   we
remove from ps any potentially irreducible set that cannot become irreducible because xij
will not be re considered  line     
      analysis of voila construction algorithm
the construction algorithm inserts a node into the voila only if the corresponding set
is irreducible  lines   and     moreover  by keeping track of potentially irreducible sets
 lines       we generate every possible irreducible set that can be generated  thus  voila
contains only and all of the possible irreducible subsets of x 
  

fibilgic   getoor

algorithm    the voila construction algorithm 
input  set of features x and class variable y  
output  the voila data structure v  given e 
  pick an ordering of elements of x   xi    xi            xin
  is      ps  
  for j     to n
 
for each s  is  ps
 
s   s  xij   dc s     dc s   dc xij  
 
if s  is irreducible then
 
is  is   s     add a node corresponding to s  to v
 
else
 
if s  is potentially irreducible then
  
ps  ps   s   
  
  
  
  
  
  
  

remove from ps all sets that are no longer potentially irreducible
max   size of largest s in is  ll    s   s  is and  s    l 
for l     to max   
for each s  ll
for each s   ll  
if s  s  then
add an edge from s  to s to v

the worst case running time of the algorithm is still exponential in the number of
initially unobserved features  x   e  because number of irreducible sets can potentially
be exponential  the running time in practice  though  depends on the structure of the
bayesian network that the voila is based upon and the ordering of the variables in line   
for example  if the bayesian network is naive bayes  then all subsets are irreducible  no
feature d separates any other feature from the class variable   thus  the search space cannot
be reduced at all  however  naive bayes makes extremely strong assumptions which are
unlikely to hold in practice  in fact  as we empirically show in the experiments section on five
real world datasets  features often are not conditionally independent given the class variable 
there are more complex interactions between them and thus the number of irreducible
subsets is substantially smaller than the number of all possible subsets 
the for loop at line   iterates over each irreducible and potentially irreducible sets
that have been generated so far  and the number of potentially irreducible sets generated
depends on the ordering chosen  a good ordering processes features with literals with
positivity constraints in other features dependency constraints earlier  that is  for each
undirected path from y to xi that includes xj in a v structure  a good ordering puts xj
earlier in the ordering than everything between xj and xi   for instance  in our sample
bayesian network in figure   a   we should consider x  earlier than x    we refer to an
ordering as perfect if it satisfies all the positivity constraints  if a perfect ordering is used 
voila construction algorithm never generates a potentially irreducible set  unfortunately  it
  

fivalue of information lattice

is not always possible to find a perfect ordering  a perfect ordering is not possible when two
features have each other as a positivity constraint literal in their dependency constraints 
this case occurs only when there is a loop from y to y that has two or more v structures
 note that even though a bayesian network is a directed acyclic graph  it can still contain
loops  i e   undirected cycles   a perfect ordering was possible in four of the five real world
datasets that we used 
    using voila for feature value acquisition
voila makes searching the space of all possible subsets tractable in practice  using
this flexibility  it is possible to devise several different acquisition policies  we describe two
policies as example policies in this section 
the first acquisition policy aims to capture the practical setting where more than one
feature is acquired at once  the policy can be constructed using voila as follows  each
path ps of the policy   which is initially empty  is repeatedly extended by acquiring the
set s   v that has the best benef it s    s  e   the policy construction ends when no path
can be extended  i e   all candidate sets have non positive benefit values for each path of  
the second acquisition policy adds a look ahead capability to the greedy policy  that
is  rather than repeatedly extending each path ps of policy  with the feature xi that has
the highest benef it xi   s  e   we add a look ahead capability  and first find the set s   v
that has the highest benef it s    s  e   then  instead of acquiring all features in s  all
at once  like we did in the above policy  we find the feature xi  s  that has the highest
benef it xi   s  e  and acquire it to extend ps  

   experiments
we experimented with five real world medical datasets that turney        described
and used in his paper  these datasets are bupa liver disorders  heart disease  hepatitis 
pima indians diabetes  and thyroid disease  which are all available from the uci machine
learning repository  frank   asuncion         the datasets had a varying number of
features ranging from five to     four out of five datasets had binary labels  whereas the
thyroid dataset had three labels 
for each dataset  we first learned a bayesian network that both provides the joint
probability distribution p  y  x  and efficiently answers conditional independence queries
thorough d separation  pearl         we built a voila for each dataset using the learned
bayesian network  we first present statistics on each dataset  such as the number of features
and number of nodes in the voila  and then compare various acquisition policies 
    search space reduction
table   shows aggregate statistics about each dataset  describing the number of features 
the number of all possible subsets  the number of subsets in voila  and the percent reduction
in the search space  as this table shows  the number of irreducible subsets is substantially
fewer than all possible subsets  for the thyroid disease dataset  for example  the number
of possible subsets is over a million whereas the number of irreducible subsets is fewer than
  

fibilgic   getoor

table    aggregate statistics about each dataset  the number of irreducible subsets  i e  
the number of nodes in voila  is substantially fewer than the number of all possible
subsets 
dataset
bupa liver disorders
pima indians diabetes
heart disease
hepatitis
thyroid disease

features

all subsets

nodes in voila

reduction

 
 
  
  
  

  
   
     
       
         

  
   
   
      
      

   
   
   
   
   

thirty thousand  this enormous reduction in the search space makes searching through the
possible sets of features tractable in practice 
    expected total cost comparisons
we compared the expected total costs  equation    of four different acquisition policies
for each dataset  these policies are as follows 
 no acquisition  this policy does not acquire any features  it aims to minimize the
expected misclassification cost based on the prior probability distribution of the class
variable  p  y   
 markov blanket  this policy acquires every relevant feature  regardless of the misclassification costs  the market blanket of y in a bayesian network is defined as y s
parents  children  and its childrens other parents  pearl         intuitively  it is the
minimal set s  x such that y   x   s    s 
 greedy  this policy repeatedly expands each path ps of an initially empty policy 
by acquiring the feature xi that has the highest positive benef it xi   s   equation
    the policy construction ends when no path can be extended with a feature with
a positive benefit value 
 greedy la  this policy adds a look ahead capability to the greedy strategy  this
policy repeatedly expands each path ps of an initially empty policy  by first finding
the set s  that has the highest positive benef it s    s   equation    and then acquiring
the feature xi  s  that has the maximum benef it xi   s   equation     the policy
construction ends when no set with a positive benefit value can be found for any path
of the policy 
the feature costs for each dataset are described in detail by turney         in summary 
each feature can either have an independent cost  or can belong to a group of features  where
the first feature in that group incurs an additional cost  for example  the first feature from
a group of blood measurements incurs the overhead cost of drawing blood from the patient 
the feature costs are based on the data from ontario ministry of health        
  

fivalue of information lattice

table    example misclassification cost matrix  cij   for the symmetric and asymmetric misclassification costs  cij are set in way to achieve a prior expected misclassification
cost of    in the symmetric cost case  choosing the most probable class leads
to em c      whereas  in the asymmetric cost case  the choosing either class is
indifferent and both leads to the same emc of   
actual class

prior probability

pred  class

symm  cost

asymm  cost

y 

p  y            

y 
y 

 
     

 
     

y 

p  y            

y 
y 

     
 

     
 

we observed that most of the features were assigned the same cost  for example  four
out of five features in the bupa liver disorders dataset     out of    features in the hepatitis
dataset  six out of eight features in the diabetes dataset  and    out of    features in the
thyroid disease dataset were assigned the same cost  when the costs are so similar  the
problem is practically equivalent to finding the minimum size decision tree  to provide more
structure into the feature acquisition costs  we also experimented with randomly generated
feature and group costs  for each feature  we randomly generated a cost between   and     
and for each group we generated a cost between     and      we repeated the experiments
with three different seeds for each dataset 
the misclassification costs were not defined in the paper by turney         one reason
could be that it is easier to define the feature costs  but defining the cost of a misclassification can be non trivial  instead  turney tests different acquisition strategies using
various misclassification costs  we follow a similar technique with a slight modification  we
compare the above acquisition policies under both symmetric  cij   cji   and asymmetric
misclassification costs  to be able to judge how the misclassification cost structure affects
feature acquisition  we unify the presentation  and compare different acquisition strategies
under the same a priori expected misclassification costs  as defined in equation      specifically  we compare the acquisition policies under various a priori emc that are achieved by
varying the cij accordingly  we show an example misclassification table for an emc value
of   in table    for the real feature cost case  we varied the emc between   and       and
varied it from   to      for the synthetic feature cost case 
we compare the greedy  greedy la  and markov blanket policies by plotting how
much cost each policy saves with respect to the no acquisition policy  in the x axis
of the plots  we vary a priori expected misclassification cost using the methodology we
described above  we plot the savings on the y axis  for each dataset  we plot four different
scenarios  the cross product of  symmetric  asymmetric  misclassification costs  and  real 
synthetic  feature costs 
the results for the liver disorders  diabetes  heart disease  hepatitis  and thyroid
disease are given in figures             and   respectively  for each figure  symmetric
misclassification cost scenarios are given in sub figures  a  and  c   whereas the asymmetric
  

fibilgic   getoor

 a 

 b 

 c 

 d 

figure    expected total cost  etc   comparisons for the bupa liver disorders dataset 
the a priori class distribution is as follows  p  y                      

misclassification cost scenarios are presented in  b  and  d   similarly  the real feature cost
scenarios are given in  a  and  b  and the synthetic feature cost scenarios are presented in
 c  and  d   we next summarize the results 
 we found that the greedy policy often prematurely stopped acquisition  performing
even worse than the markov blanket strategy  this is true for most of the datasets 
regardless of the feature and misclassification cost structures  the fact that the greedy
strategy can perform worse than markov blanket strategy is really troubling  at first 
it might seem rather unintuitive that greedy strategy can perform worse than markov
blanket strategy  part of the reason is that the features belong to groups and the first
feature from its group incurs an overhead cost  in greedy strategy where each feature
is considered in isolation  the overhead costs can outweigh each single features benefit 
and because greedy does not look ahead  it is reluctant to commit to acquiring the
first feature from any group 
  

fivalue of information lattice

 a 

 b 

 c 

 d 

figure    expected total cost  etc   comparisons for the pima indian diabetes dataset 
a priori class distribution is as follows  p  y                      

 greedy la strategy never performs worse than any other strategy under any setting 
 the misclassification cost structure  symmetric or asymmetric  had a considerable
effect on how the policies behaved  the differences between symmetric and asymmetric cases were particularly evident for datasets where the class distribution was more
imbalanced  such as the diabetes  figure     hepatitis  figure     and the thyroid
disease  figure    datasets  the differences due to the misclassification cost structure
can be summarized as follows 
 when the class distribution is imbalanced and the misclassification cost is symmetric  acquiring more information cannot change the classification decisions
easily due to the class imbalance  thus the features do not have high evi values 
on the other hand  if the misclassification costs are asymmetric  features tend
to have higher evi values  thus  the greedy and greedy la strategies start
acquiring features earlier in the x axis for the asymmetric cases compared to
  

fibilgic   getoor

 a 

 b 

 c 

 d 

figure    expected total cost  etc   comparisons for the heart disease dataset  a priori
class distribution is as follows  p  y                      

their symmetric counterparts  for example  for the thyroid disease dataset with
real feature costs  the greedy strategy starts acquisition only when the emc is
greater than     for symmetric misclassification costs  figure   a   whereas it
starts acquiring when the emc reaches only     for the asymmetric case  figure   b    for the synthetic feature costs  the results are more dramatic  neither
greedy or greedy la acquires any features for the symmetric cost case  figure   c    whereas they start acquisition when em c       for the asymmetric
case  figure   d   
 in the same realm with the above results  the slope of the savings for the asymmetric case is much higher compared to the symmetric case 
 the misclassification cost structure causes differences between the greedy and
greedy la policies in a few cases  for the diabetes dataset greedy policy performs worse when the misclassification costs are symmetric  figures   a  and
  

fivalue of information lattice

 a 

 b 

 c 

 d 

figure    expected total cost  etc   comparisons for the hepatitis dataset  a priori class
distribution is as follows  p  y                      

  c    whereas for the hepatitis dataset  it performs worse for the asymmetric
misclassification costs  figures   b  and   d   
 the greedy policy sometimes has an erratic  unpredictable  and unreliable performance as the expected misclassification changes  it possibly hits a local minima  gets
out of it later  and hits local minima again  figures   and   d   
we finally present an aggregate summary of the results in table    table   shows how
much the greedy policy and the greedy la policy saves over the markov blanket policy 
the results are presented as the average saving over various intervals  such as          as
this table also shows  the greedy la policy never loses compared to the markov blanket
policy  as one would expect  additionally  the greedy la policy wins over the greedy
policy for most of the cases  and it never looses  finally  greedy policy prematurely stops
acquisition  having negative savings with respect to the markov blanket strategy 
  

fibilgic   getoor

 a 

 b 

 c 

 d 

figure    expected total cost  etc   comparisons for the thyroid disease dataset  a
priori class distribution is as follows  p  y                              

   related work
decision theoretic value of information calculations provide a principled methodology
for information gathering in general  howard        lindley         influence diagrams 
for example  are popular tools for representing decisions and utility functions  howard  
matheson         however  because devising the optimal acquisition policy  i e   constructing the optimal decision tree  is intractable in general  most of the approaches to feature
acquisition have been myopic  dittmer   jensen         greedily acquiring one feature at
a time  the greedy approaches typically differ in i  the problem setup they assume  ii 
the way the features are scored  and iii  the classification model being learned  we review
existing work here  highlighting the differences between different techniques in these three
dimensions 
gaag and wessels        consider the problem of evidence gathering for diagnosis
using a bayesian network  in their setup  they gather evidence  i e   observe the values of
the variables  until the hypothesis is confirmed or disconfirmed to a desired extent  they
  

fivalue of information lattice

table    savings of greedy  gr  and greedy la  la  with respect to the markov blanket
policy  averaged over different intervals  an entry is in bold if it is worse than
greedy la  and it is in red if it is worse than markov blanket 

liver
gr

la

diabetes
gr
la

heart
gr

la

hepatitis
gr
la

thyroid
gr
la

real feature costs   symmetric misclassification costs
       
          
           
           

    
      
      
      

    
    
    
    

     
      
      
      

     
     
     
     

      
      
     
      

      
      
      
      

    
     
      
      

    
    
    
    

     
     
     
     

     
     
     
     

    
    
    
    

    
    
    
    

    
    
    
    

      
      
     
     
     
     
     
     

      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      

      
      
      
      
     
     
    
      

      
      
      
      
     
     
     
    

real feature costs   asymmetric misclassification costs
       
          
           
           

    
      
      
      

   
    
    
    

     
    
    
     

     
     
    
    

      
      
     
      

      
     
      
      

     
      
      
       

synthetic feature costs   symmetric misclassification costs
       
          
           
           
           
           
           
           

      
      
     
     
     
      
      
      

      
      
     
     
     
     
     
     

      
      
      
      
      
     
     
     

      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      

      
     
     
     
     
     
     
     

synthetic feature costs   asymmetric misclassification costs
       
          
           
           
           
           
           
           

      
      
     
     
     
     
      
      

      
      
     
     
     
     
     
     

      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      

      
     
      
      
      
       
       
       

      
     
     
     
     
     
     
     

propose an acquisition algorithm that greedily computes the expected utility of acquiring a
feature and chooses the one with the highest utility  they define the utility as the absolute
value of the change in the probability distribution of the hypothesis being tested 
in more recent work  sent and gaag        consider the problem of acquiring more than
a single feature at each step  they define subgoals and cluster the features for each subgoal 
the subgoals and clustering of the features are provided by the domain experts  then  they
in the non myopic case  they pick a cluster by calculating their expected values  however 
  

fibilgic   getoor

because clusters can be big  calculating the expected value of a cluster can be problematic 
thus  they also provide a semi myopic algorithm where they pick the cluster that has the
best  myopic  feature 
nunez        introduces a decision tree algorithm called eg  that is sensitive to the
feature costs  rather than splitting the decision tree at a feature that has high information
gain  eg  chooses a feature that has least information cost function  which is defined as
the ratio of a features cost to its discriminative efficiency  eg  is  however  is not directly
optimized to balance the misclassification cost and feature acquisition cost  rather it is
optimized for     loss while taking the feature costs into account  similarly  tan       
modifies the id  algorithm  quinlan        to account for feature costs  tan considers the
domain where a robot needs to sense  recognize  and act  and the number of features is very
large  for the robot to act efficiently  it needs to trade off accuracy for efficiency 
turney        builds a decision tree called icet  standing for inexpensive classification
with expensive tests  using a genetic search algorithm  grefenstette        and using
nunezs        criteria to build c    decision trees  quinlan         unlike nunez  turney
takes misclassification costs into account  in addition to the feature costs  to evaluate a
given decision tree and looks for a good decision tree using genetic search algorithms 
yang et al         build cost sensitive decision trees and naive bayes classifiers that
take both feature costs and misclassification costs into account  unlike nunez         who
scores features based on information gain and cost ratio  yang et al  score features based on
expected reduction in the total cost  i e   sum of the feature cost and the misclassification
cost  on the training data  by doing so  they take feature costs and misclassification costs
into account directly at learning time 
bayer zubek        formulates the feature acquisition problem as a markov decision
process and provides both greedy and systematic search algorithms to develop diagnostic
policies  bayer zubek takes both feature cost and misclassification costs into account and
automatically finds an acquisition plan that balances the two costs  she introduces an
admissible heuristic for ao  search and describes regularization techniques to reduce overfitting to the training data 
saar tsechansky  melville  and provost        consider active feature acquisition for
classifier induction  specifically  they are given a training data with missing feature values  and a cost matrix that defines the cost of acquiring each feature value  they describe
an incremental algorithm that can select the best feature to acquire iteratively to build a
model that is expected to have high future performance  the utility of acquiring a feature
is estimated in terms of expected performance improvement per unit cost  the two characteristics that make this work different from most of the previous work is that i  the authors
do not assume a fixed budget a priori  rather they build the model incrementally  ii  each
feature can have a different cost for each instance 
finally  greiner  grove  and roth        analyze the sample complexity of dynamic
programming algorithms that performs value iteration to search for the best diagnostic
policies  they analyze the problem of learning the optimal policy  using a variant of the
probably approximately correct  pac  model  they show that the learning can be achieved
efficiently when the active classifier is allowed to perform only  at most  a constant number
of tests and show that learning the optimal policy is often intractable in more general
environments 
  

fivalue of information lattice

   future work
in this article  we have only scratched the surface of incorporating constraints between
features in order to reduce the search space and make reasoning with sets tractable  we
have discovered two types of constraints  features that render other features useless  and
features that are useless without other features  purely from the underlying probability
distribution  we have shown that these automatically discovered constraints helped reduce
the search space dramatically  in practice  it is possible to discover additional types of
constraints that can potentially be used reduce the search space further  for e g   ordering
constraints where certain procedures always precede other procedures   constraints can
also be defined based on observed feature values  for example  a treadmill test might not be
performed for patients of old age  patients can decline certain procedures and medications 
eliciting these constraints from the domain experts and utilizing them to further reduce
the search space is a promising future direction 
most of the existing feature acquisition frameworks  including this one  are a major
simplification of what happens in practice  we have assumed that acquiring the values of the
features does not change the class value or values of other variables  however  in practice 
feature value measurements can have side effects  for example  in medical diagnosis while
certain measurements are non invasive and do not change the status of the patient  others
might include medications that can affect the outcome  similarly  in fault diagnosis and
repair  the purpose is not only to diagnose but it is to repair the fault  so some actions can
in fact repair the fault  in essence changing the class value  taking these extra side effects
into account will make feature acquisition frameworks more realistic 

   conclusion
the typical approach to feature acquisition has been greedy in the past primarily due to
the sheer size of the possible subsets of features  we described a general technique that can
optimally prune the search space by exploiting the conditional independence relationships
between the features and the class variable  we empirically showed that exploiting the conditional independence relationships can substantially reduce the number of possible subsets 
we also introduced a novel data structure called value of information lattice  voila  that
can both efficiently reduce the search space using the conditional independence relationships and also can share probabilistic inference computations between different subsets of
features  by using voila  we were able to add a full look ahead capability to the greedy
acquisition policy  which would not be practical otherwise  we experimentally showed on
five real world medical datasets that the greedy strategy often stopped feature acquisition
prematurely  performing worse than even a policy that acquires all the features 

acknowledgments
we thank the reviewers for their helpful and constructive feedback  this material is
based on work supported by the national science foundation under grant no          
  

fibilgic   getoor

references
bayer zubek  v          learning diagnostic policies from examples by systematic search 
in annual conference on uncertainty in artificial intelligence 
bilgic  m     getoor  l          voila  efficient feature value acquisition for classification 
in aaai conference on artificial intelligence  pp           
dittmer  s     jensen  f          myopic value of information in influence diagrams  in
annual conference on uncertainty in artificial intelligence  pp         
frank  a     asuncion  a          uci machine learning repository  
gaag  l     wessels  m          selective evidence gathering for diagnostic belief networks 
aisb quarterly  pp       
grefenstette  j          optimization of control parameters for genetic algorithms  ieee
transactions on systems  man and cybernetics                 
greiner  r   grove  a  j     roth  d          learning cost sensitive active classifiers 
artificial intelligence                  
heckerman  d   horvitz  e     middleton  b          an approximate nonmyopic computation for value of information  ieee transactions on pattern analysis and machine
intelligence                 
howard  r  a     matheson  j  e          readings on the principles and applications of
decision analysis  chap  influence diagrams  strategic decision group 
howard  r  a          information value theory  ieee transactions on systems science
and cybernetics              
hyafil  l     rivest  r  l          constructing optimal binary decision trees is npcomplete  information processing letters              
lindley  d  v          on a measure of the information provided by an experiment  annals
of mathematical statistics              
nunez  m          the use of background knowledge in decision tree induction  machine
learning                
of health  o  m          schedule of benefits  physician services under the health insurance
act  
pearl  j          probabilistic reasoning in intelligent systems  morgan kaufmann  san
francisco 
quinlan  j  r          induction of decision trees  machine learning               
quinlan  j  r          c     programs for machine learning  morgan kaufmann publishers
inc   san francisco  ca  usa 
saar tsechansky  m   melville  p     provost  f          active feature value acquisition 
management science                 
sent  d     gaag  l  c          enhancing automated test selection in probabilistic networks  in proceedings of the   th conference on artificial intelligence in medicine 
pp         
  

fivalue of information lattice

tan  m          csl  a cost sensitive learning system for sensing and grasping objects  in
ieee international conference on robotics and automation 
turney  p  d          cost sensitive classification  empirical evaluation of a hybrid genetic
decision tree induction algorithm  journal of artificial intelligence research        
    
yang  q   ling  c   chai  x     pan  r          test cost sensitive classification on data
with missing values  ieee transactions on knowledge and data engineering         
       

  

fi