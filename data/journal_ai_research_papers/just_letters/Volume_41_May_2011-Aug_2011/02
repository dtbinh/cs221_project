journal artificial intelligence research                

submitted        published      

value information lattice  exploiting probabilistic
independence effective feature subset acquisition
mustafa bilgic

mbilgic iit edu

illinois institute technology
chicago  il       usa

lise getoor

getoor cs umd edu

university maryland
college park  md       usa

abstract
address cost sensitive feature acquisition problem  misclassifying instance costly expected misclassification cost reduced acquiring
values missing features  acquiring features costly well  objective acquire right set features sum feature acquisition
cost misclassification cost minimized  describe value information lattice
 voila   optimal efficient feature subset acquisition framework  unlike common
practice  acquire features greedily  voila reason subsets features 
voila efficiently searches space possible feature subsets discovering exploiting
conditional independence properties features reuses probabilistic inference computations speed process  empirical evaluation five
medical datasets  show greedy strategy often reluctant acquire features 
cannot forecast benefit acquiring multiple features combination 

   introduction
often need make decisions take appropriate actions complex uncertain
world  important subset decisions formulated classification problem 
instance described set features one finite categorical options
chosen based features  examples include medical diagnosis patients
described lab tests diagnosis made disease state patient 
spam detection email described content email client needs
decide whether email spam 
much research done learn effective efficient classifiers assuming
features describing entities fully given  even though complete data
assumption might hold domains  practice features describe entities
often missing values  certain domains medical diagnosis decision
made based number features include laboratory test results  missing feature
values acquired cost performing related tests  cases  need
decide tests perform order  answer question  course 
depends important get correct classification decision  put alternatively 
cost incorrect classification  e g   misdiagnosis  determines much
willing spend expensive tests  thus  need devise feature acquisition policy
determine tests perform order stop make
c
    
ai access foundation  rights reserved 

fibilgic   getoor

final classification decision total incurred cost  feature acquisition cost
expected misclassification cost  minimized 
devising optimal policy general requires considering possible permutations
features expected values  provide intuition  features might
useful acquired together  cost benefit acquiring features
depend features acquired values turned
be  devising optimal policy intractable general  previous work
greedy  gaag   wessels        yang  ling  chai    pan         approximated value
information calculations  heckerman  horvitz    middleton         developed
heuristic feature scoring techniques  nunez        turney        
greedy approach  however  least two major limitations  first 
considers feature isolation  cannot accurately forecast value acquiring
multiple features together  causing produce sub optimal policies  second  greedy
strategy assumes features acquired sequentially value feature
observed acquiring next one  assumption  however  often
practical  example  doctors typically order batches measurements simultaneously
blood count  cholesterol level  etc   possibly order another batch
results arrive  two limitations greedy approach make necessary reason
sets features 
reasoning sets features  hand  poses serious tractability challenges 
first all  number subsets exponential size feature set  second 
judging value acquiring set features requires taking expectation
possible values features set  exponential number
features  good news  however  need consider possible subsets
features practice  certain features render features useless  features
useful acquired together  example  x ray might render skin test
useless diagnosing tuberculosis  similarly  chest pain alone might useful
differentiating cold heart disease  becomes useful combined
features  blood test 
article  describe data structure discovers exploits types
constraints  features render features useless features useful
acquired together  underlying probability distribution  propose value
information lattice  voila  reduces space possible subsets exploiting
constraints features  additionally  voila makes possible share value
information calculations different feature sets reduce computation time 
article builds upon earlier work  bilgic   getoor         contributions
article include 
introduce two additional techniques sharing computations different
subsets features  new techniques based information caching
utilizing paths underlying bayesian network 
experiment asymmetric misclassification costs addition symmetric
costs  asymmetric setup reflects realistic case provides new insights 
addition feature acquisition costs defined turney         generate
experiment synthetic feature costs  synthetic feature costs capture
  

fivalue information lattice

complex feature acquisition costs allows leeway various acquisition
strategies differ 
remainder article organized follows  describe notation
problem formulation section    describe reduce search space
share computations using voila section    show experimental results section   
discuss related work section    discuss future work section    conclude
section   

   notation problem formulation
main task classify given instance missing feature values incur
minimum acquisition misclassification cost  let instance described set
features x    x    x            xn   let random variable representing class 
assume joint probability distribution p  y  x  given concern
feature acquisition inference  note conditional distribution p  y  x 
appropriate  features assumed unobserved initially   purpose
article  assume given bayesian network  joint probabilistic
model allows us efficiently answer conditional independence queries used 
notation  bold face letter represents set random variables non bold face
letter represents single random variable  example x represents set features 
whereas xi x represents feature x represents class variable  additionally 
capital letter represents random variable  lowercase letter represents particular
value variable  applies individual variables sets variables 
example  represents variable  represents particular value take 
addition probabilistic model  given cost models specify
feature acquisition costs misclassification costs  formally  assume
feature acquisition cost function given subset features  s  set features
whose values known  evidence  e  returns non negative real number c s   e  
assume misclassification cost model returns misclassification
cost cij incurred assigned yi correct assignment yj   cost
functions  model non static feature acquisition costs  is  cost acquiring
feature xi depend acquired far values
 e  well acquired conjunction feature  s    xi     moreover 
misclassification cost model assume symmetric costs  different kids errors  false
positives negatives  different costs 
figure   shows simple example configuration two features  x  x   
class variable   simple example  joint distribution p  x    represented
table  feature costs simple independent costs x  x    misclassification cost symmetric types misclassifications cost correct
classification cost anything 
diagnostic policy decision tree node represents feature
branches nodes represent possible values features  path
policy  ps   represents ordered sequence feature values s  often use ps
represent ordered version s  typically  order features set
important computing feature costs  cost feature depend values
  

fibilgic   getoor

figure    example configuration two features x  x  class variable  
table left right represent  joint probability distribution p  x    x      
feature costs  misclassification costs 

previously acquired features  order features irrelevant computing
probability p  s   example conditional policy using example configuration
figure   given figure   
policy two types costs  feature acquisition cost misclassification
cost  costs defined terms costs associated following paths
policy  first describe compute feature acquisition cost path
describe compute associated expected misclassification cost  finally  show
compute expected total cost policy using total costs associated
path 
naive version  feature cost path ps sum costs
features appear path  however  practice  cost feature depend
features acquired far observed values acquired features 
example  performing treadmill test  asking patient run treadmill
measure heart beat  etc   riskier ordered cholesterol test
result turned high  putting patient high risk heart disease  account
types costs  order features ps matters  total feature cost
path summation individual feature costs conditioned values
features precede features consideration 
f c ps    

n
x

c ps  j    ps      j  

j  

ps  j  represents j th feature ps ps      j  represents feature values  
j ps  
reach end path  need make classification decision 
case  simply utilize bayesian decision theory choose decision minimum
risk  i e   misclassification cost   find decision using probabilistic model
  

fivalue information lattice

figure    example conditional policy features x    x  class variable  
non leaf node represents feature acquisition  probability distribution
possible values  cost feature  path  e g   x    t  x     
acquisition cost expected misclassification cost  policy overall
expected total cost etc  sum total costs path  weighted
probability following path 

compute probability distribution p  y   ps   choose value leads
minimum expected cost  note order features values matter
case  p  y   ps     p  y   s   expected misclassification path  em c ps   
  

fibilgic   getoor

computed follows 
em c ps     em c s    min
yi

x

p  y   yj   s  cij

   

yj

total cost incur following path policy simply sum feature
expected misclassification costs path 
c ps     f c ps     em c ps  
finally  compute expected total cost policy using total costs
individual paths ps   path ps probability occurrence real world 
probability easily computed generative probability model assumed  simply p  s   expected total cost policy sum total
cost path  c ps    weighted probability following path  p  s  
et c    

x

p  s t c ps  

   

ps

objective feature acquisition inference is  given joint probabilistic
model cost models acquisition misclassification  find policy
minimum expected total cost  however  building optimal decision tree known
np complete  hyafil   rivest         thus  research greedy choosing
best feature reduces misclassification costs lowest cost  e g  
gaag   wessels        dittmer   jensen        developed heuristic feature scoring
techniques  e g   nunez        tan        
greedy strategy  path policy extended feature reduces
misclassification cost lowest cost  specifically  path
ps replaced new paths psx    psx            psxni x i   x i           xni values


xi take xi feature highest benefit  define benefit
feature xi given path ps reduction total cost path path
expanded possible values xi   formally 
benef it xi   ps     c ps  

n
x

p  xji   s t c psxj  


j  

  f c ps     em c s 

n
x



p  xji   s  f c psxj     em c s xji  


j  


  f c ps  

n
x


p  xji   s f c psxj     em c s 


j  

n
x

p  xji   s em c s xji  

j  

  c xi   s    em c s 

p  xji   s em c s xji  

j  

  f c ps    f c ps     c xi   s     em c s 
n
x

n
x

p  xji   s em c s xji  

j  

  

fivalue information lattice

note that  last two terms equivalent definition expected value information  evi   howard        
ev i xi   s    em c s 

n
x

p  xji   s em c s xji  

   

j  

substituting evi  definition benefit becomes intuitive 
benef it xi   ps     benef it xi   s    ev i xi   s  c xi   s 

   

definition  greedy strategy iteratively finds feature highest
positive benefit  value cost difference   acquires it  stops acquisition
features positive benefit value 
note straightforward define evi benefit set s  features
single feature  difference expectation needs
taken joint assignments  s    features set s   
ev i s    s    em c s 

x

p  s    s em c s s   

   

s 

and 
benef it s    s    ev i s    s  c s    s 

   

problems greedy strategy mentioned
p earlier  first 
short sighted  exist sets x benef it s   
benef it xi   
xi

easier see  example  xor function    x  xor x    x  x 
alone useful determinative together  due relationship  greedy
policy guaranteed optimal  moreover  greedy policy prematurely stop
acquisition single feature seems provide positive benefit 
second problem greedy strategy often need acquire set
features simultaneously  example  doctor orders set lab tests s he sends
patient lab  blood count  cholesterol level  etc  rather ordering single test 
waiting result ordering next one  however  traditional greedy strategy
cannot handle reasoning sets features naturally 
would able reason sets features two reasons 
objective article is  given existing potentially empty set already observed
features e observed values e  find set highest benefit 
l x   e    argmax benef it s   e 

   

sx e

two problems formulation  first  number subsets x   e
exponential size x   e  second  set s  need take expectation
joint assignments features set  address two problems using
data structure describe next 
  

fibilgic   getoor

   value information lattice  voila 
voila makes reasoning sets features tractable reducing space possible
sets allowing sharing evi computations different sets  section 
first explain reduce space explain techniques computation
sharing 
    reducing space possible sets
domains  often complex interactions features
class label  contrary naive bayes assumption  features often conditionally
independent given class label  features useless features
already acquired  example chest x ray typically determinative skin
test tuberculosis  similarly  features useless alone unless accompanied
features  example  chest pain alone might due variety sicknesses 
accompanied high cholesterol  could indicate heart disease  whereas
combined fever  cold might probable  types interactions
features allow us reduce space candidate feature sets 
mentioned problem formulation  assumed already
joint probabilistic model features class variable  p  y  x   find
two types feature interactions asking probabilistic independence queries using
p  y  x   specifically  assume given bayesian network represents
p  y  x   bayesian network allow us find types interactions
standard d separation algorithms 
definition   set x   e irreducible respect evidence e xi s  xi
conditionally independent given e    xi   
given bayesian network x   straightforward check irreducibility
d separation  pearl        
proposition   let s  maximal irreducible subset respect e  then  ev i s  
e    ev i s    e  
proof  let s       s    s  maximal irreducible set  s  e d separates s    
otherwise  could make s  larger including non d separated element s  s  
s    thus  p  y   e  s    p  y   e  s    s       p  y   e  s     substitution equations
    yields desired property 
note assumption c s    e  c s   e  s  s  suffices
consider irreducible sets find optimal solution objective function
equation      voila data structure contains irreducible feature subsets
x  respect particular set evidence e  next define voila formally 
definition   voila v directed acyclic graph node corresponding
possible irreducible set features  directed edge feature set
node corresponds direct  maximal  subset s  subset relationships
lattice defined directed paths v 
  

fivalue information lattice

 a 

 b 

figure     a  simple bayesian network illustrating dependencies attributes
class variable   b  voila corresponding network 

figure   a  shows simple bayesian network corresponding voila  respect
empty evidence set  shown figure   b   notice voila contains
irreducible subsets given bayesian network  instance  voila contain
sets include x  x  x  d separates x    observe
number irreducible subsets   contrast         possible subsets  moreover 
note largest subset size   contrast    smaller feature sets sizes
dramatic effect value information calculations  fact  savings
make solving objective function optimally  equation      feasible practice 
    sharing evi calculations
finding set highest benefit  equation    requires computing ev i s 
 equation     however  computing ev i s  requires taking expectation possible
values features s  moreover  searching best set among irreducible sets
requires us compute evi irreducible sets  make computations tractable
practice  voila allows computation sharing nodes  article  describe
three possible ways sharing computations nodes voila 
  

fibilgic   getoor

      subset relationships
voila exploits subset relationships different feature sets order avoid
computing evi nodes  first all  directed path node s  s 
voila  s  s  thus ev i s    e  ev i s    e     assume
directed path si sj ev i si   e    ev i sj   e   then  nodes
path evi  thus need computation
subsets  algorithm makes use observation given algorithm   
algorithm    efficient evi computation using voila 
input  voila v current evidence e
output  voila updated correct evi values
  root node s 
 
value ev i s   e   ub s  value  lb s  value
 
ub descendants s   value
 
 
 
 
 
 
  

leaf node s 
value ev i s   e   ub s  value  lb s  value
lb ancestors s   value
node lb s     ub s 
value ev i s   e   ub s  value  lb s  value
lb ancestors s   value
ub descendants s   value

important point nodes voila irreducible sets  unless
totally useless features change p  y   observed 
two distinct nodes evi values exactly equal  however  statement
true context specific independencies  independencies hold
certain assignments variables  underlying bayesian network 
description implementation  used standard d separation variable level  one
imagine going one step define irreducible sets variable
level d separation context specific independencies 
order share computations different nodes lattice  keep lower
upper bounds evi node  lower bound determined values
descendants node whereas upper bound determined values
ancestors  first  initialize bounds computing value information
boundary lattice  i e   root node s  leaf node s   lines         then 
loop nodes whose upper bounds lower bounds equal  line      
computing values updating bounds ancestors descendants 
algorithm terminates upper bounds lower bounds nodes become
tight  order choose nodes line   number sets
value calculated minimum still open question  possible heuristic perform
   superset always higher equivalent evi  equation      subset 
   need compute evi root nodes  suffices compute node corresponds
markov blanket   explained detail next section 

  

fivalue information lattice

binary search choose middle node path two nodes values
already calculated 
      information pathways underlying bayesian network
second mechanism voila uses share evi computations edges
underlying bayesian network  specifically make use following fact 
proposition   s  s    s  d separates s  respect e 
ev i s    e  ev i s    e  
proof  consider s     s  s    subset relationship  know ev i s    
e  ev i s    e  ev i s     e  ev i s    e  
ev i s     e    em c y   e 

x

p  s     e em c y   e  s    

s  

  em c y   e 

xx
s 

  em c y   e 

xx
s 

  em c y   e 

x

p  s    s    e em c y   e  s    s   

s 

p  s    s    e em c y   e  s   

s 

p  s    e em c y   e  s   

s 

  ev i s    e 
ev i s    e 
third line follows second fact s  d separates s  thus
p  y   s    s      p  y   s    
corollary  markov blanket    i e   parents  children  childrens
parents   set highest evi search space  d separates
remaining variables   using corollary  need compute evi
root nodes algorithm    compute evi root node corresponds
markov blanket serves upper bound evi remaining
root nodes 
relationships well exploited exploited subset relationships
above  instead using subset relationships  use subset independence relationships  one simple way make use algorithm   without modification
add edges s  s  independence property holds  example s  s  according toy network figure   a  would s     x   
s     x     thus  add directed edge x  x  voila figure   b 
algorithm   work fine 
      incremental inference
third last mechanism voila uses computation sharing
caching probabilities nodes  candidate set v  need compute
ev i s   e  requires computing p  s   e  em c y   s  e   cache
  

fibilgic   getoor

conditional probabilities node v  compute
p  s   e   find one
p
supersets si    xi   compute p  s   e    xi p  s  xi   xi   e  
computing em c y   s  e  requires computing p  y   s  e   perform computation efficiently  cache state junction tree node voila  then 
find subset  sj     sj  xj    compute p  y   s  e  integrating
extra evidence junction tree node sj used compute p  y   sj   e  
    constructing voila
efficient construction voila straightforward task  brute force approach
would enumerate possible subsets x   e subset check whether
irreducible  however  brute force approach clearly impractical  number
nodes voila expected much fewer number possible subsets x e 
smart sets consider inclusion v  construct
efficiently  is  instead generating possible candidates checking whether
irreducible not  try generate irreducible sets  first introduce
notion dependency constraint explain use dependency constraints
efficiently construct voila 
definition   dependency constraint feature xi respect e
constraint e ensures dependency xi exists 
instance  running example  dependency constraint x  x   
words  order x  relevant  x  included e  similarly 
dependency constraint x  x    meaning x  must included se  specifically 
dependency constraint feature xi requires xj path xi
included e xj part v structure  xj part v structure 
either xj one descendants must included e  we refer latter
constraints positivity constraints   algorithm uses ideas compute
dependency constraints feature given algorithm   
algorithm    dependency constraint computation xi  
input  xi  
output  dependency constraint xi   denoted dc xi  
  dc xi   false
  undirected path pj xi
 
dcj  xi   true
 
xk path pj
 
xk cause v structure
 
dcj  xi   dcj  xi   xk
 
else
 
dcj  xi   dcj  xi    xk descendants xk   
 

dc xi   dc xi   dcj  xi  

  

fivalue information lattice

dependency constraints used check whether set irreducible
potentially irreducible  intuitively  set potentially irreducible irreducible
possible make set irreducible adding features it  formally 
definition   set x   e potentially irreducible respect evidence e if 
irreducible exists non empty set features s  x    e s 
s  irreducible 
potential irreducibility possible due non monotonic nature d separation  is 
feature d separated become dependent consider combination
features  example  running example   x    irreducible  x 
d separated   whereas  x    x    irreducible 
use dependency constraints check whether set irreducible potentially
irreducible  set irreducible dependency elements
exists  dependency constraint set conjunction dependency
constraints members  irreducibility checked setting elements
e true setting remaining elements x false evaluating
sets dependency constraint  running example  dependency constraint set
 x    x    x  x    assuming e     set members  x    x    true 
set remaining features  x  x    false  x  x  evaluates false thus
set irreducible  makes sense given evidence  x  independent
   x    useful feature set consider acquisition   x    x    not 
checking potential irreducibility similar  set elements e true
above  then  set positivity constraints members true  finally 
set everything else false  using example above  check whether  x    x   
potentially irreducible  set x    true  x    true  set x    true
positivity constraint x    set remaining features  x    false  evaluating
constraint x  x  yields true  showing  x    x    potentially irreducible  while
irreducible  
given definitions irreducibility potential irreducibility mechanisms
check properties notion dependency constraints  next describe
algorithm construct voila 
voila construction proceeds bottom fashion  beginning lowest level 
initially contains empty set constructs new irreducible feature sets
adding one feature time voila structure  algorithm   gives details
algorithm  algorithm keeps track irreducible feature sets is  set
potentially irreducible feature sets ps  done processing feature xij  
remove ps potentially irreducible set cannot become irreducible xij
re considered  line     
      analysis voila construction algorithm
construction algorithm inserts node voila corresponding set
irreducible  lines       moreover  keeping track potentially irreducible sets
 lines       generate every possible irreducible set generated  thus  voila
contains possible irreducible subsets x 
  

fibilgic   getoor

algorithm    voila construction algorithm 
input  set features x class variable  
output  voila data structure v  given e 
  pick ordering elements x   xi    xi            xin
      ps
  j     n
 
ps
 
s  xij   dc s    dc s  dc xij  
 
s  irreducible
 
 s     add node corresponding s  v
 
else
 
s  potentially irreducible
  
ps ps  s   
  
  
  
  
  
  
  

remove ps sets longer potentially irreducible
max   size largest is     s    s    l 
l     max  

s  ll  
s 
add edge s  v

worst case running time algorithm still exponential number
initially unobserved features  x   e  number irreducible sets potentially
exponential  running time practice  though  depends structure
bayesian network voila based upon ordering variables line   
example  bayesian network naive bayes  subsets irreducible  no
feature d separates feature class variable   thus  search space cannot
reduced all  however  naive bayes makes extremely strong assumptions
unlikely hold practice  fact  empirically show experiments section five
real world datasets  features often conditionally independent given class variable 
complex interactions thus number irreducible
subsets substantially smaller number possible subsets 
loop line   iterates irreducible potentially irreducible sets
generated far  number potentially irreducible sets generated
depends ordering chosen  good ordering processes features literals
positivity constraints features dependency constraints earlier  is 
undirected path xi includes xj v structure  good ordering puts xj
earlier ordering everything xj xi   instance  sample
bayesian network figure   a   consider x  earlier x    refer
ordering perfect satisfies positivity constraints  perfect ordering used 
voila construction algorithm never generates potentially irreducible set  unfortunately 
  

fivalue information lattice

always possible find perfect ordering  perfect ordering possible two
features positivity constraint literal dependency constraints 
case occurs loop two v structures
 note even though bayesian network directed acyclic graph  still contain
loops  i e   undirected cycles   perfect ordering possible four five real world
datasets used 
    using voila feature value acquisition
voila makes searching space possible subsets tractable practice  using
flexibility  possible devise several different acquisition policies  describe two
policies example policies section 
first acquisition policy aims capture practical setting one
feature acquired once  policy constructed using voila follows 
path ps policy  which initially empty  repeatedly extended acquiring
set s  v best benef it s    s  e   policy construction ends path
extended  i e   candidate sets non positive benefit values path  
second acquisition policy adds look ahead capability greedy policy 
is  rather repeatedly extending path ps policy feature xi
highest benef it xi   s  e   add look ahead capability  first find set s  v
highest benef it s    s  e   then  instead acquiring features s 
once  policy  find feature xi s  highest
benef it xi   s  e  acquire extend ps  

   experiments
experimented five real world medical datasets turney        described
used paper  datasets bupa liver disorders  heart disease  hepatitis 
pima indians diabetes  thyroid disease  available uci machine
learning repository  frank   asuncion         datasets varying number
features ranging five     four five datasets binary labels  whereas
thyroid dataset three labels 
dataset  first learned bayesian network provides joint
probability distribution p  y  x  efficiently answers conditional independence queries
thorough d separation  pearl         built voila dataset using learned
bayesian network  first present statistics dataset  number features
number nodes voila  compare various acquisition policies 
    search space reduction
table   shows aggregate statistics dataset  describing number features 
number possible subsets  number subsets voila  percent reduction
search space  table shows  number irreducible subsets substantially
fewer possible subsets  thyroid disease dataset  example  number
possible subsets million whereas number irreducible subsets fewer
  

fibilgic   getoor

table    aggregate statistics dataset  number irreducible subsets  i e  
number nodes voila  substantially fewer number possible
subsets 
dataset
bupa liver disorders
pima indians diabetes
heart disease
hepatitis
thyroid disease

features

subsets

nodes voila

reduction

 
 
  
  
  

  
   
     
       
         

  
   
   
      
      

   
   
   
   
   

thirty thousand  enormous reduction search space makes searching
possible sets features tractable practice 
    expected total cost comparisons
compared expected total costs  equation    four different acquisition policies
dataset  policies follows 
acquisition  policy acquire features  aims minimize
expected misclassification cost based prior probability distribution class
variable  p  y   
markov blanket  policy acquires every relevant feature  regardless misclassification costs  market blanket bayesian network defined
parents  children  childrens parents  pearl         intuitively 
minimal set x  x   s    s 
greedy  policy repeatedly expands path ps initially empty policy
acquiring feature xi highest positive benef it xi   s   equation
    policy construction ends path extended feature
positive benefit value 
greedy la  policy adds look ahead capability greedy strategy 
policy repeatedly expands path ps initially empty policy first finding
set s  highest positive benef it s    s   equation    acquiring
feature xi s  maximum benef it xi   s   equation     policy
construction ends set positive benefit value found path
policy 
feature costs dataset described detail turney         summary 
feature either independent cost  belong group features 
first feature group incurs additional cost  example  first feature
group blood measurements incurs overhead cost drawing blood patient 
feature costs based data ontario ministry health        
  

fivalue information lattice

table    example misclassification cost matrix  cij   symmetric asymmetric misclassification costs  cij set way achieve prior expected misclassification
cost    symmetric cost case  choosing probable class leads
em c      whereas  asymmetric cost case  choosing either class
indifferent leads emc   
actual class

prior probability

pred  class

symm  cost

asymm  cost

y 

p  y            

y 
y 

 
     

 
     

y 

p  y            

y 
y 

     
 

     
 

observed features assigned cost  example  four
five features bupa liver disorders dataset        features hepatitis
dataset  six eight features diabetes dataset        features
thyroid disease dataset assigned cost  costs similar 
problem practically equivalent finding minimum size decision tree  provide
structure feature acquisition costs  experimented randomly generated
feature group costs  feature  randomly generated cost       
group generated cost          repeated experiments
three different seeds dataset 
misclassification costs defined paper turney         one reason
could easier define feature costs  defining cost misclassification non trivial  instead  turney tests different acquisition strategies using
various misclassification costs  follow similar technique slight modification 
compare acquisition policies symmetric  cij   cji   asymmetric
misclassification costs  able judge misclassification cost structure affects
feature acquisition  unify presentation  compare different acquisition strategies
priori expected misclassification costs  defined equation      specifically  compare acquisition policies various priori emc achieved
varying cij accordingly  show example misclassification table emc value
  table    real feature cost case  varied emc        
varied        synthetic feature cost case 
compare greedy  greedy la  markov blanket policies plotting
much cost policy saves respect acquisition policy  x axis
plots  vary priori expected misclassification cost using methodology
described above  plot savings axis  dataset  plot four different
scenarios  cross product  symmetric  asymmetric  misclassification costs   real 
synthetic  feature costs 
results liver disorders  diabetes  heart disease  hepatitis  thyroid
disease given figures               respectively  figure  symmetric
misclassification cost scenarios given sub figures  a   c   whereas asymmetric
  

fibilgic   getoor

 a 

 b 

 c 

 d 

figure    expected total cost  etc   comparisons bupa liver disorders dataset 
priori class distribution follows  p  y                      

misclassification cost scenarios presented  b   d   similarly  real feature cost
scenarios given  a   b  synthetic feature cost scenarios presented
 c   d   next summarize results 
found greedy policy often prematurely stopped acquisition  performing
even worse markov blanket strategy  true datasets 
regardless feature misclassification cost structures  fact greedy
strategy perform worse markov blanket strategy really troubling  first 
might seem rather unintuitive greedy strategy perform worse markov
blanket strategy  part reason features belong groups first
feature group incurs overhead cost  greedy strategy feature
considered isolation  overhead costs outweigh single features benefit 
greedy look ahead  reluctant commit acquiring
first feature group 
  

fivalue information lattice

 a 

 b 

 c 

 d 

figure    expected total cost  etc   comparisons pima indian diabetes dataset 
priori class distribution follows  p  y                      

greedy la strategy never performs worse strategy setting 
misclassification cost structure  symmetric asymmetric  considerable
effect policies behaved  differences symmetric asymmetric cases particularly evident datasets class distribution
imbalanced  diabetes  figure     hepatitis  figure     thyroid
disease  figure    datasets  differences due misclassification cost structure
summarized follows 
class distribution imbalanced misclassification cost symmetric  acquiring information cannot change classification decisions
easily due class imbalance  thus features high evi values 
hand  misclassification costs asymmetric  features tend
higher evi values  thus  greedy greedy la strategies start
acquiring features earlier x axis asymmetric cases compared
  

fibilgic   getoor

 a 

 b 

 c 

 d 

figure    expected total cost  etc   comparisons heart disease dataset  priori
class distribution follows  p  y                      

symmetric counterparts  example  thyroid disease dataset
real feature costs  greedy strategy starts acquisition emc
greater     symmetric misclassification costs  figure   a   whereas
starts acquiring emc reaches     asymmetric case  figure   b    synthetic feature costs  results dramatic  neither
greedy greedy la acquires features symmetric cost case  figure   c    whereas start acquisition em c       asymmetric
case  figure   d   
realm results  slope savings asymmetric case much higher compared symmetric case 
misclassification cost structure causes differences greedy
greedy la policies cases  diabetes dataset greedy policy performs worse misclassification costs symmetric  figures   a 
  

fivalue information lattice

 a 

 b 

 c 

 d 

figure    expected total cost  etc   comparisons hepatitis dataset  priori class
distribution follows  p  y                      

  c    whereas hepatitis dataset  performs worse asymmetric
misclassification costs  figures   b    d   
greedy policy sometimes erratic  unpredictable  unreliable performance expected misclassification changes  possibly hits local minima  gets
later  hits local minima  figures     d   
finally present aggregate summary results table    table   shows
much greedy policy greedy la policy saves markov blanket policy 
results presented average saving various intervals          
table shows  greedy la policy never loses compared markov blanket
policy  one would expect  additionally  greedy la policy wins greedy
policy cases  never looses  finally  greedy policy prematurely stops
acquisition  negative savings respect markov blanket strategy 
  

fibilgic   getoor

 a 

 b 

 c 

 d 

figure    expected total cost  etc   comparisons thyroid disease dataset 
priori class distribution follows  p  y                              

   related work
decision theoretic value information calculations provide principled methodology
information gathering general  howard        lindley         influence diagrams 
example  popular tools representing decisions utility functions  howard  
matheson         however  devising optimal acquisition policy  i e   constructing optimal decision tree  intractable general  approaches feature
acquisition myopic  dittmer   jensen         greedily acquiring one feature
time  greedy approaches typically differ i  problem setup assume  ii 
way features scored  iii  classification model learned  review
existing work here  highlighting differences different techniques three
dimensions 
gaag wessels        consider problem evidence gathering diagnosis
using bayesian network  setup  gather evidence  i e   observe values
variables  hypothesis confirmed disconfirmed desired extent 
  

fivalue information lattice

table    savings greedy  gr  greedy la  la  respect markov blanket
policy  averaged different intervals  entry bold worse
greedy la  red worse markov blanket 

liver
gr

la

diabetes
gr
la

heart
gr

la

hepatitis
gr
la

thyroid
gr
la

real feature costs   symmetric misclassification costs
       
          
           
           

    
      
      
      

    
    
    
    

     
      
      
      

     
     
     
     

      
      
     
      

      
      
      
      

    
     
      
      

    
    
    
    

     
     
     
     

     
     
     
     

    
    
    
    

    
    
    
    

    
    
    
    

      
      
     
     
     
     
     
     

      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      

      
      
      
      
     
     
    
      

      
      
      
      
     
     
     
    

real feature costs   asymmetric misclassification costs
       
          
           
           

    
      
      
      

   
    
    
    

     
    
    
     

     
     
    
    

      
      
     
      

      
     
      
      

     
      
      
       

synthetic feature costs   symmetric misclassification costs
       
          
           
           
           
           
           
           

      
      
     
     
     
      
      
      

      
      
     
     
     
     
     
     

      
      
      
      
      
     
     
     

      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      

      
     
     
     
     
     
     
     

synthetic feature costs   asymmetric misclassification costs
       
          
           
           
           
           
           
           

      
      
     
     
     
     
      
      

      
      
     
     
     
     
     
     

      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      

      
     
      
      
      
       
       
       

      
     
     
     
     
     
     
     

propose acquisition algorithm greedily computes expected utility acquiring
feature chooses one highest utility  define utility absolute
value change probability distribution hypothesis tested 
recent work  sent gaag        consider problem acquiring
single feature step  define subgoals cluster features subgoal 
subgoals clustering features provided domain experts  then 
non myopic case  pick cluster calculating expected values  however 
  

fibilgic   getoor

clusters big  calculating expected value cluster problematic 
thus  provide semi myopic algorithm pick cluster
best  myopic  feature 
nunez        introduces decision tree algorithm called eg  sensitive
feature costs  rather splitting decision tree feature high information
gain  eg  chooses feature least information cost function  defined
ratio features cost discriminative efficiency  eg  is  however  directly
optimized balance misclassification cost feature acquisition cost  rather
optimized     loss taking feature costs account  similarly  tan       
modifies id  algorithm  quinlan        account feature costs  tan considers
domain robot needs sense  recognize  act  number features
large  robot act efficiently  needs trade off accuracy efficiency 
turney        builds decision tree called icet  standing inexpensive classification
expensive tests  using genetic search algorithm  grefenstette        using
nunezs        criteria build c    decision trees  quinlan         unlike nunez  turney
takes misclassification costs account  in addition feature costs  evaluate
given decision tree looks good decision tree using genetic search algorithms 
yang et al         build cost sensitive decision trees naive bayes classifiers
take feature costs misclassification costs account  unlike nunez        
scores features based information gain cost ratio  yang et al  score features based
expected reduction total cost  i e   sum feature cost misclassification
cost  training data  so  take feature costs misclassification costs
account directly learning time 
bayer zubek        formulates feature acquisition problem markov decision
process provides greedy systematic search algorithms develop diagnostic
policies  bayer zubek takes feature cost misclassification costs account
automatically finds acquisition plan balances two costs  introduces
admissible heuristic ao  search describes regularization techniques reduce overfitting training data 
saar tsechansky  melville  provost        consider active feature acquisition
classifier induction  specifically  given training data missing feature values  cost matrix defines cost acquiring feature value  describe
incremental algorithm select best feature acquire iteratively build
model expected high future performance  utility acquiring feature
estimated terms expected performance improvement per unit cost  two characteristics make work different previous work i  authors
assume fixed budget priori  rather build model incrementally  ii 
feature different cost instance 
finally  greiner  grove  roth        analyze sample complexity dynamic
programming algorithms performs value iteration search best diagnostic
policies  analyze problem learning optimal policy  using variant
probably approximately correct  pac  model  show learning achieved
efficiently active classifier allowed perform  at most  constant number
tests show learning optimal policy often intractable general
environments 
  

fivalue information lattice

   future work
article  scratched surface incorporating constraints
features order reduce search space make reasoning sets tractable 
discovered two types constraints  features render features useless 
features useless without features  purely underlying probability
distribution  shown automatically discovered constraints helped reduce
search space dramatically  practice  possible discover additional types
constraints potentially used reduce search space  for e g   ordering
constraints certain procedures always precede procedures   constraints
defined based observed feature values  example  treadmill test might
performed patients old age  patients decline certain procedures medications 
eliciting constraints domain experts utilizing reduce
search space promising future direction 
existing feature acquisition frameworks  including one  major
simplification happens practice  assumed acquiring values
features change class value values variables  however  practice 
feature value measurements side effects  example  medical diagnosis
certain measurements non invasive change status patient  others
might include medications affect outcome  similarly  fault diagnosis
repair  purpose diagnose repair fault  actions
fact repair fault  essence changing class value  taking extra side effects
account make feature acquisition frameworks realistic 

   conclusion
typical approach feature acquisition greedy past primarily due
sheer size possible subsets features  described general technique
optimally prune search space exploiting conditional independence relationships
features class variable  empirically showed exploiting conditional independence relationships substantially reduce number possible subsets 
introduced novel data structure called value information lattice  voila 
efficiently reduce search space using conditional independence relationships share probabilistic inference computations different subsets
features  using voila  able add full look ahead capability greedy
acquisition policy  would practical otherwise  experimentally showed
five real world medical datasets greedy strategy often stopped feature acquisition
prematurely  performing worse even policy acquires features 

acknowledgments
thank reviewers helpful constructive feedback  material
based work supported national science foundation grant no          
  

fibilgic   getoor

references
bayer zubek  v          learning diagnostic policies examples systematic search 
annual conference uncertainty artificial intelligence 
bilgic  m     getoor  l          voila  efficient feature value acquisition classification 
aaai conference artificial intelligence  pp           
dittmer  s     jensen  f          myopic value information influence diagrams 
annual conference uncertainty artificial intelligence  pp         
frank  a     asuncion  a          uci machine learning repository  
gaag  l     wessels  m          selective evidence gathering diagnostic belief networks 
aisb quarterly  pp       
grefenstette  j          optimization control parameters genetic algorithms  ieee
transactions systems  man cybernetics                 
greiner  r   grove  a  j     roth  d          learning cost sensitive active classifiers 
artificial intelligence                  
heckerman  d   horvitz  e     middleton  b          approximate nonmyopic computation value information  ieee transactions pattern analysis machine
intelligence                 
howard  r  a     matheson  j  e          readings principles applications
decision analysis  chap  influence diagrams  strategic decision group 
howard  r  a          information value theory  ieee transactions systems science
cybernetics              
hyafil  l     rivest  r  l          constructing optimal binary decision trees npcomplete  information processing letters              
lindley  d  v          measure information provided experiment  annals
mathematical statistics              
nunez  m          use background knowledge decision tree induction  machine
learning                
health  o  m          schedule benefits  physician services health insurance
act  
pearl  j          probabilistic reasoning intelligent systems  morgan kaufmann  san
francisco 
quinlan  j  r          induction decision trees  machine learning               
quinlan  j  r          c     programs machine learning  morgan kaufmann publishers
inc   san francisco  ca  usa 
saar tsechansky  m   melville  p     provost  f          active feature value acquisition 
management science                 
sent  d     gaag  l  c          enhancing automated test selection probabilistic networks  proceedings   th conference artificial intelligence medicine 
pp         
  

fivalue information lattice

tan  m          csl  cost sensitive learning system sensing grasping objects 
ieee international conference robotics automation 
turney  p  d          cost sensitive classification  empirical evaluation hybrid genetic
decision tree induction algorithm  journal artificial intelligence research        
    
yang  q   ling  c   chai  x     pan  r          test cost sensitive classification data
missing values  ieee transactions knowledge data engineering         
       

  


