journal of artificial intelligence research                  

submitted        published      

global inference for sentence compression
an integer linear programming approach
james clarke

jclarke ed ac uk

mirella lapata

mlap inf ed ac uk

school of informatics
university of edinburgh
  buccleuch place
edinburgh eh   lw  uk

abstract
sentence compression holds promise for many applications ranging from summarization
to subtitle generation  our work views sentence compression as an optimization problem
and uses integer linear programming  ilp  to infer globally optimal compressions in the
presence of linguistically motivated constraints  we show how previous formulations of
sentence compression can be recast as ilps and extend these models with novel global
constraints  experimental results on written and spoken texts demonstrate improvements
over state of the art models 

   introduction
the computational treatment of sentence compression has recently attracted much attention
in the literature  the task can be viewed as producing a summary of a single sentence that
retains the most important information and remains grammatical  jing         a sentence
compression mechanism would greatly benefit a wide range of applications  for example 
in summarization  it could improve the conciseness of the generated summaries  jing       
lin        zajic  door  lin    schwartz         other examples include compressing text
to be displayed on small screens such as mobile phones or pdas  corston oliver        
subtitle generation from spoken transcripts  vandeghinste   pan         and producing
audio scanning devices for the blind  grefenstette        
sentence compression is commonly expressed as a word deletion problem  given an input source sentence of words x   x    x            xn   the aim is to produce a target compression
by removing any subset of these words  knight   marcu         the compression problem has been extensively studied across different modeling paradigms  both supervised and
unsupervised  supervised models are typically trained on a parallel corpus of source sentences and target compressions and come in many flavors  generative models aim to model
the probability of a target compression given the source sentence either directly  galley
  mckeown        or indirectly using the noisy channel model  knight   marcu       
turner   charniak         whereas discriminative formulations attempt to minimize error
rate on a training set  these include decision tree learning  knight   marcu         maximum entropy  riezler  king  crouch    zaenen         support vector machines  nguyen 
shimazu  horiguchi  ho    fukushi         and large margin learning  mcdonald        
c
    
ai access foundation  all rights reserved 

ficlarke   lapata

unsupervised methods dispense with the parallel corpus and generate compressions either
using rules  turner   charniak        or a language model  hori   furui        
despite differences in formulation  all these approaches model the compression process
using local information  for instance  in order to decide which words to drop  they exploit
information about adjacent words or constituents  local models can do a good job at
producing grammatical compressions  however they are somewhat limited in scope since
they cannot incorporate global constraints on the compression output  such constraints
consider the sentence as a whole instead of isolated linguistic units  words or constituents  
to give a concrete example we may want to ensure that each target compression has a verb 
provided that the source had one in the first place  or that verbal arguments are present in
the compression  or that pronouns are retained  such constraints are fairly intuitive and
can be used to instill not only linguistic but also task specific information into the model 
for instance  an application which compresses text to be displayed on small screens would
presumably have a higher compression rate than a system generating subtitles from spoken
text  a global constraint could force the former system to generate compressions with a
fixed rate or a fixed number of words 
existing approaches do not model global properties of the compression problem for a
good reason  finding the best compression for a source sentence given the space of all
possible compressions   this search process is often referred to as decoding or inference 
can become intractable for too many constraints and overly long sentences  typically  the
decoding problem is solved efficiently using dynamic programming often in conjunction
with heuristics that reduce the search space  e g   turner   charniak         dynamic
programming guarantees we will find the global optimum provided the principle of optimality holds  this principle states that given the current state  the optimal decision for each
of the remaining stages does not depend on previously reached stages or previously made
decisions  winston   venkataramanan         however  we know this to be false in the
case of sentence compression  for example  if we have included modifiers to the left of a
noun in a compression then we should probably include the noun too or if we include a verb
we should also include its arguments  with a dynamic programming approach we cannot
easily guarantee such constraints hold 
in this paper we propose a novel framework for sentence compression that incorporates
constraints on the compression output and allows us to find an optimal solution  our
formulation uses integer linear programming  ilp   a general purpose exact framework for
np hard problems  specifically  we show how previously proposed models can be recast
as integer linear programs  we extend these models with constraints which we express as
linear inequalities  decoding in this framework amounts to finding the best solution given
a linear  scoring  function and a set of linear constraints that can be either global or local 
although ilp has been previously used for sequence labeling tasks  roth   yih       
punyakanok  roth  yih    zimak         its application to natural language generation
is less widespread  we present three compression models within the ilp framework  each
representative of an unsupervised  knight   marcu         semi supervised  hori   furui 
       and fully supervised modeling approach  mcdonald         we propose a small
number of constraints ensuring that the compressions are structurally and semantically
   there are  n possible compressions where n is the number of words in a sentence 

   

figlobal inference for sentence compression

valid and experimentally evaluate their impact on the compression task  in all cases  we
show that the added constraints yield performance improvements 
the remainder of this paper is organized as follows  section   provides an overview
of related work  in section   we present the ilp framework and the compression models
we employ in our experiments  our constraints are introduced in section      section    
discusses our experimental set up and section   presents our results  discussion of future
work concludes the paper 

   related work
in this paper we develop several ilp based compression models  before presenting these
models  we briefly summarize previous work addressing sentence compression with an emphasis on data driven approaches  next  we describe how ilp techniques have been used
in the past to solve other inference problems in natural language processing  nlp  
    sentence compression
jing        was perhaps the first to tackle the sentence compression problem  her approach
uses multiple knowledge sources to determine which phrases in a sentence to remove  central
to her system is a grammar checking module that specifies which sentential constituents
are grammatically obligatory and should therefore be present in the compression  this
is achieved using simple rules and a large scale lexicon  other knowledge sources include
wordnet and corpus evidence gathered from a parallel corpus of source target sentence
pairs  a phrase is removed only if it is not grammatically obligatory  not the focus of the
local context and has a reasonable deletion probability  estimated from a parallel corpus  
in contrast to jing         the bulk of the research on sentence compression relies exclusively on corpus data for modeling the compression process without recourse to extensive knowledge sources  e g   wordnet   a large number of approaches are based on the
noisy channel model  knight   marcu         these approaches consist of a language
model p  y   whose role is to guarantee that compression output is grammatical   a channel
model p  x y   capturing the probability that the source sentence x is an expansion of the
target compression y   and a decoder  which searches for the compression y that maximizes
p  y p  x y    the channel model is acquired from a parsed version of a parallel corpus  it
is essentially a stochastic synchronous context free grammar  aho   ullman        whose
rule probabilities are estimated using maximum likelihood  modifications of this model are
presented by turner and charniak        and galley and mckeown        with improved
results 
in discriminative models  knight   marcu        riezler et al         mcdonald       
nguyen et al         sentences are represented by a rich feature space  also induced from
parse trees  and the goal is to learn which words or word spans should be deleted in a given
context  for instance  in knight and marcus        decision tree model  compression is
performed deterministically through a tree rewriting process inspired by the shift reduce
parsing paradigm  nguyen et al         render this model probabilistic through the use
of support vector machines  mcdonald        formalizes sentence compression in a largemargin learning framework without making reference to shift reduce parsing  in his model
compression is a classification task  pairs of words from the source sentence are classified
   

ficlarke   lapata

as being adjacent or not in the target compression  a large number of features are defined
over words  parts of speech  phrase structure trees and dependencies  these features are
gathered over adjacent words in the compression and the words in between which were
dropped  see section       for a more detailed account  
while most compression models have been developed with written text in mind  hori
and furui        propose a model for automatically transcribed spoken text  their model
generates compressions through word deletion without using parallel data or syntactic information in any way  assuming a fixed compression rate  it searches for the compression
with the highest score using a dynamic programming algorithm  the scoring function consists of a language model responsible for producing grammatical output  a significance score
indicating whether a word is topical or not  and a score representing the speech recognizers
confidence in transcribing a given word correctly 
    integer linear programming in nlp
ilps are constrained optimization problems where both the objective function and the
constraints are linear equations with integer variables  see section     for more details   ilp
techniques have been recently applied to several nlp tasks  including relation extraction
 roth   yih         semantic role labeling  punyakanok et al          the generation of
route directions  marciniak   strube         temporal link analysis  bramsen  deshpande 
lee    barzilay         set partitioning  barzilay   lapata         syntactic parsing  riedel
  clarke         and coreference resolution  denis   baldridge        
most of these approaches combine a local classifier with an inference procedure based
on ilp  the classifier proposes possible answers which are assessed in the presence of global
constraints  ilp is used to make a final decision that is consistent with the constraints
and likely according to the classifier  for example  the semantic role labeling task involves
identifying the verb argument structure for a given sentence  punyakanok et al         first
use snow  a multi class classifier   roth         to identify and label candidate arguments 
they observe that the labels assigned to arguments in a sentence often contradict each other 
to resolve these conflicts they propose global constraints  e g   each argument should be
instantiated once for a given verb  every verb should have at least one argument  and use
ilp to reclassify the output of snow 
dras        develops a document paraphrasing model using ilp  the key premise of
his work is that in some cases one may want to rewrite a document so as to conform to
some global constraints such as length  readability  or style  the proposed model has three
ingredients  a set of sentence level paraphrases for rewriting the text  a set of global constraints  and an objective function which quantifies the effect incurred by the paraphrases 
under this formulation  ilp can be used to select which paraphrases to apply so that the
global constraints are satisfied  paraphrase generation falls outside the scope of the ilp
model  sentence rewrite operations are mainly syntactic and provided by a module based
on synchronous tree adjoining grammar  s tag  shieber   schabes         unfortunately 
only a proof of concept is presented  implementation and evaluation of this module are left
to future work 
   snows learning algorithm is a variation of the winnow update rule 

   

figlobal inference for sentence compression

our work models sentence compression as an optimization problem  we show how previously proposed models can be reformulated in the context of integer linear programming
which allows us to easily incorporate constraints during the decoding process  our constraints are linguistically and semantically motivated and are designed to bring less local
syntactic knowledge into the model and help preserve the meaning of the source sentence 
previous work has identified several important features for the compression task  knight
  marcu        mcdonald         however  the use of global constraints is novel to our
knowledge  although sentence compression has not been explicitly formulated in terms of
optimization  previous approaches rely on some optimization procedure for generating the
best compression  the decoding process in the noisy channel model searches for the best
compression given the source and channel models  however  the compression found is usually sub optimal as heuristics are used to reduce the search space or is only locally optimal
due to the search method employed  for example  in the work of turner and charniak
       the decoder first searches for the best combination of rules to apply  as it traverses
the list of compression rules  it removes sentences outside the     best compressions  according to the channel model   this list is eventually truncated to    compressions  in
other models  hori   furui        mcdonald        the compression score is maximized
using dynamic programming which however can yield suboptimal results  see the discussion
in section    
contrary to most other nlp work using ilp  a notable exception is roth   yih        
we do not view compression generation as a two stage process where learning and inference
are carried out sequentially  i e   first a local classifier hypothesizes a list of possible answers and then the best answer is selected using global constraints   our models integrate
learning with inference in a unified framework where decoding takes place in the presence
of all available constraints  both local and global  moreover  we investigate the influence
of our constraint set across models and learning paradigms  previous work typically formulates constraints for a single model  e g   the snow classifier  and learning paradigm
 e g   supervised   we therefore assess how the constraint based framework advocated in
this article influences the performance of expressive models  which require large amounts of
parallel data  and non expressive ones  which use very little parallel data or none at all   in
other words  we are able to pose and answer the following question  what kinds of models
benefit most from constraint based inference 
our work is close in spirit but rather different in content to dras         we concentrate
on compression  a specific paraphrase type  and apply our models on the sentence level  our
constraints thus do not affect the document as a whole but individual sentences  furthermore  compression generation is an integral part of our ilp models  whereas dras assumes
that paraphrases are generated by a separate process 

   framework
in this section we present the details of the proposed framework for sentence compression 
as mentioned earlier  our work models sentence compression directly as an optimization
problem  there are  n possible compressions for each source sentence and while many
of these will be unreasonable  it is unlikely that only one compression will be satisfactory  knight   marcu         ideally  we require a function that captures the operations
   

ficlarke   lapata

 or rules  that can be performed on a sentence to create a compression while at the same
time factoring how desirable each operation makes the resulting compression  we can then
perform a search over all possible compressions and select the best one  as determined by
how desirable it is  a wide range of models can be expressed under this framework  the
prerequisites for implementing these are fairly low  we only require that the decoding process be expressed as a linear function with a set of linear constraints  in practice  many
models rely on a markov assumption for factorization which is usually solved with a dynamic programming based decoding process  such algorithms can be formulated as integer
linear programs with little effort 
we first give a brief introduction into integer linear programming  an extension of linear
programming for readers unfamiliar with mathematical programming  our compression
models are next described in section     and constraints in section     
    linear programming
linear programming  lp  problems are optimization problems with constraints  they
consist of three parts 
 decision variables  these are variables under our control which we wish to assign
optimal values to 
 a linear function  the objective function   this is the function we wish to minimize or
maximize  this function is influences by the values assigned to the decision variables 
 constraints  most problems will only allow the decision variables to take certain
values  these restrictions are the constraints 
these terms are best demonstrated with a simple example taken from winston and
venkataramanan         imagine a manufacturer of tables and chairs which we shall call
the telfa corporation  to produce a table    hour of labor and   square board feet of wood
is required  chairs require   hour of labor and   square board feet of wood  telfa have
  hours of labor and    square board feet of wood available  the profit made from each
table is   gbp and   gbp for chairs  we wish to determine the number of tables and
chairs that should be manufactured to maximize telfas profit 
first  we must determine the decision variables  in our case we define 
x    number of tables manufactured
x    number of chairs manufactured
our objective function is the value we wish to maximize  namely the profit 
profit    x     x 
there are two constraints in this problem  we must not exceed   hours of labor and no
more than    square board feet of wood must be used  also  we cannot create a negative
amount of chairs or tables 
   

figlobal inference for sentence compression

labor constraint
x     x 
wood constraint
 x     x 
variable constraints
x 
x 

  
   
  
  

once the decision variables  objective function and constraints have been determined we
can express the lp model 
max z    x     x   objective function 
subject to  s t  
x    x 
 x     x 
x 
x 

    labor constraint 
     wood constraint 
  
  

two of the most basic concepts involved in solving lp problems are the feasibility region
and optimal solution  the optimal solution is one in which all constraints are satisfied
and the objective function is minimized or maximized  a specification of the value for
each decision variable is referred to as a point  the feasibility region for a lp is a region
consisting of the set of all points that satisfy all the lps constraints  the optimal solution
lies within this feasibility region  it is the point with the minimum or maximum objective
function value 
a set of points satisfying a single linear inequality is a half space  the feasibility region
is defined by a the intersection of m half spaces  for m linear inequalities  and forms a
polyhedron  our telfa example forms a polyhedral set  a polyhedral convex set  from
the intersection of our four constraints  figure  a shows the feasible region for the telfa
example  to find the optimal solution we graph a line  or hyperplane  on which all points
have the same objective function value  in maximization problems it is called the isoprofit
line and in minimization problems the isocost line  one isoprofit line is represented by the
dashed black line in figure  a  once we have one isoprofit line we can find all other isoprofit
lines by moving parallel to the original isoprofit line 
the extreme points of the polyhedral set are defined as the intersections of the lines
that form the boundaries of the polyhedral set  points a b c and d in figure  a   it can
be shown that any lp that has an optimal solution  has an extreme point that is globally
optimal  this reduces the search space of the optimization problem to finding the extreme
point with the highest or lowest value  the simplex algorithm  dantzig        solves lps
by exploring the extreme points of a polyhedral set  specifically  it moves from one extreme
point to an adjacent extreme point  extreme points that lie on the same line segment  until
an optimal extreme point is found  although the simplex algorithm has an exponential
worst case complexity  in practice the algorithm is very efficient 
  
 
the optimal solution for the telfa example is z      
    x        x        thus  to
achieve a maximum profit of       gbp they must build      tables and      chairs  this
is obviously impossible as we would not expect people to buy fractions of tables and chairs 
here  we want to be able to constrain the problem such that the decision variables can only
take integer values  this can be done with integer linear programming 
   

ficlarke   lapata

a 

b 

  

  

 

 

  lps feasible region

 x     x      

 x    x      

 

 

 

 

  b

 

x   

x   

 

 

  ip feasible point
  ip relaxations feasible region

 

 

optimal lp solution

optimal lp solution
 

c

 

x     x     

 
 

a

 

 

 

 

x 

 

d
 

 

x     x     

  

 

 

 

 

 

 

x 

 

 

 

 

figure    feasible region for the telfa example using linear  graph  a   and integer linear
 graph  b   programming

    integer linear programming
integer linear programming  ilp  problems are lp problems in which some or all of the
variables are required to be non negative integers  they are formulated in a similar manner
to lp problems with the added constraint that all decision variables must take non negative
integer values 
to formulate the telfa problem as an ilp model we merely add the constraints that x 
and x  must be integer  this gives 
max z    x     x   objective function 
subject to  s t  
x    x 
 x     x 
x 
x 


   labor constraint 

    wood constraint 
    x  integer
    x  integer

for lp models  it can be proved that the optimal solution lies on an extreme point of
the feasible region  in the case of integer linear programs  we only wish to consider points
that are integer values  this is illustrated in figure  b for the telfa problem  in contrast to
linear programming  which can be solved efficiently in the worst case  integer programming
problems are in many practical situations np hard  cormen  leiserson    rivest        
   

figlobal inference for sentence compression

fortunately  ilps are a well studied optimization problem and a number of techniques have
been developed to find the optimal solution  two such techniques are the cutting planes
method  gomory        and the branch and bound method  land   doig         we
briefly discuss these methods here  for a more detailed treatment we refer the interested
reader to winston and venkataramanan        or nemhauser and wolsey        
the cutting planes method adds extra constraints to slice parts of the feasible region
until it contains only integer extreme points  however  this process can be difficult or
impossible  nemhauser   wolsey         the branch and bound method enumerates all
points in the ilps feasible region but prunes those sections in the region which are known
to be sub optimal  it does this by relaxing the integer constraints and solving the resulting
lp problem  known as the lp relaxation   if the solution of the lp relaxation is integral 
then it is the optimal solution  otherwise  the resulting solution provides an upper bound
on the solution for the ilp  the algorithm proceeds by creating two new sub problems based
on the non integer solution for one variable at a time  these are solved and the process
repeats until the optimal integer solution is found 
using the branch and bound method  we find that the optimal solution to the telfa
problem is z       x       x       thus  to achieve a maximum profit of    gbp  telfa
must manufacture   tables and   chairs  this is a relatively simple problem  which could be
solved merely by inspection  most ilp problems will involve many variables and constraints
resulting in a feasible region with a large number of integer points  the branch and bound
procedure can efficiently solve such ilps in a matter of seconds and forms part of many
commercial ilp solvers  in our experiments we use lp solve     a free optimization package
which relies on the simplex algorithm and brand and bound methods for solving ilps 
note that under special circumstances other solving methods may be applicable  for
example  implicit enumeration can be used to solve ilps where all the variables are binary
 also known as pure    problems   implicit enumeration is similar to the branch andbound method  it systematically evaluates all possible solutions  without however explicitly
solving a  potentially  large number of lps derived from the relaxation  this removes
much of the computational complexity involved in determining if a sub problem is infeasible  furthermore  for a class of ilp problems known as minimum cost network flow
problems  mcnfp   the lp relaxation always yields an integral solution  these problems
can therefore be treated as lp problems 
in general  a model will yield an optimal solution in which all variables are integers if
the constraint matrix has a property known as total unimodularity  a matrix a is totally
unimodular if every square sub matrix of a has its determinant equal to       or   
it is the case that the more the constraint matrix looks totally unimodular  the easier
the problem will be to solve by branch and bound methods  in practice it is good to
formulate ilps where as many variables as possible have coefficients of       or   in the
constraints  winston   venkataramanan        
    constraints and logical conditions
although integer variables in ilp problems may take arbitrary values  these are frequently
are restricted to   and    binary variables     variables  are particularly useful for rep   the software is available from http   lpsolve sourceforge net  

   

ficlarke   lapata

condition
implication
iff
or
xor
and
not

statement
if a then b
a if and only if b
a or b or c
a xor b xor c
a and b
not a

constraint
ba 
ab  
a b c 
a b c  
a      b    
 a  

table    how to represent logical conditions using binary variables and constraints in ilp 

resenting a variety of logical conditions within the ilp framework through the use of constraints  table   lists several logical conditions and their equivalent constraints 
we can also express transitivity  i e   c if and only if a and b  although it is often thought that transitivity can only be expressed as a polynomial expression of binary
variables  i e   ab   c   it is possible to replace the latter by the following linear inequalities  williams        

    c    a   
    c    b   
c       a        b    
this can be easily extended to model indicator variables representing whether a set of binary
variables can take certain values 
    compression models
in this section we describe three compression models which we reformulate as integer linear
programs  our first model is a simple language model which has been used as a baseline in
previous research  knight   marcu         our second model is based on the work of hori
and furui         it combines a language model with a corpus based significance scoring
function  we omit here the confidence score derived from the speech recognizer since our
models are applied to text only   this model requires a small amount of parallel data to
learn weights for the language model and the significance score 
our third model is fully supervised  it uses a discriminative large margin framework
 mcdonald         and is trained trained on a larger parallel corpus  we chose this model
instead of the more popular noisy channel or decision tree models  for two reasons  a practical one and a theoretical one  first  mcdonalds        model delivers performance superior
to the decision tree model  which in turn performs comparably to the noisy channel   second  the noisy channel is not an entirely appropriate model for sentence compression  it
uses a language model trained on uncompressed sentences even though it represents the
probability of compressed sentences  as a result  the model will consider compressed sentences less likely than uncompressed ones  a further discussion is provided by turner  
charniak        
   

figlobal inference for sentence compression

      language model
a language model is perhaps the simplest model that springs to mind  it does not require
a parallel corpus  although a relatively large monolingual corpus is necessary for training  
and will naturally prefer short sentences to longer ones  furthermore  a language model can
be used to drop words that are either infrequent or unseen in the training corpus  knight
and marcu        use a bigram language model as a baseline against their noisy channel
and decision tree models 
let x   x    x            xn denote a source sentence for which we wish to generate a target
compression  we introduce a decision variable for each word in the source and constrain it
to be binary  a value of   represents a word being dropped  whereas a value of   includes
the word in the target compression  let 
i  

 

  if xi is in the compression
i           n 
  otherwise

if we were using a unigram language model  our objective function would maximize the
overall sum of the decision variables  i e   words  multiplied by their unigram probabilities
 all probabilities throughout this paper are log transformed  
max

n
x

i  p  xi  

   

i  

thus  if a word is selected  its corresponding i is given a value of    and its probability
p  xi   according to the language model will be counted in our total score 
a unigram language model will probably generate many ungrammatical compressions 
we therefore use a more context aware model in our objective function  namely a trigram
model  dynamic programming would be typically used to decode a language model by
traversing the sentence in a left to right manner  such an algorithm is efficient and provides
all the context required for a conventional language model  however  it can be difficult
or impossible to incorporate global constraints into such a model as decisions on word
inclusion cannot extend beyond a three word window  by formulating the decoding process
for a trigram language model as an integer linear program we are able to take into account
constraints that affect the compressed sentence more globally  this process is a much more
involved task than in the unigram case where there is no context  instead we must now
make decisions based on word sequences rather than isolated words  we first create some
additional decision variables 
i  

 

ij  



  

ijk  

  if xi starts the compression
i           n 
  otherwise

if sequence xi   xj ends
the compression
i           n    

   otherwise
j   i           n 



  

if sequence xi   xj   xk i           n    
is in the compression j   i           n    

   otherwise
k   j           n 
   

ficlarke   lapata

our objective function is given in equation      this is the sum of all possible trigrams
that can occur in all compressions of the source sentence where x  represents the start
token and xi is the ith word in sentence x  equation     constrains the decision variables
to be binary 
max z  

n
x

i  p  xi  start 
i  
n 
n
x n 
x x

 

ijk  p  xk  xi   xj  

i   j i   k j  

 

n 
x

n
x

ij  p  end xi   xj  

   

i   j i  

subject to 

i   i   ij   ijk     or  

   

the objective function in     allows any combination of trigrams to be selected  this
means that invalid trigram sequences  e g   two or more trigrams containing the end token 
could appear in the target compression  we avoid this situation by introducing sequential
constraints  on the decision variables i   ijk   i   and ij   that restrict the set of allowable
trigram combinations 
constraint  

exactly one word can begin a sentence 
n
x

i    

   

i  

constraint   if a word is included in the sentence it must either start the sentence or be
preceded by two other words or one other word and the start token x   
k  k 

k 
x k 
x

ijk    

   

i   j  

k   k           n 
constraint   if a word is included in the sentence it must either be preceded by one
word and followed by another or it must be preceded by one word and end the sentence 
j 

j 
x

n
x

ijk 

i   k j  

j 
x

ij    

   

i  

j   j           n 

constraint   if a word is in the sentence it must be followed by two words or followed
by one word and then the end of the sentence or it must be preceded by one word and end
the sentence 
i 

n 
x

n
x

j i   k j  

ijk 

n
x

j i  

   

ij 

i 
x

hi    

h  

i   i           n 

   

figlobal inference for sentence compression

constraint  

exactly one word pair can end the sentence 
n 
x

n
x

ij    

   

i   j i  

the sequential constraints described above ensure that the second order factorization  for
trigrams  holds and are different from our compression specific constraints which are presented in section     
unless normalized by sentence length  a language model will naturally prefer one word
output  this normalization is however non linear and cannot be incorporated into our ilp
formulation  instead  we impose a constraint on the length of the compressed sentence 
equation     below forces the compression to contain at least b tokens 
n
x

i  b

   

i  

alternatively  we could force the compression to be exactly b tokens  by substituting the
inequality with an equality in      or to be less than b tokens  by replacing  with    
the constraint in     is language model specific and is not used elsewhere 
      significance model
the language model just described has no notion of which content words to include in the
compression and thus prefers words it has seen before  but words or constituents will be of
different relative importance in different documents or even sentences 
inspired by hori and furui         we add to our objective function  see equation     
a significance score designed to highlight important content words  in hori and furuis
original formulation each word is weighted by a score similar to un normalized tf  idf   the
significance score is not applied indiscriminately to all words in a sentence but solely to
topic related words  namely nouns and verbs  our score differs in one respect  it combines
document level with sentence level significance  so in addition to tf  idf   each word is
weighted by its level of embedding in the syntactic tree 
intuitively  in a sentence with multiply nested clauses  more deeply embedded clauses
tend to carry more semantic content  this is illustrated in figure   which depicts the
clause embedding for the sentence mr field has said he will resign if he is not reselected 
a move which could divide the party nationally  here  the most important information is
conveyed by clauses s   he will resign  and s   if he is not reselected  which are embedded 
accordingly  we should give more weight to words found in these clauses than in the main
clause  s  in figure     a simple way to enforce this is to give clauses weight proportional
to the level of embedding  our modified significance score becomes 
i xi    

fa
l
 fi log
n
fi

    

where xi is a topic word  fi and fi are the frequency of xi in the document and corpus
respectively  fa is the sum of all topic words in the corpus  l is the number of clause
   compression rate can be also limited to a range by including two inequality constraints 

   

ficlarke   lapata

s 
s 
mr field has said
s 
he will resign
s 
if he is not reselected
  a move
sbar
which could divide the party nationally

figure    the clause embedding of the sentence mr field has said he will resign if he is
not reselected  a move which could divide the party nationally  nested boxes
correspond to nested clauses 

constituents above xi   and n is the deepest level of clause embedding  fa and fi are
estimated from a large document collection  fi is document specific  whereas nl is sentencespecific  so  in figure   the term nl is           for clause s               for clause s    and
so on  individual words inherit their weight from their clauses 
the modified objective function with the significance score is given below 
max z  

n
x

i  i xi    

i  
n 
x n 
x

 

n
x

i  p  xi  start 

i  

n
x

ijk  p  xk  xi   xj  

i   j i   k j  

 

n 
x

n
x

ij  p  end xi   xj  

    

i   j i  

we also add a weighting factor    to the objective  in order to counterbalance the importance of the language model and the significance score  the weight is tuned on a small
parallel corpus  the sequential constraints from equations        are again used to ensure
that the trigrams are combined in a valid way 
      discriminative model
as a fully supervised model  we used the discriminative model presented by mcdonald
        this model uses a large margin learning framework coupled with a feature set
defined on compression bigrams and syntactic structure 
let x   x            xn denote a source sentence with a target compression y   y            ym
where each yj occurs in x  the function l yi             n  maps word yi in the target com   

figlobal inference for sentence compression

pression to the index of the word in the source sentence  x  we also include the constraint
that l yi     l yi     which forces each word in x to occur at most once in the compression
y  let the score of a compression y for a sentence x be 
    

s x  y 

this score is factored using a first order markov assumption on the words in the target
compression to give 
s x  y   

 y 
x

s x  l yj     l yj   

    

j  

the score function is defined to be the dot product between a high dimensional feature
representation and a corresponding weight vector 
s x  y   

 y 
x

w  f  x  l yj     l yj   

    

j  

decoding in this model amounts to finding the combination of bigrams that maximizes
the scoring function in       mcdonald        uses a dynamic programming approach
where the maximum score is found in a left to right manner  the algorithm is an extension
of viterbi for the case in which scores factor over dynamic sub strings  sarawagi   cohen 
      mcdonald  crammer    pereira      a   this allows back pointers to be used to
reconstruct the highest scoring compression as well as the k best compressions 
again this is similar to the trigram language model decoding process  see section        
except that here a bigram model is used  consequently  the ilp formulation is slightly
simpler than that of the trigram language model  let 
i  

 

  if xi is in the compression
    i  n 
  otherwise

we then introduce some more decision variables 
i  
i  
ij  

 

 

 

  if xi starts the compression
i           n 
  otherwise

  if word xi ends the compression
  otherwise
i           n 

  if sequence xi   xj is in the compression i           n    
  otherwise
j   i           n 

the discriminative model can be now expressed as 
max z  

n
x

i
i  
n 
x

 s x     i 

 

i  s x  i  n     

 

n
x

i   j i  
n
x
i  

   

ij  s x  i  j 
    

ficlarke   lapata

constraint  

exactly one word can begin a sentence 
n
x

i    

    

i  

constraint   if a word is included in the sentence it must either start the compression
or follow another word 

j  j 

j
x

ij    

    

i  

j   j           n 
constraint   if a word is included in the sentence it must be either followed by another
word or end the sentence 

i 

n
x

ij  i    

    

j i  

i   i           n 

constraint  

exactly one word can end a sentence 
n
x

i    

    

i  

again  the sequential constraints in equations          are necessary to ensure that the
resulting combination of bigrams are valid 
the current formulation provides a single optimal compression given the model  however  mcdonalds        dynamic programming algorithm is capable of returning the k best
compressions  this is useful for their learning algorithm described later  in order to produce
k best compressions  we must rerun the ilp with extra constraints which forbid previous
solutions  in other words  we first formulate the ilp as above  solve it  add its solution to
the k best list  and then create a set of constraints that forbid the configuration of i decision
variables which form the current solution  the procedure is repeated until k compressions
are found 
the computation of the compression score crucially relies on the dot product between
a high dimensional feature representation and a corresponding weight vector  see equation        mcdonald        employs a rich feature set defined over adjacent words and
individual parts of speech  dropped words and phrases from the source sentence  and dependency structures  also of the source sentence   these features are designed to mimic the
information presented in the previous noisy channel and decision tree models of knight and
marcu         features over adjacent words are used as a proxy to the language model of
the noisy channel  unlike other models  which treat the parses as gold standard  mcdonald
uses the dependency information as another form of evidence  faced with parses that are
noisy the learning algorithm can reduce the weighting given to those features if they prove
   

figlobal inference for sentence compression

poor discriminators on the training data  thus  the model should be much more robust
and portable across different domains and training corpora 
the weight vector  w is learned using the margin infused relaxed algorithm  mira 
crammer   singer        a discriminative large margin online learning technique  mcdonald  crammer    pereira      b   this algorithm learns by compressing each sentence and
comparing the result with the gold standard  the weights are updated so that the score of
the correct compression  the gold standard  is greater than the score of all other compressions by a margin proportional to their loss  the loss function is the number of words falsely
retained or dropped in the incorrect compression relative to the gold standard  a source
sentence will have exponentially many compressions and thus exponentially many margin
constraints  to render learning computationally tractable  mcdonald et al       b  create
constraints only on the k compressions that currently have the highest score  bestk  x  w  
    constraints
we are now ready to describe our compression specific constraints  the models presented
in the previous sections contain only sequential constraints and are thus equivalent to their
original formulation  our constraints are linguistically and semantically motivated in a
similar fashion to the grammar checking component of jing         however  they do
not rely on any additional knowledge sources  such as a grammar lexicon or wordnet 
beyond the parse and grammatical relations of the source sentence  we obtain these from
rasp  briscoe   carroll         a domain independent  robust parsing system for english 
however  any other parser with broadly similar output  e g   lin        could also serve our
purposes  our constraints revolve around modification  argument structure  and discourse
related factors 
modifier constraints modifier constraints ensure that relationships between head words
and their modifiers remain grammatical in the compression 
i  j   

    

i  j   xj  xi s ncmods
i  j   

    

i  j   xj  xi s detmods
equation      guarantees that if we include a non clausal modifier   ncmod  in the compression  such as an adjective or a noun  then the head of the modifier must also be included 
this is repeated for determiners  detmod  in       in table   we illustrate how these constraints disallow the deletion of certain words  starred sentences denote compressions that
would not be possible given our constraints   for example  if the modifier word pasok from
sentence   a  is in the compression  then its head party will also included  see   b   
we also want to ensure that the meaning of the source sentence is preserved in the
compression  particularly in the face of negation  equation      implements this by forcing
not in the compression when the head is included  see sentence   b  in table     a similar
constraint is added for possessive modifiers  e g   his  our   including genitives  e g   johns
   clausal modifiers  cmod  are adjuncts modifying entire clauses  in the example he ate the cake because
he was hungry  the because clause is a modifier of the sentence he ate the cake 

   

ficlarke   lapata

 a 
 b 
 a 
 b 
 c 
 a 
 b 
 c 
 d 
 e 
 f 

he became a power player in greek politics in       when he founded the
socialist pasok party 
 he became a power player in greek politics in       when he founded the
pasok 
we took these troubled youth who dont have fathers  and brought them into
a room to dads who dont have their children 
 we took these troubled youth who do have fathers  and brought them into a
room to dads who do have their children 
 we took these troubled youth who dont have fathers  and brought them into
a room to dads who dont have children 
the chain stretched from uganda to grenada and nicaragua  since the     s 
 stretched from uganda to grenada and nicaragua  since the     s 
 the chain from uganda to grenada and nicaragua  since the     s 
 the chain stretched uganda to grenada and nicaragua  since the     s 
 the chain stretched from to grenada and nicaragua  since the     s 
 the chain stretched from uganda to grenada nicaragua  since the     s 
table    examples of compressions disallowed by our set of constraints 

gift   as shown in equation       an example of the possessive constraint is given in
sentence   c  in table   
i  j    

    

i  j   xj  xi s ncmods  xj   not
i  j    

    

i  j   xj  xi s possessive mods
argument structure constraints we also define a few intuitive constraints that take
the overall sentence structure into account  the first constraint  equation       ensures
that if a verb is present in the compression then so are its arguments  and if any of the
arguments are included in the compression then the verb must also be included  we thus
force the program to make the same decision on the verb  its subject  and object  see
sentence   b  in table    
i  j    

    

i  j   xj  subject object of verb xi
our second constraint forces the compression to contain at least one verb provided the
source sentence contains one as well 
x

i   

    

i xi verbs

the constraint entails that it is not possible to drop the main verb stretched from sentence   a   see also sentence   c  in table    
   

figlobal inference for sentence compression

other sentential constraints include equations      and      which apply to prepositional phrases and subordinate clauses  these constraints force the introducing term
 i e   the preposition  or subordinator  to be included in the compression if any word from
within the syntactic constituent is also included  by subordinator we mean wh words
 e g   who  which  how  where   the word that  and subordinating conjunctions  e g   after 
although  because   the reverse is also true  i e   if the introducing term is included  at
least one other word from the syntactic constituent should also be included 
i  j   

    

i  j   xj  pp sub
xi starts pp sub
x

i  j   

    

i xi pp sub

j   xj starts pp sub
as an example consider sentence   d  from table    here  we cannot drop the preposition
from if uganda is in the compression  conversely  we must include from if uganda is in the
compression  see sentence   e   
we also wish to handle coordination  if two head words are conjoined in the source
sentence  then if they are included in the compression the coordinating conjunction must
also be included 
    i     j   

    

    i     k   

    

i       j         k     

    

i  j  k   xj  xk conjoined by xi
consider sentence   f  from table    if both uganda and nicaragua are present in the
compression  then we must include the conjunction and 
finally  equation      disallows anything within brackets in the source sentence from
being included in the compression  this is a somewhat superficial attempt at excluding
parenthetical and potentially unimportant material from the compression 
i    

    

i   xi  bracketed words  inc parentheses 
discourse constraints our discourse constraint concerns personal pronouns  specifically  equation      forces personal pronouns to be included in the compression  the
constraint is admittedly more important for generating coherent documents  as opposed to
individual sentences   it nevertheless has some impact on sentence level compressions  in
particular when verbal arguments are missed by the parser  when these are pronominal 
constraint      will result in more grammatical output since some of the argument structure
of the source sentence will be preserved in the compression 
i    
i   xi  personal pronouns
   

    

ficlarke   lapata

we should note that some of the constraints described above would be captured by
models that learn synchronous deletion rules from a corpus  for example  the noisy channel
model of knight and marcu        learns not to drop the head when the latter is modified
by an adjective or a noun  since the transformations dt nn  dt or ajd nn  adj are
almost never seen in the data  similarly  the coordination constraint  equations          
would be enforced using turner and charniaks        special rules  they enhance their
parallel grammar with rules modeling more structurally complicated deletions than those
attested in their corpus  in designing our constraints we aimed at capturing appropriate
deletions for many possible models  including those that do not rely on a training corpus
or do not have an explicit notion of a parallel grammar  e g   mcdonald         the
modification constraints would presumably be redundant for the noisy channel model  which
could otherwise benefit from more specialized constraints  e g   targeting sparse rules or
noisy parse trees  however we leave this to future work 
another feature of the modeling framework presented here is that deletions  or nondeletions  are treated as unconditional decisions  for example  we require not to drop the
noun in adjective noun sequences if the adjective is not deleted as well  we also require to
always include a verb in the compression if the source sentence has one  these hardwired decisions could in some cases prevent valid compressions from being considered  for instance 
it is not possible to compress the sentence this is not appropriate behavior to this is
not appropriate orbob loves mary and john loves susan to bob loves mary and john
susan  admittedly we lose some expressive power  yet we ensure that the compressions
will be broadly grammatically  even for unsupervised or semi supervised models  furthermore  in practice we find that our models consistently outperform non constraint based
alternatives  without extensive constraint engineering 
    solving the ilp
as we mentioned earlier  section       solving ilps is np hard  in cases where the coefficient matrix is unimodular  it can be shown that the optimal solution to the linear
program is integral  although the coefficient matrix in our problems is not unimodular  we
obtained integral solutions for all sentences we experimented with  approximately       
see section     for details   we conjecture that this is due to the fact that all of our variables have       or   coefficients in the constraints and therefore our constraint matrix
shares many properties of a unimodular matrix  we generate and solve an ilp for every
sentence we wish to compress  solve times are less than a second per sentence  including
input output overheads  for all models presented here 

   experimental set up
our evaluation experiments were motivated by three questions      do the constraintbased compression models deliver performance gains over non constraint based ones  we
expect better compressions for the model variants which incorporate compression specific
constraints      are there differences among constraint based models  here  we would like
to investigate how much modeling power is gained by the addition of the constraints  for
example  it may be the case that a state of the art model like mcdonalds        does not
benefit much from the addition of constraints  and that their effect is much bigger for less
   

figlobal inference for sentence compression

sophisticated models      how do the models reported in this paper port across domains 
in particular  we are interested in assessing whether the models and proposed constraints
are general and robust enough to produce good compressions for both written and spoken
texts 
we next describe the data sets on which our models were trained and tested  section      
explain how model parameters were estimated  section      and present our evaluation setup
 section       we discuss our results in section   
    corpora
our intent was to assess the performance of the models just described on written and spoken
text  the appeal of written text is understandable since most summarization work today
focuses on this domain  speech data not only provides a natural test bed for compression
applications  e g   subtitle generation  but also poses additional challenges  spoken utterances can be ungrammatical  incomplete  and often contain artefacts such as false starts 
interjections  hesitations  and disfluencies  rather than focusing on spontaneous speech
which is abundant in these artefacts  we conduct our study on the less ambitious domain
of broadcast news transcripts  this lies in between the extremes of written text and spontaneous speech as it has been scripted beforehand and is usually read off on autocue 
previous work on sentence compression has almost exclusively used the ziff davis corpus
for training and testing purposes  this corpus originates from a collection of news articles
on computer products  it was created automatically by matching sentences that occur in
an article with sentences that occur in an abstract  knight   marcu         the abstract
sentences had to contain a subset of the source sentences words and the word order had
to remain the same  in earlier work  clarke   lapata        we have argued that the
ziff davis corpus is not ideal for studying compression for several reasons  first  we showed
that human authored compressions differ substantially from the ziff davis which tends to
be more aggressively compressed  second  humans are more likely to drop individual words
than lengthy constituents  third  the test portion of the ziff davis contains solely    sentences  this is an extremely small data set to reveal any statistically significant differences
among systems  in fact  previous studies relied almost exclusively on human judgments for
assessing the well formedness of the compressed output  and significance tests are reported
for by subjects analyses only 
we thus focused in the present study on manually created corpora  specifically  we
asked annotators to perform sentence compression by removing tokens on a sentence bysentence basis  annotators were free to remove any words they deemed superfluous provided
their deletions   a  preserved the most important information in the source sentence  and
 b  ensured the compressed sentence remained grammatical  if they wished  they could leave
a sentence uncompressed by marking it as inappropriate for compression  they were not
allowed to delete whole sentences even if they believed they contained no information content
with respect to the story as this would blur the task with abstracting  following these
guidelines  our annotators produced compressions of    newspaper articles        sentences 
from the british national corpus  bnc  and the american news text corpus  henceforth
written corpus  and    stories        sentences  from the hub        english broadcast
news corpus  henceforth spoken corpus   the written corpus contains articles from the la
   

ficlarke   lapata

times  washington post  independent  the guardian and daily telegraph  the spoken
corpus contains broadcast news from a variety of networks  cnn  abc  cspan and npr 
which have been manually transcribed and segmented at the story and sentence level  both
corpora have been split into training  development and testing sets  randomly on article
boundaries  with each set containing full stories  and are publicly available from http 
  homepages inf ed ac uk s        data  
    parameter estimation
in this work we present three compression models ranging from unsupervised to semisupervised  and fully supervised  the unsupervised model simply relies on a trigram language model for driving compression  see section         this was estimated from    million tokens of the north american corpus using the cmu cambridge language modeling
toolkit  clarkson   rosenfeld        with a vocabulary size of        tokens and goodturing discounting  to discourage one word output we force the ilp to generate compressions whose length is no less than     of the source sentence  see the constraint in      
the semi supervised model is the weighted combination of a word based significance score
with a language model  see section         the significance score was calculated using
   million tokens from the american news text corpus  we optimized its weight  see
equation       on a small subset of the training data  three documents in each case  using powells method  press  teukolsky  vetterling    flannery        and a loss function
based on the f score of the grammatical relations found in the gold standard compression
and the systems best compression  see section     for details   the optimal weight was
approximately     for the written corpus and     for the spoken corpus 
mcdonalds        supervised model was trained on the written and spoken training
sets  our implementation used the same feature sets as mcdonald  the only difference
being that our phrase structure and dependency features were extracted from the output of
roarks        parser  mcdonald uses charniaks        parser which performs comparably 
the model was learnt using k best compressions  on the development data  we found that
k      provided the best performance 
    evaluation
previous studies have relied almost exclusively on human judgments for assessing the wellformedness of automatically derived compressions  these are typically rated by naive subjects on two dimensions  grammaticality and importance  knight   marcu         although
automatic evaluation measures have been proposed  riezler et al         bangalore  rambow    whittaker        their use is less widespread  we suspect due to the small size of
the test portion of the ziff davis corpus which is commonly used in compression work 
we evaluate the output of our models in two ways  first  we present results using
an automatic evaluation measure put forward by riezler et al          they compare
the grammatical relations found in the system compressions against those found in a gold
standard  this allows us to measure the semantic aspects of summarization quality in terms
of grammatical functional information and can be quantified using f score  furthermore 
   the splits are            sentences for the written corpus and            sentences for the spoken
corpus 

   

figlobal inference for sentence compression

in clarke and lapata        we show that relations based f score correlates reliably with
human judgments on compression output  since our test corpora are larger than ziffdavis  by more than a factor of ten   differences among systems can be highlighted using
significance testing 
our implementation of the f score measure used the grammatical relations annotations
provided by rasp  briscoe   carroll         this parser is particularly appropriate for the
compression task since it provides parses for both full sentences and sentence fragments and
is generally robust enough to analyze semi grammatical sentences  we calculated f score
over all the relations provided by rasp  e g   subject  direct indirect object  modifier    
in total  
in line with previous work we also evaluate our models by eliciting human judgments 
following the work of knight and marcu         we conducted two separate experiments 
in the first experiment participants were presented with a source sentence and its target
compression and asked to rate how well the compression preserved the most important
information from the source sentence  in the second experiment  they were asked to rate
the grammaticality of the compressed outputs  in both cases they used a five point rating
scale where a high number indicates better performance  we randomly selected    sentences
from the test portion of each corpus  these sentences were compressed automatically by
the three models presented in this paper with and without constraints  we also included
gold standard compressions  our materials thus consisted of                sourcetarget sentences  a latin square design ensured that subjects did not see two different
compressions of the same sentence  we collected ratings from    unpaid volunteers  all self
reported native english speakers  both studies were conducted over the internet using a
custom build web interface  examples of our experimental items are given in table   

   results
let us first discuss our results when compression output is evaluated in terms of f score 
tables   and   illustrate the performance of our models on the written and spoken corpora 
respectively  we also present the compression rate  for each system  in all cases the
constraint based models   constr  yield better f scores than the non constrained ones 
the difference is starker for the semi supervised model  sig   the constraints bring an
improvement of       on the written corpus and       on the spoken corpus  we further
examined whether performance differences among models are statistically significant  using
the wilcoxon test  on the written corpus all constraint models significantly outperform the
models without constraints  the same tendency is observed on the spoken corpus except for
the model of mcdonald        which performs comparably with and without constraints 
we also wanted to establish which is the best constraint model  on both corpora we
find that the language model performs worst  whereas the significance model and mcdonald
perform comparably  i e   the f score differences are not statistically significant   to get
a feeling for the difficulty of the task  we calculated how much our annotators agreed in
their compression output  the inter annotator agreement  f score  on the written corpus
was       and on the spoken corpus        the agreement is higher on spoken texts since
they consists of many short utterances  e g   okay  thats it for now  good night  that can
   the term refers to the percentage of words retained from the source sentence in the compression 

   

ficlarke   lapata

source

the aim is to give councils some control over the future growth of second
homes 
gold
the aim is to give councils control over the growth of homes 
lm
the aim is to the future 
lm constr the aim is to give councils control 
sig
the aim is to give councils control over the future growth of homes 
sig constr the aim is to give councils control over the future growth of homes 
mcd
the aim is to give councils 
mcd constr the aim is to give councils some control over the growth of homes 
source
the clinton administration recently unveiled a new means to encourage
brownfields redevelopment in the form of a tax incentive proposal 
gold
the clinton administration unveiled a new means to encourage brownfields redevelopment in a tax incentive proposal 
lm
the clinton administration in the form of tax 
lm constr the clinton administration unveiled a means to encourage redevelopment in the form 
sig
the clinton administration unveiled a encourage brownfields redevelopment form tax proposal 
sig constr the clinton administration unveiled a means to encourage brownfields
redevelopment in the form of tax proposal 
mcd
the clinton unveiled a means to encourage brownfields redevelopment
in a tax incentive proposal 
mcd constr the clinton administration unveiled a means to encourage brownfields
redevelopment in the form of a incentive proposal 
table    example compressions produced by our systems  source  source sentence  gold 
gold standard compression  lm  language model compression  lm constr  language model compression with constraints  sig  significance model  sig constr 
significance model with constraints  mcd  mcdonalds        compression model 
mcd constr  mcdonalds        compression model with constraints  

be compressed only very little or not all  note that there is a marked difference between the
automatic and human compressions  our best performing systems are inferior to human
output by more than    f score percentage points 
differences between the automatic systems and the human output are also observed
with respect to the compression rate  as can be seen the language model compresses most
aggressively  whereas the significance model and mcdonald tend to be more conservative
and closer to the gold standard  interestingly  the constraints do not necessarily increase
the compression rate  the latter increases for the significance model but decreases for
the language model and remains relatively constant for mcdonald  it is straightforward to
impose the same compression rate for all constraint based models  e g   by forcing the model
p
to retain b tokens ni   i   b   however  we refrained from doing this since we wanted our
   

figlobal inference for sentence compression

models
lm
sig
mcd
lm constr
sig constr
mcd constr
gold

compr
    
    
    
    
    
    
    

f score
    
    
    
    
    
    


table    results on the written corpus  compression rate  compr  and grammatical relation f score  f score       constr model is significantly different from model
without constraints     significantly different from lm constr 
models
lm
sig
mcd
lm constr
sig constr
mcd constr
gold

compr
    
    
    
    
    
    
    

f score
    
    
    
    
    
    


table    results on the spoken corpus  compression rate  compr  and grammatical relation f score  f score       constr model is significantly different from without
constraints     significantly different from lm constr 

models to regulate the compression rate for each sentence individually according to its
specific information content and structure 
we next consider the results of our human study which assesses in more detail the quality
of the generated compressions on two dimensions  namely grammaticality and information
content  f score conflates these two dimensions and therefore in theory could unduly reward
a system that produces perfectly grammatical output without any information loss  tables  
and   show the mean ratings  for each system  and the gold standard  on the written and
spoken corpora  respectively  we first performed an analysis of variance  anova  to
examine the effect of different system compressions  the anova revealed a reliable effect
on both grammaticality and importance for each corpus  the effect was significant by both
subjects and items  p          
we next examine the impact of the constraints   constr in the tables   in most cases
we observe an increase in ratings for both grammaticality and importance when a model
is supplemented constraints  post hoc tukey tests reveal that the grammaticality and
importance ratings of the language model and significance model significantly improve with
   all statistical tests reported subsequently were done using the mean ratings 

   

ficlarke   lapata

models

grammar
     

importance

lm
sig
mcd

    

     
     
    

lm constr
sig constr
mcd constr
gold

    
    
    
    

     
    
    
    

     

table    results on the written text corpus  average grammaticality score  grammar  and
average importance score  importance  for human judgments      constr model
is significantly different from model without constraints     significantly different
from gold standard      significantly different from mcd constr 

models

grammar
     

importance

lm
sig
mcd

     
    

    
    
    

lm constr
sig constr
mcd constr
gold

    
    
    
    

     
    
    
    

table    results on the spoken text corpus  average grammaticality score  grammar  and
average importance score  importance  for human judgments      constr model
is significantly different from model without constraints     significantly different
from gold standard      significantly different from mcd constr 

the constraints            in contrast  mcdonalds system sees a numerical improvement
with the additional constraints  but this difference is not statistically significant  these
tendencies are observed on the spoken and written corpus 
upon closer inspection  we can see that the constraints influence considerably the
grammaticality of the unsupervised and semi supervised systems  tukey tests reveal that
lm constr and sig constr are as grammatical as mcd constr  in terms of importance 
sig constr and mcd constr are significantly better than lm constr            this
is not surprising given that lm constr is a very simple model without a mechanism for
highlighting important words in a sentence  interestingly  sig constr performs as well
as mcd constr in retaining the most important words  despite the fact that it requires
minimal supervision  although constraint based models overall perform better than models without constraints  they receive lower ratings  for grammaticality and importance  in
comparison to the gold standard  and the differences are significant in most cases 
   

figlobal inference for sentence compression

in summary  we observe that the constraints boost performance  this is more pronounced for compression models that are either unsupervised or use small amounts of
parallel data  for example  a simple model like sig yields performance comparable to
mcdonald        when constraints are taken into account  this is an encouraging result
suggesting that ilp can be used to create good compression models with relatively little
effort  i e   without extensive feature engineering or elaborate knowledge sources   performance gains are also obtained for competitive models like mcdonalds that are fully
supervised  but these gains are smaller  presumably because the initial model contains a
rich feature representation consisting of syntactic information and generally does a good job
at producing grammatical output  finally  our improvements are consistent across corpora
and evaluation paradigms 

   conclusions
in this paper we have presented a novel method for automatic sentence compression  a key
aspect of our approach is the use of integer linear programming for inferring globally optimal
compressions in the presence of linguistically motivated constraints  we have shown how
previous formulations of sentence compression can be recast as ilps and extended these
models with local and global constraints ensuring that the compressed output is structurally
and semantic well formed  contrary to previous work that has employed ilp solely for
decoding  our models integrate learning with inference in a unified framework 
our experiments have demonstrated the advantages of the approach  constraint based
models consistently bring performance gains over models without constraints  these improvements are more impressive for models that require little or no supervision  a case
in point here is the significance model discussed above  the no constraints incarnation of
this model performs poorly and considerably worse than mcdonalds        state of the art
model  the addition of constraints improves the output of this model so that its performance is indistinguishable from mcdonald  note that the significance model requires a
small amount of training data     parallel sentences   whereas mcdonald is trained on hundreds of sentences  it also presupposes little feature engineering  whereas mcdonald utilizes
thousands of features  some effort is associated with framing the constraints  however these
are created once and are applied across models and corpora  we have also observed small
performance gains for mcdonalds system when the latter is supplemented with constraints 
larger improvements are possible with more sophisticated constraints  however our intent
was to devise a set of general constraints that are not tuned to the mistakes of any specific
system in particular 
future improvements are many and varied  an obvious extension concerns our constraint set  currently our constraints are mostly syntactic and consider each sentence in
isolation  by incorporating discourse constraints we could highlight words that are important at the document level  presumably words topical in a document should be retained in
the compression  other constraints could manipulate the compression rate  for example 
we could encourage a higher compression rate for longer sentences  another interesting
direction includes the development of better objective functions for the compression task 
the objective functions presented so far rely on first or second order markov assumptions 
alternative objectives could take into account the structural similarity between the source
   

ficlarke   lapata

sentence and its target compression  or whether they share the same content which could
be operationalized in terms of entropy 
beyond the task and systems presented in this paper  we believe the approach holds
promise for other generation applications using decoding algorithms for searching the space
of possible outcomes  examples include sentence level paraphrasing  headline generation 
and summarization 

acknowledgments
we are grateful to our annotators vasilis karaiskos  beata kouchnir  and sarah luger 
thanks to jean carletta  frank keller  steve renals  and sebastian riedel for helpful
comments and suggestions and to the anonymous referees whose feedback helped to substantially improve the present paper  lapata acknowledges the support of epsrc  grant
gr t           a preliminary version of this work was published in the proceedings of
acl      

references
aho  a  v     ullman  j  d          syntax directed translations and the pushdown assembler  journal of computer and system sciences          
bangalore  s   rambow  o     whittaker  s          evaluation metrics for generation 
in proceedings of the first international conference on natural language generation 
pp      mitzpe ramon  israel 
barzilay  r     lapata  m          aggregation via set partitioning for natural language
generation  in proceedings of the human language technology conference of the
north american chapter of the association for computational linguistics  pp     
     new york  ny  usa 
bramsen  p   deshpande  p   lee  y  k     barzilay  r          inducing temporal graphs 
in proceedings of the      conference on empirical methods in natural language
processing  pp          sydney  australia 
briscoe  e  j     carroll  j          robust accurate statistical annotation of general text  in
proceedings of the third international conference on language resources and evaluation  pp            las palmas  gran canaria 
charniak  e          a maximum entropy inspired parser  in proceedings of the  st north
american annual meeting of the association for computational linguistics  pp     
     seattle  wa  usa 
clarke  j     lapata  m          models for sentence compression  a comparison across
domains  training requirements and evaluation measures  in proceedings of the   st
international conference on computational linguistics and   th annual meeting of
the association for computational linguistics  pp          sydney  australia 
clarkson  p     rosenfeld  r          statistical language modeling using the cmu
cambridge toolkit  in proceedings of eurospeech    pp            rhodes  greece 
   

figlobal inference for sentence compression

cormen  t  h   leiserson  c  e     rivest  r  l          intoduction to algorithms  the
mit press 
corston oliver  s          text compaction for display on very small screens  in proceedings of the workshop on automatic summarization at the  nd meeting of the north
american chapter of the association for computational linguistics  pp        pittsburgh  pa  usa 
crammer  k     singer  y          ultraconservative online algorithms for multiclass problems  journal of machine learning research            
dantzig  g  b          linear programming and extensions  princeton university press 
princeton  nj  usa 
denis  p     baldridge  j          joint determination of anaphoricity and coreference
resolution using integer programming  in human language technologies       the
conference of the north american chapter of the association for computational linguistics  proceedings of the main conference  pp          rochester  ny 
dras  m          tree adjoining grammar and the reluctant paraphrasing of text  ph d 
thesis  macquarie university 
galley  m     mckeown  k          lexicalized markov grammars for sentence compression 
in in proceedings of the north american chapter of the association for computational
linguistics  pp          rochester  ny  usa 
gomory  r  e          solving linear programming problems in integers  in bellman 
r     hall  m   eds    combinatorial analysis  proceedings of symposia in applied
mathematics  vol      providence  ri  usa 
grefenstette  g          producing intelligent telegraphic text reduction to provide an
audio scanning service for the blind  in hovy  e     radev  d  r   eds    proceedings
of the aaai symposium on intelligent text summarization  pp          stanford 
ca  usa 
hori  c     furui  s          speech summarization  an approach through word extraction
and a method for evaluation  ieice transactions on information and systems  e  d           
jing  h          sentence reduction for automatic text summarization  in proceedings of
the  th applied natural language processing conference  pp          seattle wa 
usa 
knight  k     marcu  d          summarization beyond sentence extraction  a probabilistic
approach to sentence compression  artificial intelligence                 
land  a  h     doig  a  g          an automatic method for solving discrete programming
problems  econometrica             
lin  c  y          improving summarization performance by sentence compression  a pilot
study  in proceedings of the  th international workshop on information retrieval with
asian languages  pp      sapporo  japan 
lin  d          latat  language and text analysis tools  in proceedings of the first human
language technology conference  pp          san francisco  ca  usa 
   

ficlarke   lapata

marciniak  t     strube  m          beyond the pipeline  discrete optimization in nlp  in
proceedings of the ninth conference on computational natural language learning 
pp          ann arbor  mi  usa 
mcdonald  r          discriminative sentence compression with soft syntactic constraints 
in proceedings of the   th conference of the european chapter of the association for
computational linguistics  trento  italy 
mcdonald  r   crammer  k     pereira  f       a   flexible text segmentation with structured multilabel classification  in proceedings of human language technology conference and conference on empirical methods in natural language processing  pp 
        vancouver  bc  canada 
mcdonald  r   crammer  k     pereira  f       b   online large margin training of dependency parsers  in   rd annual meeting of the association for computational
linguistics  pp        ann arbor  mi  usa 
nemhauser  g  l     wolsey  l  a          integer and combinatorial optimization  wileyinterscience series in discrete mathematicals and opitmization  wiley  new york  ny 
usa 
nguyen  m  l   shimazu  a   horiguchi  s   ho  t  b     fukushi  m          probabilistic
sentence reduction using support vector machines  in proceedings of the   th international conference on computational linguistics  pp          geneva  switzerland 
press  w  h   teukolsky  s  a   vetterling  w  t     flannery  b  p          numerical
recipes in c  the art of scientific computing  cambridge university press  new
york  ny  usa 
punyakanok  v   roth  d   yih  w     zimak  d          semantic role labeling via integer linear programming inference  in proceedings of the international conference on
computational linguistics  pp            geneva  switzerland 
riedel  s     clarke  j          incremental integer linear programming for non projective
dependency parsing  in proceedings of the      conference on empirical methods in
natural language processing  pp          sydney  australia 
riezler  s   king  t  h   crouch  r     zaenen  a          statistical sentence condensation
using ambiguity packing and stochastic disambiguation methods for lexical functional
grammar  in human language technology conference and the  rd meeting of the
north american chapter of the association for computational linguistics  pp     
     edmonton  canada 
roark  b          probabilistic top down parsing and language modeling  computational
linguistics                 
roth  d          learning to resolve natural language ambiguities  a unified approach  in
in proceedings of the   th of the american association for artificial intelligence  pp 
        madison  wi  usa 
roth  d     yih  w          a linear programming formulation for global inference in
natural language tasks  in proceedings of the annual conference on computational
natural language learning  pp      boston  ma  usa 
   

figlobal inference for sentence compression

roth  d     yih  w          integer linear programming inference for conditional random
fields  in proceedings of the international conference on machine learning  pp     
     bonn 
sarawagi  s     cohen  w  w          semi markov conditional random fields for information extraction  in advances in neural information processing systems  vancouver 
bc  canada 
shieber  s     schabes  y          synchronous tree adjoining grammars  in proceedings of the   th international conference on computational linguistics  pp         
helsinki  finland 
turner  j     charniak  e          supervised and unsupervised learning for sentence
compression  in proceedings of the   rd annual meeting of the association for computational linguistics  pp          ann arbor  mi  usa 
vandeghinste  v     pan  y          sentence compression for automated subtitling  a
hybrid approach  in marie francine moens  s  s   ed    text summarization branches
out  proceedings of the acl    workshop  pp        barcelona  spain 
williams  h  p          model building in mathematical programming   th edition   wiley 
winston  w  l     venkataramanan  m          introduction to mathematical programming  applications and algorithms   th edition   duxbury 
zajic  d   door  b  j   lin  j     schwartz  r          multi candidate reduction  sentence
compression as a tool for document summarization tasks  information processing
management special issue on summarization                   

   

fi