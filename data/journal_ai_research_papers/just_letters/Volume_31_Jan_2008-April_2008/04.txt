journal of artificial intelligence research                  

submitted        published      

creating relational data from unstructured and
ungrammatical data sources
matthew michelson
craig a  knoblock

michelso isi edu
knoblock isi edu

university of southern california
information sciences instistute
     admiralty way
marina del rey  ca       usa

abstract
in order for agents to act on behalf of users  they will have to retrieve and integrate
vast amounts of textual data on the world wide web  however  much of the useful data
on the web is neither grammatical nor formally structured  making querying difficult 
examples of these types of data sources are online classifieds like craigslist  and auction
item listings like ebay   we call this unstructured  ungrammatical data posts  the
unstructured nature of posts makes query and integration difficult because the attributes
are embedded within the text  also  these attributes do not conform to standardized values 
which prevents queries based on a common attribute value  the schema is unknown and
the values may vary dramatically making accurate search difficult  creating relational
data for easy querying requires that we define a schema for the embedded attributes and
extract values from the posts while standardizing these values  traditional information
extraction  ie  is inadequate to perform this task because it relies on clues from the data 
such as structure or natural language  neither of which are found in posts  furthermore 
traditional information extraction does not incorporate data cleaning  which is necessary to
accurately query and integrate the source  the two step approach described in this paper
creates relational data sets from unstructured and ungrammatical text by addressing both
issues  to do this  we require a set of known entities called a reference set  the first step
aligns each post to each member of each reference set  this allows our algorithm to define a
schema over the post and include standard values for the attributes defined by this schema 
the second step performs information extraction for the attributes  including attributes not
easily represented by reference sets  such as a price  in this manner we create a relational
structure over previously unstructured data  supporting deep and accurate queries over the
data as well as standard values for integration  our experimental results show that our
technique matches the posts to the reference set accurately and efficiently and outperforms
state of the art extraction systems on the extraction task from posts 

   introduction
the future vision of the web includes computer agents searching for information  making
decisions and taking actions on behalf of human users  for instance  an agent could query
a number of data sources to find the lowest price for a given car and then email the user
the car listing  along with directions to the seller and available appointments to see the car 
   www craigslist org
   www ebay com
c
    
ai access foundation  all rights reserved 

fimichelson   knoblock

this requires the agent to contain two data gathering mechanisms  the ability to query
sources and the ability to integrate relevant sources of information 
however  these data gathering mechanisms assume that the sources themselves are designed to support relational queries  such as having well defined schema and standard values
for the attributes  yet this is not always the case  there are many data sources on the
world wide web that would be useful to query  but the textual data within them is unstructured and is not designed to support querying  we call the text of such data sources
posts  examples of posts include the text of ebay auction listings  internet classifieds
like craigslist  bulletin boards such as bidding for travel    or even the summary text below
the hyperlinks returned after querying google  as a running example  consider the three
posts for used car classifieds shown in table   

table    three posts for honda civics from craigslist
craigslist post
   civic  speed runs great obo  ri       
     dr honda civc lx stick shift      
   del sol si vtec  glendale       

the current method to query posts  whether by an agent or a person  is keyword search 
however  keyword search is inaccurate and cannot support relational queries  for example 
a difference in spelling between the keyword and that same attribute within a post would
limit that post from being returned in the search  this would be the case if a user searched
the example listings for civic since the second post would not be returned  another factor
which limits keyword accuracy is the exclusion of redundant attributes  for example  some
classified posts about cars only include the car model  and not the make  since the make is
implied by the model  this is shown in the first and third post of table    in these cases 
if a user does a keyword search using the make honda  these posts will not be returned 
moreover  keyword search is not a rich query framework  for instance  consider the
query  what is the average price for all hondas from      or later  to do this with
keyword search requires a user to search on honda and retrieve all that are from     
or later  then the user must traverse the returned set  keeping track of the prices and
removing incorrectly returned posts 
however  if a schema with standardized attribute values is defined over the entities in
the posts  then a user could run the example query using a simple sql statement and
do so accurately  addressing both problems created by keyword search  the standardized
attribute values ensure invariance to issues such as spelling differences  also  each post is
associated with a full schema with values  so even though a post might not contain a car
make  for instance  its schema does and has the correct value for it  so it will be returned
in a query on car makes  furthermore  these standardized values allow for integration of
the source with outside sources  integrating sources usually entails joining the two sources
directly on attributes or translations of the attributes  without standardized values and
   www biddingfortravel com

   

firelational data from unstructured data sources

a schema  it would not be possible to link these ungrammatical and unstructured data
sources with outside sources  this paper addresses the problem of adding a schema with
standardized attributes over the set of posts  creating a relational data set that can support
deep and accurate queries 
one way to create a relational data set from the posts is to define a schema and
then fill in values for the schema elements using techniques such as information extraction  this is sometimes called semantic annotation  for example  taking the second
post of table   and semantically annotating it might yield      dr honda civc lx stick
shift        make honda   make   model civc   model   trim  dr lx   trim 
 year        year   price        price   however  traditional information extraction  relies on grammatical and structural characteristics of the text to identify the attributes
to extract  yet posts by definition are not structured or grammatical  therefore  wrapper
extraction technologies such as stalker  muslea  minton    knoblock        or roadrunner
 crescenzi  mecca    merialdo        cannot exploit the structure of the posts  nor are
posts grammatical enough to exploit natural language processing  nlp  based extraction
techniques such as those used in whisk  soderland        or rapier  califf   mooney 
      
beyond the difficulties in extracting the attributes within a post using traditional extraction methods  we also require that the values for the attributes are standardized  which
is a process known as data cleaning  otherwise  querying our newly relational data would
be inaccurate and boil down to keyword search  for instance  using the annotation above 
we would still need to query where the model is civc to return this record  traditional
extraction does not address this 
however  most data cleaning algorithms assume that there are tuple to tuple transformations  lee  ling  lu    ko        chaudhuri  ganjam  ganti    motwani         that
is  there is some function that maps the attributes of one tuple to the attributes of another  this approach would not work on ungrammatical and unstructured data  where all
the attributes are embedded within the post  which maps to a set of attributes from the
reference set  therefore we need to take a different approach to the problems of figuring
out the attributes within a post and cleaning them 
our approach to creating relational data sets from unstructured and ungrammatical
posts exploits reference sets  a reference set consists of collections of known entities
with the associated  common attributes  a reference set can be an online  or offline  set
of reference documents  such as the cia world fact book   it can also be an online  or
offline  database  such as the comics price guide   with the semantic web one can envision
building reference sets from the numerous ontologies that already exist  using standardized
ontologies to build reference sets allows a consensus agreement upon reference set values 
which implies higher reliability for these reference sets over others that might exist as one
experts opinion  using our car example  a reference set might be the edmunds car buying
guide    which defines a schema for cars as well as standard values for attributes such as
the model and the trim  in order to construct reference sets from web sources  such as the
   http   www cia gov cia publications factbook 
   www comicspriceguide com
   www edmunds com

   

fimichelson   knoblock

edmunds car buying guide  we use wrapper technologies  agent builder  in this case  to
scrape data from the web source  using the schema that the source defines for the car 
to use a reference set to build a relational data set we exploit the attributes in the
reference set to determine the attributes from the post that can be extracted  the first step
of our algorithm finds the best matching member of the reference set for the post  this is
called the record linkage step  by matching a post to a member of the reference set we
can define schema elements for the post using the schema of the reference set  and we can
provide standard attributes for these attributes by using the attributes from the reference
set when a user queries the posts 
next  we perform information extraction to extract the actual values in the post that
match the schema elements defined by the reference set  this step is the information
extraction step  during the information extraction step  the parts of the post are extracted
that best match the attribute values from the reference set member chosen during the
record linkage step  in this step we also extract attributes that are not easily represented
by reference sets  such as prices or dates  although we already have the schema and
standardized attributes required to create a relational data set over the posts  we still
extract the actual attributes embedded within the post so that we can more accurately
learn to extract the attributes not represented by a reference set  such as prices and dates 
while these attributes can be extracted using regular expressions  if we extract the actual
attributes within the post we might be able to do so more accurately  for example  consider
the ford     car  without actually extracting the attributes within a post  we might
extract     as a price  when it is actually a car name  our overall approach is outlined
in figure   
although we previously describe a similar approach to semantically annotating posts
 michelson   knoblock         this paper extends that research by combining the annotation with our work on more scalable record matching  michelson   knoblock         not
only does this make the matching step for our annotation more scalable  it also demonstrates
that our work on efficient record matching extends to our unique problem of matching posts 
with embedded attributes  to structured  relational data  this paper also presents a more
detailed description than our past work  including a more thorough evaluation of the procedure than previously  using larger experimental data sets including a reference set that
includes tens of thousands of records 
this article is organized as follows  we first describe our algorithm for aligning the
posts to the best matching members of the reference set in section    in particular  we
show how this matching takes place  and how we efficiently generate candidate matches
to make the matching procedure more scalable  in section    we demonstrate how to
exploit the matches to extract the attributes embedded within the post  we present some
experiments in section    validating our approaches to blocking  matching and information
extraction for unstructured and ungrammatical text  we follow with a discussion of these
results in section   and then present related work in section    we finish with some final
thoughts and conclusions in section   

   a product of fetch technologies http   www fetch com products asp

   

firelational data from unstructured data sources

figure    creating relational data from unstructured sources

   aligning posts to a reference set
to exploit the reference set attributes to create relational data from the posts  the algorithm needs to first decide which member of the reference set best matches the post  this
matching  known as record linkage  fellegi   sunter         provides the schema and attribute values necessary to query and integrate the unstructured and ungrammatical data
source  record linkage can be broken into two steps  generating candidate matches  called
blocking  and then separating the true matches from these candidates in the matching
step 
in our approach  the blocking generates candidate matches based on similarity methods
over certain attributes from the reference set as they compare to the posts  for our cars
example  the algorithm may determine that it can generate candidates by finding common
tokens between the posts and the make attribute of the reference set  this step is detailed
in section     and is crucial in limiting the number of candidates matches we later examine
during the matching step  after generating candidates  the algorithm generates a large set
of features between each post and its candidate matches from the reference set  using these
features  the algorithm employs machine learning methods to separate the true matches
from the false positives generated during blocking  this matching is detailed in section     
   

fimichelson   knoblock

    generating candidates by learning blocking schemes for record linkage
it is infeasible to compare each post to all of the members of a reference set  therefore a
preprocessing step generates candidate matches by comparing all the records between the
sets using fast  approximate methods  this is called blocking because it can be thought of
as partitioning the full cross product of record comparisons into mutually exclusive blocks
 newcombe         that is  to block on an attribute  first we sort or cluster the data sets
by the attribute  then we apply the comparison method to only a single member of a block 
after blocking  the candidate matches are examined in detail to discover true matches 
there are two main goals of blocking  first  blocking should limit the number of candidate matches  which limits the number of expensive  detailed comparisons needed during
record linkage  second  blocking should not exclude any true matches from the set of candidate matches  this means there is a trade off between finding all matching records and
limiting the size of the candidate matches  so  the overall goal of blocking is to make the
matching step more scalable  by limiting the number of comparisons it must make  while
not hindering its accuracy by passing as many true matches to it as possible 
most blocking is done using the multi pass approach  hernandez   stolfo         which
combines the candidates generated during independent runs  for example  with our cars
data  we might make one pass over the data blocking on tokens in the car model  while
another run might block using tokens of the make along with common tokens in the trim
values  one can view the multi pass approach as a rule in disjunctive normal form  where
each conjunction in the rule defines each run  and the union of these rules combines the
candidates generated during each run  using our example  our rule might become   tokenmatch  model     token match  year      token match  make     the effectiveness of the
multi pass approach hinges upon which methods and attributes are chosen in the conjunctions 
note that each conjunction is a set of  method  attribute  pairs  and we do not make
restrictions on which methods can be used  the set of methods could include full string
metrics such as cosine similarity  simple common token matching as outlined above  or even
state of the art n gram methods as shown in our experiments  the key for methods is not
necessarily choosing the fastest  though we show how to account for the method speed
below   but rather choosing the methods that will generate the smallest set of candidate
matches that still cover the true positives  since it is the matching step that will consume
the most time 
therefore  a blocking scheme should include enough conjunctions to cover as many true
matches as it can  for example  the first conjunct might not cover all of the true matches
if the datasets being compared do not overlap in all of the years  so the second conjunct
can cover the rest of the true matches  this is the same as adding more independent runs
to the multi pass approach 
however  since a blocking scheme includes as many conjunctions as it needs  these
conjunctions should limit the number of candidates they generate  for example  the second
conjunct is going to generate a lot of unnecessary candidates since it will return all records
that share the same make  by adding more  method  attribute  pairs to a conjunction  we
can limit the number of candidates it generates  for example  if we change   token match 
   

firelational data from unstructured data sources

make   to   token match  make    token match  trim   we still cover new true matches 
but we generate fewer additional candidates 
therefore effective blocking schemes should learn conjunctions that minimize the false
positives  but learn enough of these conjunctions to cover as many true matches as possible  these two goals of blocking can be clearly defined by the reduction ratio and pairs
completeness  elfeky  verykios    elmagarmid        
the reduction ratio  rr  quantifies how well the current blocking scheme minimizes
the number of candidates  let c be the number of candidate matches and n be the size of
the cross product between both data sets 
rr      c n
it should be clear that adding more  method attribute  pairs to a conjunction increases
its rr  as when we changed   token match  zip   to   token match  zip    token match 
first name   
pairs completeness  pc  measures the coverage of true positives  i e   how many of the
true matches are in the candidate set versus those in the entire set  if sm is the number of
true matches in the candidate set  and nm is the number of matches in the entire dataset 
then 
p c   sm  nm
adding more disjuncts can increase our pc  for example  we added the second conjunction to our example blocking scheme because the first did not cover all of the matches 
the blocking approach in this paper  blocking scheme learner  bsl   learns effective
blocking schemes in disjunctive normal form by maximizing the reduction ratio and pairs
completeness  in this way  bsl tries to maximize the two goals of blocking  previously we
showed bsl aided the scalability of record linkage  michelson   knoblock         and this
paper extends that idea by showing that it also can work in the case of matching posts to
the reference set records 
the bsl algorithm uses a modified version of the sequential covering algorithm  sca  
used to discover disjunctive sets of rules from labeled training data  mitchell         in
our case  sca will learn disjunctive sets of conjunctions consisting of  method  attribute 
pairs  basically  each call to learn one rule generates a conjunction  and bsl keeps
iterating over this call  covering the true matches left over after each iteration  this way
sca learns a full blocking scheme  the bsl algorithm is shown in table   
there are two modifications to the classic sca algorithm  which are shown in bold 
first  bsl runs until there are no more examples left to cover  rather than stopping at
some threshold  this ensures that we maximize the number of true matches generated as
candidates by the final blocking rule  pairs completeness   note that this might  in turn 
yield a large number of candidates  hurting the reduction ratio  however  omitting true
matches directly affects the accuracy of record linkage  and blocking is a preprocessing step
for record linkage  so it is more important to cover as many true matches as possible  this
way bsl fulfills one of the blocking goals  not eliminating true matches if possible  second 
if we learn a new conjunction  in the learn one rule step  and our current blocking
scheme has a rule that already contains the newly learned rule  then we can remove the
rule containing the newly learned rule  this is an optimization that allows us to check rule
containment as we go  rather than at the end 
   

fimichelson   knoblock

table    modified sequential covering algorithm
sequential covering class  attributes  examples 
learnedrules    
rule  learn one rule  class  attributes  examples 
while examples left to cover  do
learnedrules  learnedrules  rule
examples  examples    examples covered by rule 
rule  learn one rule  class  attributes  examples 
if rule contains any previously learned rules  remove these
contained rules 
return learnedrules

the rule containment is possible because we can guarantee that we learn less restrictive
rules as we go  we can prove this guarantee as follows  our proof is done by contradiction 
assume we have two attributes a and b  and a method x  also  assume that our previously
learned rules contain the following conjunction    x  a   and we currently learned the rule
  x  a   x  b    that is  we assume our learned rules contains a rule that is less
specific than the currently learned rule  if this were the case  then there must be at least
one training example covered by   x  a   x  b   that is not covered by   x  a    since
sca dictates that we remove all examples covered by   x  a   when we learn it  clearly 
this cannot happen  since any examples covered by the more specific   x  a   x  b  
would have been covered by   x  a   already and removed  which means we could not have
learned the rule   x  a   x  b    thus  we have a contradiction 
as we stated before  the two main goals of blocking are to minimize the size of the candidate set  while not removing any true matches from this set  we have already mentioned
how bsl maximizes the number of true positives in the candidate set and now we describe
how bsl minimizes the overall size of the candidate set  which yields more scalable record
linkage  to minimize the candidate sets size  we learn as restrictive a conjunction as we
can during each call to learn one rule during the sca  we define restrictive as minimizing the number of candidates generated  as long as a certain number of true matches are
still covered   without this restriction  we could learn conjunctions that perfectly minimize
the number of candidates  they simply return none  
to do this  the learn one rule step performs a general to specific beam search  it
starts with an empty conjunction and at each step adds the  method  attribute  pair that
yields the smallest set of candidates that still cover at least a set number of true matches 
that is  we learn the conjunction that maximizes the reduction ratio  while at the same
time covering a minimum value of pairs completeness  we use a beam search to allow for
some backtracking  since the search is greedy  however  since the beam search goes from
general to specific  we can ensure that the final rule is as restrictive as possible  the full
learn one rule is given in table   
the constraint that a conjunction has a minimum pc ensures that the learned conjunction does not over fit to the data  without this restriction  it would be possible for
learn one rule to learn a conjunction that returns no candidates  uselessly producing
an optimal rr 
   

firelational data from unstructured data sources

the algorithms behavior is well defined for the minimum pc threshold  consider 
the case where the algorithm is learning as restrictive a rule as it can with the minimum
coverage  in this case  the parameter ends up partitioning the space of the cross product of
example records by the threshold amount  that is  if we set the threshold amount to    
of the examples covered  the most restrictive first rule covers     of the examples  the
next rule covers     of what is remaining  which is     of the examples  the next will
cover       of the examples  etc  in this sense  the parameter is well defined  if we set the
threshold high  we will learn fewer  less restrictive conjunctions  possibly limiting our rr 
although this may increase pc slightly  if we set it lower  we cover more examples  but we
need to learn more conjuncts  these newer conjuncts  in turn  may be subsumed by later
conjuncts  so they will be a waste of time to learn  so  as long as this parameter is small
enough  it should not affect the coverage of the final blocking scheme  and smaller than that
just slows down the learning  we set this parameter to     for our experiments   
now we analyze the running time of bsl and we show how bsl can take into account
the running time of different blocking methods  if need be  assume that we have x  method 
attribute  pairs such as  token  f irst  name   now  assume that our beam size is b  since
we use general to specific beam search in our learn one rule procedure  also  for the time
being  assume each  method  attribute  pair can generate its blocking candidates in o   
time   we relax this assumption later   each time we hit learn one rule within bsl  we
will try all rules in the beam with all of the  attribute  method  pairs not in the current
beam rules  so  in the worst case  this takes o bx  each time  since for each  method 
attribute  pair in the beam  we try it against all other  method  attribute  pairs  now  in
the worst case  each learned disjunct would only cover   training example  so our rule is
a disjunction of all pairs x  therefore  we run the learn one rule x times  resulting in a
learning time of o bx     if we have e training examples  the full training time is o ebx    
for bsl to learn the blocking scheme 
now  while we assumed above that each  method  attribute  runs in o    time  this is
clearly not the case  since there is a substantial amount of literature on blocking methods and
   setting this parameter lower than     had an insignificant effect on our results  and setting it much
higher  to      only increased the pc by a small amount  if at all   while decreasing the rr 

table    learning a conjunction of  method  attribute  pairs
learn one rule  attributes  examples  min thresh  k 
best conjunction    
candidate conjunctions  all  method  attribute  pairs
while candidate conjunctions not empty  do
for each ch  candidate conjunctions
if not first iteration
ch  ch   method attribute 
remove any ch that are duplicates  inconsistent or not max  specific
if reduction ratio ch    reduction ratio best conjunction 
and pairs completeness ch   min thresh
best conjunction  ch
candidate conjunctions  best k members of candidate conjunctions
return best conjunction

   

fimichelson   knoblock

further the blocking times can vary significantly  bilenko  kamath    mooney         let
us define a function tx  e  that represents how long it takes for a single  method  attribute 
pair in x to generate the e candidates in our training example  using this notation  our
learn one rule time becomes o b xtx  e     we run tx  e  time for each pair in x  and so our
full training time becomes o eb xtx  e       clearly such a running time will be dominated
by the most expensive blocking methodology  once a rule is learned  it is bounded by the
time it takes to run the rule and  method  attribute  pairs involved  so it takes o xtx  n   
where n is the number of records we are classifying 
from a practical standpoint  we can easily modify bsl to account for the time it takes
certain blocking methods to generate their candidates  in the learn one rule step  we
change the performance metric to reflect both reduction ratio and blocking time as a
weighted average  that is  given wrr as the weight for reduction ratio and wb as the
weight for the blocking time  we modify learn one rule to maximize the performance of
any disjunct based on this weighted average  table   shows the modified version of learnone rule  and the changes are shown in bold 

table    learning a conjunction of  method  attribute  pairs using weights
learn one rule  attributes  examples  min thresh  k 
best conj    
candidate conjunctions  all  method  attribute  pairs
while candidate conjunctions not empty  do
for each ch  candidate conjunctions
if not first iteration
ch  ch   method attribute 
remove any ch that are duplicates  inconsistent or not max  specific
score ch    wrr reduction ratio ch  wb block time ch 
score best conj    wrr reduction ratio best conj  wb block time best conj 
if score ch    score best conj 
and pairs completeness ch   min thresh
best conj  ch
candidate conjunctions  best k members of candidate conjunctions
return best conj

note that when we set wb to    we are using the same version of learn one rule
as used throughout this paper  where we only consider the reduction ratio  since our
methods  token and n gram match  are simple to compute  requiring more time to build
the initial index than to do the candidate generation  we can safely set wb to    also 
making this trade off of time versus reduction might not always be an appropriate decision 
although a method may be fast  if it does not sufficiently reduce the reduction ratio  then
the time it takes the record linkage step might increase more than the time it would have
taken to run the blocking using a method that provides a larger increase in reduction ratio 
since classification often takes much longer than candidate generation  the goal should be
to minimize candidates  maximize reduction ratio   which in turn minimizes classification
time  further  the key insight of bsl is not only that we choose the blocking method 
but more importantly that we choose the appropriate attributes to block on  in this sense 
bsl is more like a feature selection algorithm than a blocking method  as we show in our
   

firelational data from unstructured data sources

experiments  for blocking it is more important to pick the right attribute combinations  as
bsl does  even using simple methods  than to do blocking using the most sophisticated
methods 
we can easily extend our bsl algorithm to handle the case of matching posts to members
of the reference set  this is a special case because the posts have all the attributes embedded
within them while the reference set data is relational and structured into schema elements 
to handle this special case  rather than matching attribute and method pairs across the
data sources during our learn one rule  we instead compare attribute and method
pairs from the relational data to the entire post  this is a small change  showing that the
same algorithm works well even in this special case 
once we learn a good blocking scheme  we can now efficiently generate candidates from
the post set to align to the reference set  this blocking step is essential for mapping large
amounts of unstructured and ungrammatical data sources to larger and larger reference
sets 
    the matching step
from the set of candidates generated during blocking one can find the member of the
reference set that best matches the current post  that is  one data sources record  the
post  must align to a record from the other data source  the reference set candidates  
while the whole alignment procedure is referred to as record linkage  fellegi   sunter 
       we refer to finding the particular matches after blocking as the matching step 

figure    the traditional record linkage problem
however  the record linkage problem presented in this article differs from the traditional
record linkage problem and is not well studied  traditional record linkage matches a record
from one data source to a record from another data source by relating their respective 
decomposed attributes  for instance  using the second post from table    and assuming
decomposed attributes  the make from the post is compared to the make of the reference
   

fimichelson   knoblock

figure    the problem of matching a post to the reference set
set  this is also done for the models  the trims  etc  the record from the reference set that
best matches the post based on the similarities between the attributes would be considered
the match  this is represented in figure    yet  the attributes of the posts are embedded
within a single piece of text and not yet identified  this text is compared to the reference
set  which is already decomposed into attributes and which does not have the extraneous
tokens present in the post  figure   depicts this problem  with this type of matching
traditional record linkage approaches do not apply 
instead  the matching step compares the post to all of the attributes of the reference set
concatenated together  since the post is compared to a whole record from the reference set
 in the sense that it has all of the attributes   this comparison is at the record level and
it approximately reflects how similar all of the embedded attributes of the post are to all of
the attributes of the candidate match  this mimics the idea of traditional record linkage 
that comparing all of the fields determines the similarity at the record level 
however  by using only the record level similarity it is possible for two candidates to
generate the same record level similarity while differing on individual attributes  if one of
these attributes is more discriminative than the other  there needs to be some way to reflect
that  for example  consider figure    in the figure  the two candidates share the same make
and model  however  the first candidate shares the year while the second candidate shares
the trim  since both candidates share the same make and model  and both have another
attribute in common  it is possible that they generate the same record level comparison  yet 
a trim on car  especially with a rare thing like a hatchback should be more discriminative
than sharing a year  since there are lots of cars with the same make  model and year  that
differ only by the trim  this difference in individual attributes needs to be reflected 
to discriminate between attributes  the matching step borrows the idea from traditional
record linkage that incorporating the individual comparisons between each attribute from
   

firelational data from unstructured data sources

figure    two records with equal record level but different field level similarities

each data source is the best way to determine a match  that is  just the record level
information is not enough to discriminate matches  field level comparisons must be exploited
as well  to do field level comparisons the matching step compares the post to each
individual attribute of the reference set 
these record and field level comparisons are represented by a vector of different similarity functions called rl scores  by incorporating different similarity functions  rl scores
reflects the different types of similarity that exist between text  hence  for the record level
comparison  the matching step generates the rl scores vector between the post and all of
the attributes concatenated  to generate field level comparisons  the matching step calculates the rl scores between the post and each of the individual attributes of the reference
set  all of these rl scores vectors are then stored in a vector called vrl   once populated 
vrl represents the record and field level similarities between a post and a member of the
reference set 
in the example reference set from figure    the schema has   attributes  make  model 
trim  year    assuming the current candidate is  honda  civic   d lx        
then the vrl looks like 

vrl   rl
rl
rl
rl
rl

scores post 
scores post 
scores post 
scores post 
scores post 

honda  
civic  
 d lx  
      
honda civic  d lx       

or more generally 
   

fimichelson   knoblock

vrl   rl scores post 
rl scores post 
    
rl scores post 
rl scores post 

attribute    
attribute    
attributen   
attribute  attribute        attributen   

the rl scores vector is meant to include notions of the many ways that exist to define
the similarity between the textual values of the data sources  it might be the case that
one attribute differs from another in a few misplaced  missing or changed letters  this sort
of similarity identifies two attributes that are similar  but misspelled  and is called edit
distance  another type of textual similarity looks at the tokens of the attributes and
defines similarity based upon the number of tokens shared between the attributes  this
token level similarity is not robust to spelling mistakes  but it puts no emphasis on the
order of the tokens  whereas edit distance requires that the order of the tokens match in
order for the attributes to be similar  lastly  there are cases where one attribute may sound
like another  even if they are both spelled differently  or one attribute may share a common
root word with another attribute  which implies a stemmed similarity  these last two
examples are neither token nor edit distance based similarities 
to capture all these different similarity types  the rl scores vector is built of three vectors that reflect the each of the different similarity types discussed above  hence  rl scores
is 
rl scores post  attribute   token scores post  attribute  
edit scores post  attribute  
other scores post  attribute  
the vector token scores comprises three token level similarity scores  two similarity
scores included in this vector are based on the jensen shannon distance  which defines
similarities over probability distributions of the tokens  one uses a dirichlet prior  cohen 
ravikumar    feinberg        and the other smooths its token probabilities using a jelenikmercer mixture model  zhai   lafferty         the last metric in the token scores vector
is the jaccard similarity 
with all of the scores included  the token scores vector takes the form 
token scores post  attribute   jensen shannon dirichlet post  attribute  
jensen shannon jm mixture post  attribute  
jaccard post  attribute  
the vector edit scores consists of the edit distance scores which are comparisons between
strings at the character level defined by operations that turn one string into another  for
instance  the edit scores vector includes the levenshtein distance  levenshtein         which
returns the minimum number of operations to turn string s into string t  and the smithwaterman distance  smith   waterman        which is an extension to the levenshtein
distance  the last score in the vector edit scores is the jaro winkler similarity  winkler
  thibaudeau         which is an extension of the jaro metric  jaro        used to find
similar proper nouns  while not a strict edit distance  because it does not regard operations
of transformations  the jaro winkler metric is a useful determinant of string similarity 
with all of the character level metrics  the edit scores vector is defined as 
   

firelational data from unstructured data sources

edit scores post  attribute   levenshtein post  attribute  
smith waterman post  attribute  
jaro winkler post  attribute  
all the similarities in the edit scores and token scores vector are defined in the secondstring package  cohen et al         which was used for the experimental implementation
as described in section   
lastly  the vector other scores captures the two types of similarity that did not fit into
either the token level or edit distance similarity vector  this vector includes two types
of string similarities  the first is the soundex score between the post and the attribute 
soundex uses the phonetics of a token as a basis for determining the similarity  that
is  misspelled words that sound the same will receive a high soundex score for similarity 
the other similarity is based upon the porter stemming algorithm  porter         which
removes the suffixes from strings so that the root words can be compared for similarity 
this helps alleviate possible errors introduced by the prefix assumption introduced by the
jaro winkler metric  since the stems are scored rather than the prefixes  including both of
these scores  the other scores vector becomes 
other scores post  attribute   porter stemmer post  attribute  
soundex post  attribute  

figure    the full vector of similarity scores used for record linkage
figure   shows the full composition of vrl   with all the constituent similarity scores 
once a vrl is constructed for each of the candidates  the matching step then performs
a binary rescoring on each vrl to further help determine the best match amongst the candidates  this rescoring helps determine the best possible match for the post by separating
   

fimichelson   knoblock

out the best candidate as much as possible  because there might be a few candidates with
similarly close values  and only one of them is a best match  the rescoring emphasizes the
best match by downgrading the close matches so that they have the same element values as the more obvious non matches  while boosting the difference in score with the best
candidates elements 
to rescore the vectors of candidate set c  the rescoring method iterates through the
elements xi of all vrl c  and the vrl  s  that contain the maximum value for xi map this
xi to    while all of the other vrl  s  map xi to    mathematically  the rescoring method is 
vrlj  c  j         c 
fi
fi
fi
fi
xi  vrlj   i        fivrlj fi
 

f  xi   vrlj    

   xi   max xt  vrls   vrls  c  t   i  s         c  
   otherwise

for example  suppose c contains   candidates  vrl  and vrl   
vrl                                        
vrl                                        
after rescoring they become 
vrl                             
vrl                             
after rescoring  the matching step passes each vrl to a support vector machine  svm 
 joachims        trained to label them as matches or non matches  the best match is the
candidate that the svm classifies as a match  with the maximally positive score for the
decision function  if more than one candidate share the same maximum score from the
decision function  then they are thrown out as matches  this enforces a strict     mapping
between posts and members of the reference set  however  a   n relationship can be captured
by relaxing this restriction  to do this the algorithm keeps either the first candidate with
the maximal decision score  or chooses one randomly from the set of candidates with the
maximum decision score 
although we use svms in this paper to differentiate matches from non matches  the
algorithm is not strictly tied to this method  the main characteristics for our learning
problem are that the feature vectors are sparse  because of the binary rescoring  and the
concepts are dense  since many useful features may be needed and thus none should be
pruned by feature selection   we also tried to use a nave bayes classifier for our matching
task  but it was monumentally overwhelmed by the number of features and the number
of training examples  yet this is not to say that other methods that can deal with sparse
feature vectors and dense concepts  such as online logistic regression or boosting  could not
be used in place of svm 
after the match for a post is found  the attributes of the matching reference set member
are added as annotation to the post by including the values of the reference set attributes
with tags that reflect the schema of the reference set  the overall matching algorithm is
shown in figure   
   

firelational data from unstructured data sources

figure    our approach to matching posts to records from a reference set
in addition to providing a standardized set of values to query the posts  these standardized values allow for integration with outside sources because the values can be standardized
to canonical values  for instance  if we want to integrate our car classifieds with a safety
ratings website  we can now easily join the sources across the attribute values  in this
manner  by approaching annotation as a record linkage problem  we can create relational
data from unstructured and ungrammatical data sources  however  to aid in the extraction
of attributes not easily represented in reference sets  we perform information extraction on
the posts as well 

   extracting data from posts
although the record linkage step creates most of the relational data from the posts  there
are still attributes we would like to extract from the post which are not easily represented
by reference sets  which means the record linkage step can not be used for these attributes 
examples of such attributes are dates and prices  although many of these such attributes
can be extracted using simple techniques  such as regular expressions  we can make their
extraction and annotation ever more accurate by using sophisticated information extraction 
to motivate this idea  consider the ford car model called the      if we just used regular
expressions  we might extract     as the price of the car  but this would not be the case 
however  if we try to extract all of the attributes  including the model  then we would
extract     as the model correctly  furthermore  we might want to extract the actual
attributes from a post  as they are  and our extraction algorithm allows this 
to perform extraction  the algorithm infuses information extraction with extra knowledge  rather than relying on possibly inconsistent characteristics  to garner this extra
   

fimichelson   knoblock

knowledge  the approach exploits the idea of reference sets by using the attributes from
the matching reference set member as a basis for identifying similar attributes in the post 
then  the algorithm can label these extracted values from the post with the schema from
the reference set  thus adding annotation based on the extracted values 
in a broad sense  the algorithm has two parts  first we label each token with a possible
attribute label or as junk to be ignored  after all the tokens in a post are labeled  we
then clean each of the extracted labels  figure   shows the whole procedure graphically 
in detail  using the second post from table    each of the steps shown in this figure are
described in detail below 

figure    extraction process for attributes
to begin the extraction process  the post is broken into tokens  using the first post
from table   as an example  set of tokens becomes       civic   speed       each of
these tokens is then scored against each attribute of the record from the reference set that
was deemed the match 
to score the tokens  the extraction process builds a vector of scores  vie   like the vrl
vector of the matching step  vie is composed of vectors which represent the similarities
between the token and the attributes of the reference set  however  the composition of
vie is slightly different from vrl   it contains no comparison to the concatenation of all
the attributes  and the vectors that compose vie are different from those that compose
vrl   specifically  the vectors that form vie are called ie scores  and are similar to the
   

firelational data from unstructured data sources

rl scores that compose vrl   except they do not contain the token scores component  since
each ie scores only uses one token from the post at a time 
the rl scores vector 
rl scores post  attribute   token scores post  attribute  
edit scores post  attribute  
other scores post  attribute  
becomes 
ie scores token  attribute   edit scores token  attribute  
other scores token  attribute  
the other main difference between vie and vrl is that vie contains a unique vector
that contains user defined functions  such as regular expressions  to capture attributes that
are not easily represented by reference sets  such as prices or dates  these attribute types
generally exhibit consistent characteristics that allow them to be extracted  and they are
usually infeasible to represent in reference sets  this makes traditional extraction methods
a good choice for these attributes  this vector is called common scores because the types
of characteristics used to extract these attributes are common enough between to be used
for extraction 
using the first post of table    assume the reference set match has the make honda 
the model civic and the year       this means the matching tuple would be  honda 
civic         this match generates the following vie for the token civic of the post 
vie   common scores civic  
ie scores civic honda  
ie scores civic civic  
ie scores civic       
more generally  for a given token  vie looks like 
vie   common scores token  
ie scores token  attribute    
ie scores token  attribute   
    
ie scores token  attributen   
each vie is then passed to a structured svm  tsochantaridis  joachims  hofmann 
  altun        tsochantaridis  hofmann  joachims    altun        trained to give it an
attribute type label  such as make  model  or price  intuitively  similar attribute types
should have similar vie vectors  the makes should generally have high scores against the
make attribute of the reference set  and small scores against the other attributes  further 
structured svms are able to infer the extraction labels collectively  which helps in deciding
between possible token labels  this makes the use of structured svms an ideal machine
learning method for our task  note that since each vie is not a member of a cluster where
the winner takes all  there is no binary rescoring 
since there are many irrelevant tokens in the post that should not be annotated  the svm
learns that any vie that does associate with a learned attribute type should be labeled as
   

fimichelson   knoblock

junk  which can then be ignored  without the benefits of a reference set  recognizing junk
is difficult because the characteristics of the text in the posts are unreliable  for example  if
extraction relies solely on capitalization and token location  the junk phrase great deal
might be annotated as an attribute  many traditional extraction systems that work in
the domain of ungrammatical and unstructured text  such as addresses and bibliographies 
assume that each token of the text must be classified as something  an assumption that
cannot be made with posts 
nonetheless  it is possible that a junk token will receive an incorrect class label  for
example  if a junk token has enough matching letters  it might be labeled as a trim  since
trims may only be a single letter or two   this leads to noisy tokens within the whole
extracted trim attribute  therefore  labeling tokens individually gives an approximation of
the data to be extracted 
the extraction approach can overcome the problems of generating noisy  labeled tokens
by comparing the whole extracted field to its analogue reference set attribute  after all
tokens from a post are processed  whole attributes are built and compared to the corresponding attributes from the reference set  this allows removal of the tokens that introduce
noise in the extracted attribute 
the removal of noisy tokens from an extracted attribute starts with generating two
baseline scores between the extracted attribute and the reference set attribute  one is a
jaccard similarity  to reflect the token level similarity between the two attributes  however 
since there are many misspellings and such  an edit distance based similarity metric  the
jaro winkler metric  is also used  these baselines demonstrate how accurately the system
extracted classified the tokens in isolation 
using the first post of table   as our ongoing example  assume the phrase civic  ri 
was extracted as the model  this might occur if there is a car with the model civic rx 
for instance  in isolation  the token  ri  could be the rx of the model  comparing this
extracted car model to the reference attribute civic generates a jaccard similarity of    
and a jaro winkler score of       this is shown at the top of figure   
next  the cleaning method goes through the extracted attribute  removing one token at
a time and calculating new jaccard and jaro winkler similarities  if both new scores are
higher than the baselines  that token becomes a removal candidate  after all the tokens are
processed in this way  the removal candidate with the highest scores is removed  and the
whole process is repeated  the scores derived using the removed token then become the
new baseline to compare against  the process ends when there are no more tokens that
yield improved scores over the baselines 
shown as iteration   in figure    the cleaning method finds that  ri  is a removal
candidate since removing this token from the extracted car model yields a jaccard score of
    and a jaro winkler score of      which are both higher than the baseline scores  since
it has the highest scores after trying each token in the iteration  it is removed and the
baseline scores update  then  since none of the remaining tokens provide improved scores
 since there are none   the process terminates  yielding a more accurate attribute value 
this is shown as iteration   in figure    note that this process would keep iterating 
until no tokens can be removed that improve the scores over the baseline  the pseudocode
for the algorithm is shown in figure   
   

firelational data from unstructured data sources

figure    improving extraction accuracy with reference set attributes
note  however  that we do not limit the machine learning component of our extraction
algorithm to svms  instead  we claim that in some cases  reference sets can aid extraction
in general  and to test this  in our architecture we can replace the svm component with
other methods  for example  in our extraction experiments we replace the svm extractor
with a conditional random field  crf   lafferty  mccallum    pereira        extractor
that uses the vie as features 
therefore  the whole extraction process takes a token of the text  creates the vie and
passes this to the machine learning extractor which generates a label for the token  then
each field is cleaned and the extracted attribute is saved 

   results
the phoebus system was built to experimentally validate our approach to building relational
data from unstructured and ungrammatical data sources  specifically  phoebus tests the
techniques accuracy in both the record linkage and the extraction  and incorporates the
bsl algorithm for learning and using blocking schemes  the experimental data  comes from
three domains of posts  hotels  comic books  and cars 
the data from the hotel domain contains the attributes hotel name  hotel area  star
rating  price and dates  which are extracted to test the extraction algorithm  this data
comes from the bidding for travel website  which is a forum where users share successful
bids for priceline on items such as airline tickets and hotel rates  the experimental data
is limited to postings about hotel rates in sacramento  san diego and pittsburgh  which
compose a data set with      posts  with      of these posts having a match in the reference
set  the reference set comes from the bidding for travel hotel guides  which are special
   www biddingfortravel com

   

fimichelson   knoblock

algorithm      cleanattribute e  r 
comment  clean extracted attribute e using reference set attribute r
removalcandidates c  null
jarow inklerbaseline  jarowinkler e  r 
jaccardbaseline  jaccard e  r 
for each token t  e

x t  removetoken t  e 




jarow inklerxt  jarowinkler x t   r 






xt  jaccard x t   r 

jaccard

jarow
inklerxt  jarow inklerbaseline

do


if and




jaccard  jaccard


xt
baseline

n



then c  c  t
 

if

c   null
return  e 
 
e  removemaxcandidate c e 
else
cleanattribute e  r 

figure    algorithm to clean an extracted attribute
posts listing all of the hotels ever posted about a given area  these special posts provide
hotel names  hotel areas and star ratings  which are the reference set attributes  therefore 
these are the   attributes for which the standardized values are used  allowing us to treat
these posts as a relational data set  this reference set contains     records 
the experimental data for the comic domain comes from posts for items for sale on
ebay  to generate this data set  ebay was searched by the keywords incredible hulk and
fantastic four in the comic books section of their website   this returned some items
that are not comics  such as tshirts and some sets of comics not limited to those searched
for  which makes the problem more difficult   the returned records contain the attributes
comic title  issue number  price  publisher  publication year and the description  which are
extracted   note  the description is a few word description commonly associated with a
comic book  such as  st appearance the rhino   the total number of posts in this data
set is      of which     have matches  the comic domain reference set uses data from the
comics price guide     which lists all the incredible hulk and fantastic four comics  this
reference set has the attributes title  issue number  description  and publisher and contains
    records 
the cars data consists of posts made to craigslist regarding cars for sale  this dataset
consists of classifieds for cars from los angeles  san francisco  boston  new york  new
    http   www comicspriceguide com 

   

firelational data from unstructured data sources

jersey and chicago  there are a total of       posts in this data set  and each post
contains a make  model  year  trim and price  the reference set for the cars domain comes
from the edmunds   car buying guide  from this data set we extracted the make  model 
year and trim for all cars from      to       resulting in        records  there are       
matches between the posts to craigslist and the cars from edmunds 
unlike the hotels and comics domains  a strict     relationship between the post and
reference set was not enforced in the cars domain  as described previously  phoebus relaxed the     relationship to form a   n relationship between the posts and the reference
set  sometimes the records do not contain enough attributes to discriminate a single best
reference member  for instance  posts that contain just a model and a year might match a
couple of reference set records that would differ on the trim attribute  but have the same
make  model  and year  yet  we can still use this make  model and year accurately for
extraction  so  in this case  as mentioned previously  we pick one of the matches  this way 
we exploit the attributes that we can from the reference set  since we have confidence in
those 
for the experiments  posts in each domain are split into two folds  one for training and
one for testing  this is usually called two fold cross validation  however  in many cases twofold cross validation results in using     of the data for training and     for testing  we
believe that this is too much data to have to label  especially as data sets become large  so
our experiments instead focus on using less training data  one set of experiments uses    
of the posts for training and tests on the remaining      and the second set of experiments
uses just     of the posts to train  testing on the remaining      we believe that training
on small amounts of data  such as      is an important empirical procedure since real
world data sets are large and labeling     of such large data sets is time consuming and
unrealistic  in fact  the size of the cars domain prevented us from using     of the data for
training  since the machine learning algorithms could not scale to the number of training
tuples this would generate  so for the cars domain we only run experiments training on
    of the data  all experiments are performed    times  and the average results for these
   trials are reported 
    record linkage results
in this subsection we report our record linkage results  broken down into separate discussions
of our blocking results and our matching results 
      blocking results
in order for the bsl algorithm to learn a blocking scheme  it must be provided with methods
it can use to compare the attributes  for all domains and experiments we use two common
methods  the first  which we call token  compares any matching token between the
attributes  the second method  ngram   considers any matching   grams between the
attributes 
it is important to note that a comparison between bsl and other blocking methods  such
as the canopies method  mccallum  nigam    ungar        and bigram indexing  baxter 
christen    churches         is slightly misaligned because the algorithms solve different
    www edmunds com

   

fimichelson   knoblock

problems  methods such as bigram indexing are techniques that make the process of each
blocking pass on an attribute more efficient  the goal of bsl  however  is to select which
attribute combinations should be used for blocking as a whole  trying different attribute and
method pairs  nonetheless  we contend that it is more important to select the right attribute
combinations  even using simple methods  than it is to use more sophisticated methods  but
without insight as to which attributes might be useful  to test this hypothesis  we compare
bsl using the token and   gram methods to bigram indexing over all of the attributes 
this is equivalent to forming a disjunction over all attributes using bigram indexing as the
method  we chose bigram indexing in particular because it is designed to perform fuzzy
blocking which seems necessary in the case of noisy post data  as stated previously  baxter
et al          we use a threshold of     for bigram indexing  since that works the best  we
also compare bsl to running a disjunction over all attributes using the simple token method
only  in our results  we call this blocking rule disjunction  this disjunction mirrors the
idea of picking the simplest possible blocking method  namely using all attributes with a
very simple method 
as stated previously  the two goals of blocking can be quantified by the reduction ratio
 rr  and the pairs completeness  pc   table   shows not only these values but also how
many candidates were generated on average over the entire test set  comparing the three
different approaches  table   also shows how long it took each method to learn the rule
and run the rule  lastly  the column time match shows how long the classifier needs to
run given the number of candidates generated by the blocking scheme 
table   shows a few example blocking schemes that the algorithm generated  for a
comparison of the attributes bsl selected to the attributes picked manually for different
domains where the data is structured the reader is pointed to our previous work on the
topic  michelson   knoblock        
the results of table   validate our idea that it is more important to pick the correct
attributes to block on  using simple methods  than to use sophisticated methods without
attention to the attributes  comparing the bsl rule to the bigram results  the combination
of pc and rr is always better using bsl  note that although in the cars domain bigram
took significantly less time with the classifier due to its large rr  it did so because it only
had a pc of     in this case  bigrams was not even covering    of the true matches 
further  the bsl results are better than using the simplest method possible  the disjuction   especially in the cases where there are many records to test upon  as the number of
records scales up  it becomes increasingly important to gain a good rr  while maintaining
a good pc value as well  this savings is dramatically demonstrated by the cars domain 
where bsl outperformed the disjunction in both pc and rr 
one surprising aspect of these results is how prevalent the token method is within all the
domains  we expect that the ngram method would be used almost exclusively since there
are many spelling mistakes within the posts  however  this is not the case  we hypothesize
that the learning algorithm uses the token methods because they occur with more regularity
across the posts than the common ngrams would since the spelling mistakes might vary quite
differently across the posts  this suggests that there might be more regularity  in terms of
what we can learn from the data  across the posts than we initially surmised 
another interesting result is the poor reduction ratio of the comic domain  this happens
because most of the rules contain the disjunct that finds a common token within the comic
   

firelational data from unstructured data sources

hotels      
bsl
disjunction
bigrams
hotels      
bsl
disjunction
bigrams
comics      
bsl
disjunction
bigrams
comics      
bsl
disjunction
bigrams
cars      
bsl
disjunction
bigrams

rr

pc

  cands

time learn  s 

time run  s 

time match  s 

     
     
     

     
     
     

      
      
      

     
 
 

     
     
   

     
      
      

     
     
     

     
     
     

      
      
      

     
 
 

     
      
    

     
      
      

     
     
     

     
      
     

       
       
       

     
 
 

     
     
      

      
      
      

     
     
     

     
      
     

       
       
       

     
 
 

     
      
      

        
        
        

     
     
     

     
     
    

         
         
         

      
 
 

      
      
      

         
         
        

table    blocking results using the bsl algorithm  amount of data used for training shown
in parentheses  

hotels domain      
  hotel area token    hotel name token    star rating  token      hotel name  ngram   
hotels domain      
  hotel area token    hotel name token      hotel name ngram   
comic domain      
  title  token  
comic domain      
  title  token      issue number token    publisher token    title ngram   
cars domain      
  make token      model ngram       year token    make ngram   
table    some example blocking schemes learned for each of the domains 

   

fimichelson   knoblock

title  this rule produces such a poor reduction ratio because the value for this attribute is
the same across almost all reference set records  that is to say  when there are just a few
unique values for the bsl algorithm to use for blocking  the reduction ratio will be small 
in this domain  there are only two values for the comic title attribute  fantastic four and
incredible hulk  so it makes sense that if blocking is done using the title attribute only 
the reduction is about half  since blocking on the value fantastic four just gets rid of the
incredible hulk comics  this points to an interesting limitation of the bsl algorithm  if
there are not many distinct values for the different attribute and method pairs that bsl
can use to learn from  then this lack of values cripples the performance of the reduction
ratio  intuitively though  this makes sense  since it is hard to distinguish good candidate
matches from bad candidate matches if they share the same attribute values 
another result worth mentioning is that in the hotels domain we get a lower rr but
the same pc when we use less training data  this happens because our bsl algorithm
runs until it has no more examples to cover  so if those last few examples introduce a new
disjunct that produces a lot of candidates  while only covering a few more true positives 
then this would cause the rr to decrease  while keeping the pc at the same high rate 
this is in fact what happens in this case  one way to curb this behavior would be to set
some sort of stopping threshold for bsl  but as we said  maximizing the pc is the most
important thing  so we choose not to do this  we want bsl to cover as many true positives
as it can  even if that means losing a bit in the reduction 
in fact  we next test this notion explicitly  we set a threshold in the sca such that
after     of the training examples are covered  the algorithm stops and returns the learned
blocking scheme  this helps to avoid the situation where bsl learns a very general conjunction  solely to cover the last few remaining training examples  when that happens  bsl
might end up lowering the rr  at the expense of covering just those last training examples 
because the rule learned to cover those last examples is overly general and returns too many
candidate matches 
domain
hotels domain
no thresh      
    thresh      
comic domain
no thresh      
    thresh      
cars domain
no thresh      
    thresh      

record linkage
f measure

rr

pc

     
     

     
     

     
     

     
     

     
     

     
     

     
     

     
     

     
     

table    a comparison of bsl covering all training examples  and covering     of the
training examples

   

firelational data from unstructured data sources

table   shows that when we use a threshold in the hotels and cars domain we see a
statistically significant drop in pairs completeness with a statistically significant increase
in reduction ratio    this is expected behavior since the threshold causes bsl to kick
out of sca before it can cover the last few training examples  which in turn allows bsl
to retain a rule with high rr  but lower pc  however  when we look at the record linkage
results  we see that this threshold does in fact have a large effect    although there is no
statistically significant difference in the f measure for record linkage in the hotels domain 
the difference in cars domain is dramatic  when we use a threshold  the candidates not
discovered by the rule generated when using the threshold have an effect of     on the final
f measure match results    therefore  since the f measure results differ by so much  we
conclude that it is worthwhile to maximize pc when learning rules with bsl  even if the
rr may decrease  that is to say  even in the presence of noise  which in turn may lead to
overly generic blocking schemes  bsl should try to maximize the true matches it covers 
because avoiding even the most difficult cases to cover may affect the matching results  as
we see in table    this is especially true in the cars domain where matching is much more
difficult than in the hotels domain 
interestingly  in the comic domain we do not see a statistically significant difference
in the rr and pc  this is because across trials we almost always learn the same rule
whether we use a threshold or not  and this rule covers enough training examples that the
threshold is not hit  further  there is no statistically significant change in the f measure
record linkage results for this domain  this is expected since bsl would generate the same
candidate matches  whether it uses the threshold or not  since in both cases it almost always
learns the same blocking rules 
our results using bsl are encouraging because they show that the algorithm also works
for blocking when matching unstructured and ungrammatical text to a relational data
source  this means the algorithm works in this special case too  not just the case of
traditional record linkage where we are matching one structured source to another  this
means our overall algorithm for semantic annotation is much more scalable because we are
using fewer candidate matches than in our previous work  michelson   knoblock        
      matching results
since this alignment approach hinges on leveraging reference sets  it becomes necessary to
show the matching step performs well  to measure this accuracy  the experiments employ
the usual record linkage statistics 
p recision  
recall  

 correctm atches
 t otalm atchesm ade
 correctm atches
 p ossiblem atches

    bold means statistically significant using a two tailed t test with  set to     
    please see subsection       for a description of the record linkage experiments and results 
    much of this difference is attributed to the non threshold version of the algorithm learning a final
predicate that includes the make attribute by itself  which the version with a threshold does not learn 
since each make attribute value covers many records  it generates many candidates which results in
increasing pc while reducing rr 

   

fimichelson   knoblock

f  m easure  

   p recision  recall
p recison   recall

the record linkage approach in this article is compared to whirl  cohen        
whirl performs record linkage by performing soft joins using vector based cosine similarities between the attributes  other record linkage systems require decomposed attributes for
matching  which is not the case with the posts  whirl serves as the benchmark because it
does not have this requirement  to mirror the alignment task of phoebus  the experiment
supplies whirl with two tables  the test set of posts  either     or     of the posts  and
the reference set with the attributes concatenated to approximate a record level match  the
concatenation is also used because when matching on each individual attribute  it is not
obvious how to combine the matching attributes to construct a whole matching reference
set member 
to perform the record linkage  whirl does soft joins across the tables  which produces
a list of matches  ordered by descending similarity score  for each post with matches from
the join  the reference set member s  with the highest similarity score s  is called its match 
in the cars domain the matches are   n  so this means that only   match from the reference
set will be exploited later in the information extraction step  to mirror this idea  the number
of possible matches in a   n domain is counted as the number of posts that have a match in
the reference set  rather than the reference set members themselves that match  also  this
means that we only add a single match to our total number of correct matches for a given
post  rather than all of the correct matches  since only one matters  this is done for both
whirl and phoebus  and more accurately reflects how well each algorithm would perform
as the processing step before our information extraction step 
the record linkage results for both phoebus and whirl are shown in table    note
that the amount of training data for each domain is shown in parentheses  all results
are statistically significant using a two tailed paired t test with        except for the
precision between whirl and phoebus in the cars domain  and the precision between
phoebus trained on     and     of the training data in the comic domain 
phoebus outperforms whirl because it uses many similarity types to distinguish
matches  also  since phoebus uses both a record level and attribute level similarities 
it is able to distinguish between records that differ in more discriminative attributes  this
is especially apparent in the cars domain  first  these results indicate the difficulty of
matching car posts to the large reference set  this is the largest experimental domain yet
used for this problem  and it is encouraging how well our approach outperforms the baseline  it is also interesting that the results suggest that both techniques are equally accurate
in terms of precision  in fact  there is no statistically significant difference between them
in this sense  but phoebus is able to retrieve many more relevant matches  this means
phoebus can capture more rich features that predict matches than whirls cosine similarity alone  we expect this behavior because phoebus has a notion of both field and token
level similarity  using many different similarity measures  this justifies our use of the many
similarity types and field and record level information  since our goal is to find as many
matches as we can 
it is also encouraging that using only     of the data for labeling  phoebus is able to
perform almost as well as using     of the data for training  since the amount of data on
the web is vast  only having to label     of the data to get comparative results is preferable
   

firelational data from unstructured data sources

hotel
phoebus      
phoebus      
whirl
comic
phoebus      
phoebus      
whirl
cars
phoebus      
whirl

precision

recall

f measure

     
     
     

     
     
     

     
     
     

     
     
     

     
     
     

     
     
     

     
     

     
     

     
     

table    record linkage results

when the cost of labeling data is great  especially since the clean annotation  and hence
relational data  comes from correctly matching the posts to the reference set  not having
to label much of the data is important if we want this technique to be widely applicable 
in fact  we faced this practical issue ourselves in the cars domain where we were unable
to use     for training since the machine learning method would not scale to the number
of candidates generated by this much training data  so  the fact that we can report good
results with just     training data allows us to extend this work to the much larger cars
domain 
while our method performs well and outperforms whirl  from the results above  it is
not clear whether it is the use of the many string metrics  the inclusion of the attributes and
their concatenation or the svm itself that provides this advantage  to test the advantages
of each piece  we ran several experiments isolating each of these ideas 
first  we ran phoebus matching on only the concatenation of the attributes from the
reference set  rather than the concatenation and all the attributes individually  earlier  we
stated that we use the concatenation to mirror the idea of record level similarity and we also
use each attribute to mirror field level similarity  it is our hypothesis that in some cases 
a post will match different reference set records with the same record level score  using
the concatenation   but it will do so matching on different attributes  by removing the
individual attributes and leaving only the concatenation of them for matching  we can test
how the concatenation influences the matching in isolation  table   shows these results for
the different domains 
for the cars and comic domains we see an improvement in f measure  indicating that
that using the attributes and the concatenation is much better for matching than using the
concatenation alone  this supports our notion that we also need a method to capture the
significance of matching individual attributes since some attributes are better indicators of
matching than others  it is also interesting to note that for both these domains  whirl does
a better job than the machine learning using only the concatenation  even though whirl
   

fimichelson   knoblock

hotels
phoebus      
concatenation only
whirl
comic
phoebus      
concatenation only
whirl
cars
phoebus      
concatenation only
whirl

precision

recall

f measure

     
     
     

     
     
     

     
     
     

     
     
     

     
     
     

     
     
     

     
     
     

     
     
     

     
     
     

table    matching using only the concatenation

also uses a concatenation of the attributes  this is because whirl uses informationretrieval style matching to find the best match  and the machine learning technique tries to
learn the characteristics of the best match  clearly  it is very difficult to learn what such
characteristics are 
in the hotels domain  we do not find a statistically significant difference in f measure
using the concatenation alone  this means that the concatenation is sufficient to determine
the matches  so there is no need for individual fields to play a role  more specifically 
the hotel name and area seem to be the most important attributes for matching and by
including them as part of the concatenation  the concatenation is still distinguishable enough
between all records to determine matches  since in two of the three domains we see a
huge improvement  and we never lose in f measure  using both the concatenation and the
individual attributes is valid for the matching  also  since in two domains the concatenation
alone was worse than whirl  we conclude that part of the reason phoebus can outperform
whirl is the use of the individual attributes for matching 
our next experiment tests how important it is to include all of the string metrics in our
feature vector for matching  to test this idea  we compare using all the metrics to using
just one  the jensen shannon distance  we choose the jensen shannon distance because it
outperformed both tf idf and even a soft tf idf  one that accounts for fuzzy token
matches  in the task of selecting the right reference sets for a given set of posts  michelson
  knoblock         these results are shown in table    
as table    shows  using all the metrics yielded a statistically significant  large improvement in f measure for the comic and cars domains  this means that some of the
other string metrics  such as the edit distances  were capturing similarities that the jensenshannon distance alone did not  interestingly  in both domains  using phoebus with only
the jensen shannon distance does not dominate whirls performance  therefore  the
results of table    and table   demonstrate that phoebus benefits from the combination
   

firelational data from unstructured data sources

hotels
phoebus      
jensen shannon only
whirl
comic
phoebus      
jensen shannon only
whirl
cars
phoebus      
jensen shannon only
whirl

precision

recall

f measure

     
     
     

     
     
     

     
     
     

     
     
     

     
     
     

     
     
     

     
     
     

     
     
     

     
     
     

table     using all string metrics versus using only the jensen shannon distance

of many  varied similarity metrics along with the use of individual attributes for field level
similarities  and both of these aspects contribute to phoebus outperforming whirl 
in the case of the hotels data  there is not a statistically significant difference in the
matching results  so in this case the other metrics do not provide relevant information for
matching  therefore  all the matches missed by the jensen shannon only method are also
missed when we include all of the metrics  hence  either these missed matches are very
difficult to discover  or we do not have a string metric in our method yet that can capture
the similarity  for example  when the post has a token dt and the reference set record it
should match has a hotel area of downtown  then an abbreviation metric could capture
this relationship  however  phoebus does not include an abbreviation similarity measure 
since none of the techniques in isolation consistently outperforms whirl  we conclude
that phoebus outperforms whirl because it combines multiple string metrics  it uses both
individual attributes and the concatenation  and  as stated in section      the svm classifier
is well suited for our record linkage task  these results also justify our inclusion of many
metrics and the individual attributes  along with our use of svm as our classifier 
our last matching experiment justifies our binary rescoring mechanism  table    shows
the results of performing the binary rescoring for record linkage versus not performing this
binary recoring  we hypothesize earlier in this paper that the binary rescoring will allow
the classifier to more accurately make match decisions because the rescoring separates out
the best candidate as much as possible  table    shows this to be the case  as across all
domains when we perform the binary rescoring we gain a statistically significant amount
in the f measure  this shows that the record linkage is more easily able to identify the
true matches from the possible candidates when the only difference in the record linkage
algorithm is the use of binary rescoring 
   

fimichelson   knoblock

hotels
phoebus      
no binary rescoring
phoebus      
no binary rescoring
comic
phoebus      
no binary rescoring
phoebus      
no binary rescoring
cars
phoebus      
no binary rescoring

precision

recall

f measure

     
     
     
     

     
     
     
     

     
     
     
     

     
     
     
     

     
     
     
     

     
     
     
     

     
     

     
     

     
     

table     record linkage results with and without binary rescoring

    extraction results
this section presents results that experimentally validate our approach to extracting the
actual attributes embedded within the post  we also compare our approach to two other
information extraction methods that rely on the structure or grammar of the posts 
first  the experiments compare phoebus against a baseline conditional random field
 crf   lafferty et al         extractor  a conditional random field is a probabilistic
model that can label and segment data  in labeling tasks  such as part of speech tagging  crfs outperform hidden markov models and maximum entropy markov models 
therefore  by representing the state of the art probabilistic graphical model  they present a
strong comparison to our approach to extraction  crfs have also been used effectively for
information extraction  for instance  crfs have been used to combine information extraction and coreference resolution with good results  wellner  mccallum  peng    hay        
these experiments use the simple tagger implementation of crfs from the mallet
 mccallum        suite of text processing tools 
further  as stated in section   on extraction  we also created a version of phoebus that
uses crfs  which we call phoebuscrf  phoebuscrf uses the same extraction features
 vie   as phoebus using the svm  such as the common score regular expressions and the
string similarity metrics  we include phoebuscrf to show that extraction in general can
benefit from our reference set matching 
second  the experiments compare phoebus to natural language processing  nlp  based
extraction techniques  since the posts are ungrammatical and have unreliable lexical characteristics  these nlp based systems are not expected to do as well on this type of data 
the amilcare system  ciravegna         which uses shallow nlp for extraction  has been
shown to outperform other symbolic systems in extraction tasks  and so we use amilcare
as the other system to compare against  since amilcare can exploit gazetteers for extra
   

firelational data from unstructured data sources

information  for our experiments amilcare receives the reference data as a gazetteer to aid
the extraction  both simple tagger and amilcare are used with default settings 
lastly  we compare phoebus trained using     of the data for training to phoebus
trained using     of the data   we do this for phoebuscrf as well   in our experimental
results  the amount of training data is put in parentheses 
one component of the extraction vector vie is the vector common scores  which includes
user defined functions  such as regular expressions  since these are the only domain specific
functions used in the algorithm  the common scores for each domain must be specified 
for the hotels domain  the common scores includes the functions matchpriceregex and
matchdateregex  each of these functions gives a positive score if a token matches a price or
date regular expression  and   otherwise  for the comic domain  common scores contains
the functions matchpriceregex and matchyearregex  which also give positive scores when a
token matches the regular expression  in the cars domain  common scores uses the function
matchpriceregex  since year is an attribute of the reference set  we do not use a common
score to capture its form  
for the cars data set  not all of the posts were labeled for training and testing the
extraction  for this domain  we only labeled     of the posts for extraction  and use these
for training and testing the extraction algorithm  note  however  that phoebus does perform
the extraction on all of the posts  it just is not able to report results for those  in fact  a
running demo of phoebus  in the cars domain is live   
the extraction results are presented using precision  recall and f measure  note that
these extraction results are field level results  this means that an extraction is counted
as correct only if all tokens that compromise that field in the post are correctly labeled 
although this is a much stricter rubric of correctness  it more accurately models how useful
an extraction system would be  tables        and    show the results of correctly labeling
the tokens within the posts with the correct attribute label for the hotel  comic and cars
domains  respectively  attributes in italics are attributes that exist in the reference set 
the column freq shows the average number of fields in the test set that have the associated
label  also  observe that a   means that results between the highest phoebus score  phoebus
or phoebuscrf  and the highest baseline  amilcare or simple tagger crf  f measure are
not statistically significant using a two tailed paired t test with       
phoebus and phoebuscrf outperform the other systems on almost all attributes     of
     as shown in table     in fact  there was only one attribute where the baseline system
was the best  using amilcare to extract the date attribute in the hotels domain  for this
attribute  phoebus and phoebuscrf both use the common score regular expression as the
main identifying feature  since this regular expression is user supplied  we propose that a
better regular expression could make phoebus phoebuscrf extract these dates even more
accurately  overcoming this baseline  since both systems perform well using the reference
set data to aid the extraction  these results show that using reference sets can greatly aid
extraction  this is especially evident when we compare phoebuscrf to the simple tagger
crf  since the difference between these two extraction methods is the reference set attribute
similarity scores and the common scores 
    http   www isi edu integration phoebus demos html this demo uses an extraction model trained on
the     labeled extraction examples  and has been running live for months as of the writing of this
article 

   

fimichelson   knoblock

area

date

name

price

star

phoebus      
phoebus      
phoebuscrf      
phoebuscrf      
simple tagger crf      
amilcare      
phoebus      
phoebus      
phoebuscrf      
phoebuscrf      
simple tagger crf      
amilcare      
phoebus      
phoebus      
phoebuscrf      
phoebuscrf      
simple tagger crf      
amilcare      
phoebus      
phoebus      
phoebuscrf     
phoebuscrf      
simple tagger crf      
amilcare      
phoebus      
phoebus      
phoebuscrf      
phoebuscrf      
simple tagger crf      
amilcare      

hotel
recall
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

precision
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

f measure
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

frequency
    

    

    

    

    

table     field level extraction results  hotels domain

   

firelational data from unstructured data sources

descript 

issue

price

publisher

title

year

phoebus      
phoebus      
phoebuscrf      
phoebuscrf      
simple tagger crf      
amilcare      
phoebus      
phoebus      
phoebuscrf      
phoebuscrf      
simple tagger crf      
amilcare      
phoebus      
phoebus      
phoebuscrf      
phoebuscrf      
simple tagger crf      
amilcare      
phoebus      
phoebus      
phoebuscrf      
phoebuscrf      
simple tagger crf      
amilcare      
phoebus      
phoebus      
phoebuscrf      
phoebuscrf      
simple tagger crf      
amilcare      
phoebus      
phoebus      
phoebuscrf      
phoebuscrf      
simple tagger crf      
amilcare      

comic
recall
     
     
     
     
     
    
     
     
     
     
     
     
     
     
     
     
     
     
      
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

precision
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

f measure
     
     
     
     
      
     
     
     
     
     
     
     
      
     
     
     
     
     
     
     
     
     
     
     
     
     
      
     
     
     
     
     
     
     
     
     

frequency
   

    

   

   

    

    

table     field level extraction results  comic domain 

   

fimichelson   knoblock

make

model

price

trim

year

phoebus      
phoebuscrf      
simple tagger crf      
amilcare      
phoebus      
phoebuscrf      
simple tagger crf      
amilcare      
phoebus      
phoebuscrf      
simple tagger crf      
amilcare      
phoebus      
phoebuscrf      
simple tagger crf      
amilcare      
phoebus      
phoebuscrf      
simple tagger crf      
amilcare      

cars
recall
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

precision
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

f measure
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

frequency
    

    

    

    

    

table     field level extraction results  cars domain 

domain
hotel
comic
cars
all

phoebus
 
 
 
 

num  of max  f measures
phoebuscrf amilcare simple tagger
 
 
 
 
 
 
 
 
 
 
 
 

total attributes
 
 
 
  

table     summary results for extraction showing the number of times each system had
statistically significant highest f measure for an attribute 

   

firelational data from unstructured data sources

phoebus performs especially well in the cars domain  where it is the best system on
all the attributes  one interesting thing to note about this result is that while the record
linkage results are not spectacular for the cars domain  they are good enough to yield very
high extraction results  this is because most times when the system is not picking the best
match from the reference set  it is still picking one that is close enough such that most
of the reference set attributes are useful for extraction  this is why the trim extraction
results are the lowest  because that is often the attribute that determines a match from a
non match  the record linkage step likely selects a car that is close  but differs in the trim 
so the match is incorrect and the trim will most likely not be extracted correctly  but the
rest of the attributes can be extracted using the reference set member 
a couple of other interesting notes come from these results  one of the most intriguing
aspects of these results is that they allow us to estimate some level of structure for different
attributes within a domain  since crfs rely more on the structure of the tokens within
a post than the structured svm method  we hypothesize that in the domains with more
structure  phoebuscrf should perform best and in the domains with the least structure 
phoebus should perform best  table    shows this to be the case  phoebuscrf dominates
the hotels domain  where  for example  many posts have a structure where the star rating
comes before the hotel name  so using such structure should allow the extractor to get
the hotel name more accurately than not using this information  therefore we see that
overall there is structure within the hotels domain because phoebuscrf is the method
that performs best  not phoebus  contrast this with the cars domain  which is highly
unstructured  where phoebus performs the best across all attributes  in this domain there
are many missing tokens and the order of attributes is more varied  the comic domain is
varied with both some attributes that exhibit structure and some that do not  and as table
   shows  so are the cases where phoebus or phoebuscrf dominates  however  although
the hotels data exhibits some structure  the important aspect of this research is that using
phoebus allows one to perform extraction without assuming any structure in the data 
also  a result worth noting is that the price attribute in the comic domain is a bit
misleading  in fact  none of the systems were statistically significant with respect to each
other because there were so few prices to extract that the f measures were all over for all
the systems 
another aspect that came to light with statistical significance is the generalization of the
algorithm  for the hotels and comic domains  where we were able to use both     and    
of the data for training  there are not many cases with a statistically significant difference
in the f measures for the extracted attributes using phoebus  in the hotels domain the
name  the area and date had statistically significant f measures between training on    
and     of the data  and in the comic domain only the difference in f measure between the
issue and description attributes were significant  though the description was borderline  
this means of the    attributes in both domains  roughly half of them were insignificant 
therefore there is little difference in extraction whether we use     of the data for training
or      so the extraction algorithm generalizes very well  this is important since labeling
data for extraction is very time consuming and expensive 
one interesting result to note is that except for the comic price  which was insignificant
for all systems  and the hotel date  which was close   phoebus  using either     or    
training data  outperformed all of the other systems on the attributes that were not included
   

fimichelson   knoblock

in the reference set  this lends credibility to our claim earlier in the section that by training
the system to extract all of the attributes  even those in the reference set  we can more
accurately extract attributes not in the reference set because we are training the system to
identify what something is not 
the overall performance of phoebus validates this approach to semantic annotation  by
infusing information extraction with the outside knowledge of reference sets  phoebus is
able to perform well across three different domains  each representative of a different type
of source of posts  the auction sites  internet classifieds and forum bulletin boards 

   discussion
the goal of this research is to produce relational data from unstructured and ungrammatical
data sources so that they can be accurately queried and integrated with other sources  by
representing the attributes embedded within a post with the standardized values from the
reference set  we can support structural queries and integration  for instance  we can
perform aggregate queries because we can treat the data source as a relational database
now  furthermore  we have standardized values for performing joins across data sources 
a key for integration of multiple sources  these standardized values also aid in the cases
where the post actually does not contain the attribute  for instance  in table    two of
the listings do not include the make honda  however  once matched to the reference set 
they contain a standardized value for this attribute which can then be used for querying and
integrating these posts  this is especially powerful since the posts never explicitly stated
these attribute values  the reference set attributes also provide a solution for the cases
where the extraction is extremely difficult  for example  none of the systems extracted
the description attribute of the comic domain well  however  if one instead considers
the description attribute from the reference set  which is quantified by the record linkage
results for the comic domain  this yields an improvement of over     in the f measure for
identifying the description for a post 
it may seem that using the reference set attributes for annotation is enough since the
values are already cleaned  and that extraction is unnecessary  however  this is not the case 
for one thing  one may want to see the actual values entered for different attributes  for
instance  a user might want to discover the most common spelling mistake or abbreviation
for a attribute  also  there are cases when the extraction results outperform the record
linkage results  this happens because even if a post is matched to an incorrect member of
the reference set  that incorrect member is most likely very close to the correct match  and
so it can be used to correctly extract much of the information  for a strong example of this 
consider the cars domain  the f measure for the record linkage results are not as good as
those for the extraction results in this domain  this means most matches that were chosen
where probably incorrect because they differ from the correct match by something small 
for example  a true match could have the trim as   door while the incorrectly chosen
match might have the trim   door  but there would still be enough information  such as
the rest of the trim tokens  the year  the make and the model to correctly extract those
different attributes from the post itself  by performing the extraction for the values from
the post itself  we can overcome the mistakes of the record linkage step because we can still
exploit most of the information in the incorrectly chosen reference set member 
   

firelational data from unstructured data sources

extraction on all of the attributes also helps our system classify  and ignore  junk
tokens  labeling something as junk is much more descriptive if it is labeled junk out of
many possible class labels that could share lexical characteristics  this helps improve the
extraction results on items that are not in the reference set  such as prices and dates 
on the topic of reference sets  it is important to note that the algorithm is not tied to
a single reference set  the algorithm extends to include multiple reference sets by iterating
the process for each reference set used 
consider the following two cases  first  suppose a user wants to extract conference
names and cities and she has individual lists of each  if the approach is confined to using
one reference set  that would require constructing a reference set that contains the power set
of cities crossed with conference names  this approach would not scale for many attributes
from distinct sources  however  if these lists are used as two reference sets  one for each
attribute  the algorithm can run once with the conference name data  and once with a
reference set of cities  this iterative exploitation of the reference sets allows for n reference
set attributes to be added without a combinatorial explosion 
the next interesting case is when a post contains more than one of the same attribute 
for example  a user needs to extract two cities from some post  if one reference set is used 
then it includes the cross product of all cities  however  using a single reference set of city
names can be done by slightly modifying the algorithm  the new algorithm makes a first
pass with the city reference set  during this pass  the record linkage match will either be
one of the cities that matches best  or a tie between them  in the case of a tie  choose the
first match  using this reference city  our system can then extract the city from the post 
and remove it from the post  then our system simply runs the process again  which will
catch the second city  using the same  single reference set  this could be repeated as many
times as needed 
one issue that arises with reference sets is the discrepancy between users knowledge
and the domain experts who generally create the reference sets  in the cars domain  for
instance  users will interchangeably use the attribute values hatchback  liftback  and
wagon  the reference set never includes the term liftback which suggests it is a synonym
for hatchback used in common speech  but not in edmunds automobile jargon  the term
wagon is used by edmunds  but it is not used for some of the cars that users describe
as hatchbacks  this implies a slight difference in meaning between the two  according to
the reference set authors 
two issues arise from these discrepancies  the first is the users interchanging the words
can cause some problems for the extraction and for the record linkage  but this can be
overcome by incorporating some sort of thesaurus into the algorithm  during record linkage 
a thesaurus could expand certain attribute values used for matching  for example including
hatchback and liftback when the reference set attribute includes the term wagon 
however  there are more subtle issues here  it is mostly not the case that a hatchback is
called a wagon but it does happen that a wagon is called a hatchback  the frequency
of replacement must be taken into consideration so that errant matches are not created 
how to automate this is a line of future research  the other issue arises from trusting
the correctness of the edmunds source  we assume edmunds is right to define one car
as a wagon which has a different meaning from classifying it as a hatchback  in fact 
   

fimichelson   knoblock

edmunds classifies the mazda protege  as a wagon  while kelly blue book   classifies it
as a hatchback  this seems to invalidate the idea that wagon is different in meaning
from hatchback  they appear to be simple synonyms  but this would remain unknown
without the outside knowledge of kelly blue book  more generally  one assumes that the
reference set is a correct set of standardized values  but this is not an absolute truth  that is
why the most meaningful reference sets are those that can be constructed from agreed upon
ontologies from the semantic web  for instance  a reference set derived from an ontology
for cars created by all of the biggest automotive businesses should alleviate many of the
issues in meaning  and a thesaurus scheme could work out the discrepancies introduced by
the users  rather than the reference sets 

   related work
our research is driven by the principal that the cost of annotating documents for the
semantic web should be free  that is  automatic and invisible to users  hendler        
many researchers have followed this path  attempting to automatically mark up documents
for the semantic web  as proposed here  cimiano  handschuh    staab        dingli 
ciravegna    wilks        handschuh  staab    ciravegna        vargas vera  motta 
domingue  lanzoni  stutt    ciravegna         however  these systems rely on lexical
information  such as part of speech tagging or shallow natural language processing to do
their extraction annotation  e g   amilcare  ciravegna         this is not an option when
the data is ungrammatical  like the post data  in a similar vein  there are systems such as
adel  lerman  gazen  minton    knoblock        which rely on the structure to identify
and annotate records in web pages  again  the failure of the posts to exhibit structure
makes this approach inappropriate  so  while there is a fair amount of work in automatic
labeling  there is little emphasis on techniques that could label text that is both unstructured
and ungrammatical 
although the idea of record linkage is not new  fellegi   sunter        and is well studied
even now  bilenko   mooney        most current research focuses on matching one set of
records to another set of records based on their decomposed attributes  there is little work
on matching data sets where one record is a single string composed of the other data sets
attributes to match on  as in the case with posts and reference sets  the whirl system
 cohen        allows for record linkage without decomposed attributes  but as shown in
section     phoebus outperforms whirl  since whirl relies solely on the vector based
cosine similarity between the attributes  while phoebus exploits a larger set of features to
represent both field and record level similarity  we note with interest the erocs system
 chakaravarthy  gupta  roy    mohania        where the authors tackle the problem of
linking full text documents with relational databases  the technique involves filtering out
all non nouns from the text  and then finding the matches in the database  this is an
intriguing approach  interesting future work would involve performing a similar filtering for
larger documents and then applying the phoebus algorithm to match the remaining nouns
to reference sets 
using the reference sets attributes as normalized values is similar to the idea of data
cleaning  however  most data cleaning algorithms assume tuple to tuple transformations
    www kbb com

   

firelational data from unstructured data sources

 lee et al         chaudhuri et al          that is  some function maps the attributes of
one tuple to the attributes of another  this approach would not work on ungrammatical
and unstructured data  where all attributes are embedded within the post  which maps to
a set of attributes from the reference set 
although our work describes a technique for information extraction  many methods 
such as conditional random fields  crf   assume at least some structure in the extracted
attributes to do the extraction  as our extraction experiments show  phoebus outperforms such methods  such as the simple tagger implementation of conditional random
fields  mccallum         other ie approaches  such as datamold  borkar  deshmukh   
sarawagi        and cram  agichtein   ganti         segment whole records  like bibliographies  into attributes  with little structural assumption  in fact  cram even uses
reference sets to aid its extraction  however  both systems require that every token of a
record receive a label  which is not possible with posts that are filled with irrelevant  junk
tokens  along the lines of cram and datamold  the work of bellare and mccallum       
uses a reference set to train a crf to extract data  which is similar to our phoebuscrf
implementation  however  there are two differences between phoebuscrf and their work
 bellare   mccallum         first  the work of bellare and mccallum        mentions
that reference set records are matched using simple heuristics  but it is unclear how this is
done  in our work  matching is done explicitly and accurately through record linkage  second  their work only uses the records from the reference set to label tokens for training an
extraction module  while phoebuscrf uses the actual values from the matching reference
set record to produce useful features for extraction and annotation 
another ie approach similar to ours performs named entity recognition using semicrfs with a dictionary component  cohen   sarawagi         which functions like a
reference set  however  in their work the dictionaries are defined as lists of single attribute
entities  so finding an entity in the dictionary is a look up task  our reference sets are
relational data  so finding the match becomes a record linkage task  further  their work on
semi crfs  cohen   sarawagi        focuses on the task of labeling segments of tokens
with a uniform label  which is especially useful for named entity recognition  in the case
of posts  however  phoebus needs to relax such a restriction because in some cases such
segments will be interrupted  as the case of a hotel name with the area in the middle of
the hotel name segment  so  unlike their work  phoebus makes no assumptions about the
structure of posts  recently  semi crfs have been extended to use database records in the
task of integrating unstructured data with relational databases  mansuri   sarawagi        
this work is similar to ours in that it links unstructured data  such as paper citations  with
relational databases  such as reference sets of authors and venues  the difference is that we
view this as a record linkage task  namely finding the right reference set tuple to match  in
their paper  even though they use matches from the database to aid extraction  they view
the linkage task as an extraction procedure followed by a matching task  lastly  we are
not the first to consider structured svms for information extraction  previous work used
structured svms to perform named entity recognition  tsochantaridis et al         but
their extraction task does not use reference sets 
our method of aiding information extraction with outside information  in the form of
reference sets  is similar to the work on ontology based information extraction  embley 
campbell  jiang  liddle  ng  quass    smith         later versions of their work even talk
   

fimichelson   knoblock

about using ontology based information extraction as a means to semantically annotate unstructured data such as car classifieds  ding  embley    liddle         however  in contrast
to our work  the information extraction is performed by a keyword lookup into the ontology
along with structural and contextual rules to aid the labeling  the ontology itself contains
keyword misspellings and abbreviations  so that the look up can be performed in the presence of noisy data  we believe the ontology based extraction approach is less scalable than
a record linkage type matching task because creating and maintaining the ontology requires
extensive data engineering in order to encompass all possible common spelling mistakes and
abbreviations  further  if new data is added to the ontology  additional data engineering
must be performed  in our work  we can simply add new tuples to our reference set  lastly 
in contrast to our work  this ontology based work assumes contextual and structural rules
will apply  making an assumption about the data to extract from  in our work  we make
no such assumptions about the structure of the text we are extracting from 
yet another interesting approach to information extraction using ontologies is the textpresso system which extracts data from biological text  muller   sternberg         this
system uses a regular expression based keyword look up to label tokens in some text based
on the ontology  once all tokens are labeled  textpresso can perform fact extraction
by extracting sequences of labeled tokens that fit a particular pattern  such as gene allele
reference associations  although this system again uses a reference set for extraction  it
differs in that it does a keyword look up into the lexicon 
in recent work on learning efficient blocking schemes bilenko et al          developed a
system for learning disjunctive normal form blocking schemes  however  they learn their
schemes using a graphical set covering algorithm  while we use a version of the sequential
covering algorithm  sca   there are also similarities between our bsl algorithm and work
on mining association rules from transaction data  agrawal  imielinski    swami        
both algorithms discover propositional rules  further  both algorithms use multiple passes
over a data set to discover their rules  however despite these similarities  the techniques
really solve different problems  bsl generates a set of candidate matches with a minimal
number of false positives  to do this  bsl learns conjunctions that are maximally specific
 eliminating many false positives  and unions them together as a single disjunctive rule  to
cover the different true positives   since the conjunctions are maximally specific  bsl uses
sca underneath  which learns rules in a depth first  general to specific manner  mitchell 
       on the other hand  the work of mining association rules  agrawal et al         looks
for actual patterns in the data that represent some internal relationships  there may be
many such relationships in the data that could be discovered  so this approach covers the
data in a breadth first fashion  selecting the set of rules at each iteration and extending
them by appending to each a new possible item 

   conclusion
this article presents an algorithm for semantically annotating text that is ungrammatical
and unstructured  unstructured  ungrammatical sources contain much information  but
cannot support structured queries  our technique allows for more informative use of the
sources  using our approach  ebay agents could monitor the auctions looking for the best
deals  or a user could find the average price of a four star hotel in san diego  such semantic
   

firelational data from unstructured data sources

annotation is necessary as society transitions into the semantic web  where information
requires annotation to be useful for agents  but users are unwilling to do the extra work to
provide the required annotation 
in the future  our technique could link with a mediator framework  thakkar  ambite   
knoblock        for automatically acquiring reference sets  this is similar to automatically
incorporating secondary sources for record linkage  michalowski  thakkar    knoblock 
       the automatic formulation of queries to retrieve the correct domain reference set
is a direction of future research  with a mediator framework in place  phoebus could
incorporate as many reference sets as needed for full coverage of possible attribute values
and attribute types 
unsupervised approaches to record linkage and extraction are also topics of future research  by including unsupervised record linkage and extraction with a mediator component  the approach would be entirely self contained  making semantic annotation of posts
a more automatic process  also  the current implementation only gives one class label per
token  ideally phoebus would give a token all possible labels  and then remove the extraneous tokens when the systems cleans the attributes  as described in section    this
disambiguation should lead to much higher accuracy during extraction 
future work could investigate the inclusion of thesauri for terms in the attributes  with
the frequency of replacement of the terms taken into consideration  also  exploring technologies that automatically construct the reference sets  and eventually thesauri  from the
numerous ontologies on the semantic web is an intriguing research path 
the long term goal for annotation and extraction from unstructured  ungrammatical
sources involves automating the entire process  if the record linkage and extraction methods
could become unsupervised  then our approach could automatically generate and incorporate the reference sets  and then apply them to automatically annotate the data source 
this would be an ideal approach for making the semantic web more useful  with no user
involvement 

acknowledgments
this research is based upon work supported in part by the national science foundation under award number iis          in part by the air force office of scientific research under
grant number fa                and in part by the defense advanced research projects
agency  darpa   through the department of the interior  nbc  acquisition services division  under contract no  nbchd       
the u s government is authorized to reproduce and distribute reports for governmental purposes notwithstanding any copyright annotation thereon  the views and conclusions
contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements  either expressed or implied  of any of the
above organizations or any person connected with them 
   

fimichelson   knoblock

references
agichtein  e     ganti  v          mining reference tables for automatic text segmentation 
in the proceedings of the   th acm conference on knowledge discovery and data
mining  pp          acm press 
agrawal  r   imielinski  t     swami  a          mining association rules between sets of
items in large databases  in proceedings of the acm sigmod international conference on management of data  pp          acm press 
baxter  r   christen  p     churches  t          a comparison of fast blocking methods for
record linkage  in proceedings of the  th acm sigkdd workshop on data cleaning 
record linkage  and object identification  pp       
bellare  k     mccallum  a          learning extractors from unlabeled text using relevant
databases  in proceedings of the aaai workshop on information integration on the
web  pp       
bilenko  m   kamath  b     mooney  r  j          adaptive blocking  learning to scale up
record linkage and clustering  in proceedings of the  th ieee international conference
on data mining  pp       
bilenko  m     mooney  r  j          adaptive duplicate detection using learnable string
similarity measures  in proceedings of the  th acm international conference on
knowledge discovery and data mining  pp        acm press 
borkar  v   deshmukh  k     sarawagi  s          automatic segmentation of text into
structured records  in proceedings of the acm sigmod international conference on
management of data  pp          acm press 
califf  m  e     mooney  r  j          relational learning of pattern match rules for
information extraction  in proceedings of the   th national conference on artificial
intelligence  pp         
chakaravarthy  v  t   gupta  h   roy  p     mohania  m          efficiently linking text
documents with relevant structured information  in proceedings of the international
conference on very large data bases  pp          vldb endowment 
chaudhuri  s   ganjam  k   ganti  v     motwani  r          robust and efficient fuzzy
match for online data cleaning  in proceedings of acm sigmod international conference on management of data  pp          acm press 
cimiano  p   handschuh  s     staab  s          towards the self annotating web  in
proceedings of the   th international conference on world wide web  pp         
acm press 
ciravegna  f          adaptive information extraction from text by rule induction and
generalisation   in proceedings of the   th international joint conference on artificial
intelligence  pp           
   

firelational data from unstructured data sources

cohen  w     sarawagi  s          exploiting dictionaries in named entity extraction  combining semi markov extraction processes and data integration methods  in proceedings
of the   th acm international conference on knowledge discovery and data mining 
pp        seattle  washington  acm press 
cohen  w  w          data integration using similarity joins and a word based information
representation language  acm transactions on information systems                 
cohen  w  w   ravikumar  p     feinberg  s  e          a comparison of string metrics
for matching names and records  in proceedings of the acm sigkdd workshop on
data cleaning  record linkage  and object consoliation  pp       
crescenzi  v   mecca  g     merialdo  p          roadrunner  towards automatic data
extraction from large web sites  in proceedings of   th international conference on
very large data bases  pp          vldb endowment 
ding  y   embley  d  w     liddle  s  w          automatic creation and simplified querying of semantic web content  an approach based on information extraction ontologies 
in proceedings of the asian semantic web conference  pp         
dingli  a   ciravegna  f     wilks  y          automatic semantic annotation using unsupervised information extraction and integration  in proceedings of the k cap workshop on knowledge markup and semantic annotation 
elfeky  m  g   verykios  v  s     elmagarmid  a  k          tailor  a record linkage
toolbox  in proceedings of   th international conference on data engineering  pp 
     
embley  d  w   campbell  d  m   jiang  y  s   liddle  s  w   ng  y  k   quass  d    
smith  r  d          conceptual model based data extraction from multiple record
web pages  data knowledge engineering                 
fellegi  i  p     sunter  a  b          a theory for record linkage  journal of the american
statistical association               
handschuh  s   staab  s     ciravegna  f          s cream   semi automatic creation of
metadata  in proceedings of the   th international conference on knowledge engineering and knowledge management  pp          springer verlag 
hendler  j          agents and the semantic web  ieee intelligent systems               
hernandez  m  a     stolfo  s  j          real world data is dirty  data cleansing and the
merge purge problem  data mining and knowledge discovery             
jaro  m  a          advances in record linkage methodology as applied to matching the
     census of tampa  florida  journal of the american statistical association     
       
joachims  t          advances in kernel methods   support vector learning  chap     
making large scale svm learning practical  mit press 
   

fimichelson   knoblock

lafferty  j   mccallum  a     pereira  f          conditional random fields  probabilistic models for segmenting and labeling sequence data  in proceedings of the   th
international conference on machine learning  pp          morgan kaufmann 
lee  m  l   ling  t  w   lu  h     ko  y  t          cleansing data for mining and
warehousing  in proceedings of the   th international conference on database and
expert systems applications  pp          springer verlag 
lerman  k   gazen  c   minton  s     knoblock  c  a          populating the semantic web 
in proceedings of the aaai workshop on advances in text extraction and mining 
levenshtein  v  i          binary codes capable of correcting deletions  insertions  and
reversals  english translation in soviet physics doklady                 
mansuri  i  r     sarawagi  s          integrating unstructured data into relational
databases  in proceedings of the international conference on data engineering  p     
ieee computer society 
mccallum  a         
mallet 
http   mallet cs umass edu 

a

machine

learning

for

language

toolkit 

mccallum  a   nigam  k     ungar  l  h          efficient clustering of high dimensional
data sets with application to reference matching  in proceedings of the  th acm
sigkdd  pp         
michalowski  m   thakkar  s     knoblock  c  a          automatically utilizing secondary
sources to align information across sources  in ai magazine  special issue on semantic
integration  vol      pp       
michelson  m     knoblock  c  a          semantic annotation of unstructured and ungrammatical text  in proceedings of the   th international joint conference on artificial
intelligence  pp           
michelson  m     knoblock  c  a          learning blocking schemes for record linkage  in
proceedings of the   st national conference on artificial intelligence 
michelson  m     knoblock  c  a          unsupervised information extraction from unstructured  ungrammatical data sources on the world wide web  international journal
of document analysis and recognition  ijdar   special issue on noisy text analytics 
mitchell  t  m          machine learning  mcgraw hill  new york 
muller  h  m     sternberg  e  e  k  p  w          textpresso  an ontology based information retrieval and extraction system for biological literature  plos biology         
muslea  i   minton  s     knoblock  c  a          hierarchical wrapper induction for
semistructured information sources  autonomous agents and multi agent systems 
               
   

firelational data from unstructured data sources

newcombe  h  b          record linkage  the design of efficient systems for linking records
into individual and family histories  american journal of human genetics         
       
porter  m  f          an algorithm for suffix stripping  program                 
smith  t  f     waterman  m  s          identification of common molecular subsequences 
journal of molecular biology              
soderland  s          learning information extraction rules for semi structured and free
text  machine learning                   
thakkar  s   ambite  j  l     knoblock  c  a          a data integration approach to
automatically composing and optimizing web services  in proceedings of the icaps
workshop on planning and scheduling for web and grid services 
tsochantaridis  i   hofmann  t   joachims  t     altun  y          support vector machine
learning for interdependent and structured output spaces  in proceedings of the   st
international conference on machine learning  p       acm press 
tsochantaridis  i   joachims  t   hofmann  t     altun  y          large margin methods
for structured and interdependent output variables  journal of machine learning
research              
vargas vera  m   motta  e   domingue  j   lanzoni  m   stutt  a     ciravegna  f         
mnm  ontology driven semi automatic and automatic support for semantic markup 
in proceedings of the   th international conference on knowledge engineering and
management  pp         
wellner  b   mccallum  a   peng  f     hay  m          an integrated  conditional model
of information extraction and coreference with application to citation matching  in
proceedings of the   th conference on uncertainty in artificial intelligence  pp     
    
winkler  w  e     thibaudeau  y          an application of the fellegi sunter model of
record linkage to the      u s  decennial census  tech  rep   statistical research
report series rr      u s  bureau of the census 
zhai  c     lafferty  j          a study of smoothing methods for language models applied
to ad hoc information retrieval  in proceedings of the   th acm sigir conference
on research and development in information retrieval  pp          acm press 

   

fi