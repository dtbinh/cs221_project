journal artificial intelligence research                

submitted        published      

planning durative actions stochastic domains
mausam
daniel s  weld

mausam   cs   washington   edu
weld   cs   washington   edu

dept computer science engineering
box         university washington
seattle  wa       usa

abstract
probabilistic planning problems typically modeled markov decision process  mdp  
mdps  otherwise expressive model  allow sequential  non durative actions 
poses severe restrictions modeling solving real world planning problem  extend
mdp model incorporate    simultaneous action execution     durative actions     stochastic durations  develop several algorithms combat computational explosion introduced
features  key theoretical ideas used building algorithms modeling complex problem mdp extended state action space  pruning irrelevant actions  sampling
relevant actions  using informed heuristics guide search  hybridizing different planners
achieve benefits both  approximating problem replanning  empirical evaluation
illuminates different merits using various algorithms  viz   optimality  empirical closeness
optimality  theoretical error bounds  speed 

   introduction
recent progress achieved planning researchers yielded new algorithms relax  individually  many classical assumptions  example  successful temporal planners sgplan 
sapa  etc   chen  wah    hsu          kambhampati        able model actions take
time  probabilistic planners gpt  lao   spudd  etc   bonet   geffner        hansen  
zilberstein        hoey  st aubin  hu    boutilier        deal actions probabilistic
outcomes  etc  however  order apply automated planning many real world domains must
eliminate larger groups assumptions concert  example  nasa researchers note
optimal control nasa mars rover requires reasoning uncertain  concurrent  durative
actions mixture discrete metric fluents  bresina  dearden  meuleau  smith    washington         todays planners handle large problems deterministic concurrent
durative actions  mdps provide clear framework non concurrent durative actions
face uncertainty  researchers considered concurrent  uncertain  durative actions
focus paper 
example consider nasa mars rovers  spirit oppurtunity  goal
gathering data different locations various instruments  color infrared cameras  microscopic imager  mossbauer spectrometers etc   transmitting data back earth  concurrent
actions essential since instruments turned on  warmed calibrated  rover
moving  using instruments transmitting data  similarly  uncertainty must explicitly
confronted rovers movement  arm control actions cannot accurately predicted 
furthermore  actions  e g   moving locations setting experiments  take
time  fact  temporal durations uncertain rover might lose way
c
    
ai access foundation  rights reserved 

fim ausam   w eld

take long time reach another location  etc  able solve planning problems encountered rover  planning framework needs explicitly model domain constructs
concurrency  actions uncertain outcomes uncertain durations 
paper present unified formalism models domain features together 
concurrent markov decision processes  comdps  extend mdps allowing multiple actions per
decision epoch  use comdps base model planning problems involving concurrency 
problems durative actions  concurrent probabilistic temporal planning  cptp   formulated
comdps extended state space  formulation able incorporate uncertainty
durations form probabilistic distributions 
solving planning problems poses several computational challenges  concurrency  extended durations  uncertainty durations lead explosive growth state space 
action space branching factor  develop two techniques  pruned rtdp sampled rtdp
address blowup concurrency  develop dur family algorithms handle
stochastic durations  algorithms explore different points running time vs  solutionquality tradeoff  different algorithms propose several speedup mechanisms    pruning provably sub optimal actions bellman backup     intelligent sampling action
space     admissible inadmissible heuristics computed solving non concurrent problems    
hybridizing two planners obtain hybridized planner finds good quality solution intermediate running times     approximating stochastic durations mean values replanning    
exploiting structure multi modal duration distributions achieve higher quality approximations 
rest paper organized follows  section   discuss fundamentals
mdps real time dynamic programming  rtdp  solution method  section   describe
model concurrent mdps  section   investigates theoretical properties temporal
problems  section   explains formulation cptp problem deterministic durations 
algorithms extended case stochastic durations section    section supported
empirical evaluation techniques presented section  section   survey
related work area  conclude future directions research sections     

   background
planning problems probabilistic uncertainty often modeled using markov decision processes  mdps   different research communities looked slightly different formulations
mdps  versions typically differ objective functions  maximizing reward vs  minimizing
cost   horizons  finite  infinite  indefinite  action representations  dbn vs  parametrized action
schemata   formulations similar nature  algorithms solve
them  though  methods proposed paper applicable variants models 
clarity explanation assume particular formulation  known stochastic shortest path
problem  bertsekas        
define markov decision process  m  tuple hs  a  ap  pr  c  g  s 
finite set discrete states  use factored mdps  i e   compactly represented
terms set state variables 
finite set actions 
  

fip lanning urative actions tochastic omains

state variables   x    x    x    x    p  
action
precondition
effect
toggle x 
p  
x  x 
toggle x 
p  
x  x 
toggle x 
true
x  x 
change
toggle x 
true
x  x 
change
toggle p  
true
p   p  
goal   x       x       x       x     

probability
 
 
   
   
   
   
 

figure    probabilistic strips definition simple mdp potential parallelism
ap defines applicability function  ap   p a   denotes set actions
applied given state  p represents power set  
pr          transition function  write pr s   s  a  denote
probability arriving state s  executing action state s 
c      cost model  write c s  a  s    denote cost incurred
state s  reached executing action state s 
g set absorbing goal states  i e   process ends one states
reached 
s  start state 
assume full observability  i e   execution system complete access new state
action performed  seek find optimal  stationary policy i e   function
  minimizes expected cost  over indefinite horizon  incurred reach goal
state  note cost function  j     mapping states expected cost reaching goal
state defines policy follows 
j  s    argmin

x

pr s   s  a  c s  a  s      j s   




   

aap s  s 

optimal policy derives optimal cost function  j   satisfies following pair
bellman equations 
j  s       g else
j  s    min

aap s 

x

pr s   s  a  c s  a  s      j  s   




   

s 

example  figure   defines simple mdp four state variables  x            x    need
set using toggle actions  actions  e g   toggle x  probabilistic 
various algorithms developed solve mdps  value iteration dynamic programming approach optimal cost function  the solution equations    calculated
limit series approximations  considering increasingly long action sequences  jn  s 
  

fim ausam   w eld

cost state iteration n  cost state next iteration calculated
process called bellman backup follows 
jn    s    min

aap s 

x

pr s   s  a  c s  a  s      jn  s   




   

s 

value iteration terminates s   jn  s  jn   s     termination guaranteed      furthermore  limit  sequence  ji   guaranteed converge
optimal cost function  j   regardless initial values long goal reached every reachable state non zero probability  unfortunately  value iteration tends quite slow 
since explicitly updates every state   s  exponential number domain features  one
optimization restricts search part state space reachable initial state s    two algorithms exploiting reachability analysis lao   hansen   zilberstein        focus 
rtdp  barto  bradtke    singh        
rtdp  conceptually  lazy version value iteration states get updated proportion frequency visited repeated executions greedy policy 
rtdp trial path starting s    following greedy policy updating costs
states visited using bellman backups  trial ends goal reached number
updates exceeds threshold  rtdp repeats trials convergence  note common states
updated frequently  rtdp wastes time states unreachable  given current
policy  rtdps strength ability quickly produce relatively good policy  however  complete
convergence  at every relevant state  slow less likely  but potentially important  states get
updated infrequently  furthermore  rtdp guaranteed terminate  labeled rtdp  lrtdp 
fixes problems clever labeling scheme focuses attention states value
function yet converged  bonet   geffner         labeled rtdp guaranteed terminate 
guaranteed converge  approximation optimal cost function  for states reachable using optimal policy  initial cost function admissible  costs  c  positive
goal reachable reachable states non zero probability 
mdps powerful framework model stochastic planning domains  however  mdps make
two unrealistic assumptions    actions need executed sequentially     actions
instantaneous  unfortunately  many real world domains assumptions
unrealistic  example  concurrent actions essential mars rover  since instruments
turned on  warmed calibrated rover moving  using instruments
transmitting data  moreover  action durations non zero stochastic rover might
lose way navigating may take long time reach destination  may make multiple
attempts finding accurate arm placement  paper successively relax two
assumptions build models algorithms scale spite additional complexities
imposed general models 

   concurrent markov decision processes
define new model  concurrent mdp  comdp   allows multiple actions executed
parallel  model different semi mdps generalized state semi mdps  younes
  simmons      b  incorporate action durations explicitly  comdps focus
adding concurrency mdp framework  input comdp slightly different
mdp hs  a  apk   prk   ck   g  s  i  new applicability function  probability model cost
  

fip lanning urative actions tochastic omains

 apk   prk ck respectively  encode distinction allowing sequential executions
single actions versus simultaneous executions sets actions 
    model
set states  s   set actions  a   goals  g  start state  s    follow input mdp 
difference lies fact instead executing one action time  may execute
multiple them  let us define action combination  a  set one actions
executed parallel  action combination new unit operator available agent 
comdp takes following new inputs
apk defines new applicability function  apk   p p a    denotes set action
combinations applied given state 
prk   p a         transition function  write prk  s   s  a  denote
probability arriving state s  executing action combination state s 
ck   p a     cost model  write ck  s  a  s    denote cost incurred
state s  reached executing action combination state s 
essence  comdp takes action combination unit operator instead single action 
approach convert comdp equivalent mdp  mk   specified
tuple hs  p a   apk   prk   ck   g  s  solve using known mdp algorithms 
    case study  comdp probabilistic strips
general comdp could require exponentially larger input mdp  since transition model  cost model applicability function defined terms action combinations
opposed actions  compact input representation general comdp interesting  open
research question future  work  consider special class compact comdp
one defined naturally via domain description similar probabilistic strips
representation mdps  boutilier  dean    hanks        
given domain encoded probabilistic strips compute safe set co executable
actions  safe semantics  probabilistic dynamics gets defined consistent way
describe below 
      pplicability f unction
first discuss compute sets actions executed parallel since
actions may conflict other  adopt classical planning notion mutual exclusion  blum   furst        apply factored action representation probabilistic strips 
two distinct actions mutex  may executed concurrently  state one following occurs 
   inconsistent preconditions
   outcome one action conflicts outcome
   precondition one action conflicts  possibly probabilistic  effect other 
  

fim ausam   w eld

   effect one action possibly modifies feature upon another actions transition
function conditioned upon 
additionally  action never mutex itself  essence  non mutex actions interact effects executing sequence a    a  equals a    a  semantics
parallel executions clear 
example  continuing figure    toggle x    toggle x  toggle x  execute parallel
toggle x  toggle x  mutex conflicting preconditions  similarly  toggle x 
toggle p   mutex effect toggle p   interferes precondition toggle x   
toggle x  outcomes depended toggle x  would mutex too  due point   above 
example  toggle x  toggle x  mutex effect toggle x  follows  togglex  probability x  x      else       
applicability function defined set action combinations  a  action
independently applicable actions pairwise non mutex other 
note pairwise concurrency sufficient ensure problem free concurrency multiple
actions a  formally apk defined terms original definition ap follows 
apk  s     a a a  a  a  a  a  ap s  mutex a  a    

   

      ransition f unction
let    a    a            ak   action combination applicable s  since none actions
mutex  transition function may calculated choosing arbitrary order apply
follows 
prk  s   s  a   

x

   

x

pr s   s  a   pr s   s    a          pr s   sk    ak  

   

s   s      sk

define applicability function transition function allowing consistent set actions executable concurrently  alternative definitions possible 
instance  one might willing allow executing two actions together probability
conflict small  conflict may defined two actions asserting contradictory effects
one negating precondition other  case  new state called failure could created system transitions state case conflict  transition may
computed reflect low probability transition failure state 
although impose model conflict free  techniques dont actually depend assumption explicitly extend general comdps 
      c ost model
make small change probabilistic strips representation  instead defining single
cost  c  action  define additively sum resource time components follows 
let durative cost  i e   cost due time taken complete action 
let r resource cost  i e   cost resources used action 
  

fip lanning urative actions tochastic omains

assuming additivity think cost action c s  a  s      t s  a  s      r s  a  s    
sum time resource usage  hence  cost model combination actions terms
components may defined as 
ck  s   a    a         ak    s     

k
x

r s  ai   s      max  t s  ai   s    
i    k

i  

   

example  mars rover might incur lower cost preheats instrument changing
locations executes actions sequentially  total time reduced
energy consumed change 
    solving comdp mdp algorithms
taken concurrent mdp allowed concurrency actions formulated equivalent mdp  mk   extended action space  rest paper use term comdp
refer equivalent mdp mk  
      b ellman equations
extend equations   set equations representing solution comdp 
jk  s       g else
jk  s    min

aapk  s 

x

n



prk  s   s  a  ck  s  a  s      jk  s   

   

s 

equations traditional mdp  except instead considering single
actions backup state  need consider applicable action combinations  thus 
small change must made traditional algorithms  e g   value iteration  lao   labeled rtdp  
however  since number action combinations worst case exponential  a   efficiently
solving comdp requires new techniques  unfortunately  structure exploit easily 
since optimal action state classical mdp solution may even appear optimal
action combination associated concurrent mdp 
theorem   actions optimal combination comdp  mk   may individually suboptimal mdp m 
proof  domain figure   let us additional action toggle x   toggles x 
x  probability     toggles exactly one x  x  probability      each  let
actions take one time unit each  therefore cost action combination one well 
let start state x       x       x       x      p        mdp optimal
action start state toggle x     however  comdp mk optimal combination
 toggle x    toggle x      
    pruned bellman backups
recall trial  labeled rtdp performs bellman backups order calculate costs
applicable actions  or case  action combinations  chooses best action  combination   describe two pruning techniques reduce number backups computed 
  

fim ausam   w eld

let qk  s  a  expected cost incurred executing action combination state
following greedy policy  i e 
qkn  s  a   

x

n



prk  s   s  a  ck  s  a  s      jkn   s   

   

s 

bellman update thus rewritten as 
jkn  s   

min

aapk  s 

qkn  s  a 

   

      c ombo  s kipping
since number applicable action combinations exponential  would prune
suboptimal combinations  following theorem imposes lower bound qk  s  a  terms
costs qk  values single actions  theorem costs actions may depend
action starting ending state  i e   states s  s  c s  a  s      c a  
theorem   let    a    a            ak   action combination applicable state s 
comdp probabilistic strips  costs dependent actions qkn values
monotonically non decreasing
qk  s  a  max qk  s   ai      ck  a 
i    k

k
x

 

ck   ai   

i  

proof 
qkn  s  a    ck  a   

x

prk  s   s  a jkn   s   

 using eqn    

s 



x

prk  s   s  a jkn   s      qkn  s  a  ck  a 

    

s 

qkn  s   a       ck   a      

x

pr s    s  a   jkn   s    

s  

 

ck   a      

x

 

  

pr s  s  a    ck   a      

s  

x

   

  

   

pr s  s   a   jkn   s  

s   

 using eqns      
  ck   a       ck   a      

x

   

   

prk  s  s   a    a    jkn   s  

s   


 

k
x
i  
k
x

ck   ai     

x

prk  s   s  a jknk  s   

 repeating actions a 

s 

ck   ai       qknk    s  a  ck  a  

i  

replacing n n   k  
  

 using eqn     

fip lanning urative actions tochastic omains

qkn  s  a  qkn k   s   a       ck  a 

k
x

 

ck   ai   

i  

qkn  s   a       ck  a 

k
x

 

ck   ai   

 monotonicity qkn  

i  



max qkn  s   ai      ck  a 

i    k

k
x

 

ck   ai   

i  

 

proof assumes equation   probabilistic strips  following corollary
used prune suboptimal action combinations 
corollary   let djkn  s e upper bound jkn  s  
djkn  s e   max qkn  s   ai      ck  a 
i    k

k
x

 

ck   ai   

i  

cannot optimal state iteration 
proof  let    a    a            ak   optimal combination state iteration n  then 
djkn  s e jkn  s 
jkn  s    qkn  s   
combining theorem  
djkn  s e maxi    k qkn  s   ai     

ck  an  



k
x

 

ck   ai     

i  

corollary   justifies pruning rule  combo skipping  preserves optimality iteration
algorithm maintains cost function monotonicity  powerful bellman backup
based algorithms preserve monotonicity started admissible cost function  apply
combo skipping  one must compute qk  s   a   values single actions applicable
s  calculate djkn  s e one may use optimal combination state previous iteration
 aopt   compute qkn  s  aopt    value gives upper bound value jkn  s  
example  consider figure    let single action incur unit cost  let cost action combination be  ck  a              a   let state               represent ordered values x       x   
   x       x       p        suppose  nth iteration  cost function assigns values 
jkn  s       jkn  s                     jkn  s                     jkn  s                     let aopt
state  toggle x    toggle x     now  qkn    s   toggle x       ck   toggle x       jkn  s       
qkn    s  aopt     ck  aopt                 jkn  s          jkn  s          jkn  s         
apply corollary   skip combination  toggle x    toggle x    iteration  since
using toggle x  a    djkn    s e   qkn    s  aopt                              
experiments show combo skipping yields considerable savings  unfortunately  comboskipping weakness prunes combination single iteration  contrast 
second rule  combo elimination  prunes irrelevant combinations altogether 
  

fim ausam   w eld

      c ombo  e limination
adapt action elimination theorem traditional mdps  bertsekas        prove similar
theorem comdps 
theorem   let action combination applicable state s  let bqk  s  a c denote
lower bound qk  s  a   bqk  s  a c   djk  s e never optimal combination
state s 
proof  comdp mdp new action space  original proof mdps  bertsekas 
      holds replacing action action combination   
order apply theorem pruning  one must able evaluate upper lower
bounds  using admissible cost function starting rtdp search  or value iteration 
lao  etc    current cost jkn  s  guaranteed lower bound optimal cost  thus 
qkn  s  a  lower bound qk  s  a   thus  easy compute left hand side
inequality  calculate upper bound optimal jk  s   one may solve mdp m 
i e   traditional mdp forbids concurrency  much faster solving comdp 
yields upper bound cost  forbidding concurrency restricts policy use
strict subset legal action combinations  notice combo elimination used general
mdps restricted comdps probabilistic strips 
example  continuing previous example  let a  toggle x    qkn    s  a    ck  a   
jkn  s        djk  s e          from solving mdp m              eliminated
state remaining iterations   
used fashion  combo elimination requires additional overhead optimally solving
single action mdp m  since algorithms rtdp exploit state space reachability limit
computation relevant states  computation incrementally  new states visited
algorithm 
combo elimination requires computation current value qk  s  a   for lower
bound qk  s  a    differs combo skipping avoids computation  however 
combo elimination prunes combination  never needs reconsidered  thus 
tradeoff  one perform expensive computation  hoping long term pruning  try
cheaper pruning rule fewer benefits  since q value computation costly step  adopt
following heuristic  first  try combo skipping  fails prune combination  attempt
combo elimination  succeeds  never consider again  tried implementing
heuristics  as     combination skipped repeatedly  try prune altogether combo elimination     every state  try combo elimination probability p  neither
alternative performed significantly better  kept original  lower overhead  heuristic 
since combo skipping change step labeled rtdp combo elimination removes provably sub optimal combinations  pruned labeled rtdp maintains convergence  termination  optimality efficiency  used admissible heuristic 
    sampled bellman backups
since fundamental challenge posed comdps explosion action combinations  sampling promising method reduce number bellman backups required per state 
describe variant rtdp  called sampled rtdp  performs backups random set
  

fip lanning urative actions tochastic omains

action combinations    choosing distribution favors combinations likely
optimal  generate distribution by 
   using combinations previously discovered low qk  values  recorded memoizing best combinations per state  iteration 
   calculating qk  values applicable single actions  using current cost function 
biasing sampling combinations choose ones contain actions low
qk  values 

algorithm   sampled bellman backup state  m 
  
  
  
  
  
  
  
  
  
   
   

function   samplecomb state  i  l 
  
  
  
  
  
  
  
  
  
   
   
   
   
   
   

  returns best combination found

list l     a list applicable actions values
action
compute qk  state   action  
insert ha    qk  state   action  i l
    m 
newcomb   samplecomb state  i  l  
compute qk  state  newcomb 
clear memoizedlist state 
compute qmin minimum qk values computed line  
store combinations qk  state  a    qmin memoizedlist state 
return first entry memoizedlist state 

  returns ith combination sampled backup

size memoizedlist state  
return ith entry memoizedlist state    return combination memoized previous iteration
newcomb  
repeat
randomly sample action l proportional value
insert newcomb
remove actions mutex l
l empty
done   true
else  newcomb      
done   false   sample least   actions per combination
else
 newcomb 
done   true prob   newcomb   
done
return newcomb

approach exposes exploration   exploitation trade off  exploration  here  refers testing wide range action combinations improve understanding relative merit  exploitation  hand  advocates performing backups combinations previously
shown best  manage tradeoff carefully maintaining distribution
combinations  first  memoize best combinations per state  always backed up
   similar action sampling approach used context space shuttle scheduling reduce number
actions considered value function computation  zhang   dietterich        

  

fim ausam   w eld

bellman update  combinations constructed incremental probabilistic process 
builds combination first randomly choosing initial action  weighted individual qk  value   deciding whether add non mutex action stop growing combination 
many implementations possible high level idea  tried several
found results similar them  algorithm   describes implementation used
experiments  algorithm takes state total number combinations input
returns best combination obtained far  memoizes best combinations
state memoizedlist  function   helper function returns ith combination either
one best combinations memoized previous iteration new sampled combination 
notice line    function    forces sampled combinations least size    since
individual actions already backed  line   algo    
      ermination ptimality
since system consider every possible action combination  sampled rtdp guaranteed choose best combination execute state  result  even started
admissible heuristic  algorithm may assign jkn  s  cost greater optimal jk  s 
i e   jkn  s  values longer admissible  better combination chosen subsequent
iteration  jkn    s  might set lower value jkn  s   thus sampled rtdp monotonic 
unfortunate  since admissibility monotonicity important properties required termination  optimality labeled rtdp  indeed  sampled rtdp loses important theoretical
properties  good news extremely useful practice  experiments  sampled
rtdp usually terminates quickly  returns costs extremely close optimal 
      mproving olution q uality
investigated several heuristics order improve quality solutions found
sampled rtdp  heuristics compensate errors due partial search lack admissibility 
heuristic    whenever sampled rtdp asserts convergence state  immediately
label converged  which would preclude exploration  bonet   geffner         
instead first run complete backup phase  using admissible combinations  rule
easy to detect inconsistencies 
heuristic    run sampled rtdp completion  use cost function produces  j    
initial heuristic estimate  j      subsequent run pruned rtdp  usually 
heuristic  though inadmissible  highly informative  hence  pruned rtdp terminates quite
quickly 
heuristic    run sampled rtdp pruned rtdp  heuristic    except instead
using j    cost function directly initial estimate  scale linearly downward i e  
use j        cj    constant c         guarantees hope
lies admissible side optimal  experience often case
c        run pruned rtdp yields optimal policy quickly 
   ensure termination implemented policy  number trials exceeds threshold  force monotonicity
cost function  achieve termination reduce quality solution 

  

fip lanning urative actions tochastic omains

experiments showed heuristic   returns cost function close optimal  adding
heuristic   improves value moderately  combination heuristics     returns
optimal solution experiments 
    experiments  concurrent mdp
concurrent mdp fundamental formulation  modeling concurrent actions general planning
domain  first compare various techniques solve comdps  viz   pruned sampled rtdp 
following sections use techniques model problems durative actions 
tested algorithms problems three domains  first domain probabilistic
variant nasa rover domain      aips planning competition  long   fox        
multiple objects photographed various rocks tested resulting
data communicated back base station  cameras need focused  arms need
positioned usage  since rover multiple arms multiple cameras  domain
highly parallel  cost function includes resource time components  executing multiple actions parallel cheaper executing sequentially  generated problems
      state variables        reachable states average number applicable
combinations per state  avg ap s    measures amount concurrency problem 
     
tested probabilistic version machineshop domain multiple subtasks  e g  
roll  shape  paint  polish etc    need performed different objects using different
machines  machines perform parallel  capable every task  tested
problems       state variables around        reachable states  avg ap s   ranged
         various problems 
finally  tested artificial domain similar one shown figure   much
complex  domain  boolean variables need toggled  however  toggling probabilistic nature  moreover  certain pairs actions conflicting preconditions thus 
varying number mutex actions may control domains degree parallelism 
problems domain    state variables        reachable states  avg ap s  
           
used labeled rtdp  implemented gpt  bonet   geffner         base mdp
solver  implemented c    implemented  various algorithms  unpruned rtdp  u rtdp   pruned rtdp using combo skipping  ps  rtdp   pruned rtdp using combo
skipping combo elimination  pse  rtdp   sampled rtdp using heuristic    s rtdp  sampled rtdp using heuristics      value functions scaled      s   rtdp   tested
algorithms number problem instantiations three domains  generated
varying number objects  degrees parallelism  distances goal  experiments
performed     ghz pentium processor   gb ram 
observe  figure   a b   pruning significantly speeds algorithm  comparison pse  rtdp s rtdp s   rtdp  figure   a b   shows sampling dramatic
speedup respect pruned versions  fact  pure sampling  s rtdp  converges extremely
quickly  s   rtdp slightly slower  however  s   rtdp still much faster pse  rtdp 
comparison qualities solutions produced s rtdp s   rtdp w r t  optimal shown
table    observe solutions produced s rtdp always nearly optimal  since
   code may downloaded http   www cs washington edu ai comdp comdp tgz

  

fim ausam   w eld

comparison pruned unpruned rtdp rover domain

comparison pruned unpruned rtdp factory domain
     

y x
ps rtdp
pse rtdp

     

times pruned rtdp  in sec 

times pruned rtdp  in sec 

     

     
     
     
    

y x
ps rtdp
pse rtdp

     
    
    
    
    

 

 
 

    
                       
times unpruned rtdp  in sec 

     

 

    
    
    
    
     
times unpruned rtdp  in sec 

     

figure     a b   pruned vs  unpruned rtdp rover machineshop domains respectively  pruning
non optimal combinations achieves significant speedups larger problems 

comparison pruned sampled rtdp rover domain

comparison pruned sampled rtdp factory domain
    

y x
s rtdp
s  rtdp

    

times sampled rtdp  in sec 

times sampled rtdp  in sec 

     

    
    
    
 

y x
s rtdp
s  rtdp

    
    
    
    
    
    
    
 

 

                                             
times pruned rtdp  pse rtdp     in sec 

 

                                       
times pruned rtdp  pse rtdp     in sec 

figure     a b   sampled vs pruned rtdp rover machineshop domains respectively  random
sampling action combinations yields dramatic improvements running times 

  

fip lanning urative actions tochastic omains

comparison algorithms size problem rover domain
     

s rtdp
s  rtdp
pse rtdp
u rtdp

     

s rtdp
s  rtdp
pse rtdp
u rtdp

     

     

times  in sec 

times  in sec 

comparison different algorithms artificial domain
     

     
     

     

    
    
 

 
 

 e   

 e   
   e   
reach  s   avg ap s  

 e   

   e   

 

    

    

         
avg ap s  

                 

figure     a b   comparison different algorithms size problems rover artificial domains  problem size increases  gap sampled pruned approaches widens
considerably 

results varying number samples rover problem  

   
   
   
   
   
  
 

running times
values start state

   

   

   

                   
concurrency   avg ap s    a 

   

   

    

   

     

   

     

   

     

   

     

  

j  s  

 
 

     

value start state

   

s rtdp pse rtdp

times sampled rtdp  in sec 

speedup   sampled rtdp pruned rtdp

speedup vs  concurrency artificial domain
   

  

  

  

              
number samples

     
      

figure     a   relative speed vs  concurrency artificial domain   b    variation quality solution
efficiency algorithm  with     confidence intervals  number samples sampled rtdp one particular problem rover domain  number samples increase 
quality solution approaches optimal time still remains better pse  rtdp  which
takes     sec  problem  

  

fim ausam   w eld

problem
rover 
rover 
rover 
rover 
rover 
rover 
rover 
artificial 
artificial 
artificial 
machineshop 
machineshop 
machineshop 
machineshop 
machineshop 

j s     s rtdp 
       
       
       
       
      
       
       
      
      
      
       
       
       
       
      

j  s     optimal 
       
       
       
       
      
       
       
      
      
      
       
       
       
       
      

error
      
 
 
     
 
 
     
 
 
 
     
     
     
 
     

table    quality solutions produced sampled rtdp
error s rtdp small  scaling     makes admissible initial cost function pruned
rtdp  indeed  experiments  s   rtdp produced optimal solution 
figure   a b  demonstrates running times vary problem size  use product
number reachable states average number applicable action combinations per state
estimate size problem  the number reachable states artificial domains
same  hence x axis figure   b  avg ap s     figures  verify
number applicable combinations plays major role running times concurrent mdp
algorithms  figure   a   fix factors vary degree parallelism  observe
speedups obtained s rtdp increase concurrency increases  encouraging result 
expect s rtdp perform well large problems inolving high concurrency  even
approaches fail 
figure   b   present another experiment vary number action combinations sampled backup  solution quality inferior sampling
combinations  quickly approaches optimal increasing number samples 
experiments sample    combinations per state 

   challenges temporal planning
comdp model powerful enough model concurrency actions  still assumes
action instantaneous  incorporate actual action durations modeling problem 
essential increase scope current models real world domains 
present model algorithms discuss several new theoretical challenges
imposed explicit action durations  note results section apply wide range
planning problems 
regardless whether durations uncertain fixed
regardless whether effects stochastic deterministic 
  

fip lanning urative actions tochastic omains

actions uncertain duration modeled associating distribution  possibly conditioned
outcome stochastic effects  execution times  focus problems whose objective
achieve goal state minimizing total expected time  make span   results extend
cost functions combine make span resource usage  raises question
goal counts achieved  require that 
assumption   executing actions terminate goal considered achieved 
assumption   action  started  cannot terminated prematurely 
start asking question restricted set time points optimality
preserved even actions started points 
definition   time point new action allowed start execution called decision
epoch  time point pivot either   time new effect might occur  e g  
end actions execution  new precondition may needed existing precondition may
longer needed  happening either   time effect actually occurs new
precondition definitely needed existing precondition longer needed 
intuitively  happening point change world state action constraints actually
happens  e g   new effect new precondition   execution crosses pivot  a possible
happening   information gained agents execution system  e g   didnt effect
occur  may change direction future action choices  clearly  action durations
deterministic  set pivots set happenings 
example  consider action whose durations follow uniform integer duration  
    started time   timepoints                   pivots  certain execution
finishes time      and    happening  for execution    
definition   action pddl    action  fox   long        following hold 
effects realized instantaneously either  at start   at end   i e   beginning
completion action  respectively  
preconditions may need hold instaneously start  at start   end  at
end  complete execution action  over all  

  durative action
 duration     duration   
 condition  and  over p    at end q  
 effect  at end goal  
  durative action b
 duration     duration   
 effect  and  at start q   at end  not p     

figure    domain illustrate expressive action model may require arbitrary decision epochs
solution  example  b needs start   units execution reach goal 

  

fim ausam   w eld

theorem   pddl    domain restricting decision epochs pivots causes incompleteness
 i e   problem may incorrectly deemed unsolvable  
proof  consider deterministic temporal planning domain figure   uses pddl    notation
 fox   long         initial state p  true q false  way reach goal
start time  e g       b timepoint open interval  t            clearly 
new information gained time points interval none pivot  still 
required solving problem   
intuitively  instantaneous start end effects two pddl    actions may require certain
relative alignment within achieve goal  alignment may force one action start
somewhere  possibly non pivot point  midst others execution  thus requiring
intermediate decision epochs considered 
temporal planners may classified one two architectures  constraint posting
approaches times action execution gradually constrained planning  e g  
zeno lpg  see penberthy weld        gerevini serina        extended statespace methods  e g   tp  sapa  see haslum geffner        kambhampati        
theorem   holds architectures strong computational implications state space
planners limiting attention subset decision epochs speed planners 
theorem shows planners sapa prottle  little  aberdeen    thiebaux       
incomplete  fortunately  assumption restricts set decision epochs considerably 
definition   action tgp style action  following hold 
effects realized unknown point action execution  thus used
action completed 
preconditions must hold beginning action 
preconditions  and features transition function conditioned  must
changed actions execution  except effect action itself 
thus  two tgp style actions may execute concurrently clobber others preconditions effects  case tgp style actions set happenings nothing set
time points action terminates  tgp pivots set points action might
terminate   of course sets additionally include zero  
theorem   actions tgp style  set decision epochs may restricted pivots
without sacrificing completeness optimality 
proof sketch  contradiction  suppose optimal policy satisfies theorem 
must exist path optimal policy one must start action  a  time even
though action could terminated t  since planner hasnt gained
information t  case analysis  which requires actions tgp style  shows one could
started earlier execution path without increasing make span  detailed proof
discussed appendix   
case deterministic durations  set happenings set pivots  hence
following corollary holds 
   original tgp  smith   weld        considered deterministic actions fixed duration  use
phrase tgp style general way  without restrictions 

  

fip lanning urative actions tochastic omains

probabillity     
a 

s 

a 

g

a 
makespan   
probability    
a 

s 
a 

g

b 
makespan   

 

 

 

 

 

time

figure    pivot decision epochs necessary optimal planning face nonmonotonic continuation  domain  goal achieved h a    a     a  hb  i  a  duration  
   b  mutex a    optimal policy starts a  then  a  finish
time    starts b   otherwise starts a    

corollary   actions tgp style deterministic durations  set decision
epochs may restricted happenings without sacrificing completeness optimality 
planning uncertain durations may huge number pivots  useful
constrain range decision epochs 
definition   action independent duration correlation probabilistic
effects duration 
definition   action monotonic continuation expected time action termination
nonincreasing execution 
actions without probabilistic effects  nature  independent duration  actions monotonic continuations common  e g  uniform  exponential  gaussian  many duration distributions  however  actions bimodal multi modal distributions dont monotonic continuations  example consider action uniform distribution       
action doesnt terminate    expected time completion calculated        
  times         respectively  monotonically decreasing  example
non monotonic continuation see figure    
conjecture   actions tgp style  independent duration monotonic continuation 
set decision epochs may restricted happenings without sacrificing completeness
optimality 
actions continuation nonmonotonic failure terminate increase expected
time remaining cause another sub plan preferred  see figure     similarly  actions
duration isnt independent failure terminate changes probability eventual effects
may prompt new actions started 
exploiting theorems conjecture may significantly speed planning since
able limit number decision epochs needed decision making  use theoretical
understanding models  first  simplicity  consider case tgp style actions
deterministic durations  section    relax restriction allowing stochastic durations 
unimodal well multimodal 
  

fim ausam   w eld

togglep  
p    effect 
conflict
p    precondition 
togglex 
 

 

 

 

 

  

figure    sample execution demonstrating conflict due interfering preconditions effects   the
actions shaded disambiguate preconditions effects 

   temporal planning deterministic durations
use abbreviation cptp  short concurrent probabilistic temporal planning  refer
probabilistic planning problem durative actions  cptp problem input model
similar comdps except action costs  c s  a  s     replaced deterministic
durations   a   i e   input form hs  a  pr    g  s  i  study objective minimizing expected time  make span  reaching goal  rest paper make
following assumptions 
assumption   action durations integer valued 
assumption negligible effect expressiveness one convert problem
rational durations one satisfies assumption   scaling durations g c d 
denominators  case irrational durations  one always find arbitrarily close approximation original problem approximating irrational durations rational numbers 
reasons discussed previous section adopt tgp temporal action model smith
weld         rather complex pddl     fox   long         specifically 
assumption   actions follow tgp model 
restrictions consistent previous definition concurrency  specifically 
mutex definitions  of comdps probabilistic strips  hold required assumptions  illustration  consider figure    describes situation two actions
interfering preconditions effects executed concurrently  see not  suppose
initially p   false two actions toggle x  toggle p   started time      respectively  p   precondition toggle x    whose duration    needs remain false
time    toggle p   may produce effects anytime      may conflict
preconditions executing action  hence  forbid concurrent execution
toggle x  toggle p   ensure completely predictable outcome distribution 
definition concurrency  dynamics model remains consistent
equation    thus techniques developed comdps derived probabilistic strips actions
may used 
  

fip lanning urative actions tochastic omains

aligned epoch policy execution
 takes   units 
togglex 
t 

f

f

f

f

t  t  t  t 

 

 



  
time

togglex 
f

f

f

f

t  t  t  t  t 



interwoven epoch policy execution
 takes   units 

figure    comparison times taken sample execution interwoven epoch policy alignedepoch policy  trajectories toggle x   t   action fails four times succeeding 
aligned policy must wait actions complete starting more  takes
time interwoven policy  start actions middle 

    formulation comdp
model cptp problem comdp  thus mdp  one way  list
two prominent formulations below  first formulation  aligned epoch comdp models
problem approximately solves quickly  second formulation  interleaved epochs models
problem exactly results larger state space hence takes longer solve using existing
techniques  subsequent subsections explore ways speed policy construction
interleaved epoch formulation 
      ligned e poch earch pace
simple way formulate cptp model standard comdp probabilistic strips 
action costs set durations cost combination maximum
duration constituent actions  as equation     formulation introduces substantial
approximation cptp problem  true deterministic domains too  illustrate
using example involving stochastic effects  figure   compares trajectories
toggle x   t   actions fails four consecutive times succeeding  figure  f
denote failure success uncertain actions  respectively  vertical dashed lines represent
time points action started 
consider actual executions resulting policies  aligned epoch case  figure  
top   combination actions started state  next decision taken
effects actions observed  hence name aligned epochs   contrast  figure  
bottom shows decision epoch optimal execution cptp problem  many actions
may midway execution  explicitly take account actions
remaining execution times making subsequent decision  thus  actual state space
cptp decision making substantially different simple aligned epoch model 
note due corollary   sufficient consider new decision epoch happening 
i e   time point one actions complete  thus  using assumption   infer
decision epochs discrete  integer   course  optimal policies property 
  

fim ausam   w eld

state variables   x    x    x    x    p  
action
 a  precondition
toggle x 
 
p  
toggle x 
 
p  
toggle x 
 
true

effect
x  x 
x  x 
x  x 
change
toggle x 
 
true
x  x 
change
toggle p  
 
true
p   p  
goal   x       x       x       x     

probability
 
 
   
   
   
   
 

figure     domain example   extended action durations 
easy see exists least one optimal policy action begins
happening  hence search space reduces considerably 
      nterwoven e poch earch pace
adapt search space representation haslum geffner         similar
research  bacchus   ady          kambhampati         original state space
section   augmented including set actions currently executing times passed
since started  formally  let new interwoven state    ordered pair hx 
where 
xs
    a    a a       a  
x represents values state variables  i e  x state original state space 
denotes set ongoing actions times
passed since start   thus
n
overall interwoven epoch search space     aa  a  z a    z a  represents
n
set                 a    
denotes cartesian product multiple sets 
define set actions already execution  words  projection
ignoring execution times progress 
   a  a      hx  i 
example  continuing example domain figure     suppose state s  state
variables false  suppose action toggle x  started   units ago current time 
state would represented hx    y  x    f  f  f  f  f   y     toggle x        the five
state variables listed order  x    x    x    x  p      set as  would  toggle x    
allow possibility simply waiting action complete execution  is  deciding decision epoch start additional action  augment set no op action 
applicable states   hx      i e  states action still
executed   state s  no op action mutex non executing actions  i e  
    words  decision epoch either no op started combination
   use subscript   denote interwoven state space  s      value function  j      etc  

  

fip lanning urative actions tochastic omains

involving no op  define no op variable duration  equal time another
already executing action completes  next  s  a  defined below  
interwoven applicability set defined as 
 

ap    s   

apk  x    else
 noop  a aas apk  x  aas    

transition function  need define probability transition function  pr    
interwoven state space  decision epoch let agent state    x     suppose
agent decides execute action combination a  define ynew set similar
consisting actions starting  formally ynew     a   a   a a   system 
next decision epoch next time executing action terminates  let us call time
next  s  a   notice next  s  a  depends executing newly started actions  formally 
next  s  a   

min

 a  y ynew

 a 

moreover  multiple actions may complete simultaneously  define anext  s  a 
set actions complete exactly next  s  a  timesteps   component state
decision epoch next  s  a  time
ynext  s  a      a    next  s  a    a    ynew    a    next  s  a  
let s hx  let s   hx       i  transition function cptp defined as 
prk  x    x  anext  s  a      ynext  s  a 
 
otherwise

 
 

pr    s  s  a  

words  executing action combination state   hx  takes agent
decision epoch next  s  a  ahead time  specifically first time combination
anext  s  a  completes  lets us calculate ynext  s  a   new set actions still executing
times elapsed  also  tgp style actions  probability distribution different
state variables modified independently  thus probability transition function due comdp
probabilistic strips used decide new distribution state variables 
combination anext  s  a  taken state x 
example  continuing previous example  let agent state s  execute action combination    toggle x     next  s    a       since toggle x  finish first  thus 
anext  s    a    toggle x     ynext  s    a      toggle x        hence  probability distribution
states executing combination state s 
  f  f  f  t  f    ynext  s    a   probability      
  f  f  f  f  f    ynext  s    a   probability      
   precise definition model create multiple no opt actions different constant durations no opt
applicable interwoven state one   next  s  a  

  

fim ausam   w eld

start goal states  interwoven space  start state hs    new set goal
states g      hx  i x g  
redefining start goal states  applicability function  probability transition
function  finished modeling cptp problem comdp interwoven state space 
use techniques comdps  and mdps well  solve problem  particular 
use bellman equations described below 
bellman equations  set equations solution cptp problem written as 
j    s       g   else



    








next  s  a    pr    s   s  a j    s   
j    s    min



aap    s 


s 



x

use dursamp refer sampled rtdp algorithm search space  main
bottleneck naively inheriting algorithms dursamp huge size interwoven state
space  worst case  when actions executed concurrently  size state space
q
 s    aa  a    get bound observing action a   a  number
possibilities  either executing remaining times                a    
thus need reduce abstract aggregate state space order make problem
tractable  present several heuristics used speed search 
    heuristics
present admissible inadmissible heuristics used initial cost
function dursamp algorithm  first heuristic  maximum concurrency  solves underlying mdp thus quite efficient compute  second heuristic  average concurrency 
inadmissible  tends informed maximum concurrency heuristic 
      aximum c oncurrency h euristic
prove optimal expected cost traditional  serial  mdp divided maximum
number actions executed parallel lower bound expected make span
reaching goal cptp problem  let j x  denote value state x traditional
mdp costs action equal duration  let q x  a  denote expected cost reach
goal initially actions combination executed greedy serial policy followed
p
thereafter  formally  q x  a    x   prk  x    x  a j x      let j    s  value equivalent
cptp problem interwoven epoch state space  let concurrency state
maximum number actions could executed state concurrently  define maximum
concurrency domain  c  maximum number actions concurrently executed
world state domain  following theorem used provide admissible
heuristic cptp problems 
theorem   let   hx  i 
j    s 

j    s 


j  x 
 
c
q  x   
  
c
  

    

fip lanning urative actions tochastic omains

proof sketch  consider trajectory make span l  from state   hx  goal state 
cptp problem using optimal policy  make concurrent actions sequential executing
chronological order started  concurrent actions non interacting 
outcomes stage similar probabilities  maximum make span sequential
trajectory cl  assuming c actions executing points semi mdp trajectory   hence
j x  using  possibly non stationary  policy would cj    s   thus j  x  cj    s  


second inequality proven similar way   
cases bounds tight  example  consider deterministic planning
problem optimal plan concurrently executing c actions unit duration  makespan       sequential version  actions would taken sequentially  make span  
c  
following theorem  maximum concurrency  mc  heuristic state   hx 
defined follows 
q  x   
j  x 
else hm c  s   
  hm c  s   
c
c
maximum concurrency c calculated static analysis domain onetime expense  complete heuristic function evaluated solving mdp states 
however  many states may never visited  implementation  calculation
demand  states visited  starting mdp current state  rtdp run
seeded previous value function  thus computation thrown away
relevant part state space explored  refer dursamp initiated mc heuristic
durmc
samp  
      average c oncurrency h euristic
instead using maximum concurrency c heuristic use average concurrency
domain  ca   get average concurrency  ac  heuristic  call resulting algorithm
durac
samp   ac heuristic admissible  experiments typically informed
heuristic  moreover  case actions duration  ac heuristic equals
mc heuristic 
    hybridized algorithm
present approximate method solve cptp problems  many kinds
possible approximation methods  technique exploits intuition best focus computation probable branches current policys reachable space  danger
approach chance that  execution  agent might end unlikely branch 
poorly explored  indeed might blunder dead end case  undesirable  apparently attractive policy might true expected make span infinity 
since  wish avoid dead ends  explore desirable notion propriety 
definition   propriety  policy proper state guaranteed lead  eventually  goal
state  i e   avoids dead ends cycles   barto et al          define planning algorithm
proper always produces proper policy  when one exists  initial state 
describe anytime approximation algorithm  quickly generates proper policy
uses additional available computation time improve policy  focusing
likely trajectories 
  

fim ausam   w eld

      h ybridized p lanner
algorithm  durhyb   created hybridizing two policy creation algorithms  indeed 
novel notion hybridization general powerful  applying many mdp like problems  however  paper focus use hybridization cptp  hybridization uses
anytime algorithm rtdp create policy frequently visited states  uses faster  and
presumably suboptimal  algorithm infrequent states 
case cptp  algorithm hybridizes rtdp algorithms interwoven epoch
aligned epoch models  aligned epochs  rtdp converges relatively quickly  state
space smaller  resulting policy suboptimal cptp problem  policy
waits currently executing actions terminate starting new actions  contrast 
rtdp interwoven epochs generates optimal policy  takes much longer converge 
insight run rtdp interwoven space long enough generate policy
good common states  stop well converges every state  then  ensure
rarely explored states proper policy  substitute aligned policy  returning hybridized
policy 
algorithm   hybridized algorithm durhyb  r  k  m 
    

  
initialize j    s  admissible heuristic
   repeat
  
perform rtdp trials
  
compute hybridized policy  hyb   using interwoven epoch policy k familiar states aligned 

epoch policy otherwise
clean hyb removing dead ends cycles
j   hs    evaluation hyb start state


j    hs   i j    hs   i 


  
 r
j    hs   i 

   return hybridized policy hyb

  
  

thus key question decide states well explored not 
define familiarity state number times visited previous rtdp
trials  reachable state whose familiarity less constant  k  aligned policy created
it  furthermore  dead end state reached using greedy interwoven policy  create
aligned policy immediate precursors state  cycle detected    compute
aligned policy states part cycle 
yet said hybridized algorithm terminates  use rtdp helps us defining
simple termination condition parameter varied achieve desired
closeness optimality well  intuition simple  consider first  optimal labeled rtdp 
starts admissible heuristic guarantees value start state  j    hs    i  
remains admissible  thus less equal optimal   contrast  hybridized policys makespan always longer equal optimal  thus time progresses  values approach
optimal make span opposite sides  whenever two values within optimality ratio  r  
know algorithm found solution  close optimal 
   implementation cycles detected using simulation 

  

fip lanning urative actions tochastic omains

finally  evaluation hybridized policy done using simulation  perform
fixed number rtdp trials  algorithm   summarizes details algorithm  one see
combined policy proper two reasons     policy state aligned
policy  proper rtdp aligned epoch model run convergence 
   rest states explicitly ensured cycles dead ends 
    experiments  planning deterministic durations
continuing section      set experiments evaluate various techniques
solving problems involving explicit deterministic durations  compare computation time
solution quality five methods  interwoven sampled rtdp heuristic  dursamp   
ac
maximum concurrency  durmc
samp    average concurrency  dursamp   heuristics  hybridized
algorithm  durhyb   sampled rtdp aligned epoch model  durae    test
rover  machineshop aritificial domains  use artificial domain see relative
performance techniques varies amount concurrency domain 
      e xperimental etup
modify domains used section     additionally including action durations  nasa
rover machineshop domains  generate problems       state variables       actions  whose duration range       problems                reachable states interwoven epoch state space     
use artificial domain control experiments study effect degree parallelism 
problems domain    state variables               reachable states
durations actions     
use implementation sampled rtdp  implement heuristics  maximum concurrency  hm c    average concurrency  hac    initialization value function  calculate
heuristics demand states visited  instead computing complete heuristic
whole state space once  implement hybridized algorithm initial value
function set hm c heuristic  parameters r  k  kept               
respectively  test algorithms number problem instances three
domains  generate varying number objects  degrees parallelism  durations
actions distances goal 
      c omparison running imes
figures    a  b     a  show variations running times algorithms different
problems rover  machineshop artificial domains  respectively  first three bars represent
base sampled rtdp without heuristic  hm c   hac   respectively  fourth
bar represents hybridized algorithm  using hm c heuristic  fifth bar computation
aligned epoch sampled rtdp costs set maximum action duration  white
region fourth bar represents time taken aligned epoch rtdp computations
hybridized algorithm  error bars represent     confidence intervals running times  note
plots log scale 
   note policies returned dursamp guaranteed optimal  thus implemented algorithms
approximate  replace dursamp pruned rtdp  durprun   optimality desired 

  

fim ausam   w eld

rover  

mach  

    

    

    

mach  

mach  

mach  

mach  

mach  

 

ac
h
ae

rover  

 

ac
h
ae

rover  

time sec  on log scale 

    

    

    

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

    

    

 

ac
h
ae

time sec  on log scale 

rover  

 

ac
h
ae

rover  

rover  

 

ac
h
ae

    

    

figure      a b   running times  on log scale  rover machineshop domain  respectively 
problem five bars represent times taken algorithms  dursamp      durmc
samp
 ae   durac
 ac  
dur
 h  

dur
 ae  
respectively 

white
bar

dur
hyb
ae
hyb
samp
denotes portion time taken aligned epoch rtdp 

algos
durmc
samp
durac
samp
durhyb
durae

speedup compared dursamp
rover
machineshop artificial average
        
        
                 
        
        
                 
        
        
                
        
        
                 

table    ratio time taken   s rtdp heuristics algorithm 
heuristics produce     times speedups  hybridized algo produces   x speedup  aligned
epoch search produces    x speedup  sacrifices solution quality 

notice durae solves problems extremely quickly  natural since alignedepoch space much smaller  use hm c hac always speeds search   model 
comparing heuristics amongst themselves  find average concurrency heuristic mostly
performs faster maximum concurrency presumably hac informed heuristic practice  although cost inadmissible  find couple cases hac
doesnt perform better  could focusing search incorrect region  given
inadmissible nature 
rover domain  hybridized algorithm performs fastest  fact  speedups
dramatic compared methods  domains  results comparable small
problems  however  large problems two domains  hybridized outperforms others
huge margin  fact largest problem artificial domain  none heuristics able
converge  within day  durhyb durae converge solution 
  

fip lanning urative actions tochastic omains

    

art  

art  

art  

art  

 

ac
h
ae

art  

 

ac
h
ae

   
art       art       art       art        art        art        art        

art  

art  

ratio make span optimal

time sec  on log scale 

   
    

    

    

   
   
   
   
 
   

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

 

ac
h
ae

   

    

figure      a b   comparison different algorithms  running times solution quality respectively 
artificial domain  degree parallelism increases problems become harder 
largest problem solved durhyb durae  

table   shows speedups obtained various algorithms compared basic dursamp  
rover artificial domains speedups obtained durhyb durae much
prominent machineshop domain  averaging domains  h produces   x speedup
ae produces    x speedup 
      c omparison olution q uality
figures    a  b     b  show quality policies obtained five methods
domains  measure quality simulating generated policy across multiple trials 
reporting average time taken reach goal  plot ratio so measured expected
make span optimal expected make span    table   presents solution qualities method 
averaged problems domain  note aligned epoch policies usually yield
significantly longer make spans  e g       longer   thus one must make quality sacrifice
speedy policy construction  contrast  hybridized algorithm extorts small sacrifice
quality exchange speed 
      variation c oncurrency
figure    a  represents attempt see relative performance algorithms changed
increasing concurrency  along top figure  problem names  numbers brackets 
list average number applicable combinations mdp state  avgss    ap s   

range         concurrent actions  note difficult problems lot parallelism  dursamp slows dramatically  regardless heuristic  contrast  durhyb still able
quickly produce policy  almost loss quality  figure    b   
   large problems optimal algorithm converge  those  take optimal  best policy found
runs 

  

fim ausam   w eld

rover  

    mach  
ratio make span optimal

   
   
   
   
 

mach  

   
   
 

 

ac
h
ae

 

ac
h
ae

mach  

   

   
 

ac
h
ae

mach  

   

   
 

ac
h
ae

mach  

   

   

 

ac
h
ae

mach  

   

   

 

ac
h
ae

ratio make span optimal

   

 

ac
h
ae

rover  

 

ac
h
ae

rover  

 

ac
h
ae

rover  

 

ac
h
ae

rover  

 

ac
h
ae

rover  

 

ac
h
ae

   

   

figure      a b   comparison make spans solution found optimal plotted   yaxes  rover machineshop domains  respectively  algorithms except durae produce
solutions quite close optimal 

algos
dursamp
durmc
samp
durac
samp
durhyb
durae

rover
        
        
        
        
        

average quality
machineshop artificial
        
        
        
        
        
        
        
        
        
        

average
        
        
        
        
        

table    overall solution quality produced algorithms  note algorithms except durae produce policies whose quality quite close optimal  average durae produces make spans
     optimal 

   optimal planning uncertain durations
extend techniques previous section case action durations deterministic  before  consider tgp style actions discrete temporal model  assume
independent durations  monotonic continuations  section     relaxes latter  extending
algorithms handle multimodal duration distributions  aim minimize
expected time required reach goal 
    formulating comdp
formulate planning problem comdp similar section     
parameters comdp used directly work deterministic durations  need
recompute transition function 
  

fip lanning urative actions tochastic omains

state space  aligned epoch state space well interwoven epoch space  defined
section     adequate model planning problem  determine size interwoven
space  replace duration action max duration  let  a  denote maximum
time within action complete  overall interwoven epoch search space    




 a  zm  a    zm  a  represents set                 a    
denotes
cartesian product multiple sets 
action space  state may apply combination actions applicability function
reflecting fact combination actions safe w r t  and w r t  already executing
actions case interwoven space  previous sections  previous state space
action space work well problem  transition function definition needs change  since
need take account uncertainty durations 
transition function  uncertain durations require significant changes probability transition
function  pr     interwoven space definitions section        since assumptions justify conjecture    need consider happenings choosing decision epochs 
n

n

aa

algorithm   computetransitionfunc s hx  i a 
  
  
  
  
  
  
  
  
  
   
   
   

  a     
mintime min a  y minimum remaining time
maxtime min a  y maximum remaining time
integer  mintime  maxtime 
set actions could possibly terminate
non empty subsets asubt
pc  prob  exactly asubt terminates  see equation     
w   xt   pw     xt world state  pw probability asubt terminates yielding xt   
 xt   pw   w
yt   a    t     a    y 
  asubt  
insert  hxt   yt i  pw pc   output
return output

computation transition function described algorithm    although next decision
epoch determined happening  still need consider pivots next state calculations
potential happenings  mintime minimum time executing action could
terminate  maxtime minimum time guaranteed least one action
terminate  times mintime maxtime compute possible combinations
could terminate resulting next interwoven state  probability  pc    line    may
computed using following formula 
pc  



 prob  terminates   t a hasn  terminated till  

 a a  y aasubt



 prob  b doesn  terminate b   t b hasn  terminated till b  

    

 b b  y basub
 


considering pivots makes algorithm computationally intensive may
many pivots many action combinations could end one  many outcomes each 
implementation  cache transition function recompute
information state 
  

fim ausam   w eld

start goal states  start state goal set developed deterministic durations
work unchanged durations stochastic  so  start state hs    goal set
g      hx  i x g  
thus modeled problem comdp interwoven state space  redefined start goal states  probability transition function  use techniques
comdps solve problem  particular  use bellman equations below 
bellman equations interwoven epoch space  define el  s  a  s    time elapsed two interwoven states s  combination executed s  set equations
solution problem written as 

j    s       g   else

n

x
pr    s   s  a  el  s  a  s      j    s   
j    s    min


aap    s   
 

    

compare equations equation     one difference besides new transition
function time elapsed within summation sign  time elapsed depends
next interwoven state 
modeled problem comdp use algorithms section    use
dur denote family algorithms cptp problems involving stochastic durations 
main bottleneck solving problem  besides size interwoven state space 
high branching factor 
      p olicy c onstruction   rtdp   h ybridized p lanning
since modeled problem comdp new interwoven space  may use pruned
rtdp  durprun   sampled rtdp  dursamp   policy construction  since cost function problem  el   depends current next state  combo skipping
apply problem  thus durprun refers rtdp combo elimination 
furthermore  small adaptations necessary incrementally compute  admissible 
maximum concurrency  m c   more informed  inadmissible  average concurrency  ac 
heuristics  example  serial mdp  in rhs equation     need compute
average duration action use actions cost 
likewise  speed planning hybridizing  durhyb   rtdp algorithms interwoven aligned epoch comdps produce near optimal policy significantly less time 
dynamics aligned epoch space section   one exception  cost
combination  case deterministic durations  simply max duration constituent
actions  novel twist stems fact uncertain durations require computation cost
action combination expected time last action combination terminate 
example  suppose two actions  uniform duration distributions        started
concurrently  probabilities actions finished times         and earlier                respectively  thus expected duration completion combination
 let us call ae                             
  

fip lanning urative actions tochastic omains

    expected duration planner
modeled comdp full blown interwoven space  stochastic durations cause
exlposive growth branching factor  general  n actions started possible
durations r probabilistic effects   m     r     n rn      rn
potential successors  number may computed follows  duration  
  subset actions could complete action could result r outcomes  hence  total
p
number successors per duration i    n  n ci ri    r     n rn    moreover  none
actions finish time   last step actions terminate leading rn outcomes 
so  total number successors  m     r     n rn      rn   thus  branching factor
multiplicative duration uncertainty exponential concurrency 
manage extravagant computation must curb branching factor  one method
ignore duration distributions  assign action constant duration equal mean
distribution  apply deterministic duration planner dursamp   however 
executing deterministic duration policy setting durations actually stochastic 
action likely terminate time different mean  expected duration  durexp
planner addresses problem augmenting deterministic duration policy created account
unexpected outcomes 
      nline v ersion
procedure easiest understand online version  algorithm     wait unexpected
happens  pause execution  re plan  original estimate actions duration implausible 
compute revised deterministic estimate terms ea  min  expected value
duration given terminated time min  thus  ea     compute expected
duration a 
algorithm   online durexp
   build deterministic duration policy start state s 
   repeat
  
execute action combination specified policy
  
wait interrupt
  
case  action terminated expected    do nothing 
  
case  action terminates early
  
extend policy current state
  
case  action didnt terminate expected
  
extend policy current state revising

duration follows 
   
time elapsed since started executing
   
nextexp dea    e
   
nextexp  
   
nextexp dea  nextexp e
   
endwhile
   
revised duration nextexp
   
endwait
    goal reached

  

fim ausam   w eld

example  let duration action follow uniform distribution      
expected value gets assigned first run algorithm  dea    e     running
algorithm  suppose action didnt terminate   reach state running
for  say    time units  case  revised expected duration would  dea    e       
similarly  doesnt terminate    either next expected duration would    
finally     words states executing times      expected
terminate    times      expected completion             
doesnt terminate         
      ffline v ersion
algorithm offline version re planning contingencies done ahead
time fairness used version experiments  although offline algorithm plans
possible action durations  still much faster algorithms  reason
planning problems solved significantly smaller  less branching factor  smaller
reachable state space   previous computation succinctly stored form
hinterwoven state  valuei pairs thus reused  algorithm   describes offline planner
subsequent example illustrates savings 
algorithm   offline durexp
   build deterministic duration policy start state s    get current j     values


   insert s  queue open
   repeat
  
state   open pop  
  
currstate s t  pr    currstate state     state      



currstate goal currstate set visited
visited insert currstate 
j    currstate  converged
required  change expected durations actions currently executing
currstate 
   
solve deterministic duration planning problem start state currstate
   
insert currstate queue open
    open empty
  
  
  
  

line   algorithm   assigns new expected duration actions currently running
current state completd time previous termination point 
reassignment follows similar case online version  line     
example  consider domain two state variables  x  x    two actions set x 
set x    task set variables  initially false   assume set x  always
succeeds whereas set x  succeeds     probability  moreover  let actions
uniform duration distribution          case complete interwoven epoch search
could touch    interwoven states  each state variable could true false  action could
running  running   unit  running   units   instead  build deterministic
duration policy actions deterministic duration    total number states
touched    interwoven states  each action could running
running   unit  
  

fip lanning urative actions tochastic omains

problem
a 

b 

g

c 



g

optimal solution  trajectory    pr       makespan   
a 

b 

g

c 

optimal solution  trajectory    pr       makespan   
a 
c 



g

dur exp solution  makespan   
a 

time

 

b 
 

g
 

  

figure     example domain durexp algorithm compute optimal solution 
now  suppose deterministic planner decides execute actions start state 
committed combination  easy see certain states never reached 
example  state h x    x       setx       i never visited  since set x  completes
guaranteed x  set  fact  example    new states initiate offline
replanning  line    algo     viz   h x    x       setx       i  h x    x       setx       i 
h x    x       setx       i  
      p roperties
unfortunately  durexp algorithm guaranteed produce optimal policy  bad
policies generated expected duration planner  experiments show durexp
typically generates policies extremely close optimal  even worst case pathological
domain able construct leads expected make span     longer
optimal  in limit   example illustrated below 
example  consider domain actions a  n   b  n   c  n d  ai bi
takes time  i   ci probabilistic duration  probability      ci takes   unit time 
remaining probability  takes  i       time  thus  expected duration ci
 i      takes   units  sub problem spi   goal may reached executing ai followed
bi   alternatively  goal may reached first executing ci recursively solving
sub problem spi    domain  durexp algorithm always compute hai   bi
best solution  however  optimal policy starts  ai   ci    ci terminates   
policy executes solution spi    otherwise  waits ai terminates executes bi  
figure    illustrates sub problem sp  optimal policy expected make span
   vs  durexp make span     general  expected make span optimal policy
 
spn      n       n       n      thus  limn exp
opt       
    multi modal duration distributions
planners previous two sections benefited considering small set happenings
instead pivots  approach licensed conjecture    unfortunately  simplification
  

fim ausam   w eld

warranted case actions multi modal duration distributions  common
complex domains factors cant modeled explicitly  example  amount
time mars rover transmit data might bimodal distribution normally would
take little time  dust storm progress  unmodeled  could take much longer 
handle cases model durations mixture gaussians parameterized triple
hamplitude  mean  variancei 
      c mdp f ormulation
although cannot restrict decision epochs happenings  need consider pivots 
required actions multi modal distributions  fact  suffices consider pivots
regions distribution expected time to completion increases  cases
need consider happenings 
two changes required transition function algorithm    line    maxtime
computation involves time next pivot increasing remaining time region
actions multi modal distributions  thus forcing us take decision points  even
action terminates   another change  in line    allows non empty subset asub  
maxtime  is  next state computed even without action termination  making
changes transition function reformulate problem comdp interwoven space
thus solve  using previous methods pruned sampled rtdp  hybrid algorithm expectedduration algorithm 
      rchetypal  d uration p lanner
develop multi modal variation expected duration planner  called durarch   instead assigning action single deterministic duration equal expected value  planner
assigns probabilistic duration various outcomes means different modes
distribution probabilities probability mass mode  enhancement
reflects intuitive understanding multi modal distributions experiments confirm
durarch produces solutions shorter make spans durexp  
    experiments  planning stochastic durations
evaluate techniques solving planning problems involving stochastic durations 
compare computation time solution quality  make span  five planners domains
without multi modal duration distributions  re evaluate effectiveness
maximum   mc  average concurrency  ac  heuristics domains 
      e xperimental etup
modify rover  machineshop  artificial domains additionally including uncertainty
action durations  set experiments  largest problem   million world states
      reachable  algorithms explored           distinct states
interwoven state space planning  domains contained many    actions 
actions many    possible durations  details domains please refer
longer version  mausam        
  

fip lanning urative actions tochastic omains

planning time  in sec 

    
    
    

rover

machine shop

    
pruned
durprun
dursamp
sampled
durhyb
hybrid

    
    

durexp
exp dur

 
  

  

  

  

  

  

  

  

  

   problems

figure     planning time comparisons rover machineshop domains  variation along algorithms
initialized average concurrency  ac  heuristic  durexp performs best 

algos
dursamp
durhyb
durexp

average quality make span
rover machineshop artificial
     
     
     
     
     
     
     
     
     

table    three planners produce near optimal policies shown table ratios
optimal make span   

      c omparing running imes
compare algorithms without heuristics reaffirm heuristics significantly
speed computation problems  indeed  problems large solved without
heuristics  comparing amongst find ac beats c regardless
planning algorithm  isnt surprising since ac sacrifices admissibility 
figure    reports running times various algorithms  initialized ac heuristic 
rover machine shop domains durations unimodal  durexp out performs
planners substantial margins  algorithm solving comparatively simpler
problem  fewer states expanded thus approximation scales better others solving 
example  two machine shop problems  large planners 
cases hybridization speeds planning significant amounts  performs better durexp
artificial domain 
      c omparing olution q uality
measure quality simulating generated policy across multiple trials  report ratio
average expected make span optimal expected make span domains unimodal
distributions table    find make spans inadmissible heuristic ac par
    optimal algorithm doesnt converge  use best solution found across runs optimal 

  

fim ausam   w eld

  
  
  

    
durprun
pruned
dursamp
sampled

   

j  s  

planning time  log scale 

     

durhyb
hybrid
durarch
arch dur
durexp
exp dur

  

  
durprun
dur prun
dursamp
dur samp

  
  

durhyb
dur hyb
durarch
dur arch

  

durexp
dur exp

  

  

  

  

  

  

   problems

  

  

  

  

  

   problems

figure     comparisons machine shop domain multi modal distributions   a  computation
time comparisons  durexp durarch perform much better algos   b  makespans returned different algos  solutions returned dursamp almost optimal  overall
durarch finds good balance running time solution quality 

admissible heuristic c  hybridized planner approximate userdefined bound  experiments  set bound    find make spans returned
algorithm quite close optimal always differ     durexp
quality guarantees  still solutions returned problems tested upon nearly good
algorithms  thus  believe approximation quite useful scaling larger
problems without losing solution quality 
      ultimodal omains
develop multi modal variants domains  e g   machine shop domain  time fetching paint bimodal  if stock  paint fetched fast  else needs ordered  
alternative costly paint action doesnt require fetching paint  solutions produced
dursamp made use pivots decision epochs starting costly paint action case
fetch action didnt terminate within first mode bimodal distribution  i e  paint
stock  
running time comparisons shown figure    a  log scale  find durexp
terminates extremely quickly durarch far behind  however  make span comparisons figure    b  clearly illustrate approximations made methods order achieve
planning time  durarch exhibits good balance planning time solution quality 

   related work
paper extends prior work  originally reported several conference publications  mausam
  weld                  a      b  
temporal planners may classified using constraint posting extended state space methods  discussed earlier section     constraint approach promising   if any  probabilistic planners implemented using architecture  one exception buridan  kush  

fip lanning urative actions tochastic omains

stochastic

deterministic

concurrent
durative
non durative
dur  tempastic 
concurrent mdp 
gsmdp  prottle 
factorial mdp 
fpg  aberdeen et al 
paragraph
temporal planning
step optimal planning
 tp   sapa  mips
 graphplan  satplan 
tlplan  etc  

non concurrent
durative
non durative
time dependent mdp 
mdp
ixtet  circa 
 rtdp  lao   etc  
foss   onder
planning
classical planning
numerical resources
 hsp  ff  etc  
 sapa  metric ff  cpt 

figure     table listing various planners implement different subsets concurrent  stochastic  durative actions 

merick  hanks    weld         performed poorly  contrast  mdp community
proven state space approach successful  since powerful deterministic temporal planners 
various planning competitions  use state space approach  adopt
algorithms combine temporal planning mdps  may interesting incorporate
constraint based approaches probabilistic paradigm compare techniques
paper 
    comparison semi mdps
semi markov decision process extension mdps allows durative actions take variable time  discrete time semi mdp solved solving set equations direct
extension equations    techniques solving discrete time semi mdps natural generalizations mdps  main distinction semi mdp formulation
concurrent probabilistic temporal planning stochastic durations concerns presence concurrently executing actions model  semi mdp allow concurrent actions
assumes one executing action time  allowing concurrency actions intermediate decision epochs  algorithms need deal large state action spaces  encountered
semi mdps 
furthermore  younes simmons shown general case  semi mdps incapable modeling concurrency  problem concurrent actions stochastic continuous
durations needs another model known generalized semi markov decision process  gsmdp 
precise mathematical formulation  younes   simmons      b  
    concurrency stochastic  durative actions
tempastic  younes   simmons      a  uses rich formalism  e g  continuous time  exogenous
events  expressive goal language  generate concurrent plans stochastic durative actions  tempastic uses completely non probabilistic planner generate plan treated
candidate policy repaired failure points identified  method guarantee
completeness proximity optimal  moreover  attention paid towards heuristics
search control making implementation impractical 
gsmdps  younes   simmons      b  extend continuous time mdps semi markov mdps 
modeling asynchronous events processes  younes simmonss approaches handle
  

fim ausam   w eld

strictly expressive model due modeling continuous time  solve
gsmdps approximation standard mdp using phase type distributions  approach
elegant  scalability realistic problems yet demonstrated  particular  approximate  discrete mdp model require many states yet still behave differently
continuous original 
prottle  little et al         solves problems action language expressive
ours  effects occur middle action execution dependent durations supported 
prottle uses rtdp type search guided heuristics computed probabilistic planning
graph  however  plans finite horizon thus acyclic state space  difficult
compare prottle approach prottle optimizes different objective function  probability reaching goal   outputs finite length conditional plan opposed cyclic plan
policy  guaranteed reach goal 
fpg  aberdeen   buffet        learns separate neural network action individually
based current state  execution phase decision  i e   whether action needs
executed not  taken independently decisions regarding actions  way fpg able
effectively sidestep blowup caused exponential combinations actions  practice
able quickly compute high quality solutions 
rohanimanesh mahadevan        investigate concurrency hierarchical reinforcement
learning framework  abstract actions represented markov options  propose
algorithm based value iteration  focus calculating joint termination conditions rewards received  rather speeding policy construction  hence  consider possible markov
option combinations backup 
aberdeen et al         plan concurrent  durative actions deterministic durations
specific military operations domain  apply various domain dependent heuristics speed
search extended state space 
    concurrency stochastic  non durative actions
meuleau et al  singh   cohn deal special type mdp  called factorial mdp 
represented set smaller weakly coupled mdps separate mdps completely
independent except common resource constraints  reward cost models
purely additive  meuleau  hauskrecht  kim  peshkin  kaelbling  dean    boutilier        singh
  cohn         describe solutions sub mdps independently solved
sub policies merged create global policy  thus  concurrency actions different
sub mdps by product work  singh   cohn present optimal algorithm  similar
combo elimination used durprun    whereas domain specific heuristics meuleau et al 
guarantees  work factorial mdps assumes weak coupling exists
identified  factoring mdp hard problem itself 
paragraph  little   thiebaux        formulates planning concurrency regression
search probabilistic planning graph  uses techniques nogood learning mutex
reasoning speed policy construction 
guestrin et al  solve multi agent mdp problem using linear programming  lp  formulation expressing value function linear combination basis functions  assuming
basis functions depend agents  able reduce size lp
 guestrin  koller    parr        
  

fip lanning urative actions tochastic omains

    stochastic  non concurrent  durative actions
many researchers studied planning stochastic  durative actions absence concurrency 
example  foss onder        use simple temporal networks generate plans
objective function time component  simple temporal networks allow effective temporal
constraint reasoning methods generate temporally contingent plans 
boyan littman        propose time dependent mdps model problems actions
concurrent time dependent  stochastic durations  solution generates piecewise linear value functions 
nasa researchers developed techniques generating non concurrent plans uncertain continuous durations using greedy algorithm incrementally adds branches straightline plan  bresina et al         dearden  meuleau  ramakrishnan  smith    washington        
handle continuous variables uncertain continuous effects  solution heuristic
quality policies unknown  also  since consider limited contingencies 
solutions guaranteed reach goal 
ixtet temporal planner uses constraint based reasoning within partial order planning
 laborie   ghallab         embeds temporal properties actions constraints
optimize make span  circa example system plans uncertain durations
action associated unweighted set durations  musliner  murphy    shin        
    deterministic  concurrent  durative actions
planning deterministic actions comparitively simpler problem much work
planning uncertainty based previous  deterministic planning research  instance 
interwoven state representation transition function extensions extended state representations tp   sapa  tlplan  haslum   geffner          kambhampati       
bacchus   ady        
planners  mips altaltp   investigated fast generation parallel plans
deterministic settings  edelkamp        nigenda   kambhampati        jensen veloso
       extend problems disjunctive uncertainty 

   future work
presented comprehensive set techniques handle probabilistic outcomes  concurrent
durative actions single formalism  direct attention towards different relaxations
extensions proposed model  particular  explore objective functions  infinite
horizon problems  continuous valued duration distributions  temporally expressive action models 
degrees goal satisfaction interruptibility actions 
    extension cost functions
planning problems durative actions  sections   beyond  focused make span
minimization problems  however  techniques quite general applicable  directly
minor variations  variety cost metrics  illustration  consider mixed cost
optimization problem addition duration action  given
amount resource consumed per action  wish minimize sum make span
total resource usage  assuming resource consumption unaffected concurrent
  

fim ausam   w eld

execution  easily compute new max concurrency heuristic  mixed cost counterpart
equations    is 
jt  x 
  jr  x 
 
c
qt  x   
  qr  x      
c

j    s 

j    s 


    

here  jt single action mdp assignng costs durations jr single
action mdp assigning costs resource consumptions  informed average concurrency
heuristic similarly computed replacing maximum concurrency average concurrency 
hybridized algorithm follows fashion  fast algorithm comdp
solved using techniques section   
lines  objective function minimize make span given certain maximum
resource usage  total amount resource remaining included state space
comdps underlying single action mdps etc  techniques may used 
    infinite horizon problems
paper defined techniques case indefinite horizon problems 
absorbing state defined reachable  problems alternative formulation
preferred allows infinite execution discounts future costs multiplying
discount factor step  again  techniques suitably extended scenario 
example  theorem   gets modified following 
qk  s  a 

 k

qk  s   a       ck  a 

k
x

 



ik

ck   ai   

i  

recall theorem provides us pruning rule  combo skipping  thus  use
pruned rtdp new pruning rule 
    extensions continuous duration distributions
confined actions discrete durations  refer assumption    
investigate effects dealing directly continuous uncertainty duration distributions  let fit  t dt probability action ai completing times       dt 
conditioned action ai finishing time   similarly  define fit  t  probability
action finishing time    
let us consider extended state hx    a      i  denotes action a  started units
ago world state x  let a  applicable action started extended state  define
  min m  a   t   a      denotes maximum possible duration execution
action  intuitively  time least one action complete 

q   n    hx    a      i  a     

z
 

z
 

h



f t  t f    t    j   n  hx     a    t i  dt  
h



f t  t f    t    j   n  hx     a       i  dt
  

    

fi 

 

 

time

 

 

  

  

  

expected time reach goal

expected remaining time action a 

duration distribution a 

p lanning urative actions tochastic omains

 
 
 
 

 

 

 

time

 

 

  

 
 
 
 

 

 

 

 

 

  

time

figure     durations continuous  real valued  rather discrete  may infinite number
potentially important decision epochs  domain  crucial decision epoch could required
time        depending length possible alternate plans 

x  x  world states obtained applying deterministic actions a  a 
respectively x  recall j   n    s    mina q   n    s  a   fixed point computation

form  desire jn   jn functional form     going equation
seems difficult achieve  except perhaps specific action distributions
special planning problems  example  distributions constant
concurrency domain  equations easily solvable  interesting cases 
solving equations challenging open question 
furthermore  dealing continuous multi modal distributions worsens decision epochs
explosion  illustrate help example 
example  consider domain figure   except let action a  bimodal distribution 
two modes uniform          respectively shown figure    a  
let a  small duration  figure    b  shows expected remaining termination times
a  terminates time     notice due bimodality  expected remaining execution time
increases      expected time reach goal using plan h a    a     a  shown
third graph  suppose  started  a    a     need choose next decision
epoch  easy see optimal decision epoch could point    
would depend alternative routes goal  example  duration b       
optimal time point start alternative route      right expected time reach goal
using first plan exceeds       
thus  choice decision epochs depends expected durations alternative routes 
values known advance  fact ones calculated planning
phase  therefore  choosing decision epochs ahead time seem possible  makes
optimal continuous multi modal distribution planning problem mostly intractable reasonable
sized problem 
    generalizing tgp action model
assumption tgp style actions enables us compute optimal policies  since prune
number decision epochs  case complex action models pddl     fox   long        
old  deterministic state space planners incomplete  reasons  algorithms
    idea exploited order plan continuous resources  feng  dearden  meuleau    washington 
      

  

fim ausam   w eld

incomplete problems ppddl      recently  cushing et al  introduced tempo  statespace planner  uses lifting time achieve completeness  cushing  kambhampati 
mausam    weld         pursuit finding complete  state space  probabilistic planner
complex action models  natural step consider tempo like representation probabilistic
setting  working details seems relatively straightforward  important research
challenge find right heuristics streamline search algorithm scale 
    extensions
several extensions basic framework suggested  different
construct introduces additional structure need exploit knowledge order design
fast algorithms  many times  basic algorithms proposed paper may easily adapted
situations  sometimes may not  list two important extensions below 
notion goal satisfaction  different problems may require slightly different notions
goal reached  example  assumed thus far goal officially
achieved executed actions terminated  alternatively  one might consider goal
achieved satisfactory world state reached  even though actions may
midst execution  intermediate possibilities goal requires specific
actions necessarily end  changing definition goal set  problems
modeled comdp  hybridized algorithm heuristics easily adapted
case 
interruptible actions  assumed that  started  action cannot terminated 
however  richer model may allow preemptions  well continuation interrupted
action  problems  actions could interrupted will  significantly
different flavor  interrupting action new kind decision requires full study
might action termination useful  large extent  planning similar
finding different concurrent paths goal starting together  since one
always interrupt executing paths soon goal reached  instance  example
figure   longer holds since b  started time    later terminated needed
shorten make span 
    effect large durations
weakness extended state space approaches  deterministic well probabilistic
settings  dependence absolute durations  or accurate  greatest common
divisor action durations   instance  domain action large duration 
say     another concurrently executable action duration    world states
explored tuples  a       a              a        a       general  many states
behave similarly certain decision boundaries important  start b
executing    units c otherwise one example decision boundary 
instead representing flat discrete states individually  planning aggregate space
state represents several extended states help alleviate inefficiency 
however  obvious achieve aggregation automatically  since adapting
well known methods aggregation hold case  instance  spudd  hoey
  

fip lanning urative actions tochastic omains

et al         uses algebraic decision diagrams represent abstract states jvalue  aggregating valued states may enough us  since expected time
completion depends linearly amount time left longest executing action  so 
states differ amount time action executing able
aggregate together  similar way  feng et al         use piecewise constant piecewise
linear representations adaptively discretize continuous variables  case   a 
variables  executing active given time  modeling
sparse high dimensional value function easy either  able exploit structure due
action durations essential future direction order scale algorithms complex real
world domains 

   conclusions
although concurrent durative actions stochastic effects characterize many real world domains  planners handle challenges concert  paper proposes unified statespace based framework model solve problems  state space formulations popular
deterministic temporal planning well probabilistic planning  however 
features bring additional complexities formulation afford new solution techniques 
develop dur family algorithms alleviates complexities  evaluate techniques running times qualities solutions produced  moreover  study theoretical
properties domains identify key conditions fast  optimal algorithms
possible  make following contributions 
   define concurrent mdps  comdp  extension mdp model formulate
stochastic planning problem concurrent actions  comdp cast back new
mdp extended action space  action space possibly exponential
number actions  solving new mdp naively may take huge performance hit 
develop general notions pruning sampling speed algorithms  pruning
refers pruning provably sub optimal action combinations state  thus performing less computation still guaranteeing optimal solutions  sampling based solutions
rely intelligent sampling action combinations avoid dealing exponential
number  method converges orders magnitude faster methods produces
near optimal solutions 
   formulate planning concurrent  durative actions comdp two modified
state spaces aligned epoch  interwoven epoch  aligned epoch based solutions
run fast  interwoven epoch algorithms yield much higher quality solutions  define two heuristic functions maximum concurrency  mc   average concurrency  ac 
guide search  mc admissible heuristic  whereas ac  inadmissible  typically more informed leading better computational gains  call algorithms dur
family algorithms  subscripts samp prun refer sampling pruning respectively 
optional superscripts ac mc refer heuristic employed  optional   
dur notifies problem stochastic durations  example  labeled rtdp
deterministic duration problem employing sampling started ac heuristic
abbreviated durac
samp  
  

fim ausam   w eld

   develop general technique hybridizing two planners  hybridizing interwovenepoch aligned epoch comdps yields much efficient algorithm  durhyb  
algorithm parameter  varied trade off speed optimality 
experiments  durhyb quickly produces near optimal solutions  larger problems 
speedups algorithms quite significant  hybridized algorithm
used anytime fashion thus producing good quality proper policies  policies
guaranteed reach goal  within desired time  moreover  idea hybridizing two
planners general notion  recently applied solving general stochastic planning
problems  mausam  bertoli    weld        
   uncertainty durations leads complexities addition state action
spaces  blowup branching factor number decision epochs 
bound space decision epochs terms pivots  times actions may potentially terminate  conjecture restrictions  thus making problem tractable 
propose two algorithms  expected duration planner  durexp   archetypal
duration planner  durarch    successively solve small planning problems
limited duration uncertainty  respectively  durarch able make use
additional structure offered multi modal duration distributions  algorithms perform
much faster techniques  moreover  durarch offers good balance
planning time vs  solution quality tradeoff 
   besides focus stochastic actions  expose important theoretical issues related
durative actions repercussions deterministic temporal planners well 
particular  prove common state space temporal planners incomplete face
expressive action models  e g   pddl      result may strong impact
future temporal planning research  cushing et al         
overall  paper proposes large set techniques useful modeling solving
planning problems employing stochastic effects  concurrent executions durative actions
duration uncertainties  algorithms range fast suboptimal solutions  relatively slow
optimal  various algorithms explore different intermediate points spectrum
presented  hope techniques useful scaling planning techniques real
world problems future 

acknowledgments
thank blai bonet providing source code gpt well comments course
work  thankful sumit sanghai theorem proving skills advice various
stages research  grateful derek long anonymous reviewers paper
gave several thoughtful suggestions generalizing theory improving clarity
text  thank subbarao kambhampati  daniel lowd  parag  david smith others
provided useful comments drafts parts research  work performed university washington           supported generous grants national
aeronautics space administration  award nag          national science foundation  award
iis           office naval research  awards n                 n                
wrf   tj cable professorship 
  

fip lanning urative actions tochastic omains

references
aberdeen  d   thiebaux  s     zhang  l          decision theoretic military operations planning 
icaps   
aberdeen  d     buffet  o         
gradients  icaps   

concurrent probabilistic temporal planning policy 

bacchus  f     ady  m          planning resources concurrency  forward chaining
approach  ijcai    pp         
barto  a   bradtke  s     singh  s          learning act using real time dynamic programming 
artificial intelligence            
bertsekas  d          dynamic programming optimal control  athena scientific 
blum  a     furst  m          fast planning planning graph analysis  artificial intelligence 
               
bonet  b     geffner  h          labeled rtdp  improving convergence real time dynamic
programming  icaps    pp       
bonet  b     geffner  h          mgpt  probabilistic planner based heuristic search  jair 
        
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions
computational leverage  j  artificial intelligence research          
boyan  j  a     littman  m  l          exact solutions time dependent mdps  nips    p 
     
bresina  j   dearden  r   meuleau  n   smith  d     washington  r          planning continuous time resource uncertainty   challenge ai  uai   
chen  y   wah  b  w     hsu  c          temporal planning using subgoal partitioning resolution sgplan  jair          
cushing  w   kambhampati  s   mausam    weld  d  s          temporal planning really
temporal   ijcai   
dearden  r   meuleau  n   ramakrishnan  s   smith  d  e     washington  r          incremental
contingency planning  icaps   workshop planning uncertainty incomplete information 
do  m  b     kambhampati  s          sapa  domain independent heuristic metric temporal
planner  ecp   
do  m  b     kambhampati  s          sapa  scalable multi objective metric temporal planner 
jair             
edelkamp  s          taming numbers duration model checking integrated planning
system  journal artificial intelligence research             
  

fim ausam   w eld

feng  z   dearden  r   meuleau  n     washington  r          dynamic programming structured continuous markov decision processes  uai    p      
foss  j     onder  n          generating temporally contingent plans  ijcai   workshop
planning learning apriori unknown dynamic domains 
fox  m     long  d          pddl     extension pddl expressing temporal planning
domains   jair special issue  rd international planning competition            
gerevini  a     serina  i          lpg  planner based local search planning graphs
action graphs  aips    p      
guestrin  c   koller  d     parr  r          max norm projections factored mdps  ijcai   
pp         
hansen  e     zilberstein  s          lao   heuristic search algorithm finds solutions
loops  artificial intelligence            
haslum  p     geffner  h          heuristic planning time resources  ecp   
hoey  j   st aubin  r   hu  a     boutilier  c          spudd  stochastic planning using decision
diagrams  uai    pp         
jensen  r  m     veloso  m          obdd based universal planning synchronized agents
non deterministic domains  journal artificial intelligence research          
kushmerick  n   hanks  s     weld  d          algorithm probabilistic planning  artificial
intelligence                  
laborie  p     ghallab  m          planning sharable resource constraints  ijcai    p 
     
little  i   aberdeen  d     thiebaux  s          prottle  probabilistic temporal planner 
aaai   
little  i     thiebaux  s          concurrent probabilistic planning graphplan framework 
icaps   
long  d     fox  m           rd international planning competition  results analysis 
jair          
mausam         stochastic planning concurrent  durative actions  ph d  dissertation  university washington 
mausam  bertoli  p     weld  d          hybridized planner stochastic domains  ijcai   
mausam    weld  d          solving concurrent markov decision processes  aaai   
mausam    weld  d          concurrent probabilistic temporal planning  icaps    pp     
    
  

fip lanning urative actions tochastic omains

mausam    weld  d       a   challenges temporal planning uncertain durations 
icaps   
mausam    weld  d       b   probabilistic temporal planning uncertain durations 
aaai   
meuleau  n   hauskrecht  m   kim  k  e   peshkin  l   kaelbling  l   dean  t     boutilier  c 
        solving large weakly coupled markov decision processes  aaai    pp 
       
musliner  d   murphy  d     shin  k          world modeling dynamic construction
real time control plans  artificial intelligence            
nigenda  r  s     kambhampati  s          altalt p  online parallelization plans heuristic
state search  journal artificial intelligence research             
penberthy  j     weld  d          temporal planning continuous change  aaai    p       
rohanimanesh  k     mahadevan  s          decision theoretic planning concurrent temporally extended actions  uai    pp         
singh  s     cohn  d          dynamically merge markov decision processes  nips   
mit press 
smith  d     weld  d          temporal graphplan mutual exclusion reasoning  ijcai   
pp         stockholm  sweden  san francisco  ca  morgan kaufmann 
vidal  v     geffner  h          branching pruning  optimal temporal pocl planner based
constraint programming  aij                 
younes  h  l  s     simmons  r  g       a   policy generation continuous time stochastic
domains concurrency  icaps    p      
younes  h  l  s     simmons  r  g       b   solving generalized semi markov decision processes
using continuous phase type distributions  aaai    p      
zhang  w     dietterich  t  g          reinforcement learning approach job shop scheduling 
ijcai    pp           

appendix
proof theorem  
prove statement theorem    i e   actions tgp style set pivots
suffices optimal planning  proof make use fact actions tgp style
consistent execution concurrent plan requires two executing actions non mutex
 refer section   explanation that   particular  none effects conflict
precondition one conflict effects another 
prove theorem contradition  let us assume problem optimal solution
requires least one action start non pivot  let us consider one optimal plans 
  

fim ausam   w eld

first non pivot point action needs start non pivot minimized  let
us name time point let action starts point a  prove case
analysis may  well  start time   without changing nature plan   
non pivot contradict hypothesis minimum first non pivot point 
  pivot hypothesis contradicted  need to  start non pivot 
prove left shifted   unit  take one trajectory time  recall
actions could several durations  consider actions playing role    t     a    
   a    a  refers duration trajectory  considering points
suffice  since system state change points trajectory  prove
execution none actions affected left shift  following twelve
cases 
   actions b start    b cant end  t non pivot   thus b execute
concurrently t  implies b non mutex  thus b may well start together 
   actions b continue execution    use argument similar case   above 
   actions b end    b tgp style  effects realized open interval
ending    therefore  start conflict end b 
   actions b start t  b start together hence dependent
preconditions  also  non mutex  starting times shifted
direction 
   actions b continue execution t  b started   refer case   above  not 
  similar points b 
   actions b end t  case possible due assumption non pivot 
   actions b start    a     since continued execution point  b
non mutex  thus effects clobber bs preconditions  hence  b still executed
realizing effects 
   actions b continue execution    a     b non mutex  may end
earlier without effect b 
   actions b end    a     b executing concurrently  thus
non mutex  may end together 
    actions b start    a   b may still start    a   since state    a 
doesnt change 
    actions b continue execution    a   b started    a    refer case
  above  else state change    a  cause effect b 
    actions b end    a   b non mutex executing concurrently  thus  effects dont clobber bs preconditions  hence  may end earlier 
since left shifted trajectories  therefore left shift legal  also 
multiple actions start may shifted one one using argument 
hence proved   
  


