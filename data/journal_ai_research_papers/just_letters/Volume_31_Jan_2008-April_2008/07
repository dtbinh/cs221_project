journal artificial intelligence research                  

submitted        published      

global inference sentence compression
integer linear programming approach
james clarke

jclarke ed ac uk

mirella lapata

mlap inf ed ac uk

school informatics
university edinburgh
  buccleuch place
edinburgh eh   lw  uk

abstract
sentence compression holds promise many applications ranging summarization
subtitle generation  work views sentence compression optimization problem
uses integer linear programming  ilp  infer globally optimal compressions
presence linguistically motivated constraints  show previous formulations
sentence compression recast ilps extend models novel global
constraints  experimental results written spoken texts demonstrate improvements
state of the art models 

   introduction
computational treatment sentence compression recently attracted much attention
literature  task viewed producing summary single sentence
retains important information remains grammatical  jing         sentence
compression mechanism would greatly benefit wide range applications  example 
summarization  could improve conciseness generated summaries  jing       
lin        zajic  door  lin    schwartz         examples include compressing text
displayed small screens mobile phones pdas  corston oliver        
subtitle generation spoken transcripts  vandeghinste   pan         producing
audio scanning devices blind  grefenstette        
sentence compression commonly expressed word deletion problem  given input source sentence words x   x    x            xn   aim produce target compression
removing subset words  knight   marcu         compression problem extensively studied across different modeling paradigms  supervised
unsupervised  supervised models typically trained parallel corpus source sentences target compressions come many flavors  generative models aim model
probability target compression given source sentence either directly  galley
  mckeown        indirectly using noisy channel model  knight   marcu       
turner   charniak         whereas discriminative formulations attempt minimize error
rate training set  include decision tree learning  knight   marcu         maximum entropy  riezler  king  crouch    zaenen         support vector machines  nguyen 
shimazu  horiguchi  ho    fukushi         large margin learning  mcdonald        
c
    
ai access foundation  rights reserved 

ficlarke   lapata

unsupervised methods dispense parallel corpus generate compressions either
using rules  turner   charniak        language model  hori   furui        
despite differences formulation  approaches model compression process
using local information  instance  order decide words drop  exploit
information adjacent words constituents  local models good job
producing grammatical compressions  however somewhat limited scope since
cannot incorporate global constraints compression output  constraints
consider sentence whole instead isolated linguistic units  words constituents  
give concrete example may want ensure target compression verb 
provided source one first place  verbal arguments present
compression  pronouns retained  constraints fairly intuitive
used instill linguistic task specific information model 
instance  application compresses text displayed small screens would
presumably higher compression rate system generating subtitles spoken
text  global constraint could force former system generate compressions
fixed rate fixed number words 
existing approaches model global properties compression problem
good reason  finding best compression source sentence given space
possible compressions   this search process often referred decoding inference 
become intractable many constraints overly long sentences  typically 
decoding problem solved efficiently using dynamic programming often conjunction
heuristics reduce search space  e g   turner   charniak         dynamic
programming guarantees find global optimum provided principle optimality holds  principle states given current state  optimal decision
remaining stages depend previously reached stages previously made
decisions  winston   venkataramanan         however  know false
case sentence compression  example  included modifiers left
noun compression probably include noun include verb
include arguments  dynamic programming approach cannot
easily guarantee constraints hold 
paper propose novel framework sentence compression incorporates
constraints compression output allows us find optimal solution 
formulation uses integer linear programming  ilp   general purpose exact framework
np hard problems  specifically  show previously proposed models recast
integer linear programs  extend models constraints express
linear inequalities  decoding framework amounts finding best solution given
linear  scoring  function set linear constraints either global local 
although ilp previously used sequence labeling tasks  roth   yih       
punyakanok  roth  yih    zimak         application natural language generation
less widespread  present three compression models within ilp framework 
representative unsupervised  knight   marcu         semi supervised  hori   furui 
       fully supervised modeling approach  mcdonald         propose small
number constraints ensuring compressions structurally semantically
    n possible compressions n number words sentence 

   

figlobal inference sentence compression

valid experimentally evaluate impact compression task  cases 
show added constraints yield performance improvements 
remainder paper organized follows  section   provides overview
related work  section   present ilp framework compression models
employ experiments  constraints introduced section      section    
discusses experimental set up section   presents results  discussion future
work concludes paper 

   related work
paper develop several ilp based compression models  presenting
models  briefly summarize previous work addressing sentence compression emphasis data driven approaches  next  describe ilp techniques used
past solve inference problems natural language processing  nlp  
    sentence compression
jing        perhaps first tackle sentence compression problem  approach
uses multiple knowledge sources determine phrases sentence remove  central
system grammar checking module specifies sentential constituents
grammatically obligatory therefore present compression 
achieved using simple rules large scale lexicon  knowledge sources include
wordnet corpus evidence gathered parallel corpus source target sentence
pairs  phrase removed grammatically obligatory  focus
local context reasonable deletion probability  estimated parallel corpus  
contrast jing         bulk research sentence compression relies exclusively corpus data modeling compression process without recourse extensive knowledge sources  e g   wordnet   large number approaches based
noisy channel model  knight   marcu         approaches consist language
model p  y   whose role guarantee compression output grammatical   channel
model p  x y   capturing probability source sentence x expansion
target compression y   decoder  which searches compression maximizes
p  y p  x y    channel model acquired parsed version parallel corpus 
essentially stochastic synchronous context free grammar  aho   ullman        whose
rule probabilities estimated using maximum likelihood  modifications model
presented turner charniak        galley mckeown        improved
results 
discriminative models  knight   marcu        riezler et al         mcdonald       
nguyen et al         sentences represented rich feature space  also induced
parse trees  goal learn words word spans deleted given
context  instance  knight marcus        decision tree model  compression
performed deterministically tree rewriting process inspired shift reduce
parsing paradigm  nguyen et al         render model probabilistic use
support vector machines  mcdonald        formalizes sentence compression largemargin learning framework without making reference shift reduce parsing  model
compression classification task  pairs words source sentence classified
   

ficlarke   lapata

adjacent target compression  large number features defined
words  parts of speech  phrase structure trees dependencies  features
gathered adjacent words compression words in between
dropped  see section       detailed account  
compression models developed written text mind  hori
furui        propose model automatically transcribed spoken text  model
generates compressions word deletion without using parallel data syntactic information way  assuming fixed compression rate  searches compression
highest score using dynamic programming algorithm  scoring function consists language model responsible producing grammatical output  significance score
indicating whether word topical not  score representing speech recognizers
confidence transcribing given word correctly 
    integer linear programming nlp
ilps constrained optimization problems objective function
constraints linear equations integer variables  see section     details   ilp
techniques recently applied several nlp tasks  including relation extraction
 roth   yih         semantic role labeling  punyakanok et al          generation
route directions  marciniak   strube         temporal link analysis  bramsen  deshpande 
lee    barzilay         set partitioning  barzilay   lapata         syntactic parsing  riedel
  clarke         coreference resolution  denis   baldridge        
approaches combine local classifier inference procedure based
ilp  classifier proposes possible answers assessed presence global
constraints  ilp used make final decision consistent constraints
likely according classifier  example  semantic role labeling task involves
identifying verb argument structure given sentence  punyakanok et al         first
use snow  multi class classifier   roth         identify label candidate arguments 
observe labels assigned arguments sentence often contradict other 
resolve conflicts propose global constraints  e g   argument
instantiated given verb  every verb least one argument  use
ilp reclassify output snow 
dras        develops document paraphrasing model using ilp  key premise
work cases one may want rewrite document conform
global constraints length  readability  style  proposed model three
ingredients  set sentence level paraphrases rewriting text  set global constraints  objective function quantifies effect incurred paraphrases 
formulation  ilp used select paraphrases apply
global constraints satisfied  paraphrase generation falls outside scope ilp
model sentence rewrite operations mainly syntactic provided module based
synchronous tree adjoining grammar  s tag  shieber   schabes         unfortunately 
proof of concept presented  implementation evaluation module left
future work 
   snows learning algorithm variation winnow update rule 

   

figlobal inference sentence compression

work models sentence compression optimization problem  show previously proposed models reformulated context integer linear programming
allows us easily incorporate constraints decoding process  constraints linguistically semantically motivated designed bring less local
syntactic knowledge model help preserve meaning source sentence 
previous work identified several important features compression task  knight
  marcu        mcdonald         however  use global constraints novel
knowledge  although sentence compression explicitly formulated terms
optimization  previous approaches rely optimization procedure generating
best compression  decoding process noisy channel model searches best
compression given source channel models  however  compression found usually sub optimal heuristics used reduce search space locally optimal
due search method employed  example  work turner charniak
       decoder first searches best combination rules apply  traverses
list compression rules  removes sentences outside     best compressions  according channel model   list eventually truncated    compressions 
models  hori   furui        mcdonald        compression score maximized
using dynamic programming however yield suboptimal results  see discussion
section    
contrary nlp work using ilp  a notable exception roth   yih        
view compression generation two stage process learning inference
carried sequentially  i e   first local classifier hypothesizes list possible answers best answer selected using global constraints   models integrate
learning inference unified framework decoding takes place presence
available constraints  local global  moreover  investigate influence
constraint set across models learning paradigms  previous work typically formulates constraints single model  e g   snow classifier  learning paradigm
 e g   supervised   therefore assess constraint based framework advocated
article influences performance expressive models  which require large amounts
parallel data  non expressive ones  which use little parallel data none all  
words  able pose answer following question  kinds models
benefit constraint based inference 
work close spirit rather different content dras         concentrate
compression  specific paraphrase type  apply models sentence level 
constraints thus affect document whole individual sentences  furthermore  compression generation integral part ilp models  whereas dras assumes
paraphrases generated separate process 

   framework
section present details proposed framework sentence compression 
mentioned earlier  work models sentence compression directly optimization
problem   n possible compressions source sentence many
unreasonable  unlikely one compression satisfactory  knight   marcu         ideally  require function captures operations
   

ficlarke   lapata

 or rules  performed sentence create compression
time factoring desirable operation makes resulting compression 
perform search possible compressions select best one  determined
desirable is  wide range models expressed framework 
prerequisites implementing fairly low  require decoding process expressed linear function set linear constraints  practice  many
models rely markov assumption factorization usually solved dynamic programming based decoding process  algorithms formulated integer
linear programs little effort 
first give brief introduction integer linear programming  extension linear
programming readers unfamiliar mathematical programming  compression
models next described section     constraints section     
    linear programming
linear programming  lp  problems optimization problems constraints 
consist three parts 
decision variables  variables control wish assign
optimal values to 
linear function  the objective function   function wish minimize
maximize  function influences values assigned decision variables 
constraints  problems allow decision variables take certain
values  restrictions constraints 
terms best demonstrated simple example taken winston
venkataramanan         imagine manufacturer tables chairs shall call
telfa corporation  produce table    hour labor   square board feet wood
required  chairs require   hour labor   square board feet wood  telfa
  hours labor    square board feet wood available  profit made
table   gbp   gbp chairs  wish determine number tables
chairs manufactured maximize telfas profit 
first  must determine decision variables  case define 
x    number tables manufactured
x    number chairs manufactured
objective function value wish maximize  namely profit 
profit    x     x 
two constraints problem  must exceed   hours labor
   square board feet wood must used  also  cannot create negative
amount chairs tables 
   

figlobal inference sentence compression

labor constraint
x     x 
wood constraint
 x     x 
variable constraints
x 
x 

 
  
 
 

decision variables  objective function constraints determined
express lp model 
max z    x     x   objective function 
subject  s t  
x    x 
 x     x 
x 
x 

   labor constraint 
    wood constraint 
 
 

two basic concepts involved solving lp problems feasibility region
optimal solution  optimal solution one constraints satisfied
objective function minimized maximized  specification value
decision variable referred point  feasibility region lp region
consisting set points satisfy lps constraints  optimal solution
lies within feasibility region  point minimum maximum objective
function value 
set points satisfying single linear inequality half space  feasibility region
defined intersection half spaces  for linear inequalities  forms
polyhedron  telfa example forms polyhedral set  a polyhedral convex set 
intersection four constraints  figure  a shows feasible region telfa
example  find optimal solution graph line  or hyperplane  points
objective function value  maximization problems called isoprofit
line minimization problems isocost line  one isoprofit line represented
dashed black line figure  a  one isoprofit line find isoprofit
lines moving parallel original isoprofit line 
extreme points polyhedral set defined intersections lines
form boundaries polyhedral set  points b c figure  a  
shown lp optimal solution  extreme point globally
optimal  reduces search space optimization problem finding extreme
point highest lowest value  simplex algorithm  dantzig        solves lps
exploring extreme points polyhedral set  specifically  moves one extreme
point adjacent extreme point  extreme points lie line segment 
optimal extreme point found  although simplex algorithm exponential
worst case complexity  practice algorithm efficient 
  
 
optimal solution telfa example z      
    x        x        thus 
achieve maximum profit       gbp must build      tables      chairs 
obviously impossible would expect people buy fractions tables chairs 
here  want able constrain problem decision variables
take integer values  done integer linear programming 
   

ficlarke   lapata

a 

b 

  

  

 

 

  lps feasible region

 x     x      

 x    x      

 

 

 

 

  b

 

x   

x   

 

 

  ip feasible point
  ip relaxations feasible region

 

 

optimal lp solution

optimal lp solution
 

c

 

x     x     

 
 



 

 

 

 

x 

 


 

 

x     x     

  

 

 

 

 

 

 

x 

 

 

 

 

figure    feasible region telfa example using linear  graph  a   integer linear
 graph  b   programming

    integer linear programming
integer linear programming  ilp  problems lp problems
variables required non negative integers  formulated similar manner
lp problems added constraint decision variables must take non negative
integer values 
formulate telfa problem ilp model merely add constraints x 
x  must integer  gives 
max z    x     x   objective function 
subject  s t  
x    x 
 x     x 
x 
x 


   labor constraint 

    wood constraint 
   x  integer
   x  integer

lp models  proved optimal solution lies extreme point
feasible region  case integer linear programs  wish consider points
integer values  illustrated figure  b telfa problem  contrast
linear programming  solved efficiently worst case  integer programming
problems many practical situations np hard  cormen  leiserson    rivest        
   

figlobal inference sentence compression

fortunately  ilps well studied optimization problem number techniques
developed find optimal solution  two techniques cutting planes
method  gomory        branch and bound method  land   doig        
briefly discuss methods here  detailed treatment refer interested
reader winston venkataramanan        nemhauser wolsey        
cutting planes method adds extra constraints slice parts feasible region
contains integer extreme points  however  process difficult
impossible  nemhauser   wolsey         branch and bound method enumerates
points ilps feasible region prunes sections region known
sub optimal  relaxing integer constraints solving resulting
lp problem  known lp relaxation   solution lp relaxation integral 
optimal solution  otherwise  resulting solution provides upper bound
solution ilp  algorithm proceeds creating two new sub problems based
non integer solution one variable time  solved process
repeats optimal integer solution found 
using branch and bound method  find optimal solution telfa
problem z       x       x       thus  achieve maximum profit    gbp  telfa
must manufacture   tables   chairs  relatively simple problem  could
solved merely inspection  ilp problems involve many variables constraints
resulting feasible region large number integer points  branch and bound
procedure efficiently solve ilps matter seconds forms part many
commercial ilp solvers  experiments use lp solve     free optimization package
relies simplex algorithm brand and bound methods solving ilps 
note special circumstances solving methods may applicable 
example  implicit enumeration used solve ilps variables binary
 also known pure    problems   implicit enumeration similar branch andbound method  systematically evaluates possible solutions  without however explicitly
solving  potentially  large number lps derived relaxation  removes
much computational complexity involved determining sub problem infeasible  furthermore  class ilp problems known minimum cost network flow
problems  mcnfp   lp relaxation always yields integral solution  problems
therefore treated lp problems 
general  model yield optimal solution variables integers
constraint matrix property known total unimodularity  matrix totally
unimodular every square sub matrix determinant equal         
case constraint matrix looks totally unimodular  easier
problem solve branch and bound methods  practice good
formulate ilps many variables possible coefficients        
constraints  winston   venkataramanan        
    constraints logical conditions
although integer variables ilp problems may take arbitrary values  frequently
restricted      binary variables     variables  particularly useful rep   software available http   lpsolve sourceforge net  

   

ficlarke   lapata

condition
implication
iff

xor



statement
b
b
b c
xor b xor c
b


constraint
ba 
ab  
a b c 
a b c  
     b    
 a  

table    represent logical conditions using binary variables constraints ilp 

resenting variety logical conditions within ilp framework use constraints  table   lists several logical conditions equivalent constraints 
express transitivity  i e   c b  although often thought transitivity expressed polynomial expression binary
variables  i e   ab   c   possible replace latter following linear inequalities  williams        

   c     
   c    b  
c      a       b   
easily extended model indicator variables representing whether set binary
variables take certain values 
    compression models
section describe three compression models reformulate integer linear
programs  first model simple language model used baseline
previous research  knight   marcu         second model based work hori
furui         combines language model corpus based significance scoring
function  we omit confidence score derived speech recognizer since
models applied text only   model requires small amount parallel data
learn weights language model significance score 
third model fully supervised  uses discriminative large margin framework
 mcdonald         trained trained larger parallel corpus  chose model
instead popular noisy channel decision tree models  two reasons  practical one theoretical one  first  mcdonalds        model delivers performance superior
decision tree model  which turn performs comparably noisy channel   second  noisy channel entirely appropriate model sentence compression 
uses language model trained uncompressed sentences even though represents
probability compressed sentences  result  model consider compressed sentences less likely uncompressed ones  a discussion provided turner  
charniak        
   

figlobal inference sentence compression

      language model
language model perhaps simplest model springs mind  require
parallel corpus  although relatively large monolingual corpus necessary training  
naturally prefer short sentences longer ones  furthermore  language model
used drop words either infrequent unseen training corpus  knight
marcu        use bigram language model baseline noisy channel
decision tree models 
let x   x    x            xn denote source sentence wish generate target
compression  introduce decision variable word source constrain
binary  value   represents word dropped  whereas value   includes
word target compression  let 
 

 

  xi compression
         n 
  otherwise

using unigram language model  objective function would maximize
overall sum decision variables  i e   words  multiplied unigram probabilities
 all probabilities throughout paper log transformed  
max

n
x

p  xi  

   

i  

thus  word selected  corresponding given value    probability
p  xi   according language model counted total score 
unigram language model probably generate many ungrammatical compressions 
therefore use context aware model objective function  namely trigram
model  dynamic programming would typically used decode language model
traversing sentence left to right manner  algorithm efficient provides
context required conventional language model  however  difficult
impossible incorporate global constraints model decisions word
inclusion cannot extend beyond three word window  formulating decoding process
trigram language model integer linear program able take account
constraints affect compressed sentence globally  process much
involved task unigram case context  instead must
make decisions based word sequences rather isolated words  first create
additional decision variables 
 

 

ij  



 

ijk  

  xi starts compression
         n 
  otherwise

sequence xi   xj ends
compression
         n   

  otherwise
j  i           n 



 

sequence xi   xj   xk          n   
compression j  i           n   

  otherwise
k  j           n 
   

ficlarke   lapata

objective function given equation      sum possible trigrams
occur compressions source sentence x  represents start
token xi ith word sentence x  equation     constrains decision variables
binary 
max z  

n
x

p  xi  start 
i  
n 
n
x n 
x x

 

ijk p  xk  xi   xj  

i   j i   k j  

 

n 
x

n
x

ij p  end xi   xj  

   

i   j i  

subject to 

    ij   ijk      

   

objective function     allows combination trigrams selected 
means invalid trigram sequences  e g   two trigrams containing end token 
could appear target compression  avoid situation introducing sequential
constraints  on decision variables   ijk     ij   restrict set allowable
trigram combinations 
constraint  

exactly one word begin sentence 
n
x

   

   

i  

constraint   word included sentence must either start sentence
preceded two words one word start token x   
k k

k 
x k 
x

ijk    

   

i   j  

k   k          n 
constraint   word included sentence must either preceded one
word followed another must preceded one word end sentence 
j

j 
x

n
x

ijk

i   k j  

j 
x

ij    

   

i  

j   j          n 

constraint   word sentence must followed two words followed
one word end sentence must preceded one word end
sentence 


n 
x

n
x

j i   k j  

ijk

n
x

j i  

   

ij

i 
x

hi    

h  

           n 

   

figlobal inference sentence compression

constraint  

exactly one word pair end sentence 
n 
x

n
x

ij    

   

i   j i  

sequential constraints described ensure second order factorization  for
trigrams  holds different compression specific constraints presented section     
unless normalized sentence length  language model naturally prefer one word
output  normalization however non linear cannot incorporated ilp
formulation  instead  impose constraint length compressed sentence 
equation     forces compression contain least b tokens 
n
x

b

   

i  

alternatively  could force compression exactly b tokens  by substituting
inequality equality      less b tokens  by replacing    
constraint     language model specific used elsewhere 
      significance model
language model described notion content words include
compression thus prefers words seen before  words constituents
different relative importance different documents even sentences 
inspired hori furui         add objective function  see equation     
significance score designed highlight important content words  hori furuis
original formulation word weighted score similar un normalized tf idf  
significance score applied indiscriminately words sentence solely
topic related words  namely nouns verbs  score differs one respect  combines
document level sentence level significance  addition tf idf   word
weighted level embedding syntactic tree 
intuitively  sentence multiply nested clauses  deeply embedded clauses
tend carry semantic content  illustrated figure   depicts
clause embedding sentence mr field said resign reselected 
move could divide party nationally  here  important information
conveyed clauses s   he resign  s   if reselected  embedded 
accordingly  give weight words found clauses main
clause  s  figure     simple way enforce give clauses weight proportional
level embedding  modified significance score becomes 
i xi    

fa
l
log
n


    

xi topic word  frequency xi document corpus
respectively  fa sum topic words corpus  l number clause
   compression rate limited range including two inequality constraints 

   

ficlarke   lapata

s 
s 
mr field said
s 
resign
s 
reselected
  move
sbar
could divide party nationally

figure    clause embedding sentence mr field said resign
reselected  move could divide party nationally  nested boxes
correspond nested clauses 

constituents xi   n deepest level clause embedding  fa
estimated large document collection  document specific  whereas nl sentencespecific  so  figure   term nl           clause s               clause s   
on  individual words inherit weight clauses 
modified objective function significance score given below 
max z  

n
x

i xi    

i  
n 
x n 
x

 

n
x

p  xi  start 

i  

n
x

ijk p  xk  xi   xj  

i   j i   k j  

 

n 
x

n
x

ij p  end xi   xj  

    

i   j i  

add weighting factor    objective  order counterbalance importance language model significance score  weight tuned small
parallel corpus  sequential constraints equations        used ensure
trigrams combined valid way 
      discriminative model
fully supervised model  used discriminative model presented mcdonald
        model uses large margin learning framework coupled feature set
defined compression bigrams syntactic structure 
let x   x            xn denote source sentence target compression   y            ym
yj occurs x  function l yi            n  maps word yi target com   

figlobal inference sentence compression

pression index word source sentence  x  include constraint
l yi     l yi     forces word x occur compression
y  let score compression sentence x be 
    

s x  y 

score factored using first order markov assumption words target
compression give 
s x  y   

 y 
x

s x  l yj     l yj   

    

j  

score function defined dot product high dimensional feature
representation corresponding weight vector 
s x  y   

 y 
x

w f  x  l yj     l yj   

    

j  

decoding model amounts finding combination bigrams maximizes
scoring function       mcdonald        uses dynamic programming approach
maximum score found left to right manner  algorithm extension
viterbi case scores factor dynamic sub strings  sarawagi   cohen 
      mcdonald  crammer    pereira      a   allows back pointers used
reconstruct highest scoring compression well k best compressions 
similar trigram language model decoding process  see section        
except bigram model used  consequently  ilp formulation slightly
simpler trigram language model  let 
 

 

  xi compression
   n 
  otherwise

introduce decision variables 
 
 
ij  

 

 

 

  xi starts compression
         n 
  otherwise

  word xi ends compression
  otherwise
         n 

  sequence xi   xj compression          n   
  otherwise
j  i           n 

discriminative model expressed as 
max z  

n
x


i  
n 
x

s x     i 

 

s x  i  n     

 

n
x

i   j i  
n
x
i  

   

ij s x  i  j 
    

ficlarke   lapata

constraint  

exactly one word begin sentence 
n
x

   

    

i  

constraint   word included sentence must either start compression
follow another word 

j j

j
x

ij    

    

i  

j   j          n 
constraint   word included sentence must either followed another
word end sentence 



n
x

ij    

    

j i  

           n 

constraint  

exactly one word end sentence 
n
x

   

    

i  

again  sequential constraints equations          necessary ensure
resulting combination bigrams valid 
current formulation provides single optimal compression given model  however  mcdonalds        dynamic programming algorithm capable returning k best
compressions  useful learning algorithm described later  order produce
k best compressions  must rerun ilp extra constraints forbid previous
solutions  words  first formulate ilp above  solve it  add solution
k best list  create set constraints forbid configuration decision
variables form current solution  procedure repeated k compressions
found 
computation compression score crucially relies dot product
high dimensional feature representation corresponding weight vector  see equation        mcdonald        employs rich feature set defined adjacent words
individual parts of speech  dropped words phrases source sentence  dependency structures  also source sentence   features designed mimic
information presented previous noisy channel decision tree models knight
marcu         features adjacent words used proxy language model
noisy channel  unlike models  treat parses gold standard  mcdonald
uses dependency information another form evidence  faced parses
noisy learning algorithm reduce weighting given features prove
   

figlobal inference sentence compression

poor discriminators training data  thus  model much robust
portable across different domains training corpora 
weight vector  w learned using margin infused relaxed algorithm  mira 
crammer   singer        discriminative large margin online learning technique  mcdonald  crammer    pereira      b   algorithm learns compressing sentence
comparing result gold standard  weights updated score
correct compression  the gold standard  greater score compressions margin proportional loss  loss function number words falsely
retained dropped incorrect compression relative gold standard  source
sentence exponentially many compressions thus exponentially many margin
constraints  render learning computationally tractable  mcdonald et al       b  create
constraints k compressions currently highest score  bestk  x  w  
    constraints
ready describe compression specific constraints  models presented
previous sections contain sequential constraints thus equivalent
original formulation  constraints linguistically semantically motivated
similar fashion grammar checking component jing         however 
rely additional knowledge sources  such grammar lexicon wordnet 
beyond parse grammatical relations source sentence  obtain
rasp  briscoe   carroll         domain independent  robust parsing system english 
however  parser broadly similar output  e g   lin        could serve
purposes  constraints revolve around modification  argument structure  discourse
related factors 
modifier constraints modifier constraints ensure relationships head words
modifiers remain grammatical compression 
j  

    

i  j   xj xi ncmods
j  

    

i  j   xj xi detmods
equation      guarantees include non clausal modifier   ncmod  compression  such adjective noun  head modifier must included 
repeated determiners  detmod        table   illustrate constraints disallow deletion certain words  starred sentences denote compressions
would possible given constraints   example  modifier word pasok
sentence   a  compression  head party included  see   b   
want ensure meaning source sentence preserved
compression  particularly face negation  equation      implements forcing
compression head included  see sentence   b  table     similar
constraint added possessive modifiers  e g   his  our   including genitives  e g   johns
   clausal modifiers  cmod  adjuncts modifying entire clauses  example ate cake
hungry  because clause modifier sentence ate cake 

   

ficlarke   lapata

 a 
 b 
 a 
 b 
 c 
 a 
 b 
 c 
 d 
 e 
 f 

became power player greek politics       founded
socialist pasok party 
 he became power player greek politics       founded
pasok 
took troubled youth dont fathers  brought
room dads dont children 
 we took troubled youth fathers  brought
room dads children 
 we took troubled youth dont fathers  brought
room dads dont children 
chain stretched uganda grenada nicaragua  since     s 
 stretched uganda grenada nicaragua  since     s 
 the chain uganda grenada nicaragua  since     s 
 the chain stretched uganda grenada nicaragua  since     s 
 the chain stretched grenada nicaragua  since     s 
 the chain stretched uganda grenada nicaragua  since     s 
table    examples compressions disallowed set constraints 

gift   shown equation       example possessive constraint given
sentence   c  table   
j    

    

i  j   xj xi ncmods xj  
j    

    

i  j   xj xi possessive mods
argument structure constraints define intuitive constraints take
overall sentence structure account  first constraint  equation       ensures
verb present compression arguments 
arguments included compression verb must included  thus
force program make decision verb  subject  object  see
sentence   b  table    
j    

    

i  j   xj subject object verb xi
second constraint forces compression contain least one verb provided
source sentence contains one well 
x

 

    

i xi verbs

constraint entails possible drop main verb stretched sentence   a   see sentence   c  table    
   

figlobal inference sentence compression

sentential constraints include equations           apply prepositional phrases subordinate clauses  constraints force introducing term
 i e   preposition  subordinator  included compression word
within syntactic constituent included  subordinator mean wh words
 e g   who  which  how  where   word that  subordinating conjunctions  e g   after 
although  because   reverse true  i e   introducing term included 
least one word syntactic constituent included 
j  

    

i  j   xj pp sub
xi starts pp sub
x

j  

    

i xi pp sub

j   xj starts pp sub
example consider sentence   d  table    here  cannot drop preposition
uganda compression  conversely  must include uganda
compression  see sentence   e   
wish handle coordination  two head words conjoined source
sentence  included compression coordinating conjunction must
included 
       j  

    

       k  

    

     j        k    

    

i  j  k   xj xk conjoined xi
consider sentence   f  table    uganda nicaragua present
compression  must include conjunction and 
finally  equation      disallows anything within brackets source sentence
included compression  somewhat superficial attempt excluding
parenthetical potentially unimportant material compression 
   

    

  xi bracketed words  inc parentheses 
discourse constraints discourse constraint concerns personal pronouns  specifically  equation      forces personal pronouns included compression 
constraint admittedly important generating coherent documents  as opposed
individual sentences   nevertheless impact sentence level compressions 
particular verbal arguments missed parser  pronominal 
constraint      result grammatical output since argument structure
source sentence preserved compression 
   
  xi personal pronouns
   

    

ficlarke   lapata

note constraints described would captured
models learn synchronous deletion rules corpus  example  noisy channel
model knight marcu        learns drop head latter modified
adjective noun  since transformations dt nn dt ajd nn adj
almost never seen data  similarly  coordination constraint  equations          
would enforced using turner charniaks        special rules enhance
parallel grammar rules modeling structurally complicated deletions
attested corpus  designing constraints aimed capturing appropriate
deletions many possible models  including rely training corpus
explicit notion parallel grammar  e g   mcdonald        
modification constraints would presumably redundant noisy channel model 
could otherwise benefit specialized constraints  e g   targeting sparse rules
noisy parse trees  however leave future work 
another feature modeling framework presented deletions  or nondeletions  treated unconditional decisions  example  require drop
noun adjective noun sequences adjective deleted well  require
always include verb compression source sentence one  hardwired decisions could cases prevent valid compressions considered  instance 
possible compress sentence appropriate behavior
appropriate orbob loves mary john loves susan bob loves mary john
susan  admittedly lose expressive power  yet ensure compressions
broadly grammatically  even unsupervised semi supervised models  furthermore  practice find models consistently outperform non constraint based
alternatives  without extensive constraint engineering 
    solving ilp
mentioned earlier  section       solving ilps np hard  cases coefficient matrix unimodular  shown optimal solution linear
program integral  although coefficient matrix problems unimodular 
obtained integral solutions sentences experimented  approximately       
see section     details   conjecture due fact variables         coefficients constraints therefore constraint matrix
shares many properties unimodular matrix  generate solve ilp every
sentence wish compress  solve times less second per sentence  including
input output overheads  models presented here 

   experimental set up
evaluation experiments motivated three questions      constraintbased compression models deliver performance gains non constraint based ones 
expect better compressions model variants incorporate compression specific
constraints      differences among constraint based models  here  would
investigate much modeling power gained addition constraints 
example  may case state of the art model mcdonalds       
benefit much addition constraints  effect much bigger less
   

figlobal inference sentence compression

sophisticated models      models reported paper port across domains 
particular  interested assessing whether models proposed constraints
general robust enough produce good compressions written spoken
texts 
next describe data sets models trained tested  section      
explain model parameters estimated  section      present evaluation setup
 section       discuss results section   
    corpora
intent assess performance models described written spoken
text  appeal written text understandable since summarization work today
focuses domain  speech data provides natural test bed compression
applications  e g   subtitle generation  poses additional challenges  spoken utterances ungrammatical  incomplete  often contain artefacts false starts 
interjections  hesitations  disfluencies  rather focusing spontaneous speech
abundant artefacts  conduct study less ambitious domain
broadcast news transcripts  lies in between extremes written text spontaneous speech scripted beforehand usually read autocue 
previous work sentence compression almost exclusively used ziff davis corpus
training testing purposes  corpus originates collection news articles
computer products  created automatically matching sentences occur
article sentences occur abstract  knight   marcu         abstract
sentences contain subset source sentences words word order
remain same  earlier work  clarke   lapata        argued
ziff davis corpus ideal studying compression several reasons  first  showed
human authored compressions differ substantially ziff davis tends
aggressively compressed  second  humans likely drop individual words
lengthy constituents  third  test portion ziff davis contains solely    sentences  extremely small data set reveal statistically significant differences
among systems  fact  previous studies relied almost exclusively human judgments
assessing well formedness compressed output  significance tests reported
by subjects analyses only 
thus focused present study manually created corpora  specifically 
asked annotators perform sentence compression removing tokens sentence bysentence basis  annotators free remove words deemed superfluous provided
deletions   a  preserved important information source sentence 
 b  ensured compressed sentence remained grammatical  wished  could leave
sentence uncompressed marking inappropriate compression 
allowed delete whole sentences even believed contained information content
respect story would blur task abstracting  following
guidelines  annotators produced compressions    newspaper articles        sentences 
british national corpus  bnc  american news text corpus  henceforth
written corpus     stories        sentences  hub        english broadcast
news corpus  henceforth spoken corpus   written corpus contains articles la
   

ficlarke   lapata

times  washington post  independent  guardian daily telegraph  spoken
corpus contains broadcast news variety networks  cnn  abc  cspan npr 
manually transcribed segmented story sentence level 
corpora split training  development testing sets  randomly article
boundaries  with set containing full stories  publicly available http 
  homepages inf ed ac uk s        data  
    parameter estimation
work present three compression models ranging unsupervised semisupervised  fully supervised  unsupervised model simply relies trigram language model driving compression  see section         estimated    million tokens north american corpus using cmu cambridge language modeling
toolkit  clarkson   rosenfeld        vocabulary size        tokens goodturing discounting  discourage one word output force ilp generate compressions whose length less     source sentence  see constraint      
semi supervised model weighted combination word based significance score
language model  see section         significance score calculated using
   million tokens american news text corpus  optimized weight  see
equation       small subset training data  three documents case  using powells method  press  teukolsky  vetterling    flannery        loss function
based f score grammatical relations found gold standard compression
systems best compression  see section     details   optimal weight
approximately     written corpus     spoken corpus 
mcdonalds        supervised model trained written spoken training
sets  implementation used feature sets mcdonald  difference
phrase structure dependency features extracted output
roarks        parser  mcdonald uses charniaks        parser performs comparably 
model learnt using k best compressions  development data  found
k      provided best performance 
    evaluation
previous studies relied almost exclusively human judgments assessing wellformedness automatically derived compressions  typically rated naive subjects two dimensions  grammaticality importance  knight   marcu         although
automatic evaluation measures proposed  riezler et al         bangalore  rambow    whittaker        use less widespread  suspect due small size
test portion ziff davis corpus commonly used compression work 
evaluate output models two ways  first  present results using
automatic evaluation measure put forward riezler et al          compare
grammatical relations found system compressions found gold
standard  allows us measure semantic aspects summarization quality terms
grammatical functional information quantified using f score  furthermore 
   splits            sentences written corpus            sentences spoken
corpus 

   

figlobal inference sentence compression

clarke lapata        show relations based f score correlates reliably
human judgments compression output  since test corpora larger ziffdavis  by factor ten   differences among systems highlighted using
significance testing 
implementation f score measure used grammatical relations annotations
provided rasp  briscoe   carroll         parser particularly appropriate
compression task since provides parses full sentences sentence fragments
generally robust enough analyze semi grammatical sentences  calculated f score
relations provided rasp  e g   subject  direct indirect object  modifier    
total  
line previous work evaluate models eliciting human judgments 
following work knight marcu         conducted two separate experiments 
first experiment participants presented source sentence target
compression asked rate well compression preserved important
information source sentence  second experiment  asked rate
grammaticality compressed outputs  cases used five point rating
scale high number indicates better performance  randomly selected    sentences
test portion corpus  sentences compressed automatically
three models presented paper without constraints  included
gold standard compressions  materials thus consisted              sourcetarget sentences  latin square design ensured subjects see two different
compressions sentence  collected ratings    unpaid volunteers  self
reported native english speakers  studies conducted internet using
custom build web interface  examples experimental items given table   

   results
let us first discuss results compression output evaluated terms f score 
tables     illustrate performance models written spoken corpora 
respectively  present compression rate  system  cases
constraint based models   constr  yield better f scores non constrained ones 
difference starker semi supervised model  sig   constraints bring
improvement       written corpus       spoken corpus 
examined whether performance differences among models statistically significant  using
wilcoxon test  written corpus constraint models significantly outperform
models without constraints  tendency observed spoken corpus except
model mcdonald        performs comparably without constraints 
wanted establish best constraint model  corpora
find language model performs worst  whereas significance model mcdonald
perform comparably  i e   f score differences statistically significant   get
feeling difficulty task  calculated much annotators agreed
compression output  inter annotator agreement  f score  written corpus
      spoken corpus        agreement higher spoken texts since
consists many short utterances  e g   okay  thats now  good night 
   term refers percentage words retained source sentence compression 

   

ficlarke   lapata

source

aim give councils control future growth second
homes 
gold
aim give councils control growth homes 
lm
aim future 
lm constr aim give councils control 
sig
aim give councils control future growth homes 
sig constr aim give councils control future growth homes 
mcd
aim give councils 
mcd constr aim give councils control growth homes 
source
clinton administration recently unveiled new means encourage
brownfields redevelopment form tax incentive proposal 
gold
clinton administration unveiled new means encourage brownfields redevelopment tax incentive proposal 
lm
clinton administration form tax 
lm constr clinton administration unveiled means encourage redevelopment form 
sig
clinton administration unveiled encourage brownfields redevelopment form tax proposal 
sig constr clinton administration unveiled means encourage brownfields
redevelopment form tax proposal 
mcd
clinton unveiled means encourage brownfields redevelopment
tax incentive proposal 
mcd constr clinton administration unveiled means encourage brownfields
redevelopment form incentive proposal 
table    example compressions produced systems  source  source sentence  gold 
gold standard compression  lm  language model compression  lm constr  language model compression constraints  sig  significance model  sig constr 
significance model constraints  mcd  mcdonalds        compression model 
mcd constr  mcdonalds        compression model constraints  

compressed little all  note marked difference
automatic human compressions  best performing systems inferior human
output    f score percentage points 
differences automatic systems human output observed
respect compression rate  seen language model compresses
aggressively  whereas significance model mcdonald tend conservative
closer gold standard  interestingly  constraints necessarily increase
compression rate  latter increases significance model decreases
language model remains relatively constant mcdonald  straightforward
impose compression rate constraint based models  e g   forcing model
p
retain b tokens ni     b   however  refrained since wanted
   

figlobal inference sentence compression

models
lm
sig
mcd
lm constr
sig constr
mcd constr
gold

compr
    
    
    
    
    
    
    

f score
    
    
    
    
    
    


table    results written corpus  compression rate  compr  grammatical relation f score  f score      constr model significantly different model
without constraints    significantly different lm constr 
models
lm
sig
mcd
lm constr
sig constr
mcd constr
gold

compr
    
    
    
    
    
    
    

f score
    
    
    
    
    
    


table    results spoken corpus  compression rate  compr  grammatical relation f score  f score      constr model significantly different without
constraints    significantly different lm constr 

models regulate compression rate sentence individually according
specific information content structure 
next consider results human study assesses detail quality
generated compressions two dimensions  namely grammaticality information
content  f score conflates two dimensions therefore theory could unduly reward
system produces perfectly grammatical output without information loss  tables  
  show mean ratings  system  and gold standard  written
spoken corpora  respectively  first performed analysis variance  anova 
examine effect different system compressions  anova revealed reliable effect
grammaticality importance corpus  the effect significant
subjects items  p          
next examine impact constraints   constr tables   cases
observe increase ratings grammaticality importance model
supplemented constraints  post hoc tukey tests reveal grammaticality
importance ratings language model significance model significantly improve
   statistical tests reported subsequently done using mean ratings 

   

ficlarke   lapata

models

grammar
     

importance

lm
sig
mcd

    

     
     
    

lm constr
sig constr
mcd constr
gold

    
    
    
    

     
    
    
    

     

table    results written text corpus  average grammaticality score  grammar 
average importance score  importance  human judgments     constr model
significantly different model without constraints    significantly different
gold standard      significantly different mcd constr 

models

grammar
     

importance

lm
sig
mcd

     
    

    
    
    

lm constr
sig constr
mcd constr
gold

    
    
    
    

     
    
    
    

table    results spoken text corpus  average grammaticality score  grammar 
average importance score  importance  human judgments     constr model
significantly different model without constraints    significantly different
gold standard      significantly different mcd constr 

constraints            contrast  mcdonalds system sees numerical improvement
additional constraints  difference statistically significant 
tendencies observed spoken written corpus 
upon closer inspection  see constraints influence considerably
grammaticality unsupervised semi supervised systems  tukey tests reveal
lm constr sig constr grammatical mcd constr  terms importance 
sig constr mcd constr significantly better lm constr           
surprising given lm constr simple model without mechanism
highlighting important words sentence  interestingly  sig constr performs well
mcd constr retaining important words  despite fact requires
minimal supervision  although constraint based models overall perform better models without constraints  receive lower ratings  for grammaticality importance 
comparison gold standard  differences significant cases 
   

figlobal inference sentence compression

summary  observe constraints boost performance  pronounced compression models either unsupervised use small amounts
parallel data  example  simple model sig yields performance comparable
mcdonald        constraints taken account  encouraging result
suggesting ilp used create good compression models relatively little
effort  i e   without extensive feature engineering elaborate knowledge sources   performance gains obtained competitive models mcdonalds fully
supervised  gains smaller  presumably initial model contains
rich feature representation consisting syntactic information generally good job
producing grammatical output  finally  improvements consistent across corpora
evaluation paradigms 

   conclusions
paper presented novel method automatic sentence compression  key
aspect approach use integer linear programming inferring globally optimal
compressions presence linguistically motivated constraints  shown
previous formulations sentence compression recast ilps extended
models local global constraints ensuring compressed output structurally
semantic well formed  contrary previous work employed ilp solely
decoding  models integrate learning inference unified framework 
experiments demonstrated advantages approach  constraint based
models consistently bring performance gains models without constraints  improvements impressive models require little supervision  case
point significance model discussed above  no constraints incarnation
model performs poorly considerably worse mcdonalds        state of the art
model  addition constraints improves output model performance indistinguishable mcdonald  note significance model requires
small amount training data     parallel sentences   whereas mcdonald trained hundreds sentences  presupposes little feature engineering  whereas mcdonald utilizes
thousands features  effort associated framing constraints  however
created applied across models corpora  observed small
performance gains mcdonalds system latter supplemented constraints 
larger improvements possible sophisticated constraints  however intent
devise set general constraints tuned mistakes specific
system particular 
future improvements many varied  obvious extension concerns constraint set  currently constraints mostly syntactic consider sentence
isolation  incorporating discourse constraints could highlight words important document level  presumably words topical document retained
compression  constraints could manipulate compression rate  example 
could encourage higher compression rate longer sentences  another interesting
direction includes development better objective functions compression task 
objective functions presented far rely first second order markov assumptions 
alternative objectives could take account structural similarity source
   

ficlarke   lapata

sentence target compression  whether share content could
operationalized terms entropy 
beyond task systems presented paper  believe approach holds
promise generation applications using decoding algorithms searching space
possible outcomes  examples include sentence level paraphrasing  headline generation 
summarization 

acknowledgments
grateful annotators vasilis karaiskos  beata kouchnir  sarah luger 
thanks jean carletta  frank keller  steve renals  sebastian riedel helpful
comments suggestions anonymous referees whose feedback helped substantially improve present paper  lapata acknowledges support epsrc  grant
gr t           preliminary version work published proceedings
acl      

references
aho  a  v     ullman  j  d          syntax directed translations pushdown assembler  journal computer system sciences          
bangalore  s   rambow  o     whittaker  s          evaluation metrics generation 
proceedings first international conference natural language generation 
pp      mitzpe ramon  israel 
barzilay  r     lapata  m          aggregation via set partitioning natural language
generation  proceedings human language technology conference
north american chapter association computational linguistics  pp     
     new york  ny  usa 
bramsen  p   deshpande  p   lee  y  k     barzilay  r          inducing temporal graphs 
proceedings      conference empirical methods natural language
processing  pp          sydney  australia 
briscoe  e  j     carroll  j          robust accurate statistical annotation general text 
proceedings third international conference language resources evaluation  pp            las palmas  gran canaria 
charniak  e          maximum entropy inspired parser  proceedings  st north
american annual meeting association computational linguistics  pp     
     seattle  wa  usa 
clarke  j     lapata  m          models sentence compression  comparison across
domains  training requirements evaluation measures  proceedings   st
international conference computational linguistics   th annual meeting
association computational linguistics  pp          sydney  australia 
clarkson  p     rosenfeld  r          statistical language modeling using cmu
cambridge toolkit  proceedings eurospeech    pp            rhodes  greece 
   

figlobal inference sentence compression

cormen  t  h   leiserson  c  e     rivest  r  l          intoduction algorithms 
mit press 
corston oliver  s          text compaction display small screens  proceedings workshop automatic summarization  nd meeting north
american chapter association computational linguistics  pp        pittsburgh  pa  usa 
crammer  k     singer  y          ultraconservative online algorithms multiclass problems  journal machine learning research            
dantzig  g  b          linear programming extensions  princeton university press 
princeton  nj  usa 
denis  p     baldridge  j          joint determination anaphoricity coreference
resolution using integer programming  human language technologies      
conference north american chapter association computational linguistics  proceedings main conference  pp          rochester  ny 
dras  m          tree adjoining grammar reluctant paraphrasing text  ph d 
thesis  macquarie university 
galley  m     mckeown  k          lexicalized markov grammars sentence compression 
proceedings north american chapter association computational
linguistics  pp          rochester  ny  usa 
gomory  r  e          solving linear programming problems integers  bellman 
r     hall  m   eds    combinatorial analysis  proceedings symposia applied
mathematics  vol      providence  ri  usa 
grefenstette  g          producing intelligent telegraphic text reduction provide
audio scanning service blind  hovy  e     radev  d  r   eds    proceedings
aaai symposium intelligent text summarization  pp          stanford 
ca  usa 
hori  c     furui  s          speech summarization  approach word extraction
method evaluation  ieice transactions information systems  e  d           
jing  h          sentence reduction automatic text summarization  proceedings
 th applied natural language processing conference  pp          seattle wa 
usa 
knight  k     marcu  d          summarization beyond sentence extraction  probabilistic
approach sentence compression  artificial intelligence                 
land  a  h     doig  a  g          automatic method solving discrete programming
problems  econometrica             
lin  c  y          improving summarization performance sentence compression pilot
study  proceedings  th international workshop information retrieval
asian languages  pp      sapporo  japan 
lin  d          latat  language text analysis tools  proceedings first human
language technology conference  pp          san francisco  ca  usa 
   

ficlarke   lapata

marciniak  t     strube  m          beyond pipeline  discrete optimization nlp 
proceedings ninth conference computational natural language learning 
pp          ann arbor  mi  usa 
mcdonald  r          discriminative sentence compression soft syntactic constraints 
proceedings   th conference european chapter association
computational linguistics  trento  italy 
mcdonald  r   crammer  k     pereira  f       a   flexible text segmentation structured multilabel classification  proceedings human language technology conference conference empirical methods natural language processing  pp 
        vancouver  bc  canada 
mcdonald  r   crammer  k     pereira  f       b   online large margin training dependency parsers    rd annual meeting association computational
linguistics  pp        ann arbor  mi  usa 
nemhauser  g  l     wolsey  l  a          integer combinatorial optimization  wileyinterscience series discrete mathematicals opitmization  wiley  new york  ny 
usa 
nguyen  m  l   shimazu  a   horiguchi  s   ho  t  b     fukushi  m          probabilistic
sentence reduction using support vector machines  proceedings   th international conference computational linguistics  pp          geneva  switzerland 
press  w  h   teukolsky  s  a   vetterling  w  t     flannery  b  p          numerical
recipes c  art scientific computing  cambridge university press  new
york  ny  usa 
punyakanok  v   roth  d   yih  w     zimak  d          semantic role labeling via integer linear programming inference  proceedings international conference
computational linguistics  pp            geneva  switzerland 
riedel  s     clarke  j          incremental integer linear programming non projective
dependency parsing  proceedings      conference empirical methods
natural language processing  pp          sydney  australia 
riezler  s   king  t  h   crouch  r     zaenen  a          statistical sentence condensation
using ambiguity packing stochastic disambiguation methods lexical functional
grammar  human language technology conference  rd meeting
north american chapter association computational linguistics  pp     
     edmonton  canada 
roark  b          probabilistic top down parsing language modeling  computational
linguistics                 
roth  d          learning resolve natural language ambiguities  unified approach 
proceedings   th american association artificial intelligence  pp 
        madison  wi  usa 
roth  d     yih  w          linear programming formulation global inference
natural language tasks  proceedings annual conference computational
natural language learning  pp      boston  ma  usa 
   

figlobal inference sentence compression

roth  d     yih  w          integer linear programming inference conditional random
fields  proceedings international conference machine learning  pp     
     bonn 
sarawagi  s     cohen  w  w          semi markov conditional random fields information extraction  advances neural information processing systems  vancouver 
bc  canada 
shieber  s     schabes  y          synchronous tree adjoining grammars  proceedings   th international conference computational linguistics  pp         
helsinki  finland 
turner  j     charniak  e          supervised unsupervised learning sentence
compression  proceedings   rd annual meeting association computational linguistics  pp          ann arbor  mi  usa 
vandeghinste  v     pan  y          sentence compression automated subtitling 
hybrid approach  marie francine moens  s  s   ed    text summarization branches
out  proceedings acl    workshop  pp        barcelona  spain 
williams  h  p          model building mathematical programming   th edition   wiley 
winston  w  l     venkataramanan  m          introduction mathematical programming  applications algorithms   th edition   duxbury 
zajic  d   door  b  j   lin  j     schwartz  r          multi candidate reduction  sentence
compression tool document summarization tasks  information processing
management special issue summarization                   

   


