journal of artificial intelligence research                

submitted        published      

planning with durative actions in stochastic domains
mausam
daniel s  weld

mausam   cs   washington   edu
weld   cs   washington   edu

dept of computer science and engineering
box         university of washington
seattle  wa       usa

abstract
probabilistic planning problems are typically modeled as a markov decision process  mdp  
mdps  while an otherwise expressive model  allow only for sequential  non durative actions  this
poses severe restrictions in modeling and solving a real world planning problem  we extend the
mdp model to incorporate     simultaneous action execution     durative actions  and    stochastic durations  we develop several algorithms to combat the computational explosion introduced by
these features  the key theoretical ideas used in building these algorithms are  modeling a complex problem as an mdp in extended state action space  pruning of irrelevant actions  sampling
of relevant actions  using informed heuristics to guide the search  hybridizing different planners
to achieve benefits of both  approximating the problem and replanning  our empirical evaluation
illuminates the different merits in using various algorithms  viz   optimality  empirical closeness to
optimality  theoretical error bounds  and speed 

   introduction
recent progress achieved by planning researchers has yielded new algorithms which relax  individually  many of the classical assumptions  for example  successful temporal planners like sgplan 
sapa  etc   chen  wah    hsu        do   kambhampati        are able to model actions that take
time  and probabilistic planners like gpt  lao   spudd  etc   bonet   geffner        hansen  
zilberstein        hoey  st aubin  hu    boutilier        can deal with actions with probabilistic
outcomes  etc  however  in order to apply automated planning to many real world domains we must
eliminate larger groups of the assumptions in concert  for example  nasa researchers note that
optimal control for a nasa mars rover requires reasoning about uncertain  concurrent  durative
actions and a mixture of discrete and metric fluents  bresina  dearden  meuleau  smith    washington         while todays planners can handle large problems with deterministic concurrent
durative actions  and mdps provide a clear framework for non concurrent durative actions in the
face of uncertainty  few researchers have considered concurrent  uncertain  durative actions  the
focus of this paper 
as an example consider the nasa mars rovers  spirit and oppurtunity  they have the goal of
gathering data from different locations with various instruments  color and infrared cameras  microscopic imager  mossbauer spectrometers etc   and transmitting this data back to earth  concurrent
actions are essential since instruments can be turned on  warmed up and calibrated  while the rover
is moving  using other instruments or transmitting data  similarly  uncertainty must be explicitly
confronted as the rovers movement  arm control and other actions cannot be accurately predicted 
furthermore  all of their actions  e g   moving between locations and setting up experiments  take
time  in fact  these temporal durations are themselves uncertain  the rover might lose its way and
c
    
ai access foundation  all rights reserved 

fim ausam   w eld

take a long time to reach another location  etc  to be able to solve the planning problems encountered by a rover  our planning framework needs to explicitly model all these domain constructs 
concurrency  actions with uncertain outcomes and uncertain durations 
in this paper we present a unified formalism that models all these domain features together 
concurrent markov decision processes  comdps  extend mdps by allowing multiple actions per
decision epoch  we use comdps as the base to model all planning problems involving concurrency 
problems with durative actions  concurrent probabilistic temporal planning  cptp   are formulated
as comdps in an extended state space  the formulation is also able to incorporate the uncertainty
in durations in the form of probabilistic distributions 
solving these planning problems poses several computational challenges  concurrency  extended durations  and uncertainty in those durations all lead to explosive growth in the state space 
action space and branching factor  we develop two techniques  pruned rtdp and sampled rtdp to
address the blowup from concurrency  we also develop the dur family of algorithms to handle
stochastic durations  these algorithms explore different points in the running time vs  solutionquality tradeoff  the different algorithms propose several speedup mechanisms such as     pruning of provably sub optimal actions in a bellman backup     intelligent sampling from the action
space     admissible and inadmissible heuristics computed by solving non concurrent problems    
hybridizing two planners to obtain a hybridized planner that finds good quality solution in intermediate running times     approximating stochastic durations by their mean values and replanning    
exploiting the structure of multi modal duration distributions to achieve higher quality approximations 
the rest of the paper is organized as follows  in section   we discuss the fundamentals of
mdps and the real time dynamic programming  rtdp  solution method  in section   we describe
the model of concurrent mdps  section   investigates the theoretical properties of the temporal
problems  section   explains our formulation of the cptp problem for deterministic durations  the
algorithms are extended for the case of stochastic durations in section    each section is supported
with an empirical evaluation of the techniques presented in that section  in section   we survey the
related work in the area  we conclude with future directions of research in sections   and   

   background
planning problems under probabilistic uncertainty are often modeled using markov decision processes  mdps   different research communities have looked at slightly different formulations of
mdps  these versions typically differ in objective functions  maximizing reward vs  minimizing
cost   horizons  finite  infinite  indefinite  and action representations  dbn vs  parametrized action
schemata   all these formulations are very similar in nature  and so are the algorithms to solve
them  though  the methods proposed in the paper are applicable to all the variants of these models 
for clarity of explanation we assume a particular formulation  known as the stochastic shortest path
problem  bertsekas        
we define a markov decision process  m  as a tuple hs  a  ap  pr  c  g  s  i in which
 s is a finite set of discrete states  we use factored mdps  i e   s is compactly represented in
terms of a set of state variables 
 a is a finite set of actions 
  

fip lanning with d urative actions in s tochastic d omains

state variables   x    x    x    x    p  
action
precondition
effect
toggle x 
p  
x   x 
toggle x 
p  
x   x 
toggle x 
true
x   x 
no change
toggle x 
true
x   x 
no change
toggle p  
true
p    p  
goal   x       x       x       x     

probability
 
 
   
   
   
   
 

figure    probabilistic strips definition of a simple mdp with potential parallelism
 ap defines an applicability function  ap   s  p a   denotes the set of actions that can be
applied in a given state  p represents the power set  
 pr   s  a  s         is the transition function  we write pr s   s  a  to denote the
probability of arriving at state s  after executing action a in state s 
 c   s  a  s     is the cost model  we write c s  a  s    to denote the cost incurred when
the state s  is reached after executing action a in state s 
 g  s is a set of absorbing goal states  i e   the process ends once one of these states is
reached 
 s  is a start state 
we assume full observability  i e   the execution system has complete access to the new state
after an action has been performed  we seek to find an optimal  stationary policy  i e   a function
  s  a that minimizes the expected cost  over an indefinite horizon  incurred to reach a goal
state  note that any cost function  j  s     mapping states to the expected cost of reaching a goal
state defines a policy as follows 
j  s    argmin

x

pr s   s  a  c s  a  s      j s   


 

   

aap s  s  s

the optimal policy derives from the optimal cost function  j    which satisfies the following pair
of bellman equations 
j   s       if s  g else
j   s    min

aap s 

x

pr s   s  a  c s  a  s      j   s   


 

   

s  s

for example  figure   defines a simple mdp where four state variables  x            x    need to be
set using toggle actions  some of the actions  e g   toggle x  are probabilistic 
various algorithms have been developed to solve mdps  value iteration is a dynamic programming approach in which the optimal cost function  the solution to equations    is calculated as the
limit of a series of approximations  each considering increasingly long action sequences  if jn  s 
  

fim ausam   w eld

is the cost of state s in iteration n  then the cost of state s in the next iteration is calculated with a
process called a bellman backup as follows 
jn    s    min

aap s 

x

pr s   s  a  c s  a  s      jn  s   


 

   

s  s

value iteration terminates when s  s   jn  s   jn   s      and this termination is guaranteed for       furthermore  in the limit  the sequence of  ji   is guaranteed to converge to the
optimal cost function  j    regardless of the initial values as long as a goal can be reached from every reachable state with non zero probability  unfortunately  value iteration tends to be quite slow 
since it explicitly updates every state  and  s  is exponential in the number of domain features  one
optimization restricts search to the part of state space reachable from the initial state s    two algorithms exploiting this reachability analysis are lao   hansen   zilberstein        and our focus 
rtdp  barto  bradtke    singh        
rtdp  conceptually  is a lazy version of value iteration in which the states get updated in proportion to the frequency with which they are visited by the repeated executions of the greedy policy 
an rtdp trial is a path starting from s    following the greedy policy and updating the costs of
the states visited using bellman backups  the trial ends when a goal is reached or the number of
updates exceeds a threshold  rtdp repeats these trials until convergence  note that common states
are updated frequently  while rtdp wastes no time on states that are unreachable  given the current
policy  rtdps strength is its ability to quickly produce a relatively good policy  however  complete
convergence  at every relevant state  is slow because less likely  but potentially important  states get
updated infrequently  furthermore  rtdp is not guaranteed to terminate  labeled rtdp  lrtdp 
fixes these problems with a clever labeling scheme that focuses attention on states where the value
function has not yet converged  bonet   geffner         labeled rtdp is guaranteed to terminate 
and is guaranteed to converge to the  approximation of the optimal cost function  for states reachable using the optimal policy  if the initial cost function is admissible  all costs  c  positive and a
goal reachable from all reachable states with non zero probability 
mdps are a powerful framework to model stochastic planning domains  however  mdps make
two unrealistic assumptions     all actions need to be executed sequentially  and    all actions
are instantaneous  unfortunately  there are many real world domains where these assumptions are
unrealistic  for example  concurrent actions are essential for a mars rover  since instruments can
be turned on  warmed up and calibrated while the rover is moving  and using other instruments
for transmitting data  moreover  the action durations are non zero and stochastic  the rover might
lose its way while navigating and may take a long time to reach its destination  it may make multiple
attempts before finding the accurate arm placement  in this paper we successively relax these two
assumptions and build models and algorithms that can scale up in spite of the additional complexities
imposed by the more general models 

   concurrent markov decision processes
we define a new model  concurrent mdp  comdp   which allows multiple actions to be executed
in parallel  this model is different from semi mdps and generalized state semi mdps  younes
  simmons      b  in that it does not incorporate action durations explicitly  comdps focus on
adding concurrency in an mdp framework  the input to a comdp is slightly different from that of
an mdp  hs  a  apk   prk   ck   g  s  i  the new applicability function  probability model and cost
  

fip lanning with d urative actions in s tochastic d omains

 apk   prk and ck respectively  encode the distinction between allowing sequential executions of
single actions versus the simultaneous executions of sets of actions 
    the model
the set of states  s   set of actions  a   goals  g  and the start state  s    follow the input of an mdp 
the difference lies in the fact that instead of executing only one action at a time  we may execute
multiple of them  let us define an action combination  a  as a set of one or more actions to be
executed in parallel  with an action combination as a new unit operator available to the agent  the
comdp takes the following new inputs
 apk defines the new applicability function  apk   s  p p a    denotes the set of action
combinations that can be applied in a given state 
 prk   s  p a   s         is the transition function  we write prk  s   s  a  to denote the
probability of arriving at state s  after executing action combination a in state s 
 ck   s  p a   s     is the cost model  we write ck  s  a  s    to denote the cost incurred
when the state s  is reached after executing action combination a in state s 
in essence  a comdp takes an action combination as a unit operator instead of a single action 
our approach is to convert a comdp into an equivalent mdp  mk   that can be specified by the
tuple hs  p a   apk   prk   ck   g  s  i and solve it using the known mdp algorithms 
    case study  comdp over probabilistic strips
in general a comdp could require an exponentially larger input than does an mdp  since the transition model  cost model and the applicability function are all defined in terms of action combinations
as opposed to actions  a compact input representation for a general comdp is an interesting  open
research question for the future  in this work  we consider a special class of compact comdp
 one that is defined naturally via a domain description very similar to the probabilistic strips
representation for mdps  boutilier  dean    hanks        
given a domain encoded in probabilistic strips we can compute a safe set of co executable
actions  under this safe semantics  the probabilistic dynamics gets defined in a consistent way as
we describe below 
      a pplicability f unction
we first discuss how to compute the sets of actions that can be executed in parallel since some
actions may conflict with each other  we adopt the classical planning notion of mutual exclusion  blum   furst        and apply it to the factored action representation of probabilistic strips 
two distinct actions are mutex  may not be executed concurrently  if in any state one of the following occurs 
   they have inconsistent preconditions
   an outcome of one action conflicts with an outcome of the other
   the precondition of one action conflicts with the  possibly probabilistic  effect of the other 
  

fim ausam   w eld

   the effect of one action possibly modifies a feature upon which another actions transition
function is conditioned upon 
additionally  an action is never mutex with itself  in essence  the non mutex actions do not interact  the effects of executing the sequence a    a  equals those for a    a   and so the semantics
for parallel executions is clear 
example  continuing with figure    toggle x    toggle x  and toggle x  can execute in parallel but
toggle x  and toggle x  are mutex as they have conflicting preconditions  similarly  toggle x  and
toggle p   are mutex as the effect of toggle p   interferes with the precondition of toggle x    if
toggle x  s outcomes depended on toggle x  then they would be mutex too  due to point   above 
for example  toggle x  toggle x  will be mutex if the effect of toggle x  was as follows  if togglex  then the probability of x   x  is     else       
the applicability function is defined as the set of action combinations  a  such that each action
in a is independently applicable in s and all of the actions are pairwise non mutex with each other 
note that pairwise concurrency is sufficient to ensure problem free concurrency of all multiple
actions in a  formally apk can be defined in terms of our original definition ap as follows 
apk  s     a  a a  a   a  a  a   ap s   mutex a  a    

   

      t ransition f unction
let a    a    a            ak   be an action combination applicable in s  since none of the actions are
mutex  the transition function may be calculated by choosing any arbitrary order in which to apply
them as follows 
prk  s   s  a   

x

   

x

pr s   s  a   pr s   s    a          pr s   sk    ak  

   

s   s      sk s

while we define the applicability function and the transition function by allowing only a consistent set of actions to be executable concurrently  there are alternative definitions possible  for
instance  one might be willing to allow executing two actions together if the probability that they
conflict is very small  a conflict may be defined as two actions asserting contradictory effects or
one negating the precondition of the other  in such a case  a new state called failure could be created such that the system transitions to this state in case of a conflict  and the transition may be
computed to reflect a low probability transition to this failure state 
although we impose that the model be conflict free  most of our techniques dont actually depend on this assumption explicitly and extend to general comdps 
      c ost model
we make a small change to the probabilistic strips representation  instead of defining a single
cost  c  for each action  we define it additively as a sum of resource and time components as follows 
 let t be the durative cost  i e   cost due to time taken to complete the action 
 let r be the resource cost  i e   cost of resources used for the action 
  

fip lanning with d urative actions in s tochastic d omains

assuming additivity we can think of cost of an action c s  a  s      t s  a  s      r s  a  s     to be
sum of its time and resource usage  hence  the cost model for a combination of actions in terms of
these components may be defined as 
ck  s   a    a         ak    s     

k
x

r s  ai   s      max  t s  ai   s    
i    k

i  

   

for example  a mars rover might incur lower cost when it preheats an instrument while changing
locations than if it executes the actions sequentially  because the total time is reduced while the
energy consumed does not change 
    solving a comdp with mdp algorithms
we have taken a concurrent mdp that allowed concurrency in actions and formulated it as an equivalent mdp  mk   in an extended action space  for the rest of the paper we will use the term comdp
to also refer to the equivalent mdp mk  
      b ellman equations
we extend equations   to a set of equations representing the solution to a comdp 
jk  s       if s  g else
jk  s    min

aapk  s 

x

n

o

prk  s   s  a  ck  s  a  s      jk  s   

   

s  s

these equations are the same as in a traditional mdp  except that instead of considering single
actions for backup in a state  we need to consider all applicable action combinations  thus  only this
small change must be made to traditional algorithms  e g   value iteration  lao   labeled rtdp  
however  since the number of action combinations is worst case exponential in  a   efficiently
solving a comdp requires new techniques  unfortunately  there is no structure to exploit easily 
since an optimal action for a state from a classical mdp solution may not even appear in the optimal
action combination for the associated concurrent mdp 
theorem   all actions in an optimal combination for a comdp  mk   may be individually suboptimal for the mdp m 
proof  in the domain of figure   let us have an additional action toggle x   that toggles both x 
and x  with probability     and toggles exactly one of x  and x  with probability      each  let
all the actions take one time unit each  and therefore cost of any action combination is one as well 
let the start state be x       x       x       x      and p        for the mdp m the only optimal
action for the start state is toggle x     however  for the comdp mk the optimal combination is
 toggle x    toggle x      
    pruned bellman backups
recall that during a trial  labeled rtdp performs bellman backups in order to calculate the costs of
applicable actions  or in our case  action combinations  and then chooses the best action  combination   we now describe two pruning techniques that reduce the number of backups to be computed 
  

fim ausam   w eld

let qk  s  a  be the expected cost incurred by executing an action combination a in state s and then
following the greedy policy  i e 
qkn  s  a   

x

n

o

prk  s   s  a  ck  s  a  s      jkn   s   

   

s  s

a bellman update can thus be rewritten as 
jkn  s   

min

aapk  s 

qkn  s  a 

   

      c ombo  s kipping
since the number of applicable action combinations can be exponential  we would like to prune
suboptimal combinations  the following theorem imposes a lower bound on qk  s  a  in terms of
the costs and the qk  values of single actions  for this theorem the costs of the actions may depend
only on the action and not the starting or ending state  i e   for all states s  s  c s  a  s      c a  
theorem   let a    a    a            ak   be an action combination which is applicable in state s  for
a comdp over probabilistic strips  if costs are dependent only on actions and qkn values are
monotonically non decreasing then
qk  s  a   max qk  s   ai      ck  a  
i    k

k
x

 

ck   ai   

i  

proof 
qkn  s  a    ck  a   

x

prk  s   s  a jkn   s   

 using eqn    

s 



x

prk  s   s  a jkn   s      qkn  s  a   ck  a 

    

s 

qkn  s   a       ck   a      

x

pr s    s  a   jkn   s    

s  

 

 ck   a      

x

 

  

pr s  s  a    ck   a      

s  

x

   

  

   

pr s  s   a   jkn   s  

s   

 using eqns    and   
  ck   a       ck   a      

x

   

   

prk  s  s   a    a    jkn   s  

s   


 

k
x
i  
k
x

ck   ai     

x

prk  s   s  a jknk  s   

 repeating for all actions in a 

s 

ck   ai       qknk    s  a   ck  a  

i  

replacing n by n   k   
  

 using eqn     

fip lanning with d urative actions in s tochastic d omains

qkn  s  a   qkn k   s   a       ck  a  

k
x

 

ck   ai   

i  

 qkn  s   a       ck  a  

k
x

 

ck   ai   

 monotonicity of qkn  

i  



max qkn  s   ai      ck  a  

i    k

k
x

 

ck   ai   

i  

 

the proof above assumes equation   from probabilistic strips  the following corollary can
be used to prune suboptimal action combinations 
corollary   let djkn  s e be an upper bound of jkn  s   if
djkn  s e   max qkn  s   ai      ck  a  
i    k

k
x

 

ck   ai   

i  

then a cannot be optimal for state s in this iteration 
proof  let an    a    a            ak   be the optimal combination for state s in this iteration n  then 
djkn  s e  jkn  s 
jkn  s    qkn  s  an  
combining with theorem  
djkn  s e  maxi    k qkn  s   ai     

ck  an  



k
x

 

ck   ai     

i  

corollary   justifies a pruning rule  combo skipping  that preserves optimality in any iteration
algorithm that maintains cost function monotonicity  this is powerful because all bellman backup
based algorithms preserve monotonicity when started with an admissible cost function  to apply
combo skipping  one must compute all the qk  s   a   values for single actions a that are applicable
in s  to calculate djkn  s e one may use the optimal combination for state s in the previous iteration
 aopt   and compute qkn  s  aopt    this value gives an upper bound on the value jkn  s  
example  consider figure    let a single action incur unit cost  and let the cost of an action combination be  ck  a              a   let state s               represent the ordered values x       x   
   x       x       and p        suppose  after the nth iteration  the cost function assigns the values 
jkn  s       jkn  s                     jkn  s                     jkn  s                     let aopt for
state s be  toggle x    toggle x     now  qkn    s   toggle x       ck   toggle x       jkn  s       
and qkn    s  aopt     ck  aopt                 jkn  s          jkn  s          jkn  s         
so now we can apply corollary   to skip combination  toggle x    toggle x    in this iteration  since
using toggle x  as a    we have djkn    s e   qkn    s  aopt                               
experiments show that combo skipping yields considerable savings  unfortunately  comboskipping has a weakness  it prunes a combination for only a single iteration  in contrast  our
second rule  combo elimination  prunes irrelevant combinations altogether 
  

fim ausam   w eld

      c ombo  e limination
we adapt the action elimination theorem from traditional mdps  bertsekas        to prove a similar
theorem for comdps 
theorem   let a be an action combination which is applicable in state s  let bqk  s  a c denote
a lower bound of qk  s  a   if bqk  s  a c   djk  s e then a is never the optimal combination for
state s 
proof  because a comdp is an mdp in a new action space  the original proof for mdps  bertsekas 
      holds after replacing an action by an action combination   
in order to apply the theorem for pruning  one must be able to evaluate the upper and lower
bounds  by using an admissible cost function when starting rtdp search  or in value iteration 
lao  etc    the current cost jkn  s  is guaranteed to be a lower bound of the optimal cost  thus 
qkn  s  a  will also be a lower bound of qk  s  a   thus  it is easy to compute the left hand side
of the inequality  to calculate an upper bound of the optimal jk  s   one may solve the mdp m 
i e   the traditional mdp that forbids concurrency  this is much faster than solving the comdp 
and yields an upper bound on cost  because forbidding concurrency restricts the policy to use a
strict subset of legal action combinations  notice that combo elimination can be used for all general
mdps and is not restricted to only comdps over probabilistic strips 
example  continuing with the previous example  let a  toggle x    then qkn    s  a    ck  a   
jkn  s        and djk  s e          from solving mdp m   as            a can be eliminated for
state s in all remaining iterations   
used in this fashion  combo elimination requires the additional overhead of optimally solving
the single action mdp m  since algorithms like rtdp exploit state space reachability to limit
computation to relevant states  we do this computation incrementally  as new states are visited by
our algorithm 
combo elimination also requires computation of the current value of qk  s  a   for the lower
bound of qk  s  a    this differs from combo skipping which avoids this computation  however 
once combo elimination prunes a combination  it never needs to be reconsidered  thus  there is
a tradeoff  should one perform an expensive computation  hoping for long term pruning  or try a
cheaper pruning rule with fewer benefits  since q value computation is the costly step  we adopt
the following heuristic  first  try combo skipping  if it fails to prune the combination  attempt
combo elimination  if it succeeds  never consider it again  we also tried implementing some other
heuristics  such as     if some combination is being skipped repeatedly  then try to prune it altogether with combo elimination     in every state  try combo elimination with probability p  neither
alternative performed significantly better  so we kept our original  lower overhead  heuristic 
since combo skipping does not change any step of labeled rtdp and combo elimination removes provably sub optimal combinations  pruned labeled rtdp maintains convergence  termination  optimality and efficiency  when used with an admissible heuristic 
    sampled bellman backups
since the fundamental challenge posed by comdps is the explosion of action combinations  sampling is a promising method to reduce the number of bellman backups required per state  we
describe a variant of rtdp  called sampled rtdp  which performs backups on a random set of
  

fip lanning with d urative actions in s tochastic d omains

action combinations    choosing from a distribution that favors combinations that are likely to be
optimal  we generate our distribution by 
   using combinations that were previously discovered to have low qk  values  recorded by memoizing the best combinations per state  after each iteration 
   calculating the qk  values of all applicable single actions  using current cost function  and
then biasing the sampling of combinations to choose the ones that contain actions with low
qk  values 

algorithm   sampled bellman backup state  m 
  
  
  
  
  
  
  
  
  
   
   

function   samplecomb state  i  l 
  
  
  
  
  
  
  
  
  
   
   
   
   
   
   

  returns the best combination found

list l      a list of all applicable actions with their values
for all action  a do
compute qk  state   action  
insert ha    qk  state   action  i in l
for all i      m  do
newcomb   samplecomb state  i  l  
compute qk  state  newcomb 
clear memoizedlist state 
compute qmin as the minimum of all qk values computed in line  
store all combinations a with qk  state  a    qmin in memoizedlist state 
return the first entry in memoizedlist state 

  returns ith combination for the sampled backup

if i  size memoizedlist state   then
return ith entry in memoizedlist state    return the combination memoized in previous iteration
newcomb   
repeat
randomly sample an action a from l proportional to its value
insert a in newcomb
remove all actions mutex with a from l
if l is empty then
done   true
else if  newcomb       then
done   false   sample at least   actions per combination
else
 newcomb 
done   true with prob   newcomb   
until done
return newcomb

this approach exposes an exploration   exploitation trade off  exploration  here  refers to testing a wide range of action combinations to improve understanding of their relative merit  exploitation  on the other hand  advocates performing backups on the combinations that have previously
been shown to be the best  we manage the tradeoff by carefully maintaining the distribution over
combinations  first  we only memoize best combinations per state  these are always backed up
   a similar action sampling approach was also used in the context of space shuttle scheduling to reduce the number of
actions considered during value function computation  zhang   dietterich        

  

fim ausam   w eld

in a bellman update  other combinations are constructed by an incremental probabilistic process 
which builds a combination by first randomly choosing an initial action  weighted by its individual qk  value   then deciding whether to add a non mutex action or stop growing the combination 
there are many implementations possible for this high level idea  we tried several of those and
found the results to be very similar in all of them  algorithm   describes the implementation used
in our experiments  the algorithm takes a state and a total number of combinations m as an input
and returns the best combination obtained so far  it also memoizes all the best combinations for the
state in memoizedlist  function   is a helper function that returns the ith combination that is either
one of the best combinations memoized in the previous iteration or a new sampled combination 
also notice line    in function    it forces the sampled combinations to be at least size    since all
individual actions have already been backed up  line   of algo    
      t ermination and o ptimality
since the system does not consider every possible action combination  sampled rtdp is not guaranteed to choose the best combination to execute at each state  as a result  even when started with
an admissible heuristic  the algorithm may assign jkn  s  a cost that is greater than the optimal jk  s 
 i e   the jkn  s  values are no longer admissible  if a better combination is chosen in a subsequent
iteration  jkn    s  might be set a lower value than jkn  s   thus sampled rtdp is not monotonic 
this is unfortunate  since admissibility and monotonicity are important properties required for termination  and optimality in labeled rtdp  indeed  sampled rtdp loses these important theoretical
properties  the good news is that it is extremely useful in practice  in our experiments  sampled
rtdp usually terminates quickly  and returns costs that are extremely close to the optimal 
      i mproving s olution q uality
we have investigated several heuristics in order to improve the quality of the solutions found by
sampled rtdp  our heuristics compensate for the errors due to partial search and lack of admissibility 
 heuristic    whenever sampled rtdp asserts convergence of a state  do not immediately
label it as converged  which would preclude further exploration  bonet   geffner         
instead first run a complete backup phase  using all the admissible combinations  to rule out
any easy to detect inconsistencies 
 heuristic    run sampled rtdp to completion  and use the cost function it produces  j s    
as the initial heuristic estimate  j      for a subsequent run of pruned rtdp  usually  such a
heuristic  though inadmissible  is highly informative  hence  pruned rtdp terminates quite
quickly 
 heuristic    run sampled rtdp before pruned rtdp  as in heuristic    except instead of
using the j s    cost function directly as an initial estimate  scale linearly downward  i e  
use j        cj s    for some constant c          while there are no guarantees we hope that
this lies on the admissible side of the optimal  in our experience this is often the case for
c        and the run of pruned rtdp yields the optimal policy very quickly 
   to ensure termination we implemented the policy  if number of trials exceeds a threshold  force monotonicity on the
cost function  this will achieve termination but will reduce quality of solution 

  

fip lanning with d urative actions in s tochastic d omains

experiments showed that heuristic   returns a cost function that is close to optimal  adding
heuristic   improves this value moderately  and a combination of heuristics   and   returns the
optimal solution in our experiments 
    experiments  concurrent mdp
concurrent mdp is a fundamental formulation  modeling concurrent actions in a general planning
domain  we first compare the various techniques to solve comdps  viz   pruned and sampled rtdp 
in following sections we use these techniques to model problems with durative actions 
we tested our algorithms on problems in three domains  the first domain was a probabilistic
variant of nasa rover domain from the      aips planning competition  long   fox         in
which there are multiple objects to be photographed and various rocks to be tested with resulting
data communicated back to the base station  cameras need to be focused  and arms need to be
positioned before usage  since the rover has multiple arms and multiple cameras  the domain is
highly parallel  the cost function includes both resource and time components  so executing multiple actions in parallel is cheaper than executing them sequentially  we generated problems with
      state variables having up to        reachable states and the average number of applicable
combinations per state  avg ap s    which measures the amount of concurrency in a problem  is
up to      
we also tested on a probabilistic version of a machineshop domain with multiple subtasks  e g  
roll  shape  paint  polish etc    which need to be performed on different objects using different
machines  machines can perform in parallel  but not all are capable of every task  we tested
on problems with       state variables and around        reachable states  avg ap s   ranged
between     and      on the various problems 
finally  we tested on an artificial domain similar to the one shown in figure   but much more
complex  in this domain  some boolean variables need to be toggled  however  toggling is probabilistic in nature  moreover  certain pairs of actions have conflicting preconditions and thus  by
varying the number of mutex actions we may control the domains degree of parallelism  all the
problems in this domain had    state variables and about        reachable states  with avg ap s  
between      and       
we used labeled rtdp  as implemented in gpt  bonet   geffner         as the base mdp
solver  it is implemented in c    we implemented  various algorithms  unpruned rtdp  u rtdp   pruned rtdp using only combo skipping  ps  rtdp   pruned rtdp using both combo
skipping and combo elimination  pse  rtdp   sampled rtdp using heuristic    s rtdp  and sampled rtdp using both heuristics   and    with value functions scaled with      s   rtdp   we tested
all of these algorithms on a number of problem instantiations from our three domains  generated by
varying the number of objects  degrees of parallelism  and distances to goal  the experiments were
performed on a     ghz pentium processor with a   gb ram 
we observe  figure   a b   that pruning significantly speeds the algorithm  but the comparison of pse  rtdp with s rtdp and s   rtdp  figure   a b   shows that sampling has a dramatic
speedup with respect to the pruned versions  in fact  pure sampling  s rtdp  converges extremely
quickly  and s   rtdp is slightly slower  however  s   rtdp is still much faster than pse  rtdp 
the comparison of qualities of solutions produced by s rtdp and s   rtdp w r t  optimal is shown
in table    we observe that solutions produced by s rtdp are always nearly optimal  since the
   the code may be downloaded at http   www cs washington edu ai comdp comdp tgz

  

fim ausam   w eld

comparison of pruned and unpruned rtdp for rover domain

comparison of pruned and unpruned rtdp for factory domain
     

y x
ps rtdp
pse rtdp

     

times for pruned rtdp  in sec 

times for pruned rtdp  in sec 

     

     
     
     
    

y x
ps rtdp
pse rtdp

     
    
    
    
    

 

 
 

    
                       
times for unpruned rtdp  in sec 

     

 

    
    
    
    
     
times for unpruned rtdp  in sec 

     

figure     a b   pruned vs  unpruned rtdp for rover and machineshop domains respectively  pruning
non optimal combinations achieves significant speedups on larger problems 

comparison of pruned and sampled rtdp for rover domain

comparison of pruned and sampled rtdp for factory domain
    

y x
s rtdp
s  rtdp

    

times for sampled rtdp  in sec 

times for sampled rtdp  in sec 

     

    
    
    
 

y x
s rtdp
s  rtdp

    
    
    
    
    
    
    
 

 

                                             
times for pruned rtdp  pse rtdp     in sec 

 

                                       
times for pruned rtdp  pse rtdp     in sec 

figure     a b   sampled vs pruned rtdp for rover and machineshop domains respectively  random
sampling of action combinations yields dramatic improvements in running times 

  

fip lanning with d urative actions in s tochastic d omains

comparison of algorithms with size of problem for rover domain
     

s rtdp
s  rtdp
pse rtdp
u rtdp

     

s rtdp
s  rtdp
pse rtdp
u rtdp

     

     

times  in sec 

times  in sec 

comparison of different algorithms for artificial domain
     

     
     

     

    
    
 

 
 

 e   

 e   
   e   
reach  s   avg ap s  

 e   

   e   

 

    

    

         
avg ap s  

                 

figure     a b   comparison of different algorithms with size of the problems for rover and artificial domains  as the problem size increases  the gap between sampled and pruned approaches widens
considerably 

results on varying the number of samples for rover problem  

   
   
   
   
   
  
 

running times
values of start state

   

   

   

                   
concurrency   avg ap s    a 

   

   

    

   

     

   

     

   

     

   

     

  

j  s  

 
 

     

value of the start state

   

s rtdp pse rtdp

times for sampled rtdp  in sec 

speedup   sampled rtdp pruned rtdp

speedup vs  concurrency for artificial domain
   

  

  

  

              
number of samples

     
      

figure     a   relative speed vs  concurrency for artificial domain   b    variation of quality of solution
and efficiency of algorithm  with     confidence intervals  with the number of samples in sampled rtdp for one particular problem from the rover domain  as number of samples increase 
the quality of solution approaches optimal and time still remains better than pse  rtdp  which
takes     sec  for this problem  

  

fim ausam   w eld

problem
rover 
rover 
rover 
rover 
rover 
rover 
rover 
artificial 
artificial 
artificial 
machineshop 
machineshop 
machineshop 
machineshop 
machineshop 

j s     s rtdp 
       
       
       
       
      
       
       
      
      
      
       
       
       
       
      

j   s     optimal 
       
       
       
       
      
       
       
      
      
      
       
       
       
       
      

error
      
 
 
     
 
 
     
 
 
 
     
     
     
 
     

table    quality of solutions produced by sampled rtdp
error of s rtdp is small  scaling it by     makes it an admissible initial cost function for the pruned
rtdp  indeed  in all experiments  s   rtdp produced the optimal solution 
figure   a b  demonstrates how running times vary with problem size  we use the product of
the number of reachable states and the average number of applicable action combinations per state
as an estimate of the size of the problem  the number of reachable states in all artificial domains is
the same  hence the x axis for figure   b  is avg ap s     from these figures  we verify that the
number of applicable combinations plays a major role in the running times of the concurrent mdp
algorithms  in figure   a   we fix all factors and vary the degree of parallelism  we observe that the
speedups obtained by s rtdp increase as concurrency increases  this is a very encouraging result 
and we can expect s rtdp to perform well on large problems inolving high concurrency  even if
the other approaches fail 
in figure   b   we present another experiment in which we vary the number of action combinations sampled in each backup  while solution quality is inferior when sampling only a few
combinations  it quickly approaches the optimal on increasing the number of samples  in all other
experiments we sample    combinations per state 

   challenges for temporal planning
while the comdp model is powerful enough to model concurrency in actions  it still assumes each
action to be instantaneous  we now incorporate actual action durations in the modeling the problem 
this is essential to increase the scope of current models to real world domains 
before we present our model and the algorithms we discuss several new theoretical challenges
imposed by explicit action durations  note that the results in this section apply to a wide range of
planning problems 
 regardless of whether durations are uncertain or fixed
 regardless of whether effects are stochastic or deterministic 
  

fip lanning with d urative actions in s tochastic d omains

actions of uncertain duration are modeled by associating a distribution  possibly conditioned
on the outcome of stochastic effects  over execution times  we focus on problems whose objective
is to achieve a goal state while minimizing total expected time  make span   but our results extend
to cost functions that combine make span and resource usage  this raises the question of when a
goal counts as achieved  we require that 
assumption   all executing actions terminate before the goal is considered achieved 
assumption   an action  once started  cannot be terminated prematurely 
we start by asking the question is there a restricted set of time points such that optimality is
preserved even if actions are started only at these points 
definition   any time point when a new action is allowed to start execution is called a decision
epoch  a time point is a pivot if it is either   or a time when a new effect might occur  e g   the
end of an actions execution  or a new precondition may be needed or an existing precondition may
no longer be needed  a happening is either   or a time when an effect actually occurs or a new
precondition is definitely needed or an existing precondition is no longer needed 
intuitively  a happening is a point where a change in the world state or action constraints actually
happens  e g   by a new effect or a new precondition   when execution crosses a pivot  a possible
happening   information is gained by the agents execution system  e g   did or didnt the effect
occur  which may change the direction of future action choices  clearly  if action durations are
deterministic  then the set of pivots is the same as the set of happenings 
example  consider an action a whose durations follow a uniform integer duration between   and
    if it is started at time   then all timepoints                   are pivots  if in a certain execution it
finishes at time   then    and    is a happening  for this execution    
definition   an action is a pddl    action  fox   long        if the following hold 
 the effects are realized instantaneously either  at start  or  at end   i e   at the beginning or
the at the completion of the action  respectively  
 the preconditions may need to hold instaneously before the start  at start   before the end  at
end  or over the complete execution of the action  over all  

  durative action a
 duration     duration   
 condition  and  over all p    at end q  
 effect  at end goal  
  durative action b
 duration     duration   
 effect  and  at start q   at end  not p     

figure    a domain to illustrate that an expressive action model may require arbitrary decision epochs for a
solution  in this example  b needs to start at   units after as execution to reach goal 

  

fim ausam   w eld

theorem   for a pddl    domain restricting decision epochs to pivots causes incompleteness
 i e   a problem may be incorrectly deemed unsolvable  
proof  consider the deterministic temporal planning domain in figure   that uses pddl    notation
 fox   long         if the initial state is p  true and q false  then the only way to reach goal is
to start a at time t  e g       and b at some timepoint in the open interval  t      t       clearly  no
new information is gained at any of the time points in this interval and none of them is a pivot  still 
they are required for solving the problem   
intuitively  the instantaneous start and end effects of two pddl    actions may require a certain
relative alignment within them to achieve the goal  this alignment may force one action to start
somewhere  possibly at a non pivot point  in the midst of the others execution  thus requiring
intermediate decision epochs to be considered 
temporal planners may be classified as having one of two architectures  constraint posting
approaches in which the times of action execution are gradually constrained during planning  e g  
zeno and lpg  see penberthy and weld        gerevini and serina        and extended statespace methods  e g   tp  and sapa  see haslum and geffner        do and kambhampati        
theorem   holds for both architectures but has strong computational implications for state space
planners because limiting attention to a subset of decision epochs can speed these planners  the
theorem also shows that planners like sapa and prottle  little  aberdeen    thiebaux        are
incomplete  fortunately  an assumption restricts the set of decision epochs considerably 
definition   an action is a tgp style action  if all of the following hold 
 the effects are realized at some unknown point during action execution  and thus can be used
only once the action has completed 
 the preconditions must hold at the beginning of an action 
 the preconditions  and the features on which its transition function is conditioned  must not
be changed during an actions execution  except by an effect of the action itself 
thus  two tgp style actions may not execute concurrently if they clobber each others preconditions or effects  for the case of tgp style actions the set of happenings is nothing but the set of
time points when some action terminates  tgp pivots are the set of points when an action might
terminate   of course both these sets additionally include zero  
theorem   if all actions are tgp style  then the set of decision epochs may be restricted to pivots
without sacrificing completeness or optimality 
proof sketch  by contradiction  suppose that no optimal policy satisfies the theorem  then there
must exist a path through the optimal policy in which one must start an action  a  at time t even
though there is no action which could have terminated at t  since the planner hasnt gained any
information at t  a case analysis  which requires actions to be tgp style  shows that one could
have started a earlier in the execution path without increasing the make span  the detailed proof is
discussed in the appendix   
in the case of deterministic durations  the set of happenings is same as the set of pivots  hence
the following corollary holds 
   while the original tgp  smith   weld        considered only deterministic actions of fixed duration  we use the
phrase tgp style in a more general way  without these restrictions 

  

fip lanning with d urative actions in s tochastic d omains

probabillity     
a 

s 

a 

g

a 
makespan   
probability    
a 

s 
a 

g

b 
makespan   

 

 

 

 

 

time

figure    pivot decision epochs are necessary for optimal planning in face of nonmonotonic continuation  in this domain  goal can be achieved by h a    a     a  i or hb  i  a  has duration  
or    and b  is mutex with a    the optimal policy starts a  and then  if a  does not finish
at time    it starts b   otherwise it starts a    

corollary   if all actions are tgp style with deterministic durations  then the set of decision
epochs may be restricted to happenings without sacrificing completeness or optimality 
when planning with uncertain durations there may be a huge number of pivots  it is useful to
further constrain the range of decision epochs 
definition   an action has independent duration if there is no correlation between its probabilistic
effects and its duration 
definition   an action has monotonic continuation if the expected time until action termination is
nonincreasing during execution 
actions without probabilistic effects  by nature  have independent duration  actions with monotonic continuations are common  e g  those with uniform  exponential  gaussian  and many other duration distributions  however  actions with bimodal or multi modal distributions dont have monotonic continuations  for example consider an action with uniform distribution over        if the
action doesnt terminate until    then the expected time until completion is calculated as        
and   for times       and   respectively  which is monotonically decreasing  for an example of
non monotonic continuation see figure    
conjecture   if all actions are tgp style  have independent duration and monotonic continuation 
then the set of decision epochs may be restricted to happenings without sacrificing completeness or
optimality 
if an actions continuation is nonmonotonic then failure to terminate can increase the expected
time remaining and cause another sub plan to be preferred  see figure     similarly  if an actions
duration isnt independent then failure to terminate changes the probability of its eventual effects
and this may prompt new actions to be started 
by exploiting these theorems and conjecture we may significantly speed planning since we are
able to limit the number of decision epochs needed for decision making  we use this theoretical
understanding in our models  first  for simplicity  we consider only the case of tgp style actions
with deterministic durations  in section    we relax this restriction by allowing stochastic durations 
both unimodal as well as multimodal 
  

fim ausam   w eld

togglep  
p    effect 
conflict
p    precondition 
togglex 
 

 

 

 

 

  

figure    a sample execution demonstrating conflict due to interfering preconditions and effects   the
actions are shaded to disambiguate them with preconditions and effects 

   temporal planning with deterministic durations
we use the abbreviation cptp  short for concurrent probabilistic temporal planning  to refer to
the probabilistic planning problem with durative actions  a cptp problem has an input model
similar to that of comdps except that action costs  c s  a  s     are replaced by their deterministic
durations   a   i e   the input is of the form hs  a  pr    g  s  i  we study the objective of minimizing the expected time  make span  of reaching a goal  for the rest of the paper we make the
following assumptions 
assumption   all action durations are integer valued 
this assumption has a negligible effect on expressiveness because one can convert a problem
with rational durations into one that satisfies assumption   by scaling all durations by the g c d  of
the denominators  in case of irrational durations  one can always find an arbitrarily close approximation to the original problem by approximating the irrational durations by rational numbers 
for reasons discussed in the previous section we adopt the tgp temporal action model of smith
and weld         rather than the more complex pddl     fox   long         specifically 
assumption   all actions follow the tgp model 
these restrictions are consistent with our previous definition of concurrency  specifically  the
mutex definitions  of comdps over probabilistic strips  hold and are required under these assumptions  as an illustration  consider figure    it describes a situation in which two actions with
interfering preconditions and effects can not be executed concurrently  to see why not  suppose
initially p   was false and two actions toggle x  and toggle p   were started at time   and    respectively  as p   is a precondition of toggle x    whose duration is    it needs to remain false
until time    but toggle p   may produce its effects anytime between   and    which may conflict
with the preconditions of the other executing action  hence  we forbid the concurrent execution of
toggle x  and toggle p   to ensure a completely predictable outcome distribution 
because of this definition of concurrency  the dynamics of our model remains consistent with
equation    thus the techniques developed for comdps derived from probabilistic strips actions
may be used 
  

fip lanning with d urative actions in s tochastic d omains

an aligned epoch policy execution
 takes   units 
togglex 
t 

f

f

f

f

t  t  t  t 

 

 

s

  
time

togglex 
f

f

f

f

t  t  t  t  t 

s

an interwoven epoch policy execution
 takes   units 

figure    comparison of times taken in a sample execution of an interwoven epoch policy and an alignedepoch policy  in both trajectories the toggle x   t   action fails four times before succeeding 
because the aligned policy must wait for all actions to complete before starting any more  it takes
more time than the interwoven policy  which can start more actions in the middle 

    formulation as a comdp
we can model a cptp problem as a comdp  and thus as an mdp  in more than one way  we list
the two prominent formulations below  our first formulation  aligned epoch comdp models the
problem approximately but solves it quickly  the second formulation  interleaved epochs models
the problem exactly but results in a larger state space and hence takes longer to solve using existing
techniques  in subsequent subsections we explore ways to speed up policy construction for the
interleaved epoch formulation 
      a ligned e poch s earch s pace
a simple way to formulate cptp is to model it as a standard comdp over probabilistic strips 
in which action costs are set to their durations and the cost of a combination is the maximum
duration of the constituent actions  as in equation     this formulation introduces a substantial
approximation to the cptp problem  while this is true for deterministic domains too  we illustrate
this using our example involving stochastic effects  figure   compares the trajectories in which the
toggle x   t   actions fails for four consecutive times before succeeding  in the figure  f and s
denote failure and success of uncertain actions  respectively  the vertical dashed lines represent the
time points when an action is started 
consider the actual executions of the resulting policies  in the aligned epoch case  figure  
top   once a combination of actions is started at a state  the next decision can be taken only when
the effects of all actions have been observed  hence the name aligned epochs   in contrast  figure  
bottom shows that at a decision epoch in the optimal execution for a cptp problem  many actions
may be midway in their execution  we have to explicitly take into account these actions and their
remaining execution times when making a subsequent decision  thus  the actual state space for
cptp decision making is substantially different from that of the simple aligned epoch model 
note that due to corollary   it is sufficient to consider a new decision epoch only at a happening 
i e   a time point when one or more actions complete  thus  using assumption   we infer that these
decision epochs will be discrete  integer   of course  not all optimal policies will have this property 
  

fim ausam   w eld

state variables   x    x    x    x    p  
action
 a  precondition
toggle x 
 
p  
toggle x 
 
p  
toggle x 
 
true

effect
x   x 
x   x 
x   x 
no change
toggle x 
 
true
x   x 
no change
toggle p  
 
true
p    p  
goal   x       x       x       x     

probability
 
 
   
   
   
   
 

figure     the domain of example   extended with action durations 
but it is easy to see that there exists at least one optimal policy in which each action begins at a
happening  hence our search space reduces considerably 
      i nterwoven e poch s earch s pace
we adapt the search space representation of haslum and geffner         which is similar to that
in other research  bacchus   ady        do   kambhampati         our original state space s
in section   is augmented by including the set of actions currently executing and the times passed
since they were started  formally  let the new interwoven state  s  s   be an ordered pair hx  y i
where 
 xs
 y     a    a  a         a  
here x represents the values of the state variables  i e  x is a state in the original state space 
and y denotes the set of ongoing actions a and the times
 passed since their start   thus the
n
overall interwoven epoch search space is s     s  aa  a   z a    where z a  represents
n
the set                 a      and
denotes the cartesian product over multiple sets 
also define as to be the set of actions already in execution  in other words  as is a projection
of y ignoring execution times in progress 
as    a  a     y  s   hx  y i 
example  continuing our example with the domain of figure     suppose state s  has all state
variables false  and suppose the action toggle x  was started   units ago from the current time  such
a state would be represented as hx    y  i with x    f  f  f  f  f   and y     toggle x        the five
state variables are listed in the order  x    x    x    x  and p      the set as  would be  toggle x    
to allow the possibility of simply waiting for some action to complete execution  that is  deciding at a decision epoch not to start any additional action  we augment the set a with a no op action 
which is applicable in all states s   hx  y i where y      i e  states in which some action is still
being executed   for a state s  the no op action is mutex with all non executing actions  i e   those in
a   as   in other words  at any decision epoch either a no op will be started or any combination not
   we use the subscript   to denote the interwoven state space  s      value function  j      etc  

  

fip lanning with d urative actions in s tochastic d omains

involving no op  we define no op to have a variable duration  equal to the time after which another
already executing action completes  next  s  a  as defined below  
the interwoven applicability set can be defined as 
 

ap    s   

apk  x  if y    else
 noop  a aas  apk  x  and aas    

transition function  we also need to define the probability transition function  pr     for the
interwoven state space  at some decision epoch let the agent be in state s    x  y    suppose
that the agent decides to execute an action combination a  define ynew as the set similar to y
but consisting of the actions just starting  formally ynew     a   a   a  a   in this system  the
next decision epoch will be the next time that an executing action terminates  let us call this time
next  s  a   notice that next  s  a  depends on both executing and newly started actions  formally 
next  s  a   

min

 a  y ynew

 a   

moreover  multiple actions may complete simultaneously  define anext  s  a   a  as to be
the set of actions that will complete exactly in next  s  a  timesteps  the y  component of the state
at the decision epoch after next  s  a  time will be
ynext  s  a      a     next  s  a    a     y  ynew    a      next  s  a  
let s hx  y i and let s   hx     y   i  the transition function for cptp can now be defined as 
prk  x    x  anext  s  a   if y    ynext  s  a 
 
otherwise

 
 

pr    s  s  a  

in other words  executing an action combination a in state s   hx  y i takes the agent to a
decision epoch next  s  a  ahead in time  specifically to the first time when some combination
anext  s  a  completes  this lets us calculate ynext  s  a   the new set of actions still executing
with their times elapsed  also  because of tgp style actions  the probability distribution of different
state variables is modified independently  thus the probability transition function due to comdp
over probabilistic strips can be used to decide the new distribution of state variables  as if the
combination anext  s  a  were taken in state x 
example  continuing with the previous example  let the agent in state s  execute the action combination a    toggle x     then next  s    a       since toggle x  will finish the first  thus 
anext  s    a    toggle x     ynext  s    a      toggle x        hence  the probability distribution of
states after executing the combination a in state s  will be
   f  f  f  t  f    ynext  s    a   probability      
   f  f  f  f  f    ynext  s    a   probability      
   a precise definition of the model will create multiple no opt actions with different constant durations t and the no opt
applicable in an interwoven state will be the one with t   next  s  a  

  

fim ausam   w eld

start and goal states  in the interwoven space  the start state is hs    i and the new set of goal
states is g      hx  i x  g  
by redefining the start and goal states  the applicability function  and the probability transition
function  we have finished modeling a cptp problem as a comdp in the interwoven state space 
now we can use the techniques of comdps  and mdps as well  to solve our problem  in particular 
we can use our bellman equations as described below 
bellman equations  the set of equations for the solution of a cptp problem can be written as 
j    s       if s  g   else



    








next  s  a    pr    s   s  a j    s   
j    s    min



aap    s  


s  s 



x

we will use dursamp to refer to the sampled rtdp algorithm over this search space  the main
bottleneck in naively inheriting algorithms like dursamp is the huge size of the interwoven state
space  in the worst case  when all actions can be executed concurrently  the size of the state space is
q
 s     aa  a    we get this bound by observing that for each action a  there are  a  number
of possibilities  either a is not executing or it is and has remaining times                a     
thus we need to reduce or abstract aggregate our state space in order to make the problem
tractable  we now present several heuristics which can be used to speed the search 
    heuristics
we present both an admissible and an inadmissible heuristics that can be used as the initial cost
function for dursamp algorithm  the first heuristic  maximum concurrency  solves the underlying mdp and is thus quite efficient to compute  the second heuristic  average concurrency  is
inadmissible  but tends to be more informed than the maximum concurrency heuristic 
      m aximum c oncurrency h euristic
we prove that the optimal expected cost in a traditional  serial  mdp divided by the maximum
number of actions that can be executed in parallel is a lower bound for the expected make span of
reaching a goal in a cptp problem  let j x  denote the value of a state x  s in a traditional
mdp with costs of an action equal to its duration  let q x  a  denote the expected cost to reach the
goal if initially all actions in the combination a are executed and the greedy serial policy is followed
p
thereafter  formally  q x  a    x   s prk  x    x  a j x      let j    s  be the value for equivalent
cptp problem with s as in our interwoven epoch state space  let concurrency of a state be the
maximum number of actions that could be executed in the state concurrently  we define maximum
concurrency of a domain  c  as the maximum number of actions that can be concurrently executed
in any world state in the domain  the following theorem can be used to provide an admissible
heuristic for cptp problems 
theorem   let s   hx  y i 
j    s  

j    s  


j   x 
for y   
c
q  x  as  
for y    
c
  

    

fip lanning with d urative actions in s tochastic d omains

proof sketch  consider any trajectory of make span l  from a state s   hx  i to a goal state  in a
cptp problem using its optimal policy  we can make all concurrent actions sequential by executing
them in the chronological order of being started  as all concurrent actions are non interacting  the
outcomes at each stage will have similar probabilities  the maximum make span of this sequential
trajectory will be cl  assuming c actions executing at all points in the semi mdp trajectory   hence
j x  using this  possibly non stationary  policy would be at most cj    s   thus j   x   cj    s  


the second inequality can be proven in a similar way   
there are cases where these bounds are tight  for example  consider a deterministic planning
problem in which the optimal plan is concurrently executing c actions each of unit duration  makespan       in the sequential version  the same actions would be taken sequentially  make span  
c  
following this theorem  the maximum concurrency  mc  heuristic for a state s   hx  y i is
defined as follows 
q  x  as  
j   x 
else hm c  s   
if y    hm c  s   
c
c
the maximum concurrency c can be calculated by a static analysis of the domain and is a onetime expense  the complete heuristic function can be evaluated by solving the mdp for all states 
however  many of these states may never be visited  in our implementation  we do this calculation
on demand  as more states are visited  by starting the mdp from the current state  each rtdp run
can be seeded by the previous value function  thus no computation is thrown away and only the
relevant part of the state space is explored  we refer to dursamp initiated with the mc heuristic by
durmc
samp  
      average c oncurrency h euristic
instead of using maximum concurrency c in the above heuristic we use the average concurrency
in the domain  ca   to get the average concurrency  ac  heuristic  we call the resulting algorithm
durac
samp   the ac heuristic is not admissible  but in our experiments it is typically a more informed
heuristic  moreover  in the case where all the actions have the same duration  the ac heuristic equals
the mc heuristic 
    hybridized algorithm
we present an approximate method to solve cptp problems  while there can be many kinds of
possible approximation methods  our technique exploits the intuition that it is best to focus computation on the most probable branches in the current policys reachable space  the danger of this
approach is the chance that  during execution  the agent might end up in an unlikely branch  which
has been poorly explored  indeed it might blunder into a dead end in such a case  this is undesirable  because such an apparently attractive policy might have a true expected make span of infinity 
since  we wish to avoid dead ends  we explore the desirable notion of propriety 
definition   propriety  a policy is proper at a state if it is guaranteed to lead  eventually  to the goal
state  i e   it avoids all dead ends and cycles   barto et al          we define a planning algorithm
proper if it always produces a proper policy  when one exists  for the initial state 
we now describe an anytime approximation algorithm  which quickly generates a proper policy
and uses any additional available computation time to improve the policy  focusing on the most
likely trajectories 
  

fim ausam   w eld

      h ybridized p lanner
our algorithm  durhyb   is created by hybridizing two other policy creation algorithms  indeed 
our novel notion of hybridization is both general and powerful  applying to many mdp like problems  however  in this paper we focus on the use of hybridization for cptp  hybridization uses an
anytime algorithm like rtdp to create a policy for frequently visited states  and uses a faster  and
presumably suboptimal  algorithm for the infrequent states 
for the case of cptp  our algorithm hybridizes the rtdp algorithms for interwoven epoch and
aligned epoch models  with aligned epochs  rtdp converges relatively quickly  because the state
space is smaller  but the resulting policy is suboptimal for the cptp problem  because the policy
waits for all currently executing actions to terminate before starting any new actions  in contrast 
rtdp for interwoven epochs generates the optimal policy  but it takes much longer to converge 
our insight is to run rtdp on the interwoven space long enough to generate a policy which is
good on the common states  but stop well before it converges in every state  then  to ensure that the
rarely explored states have a proper policy  we substitute the aligned policy  returning this hybridized
policy 
algorithm   hybridized algorithm durhyb  r  k  m 
   for all s  s   do

  
initialize j    s  with an admissible heuristic
   repeat
  
perform m rtdp trials
  
compute hybridized policy  hyb   using interwoven epoch policy for k familiar states and aligned 

epoch policy otherwise
clean hyb by removing all dead ends and cycles
j   hs    i  evaluation of hyb from the start state
 

j    hs   i j    hs   i 


   until
 r
j    hs   i 

   return hybridized policy hyb

  
  

thus the key question is how to decide which states are well explored and which are not  we
define the familiarity of a state s to be the number of times it has been visited in previous rtdp
trials  any reachable state whose familiarity is less than a constant  k  has an aligned policy created
for it  furthermore  if a dead end state is reached using the greedy interwoven policy  then we create
an aligned policy for the immediate precursors of that state  if a cycle is detected    then we compute
an aligned policy for all the states which are part of the cycle 
we have not yet said how the hybridized algorithm terminates  use of rtdp helps us in defining
a very simple termination condition with a parameter that can be varied to achieve the desired
closeness to optimality as well  the intuition is very simple  consider first  optimal labeled rtdp 
this starts with an admissible heuristic and guarantees that the value of the start state  j    hs    i  
remains admissible  thus less than or equal to optimal   in contrast  the hybridized policys makespan is always longer than or equal to optimal  thus as time progresses  these values approach the
optimal make span from opposite sides  whenever the two values are within an optimality ratio  r  
we know that the algorithm has found a solution  which is close to the optimal 
   in our implementation cycles are detected using simulation 

  

fip lanning with d urative actions in s tochastic d omains

finally  evaluation of the hybridized policy is done using simulation  which we perform after a
fixed number of m rtdp trials  algorithm   summarizes the details of the algorithm  one can see
that this combined policy is proper for two reasons     if the policy at a state is from the aligned
policy  then it is proper because the rtdp for the aligned epoch model was run to convergence  and
   for the rest of the states it has explicitly ensured that there are no cycles or dead ends 
    experiments  planning with deterministic durations
continuing from section      in this set of experiments we evaluate the various techniques for
solving problems involving explicit deterministic durations  we compare the computation time and
solution quality of five methods  interwoven sampled rtdp with no heuristic  dursamp    with the
ac
maximum concurrency  durmc
samp    and average concurrency  dursamp   heuristics  the hybridized
algorithm  durhyb   and sampled rtdp on the aligned epoch model  durae    we test on our
rover  machineshop and aritificial domains  we also use our artificial domain to see if the relative
performance of the techniques varies with the amount of concurrency in the domain 
      e xperimental s etup
we modify the domains used in section     by additionally including action durations  for nasa
rover and machineshop domains  we generate problems with       state variables and       actions  whose duration range between   and     the problems have between                reachable states in the interwoven epoch state space  s    
we use artificial domain for control experiments to study the effect of degree of parallelism 
all the problems in this domain have    state variables and               reachable states and
durations of actions between   and   
we use our implementation of sampled rtdp  and implement all heuristics  maximum concurrency  hm c    average concurrency  hac    for the initialization of the value function  we calculate
these heuristics on demand for the states visited  instead of computing the complete heuristic for the
whole state space at once  we also implement the hybridized algorithm in which the initial value
function was set to the hm c heuristic  the parameters r  k  and m are kept at           and     
respectively  we test each of these algorithms on a number of problem instances from the three
domains  which we generate by varying the number of objects  degrees of parallelism  durations of
the actions and distances to the goal 
      c omparison of running t imes
figures    a  b  and    a  show the variations in the running times for the algorithms on different
problems in rover  machineshop and artificial domains  respectively  the first three bars represent
the base sampled rtdp without any heuristic  with hm c   and with hac   respectively  the fourth
bar represents the hybridized algorithm  using the hm c heuristic  and the fifth bar is computation
of the aligned epoch sampled rtdp with costs set to the maximum action duration  the white
region in the fourth bar represents the time taken for the aligned epoch rtdp computations in the
hybridized algorithm  the error bars represent     confidence intervals on the running times  note
that the plots are on a log scale 
   note that policies returned by dursamp are not guaranteed to be optimal  thus all the implemented algorithms are
approximate  we can replace dursamp by pruned rtdp  durprun   if optimality is desired 

  

fim ausam   w eld

rover  

mach  

    

    

    

mach  

mach  

mach  

mach  

mach  

 
m
ac
h
ae

rover  

 
m
ac
h
ae

rover  

time in sec  on log scale 

    

    

    

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

    

    

 
m
ac
h
ae

time in sec  on log scale 

rover  

 
m
ac
h
ae

rover  

rover  

 
m
ac
h
ae

    

    

figure      a b   running times  on a log scale  for the rover and machineshop domain  respectively  for
each problem the five bars represent the times taken by the algorithms  dursamp      durmc
samp
 ae   durac
 ac  
dur
 h  
and
dur
 ae  
respectively 
the
white
bar
on
dur
hyb
ae
hyb
samp
denotes the portion of time taken by aligned epoch rtdp 

algos
durmc
samp
durac
samp
durhyb
durae

speedup compared with dursamp
rover
machineshop artificial average
        
        
                 
        
        
                 
        
        
                
        
        
                 

table    the ratio of the time taken by s   s rtdp with no heuristics to that of each algorithm  our
heuristics produce     times speedups  the hybridized algo produces about a   x speedup  aligned
epoch search produces    x speedup  but sacrifices solution quality 

we notice that durae solves the problems extremely quickly  this is natural since the alignedepoch space is much smaller  use of both hm c and hac always speeds search in the s   model 
comparing the heuristics amongst themselves  we find that average concurrency heuristic mostly
performs faster than maximum concurrency  presumably because hac is a more informed heuristic in practice  although at the cost of being inadmissible  we find a couple of cases in which hac
doesnt perform better  this could be because it is focusing the search in the incorrect region  given
its inadmissible nature 
for the rover domain  the hybridized algorithm performs fastest  in fact  the speedups are
dramatic compared to other methods  in other domains  the results are more comparable for small
problems  however  for large problems in these two domains  hybridized outperforms the others by
a huge margin  in fact for the largest problem in artificial domain  none of the heuristics are able to
converge  within a day  and only durhyb and durae converge to a solution 
  

fip lanning with d urative actions in s tochastic d omains

    

art  

art  

art  

art  

 
m
ac
h
ae

art  

 
m
ac
h
ae

   
art       art       art       art        art        art        art        

art  

art  

ratio of make span to optimal

time in sec  on log scale 

   
    

    

    

   
   
   
   
 
   

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

 
m
ac
h
ae

   

    

figure      a b   comparison of the different algorithms  running times and solution quality respectively 
for the artificial domain  as degree of parallelism increases the problems become harder  the
largest problem is solved only by durhyb and durae  

table   shows the speedups obtained by various algorithms compared to the basic dursamp  
in the rover and artificial domains the speedups obtained by durhyb and durae are much more
prominent than in the machineshop domain  averaging over all domains  h produces a   x speedup
and ae produces more than a    x speedup 
      c omparison of s olution q uality
figures    a  b  and    b  show the quality of the policies obtained by the same five methods on the
same domains  we measure quality by simulating the generated policy across multiple trials  and
reporting the average time taken to reach the goal  we plot the ratio of the so measured expected
make span to the optimal expected make span    table   presents solution qualities for each method 
averaged over all problems in a domain  we note that the aligned epoch policies usually yield
significantly longer make spans  e g       longer   thus one must make a quality sacrifice for their
speedy policy construction  in contrast  the hybridized algorithm extorts only a small sacrifice in
quality in exchange for its speed 
      variation with c oncurrency
figure    a  represents our attempt to see if the relative performance of the algorithms changed with
increasing concurrency  along the top of the figure  by the problem names  are numbers in brackets 
these list the average number of applicable combinations in each mdp state  avgss    ap s    and

range from    to      concurrent actions  note that for the difficult problems with a lot of parallelism  dursamp slows dramatically  regardless of heuristic  in contrast  the durhyb is still able to
quickly produce a policy  and at almost no loss in quality  figure    b   
   in some large problems the optimal algorithm did not converge  for those  we take as optimal  the best policy found
in our runs 

  

fim ausam   w eld

rover  

    mach  
ratio of make span to optimal

   
   
   
   
 

mach  

   
   
 

 
m
ac
h
ae

 
m
ac
h
ae

mach  

   

   
 
m
ac
h
ae

mach  

   

   
 
m
ac
h
ae

mach  

   

   

 
m
ac
h
ae

mach  

   

   

 
m
ac
h
ae

ratio of make span to optimal

   

 
m
ac
h
ae

rover  

 
m
ac
h
ae

rover  

 
m
ac
h
ae

rover  

 
m
ac
h
ae

rover  

 
m
ac
h
ae

rover  

 
m
ac
h
ae

   

   

figure      a b   comparison of make spans of the solution found with the optimal plotted as   on the yaxes  for rover and machineshop domains  respectively  all algorithms except durae produce
solutions quite close to the optimal 

algos
dursamp
durmc
samp
durac
samp
durhyb
durae

rover
        
        
        
        
        

average quality
machineshop artificial
        
        
        
        
        
        
        
        
        
        

average
        
        
        
        
        

table    overall solution quality produced by all algorithms  note that all algorithms except durae produce policies whose quality is quite close to optimal  on average durae produces make spans
that are about      of the optimal 

   optimal planning with uncertain durations
we now extend the techniques of previous section for the case when action durations are not deterministic  as before  we consider tgp style actions and a discrete temporal model  we assume
independent durations  and monotonic continuations  but section     relaxes the latter  extending
our algorithms to handle multimodal duration distributions  as before we aim to minimize the
expected time required to reach a goal 
    formulating as a comdp
we now formulate our planning problem as a comdp similar to section      while some of the
parameters of the comdp can be used directly from our work on deterministic durations  we need
to recompute the transition function 
  

fip lanning with d urative actions in s tochastic d omains

state space  both the aligned epoch state space as well as the interwoven epoch space  as defined
in section     are adequate to model this planning problem  to determine the size of the interwoven
space  we replace the duration of an action by its max duration  let m  a  denote the maximum
time within which action a will complete  the overall interwoven epoch search space is s     s 




 a   zm  a    where zm  a  represents the set                m  a      and
denotes
the cartesian product over multiple sets 
action space  at any state we may apply a combination of actions with the applicability function
reflecting the fact that the combination of actions is safe w r t itself  and w r t  already executing
actions in case of interwoven space  as in the previous sections  while the previous state space and
action space work well for our problem  the transition function definition needs to change  since we
now need to take into account the uncertainty in durations 
transition function  uncertain durations require significant changes to the probability transition
function  pr     for the interwoven space from the definitions of section        since our assumptions justify conjecture    we need only consider happenings when choosing decision epochs 
n

n

aa

algorithm   computetransitionfunc s hx  y i a 
  
  
  
  
  
  
  
  
  
   
   
   

y  y    a      a  a
mintime  min a  y minimum remaining time for a
maxtime  min a  y maximum remaining time for a
for all integer t   mintime  maxtime  do
at  set of actions that could possibly terminate after t
for all non empty subsets asubt  at do
pc   prob  that exactly asubt terminates after t  see equation     
w    xt   pw     xt is a world state  pw is the probability that asubt terminates yielding xt   
for all  xt   pw    w do
yt    a     t     a     y  a 
  asubt  
insert  hxt   yt i  pw  pc   in output
return output

the computation of transition function is described in algorithm    although the next decision
epoch is determined by a happening  we still need to consider all pivots for the next state calculations
as all these are potential happenings  mintime is the minimum time when an executing action could
terminate  maxtime is the minimum time by which it is guaranteed that at least one action will
terminate  for all times between mintime and maxtime we compute the possible combinations that
could terminate then and the resulting next interwoven state  the probability  pc    line    may be
computed using the following formula 
pc  

y

 prob  a terminates at a   t a hasn  t terminated till a   

 a a  y aasubt

y

 prob  b doesn  t terminate at b   t b hasn  t terminated till b  

    

 b b  y basub
 
t

considering all pivots makes the algorithm computationally intensive because there may be
many pivots and many action combinations could end at each one  and with many outcomes each 
in our implementation  we cache the transition function so that we do not have to recompute the
information for any state 
  

fim ausam   w eld

start and goal states  the start state and goal set that we developed for the deterministic durations
work unchanged when the durations are stochastic  so  the start state is hs    i and the goal set is
g      hx  i x  g  
thus we have modeled our problem as a comdp in the interwoven state space  we have redefined the start and goal states  and the probability transition function  now we can use the techniques
of comdps to solve our problem  in particular  we can use our bellman equations as below 
bellman equations for interwoven epoch space  define el  s  a  s    as the time elapsed between two interwoven states s and s  when combination a is executed in s  the set of equations for
the solution of our problem can be written as 

j    s       if s  g   else

n
o
x
pr    s   s  a  el  s  a  s      j    s   
j    s    min


aap    s   
 s s  

    

compare these equations with equation     there is one difference besides the new transition
function  the time elapsed is within the summation sign  this is because time elapsed depends
also on the next interwoven state 
having modeled this problem as a comdp we again use our algorithms of section    we use
dur to denote the family of algorithms for the cptp problems involving stochastic durations 
the main bottleneck in solving these problem  besides the size of the interwoven state space  is the
high branching factor 
      p olicy c onstruction   rtdp   h ybridized p lanning
since we have modeled our problem as a comdp in the new interwoven space  we may use pruned
rtdp  durprun   and sampled rtdp  dursamp   for policy construction  since the cost function in our problem  el   depends also on the current and the next state  combo skipping does not
apply for this problem  thus durprun refers to rtdp with only combo elimination 
furthermore  only small adaptations are necessary to incrementally compute the  admissible 
maximum concurrency  m c  and  more informed  but inadmissible  average concurrency  ac 
heuristics  for example  for the serial mdp  in the rhs of equation     we now need to compute
the average duration of an action and use that as the actions cost 
likewise  we can further speed planning by hybridizing  durhyb   rtdp algorithms for interwoven and aligned epoch comdps to produce a near optimal policy in significantly less time 
the dynamics of aligned epoch space is same as that in section   with one exception  the cost of a
combination  in the case of deterministic durations  was simply the max duration of the constituent
actions  the novel twist stems from the fact that uncertain durations require computation of the cost
of an action combination as the expected time that the last action in the combination will terminate 
for example  suppose two actions  both with uniform duration distributions over        are started
concurrently  the probabilities that both actions will have finished by times      and    and no earlier  are           and     respectively  thus the expected duration of completion of the combination
 let us call it ae   is                           
  

fip lanning with d urative actions in s tochastic d omains

    expected duration planner
when modeled as a comdp in the full blown interwoven space  stochastic durations cause an
exlposive growth in the branching factor  in general  if n actions are started each with m possible
durations and each having r probabilistic effects  then there are  m      r     n  rn       rn
potential successors  this number may be computed as follows  for each duration between   and
m    any subset of actions could complete and each action could result in r outcomes  hence  total
p
number of successors per duration is i    n  n ci ri    r     n  rn     moreover  if none of the
actions finish until time m    then at the last step all actions terminate leading into rn outcomes 
so  total number of successors is  m      r     n  rn       rn   thus  the branching factor is
multiplicative in the duration uncertainty and exponential in the concurrency 
to manage this extravagant computation we must curb the branching factor  one method is
to ignore duration distributions  we can assign each action a constant duration equal to the mean
of its distribution  then apply a deterministic duration planner such as dursamp   however  when
executing the deterministic duration policy in a setting where durations are actually stochastic  an
action will likely terminate at a time different than its mean  expected duration  the durexp
planner addresses this problem by augmenting the deterministic duration policy created to account
for these unexpected outcomes 
      o nline v ersion
the procedure is easiest to understand in its online version  algorithm     wait until the unexpected
happens  pause execution  and re plan  if the original estimate of an actions duration is implausible 
we compute a revised deterministic estimate in terms of ea  min   the expected value of as
duration given that it has not terminated by time min  thus  ea     will compute the expected
duration of a 
algorithm   online durexp
   build a deterministic duration policy from the start state s 
   repeat
  
execute action combination specified by policy
  
wait for interrupt
  
case  action a terminated as expected    do nothing 
  
case  action a terminates early
  
extend policy from current state
  
case  action a didnt terminate as expected
  
extend policy from current state revising

as duration as follows 
   
  time elapsed since a started executing
   
nextexp  dea    e
   
while nextexp    do
   
nextexp  dea  nextexp e
   
endwhile
   
as revised duration  nextexp  
   
endwait
    until goal is reached

  

fim ausam   w eld

example  let the duration of an action a follow a uniform distribution between   and     the
expected value that gets assigned in the first run of the algorithm  dea    e  is    while running the
algorithm  suppose the action didnt terminate by   and we reach a state where a has been running
for  say    time units  in that case  a revised expected duration for a would be  dea    e       
similarly  if it doesnt terminate by    either then the next expected duration would be     and
finally     in other words for all states where a has been executing for times   to    it is expected to
terminate at    for all times between   and    the expected completion is at     for    to    it is   
and if it doesnt terminate at    then it is      
      o ffline v ersion
this algorithm also has an offline version in which re planning for all contingencies is done ahead of
time and for fairness we used this version in the experiments  although the offline algorithm plans
for all possible action durations  it is still much faster than the other algorithms  the reason is that
each of the planning problems solved is now significantly smaller  less branching factor  smaller
reachable state space   and all the previous computation can be succinctly stored in the form of the
hinterwoven state  valuei pairs and thus reused  algorithm   describes this offline planner and the
subsequent example illustrates the savings 
algorithm   offline durexp
   build a deterministic duration policy from the start state s    get current j   and    values


   insert s  in the queue open
   repeat
  
state   open pop  
  
for all currstate s t  pr    currstate state      state       do



if currstate is not goal and currstate is not in the set visited then
visited insert currstate 
if j    currstate  has not converged then
if required  change the expected durations of the actions that are currently executing in
currstate 
   
solve a deterministic duration planning problem with the start state currstate
   
insert currstate in the queue open
    until open is empty
  
  
  
  

line   of algorithm   assigns a new expected duration for all actions that are currently running
in the current state and have not completd by the time of their previous termination point  this
reassignment follows the similar case in the online version  line     
example  consider a domain with two state variables  x  and x    with two actions set x  and
set x    the task is to set both variables  initially they are both false   assume that set x  always
succeeds whereas set x  succeeds with only     probability  moreover  let both actions have a
uniform duration distribution of       or    in such a case a complete interwoven epoch search
could touch    interwoven states  each state variable could be true or false  each action could be
not running  running for   unit  and running for   units   instead  if we build a deterministic
duration policy then each actions deterministic duration will be    and so the total number of states
touched will be from the    interwoven states  each action could now only be not running or
running for   unit  
  

fip lanning with d urative actions in s tochastic d omains

problem
a 

b 

g

c 

d

g

optimal solution  trajectory    pr       makespan   
a 

b 

g

c 

optimal solution  trajectory    pr       makespan   
a 
c 

d

g

dur exp solution  makespan   
a 

time

 

b 
 

g
 

  

figure     an example of a domain where the durexp algorithm does not compute an optimal solution 
now  suppose that the deterministic planner decides to execute both actions in the start state 
having committed to this combination  it is easy to see that certain states will never be reached  for
example  the state h x    x       setx       i can never be visited  since once set x  completes
it is guaranteed that x  will be set  in fact  in our example  only   new states will initiate offline
replanning  line    in algo     viz   h x    x       setx       i  h x    x       setx       i  and
h x    x       setx       i  
      p roperties
unfortunately  our durexp algorithm is not guaranteed to produce an optimal policy  how bad
are the policies generated by the expected duration planner  the experiments show that durexp
typically generates policies which are extremely close to optimal  even the worst case pathological
domain we are able to construct leads to an expected make span which is only     longer than
optimal  in the limit   this example is illustrated below 
example  we consider a domain which has actions a  n   b  n   c  n and d  each ai and bi
takes time  i   each ci has a probabilistic duration  with probability      ci takes   unit of time 
and with the remaining probability  it takes  i       time  thus  the expected duration of ci is
 i      d takes   units  in sub problem spi   the goal may be reached by executing ai followed
by bi   alternatively  the goal may be reached by first executing ci and then recursively solving
the sub problem spi    in this domain  the durexp algorithm will always compute hai   bi i
as the best solution  however  the optimal policy starts both  ai   ci    if ci terminates at    the
policy executes the solution for spi    otherwise  it waits until ai terminates and then executes bi  
figure    illustrates the sub problem sp  in which the optimal policy has an expected make span
of    vs  durexp s make span of     in general  the expected make span of the optimal policy on
 
spn is      n       n       n      thus  limn exp
opt       
    multi modal duration distributions
the planners of the previous two sections benefited by considering the small set of happenings
instead of pivots  an approach licensed by conjecture    unfortunately  this simplification is not
  

fim ausam   w eld

warranted in the case of actions with multi modal duration distributions  which can be common
in complex domains where all factors cant be modeled explicitly  for example  the amount of
time for a mars rover to transmit data might have a bimodal distribution  normally it would
take little time  but if a dust storm were in progress  unmodeled  it could take much longer  to
handle these cases we model durations with a mixture of gaussians parameterized by the triple
hamplitude  mean  variancei 
      c o mdp f ormulation
although we cannot restrict decision epochs to happenings  we need not consider all pivots  they
are required only for actions with multi modal distributions  in fact  it suffices to consider pivots in
regions of the distribution where the expected time to completion increases  in all other cases we
need consider only happenings 
two changes are required to the transition function of algorithm    in line    the maxtime
computation now involves time until the next pivot in the increasing remaining time region for
all actions with multi modal distributions  thus forcing us to take a decision at those points  even
when no action terminates   another change  in line    allows a non empty subset asub t for t  
maxtime  that is  next state is computed even without any action termination  by making these
changes in the transition function we reformulate our problem as a comdp in the interwoven space
and thus solve  using our previous methods of pruned sampled rtdp  hybrid algorithm or expectedduration algorithm 
      a rchetypal  d uration p lanner
we also develop a multi modal variation of the expected duration planner  called durarch   instead of assigning an action a single deterministic duration equal to the expected value  this planner
assigns it a probabilistic duration with various outcomes being the means of the different modes in
the distribution and the probabilities being the probability mass in each mode  this enhancement
reflects our intuitive understanding for multi modal distributions and the experiments confirm that
durarch produces solutions having shorter make spans than those of durexp  
    experiments  planning with stochastic durations
we now evaluate our techniques for solving planning problems involving stochastic durations  we
compare the computation time and solution quality  make span  of our five planners for domains
with and without multi modal duration distributions  we also re evaluate the effectiveness of the
maximum   mc  and average concurrency  ac  heuristics for these domains 
      e xperimental s etup
we modify our rover  machineshop  and artificial domains by additionally including uncertainty
in action durations  for this set of experiments  our largest problem had   million world states
of which       were reachable  our algorithms explored up to           distinct states in the
interwoven state space during planning  the domains contained as many as    actions  and some
actions had as many as    possible durations  for more details on the domains please refer to the
longer version  mausam        
  

fip lanning with d urative actions in s tochastic d omains

planning time  in sec 

    
    
    

rover

machine shop

    
pruned
durprun
dursamp
sampled
durhyb
hybrid

    
    

durexp
exp dur

 
  

  

  

  

  

  

  

  

  

   problems

figure     planning time comparisons for rover and machineshop domains  variation along algorithms
when all initialized by the average concurrency  ac  heuristic  durexp performs the best 

algos
dursamp
durhyb
durexp

average quality of make span
rover machineshop artificial
     
     
     
     
     
     
     
     
     

table    all three planners produce near optimal policies as shown by this table of ratios to the
optimal make span   

      c omparing running t imes
we compare all algorithms with and without heuristics and reaffirm that the heuristics significantly
speed up the computation on all problems  indeed  some problems are too large to be solved without
heuristics  comparing them amongst themselves we find that ac beats m c  regardless of the
planning algorithm  this isnt surprising since ac sacrifices admissibility 
figure    reports the running times of various algorithms  initialized with the ac heuristic  on
the rover and machine shop domains when all durations are unimodal  durexp out performs
the other planners by substantial margins  as this algorithm is solving a comparatively simpler
problem  fewer states are expanded and thus the approximation scales better than others  solving 
for example  two machine shop problems  which were too large for most other planners  in most
cases hybridization speeds planning by significant amounts  but it performs better than durexp
only for the artificial domain 
      c omparing s olution q uality
we measure quality by simulating the generated policy across multiple trials  we report the ratio
of average expected make span and the optimal expected make span for domains with all unimodal
distributions in table    we find that the make spans of the inadmissible heuristic ac are at par
    if the optimal algorithm doesnt converge  we use the best solution found across all runs as optimal 

  

fim ausam   w eld

  
  
  

    
durprun
pruned
dursamp
sampled

   

j  s  

planning time  log scale 

     

durhyb
hybrid
durarch
arch dur
durexp
exp dur

  

  
durprun
dur prun
dursamp
dur samp

  
  

durhyb
dur hyb
durarch
dur arch

  

durexp
dur exp

  

  

  

  

  

  

   problems

  

  

  

  

  

   problems

figure     comparisons in the machine shop domain with multi modal distributions   a  computation
time comparisons  durexp and durarch perform much better than other algos   b  makespans returned by different algos  solutions returned by dursamp are almost optimal  overall
durarch finds a good balance between running time and solution quality 

with those of the admissible heuristic m c  the hybridized planner is approximate with a userdefined bound  in our experiments  we set the bound to    and find that the make spans returned
by the algorithm are quite close to the optimal and do not always differ by     durexp has no
quality guarantees  still the solutions returned on the problems we tested upon are nearly as good as
other algorithms  thus  we believe that this approximation will be quite useful in scaling to larger
problems without losing solution quality 
      m ultimodal d omains
we develop multi modal variants of our domains  e g   in the machine shop domain  time for fetching paint was bimodal  if in stock  paint can be fetched fast  else it needs to be ordered   there
was an alternative but costly paint action that doesnt require fetching of paint  solutions produced
by dursamp made use of pivots as decision epochs by starting the costly paint action in case the
fetch action didnt terminate within the first mode of the bimodal distribution  i e  paint was out of
stock  
the running time comparisons are shown in figure    a  on a log scale  we find that durexp
terminates extremely quickly and durarch is not far behind  however  the make span comparisons in figure    b  clearly illustrate the approximations made by these methods in order to achieve
planning time  durarch exhibits a good balance of planning time and solution quality 

   related work
this paper extends our prior work  originally reported in several conference publications  mausam
  weld                  a      b  
temporal planners may be classified as using constraint posting or extended state space methods  discussed earlier in section     while the constraint approach is promising  few  if any  probabilistic planners have been implemented using this architecture  one exception is buridan  kush  

fip lanning with d urative actions in s tochastic d omains

stochastic

deterministic

concurrent
durative
non durative
dur  tempastic 
concurrent mdp 
gsmdp  prottle 
factorial mdp 
fpg  aberdeen et al 
paragraph
temporal planning
step optimal planning
 tp   sapa  mips
 graphplan  satplan 
tlplan  etc  

non concurrent
durative
non durative
time dependent mdp 
mdp
ixtet  circa 
 rtdp  lao   etc  
foss   onder
planning with
classical planning
numerical resources
 hsp  ff  etc  
 sapa  metric ff  cpt 

figure     a table listing various planners that implement different subsets of concurrent  stochastic  durative actions 

merick  hanks    weld         which performed poorly  in contrast  the mdp community has
proven the state space approach successful  since the powerful deterministic temporal planners 
which have won the various planning competitions  also use the state space approach  we adopt it
for our algorithms that combine temporal planning with mdps  it may be interesting to incorporate
constraint based approaches in a probabilistic paradigm and compare against the techniques of this
paper 
    comparison with semi mdps
a semi markov decision process is an extension of mdps that allows durative actions to take variable time  a discrete time semi mdp can be solved by solving a set of equations that is a direct
extension of equations    the techniques for solving discrete time semi mdps are natural generalizations of those for mdps  the main distinction between a semi mdp and our formulation of
concurrent probabilistic temporal planning with stochastic durations concerns the presence of concurrently executing actions in our model  a semi mdp does not allow for concurrent actions and
assumes one executing action at a time  by allowing concurrency in actions and intermediate decision epochs  our algorithms need to deal with large state and action spaces  which is not encountered
by semi mdps 
furthermore  younes and simmons have shown that in the general case  semi mdps are incapable of modeling concurrency  a problem with concurrent actions and stochastic continuous
durations needs another model known as generalized semi markov decision process  gsmdp 
for a precise mathematical formulation  younes   simmons      b  
    concurrency and stochastic  durative actions
tempastic  younes   simmons      a  uses a rich formalism  e g  continuous time  exogenous
events  and expressive goal language  to generate concurrent plans with stochastic durative actions  tempastic uses a completely non probabilistic planner to generate a plan which is treated
as a candidate policy and repaired as failure points are identified  this method does not guarantee
completeness or proximity to the optimal  moreover  no attention was paid towards heuristics or
search control making the implementation impractical 
gsmdps  younes   simmons      b  extend continuous time mdps and semi markov mdps 
modeling asynchronous events and processes  both of younes and simmonss approaches handle
  

fim ausam   w eld

a strictly more expressive model than ours due to their modeling of continuous time  they solve
gsmdps by approximation with a standard mdp using phase type distributions  the approach
is elegant  but its scalability to realistic problems is yet to be demonstrated  in particular  the approximate  discrete mdp model can require many states yet still behave very differently than the
continuous original 
prottle  little et al         also solves problems with an action language more expressive than
ours  effects can occur in the middle of action execution and dependent durations are supported 
prottle uses an rtdp type search guided by heuristics computed from a probabilistic planning
graph  however  it plans for a finite horizon  and thus for an acyclic state space  it is difficult to
compare prottle with our approach because prottle optimizes a different objective function  probability of reaching a goal   outputs a finite length conditional plan as opposed to a cyclic plan or
policy  and is not guaranteed to reach the goal 
fpg  aberdeen   buffet        learns a separate neural network for each action individually
based on the current state  in the execution phase the decision  i e   whether an action needs to be
executed or not  is taken independently of decisions regarding other actions  in this way fpg is able
to effectively sidestep the blowup caused by exponential combinations of actions  in practice it is
able to very quickly compute high quality solutions 
rohanimanesh and mahadevan        investigate concurrency in a hierarchical reinforcement
learning framework  where abstract actions are represented by markov options  they propose an
algorithm based on value iteration  but their focus is calculating joint termination conditions and rewards received  rather than speeding policy construction  hence  they consider all possible markov
option combinations in a backup 
aberdeen et al         plan with concurrent  durative actions with deterministic durations in a
specific military operations domain  they apply various domain dependent heuristics to speed the
search in an extended state space 
    concurrency and stochastic  non durative actions
meuleau et al  and singh   cohn deal with a special type of mdp  called a factorial mdp  that
can be represented as a set of smaller weakly coupled mdps  the separate mdps are completely
independent except for some common resource constraints  and the reward and cost models are
purely additive  meuleau  hauskrecht  kim  peshkin  kaelbling  dean    boutilier        singh
  cohn         they describe solutions in which these sub mdps are independently solved and
the sub policies are merged to create a global policy  thus  concurrency of actions of different
sub mdps is a by product of their work  singh   cohn present an optimal algorithm  similar to
combo elimination used in durprun    whereas domain specific heuristics in meuleau et al  have no
such guarantees  all of the work in factorial mdps assumes that a weak coupling exists and has
been identified  but factoring an mdp is a hard problem in itself 
paragraph  little   thiebaux        formulates the planning with concurrency as a regression
search over the probabilistic planning graph  it uses techniques like nogood learning and mutex
reasoning to speed policy construction 
guestrin et al  solve the multi agent mdp problem by using a linear programming  lp  formulation and expressing the value function as a linear combination of basis functions  by assuming
that these basis functions depend only on a few agents  they are able to reduce the size of the lp
 guestrin  koller    parr        
  

fip lanning with d urative actions in s tochastic d omains

    stochastic  non concurrent  durative actions
many researchers have studied planning with stochastic  durative actions in absence of concurrency 
for example  foss and onder        use simple temporal networks to generate plans in which the
objective function has no time component  simple temporal networks allow effective temporal
constraint reasoning and their methods can generate temporally contingent plans 
boyan and littman        propose time dependent mdps to model problems with actions that
are not concurrent and have time dependent  stochastic durations  their solution generates piecewise linear value functions 
nasa researchers have developed techniques for generating non concurrent plans with uncertain continuous durations using a greedy algorithm which incrementally adds branches to a straightline plan  bresina et al         dearden  meuleau  ramakrishnan  smith    washington        
while they handle continuous variables and uncertain continuous effects  their solution is heuristic
and the quality of their policies is unknown  also  since they consider only limited contingencies 
their solutions are not guaranteed to reach the goal 
ixtet is a temporal planner that uses constraint based reasoning within partial order planning
 laborie   ghallab         it embeds temporal properties of actions as constraints and does not
optimize make span  circa is an example of a system that plans with uncertain durations where
each action is associated with an unweighted set of durations  musliner  murphy    shin        
    deterministic  concurrent  durative actions
planning with deterministic actions is a comparitively simpler problem and much of the work in
planning under uncertainty is based on the previous  deterministic planning research  for instance 
our interwoven state representation and transition function are extensions of the extended state representations in tp   sapa  and tlplan  haslum   geffner        do   kambhampati       
bacchus   ady        
other planners  like mips and altaltp   have also investigated fast generation of parallel plans
in deterministic settings  edelkamp        nigenda   kambhampati        and jensen and veloso
       extend it to problems with disjunctive uncertainty 

   future work
having presented a comprehensive set of techniques to handle probabilistic outcomes  concurrent
and durative actions in a single formalism  we now direct our attention towards different relaxations
and extensions to the proposed model  in particular  we explore other objective functions  infinite
horizon problems  continuous valued duration distributions  temporally expressive action models 
degrees of goal satisfaction and interruptibility of actions 
    extension to other cost functions
for the planning problems with durative actions  sections   and beyond  we focused on make span
minimization problems  however  our techniques are quite general and are applicable  directly
or with minor variations  to a variety of cost metrics  as an illustration  consider the mixed cost
optimization problem in which in addition to the duration of each action  we are also given the
amount of resource consumed per action  and we wish to minimize the the sum of make span
and total resource usage  assuming that the resource consumption is unaffected by concurrent
  

fim ausam   w eld

execution  we can easily compute a new max concurrency heuristic  the mixed cost counterpart
for equations    is 
jt  x 
  jr  x 
for y   
c
qt  x  as  
  qr  x  as   for y    
c

j    s  

j    s  


    

here  jt is for the single action mdp assignng costs to be durations and jr is for the single
action mdp assigning costs to be resource consumptions  a more informed average concurrency
heuristic can be similarly computed by replacing maximum concurrency by average concurrency 
the hybridized algorithm follows in the same fashion  with the fast algorithm being a comdp
solved using techniques of section   
on the same lines  if the objective function is to minimize make span given a certain maximum
resource usage  then the total amount of resource remaining can be included in the state space for
all the comdps and underlying single action mdps etc  and the same techniques may be used 
    infinite horizon problems
until now this paper has defined the techniques for the case of indefinite horizon problems  in
which an absorbing state is defined as is reachable  for other problems an alternative formulation is
preferred that allows for infinite execution but discounts the future costs by multiplying them by a
discount factor in each step  again  our techniques can be suitably extended for such scenario  for
example  theorem   gets modified to the following 
qk  s  a   

 k

qk  s   a       ck  a  

k
x

 



ik

ck   ai   

i  

recall that this theorem provides us with the pruning rule  combo skipping  thus  we can use
pruned rtdp with the new pruning rule 
    extensions to continuous duration distributions
until now we have confined ourselves to actions with discrete durations  refer to assumption    
we now investigate the effects of dealing directly with continuous uncertainty in the duration distributions  let fit  t dt be the probability of action ai completing between times t   t and t   t   dt 
conditioned on action ai not finishing until time t   similarly  define fit  t  to be the probability of
the action finishing after time t   t  
let us consider the extended state hx    a    t   i  which denotes that action a  started t units
ago in the world state x  let a  be an applicable action that is started in this extended state  define
m   min m  a   t  m  a      where m denotes the maximum possible duration of execution
for each action  intuitively  m is the time by which at least one action will complete  then

q   n    hx    a    t   i  a     

z m
 

z m
 

h

i

f t  t f    t  t   j   n  hx     a    t i  dt  
h

i

f t  t f    t  t   j   n  hx     a    t   t  i  dt
  

    

fi 

 

 

time

 

 

  

  

  

expected time to reach the goal

expected remaining time for action a 

duration distribution of a 

p lanning with d urative actions in s tochastic d omains

 
 
 
 

 

 

 

time

 

 

  

 
 
 
 

 

 

 

 

 

  

time

figure     if durations are continuous  real valued  rather than discrete  there may be an infinite number of
potentially important decision epochs  in this domain  a crucial decision epoch could be required
at any time in         depending on the length of possible alternate plans 

here x  and x  are world states obtained by applying the deterministic actions a  and a 
respectively on x  recall that j   n    s    mina q   n    s  a   for a fixed point computation of

this form  we desire that jn   and jn have the same functional form     going by the equation
above this seems very difficult to achieve  except perhaps for very specific action distributions in
some special planning problems  for example  if all distributions are constant or if there is no
concurrency in the domain  then these equations are easily solvable  but for more interesting cases 
solving these equations is a challenging open question 
furthermore  dealing with continuous multi modal distributions worsens the decision epochs
explosion  we illustrate this with the help of an example 
example  consider the domain of figure   except that let action a  have a bimodal distribution 
the two modes being uniform between     and      respectively as shown in figure    a   also
let a  have a very small duration  figure    b  shows the expected remaining termination times
if a  terminates at time     notice that due to bimodality  this expected remaining execution time
increases between   and    the expected time to reach the goal using plan h a    a     a  i is shown
in the third graph  now suppose  we have started  a    a     and we need to choose the next decision
epoch  it is easy to see that the optimal decision epoch could be any point between   and   and
would depend on the alternative routes to the goal  for example  if duration of b  is       then the
optimal time point to start the alternative route is      right after the expected time to reach the goal
using first plan exceeds       
thus  the choice of decision epochs depends on the expected durations of the alternative routes 
but these values are not known in advance  in fact these are the ones being calculated in the planning
phase  therefore  choosing decision epochs ahead of time does not seem possible  this makes the
optimal continuous multi modal distribution planning problem mostly intractable for any reasonable
sized problem 
    generalizing the tgp action model
the assumption of tgp style actions enables us to compute optimal policies  since we can prune the
number of decision epochs  in the case of complex action models like pddl     fox   long        
all old  deterministic state space planners are incomplete  for the same reasons  our algorithms are
    this idea has been exploited in order to plan with continuous resources  feng  dearden  meuleau    washington 
      

  

fim ausam   w eld

incomplete for problems in ppddl      recently  cushing et al  have introduced tempo  a statespace planner  which uses lifting over time in to achieve completeness  cushing  kambhampati 
mausam    weld         in pursuit of finding a complete  state space  probabilistic planner for
complex action models  a natural step is to consider a tempo like representation in a probabilistic
setting  while working out the details seems relatively straightforward  the important research
challenge will be to find the right heuristics to streamline the search so that the algorithm can scale 
    other extensions
there are several other extensions to the basic framework that we have suggested  each different
construct introduces additional structure and we need to exploit the knowledge in order to design
fast algorithms  many times  the basic algorithms proposed in this paper may be easily adapted to
such situations  sometimes they may be not  we list two of the important extensions below 
 notion of goal satisfaction  different problems may require slightly different notions of
when a goal is reached  for example  we have assumed thus far that a goal is not officially
achieved until all executed actions have terminated  alternatively  one might consider a goal
to be achieved if a satisfactory world state is reached  even though some actions may be in the
midst of execution  there are intermediate possibilities in which a goal requires some specific
actions to necessarily end  just by changing the definition of the goal set  these problems can
be modeled as a comdp  the hybridized algorithm and the heuristics can be easily adapted
for this case 
 interruptible actions  we have assumed that  once started  an action cannot be terminated 
however  a richer model may allow preemptions  as well as the continuation of an interrupted
action  the problems  in which all actions could be interrupted at will  have a significantly
different flavor  interrupting an action is a new kind of decision and requires a full study of
when might an action termination be useful  to a large extent  planning with these is similar
to finding different concurrent paths to the goal and starting all of them together  since one can
always interrupt all the executing paths as soon as the goal is reached  for instance  example
in figure   no longer holds since b  can be started at time    and later terminated as needed
to shorten the make span 
    effect of large durations
a weakness of all extended state space approaches  both in deterministic as well as probabilistic
settings  is the dependence on absolute durations  or to be more accurate  the greatest common
divisor of action durations   for instance  if the domain has an action a with a large duration 
say     and another concurrently executable action with duration    then all world states will be
explored with the tuples  a       a              a        a       in general  many of these states will
behave similarly and there will be certain decision boundaries that will be important  start b if
a has been executing for    units and c otherwise is one example of such a decision boundary 
instead of representing all these flat discrete states individually  planning in an aggregate space in
which each state represents several extended states will help alleviate this inefficiency 
however  it is not obvious how to achieve such an aggregation automatically  since adapting
the well known methods for aggregation do not hold in our case  for instance  spudd  hoey
  

fip lanning with d urative actions in s tochastic d omains

et al         uses algebraic decision diagrams to represent abstract states that have the same jvalue  aggregating the same valued states may not be enough for us  since the expected time of
completion depends linearly on the amount of time left for the longest executing action  so  all
the states which differ only by the amount of time an action has been executing will not be able
to aggregate together  in a similar way  feng et al         use piecewise constant and piecewise
linear representations to adaptively discretize continuous variables  in our case  we have  a  of
such variables  while only a few of them that are executing are active at a given time  modeling a
sparse high dimensional value function is not easy either  being able to exploit this structure due
to action durations is an essential future direction in order to scale the algorithms to complex real
world domains 

   conclusions
although concurrent and durative actions with stochastic effects characterize many real world domains  few planners can handle all these challenges in concert  this paper proposes a unified statespace based framework to model and solve such problems  state space formulations are popular
both in deterministic temporal planning as well as in probabilistic planning  however  each of these
features bring in additional complexities to the formulation and afford new solution techniques  we
develop the dur family of algorithms to alleviates these complexities  we evaluate the techniques on the running times and qualities of solutions produced  moreover  we study the theoretical
properties of these domains and also identify key conditions under which fast  optimal algorithms
are possible  we make the following contributions 
   we define concurrent mdps  comdp   an extension of the mdp model to formulate a
stochastic planning problem with concurrent actions  a comdp can be cast back into a new
mdp with an extended action space  because this action space is possibly exponential in
the number of actions  solving the new mdp naively may take a huge performance hit  we
develop the general notions of pruning and sampling to speed up the algorithms  pruning
refers to pruning of the provably sub optimal action combinations for each state  thus performing less computation but still guaranteeing optimal solutions  sampling based solutions
rely on an intelligent sampling of action combinations to avoid dealing with their exponential
number  this method converges orders of magnitude faster than other methods and produces
near optimal solutions 
   we formulate the planning with concurrent  durative actions as a comdp in two modified
state spaces  aligned epoch  and interwoven epoch  while aligned epoch based solutions
run very fast  interwoven epoch algorithms yield a much higher quality solutions  we also define two heuristic functions  maximum concurrency  mc   and average concurrency  ac 
to guide the search  mc is an admissible heuristic  whereas ac  while inadmissible  is typically more informed leading to better computational gains  we call our algorithms the dur
family of algorithms  the subscripts samp or prun refer to sampling and pruning respectively 
optional superscripts ac or mc refer to the heuristic employed  if any and an optional   
before dur notifies a problem with stochastic durations  for example  labeled rtdp for
a deterministic duration problem employing sampling and started with ac heuristic will be
abbreviated as durac
samp  
  

fim ausam   w eld

   we also develop the general technique of hybridizing two planners  hybridizing interwovenepoch and aligned epoch comdps yields a much more efficient algorithm  durhyb   the
algorithm has a parameter  which can be varied to trade off speed against optimality  in
our experiments  durhyb quickly produces near optimal solutions  for larger problems  the
speedups over other algorithms are quite significant  the hybridized algorithm can also be
used in an anytime fashion thus producing good quality proper policies  policies that are
guaranteed to reach the goal  within a desired time  moreover  the idea of hybridizing two
planners is a general notion  recently it has been applied to solving general stochastic planning
problems  mausam  bertoli    weld        
   uncertainty in durations leads to more complexities because in addition to state and action
spaces  there is also a blowup in the branching factor and in the number of decision epochs 
we bound the space of decision epochs in terms of pivots  times when actions may potentially terminate  and conjecture further restrictions  thus making the problem tractable  we
also propose two algorithms  the expected duration planner  durexp   and the archetypal
duration planner  durarch    which successively solve small planning problems each with
no or limited duration uncertainty  respectively  durarch is also able to make use of the
additional structure offered by multi modal duration distributions  these algorithms perform
much faster than other techniques  moreover  durarch offers a good balance between
planning time vs  solution quality tradeoff 
   besides our focus on stochastic actions  we expose important theoretical issues related with
durative actions which have repercussions to deterministic temporal planners as well  in
particular  we prove that all common state space temporal planners are incomplete in the face
of expressive action models  e g   pddl      a result that may have a strong impact on the
future temporal planning research  cushing et al         
overall  this paper proposes a large set of techniques that are useful in modeling and solving
planning problems employing stochastic effects  concurrent executions and durative actions with
duration uncertainties  the algorithms range from fast but suboptimal solutions  to relatively slow
but optimal  various algorithms that explore different intermediate points in this spectrum are also
presented  we hope that our techniques will be useful in scaling the planning techniques to real
world problems in the future 

acknowledgments
we thank blai bonet for providing the source code of gpt as well as for comments in the course
of this work  we are thankful to sumit sanghai for his theorem proving skills and advice at various
stages of this research  we are grateful to derek long and the anonymous reviewers of this paper
who gave several thoughtful suggestions for generalizing the theory and improving the clarity of the
text  we also thank subbarao kambhampati  daniel lowd  parag  david smith and all others who
provided useful comments on drafts on parts of this research  this work was performed at university of washington between      and      and was supported by generous grants from national
aeronautics and space administration  award nag          national science foundation  award
iis           and office of naval research  awards n                 n                 and
the wrf   tj cable professorship 
  

fip lanning with d urative actions in s tochastic d omains

references
aberdeen  d   thiebaux  s     zhang  l          decision theoretic military operations planning 
in icaps   
aberdeen  d     buffet  o         
gradients  in icaps   

concurrent probabilistic temporal planning with policy 

bacchus  f     ady  m          planning with resources and concurrency  a forward chaining
approach  in ijcai    pp         
barto  a   bradtke  s     singh  s          learning to act using real time dynamic programming 
artificial intelligence            
bertsekas  d          dynamic programming and optimal control  athena scientific 
blum  a     furst  m          fast planning through planning graph analysis  artificial intelligence 
               
bonet  b     geffner  h          labeled rtdp  improving the convergence of real time dynamic
programming  in icaps    pp       
bonet  b     geffner  h          mgpt  a probabilistic planner based on heuristic search  jair 
        
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions
and computational leverage  j  artificial intelligence research          
boyan  j  a     littman  m  l          exact solutions to time dependent mdps  in nips    p 
     
bresina  j   dearden  r   meuleau  n   smith  d     washington  r          planning under continuous time and resource uncertainty   a challenge for ai  in uai   
chen  y   wah  b  w     hsu  c          temporal planning using subgoal partitioning and resolution in sgplan  jair          
cushing  w   kambhampati  s   mausam    weld  d  s          when is temporal planning really
temporal   in ijcai   
dearden  r   meuleau  n   ramakrishnan  s   smith  d  e     washington  r          incremental
contingency planning  in icaps   workshop on planning under uncertainty and incomplete information 
do  m  b     kambhampati  s          sapa  a domain independent heuristic metric temporal
planner  in ecp   
do  m  b     kambhampati  s          sapa  a scalable multi objective metric temporal planner 
jair             
edelkamp  s          taming numbers and duration in the model checking integrated planning
system  journal of artificial intelligence research             
  

fim ausam   w eld

feng  z   dearden  r   meuleau  n     washington  r          dynamic programming for structured continuous markov decision processes  in uai    p      
foss  j     onder  n          generating temporally contingent plans  in ijcai   workshop on
planning and learning in apriori unknown or dynamic domains 
fox  m     long  d          pddl     an extension to pddl for expressing temporal planning
domains   jair special issue on  rd international planning competition            
gerevini  a     serina  i          lpg  a planner based on local search for planning graphs with
action graphs  in aips    p      
guestrin  c   koller  d     parr  r          max norm projections for factored mdps  in ijcai   
pp         
hansen  e     zilberstein  s          lao   a heuristic search algorithm that finds solutions with
loops  artificial intelligence            
haslum  p     geffner  h          heuristic planning with time and resources  in ecp   
hoey  j   st aubin  r   hu  a     boutilier  c          spudd  stochastic planning using decision
diagrams  in uai    pp         
jensen  r  m     veloso  m          obdd based universal planning for synchronized agents in
non deterministic domains  journal of artificial intelligence research          
kushmerick  n   hanks  s     weld  d          an algorithm for probabilistic planning  artificial
intelligence                  
laborie  p     ghallab  m          planning with sharable resource constraints  in ijcai    p 
     
little  i   aberdeen  d     thiebaux  s          prottle  a probabilistic temporal planner  in
aaai   
little  i     thiebaux  s          concurrent probabilistic planning in the graphplan framework  in
icaps   
long  d     fox  m          the  rd international planning competition  results and analysis 
jair          
mausam         stochastic planning with concurrent  durative actions  ph d  dissertation  university of washington 
mausam  bertoli  p     weld  d          a hybridized planner for stochastic domains  in ijcai   
mausam    weld  d          solving concurrent markov decision processes  in aaai   
mausam    weld  d          concurrent probabilistic temporal planning  in icaps    pp     
    
  

fip lanning with d urative actions in s tochastic d omains

mausam    weld  d       a   challenges for temporal planning with uncertain durations  in
icaps   
mausam    weld  d       b   probabilistic temporal planning with uncertain durations  in
aaai   
meuleau  n   hauskrecht  m   kim  k  e   peshkin  l   kaelbling  l   dean  t     boutilier  c 
        solving very large weakly coupled markov decision processes  in aaai    pp 
       
musliner  d   murphy  d     shin  k          world modeling for the dynamic construction of
real time control plans  artificial intelligence            
nigenda  r  s     kambhampati  s          altalt p  online parallelization of plans with heuristic
state search  journal of artificial intelligence research             
penberthy  j     weld  d          temporal planning with continuous change  in aaai    p       
rohanimanesh  k     mahadevan  s          decision theoretic planning with concurrent temporally extended actions  in uai    pp         
singh  s     cohn  d          how to dynamically merge markov decision processes  in nips   
the mit press 
smith  d     weld  d          temporal graphplan with mutual exclusion reasoning  in ijcai   
pp         stockholm  sweden  san francisco  ca  morgan kaufmann 
vidal  v     geffner  h          branching and pruning  an optimal temporal pocl planner based
on constraint programming  aij                 
younes  h  l  s     simmons  r  g       a   policy generation for continuous time stochastic
domains with concurrency  in icaps    p      
younes  h  l  s     simmons  r  g       b   solving generalized semi markov decision processes
using continuous phase type distributions  in aaai    p      
zhang  w     dietterich  t  g          a reinforcement learning approach to job shop scheduling 
in ijcai    pp           

appendix a
proof of theorem  
we now prove the statement of theorem    i e   if all actions are tgp style then the set of pivots
suffices for optimal planning  in the proof make use of the fact that if all actions are tgp style then
a consistent execution of any concurrent plan requires that any two executing actions be non mutex
 refer to section   for an explanation on that   in particular  none of their effects conflict and a
precondition of one does not conflict with the effects of the another 
we prove our theorem by contradition  let us assume that for a problem each optimal solution
requires at least one action to start at a non pivot  let us consider one of those optimal plans  in
  

fim ausam   w eld

which the first non pivot point at which an action needs to start at a non pivot is minimized  let
us name this time point t and let the action that starts at that point be a  we now prove by a case
analysis that we may  as well  start a at time t    without changing the nature of the plan  if t   
is also a non pivot then we contradict the hypothesis that t is the minimum first non pivot point  if
t    is pivot then our hypothesis is contradicted because a does not  need to  start at a non pivot 
to prove that a can be left shifted by   unit  we take up one trajectory at a time  recall that
actions could have several durations  and consider all actions playing a role at t     t  t    a     
and t    a   where  a  refers to the duration of a in this trajectory  considering these points
suffice  since the system state does not change at any other points on the trajectory  we prove that
the execution of none of these actions is affected by this left shift  there are the following twelve
cases 
   actions b that start at t     b cant end at t  t is a non pivot   thus a and b execute
concurrently after t  implies a and b are non mutex  thus a and b may as well start together 
   actions b that continue execution at t     use the argument similar to case   above 
   actions b that end at t     because b is tgp style  its effects are realized in the open interval
ending at t     therefore  start of a does not conflict with the end of b 
   actions b that start at t  a and b start together and hence are not dependent on each other
for preconditions  also  they are non mutex  so their starting times can be shifted in any
direction 
   actions b that continue execution at t  if b was started at t    refer to case   above  if not  t
and t    are both similar points for b 
   actions b that end at t  case not possible due to the assumption that t is a non pivot 
   actions b that start at t    a      since a continued execution at this point  a and b are
non mutex  thus as effects do not clobber bs preconditions  hence  b can still be executed
after realizing as effects 
   actions b that continue execution at t    a      a and b are non mutex  so a may end
earlier without any effect of b 
   actions b that end at t    a      a and b were executing concurrently  thus they are
non mutex  so they may end together 
    actions b that start at t    a   b may still start at t    a   since the state of t    a 
doesnt change 
    actions b that continue execution at t    a   if b was started at t    a     refer to case
  above  else there is no state change at t    a  to cause any effect on b 
    actions b that end at t    a   a and b are non mutex because they were executing concurrently  thus  as effects dont clobber bs preconditions  hence  a may end earlier 
since a can be left shifted in all the trajectories  therefore the left shift is legal  also  if there
are multiple actions a that start at t they may each be shifted one by one using the same argument 
hence proved   
  

fi