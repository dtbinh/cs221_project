journal artificial intelligence research               

submitted        published      

using local alignments relation recognition
sophia katrenko
pieter adriaans
maarten van someren

s katrenko uva nl
p w adriaans uva nl
m w vansomeren uva nl

informatics institute  university amsterdam
science park          xg amsterdam  netherlands

abstract
paper discusses problem marrying structural similarity semantic relatedness information extraction text  aiming accurate recognition relations 
introduce local alignment kernels explore various possibilities using
task  give definition local alignment  la  kernel based smith waterman
score sequence similarity measure proceed range possibilities computing similarity elements sequences  show distributional similarity
measures obtained unlabeled data incorporated learning task semantic knowledge  experiments suggest la kernel yields promising results
various biomedical corpora outperforming two baselines large margin  additional
series experiments conducted data sets seven general relation types 
performance la kernel comparable current state of the art results 

   introduction
despite fact much work done automatic relation extraction  or recognition  past decades  remains popular research topic  main reason
keen interest relation recognition lies utility  concepts semantic relations
identified  used variety applications question answering
 qa   ontology construction  hypothesis generation others 
ontology construction  relation studied is a relation  or hypernymy   organizes concepts taxonomy  snow  jurafsky    ng         information retrieval  semantic relations used two ways  refine queries actual
retrieval  manipulate output returned search engine  e g  identifying
whether fragment text contains given relation not   widely used relations
query expansion hypernymy  or broader terms thesaurus  synonymy 
semantic relations useful different stages question answering 
taken account identifying type question considered actual answer extraction time  van der plas         yet another application
relations constructing new scientific hypothesis given evidence found text 
type knowledge discovery text often based co occurrence analysis and  many
cases  corroborated via experiments laboratories  swanson   smalheiser        
another reason extraction semantic relations interest lies diversity
relations  different relations need different extraction methods  many existing information
extraction systems originally designed work generic data  grishman   sundheim         became evident domain knowledge often necessary successful

c
    
ai access foundation  rights reserved 

fikatrenko  adriaans    van someren

extraction  instance  relation extraction biomedical domain would require
accurate recognition named entities gene names  clegg         area
food needs information relevant named entities toxic substances 
generic relations syntactic information often sufficient  consider 
instance  following sentences  with arguments relations written italics  
   

mary looked back whispered  know every tree forest  every scent 
 part whole relation 

   

person infected particular flu virus strain develops antibodies
virus   cause effect relation 

   

apples basket   content container relation 

sentences exemplify binary relations  namely part whole  tree part
forest   cause effect  virus causes flu  content container  apples contained
basket   easily notice syntactic context         same  namely 
arguments cases connected preposition in  however 
context highly ambiguous even though allows us reduce number
potential semantic relations  still sufficient able discriminate
part   whole content   container relation  words  world knowledge
trees  forests  apples baskets necessary classify relations correctly 
situation changes even drastically consider example      here  explicit
indication causation  nevertheless  knowing flu virus is 
able infer cause   effect relation holds 
examples              highlight several difficulties characterize semantic
relation extraction  generic relations often occur nominal complexes flu
virus     lack sentential context boosts approaches paraphrasing  nakov 
       however  even noun compounds one combine world knowledge
compounds context arrive correct interpretation 
computational approaches relation recognition problem often rely two step
procedure  first  relation arguments identified  depending relation hand 
step often involves named entity recognition arguments relations 
second step check whether relation holds  relation arguments provided  e g  
basket apples       relation extraction reduced second step  previous
work relation extraction suggests case accuracy relation recognition
much higher case discovered automatically  bunescu
et al          furthermore  existing solutions relation extraction  including work
presented paper  focus relation examples occur within single sentence
consider discourse  mcdonald         recognizing relations wider scope
interesting enterprise  would require take account anaphora resolution
types linguistic analysis 
approaches relation extraction based hand written patterns timeconsuming many cases need expert formulate test patterns  although
patterns often precise  usually produce poor recall  thomas et al         
general  hand written patterns two types  first type sequential based
 

fiusing local alignments relation recognition

frequently occurring sequences words sentence  hand written sequential patterns
initially used extraction hypernymy  hearst         several attempts
extend relations  second type patterns  khoo  chan    niu        take
syntactic structure sentence account  dependency structure sentence
usually represented tree patterns become subtrees  patterns
sometimes referred graphical patterns  identify examples cause effect
relation  khoo et al         applied type patterns texts medical domain 
study showed graphical patterns sensitive errors made parsers 
cover examples test data extract many spurious instances 
alternative using hand written patterns supervised machine learning  then 
relations labeled used train classifier recognize relations new
texts  one approach learn generalized extraction patterns patterns expressed
characters  words syntactic categories words  approaches involve clustering
based co occurrence  davidov   rappoport         recent years kernel based methods
become popular handle high dimensional problems  zelenko et al  
      bunescu   mooney        airola et al          methods transform text fragments  complete sentences segments around named entitites verbs  vectors 
apply support vector machines classify new fragments 
machine learning methods use prior knowledge given system
addition labeled examples  scholkopf        p       use prior knowledge often
motivated by  example  poor quality data data sparseness  prior knowledge
used many ways  changing representation existing training examples adding
examples unlabelled data  nlp tasks  prior knowledge exists form
manually  or automatically  constructed ontologies large collections unannotated data 
enrich textual data thereby improve recognition relations  sekimizu 
park    tsujii        tribble   fahlman         recently  zhang et al         showed
semantic correlation words learned unlabelled text collections  transferred
among documents used improve document classification  general 
use large collections text allows us derive almost information needed  done
varying accuracy  contrast  existing resources created humans provide
precise information  less likely cover possible areas interest 
paper  work bunescu mooney         use syntactic
structure sentences  particular  dependency paths  stems observation
linguistic units organized complex structures understanding words
word senses relate often requires contextual information  relation extraction
viewed supervised classification problem  training set consists examples
given relation goal construct model applied new  unseen
data set  recognize instances given relation new data set  recognition
relations use kernel based classifier applied dependency paths  however 
instead vector based kernel directly use similarity dependency paths
show information existing ontologies large text corpora employed 
paper organized follows  start reviewing existing kernel methods
work sequences  section     section    give definition local alignment kernel
based smith waterman measure  proceed discussing used
context natural language processing  nlp  tasks  particularly extracting
 

fikatrenko  adriaans    van someren

relations text  section       method described  report two types
data sets  biomedical generic  used experiments  section    elaborate
experiments  sections       section   discusses findings detail 
section   concludes paper discussing possible future directions 

   kernel methods
past years witnessed boost interest kernel methods  theoretical analysis
practical applications various fields  burges        shawe taylor   christianini 
       idea method works different structures representations 
starting simplest representation using limited number attributes complex
structures trees  seems indeed attractive 
define kernel function  recall standard setting supervised classification  training set n objects  instances   x    y              xn   yn   x            xn x
input examples input space x corresponding labels y            yn       
goal infer function h   x        approximates target function t 
however  h still err data reflected loss function  l h xi    yi   
several loss functions proposed literature far  best known
zero one loss  loss function outputs   time method errs
data point  h xi      yi      otherwise 
key idea kernel methods lies implicit mapping objects highdimensional space  by using mapping function   considering inner product
 similarity  k xi   xj       xi     xj      rather representing explicitly  functions used kernel methods
symmetric positive semi definite 
pbe
n pn
whereby positive semi definiteness defined i   j   ci cj k xi   xj     n     
objects x            xn x   choice real numbers c            cn r  function
positive semi definite  algorithm may find global optimal solution 
requirements w r t  symmetry positive semi definiteness met  kernel called
valid 
using idea kernel mapping  cortes vapnik        introduced support vector
machines  svm  method seeks linear separation two classes
input points function f  x  f  x    wt  x    b  wt rp   b r
h x    sign f  x    here  wt stands slope linear function b
offset  often  exist several functions separate data well 
equally good  hyperplane separates mapped examples largest possible
margin would best option  vapnik        
svms solve following optimization problem 
n

x
 
k w k   c
l h xi    yi  
f  x  wt x b  
argmin

   

i  

equation    first part equation corresponds margin maximization
 by minimizing    k w k     second takes account error training
set minimized  where c penalty term   hyperplane found
may correspond non linear boundary original input space  exist number
 

fiusing local alignments relation recognition

standard kernels linear kernel  gaussian kernel others  information
data problem motivate choice particular kernel 
shown haussler        complex kernel  referred convolution kernel  
defined using simpler kernels 
forms machine learning representations using prior knowledge defined
along methods exploiting it  inductive logic programming offers one possible
solution use explicitly  form additional horn clauses  camacho        
bayesian learning paradigm information hypothesis without seeing data encoded bayesian prior  mitchell        higher level distribution hierarchical
bayesian setting  less obvious though represent use prior knowledge
learning frameworks  case svms  three possible ways incorporating
prior knowledge  lauer   bloch         named sampling methods  prior knowledge used generate new data   kernel methods  prior knowledge incorporated
kernel function by  instance  creating new kernel   optimization methods
 prior knowledge used reformulate optimization problem by  example  adding
additional constraints   choice kernel based general statistical properties
domain  attractive possibility incorporate explicit domain knowledge
kernel  improve kernel smoothing space  instances
similar higher probability belonging class kernel without
prior knowledge 
follows  review number kernels strings proposed
research community past years  natural domain look
biomedical field many problems formulated string classification
 protein classification amino acid sequences  name few   sequence representation
is  however  applicable biomedical area  considered
many natural language processing tasks  introducing kernels used
biomedicine  move nlp domain present recent work relation extraction
employing kernel methods 
    spectrum kernel
leslie  eskin  noble        proposed discriminative approach protein classification 
sequence x x   authors define m spectrum set contiguous
subsequences x whose length equal m  possible m long subsequences q
indexed frequency occurrence  q  x    consequently  feature map
sequence x alphabet equals  x     q  x  qam   spectrum kernel two
sequences x defined inner product corresponding feature maps 
ks  x  y      x    y    
now  even assuming contiguous subsequences small m  feature space consider
large  authors propose detect subsequences length using suffix
tree method guarantees fast computation kernel matrix  spectrum kernel
tested task protein homology detection  best results achieved
setting relatively small number      novelty leslie et al s        method
lies generality low computational complexity 

 

fikatrenko  adriaans    van someren

    mismatch kernels
mismatch kernel introduced later leslie et al         essentially extension latter  obvious limitation spectrum kernel considered
subsequences contiguous match exactly  mismatch kernel contiguity preserved match criterion changed  words  instead looking
possible subsequences length given subsequence  one searching
possible subsequences length allowing r mismatches  comparison
result larger subset subsequences  kernels defined way still calculated rather fast  kernel formulated similarly spectrum kernel
major difference computing feature map
p sequences  precisely  feature
map sequence x defined m r  x    qs m r  q  m r  q       q  am  
 q  binary indicates whether sequence belongs set m length sequences
differ q r elements          clear r set
   mismatch kernel reduced spectrum kernel  complexity mismatch
kernel computation linear respect sum sequence lengths 
authors show mismatch kernel yields state of the art performance protein classification task outputs subsequences informative
biological point view 
    kernel methods nlp
one merits kernel methods possibility designing kernels different structures  strings trees  nlp field  and relation extraction  particular 
work roughly falls two categories  first  kernels defined plain
text using sequences words  second uses linguistic structures dependency
paths trees output shallow parsing  short review take
chronological perspective rather start methods based sequences
proceed approaches make use syntactic information 
year spectrum kernel designed  lodhi et al         introduced string subsequence kernels provide flexible means work text data 
particular  subsequences necessarily contiguous weighted according
length  using decay factor    length subsequences fixed advance 
authors claim even without use linguistic information kernels
able capture semantic information  reflected better performance
text classification task compared bag of words approach  lodhi et al s       
kernel works sequences characters  kernel proposed cancedda et al        
applied word sequences  string kernels extended syllable kernels
proved well text categorization  saunders  tschach    shawe taylor        
kernels defined recursively  computation efficient 
instance  time complexity lodhi et al s        kernel o n s  t    n
length subsequence  documents 
      subsequence kernels
recognition binary relations  natural way consider words located
around relation arguments  approach taken bunescu mooney
 

fiusing local alignments relation recognition

     b  whose choice sequences motivated textual patterns found corpora 
instance  observed relations expressed subject verb object constructions others part noun prepositional phrases  result  three types
sequences considered  fore between  words two named entities  
 words two entities  between after  words two
entities   length sequences restricted  handle data sparseness  authors
generalize existing sequences using pos tags  entity types wordnet synsets 
generalized subsequence kernel recursively defined number weighted sparse subsequences two sequences share  absence syntactic information  assumption
made long subsequences likely represent positive examples
penalized  subsequence kernel computed three types sequences
resulting relation kernel defined sum three subkernels  experimental results
biomedical corpus encouraging  showing relation kernel performs better
manually written patterns approach based longest common subsequences 
method proposed giuliano et al         largely inspired work bunescu
mooney      b   however  instead looking subsequences three types sequences  authors treat bag of words define called global kernel
follows  first  sequence type  pattern  p represented vector whose elements
counts many times token used p   local kernel defined similarly
using words surrounding named entities  left right context   final shallow
linguistic kernel defined combination global local kernels  experiments biomedical corpora suggest kernel outperforms subsequence kernel
bunescu mooney 
      distributional kernels
recently  seaghdha copestake        introduced distributional kernels co  occurrence probability distributions  co occurrence statistics use form
either syntactic relations n grams  show possible derive kernels
distances jensen shannon divergence  jsd  euclidean distance  l     lee        
jsd smoothed version kullback leibler divergence  information theoretic measure dissimilarity two probability distributions  main motivation behind
approach lies fact distributional similarity measures proved useful
nlp tasks  extract co occurrence information  authors use two corpora  british
national corpus  bnc  web  t   gram corpus  which contains   grams
observed frequency counts collected web   distributional kernels
proved successful number tasks compound interpretation  relation
extraction verb classification  them  jsd kernel clearly outperforms
gaussian linear kernels  moreover  estimating distributional similarity bnc
corpus yields performance similar results obtained web  t   gram corpus 
interesting finding bnc corpus used estimate similarity
syntactic relations whereas latter corpus contains n grams only  importantly 
method seaghdha copestake provides empirical support claim using
distributional similarity beneficial relation extraction 

 

fikatrenko  adriaans    van someren

      kernels syntactic structures
kernels defined unpreprocessed text data seem attractive applied
directly text language  however  general are  lose precision compared methods use syntactic analysis  re ranking parsing
trees  collins   duffy        one first applications kernel methods nlp
problems  accomplish goal  authors rely subtrees pair trees
common  later on  moschitti        explored convolution kernels dependency
constituency structures semantic role labeling question classification  work
introduces novel kernel called partial tree kernel  pt   essentially built
two kernels proposed before  subtree kernel  st  contains descendant nodes
target root  including leaves  subset tree kernel  sst  flexible
allows internal subtrees necessarily encompass leaves  partial tree
generalization subset tree whereby partial structures grammar allowed  i e  
parts production rules  vp  v   form valid pt   moschitti demonstrated
pts obtain better performance dependency structures ssts  latter
yield better results constituent trees 
      kernel shallow parsing output
zelenko et al         use shallow parsing designed kernels extract relations text 
contrast full parsing  shallow parsing produces partial interpretations sentences 
node tree enriched information roles  that correspond
arguments relation   similarity two trees determined similarity
nodes  depending similarity computed  zelenko et al  define two types
kernels  contiguous subtree kernels sparse kernels  types tested two types
relations  person affiliation organization location exhibiting good performance 
particular  sparse kernels outperform contiguous subtree kernels leading conclusion
partial matching important dealing typically sparse natural language
data  however  computation sparse kernel takes o mn    time  where n
number children two relation examples  i e  shallow trees  consideration 
n   algorithm contiguous subtree kernel runs time o mn  
      shortest path kernel
bunescu mooneys      a  shortest path kernel represents yet another approach
relation extraction kernel based relies information found dependency trees 
main assumption entire dependency structure relevant  one
focus path connecting two relation arguments instead  similar
paths are  likely two relation examples belong category  spirit
previous work  bunescu mooney seek generalizations existing paths
adding information sources part speech  pos  categories named entity types 
shortest path relation arguments extracted kernel two
sequences  paths  x    x            xn   x     x             x m   computed follows 

 

fiusing local alignments relation recognition

 

kb  x  x    



 q
n

 
i   f  xi   xi  

   n
m n

   

equation    f  xi   x i   number features shared xi x i   bunescu
mooney      a  use several features word  e g   protesters   part speech tag  e g  
n n s   generalized part speech tag  e g   n oun   entity type  e g   p erson  
applicable  addition  direction feature     employed  reproduce
example paper 
example   given two dependency paths exemplify relation located
actions brcko arrival beijing  paths expanded
additional features mentioned above  easy see comparing path    
path     gives us score                   





brcko



actions
nnp

p rp
   n n   
  
n oun

p erson
n oun
locat ion













   


beijing

nnp

  

n oun
locat ion

   





arrival

   n n
  
p rp
n oun
p erson








time complexity shortest path kernel o n   n stands length
dependency path 
dependency paths considered recent work relation recognition  erkan 
ozgur    radev         here  erkan et al         use dependency paths input
compare means cosine similarity edit distance  authors motivate
choice need compare dependency paths different length  further  various machine learning methods used classification  including svm transuctive svm
 tsvm   extension svm  joachims         particular  tsvm makes use
labeled unlabeled data first classifying unlabeled examples searching
maximum margin separates positive negative instances sets 
authors conclude edit distance performs better cosine similarity measure 
tsvm slightly outperforms svm 
airola et al         propose graph kernel makes use entire dependency
structure  work  sentence represented two subgraphs  one
built dependency analysis  corresponds linear structure
sentence  further  kernel defined paths two vertices graph 
method airola et al         achieves state of the art performance biomedical
data sets  discussed  together shortest path kernel work

 

fikatrenko  adriaans    van someren

erkan et al          section   relation extraction biomedical domain
paper 
finally  kernels defined graphs syntactic structures 
graphs semantic network  illustrated seaghdha         uses graph
kernels graph built hyponymy relations wordnet  even though
syntactic information utilized  kernels proved perform well extraction
various generic relations 
kernels reviewed section deal sequences trees albeit different ways  empirical findings suggest kernels allow partial matching usually
perform better compared methods similarity defined exact match 
alleviate problem exact matching  researchers suggested generalizing
elements existing structures  bunescu   mooney      a  others opted flexible
comparison  view  types methods complement  saunders
et al          flexible partial matching methods are  may suffer low precision penalization mismatch low  holds approaches use
generalization strategies may easily overgeneralize  possible solution would
combine both  provided mismatches penalized well generalizations
semantically plausible rather based part speech categories  idea
explored present paper evaluated relation recognition task 
nutshell  goals paper following   i  study possibilities
using local alignment kernel relation extraction text   ii  exploration
use prior knowledge alignment kernel  iii  extensive evaluation
automatic recognition two types relations  biomedical generic 

   local alignment kernel
one note short overview kernels designed nlp many
researchers use partial structures propose variants subsequence kernels  bunescu
  mooney      b   partial tree kernel  moschitti         kernel shallow parsing
output  zelenko et al         relation extraction  paper focus dependency
paths input formulate following requirements kernel function 
allow partial matching similarity measured paths
different length
possible incorporate prior knowledge
recall prior knowledge mean information comes either larger corpora existing resources ontologies  instance  knowing development
synonymous evolution contexts help recognize two different words
close semantically  information especially useful meaning relevant
detecting relations may differ form 
following subsection define local alignment kernel satisfies
requirements show incorporate prior knowledge 

  

fiusing local alignments relation recognition

    smith waterman measure local alignments
work motivated recent advances biomedical field  shown
possible design valid kernels based similarity measure strings  saigo 
vert    akutsu         example  saigo  vert  ueda  akutsu        consider
smith waterman  sw  similarity measure  smith   waterman         see below  measure similarity two sequences amino acids 
string distance measures divided measures based terms  edit distance
hidden markov models  hmm   cohen  ravikumar    fienberg         term based
distances measures based tf idf score  consider pair word sequences
two sets words ignoring order  contrast  string edit distances  or string similarity
measures  treat entire sequences compare using transformation operations 
convert sequence x sequence x    examples levenshtein distance 
needleman wunsch  needleman   wunsch        smith waterman  smith
  waterman        measures  levenshtein distance used natural
language processing field component variety tasks  including semantic role
labeling  sang et al          construction paraphrase corpora  dolan  quirk    brockett 
       evaluation machine translation output  leusch  ueffing    ney         others 
smith waterman measure mostly used biological domain  are  however 
applications modified smith waterman measure text data well  monge  
elkan        cohen et al          hmm based measures present probabilistic extensions
edit distances  smith  yeganova    wilbur        
hypothesis string similarity measures best basis kernel
relation extraction  case  order words appear likely relevant
sparse data usually prevents estimation probabilities  as work smith et al  
       general  two sequences aligned several possible ways  possible
search either alignment spans entire sequences  global alignment  
alignment based similar subsequences  local alignment   case
sequences amino acids relation extraction  local patterns likely
important factor determines similarity  therefore need similarity measure
emphasizes local alignments 
formally  define pairwise alignment l elements two sequences
x   x  x        xn x    x   x         x m   pairing    l  i  j    l              l    n 
  j m    l n    l m  example    ii   third element first sequence
aligned first element second one  denoted          
example   given sequences x abacde x   ace  two possible alignments  with gaps
indicated    follows 
 i  global alignment



b
 


 

c
c


 

e
e

alignment 

                                




c
c


 

e
e

alignment 

                                

 ii  local alignment

 

b
 

  

fikatrenko  adriaans    van someren

example  number gaps inserted x  align x number
elements match cases  yet  biological
linguistic context may prefer alignment  ii   closely matching substrings 
local alignments  better indicator similarity shared items far apart 
is  therefore  better use measure puts less weight gaps
start end strings  as example    ii    done using local
alignment mechanism searches similar subsequences two sequences 
local alignments employed sequences dissimilar different length 
global alignments considered sequences roughly length 
measures mentioned above  smith waterman measure local alignment
measure  needleman wunsch measure compares two sequences based global
alignments 
definition    global alignment  given two sequences x   x        xn x    x         x m  
global alignment pair sequences y  length 
obtained inserting zero gaps first element either x x   
element x x   
definition    local alignment  given two sequences x   x        xn x    x         x m  
local alignment pair subsequences x x    whose similarity
maximal 
clarify mean local global alignments  give definition
smith waterman needleman wunsch measures  given two sequences x   x  x        xn
x    x   x         x m length n respectively  smith waterman measure defined
similarity score best local alignment 

sw x  x     

max

a x x   

s x  x     

   

equation above  s x  x      score local alignment sequence x x 
denotes set possible alignments  best local alignment efficiently
found using dynamic programming  this  one fills matrix sw partial
alignments follows 

 



sw i    j      d xi   x j  
sw  i  j    max
 in 
sw i    j  g


 jm
sw i  j    g

   

equation    d xi   x j   denotes substitution score two elements xi x j
g stands gap penalty  using equation possible find partial alignments 
stored matrix cell  i  j  reflects score alignment x        xi

  

fiusing local alignments relation recognition


c
e

 
 
 
 


 
 
 
 

b
 
 
 
 


 
 
 
 

c
 
 
 
 


 
 
 
 

e
 
 
 
 


c
e

 a  smith waterman measure

 
 
 
 


 
 
 
 

b
 
 
 
 


 
 
 
 

c
 
  
 
 


 
  
 
 

e
 
  
 
 

 b  needleman wunsch measure

table    matrices computing smith waterman needleman wunsch scores sequences x abacde x   ace  gap g      substitution score d xi   x j      
xi   x j   d xi   x j       xi    x j  

x         x j   cell largest value matrix contains smith waterman
score 
needleman wunsch measure  searches global alignments  defined similarly  except fact cells matrix contain negative scores 

nw i    j      d xi   x j  
nw  i  j    max
nw i    j  g
 in 

nw i  j    g
 jm

    

smith waterman measure seen modification needleman wunsch
method  disallowing negative scores matrix  regions high dissimilarity
avoided and  result  local alignments preferred  moreover  needlemanwunsch score equals largest value last column last row  smith waterman
similarity score corresponds largest value matrix 
let us reconsider example   show global local alignments alignments
two sequences x abacde x   ace obtained  arrive actual alignments  one
set gap parameter g substitution scores  assume use following
settings  gap g      substitution score d xi   x j       xi   x j   d xi   x j      
xi    x j   values chosen illustrative purpose only  realistic
case  e g   alignment protein sequences  choice substitution scores usually
motivated biological evidence  gapping  smith waterman        suggested
use gap value least equal difference match  d xi   x j   
xi   x j   mismatch  d xi   x j    xi    x j    then  smith waterman needlemanwunsch similarity scores x x  calculated according equation  
equation    given table   
first  first row first column matrix initialized    then 
matrix filled computing maximum score cell defined equation  
equation     score best local alignment equal largest element
  

fikatrenko  adriaans    van someren

matrix      needleman wunsch score    note possible trace back
steps taken arrive final alignment  the cells boldface   left right
step corresponds insertion  top down step deletion  these lead gaps  
diagonal step implies alignment two sequences elements 
since prefer use local alignments dependency paths  natural choice would
use smith waterman measure kernel function  however  saigo et al        
observed smith waterman measure may result valid kernel
may positive semi definite  give definition la kernel  states
two sequences similar many local alignments high scores 
equation    
kl  x  x     

x

 

es x x   

    

a x x   

here  s x  x      local alignment score      scaling parameter 
define la kernel kl  as equation     two sequences x x    needed
take account transformation operations used local alignments  first  one
define kernel elements corresponds individual alignments  ka   second 
since type alignment allows gaps  another kernel gapping  kg   last
least  recall local alignments parts sequences may aligned 
elements x x  may left out  elements influence alignment
score kernel used cases  k    set constant  k   x  x         finally 
la kernel composition several kernels  k    ka   kg    spirit
convolution kernels  haussler        
according saigo et al          similarity aligned sequences elements  ka kernel 
defined follows 

 
 x   
     x     
   
 
ka  x  x    
    
  
d x x
e
otherwise
either x  x  one element  kernel would result    otherwise 
calculated using substitution score d x  x    x x    score reflects
similar two sequences elements and  depending domain  computed using
prior knowledge given domain 
gapping kernel defined similarly alignment kernel equation     whereby
scaling parameter preserved  gap penalties used instead similarity
function two elements 
 

kg  x  x      e g  x   g  x    

    

here  g stands gap function  naturally  gap length   function returns
zero  gaps length n  reasonable define gap terms gap opening
gap extension e  g n      e  n     case possible decide whether longer
gaps penalized shorter ones  much  instance 
  

fiusing local alignments relation recognition

three consecutive gaps alignment  first gap counted gap opening 
two gap extension  consecutive gaps  i e   gaps length n      gap
equal importance  gap opening equal gap extension  if  however 
length gaps matter  one would prefer penalize gap opening more 
give little weight gap extension 
kernels combined follows 
k r   x  x      k   ka kg  r  ka k 

    

equation     k r   x  x    stands alignment r elements x x  possibly
r   gaps  similarity aligned elements calculated ka   gapping kg   since
could r   gaps  corresponds following part equation 
 ka kg  r    further  rth aligned element  one ka added  given
discussion above  k  added initial final part  follows equation    
elements x x  aligned  k r  equals k       elements x
x  aligned gaps  value k r   ka  r  
finally  la kernel equal sum taken possible local alignments
sequences x x   

 

kl  x  x    


x

k i   x  x   

    

i  

results biological domain suggest kernels based smith waterman
distance relevant comparison amino acids string kernels  saigo et al  
       clear whether holds applied natural language processing tasks 
view  could depend parameters used  substitution
matrix penalty gaps 
      computational complexity
la kernel  many kernels discussed section    efficiently calculated using dynamic programming  two sequences x x    length n respectively 
complexity proportional n m  additional costs may come substitution matrix  which  unlike biomedical domain  become large  however 
look up substitution scores done efficient manner well  leads
fast kernel computation  instance  calculating kernel matrix largest data
set used paper  aimed        instances   takes     seconds      ghz intel r 
core tm   machine 
    designing local alignment kernel relation extraction
smith waterman measure based transformations  particular deletions elements different strings  however  elements different may still
similar degree  similarities used part similarity measure 
example  two elements words different synonyms 
count less different completely unrelated  call
  

fikatrenko  adriaans    van someren

similarities substitution scores  equation     define two different ways 
basis distributional similarity basis semantic relatedness ontology 
example   would able infer brcko similar beijing  even
though two words match exactly  furthermore  phrases arrival
beijing arrival january  would kernel say brcko
similar beijing january  use information prior knowledge
makes possible measure similarity two words  one test set
training set  even match exactly  review two types
measures based statistical distributions relatedness wordnet 
      distributional similarity measures
number distributional similarity measures proposed years  including
cosine  dice jaccard coefficients  distributional similarity measures extensively studied  lee        weeds  weir    mccarthy         main hypothesis
behind distributional measures words occurring context
similar meaning  firth         context defined either using proximity text 
employing grammatical relations  paper  use first option context
sequence words text length set advance 
measure

formula

cosine

d xi   x j     p

dice

d xi   x j    

l 

d xi   x j    

p  c xi  p  c x j  
p
   
 
c p  c xi  
c p  c xj  

p

c

 f  xi  f  x j  
f  xi  f  x j  

qp

c  p  c xi  

p  c x j    

table    list functions used estimate distributional similarity measures 
chosen following measures  dice  cosine l   euclidean  whose definitions given table    definition cosine l   possible use either
frequency counts probability estimates derived unsmoothed relative frequencies 
here  adopt definitions given lee         based probability estimates p   recall x x  two sequences would wish compare 
corresponding elements xi x j   further  c stands context  definition
dice coefficient  f  xi      c   p  c xi         mainly interested symmetric measures
 d xi   x j     d x j   xi    symmetric positive semi definite matrix required kernel methods  euclidean measure defined table   necessarily vary  
   reason  given list pairs words  xi   x j   xi fixed j             
corresponding l  score  maximum value maxj d xi   x j   detected used
normalize scores list  furthermore  unlike dice cosine  return  
case two words equal  euclidean score equals    next step  substract
obtained normalized value   ascertain scores within interval       
  

fiusing local alignments relation recognition

largest value     assigned identical words  view  procedure
make comparison selected distributional similarity measures respect
influence la kernel transparent 
distributional similarity measures suitable information available 
case data annotated means taxonomy  e g   wordnet  
possible consider measures defined taxonomy  availability hand crafted
resources  wordnet  comprise various relations concepts  enables
making distinctions different concepts subtle way 
      wordnet relatedness measures
generic relations  commonly used resource wordnet  fellbaum        
lexical database english  wordnet  words grouped together synsets
synset consists list synonymous words collocations  e g   fountain pen  
pointers describe relations synset synsets  fellbaum        
wordnet employed different purposes studying semantic constraints
certain relation types  girju  badulescu    moldovan        katrenko   adriaans        
enriching training set  giuliano et al         nulty        
compare two concepts given synsets c  c  use five different measures
proposed past years  rely notions length
shortest path two concepts c  c    len c    c     depth node
wordnet hierarchy  which equal length path root given
synset ci    dep ci    least common subsumer  or lowest super ordinate  c 
c    lcs c    c     turn synset  measures exclusively based
notions belong conceptual similarity proposed palmer wu         simwup
equation     formula scaled semantic similarity introduced leacock
chodorow         simlch equation        major difference lies
fact simlch consider least common subsumer c  c  uses
maximum depth wordnet hierarchy instead  conceptual similarity ignores
focuses subhierarchy includes synsets 

simwup  c    c     

  dep lcs c    c    
len c    lcs c    c       len c    lcs c    c         dep lcs c    c    

simlch  c    c      log

len c    c   
  maxcw ordn et dep c 

    

    

aiming combining information several sources  resnik        introduced yet another measure grounded information content  simres equation      intuitively 
two synsets c  c  located deeper hierarchy path one synset
another short  similar  path two synsets long
least common subsumer placed relatively close root  indicates synsets
   equations similarity measures defined wordnet  subscripts refer similarity measure
 e g   lch  wup simlch simwup   respectively 

  

fikatrenko  adriaans    van someren

c  c  much common  quantify intuition  necessary derive
probability estimate lcs c    c    done employing existing corpora 
precisely  p lcs c    c     stands probability encountering instance concept
lcs c    c    
simres  c    c      log p lcs c    c    

    

one biggest shortcomings resniks method fact least
common subsumer appears equation     one easily imagine full blown hierarchy
relatedness concepts subsumed lcs ci   cj   heavily vary 
words  using lcs only  one able make subtle distinctions two
pairs concepts share least common subsumer  overcome this  jiang
conrath        proposed solution takes account information synsets
compared  simjcn equation      comparing equation    equation    
notice equation incorporates probability encountering
lcs c    c     probability estimates c  c   
simjcn  c    c        log p lcs c    c      log p c      log p c    

    

lin        defined similarity two concepts using much commonality
differences involved  similarly two previous approaches  uses
information theoretic notions derives similarity measure simlin given equation    

simlin  c    c     

  log p lcs c    c    
log p c      log p c   

    

past  semantic relatedness measures evaluated different nlp tasks  budanitsky   hirst        ponzetto   strube        concluded measure
performs best problems  evaluation  use semantic relatedness
validation generic relations study depth contribute final results 
      substitution matrix relation extraction
now  discussed two possible ways calculating substitution score d     
using either distributional similarity measures  measures defined wordnet  however 
dependency paths generated parsers may contain words  or lemmata  
syntactic functions subjects  objects  modifiers  others  take
account  revise definition d      assume sequences x   x  x        xn
x    x   x         x m contain words  xi w w refers set words  syntactic
functions accompanied direction  xi
  w    elements w unique words  or
lemmata  found dependency paths  instance  paths
actions brcko arrival beijing example     section       
w    his  actions  in  brcko  arrival  beijing   dependency paths use present
work include information syntactic functions  instance awareness
joy  case  w    awareness  come  joy  w    
  

prep f rom

prep f rom nsubj



    



nsubj

come

fiusing local alignments relation recognition

then 

d xi   x j  




 
 
d   xi   x j    


 



 

xi   x j w
xi   x j
  w   xi   x j
 
xi   xj
  w   xi    x j
xi w   x j
 w
xi
  w   x j w

    

equation      states whenever element xi sequence x compared
element x j sequence x    substitution score equal either  i  similarity
score case elements words  lemmata    ii     elements
syntactic function   iii     case 
follows discussion similarity measures above  two ways define
d xi   x j    using either distributional similarity xi x j  section        
wordnet similarity  provided annotated wordnet synsets  section        

   experimental set up
section  describe data sets used experiments provide
information data collections used estimating distributional similarity 
    data
evaluate performance la kernel  consider two types text data  domainspecific data  comes biomedical domain generic domain independent
data represents variety well known widely used relations partwhole cause effect 
work  extract dependency path two nodes corresponding
arguments binary relation  assume analysis results tree since
acyclic graph  exists unique path pair nodes 
consider  however  structures might derived full syntactic analysis
in  example  subtrees  moschitti        
      biomedical relations
corpora use three corpora come biomedical field contain annotations either interacting proteins   bc ppi         sentences   aimed  bunescu   mooney 
    b  interactions among proteins genes lll     sentences training set
   test set  nedellec         bc ppi corpus created sampling sentences biocreative challenge  aimed corpus sampled medline
collection  lll corpus composed querying medline term bacillus subtilis  difference among three corpora lies directionality interactions 
table   shows  relations aimed corpus strictly symmetric  lll asymmetric bc ppi contains types  differences number training instances
aimed corpus explained fact correspond dependency
   available http   www  informatik hu berlin de  hakenber  

  

fikatrenko  adriaans    van someren

paths named entities  parsing fails produces several disconnected graphs per
sentence  dependency path extracted 
parser
linkparser
linkparser
stanford
stanford
enju

data set
lll  train 
lll  test 
bc ppi
aimed
aimed

 examples
   
   
   
    
    

 pos
   
  
   
   
   

direction
asymmetric
asymmetric
mixed
symmetric
symmetric

a  even though actual annotations test data given  number interactions
test data set provided lll organizers 

table    statistics biomedical data sets lll  bc ppi  aimedd  table   pos
stands number positive examples per data set  examples indicates
number examples total 

goal relation extraction three cases output correct interactions
biomedical entities  genes proteins  found input data 
biomedical entities already provided  need named entity recognition 
discrepancy training test sets used lll challenge 
unlike training set  sentence example least one interaction 
test set contains sentences interaction  organizers lll challenge distinguish sentences without coreferences  sentences coreferences
usually appositions  shown one examples below  first sentence        
example sentence without coreferences  with interaction ykud sigk  
whereas second one sentence coreference  with interaction spoiva
sigmae   precisely  spoiva refers phrase one genes
known interact sigmae  therefore infer spoiva interacts sigmae  sentences without coreferences form subset  refer lll nocoref 
sentences coreferences part separate subset lll coref 
     ykud transcribed sigk rna polymerase t  sporulation 
     finally  show proper localization spoiva required expression one
genes which  spoiva  control mother cell
transcription factor sigmae 
assumed relations sentences coreferences harder recognize  show la kernel performs subsets  report experimental results full set test data  lll all   subsets  lll coref lll nocoref  
syntactic analysis analyzed bc ppi corpus stanford parser  lll
corpus already preprocessed linkparser output checked
experts  enable comparison previous work  used aimed corpus parsed

  

fiusing local alignments relation recognition

stanford parser   enju parser    which exactly correspond input
experiments erkan et al        stre et al          unlike stanford parser 
enju based head driven phrase structure grammar  hpsg   output
enju parser presented two ways  either predicate argument structure
phrase structure tree  predicate argument structures describe relations words
sentence  phrase structure presents sentence structure form clauses
phrases  addition  enju trained genia corpus includes model
parsing biomedical texts 
     cbf  contains three proteins  cbf a  cbf b cbf c 

contains

dobj

nsubj

proteins

cbf 

num

conj

conj conj

three

cbf a
nsubj

cbf b
dobj

cbf  contains proteins
nsubj
dobj
cbf  contains proteins
nsubj
dobj
cbf  contains proteins

cbf b

conj

cbf a
cbf b
conj
cbf c
conj

figure    stanford parser output representation example      
figure   shows dependency tree obtained stanford parser sentence
      sentence mentions three interactions among proteins  precisely 
cbf  cbf a  cbf  cbf b  cbf  cbf c  three dependency
paths contain words  lemmata  syntactic functions  such subj subject  plus
direction traversing tree  figure   presents output sentence provided
enju parser  upper part refers phrase structure tree lower part
shows paths extracted predicate argument structure  two parsers clearly
differ output  first  stanford parser conveniently generates paths
three interaction pairs enju analyzer not  second  output
stanford parser excludes prepositions conjunctions attached syntactic
functions whereas enju analyzer lists parsing results  differences
   available http   nlp stanford edu software lex parser shtml 
   available http   www tsujii is s u tokyo ac jp enju  

  

fikatrenko  adriaans    van someren

lead different input sequences later fed la kernel  consequently 
variations input may translate differences final performance 

cbf 
cbf 
cbf 

arg  verb



arg  verb



arg  verb



contain
contain
contain

arg  verb



arg  verb



arg  verb



protein
protein
protein

arg  app



arg  app



arg  app



 
 
 

arg  app



arg  app



arg  app



cbf a
cbf a
cbf a

arg  coord



arg  coord



 

arg  coord





cbf b

arg  coord



cbf c

figure    enjus output representation example      
addition  work employing aimed  dependency paths
figure   figure   preprocessed following way  actual named entities
arguments relation replaced label  e g  protein  consequently 
nsubj

dobj

conj

first path figure   becomes protein contains proteins protein 
able compare results aimed performance reported work
erkan et al         stre et al          use exactly dependency paths
argument labels  however  study whether using labels instead actual named entities
impact final results lll data set  carry two experiments 
first one  dependency paths contain named entities  whereas second contain
labels  second experiment referred adding word label name  as
lll all label table    
      generic relations
second type relations consider generic relations  arguments
sometimes annotated using external resources wordnet  makes possible
use semantic relatedness measures defined them  example approach

  

fiusing local alignments relation recognition

data used semeval      challenge  task     classification semantic relations
nominals  girju et al         
goal task   classify seven semantic relations  cause   effect  instrument   agency  product   producer  origin   entity  theme   tool  part whole content   container   whose examples collected web using
predefined queries  words  given set examples relation  expected output would binary classification whether example belongs given
relation not  arguments relation annotated synsets wordnet
hierarchy  figure    given sentence pair  spiritual awareness  joy 
corresponding synsets joy         awareness          would mean classifier decide whether pair example cause effect relation 
particular sentence retrieved quering web phrase joy comes   
synsets manually selected wordnet hierarchy  seven semantic
relations used challenge  gives seven binary classification problems 

genuine  e  joy  e   comes  e  spiritual awareness  e   life absolute clarity direction  living purpose 
wordnet e     joy          wordnet e     awareness         
query  joy comes    cause effect e   e     true

figure    annotated example cause   effect semeval       task  
training data set 

relation type
origin   entity
product   producer
theme   tool
instrument   agency
part   whole
content   container
cause   effect

 examples  train 
   
   
   
   
   
   
   

 pos  train 
  
  
  
  
  
  
  

 examples  test 
  
  
  
  
  
  
  

direction
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric

table    distribution semeval       task   examples  training test data  
 pos stands number positive examples per data set  examples
indicates number examples total 

syntactic analysis generate dependency paths  seven data sets used semeval       task    analyzed stanford parser  dependency path sentence
figure   given      
  

fikatrenko  adriaans    van someren

     awareness n  

prep f rom



nsubj

come joy n  

here  words annotated wordnet pos tag attached  followed sense 
instance  awareness noun current context first sense used 
corresponds awareness n   
    substitution matrix
build substitution matrix la kernel  use either distributional similarity
wordnet semantic relatedness measures  data set dependency paths 
contains unique elements  words syntactic functions   size matrix t 
k elements words  number substitution scores computed
distributional similarity  or semantic relatedness  measures equals k k         due
fact measures use symmetric  substitution matrix built
corpus used experiments  results three substitution matrices
biomedical domain  for bc ppi  lll  aimed  seven substitution matrices
generic relations  follows  discuss settings used calculating
substitution matrix detail 
distributional similarity estimated either using contextual information  o
seaghdha   copestake         exploring grammatical relations words  lee 
       work opt contextual information  motivated presence
words belonging different parts speech dependency paths  instance 
even though  according dependency grammar theory  melcuk         adjectives
govern words  may still occur dependency paths  words  even
parsing fail  may produce unreliable syntactic structures  able compare
words part speech  decided estimate distributional similarity based
contextual information  rather grammatical relations 
computing distributional similarity  may happen given word xi
occur corpus  handle cases  always set d xi   xi        the largest possible
similarity score   d xi   x j       xi    x j  the lowest possible similarity score  
      biomedical domain
estimate distributional similarity biomedical domain  use trec     
genomics collection  hersch  cohen  roberts    rakapalli        contains        
documents    journals  documents preprocessed removing htmltags  citations text reference sections stemmed porter stemmer  van
rijsbergen  robertson    porter         furthermore  query likelihood approach
dirichlet smoothing  chen   goodman        used retrieve document passages given
query  passages ranked according likelihood generating query  dirichlet smoothing used avoid zero probabilities poor probability estimates  which may
happen words occur documents   k unique words occurring set
dependency paths sequences fed queries collect corpus estimating similarity 
immediate context surrounding pair words used calculate distributional
similarity words  set context window      tokens right  

  

fiusing local alignments relation recognition

tokens left word focus  perform kind preprocessing
pos tagging 
      generic relations
generic relations  use wordnet relatedness measures described section       
already shown wordnet relatedness measures work synsets 
assumes words manually annotated information wordnet 
since done relations arguments  see example figure    
words sentences  and  correspondingly  dependency paths   build
substitution matrix follows  two words annotated wordnet  substitution score equals value returned relatedness measure used  word
pair  equals   whenever words identical    otherwise    example 
consider words dependency path      wu palmer  wup  relatedness
measure  substitution scores obtain follows 

d awareness n    awareness n       
d awareness n    come     
d awareness n    joy n          
d prep from  come     
d prep from  joy n       
d come  nsubj     
d nsubj  nsubj     
d joy n    joy n       

d awareness n    prep from     
d awareness n    nsubj     
d prep from  prep from     
d prep from  nsubj     
d come  come     
d come  joy n       
d nsubj  joy n       

figure    substitution scores dependency path      using wup measure 
syntactic relations  prep from  subj  accompanied direction
dependency tree traversal  either   

dependency path         unique elements  t     annotated
wordnet synsets  k   consequently             substitution scores total 
  computed using wordnet relatedness 
compute wordnet relatedness  use wordnet  similarity package wordnet      pedersen  patwardhan    michelizzi        
    baselines kernel settings
section  discuss two baselines kernel settings 
      baselines
test well local alignment kernels perform compared kernels proposed past 
implemented shortest path kernel described work bunescu mooney
   applies cases relation arguments could annotated wordnet
information 

  

fikatrenko  adriaans    van someren

     a   section        one baselines  baseline i   method seems
natural choice operates data structures  dependency paths  
similarly bunescu mooneys      a  work  experiments use lemma  part
speech tag direction  consider entity type negative polarity items 
choice la kernel paper motivated ability
compare sequences flexible way  possibility explore additional
information  not present training set  via substitution matrix  baseline 
baseline ii  used test whether choice similarity measures affects results 
case  substitution scores d     calculated using distributional similarity
wordnet relatedness  generated randomly within interval        
      kernel settings
kernels compute used together support vector machine tool libsvm
 chang   lin        detect hyperplanes separating positive examples negative
ones  plugging kernel matrices    fold cross validation libsvm 
normalized equation    

 

 

k x  y 

k x       p

k x  x k y  y 

    

handle imbalanced data sets  most notably aimed bc ppi   examples
weighted using inverse class probability  i e  training examples class weighted
  prob a  prob a  fraction training examples class a   significance
tests done using two tailed paired t test confidence level               
addition  experiments tuned penalty parameter c  equation   
range                          
use la kernel  one set following parameters  gap opening cost 
gap extension cost  scaling parameter   cross validation experiments 
gap opening cost set      extension cost     scaling parameter
   choice scaling value motivated experiments amino acids
biological domain  saigo et al          initial experiments  present
study parameter values varied 

   experiment i  domain specific relations
goal evaluation study behavior la kernel domain specific
relations biomedical domain  section  report experiments conducted
three biomedical corpora using la kernel based distributional similarity measures  two baselines results published previously  e g   using graph kernel airola
et al        tree kernel stre et al          best knowledge  string
kernels applied dependency paths yet  however  gap weighted string
kernel  described section    allows gapping thus compared la
kernel  test lodhi et al s        kernel performs dependency paths  use

  

fiusing local alignments relation recognition

three corpora  tuned parameters string kernel set length
subsequences   decay factor       
    lll bc ppi data sets
subsection presents results two biomedical data sets  bc ppi lll  whenever
possible  discuss performance previously reported literature 
   fold cross validation results bc ppi corpus presented table  
lll training data set table    la kernel based distributional similarity
measures  la dice  la cosine la l   performs significantly better two baselines  recall baseline corresponds shortest path approach  section       
baseline ii la kernel randomly generated substitution scores  contrast
baseline i  able handle sequences different lengths including gaps  according
equation    comparison two sequences different lengths results   score 
nevertheless  still yields high recall  precision much lower  explained
fact shortest path uses pos tags  even though two sequences
length different  comparison may still result non zero score  provided
part speech tags match  furthermore  baseline ii suggests accurate estimation substitution scores important achieving good performance  baseline ii may
yield better results baseline i  randomly generated substitution scores degrade
performance 
method
la dice
la cosine
la l 
baseline
baseline ii
gap weighted string kernel  lodhi et al        

precision
     
     
     
     
     
     

recall
     
     
     
     
     
     

f score
     
     
     
     
     
     

table       fold cross validation bc ppi data set 
first glance  choice distributional similarity measures affect
overall performance yielded la kernel  bc ppi data  method based
l  measure outperforms methods based dice  p     cosine 
differences latter case significant  statistically significant differences
observed method based dice cosine 
contrast bc ppi data set  kernels use dice cosine measures
lll data set significantly outperform one based l   at p       
p          respectively  
data sets  la method using distributional similarity measures significantly
outperforms baselines  interestingly  gap weighted string kernel lodhi et al 
       yields good performance seems better choice subsequence
   lodhi et al         mentioned paper f  numbers  with respect ssk  seem
peak subsequence length     

  

fikatrenko  adriaans    van someren

kernel based shallow linguistic information  giuliano et al          recent work
lll  fundel  kueffner    zimmer        employs dependency information but  contrast
method  serves representation extraction rules defined  airola
et al         apply graph kernel based approach extract interactions use  among
others  lll aimed data sets  seen table    method yields results
comparable gap weighted string kernel dependency paths 
best knowledge  performance achieved la kernel lll training set
highest  in terms f score  among results reported
literature 
method
la dice
la cosine
la l 
baseline
baseline ii
graph kernel  airola et al        
gap weighted string kernel  lodhi et al        
shallow linguistic kernel  giuliano et al        
rule based method  fundel et al        

precision
     
     
     
     
     
    
     
     
  

recall
     
     
     
      
     
    
     
     
  

f score
     
     
     
     
     
    
     
     
  

table       fold cross validation lll all training data set 
apply method lll test data  table       even though performance test set poorer  la dice outperforms baselines  addition 
gap weighted string kernel  lodhi et al         seems perform much worse test
set  la kernel  precision high  recall decreases  and drastically
data subset includes co references   might due fact
sentences incomplete parses generated and  consequently  dependency paths
entities found         possible interaction pairs generated
test data  dependency path extracted  contrast  approach reported
giuliano et al         make use syntactic information  data subset
without coreferences achieves higher recall 
hand  lower recall caused using actual names proteins
genes arguments  work reported before  relation arguments
named entities often replaced types  e g   protein  used
input learning algorithm  conducted additional experiments using named entity
types dependency paths  led great improvement terms recall
f score  table    lll coref label  lll nocoref label  lll coref label   method
clearly outperforms shallow linguistic kernel achieves better results
best performing system lll competition  sbest    which  according nedellec        
applied markov logic syntactic paths 
   airola et al         report performance lll data set and  reason  information
graph all paths kernel included table   

  

fiusing local alignments relation recognition

data set
lll coref
lll nocoref
lll all
lll all
lll all
lll coref label
lll nocoref label
lll all label
lll coref
lll nocoref
lll all
lll all
lll all

method
la dice
la dice
la dice
baseline
baseline ii
la dice
la dice
la dice
shallow linguistic kernel  giuliano
shallow linguistic kernel  giuliano
shallow linguistic kernel  giuliano
gap weighted string kernel  lodhi
sbest  nedellec       

et
et
et
et

al  
al  
al  
al  

     
     
     
     

precision
    
    
    
    
    
    
    
    
    
    
    
    
    

recall
    
    
    
    
    
    
    
    
    
    
    
    
    

f score
    
    
    
    
    
    
    
    
    
    
    
    
    

table    results lll test data set 

    aimed data set
yet another data set consider aimed  data set often used
experiments relation extraction biomedical domain  enables comparison
methods  noted  however  particular case  corpus
collection documents  abstracts   may lead two ways performing    fold
cross validation  one possibility lies randomly splitting data    parts 
cross validation level documents  experiments report
done using first setting directly compared methods described
work stre et al          erkan et al         giuliano et al          addition 
use dependency paths la kernel ones employed stre et al 
erkan et al   results airola et al         bunescu        obtained
cross validating level documents 
conducted experiments setting distributional measure dice  referred
la dice table    upper part table used dependency paths generated
stanford parser lower part obtained enju  discussed
section    erkan et al         use similarity measures compare dependency paths 
consider additional sources whose information incorporated
learning procedure  they  however  experiment supervised  svm  semi supervised
learning  tsvm   number training instances varied  table   shows best
performance achieved erkan et al s        method  among models based
svm  one cosine distance  svm cos  yields best results  tsvm
setting  one edit measure performs best  observe la dice slightly
outperforms has  particular  high precision 
work  stre et al         explore several parsers combinations features 
features include paths enju  word dependencies generated
data driven ksdep parser  word features  ksdep parser based probabilistic

  

fikatrenko  adriaans    van someren

shift reduce algorithm  sagae   tsujii         general  method stre et al 
uses svm  case focuses tree kernels  discussed section         make
fair comparison  conducted experiments paths obtained deep syntactic analysis
 enju parser  compared scores stre et al s        results  contrast
previous experiments  achieve higher recall lower precision  overall  la
kernel yields better performance one reported stre et al  however 
different sets features combined  parses enju ksdep plus word features enju ksdep w table     overall performance improved 
method
la dice
baseline  bunescu       
baseline ii
svm cos  erkan et al        
tsvm edit  erkan et al        
gap weighted string kernel  lodhi et al        
la dice
tree kernel  stre et al        
tree kernel  stre et al        
graph kernel  airola et al        
shallow linguistic kernel  giuliano et al        

parser
stanford
collins
stanford
stanford
stanford
stanford
enju
enju
enju ksdep w
charniak lease
none

precision
     
     
     
     
     
     
     
    
    
    
    

recall
     
     
     
     
     
     
     
    
    
    
    

f score
     
     
     
     
     
     
     
    
    
    
    

table       fold cross validation aimed data set 
bunescu        reports evaluation results aimed corpus form
precision recall curve  consider highest precision obtained experiments               depending input   roughly corresponds recall
    plot  referred baseline table     sum  shortest path approach
never approaches performance la kernel biomedical data sets
studied here  baseline  baseline ii  achieves lowest scores
methods presented here 
table   illustrates various methods trained aimed corpus 
many different parsers used  noted graph kernel
trained tested syntactic representation generated charniak lease
parser  shortest path kernel explored dependency paths obtained
collins parser  charniak lease parser statistical parser trained biomedical
data  lease   charniak         whose phrase structures transformed dependencies  likewise  collins parser statistical parser  collins         leads
question whether choice syntactic parser significant impact extraction
results  compare impact syntactic parsers relation extraction aimed 
miyao et al         conducted complex study eight parsers  including stanford analyzer  five parse representations     consider two cases  first one 
parsers trained biomedical data  regardless parser used
experiments  accuracy extraction task similar  second experiment 
   either various dependency tree formats  e  g   stanford dependency format   phrase
structures  predicate arguments structures 

  

fiusing local alignments relation recognition

parsers re trained domain specific data  case  shown
relation extraction results improved  actual gain  however  vary
one parser another 
aimed data  la kernel dice measure gives state of the art results 
outperformed approaches use information dependency
paths 
    la kernel parameters
saigo et al         already shown scaling parameter  equation    
significant impact accuracy  carried additional experiments varying
gap values value   results visualized figure    opening extension
gap values separated slash symbol values x axis form a b
read opening gap set extension gap equal b 
kernel matrices normalized examples weighted  according previous
experiments  results yielded dice measure significantly differ
ones achieved cosine measure selected dice measure conduct
experiments  performance bc ppi data set shown figure   

f score
  
  
  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  
  
  

    
     
gaps

    
     

   

   

   

   

 

 

scaling

figure    varying gaps scaling    parameter bc ppi data set     fold
cross validation   f score 

  

fikatrenko  adriaans    van someren

  
  
  
  
  
  
  
  
  
  
  

precision
  
  
  
  
  
  

 

  

 

    

   
     
gaps

scaling

   

    

   
     

   

figure    varying gaps scaling    parameter bc ppi data set     fold
cross validation   precision 

  
  

recall

  

  

  

  

  

  

  

  

  

  

 

  

 

    

   
     
gaps

   

    

scaling

   
     

   

figure    varying gaps scaling    parameter bc ppi data set     fold
cross validation   recall 
  

fiusing local alignments relation recognition

results figure   indicate decreasing leads decrease overall performance  moreover  varying gap values causes subtle changes f score 
changes drastic changes due lower  
changes f score likely explained variances precision
recall  investigate matter  look measures depend parameter
changes  set low value  one expect nearly diminish impact
substitution matrix  i e  similarity among elements  reason hypothesize
larger values scaling parameter result higher recall  indeed  figure  
supports hypothesis recall plot resembles one f score  varying
parameter values much lower impact precision  figure    nonetheless precision
decrease parameter becomes larger 
overall  seems influence final results most  although gap values make
contribution well  according results obtained  setting extension gap e
large value  or equal opening gap o  undesirable  since scaling parameter
applied substitution matrix gap values well  setting
    decreases effects gap penalization similarity elements  consequently 
best performance achieved setting    suggests final performance
la kernel influenced combination parameters choice crucial
obtaining good performance 

   experiment ii  generic relations
another series experiments carried seven generic relations semeval
       challenge  task    choice data sets case motivated two
factors  first  semantic relations used differ relations biomedical
domain  second  since arguments relations annotated wordnet  becomes
possible explore information wordnet use prior knowledge la
kernel 
many participants challenge considered wordnet either explicitly  tribble  
fahlman        kim   baldwin         part complex system  giuliano et al  
       since always obvious use wordnet yields best performance  many researchers made additional decisions use supersenses  hendrickx et al          selection predefined number high level concepts  nulty        
cutting wordnet hierarchy certain level  bedmar et al          systems
one nakov        based solely information collected web 
even though became evident best performing systems used wordnet  variance results remarkable clear whether difference performance
explained machine learning methods used  combination features 
factors 
semeval      task   data set includes relation examples nominal
compounds  like coffee maker   greatly reduces availability information
two arguments dependency paths  relation arguments case linked
one grammatical relation  e g   coffee maker linked grammatical relation
nn  corresponds noun compound   assume  therefore  information coming
wordnet especially helpful dependency paths short 

  

fikatrenko  adriaans    van someren

experiments used   relatedness measures defined earlier section     plus one additional
measure called random  random measure indicates relatedness values
two relation arguments generated randomly  within         thus
suitable baseline  baseline ii   similarly experiments biomedical domain 
another baseline shortest path kernel  baseline i   note task   overview
paper  girju et al         reported three baselines  which  case   i  guessing
true false examples  depending class majority class test
set  baseline iii    ii  always guessing true  baseline iv    iii  guessing true false
probability corresponds class distribution test set  baseline v  
first question interest implications choice semantic relatedness
measure performance la kernel  answer question  perform
   fold cross validation training set  figure    figure    figure      among
  measures jcn resnik fail perform better random score 
cases  resnik score outperformed measures  behaviour leacockchodorow score  lch  jcn varies one semantic relation another  instance  use
jcn seems boost precision cause effect  part whole  product   producer 
theme   tool  remaining three relations clearly best performing
measure 
check whether differences relatedness measures  carried
significance tests comparing measures relations  findings summarized
table    here  symbol two relatedness measures stands measure
equivalence  or  words  indicates significant difference  similarly
experiments biomedical field  significance tests conducted using
two tailed paired t test confidence level      addition  two measures
b    b means performs significantly better b  instance  ranking
cause   effect table   read follows  two best performing measures
wup lch  significantly outperform lin  followed random res  which 
turn  yield significantly better results jcn  seen table wup
lch clearly best performing measures seven relations  each best
measure six seven relations  
relation type
cause   effect
instrument   agency
product   producer
origin   entity
theme   tool
part   whole
content   container

ranking
wup lch   lin
wup lch   lin
wup lch   lin
wup lch   lin
lch   lin wup
wup lin lch
wup   lch   lin

 
 

 
 
 


res random   jcn
res   jcn random
jcn res   random
res jcn   random
res   jcn   random
res   jcn random
res   jcn random

table    ranking relatedness measures respect accuracy training sets   stands measure equivalence    b indicates measure
significantly outperforms b  

  

fiusing local alignments relation recognition

relation  applied best performing measure training set
particular relation test data  results reported table     average  la
kernel employing wordnet relatedness measures significantly outperforms two baselines 
moreover  compared best results semeval      competition  beamer
et al          method approaches performance yielded best system  bestsv   
system used various lexical  syntactic  semantic feature sets 
expanded training set adding examples many different sources  already
mentioned section   recent work seaghdha        explores wordnet
structure graph kernels classify semantic relations  overall performance
achieved method  table     comparable one la kernel 
unclear whether semantic relations one approaches performs
better 
relation type
cause   effect
instrument   agency
product   producer
origin   entity
theme   tool
part   whole
content   container
average
baseline
baseline ii
baseline iii
baseline iv
baseline v
bestsv
gap weighted string kernel  lodhi et al        
wordnet kernels  o seaghdha       

accuracy
     
     
     
     
     
     
     
     
     
     
    
    
    
    
     
    

precision
     
     
     
     
     
     
     
     
     
     
    
    
    
    
    
 

recall
     
     
     
     
     
     
     
     
     
     
    
     
    
    
     
 

f score
     
     
     
     
     
     
     
     
     
     
    
    
    
    
     
    

measure
lch
wup
lch
wup
lch
wup
wup

table     results semeval       task   test data set  selecting best performing
measure training set relation  

addition  report results semeval task   test set per relatedness measure
 table      averages seven relations  similarly findings
training set  wup lch best performing measures test data well 
one would expect optimal use prior knowledge allow us reduce
number training instances without significant changes performance  study
 and whether  amount training data influences results test set  split
training set several subsets  creating model subset applying
semeval       task   test data  split corresponds split used challenge
organizers  figure     suggests  relations recognized well even relatively
small data sample used  exception theme tool relation increasing
   model trained    origin entity examples classifies none test examples positive 
reason point figure   relation given    training examples 

  

fikatrenko  adriaans    van someren

training data clearly helps  finding line results giuliano et al        
whose system combination kernels data  results indicate
relations one  theme tool  extracted well  even quarter
training set used 
relatedness measure
wup
lch
lin
res
jcn
random

accuracy
     
     
     
     
     
     

precision
     
     
     
     
     
     

recall
     
     
     
     
     
     

f score
     
     
     
     
     
     

table     results semeval       task   test data set  averages   relations
per wordnet relatedness measure 

learning curve
   
  
  
  

f score

  
  
cause effect
instrument agency
product producer
origin entity
theme tool
part whole
content container

  
  
  
  
 

  

  
   
training examples

   

figure    learning curve semeval       task   test data set 
recent work semeval task   data set includes investigation distributional kernels  o seaghdha   copestake         pattern clusters  davidov   rappoport 
       relational similarity  nakov   hearst         wordnet kernels  unlike wordnet
kernels  first three approaches use wordnet  seaghdha copestake       
report accuracy      f score      best results yielded distributional kernels best performance davidov rappoports        method
accuracy       f score       wordnet kernels  similarly findings
la kernel  yield better accuracy methods using wordnet        

  

fiusing local alignments relation recognition

cause effect
   

precision
recall
f score

  
  
  
  
  
  
  
  
  
 

wup

lin

lch
res
similarity measure

jcn

random

instrument agency
   

precision
recall
f score

  
  
  
  
  
  
  
  
  
 

wup

lin

lch
res
similarity measure

jcn

random

product producer
   

precision
recall
f score

  
  
  
  
  
  
  
  
  
 

wup

lin

lch
res
similarity measure

jcn

random

figure       fold cross validation training set  cause   effect  instrument agency product   producer relations  

  

fikatrenko  adriaans    van someren

origin entity
   

precision
recall
f score

  
  
  
  
  
  
  
  
  
 

wup

lin

lch
res
similarity measure

jcn

random

theme tool
   

precision
recall
f score

  
  
  
  
  
  
  
  
  
 

wup

lin

lch
res
similarity measure

jcn

random

part whole
   

precision
recall
f score

  
  
  
  
  
  
  
  
  
 

wup

lin

lch
res
similarity measure

jcn

random

figure        fold cross validation training set  origin   entity  theme   tool
part   whole relations  

  

fiusing local alignments relation recognition

content container
   

precision
recall
f score

  
  
  
  
  
  
  
  
  
 

wup

lin

lch
res
similarity measure

jcn

random

figure        fold cross validation training set  content   container relation  

f score comparable performance reported seaghdha copestake       
davidov rappoport        

   discussion
section revisit goals stated end section   discuss
findings detail 
    la kernel relation extraction
introduced la kernel  proven effective biomedical problems 
nlp domain showed well suited relation extraction  particular  experiments two different domains either outperform existing methods yield
performance par existing state of the art kernels 
one motivations using la kernel relation extraction task
exploit prior knowledge  here  explore two possibilities  distributional similarity
information provided wordnet 
      distributional similarity measures
setting  consider three distributional measures already studied
before  instance  lee        uses detect similar nouns based verb object
co occurrence pairs  results suggest jaccard coefficient  which related
dice measure  one best performing measures followed others including
cosine  euclidean distance fell group largest error rates  given previous
work lee         one would expect euclidean distance achieve worse results
  

fikatrenko  adriaans    van someren

two measures  indeed  lll corpus  la kernel employing l  shows
significant decrease performance  measures  method using dice
significantly outperforms one based l  measure lll corpus
significant improvement bc ppi data set  based experiments
conducted  conclude la kernel using dice cosine measures performs
similarly lll data set bc ppi corpus  given results various biomedical corpora  and different settings experimented with   obtained experimental
support choosing dice cosine measure euclidean distance 
      wordnet similarity measures
generic relations  semantic relatedness plays significant role  difference
f score models use semantic relatedness kernel relatedness
values generated randomly  baseline ii  amounts nearly      measures exhibit
different performance seven generic relations considered 
observe  instance  wup  lch  lin almost always yield best results  matter
relation considered  found resnik score jiang conraths measure
yield lower results measures  even though f scores per relation vary
quite substantially  by placing cause effect  theme tool  origin entity among
difficult relations extract   two measures  wup lch  top performing
measures seven relations  two measures explore wordnet taxonomy using
length paths two concepts  depth wordnet hierarchy and 
consequently  belong path based measures  three measures  res  lin
jcn information content based measures  relatedness two concepts
defined amount information share  experiments la
kernel generic relation recognition suggest that  particular case  path based
measures preferred information content based measures 
stress  however  evaluation semantic relatedness measures context relation recognition  one means draw conclusion
top measures nlp tasks stay same  example  budanitsky
hirst        use semantic relatedness measures detect malapropism show
jiang conraths measure  jcn  yields best results  followed lins measure  lin  
one leacock chodorow  lch   resniks measure  res  
results quite similar findings consider res measure  jcn
top accuracy ranking list seven semantic relations
studied 
    factors parameters influence la kernel performance
experiments two domains shown la kernel either outperforms existing
methods corpora  yields performance par existing state of the art
kernels 
      baselines
advantage la kernel bunescu shortest path method  baseline i 
capable handling paths different lengths  allowing gaps penalizing them 
  

fiusing local alignments relation recognition

final kernel matrix becomes less sparse  shortest path approach attempts
generalize dependency paths  usually overgeneralizes leads high
recall scores  table   table    poor overall performance  one explanation
overgeneralization may method accounts well structural similarity  provided
sequences length  fails provide finer distinctions among dependency
paths  consider  example  two sequences trip makes tram coffee makes
guy  whereby first path represents negative instance product producer
relation second path corresponds positive one  even though match
exactly  elements match nouns singular  consequently  comparison
according shortest path method result relatively high similarity score 
contrast  la kernel consider similarity elements pairs trip coffee
tram guy obtain low scores 
addition  baseline ii  based randomly generated substitution scores 
performs poor data sets  or comparable baseline i   leads us conclusion
accurate estimation similarities another reason la kernel performs well
relation extraction 
      comparison methods
already pointed out  obvious shortcoming baseline inability
handle dependency paths different length  reason  applied
gap weighted string kernel  lodhi et al         data sets  case  dependency
paths compared flexible way gapping allowed  additional
information used  kernel outperforms baseline increasing precision relation
extraction preserving relatively high recall  data set fails yield
good results lll test data  believe due differences lll
training test data  data sets  la kernel achieves better performance
gap weighted string kernel  margin  however  different different data sets 
biomedical domain  differences two methods clearly seen
bc ppi lll data sets  results aimed corpus comparable 
however  methods tested aimed get higher scores unless use
features dependency paths  holds types cross validation used
corpus  generic relations  difference la kernel gapweighted string kernel much larger  particular  case gap weighted kernel 
precision high  recall much lower  explained fact generic
relations benefit knowledge found wordnet recall achieved la kernel
is  therefore  high  gap weighted kernel access information found
dependency paths and  reason  fails find relations 
la kernel achieves best performance lll training set  outperforming
graph kernel  airola et al          shallow linguistic kernel  giuliano et al        
rule based system fundel et al          three used different input
methods  varying plain text dependency structures  reason  direct
comparison unfortunately possible  conclude methods employing
dependency information always among best performing approaches 

  

fikatrenko  adriaans    van someren

two approaches whose performance reported aimed data set include tree kernel  stre et al         tsvm  erkan et al         
explore syntactic information different ways  stre et al  consider subtrees 
method erkan et al  similarities approach relies
dependency path comparison  comparison  use information already
available dependency paths  svm setting   dependency paths  tsvm setting   according lauer bloch         tsvms fall category using prior
knowledge sampling methods  explores prior knowledge generating new
examples  contrast  employ information large unlabeled text sources order
enable finer comparison dependency paths always work supervised learning
setting  using evaluation procedure work stre et al  erkan et al 
show la kernel outperforms methods  differences data set
much smaller data sets used 
      la parameters
demonstrated choice la parameters crucial achieving good performance  experiments  scaling parameter contributes overall performance
most  parameters gap values taken account well 
approaches infinity  la kernel approximates smith waterman distance 
increasing necessarily positive impact final performance 
finding line results reported saigo et al         homology detection
task  best performance yielded setting scaling parameter   bit higher 
penalizing gap extension less gap opening 

   conclusions future work
presented novel approach relation extraction based local alignments sequences  using la kernel provides us opportunity explore various
sources information study role relation recognition  possible future directions include  therefore  examination distributional similarity measures  studying
impact extraction generic relations  looking sources information could helpful relation recognition  may interesting consider
relational similarity  turney         looks correspondence relation
instances  case  one able infer doctor corresponds scalpel
similar way fisherman net  where  scalpel  doctor   net  fisherman 
examples instrument   agency  
despite sparseness problem might occur wordnet based measures
used  measures advantage distributional measures treating elements compared concepts rather words  nlp community  steps
already taken solve problem clustering words large corpora aiming
word sense discovery  pennacchiotti   pantel         recently  mohammad       
thesis investigated compatibility distributional measures ontological ones 
using corpus statistics thesaurus  author introduced distributional profiles
senses defined distance measures them  even though new approach calculat 

  

fiusing local alignments relation recognition

ing similarity tested generic corpora  would certain interest apply
domain specific data 
overall  local alignment kernels provide flexible means work data sequences 
first  allow partial match sequences particularly important
dealing text  second  possible incorporate prior knowledge learning
process preserving kernel validity  general  la kernels applied
nlp problems long input data form sequences 

acknowledgments
authors wish thank simon carter gerben de vries comments
proofreading  three anonymous reviewers highly valuable feedback 
acknowledge input adaptive information management  aim  group
university amsterdam  preliminary version work dicussed
  nd international conference computational linguistics  coling      
seventh international tbilisi symposium language  logic computation        
work carried context virtual laboratory e science project
 www vl e nl   project supported bsik grant dutch ministry
education  culture science  oc w  part ict innovation program
ministry economic affairs  ez  

references
airola  a   pyysalo  s   bjorne  j   pahikkala  t   ginter  f     salakoski  t          allpaths graph kernel protein protein interaction extraction evaluation crosscorpus learning  bmc bioinformatics     suppl ii  
beamer  b   bhat  s   chee  b   fister  a   rozovskaya  a     girju  r          uiuc 
knowledge rich approach identifying semantic relations nominals 
proceedings workshop semantic evaluations  semeval   prague  czech
republic 
bedmar  i  s   samy  d     martinez  j  l          uc m  classification semantic relations
nominals using sequential minimal optimization  semeval      
budanitsky  a     hirst  g          evaluating wordnet based measures lexical semantic
relatedness  computational linguistics               
bunescu  r  c          learning information extraction  ph d  thesis  department
computer sciences  university texas austin 
bunescu  r  c   ge  r   kate  r  j   marcotte  e  m   mooney  r  j   ramani  a  k    
wong  y  w          comparative experiments learning information extractors
proteins interactions  artificial intelligence medicine             
bunescu  r  c     mooney  r  j       a   shortest path dependency kernel relation
extraction  joint conference human language technology   empirical methods
natural language processing  hlt emnlp   vancouver  bc 

  

fikatrenko  adriaans    van someren

bunescu  r  c     mooney  r  j       b   subsequence kernels relation extraction 
proceedings   th conference neural information processing systems 
vancouver  bc 
bunescu  r  c     mooney  r  j          text mining natural language processing 
chap  extracting relations text  word sequences dependency paths 
springer 
burges  c  j  c          tutorial support vector machines pattern recognition 
data mining knowledge discovery                
camacho  r          use background knowledge inductive logic programming 
report 
cancedda  n   gaussier  e   goutte  c     renders  j  m          word sequence kernels 
journal machine learning research              
chang  c  c     lin  c  j          libsvm  library support vector machines  software
available http   www csie ntu edu tw  cjlin libsvm 
chen  s  f     goodman  j          empirical study smoothing techniques language
modeling  acl   
clegg  a  b          computational linguistic approaches biological text mining  ph d 
thesis  university london 
cohen  w  w   ravikumar  p     fienberg  s          comparison string distance
metrics name matching tasks  iiweb       pp       
collins  m          head driven statistical models natural language parsing  ph d 
thesis  university pennsylvania 
collins  m     duffy  n          convolution kernels natural language  advances
neural information processing systems     pp          mit press 
cortes  c     vapnik  v          support vector networks  machine learning         
       
davidov  d     rappoport  a          classification semantic relationships
nominals using pattern clusters  proceedings acl    hlt  pp         
dolan  w  b   quirk  c     brockett  c          unsupervised construction large paraphrase corpora  exploiting massively parallel news sources  coling       geneva 
switzerland 
erkan  g   ozgur  a     radev  d  r          semi supervised classification extracting
protein interaction sentences using dependency parsing       joint conference
empirical methods natural language processing computational natural
language learning  pp         
fellbaum  c          wordnet  electronic lexical database  mit press 
firth  j  r          synopsis linguistic theory           studies linguistic analysis 
philological society  oxford  reprinted palmer  f   ed         
fundel  k   kueffner  r     zimmer  r          relex   relation extraction using dependency
parse trees  bioinformatics         
  

fiusing local alignments relation recognition

girju  r   badulescu  a     moldovan  d          automatic discovery part whole relations  computational linguistics                
girju  r   nakov  p   nastase  v   szpakowicz  s   turney  p     yuret  d          semeval     task     classification semantic relations nominals  acl      
girju  r   nakov  p   nastase  v   szpakowicz  s   turney  p     yuret  d          classification semantic relations nominals  language resources evaluation 
               
giuliano  c   lavelli  a   pighin  d     romano  l          fbk irst  kernel methods
semantic relation extraction  semeval      
giuliano  c   lavelli  a     romano  l          exploiting shallow linguistic information
relation extraction biomedical literature  eacl      
grishman  r     sundheim  b          message understanding conference      brief
history  proceedings   th international conference computational linguistics 
haussler  d          convolution kernels discrete structures  tech  rep  ucs crl       
uc santa cruz 
hearst  m          automatic acquisition hyponyms large text data  proceedings
coling     pp         
hendrickx  i   morante  r   sporleder  c     van den bosch  a          ilk  machine
learning semantic relations shallow features almost data  semeval     
hersch  w   cohen  a  m   roberts  p     rakapalli  h  k          trec      genomics
track overview  proceedings   th text retrieval conference 
jiang  j  j     conrath  d  w          semantic similarity based corpus statistics
lexical taxonomy  proceedings international conference research
computational linguistics  rocling x   pp       
joachims  t          transductive inference text classification using support vector
machines  proceedings icml 
katrenko  s     adriaans  p          semantic types generic relation arguments 
detection evaluation  proceedings   th annual meeting association computational linguistics  human language technologies  acl hlt  
columbus  usa 
khoo  c  s  g   chan  s     niu  y          extracting causal knowledge medical database using graphical patterns  proceedings   th annual meeting
association computational linguistics  pp          morristown  nj  usa 
association computational linguistics 
kim  s  n     baldwin  t          melb kb  nominal classifications noun compound
interpretation  semeval      
lauer  f     bloch  g          incorporating prior knowledge support vector machines
classification  review  neurocomputing               
  

fikatrenko  adriaans    van someren

leacock  c     chodorow  m          combining local context wordnet similarity
word sense identification  mit press  cambridge  ma 
lease  m     charniak  e          parsing biomedical literature  proceedings ijcnlp 
lee  l          measures distributional similarity  proceedings   th annual meeting association computational linguistics computational linguistics 
pp       
leslie  c   eskin  e   cohen  a   weston  j     noble  w  s          mismatch string kernels
discriminative protein classification  bioinformatics                 
leslie  c   eskin  e     noble  w  s          spectrum kernel  string kernel svm
protein classification  pacific symposium biocomputing    pp         
leusch  g   ueffing  n     ney  h          novel string to string distance measure
applications machine translation evaluation  machine translation summit ix 
pp          new orleans  lo 
lin  d          information theoretic definition similarity  proceedings   th
international conference machine learning  pp         
lodhi  h   saunders  c   shawe taylor  j   christianini  n     watkins  c          text
classification using string kernels  journal machine learning research            
mcdonald  r          extracting relations unstructured text  tech  rep  ms cis       upenn 
melcuk  i          dpendency syntax  theory practice  suny press 
mitchell  t          machine learning  mcgraw hill 
miyao  y   stre  r   sagae  k   matsuzaki  t     tsuji  j          task oriented evaluation
syntactic parsers representations  proceedings acl    hlt  pp    
   
mohammad  s          measuring semantic distance using distributional profiles concepts  ph d  thesis  graduate department computer science university
toronto 
monge  a  e     elkan  c          field matching problem  algorithms applications 
kdd       pp         
moschitti  a          efficient convolution kernels dependency constituent syntactic
trees  ecml       pp         
nakov  p          ucb  system description semeval task     semeval      
nakov  p          paraphrasing verbs noun compound interpretation  proceedings
workshop multiword expressions  mwe     conjunction language
resources evaluation conference  marrakech  morocco       
nakov  p     hearst  m  a          solving relational similarity problems using web
corpus  proceedings acl    hlt 
nedellec  c          learning language logic   genic interaction extraction challenge 
proceedings learning language logic workshop 
  

fiusing local alignments relation recognition

needleman  s  b     wunsch  c  d          general method applicable search
similarities amino acid sequence two proteins  journal molecular biology 
               
nulty  p          ucd pn  classification semantic relations nominals using
wordnet web counts  semeval      
seaghdha  d          semantic classification wordnet kernels  proceedings
north american chapter association computational linguistics   human
language technologies conference  naacl hlt   boulder  co 
seaghdha  d     copestake  a          semantic classification distributional kernels 
proceedings coling       manchester  uk 
palmer  m     wu  z          verb semantics english chinese translation  tech  rep  
technical report no  ms cis        department computer   information science 
university pennsylvania 
pedersen  t   patwardhan  s     michelizzi  j          wordnet  similarity   measuring
relatedness concepts  proceedings nineteenth national conference
artificial intelligence  aaai      pp            san jose  ca 
pennacchiotti  m     pantel  p          ontologizing semantic relations  acl     proceedings   st international conference computational linguistics
  th annual meeting association computational linguistics  pp         
morristown  nj  usa  association computational linguistics 
ponzetto  s  p     strube  m          knowledge derived wikipedia computing
semantic relatedness  journal artificial intelligence research             
resnik  p          using information content evaluate semantic similarity  proceedings
  th international joint conference artificial intelligence  pp         
stre  r   sagae  k     tsuji  j          syntactic features protein protein interaction
extraction   nd international symposium languages biology medicine 
pp          
sagae  k     tsujii  j          dependency parsing domain adaptation lr models
parser ensembles  proceedings emnlp conll 
saigo  h   vert  j  p     akutsu  t          optimizing amino acid substitution matrices
local alignment kernel  bmc bioinformatics        
saigo  h   vert  j  p   ueda  n     akutsu  t          protein homology detection using
string alignment kernels  bioinformatics                    
sang  e  f  t  k   canisius  s   van den bosch  a     bogers  t          applying spelling
error correction techniques improving semantic role labeling  proceedings
ninth conference natural language learning  conll       ann arbor  mi 
saunders  c   tschach  h     shawe taylor  j          syllables string kernel
extensions  icml      
scholkopf  b          support vector learning  ph d  thesis  berlin technical university 

  

fikatrenko  adriaans    van someren

sekimizu  t   park  h  s     tsujii  j          identifying interaction genes
gene products based frequently seen verbs medline abstracts  genome
informatics          
shawe taylor  j     christianini  n          support vector machines kernelbased learning methods  cambridge university press 
smith  l  h   yeganova  l     wilbur  w  j          hidden markov models optimized
sequence alignment  computational biology chemistry                
smith  t  f     waterman  m  s          identification common molecular subsequences 
journal molecular biology              
snow  r   jurafsky  d     ng  a  y          learning named entity hyponyms question
answering  proceedings coling acl 
swanson  d  r     smalheiser  n  r          implicit text linkages medline records 
using arrowsmith aid scientific discovery  library trends         
thomas  j   milward  d   ouzounis  c     pulman  s          automatic extraction
protein interactions scientific abstracts  proceedings pacific symposium
biocomputing 
tribble  a     fahlman  s  e          cmu at  semantic distance background knowledge identifying semantic relations  semeval      
turney  p  d          similarity semantic relations  computational linguistics         
       
van der plas  l          automatic lexico semantic acquisition question answering 
ph d  thesis  university groningen 
van rijsbergen  c  j   robertson  s  e     porter  m  f          new models probabilistic
information retrieval  tech  rep        british library research development
report 
vapnik  v          estimation dependences based empirical data  new york 
springer verlag 
weeds  j   weir  d     mccarthy  d          characterising measures lexical distributional similarity  proceedings coling      
zelenko  d   aone  c     richardella  a          kernel methods relation extraction 
journal machine learning research              
zhang  y   schneider  j     dubrawski  a          learning semantic correlation 
alternative way gain unlabeled text  proceedings   nd conference
neural information processing systems  vancouver  bc 

  


