journal of artificial intelligence research                  

submitted        published      

a minimum relative entropy principle
for learning and acting
pedro a  ortega
daniel a  braun

peortega dcc uchile cl
dab   cam ac uk

department of engineering
university of cambridge
cambridge cb   pz  uk

abstract
this paper proposes a method to construct an adaptive agent that is universal with
respect to a given class of experts  where each expert is designed specifically for a particular
environment  this adaptive control problem is formalized as the problem of minimizing
the relative entropy of the adaptive agent from the expert that is most suitable for the
unknown environment  if the agent is a passive observer  then the optimal solution is the
well known bayesian predictor  however  if the agent is active  then its past actions need
to be treated as causal interventions on the i o stream rather than normal probability
conditions  here it is shown that the solution to this new variational problem is given
by a stochastic controller called the bayesian control rule  which implements adaptive
behavior as a mixture of experts  furthermore  it is shown that under mild assumptions 
the bayesian control rule converges to the control law of the most suitable expert 

   introduction
when the behavior of an environment under any control signal is fully known  then the
designer can choose an agent that produces the desired dynamics  instances of this problem include hitting a target with a cannon under known weather conditions  solving a maze
having its map and controlling a robotic arm in a manufacturing plant  however  when
the environment is unknown  then the designer faces the problem of adaptive control  for
example  shooting the cannon lacking the appropriate measurement equipment  finding the
way out of an unknown maze and designing an autonomous robot for martian exploration 
adaptive control turns out to be far more difficult than its non adaptive counterpart  this
is because any good policy has to carefully trade off explorative versus exploitative actions 
i e  actions for the identification of the environments dynamics versus actions to control it
in a desired way  even when the environments dynamics are known to belong to a particular class for which optimal agents are available  constructing the corresponding optimal
adaptive agent is in general computationally intractable even for simple toy problems  duff 
       thus  finding tractable approximations has been a major focus of research 
recently  it has been proposed to reformulate the problem statement for some classes of
control problems based on the minimization of a relative entropy criterion  for example  a
large class of optimal control problems can be solved very efficiently if the problem statement
is reformulated as the minimization of the deviation of the dynamics of a controlled system
from the uncontrolled system  todorov              kappen  gomez    opper         in
this work  a similar approach is introduced for adaptive control  if a class of agents is
c
    
ai access foundation  all rights reserved 

fiortega   braun

given  where each agent is tailored to a different environment  then adaptive controllers can
be derived from a minimum relative entropy principle  in particular  one can construct an
adaptive agent that is universal with respect to this class by minimizing the average relative
entropy from the environment specific agent 
however  this extension is not straightforward  there is a syntactical difference between
actions and observations that has to be taken into account when formulating the variational
problem  more specifically  actions have to be treated as interventions obeying the rules of
causality  pearl        spirtes  glymour    scheines        dawid         if this distinction
is made  the variational problem has a unique solution given by a stochastic control rule
called the bayesian control rule  this control rule is particularly interesting because it
translates the adaptive control problem into an on line inference problem that can be applied
forward in time  furthermore  this work shows that under mild assumptions  the adaptive
agent converges to the environment specific agent 
the paper is organized as follows  section   introduces notation and sets up the adaptive
control problem  section   formulates adaptive control as a minimum relative entropy
problem  after an initial  nave approach  the need for causal considerations is motivated 
then  the bayesian control rule is derived from a revised relative entropy criterion  in
section    the conditions for convergence are examined and a proof is given  section  
illustrates the usage of the bayesian control rule for the multi armed bandit problem and
undiscounted markov decision processes  section   discusses properties of the bayesian
control rule and relates it to previous work in the literature  section   concludes 

   preliminaries
in the following both agent and environment are formalized as causal models over i o
sequences  agent and environment are coupled to exchange symbols following a standard
interaction protocol having discrete time  observation and control signals  the treatment
of the dynamics are fully probabilistic  and in particular  both actions and observations are
random variables  which is in contrast to the typical decision theoretic agent formulation
treating only observations as random variables  russell   norvig         all proofs are
provided in the appendix 
notation  a set is denoted by a calligraphic letter like a  the words set   alphabet
and element   symbol are used to mean the same thing respectively  strings are finite
concatenations of symbols and sequences are infinite
concatenations  an denotes the set
s

of strings of length n based on a  and a    n  an is the set of finite strings  furthermore  a     a  a         ai  a for all i                is defined as the set of one way
infinite sequences based on the alphabet a  tuples are written with parentheses  a    a    a   
or as strings a  a  a    the notation ai    a  a        ai is a shorthand for a string starting from the first index  also  symbols are underlined to glue them together like ao in
aoi    a  o  a  o        ai oi   the function log x  is meant to be taken w r t  base    unless
indicated otherwise 
interactions  the possible i o symbols are drawn from two finite sets  let o denote the
set of inputs  observations  and let a denote the set of outputs  actions   the set z    ao
is the interaction set  a string aot or ao t at is an interaction string  optionally ending in
   

fia minimum relative entropy principle for learning and acting

at or ot   where ak  a and ok  o  similarly  a one sided infinite sequence a  o  a  o        is
an interaction sequence  the set of interaction strings of length t is denoted by z t   the
sets of  finite  interaction strings and sequences are denoted as z  and z  respectively 
the interaction string of length   is denoted by  
i o system  agents and environments are formalized as i o systems  an i o system
is a probability distribution pr over interaction sequences z    pr is uniquely determined
by the conditional probabilities
pr at  ao t   

pr ot  ao t at  

   

for each aot  z    these conditional probabilities can either represent a generative law
 propensity  in case of issuing a symbol or an evidential probability  plausibility  in the
case of observing a symbol  which of the two interpretations applies in a particular case
becomes apparent once the i o system is coupled to another i o system 

agent
p

a  o  a  o  a  o  a  o  a  o 

environment
q

figure    the model of interactions  the agent p and the environment q define a probability distribution over interaction sequences 

interaction system  let p  q be two i o systems  an interaction system  p  q  is a
coupling of the two systems giving rise to the generative distribution g that describes the
probabilities that actually govern the i o stream once the two systems are coupled  g is
specified by the equations
g at  ao t      p at  ao t  
g ot  ao t at      q ot  ao t at  
valid for all aot  z    here  g models the true probability distribution over interaction
sequences that arises by coupling two systems through their i o streams  more specifically 
for the system p  p at  ao t   is the probability of producing action at  a given history
ao t and p ot  ao t at   is the predicted probability of the observation ot  o given history
   

fiortega   braun

ao t at   hence  for p  the sequence o  o        is its input stream and the sequence a  a       
is its output stream  in contrast  the roles of actions and observations are reversed in the
case of the system q  thus  the sequence o  o        is its output stream and the sequence
a  a        is its input stream  the previous model of interaction is fairly general  and many
other interaction protocols can be translated into this scheme  as a convention  given an
interaction system  p  q   p is an agent to be constructed by the designer  and q is an
environment to be controlled by the agent  figure   illustrates this setup 
control problem  an environment q is said to be known iff the agent p has the property
that for any aot  z   
p ot  ao t at     q ot  ao t at   
intuitively  this means that the agent knows the statistics of the environments future
behavior under any past  and in particular  it knows the effects of given controls  if the
environment is known  then the designer of the agent can build a custom made policy into
p such that the resulting generative distribution g produces interaction sequences that are
desirable  this can be done in multiple ways  for instance  the controls can be chosen
such that the resulting policy maximizes a given utility criterion  or such that the resulting
trajectory of the interaction system stays close enough to a prescribed trajectory  formally 
if q is known  and if the conditional probabilities p at  ao t   for all aot  z  have been
chosen such that the resulting generative distribution g over interaction sequences given
by
g at  ao t     p at  ao t  

g ot  ao t at     q ot  ao t at     p ot  ao t at  
is desirable  then p is said to be tailored to q 
adaptive control problem  if the environment q is unknown  then the task of designing an appropriate agent p constitutes an adaptive control problem  specifically  this
work deals with the case when the designer already has a class of agents that are tailored
to the class of possible environments  formally  it is assumed that q is going to be drawn
with probability p  m  from a set q     qm  mm of possible systems before the interaction starts  where m is a countable set  furthermore  one has a set p     pm  mm
of systems such that for each m  m  pm is tailored to qm and the interaction system
 pm   qm   has a generative distribution gm that produces desirable interaction sequences 
how can the designer construct a system p such that its behavior is as close as possible to
the custom made system pm under any realization of qm  q 

   adaptive systems
the main goal of this paper is to show that the problem of adaptive control outlined in
the previous section can be reformulated as a universal compression problem  this can be
informally motivated as follows  suppose the agent p is implemented as a machine that is
interfaced with the environment q  whenever the agent interacts with the environment 
the agents state changes as a necessary consequence of the interaction  this change in
state can take place in many possible ways  by updating the internal memory  consulting
   

fia minimum relative entropy principle for learning and acting

a random number generator  changing the physical location and orientation  and so forth 
naturally  the design of the agent facilitates some interactions while it complicates others 
for instance  if the agent has been designed to explore a natural environment  then it might
incur into a very low memory footprint when recording natural images  while being very
memory inefficient when recording artificially created images  if one abstracts away from
the inner workings of the machine and decides to encode the state transitions as binary
strings  then the minimal amount of resources in bits that are required to implement these
state changes can be derived directly from the associated probability distribution p  in
the context of adaptive control  an agent can be constructed such that it minimizes the
expected amount of changes necessary to implement the state transitions  or equivalently 
such that it maximally compresses the experience  thereby  compression can be taken as a
stand alone principle to design adaptive agents 
    universal compression and nave construction of adaptive agents
in coding theory  the problem of compressing a sequence of observations from an unknown
source is known as the adaptive coding problem  this is solved by constructing universal compressors  i e  codes that adapt on the fly to any source within a predefined class
 mackay         such codes are obtained by minimizing the average deviation of a predictor from the true source  and then by constructing codewords using the predictor  in this
subsection  this procedure will be used to derive an adaptive agent  ortega   braun        
formally  the deviation of a predictor p from the true distribution pm is measured
by the relative entropy     a first approach would be to construct an agent b so as to
minimize the total expected relative entropy to pm   this is constructed as follows  define
the history dependent relative entropies over the action at and observation ot as
x
pm  at  ao t  
at
 ao t     
dm
pm  at  ao t   log
pr at  ao t  
a
t

ot
 ao t at     
dm

x
ot

pm  ot  ao t at   log

pm  ot  ao t at  
 
pr ot  ao t at  

where pm  ot  ao t at     qm  ot  ao t at   because the qm are known and where pr will be the
argument of the variational problem  then  one removes the dependency on the past by
averaging over all possible histories 
x
at
at
  
 ao t  
pm  ao t  dm
dm
ao t

ot
dm

  

x

ot
 ao t at   
pm  ao t at  dm

ao t at

finally  the total expected relative entropy of pr from pm is obtained by summing up all
time steps and then by averaging over all choices of the true environment 
d    lim sup
t

x

p  m 

m

t
x
   


o
a
 
  dm
dm

   

   the relative entropy is also known as the kl divergence and it measures the average amount of extra
bits that are necessary to encode symbols due to the usage of the  wrong  predictor 

   

fiortega   braun

using      one can define a variational problem with respect to pr  the agent b that one
is looking for is the system pr that minimizes the total expected relative entropy in      i e 
b    arg min d pr  
pr

the solution to equation   is the system b defined by the set of equations
x
b at  ao t    
pm  at  ao t  wm  ao t  
m

b ot  ao t at    

x
m

pm  ot  ao t at  wm  ao t at  

   

   

valid for all aot  z    where the mixture weights are
p  m pm  ao t  

m p  m  pm  ao t  
p  m pm  ao t at  
 
wm  ao t at      p

m p  m  pm  ao t at  
wm  ao t      p

   

for reference  see the work of haussler and opper        and opper         it is clear
that b is just the bayesian mixture over the agents pm   if one defines the conditional
probabilities
p  at  m  ao t      pm  at  ao t  
   
p  ot  m  ao t at      pm  at  ao t at  

for all aot  z    then equation   can be rewritten as
b at  ao t    
b ot  ao t at    

x
m

x
m

p  at  m  ao t  p  m ao t     p  at  ao t  
p  ot  m  ao t at  p  m ao t at     p  ot  ao t at  

   

where the p  m ao t     wm  ao t   and p  m ao t at     wm  ao t at   are just the posterior
probabilities over the elements in m given the past interactions  hence  the conditional
probabilities in     that minimize the total expected divergence are just the predictive
distributions p  at  ao t   and p  ot  ao t at   that one obtains by standard probability theory 
and in particular  bayes rule  this is interesting  as it provides a teleological interpretation
for bayes rule 
the behavior of b can be described as follows  at any given time t  b maintains a
mixture over systems pm   the weighting over them is given by the mixture coefficients
wm   whenever a new action at or a new observation ot is produced  by the agent or
the environment respectively   the weights wm are updated according to bayes rule  in
addition  b issues an action at suggested by a system pm drawn randomly according to the
weights wt  
however  there is an important problem with b that arises due to the fact that it is not
only a system that is passively observing symbols  but also actively generating them  in
the subjective interpretation of probability theory  conditionals play the role of observations
   

fia minimum relative entropy principle for learning and acting

made by the agent that have been generated by an external source  this interpretation suits
the symbols o    o    o          because they have been issued by the environment  however  symbols that are generated by the system itself require a fundamentally different belief update 
intuitively  the difference can be explained as follows  observations provide information
that allows the agent inferring properties about the environment  in contrast  actions do
not carry information about the environment  and thus have to be incorporated differently
into the belief of the agent  in the following section we illustrate this problem with a simple
statistical example 
    causality
causality is the study of the functional dependencies of events  this stands in contrast to
statistics  which  on an abstract level  can be said to study the equivalence dependencies
 i e  co occurrence or correlation  amongst events  causal statements differ fundamentally
from statistical statements  examples that highlight the differences are many  such as
do smokers get lung cancer  as opposed to do smokers have lung cancer   assign
y  f  x  as opposed to compare y   f  x  in programming languages  and a  f m
as opposed to f   m a in newtonian physics  the study of causality has recently enjoyed
considerable attention from researchers in the fields of statistics and machine learning 
especially over the last decade  significant progress has been made towards the formal
understanding of causation  shafer        pearl        spirtes et al         dawid        
in this subsection  the aim is to provide the essential tools required to understand causal
interventions  for a more in depth exposition of causality  the reader is referred to the
specialized literature 
to illustrate the need for causal considerations in the case of generated symbols  consider
the following thought experiment  suppose a statistician is asked to design a model for a
simple time series x    x    x          and she decides to use a bayesian method  assume she
collects a first observation x    x    she computes the posterior probability density function
 pdf  over the parameters  of the model given the data using bayes rule 
p  x    x      r

p x    x    p  
 
p x    x     p    d

where p x    x     is the likelihood of x  given  and p   is the prior pdf of   she can
use the model to predict the next observation by drawing a sample x  from the predictive
pdf
z
p x    x   x    x      p x    x   x    x      p  x    x    d 
where p x    x   x    x      is the likelihood of x  given x  and   note that x  is not
drawn from p x    x   x    x       she understands that the nature of x  is very different
from x    while x  is informative and does change the belief state of the bayesian model 
x  is non informative and thus is a reflection of the models belief state  hence  she would
never use x  to further condition the bayesian model  mathematically  she seems to imply
that
p  x    x    x    x      p  x    x   
   

fiortega   braun

if x  has been generated from p x   x    x    itself  but this simple independence assumption is not correct as the following elaboration of the example will show 
the statistician is now told that the source is waiting for the simulated data point x 
in order to produce a next observation x    x  which does depend on x    she hands in x 
and obtains a new observation x    using bayes rule  the posterior pdf over the parameters
is now
p x    x   x    x    x    x      p x    x     p  
r
   
p x    x   x    x    x    x       p x    x      p    d
where p x    x   x    x    x    x      is the likelihood of the new data x  given the old
data x    the parameters  and the simulated data x    notice that this looks almost like the
posterior pdf p  x    x    x    x    x    x    given by
r

p x    x   x    x    x    x      p x    x   x    x      p x    x     p  
p x    x   x    x    x    x       p x    x   x    x       p x    x      p    d

with the exception that in the latter case  the bayesian update contains the likelihoods of
the simulated data p x    x   x    x       this suggests that equation   is a variant of the
posterior pdf p  x    x    x    x    x    x    but where the simulated data x  is treated
in a different way than the data x  and x   
define the pdf p such that the pdfs p     p  x      p  x   x    x      are identical to
p    p x     and p x   x    x      respectively  but differ in p  x   x      
p  x   x         x   x    
where  is the dirac delta function  that is  p is identical to p but it assumes that the
value of x  is fixed to x  given x  and   for p   the simulated data x  is non informative 
 log  p  x    x   x          
if one computes the posterior pdf p   x    x    x    x    x    x     one obtains the result
of equation   
r

p  x    x   x    x    x    x      p  x    x   x    x      p  x    x     p   
p  x    x   x    x    x    x      p  x    x   x    x       p  x    x      p     d
p x    x   x    x    x    x      p x    x     p  
 r
 
p x    x   x    x    x    x       p x    x      p    d

thus  in order to explain equation   as a posterior pdf given the observed data x  and x 
and the generated data x    one has to intervene p in order to account for the fact that x 
is non informative given x  and   in other words  the statistician  by defining the value of
x  herself    has changed the  natural  regime that brings about the series x    x    x          
which is mathematically expressed by redefining the pdf 
two essential ingredients are needed to carry out interventions  first  one needs to
know the functional dependencies amongst the random variables of the probabilistic model 
this is provided by the causal model  i e  the unique factorization of the joint probability
   note that this is conceptually broken down into two steps  first  she samples x  from p x   x    x    
and second  she imposes the value x    x  by setting p  x   x         x   x    

   

fia minimum relative entropy principle for learning and acting

distribution over the random variables encoding the causal dependencies  in the general
case  this defines a partial order over the random variables  in the previous thought experiment  the causal model of the joint pdf p   x    x    x    is given by the set of conditional
pdfs
p    p x      p x   x       p x   x    x      
second  one defines the intervention that sets x to the value x  denoted as x  x  as
the operation on the causal model replacing the conditional probability of x by a dirac
delta function  x  x  or a kronecker delta xx for a continuous or a discrete variable x
respectively  in our thought experiment  it is easily seen that
p    x    x    x    x    x    x      p   x    x    x   x    x    x   
and thereby 
p   x    x    x    x    x    x      p  x    x    x   x    x    x    
causal models contain additional information that is not available in the joint probability
distribution alone  the appropriate model for a given situation depends on the story that
is being told  note that an intervention can lead to different results if the respective causal
models differ  thus  if the causal model had been
p x     p x   x     p x   x    x     p  x    x    x   
then the intervention x   x  would differ from p   i e 
p    x    x    x    x    x    x       p   x    x    x   x    x    x    
even though both causal models represent the same joint probability distribution  in the
following  this paper will use the shorthand notation x    x  x when the random variable
is obvious from the context 
    causal construction of adaptive agents
following the discussion in the previous section  an adaptive agent p is going to be constructed by minimizing the expected relative entropy to the expected pm   but this time
treating actions as interventions  based on the definition of the conditional probabilities in
equation    the total expected relative entropy to characterize p using interventions is going to be defined  assuming the environment is chosen first  and that each symbol depends
functionally on the environment and all the previously generated symbols  the causal model
is given by
p  m   p  a   m   p  o   m  a     p  a   m  a    o     p  o   m  a    o    a          
importantly  interventions index a set of intervened probability distributions derived from
a base probability distribution  hence  the set of fixed intervention sequences of the form
a    a          indexes probability distributions over observation sequences o    o           because
of this  one defines a set of criteria indexed by the intervention sequences  but it will be
   

fiortega   braun

clear that they all have the same solution  define the history dependent intervened relative
entropies over the action at and observation ot as
at
 ao t     
cm

x
at

ot
 ao t at     
cm

x
ot

p  at  m  ao t   log 

p  at  m  ao t  
pr at  ao t  

p  ot  m  ao t at   log 

p  ot  m  ao t at  
 
pr ot  ao t at  

where pr is a given arbitrary agent  note that past actions are treated as interventions  in
particular  p  at  m  ao t   represents the knowledge state when the past actions have already
been issued but the next action at is not known yet  then  averaging the previous relative
entropies over all pasts yields
at
 
cm

x

ao t
ot
 
cm

at
 ao t  
p  ao t  m cm

x

ao t at

ot
 ao t at   
p  ao t at  m cm

at  ao   and c ot  ao a   
here again  because of the knowledge state in time represented by cm
 t
 t t
m
the averages are taken treating past actions as interventions  finally  define the total exat   c ot   over time  averaged over
pected relative entropy of pr from pm as the sum of  cm
m
the possible draws of the environment 

c    lim sup
t

x

p  m 

m

t
x
   


o
a
  cm
cm
 

   

the variational problem consists in choosing the agent p as the system pr minimizing
c   c pr   i e 
p    arg min c pr  
    
pr

the following theorem shows that this variational problem has a unique solution  which will
be the central theme of this paper 
theorem    the solution to equation    is the system p defined by the set of equations
x

p at  ao t     p  at  ao t    

m

p ot  ao t at     p  ot  ao t at    

p  at  m  ao t  vm  ao t  

x
m

p  ot  m  ao t at  vm  ao t at  

    

valid for all aot  z    where the mixture weights are
qt 

    p  o  m  ao  a  
 
qt 


    p  o  m   ao  a  
m p  m  

vm  ao t at     vm  ao t      p

p  m 

   

    

fia minimum relative entropy principle for learning and acting

bayesian control rule  given a set of operation modes  p   m    mm
over interaction sequences in z  and a prior distribution p  m  over the
parameters m  the probability of the action at   is given by
x
p  at    m  aot  p  m aot   
    
p  at    aot    
m

where the posterior probability over operation modes is given by the recursion
p  ot  m  ao t  p  m ao t  
 


m p  ot  m   ao t  p  m  ao t  

p  m aot     p

table    summary of the bayesian control rule 
the theorem says that the optimal solution to the variational problem in      is precisely
the predictive distribution over actions and observations treating actions as interventions
and observations as conditionals  i e  it is the solution that one would obtain by applying
only standard probability and causal calculus  this provides a teleological interpretation for
the agent p akin to the nave agent b constructed in section      the behavior of p differs
in an important aspect from b  at any given time t  p maintains a mixture over systems
pm   the weighting over these systems is given by the mixture coefficients vm   in contrast
to b  p updates the weights vm only whenever a new observation ot is produced by the
environment  the update follows bayes rule but treats past actions as interventions by
dropping the evidence they provide  in addition  p issues an action at suggested by an
system m drawn randomly according to the weights vm  
    summary
adaptive control is formalized as the problem of designing an agent for an unknown environment chosen from a class of possible environments  if the environment specific agents are
known  then the bayesian control rule allows constructing an adaptive agent by combining
these agents  the resulting adaptive agent is universal with respect to the environment
class  in this context  the constituent agents are called the operation modes of the adaptive
agent  they are represented by causal models over the interaction sequences  i e  conditional
probabilities p  at  m  ao t   and p  ot  m  ao t   for all aot  z    and where m  m is the
index or parameter characterizing the operation mode  the probability distribution over
the input stream  output stream  is called the hypothesis  policy  of the operation mode 
table   collects the essential equations of the bayesian control rule  in particular  there the
rule is stated using a recursive belief update 

   convergence
the aim of this section is to develop a set of sufficient conditions of convergence and then
to provide a proof of convergence  to simplify the exposition  the analysis has been limited
   

fiortega   braun

to the case of controllers having a finite number of input output models 

    policy diagrams
in the following we use policy diagrams as a useful informal tool to analyze the effect of
policies on environments  figure   illustrates an example 

state space
s

ao

s

policy

figure    a policy diagram  one can imagine an environment as a collection of states
connected by transitions labeled by i o symbols  the zoom highlights a state s
where taking action a  a and collecting observation o  o leads to state s  
sets of states and transitions are represented as enclosed areas similar to a venn
diagram  choosing a particular policy in an environment amounts to partially
controlling the transitions taken in the state space  thereby choosing a probability
distribution over state transitions  e g  a markov chain given by the environmental
dynamics   if the probability mass concentrates in certain areas of the state space 
choosing a policy can be thought of as choosing a subset of the environments
dynamics  in the following  a policy is represented by a subset in state space
 enclosed by a directed curve  as illustrated above 

policy diagrams are especially useful to analyze the effect of policies on different hypotheses about the environments dynamics  an agent that is endowed with a set of operation
modes m can be seen as having hypotheses about the environments underlying dynamics 
given by the observation models p  ot  m  ao t at    and associated policies  given by the action models p  at  m  ao t    for all m  m  for the sake of simplifying the interpretation of
policy diagrams  we will assume the existence of a state space t    a  o   s mapping
i o histories into states  note however that no such assumptions are made to obtain the
results of this section 
    divergence processes
the central question in this section is to investigate whether the bayesian control rule converges to the correct control law or not  that is  whether p  at  aot    p  at  m   ao t   as t 
 when m is the true operation mode  i e  the operation mode such that p  ot  m   ao t at    
q ot  ao t at    as will be obvious from the discussion in the rest of this section  this is in
general not true 
as it is easily seen from equation     showing convergence amounts to show that the
posterior distribution p  m ao t   concentrates its probability mass on a subset of operation
   

fia minimum relative entropy principle for learning and acting

modes m having essentially the same output stream as m  
x
x
p  at  m  ao t  p  m ao t   
p  at  m   ao t  p  m ao t    p  at  m   ao t   
mm

mm

hence  understanding the asymptotic behavior of the posterior probabilities
p  m aot  
is crucial here  in particular  we need to understand under what conditions these quantities
converge to zero  the posterior can be rewritten as
q
p  aot  m p  m 
p  m  t    p  o  m  ao  a  
 
 p
p  m aot     p
qt




m m p  aot  m  p  m  
m m p  m  
    p  o  m   ao  a  

if all the summands but the one with index m are dropped from the denominator  one
obtains the bound
p  m aot   

t
p  m  y p  o  m  ao  a  
 
p  m  
p  o  m   ao  a  
   

which is valid for all m  m  from this inequality  it is seen that it is convenient to
analyze the behavior of the stochastic process


dt  m km    

t
x
   

ln

p  o  m   ao  a  
p  o  m  ao  a  

which is the divergence process of m from the reference m   indeed  if dt  m km    as
t    then
t
p  m 
p  m  y p  o  m  ao  a  

  lim
 edt  m km      



t p  m  
p  o  m   ao  a   t p  m  

lim

   

and thus clearly p  m aot       figure   illustrates simultaneous realizations of the
divergence processes of a controller  intuitively speaking  these processes provide lower
bounds on accumulators of surprise value measured in information units 
a divergence process is a random walk whose value at time t depends on the whole
history up to time t   what makes these divergence processes cumbersome to characterize
is the fact that their statistical properties depend on the particular policy that is applied 
hence  a given divergence process can have different growth rates depending on the policy
 figure     indeed  the behavior of a divergence process might depend critically on the
distribution over actions that is used  for example  it can happen that a divergence process
stays stable under one policy  but diverges under another  in the context of the bayesian
control rule this problem is further aggravated  because in each time step  the policy that
is applied is determined stochastically  more specifically  if m is the true operation mode 
then dt  m km  is a random variable that depends on the realization aot which is drawn
from
t
y

   

p  a  m   ao  p  o  m   ao a   
   

fiortega   braun

dt
 
 
 
 
t

 

figure    realization of the divergence processes   to   associated to a controller with
operation modes m  to m    the divergence processes   and   diverge  whereas  
and   stay below the dotted bound  hence  the posterior probabilities of m  and
m  vanish 

dt
 

 
 

 

 
 

 
t

figure    the application of different policies lead to different statistical properties of the
same divergence process 

   

fia minimum relative entropy principle for learning and acting

where the m    m            mt are drawn themselves from p  m     p  m   ao             p  mt  ao t   
to deal with the heterogeneous nature of divergence processes  one can introduce a
temporal decomposition that demultiplexes the original process into many sub processes
belonging to unique policies  let nt                   t  be the set of time steps up to time t 
let t  nt   and let m  m  m  define a sub divergence of dt  m km  as a random variable
x p  o  m   ao a  
 
gm  m  t     
ln
p  o  m  ao  a  
 t

drawn from

pm   ao   t   ao   t      


y
 t

 y

p  a  m   ao   
p  o  m   ao  a    
 t

where t    nt   t and where  ao   t  are given conditions that are kept constant  in
this definition  m plays the role of the policy that is used to sample the actions in the time
steps t   clearly  any realization of the divergence process dt  m km  can be decomposed
into a sum of sub divergences  i e 
x
gm  m  tm   
dt  m km   
    
m

where  tm  mm forms a partition of nt   figure   shows an example decomposition 
dt
 
 
 

t

 

figure    decomposition of a divergence process     into sub divergences         
the averages of sub divergences will play an important role in the analysis  define the
average over all realizations of gm  m  t   as
x
pm   ao   t   ao   t   gm  m  t   
gm  m  t     
 ao   t

notice that for any   nt  
x
p  o  m   ao  a  
p  a  m   ao   p  o  m   ao  a   ln
   
gm  m        
p  o  m  ao  a  
ao


because of gibbs inequality  in particular 

gm  m            

clearly  this holds as well for any t  nt  
m

gm  m  t      

gm  m   t       
   

    

fiortega   braun

    boundedness
in general  a divergence process is very complex  virtually all the classes of distributions
that are of interest in control go well beyond the assumptions of i i d  and stationarity  this
increased complexity can jeopardize the analytic tractability of the divergence process  such
that no predictions about its asymptotic behavior can be made anymore  more specifically 
if the growth rates of the divergence processes vary too much from realization to realization  then the posterior distribution over operation modes can vary qualitatively between
realizations  hence  one needs to impose a stability requirement akin to ergodicity to limit
the class of possible divergence processes to a class that is analytically tractable  for this
purpose the following property is introduced 
a divergence process dt  m km  is said to have bounded variation in m iff for any      
there is a c     such that for all m  m  all t and all t  nt
fi
fi
fi
fi
figm  m  t    gm  m  t  fi  c
with probability      

dt

 

 

 

t

 

figure    if a divergence process has bounded variation  then the realizations  curves    
   of a sub divergence stay within a band around the mean  curve    
figure   illustrates this property  boundedness is the key property that is going to be
used to construct the results of this section  the first important result is that the posterior
probability of the true input output model is bounded from below 
theorem    let the set of operation modes of a controller be such that for all m  m the
divergence process dt  m km  has bounded variation  then  for any       there is a      
such that for all t  n 

p  m  aot   
 m 

with probability      
    core

if one wants to identify the operation modes whose posterior probabilities vanish  then it
is not enough to characterize them as those modes whose hypothesis does not match the
true hypothesis  figure   illustrates this problem  here  three hypotheses along with their
associated policies are shown  h  and h  share the prediction made for region a but differ
   

fia minimum relative entropy principle for learning and acting

in region b  hypothesis h  differs everywhere from the others  assume h  is true  as long
as we apply policy p    hypothesis h  will make wrong predictions and thus its divergence
process will diverge as expected  however  no evidence against h  will be accumulated  it
is only when one applies policy p  for long enough time that the controller will eventually
enter region b and hence accumulate counter evidence for h   
h 

h 

b
a

h 

b
a

p 

p 
p 

figure    if hypothesis h  is true and agrees with h  on region a  then policy p  cannot
disambiguate the three hypotheses 

but what does long enough mean  if p  is executed only for a short period  then the
controller risks not visiting the disambiguating region  but unfortunately  neither the right
policy nor the right length of the period to run it are known beforehand  hence  an agent
needs a clever time allocating strategy to test all policies for all finite time intervals  this
motivates the following definition 
the core of an operation mode m   denoted as  m    is the subset of m containing
operation modes behaving like m under its policy  more formally  an operation mode
m
   m    i e  is not in the core  iff for any c           there is a      and a t   n 
such that for all t  t   
gm  m  t    c
with probability       where gm  m  t   is a sub divergence of dt  m km   and pr  
t     for all   nt  
in other words  if the agent was to apply m s policy in each time step with probability at
least   and under this strategy the expected sub divergence gm  m  t   of dt  m km  grows
unboundedly  then m is not in the core of m   note that demanding a strictly positive
probability of execution in each time step guarantees that the agent will run m for all
possible finite time intervals  as the following theorem shows  the posterior probabilities of
the operation modes that are not in the core vanish almost surely 
theorem    let the set of operation modes of an agent be such that for all m  m the
divergence process dt  m km  has bounded variation  if m 
   m    then p  m aot      as
t   almost surely 
    consistency
even if an operation mode m is in the core of m   i e  given that m is essentially indistinguishable from m under m s control  it can still happen that m and m have different
policies  figure   shows an example of this  the hypotheses h  and h  share region a but
   

fiortega   braun

differ in region b  in addition  both operation modes have their policies p  and p  respectively confined to region a  note that both operation modes are in the core of each other 
however  their policies are different  this means that it is unclear whether multiplexing the
policies in time will ever disambiguate the two hypotheses  this is undesirable  as it could
impede the convergence to the right control law 
h 

h 
b

b
p 

p 

a

a

figure    an example of inconsistent policies  both operation modes are in the core of each
other  but have different policies 

thus  it is clear that one needs to impose further restrictions on the mapping of hypotheses into policies  with respect to figure    one can make the following observations 
   both operation modes have policies that select subsets of region a  therefore  the
dynamics in a are preferred over the dynamics in b 
   knowing that the dynamics in a are preferred over the dynamics in b allows us to
drop region b from the analysis when choosing a policy 
   since both hypotheses agree in region a  they have to choose the same policy in order
to be consistent in their selection criterion 
this motivates the following definition  an operation mode m is said to be consistent
with m iff m   m   implies that for all       there is a t    such that for all t  t  and all
ao t at  
fi
fi
fi
fi
fip  at  m  aot    p  at  m   aot  fi    

in other words  if m is in the core of m   then ms policy has to converge to m s policy 
the following theorem shows that consistency is a sufficient condition for convergence to
the right control law 
theorem    let the set of operation modes of an agent be such that  for all m  m the
divergence process dt  m km  has bounded variation  and for all m  m  m  m is consistent
with m   then 
p  at  ao t    p  at  m   ao t  
almost surely as t   
   

fia minimum relative entropy principle for learning and acting

    summary
in this section  a proof of convergence of the bayesian control rule to the true operation
mode has been provided for a finite set of operation modes  for this convergence result to
hold  two necessary conditions are assumed  boundedness and consistency  the first one 
boundedness  imposes the stability of divergence processes under the partial influence of the
policies contained within the set of operation modes  this condition can be regarded as
an ergodicity assumption  the second one  consistency  requires that if a hypothesis makes
the same predictions as another hypothesis within its most relevant subset of dynamics 
then both hypotheses share the same policy  this relevance is formalized as the core of an
operation mode  the concepts and proof strategies strengthen the intuition about potential
pitfalls that arise in the context of controller design  in particular we could show that
the asymptotic analysis can be recast as the study of concurrent divergence processes that
determine the evolution of the posterior probabilities over operation modes  thus abstracting
away from the details of the classes of i o distributions  the extension of these results to
infinite sets of operation modes is left for future work  for example  one could think
of partitioning a continuous space of operation modes into essentially different regions
where representative operation modes subsume their neighborhoods  grunwald        

   examples
in this section we illustrate the usage of the bayesian control rule on two examples that
are very common in the reinforcement learning literature  multi armed bandits and markov
decision processes 
    bandit problems
consider the multi armed bandit problem  robbins         the problem is stated as follows 
suppose there is an n  armed bandit  i e  a slot machine with n levers  when pulled  lever
i provides a reward drawn from a bernoulli distribution with a bias hi specific to that lever 
that is  a reward r     is obtained with probability hi and a reward r     with probability
 hi   the objective of the game is to maximize the time averaged reward through iterative
pulls  there is a continuum range of stationary strategies  each one parameterized by n
probabilities  si  n
i   indicating the probabilities of pulling each lever  the difficulty arising
in the bandit problem is to balance reward maximization based on the knowledge already
acquired with attempting new actions to further improve knowledge  this dilemma is known
as the exploration versus exploitation tradeoff  sutton   barto        
this is an ideal task for the bayesian control rule  because each possible bandit has a
known optimal agent  indeed  a bandit can be represented by an n  dimensional bias vector
m    m            mn    m         n   given such a bandit  the optimal policy consists in
pulling the lever with the highest bias  that is  an operation mode is given by 
hi   p  ot     m  at   i    mi

si   p  at   i m   

   

 

  if i   maxj  mj   
  else 

fiortega   braun

m 

 

a 

 

b 
m   m 

m 

 

 

m   m    m  

m 
 

m 

 
 

m 

 

 

figure    the space of bandit configurations can be partitioned into n regions according
to the optimal lever  panel a and b show the   armed and   armed bandit cases
respectively 

to apply the bayesian control rule  it is necessary to fix a prior distribution over the
bandit configurations  assuming a uniform distribution  the bayesian control rule is
z
    
p  at     i m p  m aot  
p  at     i aot    
m

with the update rule given by
q
r
n
y
mj j     mj  fj
p  m  t    p  o  m  a  
 
p  m aot     r
qt



b rj      fj     
    p  o  m   a   dm
m p  m  
j  

    

where rj and fj are the counts of the number of times a reward has been obtained from
pulling lever j and the number of times no reward was obtained respectively  observe that
here the summation over discrete operation modes has been replaced by an integral over
the continuous space of configurations  in the last expression we see that the posterior
distribution over the lever biases is given by a product of n beta distributions  thus 
sampling an action amounts to first sample an operation mode m by obtaining each bias
mi from a beta distribution with parameters ri     and fi      and then choosing the
action corresponding to the highest bias a   arg maxi mi   the pseudo code can be seen in
algorithm   
simulation  the bayesian control rule described above has been compared against two
other agents  an  greedy strategy with decay  on line  and gittins indices  off line   the
test bed consisted of bandits with n      levers whose biases were drawn uniformly at
the beginning of each run  every agent had to play      runs for      time steps each 
then  the performance curves of the individual runs were averaged  the  greedy strategy
selects a random action with a small probability given by t and otherwise plays the
lever with highest expected reward  the parameters have been determined empirically to
the values         and         after several test runs  they have been adjusted in a way
to maximize the average performance in the last trials of our simulations  for the gittins
method  all the indices were computed up to horizon      using a geometric discounting
of           i e  close to one to approximate the time averaged reward  the results are
shown in figure    
   

fia minimum relative entropy principle for learning and acting

algorithm   bcr bandit 
for all i              n do
initialize ri and fi to zero 
end for
for t                  do
sample m using      
  interaction  
set a  arg maxi mi and issue a 
obtain o from environment 

avg  reward

 update belief 
if o     then
ra   ra    
else
fa   fa    
end if
end for

    
    

bayesian control rule
 greedy
gittins indices

    
    
 

   

   

   

   

    

 

   

   

   

   

    

  best lever

   
  
  
  
  
 

figure     comparison in the n  armed bandit problem of the bayesian control rule  solid
line   an  greedy agent  dashed line  and using gittins indices  dotted line  
      runs have been averaged  the top panel shows the evolution of the average
reward  the bottom panel shows the evolution of the percentage of times the
best lever was pulled 

   

fiortega   braun

it is seen that  greedy strategy quickly reaches an acceptable level of performance  but
then seems to stall at a significantly suboptimal level  pulling the optimal lever only     of
the time  in contrast  both the gittins strategy and the bayesian control rule show essentially the same asymptotic performance  but differ in the initial transient phase where the
gittins strategy significantly outperforms the bayesian control rule  there are at least three
observations that are worth making here  first  gittins indices have to be pre computed
off line  the time complexity scales quadratically with the horizon  and the computations
for the horizon of      steps took several hours on our machines  in contrast  the bayesian
control rule could be applied without pre computation  second  even though the gittins
method actively issues the optimal information gathering actions while the bayesian control
rule passively samples the actions from the posterior distribution over operation modes  in
the end both methods rely on the convergence of the underlying bayesian estimator  this
implies that both methods have the same information bottleneck  since the bayesian estimator requires the same amount of information to converge  thus  active information gathering
actions only affect the utility of the transient phase  not the permanent state  other efficient algorithms for bandit problems can be found in the literature  auer  cesabianchi   
fischer        
    markov decision processes
a markov decision process  mdp   is defined as a tuple  x   a  t  r   x is the state space 
a is the action space  ta  x  x     pr x  a  x  is the probability that an action a  a
taken in state x  x will lead to state x  x   and r x  a   r    r is the immediate
reward obtained in state x  x and action a  a  the interaction proceeds in time steps
t               where at time t  action at  a is issued in state xt   x   leading to a reward
rt   r xt    at   and a new state xt that starts the next time step t      a stationary closedloop control policy    x  a assigns an action to each state  for mdps there always
exists an optimal stationary deterministic policy and thus one only needs to consider such
policies  in undiscounted mdps the average rewardpper time step for a fixed policy  with
initial state x is defined as   x    limt e    t t    r    it can be shown  bertsekas 
      that   x      x   for all x  x  x under the assumption that the markov chain for
policy  is ergodic  here  we assume that the mdps are ergodic for all stationary policies 
in order to keep the intervention model particularly simple    we follow the q notation
of watkins         the optimal policy   can then be characterized in terms of the optimal
average reward  and the optimal relative q values q x  a  for each state action pair  x  a 
that are solutions to the following system of non linear equations  singh         for any

   the brute force adaptive agent for this problem would roughly look as follows  first  the agent
starts with a prior distribution over all mdps  e g  product of dirichlet distributions over the transition
probabilities  then  in each cycle  the agent samples a full transition matrix from the distribution and
solves it using dynamic programming  once it has computed the optimal policy  it uses it to issue the
next action  and then discards the policy  subsequently  it updates the distribution over mdps using
the next observed state  however  in the main text we follow a different approach that avoids solving
an mdp in every time step 

   

fia minimum relative entropy principle for learning and acting

state x  x and action a  a 
q x  a       r x  a   

x

x x

i
h
 
q x
 
a
 
pr x  x  a  max

a

fi
i
h
  fi
  r x  a    ex max
x 
a
 
q x
 
a
 
fi


    

a

the optimal policy can then be defined as    x     arg maxa q x  a  for any state x  x  
again this setup allows for a straightforward solution with the bayesian control rule 
because each learnable mdp  characterized by the q values and the average reward  has
a known solution     accordingly  an operation mode m is given by m    q     m  
r a  o      to obtain a likelihood model for inference over m  we realize that equation   
can be rewritten such that it predicts the instantaneous reward r x  a  as the sum of a mean
instantaneous reward m plus a noise term  given the q values and the average reward 
for the mdp labeled by m
r x  a    q x  a      max
q x   a     max
q x   a    e max
q x   a   x  a 
a
a
a
 
 z
 
 z
 
 
noise 

mean instantaneous reward m  x a x  

assuming that  can be reasonably approximated by a normal distribution n      p  with
precision p  we can write down a likelihood model for the immediate reward r using the
q values and the average reward  i e 
r
o
n p
p

  
p  r m  x  a  x    
    
exp   r  m  x  a  x     
 
 

in order to determine the intervention model for each operation mode  we can simply exploit
the above properties of the q values  which gives
 
  if a   arg maxa q x  a  
p  a m  x   
    
  else 
to apply the bayesian control rule  the posterior distribution p  m at   xt   needs to be
computed  fortunately  due to the simplicity of the likelihood model  one can easily devise a
conjugate prior distribution and apply standard inference methods  see appendix a     actions are again determined by sampling operation modes from this posterior and executing
the action suggested by the corresponding intervention models  the resulting algorithm is
very similar to bayesian q learning  dearden  friedman    russell        dearden  friedman    andre         but differs in the way actions are selected  the pseudo code is listed
in algorithm   
simulation  we have tested our mdp agent in a grid world example  to give an intuition
of the achieved performance  the results are contrasted with those achieved by r learning 
we have used the r learning variant presented in the work of singh        algorithm   
together with the uncertainty exploration strategy  mahadevan         the corresponding
update equations are

q x  a        q x  a     r     max
q x   a  

a

    
 
           r   max
q x
 
a
 

q x 
a 
 

a

   

fiortega   braun

algorithm   bcr mdp gibbs sampler 
initialize entries of  and  to zero 
set initial state to x  x   
for t                  do
 gibbs sweep 
sample  using      
for all q y  b  of visited states do
sample q y  b  using      
end for
  interaction  
set a  arg maxa q x  a   and issue a 
obtain o    r  x   from environment 
 update hyperparameters 
   x a x   p r
 x  a  x     x a x
 x a x   p
 x  a  x     x  a  x     p
set x  x  
end for

goal

membranes

b  bayesian control rule

c  r learning  c  

d  r learning  c   

initial       steps

a   x  maze

e  r learning  c    

f  average reward
   
   

c   
c  

   

low
probability

c    
   

last       steps

high
probability

bayesian control rule
   
 

   

   

   

   

x     time steps

figure     results for the    grid world domain  panel  a  illustrates the setup  columns
 b   e  illustrate the behavioral statistics of the algorithms  the upper and lower
row have been calculated over the first and last       time steps of randomly
chosen runs  the probability of being in a state is color encoded  and the arrows
represent the most frequent actions taken by the agents  panel  f  presents the
curves obtained by averaging ten runs 

   

fia minimum relative entropy principle for learning and acting

average reward
bcr
r learning  c      
r learning  c     
r learning  c    

              
              
              
              

table    average reward attained by the different algorithms at the end of the run  the
mean and the standard deviation has been calculated based on    runs 

where        are learning rates  the exploration strategy chooses with fixed probability
c
pexp     the action a that maximizes q x  a    f  x a 
  where c is a constant  and f  x  a 
represents the number of times that action a has been tried in state x  thus  higher values
of c enforce increased exploration 
in a study  mahadevan         a grid world is described that is especially useful as
a test bed for the analysis of rl algorithms  for our purposes  it is of particular interest
because it is easy to design experiments containing suboptimal limit cycles  figure     panel
 a   illustrates the      grid world  a controller has to learn a policy that leads it from
any initial location to the goal state  at each step  the agent can move to any adjacent
space  up  down  left or right   if the agent reaches the goal state then its next position
is randomly set to any square of the grid  with uniform probability  to start another trial 
there are also one way membranes that allow the agent to move into one direction but
not into the other  in these experiments  these membranes form inverted cups that the
agent can enter from any side but can only leave through the bottom  playing the role
of a local maximum  transitions are stochastic  the agent moves to the correct square
 
with probability p     
and to any of the free adjacent spaces  uniform distribution  with
 
probability    p        rewards are assigned as follows  the default reward is r     
if the agent traverses a membrane it obtains a reward of r      reaching the goal state
assigns r        the parameters chosen for this simulation were the following  for our
mdp agent  we have chosen hyperparameters       and       and precision p     
for r learning  we have chosen learning rates        and           and the exploration
constant has been set to c      c      and to c        a total of    runs were carried
out for each algorithm  the results are presented in figure    and table    r learning only
learns the optimal policy given sufficient exploration  panels d   e  bottom row   whereas
the bayesian control rule learns the policy successfully  in figure   f  the learning curve of
r learning for c     and c      is initially steeper than the bayesian controller  however 
the latter attains a higher average reward around time step         onwards  we attribute
this shallow initial transient to the phase where the distribution over the operation modes
is flat  which is also reflected by the initially random exploratory behavior 

   discussion
the key idea of this work is to extend the minimum relative entropy principle  i e  the
variational principle underlying bayesian estimation  to the problem of adaptive control 
   

fiortega   braun

from a coding point of view  this work extends the idea of maximal compression of the
observation stream to the whole experience of the agent containing both the agents actions
and observations  this not only minimizes the amount of bits to write when saving encoding
the i o stream  but it also minimizes the amount of bits required to produce decode an
action  mackay        ch     
this extension is non trivial  because there is an important caveat for coding i o sequences  unlike observations  actions do not carry any information that could be used for
inference in adaptive coding because actions are issued by the decoder itself  the problem
is that doing inference on ones own actions is logically inconsistent and leads to paradoxes
 nozick         this seemingly innocuous issue has turned out to be very intricate and
has been investigated intensely in the recent past by researchers focusing on the issue of
causality  pearl        spirtes et al         dawid         our work contributes to this body
of research by providing further evidence that actions cannot be treated using probability
calculus alone 
if the causal dependencies are carefully taken into account  then minimizing the relative
entropy leads to a rule for adaptive control which we called the bayesian control rule  this
rule allows combining a class of task specific agents into an agent that is universal with
respect to this class  the resulting control law is a simple stochastic control rule that is
completely general and parameter free  as the analysis in this paper shows  this control
rule converges to the true control law under mild assumptions 
    critical issues
 causality  virtually every adaptive control method in the literature successfully treats
actions as conditionals over observation streams and never worries about causality 
thus  why bother about interventions  in a decision theoretic setup  the decision
maker chooses a policy     maximizing
p the expected utility u over the outcomes
    i e       arg max e u       pr   u     choosing    is formally
equivalent to choosing the kronecker delta function  as the probability distribution
over policies  in this case  the conditional probabilities pr    and pr    coincide 
since
pr       pr  pr       pr      pr     
in this sense  the choice of the policy causally precedes the interactions  as we have
discussed in section   however  when there is uncertainty about the policy  i e  pr     
    then causal belief updates are crucial  essentially  this problem arises because
the uncertainty over the policy is resolved during the interactions  hence  treating
actions as interventions seamlessly extends them to the status of random variables 
 where do prior probabilities likelihood models policies come from  the predictor in
the bayesian control rule is essentially a bayesian predictor and thereby entails  almost  the same modeling paradigm  the designer has to define a class of hypotheses
over the environments  construct appropriate likelihood models  and choose a suitable
prior probability distribution to capture the models uncertainty  similarly  under sufficient domain knowledge  an analogous procedure can be applied to construct suitable
operation modes  however  there are many situations where this is a difficult or even
   

fia minimum relative entropy principle for learning and acting

intractable problem in itself  for example  one can design a class of operation modes
by pre computing the optimal policies for a given class of environments  formally  let
 be a class of hypotheses modeling environments and let  be class of policies  given
a utility criterion u   define the set of operation modes m     m   by constructing each operation mode as m                 where      arg max e u      
however  computing the optimal policy   is in many cases intractable  in some
cases  this can be remedied by characterizing the operation modes through optimality
equations which are solved by probabilistic inference as in the example of the mdp
agent in section      recently  we have applied a similar approach to adaptive control
problems with linear quadratic regulators  braun   ortega        
 problems of bayesian methods  the bayesian control rule treats an adaptive control
problem as a bayesian inference problem  hence  all the problems typically associated
with bayesian methods carry over to agents constructed with the bayesian control
rule  these problems are of both analytical and computational nature  for example 
there are many probabilistic models where the posterior distribution does not have a
closed form solution  also  exact probabilistic inference is in general computationally
very intensive  even though there is a large literature in efficient approximate inference algorithms for particular problem classes  bishop         not many of them are
suitable for on line probabilistic inference in more realistic environment classes 
 bayesian control rule versus bayes optimal control  directly maximizing the  subjective  expected utility for a given environment class is not the same as minimizing
the expected relative entropy for a given class of operation modes  the two methods
are based on different assumptions and optimality principles  as such  the bayesian
control rule is not a bayes optimal controller  indeed  it is easy to design experiments
where the bayesian control rule converges exponentially slower  or does not converge
at all  than a bayes optimal controller to the maximum utility  consider the following
simple example  environment   is a k state mdp in which only k consecutive actions
a reach a state with reward     any interception with a b action leads back to the
initial state  consider a second environment which is like the first but actions a and
b are interchanged  a bayes optimal controller figures out the true environment in k
actions  either k consecutive as or bs   consider now the bayesian control rule  the
optimal action in environment   is a  in environment   is b  a uniform             prior
over the operation modes stays a uniform posterior as long as no reward has been
observed  hence the bayesian control rule chooses at each time step a and b with
equal probability  with this policy it takes about  k actions to accidentally choose a
row of as  or bs  of length k  from then on the bayesian control rule is optimal
too  so a bayes optimal controller converges in time k  while the bayesian control
rule needs exponentially longer  one way to remedy this problem might be to allow
the bayesian control rule to sample actions from the same operation mode for several
time steps in a row rather than randomizing controllers in every cycle  however  if
one considers non stationary environments this strategy
  can also break down  consider  for example  an increasing mdp with k      t   in which a bayes optimal
controller converges in     steps  while the bayesian control rule does not converge
at all in most realizations  because the boundedness assumption is violated 
   

fiortega   braun

    relation to existing approaches
some of the ideas underlying this work are not unique to the bayesian control rule  the
following is a selection of previously published work in the recent bayesian reinforcement
learning literature where related ideas can be found 
 compression principles  in the literature  there is an important amount of work
relating compression to intelligence  mackay        hutter      b   in particular  it
has been even proposed that compression ratio is an objective quantitative measure of
intelligence  mahoney         compression has also been used as a basis for a theory
of curiosity  creativity and beauty  schmidhuber        
 mixture of experts  passive sequence prediction by mixing experts has been studied
extensively in the literature  cesa bianchi   lugosi         in a study on onlinepredictors  hutter      a   bayes optimal predictors are mixed  bayes mixtures can
also be used for universal prediction  hutter         for the control case  the idea
of using mixtures of expert controllers has been previously evoked in models like the
mosaic architecture  haruno  wolpert    kawato         universal learning with
bayes mixtures of experts in reactive environments has been studied in the work of
poland and hutter        and hutter        
 stochastic action selection  the idea of using actions as random variables  and the
problems that this entails  has been expressed in the work of hutter      b  problem
      the study in section   can be regarded as a thorough investigation of this open
problem  other stochastic action selection approaches are found in the thesis of wyatt        who examines exploration strategies for  po mdps  in learning automata
 narendra   thathachar        and in probability matching  duda  hart    stork 
      amongst others  in particular  the thesis discusses theoretical properties of
an extension to probability matching in the context of multi armed bandit problems 
there  it is proposed to choose a lever according to how likely it is to be optimal and
it is shown that this strategy converges  thus providing a simple method for guiding
exploration 
 relative entropy criterion  the usage of a minimum relative entropy criterion to
derive control laws underlies the kl control methods developed in the work of todorov
             and kappen et al          there  it has been shown that a large class
of optimal control problems can be solved very efficiently if the problem statement
is reformulated as the minimization of the deviation of the dynamics of a controlled
system from the uncontrolled system  a related idea is to conceptualize planning as
an inference problem  toussaint  harmeling    storkey         this approach is based
on an equivalence between maximization of the expected future return and likelihood
maximization which is both applicable to mdps and pomdps  algorithms based on
this duality have become an active field of current research  see for example the work
of rasmussen and deisenroth         where very fast model based rl techniques are
used for control in continuous state and action spaces 
   

fia minimum relative entropy principle for learning and acting

   conclusions
this work introduces the bayesian control rule  a bayesian rule for adaptive control  the
key feature of this rule is the special treatment of actions based on causal calculus and the
decomposition of an adaptive agent into a mixture of operation modes  i e  environmentspecific agents  the rule is derived by minimizing the expected relative entropy from the
true operation mode and by carefully distinguishing between actions and observations  furthermore  the bayesian control rule turns out to be exactly the predictive distribution over
the next action given the past interactions that one would obtain by using only probability
and causal calculus  furthermore  it is shown that agents constructed with the bayesian
control rule converge to the true operation mode under mild assumptions  boundedness 
which is related to ergodicity  and consistency  demanding that two indistinguishable hypotheses share the same policy 
we have presented the bayesian control rule as a way to solve adaptive control problems
based on a minimum relative entropy principle  thus  the bayesian control rule can either
be regarded as a new principled approach to adaptive control under a novel optimality
criterion or as a heuristic approximation to traditional bayes optimal control  since it
takes on a similar form to bayes rule  the adaptive control problem could then be translated
into an on line inference problem where actions are sampled stochastically from a posterior
distribution  it is important to note  however  that the problem statement as formulated
here and the usual bayes optimal approach in adaptive control are not the same  in the
future the relationship between these two problem statements deserves further investigation 

acknowledgments
we thank marcus hutter  david wingate  zoubin ghahramani  jose aliste  jose donoso 
humberto maturana and the anonymous reviewers for comments on earlier versions of this
manuscript and or inspiring discussions  we thank the ministerio de planificacion de chile
 mideplan  and the bohringer ingelheim fonds  bif  for funding 

appendix a  proofs
a   proof of theorem  
proof  the proof follows the same line of argument as the solution to equation   with
the crucial difference that
treated as interventions  consider without loss of
p actions are
at in equation    note that the relative entropy can be
generality the summand m p  m cm
written as a difference of two logarithms  where only one term depends on pr to be varied 
therefore  one can pull out the other term and write it as a constant c  this yields

c

x
m

p  m 

x

ao t

p  ao t  m 

x
at

   

p  at  m  ao t   ln pr at  ao t   

fiortega   braun

substituting p  ao t  m  by p  m ao t  p  ao t   p  m  using bayes rule and further rearrangement of the terms leads to
xx
x
 c
p  m ao t  p  ao t  
p  at  m  ao t   ln pr at  ao t  
m ao t

 c

x

p  ao t  

at

x
at

ao t

p  at  ao t   ln pr at  ao t   

p
the inner sum has the form  x p x  ln q x   i e  the cross entropy between q x  and p x  
which is minimized when q x    p x  for all x  let p denote the optimum distribution
for pr  by choosing this optimum one obtains p at  ao t     p  at  ao t   for all at   note that
the solution to this variational problem is p
independent of the p
weighting p  ao t    since the
o
a and
same argument applies to any summand m p  m cm
m p  m cm in equation   
their variational problems are mutually independent  hence 
p at  ao t     p  at  ao t  

p ot  ao t     p  ot  ao t at  

for all aot  z    for p  at  ao t    introduce the variable m via a marginalization and then
apply the chain rule 
x
p  at  ao t    
p  at    m  ao t  p  m ao t   
m

the term p  m aot   can be further developed as

p  ao t  m p  m 


m p  ao t  m  p  m  
qt 
p  m      p  a  m  ao   p  o  m  ao  a  
 p
qt 



m p  m  
    p  a  m   ao   p  o  m   ao  a  
qt 
p  m      p  o  m  ao  a  
 
 p
qt 


m p  m  
    p  o  m   ao  a  

p  m ao t     p

the first equality is obtained by applying bayes rule and the second by using the chain
rule for probabilities  to get the last equality  one applies the interventions to the causal
factorization  thus  p  a  m  ao        and p  o  m  ao  a     p  o  m  ao  a    the
equations characterizing p  ot  ao t at   are obtained similarly 
a   proof of theorem  
proof  as has been pointed out in       a particular realization of the divergence process
dt  m km  can be decomposed as
x
dt  m km   
gm  m   tm   
m

where the gm  m   tm   are sub divergences of dt  m km  and the tm form a partition of nt  
however  since dt  m km  has bounded variation for all m  m  one has for all        there
is a c m      such that for all m  m  all t  nt and all t  nt   the inequality
fi
fi
fi
fi
figm  m   tm    gm  m   tm  fi  c m 
   

fia minimum relative entropy principle for learning and acting

holds with probability         however  due to      
gm  m   tm     
for all m  m  thus 

gm  m   tm    c m  

if all the previous inequalities hold simultaneously then the divergence process can be
bounded as well  that is  the inequality
dt  m km   m c m 

    

holds with probability         m where m     m   choose
 m 
 m     max    ln pp m
     
 m 
since    ln pp m
using the
     m   it can be added to the right hand side of      

definition of dt  m km   taking the exponential and rearranging the terms one obtains


p  m  

t
y



   

 m 

p  o  m   ao  a    e

p  m 

t
y

   

p  o  m  ao  a  

where  m     m c m     m      identifying the posterior probabilities of m and m
by dividing both sides by the normalizing constant yields the inequality
p  m  aot    e m  p  m aot   
 

this inequality holds simultaneously for all m  m with probability         m and in
particular for     minm  e m     that is 
p  m  aot    p  m aot   
but since this is valid for any m  m  and because maxm  p  m aot    
p  m  aot   

 
m 

one gets


 
m

with probability
     for arbitrary      related to   through the equation     

m 
 
    
a   proof of theorem  
proof  the divergence process dt  m km  can be decomposed into a sum of sub divergences
 see equation    
x
gm  m  tm   
    
dt  m km   
m

m

furthermore  for every
 m  one has that for all       there is a c     such that for
all t  n and for all t  nt
fi
fi
fi
fi
figm  m  t    gm  m  t  fi  c m 
   

fiortega   braun

with probability         applying this bound to the summands in      yields the lower
bound
x
x

gm  m  tm   
gm  m  tm    c m 
m

m

       m  

which holds with probability 
where m     m   due to inequality     one has




that for all m    m   gm  m  tm       hence 
x

gm  m  tm    c m   gm  m  tm    m c
m

where c    maxm  c m    the members of the set tm are determined stochastically  more
specifically  the ith member is included into tm with probability p  m  aoi     m for
some      by theorem    but since m 
   m    one has that gm  m  tm     as t  

with probability      for arbitrarily chosen        this implies that
lim dt  m km   lim gm  m  tm    m c  

t

t

with probability       where      is arbitrary and related to   as              m     
using this result in the upper bound for posterior probabilities yields the final result
p  m  dt  m km 
e
    
t p  m  

   lim p  m aot    lim
t

a   proof of theorem  
proof  we will use the abbreviations pm  t     p  at  m  ao t   and wm  t     p  m ao t   
decompose p  at  ao t   as
x
x
pm  t wm  t   
pm  t wm  t  
    
p  at  ao t    
m m
   

m m  

the first sum on the right hand side is lower bounded by zero and upper bounded by
x
x
pm  t wm  t  
wm  t 
m m
   

m m
   

because pm  t      due to theorem    wm  t     as t   almost surely  given     
and        let t   m  be the time such that for all t  t   m   wm  t       choosing
t     maxm  t   m    the previous inequality holds for all m and t  t  simultaneously with
probability         m   hence 
x
x
pm  t wm  t  
wm  t    m   
    
m m
   

m m
   

to bound the second sum in      one proceeds as follows  for every member m   m   
one has that pm  t   pm  t  as t    hence  following a similar construction as above 
one can choose t  such that for all t  t  and m   m    the inequalities
fi
fi
fi
fi
fipm  t   pm  t fi   
   

fia minimum relative entropy principle for learning and acting

hold simultaneously for the precision       applying this to the second sum in equation   
yields the bounds
x
x
x


pm  t     wm  t  
pm  t wm  t  
pm  t    wm  t  
m m  

m m  

here pm  t   
that




m m  

are multiplicative constants that can be placed in front of the sum  note
 

x

m m  

wm  t      

x

m m
   

wm  t        

use of the above inequalities allows simplifying the lower and upper bounds respectively 
 x
pm  t   
wm  t    pm  t           pm  t      
m m  

pm  t    

 x


m m  

    

wm  t   pm  t       pm  t       

combining the inequalities      and      in      yields the final result 
fi
fi
fi
fi

p
 a
 ao
 t 
 

p
fi
fi        m      
t
m
 t

which holds with probability      for arbitrary      related to   as       
and arbitrary precision  



m

 

a   gibbs sampling implementation for mdp agent
inserting the likelihood given in equation    into equation    of the bayesian control rule 
one obtains the following expression for the posterior
p  m at   ot    
 

p  x  m  x  a p  r m  x  a  x  p  m a t   o t  






m p  x  m   x  a p  r m   x  a  x  p  m  a t   o t   dm
p  r m  x  a  x  p  m a t   o t  
r
 




m p  r m   x  a  x  p  m  a t   o t   dm
r

    

where we have replaced the sum by an integration over m   the finite dimensional real space
containing only the average reward and the q values of the observed states  and where we
have simplified the term p  x  m  x  a  because it is constant for all m  m 
the likelihood model p  r m   x  a  x   in equation    encodes a set of independent normal distributions over the immediate reward with means m  x  a  x   indexed by triples
 x  a  x    x  a  x   in other words  given  x  a  x    the rewards are drawn from a
normal distribution with unknown mean m  x  a  x   and known variance      the sufficient statistics are given by n x  a  x    the number of times that the transition x  x
under action a  and r x  a  x    the mean of the rewards obtained in the same transition 
the conjugate prior distribution is well known and given by a normal distribution with
hyperparameters   and    
r
n
  o
 


 
 
    
exp    m  x  a  x     
p  m  x  a  x      n            
 
   

fiortega   braun

the posterior distribution is given by
p  m  x  a  x   at   ot     n  x  a  x       x  a  x   
where the posterior hyperparameters are computed as
      p n x  a  x   r x  a  x  
    p n x  a  x  
 x  a  x         p n x  a  x   

 x  a  x    

    

by introducing the shorthand v  x     maxa q x  a   we can write the posterior distribution over  as
p   at   ot     n     s 
    
where
  

  x
 x  a  x    x  a  x    q x  a    v  x    
s
x a x
x
s 
 x  a  x   
x a x

the posterior distribution over the q values is more difficult to obtain  because each
q x  a  enters the posterior distribution both linearly and non linearly through   however 
if we fix q x  a  within the max operations  which amounts to treating each v  x  as a
constant within a single gibbs step  then the conditional distribution can be approximated
by


p  q x  a  at   ot    n q x  a     s x  a 
    
where

q x  a   

x
 
 x  a  x    x  a  x       v  x    
s x  a  
x
x
 x  a  x   
s x  a   
x

we expect this approximation to hold because the resulting update rule constitutes a contraction operation that forms the basis of most stochastic approximation algorithms  mahadevan         as a result  the gibbs sampler draws all the values from normal distributions  in each cycle of the adaptive controller  one can carry out several gibbs sweeps to
obtain a sample of m to improve the mixing of the markov chain  however  our experimental
results have shown that a single gibbs sweep per state transition performs reasonably well 
once a new parameter vector m is drawn  the bayesian control rule proceeds by taking the
optimal action given by equation     note that only the  and  entries of the transitions
that have occurred need to be represented explicitly  similarly  only the q values of visited
states need to be represented explicitly 
   

fia minimum relative entropy principle for learning and acting

references
auer  p   cesabianchi  n     fischer  p          finite time analysis of the multiarmed
bandit problem  machine learning             
bertsekas  d          dynamic programming  deterministic and stochastic models 
prentice hall  upper saddle river  nj 
bishop  c  m          pattern recognition and machine learning  springer 
braun  d  a     ortega  p  a          a minimum relative entropy principle for adaptive
control in linear quadratic regulators  in the  th conference on informatics in control 
automation and robotics  vol     pp         
cesa bianchi  n     lugosi  g          prediction  learning and games  cambridge university press 
dawid  a  p          beware of the dag   journal of machine learning research   to
appear  
dearden  r   friedman  n     andre  d          model based bayesian exploration  in
in proceedings of fifteenth conference on uncertainty in artificial intelligence  pp 
       
dearden  r   friedman  n     russell  s          bayesian q learning  in aaai
   iaai     proceedings of the fifteenth national tenth conference on artificial intelligence innovative applications of artificial intelligence  pp          american association for artificial intelligence 
duda  r  o   hart  p  e     stork  d  g          pattern classification  second edition  
wiley   sons  inc 
duff  m  o          optimal learning  computational procedures for bayes adaptive markov
decision processes  ph d  thesis  director andrew barto 
grunwald  p          the minimum description length principle  the mit press 
haruno  m   wolpert  d     kawato  m          mosaic model for sensorimotor learning
and control  neural computation               
haussler  d     opper  m          mutual information  metric entropy and cumulative
relative entropy risk  the annals of statistics               
hutter  m          self optimizing and pareto optimal policies in general environments
based on bayes mixtures  in colt 
hutter  m          optimality of universal bayesian prediction for general loss and alphabet 
journal of machine learning research            
hutter  m       a   online prediction  bayes versus experts  tech  rep   presented at
the eu pascal workshop on learning theoretic and bayesian inductive principles
 ltbip       
hutter  m       b   universal artificial intelligence  sequential decisions based on algorithmic probability  springer  berlin 
   

fiortega   braun

kappen  b   gomez  v     opper  m          optimal control as a graphical model inference
problem  jmlr  to appear  
mackay  d  j  c          information theory  inference  and learning algorithms  cambridge university press 
mahadevan  s          average reward reinforcement learning  foundations  algorithms 
and empirical results  machine learning                   
mahoney  m  v          text compression as a test for artificial intelligence  in aaai iaai 
pp         
narendra  k     thathachar  m  a  l          learning automata   a survey  ieee
transactions on systems  man  and cybernetics  smc               
nozick  r          newcombs problem and two principles of choice  in rescher  n   ed   
essays in honor of carl g  hempel  pp          reidel 
opper  m          a bayesian approach to online learning  online learning in neural
networks         
ortega  p  a     braun  d  a          a bayesian rule for adaptive control based on causal
interventions  in the third conference on artificial general intelligence  pp         
pearl  j          causality  models  reasoning  and inference  cambridge university press 
cambridge  uk 
poland  j     hutter  m          defensive universal learning with experts  in alt 
rasmussen  c  e     deisenroth  m  p          recent advances in reinforcement learning 
vol       of lecture notes on computer science  lnai  chap  probabilistic inference
for fast learning in control  pp          springer verlag 
robbins  h          some aspects of the sequential design of experiments  bulletin american
mathematical socierty             
russell  s     norvig  p          artificial intelligence  a modern approach   rd edition  
prentice hall 
schmidhuber  j          simple algorithmic theory of subjective beauty  novelty  surprise 
interestingness  attention  curiosity  creativity  art  science  music  jokes  journal of
sice               
shafer  g          the art of causal conjecture  the mit press 
singh  s  p          reinforcement learning algorithms for average payoff markovian decision
processes  in national conference on artificial intelligence  pp         
spirtes  p   glymour  c     scheines  r          causation  prediction and search   nd
edition   springer verlag  new york 
sutton  r  s     barto  a  g          reinforcement learning  an introduction  mit press 
cambridge  ma 
todorov  e          linearly solvable markov decision problems  in advances in neural
information processing systems  vol      pp           
   

fia minimum relative entropy principle for learning and acting

todorov  e          efficient computation of optimal actions  proceedings of the national
academy of sciences u s a                   
toussaint  m   harmeling  s     storkey  a          probabilistic inference for solving
 po mdps  tech  rep  edi inf rr       university of edinburgh 
watkins  c          learning from delayed rewards  ph d  thesis  university of cambridge 
cambridge  england 
wyatt  j          exploration and inference in learning from reinforcement  ph d  thesis 
department of artificial intelligence  university of edinburgh 

   

fi