journal of artificial intelligence research                  

submitted        published      

automatic induction of bellman error features
for probabilistic planning
jia hong wu
robert givan

jw   alumni   purdue   edu
givan   purdue   edu

electrical and computer engineering
purdue university  w  lafayette  in       usa

abstract
domain specific features are important in representing problem structure throughout machine
learning and decision theoretic planning  in planning  once state features are provided  domainindependent algorithms such as approximate value iteration can learn weighted combinations of
those features that often perform well as heuristic estimates of state value  e g   distance to the
goal   successful applications in real world domains often require features crafted by human experts  here  we propose automatic processes for learning useful domain specific feature sets with
little or no human intervention  our methods select and add features that describe state space regions of high inconsistency in the bellman equation  statewise bellman error  during approximate
value iteration  our method can be applied using any real valued feature hypothesis space and
corresponding learning method for selecting features from training sets of state value pairs  we
evaluate the method with hypothesis spaces defined by both relational and propositional feature
languages  using nine probabilistic planning domains  we show that approximate value iteration
using a relational feature space performs at the state of the art in domain independent stochastic
relational planning  our method provides the first domain independent approach that plays tetris
successfully  without human engineered features  

   introduction
there is a substantial gap in performance between domain independent planners and domainspecific planners  domain specific human input is able to produce very effective planners in all
competition planning domains as well as many game applications such as backgammon  chess  and
tetris  in deterministic planning  work on tlplan  bacchus   kabanza        has shown that
simple depth first search with domain specific human input  in the form of temporal logic formulas
describing acceptable paths  yields an effective planner for a wide variety of competition domains 
in stochastic planning  feature based value function representations have been used with humanselected features with great success in applications such as backgammon  sutton   barto       
tesauro        and tetris  bertsekas   tsitsiklis         the usage of features provided by human experts is often critical to the success of systems using such value function approximations 
here  we consider the problem of automating the transition from domain independent planning to
domain specific performance  replacing the human input with automatically learned domain properties  we thus study a style of planner that learns from encountering problem instances to improve
performance on subsequently encountered problem instances from the same domain 
we focus on stochastic planning using machine learned value functions represented as linear
combinations of state space features  our goal then is to augment the state space representation
c
    
ai access foundation  all rights reserved 

fiw u   g ivan

during planning with new machine discovered features that facilitate accurate representation of the
value function  the resulting learned features can be used in representing the value function for
other problem instances from the same domain  allowing amortization of the learning costs across
solution of multiple problem instances  note that this property is in contrast to most competition
planners  especially in deterministic planning  which retain no useful information between problem instances  thus  our approach to solving planning problems can be regarded as automatically
constructing domain specific planners  using domain independent techniques 
we learn features that correlate well to the statewise bellman error of value functions encountered during planning  using any provided feature language with a corresponding learner to select
features from the space  we evaluate this approach using both relational and propositional feature
spaces  there are other recent approaches to acquiring features in stochastic planning with substantial differences from our approach which we discuss in detail in section    patrascu  poupart 
schuurmans  boutilier    guestrin        gretton   thiebaux        sanner   boutilier       
keller  mannor    precup        parr  painter wakefield  li    littman         no previous work
has evaluated the selection of relational features by correlation to statewise bellman error 
recent theoretical results  parr et al         for uncontrolled markov processes show that exactly capturing statewise bellman error in new features  repeatedly  will lead to convergence to the
uncontrolled optimal value for the value function selected by linear fixed point methods for weight
training  unfortunately for machine learning approaches to selecting features  these results have
not been transferred to approximations of statewise bellman error features  for this case  the results
in the work of parr et al         are weaker and do not imply convergence  also  none of this theory has been transferred to the controlled case of interest here  where the analysis is much more
difficult because the effective  greedy  policy under consideration during value function training is
changing 
we consider the controlled case  where no known theoretical properties similar to those of parr
et al         have been shown  lacking such theory  our purpose is to demonstrate the capability
of statewise bellman error features empirically  and with rich representations that require machine
learning techniques that lack approximation guarantees  next  we give an overview of our approach  introducing markov decision processes  value functions  bellman error  feature hypothesis
languages and our feature learning methods 
we use markov decision processes  mdps  to model stochastic planning problems  an mdp is
a formal model of a single agent facing a sequence of action choices from a pre defined action space 
and transitioning within a pre defined state space  we assume there is an underlying stationary
stochastic transition model for each available action from which state transitions occur according to
the agents action choices  the agent receives reward after each action choice according to the state
visited  and possibly the action chosen   and has the objective of accumulating as much reward as
possible  possibly favoring reward received sooner  using discounting  or averaging over time  or
requiring that the reward be received by a finite horizon  
mdp solutions can be represented as state value functions assigning real numbers to states  informally  in mdp solution techniques  we desire a value function that respects the action transitions
in that good states will either have large immediate rewards or have actions available that lead to
other good states  this well known property is formalized in bellman equations that recursively
characterize the optimal value function  see section     the degree to which a given value function
fails to respect action transitions in this way  to be formalized in the next section  is referred to as
the bellman error of that value function  and can be computed at each state 
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

intuitively  statewise bellman error has high magnitude in regions of the state space which
appear to be undervalued  or overvalued  relative to the action choices available  a state with high
bellman error has a locally inconsistent value function  for example  a state is inconsistently labeled
with a low value if it has an action available that leads only to high value states  our approach is to
use machine learning to fit new features to such regions of local inconsistency in the current value
function  if the fit is perfect  the new features guarantee we can represent the bellman update
of the current value function  repeated bellman updates  called value iteration  are known to
converge to the optimal value function  we add the learned features to our representation and then
train an improved value function  adding the new features to the available feature set 
our method for learning new features and using them to approximate the value function here
can be regarded as a boosting style learning approach  a linear combination of features can be
viewed as a weighted combination of an ensemble of simple hypotheses  each new feature learned
can be viewed as a simple hypothesis selected to match a training distribution focused on regions
that the previous ensemble is getting wrong  as reflected in high statewise bellman error throughout
the region   growth of an ensemble by sequentially adding simple hypotheses selected to correct
the error of the ensemble so far is what we refer to as boosting style learning 
it is important to note that our method scores candidate features by correlation to the statewise
bellman error of the current value function  not by minimizing the statewise bellman error of some
value function found using the new candidate feature  this pre feature addition scoring is much
less expensive than scoring that involves retraining weights with the new feature  especially when
being repeated many times for different candidates  relative to the same current value function  our
use of pre feature addition scoring to select features for the controlled setting enables a much more
aggressive search for new features than the previously evaluated post feature addition approach
discussed in the work of patrascu et al         
our approach can be considered for selecting features in any feature description language for
which a learning method exists to effectively select features that match state value training data 
we consider two very different feature languages in our empirical evaluation  human constructed
features are typically compactly described using a relational language  such as english  wherein the
feature value is determined by the relations between objects in the domain  likewise  we consider
a relational feature language  based on domain predicates from the basic domain description   the
domain description may be written  for example  in a standard planning language such as ppddl in
younes  littman  weissman    asmuth         here  we take logical formulas of one free variable
to represent features that count the number of true instantiations of the formula in the state being
evaluated  for example  the number of holes feature that is used in many tetris experiments
 bertsekas   tsitsiklis        driessens  ramon    gartner        can be interpreted as counting
the number of empty squares on the board that have some other filled squares above them  such
numeric features provide a mapping from states to natural numbers 
in addition to this relational feature language  we consider using a propositional feature representation in our learning structure  although a propositional representation is less expressive
than a relational one  there exist very effective off the shelf learning packages that utilize propositional representations  indeed  we show that we can reformulate our feature learning task as a
related classification problem  and use a standard classification tool  the decision tree learner c   
 quinlan         to create binary valued features  our reformulation to classification considers
only the sign  not the magnitude  of the statewise bellman error  attempting to learn features that
characterize the positive sign regions of the state space  or likewise the negative sign regions   a
   

fiw u   g ivan

standard supervised classification problem is thus formulated and c    is then applied to generate
a decision tree feature  which we use as a new feature in our value function representation  this
propositional approach is easier to implement and may be more attractive than the relational one
when there is no obvious advantage in using relational representation  or when computing the exact
statewise bellman error for each state is significantly more expensive than estimating its sign  in
our experiments  however  we find that our relational approach produces superior results than our
propositional learner  the relational approach also demonstrates the ability to generalize features
between problem sizes in the same domain  an asset unavailable in propositional representations 
we present experiments in nine domains  each experiment starts with a single  constant feature  mapping all states to the same number  forcing also a constant value function that makes no
distinctions between states  we then learn domain specific features and weights from automatically
generated sampled state trajectories  adjusting the weights after each new feature is added  we
evaluate the performance of policies that select their actions greedily relative to the learned value
functions  we evaluate our learners using the stochastic computer game tetris and seven planning domains from the two international probabilistic planning competitions  younes et al        
bonet   givan         our method provides the first domain independent approach to playing
tetris successfully  without human engineered features   our relational learner also demonstrates
superior success ratio in the probabilistic planning competition domains as compared both to our
propositional approach and to the probabilistic planners ff replan  yoon  fern    givan       
and foalp  sanner   boutilier               additionally  we show that our propositional learner
outperforms the work of patrascu et al         on the same sysadmin domain evaluated there 

   background
here we present relevant background on the use of markov decision processes in planning 
    markov decision processes
we define here our terminology for markov decision processes  for a more thorough discussion of
markov decision processes  see the books by bertsekas and tsitsiklis        and sutton and barto
        a markov decision process  mdp  m is a tuple  s  a  r  t  s     here  s is a finite state
space containing initial state s    and a selects a non empty finite available action set a s  for each
state s in s  the reward function r assigns a real reward to each state action state triple  s  a  s  
where action a is enabled in state s  i e   a is in a s   the transition probability function t maps
state action pairs  s  a  to probability distributions over s  p s   where a is in a s  
given discount factor         and policy  mapping each state s  s to an action in a s   the
value function v   s  gives the expected discounted reward obtained from state s selecting action
 s  at each state encountered and discounting future rewards by a factor of  per time step  there

is at least one optimal policy   for which v   s   abbreviated v   s   is no less than v   s  at
every state s  for any policy   the following q function evaluates an action a with respect to a
future value function v  
x
q s  a  v    
t  s  a  s   r s  a  s     v  s    
s s

recursive bellman equations use q   to describe v  and v  as follows  first  v   s   
q s   s   v     then  v   s    maxaa s  q s  a  v     also using q    we can select an ac   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

tion greedily relative to any value function  the policy greedy v   selects  at any state s  the action
arg maxaa s  q s  a  v   
value iteration iterates the operation
u v   s    max

aa s 

x

t  s  a  s   r s  a  s     v  s    

s s

computing the bellman update u v   from v   producing a sequence of value functions converging
in the sup norm to v    regardless of the initial v used 
we define the statewise bellman error b v  s  for a value function v at a state s to be
u v   s   v  s   we will be inducing new features based on their correlation to the statewise
bellman error  or based on the sign of the statewise bellman error  the sup norm distance of a
value function v from the optimal value function v  can be bounded using the bellman error magnitude  which is defined as maxss  b v  s    e g   see williams   baird         we use the term
statewise bellman error to emphasize the distinction from the widely used sup norm bellman
error 
we note that computing u v    and thus statewise bellman error  can involve a summation over
the entire state space  whereas our fundamental motivations require avoiding such summations 
in many mdp problems of interest  the transition matrix t is sparse in a way that set of states
reachable in one step with non zero probability is small  for any current state  in such problems 
statewise bellman error can be computed effectively using an appropriate representation of t   more
generally  when t is not sparse in this manner  the sum can be effectively approximately evaluated
by sampling next states according to the distribution represented by t  
    modeling goal oriented problems
stochastic planning problems can be goal oriented  where the objective of solving the problem is
to guide the agent toward a designated state region  i e   the goal region   we model such problems
by structuring the reward and transition functions r and t so that any action in a goal state leads
with positive reward to a zero reward absorbing state  and reward is zero everywhere else  we
retain discounting to represent our preference for shorter paths to the goal  alternatively  such
problems can be modeled as stochastic shortest path mdps without discounting  bertsekas        
our techniques can easily be generalized to formalisms which allow varying action costs as well 
but we do not model such variation in this work 
more formally  we define a goal oriented mdp to be any mdp meeting the following constraints  here  we use the variables s and s for states in s and a for actions in a s   we require that
s contain a zero reward absorbing state   i e   such that r   a  s      and t    a        for all
s and a  the transition function t must assign either one or zero to triples  s  a     and we call the
region of states s for which t  s  a    is one the goal region  the reward function is constrained
so that r s  a  s   is zero unless s     in constructing goal oriented mdps from other problem
representations  we may introduce dummy actions to carry out the transitions involving  described
here 
    compactly represented mdps
in this work  we consider both propositional and relational state representations 
   

fiw u   g ivan

in relational mdps  the spaces s and a s  for each s are relationally represented  i e   there
is a finite set of objects o  state predicates p   and action names n used to define these spaces as
follows  a state fact is an application p o            on   of an n argument state predicate p to object
arguments oi   for any n    a state is any set of state facts  representing exactly the true facts in that
state  an action instance a o            os
n   is an application of an n argument action name to n objects
oi   for any n  the action space a   ss a s  is the set of all action instances 
mdps with compactly represented state and action spaces also use compact representations
for the transition and reward functions  one such compact representation is the ppddl planning
language  informally discussed in the next subsection and formally presented in the work of younes
et al         
in propositional problems  the action space is explicitly specified and the state space is compactly specified by providing a finite sequence of basic state properties called state attributes  with
boolean  integer  or real values  a propositional state is then any vector of values for the state
attributes 
given a relational mdp  an equivalent propositional mdp can be easily constructed by grounding  in which an explicit action space is constructed by forming all action name applications and a
set of state attributes is computed by forming all state predicate applications  thus removing the use
of the set of objects in the representation 
    representing ppddl planning problems using mdps
we discuss how to represent goal oriented stochastic planning problems defined in standardized
planning languages such as ppddl  younes et al         as goal oriented mdps  we limit our
focus to problems in which the goal regions can be described as  conjunctive  sets of state facts  we
reference and follow the approach used in the work of fern  yoon  and givan        here regarding
converting from planning problems to compactly represented mdps in a manner that facilitates generalization between problem instances  we first discuss several difficult representational issues and
then finally pull that discussion together in a formal definition of the mdp we analyze to represent
any given ppddl problem instance  we do not consider quantified and or disjunctive goals  but
handling such goals would be an interesting and useful extension of this work 
      p lanning d omains

and

p roblems

a planning domain is a distribution over problem instances sharing the same state predicates pw  
action names n   and action definitions  actions can take objects as parameters  and are defined
by giving discrete finite probability distributions over action outcomes  each of which is specified
using add and delete lists of state facts about the action parameters 
given a domain definition  each problem instance in the domain specifies a finite object set o 
initial state si and goal condition g  the initial state is given as a set of state facts and the goal
condition is given as a conjunction of state facts  each constructed from the predicates in pw  
   each state predicate has an associated arity indicating the number of objects it relates  the state predicate can be
applied to that number of objects from the domain to form a ground state fact that can be either true or false in
each state  states are then the different possible ways to select the true state facts  likewise  each action name has an
associated arity that is a natural number indicating the number of objects the action will act upon  the action name
can then be applied to that number of objects to form a grounded action 

   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

      ppddl r epresentation
ppddl is the standard planning language for the international probabilistic planning competitions 
in ppddl  a planning domain syntax and a planning problem syntax is defined  to completely
define a planning instance  one has to specify a domain definition and a problem definition using
the respective syntax  conditional effects and quantified preconditions are allowed in the domain
definition 
in planning competitions  it has been customary to specify planning domains by providing problem generators that accept size parameters as input and then output ppddl problem instances 
these generators thus specify size parameterized planning domains  it is important to note  however  that not all problem generators provided in the recent planning competitions specify planning
domains according to the definition used here  in particular  some problem generators vary the
action set or the state predicates between the instances generated  the relationship between the
different problem instances generated by such generators is much looser than that required by our
definition  and as such these domains are somewhat more like arbitrary collections of planning
problems 
because our logical language allows generalization between problems only if those problems
share the same state and action language  we limit our empirical evaluation in section   to domains
that were provided with problem generators that specify planning domains as just defined here 
i e   without varying the action definitions between instances  or for which we can easily code such
a generator   we refer to domains with such generators as planning domains with fixed action
definitions 
      g eneralization b etween p roblems of varying s ize
because the object set varies in size  without bound  across the problem instances of a domain  there
are infinitely many possible states within the different instances of a single domain  each mdp we
analyze has a finite state space  and so we model a planning domain as an infinite set of mdps
for which we are seeking a good policy  in the form of a good value function   one mdp for each
problem instance   
a value function for an infinite set of mdps is a mapping from the disjoint union of the state
spaces of the mdps to the real numbers  such a value function can be used greedily as a policy
in any of the mdps in the set  however  explicit representation of such a value function would
have infinite size  here  we will use knowledge representation techniques to compactly represent
value functions over the infinite set of problem instance mdps for any given planning domain  the
compact representation derives from generalization across the domains  and our approach is fundamentally about finding good generalizations between the mdps within a single planning domain 
our representation for value functions over planning domains is given below in sections     and   
in this section  we discuss how to represent as a single finite mdp any single planning problem
instance  however  we note that our objective in this work is to find good value functions for
the infinite collections of such mdps that represent planning domains  throughout this paper  we
assume that each planning domain is provided along with a means for sampling example problems
from the domain  and that the sampling is parameterized by difficulty  generally  problem size  so
   in this paper we consider two candidate representations for features  only one of these  the relational representation 
is capable of generalizing between problem sizes  for the propositional representation  we restrict all training and
testing to problem instances of the same size 

   

fiw u   g ivan

that easy example problems can be selected  although  ppddl does not provide any such problem
distributions  benchmark planning domains are often provided with problem generators defining
such distributions  where such generators are available  we use them  and otherwise we code our
own distributions over problem instances 
      g eneralizing b etween p roblems

with

varying g oals

to facilitate generalization between problem instances with different goals  and following the work
of martin and geffner        and fern et al          we translate a ppddl instance description into
an mdp where each state specifies not only what is true in the state but also what the goal is  action
transitions in this mdp will never change the goal  but the presence of that goal within the state
description allows value functions  that are defined as conditioning only on the state  to depend on
the goal as well  the goal region of the mdp will simply be those mdp states where the specified
current state information matches the specified goal information 
formally  in translating ppddl problem instances into compact mdps  we enrich the given set
of world state predicates pw by adding a copy of each predicate indicating the desired state of that
predicate  we name the goal description copy of a predicate p by prepending the word goal  to
the name  the set of all goal description copies of the predicates in pw is denoted pg   and we take
pw  pg to be the state predicates for the mdp corresponding to the planning instance  intuitively 
the presence of goal p a b  in a state indicates that the goal condition requires the fact p a  b  to be
part of the world state  the only use of the goal predicates in constructing a compact mdp from a
ppddl description is in constructing the initial state  which will have the goal conditions true for
the goal predicates 
we use the domain blocksworld as an example here to illustrate the reformulation  the same
domain is also used as an example in fern et al          the goal condition in a blocksworld
problem can be described as a conjunction of ground on top of facts  the world state predicate
on top of is in pw   as discussed above  this implies that the predicate goal on top of is in pg  
intuitively  one ground instance of that predicate  goal on top of b  b    means that for a state in
the goal region  the block b  has to be directly on the top of the block b  
      s tates

with no

available actions

ppddl allows the definition of domains where some states do not meet the preconditions for any
action to be applied  however  our mdp formalism requires at least one available action in every
state  in translating a ppddl problem instance to an mdp we define the action transitions so
that any action taken in such a dead state transitions deterministically to the absorbing  state 
because we consider such states undesirable in plan trajectories  we give these added transitions a
reward of negative one unless the source state is a goal state 
      t he r esulting mdp
we now pull together the above elements to formally describe an mdp m    s  a  r  t  s   
given a ppddl planning problem instance  as discussed in section      the set s is defined by
specifying the predicates and objects available  the ppddl description specifies the sets n of
action names and o of objects  as well as a set pw of world predicates  we construct the enriched
set p   pw  pg of state predicates and define the state space as all sets of applications of these
predicates to the objects in o  the set a s  for any state s is the set of ppddl action instances built
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

from n and o for which s satisfies the preconditions  except that if this set is empty  a s  is the set
of all ppddl action instances built from n and o  in the latter case  we say the state is dead  the
reward function r is defined as discussed previously in section      i e   r s  a  s       when the
goal condition g is true in s  r s  a  s       when s is a non goal dead state  and zero otherwise 
we define t  s  a  s   according to the semantics of ppddl augmented with the semantics of 
from section    t  s  a    will be one if s satisfies g  s is dead  or s     and zero otherwise  
transiting from one state to another never changes the goal condition description in the states given
by predicates in pg   the mdp initial state s  is just the ppddl problem initial state si augmented
by the goal condition g using the goal predicates from pg   if a propositional representation is
desired  it can be easily constructed directly from this relational representation by grounding 
    linear approximation of value functions
as many previous authors have done  patrascu et al         sanner   boutilier        bertsekas  
tsitsiklis        tesauro        tsitsiklis   roy         we address very large compactly represented s and or a by implicitly representing value functions in terms of state space features
f   s  r  our features f must select a real value for each state  we describe two approaches to
representing and selecting such features in section   
recall from section   that our goal is to learn a value function for a family of related mdp
problems  we assume that our state space features are defined across the union of the state spaces
in the family 
we represent
value functions using a linear combination of l features extracted from s  i e   as
p
v  s    li   wi fi  s   where f   s       our goal is to find features fi  each mapping states to real
values  and weights wi so that v closely approximates v    note that a single set of features and
weight vector defines a value function for all mdps in which those features are defined 
various methods have been proposed to select weights wi for linear approximations  see  e g  
sutton       or widrow   hoff         here  we review and use a trajectory based approximate
value iteration  avi  approach  other training methods can easily be substituted  avi constructs a
 
 
t
finite sequence of value functions
one  each value function
pl v    v           v   and returns the last

is represented as v  s    i   wi fi  s   to determine weights wi   from v    we draw a set
of training states s    s            sn by following policy greedy v    in different example problems
sampled from the provided problem distribution at the current level of problem difficulty   see
section   for discussion of the control of problem difficulty   the number of trajectories drawn and
the maximum length of each trajectory are parameters of this avi method  for each training state s 
we compute the bellman update u v    s  from the mdp model of the problem instance  we can
then compute wi   from the training states using
wi     wi  

  x
fi  sj   u v    sj    v   sj    
ni

   

j

where  is the learning rate and ni is the number of states s in s    s            sn for which fi  s  is
non zero  weight updates using this weight update formula descend the gradient of the l  distance
between v  and u v    on the training states  with the features first rescaled to normalize the
   note that according to our definitions in section      the dead states are now technically goal states  but have
negative rewards 

   

fiw u   g ivan

effective learning rate to correct for feature values with rare occurrence in the training set   pseudocode for our avi method and for drawing training sets by following a policy is available in online
appendix    available on jair website   on page   
here  we use the greedy policy to draw training examples in order to focus improvement on the
most relevant states  other state distributions can be generated that are not biased by the current
policy  in particular  another option worth considering  especially if feature learning is stuck  would
be the long random walk distribution discussed in the work of fern  yoon  and givan         we
leave detailed exploration of this issue for future work  for a more substantial discussion of the
issues that arise in selecting the training distribution  please see the book by sutton and barto        
it is worth noting that on policy training has been shown to converge to the optimal value function
in the closely related reinforcement learning setting using the sarsa algorithm  singh  jaakkola 
littman    szepesvari        
in general  while avi often gives excellent practical results  it is a greedy gradient descent
method in an environment that is not convex due to the maximization operation in the bellman error
function  as such  there is no guarantee on the quality of the weight vector found  even in the case
of convergence  convergence itself is not guaranteed  and  in our experiments  divergent weight
training was in fact a problem that required handling  we note that our feature discovery methods
can be used with other weight selection algorithms such as approximate linear programming  should
the properties of avi be undesirable for some application 
we have implemented small modifications to the basic weight update rule in order to use avi
effectively in our setting  these are described in section   in online appendix    available on jair
website  

   feature discovering value function construction
in planning  once state features are provided  domain independent algorithms such as avi can learn
weighted combinations of those features that often perform well as heuristic estimates of state value
 e g   distance to the goal   we now describe methods to select and add features that describe
state space regions of high inconsistency in the bellman equation  statewise bellman error  during
approximate value iteration  our methods can be applied using any real valued feature hypothesis
space with a corresponding learning method for selecting features to match a real valued function
on a training set of states  here  we will use the learner to select features that match the statewise
bellman error function 
as noted above  we use a boosting style learning approach in finding value functions  iterating
between selecting weights and generating new features by focusing on the bellman error in the
current value function  our value function representation can be viewed as a weighted ensemble of
single feature hypotheses  we start with a value function that has only a trivial feature  a constant
feature always returning the value one  with initial weight zero  we iteratively both retrain the
weights and select new features matching regions of states for which the current weighted ensemble
has high statewise bellman error 
we take a learning from small problems approach and learn features first in problems with
relatively lower difficulty  and increase problem difficulty over time  as discussed below  lower
difficulty problems are typically those with smaller state spaces and or shorter paths to positive
   in deriving this gradient descent weight update formula  each feature fi is scaled by ri  

   

q

n
 
ni

giving fi   ri fi  

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

r
initial feature vector 
r
initial weight vector w
initial problem difficulty d

difficulty at
target level or out
of time 

yes

r
final r
and w

no

increase problem
difficulty
r d  keep
and   

learn new feature
correlating to the bellman
error for states in the
training
r set  and add it
to    keep the current
problem difficulty d 

r
w

r

select w approximately
minimizing
error
r bellman
r
of v   w  

done

reweighted value
r r
function v   w 

yes

performance at
current difficulty
meets threshold 

no

generate feature
training set

figure    control flow for feature learning  boxes with double borders represent assumed subroutines for our method  we assume that the problem distribution is parameterized by
problem difficulty  such as problem size  

feedback  e g  goal states   learning initially in more difficult problems will typically lead to
inability to find positive feedback and random walk behavior  as a result learning first in lower
difficulty problems has been found more effective  martin   geffner        yoon  fern    givan 
       we show experimentally in section   that good value functions for high difficulty problems
can indeed be learned in this fashion from problems of lower  increasing difficulties 
our approach relies on two assumed subroutines  and can be instantiated in different ways by
providing different algorithms for these subroutines  first  a method of weight selection is assumed 
this method takes as input a problem domain and a fixed set of features  and selects a weight vector
for a value function for the problem domain using the provided features  we intend this method
to heuristically or approximately minimize l bellman error in its choice of weight vector  but
in practice it may be easier to adjust weights to approximate l  bellman error  second  a feature
hypothesis space and corresponding learner are assumed to be provided by the system designer 
the control flow for our approach is shown in figure    each iteration at a fixed problem
distribution selects weights for the current feature set  using any method attempting to minimize
l bellman error  to define a new value function v   selects a training set of states for feature
learning  then learns a new feature correlating well to the statewise bellman error of v   adding
that feature to the feature set  a user provided performance threshold function  detects when to
increase the problem difficulty  a formalization of this control flow is given in figure    in the form
of pseudo code 
   

fiw u   g ivan

feature discovering value function construction



inputs 
initial feature vector      initial weight vector 
w   
sequence of problem distributions d    d         dmax of increasing difficulty 
performance threshold function   
    d  v   tests the performance of value function v in distribution d 



outputs 
feature vector    weight vector 
w



 

      
w 
w    d   

  

while not  d   max or out of time 




select 
w approximately minimizing bellman error of v   
w   over dd



if   dd   
w   

  
  
  
  

then d  d    

  

else
generate a sequence of training states t using dd

  
  
  
   




learn new feature f correlating to the bellman error feature b 
w      
for the states in t






       f    
w   
w     

 
return    
w

notes 
   b     is the statewise bellman error function  as defined in section     
   the code for approximate value iteration avi  shown in online appendix    available on jair website  on
page    is an example implementation of line   



   the code for draw greedy 
w      n
   shown in online appendix   on page    is an example impletraining

mentation of line    ntraining is the number of states in the feature training set  duplicated states are removed
as specified in section     



   the beam search code for learning relational features beam search learn score   t  b 
w         is an
example implementation of line    where beam search learn is shown in figure   in section    and score is
defined in section     

figure    pseudo code for learning a set of features 
for the experiments reported in section    we evaluate the following choices for the assumed
subroutines  for all experiments we use avi to select weights for feature sets  we evaluate two
choices for the feature hypothesis space and corresponding learner  one relational and one propositional  as described in section   
separate training sets are drawn for weight selection and for the feature learning  the former
will depend on the weight selection method  and is described for avi in section      and the latter
is described in this section 
problem difficulty is increased when sampled performance of the greedy policy at the current
difficulty exceeds user specified performance thresholds  in our planning domain experiments  the
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

performance parameters measured are success ratio  percentage of trials that find the goal  and average successful plan length  the average number of steps to the goal among all successful trials  
the non goal oriented domains of tetris and sysadmin use different performance measures  average total reward for tetris and bellman error for sysadmin  to facilitate comparison with patrascu
et al         
we also assume a user provided schedule for problem difficulty increases in problems where
difficulty is parameterized by more than one parameter  e g   size may be measured in by the number
of objects of each type   further domain independent automation of the increase in difficulty is a
topic for future research  we give the difficulty increase schedules and performance thresholds for
our experiments in the section presenting the experiments  section   
    training set generation
the training set for selection of a new feature is a set of states  the training set is constructed by
repeatedly sampling an example problem instance from the problem distribution at the current level
of difficulty  and applying the current greedy policy greedy v   to that problem instance to create
a trajectory of states encountered  every state  removing duplicates  encountered is added to the
training set  the size of the feature selection training set and the maximum length of each training
trajectory are specified by the user as parameters of the algorithm 
retaining duplicate states in the training set is another option that can be considered  our preliminary empirical results have not favored this option  but it is certainly worth further exploration 
we note that the goal of finding a near optimal value function does not necessarily make reference
to a state distribution  the most widely used notion of near optimal in the theory of mdps is
the sup norm distance to v    moreover  the state distribution represented by the duplicates in our
training sets is typically the distribution under a badly flawed policy  heeding this distribution can
prevent correcting bellman error in critical states that are visited by this policy  but visited only
rarely   these states may be  for instance  rarely visited good exits from the visited state region
that are being misunderstood by the current value function   at this point  our primary justification
for removing duplicates is the empirical performance we have demonstrated in section   
similar reasoning would suggest removing duplicate states in the training sets for avi weight
training  described in section      because there are many large avi training sets generated in our
experiments  duplicate removal must be carefully handled to control runtime  for historical reasons 
our experiments shown here do not include duplicate removal for avi 
a possible problem occurs when the current greedy policy cannot reach enough states to complete the desired training set  if     consecutive trajectories are drawn without visiting a new state
before the desired training set size is reached  the process is modified as follows  at that point 
the method attempts to complete the training set by drawing trajectories using random walk  again
using sampled example problems from the current problem distribution   if this process again leads
to     consecutive trajectories without a new state  the method terminates training set generation
and uses the current training set even though it is smaller than the target size 
    applicability of the method
feature discovering value function construction as just described does not require complete access
to the underlying mdp model  our avi updates and training set generation are both based on the
following computations on the model 
   

fiw u   g ivan

   given a state s the ability to compute the action set a s  
   given a state s  action a  a s   and value function v   the ability to compute the q value
q s  a  v   
   given a state s and action a  a s   the ability to draw a state from the next state distribution
defined by t  s  a  s   
   given a state s  the ability to compute the features in the selected feature language on s and
any computations on the state required for the selected feature learner  as examples 
 a  in section    we introduce a relational feature language and learner that require knowledge of a set of domain predicates  and their arities  such that each state is a conjunctive
set of predicate facts  see section      
 b  and  also in section    we describe a propositional feature language and learner that
require knowledge of a set of propositional state attributes such that each state is a truth
assignment to the attributes 
the first three items enable the computation of the bellman update of s and the last item enables
computation of the estimated value function given the weights and features defining it as well as the
selection of new features by the feature learner  these requirements amount to substantial access to
the problem model  as a result our method must be considered a model based technique 
a consequence of these requirements is that our algorithm cannot be directly applied to the
standard reinforcement learning setting where the only model access is via acting in the world
without the ability to reset to selected states  in this setting bellman error computations for particular
states cannot necessarily be carried out  it would be possible to construct a noisy bellman error
training set in such a model free setting and it would be appropriate future work to explore the use
of such a training set in feature learning 
while the ppddl planning domains studied provide all the information needed to perform these
computations  our method also applies to domains that are not natural to represent in ppddl  these
can be analyzed by our method once the above computations can be implemented  for instance  in
our tetris experiments in section      the underlying model is represented by providing hand coded
routines for the above computations within the domain 
    analysis
mdp value iteration is guaranteed to converge to the optimal value function if conducted with
a tabular value function representation in the presence of discounting  bertsekas         although
weight selection in avi is designed to mimic value iteration  while avoiding a tabular representation 
there is no general guarantee that the weight updates will track value iteration and thus converge
to the optimal value function  in particular  there may be no weighted combination of features that
represents the optimal value function  and likewise none that represents the bellman update u v  
for some value function v produced by avi weight training process  our learning system introduces
new features to the existing feature ensemble in response to this problem  the training set used to
select the new feature pairs states with their statewise bellman error  if the learned feature exactly
captures the statewise bellman error concept  by exactly capturing the training set and generalizing
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

successfully  then the new feature space will contain the bellman update of the value function used
to generate the training data 
we aim to find features that approximate the bellman error feature  which we take to be a
function mapping states to their statewise bellman error  theoretical properties of bellman error
features in the uncontrolled markov processes  i e   without the max operator in the bellman equation  have recently been discussed in the work of parr et al          where the addition of such
features  or close approximations thereof  is proven to reduce the weighted l   norm distance between the best weight setting and the the true  uncontrolled  value v    when linear fixed point
methods are used to train the weights before feature addition  prior to that work  in wu   givan 
       and now in parallel to it  we have been empirically exploring the effects of selecting bellman
error features in the more complex controlled case  leading to the results reported here 
it is clear that if we were to simply add the bellman error feature directly  and set the corresponding weight to one  the resulting value function would be the desired bellman update u v  
of the current value function v   adding such features at each iteration would thus give us a way
to conduct value iteration exactly  without enumerating states  but each such added feature would
describe the bellman error of a value function defined in terms of previously added features  posing
a serious computational cost issue when evaluating the added features  in particular  each bellman
error feature for a value function v can be estimated at any particular state with high confidence by
evaluating the value function v at that state and at a polynomial sized sample of next states for each
action  based on chernoff bounds  
however  if the value function v is based upon a previously added bellman error feature  then
each evaluation of v requires further sampling  again  for each possible action  to compute  in this
manner  the amount of sampling needed for high confidence grows exponentially with the number of
successive added features of this type  the levels of sampling do not collapse into one expectation
because of intervening choices between actions  as is often the case in decision theoretic sampling 
our feature selection method is an attempt to tractably approximate this exact value iteration method
by learning concise and efficiently computable descriptions of the bellman error feature at each
iteration 
our method can thus be viewed as a heuristic approximation to exact value iteration  exact
value iteration is the instance of our method obtained by using an explicit state value table as the
feature representation and generating training sets for feature learning containing all states  to
obtain exact value iteration we would also omit avi training but instead set each weight to one 
when the feature language and learner can be shown to approximate explicit features tightly
enough  so that the resulting approximate bellman update is a contraction in the l norm   then it is
easy to prove that tightening approximations of v  will result if all weights are set to one  however 
for the more practical results in our experiments  we use feature representations and learners for
which no such approximation bound relative to explicit features is known 

   two candidate hypothesis spaces for features
in this section we describe two hypothesis spaces for features  a relational feature space and a
propositional feature space  along with their respective feature learning methods  for each of the
two feature spaces  we assume the learner is provided with a training set of states paired with their
statewise bellman error values 
   

fiw u   g ivan

note that these two feature space learner pairs lead to two instances of our general method and
that others can easily be defined by defining new feature spaces and corresponding learners  in this
paper we empirically evaluate the two instances presented here 
    relational features
a relational mdp is defined in terms of a set of state predicates  these state predicates are the basic
elements from which we define a feature representation language  below  we define a generalpurpose means of enriching the basic set of state predicates  the resulting enriched predicates
can be used as the predicate symbols in standard first order predicate logic  we then consider any
formula in that logic with one free variable as a feature  as follows   
a state in a relational mdp is a first order interpretation  a first order formula with one free
variable is then a function from such states to natural numbers which maps each state to the number
of objects in that state that satisfy the formula  we take such first order formulas to be real valued
features by normalizing to a real number between zero and onethis normalization is done by
dividing the feature value by the maximum value that the feature can take  which is typically the
total number of objects in the domain  but can be smaller than this in domains where objects  and
quantifiers  are typed  a similar feature representation is used in the work of fawcett        
this feature representation is used for our relational experiments  but the learner we describe
in the next subsection only considers existentially quantified conjunctions of literals  with one free
variable  as features  the space of such formulas is thus the effective feature space for our relational
experiments 
example      take blocksworld with the table as an object for example  on x  y  is
a predicate in the domain that asserts the block x is on top of the object y  where y
may be a block or the table  a possible feature for this domain can be described as y
on x  y   which is a first order formula with x as the one free variable  this formula
means that there is some other object immediately below the block object x  which
essentially excludes the table object and the block being held by the arm  if any  from
the object set described by the feature  for n blocks problems  the un normalized value
of this feature is n for states with no block being held by the arm  or n    for states
with a block being held by the arm 
      t he e nriched p redicate s et
more interesting examples are possible with the enriched predicate set that we now define  to enrich
the set of state predicates p   we add for each binary predicate p a transitive closure form of that
predicate p  and predicates min p and max p identifying minimal and maximal elements under
that predicate  in goal based domains  recall that our problem representation  from section     
includes  for each predicate p  a goal version of the predicate called goal p to represent the desired
state of the predicate p in the goal  here  we also add a means ends analysis predicate correct p to
represent p facts that are present in both the current state and the goal 
so  for objects x and y  correct p x y  is true if and only if both p x  y  and goal p x y  are
true  p  x  y  is true of objects x and y connected by a path in the binary relation p  the relation
max p x  is true if object x is a maximal element with respect to p  i e   there exists no other object
   generalizations to allow multiple free variables are straightforward but of unclear utility at this time 

   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

y such that p x  y  is true  the relation min p x  is true if object x is a minimal element with respect
to p  i e   there exists no other object y such that p y  x  is true 
we formally define the feature grammar in online appendix    available on jair website  on
page   
example      cont    the feature y correct on x  y  means that x is stacked on top of
some object y both in the current state and in the goal state  the feature y on  x  y 
means that in the current state  x is directly above some object y  i e   there is a sequence
of on relations traversing a path between x and y  inclusively  the feature max on  x 
means that x is the table object when all block towers are placed on the table  since the
table is the only object that is not on any other object  the feature min on  x  means
that there is no other object on top of x  i e   x is clear 
    learning relational features
we select first order formulas as candidate features using a beam search with a beam width w   we
present the pseudo code for beam search in figure    the search starts with basic features derived
automatically from the domain description and repeatedly derives new candidate features from the
best scoring w features found so far  adding the new features as candidates and keeping only the
best scoring w features at all times  after new candidates have been added a fixed depth d times 
the best scoring feature found overall is selected to be added to the value function representation 
candidate features are scored for the beam search by their correlation to the bellman error feature
as formalized below 
specifically  we score each candidate feature f with its correlation coefficient to the bellman
error feature b v    as estimated by a training set  the correlation coefficient between functions

  s  
 and  is defined as corr coef        e  s   s  e  s  e 
  instead of using a known
   
distribution to compute this value  we use the states in the training set s and compute a sampled
version by using the following equations to approximate the true expectation e and the true standard
deviation  of any random variable x 
  x
x s   
es  x s    
 s   
s s

x s  

corr coef sampled      s    

s

  x
 x s    e x s      
 s   
s s

es   s   s    es   s  es    s  
 
 s   s

the scoring function for feature selection is then a regularized version of the correlation coefficient
between the feature and the target function 
score f  s        corr coef sampled f    s       depth f    
where the depth of a feature is the depth in the beam search at which it first occurs  and  is a
parameter of the learner representing the degree of regularization  bias towards low depth features  
   

fiw u   g ivan

beam search learn
inputs 

feature scoring function fscore   features        

outputs 

new feature f

system parameters 

w   beam width
maxd   max number of beam search iterations
  degree of regularization  as defined in section    

  
  
  
  
  
  
  
  

i  the set of basic features  as defined in section     
d     f  i 
repeat
set beam b to the highest scoring w candidates in f  
candidate feature set f  b 
for each candidate f   b
for each candidate f    b  i   f     f 
f   f  combine f    f    

  
   
   

d  d     
until  d   maxd   or  highest score so far      d   
return the maximum scoring feature f  f  

notes 
   feature scoring function fscore f   is used to rank candidates in lines   and     a discussion of a sample
scoring function  used in our relational experiments  is given in section     
   candidate scores can be cached after calls to fscore  so that no candidate is scored twice 
   the value     d  is the largest score a feature of depth d can have 

figure    pseudo code for beam search 

the value score f  s   b v     is then the score of how well a feature f correlates to the bellman error feature  note that our features are non negative  but can still be well correlated to the
bellman error  which can be negative   and that the presence of a constant feature in our representation allows a non negative feature to be shifted automatically as needed 
it remains only to specify which features in the hypothesis space will be considered initial  or
basic  features for the beam search  and to specify a means for constructing more complex features
from simpler ones for use in extending the beam search  we first take the state predicate set p in
a domain and enrich p as described in section      after this enrichment of p   we take as basic
features the existentially quantified applications of  possibly negated  state predicates to variables
with zero or one free variable    a grammar for basic features is defined as follows 
   if the domain distinguishes any objects by naming them with constants  we allow these constants as arguments to the
predicates here as well 

   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

definition  a basic feature is an existentially quantified hliterali expression with at
most one free variable  see figure   in online appendix    available on jair website 
on page    
a feature with no free variables is treated technically as a one free variable feature where that
variable is not used  this results in a binary feature value that is either zero or the total number of
objects  because instantiating the free variable different ways always results in the same truth value 
we assume throughout that every existential quantifier is automatically renamed away from every
other variable in the system  we can also take as basic features any human provided features that
may be available  but we do not add such features in our experiments in this paper in order to clearly
evaluate our methods ability to discover domain structure on its own 
at each stage in the beam search we add new candidate features  retaining the w best scoring
features from the previous stage   the new candidate features are created as follows  any feature in
the beam is combined conjunctively with any other  or with any basic feature  the method of combination of two features is described in figure    this figure shows non deterministic pseudo code
for combining two input features  such that any way of making the non deterministic choices results
in a new candidate feature  the pseudo code refers to the feature formulas f  and f  describing the
two features  in some places  these formulas and others are written with their free variable exposed 
as f   x  and f   y   also substitution for that variable is notated by replacing it in the notation  as
in f   z  
the combination is by conjoining the feature formulas  as shown in line   of figure    however 
there is additional complexity resulting from combining the two free variables and possibly equating
bound variables between the two features  the two free variables are either equated  by substitution  or one is existentially quantified before the combination is done  in line    up to two pairs
of variables  chosen one from each contributing feature  may also be equated  with the resulting
quantifier at the front  as described in line    every such combination feature is a candidate 
this beam search construction can lead to logically redundant features that are in some cases
syntactically redundant as well  we avoid syntactically redundant features at the end of the beam
search by selecting the highest scoring feature that is not already in the feature set  logical redundancy that is not syntactic redundancy is more difficult to detect  we avoid some such redundancy
automatically by using ordering during the beam search to reduce the generation of symmetric expressions such as    and     however  testing logical equivalence between features in our
language is np hard  chandra   merlin         so we do not deploy a complete equivalence test
here 
example      assume we have two basic features z p x  z  and w q y  w   the set
of the possible candidates that can be generated by combining these two features are 
when line   in figure   runs zero times 
    x z p x  z     w q y  w    from xf   x   f   y 
    z p x  z     y w q y  w    from f   x   yf   y   and
    z p x  z     w q x  w    from f   x   f   x 
and when line   runs one time 
   u   z p u  z     q y  u     from equating x and w in item   above 
   u  x p x  u     q y  u    from equating x and z in item   above 
   

fiw u   g ivan

combine
inputs 

features f   x   f   y 

outputs 

set of features  o   

return the set of all features o  that can result from 
  

perform one of
a  f     x f   x 
b  f     y f   y 
c  f    f   x 

  

o    f   f 

  

perform the following variable equating step zero  one  or two times 
a  let v be a variable occurring in f  and o   
let e  be the expression of the form  v    v  that occurs in o 
b  let w be a variable occurring in f  and o   
let e  be the expression of the form  w    w  that occurs in o 
c  let u be a new variable  not used in o 
d  o    replace e  with    u  and replace e  with    u  in o 
e  o     u o 

notes 
   the choice between  a   b  and  c  the choice of number of iterations of step    and the choices of e  and e 
in steps  a and  b are all non deterministic choices 
   any feature that can be produced by any run of this non deterministic algorithm is included in the set of
features that is returned by combine 
   it is assumed that f  and f  have no variables in common  by renaming if necessary before this operation 

figure    a non deterministic algorithm for combining two feature formulas 

   u  p x  u    w q u  w     from equating z and y in item   above 
   u  p x  u    y q y  u     from equating z and w in item   above  and
   u  p x  u     q x  u     from equating z and w in item   above 
the first three are computed using cases  a   b  and  c  respectively  the remaining
five derive from the first three by equating bound variables from f  and f   
features generated at a depth k in this language can easily require enumerating all k tuples
of domain objects  since the cost of this evaluation grows exponentially with k  we bound the
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

maximum number of quantifiers in scope at any point in any feature formula to q  and refuse to
consider any feature violating this bound 
the values w     d  and q are the parameters controlling the relational learner we evaluate in
this paper  how we set these parameters is discussed further in the experimental setup description
in section   
we provide a brief discussion on the motivations for our feature combination method  first  we
note that additive combination of features can represent disjunctions of features    hence  we only
consider conjunction during feature combination  here  we have chosen to conjoin features in
multiple ways  varying the handling combining of the free and bound variables  we do not believe
our choice to be uniquely effective  but provide it as an example realization of the proposed featurediscovery architecture 
any choice of feature representation and combination method must trade off between the cost
of evaluation of more choices and the potential gain in quality of the selected features  here  we
have chosen to limit individual features to conjunction  effectively  we have limited the features to
horn clauses over the predicates and their negations  with univariate heads 
    propositional features
here we discuss a second candidate hypothesis space for features  using a propositional representation  we use decision trees to represent these propositional features  a detailed discussion of
classification using decision trees can be found in the book by mitchell         a decision tree is
a binary tree with internal nodes labeled by binary tests on states  edges labeled yes and no
representing results of the binary tests  and leaves labeled with classes  in our case  either zero or
one   a path through the tree from the root to a leaf with label l identifies a labeling of some set of
stateseach state consistent with the state test results on the path is viewed as labeled l by the tree 
in this way  a decision tree with real number labels at the leaves is viewed as labeling all states with
real numbers  and is thus a feature 
we learn decision trees from training sets of labeled states using the well known c    algorithm
 quinlan         this algorithm induces a tree greedily matching the training data from the root
down  we use c    to induce new featuresthe key to our algorithm is how we construct suitable
training sets for c    so that the induced features are useful in reducing bellman error 
we include as possible state tests for the decision trees we induce every grounded predicate
application  from the state predicates  as well as every previously selected decision tree feature
 each of which is a binary test because all leaf labels are zero or one  
    learning propositional features
to construct binary features  we use only the sign of the bellman error feature  not the magnitude  the sign of the statewise bellman error at each state serves as an indication of whether the
state is undervalued or overvalued by the current approximation  at least with respect to exactly
representing the bellman update of the current value function  if we can identify a collection of
undervalued states as a new feature  then assigning an appropriate positive weight to that feature
   representing the disjunction of overlapping features using additive combination can be done with a third feature
representing the conjunction  using inclusion exclusion and a negative weight on the conjunction 
   a grounded predicate application is a predicate applied to the appropriate number of objects from the problem instance 

   

fiw u   g ivan

will increase their value  similarly  identifying overvalued states with a new feature and assigning
a negative weight will decrease their value  we note that the domains of interest are generally too
large for state space enumeration  so we will need classification learning to generalize the notions
of overvalued and undervalued across the state space from training sets of sample states 
to enable our method to ignore states that are approximately converged  we discard states with
statewise bellman error near zero from either training set  specifically  among the states with negative statewise bellman error  we discard any state with such error closer to zero than the median
within that set  we do the same among the states with positive statewise bellman error  more sophisticated methods for discarding training data near the intended boundary can be considered in
future research  these will often introduce additional parameters to the method  here  we seek an
initial and simple evaluation of our overall approach  after this discarding  we define   to be
the set of all remaining training pairs with states having positive statewise bellman error  and 
likewise those with negative statewise bellman error 
we then use   as the positive examples and  as the negative examples for a supervised
classification algorithm  in our case  c    is used  the hypothesis space for classification the space
of decision trees built with tests selected from the primitive attributes defining the state space and
goal  in our case  we also use previously learned features that are decision trees over these attributes 
the concept resulting from supervised learning is then treated as a new feature for our linear approximation architecture  with an initial weight of zero 
our intent  ideally  is to develop an approximately optimal value function  such a value function
can be expected to have bellman error at many states  if not every state  however  low state wise
error in some states does not contribute to high sup norm bellman error  our discarding training
states with low statewise bellman error reflects our tolerance of such low error below some threshold
representing the degree of approximation sought  note that the technical motivation for selecting
features based upon bellman error focuses on reducing the sup norm bellman error  given this
motivation  we are not as interested in finding the exact boundary between positive and negative
bellman error as we are in identifying which states have large magnitude bellman error  so that that
large magnitude error can be addressed by feature addition  
we observe that there is limited need to separately learn a feature matching  due to the
following representability argument  consider a binary feature f and its complement f   so that
exactly one of f and f is true in each state  given the presence of a constant feature in the feature
set  adding f or f to the feature set yields the same set of representable value functions  assigning
weight w to f has the same effect as assigning weight w to f and adding w to the weight of the
constant feature  
    discussion
we discuss below the generalization capability  learning time  and heuristic elements of our feature
learning method 
      g eneralization across varying d omain s izes
the propositional feature space described above varies in size as the number of objects in a relational
domain is varied  as a result  features learned at one domain size are not generally meaningful  or
even necessarily defined  at other domain sizes  the relational approach above is  in contrast  able
to generalize naturally between different domains sizes  our experiments report on the ability of
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

the propositional technique to learn within each domain size directly  but do not attempt to use that
approach for learning from small problems to gain performance in large problems  this is a major
limitation in producing good results for large domains 
      l earning t ime
the primary motivation for giving up generalization over domain sizes in order to employ a propositional approach is that the resulting learner can use highly efficient  off the shelf classification
algorithms  the learning times reported in section   show that our propositional learner learns new
features orders of magnitude faster than the relational learner 
      h euristic e lements

of the

m ethod

as mentioned earlier  our algorithm heuristically approximates the repeated addition of bellman
error features to a linear value function approximation in order to carry out value iteration  also
as mentioned earlier  value iteration itself is guaranteed to converge to the optimal value function 
however  due to the scale of problems we target  heuristic approximations are required  we discuss
the motivations for each heuristic approximation we employ briefly here 
first  we do not compute exact bellman error features  instead  we use machine learning to fit
a training set of sample states and their bellman error values  the selection of this training set is
done heuristically  using trajectories drawn from the current greedy policy  our use of on policy
selection of training data is loosely motivated by on policy convergence results for reinforcement
learning  singh et al          and serves to focus training on relevant states   see section      
second  for the relational instance of our feature framework  the beam search method we use to
select the highest scoring relational feature  with the best fit to the bellman error  is ad hoc  greedy 
and severely resource bounded  the fit obtained to the bellman error is purely heuristic  we provide
our heuristic method for this machine learning problem only as an example  and we intend future
research to provide better relational learners and resulting better planning performance  heuristic
elements of the current method are further discussed in appendix a    our work here can be
viewed as providing a reduction from stochastic planning to structured machine learning of numeric
functions   see section    
third  for the propositional instance of our feature framework   the learner c    selects hypotheses greedily  also  our reduction to c    classification relies on an explicit tolerance of approximation in the form of the threshold used to filter training data with near zero bellman error  the
motivation for this approximation tolerance is to focus the learner on high bellman error states and
allow the method to ignore almost converged states   see section      
fourth  fundamental to this work is the use of a linear approximation of the value function and
gradient descent based weight selection  in this case avi   these approximation methods are a key
approach to handling large state spaces and create the need for feature discovery  our avi method
includes empirically motivated heuristic methods for controlling step size and sign changes in the
weights   see section   in online appendix    available on jair website  
fifth  we rely on human input to select the sequence of problem difficulties encountered during
feature discovery as well as the performance thresholds at which problem difficulty increases  we
believe this aspect of the algorithm can be automated in future research   see section    
   

fiw u   g ivan

   related work
automatic learning of relational features for approximate value function representation has surprisingly not been frequently studied until quite recently  and remains poorly understood  here  we
review recent work that is related on one or more dimensions to our contribution 
    feature selection based on bellman error magnitude
feature selection based on bellman error has recently been studied in the uncontrolled  policyevaluation  context in the work of keller et al         and parr et al          with attribute value
or explicit state spaces rather than relational feature representations  feature selection based on
bellman error is further compared to other feature selection methods in the uncontrolled context
both theoretically and empirically in the work of parr  li  taylor  painter wakefield  and littman
       
here  we extend this work to the controlled decision making setting and study the incorporation
of relational learning and the selection of appropriate knowledge representation for value functions
that generalize between problems of different sizes within the same domain 
the main contribution of the work of parr et al         is formally showing  for the uncontrolled
case of policy evaluation  that using  possibly approximate  bellman error features provably tightens approximation error bounds  i e   that adding an exact bellman error feature provably reduces
the  weighted l   norm  distance from the optimal value function that can be achieved by optimizing the weights in the linear combination of features  this result is extended in a weaker form to
approximated bellman error features  again for the uncontrolled case  the limitation to the uncontrolled case is a substantial difference from the setting of our work  the limited experiments shown
use explicit state space representations  and the technique learns a completely new set of features
for each policy evaluation conducted during policy iteration  in contrast  our method accumulates
features during value iteration  at no point limiting the focus to a single policy  constructing a
new feature set for each policy evaluation is a procedure more amenable to formal analysis than
retaining all learned features throughout value iteration because the policy being implicitly considered during value iteration  the greedy policy  is potentially changing throughout  however  when
using relational feature learning  the runtime cost of feature learning is currently too high to make
constructing new feature sets repeatedly practically feasible 
parr et al         builds on the prior work by keller et al         that also studied the uncontrolled setting  that work provides no theoretical results nor any general framework  but provides
a specific approach to using bellman error in attribute value representations  where a state is represented as a real vector  in order to select new features  the approach provides no apparent leverage
on problems where the state is not a real vector  but a structured logical interpretation  as is typical
in planning benchmarks 
    feature discovery via goal regression
other previous methods  gretton   thiebaux        sanner   boutilier        find useful features
by first identifying goal regions  or high reward regions   then identifying additional regions by regressing through the action definitions from previously identified regions  the principle exploited
is that when a given state feature indicates value in the state  then being able to achieve that feature
in one step should also indicate value in a state  regressing a feature definition through the action
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

definitions yields a definition of the states that can achieve the feature in one step  repeated regression can then identify many regions of states that have the possibility of transitioning under some
action sequence to a high reward region 
because there are exponentially many action sequences relative to plan length  there can be
exponentially many regions discovered in this way  as well as an exponential increase in the size of
the representation of each region  both exponentials are in terms of the number of regression steps
taken  to control this exponential growth in the number of features considered  regression has been
implemented with pruning optimizations that control or eliminate overlap between regions when it
can be detected inexpensively as well as dropping of unlikely paths  however  without a scoring
technique  such as the fit to the bellman error used in this paper  to select features  regression still
generates a very large number of useless new features  the currently most effective regression based
first order mdp planner  described in the work of sanner and boutilier         is only effective
when disallowing overlapping features to allow optimizations in the weight computation  yet clearly
most human designed feature sets in fact have overlapping features 
our inductive technique avoids these issues by considering only compactly represented features 
selecting those which match sampled statewise bellman error training data  we provide extensive
empirical comparison to the first order approximate linear programming technique  foalp 
from the work of sanner and boutilier        in our empirical results  our empirical evaluation
yields stronger results across a wide range of probabilistic planning benchmarks than the goalregression approach as implemented in foalp  although aspects of the approaches other than the
goal regression candidate generation vary in the comparison as well  
regression based approaches to feature discovery are related to our method of fitting bellman
error in that both exploit the fact that states that can reach valuable states must themselves be valuable  i e  both seek local consistency  in fact  regression from the goal can be viewed as a special
case of iteratively fitting features to the bellman error of the current value function  depending
on the exact problem formulation  for any k  the bellman error for the k step to go value function
will be non zero  or otherwise nontrivially structured  at the region of states that reach the goal first
in k     steps  significant differences between our bellman error approach and regression based
feature selection arise for states which can reach the goal with different probabilities at different
horizons  our approach fits the magnitude of the bellman error  and so can smoothly consider the
degree to which each state reaches the goal at each horizon  our approach also immediately generalizes to the setting where a useful heuristic value function is provided before automatic feature
learning  whereas the goal regression approach appears to require goal regions to begin regression 
in spite of these issues  we believe that both approaches are appropriate and valuable and should be
considered as important sources of automatically derived features in future work 
effective regression requires a compact declarative action model  which is not always available   
the inductive technique we present does not require even a pddl action model  as the only deductive component is the computation of the bellman error for individual states  any representation
from which this statewise bellman error can be computed is sufficient for this technique  in our empirical results we show performance for our planner on tetris  where the model is represented only
by giving a program that  given any state as input  returns the explicit next state distribution for that
state  foalp is inapplicable to such representations due to dependence on logical deductive rea   for example  in the second international probabilistic planning competition  the regression based foalp planner
required human assistance in each domain in providing the needed domain information even though the standard
pddl model was provided by the competition and was sufficient for each other planner 

   

fiw u   g ivan

soning  we believe the inductive and deductive approaches to incorporating logical representation
are both important and are complementary 
the goal regression approach is a special case of the more general approach of generating candidate features by transforming currently useful features  others that have been considered include
abstraction  specialization  and decomposition  fawcett         research on human defined concept transformations dates back at least to the landmark ai program am  davis   lenat        
our work uses only one means of generating candidate features  a beam search of logical formulas
in increasing depth  this means of candidate generation has the advantage of strongly favoring concise and inexpensive features  but may miss more complex but very accurate useful features  but
our approach directly generalizes to these other means of generating candidate features  what most
centrally distinguishes our approach from all previous work leveraging such feature transformations
is the use of statewise bellman error to score candidate features  foalp  sanner   boutilier       
      uses no scoring function  but includes all non pruned candidate features in the linear program
used to find an approximately optimal value function  the zenith system  fawcett        uses a
scoring function provided by an unspecified critic 
    previous scoring functions for mdp feature selection
a method  from the work of patrascu et al          selects features by estimating and minimizing
the l  error of the value function that results from retraining the weights with the candidate feature
included  l  error is used in that work instead of bellman error because of the difficulty of retraining
the weights to minimize bellman error  because our method focuses on fitting the bellman error
of the current approximation  without retraining with the new feature   it avoids this expensive
retraining computation during search and is able to search a much larger feature space effectively 
while the work of patrascu et al         contains no discussion of relational representation  the l 
scoring method could certainly be used with features represented in predicate logic  no work to date
has tried this  potentially too expensive  approach 
    other related work
we include discussion of additional  more distantly related research directions as appendix a  divided into the following subsections 
   other relevant feature selection methods  fahlman   lebiere        utgoff   precup       
      rivest   precup        mahadevan   maggioni        petrik        
   structural model based and model free solution methods for markov decision processes  including
 a  relational reinforcement learning  rrl  systems  dzeroski  deraedt    driessens 
      driessens   dzeroski        driessens et al         
 b  policy learning via boosting  kersting   driessens        
 c  fitted value iteration  gordon         and
 d  exact value iteration methods in first order mdps  boutilier  reiter    price       
holldobler   skvortsova        kersting  van otterlo    de raedt        
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

   inductive logic programming algorithms  muggleton        quinlan        karalic   bratko 
      
   approximate policy iteration for relational domains  fern et al          with a discussion on
relational decision list policy learners  khardon        martin   geffner        yoon et al  
      
   automatic extraction of domain knowledge  veloso  carbonell  perez  borrajo  fink   
blythe        kambhampati  katukam    qu        estlin   mooney        fox   long 
      gerevini   schubert        

   experimental setting
we present experiments in nine stochastic planning domains  including both reward oriented and
goal oriented domains  we use pentium   xeon    ghz machines with  gb memory  in this section  we give a general overview of our experiments before giving detailed results and discussion for
individual domains in section    here  first  we briefly discuss the selection of evaluation domains
in section      second  in section     we set up an evaluation of our relational feature learner by
comparison to variants that replace key aspects of the algorithm with random choice to determine
their importance  additional details  including many experimental parameter settings  can be found
in online appendix    available on jair website  in section   
    domains considered
in all the evaluation domains below  it is necessary to specify a discount factor  when modeling the
domain as an mdp with discounting  the discount factor effectively specifies the tradeoff between
the goals of reducing expected plan length and increasing success rate   is not a parameter of our
method  but of the domain being studied  and our feature learning method can be applied for any
choice of   here  for simplicity  we choose  to be      throughout all our experiments  we note
that this is the same discount factor used in the s ys a dmin domain formalization that we compare
to from the previous work by patrascu et al         
      t etris
in section     we evaluate the performance of both our relational and propositional learners using
the stochastic computer game t etris  a reward oriented domain where the goal of a player is to
maximize the accumulated reward  we compare our results to the performance of a set of handcrafted features  and the performance of randomly selected features 
      p lanning c ompetition d omains
in section      we evaluate the performance of our relational learner in seven goal oriented planning domains from the two international probabilistic planning competitions  ippcs   younes et al  
      bonet   givan         for comparison purposes  we evaluate the performance of our propositional learner on two of the seven domains  b locksworld and a variant of b oxworld described
below   results from these two domains illustrate the difficulty of learning useful propositional features in complex planning domains  we also compare the results of our relational planner with
two recent competition stochastic planners ff replan  yoon et al         and foalp  sanner  
   

fiw u   g ivan

boutilier              that have both performed well in the planning competitions  finally  we
compare our results to those obtained by randomly selecting relational features and tuning weights
for them  for a complete description of  and ppddl source for  the domains used  please see the
work of younes et al         and bonet and givan        
every goal oriented domain with a problem generator from the first or second ippc was considered for inclusion in our experiments  for inclusion  we require a planning domain with fixed
action definitions  as defined in section      that in addition has only ground conjunctive goal regions  four domains have these properties directly  and we have adapted three more of the domains
to have these properties 
   in b oxworld  we modify the problem generator so that the goal region is always a ground
conjunctive expression  we call the resulting domain c onjunctive  b oxworld 
   in f ileworld  we construct the obvious lifted version  and create a problem generator restricted to three folders because in this domain the action definitions vary with the number of
folders  we call the resulting domain l ifted  f ileworld   
   in t owers of h anoi  we create our own problem generator 
the resulting selection provides seven ippc planning domains for our empirical study  we provide
detailed discussions on the adapted domains in section   of online appendix    available on jair
website   as well as discuss the reasons for the exclusion of domains 
      s ys a dmin
we conclude our experiments by comparing our propositional learner with a previous method by patrascu et al          using the the same s ys a dmin domain used for evaluation there  this empirical
comparison on the s ys a dmin domain is shown in section     
    randomized variants of the method
our major contribution is the introduction and evaluation of a feature learning framework in the
controlled setting based on scoring with bellman error  be scoring   our empirical work instantiates this framework with a relational feature learning algorithm of our design based on greedy
beam search  here  we compare the performance of this instance of our framework with variants
that replace key aspects with randomized choice  illustrating the relative importance of those features  in the two random choice experiments  we adapt our method in one of the following two
ways 
   labeling the training states with random scores instead of bellman error scores  the target
value in our feature training set is a random number from    to    this algorithm is called
random scoring 
   narrowing the beam during search randomly rather than greedily  we eliminate scoring during the beam search  instead using random selection to narrow the beam  only at the end of
the beam search is scoring used to select the best resulting candidate  this algorithm is called
random beam narrowing 
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

the original algorithm  which labels training data with bellman error and narrows the beam greedily rather than randomly  is called greedy beam search be scoring in our plots  for these
comparisons  we only consider the relational feature representation  as that is where our beam
search method is used  experiments with the two variants introduced here  presented below in
sections       and        show that our original method selects features that perform much better
than randomly selected features  and that the greediness in the beam search is often  but not always 
important in achieving good performance 

   experimental results
we present experimental results for t etris  planning competition domains  and s ys a dmin in this
section  starting with an introduction on the structure of our result presentation 
    how to read our results
the task of evaluating a feature learning planning system is subtle and complex  this is particularly
a factor in the relational case because generalization between problem sizes and learning from small
problems must be evaluated  the resulting data is extensive and highly structured  requiring some
training of the reader to understand and interpret  here we introduce to the reader the structure of
our results 
in experiments with the propositional learning  or with randomly selected propositional features   the problem size never varies within one run of the learner  because the propositional representation from section     cannot generalize between sizes  we run a separate experiment for each
size considered  each experiment is two independent trials  each trial starts with a single trivial
feature and repeatedly adds features until a termination condition is met  after each feature addition  avi is used to select the weights for combining the features to form a value function  and the
performance of that value function is measured  by sampling the performance of the greedy policy  
we then compute the average  of the two trials  of the performance as a function of the number
of features used  since this results in a single line plot of performance as a function of number
of features  several different fixed problem size learners can be compared on one figure  with one
line for each  as is done for example in figures   and     the performance measure used varies
appropriately with the domain as presented below 
we study the ability of relational representation from section     to generalize between sizes 
this study can only be properly understood against the backdrop of the flowchart in figure    as
described in this flowchart  one trial of the learner will learn a sequence of features and encounter
a sequence of increasing problem difficulties  one iteration of the learner will either add a new
feature or increase the problem difficulty  depending on the current performance   in either case 
the weights are then retrained by avi and a performance measurement of the resulting greedy policy
is taken  because different trials may increase the size at different points  we cannot meaningfully
average the measurements from two trials  instead  we present two independent trials separately
in two tables  such as the figures   and     for the first trial  we also present the same data a
second time as a line plot showing performance as a function of number of features  where problem
size changes are annotated along the line  such as the plots in figures   and     note that success
ratio generally increases along the line when features are added  but falls when problem size is
increased   in t etris  however  we measure rows erased rather than success ratio  and rows
   

fiw u   g ivan

erased generally increases with either the addition of a new feature or the addition of new rows to
the available grid  
to interpret the tables showing trials of the relational learner  it is useful to focus on the first
two rows  labeled   of features and problem difficulty  these rows  taken together  show the
progress of the learner in adding features and and increasing problem size  each column in the table
represents the result in the indicated problem size using the indicated number of learned features 
from one column to the next  there will be a change in only one of these rowsif the performance
of the policy shown in a column is high enough  it will be the problem difficulty that increases  and
otherwise it will be the number of features that increases  further adding to the subtlety in interpreting these tables  we note that when several adjacent columns increase the number of features 
we sometimes splice out all but two of these columns to save space  thus  if several features are
added consecutively at one problem size  with slowly increasing performance  we may show only
the first and last of these columns at that problem size  with a consequent jump in the number of
features between these columns  we likewise sometimes splice out columns when several consecutive columns increase problem difficulty  we have found that these splicings not only save space
but increase readability after some practice reading these tables 
performance numbers shown in each column  success ratio and average plan length  or number
of rows erased  for t etris  refer to the performance of the weight tuned policy resulting for that
feature set at that problem difficulty  we also show in each column the performance of that value
function  without re tuning weights  on the target problem size  thus  we show quality measures
for each policy found during feature learning on both the current problem size at that point and on
the target problem size  to illustrate the progress of learning from small problems on the target size
via generalization 
we do not study here the problem of deciding when to stop adding features  instead  in both
propositional and relational experiments  trials are stopped by experimenter judgment when additional results are too expensive for the value they are giving in evaluating the algorithm  however 
we do not stop any trials when they are still improving unless unacceptable resource consumption
has occurred 
also  in each trial  the accumulated real time for the trial is measured and shown at each point
during the trial  we use real time rather than cpu time to reflect non cpu costs such as paging due
to high memory usage 
    tetris
we now present experimental results for t etris 
      overview

of

t etris

the game t etris is played in a rectangular board area  usually of size         that is initially
empty  the program selects one of the seven shapes uniformly at random and the player rotates and
drops the selected piece from the entry side of the board  which piles onto any remaining fragments
of the pieces that were placed previously  in our implementation  whenever a full row of squares
is occupied by fragments of pieces  that row is removed from the board and fragments on top of
the removed row are moved down one row  a reward is also received when a row is removed  the
process of selecting locations and rotations for randomly drawn pieces continues until the board
is full and the new piece cannot be placed anywhere in the board  t etris is stochastic since
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

the next piece to place is always randomly drawn  but this is the only stochastic element in this
game  t etris is also used as an experimental domain in previous mdp and reinforcement learning
research  bertsekas   tsitsiklis        driessens et al          a set of human selected features is
described in the book by bertsekas and tsitsiklis        that yields very good performance when
used in weighted linearly approximated value functions  we cannot fairly compare our performance
in this domain to probabilistic planners requiring ppddl input because we have found no natural
ppddl definition for t etris 
our performance metric for t etris is the number of rows erased averaged over        trial
games  the reward scaling parameter rscale  defined in section   in online appendix   on page   
is selected to be   
      t etris r elational f eature l earning r esults
we represent the t etris grid using rows and columns as objects  we use three primitive predicates 
fill c  r   meaning that the square on column c  row r is occupied  below r    r     meaning that row
r  is directly below row r    and beside c    c     meaning that column c  is directly to the left of
column c    the quantifiers used in our relational t etris hypothesis space are typed using the types
row and column 
there are also state predicates representing the piece about to drop  however  for efficiency
reasons our planner computes state value as a function only of the grid  not the next piece  this
limitation in value function expressiveness allows a significantly cheaper bellman backup computation  the one step lookahead in greedy policy execution provides implicit reasoning about the
piece being dropped  as that piece will be in the grid in all the next states 
we conduct our relational t etris experiments on a    column  n row board  with n initially
set to   rows  our threshold for increasing problem difficulty by adding one row is a score of at
least         n     rows erased  the target problem size for these experiments is    rows  the
results for the relational t etris experiments are given in figures   and   and are discussed below 
      t etris p ropositional f eature l earning r esults
for the propositional learner  we describe the t etris state with   binary attributes that represent
which of the   pieces is currently being dropped  along with one additional binary attribute for each
grid square representing whether that square is occupied  the adjacency relationships between the
grid squares are represented only through the procedurally coded action dynamics  note that the
number of state attributes depends on the size of the t etris grid  and learned features will only
apply to problems of the same grid size  as a result  we show separate results for selected problem
sizes 
we evaluate propositional feature learning in    column t etris grids of four different sizes   
rows    rows    rows  and    rows  results from these four trials are shown together in figure   and
the average accumulated time required to reach each point on figure   is shown in figure    these
results are discussed below 
      e valuating the i mportance of b ellman   error s coring and g reedy
b eam   search in t etris
figure   compares our original algorithm with alternatives that vary from it on either training set
scoring or greediness of beam search  as discussed in section      for the two alternatives  we use
   

fiw u   g ivan

trial   
  of features
problem difficulty
score
accumulated time  hr  
target size score

 
 
   
   
   

 
 
   
 
   

 
 
   
   
   

 
 
   
   
   

  
 
  
  
   

  
 
  
  
   

  
 
  
  
   

  
 
  
  
   

  
 
  
  
   

  
 
  
  
   

  
 
  
  
   

  
 
   
  
   

  
  
   
  
   

  
  
   
   
   

  
  
   
   
   

trial   
  of features
problem difficulty
score
accumulated time  hr  
target size score

 
 
   
   
   

 
 
   
   
   

 
 
  
  
   

 
 
  
  
   

  
 
  
  
   

  
 
  
  
   

  
 
  
  
   

  
  
   
  
   

  
  
   
  
   

  
  
   
  
   

  
  
   
  
   

  
  
   
   
   

  
  
   
   
   

  
  
   
   
   

  
  
   
   
   

  
  
   
   
   

  
  
   
   
   

  
  
   
   
   

  
  
   
   
   

  
  
   
   
   

figure    t etris performance  averaged over        games   score is shown in average rows
erased  and problem difficulty is shown in the number of rows on the t etris board  the
number of columns is always     difficulty increases when the average score is greater
than        n     where n is the number of rows in the t etris board  target problem
size is    rows  some columns are omitted as discussed in section     

average rows erased

tetris  relational  trial  
   
   
   
   
   
   
  
 

    
    

   

   

   

   
 

    
   

 

 

 

 

  

  

  

  

  

number of features

figure    plot of the average number of lines erased over        t etris games after each run of
avi training during the learning of relational features  trial     vertical lines indicate
difficulty increases  in the number of rows   as labeled along the plot 

   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

average rows erased

tetris  propositional
  
  
  
 
 
 
 
 
                                                                        

number of features
   

   

   

    

figure    plot of the average number of lines erased in        t etris games after each iteration
of avi training during the learning of propositional features  averaged over two trials 

accumulated time  hr  

tetris  propositional
   
   
   
   
  
  
  
  
 
                                                                        

number of features
   

   

   

    

figure    plot of the accumulated time required to reach each point in figure    averaged over two
trials 

the same schedule used for the original greedy beam search be scoring algorithm in t etris by
starting with the       problem size  however  the performance of these two alternatives is never
good enough to increase the problem size 
      e valuating h uman   designed f eatures in t etris
in addition to evaluating our relational and propositional feature learning approach  we also evaluate
how the human selected features described in the book by bertsekas and tsitsiklis        perform
in selected problem sizes  for each problem size  we start from all weights zero and use our avi
   

fiw u   g ivan

average rows erased

impact of greedy beam search and be scoring
  
  
  
  
  
 
 
 
 
 
                                                                        

number of relational features
greedy beam search be scoring  original algorithm 
random scoring  variant   
random beam narrowing  variant   

figure    plot of the average number of lines erased in        t etris games for relational features
learned from the original algorithm and the two alternatives as discussed in section     
for random scoring and random beam narrowing  the results are averages over two
independent trials  trials of these two variants are terminated when they fail to make
progress for several feature additions  for comparison purposes  trial one of the original
greedy beam search be scoring method is shown  reaching the threshold for difficulty
increase after eleven feature additions  trial two did even better  

average rows erased  trial  
average rows erased  trial  

                        
  
  
          
  
  
          

figure     the average number of lines erased in        t etris games for the best weighted
combination of human features found in each of two trials of avi and four problem
sizes 

process described in section     to train the weights for all    features until the performance appears
  
 
to   k    
as human designed features
to converge  we change the learning rate  from   k    
require a larger step size to converge rapidly  the human designed features are normalized to a
value between   and   here in our experiments  we run two independent trials for each problem size
and report the performance of the best performing weight vector found in each trial  in figure    
      p erformance c omparison b etween d ifferent a pproaches

to

t etris

several general trends emerge from the results on t etris  first of all  the addition of new learned
features is almost always increasing the performance of the resulting tuned policy  on the current
size and on the target size   until a best performance point is reached  this suggests we are in fact
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

relational prop        prop       prop       prop       
average feature learning
time  min  

   

  

  

  

  

figure     table for the average feature learning time for relational and propositional approaches 
selecting useful features  we also find clear evidence of the ability of the relational representation
to usefully generalize between problem sizes  substantial performance is developed on the target
problem size without ever training directly in that size 
we find that the best performance of learned propositional features is much lower than that of
learned relational features in all problem sizes shown here  even though a larger feature training set
size and many more learned features are used for the propositional approach  this suggests that
the rich relational representation indeed is able to better capture the dynamics in t etris than the
propositional representation 
we find that the performance of using random features in t etris is significantly worse than that
of using learned features  demonstrating that our performance improvements in feature learning are
due to useful feature selection  using bellman error   not simply due to increasing the number of
features 
our learned relational feature performance in        t etris is far worse than that obtained
by using the human selected features with avi in the same size  however  in       t etris the
relational feature performance is close to that of the human designed features  the human designed
features are engineered to perform well in the        t etris hence some concepts that are useful
in performing well in smaller problem sizes may not exist in these features 
      t ime to l earn e ach f eature
in figure    we show the average time required to learn a relational feature or a propositional feature
in t etris 
the time required to learn a relational feature is significantly longer than that required to learn
a propositional feature  even though for the propositional approach a larger feature training set size
is being used 
      c omparison

to

p revious t etris   specific l earners

in evaluating domain independent techniques on t etris  we must first put aside the strong performance already shown many times in the literature for domain dependent techniques on that domain 
then  we must face the problem that there are no published domain independent comparison points
in order to define a state of the art target to surpass  for the latter problem  we provide a baseline
from two different approaches to random feature selection  and show that our targeted feature selection dramatically improves on random selection  for the former problem  we include below a
discussion of the domain specific elements of key previous published results on t etris 
there have been many previous domain specific efforts at learning to play t etris  bertsekas
  tsitsiklis        szita   lorincz        lagoudakis  parr    littman        farias   van roy 
      kakade         typically  these provide human crafted domain dependent features  and deploy domain independent machine learning techniques to combine these features  often by tuning
   

fiw u   g ivan

weights for a linear combination   as an example  a domain specific feature counting the number
of covered up holes in the board is frequently used  this feature is plausibly derived by human
reasoning about the rules of the game  such as realizing that such holes are difficult to fill by later
action and can lead to low scores  in all prior work  the selection of this feature is by hand  not by
an automated feature selection process  such as our scoring of correlation to bellman error   other
frequently used domain specific features include column height and difference in height of adjacent columns  again apparently selected as relevant by human reasoning about the rules of the
game 
the key research question we address  then  is whether useful features can be derived automatically  so that a decision making situation like t etris can be approached by a domain independent
system without human intervention  our method is provided only a domain state representation using primitive horizontal and vertical positional predicates  and a single constant feature  to our
knowledge  before this research there is no published evaluation on t etris that does not rely
on domain specific human inputs such as those just discussed  as expected  our performance on
t etris is much weaker than that achieved by domain specific systems such as those just cited 
    probabilistic planning competition domains
throughout the evaluations of our learners in planning domains  we use a lower plan length cutoff of
     steps when evaluating success ratio during the iterative learning of features  to speed learning 
we use a longer cutoff of      steps for the final evaluation of policies for comparison with other
planners and for all evaluations on the target problem size  the reward scaling parameter rscale
 defined in section   in online appendix   on page    is selected to be   throughout the planning
domains 
for domains with multi dimensional problem sizes  it remains an open research problem on how
to change problem size in different dimensions automatically to increase difficulty during learning 
here  in c onjunctive  b oxworld and z enotravel  we hand design the sequence of increasing problem sizes 
as discussed in section        we evaluate our feature learners in a total of seven probabilistic planning competition domains  in the following paragraphs  we provide a full discussion of
b locksworld and c onjunctive  b oxworld  with abbreviated results for the other five domains  we provide a full discussion of the other five domains in appendix b 
our relational feature learner finds useful value function features in four of these domains
 b locksworld  c onjunctive  b oxworld  t ireworld  and l ifted  f ileworld     in the
other three domains  z enotravel  e xploding b locksworld  and t owers of h anoi   our
relational feature learner makes progress in representing a useful fixed size value function for the
training sizes  but fails to find features that generalize well to problems of larger sizes 
      b locksworld
in the probabilistic  non reward version of b locksworld from the first ippc  the actions pickup
and putdown have a small probability of placing the handled block on the table object instead of on
the selected destination 
for our relational learner  we start with   blocks problems  we increase from n blocks to n    
blocks whenever the success ratio exceeds     and the average successful plan length is less than
   n      the target problem size is    blocks  results are shown in figures    and    
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

trial   
  of features
problem difficulty
success ratio
plan length
accumulated time  hr  
target size sr
target size slen 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
  
      
        
 
      
               
          
                          
  
 
 
 
                     




               

trial   
  of features
problem difficulty
success ratio
plan length
accumulated time  hr  
target size sr
target size slen 

 
 
 
  
   
 


 
 
 
 
 
 
 
 
 
 
 
 
  
  
 
        
 
      
            
          
                          
 
 
                     



               

figure     b locksworld performance  averaged over     problems  for relational learner  we
add one feature per column until success ratio exceeds     and average successful plan
length is less than    n      for n blocks  and then increase problem difficulty for the
next column  plan lengths shown are successful trials only  problem difficulties are
measured in number of blocks  with a target problem size of    blocks  some columns
are omitted as discussed in section     

for our propositional learner  results for problem sizes of          and    blocks are shown in
figure    
our relational learner consistently finds value functions with perfect or near perfect success
ratio up to    blocks  this performance compares very favorably to the recent rrl  driessens
et al         results in the deterministic b locksworld  where goals are severely restricted to  for
instance  single on atoms  and the success ratio performance of around     for three to ten blocks
 for the single on goal  is still lower than that achieved here  our results in b locksworld show
the average plan length is far from optimal  we have observed large plateaus in the induced value
function  state regions where all states are given the same value so that the greedy policy wanders 
this is a problem that merits further study to understand why feature induction does not break such
plateaus  separately  we have studied the ability of local search to break out of such plateaus  wu 
kalyanam    givan        
the performance on the target size clearly demonstrates successful generalization between sizes
for the relational representation 
the propositional results demonstrate the limitations of the propositional learner regarding lack
of generalization between sizes  while very good value functions can be induced for the small
problem sizes    and   blocks   slightly larger sizes of   or    blocks render the method ineffective 
in    block problems  the initial random greedy policy cannot be improved because it never finds
   

fiw u   g ivan

success ratio

blocksworld  relational  trial  
  blocks

 

  blocks

  blocks

         blocks
   blocks

    
  blocks
   
    
    

successful plan length

 

 

 

   

 
   blocks

   
   
  blocks

   

  blocks

  blocks

   blocks

  blocks

  blocks
  blocks

 
 

 

 

 

number of features

figure     b locksworld success ratio and average successful plan length  averaged over    
problems  for the first trial from figure    using our relational learner 

the goal  in addition  these results demonstrate that learning additional features once a good policy
is found can degrade performance  possibly because avi performs worse in the higher dimensional
weight space that results 
      c onjunctive  b oxworld
the probabilistic  non reward version of b oxworld from the first ippc is similar to the more
familiar logistics domain used in deterministic planning competitions  except that an explicit connectivity graph for the cities is defined  in logistics  airports and aircraft play an important role
since it is not possible to move trucks from one airport  and the locations adjacent to it  to another airport  and the locations adjacent to it   in b oxworld  it is possible to move all the boxes
without using the aircraft since the cities may all be connected with truck routes  the stochastic
element introduced into this domain is that when a truck is being moved from one city to another 
there is a small chance of ending up in an unintended city  as described in section      we use
c onjunctive  b oxworld  a modified version of b oxworld  in our experiments 
we start with   box problems in our relational learner and increase from n boxes to n     boxes
whenever the success ratio exceeds     and the average successful plan length is better than   n 
all feature learning problem difficulties use   cities  we use two target problem sizes     boxes
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

blocksworld  propositional

success ratio

 
   
   
   
   

successful plan length

   
   
   
   
   
   
   
   
  
 

accumulated time  hr  

 

  
  
  
  
  
  
  
  
  
 

 

 

 

 

 

  

 

 

 

 

 

  

 

 

 

 

 

  

number of features

  blocks

  blocks

  blocks

   blocks

figure     b locksworld performance success ratio and average successful plan length  averaged over     problems   and accumulated run time for our propositional learner  averaged over two trials 

   

fiw u   g ivan

trial   
  of features
 
 
 
problem difficulty
 
 
 
success ratio
      
 
plan length
         
accumulated time  hr            
      
 
target size    sr
target size    slen 
           
target size    sr
              
target size    slen 
            

 
 
 
  
  
 
  
    
   

 
 
 
  
  
 
  
    
   

 
 
 
  
  
 
  
    
   

 
  
 
  
  
 
  
    
   

 
  
 
  
  
 
  
    
   

 
  
 
   
  
 
   
    
    

 
  
 
  
  
 
  
    
   

 
  
 
  
  
 
  
    
   

trial   
  of features
 
 
 
problem difficulty
 
 
 
success ratio
      
 
plan length
         
accumulated time  hr            
target size    sr
      
 
           
target size    slen 
target size    sr
             
target size    slen 
            

 
 
 
  
  
 
  
    
   

 
 
 
  
  
 
  
    
   

 
 
 
  
  
 
  
    
   

 
 
 
  
  
 
  
    
   

 
  
 
   
  
 
   
    
    

 
  
 
  
  
 
  
    
   

 
  
    
   
  
 
   
    
    

 
  
 
  
  
 
  
    
   

 
  
 
  
  
 
  
    
   

figure     c onjunctive  b oxworld performance  averaged over     problems   we add one
feature per column until success ratio is greater than     and average successful plan
length is less than   n  for n boxes  and then increase problem difficulty for the next
column  problem difficulty is shown in number of boxes  throughout the learning
process the number of cities is    plan lengths shown are successful trials only  two
target problem sizes are used  target problem size    has    boxes and   cities  target
problem size    has    boxes and    cities  some columns are omitted as discussed in
section     

and   cities  and    boxes and    cities  relational learning results are shown in figures    and    
and results for the propositional learner on   cities with       or   boxes are shown in figures    
in interpreting the c onjunctive  b oxworld results  it is important to focus on the average
successful plan length metric  in c onjunctive  b oxworld problems  random walk is able to
solve the problem nearly always  but often with very long plans     the learned features enable
more direct solutions as reflected in the average plan length metric 
only two relational features are required for significantly improved performance in the problems
we have tested  unlike the other domains we evaluate  for the c onjunctive  b oxworld domain
    we note that  oddly  the ippc competition domain used here has action preconditions prohibiting moving a box away
from its destination  these preconditions bias the random walk automatically towards the goal  for consistency with
the competition results  we retain these odd preconditions  although these preconditions are not necessary for good
performance for our algorithm 

   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

success ratio

conjuctive boxworld    cities  relational  trial  
  box

 

                and    boxes

  box

    
    
 

 

 

   

successful plan length

  box
   
   
   

   boxes
   boxes

  box
  boxes
  boxes

  

  boxes
  box

 
 

 

 

number of features

figure     c onjunctive  b oxworld success ratio and average successful plan length  averaged
over     problems  for the first trial using our relational learner 

the learned features are straightforwardly describable in english  the first feature counts how many
boxes are correctly at their target city  the second feature counts how many boxes are on trucks 
we note the lack of any features rewarding trucks for being in the right place  resulting in
longer plan lengths due to wandering on value function plateaus   such features can easily be written in our knowledge representation  e g  count the trucks located at cities that are the destinations
for some package on the truck   but require quantification over both cities and packages  the severe
limitation on quantification currently in our method for efficiency reasons prevents consideration of
these features at this point  it is also worth noting that regression based feature discovery  as studied in the work of gretton and thiebaux        and sanner and boutilier         can be expected
to identify such features regarding trucks by regressing the goal through the action of unloading
a package at the destination  combining our bellman error based method with regression based
methods is a promising future direction 
nevertheless  our relational learner discovers two concise and useful features that dramatically
reduce plan length relative to the initial policy of random walk  this is a significant success for
automated domain independent induction of problem features 
   

fiw u   g ivan

conjunctive boxworld  propositional

success ratio

 
   
   
   
   

successful plan length

 
 

 

 

 

 

  

 

 

 

 

 

  

 

 

 

 

 

  

   
   
   
   
   
   
   
   
   
  
 

accumulated time  hr  

   
   
   
   
   
   
   
   
  
 

number of features
  box

  box

  box

figure     c onjunctive  b oxworld performance  averaged over     problems  and accumulated run time for propositional learner  averaged over two trials  throughout the learning process the number of cities is   

   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

one trial of the relational feature learner in c onjunctive  b oxworld takes several days 
even though we have fixed the number of cities for the training problems at five cities  new techniques are required for improving the efficiency of feature learning before we can provide results
for training in larger numbers of cities  our results here demonstrate that the current representation
and learning methods adequately manage small city graphs even with larger and larger numbers of
boxes to deliver  and that the resulting value functions successfully generalize to    city problems 
in this domain  a well known weakness of avi is apparent  while avi often works in practice 
there is no theoretical guarantee on the quality of the weight vector found by avi training   alternatively  an approximate linear programming step could replace avi training to provide a more
expensive but perhaps more robust weight selection   in the c onjunctive  b oxworld results 
avi training goes astray when selecting weights in the    box domain size in trial    as a result 
the selected weights overemphasize the first feature  neglecting the second feature  this is revealed
in the data shown because the plan length performance degrades significantly for that one column
of data  when avi is repeated at the next problem size     boxes   good performance is restored 
a similar one column degradation of plan length occurs in trial   at the    box and    box sizes 
for our propositional experiments in the c onjunctive  b oxworld  we note that  generally 
adding learned propositional features degrades the success rate performance relative to the initial
random walk policy by introducing ineffective loops into the greedy policy  the resulting greedy
policies find the goal in fewer steps than random walk  but generally pay an unacceptable drop in
success ratio to do so  the one exception is the policy found for one box problems using just two
propositional features  which significantly reduces plan length while preserving success ratio  still 
this result is much weaker than that for our relational feature language 
these problems get more severe as problem size increases  with   box problems suffering severe
degradation in success rate with only modest gains in successful plan length  also please note
that accumulated runtime for these experiments is very large  especially for   box problems  avi
training is very expensive for policies that do not find the goal  computing the greedy policy at each
state in a long trajectory requires considering each action  and the number of available actions can
be quite large in this domain  for these reasons  the propositional technique is not evaluate at sizes
larger than three boxes 
      s ummary r esults

from

a dditional d omains

in figures    to     we present summary results from five additional probabilistic planning domains  for detailed results and full discussion of these domains  please see appendix b  from the
summary results  we can see that our feature learning approach successfully finds features that perform well across increasing problem sizes in two of these five domains  t ireworld and l ifted f ileworld    in the other three domains  z enotravel  t owers of h anoi  and e xploding
b locksworld   feature learning is able to make varying degrees of progress on fixed small problem sizes  but that progress  sometimes quite limited  does not generalize well as size increases 
      e valuating the r elative i mportance of b ellman   error s coring and
g reedy b eam   search in g oal   oriented d omains
figure    compares our original algorithm with alternatives that vary from it on either training set
scoring or greediness of beam search  as discussed in section      for each trial of each variant  we
generate a greedy policy for each domain using feature selection within our relational representation
   

fiw u   g ivan

tireworld  trial  

success ratio

 
   

  nodes

     nodes

  nodes

       nodes

  nodes

  nodes

      nodes

 

 

  nodes

   
   
   
  nodes

   
   
 

successful plan length

 

 

 

 

 

       nodes

 

  nodes

 

  nodes

  nodes

 

      nodes
     nodes

 

        nodes

 
 
 

 

 

 

 

 

number of features

zenotravel  trial  
success ratio

 
  cities    person    aircraft

   
   

  cities    people    aircraft

   

  cities    people    aircraft

 
   

successful plan length

 

 

 

 

 

 

 

 

 

 

   
   

  cities    people    aircraft

  cities    people    aircraft

   
   

  cities    person    aircraft

   
 
 

 

 

 

 

 

 

 

 

 

number of features

figure     summary results for t ireworld and z enotravel  for full discussion and detailed
results  please see appendix b 

   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

exploding blocksworld  trial  

success ratio

 
   

  blocks

  blocks
  blocks

   

  blocks
  blocks

   

  blocks

   
 

successful plan length

 

 

 

 

 

 

 

 

 

 

  

 
 

  blocks

 

  blocks

 
  blocks

  blocks

 

  blocks

  blocks

 
 
 

 

 

 

 

 

 

 

 

 

  

number of features

tower of hanoi  trial  

success ratio

 
  discs

   
   

  discs

   

  discs

   

  discs

  discs
 

successful plan length

 

 

 

 

 

 

 

 

  
 

 

  
  

  
  
  

  discs

  
  

  discs

  discs
 
 

 

 

 

 

 

 

 

 

number of features

figure     summary results for e xploding b locksworld and t owers
discussion and detailed results  please see appendix b 

   

of

h anoi  for full

fiw u   g ivan

success ratio

lifted fileworld   trial  
  file

 

  file

  file

  and   files

   to    files

   files

  to    files

   to    files

    
    
 

 

 

 

 

 

 

 

successful plan length

  
   files
   files

   files

  

   files
   files

   files
   files
   files

   files

   files

   files

  
  file

  file
  file

  files

  files

  files
  files

  file

 
 

 

 

 

 

 

 

 

number of features

figure     summary results for l ifted  f ileworld    for full discussion and detailed results 
please see appendix b 

 alternating avi training  difficulty increase  and feature generation as in the original algorithm  
during each trial  in each domain  we select the best performing policy  running the algorithm until
the target problem difficulty is reached or there is no improvement for at least three feature additions 
in the latter case generating at least nine features  we evaluate each greedy policy acquired in this
manner  measuring the average target problem size performance in each domain  and average the
results of two trials  the results are shown in figure    
in no domain does the alternative random scoring perform comparably to the original greedy
beam search be scoring  with the exception of three domain size combinations where both learners
perform very poorly  z enotravel     block e xploding b locksworld  and   disc t owers
of h anoi    the alternative random beam narrowing is sometimes adequate to replace the original
approach  but in some domains  greedy beam search is critical to our performance 
      c omparison

to

ff r eplan

and

foalp

we compare the performance of our learned policies to ff replan and foalp on each of the
ppddl evaluation domains used above  we use the problem generators provided by the planning
competitions to generate    problems for each tested problem size except for t owers of h anoi
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

domain
size
greedy beam be scoring  orig   sr
greedy beam be scoring  orig   slen 
random scoring  var     sr
random scoring  var     slen 
random beam narrowing  var     sr
random beam narrowing  var     slen 
random walk sr
random walk slen 

bw
  
    
   
 

    
   
 


box box tire zeno ex bw ex bw toh toh file
                          
 
  
 
    
 
              
    
                
  
   
 
    
 
  
 
     
                   
    
                
        
 
   
 
  
         
 
              
    
                
  
   
 
    
 
  
         
                   
    
 
           
         
 
   
 

         

figure     target problem size performance  averaged over     problems  for relational features
learned from the original algorithm and the two alternatives as discussed in section     
and from random walk  averaged over the best results of two independent trials for each
target problem size 

and l ifted  f ileworld    where there is one fixed problem for each problem size  we evaluate
the performance of each planner    times for each problem  and report in fig     the success ratio
of each planner in each problem size  averaged over all attempts   our policies  learned from the
two independent trials shown above  are indicated as rfavi    and rfavi     each planner has a
   minute time limit for each attempt  the average time required to finish a successful attempt for
the largest problem size in each domain is reported in figure    
for each of the two trials of our learner in each domain  we evaluate here the policy that performed the best in the trial on the  first  target problem size   here  a policy is a set of features
and a corresponding weight vector learned by avi during the trial   performance is measured by
success rate  with ties broken by plan length  any remaining ties are broken by taking the later
policy in the trial from those that are tied  in each case  we consider that policy to be the policy
learned from the trial 
the results show that our planners performance is incomparable with that of ff replan  winning in some domains  losing in others  and generally dominates that of foalp 
rfavi performs the best of the planners in larger b locksworld  c onjunctive b oxworld  and t ireworld problems  rfavi is essentially tied with ff replan in performance
in l ifted  f ileworld    rfavi loses to ff replan in the remaining three domains  e xploding
b locksworld  z enotravel  and t owers of h anoi  reasons for the difficulties in the last
three domains are discussed above in the sections presenting results for those domains  we note that
foalp does not have a learned policy in z enotravel  e xploding b locksworld  t owers
of h anoi   and l ifted  f ileworld   
rfavi relies on random walk to explore plateaus of states not differentiated by the selected
features  this reliance frequently results in long plan lengths and at times results in failure  we
have recently reported elsewhere on early results from ongoing work remedying this problem by
using search in place of random walk  wu et al         
the rfavi learning approach is very different from the non learning online replanning used
by ff replan  where the problem is determinized  dropping all probability parameters  it is an
   

fiw u   g ivan

rfavi   
rfavi   
ff replan
foalp

   blocks bw
       
          
         
      

   blocks bw
       
          
         
         

   blocks bw
           
           
        
        

   blocks bw
           
           
          
          

rfavi   
rfavi   
ff replan
foalp

   bx  ci box
      
      
      
      

   bx   ci box
          
          
          
          

   bx   ci box
          
          
          
          

   bx  ci box
      
      
      
         

rfavi   
rfavi   
ff replan
foalp

   nodes tire
        
        
        
        

   nodes tire
        
        
        
        

   nodes tire
        
        
        
        

   ci  pr  at zeno
           
           
      
n a

rfavi   
rfavi   
ff replan
foalp

  blocks ex bw
        
        
        
n a

   blocks ex bw   discs toh
         
        
         
        
         
        
n a
n a

  discs toh
    
    
        
n a

   bx   ci box
          
          
           
         

   files lifted file
      
      
      
n a

figure     comparison of our planner  rfavi  against ff replan and foalp  success ratio for a
total of     attempts     attempts for t owers of h anoi and l ifted  f ileworld   
for each problem size is reported  followed by the average successful plan length in
parentheses  the two rows for rfavi map to two learning trials shown in the paper 

   bw         bx    tire          zeno    ex bw   toh    files
rfavi   
   s
  s
 s
  s
 s

 s
rfavi   
   s
  s
 s
  s
 s

 s
ff replan    s
   s
 s
 s
 s
 s
  s
foalp
  s
   s
  s
n a
n a
n a
n a
figure     average runtime of the successful attempts  from the results shown in figure     on the
largest problem size for each domain 

   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

important topic for future research to try to combine the benefits obtained by these very different
planners across all domains 
the dominance of rfavi over foalp in these results implies that rfavi is at the state of the
art among first order techniques  those that work with the problem in lifted form and use lifted
generalization  although foalp uses first order structure in feature representation  the learned
features are aimed at satisfying goal predicates individually  not as a whole  we believe that the
goal decomposition technique can sometimes work well in small problems but does not scale well
to large problems 
in these comparisons  it should also be noted that foalp does not read ppddl domain descriptions directly  but requires human written domain axioms for its learning  unlike our completely
automatic technique  requiring only a few numeric parameters characterizing the domain   this
requirement for human written domain axioms is one of the reasons why foalp did not compete
in some of the competition domains and does not have a learned policy for some of the domains
tested here 
in c onjunctive  b oxworld     we note that ff replan uses an all outcomes problem determinization that does not discriminate between likely and unlikely outcomes of truck movement
actions  as a result  plans are frequently selected that rely on unlikely outcomes  perhaps choosing
to move a truck to an undesired location  relying on the unlikely outcome of accidentally moving
to the desired location   these plans will usually fail  resulting in repeated replanning until ff luckily selects the high likelihood outcome or plan execution happens to get the desired low likelihood
outcome  this behavior is in effect similar to the behavior our learned value function exhibits because  as discussed in section        our learner failed to find any feature rewarding appropriate
truck moves  both planners result in long plan lengths due to many unhelpful truck moves  however  our learned policy conducts the random walk of trucks much more efficiently  and thus more
successfully  than the online replanning of ff replan  especially in the larger problem sizes  we
believe even more dramatic improvements will be available with improved knowledge representation for features 
    sysadmin
a full description of the s ys a dmin domain is provided in the work of guestrin  koller  and parr
        here  we summarize that description  in the s ys a dmin domain  machines are connected
in different topologies  each machine might fail at each step  and the failure probability depends on
the number of failed machines connected to it  the agent works toward minimizing the number of
failed machines by rebooting machines  with one machine rebooted at each time step  for a problem
with n machines and a fixed topology  the dynamic state space can be sufficiently described by n
propositional variables  each representing the on off status of a certain machine 
we test this domain for the purpose of direct comparison of the performance of our propositional techniques to the published results in the work of patrascu et al          we test exactly the
topologies evaluated there and measure the performance measure reported there  sup norm bellman
error 
we evaluate our method on the exact same problems  same mdps  used for evaluation in the
work of patrascu et al         for testing this domain  two different kinds of topologies are tested 
    we hand convert the nested universal quantifiers and conditional effects in the original boxworld domain definition
to an equivalent form without universal quantifiers and conditional effects to allow ff replan to read the domain 

   

fiw u   g ivan

s

s

cycle topology

  legs topology

figure     illustration of the two topologies in the s ys a dmin domain     nodes   each node
represents a machine  the s label indicates a server machine  as specified in the work
of patrascu et al         

  legs and cycle  the   legs topology has three three node legs  each a linear sequence of three
connected nodes  each connected to a single central node at one end  the cycle topology arranges
the ten nodes in one large cycle  there are    nodes in each topology  these two topologies
are illustrated in figure     the target of learning in this domain is to keep as many machines
operational as possible  so the number of operating machines directly determines the reward for
each step  since there are only    nodes and the basic features are just the on off statuses of the
nodes  there are a total of      states  the reward scaling parameter rscale  defined in section   in
online appendix    available on jair website  on page    is selected to be    
the work by patrascu et al         uses l  sup norm  bellman error as the performance
measurement in s ys a dmin  our technique  as described above  seeks to reduce mean bellman
error more directly than l bellman error  we report the l bellman error  averaged over two
trials  in figure     also included in figure    are the results shown in the work of patrascu et al 
        we select the best result shown there  from various algorithmic approaches  from the   legs
and cycle topologies shown in their paper  these correspond to the d o s setting for the cycle
topology and the d x n setting for the   legs topology  in the terminology of that paper 
both topologies show that our algorithm reduces the l bellman error more effectively per
feature as well as more effectively overall than the experiments previously reported in the work of
patrascu et al          both topologies also show bellman error eventually diverges as avi cannot
handle the complexity of the error function as dimensionality increases  our algorithm can still
achieve low bellman error by remembering and restoring the best performing weighted feature set
once weakened performance is detected 
we note that our superior performance in reducing bellman error could be due entirely or in
part to the use of avi for weight training instead of approximate linear programming  alp   the
method used by patrascu et al  however  no such systematic superiority is known for avi over alp 
so these results suggest superior performance of the feature learning itself 
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

sysadmin    legs topology
  

  

  

 

bellman error

 
 
 
 
 
 
 
 
 
                                                                                

number of features
  legs  learned

  legs  patrascu

sysadmin  cycle topology
     

  
 

bellman error

 
 
 
 
 
 
 
 
 
                                                                                   

number of features
cycle  learned
cycle  patrascu

figure     l bellman error for the s ys a dmin domain     nodes  for two topologies  values for
the results from the work of patrascu et al         are taken from figure   and   of the
work of patrascu et al         

    demonstration of generalization across problem sizes
an asset of the relational feature representation presented in this paper is that learned relational
features are applicable to any problem size in the same domain  in section      we have discussed
   

fiw u   g ivan

target problem sizes
       tetris    blocks bw     box    city  bx    nodes tire    files lifted file
intermediate problem sizes        tetris    blocks bw     box    city  bx    nodes tire    files lifted file
generalize from target size
learn in intermediate size
random walk

  
   
   

       
       
    

      
       
          

        
        
        

      
      
      

figure     performance in intermediate sized problems by generalization  we show here the
performance of value functions learned in target problem sizes when evaluated on
intermediate sized problems  to demonstrate generalization between sizes  for comparison  also on intermediate sized problems  we show the performance of value functions learned directly in the intermediate size as well as the performance of random
walk  generalization results and intermediate size learning results are averages of two
trials  for t etris  average accumulated rows erased are shown  for the goal oriented
domains  success ratio and successful plan length  in parentheses  are shown for each
domain 

the modeling of a planning domain as an infinite set of mdps  one for each problem instance in
the domain  over this infinite set of mdps  a feature vector plus a weight vector defines a single
value function that is well defined for every problem instance mdp  here we discuss the question of whether our framework can find a single feature weight vector combination that generalizes
good performance across problem sizes  i e   for the value function v defined by such combination 
whether greedy v   performs similarly well in different problem sizes 
throughout section    we have demonstrated the direct application of learned feature weight
vectors to target problem sizes   without retraining of weights these results are shown in the
target size lines in the result tables for each domain  in t etris  b locksworld  c onjunctive b oxworld  t ireworld  and l ifted  f ileworld    the target size lines demonstrate direct
successful generalization to target sizes even when the current problem sizes is significantly smaller 
 in the other domains  there was either no notion of problem size  s ys a dmin   or insufficient planning progress to significantly increase problem size when learning from small problems  e xplod ing b locksworld   z enotravel   and t owers of h anoi    
in this subsection  we consider the generalization from  larger  target sizes to selected intermediate sizes in these five domains  specifically  we take the weight vectors and feature vectors
resulting from the end of the trials  i e  with weight vector retrained at the target sizes   and apply
directly to selected intermediate problem sizes without weight retraining  for the trials that terminate before learning reaches the target problem sizes     we take the weights and features that result
in the best performing policy at the terminating problem sizes  the generalization results are shown
in figure     for comparison  that table also shows the performance on the same intermediate sized
problems of the value function that was learned directly at the that size  as well as the performance
of random walk on that size 
    note that one of the trials in t etris terminates before reaching target size due to non improving performance 
and the two trials in l ifted  f ileworld   terminate as target size performance already reaches optimality before
learning reaches the target size  still  although a few of the value functions were learned at smaller sizes than the
target size  all of the value functions evaluated for generalization were learned at significantly larger sizes than the
intermediate evaluation size 

   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

in each domain shown the random walk result is much weaker than the generalization result 
showing the presence of generalization of learned value functions across problem sizes  in the
four goal oriented planning domains  applying the value functions learned in the target sizes equals
the performance achieved by value functions learned directly in the intermediate sizes  with better
performance in c onjunctive  b oxworld   in t etris  however  the generalization result does
not match the result of learning in the intermediate size  we note that in some domains  solution
strategy is invariant with respect to the problem size  e g  destroying incorrect towers to form correct
ones in b locksworld   for some domains the best plan strategy may change dramatically with
size  for example  in t etris  a larger number of rows in the board allows strategies that temporary
stack uncompleted rows  but smaller number of rows favors strategies that complete rows as quickly
as possible  thus one should not necessarily expect generalization between domain sizes in every
domainthis conclusion can be expected to hold whether we are considering the generalization of
value functions or of policies 
we have included a discussion of policy based generalization in the related work section  appendix a     focusing on our previous work on approximate policy iteration  however  we note that
policies that generalize between problems of different sizes are no more or less well defined than
value functions which generalize between such problems  in our previous api work  we defined
policies that select actions for states of any domain size  in this work we define value functions that
assign numeric values to states of any domain size  none of this work guarantees finding a good
or optimal policy or value function  as far as we know  some problems admit good compact value
functions  some admit good compact policies  some admit both  and some neither 

   discussion and future research
we have presented a general framework for automatically learning state value functions by featurediscovery and gradient based weight training  in this framework  we greedily select features from
a provided hypothesis space  which is a parameter of the method  to best correlate with bellman
error features  and use avi to find weights to associate with these features 
we have proposed two different candidate hypothesis spaces for features  one of these two
spaces is a relational one where features are first order formulas with one free variable  and a beamsearch process is used to greedily select a hypothesis  the other hypothesis space we have considered is a propositional feature representation where features are decision trees  for this hypothesis
space  we use a standard classification algorithm c     quinlan        to build a feature that best
correlates with the sign of the statewise bellman error  instead of using both the sign and magnitude 
the performance of our feature learning planners is evaluated using both reward oriented and
goal oriented planning domains  we have demonstrated that our relational planner represents the
state of the art for feature discovering probabilistic planning techniques  our propositional planner
does not perform as well as our relational planner  and cannot generalize between problem instances 
suggesting that knowledge representation is indeed critical to the success of feature discovering
planners 
although we present results for a propositional feature learning approach and a relation featurelearning approach  the knowledge representation difference is not the only difference between the
approaches  historically  our propositional approach was originally conceived as a reduction to
classification learning  and so does not attempt to capture the magnitude of the bellman error during
   

fiw u   g ivan

feature selection  but rather focuses only the sign of the error  in contrast  our relational approach
counts objects in order to match the magnitude of the bellman error 
because of this difference  we cannot attribute all of the performance differences between the
approaches to the knowledge representation choice  some differences in performance could be due
to the choice to match sign only in the propositional feature selection  a possible future experiment
to identify the sources of performance variation would use a propositional representation involving
regression trees  dzeroski  todorovski    urbancic        to capture the magnitude of the error 
this representation might possibly perform somewhat better than the decision tree representation
shown here  but of course would still not enable the generalization between sizes that the relational
feature learner exhibits 
bellman error reduction is of course just one source of guidance that might be followed in
feature discovery  during our experiments in the ippc planning domains  we find that in many
domains the successful plan length achieved is much longer than optimal  as we discussed above in
section        a possible remedy other than deploying search as in our previous work  wu et al  
      is to learn features targeting the dynamics inside plateaus  and use these features in decisionmaking when plateaus are encountered 

acknowledgments
this material is based upon work supported in part by the national science foundation under grant
no          

appendix a  other related work
a   other feature selection approaches
a     f eature s election

via

c onstructive f unction a pproximation

automatic feature extraction in sequential decision making has been studied in the work of utgoff
and precup         via constructive function approximation  utgoff   precup         this work can
be viewed as a forerunner of our more general framework  limited to propositional representations 
binary valued features  and new features that are single literal extensions of old features by conjunction  also in the work of rivest and precup        a variant of cascade correlation  fahlman
  lebiere         a constructive neural network algorithm  is combined with td learning to learn
value functions in reinforcement learning  cascade correlation incrementally adds hidden units
to multi layered neural networks  where each hidden unit is essentially a feature built upon a set
of numerically valued basic features  our work provides a framework generalizing those prior efforts into a reduction to supervised learning  with explicit reliance on the bellman error signal  so
that any feature hypothesis space and corresponding learner can be deployed  in particular  we
demonstrate our framework on both binary propositional features using c    as the learner and rich
numeric valued relational features using a greedy beam search learner  our work provides the first
evaluation of automatic feature extraction in benchmark planning domains from the several planning
competitions 
while the work of utgoff and precup        implicitly relies on bellman error  there is no
explicit construction of a bellman error training set or discussion of selecting features to correlate
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

to bellman error  for instance  their work focuses first on refining a current feature for which weight
updates are converging poorly  high variance in weight updates   whereas our work focuses first on
finding a feature that correlates to statewise bellman error  regardless of whether that feature refines
any current feature  in addition  their work selects features online while the weights of the current
features are being adjusted  so there is no stationary target value function for which the bellman
error is considered in the selection of the next new feature  in contrast  our work separates weight
training and new feature selection completely   these differences are perhaps in part due to the
reinforcement learning setting used in utgoff   precup        as opposed to the planning setting of
our work  
the selection of hidden unit feature in cascade correlation  fahlman   lebiere        is based
on the covariance between feature values and errors of the output units  for output units that are
estimating a value function  with training data providing the bellman update of that value function 
the output unit error is just bellman error  thus  the hidden units learned in the work of rivest
and precup        are approximations of bellman error features just as our learned features are  although this is not made explicit in that work  by making the goal of capturing bellman error explicit
here  we provide a general reduction that facilitates the use of any learning method to capture the
resulting feature learning training sets  in particular  we are able to naturally demonstrate generalization across domain sizes in several large domains  using a relational feature learner  in contrast 
the single test domain in the work of rivest and precup        has a small fixed size  nonetheless 
that work is an important precursor to our approach 
a     f eature c onstruction

via

s pectral a nalysis

feature learning frameworks for value functions based upon spectral analysis of state space connectivity are presented in the work of mahadevan and maggioni        and petrik         in these
frameworks  features are eigenvectors of connectivity matrices constructed from random walk  mahadevan   maggioni        or eigenvectors of probabilistic transition matrices  petrik         such
features capture aspects of long term problem behaviours  as opposed to the short term behaviours
captured by the bellman error features  bellman error reduction requires iteration to capture longterm behaviors 
reward functions are not considered at all during feature construction in the work of mahadevan and maggioni         but in the work of petrik         reward functions are incorporated in the
learning of krylov basis features  an variant of our bellman error features  parr et al          to complement the eigenvector features  however  even in petriks framework  reward is only incorporated
in features used for policy evaluation rather than in the controlled environment we consider 
essential to our work here is the use of machine learning in factored representations to handle
very large statespaces and to generalize between problems of different sizes  both of these spectral
analysis frameworks are limited in this respect  at least at the current state of development   the approach by petrik        is presented only for explicit statespaces  while a factorization approach for
scaling up to large discrete domains is proposed in the work of mahadevan and maggioni         in
that approach  features are learned for each dimension in the factorization  independent of the other
dimensions  we believe the assumption of independence between the dimensions is inappropriate
in many domains  including the benchmark planning domains considered in our work  the mahadevan and maggioni factorization approach also suffers the same drawbacks as our propositional
approach  the solution has to be recomputed for problems of different sizes in the same domain and
   

fiw u   g ivan

so lacks the flexibility to generalize between problems of different sizes provided by our relational
approach 
a   structural model based and model free solution methods for markov decision
processes
a     r elational r einforcement l earning
in the work of dzeroski et al          a relational reinforcement learning  rrl  system learns
logical regression trees to represent q functions of target mdps  this work is related to ours since
both use relational representations and automatically construct functions that capture state value 
in addition to the q function trees  a policy tree learner is also introduced in the work of dzeroski
et al         that finds policy trees based on the q function trees  we do not learn an explicit policy
description and instead use only greedy policies for evaluation 
the logical expressions in rrl regression trees are used as decision points in computing the
value function  or policy  rather than as numerically valued features for linear combination  as in our
method  generalization across problem sizes is achieved by learning policy trees  the learned value
functions apply only to the training problem sizes  to date  the empirical results from this approach
have failed to demonstrate an ability to represent the value function usefully in familiar planning
benchmark domains  while good performance is shown for simplified goals such as placing a
particular block a onto a particular block b  the technique fails to capture the structure in richer
problems such as constructing particular arrangements of blocksworld towers  rrl has not been
entered into any of the international planning competitions  these difficulties representing complex
relational value functions persist in extensions to the original rrl work  driessens   dzeroski 
      driessens et al          where again only limited applicability is shown to benchmark planning
domains such as those used in our work 
a     p olicy l earning via b oosting
in the work of kersting and driessens         a boosting approach is introduced to incrementally
learn features to represent stochastic policies  this is a policy iteration variant of our featurelearning framework  and clearly differs from our work as policy representations are learned instead
of value function representations  using the regression tree learner tilde  blockeel   de raedt 
       the feature learner demonstrated advantages against previous rrl work in the task of accomplishing on a b  in a    block problem  applicability to a simple continuous domain  the corridor
world  is also demonstrated  as in the line of rrl work  only limited applicability to benchmark
planning domains is shown here  one probable source of this limited applicability is the model free
reinforcement learning setting where the system does not model the problem dynamics explicitly 
a     f itted value i teration
gordon        has presented a method of value iteration called fitted value iteration that is suitable
for very large state spaces but does not require direct feature selection  instead  the method relies on
a provided kernel function measuring similarity between states  selection of this kernel function can
be viewed as a kind of feature selection  as the kernel identifies which state aspects are significant
in measuring similarity  to our knowledge  techniques from this class have not been applied to
large relational planning problems like those evaluated in this paper  we do note that selection of
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

a single relational kernel for all domains would measure state similarity in a domain independent
manner and thus we believe such a kernel could not adapt to the individual domains the way our
work here does  thus we would expect inferior performance from such an approach  however  this
remains to be investigated  selection of domain specific kernels for stochastic planning domains 
automatically  is also yet to be explored 
a     e xact value i teration in f irst  order mdp s
previous work has used lifted techniques to exactly solve first order mdps by reformulating exact
solution techniques from explicit mdps  such as value iteration  boutilier et al         and holldobler and skvortsova        have independently used two different first order languages  situation
calculus and fluent calculus  respectively  to define first order mdps  in both works  the bellman update procedure in value iteration is reformulated using the respective calculus  resulting in
two first order dynamic programming methods  symbolic dynamic programming  sdp   and firstorder value iteration  fovi   only a simple boxworld example with human assisted computation is
demonstrated in the sdp work  but the method serves as a basis for foalp  sanner   boutilier 
       which replaces exact techniques with heuristic approximation in order to scale the techniques
to benchmark planning domains  application of fovi on planning domains is only demonstrated
on the colored blocksworld benchmark  and is limited to under    blocks  holldobler  karabaev   
skvortsova        
in the work of kersting et al          constraint logic programming is used to define a relational
value iteration method  mdp components  such as states  actions  and rewards  are first abstracted
to form a markov decision program  a lifted version of an mdp  a relational bellman operation
 rebel  is then used to define updates of q values and state values  empirical study of the rebel
approach has been limited to    step backups from single predicate goals in the blocksworld and
logistics domains 
exact techniques suffer from difficulty in representing the full complexity of the state value
function for arbitrary goals in even mildly complex domains  these previous works serve to illustrate the central motivation for using problem features to compactly approximate the structure of a
complex value function  and thus to motivate the automatic extraction of features as studied in this
work 
a   comparison to inductive logic programming algorithms
the problem of selecting a numeric function on relational states to match the bellman error training
set is a first order regression problem for which there are some available systems described in the
inductive logic programming  ilp  literature  quinlan        karalic   bratko        
it is important to note that most ilp work has studied the learning of classifiers on relational
data  muggleton         but here we are concerned with learning numeric functions on relational
data  such as our states   the latter problem is called first order regression within the ilp literature  and has received less study than relational classification  here  we choose to design our
own proof of concept relational learner for our experiments rather than use one of the few previous
systems  separate work is needed to compare the utility of this relational learner with previous
regression systems  our purpose here is to demonstrate the utility of bellman error training data
for finding decision theoretic value function features  our simple learner here suffices to create
state of the art domain independent planning via automatic feature selection 
   

fiw u   g ivan

ilp classification systems often proceed either from general to specific  or from specific to
general  in seeking a concept to match the training data  for regression  however  there is no such
easy ordering of the numeric functions to be searched  we design instead a method that searches
a basic logical expression language from simple expressions to more complex expressions  seeking
good matches to the training data  in order to control the branching factor  while still allowing more
complex expressions to be considered  we heuristically build long expressions out of only those
short expressions that score best  in other words  we use a beam search of the space of expressions 
there are several heuristic aspects to our method  first  we define a heuristic set of basic expressions from which our search begins  second  we define an heuristic method of combining
expressions to build more complex expressions  these two heuristic elements are designed so that
any logical formula without disjunction  with one free variable  can be built by repeated combination from the basic expressions  finally  the assumption that high scoring expressions will be built
only out of high scoring parts is heuristic  and often not true   this critical heuristic assumption
makes it likely that our learner will often miss complex features that match the training data well 
there is no known method that guarantees tractably finding such features 
a   approximate policy iteration for relational domains
our planners use greedy policies derived from learned value functions  alternatively  one can directly learn representations for policies  the policy tree learning in the work of dzeroski et al 
        discussed previously in appendix a      is one such example  recent work uses a relational decision list language to learn policies for small example problems that generalize well to
perform in large problems  khardon        martin   geffner        yoon et al          due to
the inductive nature of this line of work  however  the selected policies occasionally contain severe
flaws  and no mechanism is provided for policy improvement  such policy improvement is quite
challenging due to the astronomically large highly structured state spaces and the relational policy
language 
in the work of fern et al          an approximate version of policy iteration addressing these
issues is presented  starting from a base policy  approximate policy iteration iteratively generates
training data from an improved policy  using policy rollout  and then uses the learning algorithm in
the work of yoon et al         to capture the improved policy in the compact decision list language
again  similar to our work  the learner in the work of fern et al         aims to take a flawed
solution structure and improve its quality using conventional mdp techniques  in that case  finding
an improved policy with policy rollout  and machine learning  unlike our work  in the work of fern
et al         the improved policies are learned in the form of logical decision lists  our work can be
viewed as complementary to this previous work in exploring the structured representation of value
functions where that work explored the structured representation of policies  both approaches are
likely to be relevant and important to any long term effort to solve structured stochastic decisionmaking problems 
we note that feature based representation  as considered here and generally in the mdp literature  is used to represent value functions rather than policies  compact representation of policies
can be done via value functions  with greedy execution  or more directly  for example  using decision lists  the previous api work just discussed uses a direct representation for policies  and never
uses any compact representation of value functions  instead  sampling of value functions is used in
the policy evaluation step of policy iteration 
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

one can imagine a different and novel approach to api in which the compact feature based
representation is used for value functions  with greedy execution as the policy representation  in
that approach  feature discovery similar to what we explore here for value iteration could be designed to assist the policy evaluation phase of the policy iteration  we leave further development
and evaluation of that idea to future work  we expect the two approaches to api  as well as our
current approach to value iteration  to have advantages and disadvantages that vary with the domain
in ways that have yet to be well understood  some domains have natural compact direct policy
representations  run if you see a tarantula   whereas others are naturally compactly represented
via value functions  prefer restaurants with good review ratings   research in this area must
eventually develop means to combine these compact representations effectively 
a   automatic extraction of domain knowledge
there is a substantial literature on learning to plan using methods other than direct representation
of a value function or a reactive policy  especially in the deterministic planning literature  these
techniques are related to ours in that both acquire domain specific knowledge via planning experience in the domain  much of this literature targets control knowledge for particular search based
planners  estlin   mooney        kambhampati et al         veloso et al          and is distant
from our approach in its focus on the particular planning technology used and on the limitation to
deterministic domains  it is unclear how to generalize this work to value function construction or
probabilistic domains 
however  the broader learning to plan literature also contains work producing declarative
learned domain knowledge that could well be exploited during feature discovery for value function representation  in the work of fox and long         a pre processing module called tim is
able to infer useful domain specific and problem specific structures  such as typing of objects and
state invariants  from descriptions of domain definition and initial states  while these invariants are
targeted in that work to improving the planning efficiency of a graphplan based planner  we suggest
that future work could exploit these invariants in discovering features for value function representation  similarly  in the work of gerevini and schubert         discoplan infers state constraints
from the domain definition and initial state in order to improve the performance of sat based planners  again  these constraints could be incorporated in a feature search like our method but have not
to date 

appendix b  results and discussions for five probabilistic planning competition
domains
in section      we have presented the results of our relational and propositional feature learners for
b locksworld and c onjunctive  b oxworld  here we present the results of our relational
feature learner for the following five probabilistic planning competition domains  t ireworld 
z enotravel  e xploding b locksworld  t owers of h anoi  and l ifted  f ileworld   
b   tireworld
we use the t ireworld domain from the second ippc  the agent needs to drive a vehicle through
a graph from the start node to the goal node  when moving from one node to an adjacent node 
the vehicle has a certain chance of suffering a flat tire  while still arriving at the adjacent node  
   

fiw u   g ivan

trial   
  of features
 
 
 
 
 
 
 
 
 
problem difficulty
 
 
 
 
 
 
 
 
 
success ratio
                                            
plan length
 
 
 
 
 
 
 
 
 
accumulated time  hr                               
                                            
target size sr
target size slen 
 
 
 
 
 
 
 
 
 

 
  
    
 
  
    
 

trial   
  of features
 
 
 
 
 
 
 
 
 
problem difficulty
 
 
 
 
 
 
 
     
                                            
success ratio
plan length
 
 
 
 
 
 
 
 
 
accumulated time  hr                                
target size sr
                                            
target size slen 
 
 
 
 
 
 
 
 
 

 
  
    
 
  
    
 

 
  
    
 
  
    
 

 
  
    
 
  
    
 

figure     t ireworld performance  averaged over     problems  for relational learner  we add
one feature per column until success ratio exceeds      and average successful plan
length is less than  n  for n nodes  and then increase problem difficulty for the next
column  plan lengths shown are successful trials only  problem difficulties are measured
in number of nodes  with a target problem size of    nodes  some columns are omitted
as discussed in section     

the flat tire can be replaced by a spare tire  but only if there is such a spare tire present in the node
containing the vehicle  or if the vehicle is carrying a spare tire  the vehicle can pick up a spare
tire if it is not already carrying one and there is one present in the node containing the vehicle  the
default setting for the second ippc problem generator for this domain defines a problem distribution
that includes problems for which there is no policy achieving the goal with probability one  such
problems create a tradeoff between goal achievement probability and expected number of steps to
the goal  how strongly our planner favors goal achievement versus short trajectories to the goal is
determined by the choice of the discount factor made in section     
we start with   node problems in our relational learner and increase from n nodes to n    
nodes whenever the success ratio exceeds      and the average successful plan length is better than
 n steps  the target problem size is    nodes  the results are shown in figures    and    
in t ireworld  our relational learner again is able to find features that generalize well to large
problems  our learner achieves a success ratio of about     on    node problems  it is unknown
whether any policy can exceed this success ratio on this problem distribution  however  neither
comparison planner  foalp nor ff replan  finds a higher success rate policy 
we note that some improvements in success rate in this domain will necessarily be associated
with increases in plan length because success rate improvements may be due to path deviations to
acquire spare tires 
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

trial   
  of features
problem difficulty
success ratio
plan length
accumulated time  hr  
target size sr
target size slen 

 
     
    
   
    
    
   

 
     
   
   
   
    
    

 
     
    
   
   
    
    

 
     
    
   
   
    
    

 
     
    
   
  
    
    

 
     
    
   
  
    
    

 
     
    
   
  
    
    

 
     
    
   
  
    
    

 
     
    
   
  
    
    

 
     
    
   
  
    
    

 
     
    
   
  
    
    

trial   
  of features
problem difficulty
success ratio
plan length
accumulated time  hr  
target size sr
target size slen 

 
     
    
   
   
    
   

 
     
    
   
   
    
    

 
     
    
   
   
    
    

 
     
    
   
   
    
    

 
     
    
   
   
    
    

 
     
    
   
  
    
    

 
     
    
   
  
    
    

 
     
    
 
  
    
 

 
     
    
 
  
    
 

 
     
    
 
  
    
 

 
     
    
 
  
    
 

figure     z enotravel performance  averaged over     problems  for relational learner  the
problem difficulty shown in this table lists the numbers of cities  travelers  and aircraft 
with a target problem size of    cities    travelers  and   aircraft  we add one feature
per column until success ratio exceeds      and then increase problem difficulty for the
next column  plan lengths shown are successful trials only 

b   zenotravel
we use the z enotravel domain from the second ippc  the goal of this domain is to fly all travelers from their original location to their destination  planes have  finite range  discrete  fuel levels 
and need to be re fuelled when the fuel level reaches zero to cont inue flying  each available activity
 boarding  debarking  flying  zooming  or refueling  is divided into two stages  so that an activity
x is modelled as two actions start x and finish x  each finish x activity has a  high  probability
of doing nothing  once a start action is taken  the corresponding finish action must be taken
 repeatedly  until it succeeds before any conflicting action can be started  this structure allows the
failure rates on the finish actions to simulate action costs  which were not used explicitly in the
problem representation for the competition   a plane can be moved between locations by flying or
zooming  zooming uses more fuel than flying  but has a higher success probability 
we start with a problem difficulty of   cities    traveler  and   aircraft using our relational
feature learner  whenever the success ratio exceeds      we increase the number n of travelers and
aircraft by   if the number of cities is no less than  n     and increase the number of cities by one
otherwise  the target problem size is    cities    travelers  and   aircraft  z enotravel results for
the relational learner are shown in figures    and    
   

fiw u   g ivan

the relational learner is unable to find features that enable avi to achieve the threshold success
rate       for   cities    travelers  and   aircraft  although   relational features are learned  the trials
were stopped because no improvement in performance was achieved for several iterations of feature
addition  using a broader search  w        q      and d      we are able to find better features
and extend the solvable size to several cities with success rate      not shown here as all results in
this paper use the same search parameters  but reported in wu   givan         but the runtime also
increases dramatically  to weeks  we believe the speed and effectiveness of the relational learning
needs to be improved to excel in this domain  and a likely major factor is improved knowledge
representation for features so that key concepts for z enotravel are easily represented 
trial two in figure    shows a striking event where adding a single new feature to a useful value
function results in a value function for which the greedy policy cannot find the goal at all  so that
the success ratio degrades dramatically immediately  note that in this small problem size  about
ten percent of the problems are trivial  in that the initial state satisfies the goal  after the addition
of the sixth feature in trial two  these are the only problems the policy can solve  this reflects the
unreliability of our avi weight selection technique more than any aspect of our feature discovery 
after all  avi is free to assign a zero weight to this new feature  but does not  additional study of
the control of avi and or replacement of avi by linear programming methods is indicated by this
phenomenon  however  this is a rare event in our extensive experiments 
b   exploding blocksworld
we also use e xploding b locksworld from the second ippc to evaluate our relational planner 
this domain differs from the normal blocksworld largely due to the blocks having certain probability of being detonated when they are being put down  destroying objects beneath  but not the
detonating block   blocks that are already detonated once will not be detonated again  the goal
state in this domain is described in tower fragments  where the fragments are not generally required
to be on the table  destroyed objects cannot be picked up  and blocks cannot be put down on destroyed objects  but a destroyed object can still be part of the goal if the necessary relationships
were established before or just as it was destroyed  
we start with   block problems using our relational learner and increase from n blocks to n    
blocks whenever the success ratio exceeds      the target problem sizes are   and    blocks 
e xploding b locksworld results for the relational learner are shown in figures    and    
the results in e xploding b locksworld are not good enough for the planner to increase the
difficulty beyond   block problems  and while the results show limited generalization to   block
problems  there is very little generalization to    block problems 
our performance in this domain is quite weak  we believe this is due to the presence of many
dead end states that are reachable with high probability  these are the states where either the table
or one of the blocks needed in the goal has been destroyed  before the object in question achieved the
required properties  our planner can find meaningful and relevant features  the planner discovers
that it is undesirable to destroy the table  for instance  however  the resulting partial understanding of the domain cannot be augmented by random walk  as it is in some other domains such as
b locksworld and c onjunctive  b oxworld  to enable steady improvement in value  leading to the goal  random walk in this domain invariably lands the agent in a dead end  very short
successful plan length  low probability of reaching the goal  and  not shown here  very high unsuccessful plan length  caused by wandering in a dead end region  suggest the need for new techniques
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

trial   
  of features
 
 
 
 
problem difficulty
 
 
 
 
success ratio
                   
plan length
 
 
 
 
accumulated time  hr                  
                   
target size    sr
target size    slen 
 
 
 
 
target size    sr
 
 
      
target size    slen 


   

 
 
    
 
   
    
 
    
 

trial   
  of features
 
 
 
 
 
problem difficulty
 
 
 
 
 
success ratio
                        
plan length
 
 
 
 
 
accumulated time  hr                      
target size    sr
                        
 
 
 
 
 
target size    slen 
target size    sr
 
 
           
target size    slen 


      

 
 
 
 
 
 
 
 
 
 
 
 
                            
 
 
 
 
 
 
                   
                             
 
 
 
 
 
 
                          
               

 
 
    
 
   
    
 
    
  

 
 
    
 
   
    
 
    
  

 
 
    
 
  
    
 
    
  

 
 
    
 
  
    
 
    
  

 
 
    
 
  
    
 
    
  

  
 
    
 
  
    
 
    
  

 
 
    
 
  
    
 
    
  

figure     e xploding b locksworld performance  averaged over     problems  for relational
learner  problem difficulties are measured in number of blocks  we add one feature per
column until success ratio exceeds      and then increase problem difficulty for the next
column  plan lengths shown are successful trials only  target problem size    has  
blocks  and target problem size    has    blocks 

aimed at handling dead end regions to handle this domain  these results demonstrate that our technique relies on random walk  or some other form of search  so that the learned features need not
completely describe the desired policy 
b   towers of hanoi
we use the domain t owers of h anoi from the first ippc  in this probabilistic version of the wellknown problem  the agent can move one or two discs simultaneously  but there is a small probability
of going to a dead end state on each move  and this probability depends on whether the largest disc
has been moved and which type of disc move  one or two at a time  is being used  we note that
there is only one planning problem in each problem size here 
it is important to note that      success rate is generally unachievable in this domain due to
the unavoidable dead end states 
   

fiw u   g ivan

trial   
  of features
problem difficulty
success ratio
plan length
accumulated time  hr  
target size    sr
target size    slen 
target size    sr
target size    slen 

 
 
 
 
 
 
 
 
 
 
 
       
 
 
 
 
 
 
 
 
 
 
 
 
 
 
                          
 
 
 
        
 
 
 
 
  
  
 





 



                                                     
                          
 
 
 
                  
  
 
  
  
  





 
 

 
      
 
        
 
 
 
 
 
 
 
 
  



    









trial   
  of features
problem difficulty
success ratio
plan length
accumulated time  hr  
target size    sr
target size    slen 
target size    sr
target size    slen 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
                          
 
 
 
      
 
  
  
  
 





 
                                           
                        
 
 
 
      
  
         
  





 
        
 
        
 
 
 
 
 
  
  


    






 
 
 

   
 

 


  
 
 

 
 

 


  
 
 

  
 

 


figure     t owers of h anoi performance  averaged over     problems  for relational learner 
we add one feature per column until success ratio exceeds    n  for n discs  and then
increase problem difficulty for the next column  plan lengths shown are successful trials
only  problem difficulties are measured in number of discs  with a target problem size   
of   discs and size    of   discs  some columns are omitted as discussed in section     

we start with the   disc problem in our relational learner and increase the problem difficulty
from n discs to n     discs whenever the success ratio exceeds    n    the target problem sizes are
  and   discs  t owers of h anoi results for the relational learner are shown in figures    and    
the learner is clearly able to adapt to three  and four disc problems  achieving around    
success rate on the four disc problem in both trials  the optimal solution for the four disc problem
has success rate      this policy uses single disc moves until the large disc is moved and then
uses double disc moves  policies that use only single disc moves or only double disc moves can
achieve success rates of     and      respectively  on the four disc problem  the learned solution
occasionally moves a disc in a way that does not get closer to the goal  reducing its success 
unfortunately  the trials show that an increasing number of new features are needed to adapt
to each larger problem size  and in our trials even    total features are not enough to adapt to the
five disc problem  thus  we do not know if this approach can extend even to five discs  moreover 
the results indicate poor generalization between problem sizes 
we believe it is difficult for our learner  and for humans  to represent a good value function
across problem sizes  humans deal with this domain by formulating a good recursive policy  not by
establishing any direct idea of the value of a state  finding such a recursive policy automatically is
an interesting open research question outside the scope of this paper 
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

b   lifted fileworld 
as described in section      we use the domain l ifted  f ileworld    which is a straightforwardly
lifted form of f ileworld from the first ippc  restricted to three folders  to reach the goal of filing
all files  an action needs to be taken for each file to randomly determine which folder that file should
go into  there are actions for taking out a folder  putting a file in that folder  and returning the folder
to the cabinet  the goal is reached when all files are correctly filed in the targeted folders 
we note that both f ileworld and l ifted  f ileworld   are very benign domains  there
are no reachable dead ends and very few non optimal actions  each of which is directly reversible 
random walk solves this domain with success rate one even for thirty files  the technical challenge
posed then is to minimize unnecessary steps so as to minimize plan length  the optimal policy
solves the n file problem with between  n     and  n     steps  depending on the random file types
generated 
rather than preset a plan length threshold for increasing difficulty  as a function of n   here we
adopt a policy of increasing difficulty whenever the method fails to improve plan length by adding
features  specifically  if the success ratio exceeds     and one feature is added without improving
plan length  we remove that feature and increase problem difficulty instead   
we start with   file problems in our relational learner and increase from n files to n     files
whenever the performance does not improve upon feature addition  the target problem size is   
files  l ifted  f ileworld   results for the relational learner are shown in figures    and    
the results show that our planner acquires an optimal policy for the    file target size problem
after learning four features  in each of the two trials  the results in this domain again reveal the
weakness of our avi weight selection method  although four features are enough to define an optimal policy  as problem difficulty increases  avi often fails to find the weight assignment producing
such a policy  when this happens  further feature addition can be triggered  as in trial    in this
domain  the results show that such extra features do not prevent avi from finding good weights on
subsequent iterations  as the optimal policy is recovered again with the larger feature set  nonetheless  here is another indication that improved performance may be available via work on alternative
weight selection approaches  orthogonal to the topic of feature selection 

references
bacchus  f     kabanza  f          using temporal logics to express search control knowledge for
planning  artificial intelligence              
bertsekas  d  p          dynamic programming and optimal control  athena scientific 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
blockeel  h     de raedt  l          top down induction of first order logical decision trees 
artificial intelligence              
bonet  b     givan  r          non deterministic planning track of the      international planning
competition  website  http   www ldc usb ve  bonet ipc   
    it is possible to specify a plan length threshold function for triggering increase in difficulty in this domain  as we
have done in other domains  we find that this domain is quite sensitive to the choice of that function  and in the end
it must be chosen to trigger difficulty increase only when further feature addition is fruitless at the current difficulty 
so  we have directly implemented that automatic method for triggering difficulty increase 

   

fiw u   g ivan

trial   
  of features
     
     
problem difficulty
success ratio
     
      
plan length
accumulated time  hr              
target size sr
     
target size slen 
          

 
 
 
 
   
 


 
 
 
 
   
 


 
 
 
 
   
 


 
 
 
 
   
 


 
 
 
  
   
    
  

 
  
 
  
   
 
  

 
  
 
  
   
 
  

 
  
 
  
   
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
   

 
  
 
  
  
 
  

trial   
  of features
     
problem difficulty
     
success ratio
     
      
plan length
accumulated time  hr              
     
target size sr
target size slen 
          

 
 
 
 
   
 


 
 
 
 
   
 


 
 
 
 
   
 


 
 
 
 
   
 


 
       
 
        
 
       
              
                   
            
              

 
  
 
  
   
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
   

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
 
 
  
   
    
  

 
  
 
  
   
 
  

 
  
 
  
   
 
  

figure     l ifted  f ileworld   performance  averaged over     problems  for relational
learner  we add one feature per column until success ratio exceeds     and adding one
extra feature does not improve plan length  and then increase problem difficulty for the
next column  after removing the extra feature   plan lengths shown are successful trials
only  problem difficulties are measured in number of files  with a target problem size of
   files  some columns are omitted as discussed in section     

boutilier  c   reiter  r     price  b          symbolic dynamic programming for first order mdps 
in proceedings of the seventeenth international joint conference on artificial intelligence 
pp         
chandra  a     merlin  p          optimal implementation of conjunctive queries in relational data
bases  in proceedings of the ninth annual acm symposium on theory of computing  pp 
     
davis  r     lenat  d          knowledge based systems in artificial intelligence  mcgraw hill 
new york 
driessens  k     dzeroski  s          integrating guidance into relational reinforcement learning 
machine learning             
driessens  k   ramon  j     gartner  t          graph kernels and gaussian processes for relational
reinforcement learning  machine learning            
dzeroski  s   deraedt  l     driessens  k          relational reinforcement learning  machine
learning          
dzeroski  s   todorovski  l     urbancic  t          handling real numbers in ilp  a step towards
better behavioural clones  in proceedings of the eighth european conference on machine
learning  pp         
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

estlin  t  a     mooney  r  j          learning to improve both efficiency and quality of planning 
in proceedings of the fifteenth international joint conference on artificial intelligence  pp 
         
fahlman  s     lebiere  c          the cascade correlation learning architecture  in advances in
neural information processing systems    pp           
farias  v  f     van roy  b          tetris  a study of randomized constraint sampling  in probabilistic and randomized methods for design under uncertainty  springer verlag 
fawcett  t          knowledge based feature discovery for evaluation functions  computational
intelligence              
fern  a   yoon  s     givan  r          learning domain specific control knowledge from random
walks  in proceedings of the fourteenth international conference on automated planning
and scheduling  pp         
fern  a   yoon  s     givan  r          approximate policy iteration with a policy language bias 
solving relational markov decision processes  journal of artificial intelligence research     
      
fox  m     long  d          the automatic inference of state invariants in tim  journal of artificial
intelligence research            
gerevini  a     schubert  l          inferring state constraints for domain independent planning 
in proceedings of the fifteenth national conference on artificial intelligence  pp         
gordon  g          stable function approximation in dynamic programming  in proceedings of the
twelfth international conference on machine learning  pp         
gretton  c     thiebaux  s          exploiting first order regression in inductive policy selection 
in proceedings of the twentieth conference on uncertainty in artificial intelligence  pp     
    
guestrin  c   koller  d     parr  r          max norm projections for factored mdps  in proceedings of the seventeenth international joint conference on artificial intelligence  pp         
holldobler  s   karabaev  e     skvortsova  o          flucap  a heuristic search planner for
first order mdps  journal of artificial intelligence research             
holldobler  s     skvortsova  o          a logic based approach to dynamic programming  in
proceedings of the workshop on learning and planning in markov processesadvances
and challenges at the nineteenth national conference on artificial intelligence  pp       
kakade  s          a natural policy gradient  in advances in neural information processing
systems     pp           
kambhampati  s   katukam  s     qu  y          failure driven dynamic search control for partial
order planners  an explanation based approach  artificial intelligence                  
karalic  a     bratko  i          first order regression  machine learning             
keller  p   mannor  s     precup  d          automatic basis function construction for approximate dynamic programming and reinforcement learning  in proceedings of the twenty third
international conference on machine learning  pp         
   

fiw u   g ivan

kersting  k   van otterlo  m     de raedt  l          bellman goes relational  in proceedings of
the twenty first international conference on machine learning  pp         
kersting  k     driessens  k          non parametric policy gradients  a unified treatment of
propositional and relational domains  in proceedings of the twenty fifth international conference on machine learning  pp         
khardon  r          learning action strategies for planning domains  artificial intelligence                  
lagoudakis  m  g   parr  r     littman  m  l          least squares methods in reinforcement
learning for control  in setn     proceedings of the second hellenic conference on ai  pp 
       
mahadevan  s     maggioni  m          proto value functions  a laplacian framework for learning representation and control in markov decision processes  journal of machine learning
research              
martin  m     geffner  h          learning generalized policies from planning examples using
concept languages  applied intelligence          
mitchell  t  m          machine learning  mcgraw hill 
muggleton  s          inductive logic programming  new generation computing               
parr  r   li  l   taylor  g   painter wakefield  c     littman  m          an analysis of linear
models  linear value function approximation  and feature selection for reinforcement learning 
in proceedings of the twenty fifth international conference on machine learning  pp     
    
parr  r   painter wakefield  c   li  l     littman  m          analyzing feature generation for
value function approximation  in proceedings of the twenty fourth international conference
on machine learning  pp         
patrascu  r   poupart  p   schuurmans  d   boutilier  c     guestrin  c          greedy linear valueapproximation for factored markov decision processes  in proceedings of the eighteenth
national conference on artificial intelligence  pp         
petrik  m          an analysis of laplacian methods for value function approximation in mdps 
in proceedings of the twentith international joint conference on artificial intelligence  pp 
         
quinlan  j  r          c     programs for machine learning  morgan kaufmann 
quinlan  j  r          learning first order definitions of functions  journal of artificial intelligence
research            
rivest  f     precup  d          combining td learning with cascade correlation networks  in
proceedings of the twentieth international conference on machine learning  pp         
sanner  s     boutilier  c          practical linear value approximation techniques for first order
mdps  in proceedings of the twenty second conference on uncertainty in artificial intelligence  pp         
sanner  s     boutilier  c          practical solution techniques for first order mdps  artificial
intelligence                   
   

fiautomatic i nduction of b ellman  e rror f eatures for p robabilistic p lanning

singh  s   jaakkola  t   littman  m     szepesvari  c          convergence results for single step
on policy reinforcement learning algorithms  machine learning                
sutton  r  s          learning to predict by the methods of temporal differences  machine learning 
       
sutton  r  s     barto  a  g          reinforcement learning  an introduction  mit press 
szita  i     lorincz  a          learning tetris using the noisy cross entropy method  neural
computation               
tesauro  g          temporal difference learning and td gammon  communications of the acm 
            
tsitsiklis  j     roy  b  v          an analysis of temporal difference learning with function approximation  ieee transactions on automatic control                
utgoff  p  e     precup  d          relative value function approximation  tech  rep   university
of massachusetts  department of computer science 
utgoff  p  e     precup  d          constuctive function approximation  in motoda    liu  eds   
feature extraction  construction  and selection  a data mining perspective  pp         
kluwer 
veloso  m   carbonell  j   perez  a   borrajo  d   fink  e     blythe  j          integrating planning
and learning  the prodigy architecture  journal of experimental and theoretical ai       
      
widrow  b     hoff  jr  m  e          adaptive switching circuits  ire wescon convention
record        
williams  r  j     baird  l  c          tight performance bounds on greedy policies based on
imperfect value functions  tech  rep   northeastern university 
wu  j     givan  r          discovering relational domain features for probabilistic planning 
in proceedings of the seventeenth international conference on automated planning and
scheduling  pp         
wu  j   kalyanam  r     givan  r          stochastic enforced hill climbing  in proceedings of the
eighteenth international conference on automated planning and scheduling  pp         
wu  j     givan  r          feature discovering approximate value iteration methods  in proceedings of the symposium on abstraction  reformulation  and approximation  pp         
yoon  s   fern  a     givan  r          inductive policy selection for first order mdps  in proceedings of the eighteenth conference on uncertainty in artificial intelligence  pp         
yoon  s   fern  a     givan  r          ff replan  a baseline for probabilistic planning  in proceedings of the seventeenth international conference on automated planning and scheduling  pp         
younes  h   littman  m   weissman  d     asmuth  j          the first probabilistic track of the
international planning competition  journal of artificial intelligence research             

   

fi