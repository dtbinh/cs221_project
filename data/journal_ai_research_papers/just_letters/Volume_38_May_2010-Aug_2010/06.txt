journal of artificial intelligence research                  

submitted        published      

cause identification from aviation safety incident reports
via weakly supervised semantic lexicon construction
muhammad arshad ul abedin
vincent ng
latifur khan

arshad student utdallas edu
vince hlt utdallas edu
lkhan utdallas edu

department of computer science
erik jonsson school of engineering   computer science
the university of texas at dallas
    w  campbell road  ms ec  
richardson  tx       u s a 

abstract
the aviation safety reporting system collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents  to effectively reduce these incidents  it is vital to accurately identify why these incidents occurred 
more precisely  given a set of possible causes  or shaping factors  this task of cause identification involves identifying all and only those shaping factors that are responsible for
the incidents described in a report  we investigate two approaches to cause identification 
both approaches exploit information provided by a semantic lexicon  which is automatically constructed via thelen and riloffs basilisk framework augmented with our linguistic
and algorithmic modifications  the first approach labels a report using a simple heuristic  which looks for the words and phrases acquired during the semantic lexicon learning
process in the report  the second approach recasts cause identification as a text classification problem  employing supervised and transductive text classification algorithms to
learn models from incident reports labeled with shaping factors and using the models to
label unseen reports  our experiments show that both the heuristic based approach and
the learning based approach  when given sufficient training data  outperform the baseline
system significantly 

   introduction
safety is of paramount importance when it comes to the aviation industry  in      alone 
there were      incidents    including    fatal accidents with     casualties    to improve the
aviation safety situation  the aviation safety reporting system  asrs  was established in
     to make safety incident data available to researchers  asrs collects voluntarily submitted reports about aviation safety incidents written by flight crews  attendants  controllers
and other related parties  the reports contain a number of fixed fields and a free text narrative describing the incident  however  the data has grown to be quite large over the years
and it is getting increasingly difficult  if not impossible  to analyze these reports by human
means  it has become necessary that these reports be analyzed through automated means 
   http   asrs arc nasa gov 
   http   www flightsafety gov 
c
    
ai access foundation  all rights reserved 

fiabedin  ng   khan

to take full advantage of this data to reduce safety incidents  it is necessary to extract
from the reports both what happened and why  once both are known  then it is possible to
identify the correlations between the incidents and their causes  and take fruitful measures
toward eliminating the causes  however  the fixed fields in the reports are devoted to various
aspects of what happened during the incidents  and there is no fixed field that indicates
the incidents causes  instead  the reporter discusses in the report narrative what he thinks
caused the incident  along with the incident description  thus the cause of the incident has
to be extracted by analyzing the free text narrative  as an example  a report is shown next
to illustrate the task 
report         while descending into lit we encountered instrument meteorological conditions  rime ice  rain  and moderate chop  as i turned to
a heading with the auto pilot direct lit the attitude indicator remained in a
bank  xchking  i noticed the radio magnetic indicators were    degree off
headings  i switched to    and corrected the course  the auto pilot and flight
director were kicked off  i continued to have problems with the altitude select
and auto pilot as i attempted to re engage it  it was during radar vectors to
the approach and descent to      feet that we noticed our altitude at      feet
mean sea level  we stopped the descent and climbed to      feet mean sea
level  air traffic control noted our altitude deviation at the time we noticed 
we were thankful for their backup during a time of flight director problems in
our cockpit  this occurred at the end of a    hour crew day  bad weather  instrument problems  and lack of crew rest  the first officer  pilot not flying 
in the right seat  had only   hours of rest due to inability to go to sleep the
night before  we were tired from a trip lit orl lit  we had not eaten in about
  hours  
posse et al         identify    most important cause types  or shaping factors  that can
influence the occurrence of the aviation safety incident described in an asrs report  these
shaping factors are the contextual factors that influenced the reporters behavior in the
incident and thus contributed to the occurrence of the incident  some of these factors can
be attributed to humans  e g   a pilot or a flight attendant has psychological pressure  an
overly heavy taskload  or an unprofessional attitude that impacts his performance   while
some are related to the surrounding environment  e g   physical environment such as snow 
and communication environment such as auditory interference   a detailed description of
these    shaping factors can be found in section     
in the above report  we find that the incident was influenced by three shaping factors 
namely physical environment  which concerns bad weather  as mentioned above   resource
deficiency  which concerns problems with the equipment   and duty cycle  which refers
to physical exhaustion due to long hours of duty without adequate rest or replenishment  
these three shaping factors are indicated by different words and phrases in the report  for
instance  the bad weather condition is expressed using phrases such as rime ice  rain and
moderate chop  while the details of the equipment problem appear as sentence fragments like
   to improve readability  the report has been preprocessed from its original form using the steps described
in section     

   

ficause identification via weakly supervised semantic lexicon construction

attitude indicator remained in a bank     degree off headings and flight director problems 
the issue with the long hours of duty is illustrated by the sentence fragments like    hour
crew day and tired from a trip  the goal of our cause identification task for the aviation
safety domain  then  is to identify which of the    shaping factors contributed to the incident
described in a report using the lexical cues appearing in the report narrative 
however  as mentioned earlier  the sheer volume of the data makes it prohibitive to
analyze all the reports manually and identify the associated shaping factors  thus  the
focus of our research is automated cause identification from the asrs reports  which involves
automatically analyzing the report narrative and identifying the responsible shaping factors 
this brings our problem into the domain of natural language processing  nlp  
since we have a set of texts  i e   the report narratives  and a set of possible labels for
these texts  i e   the shaping factors   this task is most naturally cast as a text classification
task  however  unlike topic based text classification  cause based text classification has not
been addressed extensively in the nlp community  previous work on causal analysis is quite
different in nature from our cause based text classification task  more specifically  previous
cause analysis works do not involve text classification  focusing instead on determining
the existence of a causal relation between two sentences or events  for instance  there has
been some work on causal analysis for question answering  where a question may involve the
cause s  of an event  e g   kaplan   berry rogghe        garcia        khoo  chan    niu 
      girju         here  the focus is on finding causal relationship between two sentence
components  as another example  causal analysis on equipment malfunction reports have
been attempted by grishman and ksiezyk         whose work is restricted to the analysis
of reports related to one specific piece of equipment they studied  they analyze cause effect
relations between events leading to the malfunction described in the reports 
cause identification from aviation safety reports is a rather challenging problem  as a
result of a number of factors specific to the asrs dataset  first  unlike many nlp problems
where the underlying corpus is composed of a set of well edited texts such as newspaper
reports  reviews  legal and medical documents    the asrs reports are mostly written in
informal manner  and since they have not been edited except for removing author identity
information  the reports tend to contain spelling and grammatical mistakes  second  they
employ a large amount of domain specific acronyms  abbreviations and terminology  third 
the incident described in a report may have been caused by more than one shaping factor 
thus reports can have multiple shaping factor labels  making the task more challenging
than binary classification  or even multi class problems where each instance has only one
label  above all  the scarcity of labeled data for this task  coupled with highly imbalanced
class distributions  makes it difficult to acquire an accurate classifier via supervised learning 
previous work on cause identification for the asrs reports was done primarily by the
researchers at nasa  see posse et al         and  to our knowledge  has involved manual
analysis of the reports  specifically  nasa brought together experts on aviation safety 
human factors  linguistics and english language to participate in a series of brainstorming
sessions  and generated a collection of seed keywords  simple expressions and template
expressions related to each shaping factor  then they labeled the reports with the shaping
factors by looking for the related expressions in the report narrative  however  there is a
   recently  work has started on processing blogs  which may not be so grammatical either  but blogs
typically are not full of domain specific terminology 

   

fiabedin  ng   khan

major weakness associated with this approach  it involves a large amount of human effort
on identifying the relevant keywords and expressions  and yet the resulting list of keywords
and expressions is by no means exhaustive  moreover  they evaluated their approach on
only    manually labeled reports  such a small scale evaluation is by no means satisfactory
as judged by current standard in nlp research  one of our contributions in this research
is the annotation of      asrs reports with shaping factors  which serve as a standard
evaluation dataset against which different cause identification methods can be compared 
in this paper  we investigate two alternative approaches to cause identification  both
of which exploit information provided by an automatically constructed semantic lexicon 
more specifically  in view of the large amount of human involvement in nasas work  we
aim to replace the manual selection of seed words with a bootstrapping approach that
automatically constructs a semantic lexicon  specifically  motivated by thelen and riloffs
       basilisk framework  we learn a semantic lexicon  which consists of a set of words and
phrases semantically related to each of the shaping factors  as follows  starting from a small
set of seed words and phrases  we augment these seeds in each iteration by automatically
finding a fixed number of words and phrases related to the seeds from the corpus and adding
them to the seed list  most importantly  however  we propose four modifications to the
basilisk framework that can potentially improve the quality of the generated lexicon  the
first is a linguistic modification  in addition to using parse based features  e g   subjectverb and verb object features  as in basilisk  we employ features that can be computed
more robustly  e g   n grams   the remaining three are all algorithmic modifications to the
basilisk framework  involving     the use of a probabilistic semantic similarity measure     
the use of a common word pool  and     the enforcement of minimum support and maximum
generality constraints for words and their extraction patterns  which favors the addition of
frequently occurring content bearing words and disfavors overly general extraction patterns 
as mentioned above  we investigate two approaches to cause identification that exploit
the automatically learned semantic lexicon  the first approach is a heuristic approach 
which  motivated by posse et al          labels a report with a shaping factor if it contains
at least a word or a phrase that is relevant to the shaping factor  unlike posse et al s
work  where these relevant words and phrases employed by the heuristic procedure are
all manually identified  we automatically acquire these words and phrases via the semisupervised semantic lexicon learning procedure described above  the second approach is
a machine learning approach that is somewhat orthogonal to nasas approach  instead
of having a human identify seed words and phrases relevant to each shaping factor  we
have humans annotate a small subset of the available incident reports with their shaping
factors  and then apply a machine learning algorithm to train a classifier to automatically
label an unseen report  using combinations of n gram features and words and phrases
automatically acquired by the aforementioned semantic lexicon learning procedure  as we
will see  we acquire this cause identifier using support vector machines  svms   which have
been shown to be effective for topic based text classification  since we only have a small
number of labeled reports  we also attempt to combine labeled and unlabeled reports using
the transductive version of svms 
since our approaches rely on simple linguistic knowledge sources that involve n grams
and words and phrases automatically acquired during the semantic lexicon learning procedure  one may argue that the use of these simple features are not sufficient for cause
   

ficause identification via weakly supervised semantic lexicon construction

identification  it is important to point out that we are by no means arguing that these
features are sufficient for cause identification  however  the use of these simple features is
relevant for the task and is motivated by the work performed by the nasa researchers 
who  as mentioned above  have manually identified seed words and phrases for each shaping
factor  posse et al          our semantic lexicon learning procedure precisely aims to learn
such words and phrases  while our error analysis reveals that these simple linguistic features
are not sufficient for learning cause identification  and that more sophisticated knowledge
sources are needed to improve performance   as one of the first attempts to tackle this cause
identification task  we believe that the use of these simple features is a good starting point
and establishes a baseline against which future studies on this domain specific problem can
be compared 
we evaluate the aforementioned two approaches on our manually annotated asrs reports  our experiments show a number of interesting results  first  the best performance is
achieved using the heuristic approach  where we label a report on the basis of the presence of
the automatically acquired lexicon words and phrases in the report  achieving an f measure
of         more importantly  this method significantly surpasses the performance of our
baseline system  which labels a report on the basis of the presence of a small set of manually
identified seed words and phrases  these results suggest that employing an automatically
acquired semantic lexicon is relevant and useful for cause based text classification of the
asrs reports  second  the words and phrases in the learned semantic lexicon  when used
as features for training svms in the classification approach  do not improve the performance
of an svm classifier that is trained solely on n gram based features when the amount of
training data is small  however  when we increase the amount of training data  by crossvalidation   using the lexicon words and phrases as features in addition to unigrams and
bigrams helps improve classifier performance statistically significantly  in particular  we
have observed an f measure of        from the svm classifiers using a combination of
unigrams  bigrams and lexicon words and phrases as features  these results again confirm
that the words and phrases from the learned semantic lexicon are relevant and valuable
features for identifying the responsible shaping factors  nevertheless  the magnitude of
the improvement indicates that there is still much room for improvement  which may be
achieved by using deeper semantic features 
in summary  we believe that our work on automated cause identification makes five
primary contributions 
 we show that instead of manually analyzing all the incident reports to identify the
relevant shaping factors  it is possible to reduce the amount of human effort required
for this task by manually analyzing only a small subset of the reports and identifying
the shaping factors of the rest of the reports by using automated methods 
 we propose several modifications to thelen and riloffs        semi supervised lexicon learning framework  and show that our modified basilisk framework allows us
to acquire a semantic lexicon that yields significantly better performance for cause
identification than the original basilisk framework  equally importantly  none of
our modifications are geared towards the cause identification task  and hence they
are applicable more generally to the semantic lexicon learning task  in fact  our addi   

fiabedin  ng   khan

tional experiments suggest that modified basilisk yields better accuracy than original
basilisk when bootstrapping general semantic categories 
 we show that semantic lexicon learning is useful for cause identification from the asrs
reports  in particular  the words and phrases from the learned semantic lexicon can
be profitably used to improve both a heuristic based approach and a learning based
approach  when given sufficient training data  to cause identification  in addition  we
believe that in any similar cause identification task where the causes are described
in the text  it may be useful to learn a semantic lexicon containing key words and
phrases related to the different types of possible causes and use these key words and
phrases as features for machine learning 
 in an attempt to deduce the weaknesses of our approaches and help direct future
research  we have performed an analysis of the errors made by the best performing
system  namely the heuristic approach using the semantic lexicon learned by our
modified basilisk method on a randomly chosen subset of the test reports 
 we have manually annotated a subset of the reports with the relevant shaping factors 
this set of annotated reports  which have been made publicly available  can serve as
a standard evaluation set for this task in future research and also for comparing to
other approaches to cause identification 
the rest of the paper is organized as follows  in section    we discuss the dataset  the
shaping factors  and how the reports were preprocessed and annotated  section   defines
the baseline  which simply looks for a small set of manually extracted seed words and
phrases in the report narratives  in section    we describe our semantic lexicon learning
procedure  which is based on the basilisk lexicon learning procedure  thelen   riloff 
      augmented with our modifications  in section    we discuss our heuristic based and
learning based approaches to cause identification  we evaluate these two approaches in
section   and discuss related work in section    finally  in section    we summarize our
conclusions and discuss future work 

   dataset
the dataset used in this research is the aviation safety incident reports publicly available
from the website of aviation safety reporting system    we used all         reports collected during the period from january      to december       each report contains a
free text narrative written by the reporter and several fixed fields about the incident like
the time and place of the incident  environment information  details about the aircrafts
involved  the reporting persons credentials  details like anomaly  detector  resolution and
consequence about the incident itself  and a description of the situation  in other words 
the fixed fields in a report contain various information about what happened  and under
what physical circumstances  but do not cover why the incident took place  as discussed
by posse et al         and ferryman  posse  rosenthal  srivastava  and statler         only
the narrative of a report contains information on the shaping factors of the incident  for
   available at http   asrs arc nasa gov search database html

   

ficause identification via weakly supervised semantic lexicon construction

this reason  we decided to analyze only the free text narrative of a report using nlp techniques to identify what the shaping factor s  of the incident may be  and we constructed
the corpus for this task by combining the narratives of these         reports 
    shaping factors
the incidents described in the asrs reports happen for a variety of reasons  posse et al 
       focus on the    shaping factors  or simply shapers  following is a short description
of these shaping factors  taken verbatim from the work of posse et al  
   attitude  any indication of unprofessional or antagonistic attitude by a controller
or flight crew member 
   communication environment  interferences with communications in the cockpit
such as noise  auditory interference  radio frequency congestion  or language barrier 
   duty cycle  a strong indication of an unusual working period e g   a long day  flying
very late at night  exceeding duty time regulations  having short and inadequate rest
periods 
   familiarity  any indication of a lack of factual knowledge  such as new to or unfamiliar with company  airport  or aircraft 
   illusion  illusions include bright lights that cause something to blend in  black hole 
white out  or sloping terrain 
   physical environment  unusual physical conditions that could impair flying or
make things difficult  such as unusually hot or cold temperatures inside the cockpit 
cluttered workspace  visual interference  bad weather  or turbulence 
   physical factors  pilot ailment that could impair flying or make things more difficult  such as being tired  fatigued  drugged  incapacitated  influenced by alcohol 
suffering from vertigo  illness  dizziness  hypoxia  nausea  loss of sight  or loss of hearing 
   preoccupation  a preoccupation  distraction  or division of attention that creates
a deficit in performance  such as being preoccupied  busy  doing something else   or
distracted 
   pressure  psychological pressure  such as feeling intimidated  pressured  pressed for
time  or being low on fuel 
    proficiency  a general deficit in capabilities  such as inexperience  lack of training 
not qualified  not current  or lack of proficiency 
    resource deficiency  absence  insufficient number  or poor quality of a resource 
such as overworked or unavailable controller  insufficient or out of date chart  equipment malfunction  inoperative  deferred  or missing equipment 
   

fiabedin  ng   khan

    taskload  indicators of a heavy workload or many tasks at once  such as shorthanded crew 
    unexpected  something sudden and surprising that is not expected 
    other  anything else that could be a shaper  such as shift change  passenger discomfort  or disorientation 
    preprocessing
for our semantic lexicon learning approach to cause identification  we need to identify
    the part of speech  pos  of each word in the text      the phrases or chunks in the
sentences  and     the grammatical roles of the words and their governing words  ideally  to
achieve high accuracies on these three tagging tasks  we would manually annotate a section
of the asrs corpus with the appropriate annotations  e g   pos tags  chunks  and train
appropriate taggers on it to tag the rest of the corpus  however  this by itself is a laborintensive task  and is beyond the scope of this paper  therefore  we have used publicly
available tools trained on standard corpora for these three tasks  it is inevitable that this
will not produce the most accurate automatic annotations of our corpus  but as we will see 
this has not caused problem in this task 
from our corpus  we first identify sentence boundaries using the tool mxterminator    second  we run the pos tagger crftagger  phan      b   which uses the penn
treebank tag set  marcus  santorini    marcinkiewicz         on the sentences detected by
mxterminator  third  we run the chunker crfchunker  phan      a  on the tagged
text to identify different types of phrases  also  the minipar parser  lin        is run on the
sentences to identify the grammatical roles of the words  however  the report text has to be
preprocessed before applying these tools for reasons described in the following paragraphs 
the reports in the asrs data set are usually informally written  using various domain
specific abbreviations and acronyms  in general  as observed by van delden and gomez
        posse et al         and ferryman et al          these narratives tend to be written
in short  abbreviated manner  and tend to contain poor grammar  also  the text has been
converted to all upper case  following is an example of the narrative of a report 
taxiing from the ramp at laf at night  made a wrong turn
and crossed rwy        the active at the time  there was
no sign to indicate which rwy i was xing  i clred both directions before xing  we were the only acft on the field
at the time  no mention on the atis of signs being out or
construction on the ramp area  the ctlr didnt question
us  it was i who brought the sit up after i had crossed
the active rwy  commuter ops of   days of hvy flying 
reduced rest  no rwy signs and busy doing last min commuter paper work changes  all contributed to the rwy
incursion     hr day   hr flt time 
   ftp   ftp cis upenn edu pub adwait jmx   trained on the wall street journal corpus

   

ficause identification via weakly supervised semantic lexicon construction

these reports need some preprocessing before nlp techniques can be applied to them 
since these off the shelf tools  e g   the pos tagger  were all trained on mixed case texts 
for example  running crftagger  which was trained on the wsj corpus with correct cases 
on the first two sentences yield the following 
   taxiing nnp from nnp the dt ramp nnp at in laf nnp at in
night nn    
   made nnp a dt wrong nnp turn nnp and cc crossed vbd
rwy nnp       cd     the dt active nnp at in the dt time nn    
as can be seen  the tagger mislabels the words taxiing  from  made  wrong
and active as proper nouns  nnp   instead of tagging them as verb  preposition  verb 
adjective and adjective respectively  this occurs because a good feature for detecting proper
nouns in a sentence is the case of its first character  since all the words begin with a capital
letter  the tagger mistakes a significant portion of these words as nnp  another reason that
the tagger performs poorly on this corpus is that a lot of abbreviations appear in the text 
for example  xing and hvy are short for crossing and heavy  but since they are not
likely to be known to a pos tagger trained on a standard well edited corpus  they would
be identified as unknown words  and most likely be tagged as nouns instead of verb and
adjective respectively  similar problems have been observed for the parsers and chunkers 
for this reason  we decided to preprocess the text by expanding the abbreviations and
restoring the cases of the words 
to expand the acronyms and abbreviations  we rely on the official list of acronyms and
abbreviations used in the asrs reports    in a small number of cases  the same abbreviation
or acronym may have more than one expansion  for example  arr may mean either arrival
or arrive  in such cases we arbitrarily chose one of the possibilities    then  to restore case 
a set of english word lists  place names and person names  were applied to the text to
identify the known words  if a word in the report text appeared in the word lists  then it
was converted to lower case  all the other unknown words were left uppercase  the result
of this process on the aforementioned narrative is as follows 
taxiing from the ramp at laf at night  made a wrong turn and crossed
runway        the active at the time  there was no sign to indicate which
runway i was crossing  i cleared both directions before crossing  we were the
only aircraft on the field at the time  no mention on the automatic terminal
information service of signs being out or construction on the ramp area  the
controller didnt question us  it was i who brought the situation up after i
had crossed the active runway  commuter operations of   days of heavy flying 
   see http   akama arc nasa gov asrsdbonline pdf asrs decode pdf 
   a better option would be to disambiguate between the alternative expansions based on context  e g  
the method followed by banko   brill         however  the number of such ambiguities in the acronyms
and abbreviations list is small      to be exact   and they are either the same pos or variations of the
same word  thus the effect of these ambiguities on the performance of the nlp tools is expected to be
minimal 
   http   wordlist sourceforge net 

   

fiabedin  ng   khan

reduced rest  no runway signs and busy doing last minute commuter paper work
changes  all contributed to the runway incursion     hour day   hour flight time 
we ran the pos tagger  crftagger  on this processed text and did not observe any
errors  for example  the tagged version of the aforementioned two sentences are 
   taxiing vbg from in the dt ramp nn at in laf nnp at in night nn    
   made vbn a dt wrong jj turn nn and cc crossed vbd runway nn       cd
    the dt active jj at in the dt time nn    
both sentences have been correctly tagged  however  our case restoration method is
arguably too simplistic  hence  to determine if we need to perform more fine grained case
restoration  we sought a measure of how much would we gain from accurately restoring
the case of the words in the sentences over the present heuristic method  to check this 
we randomly picked     sentences from the corpus  we first ran the pos tagger on these
sentences after they were case restored by the aforementioned heuristic case restoration
method  then  we manually corrected the capitalization of these sentences and re ran the
pos tagger on the case restored sentences  when the tags thus generated were compared 
we found       agreement  which means that we are not likely to gain much in terms of
pos tagging accuracy from correctly case restored text than the heuristically case restored
text  of the five differences out of      words  three were nnps mislabeled as nns  which
essentially has no effect on outcomes of our research  therefore  the marginal utility from
applying more sophisticated case restoration methods does not seem enough to justify the
additional effort necessary  and we limit our preprocessing step to the expansion of abbreviations and acronyms followed by the heuristic case restoration procedure described above 
the complete flow of preprocessing is shown in figure   
    human annotation procedure
recall that we need reports labeled with the shaping factors for training the cause identification classifiers and testing the performance of our two approaches to cause identification 
additionally  in order to learn a semantic lexicon via bootstrapping  we need a small set of
seed words and phrases related to each shaping factor as a starting point  as a result  after
performing language normalization  we performed two types of annotations      labeling a
set of reports with shaping factors  and     identifying a set of seed words and phrases from
the reports  the annotation procedure is described in more detail in the following sections 
      annotating reports with shaping factors
while nasa has previously developed a heuristic approach to tackle the cause identification
task  posse et al          this approach was evaluated on only    manually annotated reports 
which is far from satisfactory as far as establishing a strong baseline method is concerned 
thus we decided to annotate a set of reports ourselves for evaluating our automatic cause
identification methods 
out of the complete set of         reports  we chose a random set of      reports for
annotation  this subset was divided into two parts  the first part  consisting of     reports 
   

ficause identification via weakly supervised semantic lexicon construction

figure    flow chart of text preprocessing
was annotated by two persons  one undergraduate student and one graduate student   for
each report  they were asked to answer the following question 
which shaping factor s  were responsible for the incident described in the report 
our annotators were trained in a similar way as those who labeled the    reports used in
the evaluation by the nasa researchers  see posse et al          specifically  as background
reading  the annotators were referred to the works of posse et al  and ferryman et al         
both of which describe the shaping factors  and also give some examples of the words and
phrases that indicate the influence of the shaping factors on the described incidents  the
definitions of the shapers are repeated in section      following posse et al s method 
our annotators were explicitly instructed to adhere to these definitions as much as possible
when annotating the reports with shaping factors  after the annotations were completed 
the inter annotator agreement was computed using the krippendorffs         statistics
as described by artstein and poesio         using the measuring agreement on set valued
items  masi  scoring metric  passonneau         the observed inter annotator agreement 
  in this case was found to be       which indicates reliable agreement  out of the    
reports  they completely agreed on the annotations of    reports  completely disagreed on
    reports and partially agreed on    reports  the annotators were then asked to discuss
the discrepancies  during the discussion  it was found that the discrepancies could be
   

fiabedin  ng   khan

primarily attributed to the vagueness of the descriptions of the shaping factors in posse et
al s paper  some of which were interpreted differently by the two annotators 
the annotators then agreed on how the descriptions of the shapers should be interpreted 
and resolved all the differences in their annotation  after the discussion  the remaining     
reports were annotated by one of the annotators  the other annotator was also asked to
annotate a subset of these reports      reports  for cross verification purpose     and the
inter annotator agreement    in this case was observed to be       the      reports
annotated by the first annotator were divided into three sets  a training set      reports 
for training the cause identification classifiers  a held out development set      reports 
for parameter tuning  and a test set       reports  for evaluating the performance of our
approaches to cause identification  the distribution of the shaping factors in the training 
development and test sets are shown in the second  third and fourth columns of table   
      extracting seed words and phrases
in a separate process  the first author went through the first     reports that both annotators
worked on  and selected words and phrases relevant to each of the shaping factors  his
judgment of whether a word or phrase is relevant to a shaping factor was based on a careful
reading of the description of the shaping factors in the works of posse et al         and
ferryman et al          as well as the example seed words selected by the nasa experts
that were shown in these two papers  the specific task in this case was 
in each report  is there any word or phrase that is indicative of any of the
shaping factors  if there is  then identify it and assign it to the appropriate
shaping factor 
note that these seed words and phrases were chosen without regard to the shaping factor
annotation of the document  they were picked on the possibility of their being relevant to
the respective shaping factors  the number of seed words and phrases for each shaping
factor is shown in the last column of table    as we can see      seed words and phrases
were manually selected from the     training reports  for completeness  we also show all the
seed words and phrases extracted from these reports in appendix a  to facilitate further
research on this topic  the annotated data we have used in this research is made available
at http   www utdallas edu  maa       asrs html 
since there is no gold standard against which we can compare this list of annotated
words and phrases  it is difficult to directly compute its precision  however  to get a rough
idea of its precision  we asked one of the annotators to examine the list and identify all and
only those words and phrases in the list that he believes are correct  there was disagreement
over only one word  this yields a precision of         which provides suggestive evidence
that the annotation is fairly reliable  these manually identified words and phrases were
used by our baseline cause identification system  see section    and also served as seeds for
our semantic lexicon learning procedure  see section    
    it is a fairly standard procedure in nlp research to cross annotate only a subset of the data when
complexity and cost of individual annotation is high  see the works of zaidan  eisner  and piatko       
and kersey  di eugenio  jordan  and katz         for instance 

   

ficause identification via weakly supervised semantic lexicon construction

table    distribution of shaping factors in the training  test and development sets
shaping factor
reports in reports in
reports in
seed
training set
test set development words
test set
attitude
  
  
 
 
communication environment
  
  
  
 
duty cycle
 
  
 
  
familiarity
  
  
 
 
illusion
 
 
 
 
other
  
   
  
 
physical environment
  
   
  
  
physical factors
  
  
 
 
preoccupation
  
   
  
 
pressure
 
  
 
  
proficiency
  
   
  
  
resource deficiency
   
   
  
  
taskload
 
  
 
 
unexpected
 
  
 
 
total
   
    
   
   

   baseline system for cause identification
as discussed in the introduction  the goal of our research is to label the incident reports with
the shaping factors that caused the incidents  to evaluate the performance of our cause
identification methods  we need a baseline that uses the same amount of training data
as all the methods described in this paper and performs reasonably well on the test set 
given that cause identification is a relatively new and under investigated task  no standard
baseline has been adopted for this task  in fact  to our knowledge  the only related works
on cause identification for the aviation safety domain were conducted by the researchers at
nasa  see posse et al         ferryman et al          as a result  we construct a baseline
system motivated by posse et al s work  specifically  the baseline takes as input a set of
seed words and phrases manually collected for each of the shaping factors  see section        
and labels a report with the occurrence heuristic  for each seed word and phrase found
in the report  the baseline annotates the report with the shaping factor associated with
the seed  for example     hour duty day is a seed phrase associated with the shaping
factor duty cycle  then  the occurrence heuristic will label any report that contains the
phrase    hour duty daywith duty cycle  this approach is simple but attractive because
    it does not need any training      it can be evaluated very easily  by searching for the
seed words in the narrative of the report being labeled  and     a report can potentially
be labeled with more than one shaping factors  if the seed words and phrases are indeed
relevant to their respective shaping factors  then they should identify the reports related to
the shaping factors with a high degree of precision 
   

fiabedin  ng   khan

   semantic lexicon learning
as described in section    the baseline uses the seed words and phrases manually extracted
from     reports in combination with the occurrence heuristic to label the reports with
shaping factors  however  the reports used for evaluation may not contain exactly the
same words and phrases  but they may contain different variations  synonyms  or words
and phrases that are semantically similar to the seed words and phrases  thus the baseline
may not be able to label these reports correctly by only looking for the words and phrases
in the seed words list 
to address this potential problem  we propose to use semantic lexicon learning algorithms to learn more words and phrases semantically similar to the seed words and phrases
from the reports corpus containing narratives from         reports  using a weakly supervised bootstrapping algorithm may allow us to learn a large number of useful words and
phrases from the corpus that would have required huge amounts of human effort had it been
done manually  below  we first describe the general bootstrapping approach in section     
then  in section      we describe the basilisk framework for learning the semantic lexicon
from an unannotated corpus  thelen   riloff         finally  in section      we discuss our
modifications to the basilisk framework 
    weakly supervised lexicon learning
as mentioned earlier  we employ a weakly supervised bootstrapping approach for building
the semantic lexicon  we use the manually extracted seed words and phrases for each
shaping factor  described in section        to create the initial semantic lexicon  then we
select words and phrases from the unannotated reports that are semantically similar to the
words already appearing in the semantic lexicon  the reports in the corpus do not need to
be labeled with shaping factors  the semantic similarity between two words is measured
using features extracted from the corpus for each word  this process is repeated iteratively 
in each iteration  a certain number of words are added to the semantic lexicon  and the
words in this augmented lexicon are used as the seeds for the following iteration  this
process is shown in figure   

figure    flow chart of the lexicon learning procedure

   

ficause identification via weakly supervised semantic lexicon construction

    basilisk framework
basilisk  bootstrapping approach to semantic lexicon induction using semantic knowledge 
is an instantiation of the aforementioned generic semantic lexicon learning framework  thelen   riloff         the basilisk framework works by first identifying all the patterns for
extracting all the noun phrases in the corpus that appear in one of three syntactic roles 
subject  direct object  or prepositional phrase object  for example  as discussed by thelen and riloff  in the sentence john was arrested because he collaborated with smith and
murdered brown  the extraction patterns are  subject  was arrested  which extracts
john  murdered  object  which extracts brown and collaborated with  pp object 
which extracts smith  then  for each semantic category sk   a pattern pool is constructed
with patterns that tend to extract words in sk   to measure the tendency of a pattern pj
to extract words in sk   the r log f metric is used  which is defined as 
r log f  pj    

fj
 log  fj  
nj

   

here  fj is the number of  distinct  words in sk that pattern pj extracts  and nj is the
total number of  distinct  words in the corpus that pj extracts  this metric is high for both
high precision patterns  i e   patterns that extract primarily words in sk   and high recall
patterns  i e   patterns that extract a large number of words in sk    at each iteration i  the
top       i  patterns  in terms of their r log f scores  are put into the pattern pool for sk  
depleted patterns  i e   patterns that have all their extracted words already in the semantic
lexicon  are not considered in this step  then  the head nouns of all the phrases extracted
by the resulting patterns in the pattern pool are put into the word pool of sk  
next  a subset of the words in the word pool is selected to be added to the seed words
list  those words from the word pool are chosen that are most relevant to sk   more
specifically  for each word wi in the word pool for sk   first the avglog score is calculated 
which is defined as follows 

avglog  wi   sk    

w
pi
x

log   fj     

j  

w pi

   

here  w pi is the number of patterns that extract word wi   and for each pattern pj that
extracts wi   fj is the number of words extracted by pj that belong to sk   then  for each
semantic category sk   five words are chosen that have the highest avglog score for the
category sk  
for multi category learning  thelen and riloff        experimented with different scoring metrics and reported that they achieved the best performance by calculating the diff
score for each word  for a given word in the word pool for a semantic category  the diff
score takes into consideration what score this word gets for the other categories  and returns
a score based on the words score for this semantic category relative to the other categories 
more precisely  the diff score is defined as follows 
dif f  wi   sk     avglog  wi   sk    max  avglog  wi   sl   
l  k

   

   

fiabedin  ng   khan

here  sk is the semantic category for which wi is being evaluated  thus the diff score is
high if there is strong evidence that wi belongs to semantic category sk but little evidence
that it belongs to the other semantic categories  for each semantic category  the diff score
is calculated for each word in the categorys word pool  and the top five words with the
highest diff score are added to the lexicon for that category  two additional checks are
made at this stage      if a word in the word pool has been added to some other category in
an earlier iteration  that word is discarded  and     if the same word is found in more than
one word pool then it is added to the category for which it has the highest score     when
this is completed for all the semantic categories  the iteration ends  and the next iteration
begins with the augmented lexicon 
    modifications to the basilisk framework
as we will see later in this subsection  an analysis of the framework reveals that in some
cases the words selected by basilisk may not be the most relevant ones  for this reason  we
propose three algorithmic modifications to the basilisk framework      using a new semantic
similarity measure      merging the word pools to one single pool for assigning words to the
semantic categories  and     imposing minimum support and maximum generality criteria on
patterns and words added to the pattern pools and the word pools  in addition  we propose
one linguistic modification  in which we employ a type of feature that can be computed in
a robust manner from the words and phrases in the corpus  namely  the n gram features 
the rest of this subsection discusses these modifications 
      modification    new semantic similarity measure
as seen in section      the basilisk framework uses the avglog scoring function to measure
the semantic similarity between words  the diff score for multi category learning also uses
the avglog function to compute the evidence for a word belonging to a semantic category
relative to the other categories  however  a closer examination of the avglog function shows
that it may not be able to properly predict semantic similarity under all circumstances  to
understand the reason  let us first make the following observations  if pattern pj occurs
     times  but extracts words in category sk only   times  it is unlikely that pj is strongly
related to sk   similarly  if word wi occurs      times  but is extracted by pattern pj only  
times  pj should have small influence on the classification of wi   however  the avglog score
will not be able to take these factors into consideration  precisely because it considers only
the absolute number of semantic category members extracted by the patterns that extract
the word but not the frequency of extraction  to see why this is the case  let us consider the
word wi that is extracted by three patterns p    p  and p    with the frequencies as shown in
table    if each of p    p  and p  extract five distinct seed words  then the avglog score for
the word w would be       irrespective of the fact that the patterns actually extract a word
in the seed words list only a tiny fraction of their occurrence in the corpus  p  extracts a
seed word    of its occurrence  p  does so    time  and p    the pattern that extracts w
most often  extracts a lexicon word only      of the times it appears in the text  clearly 
    this approach effectively assumes that each word can belong to at most one category  this is a reasonable
assumption in this specific task since the shaping factors have very distinct meanings 

   

ficause identification via weakly supervised semantic lexicon construction

the patterns would not suggest that wi is related to the semantic category  yet it gets a
good score 
table    illustration of the problem with avglog  how unrelated words may have a high
similarity score  here wi is a word that appears in the corpus and is extracted by
the patterns p    p  and p 

patterns that extract wi
number of times wi is extracted by the pattern pj
number of times pattern pj occurs in the text
number of times a word in category sk is extracted by the pattern pj
number of category words extracted by the pattern pj
log   fj     
avglog  wi  

p 
  
   
 
 
    

p 
p 
  
  
        
 
 
 
 
         
    

keeping this in mind  we propose our probabilistic metric  semprob  which computes the
probability that the word wi belongs to the semantic category sk given that it is extracted
by the patterns p    p            pn   more specifically  semprob is calculated as follows 
semp rob  wi   sk     p rob  sk  wi  
x
 
p rob  sk  pj    p rob  pj  wi  

   

pj

in other words  semprob assumes that the semantic category sk and the word wi are
conditionally independent given pj   a pattern that extracts wi   the probabilities in this
equation are estimated using maximum likelihood estimation from the corpus  specifically 
to compute p rob  pj  wi    we divide the number of times pj extracts wi in the corpus by the
total number of times that wi appears in the corpus  to compute p rob  sk  pj    we divide
the number of times pj extracts a word in the semantic category sk by the total number
of times pj appears in the corpus  for a given word wi and a given semantic category
sk   the sum of the products of these two quantities over all the patterns that extract wi
gives the probability of category sk given word wi   this method does not suffer from the
problem faced by avglog since it depends on the probability of the word being extracted
by the patterns and the patterns probability of extracting words in the category  for the
same example in table    the semprob metric for the word wi is         illustrating how
low the probability of wi s belonging to the semantic category sk is  the details are given
in table   
      modification    common word pool
since we have to compute eqn     for every word in the word pool for each of the categories
and assign the word to the semantic category for which the probability is highest  we change
the framework so that we have only one common word pool for all the semantic categories 
   

fiabedin  ng   khan

table    illustration of the effectiveness of semprob  how unrelated words get low similarity
score 

patterns that extract wi
number of times that wi is extracted by the pattern pj
number of times pattern pj occurs in the text
number of times a word in category sk is extracted by the pattern pj
p rob  wi is extracted by pj  
p rob  pj extracts a word in sk  
p rob  wi is extracted by pj    p rob  pj extracts a word in sk  
semp rob  wi   sk     p rob  wi belongs to semantic category sk  

p 
  
   
 
   
    
     

p 
p 
  
  
   
    
 
 
   
   
    
     
            
      

we still have separate pattern pools for different semantic categories  but the words related
to patterns in the pattern pools will be put into the same common word pool  and allocated
to the most probable semantic category from there  if there are separate word pools for each
semantic category  then we have to add a fixed number of words to each category in each
iterations  such a constraint may undesirably cause a word to be added to a category that
is not the most likely  however  since we have only one word pool after our modification  we
do not have the constraint that we have to add a fixed number of words to each category 
and we can assign each word to its most likely category  thus the number of words added
to different categories may vary in the same iteration 
      modification    minimum support and maximum generality
there are some scenarios in which the semprob metric can produce undesirable results  for
example  consider a very infrequent word wi that occurs in the entire corpus exactly once 
assume that pattern pj   which extracts wi   extracts words in semantic category sk with
    probability  so  according to semprob  the probability that wi belongs to sk becomes
     however  this is not sufficient evidence for wi to belongs sk   such cases not being too
uncommon  we have imposed a minimum word frequency constraint on the words that are
put into the word pool  so that words that appear less than a certain number of times are
not considered  a pattern that appears too infrequently in the corpus can also lead to such
a problem  consider a very infrequent pattern  pj   that appears exactly twice in the corpus
and extracts two words  if one of these words happen to be a seed word  then the other
word will have a     probability to belong to the category of the seed word and pj will have
r log f value of      however  since pj is so infrequent  it does not convey a good evidence
for membership in the semantic category  and we should not allow pj to put words into the
word pool  therefore  we disallow such low frequency patterns from being included in the
pattern pool by adding the constraint that the patterns put into the pattern pool must also
have a minimum pattern frequency  besides these two constraints imposed on the frequency
of occurrence of the words and the patterns  we employ two additional constraints  the first
   

ficause identification via weakly supervised semantic lexicon construction

is the maximum pattern generality constraint  motivated by rychly and kilgarriff        
we remove from consideration patterns that are too general  i e   patterns that extract too
many words   by imposing an upper limit on the number of distinct words that a pattern
to be added to a pattern pool can extract  the second is the maximum word frequency
constraint  since content bearing words are likely to have a lower frequency  see davidov
  rappoport         we impose an upper limit on the maximum number of times a word
appears in the corpus  the four thresholds associated with these four frequency based
constraints will be tuned automatically using the held out development set 
      modification    n gram patterns
in addition to the parse tree based subject verb and verb object patterns already employed
by basilisk  we also employ n gram based extraction patterns  with the goal of more robustly capturing the context in which the words appear  we construct n gram extraction
patterns as follows  for each noun and adjective  x  in the corpus  we create two n gram
patterns for extracting x   a  the preceding n words   hxi  and  b  hxi   the succeeding
n words  for example  in the sentence     a solid line of thunderstorms was detected     
the bigram patterns for thunderstorms would be  line of hxi and hxi was detected 
the complete sentence is approaching the atl area a solid line of thunderstorms was
detected in the vicinity of the airport  and the words and their extracting bigram patterns
would be 
 atl  approaching the hxi  hxi area a
 area  the atl hxi  hxi a solid
 solid  area a hxi  hxi line of
 line  a solid hxi  hxi of thunderstorms
 thunderstorms  line of hxi  hxi was detected
 vicinity  in the hxi  hxi of the
 airport  of the hxi
in addition to constructing n gram patterns for extracting words  we also construct
n gram patterns for extracting phrases  to do so  we first remove articles  a  an  the  and
possessive pronouns and adjectives  e g   my  his  from the beginning of the phrases in the
corpus  for each noun phrase and adjective phrase  x  that appears in the corpus  we
create two n gram patterns for extracting x   a  the preceding n words   hxi  and  b 
hxi   the succeeding n words  for example  from the sentence this was the last of   legs
and approaching the end of an   hour duty day and   hour hard time flying day  we would
extract the following phrases with the following bigram patterns 
   legs  last of hxi  hxi and approaching
 end  and approaching hxi  hxi of an
   

fiabedin  ng   khan

   hour duty day  end of hxi  hxi and  
   hour hard time flying day  day and hxi
thus we use three types of patterns in our experiments  bigram patterns for extracting
words  bigram patterns for extracting phrases  and parse tree based subject verb and verbobject patterns  all these patterns were generated from the reports corpus generated by
combining the narratives of the         unlabeled reports described in section      as
we will see  not all three types of patterns are beneficial to use as far as performance
is concerned  in section    we will show how to automatically select the best subset of
patterns to use based on the development set 

   semantic lexicon based approaches to cause identification from
asrs reports
we investigate a heuristic based approach and a learning based approach to cause identification  both of which exploit information provided by an automatically acquired semantic
lexicon  this section describes the details of these two approaches 
    heuristic based approach
the heuristic based approach operates in essentially the same way as the baseline cause
identification system described in section    where the occurrence heuristic is used to label
a report with shaping factors  the only difference is that the words and phrases used
by the occurrence heuristic in the baseline are manually identified  whereas those in our
heuristic based approach are acquired by our modified basilisk procedure 
    learning based approach
our learning based approach to the cause identification problem is to recast it as a classification task  note that we have a multi class multi labeled classification task  there are   
classes and each report can be labeled with more than one class  a number of approaches
have been proposed to tackle multi class multi labeled classification tasks  in the rest of
this section  we describe the three existing approaches to multi class multi labeled text classification that we explore in our experiments  section         and provide an overview of
the theory of support vector machines  svms   the underlying learning algorithm we use
to train classifiers employed by these three approaches  section        
      three approaches to multi class multi labeled text classification
one versus all  in this approach  we train one binary classifier for each shaping factor
sk to determine whether a report will be labeled with sk   more specifically  we follow the
one versus all classification scheme  for a given sk   the reports in the training set that
contains sk in its set of labels  assigned by the annotator  are the positive instances for the
binary classifier and the rest of the reports in the training set are the negative instances 
after training  we apply the classifiers to a report in the test set independently of other
reports  and label the report with each sk for which the corresponding classifier classifies
   

ficause identification via weakly supervised semantic lexicon construction

the report as positive  thus we convert cause identification to a multi class multi labeled
document classification task 
while any learning algorithm can be used in principle to train classifiers for this oneversus all scheme  we use support vector machines   for training and testing the classifiers 
primarily due to its successes in various text classification tasks  each classifier is trained
with two types of features      unigrams and bigrams from the report narratives  and    
words and phrases from the semantic lexicon  the feature values are tf idf values 
while our shaping factor labeled data set of      reports is substantially larger than the
set of    reports annotated by the nasa researchers  see section     it is arguably fairly
small from a machine learning perspective  hence  it is conceivable that the performance
of our svm classifiers would be limited by the small size of the training data  as a result 
we investigate whether we can improve the one versus all approach using a transductive
svm  which is a version of the inductive svm described above that attempts to improve
classifier performance by combining both labeled and unlabeled data  see section      
for an overview of transductive learning   for our cause identification task  the unlabeled
reports in the test set serve as unlabeled data in the transductive learning procedure 
metalabeler  as our second approach  we employ metalabeler  tang  rajan    narayanan 
      for classifying multi class multi labeled text data  here  a model is first learned that
predicts the number of labels that an instance may have  in addition  a set of binary classifier models  one for each possible label  are learned to predict the likelihood of each label
for an instance  when an instance is classified  the first model predicts k  the number of
possible labels for that instance  and from the output of the second set of classifiers  k
labels are chosen with the highest likelihood for that instance 
in our implementation of this approach  the first model is learned using svmmulticlass  
which is an implementation of multi class svm described by crammer and singer           
the second set of classifiers are the same set described in section        but in this case 
for a given instance x  the decision functions f  x    w  x  b for each of the classifiers are
evaluated  and the positive decision values are sorted  then the top k labels corresponding
to the highest values of the decision functions are assigned to the instance  both the
multiclass classifier and the set of binary classifiers are trained using the same types of
features as in the one versus all approach  namely unigrams and bigrams from the reports 
and words and phrases from the semantic lexicon  the feature values are also the same as
in one versus all approach  namely tf idf values 
ensembles of pruned sets  in the pruned sets approach  read  pfahringer    holmes 
       the multi class multi label text classification problem is transformed into a multiclass single label text classification problem by selecting a subset of the label combinations
most frequently occurring in the dataset and assigning a unique pseudo label to each chosen
label combination 
the first step in this algorithm is to choose the label sets for training  in this step 
those label sets are chosen that meet the minimum frequency requirement in the training
set  using the minimum frequency constraint prunes away infrequently occurring label sets
that have frequency less than p  leaving only label combinations that are frequent and thus
    as implemented in the svmlight software package by joachims       
    available at http   svmlight joachims org svm multiclass html

   

fiabedin  ng   khan

more important  the training instances that are labeled with the pruned label sets are
also removed from the training set  the minimum cardinality parameter  b  is then used
to reintroduce some of the pruned instances back to the training set in order to minimize
the information loss from the pruning process  first the label sets of the rejected instances
are broken down into smaller subsets of at least size b  then those new subsets that have
frequency higher than p are reintroduced  and the pruned training instances whose label
sets are supersets of these newly accepted label sets are reinstated into the training set  the
role of the parameter b in this case is to ensure that not too many such instances with small
label sets are put back  because that will cause the average number of labels to reduce 
resulting in smaller number of labels per instance at classification time 
the next step is to learn classifiers on the selected label sets  first  each accepted label
set is assigned a unique pseudo label  thus transforming the multi label classification problem into a single label classification problem  then an ensemble of m classifiers is learned
to predict these pseudo labels given an instance  using the same multi class svm implementation as in metalabeler   where each classifier in the ensemble is trained on a different
random sample of the training data  since     the label sets for training the classifiers
represent only a subset of all the label combinations present in the original training data
and     the test data may contain label combinations that are not present in the training
data  having an ensemble of classifiers allows the system to generate label combinations not
observed at training time  for example  let the label combinations  l    l    and  l    l    be
present in the training data  then  if one classifier in the ensemble labels a test instance
with  l    l    and another classifier in the ensemble labels the same instance with  l    l    
then that instance may be labeled with  l    l    l     depending on the actual voting policy in
effect at classification time  even if this combination is not present in the training data  the
classifiers in the ensemble are built using the same two types of features as the one versusall approach  namely unigrams and bigrams from the reports and words and phrases from
the semantic lexicon learned by our modified basilisk framework 
finally  when classifying an instance  each of the m classifiers assigns one pseudo label
to the instance  these pseudo labels are then mapped back to the original label combination
and the vote for each actual label is counted and normalized by dividing by the number of
classifiers  m   in order to bring the prediction for each possible label to the range between
    and      then a threshold t is used such that each label that has a prediction value
greater than or equal to t is assigned to the instance  this scheme is used to make it possible
to assign label combinations unseen at training time to the test instances 
      an overview of support vector machines
svms have been shown to be very effective in text classification  joachims         below
we describe two versions of svms      inductive svms  which learn a classifier solely from
labeled data  and     transductive svms  which learn a classifier from both labeled and
unlabeled data 
inductive svms  given a training set consisting of data points belonging to two classes 
an inductive svm aims to find a separating hyperplane that maximizes the distance from
the separating hyperplane to the nearest data points  these nearest data points act as the
support vectors for the plane 
   

ficause identification via weakly supervised semantic lexicon construction

more formally  let d be the data set with m data points where
d     xi   ci    xi  rn   ci              i  m 

   

each point xi is represented as an n dimensional vector and is associated with a class label
ci   the inductive svm classifier attempts to find a hyperplane w  x  b     that is at the
maximum distance from the nearest data points of opposite labels  this hyperplane would
be in the middle of the two hyperplanes containing the support vectors of each class  these
 
  therefore  the
two hyperplanes are wxb     and wxb      and their distance is  w 
desired separating hyperplane can be found by solving the following quadratic programming
optimization problem 
minimize
subject to

 
 w  
 
ci  w  xi  b         i  m

   

however  in practice many classes are not linearly separable  to handle these cases  a set
of slack variables is used to represent the misclassification of point xi   then the problem
becomes 
x
 
 w     c
i
minimize
 
i

subject to

ci  w  xi  b      i   i         i  m

   

where the i are additional variables representing training errors and c is a constant representing trade off between training error and margin  more details can be found in cortes
and vapnik         in our experiments  we use the radial basis function  rbf 
kernel 



 
where every dot product is replaced by the function k  x  x     exp  x  x     for      
in addition  both  and c are chosen by cross validation on the training set 
transductive svms  in the transductive setting  in addition to the set of labeled data
points  we also exploit a set of unlabeled data points  t    xi  xi  rn      i  k   that
are taken from the test set  as described by joachims         the goal is then to minimize
the expected number of classification errors over the test set  the expected error rate is
defined in vapnik        as follows 
z
 x
   
  hl  xi     ci   dp  x    c          dp  xk   ck  
r  l   
k
i

where l   d  t   hl is the hypothesis learned from l  and   a  b  is zero if a   b
and one otherwise  the labeling ci of the test data and the hyperplane that maximizes the
separations of both training and testing positive and negative instances are found by solving
the following quadratic programming optimization problem  which is a modified version of
eqn     
x
x
 
j
 w     c
i   c 
minimize
 
i

subject to

j

ci  w  xi  b      i   i         i  m

cj w  xj  b     j   j         j  k
   

   

fiabedin  ng   khan

similar to the inductive svm in section        we use the rbf kernel in our experiments
involving the transductive svm 

   evaluation
the goal of our evaluation is to study the effectiveness of our two approaches to cause identification  namely the semantic lexicon learning approach and the classification approach 
we do so by testing the performance of the approaches on a randomly chosen set of reports
that have been manually annotated with the shaping factors that caused the incidents described in them  section         we start by describing the experimental setup  section      
followed by the baseline results  section      and the performance of our two approaches
 sections     and       we then describe the experiment where we increase the amount
of training data available to the classification approach and investigate how this impacts
performance  section       after that  we perform an analysis of the errors of the bestperforming approach  section      and conduct additional experiments in an attempt to
gain a better insight into the cause identification task that can help direct future research
 section       finally  we present a summary of the major conclusions that we draw from
the experiments  section      
    experimental setup
as described in section      out of the         reports in the entire corpus  we have manually
annotated      incident reports with the shaping factors  we have used the first     of
them to     manually extract the initial seed words and phrases for the semantic lexicon
learning procedure  and     train classifiers for identifying shaping factors associated with
a report  of the remaining reports  we have used      reports as test data and     reports
as development data  for parameter tuning  
      evaluation metrics
as mentioned in section      there are    shaping factors  and a report may be labeled
with one or more of these shaping factors  we evaluate the performance of our cause
identification approaches based on how well the automatic annotations match the human
annotations of the reports in the test set  for evaluation  we use precision  recall and
f measure  which are computed as described by sebastiani         specifically  for each
shaping factor si   i                   let ni be the number of reports in the test set that the
human annotator has labeled with si   i e   the number of true si  labeled reports in the test
set  further  let pi be the number of reports that an automatic labeling scheme ci has
labeled with si   and let tpi be the number of reports that ci has labeled correctly with si  
then  for the shaping factor si   we have the following performance metrics 
 precisioni is the fraction of reports that are really caused by shaping factor si among
all the reports that are labeled with si by the labeling scheme 
p recisioni  

   

tpi
pi

ficause identification via weakly supervised semantic lexicon construction

 recalli is the percentage of reports really caused by shaping factor si that are labeled
by the labeling scheme with the shaping factor si  
recalli  

tpi
ni

thus we obtain a measure of the labeling schemes performance for each of the shaping
factors  to obtain the overall performance of the labeling scheme  we sum these counts
 i e   ni   pi and tpi   over all shaping factors and compute the micro averaged precision 
recall and f measure from the aggregated counts as described by sebastiani and repeated
as follows 
p
tpi
p recision   pi
pi
pi
tpi
recall   pi
i ni
   p recision  recall
f  measure  
p recision   recall
thus for each labeling scheme we have one set of overall scores reflecting its performance
over all classes 
      statistical significance tests
to determine whether a labeling scheme is better than another  we apply two statistical
significance tests  mcnemars test  everitt        dietterich        and the stratified approximate randomization test  noreen         to test whether the difference in their performances is really statistically significant  mcnemars test compares two labeling schemes
on the basis of errors  i e   whether both the labeling schemes are making the same mistakes   and the stratified approximate randomization test compares the labeling schemes
on f measure  both tests have been extensively used in machine learning and nlp literature  in particular  stratified approximate randomization is the standard significance test
employed by the organizers of the message understanding conferences to determine if the
difference in f measure scores achieved by two information extraction systems is significant  see chinchor        chinchor  hirschman    lewis         since we are ultimately
concerned about the difference in f measure scores between two labeling schemes in cause
identification  our discussion of statistical significance in the rest of this section will be focused solely on the stratified approximate randomization test  for both tests  we determine
significance at the level of p        
    baseline system
recall that we use as our baseline the heuristic method described in section    where the
occurrence heuristic is used to label a report using the seed words and phrases manually
extracted from the     training reports  results  shown in the experiment   section of
table    are reported in terms of precision  p   recall  r   and f measure  f   the last
two columns show whether a particular automatic labeling scheme is significantly better
   

fiabedin  ng   khan

than the baseline with respect to mcnemars test  mn  and stratified approximate randomization test  ar   statistical significance and insignificance are denoted by a x and an
x  respectively   when evaluated on the      reports in the test set  the baseline achieves
a precision of         a recall of        and an f measure of        
table    report labeling performance of different methods 
approach feature set
p
r
f mn
ar
experiment    baseline
heuristic seed words
                  n a n a
experiment    semantic lexicon approach
lexicon from modified basilisk
                 
x
x
heuristic
lexicon from original basilisk
                 
x
x
experiment    supervised one versus all classification approach
unigrams
                 
x
x
unigrams and bigrams
                 
x
x
svm
lexicon words
                 
x
x
unigrams and lexicon words
                 
x
x
unigrams  bigrams  lexicon words                  
x
x
experiment    transductive one versus all classification approach
unigrams
                 
x
x
unigrams and bigrams
                 
x
x
svm
lexicon from modified basilisk
                 
x
x
unigrams and lexicon words
                 
x
x
unigrams  bigrams  lexicon words                  
x
x
experiment    metalabeler approach
unigrams
                 
x
x
unigrams and bigrams
                 
x
x
svm
lexicon words
                 
x
x
unigrams and lexicon words
                 
x
x
unigrams  bigrams  lexicon words                  
x
x
experiment    ensembles of pruned sets approach
unigrams
                 
x
x
unigrams and bigrams
                 
x
x
svm
lexicon from modified basilisk
                 
x
x
unigrams and lexicon words
                 
x
x
unigrams  bigrams  lexicon words                  
x
x
experiment    additional training data with   fold cross validation
unigrams
                 
x
x
unigrams and bigrams
                 
x
x
svm
lexicon words
                 
x
x
unigrams and lexicon words
                 
x
x
unigrams  bigrams  lexicon words                  
x
x

   

ficause identification via weakly supervised semantic lexicon construction

    experiments with semantic lexicon approach
recall that in the semantic lexicon learning approach  we label a report in the test set using
the occurrence heuristic in combination with the semantic lexicon learned by the modified
basilisk framework described in section      before showing the results of this approach 
we first describe how we tune the parameters of the modified basilisk framework 
      parameters
our modified basilisk framework has five parameters to tune  the first four are the thresholds resulting from the four frequency based constraints involving minimum support and
maximum generality  see modification   in section         more specifically  the four
threshold parameters are     the minimum frequency of a word  m inw        the maximum frequency of a word  m axw        the minimum frequency of a pattern  m inp    and
    the maximum number of words extracted by a pattern  m axp    in addition  recall from
section       that we have three types of patterns  namely  subject verb verb object patterns  bigram patterns for extracting words  and bigram patterns for extracting phrases  
our fifth parameter is the pattern parameter  which determines which subset of these
three types of patterns to use  our goal is to tune these five parameters jointly on the
development set  in other words  we want to find the parameter combination that yields
the best f measure when the occurrence heuristic is used to label the reports in the development set  however  to maintain computational tractability  we need to limit the number
of values that each parameter can take  specifically  we limit ourselves to five different combinations of the four threshold parameters  see table     and for each such combination 
we find which subset of the three types of patterns yields the best f measure on the development set  hence the total number of experiments we need to run is          the number
of  non empty  subsets from the three types of patterns      the number of combinations
of the first four parameters    our experiment indicates that combination   in table   
together with the bigram patterns for extracting phrases  yields the best f measure on the
development set  and is therefore chosen to be the best parameter combination involving
these five parameters 
the new words and phrases acquired in the first two iterations of modified basilisk
by using this parameter combination are shown in appendix b  here we see that no new
words are acquired in the first two iterations for eight of the    categories  the reasons
are that     unlike the original basilisk framework  modified basilisk employs a common
word pool  thus no longer requiring that five words must be added to each category in each
bootstrapping iteration  and     the application of minimum support to words has led to
the filtering of infrequently extracted words  these two reasons together ensure that the
modified basilisk framework focuses on learning high precision words for each category 
      results
the semantic lexicon learned using the best parameter combination  based on the performance on the development set  is used to label the reports the test set  as we can see from
row   of experiment   of table    the modified basilisk approach achieves a precision of
        a recall of        and an f measure of         in comparison to the baseline 
this method has a lower precision and a higher recall  the increased recall shows that more
   

fiabedin  ng   khan

table    combinations of the four threshold parameters for the modified basilisk framework 
combination
combination
combination
combination
combination
combination

 
 
 
 
 

m inw
  
  
  
  
  

m axw
    
    
    
    
    

m inp
   
   
   
   
   

m axp
   
   
   
   
   

reports are covered by the expanded lexicon  however  the learned lexicon also contains
some general words that have resulted in a drop in precision  overall  it has a higher fmeasure  which is statistically significantly better than that of the baseline according to
both significance tests  this vindicates our premise that learning more words and phrases
relevant to the shaping factors will help us identify the shaping factors of more reports 
      results using original basilisk
to better understand whether our proposed linguistic and algorithmic modifications to
the basilisk framework  see section      are indeed beneficial to our cause identification
task  we repeated the experiment described above  except that we replaced the lexicon
generated using the modified basilisk framework with one generated using the original
basilisk framework  more specifically  we implemented the original basilisk framework as
described by thelen and riloff         but with one minor difference  in the case of the
bigram patterns extracting phrases  the word pools described in section     were populated
with entire phrases instead of only head words  this was done because the seed words list
extracted in section       contains both words and phrases and hence we would like to learn
entire phrases 
the only parameter to tune for the original basilisk framework is the pattern parameter 
which  as mentioned above  determines which subset of the three types of patterns to use 
therefore  we construct seven lexicons  corresponding to the seven non empty subsets of
the three types of patterns  using the original basilisk framework  and determine which
lexicon yields the best performance on the development set  our experiment indicates that
the best development result was achieved when only the bigram patterns for extracting
phrases were used  applying the corresponding semantic lexicon in combination with the
occurrence heuristic to classify reports in the test set  we observe a precision of        
a recall of        and an f measure of         see row   of the experiment   section
of table     this lower precision and higher recall indicates that the lexicon has learned
words that are very general  i e   words that appear in many of the reports and with little
discriminative power   the new words and phrases acquired in the first two iterations of
original basilisk are shown in appendix c  as can be seen  the original basilisk framework
adds a lot of words  but many of them are not relevant to the shaping factors to which they
were added  and some are not semantically similar to the seed words for that shaping factor 
   

ficause identification via weakly supervised semantic lexicon construction

hence  although recall improves by a small amount  precision drops significantly  leading
to a precipitation in f measure  these results suggest that our proposed modifications to
the original basilisk framework are indeed beneficial as far as our cause identification task
is concerned 
    experiments with classification approach
recall that in the classification approach to cause identification  we train an svm classifier
for each shaping factor sk to determine whether a report should be labeled as sk   as desired 
this approach allows a report in the test set to potentially receive multiple labels  since the
resulting    svm classifiers are applied independently to each report  to investigate the
effect of different feature sets on the performance of cause identification  we employ five
feature sets in our experiments      unigrams only      unigrams and bigrams      lexicon
words only      unigrams and lexicon words  and     unigrams  bigrams and lexicon words 
the unigrams and bigrams were generated from the reports in the training set by first
removing stop words and ignoring case information  while the semantic lexicon was the
one constructed by our modified basilisk framework  before showing the results of our
supervised and transductive experiments  we first describe the parameters associated with
the classification approach 
      parameters
for each svm classifier  we have two parameters to tune  the first parameter is the
percentage of features to use  feature selection has been shown to improve performance
in text classification tasks  yang   pedersen         as a result  we employ information
gain  ig   one of the most effective methods for feature selection according to yang and
pedersens experimental results  since we assume that the words from the semantic lexicon
are all relevant to cause identification  we do not apply feature selection to the lexicon words 
rather  we apply feature selection only to the unigrams and bigrams  more specifically  if
only unigrams are used as features  as in the first of the five feature sets mentioned at the
beginning of this subsection   we select the n   unigrams with the highest ig  where the
value of n is tuned using the development set  when both unigrams and bigrams are used
as features  as in second and fifth feature sets   we combine the unigrams and bigrams into
one feature set and select the n   unigrams and bigrams with the highest ig  where the
value of n is again tuned using the development set  in our experiments  we tested   
values for n                      
the second parameter associated with the svm classifiers is the classification threshold 
by default  svm sets the classification threshold to    meaning that every data point with
a classification value above   is classified as positive  and the rest will be classified as
negative  however  since an svm classifier is trained to optimize classification accuracy 
the best classification threshold may not be   for our cause identification task  where the
goal is to optimize f measure  as a result  we parameterize the classification threshold 
allowing it to take one of    values                             
as usual  we tune the two parameters described above jointly rather than independently 
in other words  for each possible value combination of the percentages of features and
   

fiabedin  ng   khan

classification threshold  we compute the f measure of the classifiers on the development set
over all the classes and choose the value pair that yields the maximum f measure 
to get a better idea of how these two parameters impact performance  we show in
figure   how f measure changes on the development set as we vary the values of the
two parameters  from the experiment where the underlying svm classifiers employ only
unigrams as features  as we can see  the best f measure was achieved by employing the
top     unigrams and a classification threshold of      using the default parameter values
 no feature selection and a classification threshold of    yields a f measure of approximately
     overall  these results provide suggestive evidence that both parameters can have a
large impact on performance 
f measure vs  classification threshold
for different percentages of unigram features
   
top     unigrams
top     unigrams
top     unigrams
top     unigrams
top     unigrams
top     unigrams
top     unigrams
top     unigrams
top     unigrams
top      unigrams

  
  

f measure    

  
  
  
  
  
  
  
 
  

    

  

    
 
   
classification threshold

 

   

 

figure    variation of f measure with different percentages of unigram features and classification thresholds used for svm classification 

      supervised one versus all classifiers  results and discussions
results of the supervised one versus all classification approach using the five feature sets
described above are shown in the experiment   section of table      as we can see  when
feature sets    unigrams only  and    unigrams and lexicon words  are used  we achieve
the best results  f measure scores of        and         respectively  however  even
these best results are statistically indistinguishable from the baseline result  according to
approximate randomization test   and are significantly worse than the result produced by
    recall that in the supervised approach  the svm classifiers were trained on only the     reports in the
training set 

   

ficause identification via weakly supervised semantic lexicon construction

the modified basilisk approach  row   of experiment     see appendix d  which contains
statistical significance test results that we obtained by applying stratified approximate randomization test to each pair of experiments in table    
in fact  they also indicate that the occurrence heuristic has made more effective use
of the learned semantic lexicon than the svm classifiers  the svm classifiers trained with
only the lexicon words as features  row   of experiment    produced a significantly worse
f measure score          than that of the occurrence heuristic           due to large
drops in both recall and precision  overall  these results suggest that the supervised approach performs worse than the heuristic based semantic lexicon approach in this task  we
hypothesize that the limited amount of training data available to the svm learner has contributed to the poor performance of the supervised approach  we will test this hypothesis
in section    
two additional observations are worth mentioning  first  comparing rows   and   of
experiment    we see that the lexicon words are not useful for cause identification in the
presence of unigrams  second  comparing rows   and   and then rows   and   of experiment
   we see that using bigrams hurts performance  a likely reason can be attributed to
our feature selection method  since we choose the top n   features  the bigram features
significantly outnumber the unigram features  thus potentially diminishing the effect of
the latter  one solution to this problem is to employ separate parameters when selecting
unigrams and bigrams  but we decided against this choice  as it would lead to an explosion
in the size of the parameter space 
      transductive one versus all classifiers  results and discussions
to investigate whether it is useful to exploit unlabeled data  we employ transductive svm
to combine labeled and unlabeled data  essentially  we repeated the experiments in the
supervised one versus all classification approach  except that we trained each transductive
svm classifier using both the  labeled  reports in the training set and the  unlabeled 
reports in the test set as described in section        the two parameters  the percentage of
features used and the classification threshold  are tuned jointly to maximize f measure on
the development set  as described in the supervised approach  except that the transductive
svms used in the parameter tuning step are trained using the training set as labeled data
and the development set as unlabeled data 
results of these transductive svm classifiers are shown in the experiment   section of
table    overall  the transductive results are significantly worse than the corresponding
results in experiment    however  the conclusions that we can draw from the transductive
results are slightly different from those drawn from the supervised results  first  using
bigrams significantly improves performance when the lexicon words are absent  comparing
rows   and   of experiment    but hurts performance when the lexicon words are present
 comparing rows   and     second  adding lexicon words to the unigram only feature
set  comparing rows   and    significantly improves performance  suggesting the potential
usefulness of the lexicon features  nevertheless  experiments   and   both indicate that    
using only lexicon words as features are far from adequate  and     the best performance is
achieved when lexicon words are added to unigrams as features 
   

fiabedin  ng   khan

      results from additional supervised approaches
next  we present the results from the two additional supervised approaches  namely metalabeler and ensembles of pruned sets  section         the feature sets used by both
approaches are the same as those used by the one versus all method  as in the oneversus all method  both of these approaches use svm as the underlying learning algorithm
for classifier training 
metalabeler  the only parameter that needs to be tuned for the metalabeler approach
is the percentage of features to use  n    which was selected based on classification performance  f measure  on the development set 
results of the metalabeler approach are shown in the experiment   section of table    there are some interesting points about the these results  first  the metalabeler
method results in much better precision than the other methods  second  this method
shows consistent performance improvement when bigram features are added  as can be seen
by comparing the first and second  and fourth and fifth rows of the metalabeler results 
third  the inclusion of the lexicon word features are also found to improve performance 
as seen by comparing the first and fourth  and second and fifth  rows of the metalabeler
results  these two observations show that the metalabeler approach can properly take
advantage of the increasingly richer feature sets used in these experiments  with the best
performance occurring when all types of features are used  fifth row   unfortunately  the
approach suffers from poor recall  a fact that prevents it from even matching  let alone
surpassing  the f measure scores of the other methods  since the method discards the less
probable labels when it assigns the labels to the documents  precision is much improved
but recall suffers 
ensembles of pruned set  among the parameters of the ensembles of pruned sets
approach  the number of classifiers in the ensemble  m   and the size of the sample of the
training data on which each classifier in the ensemble was trained  were chosen to be the
same ones used by read et al          namely    and     respectively  the rest of the
parameters of the pruned set approach  namely the minimum cardinality  b   the minimum
support  p   the percentage of features to use  n    and the threshold for label assignment  t 
were selected jointly based on classification performance  f measure  on the development
set  the values from which the specific value of b was chosen was      and    the possible
values of p tested in this experiment was      and     the threshold parameter t was chosen
from the values                        and the percentage of features  n was chosen from the
values                         thus we had     parameter combinations for each feature set 
and from these parameter combinations  the combination for which the performance on the
development test set was best  in terms of f measure  was chosen for running the system
on the test set 
results of the pruned set approach are shown in the experiment   section of table   
here  we see the best performance for the combination of unigram and lexicon word features 
better than the performance using the unigrams and lexicon words individually  however 
performance degraded with the inclusion of bigrams into this combination  precision is
much lower than those of the other methods  which indicates that the selection of the label
sets from the training set of only     reports may not have been adequate 
   

ficause identification via weakly supervised semantic lexicon construction

    experiments using additional training data
the results of the above experiments are somewhat surprising  the best performing supervised classification approach  the one versus all approach  performs significantly worse
than the modified basilisk approach  we hypothesize that its poor performance can be attributed to the scarcity of  labeled  training data  to test this hypothesis  we conducted a
set of experiments in which we increased the amount of training data for the one versus all
supervised classification approach by applying cross validation  more specifically  we take
the test set of      reports and split it into five disjoint subsets of equal size  t    t            t   
then  for each i we construct the training set by merging all tj   where i    j  with the
original training set of     reports  after that  we train an svm classifier on this merged
training set and test on the set ti   when this is done over all five folds  we compute the
f measure over the entire test set  in other words  the results we report for this set of
experiments are not f measure scores averaged over the five folds  we again experimented
with the five set of features used in the supervised experiments in section      the two
parameters  the percentage of features used and the classification threshold  are tuned in
exactly the same way as in the supervised experiments 
results of this set of experiments are shown in the experiment   section of table    in
comparison to the results of experiment    f measure increases uniformly and significantly 
this provides empirical evidence that the performance of the supervised classifiers is limited
by the amount of data on which they were trained  with feature sets    unigrams and
lexicon words  and    unigrams  bigrams and lexicon words   we achieve the best results
 f measure scores of        and        respectively  the difference between which
is statistically insignificant  these two results are in turn significantly better than that of
modified basilisk  row   of experiment     according to the approximate randomization
test  in addition  except for feature set    lexicon words only   results obtained in this
experiment are significantly better than that of the baseline  again according to approximate
randomization test  overall  these results suggest the difficulty of the cause identification
task  by comparing rows   of experiments   and    we see that f measure increases by only
about    as the number of training reports is increased from     to      
a few more points deserve mentioning  as in previous learning based experiments  using only lexicon words as features yields the worst result in this set of experiments  and
combining unigrams and lexicon words still yields one of the best results  nevertheless 
in comparison to experiment    while using bigrams still does not improve performance 
it does not hurt performance  from a statistical significance point of view   perhaps more
importantly  comparing rows   and   of experiment    we see that augmenting unigrams
with lexicon words yields significantly better performance  this indicates that the lexicon
words are indeed useful features for cause identification  but their usefulness may not be
revealed when a small labeled training set is used  as seen in experiment    learning algorithms attempt to learn which features are important or relevant for the given classification
task based on the training examples they see  and the more there are training examples 
the better they are able to learn the relevance of the features  our results show a very
poignant illustration of this phenomenon  the svm learner is able to use the lexicon word
features effectively only when given a large number of training instances  this can be seen
more clearly from the svm learning curves in section        this indicates that lexicon
   

fiabedin  ng   khan

words are useful as features only when we have sufficiently large training data  however 
lexicon words may still be used effectively in ways other than as linguistic features even if
the training set is small  as we can see from the results of experiment    which uses the
lexicon words in combination with the occurrence heuristic to achieve performances that
are statistically significantly better than the baseline 
    error analysis and lessons learned
in order to gain a clearer insight into our cause identification problem and help direct
future research  we manually analyzed the errors made by the best performing system  i e  
the heuristic based approach using the semantic lexicon learned by our modified basilisk
framework  on a randomly chosen     report subset of the test set  more specifically  we
looked at the false negatives  cases in which the annotator labeled a report with a shaping
factor but the system did not  and false positives  cases in which the system labeled a
report with a shaping factor but the annotator did not   for each false negative  we tried
to determine why the system failed to correctly label the report  and for each false positive 
we tried to determine why the system labeled the report erroneously  table   shows the
number of false positives and false negatives along with the reasons for these errors that we
discovered in our analysis  the following sections discuss the errors and their reasons in
more detail  note that since a shaping factor may be indicated by more than one keyword
in a single report  there can be more than one reason for a false negative  positive  error 
thus the sum of the frequencies of different types of false negative  positive  errors is greater
than the total number of false negatives  positives  
table    error analysis details  different reasons for the false positive and false negative
errors 
false negatives
sentence fragments bigger than phrases
implicit causes that cannot be identified by keywords
phrases that were not learned
false positives
keyword was too general
keyword indicates concept that appears in the report but
does not contribute to the incident
wrongly learned keyword
keyword was used in a negative context
keyword was used in a hypothetical context

  
  
  
  
  
  
  

percentage
      
      
      

 
 
 

     
     
     

      
      

false negatives  for each false negative error  we read the report narrative to identify
some word  phrase or sentence fragment that may indicate the shaping factor that our
system missed  from this analysis  we identified three reasons for the false negatives as
follows 
   

ficause identification via weakly supervised semantic lexicon construction

   required sentence fragments larger than phrase  we identified    sentence
fragments that are bigger than phrases  i e   those that consist of two or more phrases  
for example  the sentence fragment having never been to dca before consists of  
phrases  having never been  to  dca and before  together  they convey the meaning
that the reporter was unfamiliar with dca  but it is not possible to identify a single
word or phrase that conveys the same meaning  since our framework learns only
phrases  it was not possible to learn these sentence fragments 
   cause not identifiable by specific words or phrases  in    instances  no specific
word  phrase or sentence fragment could be identified that could pinpoint the shaping
factors responsible for the incident  for example  a number of reports  including
report         describe incidents in which there is a miscommunication between the
pilot and the air traffic controller  but that miscommunication must be understood by
following their conversation  a human reading the report can easily understand that
the pilot is claiming the controller said one thing and the controller is claiming he
said something different  but to detect that kind of a scenario  a machine would need
to generate a complete model of the discourse that identifies the specific topic of the
conversation  the participants  the claims each participant makes about the topic  the
fact that the claims are contradictory  and also the fact that the contradiction arises
from miscommunication between them  the preprocessed narrative of this report is
shown in appendix e 
   missing phrases  in    cases the necessary phrase was missing from the semantic
lexicon learned by our modified basilisk framework  out of these    phrases  six
phrases were too infrequent to be considered by our modified basilisk framework due
to the minimum frequency criterion  for example  the phrase temperature flux
appears only once in the entire corpus and hence was not considered by our system 
two phrases were verb phrases  which could not have been learned as we focused
only on learning noun phrases and adjective phrases  there are four phrases that
are not semantically similar to any seed word for their shaping factors  for example 
the phrase garbled transmission is not semantically similar to any seed word for
the shaping factor communication environment  such as disturbance  static  radio
discipline  congestion and noise  finally  there are two phrases that should have been
learned by the system  but were not learned because at the time they were put into
the word pool  other words with higher scores were selected instead 
false positives  in the case of false positives  we looked into the report narrative and
the keyword that was found in the content to determine why the indication of the shaping
factor for the incident described in the report was incorrect  the different reasons that we
identified are as follows 
   too general keywords  we have observed a large number of false positives due to
keywords being too general  i e   keywords that have been extracted or learned for a
shaping factor but may appear in other phrases that are not related to that shaping
factor   for example  the keyword failure is a correct indicator of resource deficiency
as it appears in text like complete electrical failure  alternator failure  etc   but
when it appears in text like failure to follow air traffic control instructions  it does
   

fiabedin  ng   khan

not indicate resource deficiency as a shaping factor  we have identified    cases that
were caused by keywords being too general 
   concept present but not contributing to incident  another frequently faced
problem is that sometimes the concepts identified by the keywords are present in a
report  but they do not act as a shaper for the incident described in the report  for
example  in report         the reporter mentions that he was flying solo  which is
an indication of taskload  but the incident was due to physical environments  namely
snow and foggy weather  the fact that he was flying solo is merely mentioned as a
part of his description of the overall situation  the preprocessed version of this report
is also given in appendix e  in total  we observed    such cases 
   incorrectly learned words and phrases  there were six cases in which the semantic lexicon learner learned incorrect words and phrases that were not related to
the shaping factors to which they were assigned  for example  the framework incorrectly learned the word further for the shaping factor resource deficiency  and thus
a number of reports were mislabeled with resource deficiency 
   negative context  there were three cases in which the keyword appeared in a
negative context  which is typically signaled by a contextual valence shifter such as
no and hardly  polanyi   zaenen         for example  the keyword aircraft damage  an indicator of resource deficiency  appears in report        as no apparent
aircraft damage  which results in a false positive 
   hypothetical context  there was one case in which the keyword appeared in a
hypothetical context in which the reporter conjectures about a possible scenario  the
keyword single pilot  an indicator of taskload  appeared in report        as this
could happen to a pilot especially if he was single pilot  resulting in a false positive 
lessons learned  our error analysis provides valuable insight into the nature of the
problem as well as hints on how one should proceed in order to improve the performance
of the system  by analyzing the most frequent errors  we present the following lessons
learned from the analysis  first of all  it is more useful to learn high precision keywords
and phrases than general ones as the largest part of the false positive errors can be attributed
to having too general keywords  however  such high precision keywords and phrases are
more likely to have low frequencies  and hence one would have to adapt learning methods
to learn useful words and phrases from infrequent ones  second  one must take into account
the fact that relevant portions of the text may be larger than phrases  even going up to
clause or sentences  these cannot be identified by learning words or phrases  or n grams
of reasonable size  thus  more robust methods are needed that can learn useful sentence
fragments or useful sentence structures  finally  there are cases in which one cannot hope to
identify using methods that look for keywords  phrases  sentence fragments or even sentence
structures  i e   cases in which the cause of the incident has to be understood from the
discourse  and cases in which a concept is present in the description and yet plays no part
in the incident  much deeper analysis than simple bag of anything models are needed for
avoiding these two types of errors  which between themselves represent almost one third
of all errors in the analyzed subset  the former needs a method to distinguish relevant
   

ficause identification via weakly supervised semantic lexicon construction

sentences from irrelevant ones  for example  patwardhan and riloff        discuss a relevant
sentence classifier that is trained on a small set of seed patterns and a set of documents
marked as relevant and irrelevant that can be useful in this context  the latter problem
requires a discourse analysis method that  as discussed earlier  can model the conversations
and identify relations correctly  this shows that though it is possible to identify shaping
factors from these reports using words and phrases to a certain extent  much deeper natural
language techniques are needed to accurately identify the full range of causes 
    additional analyses
in this section we present the outcomes of a number of additional analyses that we performed
on our cause identification task and our approaches to this task  in section       we study
the relative difficulties of classifying the different shaping factors  in sections       and      
we show the learning curves of the semantic lexicon based approach and the learning based
approach respectively  i e   how the performances of these two approaches vary as they are
provided with different amounts of training data  finally in section       we discuss the
outcomes of the experiment conducted to determine if our modifications to the basilisk
framework is useful for learning general semantic categories 
      per class results
to get an insight into which of the classes are difficult to classify  we perform an analysis
of per class performance of two labeling schemes  the best heuristic based method  i e   the
occurrence heuristic using the lexicon learned by the modified basilisk framework   see the
first part of table    and the best learning based method  i e   the   fold svm classifiers
using unigrams  bigrams and the lexicon words as features   see the second part of table    
in conjunction with table    two classes stand out most prominently as difficult to classify
 illusion and taskload  both of these classes have very little representation in the training 
test and development sets  have a very small number of seed words  and result in very poor
performance by each of the approaches  the more easily identifiable classes were physical
environment  physical factors  resource deficiency and preoccupation  in which both the
labeling schemes had f measures better than      in general these classes had better
representation in the training  testing and development sets  and also had a reasonable
number of words and phrases in the semantic lexicon  we believe that this difference in
characteristics of the classes is a valuable insight that will be helpful in future work 
      lexicon learning curve
as mentioned in section        we have used a total of     seed words and phrases  at
a first glance  this number of seeds may seem large as far as bootstrapping experiments
are concerned  however  considering the fact that these     seeds are distributed over   
shaping factors  we only have an average of      words and phrases per shaping factor 
nevertheless  it would still be interesting to examine how cause identification performance
will be affected if we reduce the number of seeds for each shaping factor used by modified
basilisk in the bootstrapping process  as a result  we ran a set of experiments to measure
cause identification performance that uses the semantic lexicon learned by modified basilisk
when it is given different number of seed words  where the parameters specific to modified
   

fiabedin  ng   khan

table    per class performance results  the upper table shows the per class performance of
the occurrence heuristic using the lexicon learned by the modified basilisk framework 
the lower table shows the per class performance of   fold svm classifiers using unigrams 
bigrams and lexicon words as features 

shaping factor
attitude
communication environment
duty cycle
familiarity
illusion
other
physical environment
physical factors
preoccupation
pressure
proficiency
resource deficiency
taskload
unexpected
overall

tp
 
 
 
  
 
  
   
  
  
  
  
   
 
 
   

fn
  
  
  
  
 
   
  
  
  
  
   
   
  
 
   

tn
   
   
   
   
   
   
   
   
   
   
   
   
   
   
     

fp
  
  
 
  
 
  
  
 
  
  
  
   
 
  
   

precision
      
      
      
      
     
      
      
      
      
      
      
      
     
      
      

recall
      
      
      
      
     
      
      
      
      
      
      
      
     
      
      

f measure
      
      
      
      
     
      
      
      
      
      
      
      
     
      
      

shaping factor
attitude
communication environment
duty cycle
familiarity
illusion
other
physical environment
physical factors
preoccupation
pressure
proficiency
resource deficiency
taskload
unexpected
overall

tp
 
  
  
  
 
  
   
  
  
 
   
   
 
 
   

fn
  
  
  
  
 
   
  
  
  
  
   
   
  
  
   

tn
   
   
   
   
   
   
   
   
   
   
   
   
   
   
     

fp
 
  
  
  
 
  
   
  
  
 
   
   
 
 
   

precision
      
      
      
      
     
      
      
      
      
      
      
      
     
     
      

recall
     
      
      
      
     
      
      
      
      
      
      
      
     
     
      

f measure
      
      
      
      
     
      
      
      
      
      
      
      
     
     
      

   

ficause identification via weakly supervised semantic lexicon construction

basilisk are set as described in section        more specifically  we chose the top            
          and    seed words and phrases for each shaping factor  in terms of the frequency
in the entire corpus   and ran the modified basilisk framework for ten iterations using the
aforementioned parameters 
note  however  that not all shaping factors have the same number of manually selected
seed words and phrases  for example  illusion  taskload and unexpected have      and  
seed words and phrases respectively  whereas resource deficiency and physical environment
have    and    respectively  see the last column of table     hence  for those experiments
where the number of seeds used for each shaping factor exceeds the number of manually
selected seeds for a shaper  all the manually selected seeds were used  for example  since
unexpected has only three manually selected seeds  all of them were used in the experiments
in which at least three seeds are used for each shaping factor 
the occurrence heuristic was then used with the lexicons thus generated to evaluate
their performance on the test set  the resulting learning curve  in terms of f measure
on the test set of      reports  is shown in figure    in addition  since the baseline to
which we compare the performance is based on the seed words  the baseline learning curve
corresponding to each reduced seed words set is also shown  as expected  increasing the
number of seed words monotonically improves the f measure  however  the improvement
over the baseline is particularly small when fewer than seven seed words are used  and the
highest improvement is observed for seven seed words and phrases  from then on  adding
more seeds improves the overall performance  but the improvement over the baseline slowly
diminishes 
lexicon learning curve
  
  
  

f measure    

  
  
  
  
  
baseline
performance of learned lexicon

  
  
  
 

 

 

 

 
  
  
number of seeds

  

  

  

  

figure    variation of f measure with different number of seeds words per category 

   

fiabedin  ng   khan

svm learning curve
  
f measure on test set
  
  

f measure    

  
  
  
  
  
  
  
  
 

   

   

                   
number of training instances

   

        

figure    variation of f measure with different number of training reports 

      svm learning curve
as discussed in section      we hypothesize that the failure of the svm classifiers to perform better than the baseline is due to the scarcity of the training instances available to
the learner  one may argue that svm has been shown to work well for small datasets 
so  a natural question is  how much smaller will the training set be before we can see a
statistically significant drop in cause identification performance  to answer this question 
we plot a learning curve for the one versus all classification approach  using as features a
combination of unigrams  bigrams  and lexicon word features in a five fold cross validation
setting  which is the setting that yields the best performance in table    specifically  we
generated random subsets of the training sets of sizes                       instances  parameters  namely the percentage of features and the classification threshold  were chosen in the
same way as the original experiment as described in section      and the f measure was
evaluated on the entire test set  the curve is shown in figure    where each data point
is computed by averaging the results over five independent runs  as we can see  there is
a general trend of performance improvement with the increase in the number of training
instances  in addition  when trained on only     of the training set  we see that the cause
identification system started to perform statistically significantly worse than the system
that was trained on all of the available instances according to the stratified approximate
randomization test 
   

ficause identification via weakly supervised semantic lexicon construction

      general usefulness of our modifications to basilisk
in order to test whether the modifications we made to the basilisk framework are useful
to lexicon learning in general  we added two general categories to the shaping factors in
the bootstrapping experiments  namely people and equipment  these two categories were
added because  firstly  words and phrases added to these categories would be easy to verify
 i e   whether they are words or phrases representing people or equipment   and secondly 
they are similar to the original context in which the basilisk framework was originally evaluated  i e   learning words in the categories building  event  human  location 
time  and weapon from terrorism reports   these two additional categories were added
to the seed lexicon described in section        which was then bootstrapped by running original basilisk and modified basilisk separately for ten iterations  with the parameters specific
to these basilisk frameworks set in the same way as described in section        the seed
words for these two categories were selected in the same manner as done by thelen and
riloff         i e   the phrases in the corpus were sorted by frequency and the five most
frequent phrases belonging to these categories were manually identified  below are the seed
words used in the two categories 
 people  captain  controller  first officer  rptr  passenger
 equipment  aircraft  airplane  collision avoidance system ii  engine  auto pilot
in order to verify which of the words and phrases learned by the two frameworks correctly
belong to the assigned category  the first author and a computer science graduate student
not affiliated with this research went over the generated lexicons  appendices f and g
show the lexicons generated by original basilisk and modified basilisk respectively  to
facilitate analysis  we divide the words and phrases in each generated lexicon into three
categories      those that are determined as correct by both human judges      those that
are determined as correct by only one judge  and     those that are determined as incorrect
by both judges 
for the lexicon generated by original basilisk  we find that in the category people     of
the    words and phrases were determined as correct by both judges  and   were determined
by exactly one of the judges as correct  in the category equipment    of the    words and
phrases were correct according to both judges  and    were correct according to exactly one
of the judges  on the other hand  for the lexicon generated by modified basilisk  we find
that in the category people     of the    words and phrases were determined as correct by
both judges  and   were determined by exactly one of the judges as correct  in the category
equipment     of the    words and phrases were correct according to both judges  and  
were correct according to exactly one of the judges  this comparison clearly shows that
the modifications that we made to the basilisk framework are not specific to this particular
task  rather  these modifications have improved the lexicon building performance in general 
    summary of conclusions
we end this section by providing a summary of the major conclusions we draw from the
experiments 
   

fiabedin  ng   khan

 our heuristic approach to cause identification  which labels a report using the occurrence heuristic in combination with the words and phrases automatically acquired
using our modified basilisk framework  surpasses the performance of the baseline system  which applies the occurrence heuristic in combination with the seed words and
phrases manually identified from the training documents  the difference in f measure
between these two systems is statistically significant according to both mcnemars test
and the stratified approximate randomization test  this suggests that the words and
phrases in the semantic lexicon learned via modified basilisk are relevant and effective
for cause identification 
 adding the learned lexicon words to an n gram based feature set for training svm
classifiers is beneficial for cause identification only if the training set is sufficiently
large  as exhibited by the statistically significant increase in f measure  this provides
suggestive evidence that the words and phrases in the semantic lexicon learned via
modified basilisk are relevant and useful features for cause identification 
 when used in combination with the occurrence heuristic  the semantic lexicon learned
by our modified basilisk framework offers significantly better performance for the
cause identification task than the one obtained using the original basilisk framework  additional experiments reveal that modified basilisk is not only useful for
cause identification  but it also offers performance superior to original basilisk when
bootstrapping general semantic categories such as people and equipment 
 among the three multi class multi labeled text classification approaches we experimented with  one versus all works significantly better than metalabeler and pruned
sets for cause identification  transductive learning  when used in combination with
the one versus all approach  significantly hurts performance  suggesting that unlabeled data cannot be profitably exploited given the fairly small amount of labeled
data 
 our best system achieves an f measure of around        which indicates that cause
identification is a difficult task  and that there is a lot of room for improvement  to
provide directions for future research  we performed an analysis of the errors made
by the best performing system  in particular  we found that performance is currently
limited in part by several factors  first  there are a number of cases in which the
relevant text indicating the responsible shaping factor may be larger than phrases 
second  indicators for a shaping factor may be mentioned in a report without influencing the incident described in the report  finally  there are some cases in which the
shaping factors cannot be identified by simply looking at the words  phrases or even
sentence fragments  much deeper analysis is required in these cases 
 increasing the number of seed words and phrases employed by modified basilisk improves cause identification performance  but the marginal performance improvement
for each added seed diminishes with successive additions  in other words  these results
seem to suggest that using more seed words will be unlikely to improve much over the
current performance  rather it would be more promising to start with a small number
of high frequency seeds and improve upon the bootstrapping process 
   

ficause identification via weakly supervised semantic lexicon construction

 the learning curve plotted for the one versus all classification approach shows that
cause identification performance increases with the number of training instances  in
particular  when trained on only     of the training set  we see that the resulting cause
identification system performs statistically significantly worse than the one trained on
all of the available instances 
overall  while our approaches rely on automatically learned lexicon words and phrases
that are not adequate for cause identification  they are relevant for the task  as mentioned previously  their use is motivated by the labor intensive procedure that the nasa
researchers employed in manually identifying seed words and phrases for each shaping factor  posse et al          our work represents one of the first attempts to tackle this cause
identification task  and we believe that the use of simple features is a good starting point
and establishes a baseline against which future studies on this problem can be compared 
the main take home message from this research is that though it is possible to solve
the problem that we set out to solve  namely automated cause identification  by learning
relevant keywords or sentence fragments and other suitable bag of words models  there
remains a significant portion of the data that remains unlabeled or mislabeled through
these methods  to match the performance level achieved in other topical text classification
tasks  much deeper linguistic analysis like relevant sentence detection and discourse analysis
methods like identifying disagreements  disputes and hostile attitudes will be needed  this
lesson should be the cornerstone of further research in this area 

   related work
in this section  we describe some other works related to our research  in particular  our
discussion focuses on causal analysis as well as approaches to semantic lexicon construction and text classification  and is organized as follows  first  we discuss causal analysis
as it has appeared in different fields  second  we discuss the different semantic lexicon
learning algorithms  third  we discuss works that deal with extraction pattern learning 
fourth  we describe different algorithms for unsupervised word clustering and thesaurus
building  finally  we include a discussion of related work on multi class multi labeled text
classification 
causal analysis  major research on causality has been performed mainly in the fields
of philosophy and psychology  in the field of philosophy  seminal works in causality have
been conducted by hume               who provides one of the most influential definitions
of cause as an object followed by another  and where all the objects  similar to the first 
are followed by objects similar to the second  or  in other words  where  if the first object
had not been  the second never existed  this has been the basis of later  much stronger
definitions of causation  e g   lewis        ganeri  noordhof    ramachandran        
notable investigations on causation in the field of psychology include those by cheng        
who defines causation in terms of the probabilistic contrast model  griffiths and tenenbaum
        who discuss learning about cause and effect relationships using causal graphical
models  and halpern and pearl         who provide explanations of causality by means
of structural equations governing random variables representing events  although these
   

fiabedin  ng   khan

works provide important background and definitions contributing to the understanding of
causality  in order to identify causes from naturally written text we must turn to nlp 
in the field of nlp  there is little work on cause identification similar to our problem 
research on causality focuses mainly on identifying causal relations between two sentence
components  for instance  girju        describes a method for automatically discovering
lexico semantic patterns that refer to causation  in particular  she focuses on the explicit
intra sentential pattern hn p  verb n p  i  where verb is a simple causative  she also shows
how these patterns can be used to improve the performance of a system for answering
cause effect type questions  khoo et al         use graphical pattern matching to identify
causal relations from medical article abstracts  they use hand crafted patterns that are
matched against the parse trees of the sentences  the subtrees of the parse tree that match
the patterns are extracted as causes or effects  similarly  garcia        uses hand crafted
extraction patterns to identify causal relations from sentences in the french language  the
limitation of these approaches is that they focus on identifying causal relations from the
same sentence  whereas our reports are multi sentence discourses 
grishman and ksiezyk        use domain modeling  discourse analysis and causal inference to find cause effect relations between events leading up to equipment malfunctions
from short equipment failure reports  more specifically  they first apply syntactic analysis
to produce parse trees for the sentences in the reports using an augmented context free
grammar  then they apply semantic analysis to map     verbs and syntactic relations into
domain specific predicates and relations and     noun phrases into references to components
in the domain model  finally  they apply discourse analysis to these predicates to construct
a time graph showing the temporal and causal relationships between the elementary facts 
the temporal relations are derived from text structures and words  e g   when  then 
etc   and the order of appearance in text  but the causal relations are determined by querying a simulation model of the equipment that is built using domain knowledge  specifically 
each possible causal link is posed as a query to the model to test if the relation holds 
overall  their method relies heavily on the domain model of the equipment being studied 
and their research focuses on only one specific piece of equipment 
nasas own research on identifying causes of incidents from the report narratives have
been performed by posse et al          who describe a specific experiment in which they
brought together experts to manually analyze the report narratives and identify words 
phrases and expressions related to each of the shaping factors  as mentioned earlier  later
work by ferryman et al         take these manually extracted expressions as ground truth
and compare the anomalies described in the reports to the shaping factors derived from
applying these expressions to the same reports  specifically  they do not attempt to learn
these expressions automatically  rather  they focus on finding possible correlations between
the shaping factors and the anomalies 
algorithms for semantic lexicon learning  a number of semantic lexicon learning
algorithms follow an iterative bootstrapping approach  starting from a small number of
semantically labeled seed words  roark and charniak        propose a method of constructing semantic lexicons based on co occurrence statistics of nouns in conjunctions  lists
and appositives  they start with a small seed nouns list  and iteratively add similar words
to that list  the word similarity is measured by the ratio of how many times the word occur
   

ficause identification via weakly supervised semantic lexicon construction

together with a seed word to the number of times the word appear in the corpus  after
construction  they rank the words by a log likelihood statistic  dunning         however 
due to the general brevity of the reports  such co occurrences and lists are rather few in our
corpus  and it is more useful for us to use context based similarities like thelen and riloff
        they describe their basilisk framework for learning semantic lexicon using extraction patterns as features  the apply a weakly supervised bootstrapping approach in which
they start from a small manually constructed seed lexicon and iteratively add semantically
similar words to it  this  described in more detail in section      has been the basis for our
lexicon learning approach 
a number of improvements to the basilisk framework  and more generally to bootstrapping approaches  have been proposed  in the basilisk framework  the number of iterations
is a parameter that has to be chosen arbitrarily  rather than making an arbitrary choice 
yangarber        proposes a method for detecting termination of unsupervised semantic
pattern learning processes  the method requires that the documents must be labeled as
relevant or irrelevant  since such information is not available for our corpus  it is not useful for us  curran  murphy  and scholz        suggest an improvement over traditional
bootstrapping methods by discarding words and contexts that appear to be related to more
than one category  in order to minimize semantic drift and enforce mutual exclusion  on
the other hand  we handle such cases by comparing the conditional probabilities for the
different categories to which such words can belong  zhang  zhou  huang  and wu       
present bootstrapping with the graph mutual reinforcement based bootstrapping  gmr 
 hassan  hassan    emam         a modification of the basilisk method  similar to us 
they explore using n grams to capture context  but they use a different set of pattern and
word scoring formulas  for learning multiple categories simultaneously  they introduce a
scoring system based on entropy of a pattern  they report better results than basilisk on
the muc   dataset  see sundheim        
among non bootstrapping approaches  ando        presents a new method of constructing semantic lexicons from an unannotated corpus using a set of semantic classes and a set
of seed words and phrases for each semantic class  she uses spectral analysis to improve
the feature vectors by projecting the useful portions of the vectors into a subspace and removing the harmful portions of the vectors  the resultant feature vectors are then used
by a centroid based classifier using cosine similarity measure to label the words  avancini 
lavelli  sebastiani  and zanoli        take a classification approach to semantic lexicon
construction  they cast the problem as a term  meaning both words and phrases  categorization task  dual of the document categorization task   and similar to the bag of word
model  they represent the terms as bag of documents  they use a variation of the adaptive
boosting algorithm  adaboost m h kr   which is trained on a small seed lexicon and then
used to classify the noun terms in the corpus to zero  one or more semantic categories 
algorithms for learning extraction patterns  our approach to semantic lexicon construction uses extraction patterns as features  and here we present some methods that aim
to improve the extraction pattern collection process  riloff        describes the autoslog ts
system that learns extraction patterns from untagged text  however  it needs a pre classified
corpus that have the text classified as relevant or irrelevant  as we mentioned earlier  we do
not have access to such information  phillips and riloff        present a method of boot   

fiabedin  ng   khan

strapping algorithm to learn role identifying nouns  which are then used to learn important
extraction patterns  and also role identifying expressions  however  their focus is mainly on
identifying the roles of words in events 
patwardhan and riloff        provide another extraction pattern learning approach
using relevant regions  they require the documents to be pre classified into relevant and
irrelevant documents  using a small set of seed patterns  they classify the sentences in these
documents into relevant and irrelevant sentences  then semantically appropriate extraction patterns are learned using a semantic affinity metric and separated into primary and
secondary patterns  this approach is also not directly usable to us due to the unavailability
of documents pre classified into relevant and irrelevant categories 
recently  the internet has increasingly been used in natural language research  patwardhan and riloff        use the autoslog ts system  riloff        to learn domain specific
extraction patterns by processing documents retrieved by querying the web with selected
domain specific words  using the web is an interesting and promising enhancement and  as
mentioned in section    we intend to extend our work using the google corpus  brants  
franz        
algorithms for thesaurus building and unsupervised word clustering  another
area of research that is very closely related to semantic lexicon learning is thesaurus building 
building a thesaurus requires discovering groups of semantically similar words  though it
stops short of assigning semantic class labels to the words  thus it shares the problem of
measuring semantic similarity and grouping similar words with the semantic lexicon building
task  here we discuss several approaches to the thesaurus building task 
clustering has been used extensively in thesaurus building  mostly because of its unsupervised nature and ability to handle large volumes of data  the seminal work in this
direction has been by pereira  tishby  and lee         who present an unsupervised method
for soft clustering of words using distributions of the words in different contexts  this approach generates overlapping word clusters  grouping words based on the contexts that they
appear in  baker and mccallum        use pereira et al s distributional clustering technique to perform feature space reduction for supervised classification with nave bayes by
using the clusters as features  lin and pantel        present their approach of generating a
collection of sets of semantically similar words  or concepts  using their clustering method 
unicon  with dependency relations as features  pantel and lin        present another
clustering approach  clustering by committee  using contextual features with point wise
mutual information as feature values  that they compare as better than lin and pantels
results  rohwer and freitag        present a clustering based automatic thesaurus building
process from an unannotated corpus  they propose an information theoretic co clustering
algorithm that groups together highly frequent words into clusters of similar part of speech
category  then they pursue an additional process  lexicon optimization  to grow the lexicon
by assigning the less frequent words to their most likely clusters 
among non cluster based methods  davidov and rappoport        present an unsupervised method of discovering groups of words that have similar meanings  they achieve
this by     identifying high frequency words and content words      identifying symmetric
lexical relationship patterns  and     applying a graph clique set algorithm to generate word
categories from co occurrence information of the content words in the symmetric patterns 
   

ficause identification via weakly supervised semantic lexicon construction

concentrating on the performance issues that plague attempts to build thesaurus from a
large corpus  rychly and kilgarriff        present two methods of improving performance
of general context based thesaurus building algorithms  the first method is to compare
only those word pairs that have some context in common  the second method is to use the
heuristic of removing those contexts that are too general  i e   contexts that have more than
a certain number of distinct words   in our research  we have adopted the second method
 see section       they also applied a partitioned sequential approach to the construction
process  though thesaurus building does not usually require an annotated corpus or a set
of seed words and phrases  it is not directly applicable to the task of growing a semantic
lexicon where we have to learn words in specific semantic categories  this is because the
method has no control over which words are being learned and which classes the discovered
word groups belong to  it may be possible to adapt this method to that of semantic lexicon
growing by classifying the word groups into the semantic classes by using the seed words and
phrases  however  the method has to be extended to extract noun and adjective phrases 
algorithms for multi class multi labeled text classification  as mentioned previously  cause identification  when cast as a text classification problem  is a multi class
multi labeled text classification problem  since there are    shaping factors in total and
each document may be labeled with more than one shaping factors  there are several
popular approaches to solving a multi class multi labeled text classification problem  the
first  and one of the approaches followed in this research  is to independently train a binary
classifier for each class  and apply each classifier on a test instance in isolation  in our
case  the underlying learner is support vector machines  joachims         godbole and
sarawagi        suggest a number of improvements to this scheme  namely  including class
labels suggested by a preliminary set of classifiers as features  removing negative examples
too close to the classification hyperplane  and selectively removing some classes from the
one versus others classifications scheme  another notable method  followed by tsoumakas
and vlahavas        and read et al          is to treat each unique set of labels as a
new label  thus converting the problem to a multi class single labeled one  their works
differ from each other in the construction of the new labels  the former  called random
k labelsets  or rakel  builds an ensemble of classifiers by randomly sampling label sets
of size k  whereas the latter adopts the method of filtering observed label sets by minimum
support  tang et al          on the other hand  take a different approach  they train one
classifier that predicts the number of labels that a test instance would have  and then choose
that many labels for that instance based on output of another classifier that ranks the labels
by likelihood for that instance  all these works use svm as their underlying learner  in
addition  all these approaches make the assumption that the classes are correlated to a high
degree  however  an analysis of our dataset does not present evidence of such a strong
correlation  of the     documents with multiple labels in the test set  there are    unique
label sets  of which only seven have a frequency of at least five  thus increasing the number
of labels would only aggravate an already imbalanced class distribution 
among other approaches  we mention two systems that use probabilistic generative models  mccallum and nigam        propose a system that starts with a small set of keywords
and unlabeled documents  and learns a nave bayes classifier in a bootstrapping process from
the keyword induced labels by using hierarchical shrinkage and expectation maximization
   

fiabedin  ng   khan

on a held out data set  ueda and saito        present another generative model called parametric mixture models  which treats multi labeled text as a parametric mixture of words
relevant to each label  their work is closely related to latent dirichlet allocation  blei 
ng    jordan         the generative models usually assume that a document related to a
particular topic would have a high frequency of words related to that topic  in our research 
the documents are mostly devoted to description of the event that occurred  and the cause
of that event is only mentioned briefly  this makes generative models less suitable for the
task at hand as generative models would more likely generate models related to the events
and not the causes  a more comprehensive review of different approaches to multi class
multi label text classification can be found in the work of tsoumakas and katakis        

   conclusions
we have investigated two approaches to the cause identification task  the goal of which is to
understand why an aviation safety incident happened via the identification of the causes  or
shaping factors  that are responsible for the incident  both approaches exploit information
provided by a semantic lexicon  which is automatically constructed via thelen and riloffs
       basilisk framework augmented with our three algorithmic modifications  namely  the
use of a probabilistic similarity measure  the use of a common word pool  and the enforcement of minimum support and maximum generality constraints for words and extraction
patterns  and one linguistic modification  the use of n gram based extraction patterns  
the heuristic based approach labels a report by employing the occurrence heuristic  which
simply looks for the words and phrases acquired during the semantic lexicon learning process in the report  the learning based approach labels a report by employing inductive
and transductive support vector machines to learn models from reports labeled with shaping factors  our experimental results indicate that the heuristic based approach and the
supervised learning approach  when given sufficient training data  both significantly outperform our baseline  which  motivated by nasas work  labels a report simply by using
the occurrence heuristic in combination with a set of manually identified seed words and
phrases  more importantly  results of the heuristic based approach indicate that our modifications to the original basilisk framework are beneficial as far as cause identification is
concerned  and results of the learning based approach indicates the usefulness of the lexicon
words when they are used in combination with unigrams as features for training an svm
classifier  overall  what we set out to prove was that it is possible to automate the cause
identification task by manually analyzing a small number of reports and using the information thus generated to train machine learning methods to identify the shaping factors of the
rest of the reports  our experiments have been able to prove the feasibility of this approach 
and also the usefulness of learning a semantic lexicon and using the words in it as features 
nevertheless  our best system achieves an f measure of around        which indicates that
cause identification is a difficult task  and that there is a lot of room for improvement  in
particular  our analysis of the errors made by the best system on     randomly chosen test
documents provides valuable insights into the task as well as directions for future research 
from our experience from this current research  we intend to extend our work in the
following directions  first and foremost  we plan to extend our approach to handle text
fragments bigger than phrases  second  in order to improve the quality of labeling  we
   

ficause identification via weakly supervised semantic lexicon construction

propose to work on improving the lexicon learning performance further by using different
semantic similarity measures  for instance  we would like to study the performance of the
semantic similarities and weighting functions suggested by curran and moens        in
our context  third  we plan to use more thoroughly normalized text for better parsing
and tagging  as well as relevant region information  ko  park    seo        patwardhan
  riloff         fourth  we propose to augment the semantic lexicon  specifically by using
the google n grams corpus  brants   franz        to extract frequent n gram patterns
for words  fifth  we propose to explore other more recent lexicon construction methods
like unsupervised word clustering  pantel   ravichandran         spectral analysis  mutual
exclusion bootstrapping  co clustering and exploiting symmetric patterns  finally  in order
to handle the shaping factors that are difficult to identify from words occurring in the
reports  we propose to employ much deeper analysis of the text at the semantic level  we
have also taken the step of making our annotated incident reports publicly available  and
we hope that we can stimulate research on this under investigated problem in the nlp
community 

acknowledgments
the authors would like to thank the anonymous reviewers who provided us with comments
that were invaluable in improving the quality of the paper  this research was supported
in part by nasa grant nnx  ac  a  any opinions  findings  conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the
views or official policies  either expressed or implied  of nasa 

   

fiabedin  ng   khan

appendix a  seed words
below are the seed words manually extracted from the     reports in the training set  see
section       for details  
shaping factor
attitude
communication
environment
duty cycle
familiarity
illusion
other
physical
environment

physical factors
preoccupation
pressure
proficiency

resource
deficiency

taskload
unexpected

seed words
get homeitis  attitude  inattentiveness  get thereitis  complacency  overconfidence  sarcastic  inattention
disturbance  static  radio discipline  congestion  noise
   hour duty day  inadequate rest  last of   legs  heavy flying  reduced
rest  all night flight     hour day  red eye  ten leg day  all night
familiarization  not familiar  new  first departure  unfamiliar  unfamiliarity  very familiar  low time  first landing
bright lights
noise abatement policy  disoriented  confused  medical emergency 
economic considerations  disorientation  drunk passenger  confusion
cold  clouds  dark  setting sun  sun glare  obscured  visibility  hazy
stratus  birds  fog bank  solid overcast  snow  weather  rime  gust  low
weather  surface winds  jet blast  lightning  sea gulls  high ceilings 
hot  tailwind  chop  very dark  sea gull  winds  scattered  high tailwinds  extremely dark  too bright  icing  turbulence  rpted wind 
terrain  bird strike  crosswind  thunderstorm  glare  reduced visibility  high flying birds  fog  severe winter weather  cloud  ice
very tired  hypoxia  tiredness  tired  fatigued  disorientation  fatigue  no rest
distracted  preoccupied  mental lapse  busy  distrs  distraction 
attention  inattention  absorbed
hurry  running late  pressure  low on fuel  fuel considerations  behind
schedule  late  peer pressure  under pressure  rushing
mistakes  mistaken  new hire  inexperience  forgotten  less than    
hours  newly rated  training  recent pilot  inadvertently  bad turn 
misinterped
loose connection  erratic  blown  overheated  bang  collapse  no idea 
unavailable  placarded  crack  out of service  damage  smoke  inoperative  failure  leak  deferred items  communication failure  loss 
unreliable  fdrs problem  bump  shaking  master caution  inadequately lighted  unreadable  disconnected  malfunction  shudder  absence  hazard  inaccurate  unflagged  fire  broken  fluctuations 
compressor stall  deferral  unusable  wrong  intermittently  warning 
discrepancies  faulty  deferred  intermittent  missing
single pilot  solo
unexpected  suddenly  unforecast

   

ficause identification via weakly supervised semantic lexicon construction

appendix b  sample lexicon words learned by modified basilisk
below are the semantic lexicon words learned by the modified basilisk framework in the
first two iterations 
shaping factor
attitude
communication
environment
duty cycle
familiarity
illusion
other
physical
environment
physical factors
preoccupation

pressure
proficiency
resource
deficiency
taskload
unexpected

new words

aligned  fairly new  more familiar
initial confusion  minimum fuel emergency  misunderstanding 
weather emergency
trws  conflict message  cumulonimbus  large cells  numerous thunderstorms  occasionally severe  thunderstorm cells  weather buildups 
weather cell  weather en route
first factor
adequate attention  as much attention  close attention  close enough
attention  crew attention  enough attention  much attention  proper
attention  strict attention  very close attention

different  amiss  awry  obviously wrong  resulting loss  seriously
wrong  slight loss  temporary loss  terribly wrong  very wrong

   

fiabedin  ng   khan

appendix c  sample lexicon words learned by original basilisk
below are the semantic lexicon words learned by the original basilisk framework in the first
two iterations 
shaping factor
attitude

communication
environment

duty cycle

familiarity

illusion

other

physical
environment

physical factors

preoccupation

new words
air traffic control security  aileron yoke displacement  anomalous vfr omni directional radio range information  assured tfr
avoidance  betrayal  concern and urgency  forgetting air carrier x 
magnified problem  operational complacency  unseen and unknown
turbulence
     noise  bna runway    approach plate  lightspeed   k
noise  non  noise  overspd bell  active noise  clearance delivery
transmission  left engine stall  static and emergency locator transmitter  stuck trim or elevator movement
   plus    layover    different time frames    back to back  continuous duty trips    hour break     minutes    hour    minute flight time
day  pacific daylight time departure  tpa flight  xc   departure 
scheduled    leg continuous duty
s partial unfamiliarity  s perceived familiarity  command familiarity  command unfamiliarity  blue panel indication light  dispatch
work desks  generally unfamiliar  inexperience and unfamiliarity  new
everyday  past experience and familiarity
      nautical mile ssw          point      off end     feet side  elmendorf required use  about     mile downwind  airspace e  foxtrot
intersection  lateral boundaries  mile right
misinformation  flight management system heading anomalies  confusion conflict  disoriented and confused  intense panic  micro sleep 
miss numerous times  mistake inconvenience  note closure problem 
start terror
mht class celsius  strato cumulus  thur morning  clouds underneath  compacted snow and ice  fair weather cumulus  next morning
weather  puffy cumulus clouds  thin scattered clouds  well developed
cumulus clouds
hypoxia carbon monoxide  minimum equipment list          
basically tired  cardiac distress  indicating system problem  internal
bleeding  interrupted fuel flow  oncoming seizure  stress overload 
upper respiratory problems
captain and first officer attention  terminal radar approach control facility distraction  close enough attention  consequently my attention  good enough attention  lip service  mind or attention  much
mind  real attention  real close attention
continued on next page

   

ficause identification via weakly supervised semantic lexicon construction

shaping factor
pressure

proficiency

resource
deficiency

taskload

unexpected

new words
minimum equipment list pressure  consistent answer  elevator pressure  intense pressure  part   coordinated universal time  repercussion  right engine pressure  significant pressure  slow gear  wheel
pressure
  p school  cl   ground school  basic flight training  hard lesson 
initial and annual proficiency training  occurrence and strive  rating
training  several military flying clubs  situation event  time limited
simulator sessions
air traffic control loss  altitude deviation loss  apparently inoperative  either inoperative  even a reexamining  intermittent or inoperative  known traffic conflict or loss  observed loss  recently a los 
thankfully accurate
        turboprop  a    type aircraft  aviat husky a  two place
tail dragger aircraft  cessna     type aircraft  cessna model
    type aircraft  l          lga mht flight  mcdonnell douglas
md    more solo cross country flts  solo cross country privileges
significant    jolt  approximately      sec  choppy and aircraft  consistently moderate  contributing workload factor  industry issue  just
as severe  rapid and immediate  real cushion

   

fiabedin  ng   khan

appendix d  additional stratified approximate randomization tests

mlw

olw

svm u

svm ub

svm l

svm ul

svm ubl

svmt u

svmt ub

svmt l

svmt ul

svmt ubl

svm  u

svm  ub

svm  l

svm  ul

svm  ubl

methoda
sw
mlw
olw
svm u
svm ub
svm l
svm ul
svm ubl
svmt u
svmt ub
svmt l
svmt ul
svmt ubl
svm  u
svm  ub
svm  l
svm  ul
svm  ubl

sw

to ascertain the statistical significance of the difference between the f measure scores of the
different report labeling methods  we performed the stratified approximate randomization
test with       shuffles between all pairs of the results of experiments   through   in table   
the table below shows if the method in the column is statistically significantly better than
the method in the row at the level of p         as before  statistical significance and
insignificance are denoted by a x and an x  respectively 

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
 

a  legend  sw   occurrence heuristic using seed words  mlw   occurrence heuristic using modified
basilisk lexicon  olw   occurrence heuristic using original basilisk lexicon  svm u   svm using
unigrams  svm ub   svm using unigrams and bigrams  svm l   svm using lexicon words  svm ul
  svm using unigrams and lexicon words  svm ubl   svm using unigrams  bigrams and lexicon
words  svmt u   transductive svm using unigrams  svmt ub   transductive svm using unigrams
and bigrams  svmt l   transductive svm using lexicon words  svmt ul   transductive svm using
unigrams and lexicons  svmt ubl   transductive svm using unigrams  bigrams and lexicon words 
svm  u     fold svm using unigrams  svm  ub     fold svm using unigrams and bigrams  svm  l
    fold svm using lexicon words  svm  ul     fold svm using unigrams and lexicon words  svm ubl     fold svm using unigrams  bigrams and lexicon words 

   

ficause identification via weakly supervised semantic lexicon construction

appendix e  sample preprocessed reports
report acn       
returning to waukegan regional airport from practice area located between      mile
w of the airport  flying solo as a student pilot  at about      feet mean sea level visual
flight rules  cloud area about   mile w of the airport obscured view ahead so i reduced
altitude to proceed visual flight rules and returned to      feet after passing the thin
cloud line  the area to the n  containing fix references for airport location  was shrouded in
clouds and in fog at ground level  the same was true of the lake michigan shoreline to the
e  the ground was also substantially snow covered  although the airspace over the airport
was undoubtedly clear  as was the practice area  orientation to the field was lost to me 
i climbed to      feet to increase the overview  without benefit  returning to      feet 
i flew to what i believed to be n of the airfield to landfall the airport  i must have been
to the s  however  and proceeding s i flew into ord class b airspace  coinciding to being
lost  i contacted waukegan tower  not then realizing that i had flown as federal aviation
regulation s as i had  i was directed to contact ord approach on the frequency given  and
beginning with ord approach was vectored back to waukegan airport  frequency changed to
tower control and blessedly cleared to land  the time lost was between   hour    minutes
and       hours 
report acn       
the following event occurred while repositioning  by taxi  from the w side to the s
side of isp airport  i initially contacted longitude island tower asking permission to repos
from the w side to the ops base operations office of the tower  the s side   the controller
replied start taxi via taxiway w up to but hold short of runway    i read back the
instructions stating to start taxi via taxiway w holding short of runway    as i was taxiing 
there was an aircraft on taxiway w holding short of runway    performing a run up  the
controller asked if i was able to get around the aircraft  i replied that i was able  the
controller then said use caution taxiing around the aircraft and cross runway    as i was
taxiing across runway    i noticed an aircraft on short final for runway    i was clear of
the runway before the aircraft touched down  the controller then came on the frequency
and said you were instructed to hold short of runway    i replied you cleared me across
runway    the controller said call the tower when you park  i replied roger  i will call
you when i park  i called and talked to the controller a few minutes later and he said you
were instructed to hold short of runway    again i told him that he had cleared me across
the runway  i feel that pilots and controllers need to listen to each other and decipher what
is said before acting on it 

   

fiabedin  ng   khan

appendix f  lexicon learned by original basilisk for categories people
and equipment
the following table shows the words and phrases learned by the original basilisk framework
for the categories people and equipment  see section        
category
people

equipment

new words
agreed by both judges as correct  abq tower procedure specialist  acn        reporter  afsfo  avp tower specialist  air route
traffic control center specialist  air traffic control facility reps 
bdr tower specialist  bhm control  buf field operations officer 
cae tower specialist  chicago quality control  dfw maintenance
manager  flight service station dispatcher  sfolm the captain 
sii program manager  stearman pilot  tlh supervisor  bur local
controller  casino manager  cos air traffic control chief  flight test
engineers  local balloon repairman  outbound captain and first officer  repair facility and pilot  shift boss  spokesperson  station management individual  technician desk  tower supervisor manager
identified by one judge as correct  flight standards district
office orl  again maintenance supervisor  approach controller verbatim  freighter aircraft and approach  him and tower  passenger and
fatigue
agreed by both judges as incorrect  acn         acn        
at  aircraft  b            srm  emb service manual  non air
carrier aircraft  rptr acn         rptr acn         rptr
acn         rptr acn         rptr acn         rptr acn
       cabin or company  other aircraft center  reliable research resources
agreed by both judges as correct  collision avoidance system ii    distance measuring equipment screen  collision avoidance
system ii b         collision avoidance system ii ehsi  collision
avoidance system ii ivsi display  collision avoidance system ii
missed approach point page  collision avoidance system ii rr 
continued on next page

   

ficause identification via weakly supervised semantic lexicon construction

category

new words
identified by one judge as correct  collision avoidance system ii vsi overlay  resolution advisory  stopped and aircraft  collision avoidance system ii stop climb  alert  collision avoidance
system ii traffic    then climb  advisory  collision avoidance system ii traffic   traffic  aural warning  collision avoidance system
ii   mile circle  collision avoidance system ii   mile scale  collision avoidance system ii   mile scale  collision avoidance system ii popup traffic  collision avoidance system ii resolution
advisory alerts  collision avoidance system ii resolution advisory
climb priority  collision avoidance system ii resolution advisory
climb warning  collision avoidance system ii resolution advisory
signals  collision avoidance system ii resolution advisory zone 
collision avoidance system ii resolution advisory traffic advisory
alert  collision avoidance system ii resolution advisory traffic advisory alerts advisories  collision avoidance system ii resolution
advisory altitude deviation  collision avoidance system ii traffic
advisory and resolution advisory alerts  collision avoidance system ii windshear warning  collision avoidance system ii advisory alert  collision avoidance system ii advisory alert and warning 
collision avoidance system ii warning and aircraft
agreed by both judges as incorrect  collision avoidance system ii    oclock and       to   mile  collision avoidance system ii resolution advisory climb  command  collision avoidance
system ii resolution advisory area  collision avoidance system
ii resolution advisory climb or descent  collision avoidance system ii resolution advisory data tag  collision avoidance system
ii resolution advisory descent  collision avoidance system ii resolution advisory green band target  collision avoidance system ii
resolution advisory increase climb  collision avoidance system ii
resolution advisory maneuvering  collision avoidance system ii
resolution advisory messages  collision avoidance system ii resolution advisory recovery procedure  collision avoidance system
ii resolution advisory requirement  collision avoidance system ii
resolution advisory requiring climb  collision avoidance system
ii resolution advisory resolution  collision avoidance system ii
traffic advisory notification  collision avoidance system ii traffic
advisory resolution advisory aircraft  collision avoidance system
ii traffic advisory resolution advisory event  collision avoidance
system ii action requirements  collision avoidance system ii advice 
collision avoidance system ii advisories and instructions  collision
avoidance system ii caution  collision avoidance system ii quit

   

fiabedin  ng   khan

appendix g  lexicon learned by modified basilisk for categories people
and equipment
the following table shows the words and phrases learned by the modified basilisk framework
for the categories people and equipment  see section        
category
people

equipment

new words
agreed by both judges as correct   first officer   my first officer    first officer  cp  captain  captain rptr  captain trainee 
co captain  co pilot  first officer  first officer      initial operating experience captain  paxs  pilot flying and first officer 
potomac controller  rpting captain  rpting first officer  rpting pilot  rptr captain  rptr pilot  s o  zoa supervisor  air
carrier y pilot  aircraft x pilot  aircraft commander  all the passenger  analyst  and first officer  baron pilot  biplane pilot  controller 
facility person  first observer  flight attendant      flight attendants
and passenger  flying captain  forward observer  passenger  passenger and crew  passenger and flight attendants  right seat pilot  second observer  sic  specialist  student captain  supervisor controller 
tower controller  tower operator  training pilot
identified by one judge as correct  rptr  gate and passenger 
so first officer
agreed by both judges as incorrect  departure and departure 
neither the captain  which clrly
agreed by both judges as correct      auto pilot   
  auto pilot    autoplts  autoflt  autothrottle 
autothrottle and auto pilot  autothrottles  autothrottles and auto pilot  autothrust  auto pilot
     auto pilot      auto pilot b  auto pilot and autothrust  auto pilot and pms  auto pilot and throttles  autopilot autothrottles  cessna      collision avoidance system
ii system  engs     and      aircraft abcd  aircraft auto pilot 
aircraft engine  allowed aircraft  automatic pilot  automatic throttle 
automatic throttles  autopilot  center auto pilot  craft  emergency
engine  left auto pilot  left hand engine  parked plane  right autopilot
identified by one judge as correct 
problem engine  maintenance and aircraft  later aircraft  aircraft and aircraft  collision
avoidance system ii alert      constant speed drive  auto pilot
and autothrottles  wdb    perf
agreed by both judges as incorrect  aircraft beginning  aircraft
parallel  normal and aircraft  person or property  persons or property 
so aircraft  time aircraft

   

ficause identification via weakly supervised semantic lexicon construction

references
ando  r  k          semantic lexicon construction  learning from unlabeled data via
spectral analysis  in proceedings of the  th conference on computational natural
language learning  pp      
artstein  r     poesio  m          inter coder agreement for computational linguistics 
computional linguistics                 
avancini  h   lavelli  a   sebastiani  f     zanoli  r          automatic expansion of
domain specific lexicons by term categorization  acm transactions on speech and
language processing  tslp              
baker  l  d     mccallum  a  k          distributional clustering of words for text classification  in proceedings of the   st annual international acm sigir conference on
research and development in information retrieval  pp        
banko  m     brill  e          mitigating the paucity of data problem  exploring the effect of training corpus size on classifier performance for natural language processing 
in proceedings of the  st international conference on human language technology
research 
blei  d  m   ng  a  y     jordan  m  i          latent dirichlet allocation  journal of
machine learning research             
brants  t     franz  a          web  t   gram version    linguistic data consortium 
philadelphia  usa 
cheng  p  w          from covariation to causation  a causal power theory  psychological
review                  
chinchor  n          the statistical significance of the muc   results  in proceedings of
the  th message understanding conference  pp       
chinchor  n   hirschman  l     lewis  d  d          evaluating message understanding systems  an analysis of the third message understanding conference  muc    
computational linguistics             
cortes  c     vapnik  v          support vector networks  machine learning             
    
crammer  k     singer  y          on the algorithmic implementation of multiclass kernelbased vector machines  journal of machine learning research            
curran  j  r     moens  m          improvements in automatic thesaurus extraction 
in proceedings of the acl      workshop on unsupervised lexical acquisition  pp 
     
curran  j  r   murphy  t     scholz  b          minimising semantic drift with mutual
exclusion bootstrapping  in proceedings of the   th conference of the pacific association for computational linguistics  pp         
   

fiabedin  ng   khan

davidov  d     rappoport  a          efficient unsupervised discovery of word categories
using symmetric patterns and high frequency words  in proceedings of the   st international conference on computational linguistics and the   th annual meeting of
the association for computational linguistics  pp         
dietterich  t  g          approximate statistical tests for comparing supervised classification learning algorithms  neural computation                   
dunning  t          accurate methods for the statistics of surprise and coincidence  computational linguistics               
everitt  b  s          the analysis of contingency tables  chapman and hall 
ferryman  t  a   posse  c   rosenthal  l  j   srivastava  a  n     statler  i  c          what
happened  and why  toward an understanding of human error based on automated
analyses of incident reports volume ii  tech  rep  nasa tp             national
aeronautics and space administration 
ganeri  j   noordhof  p     ramachandran  m          counterfactuals and preemptive
causation  analysis                 
garcia  d          coatis  an nlp system to locate expressions of actions connected
by causality links  in proceedings of the   th european workshop on knowledge
acquisition  modeling and mangement  pp         
girju  r          automatic detection of causal relations for question answering  in proceedings of the acl      workshop on multilingual summarization and question
answering  pp       
godbole  s     sarawagi  s          discriminative methods for multi labeled classification 
in proceedings of the  th pacific asia conference on knowledge discovery and data
mining  pp       
griffiths  t  l     tenenbaum  j  b          structure and strength in causal induction 
cognitive psychology             
grishman  r     ksiezyk  t          causal and temporal text analysis  the role of the
domain model  in proceedings of the   th international conference on computational
linguistics  pp         
halpern  j  y     pearl  j          causes and explanations  a structural model approach 
part i  causes  the british journal for the philosophy of science             
hassan  h   hassan  a     emam  o          unsupervised information extraction approach
using graph mutual reinforcement  in proceedings of the      conference on empirical
methods in natural language processing  pp         
hume  d         original work published         an enquiry concerning human understanding  oxford university press  usa 
hume  d         original work published         a treatise of human nature  oxford
university press  usa 
joachims  t          advances in kernel methods   support vector learning  chap  making
large scale svm learning practical  mit press 
   

ficause identification via weakly supervised semantic lexicon construction

joachims  t          text categorization with suport vector machines  learning with many
relevant features  in proceedings of the   th european conference on machine learning  pp         
joachims  t          transductive inference for text classification using support vector
machines  in proceedings of the   th international conference on machine learning 
pp         
kaplan  r     berry rogghe  g          knowledge based acquisition of causal relationships
in text  knowledge acquisition                
kersey  c   di eugenio  b   jordan  p     katz  s          ksc pal  a peer learning agent
that encourages students to take the initiative  in proceedings of the  th workshop
on innovative use of nlp for building educational applications  pp       
khoo  c  s  g   chan  s     niu  y          extracting causal knowledge from a medical
database using graphical patterns  in proceedings of the   th annual meeting of the
association for computational linguistics  pp         
ko  y   park  j     seo  j          improving text categorization using the importance of
sentences  information processing and management               
krippendorff  k          content analysis  an introduction to its methodology  sage publications  inc 
lewis  d          causation  journal of philosophy                  
lin  d          dependency based evaluation of minipar  in proceedings of the lrec
workshop on evaluation of parsing systems  pp         
lin  d     pantel  p          induction of semantic classes from natural language text  in
proceedings of the  th acm sigkdd international conference on knowledge discovery and data mining  pp         
marcus  m  p   santorini  b     marcinkiewicz  m  a          building a large annotated
corpus of english  the penn treebank  computational linguistics                 
special issue on using large corpora 
mccallum  a     nigam  k          text classification by bootstrapping with keywords 
em and shrinkage  in proceedings of the acl workshop on unsupervised learning
in natural language processing  pp       
noreen  e  w          computer intensive methods for testing hypotheses   an introduction  wiley 
pantel  p     lin  d          discovering word senses from text  in proceedings of the  th
acm sigkdd international conference on knowledge discovery and data mining 
pp         
pantel  p     ravichandran  d          automatically labeling semantic classes  in proceedings of the human language technology conference of the north american chapter
of the association for computational linguistics  pp         
passonneau  r          computing reliability for coreference annotation  in proceedings of
the fourth international conference on language resources and evaluation  vol    
pp           
   

fiabedin  ng   khan

patwardhan  s     riloff  e          learning domain specific information extraction patterns from the web  in proceedings of the coling acl workshop on information
extraction beyond the document  pp       
patwardhan  s     riloff  e          effective information extraction with semantic affinity
patterns and relevant regions  in proceedings of the      joint conference on empirical methods in natural language processing and computational natural language
learning  pp         
pereira  f  c  n   tishby  n     lee  l          distributional clustering of english words 
in proceedings of the   st annual meeting of the association for computational linguistics  pp         
phan  x  h       a   crfchunker  crf english phrase chunker  http   crfchunker 
sourceforge net  
phan  x  h       b   crftagger  crf english pos tagger 
sourceforge net  

http   crftagger 

phillips  w     riloff  e          exploiting role identifying nouns and expressions for information extraction  in proceedings of international conference on recent advances
in natural language processing 
polanyi  l     zaenen  a          contextual valence shifters  in computing attitude and
affect in text  theory and applications  pp       springer verlag 
posse  c   matzke  b   anderson  c   brothers  a   matzke  m     ferryman  t         
extracting information from narratives  an application to aviation safety reports  in
proceedings of the      ieee aerospace conference  pp           
read  j   pfahringer  b     holmes  g          multi label classification using ensembles
of pruned sets  in proceedings of the  th ieee international conference on data
mining  pp          
riloff  e          automatically generating extraction patterns from untagged text  in
proceedings of the   th national conference on artificial intelligence  pp           
roark  b     charniak  e          noun phrase co occurrence statistics for semi automatic
semantic lexicon construction  in proceedings of the   th international conference on
computational linguistics  pp           
rohwer  r     freitag  d          towards full automation of lexicon construction  in
proceedings of the computational lexical semantics workshop at hlt naacl      
pp      
rychly  p     kilgarriff  a          an efficient algorithm for building a distributional
thesaurus  and other sketch engine developments   in proceedings of the acl     
demo and poster sessions  pp       
sebastiani  f          machine learning in automated text categorization  acm computing
surveys              
sundheim  b  m          overview of the fourth message understanding evaluation and
conference  in proceedings of the fourth message understanding conference  pp   
   
   

ficause identification via weakly supervised semantic lexicon construction

tang  l   rajan  s     narayanan  v  k          large scale multi label classification via
metalabeler  in proceedings of international world wide web conference  pp     
    
thelen  m     riloff  e          a bootstrapping method for learning semantic lexicons
using extraction pattern contexts  in proceedings of the      conference on empirical
methods in natural language processing  pp         
tsoumakas  g     katakis  i          multi label classification  an overview  international
journal of data warehousing and mining             
tsoumakas  g     vlahavas  i  p          random k  labelsets  an ensemble method for
multilabel classification  in proceedings of the   th european conference on machine
learning  vol       of lecture notes in computer science  pp         
ueda  n     saito  k          parametric mixture models for multi labeled text  in advances
in neural information processing systems     pp         
van delden  s     gomez  f          retrieving nasa problem reports  a case study
in natural language information retrieval  data and knowledge engineering         
       
vapnik  v  n          statistical learning theory  wiley 
yang  y     pedersen  j  o          a comparative study on feature selection in text categorization  in proceedings of the   th international conference on machine learning 
pp         
yangarber  r          counter training in discovery of semantic patterns  in proceedings
of the   st annual meeting of the association for computational linguistics  pp 
       
zaidan  o  f   eisner  j     piatko  c          using annotator rationales to improve machine learning for text categorization  in proceedings of the human language technology conference of the north american chapter of the association for computational
linguistics  pp         
zhang  q   zhou  y   huang  x     wu  l          graph mutual reinforcement based
bootstrapping  information retrieval technology                    

   

fi