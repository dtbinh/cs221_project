journal of artificial intelligence research               

submitted        published      

using local alignments for relation recognition
sophia katrenko
pieter adriaans
maarten van someren

s katrenko uva nl
p w adriaans uva nl
m w vansomeren uva nl

informatics institute  university of amsterdam
science park          xg amsterdam  the netherlands

abstract
this paper discusses the problem of marrying structural similarity with semantic relatedness for information extraction from text  aiming at accurate recognition of relations 
we introduce local alignment kernels and explore various possibilities of using them for this
task  we give a definition of a local alignment  la  kernel based on the smith waterman
score as a sequence similarity measure and proceed with a range of possibilities for computing similarity between elements of sequences  we show how distributional similarity
measures obtained from unlabeled data can be incorporated into the learning task as semantic knowledge  our experiments suggest that the la kernel yields promising results
on various biomedical corpora outperforming two baselines by a large margin  additional
series of experiments have been conducted on the data sets of seven general relation types 
where the performance of the la kernel is comparable to the current state of the art results 

   introduction
despite the fact that much work has been done on automatic relation extraction  or recognition  in the past few decades  it remains a popular research topic  the main reason for the
keen interest in relation recognition lies in its utility  once concepts and semantic relations
are identified  they can be used for a variety of applications such as question answering
 qa   ontology construction  hypothesis generation and others 
in ontology construction  the relation that is studied most is the is a relation  or hypernymy   which organizes concepts in a taxonomy  snow  jurafsky    ng         in information retrieval  semantic relations are used in two ways  to refine queries before actual
retrieval  or to manipulate the output that is returned by a search engine  e g  identifying
whether a fragment of text contains a given relation or not   the most widely used relations
for query expansion are hypernymy  or broader terms from a thesaurus  and synonymy 
semantic relations can also be useful at different stages of question answering  they have
to be taken into account when identifying the type of a question and they have to be considered at actual answer extraction time  van der plas         yet another application of
relations is constructing a new scientific hypothesis given the evidence found in text  this
type of knowledge discovery from text is often based on co occurrence analysis and  in many
cases  was corroborated via experiments in laboratories  swanson   smalheiser        
another reason why extraction of semantic relations is of interest lies in the diversity of
relations  different relations need different extraction methods  many existing information
extraction systems were originally designed to work for generic data  grishman   sundheim         but it became evident that domain knowledge is often necessary for successful

c
    
ai access foundation  all rights reserved 

fikatrenko  adriaans    van someren

extraction  for instance  relation extraction in the biomedical domain would require an
accurate recognition of named entities such as gene names  clegg         and in the area
of food it needs information on relevant named entities such as toxic substances 
also for generic relations syntactic information is often not sufficient  consider  for
instance  the following sentences  with the arguments of the relations written in italics  
   

mary looked back and whispered  i know every tree in this forest  every scent 
 part whole relation 

   

a person infected with a particular flu virus strain develops antibodies against that
virus   cause effect relation 

   

the apples are in the basket   content container relation 

all these sentences exemplify binary relations  namely part whole  tree is part of a
forest   cause effect  virus causes flu  and content container  apples are contained
in basket   we can easily notice that the syntactic context in     and     is the same  namely 
the arguments in both cases are connected to each other by the preposition in  however 
this context is highly ambiguous because even though it allows us to reduce the number
of potential semantic relations  it is still not sufficient to be able to discriminate between
part   whole and content   container relation  in other words  world knowledge
about trees  forests  apples and baskets is necessary to classify relations correctly  the
situation changes even more drastically if we consider example      here  there is no explicit
indication for causation  nevertheless  by knowing what a flu and a virus is  we are
able to infer that cause   effect relation holds 
the examples in          and     highlight several difficulties that characterize semantic
relation extraction  generic relations very often occur in nominal complexes such as flu
virus in     and lack of sentential context boosts such approaches as paraphrasing  nakov 
       however  even for noun compounds one has to combine world knowledge with the
compounds context to arrive at the correct interpretation 
computational approaches to the relation recognition problem often rely on a two step
procedure  first  the relation arguments are identified  depending on the relation at hand 
this step often involves named entity recognition of the arguments of the relations  the
second step is to check whether the relation holds  if relation arguments are provided  e g  
basket and apples in       relation extraction is reduced to the second step  previous
work on relation extraction suggests that in this case the accuracy of relation recognition
is much higher than in the case when they have to be discovered automatically  bunescu
et al          furthermore  most existing solutions to relation extraction  including work
presented in this paper  focus on relation examples that occur within a single sentence and
do not consider discourse  mcdonald         recognizing relations from a wider scope is
an interesting enterprise  but it would require to take into account anaphora resolution and
other types of linguistic analysis 
approaches to relation extraction that are based on hand written patterns are timeconsuming and in many cases need an expert to formulate and test the patterns  although
patterns are often precise  they usually produce poor recall  thomas et al          in
general  hand written patterns can be of two types  the first type is sequential and based
 

fiusing local alignments for relation recognition

on frequently occurring sequences of words in a sentence  hand written sequential patterns
were initially used for extraction of hypernymy  hearst         with several attempts to
extend them to other relations  the second type of patterns  khoo  chan    niu        take
the syntactic structure of a sentence into account  the dependency structure of a sentence
can usually be represented as a tree and the patterns then become subtrees  such patterns
are sometimes referred to as graphical patterns  to identify examples of the cause effect
relation  khoo et al         applied this type of patterns to texts in the medical domain 
this study showed that graphical patterns are sensitive to the errors made by the parsers 
do not cover all examples in the test data and extract many spurious instances 
an alternative to using hand written patterns is supervised machine learning  then 
relations are labeled and used to train a classifier that can recognize these relations in new
texts  one approach is to learn generalized extraction patterns where patterns are expressed
as characters  words or syntactic categories of words  other approaches involve clustering
based on co occurrence  davidov   rappoport         in recent years kernel based methods
have become popular because they can handle high dimensional problems  zelenko et al  
      bunescu   mooney        airola et al          these methods transform text fragments  complete sentences or segments around named entitites or verbs  to vectors  and
apply support vector machines to classify new fragments 
some machine learning methods use prior knowledge that is given to the system in
addition to labeled examples  scholkopf        p       the use of prior knowledge is often
motivated by  for example  poor quality of data and data sparseness  prior knowledge can be
used in many ways  from changing the representation of existing training examples to adding
more examples from unlabelled data  for nlp tasks  prior knowledge exists in the form of
manually  or automatically  constructed ontologies or large collections of unannotated data 
these enrich the textual data and thereby improve the recognition of relations  sekimizu 
park    tsujii        tribble   fahlman         recently  zhang et al         showed that
semantic correlation of words can be learned from unlabelled text collections  transferred
among documents and used further to improve document classification  in general  while
use of large collections of text allows us to derive almost any information needed  it is done
with varying accuracy  in contrast  existing resources created by humans can provide very
precise information  but it is less likely that they will cover all possible areas of interest 
in this paper  as in the work of bunescu and mooney         we use the syntactic
structure of sentences  in particular  dependency paths  this stems from the observation
that linguistic units are organized in complex structures and understanding how words or
word senses relate to each other often requires contextual information  relation extraction
is viewed as a supervised classification problem  a training set consists of examples of a
given relation and the goal is to construct a model that can be applied to a new  unseen
data set  to recognize all instances of the given relation in this new data set  for recognition
of relations we use a kernel based classifier that is applied to dependency paths  however 
instead of a vector based kernel we directly use similarity between dependency paths and
show how information from existing ontologies or large text corpora can be employed 
the paper is organized as follows  we start by reviewing existing kernel methods that
work on sequences  section     in section    we give the definition of a local alignment kernel
based on the smith waterman measure  we proceed by discussing how it can be used in
the context of natural language processing  nlp  tasks  and particularly for extracting
 

fikatrenko  adriaans    van someren

relations from text  section       once the method is described  we report on two types of
the data sets  biomedical and generic  used in the experiments  section    and elaborate
on our experiments  sections   and     section   discusses our findings in more detail 
section   concludes the paper by discussing possible future directions 

   kernel methods
the past years have witnessed a boost of interest in kernel methods  their theoretical analysis
and practical applications in various fields  burges        shawe taylor   christianini 
       the idea of having a method that works with different structures and representations 
starting from the simplest representation using a limited number of attributes to complex
structures such as trees  seems indeed very attractive 
before we define a kernel function  recall the standard setting for supervised classification  for a training set s of n objects  instances   x    y              xn   yn   where x            xn  x
are input examples in the input space x with their corresponding labels y            yn        
the goal is to infer a function h   x         such that it approximates a target function t 
however  h can still err on the data which has to be reflected in a loss function  l h xi    yi   
several loss functions have been proposed in the literature so far  the best known of which
is the zero one loss  this loss is a function that outputs   each time a method errs on a
data point  h xi      yi    and   otherwise 
the key idea of kernel methods lies in the implicit mapping of objects to a highdimensional space  by using some mapping function   and considering their inner product
 similarity  k xi   xj       xi     xj      rather than representing them explicitly  functions that can be used in kernel methods have to
symmetric and positive semi definite 
pbe
n pn
whereby positive semi definiteness is defined by i   j   ci cj k xi   xj      for any n     
any objects x            xn  x   and any choice of real numbers c            cn  r  if a function
is not positive semi definite  the algorithm may not find the global optimal solution  if
the requirements w r t  symmetry and positive semi definiteness are met  a kernel is called
valid 
using the idea of a kernel mapping  cortes and vapnik        introduced support vector
machines  svm  as a method which seeks the linear separation between two classes of the
input points by a function f  x  such that f  x    wt  x    b  wt  rp   b  r and
h x    sign f  x    here  wt stands for the slope of the linear function and b for its
offset  often  there can exist several functions that separate data well  but not all of them
are equally good  a hyperplane that separates mapped examples with the largest possible
margin would be the best option  vapnik        
svms solve the following optimization problem 
n

x
 
k w k   c
l h xi    yi  
f  x  wt x b  
argmin

   

i  

in equation    the first part of the equation corresponds to the margin maximization
 by minimizing    k w k     while the second takes into account the error on the training
set which has to be minimized  where c is a penalty term   the hyperplane that is found
may correspond to a non linear boundary in the original input space  there exist a number
 

fiusing local alignments for relation recognition

of standard kernels such as the linear kernel  the gaussian kernel and others  information
about the data or the problem can motivate the choice of a particular kernel  it has been
shown by haussler        that a complex kernel  referred to as a convolution kernel   can
be defined using simpler kernels 
other forms of machine learning representations for using prior knowledge were defined
along with the methods for exploiting it  inductive logic programming offers one possible
solution to use it explicitly  in the form of additional horn clauses  camacho         in the
bayesian learning paradigm information on the hypothesis without seeing any data is encoded in a bayesian prior  mitchell        or in a higher level distribution in a hierarchical
bayesian setting  it is less obvious though how to represent and use prior knowledge in other
learning frameworks  in the case of svms  there are three possible ways of incorporating
prior knowledge  lauer   bloch         these are named sampling methods  prior knowledge is used here to generate new data   kernel methods  prior knowledge is incorporated
in the kernel function by  for instance  creating a new kernel   and optimization methods
 prior knowledge is used to reformulate the optimization problem by  for example  adding
additional constraints   the choice of a kernel can be based on general statistical properties
of the domain  but an attractive possibility is to incorporate explicit domain knowledge into
the kernel  this can improve a kernel by smoothing the space  instances that are more
similar have a higher probability of belonging to the same class than with a kernel without
prior knowledge 
in what follows  we review a number of kernels on strings that have been proposed
in the research community over the past years  a very natural domain to look for them
is the biomedical field where many problems can be formulated as string classification
 protein classification and amino acid sequences  to name a few   sequence representation
is  however  not only applicable to the biomedical area  but can also be considered for
many natural language processing tasks  after introducing kernels that have been used in
biomedicine  we move to the nlp domain and present recent work on relation extraction
employing kernel methods 
    the spectrum kernel
leslie  eskin  and noble        proposed a discriminative approach to protein classification 
for any sequence x  x   the authors define the m spectrum as the set s of all contiguous
subsequences of x whose length is equal to m  all possible m long subsequences q  s
are indexed by the frequency of their occurrence  q  x    consequently  a feature map for
a sequence x and alphabet a equals m  x     q  x  qam   the spectrum kernel for two
sequences x and y is defined as the inner product between the corresponding feature maps 
ks  x  y     m  x   m  y    
now  even assuming contiguous subsequences for small m  the feature space to consider
is very large  the authors propose to detect all subsequences of length m by using a suffix
tree method which guarantees fast computation of the kernel matrix  the spectrum kernel
was tested on the task of protein homology detection  where the best results were achieved
by setting m to a relatively small number      the novelty of leslie et al s        method
lies in its generality and its low computational complexity 

 

fikatrenko  adriaans    van someren

    mismatch kernels
the mismatch kernel that was introduced later by leslie et al         is essentially an extension of the latter  an obvious limitation of the spectrum kernel is that all considered
subsequences are contiguous and should match exactly  in the mismatch kernel the contiguity is preserved while the match criterion is changed  in other words  instead of looking
for all possible subsequences of length m for a given subsequence  one is searching for all
possible subsequences of length m allowing up to r mismatches  such a comparison will
result in a larger subset of subsequences  but the kernels defined in this way can still be calculated rather fast  the kernel is formulated similarly to the spectrum kernel and the only
major difference is in computing the feature map
p for all sequences  more precisely  a feature
map for a sequence x is defined as m r  x    qs m r  q  where m r  q       q  am  
  q  is binary and indicates whether sequence  belongs to the set of m length sequences
that differ from q at most in r elements     or it does not      it is clear that if r is set to
   the mismatch kernel is reduced to the spectrum kernel  the complexity of the mismatch
kernel computation is linear with respect to the sum of the sequence lengths 
the authors also show that the mismatch kernel not only yields state of the art performance on a protein classification task but also outputs subsequences that are informative
from a biological point of view 
    kernel methods and nlp
one of the merits of kernel methods is the possibility of designing kernels for different structures  such as strings or trees  in the nlp field  and in relation extraction  in particular 
most work roughly falls into two categories  in the first  kernels are defined over the plain
text using sequences of words  the second uses linguistic structures such as dependency
paths or trees or the output of shallow parsing  in this short review we do not take a
chronological perspective but rather start with the methods that are based on sequences
and proceed with the approaches that make use of syntactic information 
in the same year in which the spectrum kernel was designed  lodhi et al         introduced string subsequence kernels that provide flexible means to work with text data 
in particular  subsequences are not necessarily contiguous and are weighted according to
their length  using a decay factor    the length of the subsequences is fixed in advance 
the authors claim that even without the use of any linguistic information their kernels are
able to capture semantic information  this is reflected in the better performance on the
text classification task compared to the bag of words approach  while lodhi et al s       
kernel works on sequences of characters  a kernel proposed by cancedda et al         is
applied to word sequences  string kernels can be also extended to syllable kernels which
proved to do well on text categorization  saunders  tschach    shawe taylor        
because all these kernels can be defined recursively  their computation is efficient  for
instance  the time complexity of lodhi et al s        kernel is o n s  t    where n is the
length of the subsequence  and t and s are documents 
      subsequence kernels
in the recognition of binary relations  the most natural way is to consider words located
around and between relation arguments  this approach was taken by bunescu and mooney
 

fiusing local alignments for relation recognition

     b  whose choice of sequences was motivated by textual patterns found in corpora  for
instance  they observed that some relations are expressed by subject verb object constructions while others are part of the noun and prepositional phrases  as a result  three types
of sequences were considered  fore between  words before and between two named entities  
between  words only between two entities  and between after  words between and after two
entities   the length of sequences is restricted  to handle data sparseness  the authors
generalize over existing sequences using pos tags  entity types and wordnet synsets  a
generalized subsequence kernel is recursively defined as the number of weighted sparse subsequences that two sequences share  in the absence of syntactic information  an assumption
is made that long subsequences are not likely to represent positive examples and as such
are penalized  this subsequence kernel is computed for all three types of sequences and the
resulting relation kernel is defined as a sum over the three subkernels  experimental results
on a biomedical corpus are encouraging  showing that the relation kernel performs better
than manually written patterns and an approach based on longest common subsequences 
a method proposed by giuliano et al         was largely inspired by the work of bunescu
and mooney      b   however  instead of looking for subsequences in three types of sequences  the authors treat them as a bag of words and define what is called a global kernel
as follows  first  each sequence type  pattern  p is represented by a vector whose elements
are counts of how many times each token was used in p   a local kernel is defined similarly
but only using words surrounding named entities  left and right context   a final shallow
linguistic kernel is defined as the combination of the global and the local kernels  experiments on biomedical corpora suggest that this kernel outperforms the subsequence kernel
by bunescu and mooney 
      distributional kernels
recently  o seaghdha and copestake        introduced distributional kernels on co  occurrence probability distributions  the co occurrence statistics they use are in the form of
either syntactic relations or n grams  they show that it is possible to derive kernels from
such distances as jensen shannon divergence  jsd  or euclidean distance  l     lee        
jsd is a smoothed version of the kullback leibler divergence  an information theoretic measure of the dissimilarity between two probability distributions  the main motivation behind
this approach lies in the fact that distributional similarity measures proved to be useful for
nlp tasks  to extract co occurrence information  the authors use two corpora  the british
national corpus  bnc  and the web  t   gram corpus  which contains   grams with
their observed frequency counts and was collected from the web   distributional kernels
proved to be successful for a number of tasks such as compound interpretation  relation
extraction and verb classification  on all of them  the jsd kernel clearly outperforms
gaussian and linear kernels  moreover  estimating distributional similarity on the bnc
corpus yields performance similar to the results obtained on the web  t   gram corpus 
this is an interesting finding because the bnc corpus was used to estimate similarity from
syntactic relations whereas the latter corpus contains n grams only  most importantly  the
method of o seaghdha and copestake provides empirical support for the claim that using
distributional similarity is beneficial for relation extraction 

 

fikatrenko  adriaans    van someren

      kernels for syntactic structures
kernels defined for unpreprocessed text data seem attractive because they can be applied
directly to text from any language  however  as general as they are  they can lose precision when compared to the methods that use syntactic analysis  re ranking parsing
trees  collins   duffy        was one of the first applications of kernel methods to nlp
problems  to accomplish this goal  the authors rely on the subtrees that a pair of trees have
in common  later on  moschitti        explored convolution kernels on dependency and
constituency structures to do semantic role labeling and question classification  this work
introduces a novel kernel which is called a partial tree kernel  pt   it is essentially built
on two kernels proposed before  the subtree kernel  st  that contains all descendant nodes
from a target root  including leaves  and the subset tree kernel  sst  that is more flexible
and allows internal subtrees which do not necessarily encompass leaves  a partial tree is
a generalization of a subset tree whereby partial structures of a grammar are allowed  i e  
parts of the production rules such as  vp  v   form a valid pt   moschitti demonstrated
that pts obtain better performance on dependency structures than ssts  but the latter
yield better results on constituent trees 
      kernel on shallow parsing output
zelenko et al         use shallow parsing and designed kernels to extract relations from text 
in contrast to full parsing  shallow parsing produces partial interpretations of sentences 
each node in such a tree is enriched with information on roles  that correspond to the
arguments of a relation   the similarity of two trees is determined by the similarity of
their nodes  depending on how similarity is computed  zelenko et al  define two types of
kernels  contiguous subtree kernels and sparse kernels  both types were tested on two types
of relations  person affiliation and organization location exhibiting good performance  in
particular  sparse kernels outperform contiguous subtree kernels leading to the conclusion
that partial matching is important when dealing with typically sparse natural language
data  however  the computation of the sparse kernel takes o mn    time  where m and n
are the number of children of two relation examples  i e  shallow trees  under consideration 
m  n   while the algorithm for the contiguous subtree kernel runs in time o mn  
      shortest path kernel
bunescu and mooneys      a  shortest path kernel represents yet another approach for
relation extraction that is kernel based and relies on information found in dependency trees 
a main assumption here is that not the entire dependency structure is relevant  and one
can focus on the path that is connecting two relation arguments instead  the more similar
these paths are  the more likely two relation examples belong to the same category  in spirit
with their previous work  bunescu and mooney seek generalizations over existing paths by
adding information sources like part of speech  pos  categories or named entity types 
the shortest path between relation arguments is extracted and a kernel between two
sequences  paths  x    x            xn   and x     x             x m   is computed as follows 

 

fiusing local alignments for relation recognition

 

kb  x  x    



 q
n

 
i   f  xi   xi  

m    n
m n

   

in equation    f  xi   x i   is the number of features shared by xi and x i   bunescu and
mooney      a  use several features such as word  e g   protesters   part of speech tag  e g  
n n s   generalized part of speech tag  e g   n oun   and entity type  e g   p erson   if
applicable  in addition  a direction feature   or   is employed  here we reproduce an
example from their paper 
example   given two dependency paths that exemplify the relation located such as his
 actions  in  brcko and his  arrival  in  beijing  both paths are expanded by
additional features as those mentioned above  it is easy to see that comparing path     to
path     gives us a score of                   





brcko


his
actions
 nnp
in
 p rp
       n n s      
     
 n oun
in
p erson
n oun
locat ion













   


beijing

 nnp

     

 n oun
locat ion

   





arrival
his
       n n
     
 p rp
n oun
p erson



in
in



the time complexity of the shortest path kernel is o n   where n stands for the length
of the dependency path 
dependency paths are also considered in other recent work on relation recognition  erkan 
ozgur    radev         here  erkan et al         use dependency paths as input and
compare them by means of cosine similarity or edit distance  the authors motivate their
choice by the need to compare dependency paths of different length  further  various machine learning methods are used to do classification  including svm and transuctive svm
 tsvm   which is an extension of svm  joachims         in particular  tsvm makes use
of labeled and unlabeled data by first classifying the unlabeled examples and then searching
for the maximum margin that separates positive and negative instances from both sets  the
authors conclude that edit distance performs better than the cosine similarity measure  and
that tsvm slightly outperforms svm 
airola et al         propose a graph kernel which makes use of the entire dependency
structure  in their work  each sentence is represented by two subgraphs  one of which is
built from the dependency analysis  and the other corresponds to the linear structure of the
sentence  further  a kernel is defined on all paths between any two vertices in the graph 
the method by airola et al         achieves state of the art performance on biomedical
data sets  and is further discussed  together with the shortest path kernel and the work

 

fikatrenko  adriaans    van someren

by erkan et al          in section   on relation extraction in the biomedical domain in this
paper 
finally  kernels can be defined not only on graphs of syntactic structures  but also on
graphs of a semantic network  this is illustrated by o seaghdha         who uses graph
kernels on the graph built from the hyponymy relations in wordnet  even though no
syntactic information is utilized  such kernels proved to perform well on the extraction of
various generic relations 
all kernels that we reviewed in this section deal with sequences or trees albeit in different ways  the empirical findings suggest that kernels that allow partial matching usually
perform better when compared to methods where similarity is defined on an exact match 
to alleviate the problem of exact matching  some researchers suggested generalizing over
elements in existing structures  bunescu   mooney      a  while others opted for a flexible
comparison  in our view  these types of methods can complement each other  saunders
et al          as flexible as the partial matching methods are  they may suffer from low precision when the penalization of the mismatch is low  the same holds for approaches that use
generalization strategies because they may easily overgeneralize  a possible solution would
be to combine both  provided that mismatches are penalized well and generalizations are
semantically plausible rather than based on part of speech categories  this idea is further
explored in the present paper and evaluated on the relation recognition task 
in a nutshell  the goals of this paper are the following   i  a study of the possibilities
of using the local alignment kernel for relation extraction from text   ii  an exploration of
the use of prior knowledge in the alignment kernel and  iii  an extensive evaluation with
automatic recognition of two types of relations  biomedical and generic 

   a local alignment kernel
one can note from our short overview of the kernels designed for nlp above that many
researchers use partial structures and propose variants such as subsequence kernels  bunescu
  mooney      b   a partial tree kernel  moschitti         or a kernel on shallow parsing
output  zelenko et al         for relation extraction  in this paper we focus on dependency
paths as input and formulate the following requirements for a kernel function 
 it should allow partial matching so that the similarity can be measured for paths of
different length
 it should be possible to incorporate prior knowledge
recall that by prior knowledge we mean information that comes either from larger corpora or from existing resources such as ontologies  for instance  knowing that development
is synonymous to evolution in some contexts can help to recognize that two different words
are close semantically  such information is especially useful if the meaning is relevant for
detecting relations that may differ in form 
in the following subsection we will define a local alignment kernel that satisfies these
requirements and show how to incorporate prior knowledge 

  

fiusing local alignments for relation recognition

    smith waterman measure and local alignments
our work here is motivated by the recent advances in the biomedical field  it has been shown
that it is possible to design valid kernels based on a similarity measure for strings  saigo 
vert    akutsu         for example  saigo  vert  ueda  and akutsu        consider the
smith waterman  sw  similarity measure  smith   waterman         see below  to measure the similarity between two sequences of amino acids 
string distance measures can be divided into measures based on terms  edit distance
and hidden markov models  hmm   cohen  ravikumar    fienberg         term based
distances such as measures based on the tf idf score  consider a pair of word sequences as
two sets of words ignoring their order  in contrast  string edit distances  or string similarity
measures  treat entire sequences and compare them using transformation operations  which
convert a sequence x into a sequence x    examples of these are the levenshtein distance 
and the needleman wunsch  needleman   wunsch        and smith waterman  smith
  waterman        measures  the levenshtein distance has been used in the natural
language processing field as a component in a variety of tasks  including semantic role
labeling  sang et al          construction of paraphrase corpora  dolan  quirk    brockett 
       evaluation of machine translation output  leusch  ueffing    ney         and others 
the smith waterman measure is mostly used in the biological domain  there are  however 
some applications of a modified smith waterman measure to text data as well  monge  
elkan        cohen et al          hmm based measures present probabilistic extensions of
edit distances  smith  yeganova    wilbur        
our hypothesis is that string similarity measures are the best basis for a kernel for
relation extraction  in this case  the order in which words appear is likely to be relevant
and sparse data usually prevents estimation of probabilities  as in the work of smith et al  
       in general  two sequences can be aligned in several possible ways  it is possible to
search either for an alignment which spans entire sequences  global alignment   or for an
alignment which is based on similar subsequences  local alignment   both in the case of
sequences of amino acids and in relation extraction  local patterns are likely to be the most
important factor that determines similarity  therefore we need a similarity measure that
emphasizes local alignments 
formally  we define a pairwise alignment  of at most l elements for two sequences
x   x  x        xn and x    x   x         x m   as a pairing     l  i  j    l              l     i  n 
   j  m     l  n     l  m  in example    ii   the third element of the first sequence
is aligned with the first element of the second one  which is denoted by          
example   given the sequences x abacde and x   ace  two possible alignments  with gaps
indicated by    are as follows 
 i  global alignment
a
a

b
 

a
 

c
c

d
 

e
e

alignment 

                                 

a
a

c
c

d
 

e
e

alignment 

                                 

 ii  local alignment
a
 

b
 

  

fikatrenko  adriaans    van someren

in this example  the number of gaps inserted in x  to align it with x and the number
of elements that match is the same in both cases  yet  both in the biological and in
the linguistic context we may prefer alignment  ii   because closely matching substrings 
local alignments  are a better indicator for similarity than shared items that are far apart 
it is  therefore  better to use a measure that puts less or no weight on gaps before the
start or after the end of strings  as in example    ii    this can be done using a local
alignment mechanism that searches for the most similar subsequences in two sequences 
local alignments are employed when sequences are dissimilar and are of different length 
while global alignments are considered when sequences are of roughly the same length  from
the measures we have mentioned above  the smith waterman measure is a local alignment
measure  and the needleman wunsch measure compares two sequences based on global
alignments 
definition    global alignment  given two sequences x   x        xn and x    x         x m  
their global alignment is a pair of sequences y and y  both of the same length  which are
obtained by inserting zero or more gaps before the first element of either x or x    and after
each element of x and of x   
definition    local alignment  given two sequences x   x        xn and x    x         x m  
their local alignment is a pair of subsequences  of x and  of x    whose similarity is
maximal 
to clarify what we mean by local and global alignments  we give a definition of both the
smith waterman and needleman wunsch measures  given two sequences x   x  x        xn
and x    x   x         x m of length n and m respectively  the smith waterman measure is defined
as a similarity score of their best local alignment 

sw x  x     

max

a x x   

s x  x     

   

in the equation above  s x  x      is a score of a local alignment  of sequence x and x 
and a denotes the set of all possible alignments  the best local alignment can be efficiently
found using dynamic programming  to do this  one fills in a matrix sw with partial
alignments as follows 

 



sw i     j       d xi   x j  
sw  i  j    max
 in 
 sw i     j   g


 jm
sw i  j      g

   

in equation    d xi   x j   denotes a substitution score between two elements xi and x j and
g stands for a gap penalty  using this equation it is possible to find partial alignments  that
are stored in a matrix in which the cell  i  j  reflects the score for alignment between x        xi

  

fiusing local alignments for relation recognition

a
c
e

 
 
 
 

a
 
 
 
 

b
 
 
 
 

a
 
 
 
 

c
 
 
 
 

d
 
 
 
 

e
 
 
 
 

a
c
e

 a  smith waterman measure

 
 
 
 

a
 
 
 
 

b
 
 
 
 

a
 
 
 
 

c
 
  
 
 

d
 
  
 
 

e
 
  
 
 

 b  needleman wunsch measure

table    matrices for computing smith waterman and needleman wunsch scores for sequences x abacde and x   ace  a gap g      substitution score d xi   x j       for
xi   x j   and d xi   x j       for xi    x j  

and x         x j   the cell with the largest value in the matrix contains the smith waterman
score 
the needleman wunsch measure  which searches for global alignments  is defined similarly  except for the fact that the cells in a matrix can contain negative scores 

 nw i     j       d xi   x j  
nw  i  j    max
nw i     j   g
 in 

nw i  j      g
 jm

    

the smith waterman measure can be seen as a modification of the needleman wunsch
method  by disallowing negative scores in a matrix  the regions of high dissimilarity are
avoided and  as a result  local alignments are preferred  moreover  while the needlemanwunsch score equals the largest value in the last column or last row  the smith waterman
similarity score corresponds to the largest value in the matrix 
let us reconsider example   and show how the global and local alignments for alignments
for two sequences x abacde and x   ace are obtained  to arrive at actual alignments  one
has to set the gap parameter g and the substitution scores  assume we use the following
settings  a gap g      substitution score d xi   x j       for xi   x j   and d xi   x j      
for xi    x j   these values have been chosen for illustrative purpose only  but in a realistic
case  e g   alignment of protein sequences  the choice of the substitution scores is usually
motivated by biological evidence  for gapping  smith and waterman        suggested
to use a gap value which is at least equal to the difference between a match  d xi   x j   
xi   x j   and a mismatch  d xi   x j    xi    x j    then  the smith waterman and needlemanwunsch similarity scores between x and x  can be calculated according to equation   and
equation    as given in table   
first  the first row and the first column in the matrix are initialized to    then  the
matrix is filled in by computing the maximum score for each cell as defined in equation  
and equation     the score of the best local alignment is equal to the largest element in
  

fikatrenko  adriaans    van someren

the matrix      and the needleman wunsch score is    note that it is possible to trace back
which steps are taken to arrive at the final alignment  the cells in boldface   a left right
step corresponds to an insertion  a top down step to a deletion  these lead to gaps   and a
diagonal step implies an alignment of two sequences elements 
since we prefer to use local alignments on dependency paths  a natural choice would
be to use the smith waterman measure as a kernel function  however  saigo et al        
observed that the smith waterman measure may not result in a valid kernel because it
may not be positive semi definite  they give a definition of the la kernel  which states
that two sequences are similar if they have many local alignments with high scores  as in
equation    
kl  x  x     

x

 

es x x   

    

a x x   

here  s x  x      is a local alignment score and      is a scaling parameter 
to define the la kernel kl  as in equation     for two sequences x and x    it is needed to
take into account all transformation operations that are used in local alignments  first  one
has to define a kernel on elements that corresponds to individual alignments  ka   second 
since this type of alignment allows gaps  there should be another kernel for gapping  kg   last
but not least  recall that by local alignments only parts of the sequences may be aligned  and
some elements of x and x  may be left out  these elements do not influence the alignment
score and a kernel used in these cases  k    can be set to a constant  k   x  x         finally 
the la kernel is a composition of several kernels  k    ka   and kg    which is in the spirit of
convolution kernels  haussler        
according to saigo et al          similarity of the aligned sequences elements  ka kernel 
is defined as follows 

 
if  x   
    or  x     
   
 
ka  x  x    
    
  
d x x
e
otherwise
if either x  or x  has more than one element  this kernel would result in    otherwise 
it is calculated using the substitution score d x  x    of x and x    this score reflects how
similar two sequences elements are and  depending on the domain  can be computed using
prior knowledge from the given domain 
the gapping kernel is defined similarly to the alignment kernel in equation     whereby
the scaling parameter  is preserved  but the gap penalties are used instead of a similarity
function between two elements 
 

kg  x  x      e g  x   g  x    

    

here  g stands for the gap function  naturally  for a gap of length   this function returns
zero  for gaps of length n  it is reasonable to define a gap in terms of a gap opening o and
a gap extension e  g n    o   e   n      in this case it is possible to decide whether longer
gaps should be penalized more than the shorter ones  and how much  for instance  if there
  

fiusing local alignments for relation recognition

are three consecutive gaps in the alignment  the first gap is counted as a gap opening  and
the other two as a gap extension  if in consecutive gaps  i e   gaps of length n      each gap
is of equal importance  the gap opening has to be equal to the gap extension  if  however 
the length of gaps does not matter  one would prefer to penalize the gap opening more  and
to give a little weight to the gap extension 
all these kernels can be combined as follows 
k r   x  x      k    ka  kg  r   ka  k 

    

in equation     k r   x  x    stands for an alignment of r elements in x and x  with possibly
r    gaps  similarity of the aligned elements is calculated by ka   and gapping by kg   since
there could be up to r    gaps  this corresponds to the following part of the equation 
 ka  kg  r    further  because there is the rth aligned element  one more ka is added  given
the discussion above  k  is added to the initial and final part  as follows from equation    
if there are no elements in x and x  aligned  k r  equals k    which is    if all elements of x
and x  are aligned with no gaps  the value of k r  is  ka  r  
finally  the la kernel is equal to the sum taken over all possible local alignments for
sequences x and x   

 

kl  x  x    


x

k i   x  x   

    

i  

the results in the biological domain suggest that kernels based on the smith waterman
distance are more relevant for the comparison of amino acids than string kernels  saigo et al  
       it is not clear whether this holds when applied to natural language processing tasks 
in our view  it could depend on the parameters which are used  such as the substitution
matrix and the penalty gaps 
      computational complexity
the la kernel  as many other kernels discussed in section    can be efficiently calculated using dynamic programming  for any two sequences x and x    of length n and m respectively 
its complexity is proportional to n  m  additional costs may come from the substitution matrix  which  unlike in the biomedical domain  can become very large  however  the
look up of the substitution scores can be done in an efficient manner as well  which leads
to fast kernel computation  for instance  calculating a kernel matrix for the largest data
set used in this paper  aimed        instances   takes     seconds on a      ghz intel r 
core tm   machine 
    designing a local alignment kernel for relation extraction
the smith waterman measure is based on transformations  in particular deletions of elements that are different between strings  however  elements that are different may still be
similar to some degree  these similarities can be used as part of the similarity measure 
for example  if two elements are words that are different but that are synonyms  then we
count them as less different than when they are completely unrelated  we will call these
  

fikatrenko  adriaans    van someren

similarities substitution scores  equation     and define them in two different ways  on
the basis of distributional similarity and on the basis of semantic relatedness in an ontology 
for example   we would like to be able to infer that brcko is similar to beijing  even
though these two words do not match exactly  furthermore  if we have phrases his arrival
in beijing and his arrival in january  then we would like our kernel to say that brcko is
more similar to beijing than to january  the use of such information as prior knowledge
makes it possible to measure similarity between two words  one in the test set and the
other in the training set  even if they do not match exactly  below we review two types of
measures that are based on statistical distributions and on relatedness in wordnet 
      distributional similarity measures
there are a number of distributional similarity measures proposed over the years  including
cosine  dice and jaccard coefficients  distributional similarity measures have been extensively studied before  lee        weeds  weir    mccarthy         the main hypothesis
behind distributional measures is that words occurring in the same context should have
similar meaning  firth         context can be defined either using proximity in text  or
employing grammatical relations  in this paper  we use the first option where context is a
sequence of words in text and its length is set in advance 
measure

formula

cosine

d xi   x j     p

dice

d xi   x j    

l 

d xi   x j    

p  c xi  p  c x j  
p
   
 
c p  c xi  
c p  c xj  

p

c

 f  xi  f  x j  
f  xi  f  x j  

qp

c  p  c xi  

 p  c x j    

table    a list of functions used to estimate distributional similarity measures 
we have chosen the following measures  dice  cosine and l   euclidean  whose definitions are given in table    in the definition of cosine and l   it is possible to use either
frequency counts or probability estimates derived from unsmoothed relative frequencies 
here  we adopt the definitions given by lee         which are based on probability estimates p   recall that x and x  are two sequences we would wish to compare  with their
corresponding elements xi and x j   further  c stands for a context  in the definition of the
dice coefficient  f  xi      c   p  c xi         we are mainly interested in symmetric measures
 d xi   x j     d x j   xi    because a symmetric positive semi definite matrix is required by kernel methods  the euclidean measure as defined in table   does not necessarily vary from  
to    for this reason  given a list of pairs of words  xi   x j   where xi is fixed and j              s
with their corresponding l  score  the maximum value maxj d xi   x j   is detected and used
to normalize all scores on the list  furthermore  unlike dice and cosine  which return   in
the case two words are equal  the euclidean score equals    in the next step  we substract
the obtained normalized value from   to ascertain that all scores are within an interval       
  

fiusing local alignments for relation recognition

and the largest value     is assigned to identical words  in our view  this procedure will
make a comparison of the selected distributional similarity measures with respect to their
influence on the la kernel more transparent 
distributional similarity measures are very suitable if no other information is available 
in the case that data is annotated by means of some taxonomy  e g   wordnet   it is
possible to consider measures defined over this taxonomy  availability of hand crafted
resources  such as wordnet  that comprise various relations between concepts  enables
making distinctions between different concepts in a subtle way 
      wordnet relatedness measures
for generic relations  the most commonly used resource is wordnet  fellbaum         which
is a lexical database for english  in wordnet  words are grouped together in synsets where
a synset consists of a list of synonymous words or collocations  e g   fountain pen   and
pointers that describe the relations between this synset and other synsets  fellbaum        
wordnet can be employed for different purposes such as studying semantic constraints for
certain relation types  girju  badulescu    moldovan        katrenko   adriaans        
or enriching the training set  giuliano et al         nulty        
to compare two concepts given their synsets c  and c  we use five different measures
that have been proposed in the past years  most of them rely on the notions of the length
of the shortest path between two concepts c  and c    len c    c     the depth of a node in the
wordnet hierarchy  which is equal to the length of the path from the root to the given
synset ci    dep ci    and a least common subsumer  or lowest super ordinate  between c 
and c    lcs c    c     which in turn is a synset  to the measures that are exclusively based
on these notions belong conceptual similarity proposed by palmer and wu         simwup
in equation     and the formula of scaled semantic similarity introduced by leacock and
chodorow         simlch in equation        the major difference between them lies in the
fact that simlch does not consider the least common subsumer of c  and c  but uses the
maximum depth of the wordnet hierarchy instead  conceptual similarity ignores this and
focuses on the subhierarchy that includes both synsets 

simwup  c    c     

   dep lcs c    c    
len c    lcs c    c       len c    lcs c    c          dep lcs c    c    

simlch  c    c       log

len c    c   
   maxcw ordn et dep c 

    

    

aiming at combining information from several sources  resnik        introduced yet another measure that is grounded in information content  simres in equation      intuitively 
if two synsets c  and c  are located deeper in the hierarchy and the path from one synset to
another is short  they should be similar  if the path between two synsets is long and their
least common subsumer is placed relatively close to the root  this indicates that the synsets
   in all equations of similarity measures defined over wordnet  subscripts refer to the similarity measure
itself  e g   lch  wup in simlch and in simwup   respectively 

  

fikatrenko  adriaans    van someren

c  and c  do not have much in common  to quantify this intuition  it is necessary to derive a
probability estimate for lcs c    c    which can be done by employing existing corpora  more
precisely  p lcs c    c     stands for the probability of encountering an instance of a concept
lcs c    c    
simres  c    c       log p lcs c    c    

    

one of the biggest shortcomings of resniks method is the fact that only the least
common subsumer appears in equation     one can easily imagine a full blown hierarchy
where the relatedness of the concepts subsumed by the same lcs ci   cj   can heavily vary 
in other words  by using lcs only  one is not able to make subtle distinctions between two
pairs of concepts that share the least common subsumer  to overcome this  jiang and
conrath        proposed a solution that takes into account information about the synsets
being compared  simjcn in equation      by comparing equation    against equation    
we will notice that now the equation incorporates not only the probability of encountering
lcs c    c     but also the probability estimates for c  and c   
simjcn  c    c        log p lcs c    c       log p c      log p c    

    

lin        defined the similarity between two concepts using how much commonality
and differences between them are involved  similarly to the two previous approaches  he uses
information theoretic notions and derives the similarity measure simlin given in equation    

simlin  c    c     

   log p lcs c    c    
log p c      log p c   

    

in the past  semantic relatedness measures were evaluated on different nlp tasks  budanitsky   hirst        ponzetto   strube        and it can be concluded that no measure
performs the best for all problems  in our evaluation  we use semantic relatedness for the
validation of generic relations and study in depth how they contribute to the final results 
      substitution matrix for relation extraction
until now  we have discussed two possible ways of calculating the substitution score d      by
using either distributional similarity measures  or measures defined on wordnet  however 
dependency paths which are generated by parsers may contain not only words  or lemmata  
but also syntactic functions such as subjects  objects  modifiers  and others  to take this
into account  we revise the definition of d      we assume sequences x   x  x        xn and
x    x   x         x m to contain words  xi  w where w refers to a set of words  and syntactic
functions accompanied by direction  xi 
  w    the elements of w are unique words  or
lemmata  which are found in the dependency paths  for instance  for the paths his 
actions  in  brcko and his  arrival  in  beijing in example     in section       
w    his  actions  in  brcko  arrival  beijing   the dependency paths we use in the present
work include information on syntactic functions  for instance awareness
joy  in this case  w    awareness  come  joy  and w    
  

prep f rom

prep f rom nsubj



     



nsubj

come 

fiusing local alignments for relation recognition

then 

d xi   x j  




  
 
d   xi   x j    


 



 

xi   x j  w
xi   x j 
  w   xi   x j
 
xi   xj 
  w   xi    x j
xi  w   x j 
 w
xi 
  w   x j  w

    

equation      states that whenever the element xi of the sequence x is compared against
the element x j of the sequence x    their substitution score is equal either to  i  the similarity
score in the case both elements are words  lemmata   or to  ii     if both elements are the
same syntactic function  or to  iii     in any other case 
as follows from our discussion on similarity measures above  there are two ways to define
d xi   x j    using either distributional similarity between xi and x j  section         or their
wordnet similarity  provided that they are annotated with wordnet synsets  section        

   experimental set up
in this section  we describe the data sets that we have used in the experiments and provide
information on the data collections used for estimating distributional similarity 
    data
to evaluate the performance of the la kernel  we consider two types of text data  domainspecific data  which comes from the biomedical domain and generic or domain independent
data which represents a variety of well known and widely used relations such as partwhole and cause effect 
like other work  we extract a dependency path between two nodes corresponding to the
arguments of a binary relation  we also assume that each analysis results in a tree and since
it is an acyclic graph  there exists a unique path between each pair of nodes  we do not
consider  however  other structures that might be derived from the full syntactic analysis
as in  for example  subtrees  moschitti        
      biomedical relations
corpora we use three corpora that come from the biomedical field and contain annotations of either interacting proteins   bc ppi         sentences   aimed  bunescu   mooney 
    b  or the interactions among proteins and genes lll     sentences in the training set
and    in the test set  nedellec         the bc ppi corpus was created by sampling sentences from the biocreative challenge  the aimed corpus was sampled from the medline
collection  the lll corpus was composed by querying medline with the term bacillus subtilis  the difference among all three corpora lies in the directionality of interactions  as
table   shows  relations in the aimed corpus are strictly symmetric  in lll they are asymmetric and bc ppi contains both types  the differences in the number of training instances
for the aimed corpus can be explained by the fact that they correspond to the dependency
   available from http   www  informatik hu berlin de  hakenber  

  

fikatrenko  adriaans    van someren

paths between named entities  if parsing fails or produces several disconnected graphs per
sentence  no dependency path is extracted 
parser
linkparser
linkparser
stanford
stanford
enju

data set
lll  train 
lll  test 
bc ppi
aimed
aimed

 examples
   
   
   
    
    

 pos
   
   a
   
   
   

direction
asymmetric
asymmetric
mixed
symmetric
symmetric

a  even though the actual annotations for the test data are not given  the number of interactions in the
test data set is provided by the lll organizers 

table    statistics of the biomedical data sets lll  bc ppi  and aimedd  in this table   pos
stands for the number of positive examples per data set and  examples indicates
the number of examples in total 

the goal of relation extraction in all three cases is to output all correct interactions
between biomedical entities  genes and proteins  that can be found in the input data  the
biomedical entities are already provided  so there is no need for named entity recognition 
there is a discrepancy between the training and the test sets used for the lll challenge 
unlike the training set  where each sentence has an example of at least one interaction  the
test set contains sentences with no interaction  the organizers of the lll challenge distinguish between sentences with and without coreferences  sentences with coreferences are
usually appositions  as shown in one of the examples below  the first sentence in         is
an example of a sentence without coreferences  with interaction between ykud and sigk  
whereas the second one is a sentence with coreference  with interaction between spoiva
and sigmae   more precisely  spoiva refers to the phrase one or more genes which are
known to interact with sigmae  we can therefore infer that spoiva interacts with sigmae  sentences without coreferences form a subset  which we refer to as lll nocoref  and
sentences with coreferences are part of the separate subset lll coref 
     ykud was transcribed by sigk rna polymerase from t  of sporulation 
     finally  we show that proper localization of spoiva required the expression of one
or more genes which  like spoiva  are under the control of the mother cell
transcription factor sigmae 
it is assumed here that relations in the sentences with coreferences are harder to recognize  to show how the la kernel performs on both subsets  we report the experimental results on the full set of test data  lll all   and on its subsets  lll coref and lll nocoref  
syntactic analysis we analyzed the bc ppi corpus with the stanford parser  the lll
corpus has already been preprocessed by the linkparser and its output was checked by
experts  to enable comparison with the previous work  we used the aimed corpus parsed

  

fiusing local alignments for relation recognition

by the stanford parser   and by the enju parser    which exactly correspond to the input in
the experiments by erkan et al        and stre et al          unlike the stanford parser 
enju is based on a head driven phrase structure grammar  hpsg   the output of the
enju parser can be presented in two ways  either as predicate argument structure or as a
phrase structure tree  predicate argument structures describe relations between words in
a sentence  while phrase structure presents a sentence structure in the form of clauses and
phrases  in addition  enju was trained on the genia corpus and includes a model for
parsing biomedical texts 
     cbf  contains three proteins  cbf a  cbf b and cbf c 

contains

dobj

nsubj

proteins

cbf 

num

conj and

conj and conj and

three

cbf a
nsubj

cbf b
dobj

cbf   contains  proteins
nsubj
dobj
cbf   contains  proteins
nsubj
dobj
cbf   contains  proteins

cbf b

conj and

 cbf a
 cbf b
conj and
 cbf c
conj and

figure    stanford parser output and representation for example      
figure   shows a dependency tree obtained by the stanford parser for the sentence in
      this sentence mentions three interactions among proteins  more precisely  between
cbf  and cbf a  cbf  and cbf b  and cbf  and cbf c  all three dependency
paths contain words  lemmata  and syntactic functions  such as subj for a subject  plus the
direction of traversing the tree  figure   presents the output for the same sentence provided
by the enju parser  the upper part refers to the phrase structure tree and the lower part
shows the paths extracted from the predicate argument structure  the two parsers clearly
differ in their output  first  the stanford parser conveniently generates the same paths
for all three interaction pairs while the enju analyzer does not  second  the output of the
stanford parser excludes prepositions or conjunctions that are attached to the syntactic
functions whereas the enju analyzer lists them in the parsing results  such differences
   available from http   nlp stanford edu software lex parser shtml 
   available from http   www tsujii is s u tokyo ac jp enju  

  

fikatrenko  adriaans    van someren

lead to different input sequences that are later fed into the la kernel  consequently  the
variations in input may translate into differences in the final performance 

cbf 
cbf 
cbf 

arg  verb



arg  verb



arg  verb



contain
contain
contain

arg  verb



arg  verb



arg  verb



protein
protein
protein

arg  app



arg  app



arg  app



 
 
 

arg  app



arg  app



arg  app



cbf a
cbf a
cbf a

arg  coord



arg  coord



 

arg  coord

and



cbf b

arg  coord



cbf c

figure    enjus output and representation for example      
in addition  in most work employing aimed  the dependency paths such as these in
figure   and figure   are preprocessed in the following way  the actual named entities that
are the arguments of the relation are replaced by a label  e g  protein  consequently  the
nsubj

dobj

conj and

first path in figure   becomes protein  contains  proteins  protein 
to be able to compare our results on aimed with the performance reported in the work of
erkan et al         and stre et al          we use exactly the same dependency paths with
argument labels  however  to study whether using labels instead of actual named entities
has an impact on the final results for the lll data set  we carry out two experiments  in the
first one  the dependency paths contain named entities  whereas in the second they contain
labels  the second experiment is referred to by adding a word label to its name  as
lll all label in table    
      generic relations
the second type of relations that we consider are generic relations  their arguments are
sometimes annotated using external resources such as wordnet  which makes it possible to
use semantic relatedness measures defined over them  an example of such an approach is

  

fiusing local alignments for relation recognition

data used for the semeval      challenge  task     classification of semantic relations
between nominals  girju et al         
the goal of task   was to classify seven semantic relations  cause   effect  instrument   agency  product   producer  origin   entity  theme   tool  part whole and content   container   whose examples were collected from the web using
some predefined queries  in other words  given a set of examples and a relation  the expected output would be a binary classification of whether an example belongs to the given
relation or not  the arguments of the relation were annotated by synsets from the wordnet
hierarchy  as in figure    given this sentence and a pair  spiritual awareness  joy  with the
corresponding synsets joy         and awareness          this would mean that a classifier has to decide whether this pair is an example of the cause effect relation  this
particular sentence was retrieved by quering the web with the phrase joy comes from   
the synsets were manually selected from the wordnet hierarchy  there are seven semantic
relations used in this challenge  which gives seven binary classification problems 

genuine  e  joy  e   comes from  e  spiritual awareness  e   on life and an absolute clarity of direction  living for a purpose 
wordnet e     joy          wordnet e     awareness         
query  joy comes from    cause effect e   e     true

figure    an annotated example of cause   effect from the semeval       task  
training data set 

relation type
origin   entity
product   producer
theme   tool
instrument   agency
part   whole
content   container
cause   effect

 examples  train 
   
   
   
   
   
   
   

 pos  train 
  
  
  
  
  
  
  

 examples  test 
  
  
  
  
  
  
  

direction
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric

table    distribution of the semeval       task   examples  training and test data   where
 pos stands for the number of positive examples per data set and  examples
indicates the number of examples in total 

syntactic analysis to generate dependency paths  all seven data sets used in semeval       task    were analyzed by the stanford parser  the dependency path for the sentence
in figure   is given in      
  

fikatrenko  adriaans    van someren

     awareness n  

prep f rom



nsubj

come  joy n  

here  words annotated with wordnet have their pos tag attached  followed by the sense 
for instance  awareness is a noun and in the current context its first sense is used  which
corresponds to awareness n   
    substitution matrix
to build a substitution matrix for the la kernel  we use either distributional similarity
or wordnet semantic relatedness measures  for a data set of dependency paths  which
contains t unique elements  words and syntactic functions   the size of the matrix is t  t 
if k elements out of t are words  the number of substitution scores to be computed by
distributional similarity  or semantic relatedness  measures equals k k         this is due
to the fact that the measures we use are symmetric  the substitution matrix is built for
each corpus we used in the experiments  which results in three substitution matrices for
the biomedical domain  for bc ppi  lll  and aimed  and seven substitution matrices for
generic relations  in what follows  we discuss the settings which were used for calculating
the substitution matrix in more detail 
distributional similarity can be estimated either by using contextual information  o
seaghdha   copestake         or by exploring grammatical relations between words  lee 
       in this work we opt for contextual information  this is motivated by the presence
of words belonging to different parts of speech in the dependency paths  for instance 
even though  according to dependency grammar theory  melcuk         adjectives do not
govern other words  they may still occur in the dependency paths  in other words  even if
parsing does not fail  it may produce unreliable syntactic structures  to be able to compare
words of any part of speech  we have decided to estimate distributional similarity based on
contextual information  rather than on grammatical relations 
while computing distributional similarity  it may happen that a given word xi does not
occur in the corpus  to handle such cases  we always set d xi   xi        the largest possible
similarity score   and d xi   x j       when xi    x j  the lowest possible similarity score  
      biomedical domain
to estimate distributional similarity for the biomedical domain  we use the trec     
genomics collection  hersch  cohen  roberts    rakapalli        which contains        
documents from    journals  all documents have been preprocessed by removing htmltags  citations in the text and reference sections and stemmed by the porter stemmer  van
rijsbergen  robertson    porter         furthermore  the query likelihood approach with
dirichlet smoothing  chen   goodman        is used to retrieve document passages given a
query  all passages are ranked according to their likelihood of generating the query  dirichlet smoothing is used to avoid zero probabilities and poor probability estimates  which may
happen when words do not occur in the documents   all k unique words occurring in the set
of dependency paths sequences are fed as queries to collect a corpus for estimating similarity 
immediate context surrounding each pair of words is used to calculate the distributional
similarity of these words  we set the context window to      tokens to the right and  

  

fiusing local alignments for relation recognition

tokens to the left of a word in focus  and do not perform any kind of further preprocessing
such as pos tagging 
      generic relations
for generic relations  we use all wordnet relatedness measures described in section       
we have already shown that the wordnet relatedness measures work only on synsets  which
assumes that all words have to be manually annotated with information from wordnet 
since this is done only for the relations arguments  see the example in figure     and for
no other words in sentences  and  correspondingly  in the dependency paths   we build a
substitution matrix as follows  for any two words annotated with wordnet  their substitution score equals a value returned by a relatedness measure being used  for any other word
pair  it equals   whenever the words are identical  and   otherwise    for example  if we
consider the words in the dependency path in      and the wu palmer  wup  relatedness
measure  the substitution scores that we obtain are as follows 

d awareness n    awareness n       
d awareness n    come     
d awareness n    joy n          
d prep from  come     
d prep from  joy n       
d come  nsubj     
d nsubj  nsubj     
d joy n    joy n       

d awareness n    prep from     
d awareness n    nsubj     
d prep from  prep from     
d prep from  nsubj     
d come  come     
d come  joy n       
d nsubj  joy n       

figure    the substitution scores for the dependency path in      using wup measure 
syntactic relations  prep from  subj  are accompanied by the direction of the
dependency tree traversal  either  or   

in the dependency path       there are   unique elements  t     of which are annotated
with wordnet synsets  k   consequently  there are            substitution scores in total 
  of which are computed using wordnet relatedness 
to compute wordnet relatedness  we use the wordnet  similarity package for wordnet      pedersen  patwardhan    michelizzi        
    baselines and kernel settings
in this section  we discuss two baselines and kernel settings 
      baselines
to test how well local alignment kernels perform compared to kernels proposed in the past 
we implemented the shortest path kernel described in the work of bunescu and mooney
   this also applies to the cases when the relation arguments could not have been annotated with wordnet
information 

  

fikatrenko  adriaans    van someren

     a   section        as one of the baselines  baseline i   this method seems to be the
most natural choice because it operates on the same data structures  dependency paths  
similarly to bunescu and mooneys      a  work  in our experiments we use lemma  part of
speech tag and direction  but we do not consider entity type or negative polarity of items 
the choice of the la kernel in this paper was motivated not only by its ability to
compare sequences in a flexible way  but also because of the possibility to explore additional
information  not present in the training set  via a substitution matrix  the other baseline 
baseline ii  is used to test whether the choice of similarity measures affects the results  in
this case  the substitution scores d     are not calculated using distributional similarity or
wordnet relatedness  but generated randomly within the interval        
      kernel settings
the kernels we compute are used together with the support vector machine tool libsvm
 chang   lin        to detect hyperplanes separating positive examples from negative
ones  before plugging all kernel matrices for    fold cross validation into libsvm  they are
normalized as in equation    

 

 

k x  y 

k x   y     p

k x  x k y  y 

    

to handle imbalanced data sets  most notably aimed and bc ppi   the examples are
weighted using inverse class probability  i e  all training examples of class a are weighted
  prob a  where prob a  is the fraction of training examples with class a   all significance
tests were done using a two tailed paired t test with confidence level               
in addition  in all experiments we tuned the penalty parameter c  equation    in the
range                          
to use the la kernel  one has to set the following parameters  the gap opening cost 
the gap extension cost  and the scaling parameter   in our cross validation experiments 
the gap opening cost is set to      the extension cost to     and the scaling parameter  to
   the choice of the scaling value was motivated by the experiments on amino acids in the
biological domain  saigo et al          after initial experiments  we present here a further
study where the parameter values are varied 

   experiment i  domain specific relations
the goal of this evaluation is to study the behavior of the la kernel on domain specific
relations in the biomedical domain  in this section  we report on the experiments conducted
on three biomedical corpora using the la kernel based on the distributional similarity measures  two baselines and results published previously  e g   using the graph kernel by airola
et al        or the tree kernel by stre et al          to the best of our knowledge  string
kernels have not been applied to dependency paths yet  however  a gap weighted string
kernel  described in section    also allows gapping and can be thus compared to the la
kernel  to test how lodhi et al s        kernel performs on dependency paths  we use it

  

fiusing local alignments for relation recognition

on all three corpora  we have not tuned parameters of this string kernel and set the length
of subsequences to   and the decay factor  to       
    lll and bc ppi data sets
this subsection presents results on two biomedical data sets  bc ppi and lll  whenever
possible  we also discuss the performance previously reported in the literature 
the    fold cross validation results on the bc ppi corpus are presented in table   and
on the lll training data set in table    the la kernel based on the distributional similarity
measures  la dice  la cosine and la l   performs significantly better than the two baselines  recall that baseline i corresponds to the shortest path approach  section        and
baseline ii is the la kernel with the randomly generated substitution scores  in contrast
to baseline i  it is able to handle sequences of different lengths including gaps  according
to equation    a comparison of any two sequences of different lengths results in the   score 
nevertheless  it still yields high recall  while precision is much lower  this can be explained
by the fact that the shortest path uses pos tags  even though two sequences of the same
length can be very different  their comparison may still result in a non zero score  provided
that their part of speech tags match  furthermore  baseline ii suggests that accurate estimation of substitution scores is important for achieving good performance  baseline ii may
yield better results than baseline i  but randomly generated substitution scores degrade the
performance 
method
la dice
la cosine
la l 
baseline i
baseline ii
gap weighted string kernel  lodhi et al        

precision
     
     
     
     
     
     

recall
     
     
     
     
     
     

f score
     
     
     
     
     
     

table       fold cross validation on the bc ppi data set 
at first glance  the choice of the distributional similarity measures does not affect the
overall performance yielded by the la kernel  on the bc ppi data  the method based on
the l  measure outperforms the methods based on dice  p     and on cosine  but the
differences in the latter case are not significant  no statistically significant differences were
observed between the method based on dice and cosine 
in contrast to the bc ppi data set  the kernels which use dice and cosine measures
on the lll data set significantly outperform the one based on l   at p        and
p          respectively  
on both data sets  the la method using distributional similarity measures significantly
outperforms the baselines  interestingly  the gap weighted string kernel by lodhi et al 
       yields good performance too and seems to be a better choice than the subsequence
   lodhi et al         have mentioned in their paper that the f  numbers  with respect to ssk  seem to
peak at a subsequence length between   and   

  

fikatrenko  adriaans    van someren

kernel based on shallow linguistic information  giuliano et al          recent work on
lll  fundel  kueffner    zimmer        employs dependency information but  in contrast
to our method  it serves as the representation on which extraction rules are defined  airola
et al         apply a graph kernel based approach to extract interactions and use  among
others  the lll and aimed data sets  as can be seen in table    their method yields results
which are comparable to the gap weighted string kernel on the dependency paths  to the
best of our knowledge  the performance achieved by the la kernel on the lll training set
is the highest  in terms of the f score  among the results which have been reported in the
literature 
method
la dice
la cosine
la l 
baseline i
baseline ii
graph kernel  airola et al        
gap weighted string kernel  lodhi et al        
shallow linguistic kernel  giuliano et al        
rule based method  fundel et al        

precision
     
     
     
     
     
    
     
     
  

recall
     
     
     
      
     
    
     
     
  

f score
     
     
     
     
     
    
     
     
  

table       fold cross validation on the lll all training data set 
we also apply our method to the lll test data  table       even though the performance on the test set is poorer  la dice outperforms both baselines  in addition  the
gap weighted string kernel  lodhi et al         seems to perform much worse on the test
set  for the la kernel  precision is high  while recall decreases  and most drastically for
the data subset which includes co references   this might be due to the fact that for some
sentences only incomplete parses are generated and  consequently  no dependency paths
between the entities are found  for    out of     possible interaction pairs generated on
the test data  there is no dependency path extracted  in contrast  the approach reported by
giuliano et al         does not make use of syntactic information  and on the data subset
without coreferences achieves higher recall 
on the other hand  lower recall can also be caused by using actual names of proteins
and genes as arguments  in the work reported before  the relation arguments and other
named entities are often replaced by their types  e g   protein  and these are used as
input for the learning algorithm  we conducted additional experiments using named entity
types in the dependency paths  which led to a great improvement in terms of recall and
f score  table    lll coref label  lll nocoref label  lll coref label   our method
clearly outperforms the shallow linguistic kernel and also achieves better results than the
best performing system in the lll competition  sbest    which  according to nedellec        
applied markov logic to the syntactic paths 
   airola et al         do not report on the performance on the lll data set and  for this reason  information
on the graph all paths kernel is not included in table   

  

fiusing local alignments for relation recognition

data set
lll coref
lll nocoref
lll all
lll all
lll all
lll coref label
lll nocoref label
lll all label
lll coref
lll nocoref
lll all
lll all
lll all

method
la dice
la dice
la dice
baseline i
baseline ii
la dice
la dice
la dice
shallow linguistic kernel  giuliano
shallow linguistic kernel  giuliano
shallow linguistic kernel  giuliano
gap weighted string kernel  lodhi
sbest  nedellec       

et
et
et
et

al  
al  
al  
al  

     
     
     
     

precision
    
    
    
    
    
    
    
    
    
    
    
    
    

recall
    
    
    
    
    
    
    
    
    
    
    
    
    

f score
    
    
    
    
    
    
    
    
    
    
    
    
    

table    results on the lll test data set 

    aimed data set
yet another data set that we consider is aimed  this data set has often been used for
experiments on relation extraction in the biomedical domain  which enables comparison
with other methods  it should be noted  however  that in this particular case  a corpus
is a collection of documents  abstracts   this may lead to two ways of performing    fold
cross validation  one possibility lies in randomly splitting data in    parts  while the other
is to do cross validation on the level of documents  the experiments we report here are
done using the first setting and can be directly compared against the methods described in
the work of stre et al          erkan et al         and giuliano et al          in addition 
we use the same dependency paths for the la kernel as the ones employed by stre et al 
and erkan et al   the results by airola et al         and by bunescu        are obtained
by cross validating on the level of documents 
we conducted experiments by setting the distributional measure to dice  referred to as
la dice in table    in the upper part of the table we used dependency paths generated
by the stanford parser and in the lower part those obtained by enju  as we discussed in
section    erkan et al         use similarity measures to compare dependency paths  but
they do not consider any additional sources whose information can be incorporated into the
learning procedure  they  however  experiment with supervised  svm  and semi supervised
learning  tsvm   where the number of training instances is varied  table   shows the best
performance that was achieved by erkan et al s        method  among models based
on svm  the one with cosine distance  svm cos  yields the best results  in the tsvm
setting  the one with the edit measure performs the best  we observe that la dice slightly
outperforms both and has  in particular  high precision 
in their work  stre et al         explore several parsers and combinations of features 
the features include not only paths from enju  but also word dependencies generated by
data driven ksdep parser  and word features  ksdep parser is based on a probabilistic

  

fikatrenko  adriaans    van someren

shift reduce algorithm  sagae   tsujii         in general  the method by stre et al  also
uses svm  but in this case it focuses on tree kernels  discussed in section         to make a
fair comparison  we conducted experiments on the paths obtained by deep syntactic analysis
 enju parser  and compared our scores against stre et al s        results  in contrast
to the previous experiments  we achieve higher recall but lower precision  overall  the la
kernel yields better performance than the one reported by stre et al  however  when
different sets of features are combined  parses from enju and ksdep plus word features enju ksdep w in table     the overall performance can be improved 
method
la dice
baseline i  bunescu       
baseline ii
svm cos  erkan et al        
tsvm edit  erkan et al        
gap weighted string kernel  lodhi et al        
la dice
tree kernel  stre et al        
tree kernel  stre et al        
graph kernel  airola et al        
shallow linguistic kernel  giuliano et al        

parser
stanford
collins
stanford
stanford
stanford
stanford
enju
enju
enju ksdep w
charniak lease
none

precision
     
     
     
     
     
     
     
    
    
    
    

recall
     
     
     
     
     
     
     
    
    
    
    

f score
     
     
     
     
     
     
     
    
    
    
    

table       fold cross validation on the aimed data set 
bunescu        reports the evaluation results on the aimed corpus in the form of a
precision recall curve  if we consider the highest precision that was obtained in our experiments        or        depending on the input   this roughly corresponds to a recall of
    in his plot  referred to as baseline i in table     in sum  the shortest path approach
never approaches performance of the la kernel on any of the biomedical data sets that
were studied here  the other baseline  baseline ii  achieves the lowest scores from all the
methods presented here 
table   illustrates that not only various methods have been trained on the aimed corpus 
but also many different parsers have been used  it should be noted that the graph kernel has
been trained and tested on the syntactic representation generated by the charniak lease
parser  and the shortest path kernel has explored dependency paths obtained from the
collins parser  the charniak lease parser is a statistical parser trained on the biomedical
data  lease   charniak         whose phrase structures can be transformed into dependencies  likewise  the collins parser is a statistical parser  collins         this leads to the
question whether the choice of syntactic parser has a significant impact on the extraction
results  to compare the impact of the syntactic parsers on relation extraction for aimed 
miyao et al         have conducted a complex study with eight parsers  including the stanford analyzer  and five parse representations     they consider two cases  in the first one 
parsers have not been trained on biomedical data  regardless of the parser being used in
their experiments  accuracy for the extraction task is similar  in the second experiment 
   these are either various dependency tree formats  e  g   in the stanford dependency format   or phrase
structures  or predicate arguments structures 

  

fiusing local alignments for relation recognition

parsers have been re trained on domain specific data  in this case  it has been shown that
the relation extraction results can be improved  the actual gain  however  can vary from
one parser to another 
for the aimed data  the la kernel with the dice measure gives state of the art results 
it is outperformed only by approaches that use more information than just dependency
paths 
    la kernel parameters
saigo et al         have already shown that the scaling parameter   equation     has a
significant impact on accuracy  we have also carried out additional experiments by varying
gap values and the value of   results are visualized in figure    the opening and extension
gap values are separated by the slash symbol and the values on the x axis in the form a b
should be read as the opening gap is set to a and the extension gap is equal to b  the
kernel matrices were normalized and all examples were weighted  according to our previous
experiments  the results yielded by the dice measure do not significantly differ from the
ones achieved by the cosine measure and we selected the dice measure to conduct all
experiments  the performance on the bc ppi data set is shown in figure   

f score
  
  
  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  
  
  

    
     
gaps

    
     

   

   

   

   

 

 

scaling

figure    varying gaps and the scaling    parameter on the bc ppi data set     fold
cross validation   f score 

  

fikatrenko  adriaans    van someren

  
  
  
  
  
  
  
  
  
  
  

precision
  
  
  
  
  
  

 

  

 

    

   
     
gaps

scaling

   

    

   
     

   

figure    varying gaps and the scaling    parameter on the bc ppi data set     fold
cross validation   precision 

  
  

recall

  

  

  

  

  

  

  

  

  

  

 

  

 

    

   
     
gaps

   

    

scaling

   
     

   

figure    varying gaps and the scaling    parameter on the bc ppi data set     fold
cross validation   recall 
  

fiusing local alignments for relation recognition

the results in figure   indicate that decreasing  leads to a decrease in overall performance  moreover  varying gap values causes subtle changes in the f score  but these
changes are not as drastic as changes due to the lower  
changes in the f score are more likely to be explained by variances in precision and
recall  to investigate this matter  we look at how both measures depend on parameter
changes  if  is set to a low value  one can expect that this will nearly diminish the impact
of the substitution matrix  i e  similarity among elements  for this reason we hypothesize
that larger values of the scaling parameter  should result in higher recall  indeed  figure  
supports this hypothesis and the recall plot resembles the one for the f score  varying
parameter values has a much lower impact on precision  figure    but nonetheless precision
does decrease as the  parameter becomes larger 
overall   seems to influence the final results the most  although gap values make a
contribution as well  according to the results we obtained  setting an extension gap e to a
large value  or equal to the opening gap o  is undesirable  since the scaling parameter  is
applied not only to the substitution matrix but to the gap values as well  setting  below
    decreases the effects of gap penalization and similarity of elements  consequently  the
best performance is achieved by setting  to    this suggests that the final performance of
the la kernel is influenced by a combination of parameters and their choice is crucial for
obtaining good performance 

   experiment ii  generic relations
another series of experiments was carried out on seven generic relations from the semeval
       challenge  task    the choice of the data sets in this case was motivated by two
factors  first  semantic relations used here differ from the relations from the biomedical
domain  second  since the arguments of relations are annotated with wordnet  it becomes
possible to explore information from wordnet and use it as prior knowledge for the la
kernel 
many participants of this challenge considered wordnet either explicitly  tribble  
fahlman        kim   baldwin         or as a part of a complex system  giuliano et al  
       since it is not always obvious how to use wordnet so that it yields the best performance  many researchers have made additional decisions such as use of supersenses  hendrickx et al          selection of a predefined number of high level concepts  nulty         or
cutting the wordnet hierarchy at a certain level  bedmar et al          some other systems
such as the one by nakov        were based solely on information collected from the web 
even though it became evident that the best performing systems used wordnet  the variance in the results is remarkable and it is not clear whether this difference in performance
can be explained by the machine learning methods being used  the combination of features 
or by some other factors 
the semeval      task   data set includes some relation examples which are nominal
compounds  like coffee maker   and this greatly reduces availability of information between
two arguments in the dependency paths  the relation arguments in this case are linked by
one grammatical relation  e g   coffee and maker are linked by the grammatical relation
nn  which corresponds to noun compound   we assume  therefore  information coming
from wordnet to be especially helpful when the dependency paths are that short  in all our

  

fikatrenko  adriaans    van someren

experiments we used   relatedness measures defined earlier in section     plus one additional
measure which is called random  the random measure indicates that the relatedness values
between any two relation arguments were generated randomly  within         and is thus very
suitable as a baseline  baseline ii   similarly to the experiments in the biomedical domain 
another baseline is the shortest path kernel  baseline i   note that in the task   overview
paper  girju et al         reported on three baselines  which  in their case  were  i  guessing
true or false for all examples  depending on which class is the majority class in the test
set  baseline iii    ii  always guessing true  baseline iv   and  iii  guessing true or false
with the probability that corresponds to the class distribution in the test set  baseline v  
the first question of interest is what implications the choice of semantic relatedness
measure has for the performance of the la kernel  to answer this question  we perform
   fold cross validation on the training set  figure    figure    and figure      among
all   measures only jcn and resnik fail to perform better than the random score  in most
cases  the resnik score is outperformed by other measures  the behaviour of the leacockchodorow score  lch  and jcn varies from one semantic relation to another  for instance  use
of jcn seems to boost precision for cause effect  part whole  product   producer 
and theme   tool  for the remaining three relations it is clearly not the best performing
measure 
to check whether there are differences between relatedness measures  we have carried
out significance tests comparing all measures for all relations  our findings are summarized
in table    here  the symbol  between two relatedness measures stands for the measure
equivalence  or  in other words  indicates that there is no significant difference  similarly
to the experiments in the biomedical field  all significance tests were conducted using a
two tailed paired t test with confidence level      in addition  for any two measures a and
b  a   b means that a performs significantly better than b  for instance  the ranking for
cause   effect in table   should be read as follows  the two best performing measures
are wup and lch  which significantly outperform lin  followed by random and res  which  in
turn  yield significantly better results than jcn  it can be seen from this table that wup and
lch are clearly the best performing measures for all seven relations  each of them is the best
measure for six out of seven relations  
relation type
cause   effect
instrument   agency
product   producer
origin   entity
theme   tool
part   whole
content   container

ranking
wup  lch   lin
wup  lch   lin
wup  lch   lin
wup  lch   lin
lch   lin  wup
wup  lin  lch
wup   lch   lin

 
 

 
 
 


res  random   jcn
res   jcn  random
jcn  res   random
res  jcn   random
res   jcn   random
res   jcn  random
res   jcn  random

table    ranking of the relatedness measures with respect to their accuracy on the training sets   stands for measure equivalence  a   b indicates that the measure a
significantly outperforms b  

  

fiusing local alignments for relation recognition

for each relation  we applied the best performing measure on the training set for this
particular relation to the test data  the results are reported in table     on average  the la
kernel employing the wordnet relatedness measures significantly outperforms two baselines 
moreover  when compared to the best results of the semeval      competition  beamer
et al          our method approaches performance yielded by the best system  bestsv   
this system used not only various lexical  syntactic  and semantic feature sets  but also
expanded the training set by adding examples from many different sources  we have already
mentioned in section   that the recent work by o seaghdha        explores wordnet
structure and graph kernels to classify semantic relations  the overall performance which
is achieved by this method  table     is comparable to the one by the la kernel  but it is
unclear whether there are any semantic relations for which one of the approaches performs
better 
relation type
cause   effect
instrument   agency
product   producer
origin   entity
theme   tool
part   whole
content   container
average
baseline i
baseline ii
baseline iii
baseline iv
baseline v
bestsv
gap weighted string kernel  lodhi et al        
wordnet kernels  o seaghdha       

accuracy
     
     
     
     
     
     
     
     
     
     
    
    
    
    
     
    

precision
     
     
     
     
     
     
     
     
     
     
    
    
    
    
    
 

recall
     
     
     
     
     
     
     
     
     
     
    
     
    
    
     
 

f score
     
     
     
     
     
     
     
     
     
     
    
    
    
    
     
    

measure
lch
wup
lch
wup
lch
wup
wup

table     results on the semeval       task   test data set  selecting the best performing
measure on the training set for each relation  

in addition  we report results on the semeval task   test set per relatedness measure
 table      which are averages over all seven relations  similarly to our findings on the
training set  wup and lch are the best performing measures on test data as well 
one would expect that the optimal use of prior knowledge should allow us to reduce
the number of training instances without significant changes in performance  to study how
 and whether  the amount of training data influences the results on the test set  we split
the training set in several subsets  creating a model for each subset and applying it to the
semeval       task   test data  the split corresponds to the split used by the challenge
organizers  as figure     suggests  most relations are recognized well even when a relatively
small data sample is used  the exception is the theme tool relation where increasing the
   the model trained on only    origin entity examples classifies none of the test examples as positive 
for this reason there is no point in figure   for this relation given    training examples 

  

fikatrenko  adriaans    van someren

training data clearly helps  this finding is in line with the results of giuliano et al        
whose system was a combination of kernels on the same data  their results also indicate
that all relations but one  theme tool  are extracted well  even if only a quarter of the
training set is used 
relatedness measure
wup
lch
lin
res
jcn
random

accuracy
     
     
     
     
     
     

precision
     
     
     
     
     
     

recall
     
     
     
     
     
     

f score
     
     
     
     
     
     

table     results on the semeval       task   test data set  averages for all   relations
per wordnet relatedness measure 

learning curve
   
  
  
  

f score

  
  
cause effect
instrument agency
product producer
origin entity
theme tool
part whole
content container

  
  
  
  
 

  

  
   
training examples

   

figure    learning curve on the semeval       task   test data set 
some other recent work on the semeval task   data set includes investigation of distributional kernels  o seaghdha   copestake         pattern clusters  davidov   rappoport 
       relational similarity  nakov   hearst         and wordnet kernels  unlike wordnet
kernels  the first three approaches do not use wordnet  o seaghdha and copestake       
report an accuracy of      and the f score of      as the best results yielded by distributional kernels and the best performance of davidov and rappoports        method is
an accuracy of       and the f score of       wordnet kernels  similarly to our findings
with the la kernel  yield better accuracy than methods not using wordnet         but the

  

fiusing local alignments for relation recognition

cause effect
   

precision
recall
f score

  
  
  
  
  
  
  
  
  
 

wup

lin

lch
res
similarity measure

jcn

random

instrument agency
   

precision
recall
f score

  
  
  
  
  
  
  
  
  
 

wup

lin

lch
res
similarity measure

jcn

random

product producer
   

precision
recall
f score

  
  
  
  
  
  
  
  
  
 

wup

lin

lch
res
similarity measure

jcn

random

figure       fold cross validation on the training set  cause   effect  instrument agency and product   producer relations  

  

fikatrenko  adriaans    van someren

origin entity
   

precision
recall
f score

  
  
  
  
  
  
  
  
  
 

wup

lin

lch
res
similarity measure

jcn

random

theme tool
   

precision
recall
f score

  
  
  
  
  
  
  
  
  
 

wup

lin

lch
res
similarity measure

jcn

random

part whole
   

precision
recall
f score

  
  
  
  
  
  
  
  
  
 

wup

lin

lch
res
similarity measure

jcn

random

figure        fold cross validation on the training set  origin   entity  theme   tool
and part   whole relations  

  

fiusing local alignments for relation recognition

content container
   

precision
recall
f score

  
  
  
  
  
  
  
  
  
 

wup

lin

lch
res
similarity measure

jcn

random

figure        fold cross validation on the training set  content   container relation  

f score is comparable to the performance reported by o seaghdha and copestake       
and by davidov and rappoport        

   discussion
in this section we revisit the goals that were stated at the end of section   and discuss our
findings in more detail 
    the la kernel for relation extraction
we have introduced the la kernel  which has proven to be effective for biomedical problems 
in the nlp domain and showed that it is well suited for relation extraction  in particular  the experiments in two different domains either outperform existing methods or yield
performance on par with existing state of the art kernels 
one of the motivations for using the la kernel in the relation extraction task is to
exploit prior knowledge  here  we explore two possibilities  distributional similarity and
information provided by wordnet 
      distributional similarity measures
in our setting  we consider three distributional measures that have already been studied
before  for instance  lee        uses them to detect similar nouns based on verb object
co occurrence pairs  the results suggest the jaccard coefficient  which is related to the
dice measure  to be one of the best performing measures followed by some others including
cosine  euclidean distance fell into the group with the largest error rates  given previous
work by lee         one would expect euclidean distance to achieve worse results than
  

fikatrenko  adriaans    van someren

the other two measures  indeed  on the lll corpus  the la kernel employing l  shows
a significant decrease in performance  as to the other measures  the method using dice
significantly outperforms the one based on the l  measure only on the lll corpus while
there is no significant improvement on the bc ppi data set  based on the experiments we
have conducted  we conclude that the la kernel using dice and cosine measures performs
similarly on the lll data set and the bc ppi corpus  given the results on various biomedical corpora  and different settings we have experimented with   we obtained experimental
support for choosing the dice or cosine measure over the euclidean distance 
      wordnet similarity measures
for generic relations  semantic relatedness plays a significant role  the difference in the
f score between models that use semantic relatedness and the kernel where the relatedness
values are generated randomly  baseline ii  amounts to nearly      all measures exhibit
different performance on the seven generic relations that we have considered  we can
observe  for instance  that wup  lch  and lin almost always yield the best results  no matter
what relation is considered  we found the resnik score and jiang and conraths measure
to yield lower results than other measures  even though the f scores per relation vary
quite substantially  by placing cause effect  theme tool  origin entity among
the most difficult relations to extract   two measures  wup and lch  are the top performing
measures for all seven relations  these two measures explore the wordnet taxonomy using
a length of the paths between two concepts  or their depth in the wordnet hierarchy and 
consequently  belong to the path based measures  the other three measures  res  lin and
jcn are information content based measures  and here relatedness between two concepts
is defined through the amount of information they share  our experiments with the la
kernel on generic relation recognition suggest that  in this particular case  the path based
measures should be preferred over the information content based measures 
we should stress  however  that this is the evaluation of the semantic relatedness measures in the context of relation recognition  and one can by no means draw a conclusion
that the top measures for other nlp tasks will stay the same  for example  budanitsky
and hirst        use semantic relatedness measures to detect malapropism and show that
jiang and conraths measure  jcn  yields the best results  followed by lins measure  lin  
and the one by leacock and chodorow  lch   and then by resniks measure  res   our
results are quite similar to their findings if we consider the res measure  but jcn is not on
the top of the accuracy ranking list for any of the seven semantic relations that we have
studied 
    factors and parameters that influence the la kernel performance
our experiments in two domains have shown that the la kernel either outperforms existing
methods on the same corpora  or yields performance on par with existing state of the art
kernels 
      baselines
an advantage of the la kernel over the bunescu shortest path method  baseline i  is that
it is capable of handling paths of different lengths  by allowing gaps and penalizing them 
  

fiusing local alignments for relation recognition

the final kernel matrix becomes less sparse  the shortest path approach also attempts to
generalize over the dependency paths  but it usually overgeneralizes which leads to high
recall scores  table   and table    but to poor overall performance  one explanation for
overgeneralization may be that this method accounts well for structural similarity  provided
sequences are of the same length  but fails to provide finer distinctions among dependency
paths  consider  for example  two sequences trip  makes  tram and coffee  makes
 guy  whereby the first path represents a negative instance of the product producer
relation and the second path corresponds to a positive one  even though they do not match
exactly  the elements that do not match are all nouns in singular  consequently  comparison
according to the shortest path method will result in a relatively high similarity score  in
contrast  the la kernel will consider similarity of the elements and the pairs trip coffee
and tram guy will obtain low scores 
in addition  baseline ii  which is based on randomly generated substitution scores 
performs poor for all data sets  or comparable to baseline i   this leads us to the conclusion
that accurate estimation of similarities is another reason why the la kernel performs well
on relation extraction 
      comparison with other methods
as we have already pointed out  the obvious shortcoming of baseline i is its inability to
handle dependency paths of different length  for this reason  we have also applied the
gap weighted string kernel  lodhi et al         to all data sets  in this case  dependency
paths can be compared in a flexible way because gapping is allowed  but no other additional
information is used  this kernel outperforms baseline i by increasing precision of relation
extraction while preserving a relatively high recall  the only data set where it fails to yield
good results is the lll test data  and we believe this is due to the differences in the lll
training and test data  for all data sets  the la kernel achieves better performance than
the gap weighted string kernel  the margin  however  is different for different data sets  in
the biomedical domain  the differences between the two methods can more clearly be seen
on the bc ppi and lll data sets  while the results on the aimed corpus are comparable 
however  other methods tested on aimed do not get higher scores unless they use more
features than just dependency paths  this holds for both types of cross validation used
on this corpus  for generic relations  the difference between the la kernel and the gapweighted string kernel is much larger  in particular  in the case of the gap weighted kernel 
precision is high  but recall is much lower  this can be explained by the fact that generic
relations benefit from the knowledge found in wordnet and recall achieved by the la kernel
is  therefore  high  the gap weighted kernel has access only to information found in the
dependency paths and  for this reason  fails to find more relations 
the la kernel also achieves the best performance on the lll training set  outperforming
the graph kernel  airola et al          the shallow linguistic kernel  giuliano et al        
and the rule based system by fundel et al          all three have used different input for
their methods  varying from plain text to dependency structures  for this reason  a direct
comparison is unfortunately not possible  but we can conclude that the methods employing
dependency information always are among the best performing approaches 

  

fikatrenko  adriaans    van someren

two other approaches whose performance has been reported on the aimed data set include the tree kernel  stre et al         and tsvm  erkan et al          both of them
explore syntactic information in different ways  while stre et al  consider subtrees  the
method of erkan et al  has more similarities with our approach because it relies on the
dependency path comparison  to do this comparison  they only use information already
available in the dependency paths  svm setting   or more dependency paths  tsvm setting   according to lauer and bloch         tsvms fall into the category using prior
knowledge by sampling methods  because it explores prior knowledge by generating new
examples  in contrast  we employ information from large unlabeled text sources in order to
enable finer comparison of the dependency paths and always work in the supervised learning
setting  using the same evaluation procedure as in the work of stre et al  and erkan et al 
we show that the la kernel outperforms both methods  but the differences on this data set
are much smaller than on the other data sets we have used 
      the la parameters
we have demonstrated that the choice of la parameters is crucial for achieving good performance  in our experiments  the scaling parameter  contributes to the overall performance
at most  but the other parameters such as gap values have to be taken into account as well 
when  approaches infinity  the la kernel approximates the smith waterman distance 
but increasing  does not necessarily have a positive impact on the final performance  this
finding is in line with the results reported by saigo et al         on the homology detection
task  the best performance is yielded by setting the scaling parameter to   or a bit higher 
and by penalizing the gap extension less than the gap opening 

   conclusions and future work
we have presented a novel approach to relation extraction that is based on the local alignments of sequences  using an la kernel provides us an opportunity to explore various
sources of information and to study their role in relation recognition  possible future directions include  therefore  an examination of other distributional similarity measures  studying
their impact on the extraction of generic relations  and looking for other sources of information which could be helpful for relation recognition  it may be interesting to consider
relational similarity  turney         which looks for the correspondence between relation
instances  in this case  one should be able to infer that doctor corresponds to scalpel in
a similar way as fisherman to net  where both  scalpel  doctor  and  net  fisherman  are
examples of instrument   agency  
despite the sparseness problem that might occur when wordnet based measures are
used  these measures have an advantage over the distributional measures by treating elements to be compared as concepts rather than words  in the nlp community  a few steps
have been already taken to solve this problem by clustering words in large corpora aiming
at word sense discovery  pennacchiotti   pantel         recently  mohammad        in
his thesis investigated the compatibility of distributional measures with ontological ones 
by using corpus statistics and a thesaurus  the author introduced distributional profiles of
senses and defined distance measures on them  even though this new approach to calculat 

  

fiusing local alignments for relation recognition

ing similarity was tested on generic corpora  it would be of a certain interest to apply it to
domain specific data 
overall  local alignment kernels provide a flexible means to work with data sequences 
first  they allow a partial match between sequences which is particularly important when
dealing with text  second  it is possible to incorporate prior knowledge in the learning
process while preserving kernel validity  in general  la kernels can be applied to other
nlp problems as long as the input data is in the form of sequences 

acknowledgments
the authors wish to thank simon carter and gerben de vries for their comments and
proofreading  and three anonymous reviewers for their highly valuable feedback  they
also acknowledge the input from the adaptive information management  aim  group at
the university of amsterdam  the preliminary version of this work has been dicussed
at the   nd international conference on computational linguistics  coling       and at
the seventh international tbilisi symposium on language  logic and computation        
this work was carried out in the context of the virtual laboratory for e science project
 www vl e nl   this project is supported by a bsik grant from the dutch ministry of
education  culture and science  oc w  and is part of the ict innovation program of the
ministry of economic affairs  ez  

references
airola  a   pyysalo  s   bjorne  j   pahikkala  t   ginter  f     salakoski  t          allpaths graph kernel for protein protein interaction extraction with evaluation of crosscorpus learning  bmc bioinformatics     suppl ii  
beamer  b   bhat  s   chee  b   fister  a   rozovskaya  a     girju  r          uiuc 
a knowledge rich approach to identifying semantic relations between nominals 
in proceedings of the workshop on semantic evaluations  semeval   prague  czech
republic 
bedmar  i  s   samy  d     martinez  j  l          uc m  classification of semantic relations
between nominals using sequential minimal optimization  in semeval      
budanitsky  a     hirst  g          evaluating wordnet based measures of lexical semantic
relatedness  computational linguistics               
bunescu  r  c          learning for information extraction  ph d  thesis  department of
computer sciences  university of texas at austin 
bunescu  r  c   ge  r   kate  r  j   marcotte  e  m   mooney  r  j   ramani  a  k    
wong  y  w          comparative experiments on learning information extractors for
proteins and their interactions  artificial intelligence in medicine             
bunescu  r  c     mooney  r  j       a   a shortest path dependency kernel for relation
extraction  in joint conference on human language technology   empirical methods
in natural language processing  hlt emnlp   vancouver  bc 

  

fikatrenko  adriaans    van someren

bunescu  r  c     mooney  r  j       b   subsequence kernels for relation extraction 
in proceedings of the   th conference on neural information processing systems 
vancouver  bc 
bunescu  r  c     mooney  r  j          text mining and natural language processing 
chap  extracting relations from text  from word sequences to dependency paths 
springer 
burges  c  j  c          a tutorial on support vector machines for pattern recognition 
data mining and knowledge discovery                
camacho  r          the use of background knowledge in inductive logic programming 
report 
cancedda  n   gaussier  e   goutte  c     renders  j  m          word sequence kernels 
journal of machine learning research              
chang  c  c     lin  c  j          libsvm  a library for support vector machines  software
available at http   www csie ntu edu tw  cjlin libsvm 
chen  s  f     goodman  j          an empirical study of smoothing techniques for language
modeling  in acl   
clegg  a  b          computational linguistic approaches to biological text mining  ph d 
thesis  university of london 
cohen  w  w   ravikumar  p     fienberg  s          a comparison of string distance
metrics for name matching tasks  in iiweb       pp       
collins  m          head driven statistical models for natural language parsing  ph d 
thesis  university of pennsylvania 
collins  m     duffy  n          convolution kernels for natural language  in advances in
neural information processing systems     pp          mit press 
cortes  c     vapnik  v          support vector networks  machine learning         
       
davidov  d     rappoport  a          classification of semantic relationships between
nominals using pattern clusters  in proceedings of acl    hlt  pp         
dolan  w  b   quirk  c     brockett  c          unsupervised construction of large paraphrase corpora  exploiting massively parallel news sources  in coling       geneva 
switzerland 
erkan  g   ozgur  a     radev  d  r          semi supervised classification for extracting
protein interaction sentences using dependency parsing  in      joint conference
on empirical methods in natural language processing and computational natural
language learning  pp         
fellbaum  c          wordnet  an electronic lexical database  mit press 
firth  j  r          a synopsis of linguistic theory           studies in linguistic analysis 
philological society  oxford  reprinted in palmer  f   ed         
fundel  k   kueffner  r     zimmer  r          relex   relation extraction using dependency
parse trees  bioinformatics         
  

fiusing local alignments for relation recognition

girju  r   badulescu  a     moldovan  d          automatic discovery of part whole relations  computational linguistics                
girju  r   nakov  p   nastase  v   szpakowicz  s   turney  p     yuret  d          semeval     task     classification of semantic relations between nominals  in acl      
girju  r   nakov  p   nastase  v   szpakowicz  s   turney  p     yuret  d          classification of semantic relations between nominals  language resources and evaluation 
               
giuliano  c   lavelli  a   pighin  d     romano  l          fbk irst  kernel methods for
semantic relation extraction  in semeval      
giuliano  c   lavelli  a     romano  l          exploiting shallow linguistic information
for relation extraction from biomedical literature  in eacl      
grishman  r     sundheim  b          message understanding conference      a brief
history  in proceedings of the   th international conference on computational linguistics 
haussler  d          convolution kernels on discrete structures  tech  rep  ucs crl       
uc santa cruz 
hearst  m          automatic acquisition of hyponyms from large text data  in proceedings
of coling     pp         
hendrickx  i   morante  r   sporleder  c     van den bosch  a          ilk  machine
learning of semantic relations with shallow features and almost no data  in semeval     
hersch  w   cohen  a  m   roberts  p     rakapalli  h  k          trec      genomics
track overview  in proceedings of the   th text retrieval conference 
jiang  j  j     conrath  d  w          semantic similarity based on corpus statistics
and lexical taxonomy  in proceedings of international conference on research in
computational linguistics  rocling x   pp       
joachims  t          transductive inference for text classification using support vector
machines  in proceedings of icml 
katrenko  s     adriaans  p          semantic types of some generic relation arguments 
detection and evaluation  in proceedings of the   th annual meeting of the association for computational linguistics  human language technologies  acl hlt  
columbus  usa 
khoo  c  s  g   chan  s     niu  y          extracting causal knowledge from a medical database using graphical patterns  in proceedings of the   th annual meeting
on association for computational linguistics  pp          morristown  nj  usa 
association for computational linguistics 
kim  s  n     baldwin  t          melb kb  nominal classifications as noun compound
interpretation  in semeval      
lauer  f     bloch  g          incorporating prior knowledge in support vector machines
for classification  a review  neurocomputing               
  

fikatrenko  adriaans    van someren

leacock  c     chodorow  m          combining local context and wordnet similarity for
word sense identification  mit press  cambridge  ma 
lease  m     charniak  e          parsing biomedical literature  in proceedings of ijcnlp 
lee  l          measures of distributional similarity  in proceedings of the   th annual meeting of the association for computational linguistics on computational linguistics 
pp       
leslie  c   eskin  e   cohen  a   weston  j     noble  w  s          mismatch string kernels
for discriminative protein classification  bioinformatics                 
leslie  c   eskin  e     noble  w  s          the spectrum kernel  a string kernel for svm
protein classification  in pacific symposium on biocomputing    pp         
leusch  g   ueffing  n     ney  h          a novel string to string distance measure with
applications to machine translation evaluation  in machine translation summit ix 
pp          new orleans  lo 
lin  d          an information theoretic definition of similarity  in proceedings of the   th
international conference on machine learning  pp         
lodhi  h   saunders  c   shawe taylor  j   christianini  n     watkins  c          text
classification using string kernels  journal of machine learning research            
mcdonald  r          extracting relations from unstructured text  tech  rep  ms cis       upenn 
melcuk  i          dpendency syntax  theory and practice  suny press 
mitchell  t          machine learning  mcgraw hill 
miyao  y   stre  r   sagae  k   matsuzaki  t     tsuji  j          task oriented evaluation
of syntactic parsers and their representations  in proceedings of acl    hlt  pp    
   
mohammad  s          measuring semantic distance using distributional profiles of concepts  ph d  thesis  graduate department of computer science of university of
toronto 
monge  a  e     elkan  c          the field matching problem  algorithms and applications 
in kdd       pp         
moschitti  a          efficient convolution kernels for dependency and constituent syntactic
trees  in ecml       pp         
nakov  p          ucb  system description for semeval task     in semeval      
nakov  p          paraphrasing verbs for noun compound interpretation  in proceedings of
the workshop on multiword expressions  mwe     in conjunction with the language
resources and evaluation conference  marrakech  morocco       
nakov  p     hearst  m  a          solving relational similarity problems using the web as
a corpus  in proceedings of acl    hlt 
nedellec  c          learning language in logic   genic interaction extraction challenge 
in proceedings of the learning language in logic workshop 
  

fiusing local alignments for relation recognition

needleman  s  b     wunsch  c  d          a general method applicable to the search for
similarities in the amino acid sequence of two proteins  journal of molecular biology 
               
nulty  p          ucd pn  classification of semantic relations between nominals using
wordnet and web counts  in semeval      
o seaghdha  d          semantic classification with wordnet kernels  in proceedings of the
north american chapter of the association for computational linguistics   human
language technologies conference  naacl hlt   boulder  co 
o seaghdha  d     copestake  a          semantic classification with distributional kernels 
in proceedings of coling       manchester  uk 
palmer  m     wu  z          verb semantics for english chinese translation  tech  rep  
technical report no  ms cis        department of computer   information science 
university of pennsylvania 
pedersen  t   patwardhan  s     michelizzi  j          wordnet  similarity   measuring the
relatedness of concepts  in proceedings of the nineteenth national conference on
artificial intelligence  aaai      pp            san jose  ca 
pennacchiotti  m     pantel  p          ontologizing semantic relations  in acl     proceedings of the   st international conference on computational linguistics and the
  th annual meeting of the association for computational linguistics  pp         
morristown  nj  usa  association for computational linguistics 
ponzetto  s  p     strube  m          knowledge derived from wikipedia for computing
semantic relatedness  journal of artificial intelligence research             
resnik  p          using information content to evaluate semantic similarity  in proceedings
of the   th international joint conference on artificial intelligence  pp         
stre  r   sagae  k     tsuji  j          syntactic features for protein protein interaction
extraction  in  nd international symposium on languages in biology and medicine 
pp          
sagae  k     tsujii  j          dependency parsing and domain adaptation with lr models
and parser ensembles  in proceedings of emnlp conll 
saigo  h   vert  j  p     akutsu  t          optimizing amino acid substitution matrices
with a local alignment kernel  bmc bioinformatics        
saigo  h   vert  j  p   ueda  n     akutsu  t          protein homology detection using
string alignment kernels  bioinformatics                    
sang  e  f  t  k   canisius  s   van den bosch  a     bogers  t          applying spelling
error correction techniques for improving semantic role labeling  in proceedings of the
ninth conference on natural language learning  conll       ann arbor  mi 
saunders  c   tschach  h     shawe taylor  j          syllables and other string kernel
extensions  in icml      
scholkopf  b          support vector learning  ph d  thesis  berlin technical university 

  

fikatrenko  adriaans    van someren

sekimizu  t   park  h  s     tsujii  j          identifying the interaction between genes
and gene products based on frequently seen verbs in medline abstracts  genome
informatics          
shawe taylor  j     christianini  n          support vector machines and other kernelbased learning methods  cambridge university press 
smith  l  h   yeganova  l     wilbur  w  j          hidden markov models and optimized
sequence alignment  computational biology and chemistry                 
smith  t  f     waterman  m  s          identification of common molecular subsequences 
journal of molecular biology              
snow  r   jurafsky  d     ng  a  y          learning named entity hyponyms for question
answering  in proceedings of coling acl 
swanson  d  r     smalheiser  n  r          implicit text linkages between medline records 
using arrowsmith as an aid to scientific discovery  library trends         
thomas  j   milward  d   ouzounis  c     pulman  s          automatic extraction of
protein interactions from scientific abstracts  in proceedings of pacific symposium on
biocomputing 
tribble  a     fahlman  s  e          cmu at  semantic distance and background knowledge for identifying semantic relations  in semeval      
turney  p  d          similarity of semantic relations  computational linguistics         
       
van der plas  l          automatic lexico semantic acquisition for question answering 
ph d  thesis  university of groningen 
van rijsbergen  c  j   robertson  s  e     porter  m  f          new models in probabilistic
information retrieval  tech  rep        british library research and development
report 
vapnik  v          estimation of dependences based on empirical data  new york 
springer verlag 
weeds  j   weir  d     mccarthy  d          characterising measures of lexical distributional similarity  in proceedings of coling      
zelenko  d   aone  c     richardella  a          kernel methods for relation extraction 
journal of machine learning research              
zhang  y   schneider  j     dubrawski  a          learning the semantic correlation  an
alternative way to gain from unlabeled text  in proceedings of the   nd conference
on neural information processing systems  vancouver  bc 

  

fi