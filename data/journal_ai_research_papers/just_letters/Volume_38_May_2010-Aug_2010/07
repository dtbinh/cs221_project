journal artificial intelligence research                  

submitted        published      

minimum relative entropy principle
learning acting
pedro a  ortega
daniel a  braun

peortega dcc uchile cl
dab   cam ac uk

department engineering
university cambridge
cambridge cb   pz  uk

abstract
paper proposes method construct adaptive agent universal
respect given class experts  expert designed specifically particular
environment  adaptive control problem formalized problem minimizing
relative entropy adaptive agent expert suitable
unknown environment  agent passive observer  optimal solution
well known bayesian predictor  however  agent active  past actions need
treated causal interventions i o stream rather normal probability
conditions  shown solution new variational problem given
stochastic controller called bayesian control rule  implements adaptive
behavior mixture experts  furthermore  shown mild assumptions 
bayesian control rule converges control law suitable expert 

   introduction
behavior environment control signal fully known 
designer choose agent produces desired dynamics  instances problem include hitting target cannon known weather conditions  solving maze
map controlling robotic arm manufacturing plant  however 
environment unknown  designer faces problem adaptive control 
example  shooting cannon lacking appropriate measurement equipment  finding
way unknown maze designing autonomous robot martian exploration 
adaptive control turns far difficult non adaptive counterpart 
good policy carefully trade explorative versus exploitative actions 
i e  actions identification environments dynamics versus actions control
desired way  even environments dynamics known belong particular class optimal agents available  constructing corresponding optimal
adaptive agent general computationally intractable even simple toy problems  duff 
       thus  finding tractable approximations major focus research 
recently  proposed reformulate problem statement classes
control problems based minimization relative entropy criterion  example 
large class optimal control problems solved efficiently problem statement
reformulated minimization deviation dynamics controlled system
uncontrolled system  todorov              kappen  gomez    opper        
work  similar approach introduced adaptive control  class agents
c
    
ai access foundation  rights reserved 

fiortega   braun

given  agent tailored different environment  adaptive controllers
derived minimum relative entropy principle  particular  one construct
adaptive agent universal respect class minimizing average relative
entropy environment specific agent 
however  extension straightforward  syntactical difference
actions observations taken account formulating variational
problem  specifically  actions treated interventions obeying rules
causality  pearl        spirtes  glymour    scheines        dawid         distinction
made  variational problem unique solution given stochastic control rule
called bayesian control rule  control rule particularly interesting
translates adaptive control problem on line inference problem applied
forward time  furthermore  work shows mild assumptions  adaptive
agent converges environment specific agent 
paper organized follows  section   introduces notation sets adaptive
control problem  section   formulates adaptive control minimum relative entropy
problem  initial  nave approach  need causal considerations motivated 
then  bayesian control rule derived revised relative entropy criterion 
section    conditions convergence examined proof given  section  
illustrates usage bayesian control rule multi armed bandit problem
undiscounted markov decision processes  section   discusses properties bayesian
control rule relates previous work literature  section   concludes 

   preliminaries
following agent environment formalized causal models i o
sequences  agent environment coupled exchange symbols following standard
interaction protocol discrete time  observation control signals  treatment
dynamics fully probabilistic  particular  actions observations
random variables  contrast typical decision theoretic agent formulation
treating observations random variables  russell   norvig         proofs
provided appendix 
notation  set denoted calligraphic letter a  words set   alphabet
element   symbol used mean thing respectively  strings finite
concatenations symbols sequences infinite
concatenations  denotes set


strings length n based a     n  set finite strings  furthermore      a  a         ai                defined set one way
infinite sequences based alphabet a  tuples written parentheses  a    a    a   
strings a  a  a    notation ai    a  a        ai shorthand string starting first index  also  symbols underlined glue together ao
aoi    a  o  a  o        ai oi   function log x  meant taken w r t  base    unless
indicated otherwise 
interactions  possible i o symbols drawn two finite sets  let denote
set inputs  observations  let denote set outputs  actions   set z    ao
interaction set  string aot ao t interaction string  optionally ending
   

fia minimum relative entropy principle learning acting

ot   ak ok o  similarly  one sided infinite sequence a  o  a  o       
interaction sequence  set interaction strings length denoted z  
sets  finite  interaction strings sequences denoted z z respectively 
interaction string length   denoted  
i o system  agents environments formalized i o systems  i o system
probability distribution pr interaction sequences z   pr uniquely determined
conditional probabilities
pr at  ao t   

pr ot  ao t  

   

aot z   conditional probabilities either represent generative law
 propensity  case issuing symbol evidential probability  plausibility 
case observing symbol  two interpretations applies particular case
becomes apparent i o system coupled another i o system 

agent
p

a  o  a  o  a  o  a  o  a  o 

environment
q

figure    model interactions  agent p environment q define probability distribution interaction sequences 

interaction system  let p  q two i o systems  interaction system  p  q 
coupling two systems giving rise generative distribution g describes
probabilities actually govern i o stream two systems coupled  g
specified equations
g at  ao t      p at  ao t  
g ot  ao t      q ot  ao t  
valid aot z   here  g models true probability distribution interaction
sequences arises coupling two systems i o streams  specifically 
system p  p at  ao t   probability producing action given history
ao t p ot  ao t   predicted probability observation ot given history
   

fiortega   braun

ao t   hence  p  sequence o  o        input stream sequence a  a       
output stream  contrast  roles actions observations reversed
case system q  thus  sequence o  o        output stream sequence
a  a        input stream  previous model interaction fairly general  many
interaction protocols translated scheme  convention  given
interaction system  p  q   p agent constructed designer  q
environment controlled agent  figure   illustrates setup 
control problem  environment q said known iff agent p property
aot z  
p ot  ao t     q ot  ao t   
intuitively  means agent knows statistics environments future
behavior past  particular  knows effects given controls 
environment known  designer agent build custom made policy
p resulting generative distribution g produces interaction sequences
desirable  done multiple ways  instance  controls chosen
resulting policy maximizes given utility criterion  resulting
trajectory interaction system stays close enough prescribed trajectory  formally 
q known  conditional probabilities p at  ao t   aot z
chosen resulting generative distribution g interaction sequences given

g at  ao t     p at  ao t  

g ot  ao t     q ot  ao t     p ot  ao t  
desirable  p said tailored q 
adaptive control problem  environment q unknown  task designing appropriate agent p constitutes adaptive control problem  specifically 
work deals case designer already class agents tailored
class possible environments  formally  assumed q going drawn
probability p  m  set q     qm  mm possible systems interaction starts  countable set  furthermore  one set p     pm  mm
systems m  pm tailored qm interaction system
 pm   qm   generative distribution gm produces desirable interaction sequences 
designer construct system p behavior close possible
custom made system pm realization qm q 

   adaptive systems
main goal paper show problem adaptive control outlined
previous section reformulated universal compression problem 
informally motivated follows  suppose agent p implemented machine
interfaced environment q  whenever agent interacts environment 
agents state changes necessary consequence interaction  change
state take place many possible ways  updating internal memory  consulting
   

fia minimum relative entropy principle learning acting

random number generator  changing physical location orientation  forth 
naturally  design agent facilitates interactions complicates others 
instance  agent designed explore natural environment  might
incur low memory footprint recording natural images 
memory inefficient recording artificially created images  one abstracts away
inner workings machine decides encode state transitions binary
strings  minimal amount resources bits required implement
state changes derived directly associated probability distribution p 
context adaptive control  agent constructed minimizes
expected amount changes necessary implement state transitions  equivalently 
maximally compresses experience  thereby  compression taken
stand alone principle design adaptive agents 
    universal compression nave construction adaptive agents
coding theory  problem compressing sequence observations unknown
source known adaptive coding problem  solved constructing universal compressors  i e  codes adapt on the fly source within predefined class
 mackay         codes obtained minimizing average deviation predictor true source  constructing codewords using predictor 
subsection  procedure used derive adaptive agent  ortega   braun        
formally  deviation predictor p true distribution pm measured
relative entropy     first approach would construct agent b
minimize total expected relative entropy pm   constructed follows  define
history dependent relative entropies action observation ot
x
pm  at  ao t  

 ao t     
dm
pm  at  ao t   log
pr at  ao t  



ot
 ao t     
dm

x
ot

pm  ot  ao t   log

pm  ot  ao t  
 
pr ot  ao t  

pm  ot  ao t     qm  ot  ao t   qm known pr
argument variational problem  then  one removes dependency past
averaging possible histories 
x


  
 ao t  
pm  ao t  dm
dm
ao t

ot
dm

  

x

ot
 ao t   
pm  ao t  dm

ao t

finally  total expected relative entropy pr pm obtained summing
time steps averaging choices true environment 
   lim sup


x

p  m 




x
  




 
  dm
dm

   

   relative entropy known kl divergence measures average amount extra
bits necessary encode symbols due usage  wrong  predictor 

   

fiortega   braun

using      one define variational problem respect pr  agent b one
looking system pr minimizes total expected relative entropy      i e 
b    arg min d pr  
pr

solution equation   system b defined set equations
x
b at  ao t    
pm  at  ao t  wm  ao t  


b ot  ao t    

x


pm  ot  ao t  wm  ao t  

   

   

valid aot z   mixture weights
p  m pm  ao t  

p  m  pm  ao t  
p  m pm  ao t  
 
wm  ao t      p

p  m  pm  ao t  
wm  ao t      p

   

reference  see work haussler opper        opper         clear
b bayesian mixture agents pm   one defines conditional
probabilities
p  at  m  ao t      pm  at  ao t  
   
p  ot  m  ao t      pm  at  ao t  

aot z   equation   rewritten
b at  ao t    
b ot  ao t    

x


x


p  at  m  ao t  p  m ao t     p  at  ao t  
p  ot  m  ao t  p  m ao t     p  ot  ao t  

   

p  m ao t     wm  ao t   p  m ao t     wm  ao t   posterior
probabilities elements given past interactions  hence  conditional
probabilities     minimize total expected divergence predictive
distributions p  at  ao t   p  ot  ao t   one obtains standard probability theory 
particular  bayes rule  interesting  provides teleological interpretation
bayes rule 
behavior b described follows  given time t  b maintains
mixture systems pm   weighting given mixture coefficients
wm   whenever new action new observation ot produced  by agent
environment respectively   weights wm updated according bayes rule 
addition  b issues action suggested system pm drawn randomly according
weights wt  
however  important problem b arises due fact
system passively observing symbols  actively generating them 
subjective interpretation probability theory  conditionals play role observations
   

fia minimum relative entropy principle learning acting

made agent generated external source  interpretation suits
symbols o    o    o          issued environment  however  symbols generated system require fundamentally different belief update 
intuitively  difference explained follows  observations provide information
allows agent inferring properties environment  contrast  actions
carry information environment  thus incorporated differently
belief agent  following section illustrate problem simple
statistical example 
    causality
causality study functional dependencies events  stands contrast
statistics  which  abstract level  said study equivalence dependencies
 i e  co occurrence correlation  amongst events  causal statements differ fundamentally
statistical statements  examples highlight differences many 
smokers get lung cancer  opposed smokers lung cancer   assign
f  x  opposed compare   f  x  programming languages  f m
opposed f   newtonian physics  study causality recently enjoyed
considerable attention researchers fields statistics machine learning 
especially last decade  significant progress made towards formal
understanding causation  shafer        pearl        spirtes et al         dawid        
subsection  aim provide essential tools required understand causal
interventions  in depth exposition causality  reader referred
specialized literature 
illustrate need causal considerations case generated symbols  consider
following thought experiment  suppose statistician asked design model
simple time series x    x    x          decides use bayesian method  assume
collects first observation x    x    computes posterior probability density function
 pdf  parameters model given data using bayes rule 
p  x    x      r

p x    x    p  
 
p x    x     p   

p x    x     likelihood x  given p   prior pdf  
use model predict next observation drawing sample x  predictive
pdf
z
p x    x   x    x      p x    x   x    x      p  x    x    d 
p x    x   x    x      likelihood x  given x    note x 
drawn p x    x   x    x       understands nature x  different
x    x  informative change belief state bayesian model 
x  non informative thus reflection models belief state  hence  would
never use x  condition bayesian model  mathematically  seems imply

p  x    x    x    x      p  x    x   
   

fiortega   braun

x  generated p x   x    x    itself  simple independence assumption correct following elaboration example show 
statistician told source waiting simulated data point x 
order produce next observation x    x  depend x    hands x 
obtains new observation x    using bayes rule  posterior pdf parameters

p x    x   x    x    x    x      p x    x     p  
r
   
p x    x   x    x    x    x      p x    x      p   
p x    x   x    x    x    x      likelihood new data x  given old
data x    parameters simulated data x    notice looks almost
posterior pdf p  x    x    x    x    x    x    given
r

p x    x   x    x    x    x      p x    x   x    x      p x    x     p  
p x    x   x    x    x    x      p x    x   x    x      p x    x      p   

exception latter case  bayesian update contains likelihoods
simulated data p x    x   x    x       suggests equation   variant
posterior pdf p  x    x    x    x    x    x    simulated data x  treated
different way data x  x   
define pdf p pdfs p     p  x      p  x   x    x      identical
p    p x     p x   x    x      respectively  differ p  x   x      
p  x   x         x  x    
dirac delta function  is  p identical p assumes
value x  fixed x  given x    p   simulated data x  non informative 
log  p  x    x   x          
one computes posterior pdf p   x    x    x    x    x    x     one obtains result
equation   
r

p  x    x   x    x    x    x      p  x    x   x    x      p  x    x     p   
p  x    x   x    x    x    x     p  x    x   x    x      p  x    x      p    
p x    x   x    x    x    x      p x    x     p  
 r
 
p x    x   x    x    x    x      p x    x      p   

thus  order explain equation   posterior pdf given observed data x  x 
generated data x    one intervene p order account fact x 
non informative given x    words  statistician  defining value
x  herself    changed  natural  regime brings series x    x    x          
mathematically expressed redefining pdf 
two essential ingredients needed carry interventions  first  one needs
know functional dependencies amongst random variables probabilistic model 
provided causal model  i e  unique factorization joint probability
   note conceptually broken two steps  first  samples x  p x   x    x    
second  imposes value x    x  setting p  x   x         x  x    

   

fia minimum relative entropy principle learning acting

distribution random variables encoding causal dependencies  general
case  defines partial order random variables  previous thought experiment  causal model joint pdf p   x    x    x    given set conditional
pdfs
p    p x      p x   x       p x   x    x      
second  one defines intervention sets x value x  denoted x x 
operation causal model replacing conditional probability x dirac
delta function  x x  kronecker delta xx continuous discrete variable x
respectively  thought experiment  easily seen
p    x    x    x    x    x    x      p   x    x    x  x    x    x   
thereby 
p   x    x    x    x    x    x      p  x    x    x  x    x    x    
causal models contain additional information available joint probability
distribution alone  appropriate model given situation depends story
told  note intervention lead different results respective causal
models differ  thus  causal model
p x     p x   x     p x   x    x     p  x    x    x   
intervention x  x  would differ p   i e 
p    x    x    x    x    x    x       p   x    x    x  x    x    x    
even though causal models represent joint probability distribution 
following  paper use shorthand notation x    x x random variable
obvious context 
    causal construction adaptive agents
following discussion previous section  adaptive agent p going constructed minimizing expected relative entropy expected pm   time
treating actions interventions  based definition conditional probabilities
equation    total expected relative entropy characterize p using interventions going defined  assuming environment chosen first  symbol depends
functionally environment previously generated symbols  causal model
given
p  m   p  a   m   p  o   m  a     p  a   m  a    o     p  o   m  a    o    a          
importantly  interventions index set intervened probability distributions derived
base probability distribution  hence  set fixed intervention sequences form
a    a          indexes probability distributions observation sequences o    o          
this  one defines set criteria indexed intervention sequences 
   

fiortega   braun

clear solution  define history dependent intervened relative
entropies action observation ot

 ao t     
cm

x


ot
 ao t     
cm

x
ot

p  at  m  ao t   log 

p  at  m  ao t  
pr at  ao t  

p  ot  m  ao t   log 

p  ot  m  ao t  
 
pr ot  ao t  

pr given arbitrary agent  note past actions treated interventions 
particular  p  at  m  ao t   represents knowledge state past actions already
issued next action known yet  then  averaging previous relative
entropies pasts yields

 
cm

x

ao t
ot
 
cm


 ao t  
p  ao t  m cm

x

ao t

ot
 ao t   
p  ao t  m cm

 ao   c ot  ao   
again  knowledge state time represented cm
 t
 t

averages taken treating past actions interventions  finally  define total exat   c ot   time  averaged
pected relative entropy pr pm sum  cm

possible draws environment 

c    lim sup


x

p  m 




x
  




  cm
cm
 

   

variational problem consists choosing agent p system pr minimizing
c   c pr   i e 
p    arg min c pr  
    
pr

following theorem shows variational problem unique solution 
central theme paper 
theorem    solution equation    system p defined set equations
x

p at  ao t     p  at  ao t    



p ot  ao t     p  ot  ao t    

p  at  m  ao t  vm  ao t  

x


p  ot  m  ao t  vm  ao t  

    

valid aot z   mixture weights
qt 

   p  o  m  ao   
 
qt 


   p  o  m   ao   
p  m  

vm  ao t     vm  ao t      p

p  m 

   

    

fia minimum relative entropy principle learning acting

bayesian control rule  given set operation modes  p   m    mm
interaction sequences z prior distribution p  m 
parameters m  probability action at   given
x
p  at    m  aot  p  m aot   
    
p  at    aot    


posterior probability operation modes given recursion
p  ot  m  ao t  p  m ao t  
 


p  ot  m   ao t  p  m  ao t  

p  m aot     p

table    summary bayesian control rule 
theorem says optimal solution variational problem      precisely
predictive distribution actions observations treating actions interventions
observations conditionals  i e  solution one would obtain applying
standard probability causal calculus  provides teleological interpretation
agent p akin nave agent b constructed section      behavior p differs
important aspect b  given time t  p maintains mixture systems
pm   weighting systems given mixture coefficients vm   contrast
b  p updates weights vm whenever new observation ot produced
environment  update follows bayes rule treats past actions interventions
dropping evidence provide  addition  p issues action suggested
system drawn randomly according weights vm  
    summary
adaptive control formalized problem designing agent unknown environment chosen class possible environments  environment specific agents
known  bayesian control rule allows constructing adaptive agent combining
agents  resulting adaptive agent universal respect environment
class  context  constituent agents called operation modes adaptive
agent  represented causal models interaction sequences  i e  conditional
probabilities p  at  m  ao t   p  ot  m  ao t   aot z  
index parameter characterizing operation mode  probability distribution
input stream  output stream  called hypothesis  policy  operation mode 
table   collects essential equations bayesian control rule  particular 
rule stated using recursive belief update 

   convergence
aim section develop set sufficient conditions convergence
provide proof convergence  simplify exposition  analysis limited
   

fiortega   braun

case controllers finite number input output models 

    policy diagrams
following use policy diagrams useful informal tool analyze effect
policies environments  figure   illustrates example 

state space


ao



policy

figure    policy diagram  one imagine environment collection states
connected transitions labeled i o symbols  zoom highlights state
taking action collecting observation leads state  
sets states transitions represented enclosed areas similar venn
diagram  choosing particular policy environment amounts partially
controlling transitions taken state space  thereby choosing probability
distribution state transitions  e g  markov chain given environmental
dynamics   probability mass concentrates certain areas state space 
choosing policy thought choosing subset environments
dynamics  following  policy represented subset state space
 enclosed directed curve  illustrated above 

policy diagrams especially useful analyze effect policies different hypotheses environments dynamics  agent endowed set operation
modes seen hypotheses environments underlying dynamics 
given observation models p  ot  m  ao t    associated policies  given action models p  at  m  ao t    m  sake simplifying interpretation
policy diagrams  assume existence state space    a o  mapping
i o histories states  note however assumptions made obtain
results section 
    divergence processes
central question section investigate whether bayesian control rule converges correct control law not  is  whether p  at  aot   p  at  m   ao t  
true operation mode  i e  operation mode p  ot  m   ao t    
q ot  ao t    obvious discussion rest section 
general true 
easily seen equation     showing convergence amounts show
posterior distribution p  m ao t   concentrates probability mass subset operation
   

fia minimum relative entropy principle learning acting

modes essentially output stream  
x
x
p  at  m  ao t  p  m ao t  
p  at  m   ao t  p  m ao t   p  at  m   ao t   
mm

mm

hence  understanding asymptotic behavior posterior probabilities
p  m aot  
crucial here  particular  need understand conditions quantities
converge zero  posterior rewritten
q
p  aot  m p  m 
p  m     p  o  m  ao   
 
 p
p  m aot     p
qt




p  aot  m  p  m  
p  m  
   p  o  m   ao   

summands one index dropped denominator  one
obtains bound
p  m aot  


p  m  p  o  m  ao   
 
p  m  
p  o  m   ao   
  

valid m  inequality  seen convenient
analyze behavior stochastic process


dt  m km    


x
  

ln

p  o  m   ao   
p  o  m  ao   

divergence process reference   indeed  dt  m km 
 

p  m 
p  m  p  o  m  ao   

  lim
edt  m km      



p  m  
p  o  m   ao    p  m  

lim

  

thus clearly p  m aot      figure   illustrates simultaneous realizations
divergence processes controller  intuitively speaking  processes provide lower
bounds accumulators surprise value measured information units 
divergence process random walk whose value time depends whole
history time t   makes divergence processes cumbersome characterize
fact statistical properties depend particular policy applied 
hence  given divergence process different growth rates depending policy
 figure     indeed  behavior divergence process might depend critically
distribution actions used  example  happen divergence process
stays stable one policy  diverges another  context bayesian
control rule problem aggravated  time step  policy
applied determined stochastically  specifically  true operation mode 
dt  m km  random variable depends realization aot drawn




  

p  a  m   ao  p  o  m   ao   
   

fiortega   braun

dt
 
 
 
 


 

figure    realization divergence processes     associated controller
operation modes m  m    divergence processes     diverge  whereas  
  stay dotted bound  hence  posterior probabilities m 
m  vanish 

dt
 

 
 

 

 
 

 


figure    application different policies lead different statistical properties
divergence process 

   

fia minimum relative entropy principle learning acting

m    m            mt drawn p  m     p  m   ao             p  mt  ao t   
deal heterogeneous nature divergence processes  one introduce
temporal decomposition demultiplexes original process many sub processes
belonging unique policies  let nt                   t  set time steps time t 
let nt   let m  m  define sub divergence dt  m km  random variable
x p  o  m   ao  
 
gm  m      
ln
p  o  m  ao   


drawn

pm   ao     ao       







p  a  m   ao   
p  o  m   ao     


   nt    ao   given conditions kept constant 
definition  plays role policy used sample actions time
steps   clearly  realization divergence process dt  m km  decomposed
sum sub divergences  i e 
x
gm  m  tm   
dt  m km   
    


 tm  mm forms partition nt   figure   shows example decomposition 
dt
 
 
 



 

figure    decomposition divergence process     sub divergences         
averages sub divergences play important role analysis  define
average realizations gm  m   
x
pm   ao     ao    gm  m    
gm  m      
 ao  

notice nt  
x
p  o  m   ao   
p  a  m   ao   p  o  m   ao    ln
  
gm  m        
p  o  m  ao   
ao


gibbs inequality  particular 

gm  m            

clearly  holds well nt  


gm  m      

gm  m         
   

    

fiortega   braun

    boundedness
general  divergence process complex  virtually classes distributions
interest control go well beyond assumptions i i d  stationarity 
increased complexity jeopardize analytic tractability divergence process 
predictions asymptotic behavior made anymore  specifically 
growth rates divergence processes vary much realization realization  posterior distribution operation modes vary qualitatively
realizations  hence  one needs impose stability requirement akin ergodicity limit
class possible divergence processes class analytically tractable 
purpose following property introduced 
divergence process dt  m km  said bounded variation iff     
c    m  nt




figm  m    gm  m   fi c
probability    

dt

 

 

 



 

figure    divergence process bounded variation  realizations  curves    
   sub divergence stay within band around mean  curve    
figure   illustrates property  boundedness key property going
used construct results section  first important result posterior
probability true input output model bounded below 
theorem    let set operation modes controller
divergence process dt  m km  bounded variation  then           
n 

p  m  aot  
 m 

probability    
    core

one wants identify operation modes whose posterior probabilities vanish 
enough characterize modes whose hypothesis match
true hypothesis  figure   illustrates problem  here  three hypotheses along
associated policies shown  h  h  share prediction made region differ
   

fia minimum relative entropy principle learning acting

region b  hypothesis h  differs everywhere others  assume h  true  long
apply policy p    hypothesis h  make wrong predictions thus divergence
process diverge expected  however  evidence h  accumulated 
one applies policy p  long enough time controller eventually
enter region b hence accumulate counter evidence h   
h 

h 

b


h 

b


p 

p 
p 

figure    hypothesis h  true agrees h  region a  policy p  cannot
disambiguate three hypotheses 

long enough mean  p  executed short period 
controller risks visiting disambiguating region  unfortunately  neither right
policy right length period run known beforehand  hence  agent
needs clever time allocating strategy test policies finite time intervals 
motivates following definition 
core operation mode   denoted  m    subset containing
operation modes behaving policy  formally  operation mode

   m    i e  core  iff c             t  n 
t   
gm  m    c
probability     gm  m    sub divergence dt  m km   pr 
  nt  
words  agent apply policy time step probability
least   strategy expected sub divergence gm  m    dt  m km  grows
unboundedly  core   note demanding strictly positive
probability execution time step guarantees agent run
possible finite time intervals  following theorem shows  posterior probabilities
operation modes core vanish almost surely 
theorem    let set operation modes agent
divergence process dt  m km  bounded variation 
   m    p  m aot    
almost surely 
    consistency
even operation mode core   i e  given essentially indistinguishable control  still happen different
policies  figure   shows example this  hypotheses h  h  share region
   

fiortega   braun

differ region b  addition  operation modes policies p  p  respectively confined region a  note operation modes core other 
however  policies different  means unclear whether multiplexing
policies time ever disambiguate two hypotheses  undesirable  could
impede convergence right control law 
h 

h 
b

b
p 

p 





figure    example inconsistent policies  operation modes core
other  different policies 

thus  clear one needs impose restrictions mapping hypotheses policies  respect figure    one make following observations 
   operation modes policies select subsets region a  therefore 
dynamics preferred dynamics b 
   knowing dynamics preferred dynamics b allows us
drop region b analysis choosing policy 
   since hypotheses agree region a  choose policy order
consistent selection criterion 
motivates following definition  operation mode said consistent
iff  m   implies      t    t 
ao t  




fip  at  m  aot   p  at  m   aot  fi    

words  core   ms policy converge policy 
following theorem shows consistency sufficient condition convergence
right control law 
theorem    let set operation modes agent that 
divergence process dt  m km  bounded variation  m  m  consistent
  then 
p  at  ao t   p  at  m   ao t  
almost surely  
   

fia minimum relative entropy principle learning acting

    summary
section  proof convergence bayesian control rule true operation
mode provided finite set operation modes  convergence result
hold  two necessary conditions assumed  boundedness consistency  first one 
boundedness  imposes stability divergence processes partial influence
policies contained within set operation modes  condition regarded
ergodicity assumption  second one  consistency  requires hypothesis makes
predictions another hypothesis within relevant subset dynamics 
hypotheses share policy  relevance formalized core
operation mode  concepts proof strategies strengthen intuition potential
pitfalls arise context controller design  particular could show
asymptotic analysis recast study concurrent divergence processes
determine evolution posterior probabilities operation modes  thus abstracting
away details classes i o distributions  extension results
infinite sets operation modes left future work  example  one could think
partitioning continuous space operation modes essentially different regions
representative operation modes subsume neighborhoods  grunwald        

   examples
section illustrate usage bayesian control rule two examples
common reinforcement learning literature  multi armed bandits markov
decision processes 
    bandit problems
consider multi armed bandit problem  robbins         problem stated follows 
suppose n  armed bandit  i e  slot machine n levers  pulled  lever
provides reward drawn bernoulli distribution bias hi specific lever 
is  reward r     obtained probability hi reward r     probability
 hi   objective game maximize time averaged reward iterative
pulls  continuum range stationary strategies  one parameterized n
probabilities  si  n
i   indicating probabilities pulling lever  difficulty arising
bandit problem balance reward maximization based knowledge already
acquired attempting new actions improve knowledge  dilemma known
exploration versus exploitation tradeoff  sutton   barto        
ideal task bayesian control rule  possible bandit
known optimal agent  indeed  bandit represented n  dimensional bias vector
   m            mn           n   given bandit  optimal policy consists
pulling lever highest bias  is  operation mode given by 
hi   p  ot     m    i    mi

si   p  at   i m   

   

 

    maxj  mj   
  else 

fiortega   braun

m 

 

a 

 

b 
m  m 

m 

 

 

m  m     

m 
 

m 

 
 

m 

 

 

figure    space bandit configurations partitioned n regions according
optimal lever  panel b show   armed   armed bandit cases
respectively 

apply bayesian control rule  necessary fix prior distribution
bandit configurations  assuming uniform distribution  bayesian control rule
z
    
p  at     i m p  m aot  
p  at     i aot    


update rule given
q
r
n

mj j    mj  fj
p  m     p  o  m   
 
p  m aot     r
qt



b rj      fj     
   p  o  m     dm
p  m  
j  

    

rj fj counts number times reward obtained
pulling lever j number times reward obtained respectively  observe
summation discrete operation modes replaced integral
continuous space configurations  last expression see posterior
distribution lever biases given product n beta distributions  thus 
sampling action amounts first sample operation mode obtaining bias
mi beta distribution parameters ri          choosing
action corresponding highest bias   arg maxi mi   pseudo code seen
algorithm   
simulation  bayesian control rule described compared two
agents   greedy strategy decay  on line  gittins indices  off line  
test bed consisted bandits n      levers whose biases drawn uniformly
beginning run  every agent play      runs      time steps each 
then  performance curves individual runs averaged   greedy strategy
selects random action small probability given otherwise plays
lever highest expected reward  parameters determined empirically
values               several test runs  adjusted way
maximize average performance last trials simulations  gittins
method  indices computed horizon      using geometric discounting
         i e  close one approximate time averaged reward  results
shown figure    
   

fia minimum relative entropy principle learning acting

algorithm   bcr bandit 
             n
initialize ri zero 
end
                
sample using      
  interaction  
set arg maxi mi issue a 
obtain environment 

avg  reward

 update belief 
   
ra   ra    
else
fa   fa    
end
end

    
    

bayesian control rule
 greedy
gittins indices

    
    
 

   

   

   

   

    

 

   

   

   

   

    

  best lever

   
  
  
  
  
 

figure     comparison n  armed bandit problem bayesian control rule  solid
line    greedy agent  dashed line  using gittins indices  dotted line  
      runs averaged  top panel shows evolution average
reward  bottom panel shows evolution percentage times
best lever pulled 

   

fiortega   braun

seen  greedy strategy quickly reaches acceptable level performance 
seems stall significantly suboptimal level  pulling optimal lever    
time  contrast  gittins strategy bayesian control rule show essentially asymptotic performance  differ initial transient phase
gittins strategy significantly outperforms bayesian control rule  least three
observations worth making here  first  gittins indices pre computed
off line  time complexity scales quadratically horizon  computations
horizon      steps took several hours machines  contrast  bayesian
control rule could applied without pre computation  second  even though gittins
method actively issues optimal information gathering actions bayesian control
rule passively samples actions posterior distribution operation modes 
end methods rely convergence underlying bayesian estimator 
implies methods information bottleneck  since bayesian estimator requires amount information converge  thus  active information gathering
actions affect utility transient phase  permanent state  efficient algorithms bandit problems found literature  auer  cesabianchi   
fischer        
    markov decision processes
markov decision process  mdp   defined tuple  x   a  t  r   x state space 
action space  ta  x  x     pr x  a  x  probability action
taken state x x lead state x x   r x  a  r    r immediate
reward obtained state x x action a  interaction proceeds time steps
              time t  action issued state xt  x   leading reward
rt   r xt      new state xt starts next time step      stationary closedloop control policy   x assigns action state  mdps always
exists optimal stationary deterministic policy thus one needs consider
policies  undiscounted mdps average rewardpper time step fixed policy
initial state x defined  x    limt e    t    r    shown  bertsekas 
       x     x   x  x x assumption markov chain
policy ergodic  here  assume mdps ergodic stationary policies 
order keep intervention model particularly simple    follow q notation
watkins         optimal policy characterized terms optimal
average reward optimal relative q values q x  a  state action pair  x  a 
solutions following system non linear equations  singh        

   brute force adaptive agent problem would roughly look follows  first  agent
starts prior distribution mdps  e g  product dirichlet distributions transition
probabilities  then  cycle  agent samples full transition matrix distribution
solves using dynamic programming  computed optimal policy  uses issue
next action  discards policy  subsequently  updates distribution mdps using
next observed state  however  main text follow different approach avoids solving
mdp every time step 

   

fia minimum relative entropy principle learning acting

state x x action a 
q x  a      r x  a   

x

x x


h

q x
 

 
pr x  x  a  max





h

  r x  a    ex max
x 

 
q x
 

 



    



optimal policy defined  x     arg maxa q x  a  state x x  
setup allows straightforward solution bayesian control rule 
learnable mdp  characterized q values average reward 
known solution   accordingly  operation mode given    q     
r a  o      obtain likelihood model inference m  realize equation   
rewritten predicts instantaneous reward r x  a  sum mean
instantaneous reward plus noise term given q values average reward
mdp labeled
r x  a    q x  a    max
q x       max
q x     e max
q x     x  a 



 
 z
 
 z
 
 
noise

mean instantaneous reward  x a x  

assuming reasonably approximated normal distribution n      p 
precision p  write likelihood model immediate reward r using
q values average reward  i e 
r

n p
p

 
p  r m  x  a  x    
    
exp  r  x  a  x     
 
 

order determine intervention model operation mode  simply exploit
properties q values  gives
 
    arg maxa q x   
p  a m  x   
    
  else 
apply bayesian control rule  posterior distribution p  m at   xt   needs
computed  fortunately  due simplicity likelihood model  one easily devise
conjugate prior distribution apply standard inference methods  see appendix a     actions determined sampling operation modes posterior executing
action suggested corresponding intervention models  resulting algorithm
similar bayesian q learning  dearden  friedman    russell        dearden  friedman    andre         differs way actions selected  pseudo code listed
algorithm   
simulation  tested mdp agent grid world example  give intuition
achieved performance  results contrasted achieved r learning 
used r learning variant presented work singh        algorithm   
together uncertainty exploration strategy  mahadevan         corresponding
update equations

q x  a      q x  a    r   max
q x    



    

       r   max
q x
 

 

q x 
a 
 



   

fiortega   braun

algorithm   bcr mdp gibbs sampler 
initialize entries zero 
set initial state x x   
                
 gibbs sweep 
sample using      
q y  b  visited states
sample q y  b  using      
end
  interaction  
set arg maxa q x    issue a 
obtain    r  x   environment 
 update hyperparameters 
  x a x   p r
 x  a  x    x a x
 x a x   p
 x  a  x    x  a  x     p
set x x  
end

goal

membranes

b  bayesian control rule

c  r learning  c  

d  r learning  c   

initial       steps

a   x  maze

e  r learning  c    

f  average reward
   
   

c   
c  

   

low
probability

c    
   

last       steps

high
probability

bayesian control rule
   
 

   

   

   

   

x     time steps

figure     results    grid world domain  panel  a  illustrates setup  columns
 b   e  illustrate behavioral statistics algorithms  upper lower
row calculated first last       time steps randomly
chosen runs  probability state color encoded  arrows
represent frequent actions taken agents  panel  f  presents
curves obtained averaging ten runs 

   

fia minimum relative entropy principle learning acting

average reward
bcr
r learning  c      
r learning  c     
r learning  c    

             
             
             
             

table    average reward attained different algorithms end run 
mean standard deviation calculated based    runs 

      learning rates  exploration strategy chooses fixed probability
c
pexp     action maximizes q x  a    f  x a 
  c constant  f  x  a 
represents number times action tried state x  thus  higher values
c enforce increased exploration 
study  mahadevan         grid world described especially useful
test bed analysis rl algorithms  purposes  particular interest
easy design experiments containing suboptimal limit cycles  figure     panel
 a   illustrates     grid world  controller learn policy leads
initial location goal state  step  agent move adjacent
space  up  down  left right   agent reaches goal state next position
randomly set square grid  with uniform probability  start another trial 
one way membranes allow agent move one direction
other  experiments  membranes form inverted cups
agent enter side leave bottom  playing role
local maximum  transitions stochastic  agent moves correct square
 
probability p     
free adjacent spaces  uniform distribution 
 
probability   p        rewards assigned follows  default reward r     
agent traverses membrane obtains reward r      reaching goal state
assigns r        parameters chosen simulation following 
mdp agent  chosen hyperparameters             precision p     
r learning  chosen learning rates                exploration
constant set c      c      c        total    runs carried
algorithm  results presented figure    table    r learning
learns optimal policy given sufficient exploration  panels   e  bottom row   whereas
bayesian control rule learns policy successfully  figure   f  learning curve
r learning c     c      initially steeper bayesian controller  however 
latter attains higher average reward around time step         onwards  attribute
shallow initial transient phase distribution operation modes
flat  reflected initially random exploratory behavior 

   discussion
key idea work extend minimum relative entropy principle  i e 
variational principle underlying bayesian estimation  problem adaptive control 
   

fiortega   braun

coding point view  work extends idea maximal compression
observation stream whole experience agent containing agents actions
observations  minimizes amount bits write saving encoding
i o stream  minimizes amount bits required produce decode
action  mackay        ch     
extension non trivial  important caveat coding i o sequences  unlike observations  actions carry information could used
inference adaptive coding actions issued decoder itself  problem
inference ones actions logically inconsistent leads paradoxes
 nozick         seemingly innocuous issue turned intricate
investigated intensely recent past researchers focusing issue
causality  pearl        spirtes et al         dawid         work contributes body
research providing evidence actions cannot treated using probability
calculus alone 
causal dependencies carefully taken account  minimizing relative
entropy leads rule adaptive control called bayesian control rule 
rule allows combining class task specific agents agent universal
respect class  resulting control law simple stochastic control rule
completely general parameter free  analysis paper shows  control
rule converges true control law mild assumptions 
    critical issues
causality  virtually every adaptive control method literature successfully treats
actions conditionals observation streams never worries causality 
thus  bother interventions  decision theoretic setup  decision
maker chooses policy maximizing
p expected utility u outcomes
  i e     arg max e u      pr   u     choosing formally
equivalent choosing kronecker delta function probability distribution
policies  case  conditional probabilities pr    pr    coincide 
since
pr       pr  pr      pr      pr     
sense  choice policy causally precedes interactions 
discussed section   however  uncertainty policy  i e  pr     
   causal belief updates crucial  essentially  problem arises
uncertainty policy resolved interactions  hence  treating
actions interventions seamlessly extends status random variables 
prior probabilities likelihood models policies come from  predictor
bayesian control rule essentially bayesian predictor thereby entails  almost  modeling paradigm  designer define class hypotheses
environments  construct appropriate likelihood models  choose suitable
prior probability distribution capture models uncertainty  similarly  sufficient domain knowledge  analogous procedure applied construct suitable
operation modes  however  many situations difficult even
   

fia minimum relative entropy principle learning acting

intractable problem itself  example  one design class operation modes
pre computing optimal policies given class environments  formally  let
class hypotheses modeling environments let class policies  given
utility criterion u   define set operation modes     m   constructing operation mode               arg max e u      
however  computing optimal policy many cases intractable 
cases  remedied characterizing operation modes optimality
equations solved probabilistic inference example mdp
agent section      recently  applied similar approach adaptive control
problems linear quadratic regulators  braun   ortega        
problems bayesian methods  bayesian control rule treats adaptive control
problem bayesian inference problem  hence  problems typically associated
bayesian methods carry agents constructed bayesian control
rule  problems analytical computational nature  example 
many probabilistic models posterior distribution
closed form solution  also  exact probabilistic inference general computationally
intensive  even though large literature efficient approximate inference algorithms particular problem classes  bishop         many
suitable on line probabilistic inference realistic environment classes 
bayesian control rule versus bayes optimal control  directly maximizing  subjective  expected utility given environment class minimizing
expected relative entropy given class operation modes  two methods
based different assumptions optimality principles  such  bayesian
control rule bayes optimal controller  indeed  easy design experiments
bayesian control rule converges exponentially slower  or converge
all  bayes optimal controller maximum utility  consider following
simple example  environment   k state mdp k consecutive actions
reach state reward     interception b action leads back
initial state  consider second environment first actions
b interchanged  bayes optimal controller figures true environment k
actions  either k consecutive bs   consider bayesian control rule 
optimal action environment   a  environment   b  uniform             prior
operation modes stays uniform posterior long reward
observed  hence bayesian control rule chooses time step b
equal probability  policy takes  k actions accidentally choose
row  or bs  length k  bayesian control rule optimal
too  bayes optimal controller converges time k  bayesian control
rule needs exponentially longer  one way remedy problem might allow
bayesian control rule sample actions operation mode several
time steps row rather randomizing controllers every cycle  however 
one considers non stationary environments strategy
break down  consider  example  increasing mdp k        bayes optimal
controller converges     steps  bayesian control rule converge
realizations  boundedness assumption violated 
   

fiortega   braun

    relation existing approaches
ideas underlying work unique bayesian control rule 
following selection previously published work recent bayesian reinforcement
learning literature related ideas found 
compression principles  literature  important amount work
relating compression intelligence  mackay        hutter      b   particular 
even proposed compression ratio objective quantitative measure
intelligence  mahoney         compression used basis theory
curiosity  creativity beauty  schmidhuber        
mixture experts  passive sequence prediction mixing experts studied
extensively literature  cesa bianchi   lugosi         study onlinepredictors  hutter      a   bayes optimal predictors mixed  bayes mixtures
used universal prediction  hutter         control case  idea
using mixtures expert controllers previously evoked models
mosaic architecture  haruno  wolpert    kawato         universal learning
bayes mixtures experts reactive environments studied work
poland hutter        hutter        
stochastic action selection  idea using actions random variables 
problems entails  expressed work hutter      b  problem
      study section   regarded thorough investigation open
problem  stochastic action selection approaches found thesis wyatt        examines exploration strategies  po mdps  learning automata
 narendra   thathachar        probability matching  duda  hart    stork 
      amongst others  particular  thesis discusses theoretical properties
extension probability matching context multi armed bandit problems 
there  proposed choose lever according likely optimal
shown strategy converges  thus providing simple method guiding
exploration 
relative entropy criterion  usage minimum relative entropy criterion
derive control laws underlies kl control methods developed work todorov
             kappen et al          there  shown large class
optimal control problems solved efficiently problem statement
reformulated minimization deviation dynamics controlled
system uncontrolled system  related idea conceptualize planning
inference problem  toussaint  harmeling    storkey         approach based
equivalence maximization expected future return likelihood
maximization applicable mdps pomdps  algorithms based
duality become active field current research  see example work
rasmussen deisenroth         fast model based rl techniques
used control continuous state action spaces 
   

fia minimum relative entropy principle learning acting

   conclusions
work introduces bayesian control rule  bayesian rule adaptive control 
key feature rule special treatment actions based causal calculus
decomposition adaptive agent mixture operation modes  i e  environmentspecific agents  rule derived minimizing expected relative entropy
true operation mode carefully distinguishing actions observations  furthermore  bayesian control rule turns exactly predictive distribution
next action given past interactions one would obtain using probability
causal calculus  furthermore  shown agents constructed bayesian
control rule converge true operation mode mild assumptions  boundedness 
related ergodicity  consistency  demanding two indistinguishable hypotheses share policy 
presented bayesian control rule way solve adaptive control problems
based minimum relative entropy principle  thus  bayesian control rule either
regarded new principled approach adaptive control novel optimality
criterion heuristic approximation traditional bayes optimal control  since
takes similar form bayes rule  adaptive control problem could translated
on line inference problem actions sampled stochastically posterior
distribution  important note  however  problem statement formulated
usual bayes optimal approach adaptive control same 
future relationship two problem statements deserves investigation 

acknowledgments
thank marcus hutter  david wingate  zoubin ghahramani  jose aliste  jose donoso 
humberto maturana anonymous reviewers comments earlier versions
manuscript and or inspiring discussions  thank ministerio de planificacion de chile
 mideplan  bohringer ingelheim fonds  bif  funding 

appendix a  proofs
a   proof theorem  
proof  proof follows line argument solution equation  
crucial difference
treated interventions  consider without loss
p actions
equation    note relative entropy
generality summand p  m cm
written difference two logarithms  one term depends pr varied 
therefore  one pull term write constant c  yields

c

x


p  m 

x

ao t

p  ao t  m 

x


   

p  at  m  ao t   ln pr at  ao t   

fiortega   braun

substituting p  ao t  m  p  m ao t  p  ao t   p  m  using bayes rule rearrangement terms leads
xx
x
 c
p  m ao t  p  ao t  
p  at  m  ao t   ln pr at  ao t  
ao t

 c

x

p  ao t  



x


ao t

p  at  ao t   ln pr at  ao t   

p
inner sum form x p x  ln q x   i e  cross entropy q x  p x  
minimized q x    p x  x  let p denote optimum distribution
pr  choosing optimum one obtains p at  ao t     p  at  ao t     note
solution variational problem p
independent p
weighting p  ao t    since


argument applies summand p  m cm
p  m cm equation   
variational problems mutually independent  hence 
p at  ao t     p  at  ao t  

p ot  ao t     p  ot  ao t  

aot z   p  at  ao t    introduce variable via marginalization
apply chain rule 
x
p  at  ao t    
p  at    m  ao t  p  m ao t   


term p  m aot   developed

p  ao t  m p  m 


p  ao t  m  p  m  
qt 
p  m     p  a  m  ao   p  o  m  ao   
 p
qt 



p  m  
   p  a  m   ao   p  o  m   ao   
qt 
p  m     p  o  m  ao   
 
 p
qt 


p  m  
   p  o  m   ao   

p  m ao t     p

first equality obtained applying bayes rule second using chain
rule probabilities  get last equality  one applies interventions causal
factorization  thus  p  a  m  ao        p  o  m  ao      p  o  m  ao    
equations characterizing p  ot  ao t   obtained similarly 
a   proof theorem  
proof  pointed       particular realization divergence process
dt  m km  decomposed
x
dt  m km   
gm  m   tm   


gm  m   tm   sub divergences dt  m km  tm form partition nt  
however  since dt  m km  bounded variation m  one     
c m     m  nt nt   inequality




figm  m   tm   gm  m   tm  fi c m 
   

fia minimum relative entropy principle learning acting

holds probability     however  due      
gm  m   tm    
m  thus 

gm  m   tm   c m  

previous inequalities hold simultaneously divergence process
bounded well  is  inequality
dt  m km  c m 

    

holds probability     m     m   choose
 m 
 m     max    ln pp m
    
 m 
since   ln pp m
using
   m   added right hand side      

definition dt  m km   taking exponential rearranging terms one obtains


p  m  






  

 m 

p  o  m   ao    e

p  m 




  

p  o  m  ao   

 m     c m     m     identifying posterior probabilities
dividing sides normalizing constant yields inequality
p  m  aot   e m  p  m aot   
 

inequality holds simultaneously probability     m
particular    minm  e m     is 
p  m  aot   p  m aot   
since valid m  maxm  p  m aot   
p  m  aot  

 
m 

one gets


 


probability
  arbitrary     related equation   

m 
 
   
a   proof theorem  
proof  divergence process dt  m km  decomposed sum sub divergences
 see equation    
x
gm  m  tm   
    
dt  m km   




furthermore  every
m  one      c   
n nt




figm  m    gm  m   fi c m 
   

fiortega   braun

probability     applying bound summands      yields lower
bound
x
x

gm  m  tm  
gm  m  tm   c m 




    m  

holds probability
    m   due inequality     one




     gm  m  tm      hence 
x

gm  m  tm   c m  gm  m  tm   c


c    maxm  c m    members set tm determined stochastically 
specifically  ith member included tm probability p  m  aoi    m
    theorem    since
   m    one gm  m  tm  

probability   arbitrarily chosen      implies
lim dt  m km  lim gm  m  tm   c





probability         arbitrary related         m     
using result upper bound posterior probabilities yields final result
p  m  dt  m km 
e
    
p  m  

  lim p  m aot   lim


a   proof theorem  
proof  use abbreviations pm  t     p  at  m  ao t   wm  t     p  m ao t   
decompose p  at  ao t  
x
x
pm  t wm  t   
pm  t wm  t  
    
p  at  ao t    
m m
   

m m  

first sum right hand side lower bounded zero upper bounded
x
x
pm  t wm  t 
wm  t 
m m
   

m m
   

pm  t     due theorem    wm  t    almost surely  given    
     let t   m  time t   m   wm  t      choosing
t     maxm  t   m    previous inequality holds t  simultaneously
probability     m   hence 
x
x
pm  t wm  t 
wm  t     
    
m m
   

m m
   

bound second sum      one proceeds follows  every member  m   
one pm  t  pm  t    hence  following similar construction above 
one choose t  t   m    inequalities




fipm  t  pm  t fi  
   

fia minimum relative entropy principle learning acting

hold simultaneously precision      applying second sum equation   
yields bounds
x
x
x


pm  t    wm  t  
pm  t wm  t 
pm  t  wm  t 
m m  

m m  

pm  t 





m m  

multiplicative constants placed front sum  note
 

x

m m  

wm  t     

x

m m
   

wm  t       

use inequalities allows simplifying lower upper bounds respectively 
x
pm  t 
wm  t    pm  t      pm  t     
m m  

pm  t   

x


m m  

    

wm  t  pm  t      pm  t       

combining inequalities                yields final result 





p
 a
 ao
 t 
 

p

            


 t

holds probability   arbitrary     related    
arbitrary precision  





 

a   gibbs sampling implementation mdp agent
inserting likelihood given equation    equation    bayesian control rule 
one obtains following expression posterior
p  m at   ot    
 

p  x  m  x  a p  r m  x  a  x  p  m a t   o t  






p  x  m   x  a p  r m   x  a  x  p  m  a t   o t   dm
p  r m  x  a  x  p  m a t   o t  
r
 




p  r m   x  a  x  p  m  a t   o t   dm
r

    

replaced sum integration   finite dimensional real space
containing average reward q values observed states 
simplified term p  x  m  x  a  constant m 
likelihood model p  r m   x  a  x   equation    encodes set independent normal distributions immediate reward means  x  a  x   indexed triples
 x  a  x   x x   words  given  x  a  x    rewards drawn
normal distribution unknown mean  x  a  x   known variance     sufficient statistics given n x  a  x    number times transition x x
action a  r x  a  x    mean rewards obtained transition 
conjugate prior distribution well known given normal distribution
hyperparameters      
r
n
 
 


 
 
    
exp    x  a  x    
p  m  x  a  x      n            
 
   

fiortega   braun

posterior distribution given
p  m  x  a  x   at   ot     n  x  a  x       x  a  x   
posterior hyperparameters computed
      p n x  a  x   r x  a  x  
    p n x  a  x  
 x  a  x         p n x  a  x   

 x  a  x    

    

introducing shorthand v  x     maxa q x  a   write posterior distribution
p   at   ot     n     s 
    

 

  x
 x  a  x    x  a  x   q x  a    v  x    

x a x
x
s 
 x  a  x   
x a x

posterior distribution q values difficult obtain 
q x  a  enters posterior distribution linearly non linearly   however 
fix q x  a  within max operations  amounts treating v  x 
constant within single gibbs step  conditional distribution approximated



p  q x  a  at   ot   n q x  a     s x  a 
    


q x  a   

x
 
 x  a  x    x  a  x     v  x    
s x  a 
x
x
 x  a  x   
s x  a   
x

expect approximation hold resulting update rule constitutes contraction operation forms basis stochastic approximation algorithms  mahadevan         result  gibbs sampler draws values normal distributions  cycle adaptive controller  one carry several gibbs sweeps
obtain sample improve mixing markov chain  however  experimental
results shown single gibbs sweep per state transition performs reasonably well 
new parameter vector drawn  bayesian control rule proceeds taking
optimal action given equation     note entries transitions
occurred need represented explicitly  similarly  q values visited
states need represented explicitly 
   

fia minimum relative entropy principle learning acting

references
auer  p   cesabianchi  n     fischer  p          finite time analysis multiarmed
bandit problem  machine learning             
bertsekas  d          dynamic programming  deterministic stochastic models 
prentice hall  upper saddle river  nj 
bishop  c  m          pattern recognition machine learning  springer 
braun  d  a     ortega  p  a          minimum relative entropy principle adaptive
control linear quadratic regulators   th conference informatics control 
automation robotics  vol     pp         
cesa bianchi  n     lugosi  g          prediction  learning games  cambridge university press 
dawid  a  p          beware dag   journal machine learning research   to
appear  
dearden  r   friedman  n     andre  d          model based bayesian exploration 
proceedings fifteenth conference uncertainty artificial intelligence  pp 
       
dearden  r   friedman  n     russell  s          bayesian q learning  aaai
   iaai     proceedings fifteenth national tenth conference artificial intelligence innovative applications artificial intelligence  pp          american association artificial intelligence 
duda  r  o   hart  p  e     stork  d  g          pattern classification  second edition  
wiley   sons  inc 
duff  m  o          optimal learning  computational procedures bayes adaptive markov
decision processes  ph d  thesis  director andrew barto 
grunwald  p          minimum description length principle  mit press 
haruno  m   wolpert  d     kawato  m          mosaic model sensorimotor learning
control  neural computation               
haussler  d     opper  m          mutual information  metric entropy cumulative
relative entropy risk  annals statistics               
hutter  m          self optimizing pareto optimal policies general environments
based bayes mixtures  colt 
hutter  m          optimality universal bayesian prediction general loss alphabet 
journal machine learning research            
hutter  m       a   online prediction bayes versus experts  tech  rep   presented
eu pascal workshop learning theoretic bayesian inductive principles
 ltbip       
hutter  m       b   universal artificial intelligence  sequential decisions based algorithmic probability  springer  berlin 
   

fiortega   braun

kappen  b   gomez  v     opper  m          optimal control graphical model inference
problem  jmlr  to appear  
mackay  d  j  c          information theory  inference  learning algorithms  cambridge university press 
mahadevan  s          average reward reinforcement learning  foundations  algorithms 
empirical results  machine learning                   
mahoney  m  v          text compression test artificial intelligence  aaai iaai 
pp         
narendra  k     thathachar  m  a  l          learning automata   survey  ieee
transactions systems  man  cybernetics  smc               
nozick  r          newcombs problem two principles choice  rescher  n   ed   
essays honor carl g  hempel  pp          reidel 
opper  m          bayesian approach online learning  online learning neural
networks         
ortega  p  a     braun  d  a          bayesian rule adaptive control based causal
interventions  third conference artificial general intelligence  pp         
pearl  j          causality  models  reasoning  inference  cambridge university press 
cambridge  uk 
poland  j     hutter  m          defensive universal learning experts  alt 
rasmussen  c  e     deisenroth  m  p          recent advances reinforcement learning 
vol       lecture notes computer science  lnai  chap  probabilistic inference
fast learning control  pp          springer verlag 
robbins  h          aspects sequential design experiments  bulletin american
mathematical socierty             
russell  s     norvig  p          artificial intelligence  modern approach   rd edition  
prentice hall 
schmidhuber  j          simple algorithmic theory subjective beauty  novelty  surprise 
interestingness  attention  curiosity  creativity  art  science  music  jokes  journal
sice               
shafer  g          art causal conjecture  mit press 
singh  s  p          reinforcement learning algorithms average payoff markovian decision
processes  national conference artificial intelligence  pp         
spirtes  p   glymour  c     scheines  r          causation  prediction search   nd
edition   springer verlag  new york 
sutton  r  s     barto  a  g          reinforcement learning  introduction  mit press 
cambridge  ma 
todorov  e          linearly solvable markov decision problems  advances neural
information processing systems  vol      pp           
   

fia minimum relative entropy principle learning acting

todorov  e          efficient computation optimal actions  proceedings national
academy sciences u s a                   
toussaint  m   harmeling  s     storkey  a          probabilistic inference solving
 po mdps  tech  rep  edi inf rr       university edinburgh 
watkins  c          learning delayed rewards  ph d  thesis  university cambridge 
cambridge  england 
wyatt  j          exploration inference learning reinforcement  ph d  thesis 
department artificial intelligence  university edinburgh 

   


