journal artificial intelligence research                  

submitted        published      

automatic induction bellman error features
probabilistic planning
jia hong wu
robert givan

jw   alumni   purdue   edu
givan   purdue   edu

electrical computer engineering
purdue university  w  lafayette        usa

abstract
domain specific features important representing problem structure throughout machine
learning decision theoretic planning  planning  state features provided  domainindependent algorithms approximate value iteration learn weighted combinations
features often perform well heuristic estimates state value  e g   distance
goal   successful applications real world domains often require features crafted human experts  here  propose automatic processes learning useful domain specific feature sets
little human intervention  methods select add features describe state space regions high inconsistency bellman equation  statewise bellman error  approximate
value iteration  method applied using real valued feature hypothesis space
corresponding learning method selecting features training sets state value pairs 
evaluate method hypothesis spaces defined relational propositional feature
languages  using nine probabilistic planning domains  show approximate value iteration
using relational feature space performs state of the art domain independent stochastic
relational planning  method provides first domain independent approach plays tetris
successfully  without human engineered features  

   introduction
substantial gap performance domain independent planners domainspecific planners  domain specific human input able produce effective planners
competition planning domains well many game applications backgammon  chess 
tetris  deterministic planning  work tlplan  bacchus   kabanza        shown
simple depth first search domain specific human input  form temporal logic formulas
describing acceptable paths  yields effective planner wide variety competition domains 
stochastic planning  feature based value function representations used humanselected features great success applications backgammon  sutton   barto       
tesauro        tetris  bertsekas   tsitsiklis         usage features provided human experts often critical success systems using value function approximations 
here  consider problem automating transition domain independent planning
domain specific performance  replacing human input automatically learned domain properties  thus study style planner learns encountering problem instances improve
performance subsequently encountered problem instances domain 
focus stochastic planning using machine learned value functions represented linear
combinations state space features  goal augment state space representation
c
    
ai access foundation  rights reserved 

fiw u   g ivan

planning new machine discovered features facilitate accurate representation
value function  resulting learned features used representing value function
problem instances domain  allowing amortization learning costs across
solution multiple problem instances  note property contrast competition
planners  especially deterministic planning  retain useful information problem instances  thus  approach solving planning problems regarded automatically
constructing domain specific planners  using domain independent techniques 
learn features correlate well statewise bellman error value functions encountered planning  using provided feature language corresponding learner select
features space  evaluate approach using relational propositional feature
spaces  recent approaches acquiring features stochastic planning substantial differences approach discuss detail section    patrascu  poupart 
schuurmans  boutilier    guestrin        gretton   thiebaux        sanner   boutilier       
keller  mannor    precup        parr  painter wakefield  li    littman         previous work
evaluated selection relational features correlation statewise bellman error 
recent theoretical results  parr et al         uncontrolled markov processes show exactly capturing statewise bellman error new features  repeatedly  lead convergence
uncontrolled optimal value value function selected linear fixed point methods weight
training  unfortunately machine learning approaches selecting features  results
transferred approximations statewise bellman error features  case  results
work parr et al         weaker imply convergence  also  none theory transferred controlled case interest here  analysis much
difficult effective  greedy  policy consideration value function training
changing 
consider controlled case  known theoretical properties similar parr
et al         shown  lacking theory  purpose demonstrate capability
statewise bellman error features empirically  rich representations require machine
learning techniques lack approximation guarantees  next  give overview approach  introducing markov decision processes  value functions  bellman error  feature hypothesis
languages feature learning methods 
use markov decision processes  mdps  model stochastic planning problems  mdp
formal model single agent facing sequence action choices pre defined action space 
transitioning within pre defined state space  assume underlying stationary
stochastic transition model available action state transitions occur according
agents action choices  agent receives reward action choice according state
visited  and possibly action chosen   objective accumulating much reward
possible  possibly favoring reward received sooner  using discounting  averaging time 
requiring reward received finite horizon  
mdp solutions represented state value functions assigning real numbers states  informally  mdp solution techniques  desire value function respects action transitions
good states either large immediate rewards actions available lead
good states  well known property formalized bellman equations recursively
characterize optimal value function  see section     degree given value function
fails respect action transitions way  formalized next section  referred
bellman error value function  computed state 
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

intuitively  statewise bellman error high magnitude regions state space
appear undervalued  or overvalued  relative action choices available  state high
bellman error locally inconsistent value function  example  state inconsistently labeled
low value action available leads high value states  approach
use machine learning fit new features regions local inconsistency current value
function  fit perfect  new features guarantee represent bellman update
current value function  repeated bellman updates  called value iteration  known
converge optimal value function  add learned features representation
train improved value function  adding new features available feature set 
method learning new features using approximate value function
regarded boosting style learning approach  linear combination features
viewed weighted combination ensemble simple hypotheses  new feature learned
viewed simple hypothesis selected match training distribution focused regions
previous ensemble getting wrong  as reflected high statewise bellman error throughout
region   growth ensemble sequentially adding simple hypotheses selected correct
error ensemble far refer boosting style learning 
important note method scores candidate features correlation statewise
bellman error current value function  minimizing statewise bellman error
value function found using new candidate feature  pre feature addition scoring much
less expensive scoring involves retraining weights new feature  especially
repeated many times different candidates  relative current value function 
use pre feature addition scoring select features controlled setting enables much
aggressive search new features previously evaluated post feature addition approach
discussed work patrascu et al         
approach considered selecting features feature description language
learning method exists effectively select features match state value training data 
consider two different feature languages empirical evaluation  human constructed
features typically compactly described using relational language  such english  wherein
feature value determined relations objects domain  likewise  consider
relational feature language  based domain predicates basic domain description   the
domain description may written  example  standard planning language ppddl
younes  littman  weissman    asmuth         here  take logical formulas one free variable
represent features count number true instantiations formula state
evaluated  example  number holes feature used many tetris experiments
 bertsekas   tsitsiklis        driessens  ramon    gartner        interpreted counting
number empty squares board filled squares them 
numeric features provide mapping states natural numbers 
addition relational feature language  consider using propositional feature representation learning structure  although propositional representation less expressive
relational one  exist effective off the shelf learning packages utilize propositional representations  indeed  show reformulate feature learning task
related classification problem  use standard classification tool  decision tree learner c   
 quinlan         create binary valued features  reformulation classification considers
sign  magnitude  statewise bellman error  attempting learn features
characterize positive sign regions state space  or likewise negative sign regions  
   

fiw u   g ivan

standard supervised classification problem thus formulated c    applied generate
decision tree feature  use new feature value function representation 
propositional approach easier implement may attractive relational one
obvious advantage using relational representation  computing exact
statewise bellman error state significantly expensive estimating sign 
experiments  however  find relational approach produces superior results
propositional learner  relational approach demonstrates ability generalize features
problem sizes domain  asset unavailable propositional representations 
present experiments nine domains  experiment starts single  constant feature  mapping states number  forcing constant value function makes
distinctions states  learn domain specific features weights automatically
generated sampled state trajectories  adjusting weights new feature added 
evaluate performance policies select actions greedily relative learned value
functions  evaluate learners using stochastic computer game tetris seven planning domains two international probabilistic planning competitions  younes et al        
bonet   givan         method provides first domain independent approach playing
tetris successfully  without human engineered features   relational learner demonstrates
superior success ratio probabilistic planning competition domains compared
propositional approach probabilistic planners ff replan  yoon  fern    givan       
foalp  sanner   boutilier               additionally  show propositional learner
outperforms work patrascu et al         sysadmin domain evaluated there 

   background
present relevant background use markov decision processes planning 
    markov decision processes
define terminology markov decision processes  thorough discussion
markov decision processes  see books bertsekas tsitsiklis        sutton barto
        markov decision process  mdp  tuple  s  a  r  t  s     here  finite state
space containing initial state s    selects non empty finite available action set a s 
state s  reward function r assigns real reward state action state triple  s  a   
action enabled state s  i e   a s   transition probability function maps
state action pairs  s  a  probability distributions s  p s   a s  
given discount factor       policy mapping state action a s  
value function v  s  gives expected discounted reward obtained state selecting action
 s  state encountered discounting future rewards factor per time step 

least one optimal policy v  s   abbreviated v  s   less v  s 
every state s  policy   following q function evaluates action respect
future value function v  
x
q s  a  v    
 s  a    r s  a      v  s    


recursive bellman equations use q   describe v v follows  first  v  s   
q s   s   v    then  v  s    maxaa s  q s  a  v    using q    select ac   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

tion greedily relative value function  policy greedy v   selects  state s  action
arg maxaa s  q s  a  v   
value iteration iterates operation
u v   s    max

aa s 

x

 s  a    r s  a      v  s    



computing bellman update u v   v   producing sequence value functions converging
sup norm v   regardless initial v used 
define statewise bellman error b v  s  value function v state
u v   s  v  s   inducing new features based correlation statewise
bellman error  based sign statewise bellman error  sup norm distance
value function v optimal value function v bounded using bellman error magnitude  defined maxss  b v  s    e g   see williams   baird         use term
statewise bellman error emphasize distinction widely used sup norm bellman
error 
note computing u v    thus statewise bellman error  involve summation
entire state space  whereas fundamental motivations require avoiding summations 
many mdp problems interest  transition matrix sparse way set states
reachable one step non zero probability small  current state  problems 
statewise bellman error computed effectively using appropriate representation  
generally  sparse manner  sum effectively approximately evaluated
sampling next states according distribution represented  
    modeling goal oriented problems
stochastic planning problems goal oriented  objective solving problem
guide agent toward designated state region  i e   goal region   model problems
structuring reward transition functions r action goal state leads
positive reward zero reward absorbing state  reward zero everywhere else 
retain discounting represent preference shorter paths goal  alternatively 
problems modeled stochastic shortest path mdps without discounting  bertsekas        
techniques easily generalized formalisms allow varying action costs well 
model variation work 
formally  define goal oriented mdp mdp meeting following constraints  here  use variables states actions a s   require
contain zero reward absorbing state   i e   r   a  s         a       
a  transition function must assign either one zero triples  s  a     call
region states  s  a    one goal region  reward function constrained
r s  a    zero unless     constructing goal oriented mdps problem
representations  may introduce dummy actions carry transitions involving described
here 
    compactly represented mdps
work  consider propositional relational state representations 
   

fiw u   g ivan

relational mdps  spaces a s  relationally represented  i e  
finite set objects o  state predicates p   action names n used define spaces
follows  state fact application p o              n argument state predicate p object
arguments oi   n    state set state facts  representing exactly true facts
state  action instance a o            os
n   application n argument action name n objects
oi   n  action space   ss a s  set action instances 
mdps compactly represented state action spaces use compact representations
transition reward functions  one compact representation ppddl planning
language  informally discussed next subsection formally presented work younes
et al         
propositional problems  action space explicitly specified state space compactly specified providing finite sequence basic state properties called state attributes 
boolean  integer  real values  propositional state vector values state
attributes 
given relational mdp  equivalent propositional mdp easily constructed grounding  explicit action space constructed forming action name applications
set state attributes computed forming state predicate applications  thus removing use
set objects representation 
    representing ppddl planning problems using mdps
discuss represent goal oriented stochastic planning problems defined standardized
planning languages ppddl  younes et al         goal oriented mdps  limit
focus problems goal regions described  conjunctive  sets state facts 
reference follow approach used work fern  yoon  givan        regarding
converting planning problems compactly represented mdps manner facilitates generalization problem instances  first discuss several difficult representational issues
finally pull discussion together formal definition mdp analyze represent
given ppddl problem instance  consider quantified and or disjunctive goals 
handling goals would interesting useful extension work 
      p lanning omains



p roblems

planning domain distribution problem instances sharing state predicates pw  
action names n   action definitions  actions take objects parameters  defined
giving discrete finite probability distributions action outcomes  specified
using add delete lists state facts action parameters 
given domain definition  problem instance domain specifies finite object set o 
initial state si goal condition g  initial state given set state facts goal
condition given conjunction state facts  constructed predicates pw  
   state predicate associated arity indicating number objects relates  state predicate
applied number objects domain form ground state fact either true false
state  states different possible ways select true state facts  likewise  action name
associated arity natural number indicating number objects action act upon  action name
applied number objects form grounded action 

   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

      ppddl r epresentation
ppddl standard planning language international probabilistic planning competitions 
ppddl  planning domain syntax planning problem syntax defined  completely
define planning instance  one specify domain definition problem definition using
respective syntax  conditional effects quantified preconditions allowed domain
definition 
planning competitions  customary specify planning domains providing problem generators accept size parameters input output ppddl problem instances 
generators thus specify size parameterized planning domains  important note  however  problem generators provided recent planning competitions specify planning
domains according definition used here  particular  problem generators vary
action set state predicates instances generated  relationship
different problem instances generated generators much looser required
definition  domains somewhat arbitrary collections planning
problems 
logical language allows generalization problems problems
share state action language  limit empirical evaluation section   domains
provided problem generators specify planning domains defined here 
i e   without varying action definitions instances  or easily code
generator   refer domains generators planning domains fixed action
definitions 
      g eneralization b etween p roblems varying ize
object set varies size  without bound  across problem instances domain 
infinitely many possible states within different instances single domain  mdp
analyze finite state space  model planning domain infinite set mdps
seeking good policy  in form good value function   one mdp
problem instance   
value function infinite set mdps mapping disjoint union state
spaces mdps real numbers  value function used greedily policy
mdps set  however  explicit representation value function would
infinite size  here  use knowledge representation techniques compactly represent
value functions infinite set problem instance mdps given planning domain 
compact representation derives generalization across domains  approach fundamentally finding good generalizations mdps within single planning domain 
representation value functions planning domains given sections       
section  discuss represent single finite mdp single planning problem
instance  however  note objective work find good value functions
infinite collections mdps represent planning domains  throughout paper 
assume planning domain provided along means sampling example problems
domain  sampling parameterized difficulty  generally  problem size 
   paper consider two candidate representations features  one these  relational representation 
capable generalizing problem sizes  propositional representation  restrict training
testing problem instances size 

   

fiw u   g ivan

easy example problems selected  although  ppddl provide problem
distributions  benchmark planning domains often provided problem generators defining
distributions  generators available  use them  otherwise code
distributions problem instances 
      g eneralizing b etween p roblems



varying g oals

facilitate generalization problem instances different goals  following work
martin geffner        fern et al          translate ppddl instance description
mdp state specifies true state goal is  action
transitions mdp never change goal  presence goal within state
description allows value functions  that defined conditioning state  depend
goal well  goal region mdp simply mdp states specified
current state information matches specified goal information 
formally  translating ppddl problem instances compact mdps  enrich given set
world state predicates pw adding copy predicate indicating desired state
predicate  name goal description copy predicate p prepending word goal 
name  set goal description copies predicates pw denoted pg   take
pw pg state predicates mdp corresponding planning instance  intuitively 
presence goal p a b  state indicates goal condition requires fact p a  b 
part world state  use goal predicates constructing compact mdp
ppddl description constructing initial state  goal conditions true
goal predicates 
use domain blocksworld example illustrate reformulation  the
domain used example fern et al          goal condition blocksworld
problem described conjunction ground on top of facts  world state predicate
on top of pw   discussed above  implies predicate goal on top of pg  
intuitively  one ground instance predicate  goal on top of b  b    means state
goal region  block b  directly top block b  
      tates



available actions

ppddl allows definition domains states meet preconditions
action applied  however  mdp formalism requires least one available action every
state  translating ppddl problem instance mdp define action transitions
action taken dead state transitions deterministically absorbing state 
consider states undesirable plan trajectories  give added transitions
reward negative one unless source state goal state 
      r esulting mdp
pull together elements formally describe mdp    s  a  r  t  s   
given ppddl planning problem instance  discussed section      set defined
specifying predicates objects available  ppddl description specifies sets n
action names objects  well set pw world predicates  construct enriched
set p   pw pg state predicates define state space sets applications
predicates objects o  set a s  state set ppddl action instances built
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

n satisfies preconditions  except set empty  a s  set
ppddl action instances built n o  latter case  say state dead 
reward function r defined discussed previously section      i e   r s  a       
goal condition g true s  r s  a        non goal dead state  zero otherwise 
define  s  a    according semantics ppddl augmented semantics
section    t  s  a    one satisfies g  dead      zero otherwise  
transiting one state another never changes goal condition description states given
predicates pg   mdp initial state s  ppddl problem initial state si augmented
goal condition g using goal predicates pg   propositional representation
desired  easily constructed directly relational representation grounding 
    linear approximation value functions
many previous authors done  patrascu et al         sanner   boutilier        bertsekas  
tsitsiklis        tesauro        tsitsiklis   roy         address large compactly represented and or implicitly representing value functions terms state space features
f   r  features f must select real value state  describe two approaches
representing selecting features section   
recall section   goal learn value function family related mdp
problems  assume state space features defined across union state spaces
family 
represent
value functions using linear combination l features extracted s  i e  
p
v  s    li   wi  s   f   s       goal find features  each mapping states real
values  weights wi v closely approximates v   note single set features
weight vector defines value function mdps features defined 
various methods proposed select weights wi linear approximations  see  e g  
sutton       widrow   hoff         here  review use trajectory based approximate
value iteration  avi  approach  training methods easily substituted  avi constructs
 
 

finite sequence value functions
one  value function
pl v   v           v   returns last

represented v  s    i   wi  s   determine weights wi   v   draw set
training states s    s            sn following policy greedy v   different example problems
sampled provided problem distribution current level problem difficulty   see
section   discussion control problem difficulty   number trajectories drawn
maximum length trajectory parameters avi method  training state s 
compute bellman update u v   s  mdp model problem instance 
compute wi   training states using
wi     wi  

  x
 sj   u v   sj   v  sj    
ni

   

j

learning rate ni number states s    s            sn  s 
non zero  weight updates using weight update formula descend gradient l  distance
v u v   training states  features first rescaled normalize
   note according definitions section      dead states technically goal states 
negative rewards 

   

fiw u   g ivan

effective learning rate correct feature values rare occurrence training set   pseudocode avi method drawing training sets following policy available online
appendix    available jair website   page   
here  use greedy policy draw training examples order focus improvement
relevant states  state distributions generated biased current
policy  particular  another option worth considering  especially feature learning stuck  would
long random walk distribution discussed work fern  yoon  givan        
leave detailed exploration issue future work  substantial discussion
issues arise selecting training distribution  please see book sutton barto        
worth noting on policy training shown converge optimal value function
closely related reinforcement learning setting using sarsa algorithm  singh  jaakkola 
littman    szepesvari        
general  avi often gives excellent practical results  greedy gradient descent
method environment convex due maximization operation bellman error
function  such  guarantee quality weight vector found  even case
convergence  convergence guaranteed  and  experiments  divergent weight
training fact problem required handling  note feature discovery methods
used weight selection algorithms approximate linear programming 
properties avi undesirable application 
implemented small modifications basic weight update rule order use avi
effectively setting  described section   online appendix    available jair
website  

   feature discovering value function construction
planning  state features provided  domain independent algorithms avi learn
weighted combinations features often perform well heuristic estimates state value
 e g   distance goal   describe methods select add features describe
state space regions high inconsistency bellman equation  statewise bellman error 
approximate value iteration  methods applied using real valued feature hypothesis
space corresponding learning method selecting features match real valued function
training set states  here  use learner select features match statewise
bellman error function 
noted above  use boosting style learning approach finding value functions  iterating
selecting weights generating new features focusing bellman error
current value function  value function representation viewed weighted ensemble
single feature hypotheses  start value function trivial feature  constant
feature always returning value one  initial weight zero  iteratively retrain
weights select new features matching regions states current weighted ensemble
high statewise bellman error 
take learning small problems approach learn features first problems
relatively lower difficulty  increase problem difficulty time  discussed below  lower
difficulty problems typically smaller state spaces and or shorter paths positive
   deriving gradient descent weight update formula  feature scaled ri  

   

q

n
 
ni

giving   ri  

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

r
initial feature vector
r
initial weight vector w
initial problem difficulty

difficulty
target level
time 

yes

r
final r
w



increase problem
difficulty
r d  keep
 

learn new feature
correlating bellman
error states
training
r set  add
  keep current
problem difficulty d 

r
w

r

select w approximately
minimizing
error
r bellman
r
v   w

done

reweighted value
r r
function v   w

yes

performance
current difficulty
meets threshold 



generate feature
training set

figure    control flow feature learning  boxes double borders represent assumed subroutines method  assume problem distribution parameterized
problem difficulty  such problem size  

feedback  e g  goal states   learning initially difficult problems typically lead
inability find positive feedback random walk behavior  result learning first lower
difficulty problems found effective  martin   geffner        yoon  fern    givan 
       show experimentally section   good value functions high difficulty problems
indeed learned fashion problems lower  increasing difficulties 
approach relies two assumed subroutines  instantiated different ways
providing different algorithms subroutines  first  method weight selection assumed 
method takes input problem domain fixed set features  selects weight vector
value function problem domain using provided features  intend method
heuristically approximately minimize l bellman error choice weight vector 
practice may easier adjust weights approximate l  bellman error  second  feature
hypothesis space corresponding learner assumed provided system designer 
control flow approach shown figure    iteration fixed problem
distribution selects weights current feature set  using method attempting minimize
l bellman error  define new value function v   selects training set states feature
learning  learns new feature correlating well statewise bellman error v   adding
feature feature set  user provided performance threshold function detects
increase problem difficulty  formalization control flow given figure    form
pseudo code 
   

fiw u   g ivan

feature discovering value function construction



inputs 
initial feature vector     initial weight vector
w   
sequence problem distributions d    d      dmax increasing difficulty 
performance threshold function  
    d  v   tests performance value function v distribution d 



outputs 
feature vector   weight vector
w





  
w
w     

  

 d   max time 




select
w approximately minimizing bellman error v  
w dd



 dd  
w  

  
  
  
  

   

  

else
generate sequence training states using dd

  
  
  
   




learn new feature f correlating bellman error feature b 
w    
states






    f   
w  
w     


return  
w

notes 
   b     statewise bellman error function  defined section     
   code approximate value iteration avi  shown online appendix    available jair website 
page    example implementation line   



   code draw greedy 
w    n
   shown online appendix   page    example impletraining

mentation line    ntraining number states feature training set  duplicated states removed
specified section     



   beam search code learning relational features beam search learn score   t  b 
w      
example implementation line    beam search learn shown figure   section    score
defined section     

figure    pseudo code learning set features 
experiments reported section    evaluate following choices assumed
subroutines  experiments use avi select weights feature sets  evaluate two
choices feature hypothesis space corresponding learner  one relational one propositional  described section   
separate training sets drawn weight selection feature learning  former
depend weight selection method  described avi section      latter
described section 
problem difficulty increased sampled performance greedy policy current
difficulty exceeds user specified performance thresholds  planning domain experiments 
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

performance parameters measured success ratio  percentage trials find goal  average successful plan length  the average number steps goal among successful trials  
non goal oriented domains tetris sysadmin use different performance measures  average total reward tetris bellman error sysadmin  to facilitate comparison patrascu
et al         
assume user provided schedule problem difficulty increases problems
difficulty parameterized one parameter  e g   size may measured number
objects type   domain independent automation increase difficulty
topic future research  give difficulty increase schedules performance thresholds
experiments section presenting experiments  section   
    training set generation
training set selection new feature set states  training set constructed
repeatedly sampling example problem instance problem distribution current level
difficulty  applying current greedy policy greedy v   problem instance create
trajectory states encountered  every state  removing duplicates  encountered added
training set  size feature selection training set maximum length training
trajectory specified user parameters algorithm 
retaining duplicate states training set another option considered  preliminary empirical results favored option  certainly worth exploration 
note goal finding near optimal value function necessarily make reference
state distribution  widely used notion near optimal theory mdps
sup norm distance v   moreover  state distribution represented duplicates
training sets typically distribution badly flawed policy  heeding distribution
prevent correcting bellman error critical states visited policy  visited
rarely   these states may be  instance  rarely visited good exits visited state region
misunderstood current value function   point  primary justification
removing duplicates empirical performance demonstrated section   
similar reasoning would suggest removing duplicate states training sets avi weight
training  described section      many large avi training sets generated
experiments  duplicate removal must carefully handled control runtime  historical reasons 
experiments shown include duplicate removal avi 
possible problem occurs current greedy policy cannot reach enough states complete desired training set      consecutive trajectories drawn without visiting new state
desired training set size reached  process modified follows  point 
method attempts complete training set drawing trajectories using random walk  again
using sampled example problems current problem distribution   process leads
    consecutive trajectories without new state  method terminates training set generation
uses current training set even though smaller target size 
    applicability method
feature discovering value function construction described require complete access
underlying mdp model  avi updates training set generation based
following computations model 
   

fiw u   g ivan

   given state ability compute action set a s  
   given state s  action a s   value function v   ability compute q value
q s  a  v   
   given state action a s   ability draw state next state distribution
defined  s  a    
   given state s  ability compute features selected feature language
computations state required selected feature learner  examples 
 a  section    introduce relational feature language learner require knowledge set domain predicates  and arities  state conjunctive
set predicate facts  see section      
 b  and  section    describe propositional feature language learner
require knowledge set propositional state attributes state truth
assignment attributes 
first three items enable computation bellman update last item enables
computation estimated value function given weights features defining well
selection new features feature learner  requirements amount substantial access
problem model  result method must considered model based technique 
consequence requirements algorithm cannot directly applied
standard reinforcement learning setting model access via acting world
without ability reset selected states  setting bellman error computations particular
states cannot necessarily carried out  would possible construct noisy bellman error
training set model free setting would appropriate future work explore use
training set feature learning 
ppddl planning domains studied provide information needed perform
computations  method applies domains natural represent ppddl 
analyzed method computations implemented  instance 
tetris experiments section      underlying model represented providing hand coded
routines computations within domain 
    analysis
mdp value iteration guaranteed converge optimal value function conducted
tabular value function representation presence discounting  bertsekas         although
weight selection avi designed mimic value iteration  avoiding tabular representation 
general guarantee weight updates track value iteration thus converge
optimal value function  particular  may weighted combination features
represents optimal value function  likewise none represents bellman update u v  
value function v produced avi weight training process  learning system introduces
new features existing feature ensemble response problem  training set used
select new feature pairs states statewise bellman error  learned feature exactly
captures statewise bellman error concept  by exactly capturing training set generalizing
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

successfully  new feature space contain bellman update value function used
generate training data 
aim find features approximate bellman error feature  take
function mapping states statewise bellman error  theoretical properties bellman error
features uncontrolled markov processes  i e   without max operator bellman equation  recently discussed work parr et al          addition
features  or close approximations thereof  proven reduce weighted l   norm distance best weight setting true  uncontrolled  value v   linear fixed point
methods used train weights feature addition  prior work  in wu   givan 
       parallel it  empirically exploring effects selecting bellman
error features complex controlled case  leading results reported here 
clear simply add bellman error feature directly  set corresponding weight one  resulting value function would desired bellman update u v  
current value function v   adding features iteration would thus give us way
conduct value iteration exactly  without enumerating states  added feature would
describe bellman error value function defined terms previously added features  posing
serious computational cost issue evaluating added features  particular  bellman
error feature value function v estimated particular state high confidence
evaluating value function v state polynomial sized sample next states
action  based chernoff bounds  
however  value function v based upon previously added bellman error feature 
evaluation v requires sampling  again  possible action  compute 
manner  amount sampling needed high confidence grows exponentially number
successive added features type  levels sampling collapse one expectation
intervening choices actions  often case decision theoretic sampling 
feature selection method attempt tractably approximate exact value iteration method
learning concise efficiently computable descriptions bellman error feature
iteration 
method thus viewed heuristic approximation exact value iteration  exact
value iteration instance method obtained using explicit state value table
feature representation generating training sets feature learning containing states
obtain exact value iteration would omit avi training instead set weight one 
feature language learner shown approximate explicit features tightly
enough  so resulting approximate bellman update contraction l norm  
easy prove tightening approximations v result weights set one  however 
practical results experiments  use feature representations learners
approximation bound relative explicit features known 

   two candidate hypothesis spaces features
section describe two hypothesis spaces features  relational feature space
propositional feature space  along respective feature learning methods 
two feature spaces  assume learner provided training set states paired
statewise bellman error values 
   

fiw u   g ivan

note two feature space learner pairs lead two instances general method
others easily defined defining new feature spaces corresponding learners 
paper empirically evaluate two instances presented here 
    relational features
relational mdp defined terms set state predicates  state predicates basic
elements define feature representation language  below  define generalpurpose means enriching basic set state predicates  resulting enriched predicates
used predicate symbols standard first order predicate logic  consider
formula logic one free variable feature  follows   
state relational mdp first order interpretation  first order formula one free
variable function states natural numbers maps state number
objects state satisfy formula  take first order formulas real valued
features normalizing real number zero onethis normalization done
dividing feature value maximum value feature take  typically
total number objects domain  smaller domains objects  and
quantifiers  typed  similar feature representation used work fawcett        
feature representation used relational experiments  learner describe
next subsection considers existentially quantified conjunctions literals  with one free
variable  features  space formulas thus effective feature space relational
experiments 
example      take blocksworld table object example  on x  y 
predicate domain asserts block x top object y 
may block table  possible feature domain described
on x  y   first order formula x one free variable  formula
means object immediately block object x 
essentially excludes table object block held arm  if any 
object set described feature  n blocks problems  un normalized value
feature n states block held arm  n   states
block held arm 
      e nriched p redicate et
interesting examples possible enriched predicate set define  enrich
set state predicates p   add binary predicate p transitive closure form
predicate p  predicates min p max p identifying minimal maximal elements
predicate  goal based domains  recall problem representation  from section     
includes  predicate p  goal version predicate called goal p represent desired
state predicate p goal  here  add means ends analysis predicate correct p
represent p facts present current state goal 
so  objects x y  correct p x y  true p x  y  goal p x y 
true  p  x  y  true objects x connected path binary relation p  relation
max p x  true object x maximal element respect p  i e   exists object
   generalizations allow multiple free variables straightforward unclear utility time 

   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

p x  y  true  relation min p x  true object x minimal element respect
p  i e   exists object p y  x  true 
formally define feature grammar online appendix    available jair website 
page   
example      cont    feature correct on x  y  means x stacked top
object current state goal state  feature on  x  y 
means current state  x directly object y  i e   sequence
relations traversing path x y  inclusively  feature max on  x 
means x table object block towers placed table  since
table object object  feature min on  x  means
object top x  i e   x clear 
    learning relational features
select first order formulas candidate features using beam search beam width w  
present pseudo code beam search figure    search starts basic features derived
automatically domain description repeatedly derives new candidate features
best scoring w features found far  adding new features candidates keeping
best scoring w features times  new candidates added fixed depth times 
best scoring feature found overall selected added value function representation 
candidate features scored beam search correlation bellman error feature
formalized below 
specifically  score candidate feature f correlation coefficient bellman
error feature b v    estimated training set  correlation coefficient functions

 s  
defined corr coef       e  s   s  e  s  e 
  instead using known

distribution compute value  use states training set compute sampled
version using following equations approximate true expectation e true standard
deviation random variable x 
  x
x s   
es  x s    
 s  


x s  

corr coef sampled        



  x
 x s   e x s      
 s  


es   s   s   es   s  es    s  
 
 s  s

scoring function feature selection regularized version correlation coefficient
feature target function
score f         corr coef sampled f         depth f    
depth feature depth beam search first occurs 
parameter learner representing degree regularization  bias towards low depth features  
   

fiw u   g ivan

beam search learn
inputs 

feature scoring function fscore   features       

outputs 

new feature f

system parameters 

w   beam width
maxd   max number beam search iterations
  degree regularization  defined section    

  
  
  
  
  
  
  
  

set basic features  defined section     
   f i 
repeat
set beam b highest scoring w candidates f  
candidate feature set f b 
candidate f  b
candidate f   b i   f     f 
f   f combine f    f    

  
   
   

    
 d   maxd    highest score far    d   
return maximum scoring feature f f  

notes 
   feature scoring function fscore f   used rank candidates lines       discussion sample
scoring function  used relational experiments  given section     
   candidate scores cached calls fscore  candidate scored twice 
   value    d  largest score feature depth have 

figure    pseudo code beam search 

value score f    b v     score well feature f correlates bellman error feature  note features non negative  still well correlated
bellman error  which negative   presence constant feature representation allows non negative feature shifted automatically needed 
remains specify features hypothesis space considered initial 
basic  features beam search  specify means constructing complex features
simpler ones use extending beam search  first take state predicate set p
domain enrich p described section      enrichment p   take basic
features existentially quantified applications  possibly negated  state predicates variables
zero one free variable    grammar basic features defined follows 
   domain distinguishes objects naming constants  allow constants arguments
predicates well 

   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

definition  basic feature existentially quantified hliterali expression
one free variable  see figure   online appendix    available jair website 
page    
feature free variables treated technically one free variable feature
variable used  results binary feature value either zero total number
objects  instantiating free variable different ways always results truth value 
assume throughout every existential quantifier automatically renamed away every
variable system  take basic features human provided features
may available  add features experiments paper order clearly
evaluate methods ability discover domain structure own 
stage beam search add new candidate features  retaining w best scoring
features previous stage   new candidate features created follows  feature
beam combined conjunctively other  basic feature  method combination two features described figure    figure shows non deterministic pseudo code
combining two input features  way making non deterministic choices results
new candidate feature  pseudo code refers feature formulas f  f  describing
two features  places  formulas others written free variable exposed 
f   x  f   y   substitution variable notated replacing notation 
f   z  
combination conjoining feature formulas  shown line   figure    however 
additional complexity resulting combining two free variables possibly equating
bound variables two features  two free variables either equated  by substitution  one existentially quantified combination done  line    two pairs
variables  chosen one contributing feature  may equated  resulting
quantifier front  described line    every combination feature candidate 
beam search construction lead logically redundant features cases
syntactically redundant well  avoid syntactically redundant features end beam
search selecting highest scoring feature already feature set  logical redundancy syntactic redundancy difficult detect  avoid redundancy
automatically using ordering beam search reduce generation symmetric expressions   however  testing logical equivalence features
language np hard  chandra   merlin         deploy complete equivalence test
here 
example      assume two basic features z p x  z  w q y  w   set
possible candidates generated combining two features are 
line   figure   runs zero times 
    x z p x  z    w q y  w    xf   x  f   y 
    z p x  z    y w q y  w    f   x  yf   y  
    z p x  z    w q x  w    f   x  f   x 
line   runs one time 
   u   z p u  z    q y  u     equating x w item   above 
   u  x p x  u    q y  u    equating x z item   above 
   

fiw u   g ivan

combine
inputs 

features f   x   f   y 

outputs 

set features  o   

return set features o  result from 
  

perform one
a  f     x f   x 
b  f     y f   y 
c  f    f   x 

  

o    f  f 

  

perform following variable equating step zero  one  two times 
a  let v variable occurring f  o   
let e  expression form  v    v  occurs o 
b  let w variable occurring f  o   
let e  expression form  w    w  occurs o 
c  let u new variable  used o 
d  o    replace e     u  replace e     u  o 
e  o     u o 

notes 
   choice  a   b   c  choice number iterations step    choices e  e 
steps  a  b non deterministic choices 
   feature produced run non deterministic algorithm included set
features returned combine 
   assumed f  f  variables common  renaming necessary operation 

figure    non deterministic algorithm combining two feature formulas 

   u  p x  u   w q u  w     equating z item   above 
   u  p x  u   y q y  u     equating z w item   above 
   u  p x  u    q x  u     equating z w item   above 
first three computed using cases  a   b   c  respectively  remaining
five derive first three equating bound variables f  f   
features generated depth k language easily require enumerating k tuples
domain objects  since cost evaluation grows exponentially k  bound
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

maximum number quantifiers scope point feature formula q  refuse
consider feature violating bound 
values w     d  q parameters controlling relational learner evaluate
paper  set parameters discussed experimental setup description
section   
provide brief discussion motivations feature combination method  first 
note additive combination features represent disjunctions features    hence 
consider conjunction feature combination  here  chosen conjoin features
multiple ways  varying handling combining free bound variables  believe
choice uniquely effective  provide example realization proposed featurediscovery architecture 
choice feature representation combination method must trade cost
evaluation choices potential gain quality selected features  here 
chosen limit individual features conjunction  effectively  limited features
horn clauses predicates negations  univariate heads 
    propositional features
discuss second candidate hypothesis space features  using propositional representation  use decision trees represent propositional features  detailed discussion
classification using decision trees found book mitchell         decision tree
binary tree internal nodes labeled binary tests states  edges labeled yes
representing results binary tests  leaves labeled classes  in case  either zero
one   path tree root leaf label l identifies labeling set
stateseach state consistent state test results path viewed labeled l tree 
way  decision tree real number labels leaves viewed labeling states
real numbers  thus feature 
learn decision trees training sets labeled states using well known c    algorithm
 quinlan         algorithm induces tree greedily matching training data root
down  use c    induce new featuresthe key algorithm construct suitable
training sets c    induced features useful reducing bellman error 
include possible state tests decision trees induce every grounded predicate
application  state predicates  well every previously selected decision tree feature
 each binary test leaf labels zero one  
    learning propositional features
construct binary features  use sign bellman error feature  magnitude  sign statewise bellman error state serves indication whether
state undervalued overvalued current approximation  least respect exactly
representing bellman update current value function  identify collection
undervalued states new feature  assigning appropriate positive weight feature
   representing disjunction overlapping features using additive combination done third feature
representing conjunction  using inclusion exclusion negative weight conjunction 
   grounded predicate application predicate applied appropriate number objects problem instance 

   

fiw u   g ivan

increase value  similarly  identifying overvalued states new feature assigning
negative weight decrease value  note domains interest generally
large state space enumeration  need classification learning generalize notions
overvalued undervalued across state space training sets sample states 
enable method ignore states approximately converged  discard states
statewise bellman error near zero either training set  specifically  among states negative statewise bellman error  discard state error closer zero median
within set  among states positive statewise bellman error  sophisticated methods discarding training data near intended boundary considered
future research  often introduce additional parameters method  here  seek
initial simple evaluation overall approach  discarding  define  
set remaining training pairs states positive statewise bellman error 
likewise negative statewise bellman error 
use   positive examples negative examples supervised
classification algorithm  case  c    used  hypothesis space classification space
decision trees built tests selected primitive attributes defining state space
goal  case  use previously learned features decision trees attributes 
concept resulting supervised learning treated new feature linear approximation architecture  initial weight zero 
intent  ideally  develop approximately optimal value function  value function
expected bellman error many states  every state  however  low state wise
error states contribute high sup norm bellman error  discarding training
states low statewise bellman error reflects tolerance low error threshold
representing degree approximation sought  note technical motivation selecting
features based upon bellman error focuses reducing sup norm bellman error  given
motivation  interested finding exact boundary positive negative
bellman error identifying states large magnitude bellman error  so
large magnitude error addressed feature addition  
observe limited need separately learn feature matching due
following representability argument  consider binary feature f complement f  
exactly one f f true state  given presence constant feature feature
set  adding f f feature set yields set representable value functions  assigning
weight w f effect assigning weight w f adding w weight
constant feature  
    discussion
discuss generalization capability  learning time  heuristic elements feature
learning method 
      g eneralization across varying omain izes
propositional feature space described varies size number objects relational
domain varied  result  features learned one domain size generally meaningful  or
even necessarily defined  domain sizes  relational approach is  contrast  able
generalize naturally different domains sizes  experiments report ability
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

propositional technique learn within domain size directly  attempt use
approach learning small problems gain performance large problems  major
limitation producing good results large domains 
      l earning ime
primary motivation giving generalization domain sizes order employ propositional approach resulting learner use highly efficient  off the shelf classification
algorithms  learning times reported section   show propositional learner learns new
features orders magnitude faster relational learner 
      h euristic e lements



ethod

mentioned earlier  algorithm heuristically approximates repeated addition bellman
error features linear value function approximation order carry value iteration 
mentioned earlier  value iteration guaranteed converge optimal value function 
however  due scale problems target  heuristic approximations required  discuss
motivations heuristic approximation employ briefly here 
first  compute exact bellman error features  instead  use machine learning fit
training set sample states bellman error values  selection training set
done heuristically  using trajectories drawn current greedy policy  use on policy
selection training data loosely motivated on policy convergence results reinforcement
learning  singh et al          serves focus training relevant states   see section      
second  relational instance feature framework  beam search method use
select highest scoring relational feature  with best fit bellman error  ad hoc  greedy 
severely resource bounded  fit obtained bellman error purely heuristic  provide
heuristic method machine learning problem example  intend future
research provide better relational learners resulting better planning performance  heuristic
elements current method discussed appendix a    work
viewed providing reduction stochastic planning structured machine learning numeric
functions   see section    
third  propositional instance feature framework   learner c    selects hypotheses greedily  also  reduction c    classification relies explicit tolerance approximation form threshold used filter training data near zero bellman error 
motivation approximation tolerance focus learner high bellman error states
allow method ignore almost converged states   see section      
fourth  fundamental work use linear approximation value function
gradient descent based weight selection  in case avi   approximation methods key
approach handling large state spaces create need feature discovery  avi method
includes empirically motivated heuristic methods controlling step size sign changes
weights   see section   online appendix    available jair website  
fifth  rely human input select sequence problem difficulties encountered
feature discovery well performance thresholds problem difficulty increases 
believe aspect algorithm automated future research   see section    
   

fiw u   g ivan

   related work
automatic learning relational features approximate value function representation surprisingly frequently studied quite recently  remains poorly understood  here 
review recent work related one dimensions contribution 
    feature selection based bellman error magnitude
feature selection based bellman error recently studied uncontrolled  policyevaluation  context work keller et al         parr et al          attribute value
explicit state spaces rather relational feature representations  feature selection based
bellman error compared feature selection methods uncontrolled context
theoretically empirically work parr  li  taylor  painter wakefield  littman
       
here  extend work controlled decision making setting study incorporation
relational learning selection appropriate knowledge representation value functions
generalize problems different sizes within domain 
main contribution work parr et al         formally showing  uncontrolled
case policy evaluation  using  possibly approximate  bellman error features provably tightens approximation error bounds  i e   adding exact bellman error feature provably reduces
 weighted l   norm  distance optimal value function achieved optimizing weights linear combination features  result extended weaker form
approximated bellman error features  uncontrolled case  limitation uncontrolled case substantial difference setting work  limited experiments shown
use explicit state space representations  technique learns completely new set features
policy evaluation conducted policy iteration  contrast  method accumulates
features value iteration  point limiting focus single policy  constructing
new feature set policy evaluation procedure amenable formal analysis
retaining learned features throughout value iteration policy implicitly considered value iteration  the greedy policy  potentially changing throughout  however 
using relational feature learning  runtime cost feature learning currently high make
constructing new feature sets repeatedly practically feasible 
parr et al         builds prior work keller et al         studied uncontrolled setting  work provides theoretical results general framework  provides
specific approach using bellman error attribute value representations  where state represented real vector  order select new features  approach provides apparent leverage
problems state real vector  structured logical interpretation  typical
planning benchmarks 
    feature discovery via goal regression
previous methods  gretton   thiebaux        sanner   boutilier        find useful features
first identifying goal regions  or high reward regions   identifying additional regions regressing action definitions previously identified regions  principle exploited
given state feature indicates value state  able achieve feature
one step indicate value state  regressing feature definition action
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

definitions yields definition states achieve feature one step  repeated regression identify many regions states possibility transitioning
action sequence high reward region 
exponentially many action sequences relative plan length 
exponentially many regions discovered way  well exponential increase size
representation region  exponentials terms number regression steps
taken  control exponential growth number features considered  regression
implemented pruning optimizations control eliminate overlap regions
detected inexpensively well dropping unlikely paths  however  without scoring
technique  such fit bellman error used paper  select features  regression still
generates large number useless new features  currently effective regression based
first order mdp planner  described work sanner boutilier         effective
disallowing overlapping features allow optimizations weight computation  yet clearly
human designed feature sets fact overlapping features 
inductive technique avoids issues considering compactly represented features 
selecting match sampled statewise bellman error training data  provide extensive
empirical comparison first order approximate linear programming technique  foalp 
work sanner boutilier        empirical results  empirical evaluation
yields stronger results across wide range probabilistic planning benchmarks goalregression approach implemented foalp  although aspects approaches
goal regression candidate generation vary comparison well  
regression based approaches feature discovery related method fitting bellman
error exploit fact states reach valuable states must valuable  i e  seek local consistency  fact  regression goal viewed special
case iteratively fitting features bellman error current value function  depending
exact problem formulation  k  bellman error k step to go value function
non zero  or otherwise nontrivially structured  region states reach goal first
k     steps  significant differences bellman error approach regression based
feature selection arise states reach goal different probabilities different
horizons  approach fits magnitude bellman error  smoothly consider
degree state reaches goal horizon  approach immediately generalizes setting useful heuristic value function provided automatic feature
learning  whereas goal regression approach appears require goal regions begin regression 
spite issues  believe approaches appropriate valuable
considered important sources automatically derived features future work 
effective regression requires compact declarative action model  always available   
inductive technique present require even pddl action model  deductive component computation bellman error individual states  representation
statewise bellman error computed sufficient technique  empirical results show performance planner tetris  model represented
giving program that  given state input  returns explicit next state distribution
state  foalp inapplicable representations due dependence logical deductive rea   example  second international probabilistic planning competition  regression based foalp planner
required human assistance domain providing needed domain information even though standard
pddl model provided competition sufficient planner 

   

fiw u   g ivan

soning  believe inductive deductive approaches incorporating logical representation
important complementary 
goal regression approach special case general approach generating candidate features transforming currently useful features  others considered include
abstraction  specialization  decomposition  fawcett         research human defined concept transformations dates back least landmark ai program  davis   lenat        
work uses one means generating candidate features  beam search logical formulas
increasing depth  means candidate generation advantage strongly favoring concise inexpensive features  may miss complex accurate useful features 
approach directly generalizes means generating candidate features 
centrally distinguishes approach previous work leveraging feature transformations
use statewise bellman error score candidate features  foalp  sanner   boutilier       
      uses scoring function  includes non pruned candidate features linear program
used find approximately optimal value function  zenith system  fawcett        uses
scoring function provided unspecified critic 
    previous scoring functions mdp feature selection
method  work patrascu et al          selects features estimating minimizing
l  error value function results retraining weights candidate feature
included  l  error used work instead bellman error difficulty retraining
weights minimize bellman error  method focuses fitting bellman error
current approximation  without retraining new feature   avoids expensive
retraining computation search able search much larger feature space effectively 
work patrascu et al         contains discussion relational representation  l 
scoring method could certainly used features represented predicate logic  work date
tried  potentially expensive  approach 
    related work
include discussion additional  distantly related research directions appendix a  divided following subsections 
   relevant feature selection methods  fahlman   lebiere        utgoff   precup       
      rivest   precup        mahadevan   maggioni        petrik        
   structural model based model free solution methods markov decision processes  including
 a  relational reinforcement learning  rrl  systems  dzeroski  deraedt    driessens 
      driessens   dzeroski        driessens et al         
 b  policy learning via boosting  kersting   driessens        
 c  fitted value iteration  gordon        
 d  exact value iteration methods first order mdps  boutilier  reiter    price       
holldobler   skvortsova        kersting  van otterlo    de raedt        
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

   inductive logic programming algorithms  muggleton        quinlan        karalic   bratko 
      
   approximate policy iteration relational domains  fern et al          discussion
relational decision list policy learners  khardon        martin   geffner        yoon et al  
      
   automatic extraction domain knowledge  veloso  carbonell  perez  borrajo  fink   
blythe        kambhampati  katukam    qu        estlin   mooney        fox   long 
      gerevini   schubert        

   experimental setting
present experiments nine stochastic planning domains  including reward oriented
goal oriented domains  use pentium   xeon    ghz machines  gb memory  section  give general overview experiments giving detailed results discussion
individual domains section    here  first  briefly discuss selection evaluation domains
section      second  section     set evaluation relational feature learner
comparison variants replace key aspects algorithm random choice determine
importance  additional details  including many experimental parameter settings  found
online appendix    available jair website  section   
    domains considered
evaluation domains below  necessary specify discount factor modeling
domain mdp discounting  discount factor effectively specifies tradeoff
goals reducing expected plan length increasing success rate  parameter
method  domain studied  feature learning method applied
choice   here  simplicity  choose      throughout experiments  note
discount factor used ys dmin domain formalization compare
previous work patrascu et al         
      etris
section     evaluate performance relational propositional learners using
stochastic computer game etris  reward oriented domain goal player
maximize accumulated reward  compare results performance set handcrafted features  performance randomly selected features 
      p lanning c ompetition omains
section      evaluate performance relational learner seven goal oriented planning domains two international probabilistic planning competitions  ippcs   younes et al  
      bonet   givan         comparison purposes  evaluate performance propositional learner two seven domains  b locksworld variant b oxworld described
below   results two domains illustrate difficulty learning useful propositional features complex planning domains  compare results relational planner
two recent competition stochastic planners ff replan  yoon et al         foalp  sanner  
   

fiw u   g ivan

boutilier              performed well planning competitions  finally 
compare results obtained randomly selecting relational features tuning weights
them  complete description of  ppddl source for  domains used  please see
work younes et al         bonet givan        
every goal oriented domain problem generator first second ippc considered inclusion experiments  inclusion  require planning domain fixed
action definitions  defined section      addition ground conjunctive goal regions  four domains properties directly  adapted three domains
properties 
   b oxworld  modify problem generator goal region always ground
conjunctive expression  call resulting domain c onjunctive  b oxworld 
   f ileworld  construct obvious lifted version  create problem generator restricted three folders domain action definitions vary number
folders  call resulting domain l ifted  f ileworld   
   owers h anoi  create problem generator 
resulting selection provides seven ippc planning domains empirical study  provide
detailed discussions adapted domains section   online appendix    available jair
website   well discuss reasons exclusion domains 
      ys dmin
conclude experiments comparing propositional learner previous method patrascu et al          using ys dmin domain used evaluation there  empirical
comparison ys dmin domain shown section     
    randomized variants method
major contribution introduction evaluation feature learning framework
controlled setting based scoring bellman error  be scoring   empirical work instantiates framework relational feature learning algorithm design based greedy
beam search  here  compare performance instance framework variants
replace key aspects randomized choice  illustrating relative importance features  two random choice experiments  adapt method one following two
ways 
   labeling training states random scores instead bellman error scores  target
value feature training set random number       algorithm called
random scoring 
   narrowing beam search randomly rather greedily  eliminate scoring beam search  instead using random selection narrow beam  end
beam search scoring used select best resulting candidate  algorithm called
random beam narrowing 
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

original algorithm  labels training data bellman error narrows beam greedily rather randomly  called greedy beam search be scoring plots 
comparisons  consider relational feature representation  beam
search method used  experiments two variants introduced here  presented
sections              show original method selects features perform much better
randomly selected features  greediness beam search often  but always 
important achieving good performance 

   experimental results
present experimental results etris  planning competition domains  ys dmin
section  starting introduction structure result presentation 
    read results
task evaluating feature learning planning system subtle complex  particularly
factor relational case generalization problem sizes learning small
problems must evaluated  resulting data extensive highly structured  requiring
training reader understand interpret  introduce reader structure
results 
experiments propositional learning  or randomly selected propositional features   problem size never varies within one run learner  propositional representation section     cannot generalize sizes  run separate experiment
size considered  experiment two independent trials  trial starts single trivial
feature repeatedly adds features termination condition met  feature addition  avi used select weights combining features form value function 
performance value function measured  by sampling performance greedy policy  
compute average  of two trials  performance function number
features used  since results single line plot performance function number
features  several different fixed problem size learners compared one figure  one
line each  done example figures       performance measure used varies
appropriately domain presented below 
study ability relational representation section     generalize sizes 
study properly understood backdrop flowchart figure   
described flowchart  one trial learner learn sequence features encounter
sequence increasing problem difficulties  one iteration learner either add new
feature increase problem difficulty  depending current performance   either case 
weights retrained avi performance measurement resulting greedy policy
taken  different trials may increase size different points  cannot meaningfully
average measurements two trials  instead  present two independent trials separately
two tables  figures       first trial  present data
second time line plot showing performance function number features  problem
size changes annotated along line  plots figures       note success
ratio generally increases along line features added  falls problem size
increased   in etris  however  measure rows erased rather success ratio  rows
   

fiw u   g ivan

erased generally increases either addition new feature addition new rows
available grid  
interpret tables showing trials relational learner  useful focus first
two rows  labeled   features problem difficulty  rows  taken together  show
progress learner adding features increasing problem size  column table
represents result indicated problem size using indicated number learned features 
one column next  change one rowsif performance
policy shown column high enough  problem difficulty increases 
otherwise number features increases  adding subtlety interpreting tables  note several adjacent columns increase number features 
sometimes splice two columns save space  thus  several features
added consecutively one problem size  slowly increasing performance  may show
first last columns problem size  consequent jump number
features columns  likewise sometimes splice columns several consecutive columns increase problem difficulty  found splicings save space
increase readability practice reading tables 
performance numbers shown column  success ratio average plan length  number
rows erased  etris  refer performance weight tuned policy resulting
feature set problem difficulty  show column performance value
function  without re tuning weights  target problem size  thus  show quality measures
policy found feature learning current problem size point
target problem size  illustrate progress learning small problems target size
via generalization 
study problem deciding stop adding features  instead 
propositional relational experiments  trials stopped experimenter judgment additional results expensive value giving evaluating algorithm  however 
stop trials still improving unless unacceptable resource consumption
occurred 
also  trial  accumulated real time trial measured shown point
trial  use real time rather cpu time reflect non cpu costs paging due
high memory usage 
    tetris
present experimental results etris 
      overview



etris

game etris played rectangular board area  usually size        initially
empty  program selects one seven shapes uniformly random player rotates
drops selected piece entry side board  piles onto remaining fragments
pieces placed previously  implementation  whenever full row squares
occupied fragments pieces  row removed board fragments top
removed row moved one row  reward received row removed 
process selecting locations rotations randomly drawn pieces continues board
full new piece cannot placed anywhere board  etris stochastic since
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

next piece place always randomly drawn  stochastic element
game  etris used experimental domain previous mdp reinforcement learning
research  bertsekas   tsitsiklis        driessens et al          set human selected features
described book bertsekas tsitsiklis        yields good performance
used weighted linearly approximated value functions  cannot fairly compare performance
domain probabilistic planners requiring ppddl input found natural
ppddl definition etris 
performance metric etris number rows erased averaged        trial
games  reward scaling parameter rscale  defined section   online appendix   page   
selected   
      etris r elational f eature l earning r esults
represent etris grid using rows columns objects  use three primitive predicates 
fill c  r   meaning square column c  row r occupied  below r    r     meaning row
r  directly row r    beside c    c     meaning column c  directly left
column c    quantifiers used relational etris hypothesis space typed using types
row column 
state predicates representing piece drop  however  efficiency
reasons planner computes state value function grid  next piece 
limitation value function expressiveness allows significantly cheaper bellman backup computation  one step lookahead greedy policy execution provides implicit reasoning
piece dropped  piece grid next states 
conduct relational etris experiments    column  n row board  n initially
set   rows  threshold increasing problem difficulty adding one row score
least         n    rows erased  target problem size experiments    rows 
results relational etris experiments given figures     discussed below 
      etris p ropositional f eature l earning r esults
propositional learner  describe etris state   binary attributes represent
  pieces currently dropped  along one additional binary attribute
grid square representing whether square occupied  adjacency relationships
grid squares represented procedurally coded action dynamics  note
number state attributes depends size etris grid  learned features
apply problems grid size  result  show separate results selected problem
sizes 
evaluate propositional feature learning    column etris grids four different sizes   
rows    rows    rows     rows  results four trials shown together figure  
average accumulated time required reach point figure   shown figure   
results discussed below 
      e valuating mportance b ellman   error coring g reedy
b eam   search etris
figure   compares original algorithm alternatives vary either training set
scoring greediness beam search  discussed section      two alternatives  use
   

fiw u   g ivan

trial   
  features
problem difficulty
score
accumulated time  hr  
target size score

 
 
   
   
   

 
 
   
 
   

 
 
   
   
   

 
 
   
   
   

  
 
  
  
   

  
 
  
  
   

  
 
  
  
   

  
 
  
  
   

  
 
  
  
   

  
 
  
  
   

  
 
  
  
   

  
 
   
  
   

  
  
   
  
   

  
  
   
   
   

  
  
   
   
   

trial   
  features
problem difficulty
score
accumulated time  hr  
target size score

 
 
   
   
   

 
 
   
   
   

 
 
  
  
   

 
 
  
  
   

  
 
  
  
   

  
 
  
  
   

  
 
  
  
   

  
  
   
  
   

  
  
   
  
   

  
  
   
  
   

  
  
   
  
   

  
  
   
   
   

  
  
   
   
   

  
  
   
   
   

  
  
   
   
   

  
  
   
   
   

  
  
   
   
   

  
  
   
   
   

  
  
   
   
   

  
  
   
   
   

figure    etris performance  averaged        games   score shown average rows
erased  problem difficulty shown number rows etris board 
number columns always     difficulty increases average score greater
       n     n number rows etris board  target problem
size    rows  columns omitted discussed section     

average rows erased

tetris  relational  trial  
   
   
   
   
   
   
  
 

    
    

   

   

   

   
 

    
   

 

 

 

 

  

  

  

  

  

number features

figure    plot average number lines erased        etris games run
avi training learning relational features  trial     vertical lines indicate
difficulty increases  in number rows   labeled along plot 

   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

average rows erased

tetris  propositional
  
  
  
 
 
 
 
 
                                                                        

number features
   

   

   

    

figure    plot average number lines erased        etris games iteration
avi training learning propositional features  averaged two trials 

accumulated time  hr  

tetris  propositional
   
   
   
   
  
  
  
  
 
                                                                        

number features
   

   

   

    

figure    plot accumulated time required reach point figure    averaged two
trials 

schedule used original greedy beam search be scoring algorithm etris
starting      problem size  however  performance two alternatives never
good enough increase problem size 
      e valuating h uman   designed f eatures etris
addition evaluating relational propositional feature learning approach  evaluate
human selected features described book bertsekas tsitsiklis        perform
selected problem sizes  problem size  start weights zero use avi
   

fiw u   g ivan

average rows erased

impact greedy beam search scoring
  
  
  
  
  
 
 
 
 
 
                                                                        

number relational features
greedy beam search be scoring  original algorithm 
random scoring  variant   
random beam narrowing  variant   

figure    plot average number lines erased        etris games relational features
learned original algorithm two alternatives discussed section     
random scoring random beam narrowing  results averages two
independent trials  trials two variants terminated fail make
progress several feature additions  comparison purposes  trial one original
greedy beam search be scoring method shown  reaching threshold difficulty
increase eleven feature additions  trial two even better  

average rows erased  trial  
average rows erased  trial  

                    
  
  
          
  
  
          

figure     average number lines erased        etris games best weighted
combination human features found two trials avi four problem
sizes 

process described section     train weights    features performance appears
  
 
  k    
human designed features
converge  change learning rate   k    
require larger step size converge rapidly  human designed features normalized
value     experiments  run two independent trials problem size
report performance best performing weight vector found trial  figure    
      p erformance c omparison b etween ifferent pproaches



etris

several general trends emerge results etris  first all  addition new learned
features almost always increasing performance resulting tuned policy  on current
size target size   best performance point reached  suggests fact
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

relational prop       prop      prop      prop      
average feature learning
time  min  

   

  

  

  

  

figure     table average feature learning time relational propositional approaches 
selecting useful features  find clear evidence ability relational representation
usefully generalize problem sizes  substantial performance developed target
problem size without ever training directly size 
find best performance learned propositional features much lower
learned relational features problem sizes shown here  even though larger feature training set
size many learned features used propositional approach  suggests
rich relational representation indeed able better capture dynamics etris
propositional representation 
find performance using random features etris significantly worse
using learned features  demonstrating performance improvements feature learning
due useful feature selection  using bellman error   simply due increasing number
features 
learned relational feature performance       etris far worse obtained
using human selected features avi size  however       etris
relational feature performance close human designed features  human designed
features engineered perform well       etris hence concepts useful
performing well smaller problem sizes may exist features 
      ime l earn e ach f eature
figure    show average time required learn relational feature propositional feature
etris 
time required learn relational feature significantly longer required learn
propositional feature  even though propositional approach larger feature training set size
used 
      c omparison



p revious etris   specific l earners

evaluating domain independent techniques etris  must first put aside strong performance already shown many times literature domain dependent techniques domain 
then  must face problem published domain independent comparison points
order define state of the art target surpass  latter problem  provide baseline
two different approaches random feature selection  show targeted feature selection dramatically improves random selection  former problem  include
discussion domain specific elements key previous published results etris 
many previous domain specific efforts learning play etris  bertsekas
  tsitsiklis        szita   lorincz        lagoudakis  parr    littman        farias   van roy 
      kakade         typically  provide human crafted domain dependent features  deploy domain independent machine learning techniques combine features  often tuning
   

fiw u   g ivan

weights linear combination   example  domain specific feature counting number
covered holes board frequently used  feature plausibly derived human
reasoning rules game  realizing holes difficult fill later
action lead low scores  prior work  selection feature hand 
automated feature selection process  such scoring correlation bellman error  
frequently used domain specific features include column height difference height adjacent columns  apparently selected relevant human reasoning rules
game 
key research question address  then  whether useful features derived automatically  decision making situation etris approached domain independent
system without human intervention  method provided domain state representation using primitive horizontal vertical positional predicates  single constant feature 
knowledge  research published evaluation etris rely
domain specific human inputs discussed  expected  performance
etris much weaker achieved domain specific systems cited 
    probabilistic planning competition domains
throughout evaluations learners planning domains  use lower plan length cutoff
     steps evaluating success ratio iterative learning features  speed learning 
use longer cutoff      steps final evaluation policies comparison
planners evaluations target problem size  reward scaling parameter rscale
 defined section   online appendix   page    selected   throughout planning
domains 
domains multi dimensional problem sizes  remains open research problem
change problem size different dimensions automatically increase difficulty learning 
here  c onjunctive  b oxworld z enotravel  hand design sequence increasing problem sizes 
discussed section        evaluate feature learners total seven probabilistic planning competition domains  following paragraphs  provide full discussion
b locksworld c onjunctive  b oxworld  abbreviated results five domains  provide full discussion five domains appendix b 
relational feature learner finds useful value function features four domains
 b locksworld  c onjunctive  b oxworld  ireworld  l ifted  f ileworld    
three domains  z enotravel  e xploding b locksworld  owers h anoi  
relational feature learner makes progress representing useful fixed size value function
training sizes  fails find features generalize well problems larger sizes 
      b locksworld
probabilistic  non reward version b locksworld first ippc  actions pickup
putdown small probability placing handled block table object instead
selected destination 
relational learner  start   blocks problems  increase n blocks n    
blocks whenever success ratio exceeds     average successful plan length less
   n     target problem size    blocks  results shown figures       
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

trial   
  features
problem difficulty
success ratio
plan length
accumulated time  hr  
target size sr
target size slen 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
  
      
        
 
      
               
          
                          
  
 
 
 
                     




               

trial   
  features
problem difficulty
success ratio
plan length
accumulated time  hr  
target size sr
target size slen 

 
 
 
  
   
 


 
 
 
 
 
 
 
 
 
 
 
 
  
  
 
        
 
      
            
          
                          
 
 
                     



               

figure     b locksworld performance  averaged     problems  relational learner 
add one feature per column success ratio exceeds     average successful plan
length less    n     n blocks  increase problem difficulty
next column  plan lengths shown successful trials only  problem difficulties
measured number blocks  target problem size    blocks  columns
omitted discussed section     

propositional learner  results problem sizes             blocks shown
figure    
relational learner consistently finds value functions perfect near perfect success
ratio    blocks  performance compares favorably recent rrl  driessens
et al         results deterministic b locksworld  goals severely restricted to 
instance  single atoms  success ratio performance around     three ten blocks
 for single goal  still lower achieved here  results b locksworld show
average plan length far optimal  observed large plateaus induced value
function  state regions states given value greedy policy wanders 
problem merits study understand feature induction break
plateaus  separately  studied ability local search break plateaus  wu 
kalyanam    givan        
performance target size clearly demonstrates successful generalization sizes
relational representation 
propositional results demonstrate limitations propositional learner regarding lack
generalization sizes  good value functions induced small
problem sizes      blocks   slightly larger sizes      blocks render method ineffective 
   block problems  initial random greedy policy cannot improved never finds
   

fiw u   g ivan

success ratio

blocksworld  relational  trial  
  blocks

 

  blocks

  blocks

         blocks
   blocks

    
  blocks
   
    
    

successful plan length

 

 

 

   

 
   blocks

   
   
  blocks

   

  blocks

  blocks

   blocks

  blocks

  blocks
  blocks

 
 

 

 

 

number features

figure     b locksworld success ratio average successful plan length  averaged    
problems  first trial figure    using relational learner 

goal  addition  results demonstrate learning additional features good policy
found degrade performance  possibly avi performs worse higher dimensional
weight space results 
      c onjunctive  b oxworld
probabilistic  non reward version b oxworld first ippc similar
familiar logistics domain used deterministic planning competitions  except explicit connectivity graph cities defined  logistics  airports aircraft play important role
since possible move trucks one airport  and locations adjacent it  another airport  and locations adjacent it   b oxworld  possible move boxes
without using aircraft since cities may connected truck routes  stochastic
element introduced domain truck moved one city another 
small chance ending unintended city  described section      use
c onjunctive  b oxworld  modified version b oxworld  experiments 
start   box problems relational learner increase n boxes n     boxes
whenever success ratio exceeds     average successful plan length better   n 
feature learning problem difficulties use   cities  use two target problem sizes     boxes
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

blocksworld  propositional

success ratio

 
   
   
   
   

successful plan length

   
   
   
   
   
   
   
   
  
 

accumulated time  hr  

 

  
  
  
  
  
  
  
  
  
 

 

 

 

 

 

  

 

 

 

 

 

  

 

 

 

 

 

  

number features

  blocks

  blocks

  blocks

   blocks

figure     b locksworld performance success ratio average successful plan length  averaged     problems   accumulated run time propositional learner  averaged two trials 

   

fiw u   g ivan

trial   
  features
 
 
 
problem difficulty
 
 
 
success ratio
      
 
plan length
         
accumulated time  hr            
      
 
target size    sr
target size    slen 
           
target size    sr
              
target size    slen 
            

 
 
 
  
  
 
  
    
   

 
 
 
  
  
 
  
    
   

 
 
 
  
  
 
  
    
   

 
  
 
  
  
 
  
    
   

 
  
 
  
  
 
  
    
   

 
  
 
   
  
 
   
    
    

 
  
 
  
  
 
  
    
   

 
  
 
  
  
 
  
    
   

trial   
  features
 
 
 
problem difficulty
 
 
 
success ratio
      
 
plan length
         
accumulated time  hr            
target size    sr
      
 
           
target size    slen 
target size    sr
             
target size    slen 
            

 
 
 
  
  
 
  
    
   

 
 
 
  
  
 
  
    
   

 
 
 
  
  
 
  
    
   

 
 
 
  
  
 
  
    
   

 
  
 
   
  
 
   
    
    

 
  
 
  
  
 
  
    
   

 
  
    
   
  
 
   
    
    

 
  
 
  
  
 
  
    
   

 
  
 
  
  
 
  
    
   

figure     c onjunctive  b oxworld performance  averaged     problems   add one
feature per column success ratio greater     average successful plan
length less   n  n boxes  increase problem difficulty next
column  problem difficulty shown number boxes  throughout learning
process number cities    plan lengths shown successful trials only  two
target problem sizes used  target problem size       boxes   cities  target
problem size       boxes    cities  columns omitted discussed
section     

  cities     boxes    cities  relational learning results shown figures       
results propositional learner   cities         boxes shown figures    
interpreting c onjunctive  b oxworld results  important focus average
successful plan length metric  c onjunctive  b oxworld problems  random walk able
solve problem nearly always  often long plans     learned features enable
direct solutions reflected average plan length metric 
two relational features required significantly improved performance problems
tested  unlike domains evaluate  c onjunctive  b oxworld domain
    note that  oddly  ippc competition domain used action preconditions prohibiting moving box away
destination  preconditions bias random walk automatically towards goal  consistency
competition results  retain odd preconditions  although preconditions necessary good
performance algorithm 

   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

success ratio

conjuctive boxworld    cities  relational  trial  
  box

 

                   boxes

  box

    
    
 

 

 

   

successful plan length

  box
   
   
   

   boxes
   boxes

  box
  boxes
  boxes

  

  boxes
  box

 
 

 

 

number features

figure     c onjunctive  b oxworld success ratio average successful plan length  averaged
    problems  first trial using relational learner 

learned features straightforwardly describable english  first feature counts many
boxes correctly target city  second feature counts many boxes trucks 
note lack features rewarding trucks right place  resulting
longer plan lengths due wandering value function plateaus   features easily written knowledge representation  e g  count trucks located cities destinations
package truck   require quantification cities packages  severe
limitation quantification currently method efficiency reasons prevents consideration
features point  worth noting regression based feature discovery  studied work gretton thiebaux        sanner boutilier         expected
identify features regarding trucks regressing goal action unloading
package destination  combining bellman error based method regression based
methods promising future direction 
nevertheless  relational learner discovers two concise useful features dramatically
reduce plan length relative initial policy random walk  significant success
automated domain independent induction problem features 
   

fiw u   g ivan

conjunctive boxworld  propositional

success ratio

 
   
   
   
   

successful plan length

 
 

 

 

 

 

  

 

 

 

 

 

  

 

 

 

 

 

  

   
   
   
   
   
   
   
   
   
  
 

accumulated time  hr  

   
   
   
   
   
   
   
   
  
 

number features
  box

  box

  box

figure     c onjunctive  b oxworld performance  averaged     problems  accumulated run time propositional learner  averaged two trials  throughout learning process number cities   

   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

one trial relational feature learner c onjunctive  b oxworld takes several days 
even though fixed number cities training problems five cities  new techniques required improving efficiency feature learning provide results
training larger numbers cities  results demonstrate current representation
learning methods adequately manage small city graphs even larger larger numbers
boxes deliver  resulting value functions successfully generalize    city problems 
domain  well known weakness avi apparent  avi often works practice 
theoretical guarantee quality weight vector found avi training   alternatively  approximate linear programming step could replace avi training provide
expensive perhaps robust weight selection   c onjunctive  b oxworld results 
avi training goes astray selecting weights    box domain size trial    result 
selected weights overemphasize first feature  neglecting second feature  revealed
data shown plan length performance degrades significantly one column
data  avi repeated next problem size     boxes   good performance restored 
similar one column degradation plan length occurs trial      box    box sizes 
propositional experiments c onjunctive  b oxworld  note that  generally 
adding learned propositional features degrades success rate performance relative initial
random walk policy introducing ineffective loops greedy policy  resulting greedy
policies find goal fewer steps random walk  generally pay unacceptable drop
success ratio so  one exception policy found one box problems using two
propositional features  significantly reduces plan length preserving success ratio  still 
result much weaker relational feature language 
problems get severe problem size increases    box problems suffering severe
degradation success rate modest gains successful plan length  please note
accumulated runtime experiments large  especially   box problems  avi
training expensive policies find goal  computing greedy policy
state long trajectory requires considering action  number available actions
quite large domain  reasons  propositional technique evaluate sizes
larger three boxes 
      ummary r esults



dditional omains

figures        present summary results five additional probabilistic planning domains  detailed results full discussion domains  please see appendix b 
summary results  see feature learning approach successfully finds features perform well across increasing problem sizes two five domains  ireworld l ifted f ileworld    three domains  z enotravel  owers h anoi  e xploding
b locksworld   feature learning able make varying degrees progress fixed small problem sizes  progress  sometimes quite limited  generalize well size increases 
      e valuating r elative mportance b ellman   error coring
g reedy b eam   search g oal   oriented omains
figure    compares original algorithm alternatives vary either training set
scoring greediness beam search  discussed section      trial variant 
generate greedy policy domain using feature selection within relational representation
   

fiw u   g ivan

tireworld  trial  

success ratio

 
   

  nodes

     nodes

  nodes

       nodes

  nodes

  nodes

      nodes

 

 

  nodes

   
   
   
  nodes

   
   
 

successful plan length

 

 

 

 

 

       nodes

 

  nodes

 

  nodes

  nodes

 

      nodes
     nodes

 

        nodes

 
 
 

 

 

 

 

 

number features

zenotravel  trial  
success ratio

 
  cities    person    aircraft

   
   

  cities    people    aircraft

   

  cities    people    aircraft

 
   

successful plan length

 

 

 

 

 

 

 

 

 

 

   
   

  cities    people    aircraft

  cities    people    aircraft

   
   

  cities    person    aircraft

   
 
 

 

 

 

 

 

 

 

 

 

number features

figure     summary results ireworld z enotravel  full discussion detailed
results  please see appendix b 

   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

exploding blocksworld  trial  

success ratio

 
   

  blocks

  blocks
  blocks

   

  blocks
  blocks

   

  blocks

   
 

successful plan length

 

 

 

 

 

 

 

 

 

 

  

 
 

  blocks

 

  blocks

 
  blocks

  blocks

 

  blocks

  blocks

 
 
 

 

 

 

 

 

 

 

 

 

  

number features

tower hanoi  trial  

success ratio

 
  discs

   
   

  discs

   

  discs

   

  discs

  discs
 

successful plan length

 

 

 

 

 

 

 

 

  
 

 

  
  

  
  
  

  discs

  
  

  discs

  discs
 
 

 

 

 

 

 

 

 

 

number features

figure     summary results e xploding b locksworld owers
discussion detailed results  please see appendix b 

   



h anoi  full

fiw u   g ivan

success ratio

lifted fileworld   trial  
  file

 

  file

  file

    files

      files

   files

     files

      files

    
    
 

 

 

 

 

 

 

 

successful plan length

  
   files
   files

   files

  

   files
   files

   files
   files
   files

   files

   files

   files

  
  file

  file
  file

  files

  files

  files
  files

  file

 
 

 

 

 

 

 

 

 

number features

figure     summary results l ifted  f ileworld    full discussion detailed results 
please see appendix b 

 alternating avi training  difficulty increase  feature generation original algorithm  
trial  domain  select best performing policy  running algorithm
target problem difficulty reached improvement least three feature additions 
latter case generating least nine features  evaluate greedy policy acquired
manner  measuring average target problem size performance domain  average
results two trials  results shown figure    
domain alternative random scoring perform comparably original greedy
beam search be scoring  exception three domain size combinations learners
perform poorly  z enotravel     block e xploding b locksworld    disc owers
h anoi    alternative random beam narrowing sometimes adequate replace original
approach  domains  greedy beam search critical performance 
      c omparison



ff r eplan



foalp

compare performance learned policies ff replan foalp
ppddl evaluation domains used above  use problem generators provided planning
competitions generate    problems tested problem size except owers h anoi
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

domain
size
greedy beam be scoring  orig   sr
greedy beam be scoring  orig   slen 
random scoring  var     sr
random scoring  var     slen 
random beam narrowing  var     sr
random beam narrowing  var     slen 
random walk sr
random walk slen 

bw
  
    
   
 

    
   
 


box box tire zeno ex bw ex bw toh toh file
                          
 
  
 
    
 
              
    
                
  
   
 
    
 
  
 
     
                   
    
                
        
 
   
 
  
         
 
              
    
                
  
   
 
    
 
  
         
                   
    
 
           
         
 
   
 

         

figure     target problem size performance  averaged     problems  relational features
learned original algorithm two alternatives discussed section     
random walk  averaged best results two independent trials
target problem size 

l ifted  f ileworld    one fixed problem problem size  evaluate
performance planner    times problem  report fig     success ratio
planner problem size  averaged attempts   policies  learned
two independent trials shown above  indicated rfavi    rfavi     planner
   minute time limit attempt  average time required finish successful attempt
largest problem size domain reported figure    
two trials learner domain  evaluate policy performed best trial  first  target problem size   here  policy set features
corresponding weight vector learned avi trial   performance measured
success rate  ties broken plan length  remaining ties broken taking later
policy trial tied  case  consider policy policy
learned trial 
results show planners performance incomparable ff replan  winning domains  losing others  generally dominates foalp 
rfavi performs best planners larger b locksworld  c onjunctive b oxworld  ireworld problems  rfavi essentially tied ff replan performance
l ifted  f ileworld    rfavi loses ff replan remaining three domains  e xploding
b locksworld  z enotravel  owers h anoi  reasons difficulties last
three domains discussed sections presenting results domains  note
foalp learned policy z enotravel  e xploding b locksworld  owers
h anoi   l ifted  f ileworld   
rfavi relies random walk explore plateaus states differentiated selected
features  reliance frequently results long plan lengths times results failure 
recently reported elsewhere early results ongoing work remedying problem
using search place random walk  wu et al         
rfavi learning approach different non learning online replanning used
ff replan  problem determinized  dropping probability parameters 
   

fiw u   g ivan

rfavi   
rfavi   
ff replan
foalp

   blocks bw
       
          
         
      

   blocks bw
       
          
         
         

   blocks bw
           
           
        
        

   blocks bw
           
           
          
          

rfavi   
rfavi   
ff replan
foalp

   bx  ci box
      
      
      
      

   bx   ci box
          
          
          
          

   bx   ci box
          
          
          
          

   bx  ci box
      
      
      
         

rfavi   
rfavi   
ff replan
foalp

   nodes tire
        
        
        
        

   nodes tire
        
        
        
        

   nodes tire
        
        
        
        

   ci  pr  at zeno
           
           
      
n a

rfavi   
rfavi   
ff replan
foalp

  blocks ex bw
        
        
        
n a

   blocks ex bw   discs toh
         
        
         
        
         
        
n a
n a

  discs toh
    
    
        
n a

   bx   ci box
          
          
           
         

   files lifted file
      
      
      
n a

figure     comparison planner  rfavi  ff replan foalp  success ratio
total     attempts     attempts owers h anoi l ifted  f ileworld   
problem size reported  followed average successful plan length
parentheses  two rows rfavi map two learning trials shown paper 

   bw         bx    tire          zeno    ex bw   toh    files
rfavi   
   s
  s
 s
  s
 s

 s
rfavi   
   s
  s
 s
  s
 s

 s
ff replan    s
   s
 s
 s
 s
 s
  s
foalp
  s
   s
  s
n a
n a
n a
n a
figure     average runtime successful attempts  results shown figure    
largest problem size domain 

   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

important topic future research try combine benefits obtained different
planners across domains 
dominance rfavi foalp results implies rfavi state
art among first order techniques work problem lifted form use lifted
generalization  although foalp uses first order structure feature representation  learned
features aimed satisfying goal predicates individually  whole  believe
goal decomposition technique sometimes work well small problems scale well
large problems 
comparisons  noted foalp read ppddl domain descriptions directly  requires human written domain axioms learning  unlike completely
automatic technique  requiring numeric parameters characterizing domain  
requirement human written domain axioms one reasons foalp compete
competition domains learned policy domains
tested here 
c onjunctive  b oxworld     note ff replan uses outcomes problem determinization discriminate likely unlikely outcomes truck movement
actions  result  plans frequently selected rely unlikely outcomes  perhaps choosing
move truck undesired location  relying unlikely outcome accidentally moving
desired location   plans usually fail  resulting repeated replanning luckily selects high likelihood outcome plan execution happens get desired low likelihood
outcome  behavior effect similar behavior learned value function exhibits because  discussed section        learner failed find feature rewarding appropriate
truck moves  planners result long plan lengths due many unhelpful truck moves  however  learned policy conducts random walk trucks much efficiently  and thus
successfully  online replanning ff replan  especially larger problem sizes 
believe even dramatic improvements available improved knowledge representation features 
    sysadmin
full description ys dmin domain provided work guestrin  koller  parr
        here  summarize description  ys dmin domain  machines connected
different topologies  machine might fail step  failure probability depends
number failed machines connected it  agent works toward minimizing number
failed machines rebooting machines  one machine rebooted time step  problem
n machines fixed topology  dynamic state space sufficiently described n
propositional variables  representing on off status certain machine 
test domain purpose direct comparison performance propositional techniques published results work patrascu et al          test exactly
topologies evaluated measure performance measure reported there  sup norm bellman
error 
evaluate method exact problems  same mdps  used evaluation
work patrascu et al         testing domain  two different kinds topologies tested 
    hand convert nested universal quantifiers conditional effects original boxworld domain definition
equivalent form without universal quantifiers conditional effects allow ff replan read domain 

   

fiw u   g ivan





cycle topology

  legs topology

figure     illustration two topologies ys dmin domain     nodes   node
represents machine  label indicates server machine  specified work
patrascu et al         

  legs cycle    legs topology three three node legs  each linear sequence three
connected nodes  connected single central node one end  cycle topology arranges
ten nodes one large cycle     nodes topology  two topologies
illustrated figure     target learning domain keep many machines
operational possible  number operating machines directly determines reward
step  since    nodes basic features on off statuses
nodes  total      states  reward scaling parameter rscale  defined section  
online appendix    available jair website  page    selected    
work patrascu et al         uses l  sup norm  bellman error performance
measurement ys dmin  technique  described above  seeks reduce mean bellman
error directly l bellman error  report l bellman error  averaged two
trials  figure     included figure    results shown work patrascu et al 
        select best result shown  from various algorithmic approaches    legs
cycle topologies shown paper  correspond d o s setting cycle
topology d x n setting   legs topology  terminology paper 
topologies show algorithm reduces l bellman error effectively per
feature well effectively overall experiments previously reported work
patrascu et al          topologies show bellman error eventually diverges avi cannot
handle complexity error function dimensionality increases  algorithm still
achieve low bellman error remembering restoring best performing weighted feature set
weakened performance detected 
note superior performance reducing bellman error could due entirely
part use avi weight training instead approximate linear programming  alp  
method used patrascu et al  however  systematic superiority known avi alp 
results suggest superior performance feature learning itself 
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

sysadmin    legs topology
  

  

  

 

bellman error

 
 
 
 
 
 
 
 
 
                                                                                

number features
  legs  learned

  legs  patrascu

sysadmin  cycle topology
     

  
 

bellman error

 
 
 
 
 
 
 
 
 
                                                                                   

number features
cycle  learned
cycle  patrascu

figure     l bellman error ys dmin domain     nodes  two topologies  values
results work patrascu et al         taken figure    
work patrascu et al         

    demonstration generalization across problem sizes
asset relational feature representation presented paper learned relational
features applicable problem size domain  section      discussed
   

fiw u   g ivan

target problem sizes
      tetris    blocks bw     box    city  bx    nodes tire    files lifted file
intermediate problem sizes       tetris    blocks bw     box    city  bx    nodes tire    files lifted file
generalize target size
learn intermediate size
random walk

  
   
   

       
       
    

      
       
          

        
        
        

      
      
      

figure     performance intermediate sized problems generalization  show
performance value functions learned target problem sizes evaluated
intermediate sized problems  demonstrate generalization sizes  comparison  intermediate sized problems  show performance value functions learned directly intermediate size well performance random
walk  generalization results intermediate size learning results averages two
trials  etris  average accumulated rows erased shown  goal oriented
domains  success ratio successful plan length  in parentheses  shown
domain 

modeling planning domain infinite set mdps  one problem instance
domain  infinite set mdps  feature vector plus weight vector defines single
value function well defined every problem instance mdp  discuss question whether framework find single feature weight vector combination generalizes
good performance across problem sizes  i e   value function v defined combination 
whether greedy v   performs similarly well different problem sizes 
throughout section    demonstrated direct application learned feature weight
vectors target problem sizes   without retraining weights these results shown
target size lines result tables domain  etris  b locksworld  c onjunctive b oxworld  ireworld  l ifted  f ileworld    target size lines demonstrate direct
successful generalization target sizes even current problem sizes significantly smaller 
 in domains  either notion problem size  s ys dmin   insufficient planning progress significantly increase problem size learning small problems  e xplod ing b locksworld   z enotravel   owers h anoi    
subsection  consider generalization  larger  target sizes selected intermediate sizes five domains  specifically  take weight vectors feature vectors
resulting end trials  i e  weight vector retrained target sizes   apply
directly selected intermediate problem sizes without weight retraining  trials terminate learning reaches target problem sizes     take weights features result
best performing policy terminating problem sizes  generalization results shown
figure     comparison  table shows performance intermediate sized
problems value function learned directly size  well performance
random walk size 
    note one trials etris terminates reaching target size due non improving performance 
two trials l ifted  f ileworld   terminate target size performance already reaches optimality
learning reaches target size  still  although value functions learned smaller sizes
target size  value functions evaluated generalization learned significantly larger sizes
intermediate evaluation size 

   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

domain shown random walk result much weaker generalization result 
showing presence generalization learned value functions across problem sizes 
four goal oriented planning domains  applying value functions learned target sizes equals
performance achieved value functions learned directly intermediate sizes  with better
performance c onjunctive  b oxworld   etris  however  generalization result
match result learning intermediate size  note domains  solution
strategy invariant respect problem size  e g  destroying incorrect towers form correct
ones b locksworld   domains best plan strategy may change dramatically
size  example  etris  larger number rows board allows strategies temporary
stack uncompleted rows  smaller number rows favors strategies complete rows quickly
possible  thus one necessarily expect generalization domain sizes every
domainthis conclusion expected hold whether considering generalization
value functions policies 
included discussion policy based generalization related work section  appendix a     focusing previous work approximate policy iteration  however  note
policies generalize problems different sizes less well defined
value functions generalize problems  previous api work  defined
policies select actions states domain size  work define value functions
assign numeric values states domain size  none work guarantees finding good
optimal policy value function  far know  problems admit good compact value
functions  admit good compact policies  admit both  neither 

   discussion future research
presented general framework automatically learning state value functions featurediscovery gradient based weight training  framework  greedily select features
provided hypothesis space  which parameter method  best correlate bellman
error features  use avi find weights associate features 
proposed two different candidate hypothesis spaces features  one two
spaces relational one features first order formulas one free variable  beamsearch process used greedily select hypothesis  hypothesis space considered propositional feature representation features decision trees  hypothesis
space  use standard classification algorithm c     quinlan        build feature best
correlates sign statewise bellman error  instead using sign magnitude 
performance feature learning planners evaluated using reward oriented
goal oriented planning domains  demonstrated relational planner represents
state of the art feature discovering probabilistic planning techniques  propositional planner
perform well relational planner  cannot generalize problem instances 
suggesting knowledge representation indeed critical success feature discovering
planners 
although present results propositional feature learning approach relation featurelearning approach  knowledge representation difference difference
approaches  historically  propositional approach originally conceived reduction
classification learning  attempt capture magnitude bellman error
   

fiw u   g ivan

feature selection  rather focuses sign error  contrast  relational approach
counts objects order match magnitude bellman error 
difference  cannot attribute performance differences
approaches knowledge representation choice  differences performance could due
choice match sign propositional feature selection  possible future experiment
identify sources performance variation would use propositional representation involving
regression trees  dzeroski  todorovski    urbancic        capture magnitude error 
representation might possibly perform somewhat better decision tree representation
shown here  course would still enable generalization sizes relational
feature learner exhibits 
bellman error reduction course one source guidance might followed
feature discovery  experiments ippc planning domains  find many
domains successful plan length achieved much longer optimal  discussed
section        possible remedy deploying search previous work  wu et al  
      learn features targeting dynamics inside plateaus  use features decisionmaking plateaus encountered 

acknowledgments
material based upon work supported part national science foundation grant
no          

appendix a  related work
a   feature selection approaches
a     f eature election

via

c onstructive f unction pproximation

automatic feature extraction sequential decision making studied work utgoff
precup         via constructive function approximation  utgoff   precup         work
viewed forerunner general framework  limited propositional representations 
binary valued features  new features single literal extensions old features conjunction  work rivest precup        variant cascade correlation  fahlman
  lebiere         constructive neural network algorithm  combined td learning learn
value functions reinforcement learning  cascade correlation incrementally adds hidden units
multi layered neural networks  hidden unit essentially feature built upon set
numerically valued basic features  work provides framework generalizing prior efforts reduction supervised learning  explicit reliance bellman error signal 
feature hypothesis space corresponding learner deployed  particular 
demonstrate framework binary propositional features using c    learner rich
numeric valued relational features using greedy beam search learner  work provides first
evaluation automatic feature extraction benchmark planning domains several planning
competitions 
work utgoff precup        implicitly relies bellman error 
explicit construction bellman error training set discussion selecting features correlate
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

bellman error  instance  work focuses first refining current feature weight
updates converging poorly  high variance weight updates   whereas work focuses first
finding feature correlates statewise bellman error  regardless whether feature refines
current feature  addition  work selects features online weights current
features adjusted  stationary target value function bellman
error considered selection next new feature  contrast  work separates weight
training new feature selection completely   these differences perhaps part due
reinforcement learning setting used utgoff   precup        opposed planning setting
work  
selection hidden unit feature cascade correlation  fahlman   lebiere        based
covariance feature values errors output units  output units
estimating value function  training data providing bellman update value function 
output unit error bellman error  thus  hidden units learned work rivest
precup        approximations bellman error features learned features are  although made explicit work  making goal capturing bellman error explicit
here  provide general reduction facilitates use learning method capture
resulting feature learning training sets  particular  able naturally demonstrate generalization across domain sizes several large domains  using relational feature learner  contrast 
single test domain work rivest precup        small fixed size  nonetheless 
work important precursor approach 
a     f eature c onstruction

via

pectral nalysis

feature learning frameworks value functions based upon spectral analysis state space connectivity presented work mahadevan maggioni        petrik        
frameworks  features eigenvectors connectivity matrices constructed random walk  mahadevan   maggioni        eigenvectors probabilistic transition matrices  petrik        
features capture aspects long term problem behaviours  opposed short term behaviours
captured bellman error features  bellman error reduction requires iteration capture longterm behaviors 
reward functions considered feature construction work mahadevan maggioni         work petrik         reward functions incorporated
learning krylov basis features  variant bellman error features  parr et al          complement eigenvector features  however  even petriks framework  reward incorporated
features used policy evaluation rather controlled environment consider 
essential work use machine learning factored representations handle
large statespaces generalize problems different sizes  spectral
analysis frameworks limited respect  at least current state development   approach petrik        presented explicit statespaces  factorization approach
scaling large discrete domains proposed work mahadevan maggioni        
approach  features learned dimension factorization  independent
dimensions  believe assumption independence dimensions inappropriate
many domains  including benchmark planning domains considered work  mahadevan maggioni factorization approach suffers drawbacks propositional
approach  solution recomputed problems different sizes domain
   

fiw u   g ivan

lacks flexibility generalize problems different sizes provided relational
approach 
a   structural model based model free solution methods markov decision
processes
a     r elational r einforcement l earning
work dzeroski et al          relational reinforcement learning  rrl  system learns
logical regression trees represent q functions target mdps  work related since
use relational representations automatically construct functions capture state value 
addition q function trees  policy tree learner introduced work dzeroski
et al         finds policy trees based q function trees  learn explicit policy
description instead use greedy policies evaluation 
logical expressions rrl regression trees used decision points computing
value function  or policy  rather numerically valued features linear combination 
method  generalization across problem sizes achieved learning policy trees  learned value
functions apply training problem sizes  date  empirical results approach
failed demonstrate ability represent value function usefully familiar planning
benchmark domains  good performance shown simplified goals placing
particular block onto particular block b  technique fails capture structure richer
problems constructing particular arrangements blocksworld towers  rrl
entered international planning competitions  difficulties representing complex
relational value functions persist extensions original rrl work  driessens   dzeroski 
      driessens et al          limited applicability shown benchmark planning
domains used work 
a     p olicy l earning via b oosting
work kersting driessens         boosting approach introduced incrementally
learn features represent stochastic policies  policy iteration variant featurelearning framework  clearly differs work policy representations learned instead
value function representations  using regression tree learner tilde  blockeel   de raedt 
       feature learner demonstrated advantages previous rrl work task accomplishing on a b     block problem  applicability simple continuous domain  the corridor
world  demonstrated  line rrl work  limited applicability benchmark
planning domains shown here  one probable source limited applicability model free
reinforcement learning setting system model problem dynamics explicitly 
a     f itted value teration
gordon        presented method value iteration called fitted value iteration suitable
large state spaces require direct feature selection  instead  method relies
provided kernel function measuring similarity states  selection kernel function
viewed kind feature selection  kernel identifies state aspects significant
measuring similarity  knowledge  techniques class applied
large relational planning problems evaluated paper  note selection
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

single relational kernel domains would measure state similarity domain independent
manner thus believe kernel could adapt individual domains way
work does  thus would expect inferior performance approach  however 
remains investigated  selection domain specific kernels stochastic planning domains 
automatically  yet explored 
a     e xact value teration f irst  order mdp
previous work used lifted techniques exactly solve first order mdps reformulating exact
solution techniques explicit mdps  value iteration  boutilier et al         holldobler skvortsova        independently used two different first order languages  situation
calculus fluent calculus  respectively  define first order mdps  works  bellman update procedure value iteration reformulated using respective calculus  resulting
two first order dynamic programming methods  symbolic dynamic programming  sdp   firstorder value iteration  fovi   simple boxworld example human assisted computation
demonstrated sdp work  method serves basis foalp  sanner   boutilier 
       replaces exact techniques heuristic approximation order scale techniques
benchmark planning domains  application fovi planning domains demonstrated
colored blocksworld benchmark  limited    blocks  holldobler  karabaev   
skvortsova        
work kersting et al          constraint logic programming used define relational
value iteration method  mdp components  states  actions  rewards  first abstracted
form markov decision program  lifted version mdp  relational bellman operation
 rebel  used define updates q values state values  empirical study rebel
approach limited    step backups single predicate goals blocksworld
logistics domains 
exact techniques suffer difficulty representing full complexity state value
function arbitrary goals even mildly complex domains  previous works serve illustrate central motivation using problem features compactly approximate structure
complex value function  thus motivate automatic extraction features studied
work 
a   comparison inductive logic programming algorithms
problem selecting numeric function relational states match bellman error training
set first order regression problem available systems described
inductive logic programming  ilp  literature  quinlan        karalic   bratko        
important note ilp work studied learning classifiers relational
data  muggleton         concerned learning numeric functions relational
data  such states   latter problem called first order regression within ilp literature  received less study relational classification  here  choose design
proof of concept relational learner experiments rather use one previous
systems  separate work needed compare utility relational learner previous
regression systems  purpose demonstrate utility bellman error training data
finding decision theoretic value function features  simple learner suffices create
state of the art domain independent planning via automatic feature selection 
   

fiw u   g ivan

ilp classification systems often proceed either general specific  specific
general  seeking concept match training data  regression  however 
easy ordering numeric functions searched  design instead method searches
basic logical expression language simple expressions complex expressions  seeking
good matches training data  order control branching factor  still allowing
complex expressions considered  heuristically build long expressions
short expressions score best  words  use beam search space expressions 
several heuristic aspects method  first  define heuristic set basic expressions search begins  second  define heuristic method combining
expressions build complex expressions  two heuristic elements designed
logical formula without disjunction  one free variable  built repeated combination basic expressions  finally  assumption high scoring expressions built
high scoring parts heuristic  and often true   critical heuristic assumption
makes likely learner often miss complex features match training data well 
known method guarantees tractably finding features 
a   approximate policy iteration relational domains
planners use greedy policies derived learned value functions  alternatively  one directly learn representations policies  policy tree learning work dzeroski et al 
        discussed previously appendix a      one example  recent work uses relational decision list language learn policies small example problems generalize well
perform large problems  khardon        martin   geffner        yoon et al          due
inductive nature line work  however  selected policies occasionally contain severe
flaws  mechanism provided policy improvement  policy improvement quite
challenging due astronomically large highly structured state spaces relational policy
language 
work fern et al          approximate version policy iteration addressing
issues presented  starting base policy  approximate policy iteration iteratively generates
training data improved policy  using policy rollout  uses learning algorithm
work yoon et al         capture improved policy compact decision list language
again  similar work  learner work fern et al         aims take flawed
solution structure improve quality using conventional mdp techniques  in case  finding
improved policy policy rollout  machine learning  unlike work  work fern
et al         improved policies learned form logical decision lists  work
viewed complementary previous work exploring structured representation value
functions work explored structured representation policies  approaches
likely relevant important long term effort solve structured stochastic decisionmaking problems 
note feature based representation  considered generally mdp literature  used represent value functions rather policies  compact representation policies
done via value functions  with greedy execution  directly  example  using decision lists  previous api work discussed uses direct representation policies  never
uses compact representation value functions  instead  sampling value functions used
policy evaluation step policy iteration 
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

one imagine different novel approach api compact feature based
representation used value functions  greedy execution policy representation 
approach  feature discovery similar explore value iteration could designed assist policy evaluation phase policy iteration  leave development
evaluation idea future work  expect two approaches api  well
current approach value iteration  advantages disadvantages vary domain
ways yet well understood  domains natural compact direct policy
representations  run see tarantula   whereas others naturally compactly represented
via value functions  prefer restaurants good review ratings   research area must
eventually develop means combine compact representations effectively 
a   automatic extraction domain knowledge
substantial literature learning plan using methods direct representation
value function reactive policy  especially deterministic planning literature 
techniques related acquire domain specific knowledge via planning experience domain  much literature targets control knowledge particular search based
planners  estlin   mooney        kambhampati et al         veloso et al          distant
approach focus particular planning technology used limitation
deterministic domains  unclear generalize work value function construction
probabilistic domains 
however  broader learning to plan literature contains work producing declarative
learned domain knowledge could well exploited feature discovery value function representation  work fox long         pre processing module called tim
able infer useful domain specific problem specific structures  typing objects
state invariants  descriptions domain definition initial states  invariants
targeted work improving planning efficiency graphplan based planner  suggest
future work could exploit invariants discovering features value function representation  similarly  work gerevini schubert         discoplan infers state constraints
domain definition initial state order improve performance sat based planners  again  constraints could incorporated feature search method
date 

appendix b  results discussions five probabilistic planning competition
domains
section      presented results relational propositional feature learners
b locksworld c onjunctive  b oxworld  present results relational
feature learner following five probabilistic planning competition domains  ireworld 
z enotravel  e xploding b locksworld  owers h anoi  l ifted  f ileworld   
b   tireworld
use ireworld domain second ippc  agent needs drive vehicle
graph start node goal node  moving one node adjacent node 
vehicle certain chance suffering flat tire  while still arriving adjacent node  
   

fiw u   g ivan

trial   
  features
 
 
 
 
 
 
 
 
 
problem difficulty
 
 
 
 
 
 
 
 
 
success ratio
                                            
plan length
 
 
 
 
 
 
 
 
 
accumulated time  hr                               
                                            
target size sr
target size slen 
 
 
 
 
 
 
 
 
 

 
  
    
 
  
    
 

trial   
  features
 
 
 
 
 
 
 
 
 
problem difficulty
 
 
 
 
 
 
 
     
                                            
success ratio
plan length
 
 
 
 
 
 
 
 
 
accumulated time  hr                                
target size sr
                                            
target size slen 
 
 
 
 
 
 
 
 
 

 
  
    
 
  
    
 

 
  
    
 
  
    
 

 
  
    
 
  
    
 

figure     ireworld performance  averaged     problems  relational learner  add
one feature per column success ratio exceeds      average successful plan
length less  n  n nodes  increase problem difficulty next
column  plan lengths shown successful trials only  problem difficulties measured
number nodes  target problem size    nodes  columns omitted
discussed section     

flat tire replaced spare tire  spare tire present node
containing vehicle  vehicle carrying spare tire  vehicle pick spare
tire already carrying one one present node containing vehicle 
default setting second ippc problem generator domain defines problem distribution
includes problems policy achieving goal probability one 
problems create tradeoff goal achievement probability expected number steps
goal  strongly planner favors goal achievement versus short trajectories goal
determined choice discount factor made section     
start   node problems relational learner increase n nodes n    
nodes whenever success ratio exceeds      average successful plan length better
 n steps  target problem size    nodes  results shown figures       
ireworld  relational learner able find features generalize well large
problems  learner achieves success ratio        node problems  unknown
whether policy exceed success ratio problem distribution  however  neither
comparison planner  foalp ff replan  finds higher success rate policy 
note improvements success rate domain necessarily associated
increases plan length success rate improvements may due path deviations
acquire spare tires 
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

trial   
  features
problem difficulty
success ratio
plan length
accumulated time  hr  
target size sr
target size slen 

 
     
    
   
    
    
   

 
     
   
   
   
    
    

 
     
    
   
   
    
    

 
     
    
   
   
    
    

 
     
    
   
  
    
    

 
     
    
   
  
    
    

 
     
    
   
  
    
    

 
     
    
   
  
    
    

 
     
    
   
  
    
    

 
     
    
   
  
    
    

 
     
    
   
  
    
    

trial   
  features
problem difficulty
success ratio
plan length
accumulated time  hr  
target size sr
target size slen 

 
     
    
   
   
    
   

 
     
    
   
   
    
    

 
     
    
   
   
    
    

 
     
    
   
   
    
    

 
     
    
   
   
    
    

 
     
    
   
  
    
    

 
     
    
   
  
    
    

 
     
    
 
  
    
 

 
     
    
 
  
    
 

 
     
    
 
  
    
 

 
     
    
 
  
    
 

figure     z enotravel performance  averaged     problems  relational learner 
problem difficulty shown table lists numbers cities  travelers  aircraft 
target problem size    cities    travelers    aircraft  add one feature
per column success ratio exceeds      increase problem difficulty
next column  plan lengths shown successful trials only 

b   zenotravel
use z enotravel domain second ippc  goal domain fly travelers original location destination  planes  finite range  discrete  fuel levels 
need re fuelled fuel level reaches zero cont inue flying  available activity
 boarding  debarking  flying  zooming  refueling  divided two stages  activity
x modelled two actions start x finish x  finish x activity  high  probability
nothing  start action taken  corresponding finish action must taken
 repeatedly  succeeds conflicting action started  structure allows
failure rates finish actions simulate action costs  which used explicitly
problem representation competition   plane moved locations flying
zooming  zooming uses fuel flying  higher success probability 
start problem difficulty   cities    traveler    aircraft using relational
feature learner  whenever success ratio exceeds      increase number n travelers
aircraft   number cities less  n    increase number cities one
otherwise  target problem size    cities    travelers    aircraft  z enotravel results
relational learner shown figures       
   

fiw u   g ivan

relational learner unable find features enable avi achieve threshold success
rate         cities    travelers    aircraft  although   relational features learned  trials
stopped improvement performance achieved several iterations feature
addition  using broader search  w        q           able find better features
extend solvable size several cities success rate      not shown results
paper use search parameters  reported wu   givan         runtime
increases dramatically  weeks  believe speed effectiveness relational learning
needs improved excel domain  likely major factor improved knowledge
representation features key concepts z enotravel easily represented 
trial two figure    shows striking event adding single new feature useful value
function results value function greedy policy cannot find goal all 
success ratio degrades dramatically immediately  note small problem size 
ten percent problems trivial  initial state satisfies goal  addition
sixth feature trial two  problems policy solve  reflects
unreliability avi weight selection technique aspect feature discovery 
all  avi free assign zero weight new feature  not  additional study
control avi and or replacement avi linear programming methods indicated
phenomenon  however  rare event extensive experiments 
b   exploding blocksworld
use e xploding b locksworld second ippc evaluate relational planner 
domain differs normal blocksworld largely due blocks certain probability detonated put down  destroying objects beneath  but
detonating block   blocks already detonated detonated again  goal
state domain described tower fragments  fragments generally required
table  destroyed objects cannot picked up  blocks cannot put destroyed objects  but destroyed object still part goal necessary relationships
established destroyed  
start   block problems using relational learner increase n blocks n    
blocks whenever success ratio exceeds      target problem sizes      blocks 
e xploding b locksworld results relational learner shown figures       
results e xploding b locksworld good enough planner increase
difficulty beyond   block problems  results show limited generalization   block
problems  little generalization    block problems 
performance domain quite weak  believe due presence many
dead end states reachable high probability  states either table
one blocks needed goal destroyed  object question achieved
required properties  planner find meaningful relevant features  planner discovers
undesirable destroy table  instance  however  resulting partial understanding domain cannot augmented random walk  as domains
b locksworld c onjunctive  b oxworld  enable steady improvement value  leading goal  random walk domain invariably lands agent dead end  short
successful plan length  low probability reaching goal   not shown here  high unsuccessful plan length  caused wandering dead end region  suggest need new techniques
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

trial   
  features
 
 
 
 
problem difficulty
 
 
 
 
success ratio
                   
plan length
 
 
 
 
accumulated time  hr                  
                   
target size    sr
target size    slen 
 
 
 
 
target size    sr
 
 
      
target size    slen 


  

 
 
    
 
   
    
 
    
 

trial   
  features
 
 
 
 
 
problem difficulty
 
 
 
 
 
success ratio
                        
plan length
 
 
 
 
 
accumulated time  hr                      
target size    sr
                        
 
 
 
 
 
target size    slen 
target size    sr
 
 
           
target size    slen 


     

 
 
 
 
 
 
 
 
 
 
 
 
                            
 
 
 
 
 
 
                   
                             
 
 
 
 
 
 
                          
              

 
 
    
 
   
    
 
    
  

 
 
    
 
   
    
 
    
  

 
 
    
 
  
    
 
    
  

 
 
    
 
  
    
 
    
  

 
 
    
 
  
    
 
    
  

  
 
    
 
  
    
 
    
  

 
 
    
 
  
    
 
    
  

figure     e xploding b locksworld performance  averaged     problems  relational
learner  problem difficulties measured number blocks  add one feature per
column success ratio exceeds      increase problem difficulty next
column  plan lengths shown successful trials only  target problem size     
blocks  target problem size       blocks 

aimed handling dead end regions handle domain  results demonstrate technique relies random walk  or form search  learned features need
completely describe desired policy 
b   towers hanoi
use domain owers h anoi first ippc  probabilistic version wellknown problem  agent move one two discs simultaneously  small probability
going dead end state move  probability depends whether largest disc
moved type disc move  one two time  used  note
one planning problem problem size here 
important note      success rate generally unachievable domain due
unavoidable dead end states 
   

fiw u   g ivan

trial   
  features
problem difficulty
success ratio
plan length
accumulated time  hr  
target size    sr
target size    slen 
target size    sr
target size    slen 

 
 
 
 
 
 
 
 
 
 
 
       
 
 
 
 
 
 
 
 
 
 
 
 
 
 
                          
 
 
 
        
 
 
 
 
  
  
 





 



                                                     
                          
 
 
 
                  
  
 
  
  
  





 
 

 
      
 
        
 
 
 
 
 
 
 
 
  



   









trial   
  features
problem difficulty
success ratio
plan length
accumulated time  hr  
target size    sr
target size    slen 
target size    sr
target size    slen 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
                          
 
 
 
      
 
  
  
  
 





 
                                           
                        
 
 
 
      
  
         
  





 
        
 
        
 
 
 
 
 
  
  


   






 
 
 

   
 

 


  
 
 

 
 

 


  
 
 

  
 

 


figure     owers h anoi performance  averaged     problems  relational learner 
add one feature per column success ratio exceeds    n  n discs 
increase problem difficulty next column  plan lengths shown successful trials
only  problem difficulties measured number discs  target problem size   
  discs size      discs  columns omitted discussed section     

start   disc problem relational learner increase problem difficulty
n discs n     discs whenever success ratio exceeds    n    target problem sizes
    discs  owers h anoi results relational learner shown figures       
learner clearly able adapt three  four disc problems  achieving around    
success rate four disc problem trials  optimal solution four disc problem
success rate      policy uses single disc moves large disc moved
uses double disc moves  policies use single disc moves double disc moves
achieve success rates          respectively  four disc problem  learned solution
occasionally moves disc way get closer goal  reducing success 
unfortunately  trials show increasing number new features needed adapt
larger problem size  trials even    total features enough adapt
five disc problem  thus  know approach extend even five discs  moreover 
results indicate poor generalization problem sizes 
believe difficult learner  and humans  represent good value function
across problem sizes  humans deal domain formulating good recursive policy 
establishing direct idea value state  finding recursive policy automatically
interesting open research question outside scope paper 
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

b   lifted fileworld 
described section      use domain l ifted  f ileworld    straightforwardly
lifted form f ileworld first ippc  restricted three folders  reach goal filing
files  action needs taken file randomly determine folder file
go into  actions taking folder  putting file folder  returning folder
cabinet  goal reached files correctly filed targeted folders 
note f ileworld l ifted  f ileworld   benign domains 
reachable dead ends non optimal actions  directly reversible 
random walk solves domain success rate one even thirty files  technical challenge
posed minimize unnecessary steps minimize plan length  optimal policy
solves n file problem  n      n     steps  depending random file types
generated 
rather preset plan length threshold increasing difficulty  as function n  
adopt policy increasing difficulty whenever method fails improve plan length adding
features  specifically  success ratio exceeds     one feature added without improving
plan length  remove feature increase problem difficulty instead   
start   file problems relational learner increase n files n     files
whenever performance improve upon feature addition  target problem size   
files  l ifted  f ileworld   results relational learner shown figures       
results show planner acquires optimal policy    file target size problem
learning four features  two trials  results domain reveal
weakness avi weight selection method  although four features enough define optimal policy  problem difficulty increases  avi often fails find weight assignment producing
policy  happens  feature addition triggered  trial   
domain  results show extra features prevent avi finding good weights
subsequent iterations  optimal policy recovered larger feature set  nonetheless  another indication improved performance may available via work alternative
weight selection approaches  orthogonal topic feature selection 

references
bacchus  f     kabanza  f          using temporal logics express search control knowledge
planning  artificial intelligence              
bertsekas  d  p          dynamic programming optimal control  athena scientific 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
blockeel  h     de raedt  l          top down induction first order logical decision trees 
artificial intelligence              
bonet  b     givan  r          non deterministic planning track      international planning
competition  website  http   www ldc usb ve  bonet ipc   
    possible specify plan length threshold function triggering increase difficulty domain 
done domains  find domain quite sensitive choice function  end
must chosen trigger difficulty increase feature addition fruitless current difficulty 
so  directly implemented automatic method triggering difficulty increase 

   

fiw u   g ivan

trial   
  features
     
     
problem difficulty
success ratio
     
      
plan length
accumulated time  hr              
target size sr
     
target size slen 
          

 
 
 
 
   
 


 
 
 
 
   
 


 
 
 
 
   
 


 
 
 
 
   
 


 
 
 
  
   
    
  

 
  
 
  
   
 
  

 
  
 
  
   
 
  

 
  
 
  
   
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
   

 
  
 
  
  
 
  

trial   
  features
     
problem difficulty
     
success ratio
     
      
plan length
accumulated time  hr              
     
target size sr
target size slen 
          

 
 
 
 
   
 


 
 
 
 
   
 


 
 
 
 
   
 


 
 
 
 
   
 


 
       
 
        
 
       
              
                   
            
              

 
  
 
  
   
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
  
 
  
  
 
   

 
  
 
  
  
 
  

 
  
 
  
  
 
  

 
 
 
  
   
    
  

 
  
 
  
   
 
  

 
  
 
  
   
 
  

figure     l ifted  f ileworld   performance  averaged     problems  relational
learner  add one feature per column success ratio exceeds     adding one
extra feature improve plan length  increase problem difficulty
next column  after removing extra feature   plan lengths shown successful trials
only  problem difficulties measured number files  target problem size
   files  columns omitted discussed section     

boutilier  c   reiter  r     price  b          symbolic dynamic programming first order mdps 
proceedings seventeenth international joint conference artificial intelligence 
pp         
chandra  a     merlin  p          optimal implementation conjunctive queries relational data
bases  proceedings ninth annual acm symposium theory computing  pp 
     
davis  r     lenat  d          knowledge based systems artificial intelligence  mcgraw hill 
new york 
driessens  k     dzeroski  s          integrating guidance relational reinforcement learning 
machine learning             
driessens  k   ramon  j     gartner  t          graph kernels gaussian processes relational
reinforcement learning  machine learning            
dzeroski  s   deraedt  l     driessens  k          relational reinforcement learning  machine
learning          
dzeroski  s   todorovski  l     urbancic  t          handling real numbers ilp  step towards
better behavioural clones  proceedings eighth european conference machine
learning  pp         
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

estlin  t  a     mooney  r  j          learning improve efficiency quality planning 
proceedings fifteenth international joint conference artificial intelligence  pp 
         
fahlman  s     lebiere  c          cascade correlation learning architecture  advances
neural information processing systems    pp          
farias  v  f     van roy  b          tetris  study randomized constraint sampling  probabilistic randomized methods design uncertainty  springer verlag 
fawcett  t          knowledge based feature discovery evaluation functions  computational
intelligence              
fern  a   yoon  s     givan  r          learning domain specific control knowledge random
walks  proceedings fourteenth international conference automated planning
scheduling  pp         
fern  a   yoon  s     givan  r          approximate policy iteration policy language bias 
solving relational markov decision processes  journal artificial intelligence research     
      
fox  m     long  d          automatic inference state invariants tim  journal artificial
intelligence research            
gerevini  a     schubert  l          inferring state constraints domain independent planning 
proceedings fifteenth national conference artificial intelligence  pp         
gordon  g          stable function approximation dynamic programming  proceedings
twelfth international conference machine learning  pp         
gretton  c     thiebaux  s          exploiting first order regression inductive policy selection 
proceedings twentieth conference uncertainty artificial intelligence  pp     
    
guestrin  c   koller  d     parr  r          max norm projections factored mdps  proceedings seventeenth international joint conference artificial intelligence  pp         
holldobler  s   karabaev  e     skvortsova  o          flucap  heuristic search planner
first order mdps  journal artificial intelligence research             
holldobler  s     skvortsova  o          logic based approach dynamic programming 
proceedings workshop learning planning markov processesadvances
challenges nineteenth national conference artificial intelligence  pp       
kakade  s          natural policy gradient  advances neural information processing
systems     pp           
kambhampati  s   katukam  s     qu  y          failure driven dynamic search control partial
order planners  explanation based approach  artificial intelligence                  
karalic  a     bratko  i          first order regression  machine learning             
keller  p   mannor  s     precup  d          automatic basis function construction approximate dynamic programming reinforcement learning  proceedings twenty third
international conference machine learning  pp         
   

fiw u   g ivan

kersting  k   van otterlo  m     de raedt  l          bellman goes relational  proceedings
twenty first international conference machine learning  pp         
kersting  k     driessens  k          non parametric policy gradients  unified treatment
propositional relational domains  proceedings twenty fifth international conference machine learning  pp         
khardon  r          learning action strategies planning domains  artificial intelligence                  
lagoudakis  m  g   parr  r     littman  m  l          least squares methods reinforcement
learning control  setn     proceedings second hellenic conference ai  pp 
       
mahadevan  s     maggioni  m          proto value functions  laplacian framework learning representation control markov decision processes  journal machine learning
research              
martin  m     geffner  h          learning generalized policies planning examples using
concept languages  applied intelligence          
mitchell  t  m          machine learning  mcgraw hill 
muggleton  s          inductive logic programming  new generation computing               
parr  r   li  l   taylor  g   painter wakefield  c     littman  m          analysis linear
models  linear value function approximation  feature selection reinforcement learning 
proceedings twenty fifth international conference machine learning  pp     
    
parr  r   painter wakefield  c   li  l     littman  m          analyzing feature generation
value function approximation  proceedings twenty fourth international conference
machine learning  pp         
patrascu  r   poupart  p   schuurmans  d   boutilier  c     guestrin  c          greedy linear valueapproximation factored markov decision processes  proceedings eighteenth
national conference artificial intelligence  pp         
petrik  m          analysis laplacian methods value function approximation mdps 
proceedings twentith international joint conference artificial intelligence  pp 
         
quinlan  j  r          c     programs machine learning  morgan kaufmann 
quinlan  j  r          learning first order definitions functions  journal artificial intelligence
research            
rivest  f     precup  d          combining td learning cascade correlation networks 
proceedings twentieth international conference machine learning  pp         
sanner  s     boutilier  c          practical linear value approximation techniques first order
mdps  proceedings twenty second conference uncertainty artificial intelligence  pp         
sanner  s     boutilier  c          practical solution techniques first order mdps  artificial
intelligence                   
   

fiautomatic nduction b ellman  e rror f eatures p robabilistic p lanning

singh  s   jaakkola  t   littman  m     szepesvari  c          convergence results single step
on policy reinforcement learning algorithms  machine learning                
sutton  r  s          learning predict methods temporal differences  machine learning 
       
sutton  r  s     barto  a  g          reinforcement learning  introduction  mit press 
szita  i     lorincz  a          learning tetris using noisy cross entropy method  neural
computation               
tesauro  g          temporal difference learning td gammon  communications acm 
            
tsitsiklis  j     roy  b  v          analysis temporal difference learning function approximation  ieee transactions automatic control                
utgoff  p  e     precup  d          relative value function approximation  tech  rep   university
massachusetts  department computer science 
utgoff  p  e     precup  d          constuctive function approximation  motoda    liu  eds   
feature extraction  construction  selection  data mining perspective  pp         
kluwer 
veloso  m   carbonell  j   perez  a   borrajo  d   fink  e     blythe  j          integrating planning
learning  prodigy architecture  journal experimental theoretical ai       
      
widrow  b     hoff  jr  m  e          adaptive switching circuits  ire wescon convention
record        
williams  r  j     baird  l  c          tight performance bounds greedy policies based
imperfect value functions  tech  rep   northeastern university 
wu  j     givan  r          discovering relational domain features probabilistic planning 
proceedings seventeenth international conference automated planning
scheduling  pp         
wu  j   kalyanam  r     givan  r          stochastic enforced hill climbing  proceedings
eighteenth international conference automated planning scheduling  pp         
wu  j     givan  r          feature discovering approximate value iteration methods  proceedings symposium abstraction  reformulation  approximation  pp         
yoon  s   fern  a     givan  r          inductive policy selection first order mdps  proceedings eighteenth conference uncertainty artificial intelligence  pp         
yoon  s   fern  a     givan  r          ff replan  baseline probabilistic planning  proceedings seventeenth international conference automated planning scheduling  pp         
younes  h   littman  m   weissman  d     asmuth  j          first probabilistic track
international planning competition  journal artificial intelligence research             

   


