journal artificial intelligence research                  

submitted       published     

efficient markov network structure discovery
using independence tests
facundo bromberg

fbromberg frm utn edu ar

departamento de sistemas de informacion 
universidad tecnologica nacional 
mendoza  argentina

dimitris margaritis
vasant honavar

dmarg cs iastate edu
honavar cs iastate edu

dept  computer science 
iowa state university 
ames  ia      

abstract
present two algorithms learning structure markov network data 
gsmn gsimn  algorithms use statistical independence tests infer structure successively constraining set structures consistent results
tests  recently  algorithms structure learning based maximum likelihood estimation  proved np hard markov networks due
difficulty estimating parameters network  needed computation
data likelihood  independence based approach require computation
likelihood  thus gsmn gsimn compute structure efficiently
 as shown experiments   gsmn adaptation grow shrink algorithm
margaritis thrun learning structure bayesian networks  gsimn extends gsmn additionally exploiting pearls well known properties conditional
independence relation infer novel independences known ones  thus avoiding performance statistical tests estimate them  accomplish efficiently gsimn uses
triangle theorem  introduced work  simplified version set
markov axioms  experimental comparisons artificial real world data sets show
gsimn yield significant savings respect gsmn   generating markov
network comparable cases improved quality  compare gsimn
forward chaining implementation  called gsimn fch  produces possible conditional independences resulting repeatedly applying pearls theorems known
conditional independence tests  results comparison show gsimn 
sole use triangle theorem  nearly optimal terms set independences
tests infers 

   introduction
graphical models  bayesian markov networks  important subclass statistical models possess advantages include clear semantics sound widely
accepted theoretical foundation  probability theory   graphical models used
represent efficiently joint probability distribution domain  used
numerous application domains  ranging discovering gene expression pathways
bioinformatics  friedman  linial  nachman    peer        computer vision  e g  geman

c
    
ai access foundation  rights reserved 

fibromberg  margaritis    honavar

figure    example markov network  nodes represent variables domain v  
                         

  geman        besag  york    mollie        isard        anguelov  taskar  chatalbashev 
koller  gupta  heitz    ng         one problem naturally arises construction
models data  heckerman  geiger    chickering        buntine         solution
problem  besides theoretically interesting itself  holds potential
advancing state of the art application domains models used 
paper focus task learning markov networks  mns  data
domains variables either discrete continuous distributed according
multidimensional gaussian distribution  mns graphical models consist two
parts  undirected graph  the model structure   set parameters  example
markov network shown figure    learning models data consists two interdependent tasks  learning structure network  and  given learned structure 
learning parameters  work focus problem learning structure
mn domain data 
present two algorithms mn structure learning data  gsmn  grow shrink
markov network learning algorithm  gsimn  grow shrink inference based markov
network learning algorithm   gsmn algorithm adaptation markov networks
gs algorithm margaritis thrun         originally developed learning
structure bayesian networks  gsmn works first learning local neighborhood
variable domain  also called markov blanket variable  
using information subsequent steps improve efficiency  although interesting
useful itself  use gsmn point reference performance regard
time complexity accuracy achieved gsimn  main result work 
gsimn algorithm extends gsmn using pearls theorems properties
conditional independence relation  pearl        infer additional independences
set independences resulting statistical tests previous inferences  thus avoiding
execution tests data  allows savings execution time and  data
distributed  communication bandwidth 
rest paper organized follows  next section present previous
research related problem  section   introduces notation  definitions presents
intuition behind two algorithms  section   contains main algorithms  gsmn
gsimn  well concepts practical details related operation  evaluate
gsmn gsimn present results section    followed summary
   

fiefficient markov network structure discovery using independence tests

work possible directions future research section    appendices b contain
proofs correctness gsmn gsimn 

   related work
markov networks used physics computer vision communities  geman  
geman        besag et al         anguelov et al         historically
called markov random fields  recently interest use spatial
data mining  applications geography  transportation  agriculture  climatology 
ecology others  shekhar  zhang  huang    vatsavai        
one broad popular class algorithms learning structure graphical models
score based approach  exemplified markov networks della pietra  della pietra 
lafferty         mccallum         score based approaches conduct search
space legal structures attempt discover model structure maximum score 
due intractable size search space i e   space legal graphs 
super exponential size  score based algorithms must usually resort heuristic search 
step structure search  probabilistic inference step necessary evaluate
score  e g   maximum likelihood  minimum description length  lam   bacchus       
pseudo likelihood  besag         bayesian networks inference step tractable
therefore several practical score based algorithms structure learning developed
 lam   bacchus        heckerman        acid   de campos         markov networks
however  probabilistic inference requires calculation normalizing constant  also
known partition function   problem known np hard  jerrum   sinclair       
barahona         number approaches considered restricted class graphical
models  e g  chow   liu        rebane   pearl        srebro   karger         however 
srebro karger        prove finding maximum likelihood network np hard
markov networks tree width greater   
work area structure learning undirected graphical models concentrated learning decomposable  also called chordal  mns  srebro   karger 
       example learning non decomposable mns presented work hofmann tresp         approach learning structure continuous domains
non linear relationships among domain attributes  algorithm removes edges
greedily based leave one out cross validation log likelihood score  non score based
approach work abbeel  koller  ng         introduces new class efficient algorithms structure parameter learning factor graphs  class graphical
models subsumes markov bayesian networks  approach based new
parameterization gibbs distribution potential functions forced
probability distributions  supported generalization hammersley clifford
theorem factor graphs  promising theoretically sound approach may
lead future practical efficient algorithms undirected structure learning 
work present algorithms belong independence based constraintbased approach  spirtes  glymour    scheines         independence based algorithms exploit fact graphical model implies set independences exist distribution domain  therefore data set provided input algorithm  under
assumptions  see next section   work conducting set conditional independence
   

fibromberg  margaritis    honavar

tests data  successively restricting number possible structures consistent
results tests singleton  if possible   inferring structure
possible one  desirable characteristic independence based approaches fact
require use probabilistic inference discovery structure 
also  algorithms amenable proofs correctness  under assumptions  
bayesian networks  independence based approach mainly exemplified
sgs  spirtes et al          pc  spirtes et al          algorithms learn
markov blanket step learning bayesian network structure grow shrink
 gs  algorithm  margaritis   thrun         iamb variants  tsamardinos  aliferis 
  statnikov      a   hiton pc hiton mb  aliferis  tsamardinos    statnikov 
       mmpc mmmb  tsamardinos  aliferis    statnikov      b   max min hill
climbing  mmhc   tsamardinos  brown    aliferis         widely used
field  algorithms restricted classes trees  chow   liu        polytrees
 rebane   pearl        exist 
learning markov networks previous work mainly focused learning gaussian
graphical models  assumption continuous multivariate gaussian distribution
made  results linear dependences among variables gaussian noise  whittaker        edwards         recent approaches included works dobra 
hans  jones  nevins  yao  west          castelo   roverato         pena        
schafer strimmer         focus applications gaussian graphical models
bioinformatics  make assumption continuous gaussian variables
paper  algorithms present applicable domains use
appropriate conditional independence test  such partial correlation   gsmn
gsimn algorithms presented apply case arbitrary faithful distribution assumed probabilistic conditional independence test distribution
available  algorithms first introduced bromberg  margaritis  honavar
        contributions present paper include extending results conducting
extensive evaluation experimental theoretical properties  specifically 
contributions include extensive systematic experimental evaluation proposed algorithms  a  data sets sampled artificially generated networks varying
complexity strength dependences  well  b  data sets sampled networks
representing real world domains   c  formal proofs correctness guarantee
proposed algorithms compute correct markov network structure domain 
stated assumptions 

   notation preliminaries
denote random variables capitals  e g   x  y  z  sets variables bold
capitals  e g   x  y  z   particular  denote v               n    set n
variables domain  name variables indices v  instance 
refer third variable v simply    denote data set size
 number data points   d  n   use notation  xy   z  denote
proposition x independent conditioned z  disjoint sets variables x 
y  z   x  y   z  denotes conditional dependence  use  xy   z  shorthand
  x  y     z  improve readability 
   

fiefficient markov network structure discovery using independence tests

markov network undirected graphical model represents joint probability
distribution v  node graph represents one random variables
domain  absences edges encode conditional independences among them 
assume underlying probability distribution graph isomorph  pearl        faithful
 spirtes et al          means faithful undirected graph  graph g
said faithful distribution graph connectivity represents exactly
dependencies independences existent distribution  detail  means
disjoint sets x  y  z v  x independent given z set
vertices z separates set vertices x set vertices graph g  this
sometimes called global markov property  lauritzen         words  means
that  removing vertices z g  including edges incident them  
exists  undirected  path remaining graph variable x
variable y  example  figure    set variables        separates set       
set      generally  shown  pearl        theorem    page    definition
graph isomorphism  page     necessary sufficient condition distribution
graph isomorph set independence relations satisfy following axioms
disjoint sets variables x  y  z  w individual variable  

 symmetry 
 decomposition 
 intersection 
 strong union 
 transitivity 

 xy   z 
 xy w   z 
 xy   z w 
 xw   z y 
 xy   z 
 xy   z 




 yx   z 
 xy   z   xw   z 

 
 
 

 xy w   z 
 xy   z w 
 x   z   y   z 

   

operation algorithms assume existence oracle
answer statistical independence queries  standard assumptions needed
formally proving correctness independence based structure learning algorithms
 spirtes et al         
    independence based approach structure learning
gsmn gsimn independence based algorithms learning structure
markov network domain  approach works evaluating number statistical
independence statements  reducing set structures consistent results
tests singleton  if possible   inferring structure possible one 
mentioned above  theory assume existence independence query oracle
provide information conditional independences among domain variables 
viewed instance statistical query oracle  kearns   vazirani        
practice oracle exist  however  implemented approximately
statistical test evaluated data set d  example  discrete data
pearsons conditional independence chi square      test  agresti         mutual
information test etc  continuous gaussian data statistical test used
measure conditional independence partial correlation  spirtes et al          determine
conditional independence two variables x given set z data 
   

fibromberg  margaritis    honavar

statistical test returns p value  p value test equals probability obtaining
value test statistic least extreme one actually observed
given null hypothesis true  corresponds conditional independence
case  assuming p value test p x    z   statistical test concludes
dependence p x    z  less equal threshold i e  
 x  y   z  p x    z   
quantity   sometimes referred tests confidence threshold  use
standard value        experiments  corresponds confidence
threshold     
faithful domain  shown  pearl   paz        edge exists
two variables x    v markov network domain
dependent conditioned remaining variables domain  i e  
 x    edge iff  x  y   v  x     
thus  learn structure  theoretically suffices perform n n      tests i e  
one test  x    v  x     pair variables x  v  x      unfortunately 
non trivial domains usually involves test conditions large number
variables  large conditioning sets produce sparse contingency tables  count histograms 
result unreliable tests  number possible configurations
variables grows exponentially size conditioning setfor example 
 n cells test involving n binary variables  fill table one data point
per cell would need data set least exponential size i e   n  n   exacerbating
problem  one data point per cell typically necessary reliable test 
recommended cochran             cells contingency table
less   data points test deemed unreliable  therefore gsmn
gsimn algorithms  presented below  attempt minimize conditioning set size 
choosing order examining variables irrelevant variables
examined last 

   algorithms related concepts
section present main algorithms  gsmn gsimn  supporting concepts required description  purpose aiding understanding
reader  discussing first describe abstract gsmn algorithm next
section  helps showing intuition behind algorithms laying foundation
them 
    abstract gsmn algorithm
sake clarity exposition  discussing first algorithm gsmn  
describe intuition behind describing general structure using abstract gsmn
algorithm deliberately leaves number details unspecified  filled in
concrete gsmn algorithm  presented next section  note choices

   

fiefficient markov network structure discovery using independence tests

algorithm   gsmn algorithm outline  g   gsmn  v  d  
   initialize g empty graph 
   variables x domain v
  
   learn markov blanket bx x using gs algorithm    
  
bx gs  x  v  d 
  
add undirected edge g x variable bx  
   return g

algorithm   gs algorithm  returns markov blanket bx variable x v  bx  
gs  x  v  d  
  
  
  
  
  
  
  
  
  
   
   
   

bx
   grow phase    
variable v  x 
 x  y   bx    estimated using data d 
bx bx  y  
goto      restart grow loop    
   shrink phase    
variable bx
 xy   bx  y     estimated using data d 
bx bx  y  
goto      restart shrink loop    
return bx

details source optimizations reduce algorithms computational cost 
make explicit discuss concrete gsmn gsimn algorithms 
abstract gsmn algorithm shown algorithm    given input data set
set variables v  gsmn computes set nodes  variables  bx adjacent
variable x v  completely determine structure domain mn 
algorithm consists main loop learns markov blanket bx node
 variable  x domain using gs algorithm  constructs markov network
structure connecting x variable bx  
gs algorithm first proposed margaritis thrun        shown
algorithm    consists two phases  grow phase shrink phase  grow phase
x proceeds attempting add variable current set hypothesized
neighbors x  contained bx   initially empty  bx grows variable
iteration grow loop x found dependent x
given current set hypothesized neighbors bx   due  unspecified  ordering
variables examined  this explicitly specified concrete gsmn algorithm 
presented next section   end grow phase variables bx
might true neighbors x underlying mnthese called false positives 
justifies shrink phase algorithm  removes false positive bx
testing independence x conditioned bx  y    found independent
x shrink phase  cannot true neighbor  i e   cannot edge x   
gsmn removes bx   assuming faithfulness correctness independence
query results  end shrink phase bx contains exactly neighbors x
underlying markov network 
   

fibromberg  margaritis    honavar

next section present concrete implementation gsmn  called gsmn  
augments gsmn specifying concrete ordering variables x examined
main loop gsmn  lines    algorithm     well concrete order
variables examined grow shrink phases gs algorithm  lines   
    algorithm    respectively  
    concrete gsmn algorithm
section discuss first algorithm  gsmn  grow shrink markov network
learning algorithm   learning structure markov network domain  note
reason introducing gsmn addition main contribution  gsimn
algorithm  presented later section       comparison reasons  particular  gsimn
gsmn identical structure  following order examination variables 
difference use inference gsimn  see details subsequent
sections   introducing gsmn therefore makes possible measure precisely  through
experimental results section    benefits use inference performance 
gsmn algorithm shown algorithm    structure similar abstract
gsmn algorithm  one notable difference order variables examined
specified  done initialization phase so called examination order
grow order x variable x v determined  x priority
queues initially permutation v  x permutation v  x  
position variable queue denotes priority e g               means
variable   highest priority  will examined first   followed   finally   
similarly  position variable x determines order examined
grow phase x 
initialization phase algorithm computes strength unconditional
dependence pair variable x   given unconditional p value
p x      independence test pair variables x      denoted
pxy algorithm   in practice logarithm p values computed  allows
greater precision domains dependencies may strong weak  
particular  algorithm gives higher priority  examines earlier  variables
lower average log p value  line     indicating stronger dependence  average defined
as 
x
 
avg log pxy    
log pxy   
 v   

  x

grow order x variable x  algorithm gives higher priority variables
whose p value  or equivalently log p value  variable x small  line    
ordering due intuition behind folk theorem  as koller   sahami       
puts it  states probabilistic influence association attributes tends
attenuate distance graphical model  suggests pair variables x
high unconditional p value less likely directly linked  note ordering
heuristic guaranteed hold general  example  may hold
underlying domain bayesian network e g   two spouses may independent
unconditionally dependent conditional common child  note however
example apply faithful domains i e   graph isomorph markov network 
   

fiefficient markov network structure discovery using independence tests

algorithm   gsmn   concrete implementation gsmn  g   gsmn  v  d  
  
  
  
  
  

initialize g empty graph 
   initialization    
x  v  x   
pxy p x     


initialize i              n       avg log pi j     avg log pi j    
j

j

   x v
  
bx


  
initialize x j  j             n     j   j pxx   pxx  
j

j

  
remove x x  
       main loop    
    empty
   
x dequeue  
   
   propagation phase    
   
 y   examined x  
   
f  y   examined x
   
   
t  move end x  
   
f  move end x  
   
   grow phase    
   

   
x empty
   
dequeue x  
   
pxy
   
igsmn  x  y  s  f  t 
   
 y  
   
   change grow order     
   
move x beginning  
   
w   s s   s 
   
move w beginning  
   
   change examination order    
   
w   s s   s 
   
w
   
move w beginning  
   
break line   
   
   shrink phase    
   
  s s   s 
   
igsmn  x  y   y     f  t 
   
 y  
   
bx
   
add undirected edge g x variable bx  
    return g

note correctness algorithms present depend holding i e  
prove appendices b  gsmn gsimn guaranteed return
correct structure assumptions stated section   above  note
computational cost calculation pxy low due empty conditioning set 
remaining gsmn algorithm contains main loop  lines      
variable v examined according examination order   determined
   

fibromberg  margaritis    honavar

algorithm   igsmn  x  y  s  f  t   calculate independence test  x    s  propagation  possible  otherwise run statistical test data 
  
  
  
  
  
  
  
  
  

   attempt infer dependence propagation    

return false
   attempt infer independence propagation    
f
return true
   else statistical test data    
  p x y  z         true iff p value statistical test  x    s        
return

initialization phase  main loop includes three phases  propagation phase  lines
       grow phase  lines        shrink phase  lines        propagation
phase optimization variables already computed
 i e   variables already examined  collected two sets f t  set f  t  contains
variables x
   x    sets passed independence
procedure igsmn   shown algorithm    purpose avoiding execution
tests x algorithm  justified fact that  undirected
graphs  markov blanket x x markov blanket  
variables already found contain x blanket  set f  cannot members
bx exists set variables rendered conditionally
independent x previous step  independence therefore inferred easily 
note experiments section paper  section    evaluate gsmn
without propagation phase  order measure effect propagation
optimization performance  turning propagation accomplished simply setting
sets f  as computed lines        respectively  empty set 
another difference gsmn abstract gsmn algorithm use condition pxy  line      additional optimization avoids independence test
case x found  unconditionally  independent initialization
phase  since case would imply x independent given conditioning
set axiom strong union 
crucial difference gsmn abstract gsmn algorithm gsmn
changes examination order grow order every variable x    since
x
  x   excludes grow order x itself   changes ordering proceed
follows  end grow phase variable x  new examination order  set
lines       dictates next variable w examined x last
added growing phase yet examined  i e   w still   
grow order variables found dependent x changed  done
maximize number optimizations gsimn algorithm  our main contribution
paper  shares algorithm structure gsmn   changes grow order
therefore explained detail section     gsimn presented 
final difference gsmn abstract gsmn algorithm restart
actions grow shrink phases gsmn whenever current markov blanket
modified  lines      algorithm     present gsmn   restarting

   

fiefficient markov network structure discovery using independence tests

figure    illustration operation gsmn using independence graph  figure
shows growing phase variable    variables examined according
grow order                           

loops necessary gs algorithm due original usage learning
structure bayesian networks  task  possible true member
blanket x found initially independent grow loop conditioning
set found dependent later conditioned superset s 
could happen unshielded spouse x i e   one common
children x existed direct link x underlying bayesian
network  however  behavior impossible domain distribution faithful
markov network  one assumptions   independence x given
must hold superset axiom strong union  see eqs       
restart grow shrink loops therefore omitted gsmn order save
unnecessary tests  note that  even though possible behavior impossible
faithful domains  possible unfaithful ones  experimentally evaluated
algorithms real world domains assumption markov faithfulness may
necessarily hold  section    
proof correctness gsmn presented appendix a 
    independence graphs
demonstrate operation gsmn graphically concept independence
graph  introduce  define independence graph undirected
graph conditional independences dependencies single variables
represented one annotated edges them  solid  dotted  edge
variables x annotated z represents fact x found
dependent  independent  given z  conditioning set z enclosed parentheses
edge represents independence dependence inferred eqs       as
opposed computed statistical tests   shown graphically 
   

fibromberg  margaritis    honavar

x
x
x
x

z


 x  y   z 



 xy   z 



 x  y   z   inferred 



 xy   z   inferred 

z
 z 
 z 

instance  figure    dotted edge     annotated      represents
fact                absence edge two variables indicates
absence information independence dependence variables
conditioning set 
example    figure   illustrates operation gsmn using independence graph
domain whose underlying markov network shown figure    figure shows
independence graph end grow phase variable    first examination
order    we discuss example initialization phase gsmn   instead 
assume examination    grow    orders shown figure   according
vertex separation underlying network  figure     variables            found
dependent   growing phase i e  
i          
i             
i                
i                  
therefore connected   independence graph solid edges annotated sets
                        respectively  variables         found independent i e  
i                
i                   
i                     
thus connected   dotted edges annotated                               
respectively 
    triangle theorem
section present prove theorem used subsequent gsimn
algorithm  seen  main idea behind gsimn algorithm attempt decrease number tests done exploiting properties conditional independence
relation faithful domains i e   eqs       properties seen inference rules
used derive new independences ones know true  careful
study axioms suggests two simple inference rules  stated triangle
theorem below  sufficient inferring useful independence information
inferred systematic application inference rules  confirmed
experiments section   
   

fiefficient markov network structure discovery using independence tests

figure    independence graph depicting triangle theorem  edges graph
labeled sets represent conditional independences dependencies  solid
 dotted  edge x labeled z means x dependent
 independent  given z  set label enclosed parentheses means edge
inferred theorem 

theorem    triangle theorem   given eqs       every variable x    w sets z 
z   x  y  w   z     x  y  w   z     
 x  w   z     w  y   z   

 

 x  y   z  z   

 xw   z     w  y   z  z   

 

 xy   z    

call first relation d triangle rule second i triangle rule 
proof  using strong union transitivity eqs      shown contrapositive form 
 proof d triangle rule  
strong union  x  w   z    get  x  w   z  z    
strong union  w  y   z    get  w  y   z  z    
transitivity   x  w   z  z      w  y   z  z     get  x  y   z  z    
 proof i triangle rule  
strong union  w  y   z  z    get  w  y   z    
transitivity   xw   z     w  y   z    get  xy   z    

represent triangle theorem graphically using independence graph construct section      figure   depicts two rules triangle theorem using two
independence graphs 
triangle theorem used infer additional conditional independences
tests conducted operation gsmn   example shown figure    illustrates application triangle theorem example presented
figure    independence information inferred triangle theorem shown
curved edges  note conditioning set edge enclosed parentheses  

   

fibromberg  margaritis    honavar

figure    illustration use triangle theorem example figure    set
variables enclosed parentheses correspond tests inferred triangle
theorem using two adjacent edges antecedents  example  result
               inferred i triangle rule  independence              
dependence                    

example  independence edge        inferred d triangle rule adjacent edges                annotated               respectively  annotation
inferred edge      intersection annotations               
example application i triangle rule edge         inferred edges
              annotations                  respectively  annotation
inferred edge         intersection annotations                  
    gsimn algorithm
previous section saw possibility using two rules triangle
theorem infer result novel tests grow phase  gsimn algorithm
 grow shrink inference based markov network learning algorithm   introduced section  uses triangle theorem similar fashion extend gsmn inferring value
number tests gsmn executes  making evaluation unnecessary  gsimn
gsmn work exactly way  and thus gsimn algorithm shares exactly
algorithmic description i e   follow algorithm     differences
concentrated independence procedure use  instead using independence
procedure igsmn gsmn   gsimn uses procedure igsimn   shown algorithm    procedure igsimn   addition attempting propagate blanket information obtained
examination previous variables  as igsmn does   attempts infer
value independence test provided input either strong union
axiom  listed eqs       triangle theorem  attempt successful  igsimn
returns value inferred  true false   otherwise defaults statistical test
data set  as igsmn does   purpose assisting inference process  gsimn
   

fiefficient markov network structure discovery using independence tests

algorithm   igsimn  x  y  s  f  t   calculate independence test result inference  including propagation   possible  record test result knowledge base 
  
  
  
  
  
  
  
  
  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   attempt infer dependence propagation    

return false
   attempt infer independence propagation    
f
return true
   attempt infer dependence strong union    
 a  false  kxy
return false
   attempt infer dependence d triangle rule    
w
 a  false  kxw  b  false  kw b
add  a b  false  kxy ky x  
return false
   attempt infer independence strong union    
 a  true  kxy
return true
   attempt infer independence i triangle rule    
w
 a  true  kxw s t   b  false  kw s t  b
add  a  true  kxy ky x  
return true
   else statistical test data    
  p x y  z         true iff p value statistical test  x    s        
add  s  t  kxy ky x  
return

igsimn maintain knowledge base kxy pair variables x   containing
outcomes tests evaluated far x  either data inferred  
knowledge bases empty beginning gsimn algorithm  the initialization step shown algorithm since gsmn use it   maintained
within test procedure igsimn  
explain igsimn  algorithm    detail  igsimn attempts infer independence value input triplet  x    s  applying single step backward
chaining using strong union triangle rules i e   searches knowledge base
k    kxy   x  v  antecedents instances rules input triplet
 x    s  consequent  strong union rule used direct shown
eqs      contrapositive form  direct form used infer independences  therefore refer i su rule on  contrapositive form 
i su rule becomes  x  y   w     x  y   s   referred d su rule
since used infer dependencies  according d triangle d su rules 
dependence  x  y   s  inferred knowledge base k contains
   test  x  y   a  s 
   tests  x  w   a   w  y   b  variable w   b s 
   

fibromberg  margaritis    honavar

figure    illustration operation gsimn  figure shows grow phase two
consecutively examined variables      figure shows variable
examined second      according change examination order
lines      algorithm    set variables enclosed parentheses
correspond tests inferred triangle theorem using two adjacent edges
antecedents  results                                                  
            b   shown highlighted  executed inferred tests
done  a  

respectively  according i triangle i su rules  independence  xy   s 
inferred knowledge base contains
   test  xy   a  s 
   tests  xw   a   w  y   b  variable w   b a 
respectively 
changes grow orders variables occur inside grow phase
currently examined variable x  lines      gsimn i e   algorithm   igsmn replaced igsimn     particular  if  variable   algorithm reaches line    
i e   pxy igsimn  x  y  s    false  x variables found
dependent x  i e   variables currently s  promoted beginning
grow order   illustrated figure   variable    depicts grow
phase two consecutively examined variables      figure  curved edges
show tests inferred igsimn grow phase variable    grow
order   changes                                                     grow phase
variable   complete variables           promoted  in order 
beginning queue  rationale observation increases
number tests inferred gsimn next step  change examination
grow orders described chosen inferred tests learning
blanket variable   match exactly required algorithm future step 
   

fiefficient markov network structure discovery using independence tests

particular  note example set inferred dependencies variable
found dependent     exactly required initial part grow
phase variable    shown highlighted figure   b   the first four dependencies  
independence tests inferred  not conducted   resulting computational savings 
general  last dependent variable grow phase x maximum number
dependences independences inferred provides rationale change
grow order selection algorithm examined next 
shown assumptions gsmn   structure returned
gsimn correct one i e   set bx computed gsimn algorithm equals
exactly neighbors x  proof correctness gsimn based correctness
gsmn presented appendix b 
    gsimn technical implementation details
section discuss number practical issues subtly influence accuracy
efficiency implementation gsimn  one order application i su  d su 
i triangle d triangle rules within function igsimn   given independence query
oracle  order application matterassuming one rules
inferring value independence  guaranteed produce
value due soundness axioms eqs       pearl         practice however 
oracle implemented statistical tests conducted data incorrect 
previously mentioned  particular importance observation false independences
likely occur false dependencies  one example case
domain dependencies weakin case pair variables connected  dependent 
underlying true network structure may incorrectly deemed independent paths
long enough  hand  false dependencies much rare
confidence threshold          statistical test tells us probability
false dependence chance alone     assuming i i d  data test  chance
multiple false dependencies even lower  decreasing exponentially fast  practical
observation i e   dependencies typically reliable independences  provide
rationale way igsimn algorithm works  particular  igsimn prioritizes
application rules whose antecedents contain dependencies first i e   d triangle
d su rules  followed i triangle i su rules  effect  uses statistical results
typically known greater confidence ones usually less reliable 
second practical issue concerns efficient inference  gsimn algorithm uses onestep inference procedure  shown algorithm    utilizes knowledge base k    kxy  
containing known independences dependences pair variables x  
implement inference efficiently utilize data structure k purpose
storing retrieving independence facts constant time  consists two  d arrays 
one dependencies another independencies  array n n size  n
number variables domain  cell array corresponds pair
variables  x     stores known independences  dependences  x
form list conditioning sets  conditioning set z list  knowledge
base kxy represents known independence  xy   z   dependence  x  y   z   
important note length list    two

   

fibromberg  margaritis    honavar

tests done variable x execution gsimn  done
growing shrinking phases   thus  always takes constant time retrieve store
independence  dependence   therefore inferences using knowledge base
constant time well  note uses strong union axion igsimn
algorithm constant time well  accomplished testing  at
two  sets stored kxy subset superset inclusion 

   experimental results
evaluated gsmn gsimn algorithms artificial real world data sets 
experimental results presented show simple application
pearls inference rules gsimn algorithm results significant reduction number
tests performed compared gsmn without adversely affecting quality
output network  particular report following quantities 
weighted number tests  weighted number tests computed
summation weight test executed  weight test  x    z 
defined    z   quantity reflects time complexity algorithm  gsmn
gsimn  used assess benefit gsimn using inference instead
executing statistical tests data  standard method comparison
independence based algorithms justified observation running
time statistical test triplet  x    z  proportional size n data
set number variables involved i e   o n   z       and exponential
number variables involved nave implementation might assume  
one construct non zero entries contingency table used
test examining data point data set exactly once  time proportional
number variables involved test i e   proportional   x   z       z  
execution time  order assess impact inference running time
 in addition impact statistical tests   report execution time
algorithm 
quality resulting network  measure quality two ways 
normalized hamming distance  hamming distance output
network structure underlying model another measure
quality output network  actual network used generate
data known  hamming distance defined number reversed
edges two network structures  i e   number times actual
edge true network missing returned network edge absent
true network exists algorithms output network  value zero
means output network correct structure  able compare
domains
different dimensionalities  number variables n  normalize

n
    total number node pairs corresponding domain 
accuracy  real world data sets underlying network unknown 
hamming distance calculation possible  case impossible know
true value independence  therefore approximate statistical
test entire data set  use limited  randomly chosen subset     
data set  learn network  measure accuracy compare result
   

fiefficient markov network structure discovery using independence tests

 true false  number conditional independence tests network
output  using vertex separation   tests performed full data
set 
experiments involving data sets used   statistical test estimation
conditional independences  mentioned above  rules thumb exist deem certain
tests potentially unreliable depending counts contingency table involved 
example  one rule cochran        deems test unreliable    
cells contingency table less   data points test  due requirement
answer must obtained independence algorithm conducting test  used
outcomes tests well experiments  effect possibly unreliable
tests quality resulting network measured accuracy measures  listed
above 
next section present results domains underlying probabilistic
model known  followed real world data experiments model structure
available 
    known model experiments
first set experiments underlying model  called true model true network 
known markov network  purpose set experiments conduct controlled
evaluation quality output network systematic study algorithms
behavior varying conditions domain size  number variables  amount
dependencies  average node degree network  
true network contains n variables generated randomly follows 
network initialized n nodes edges  user specified parameter
network structure average node degree equals average number neighbors
per node  given   every node set neighbors determined randomly
uniformly selecting first n  pairs random permutation possible pairs 
factor     necessary edge contributes degree two nodes 
conducted two types experiments using known network structure  exact learning
experiments sample based experiments 
      exact learning experiments
set known model experiments  assume result statistical queries
asked gsmn gsimn algorithms available  assumes existence
oracle answer independence queries  underlying model known 
oracle implemented vertex separation  benefits querying
true network independence two  first  ensures faithfulness correctness
independence query results  allows evaluation algorithms
assumptions correctness  second  tests performed much faster actual
statistical tests data  allowed us evaluate algorithms large networkswe
able conduct experiments domains containing     variables 
first report weighted number tests executed gsmn without
propagation gsimn  results summarized figure    shows ratio
weighted number tests gsimn two versions gsmn   one
   

fiwc gsimn    wc gsmn  propagation 

ratio weighted cost gsimn vs  gsmn  without propagation
 
   
   
   
   
  
  
  
  

   
   
   
   
   
 
 

  

  

  
  
  
  
  
  
domain size  number variables 

  

   

wc gsimn    wc gsmn  without propagation 

bromberg  margaritis    honavar

ratio weighted cost gsimn vs  gsmn  propagation
 
   
   
   
   
   
   
  
  
  
  

   
   
   
 
 

  

  

  
  
  
  
  
  
domain size  number variables 

  

   

figure    ratio weighted number tests gsimn gsmn without propagation  left plot  propagation  right plot  network sizes  number
nodes  n       average degree              
algorithm   ifch  x  y  s  f  t   forward chaining implementation independence test
igsimn  x  y  s  f  t  
  
  
  
  
  
  
  

   query knowledge base    
 s  t  kxy
return
result test  x    s       true iff test  x    s  returns independence    
add  s  t  kxy ky x  
run forward chaining inference algorithm k  update k 
return

hundred true networks generated randomly pair  n     figure shows
mean value  see limiting reduction  as n grows large  weighted
number tests depends primarily average degree parameter   reduction
gsimn large n dense networks        approximately     compared gsmn
propagation     compared gsmn without propagation optimization 
demonstrating benefit gsimn vs  gsmn terms number tests executed 
one reasonable question performance gsimn extent inference
procedure complete i e   tests gsimn needs operation 
number tests infers  by applying single step backward chaining
strong union axiom triangle theorem  rather executing statistical test
data  compare number tests inferred  for example using complete
automated theorem prover eqs        measure this  compared number tests
done gsimn number done alternative algorithm  call gsimnfch  gsimn forward chaining   gsimn fch differs gsimn function
ifch   shown algorithm    replaces function igsimn gsimn  ifch exhaustively
produces independence statements inferred properties eqs     
using forward chaining procedure  process iteratively builds knowledge base k
containing truth value conditional independence predicates  whenever outcome
test required  k queried  line   ifch algorithm     value test
   

fiefficient markov network structure discovery using independence tests

ratio number tests gsimn fch gsimn

  
  
  
  

   
   

ratio

 
   
   
   
   
 

 

 

 

 

 

 

 

 

        

number variables  n 

figure    ratio number tests gsimn fch gsimn network sizes  number
variables  n     n      average degrees              

found k  returned  line     not  gsimn fch performs test uses result
standard forward chaining automatic theorem prover subroutine  line    produce
independence statements inferred test result k  adding new
facts k 
comparison number tests executed gsimn vs  gsimn fch presented
figure    shows ratio number tests gsimn gsimn fch 
figure shows mean value four runs  corresponding network generated
randomly pair  n                 n     unfortunately  two
days execution gsimn fch unable complete execution domains containing
   variables more  therefore present results domain sizes    only 
figure shows n    every ratio exactly   i e   tests inferable
produced use triangle theorem gsimn  smaller domains  ratio
     exception single case   n           
      sample based experiments
set experiments evaluate gsmn  with without propagation  gsimn
data sampled true model  allows realistic assessment
performance algorithms  data sampled true  known  markov
network using gibbs sampling 
exact learning experiments previous section structure true
network required  generated randomly fashion described above  sample data
known structure however  one needs specify network parameters 
random network  parameters determine strength dependencies among connected
variables graph  following agresti         used log odds ratio measure
strength probabilistic influence two binary variables x   defined

pr x           pr x          
xy   log
 
pr x           pr x          

   

fibromberg  margaritis    honavar

hamming distance sampled data
n                 

gsmn  without propagation
gsmn  propagation
gsimn

   
   
   
 
 

 

 

  

  

  

  

  

   
   
 

  

 

 

data set size  thousands data points 

gsmn  without propagation
gsmn  propagation
gsimn

   
   
   
 
 

 

 

 

  

  

  

  

  

   
 
  

  

  

  

  

 

   
 
  

  

  

  

  

data set size  thousands data points 

 

  

  

  

  

  

   
 
 

 

  

  

  

  

  

  

   
   
 
 

 

 

  

  

  

  

  

data set size  thousands data points 

  

  

  

   
   
 
 

 

 

 

 

  

  

  

  

  

  

 
gsmn  without propagation
gsmn  propagation
gsimn

   
   
   
   
 
 

 

 

 

 

  

  

  

  

  

  

hamming distance sampled data
n                 

   

 

  

data set size  thousands data points 

gsmn  without propagation
gsmn  propagation
gsimn

 

  

   

  

 
   

  

hamming distance sampled data
n                 

   

 

 

data set size  thousands data points 

   

 

 

gsmn  without propagation
gsmn  propagation
gsimn

   

  

gsmn  without propagation
gsmn  propagation
gsimn

   

 

normalized hamming distance

normalized hamming distance

   

 

 

 

 

hamming distance sampled data
n                 

   

 

 

data set size  thousands data points 

gsmn  without propagation
gsmn  propagation
gsimn

 

 

 

hamming distance sampled data
n                 

 

 

hamming distance sampled data
n                 

 

  

 

 

 

data set size  thousands data points 

   

data set size  thousands data points 

   

   

  

   

 

normalized hamming distance

normalized hamming distance

   

 

  

   

hamming distance sampled data
n                 

   

 

  

   

data set size  thousands data points 

gsmn  without propagation
gsmn  propagation
gsimn

 

  

   

hamming distance sampled data
n                 

 

  

gsmn  without propagation
gsmn  propagation
gsimn

   

  

 

 

  

 

data set size  thousands data points 

   

 

gsmn  without propagation
gsmn  propagation
gsimn

   

hamming distance sampled data
n                 
normalized hamming distance

normalized hamming distance

hamming distance sampled data
n                 

 

 

 

data set size  thousands data points 

 
   

 

normalized hamming distance

 

   

normalized hamming distance

 

gsmn  without propagation
gsmn  propagation
gsimn

   

normalized hamming distance

   

hamming distance sampled data
n                 

 

normalized hamming distance

normalized hamming distance

normalized hamming distance

hamming distance sampled data
n                 
 

  

 
gsmn  without propagation
gsmn  propagation
gsimn

   
   
   
   
 
 

 

 

 

 

  

  

  

  

  

  

data set size  thousands data points 

figure    normalized hamming distances true network network output
gsmn  with without propagation  gsimn domain size n     
average degrees              

network parameters generated randomly log odds ratio every
pair variables connected edge graph specified value  set
experiments  used values                every pair variables
network 
figures     show plots normalized hamming distance true
network output gsmn  with without propagation  gsimn
domain sizes n      n      variables  respectively  plots show
hamming distance gsimn comparable ones gsmn algorithms

   

fiefficient markov network structure discovery using independence tests

hamming distance sampled data
n                 

gsmn  without propagation
gsmn  propagation
gsimn

   
   
   
 
 

 

 

  

  

  

  

  

   
   
 

  

 

 

data set size  thousands data points 

gsmn  without propagation
gsmn  propagation
gsimn

   
   
   
 
 

 

 

 

  

  

  

  

  

   
 
  

  

  

  

  

 

   
 
  

  

  

  

  

data set size  thousands data points 

 

  

  

  

  

  

   
 
 

 

  

  

  

  

  

  

   
   
 
 

 

 

  

  

  

  

  

data set size  thousands data points 

  

  

  

   
   
 
 

 

 

 

 

  

  

  

  

  

  

 
gsmn  without propagation
gsmn  propagation
gsimn

   
   
   
   
 
 

 

 

 

 

  

  

  

  

  

  

hamming distance sampled data
n                 

   

 

  

data set size  thousands data points 

gsmn  without propagation
gsmn  propagation
gsimn

 

  

   

  

 
   

  

hamming distance sampled data
n                 

   

 

 

data set size  thousands data points 

   

 

 

gsmn  without propagation
gsmn  propagation
gsimn

   

  

gsmn  without propagation
gsmn  propagation
gsimn

   

 

normalized hamming distance

normalized hamming distance

   

 

 

 

 

hamming distance sampled data
n                 

   

 

 

data set size  thousands data points 

gsmn  without propagation
gsmn  propagation
gsimn

 

 

 

hamming distance sampled data
n                 

 

 

hamming distance sampled data
n                 

 

  

 

 

 

data set size  thousands data points 

   

data set size  thousands data points 

   

   

  

   

 

normalized hamming distance

normalized hamming distance

   

 

  

   

hamming distance sampled data
n                 

   

 

  

   

data set size  thousands data points 

gsmn  without propagation
gsmn  propagation
gsimn

 

  

   

hamming distance sampled data
n                 

 

  

gsmn  without propagation
gsmn  propagation
gsimn

   

  

 

 

  

 

data set size  thousands data points 

   

 

gsmn  without propagation
gsmn  propagation
gsimn

   

hamming distance sampled data
n                 
normalized hamming distance

normalized hamming distance

hamming distance sampled data
n                 

 

 

 

data set size  thousands data points 

 
   

 

normalized hamming distance

 

   

normalized hamming distance

 

gsmn  without propagation
gsmn  propagation
gsimn

   

normalized hamming distance

   

hamming distance sampled data
n                 

 

normalized hamming distance

normalized hamming distance

normalized hamming distance

hamming distance sampled data
n                 
 

  

 
gsmn  without propagation
gsmn  propagation
gsimn

   
   
   
   
 
 

 

 

 

 

  

  

  

  

  

  

data set size  thousands data points 

figure    normalized hamming distance results figure   domain size n      

domain sizes n      n       average degrees              log odds ratios     
           reinforces claim inference done gsimn small
impact quality output networks 
figure    shows weighted number tests gsimn vs  gsmn  with without
propagation  sampled data set        points domains n       n      
average degree parameters              log odds ratios             gsimn
shows reduced weighted number tests respect gsmn without propagation
cases compared gsmn propagation cases  with exceptions
                            sparse networks weak dependences i e  
     reduction larger     domain sizes  reduction much larger

   

fibromberg  margaritis    honavar

weighted cost sampled data
                   data points

weighted cost sampled data
                   data points

      
      
      
     
 

      

gsmn  without propagation
gsmn  propagation
gsimn

      
      
      
     
 

  

  

weighted cost sampled data
                   data points

      
     
 

      

gsmn  without propagation
gsmn  propagation
gsimn

      
      
      
     

  

gsmn  without propagation
gsmn  propagation
gsimn

      
      
      
     

  

  

weighted cost sampled data
                   data points

weighted cost sampled data
                   data points

      
      
      
     
 

      

      
gsmn  without propagation
gsmn  propagation
gsimn

weighted number tests

gsmn  without propagation
gsmn  propagation
gsimn

      
      
      
     
 

  

      

gsmn  without propagation
gsmn  propagation
gsimn

      
      
      
     
 

  

number variables

  

  

number variables

weighted cost sampled data
                   data points

weighted cost sampled data
                   data points

      
weighted number tests

gsmn  without propagation
gsmn  propagation
gsimn

      
      
      
     
 

      

      
gsmn  without propagation
gsmn  propagation
gsimn

      
      
      
     
 

  

  
number variables

weighted cost sampled data
                   data points

      

  
number variables

      
weighted number tests

weighted number tests

      

number variables

weighted cost sampled data
                   data points

weighted number tests

weighted cost sampled data
                   data points

 
  

      

  

      

number variables

number variables

     

number variables

 

  

      

  

weighted number tests

      

      

      

weighted cost sampled data
                   data points
weighted number tests

weighted number tests

gsmn  without propagation
gsmn  propagation
gsimn

  

      

  

      

      

      

gsmn  without propagation
gsmn  propagation
gsimn

number variables

      

  

      

 
  

number variables

      

weighted number tests

gsmn  without propagation
gsmn  propagation
gsimn

      

weighted number tests

      

weighted cost sampled data
                   data points

      
weighted number tests

weighted number tests

      

      

gsmn  without propagation
gsmn  propagation
gsimn

      
      
      
     
 

  

  
number variables

  

  
number variables

figure     weighted number tests executed gsmn  with without propagation 
gsimn  d             domains sizes n          average degree
parameters               log odds ratios              

one observed exact learning experiments  actual execution times
various data set sizes network densities shown figure    largest domain
n            verifying reduction cost gsimn various data set sizes 
note reduction proportional number data points  reasonable
test executed must go entire data set construct contingency table 
confirms claim cost inference gsimn small  constant time per
test  see discussion section      compared execution time tests themselves 
indicates increasing cost benefits use gsimn even large data sets 

   

fiefficient markov network structure discovery using independence tests

execution times sampled data sets
n      variables          

execution times sampled data sets
n      variables          

   

   
gsmn  without propagation
gsmn  propagation
gsimn

gsmn  without propagation
gsmn  propagation
gsimn

   
execution time  sec 

execution time  sec 

   
   
   
   
  

   
   
   
  

 

 
 

                                                       

 

execution times sampled data sets
n      variables          

execution times sampled data sets
n      variables          

   

   
gsmn  without propagation
gsmn  propagation
gsimn

gsmn  without propagation
gsmn  propagation
gsimn

   
execution time  sec 

   
execution time  sec 

                                                       

   
   
   
  

   
   
   
  

 

 
 

                                                       

 

                                                       

figure     execution times sampled data experiments              top row 
        bottom row  domain n      variables 

      real world network sampled data experiments
conducted sampled data experiments well known real world networks 
known repository markov networks drawn real world domains  instead
utilized well known bayesian networks widely used bayesian network research
available number repositories   generate markov networks
bayesian network structures used process moralization  lauritzen       
consists two steps   a  connect pair nodes bayesian network
common child undirected edge  b  remove directions edges 
results markov network local markov property valid i e   node
conditionally independent nodes domain given direct neighbors 
procedure conditional independences may lost  this  however  affect
accuracy results compare independencies output network
moralized markov network  as opposed bayesian network  
conducted experiments using   real world domains  hailfinder  insurance  alarm 
mildew  water  domain sampled varying number data points
corresponding bayesian network using logic sampling  henrion         used input
gsmn  with without propagation  gsimn algorithms  compared
network output algorithms original moralized network using
normalized hamming distance metric previously described  results shown
   used http   compbio cs huji ac il repository   accessed december         

   

fibromberg  margaritis    honavar

hamming distance hailfinder data set

hamming distance insurance data set

hamming distance alarm data set

gsmn  without propagation
gsmn  propagation
gsimn

   
   
   
   
   
   
   
   
   
 
 

 

 

 

 

           

 
gsmn  without propagation
gsmn  propagation
gsimn

   
   
   
   
   
   
   
   
   
 

        

 

data set size  thousands data points 

 

 

 

 

           

normalized hamming distance

normalized hamming distance

   
   
   
   
   
   
   
 

        

 

 

 

 

 

           

        

data set size  thousands data points 

hamming distance water data set

gsmn  without propagation
gsmn  propagation
gsimn

   
   

gsmn  without propagation
gsmn  propagation
gsimn

   
   

data set size  thousands data points 

hamming distance mildew data set
 
   

normalized hamming distance

 
normalized hamming distance

normalized hamming distance

 

   
   
   
   
   
   
 

 
   

gsmn  without propagation
gsmn  propagation
gsimn

   
   
   
   
   
   
   
   
 

 

 

 

 

 

           

        

 

data set size  thousands data points 

 

 

 

 

           

        

data set size  thousands data points 

figure     normalized hamming distance network output gsmn  with
without propagation  gsimn true markov networks network using
varying data set sizes sampled markov networks various real world
domains modeled bayesian networks 

fig     indicate distances produced three algorithms similar 
cases  e g   water hailfinder  network resulting use gsimn
actually better  of smaller hamming distance  ones output gsmn
algorithms 
measured weighted cost three algorithms domains 
shown fig      plots show significant decrease weighted number tests
gsimn respect gsmn algorithms  cost gsimn     cost
gsmn without propagation average  savings      cost gsimn    
cost gsmn without propagation average  savings     
    real world data experiments
artificial data set studies previous section advantage allowing
controlled systematic study performance algorithms  experiments
real world data necessary realistic assessment performance  real data
challenging may come non random topologies  e g   possibly
irregular lattice many cases spatial data  underlying probability distribution
may faithful 
conducted experiments number data sets obtained uci machine
learning data set repository  newman  hettich  blake    merz         continuous variables
data sets discretized using method widely recommended introductory statistics texts  scott         dictates optimal number equally spaced discretization
bins continuous variable k       log  n   n number points
   

fiefficient markov network structure discovery using independence tests

gsmn  without propagation
gsmn  propagation
gsimn

     
     
     
     
     
     

gsmn  without propagation
gsmn  propagation
gsimn

    

weighted cost tests

weighted cost tests

     

weighted cost tests insurance data set
    
    
    
    
    
    

     
     
     
    
    
    
    

    

 

 
 

 

  

  

  

 
 

 

data set size  thousands data points 

  

  

 

 

  

  

gsmn  without propagation
gsmn  propagation
gsimn

     

    
    
    
    

     
     
     
    

 

 
 

 

  

  

  

 

data set size  thousands data points 

 

  

  

  

data set size  thousands data points 

figure     weighted cost tests conducted gsmn  with without propagation 
gsimn algorithms various real world domains modeled bayesian
networks 
weighted cost accuracy real world data sets
 
acc gsimn    acc gsmn  without propagation 
acc gsimn    acc gsmn  propagation 
wc gsimn    wc gsmn  without propagation 
wc gsimn    wc gsmn  propagation 

   
   
   
   
   
   
   
   
   
 
    
    

    

      

      

         
data set index

                  

figure     ratio weighted number tests gsimn versus gsmn difference
accuracy gsimn gsmn real data sets  ratios smaller
  positive bars indicate advantage gsimn gsmn  
numbers x axis indices data sets shown table   

data set  data set algorithm  report weighted number conditional independence tests conducted discover network accuracy  defined
below 

   

  

data set size  thousands data points 

weighted cost tests water data set

gsmn  without propagation
gsmn  propagation
gsimn

weighted cost tests

weighted cost tests

  

data set size  thousands data points 

weighted cost tests mildew data set
     

gsmn  without propagation
gsmn  propagation
gsimn

     

    

     

weighted cost tests alarm data set

weighted cost tests

weighted cost tests hailfinder data set

fibromberg  margaritis    honavar

table    weighted number tests accuracy several real world data sets 
evaluation measure  best performance gsmn  with without
propagation  gsimn indicated bold  number variables
domain denoted n number data points data set n  

 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  

data set
name
echocardiogram
ecoli
lenses
hayes roth
hepatitis
cmc
balance scale
baloons
flag
tic tac toe
bridges
car
monks  
haberman
nursery
crx
imports   
dermatology
adult

n

n

  
 
 
 
  
  
 
 
  
  
  
 
 
 
 
  
  
  
  

  
   
  
   
  
    
   
  
   
   
  
    
   
   
     
   
   
   
     

weighted number tests
gsmn
gsmn
gsimn
 w o prop    w  prop  
    
    
   
   
   
   
  
  
  
   
  
  
    
   
   
   
   
   
  
  
  
  
  
  
    
    
   
   
   
   
   
   
   
   
   
  
   
  
  
  
  
  
   
   
   
    
   
   
    
    
    
    
    
    
   
   
   

gsmn
 w o prop  
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

accuracy
gsmn
 w  prop  
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

gsimn
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

real world data structure underlying bayesian network  if any 
unknown  impossible measure hamming distance resulting network
structure  instead  measured estimated accuracy network produced gsmn
gsimn comparing result  true false  number conditional independence
tests network learned  using vertex separation  result tests
performed data set  using   test   approach similar estimating accuracy
classification task unseen instances inputs triplets  x  y  z 
class attribute value corresponding conditional independence test 
used     real world data set  randomly sampled  input gsmn gsimn
entire data set   test  corresponds hypothetical scenario
much smaller data set available researcher  approximates true value
test outcome entire data set  since number possible tests
exponential  estimated independence accuracy sampling        triplets  x  y  z 
randomly  evenly distributed among possible conditioning set sizes             n   
 i e          n    tests m   triplets constructed follows 
first  two variables x drawn randomly v  second  conditioning set
determined picking first variables random permutation v  x    
denoting set        triplets  triplet  idata  t  result test
performed entire data set inetwork  t  result test performed

   

fiefficient markov network structure discovery using independence tests

network output either gsmn gsimn  estimated accuracy defined as 

ofifi
  fifin
  inetwork  t    idata  t  fifi 
accuracy
   
 t  

data sets  table   shows detailed results accuracy weighted
number tests gsmn gsimn algorithms  results plotted
figure     horizontal axis indicating data set index appearing first column
table    figure    plots two quantities graph real world data sets 
ratio weighted number tests gsimn versus two gsmn algorithms
difference accuracies  data set  improvement gsimn
gsmn corresponds number smaller   ratios positive histogram bar
accuracy differences  observe gsimn reduced weighted number
tests every data set  maximum savings     gsmn without propagation
 for crx data set      gsmn propagation  for crx data set
well   moreover        data sets gsimn resulted improved accuracy    tie
  somewhat reduced accuracy compared gsmn propagation  for
nursery balance scale data sets  

   conclusions future research
paper presented two algorithms  gsmn gsimn  learning efficiently
structure markov network domain data using independence based
approach  as opposed np hard algorithms based maximum likelihood estimation 
evaluated performance measurement weighted number tests
require learn structure network quality networks learned
artificial real world data sets  gsimn showed decrease vast majority
artificial real world domains output network quality comparable
gsmn   cases showing improvement  addition  gsimn shown
nearly optimal number tests executed compared gsimn fch  uses
exhaustive search produce independence information inferred pearls
axioms  directions future research include investigation way topology
underlying markov network affects number tests required quality
resulting network  especially commonly occurring topologies grids  another
research topic impact number tests examination grow orderings
variables 

acknowledgments
thank adrian silvescu insightful comments accuracy measures general advice
theory undirected graphical models 

appendix a  correctness gsmn
variable x v examined main loop gsmn algorithm  lines
       set bx variable x v constructed growing shrinking set s 
   

fibromberg  margaritis    honavar

starting empty set  x connected member bx produce
structure markov network  prove procedure returns actual markov
network structure domain 
proof correctness make following assumptions 
axioms eqs      hold 
probability distribution domain strictly positive  required intersection
axiom hold  
tests conducted querying oracle  returns true value underlying model 
algorithm examines every variable x inclusion  and thus bx  
grow phase  lines        and  added grow phase 
considers removal shrinking phase  lines         note
one test executed x growing phase x  call grow
test x  line      similarly  one tests executed x
shrinking phase  test  if executed  called shrink test x  line
    
general idea behind proof show that  learning blanket x 
variable end shrinking phase dependence  x  y  
v  x     x holds  which  according theorem   end
appendix  implies edge x    immediately prove one
direction 
lemma   
  end shrink phase   xy   v  x     
proof  let us assume
  end shrink phase  then  either
added set grow phase  i e   line    never reached   removed
shrink phase  i e   line    reached   former true
 pxy     line     indicating x unconditionally independent  found
independent x line     latter true found independent x
line     cases v  x     xy   a   strong union
 xy   v  x     
opposite direction proved lemma   below  however  proof involved 
requiring auxiliary lemmas  observations  definitions  two main auxiliary
lemmas      use lemma presented next  lemma    inductively extend
conditioning set dependencies found grow shrink tests x  
remaining variables v x     lemma shows that  certain independence
holds  conditioning set dependence increased one variable 
lemma    let x  v  z v  x     z z  w v 
 x  y   z   xw   z  y       x  y   z  w    

   

fiefficient markov network structure discovery using independence tests

proof  prove contradiction  make use axioms intersection  i   strong
union  su   decomposition  d   let us assume  x  y   z   xw   z  y   
 xy   z  w    
 xy   z  w     xw   z  y   
su

 

 xy   z  w     xw   z  y   



 x y  w     z 



 

 xy   z   xw   z 

 

 xy   z  

 

contradicts assumption  x  y   z  
introduce notation definitions prove auxiliary lemmas 
denote sg value end grow phase  line     i e   set
variables found dependent x grow phase  ss value end
shrink phase  line      denote g set variables found independent
x grow phase u    u            uk   sequence variables shrunk
bx   i e   found independent x shrink phase  sequence u assumed
ordered follows    j variable ui found independent x uj
shrinking phase  prefix first variables  u            ui    u denoted
ui   test performed algorithm  define k t  integer
uk t  prefix u containing variables found independent x
loop t  furthermore  abbreviate uk t  ut  
definition u fact grow phase conditioning set
increases dependent variables only  immediately make following observation 
observation    variable ui u  denotes shrink test performed
x ui ut   ui   
relate conditioning set shrink test ut follows 
lemma    ss    x    z  shrink test   z   sg ut  y   
proof  according line    algorithm  z    y    beginning shrink
phase  line       sg   variables found independent afterward conducted
removed line     thus  time performed    sg ut
conditioning set becomes sg ut  y   
corollary     xui   sg ui   
proof  proof follows immediately lemma    observation    fact
ui   ui   ui   
following two lemmas use lemma   inductively extend conditioning set
dependence x variable ss   first lemma starts shrink
test x  a dependence   extends conditioning set ss  y    or
equivalently sg  y   ut according lemma    sg  y   
   

fibromberg  margaritis    honavar

lemma    ss shrink test    x  y   sg  y    
proof  proof proceeds proving
 x  y   sg  y   ui  
induction decreasing values i                 k t    starting   k t  
lemma follows     noticing u     
base case  i   k t    lemma       x    sg  y   ut    equals
 x    sg  y   uk t    definition ut   since ss   must case
found dependent  i e    x  y   sg  y   uk t    
inductive step  let us assume statement true   m      k t   
 x  y   sg  y   um   

   

need prove true     
 x  y   sg  y   um    
corollary   
 xum   sg um  
strong union 
 xum    sg um    y   

 xum    sg um  y     y    

   

eqs           lemma   get desired relation 
 x  y    sg  y   um    um       x  y   sg  y   um    

observation    definition sg   every test    x    z  performed
grow phase  z sg  
following lemma completes extension conditioning set dependence
x ss universe variables v  x     starting sg  y  
 where lemma   left off  extending sg g  y   
lemma    ss    x  y   sg g  y    
proof  proof proceeds proving
 x  y   sg gi  y   
induction increasing values    g   gi denotes first elements
arbitrary ordering set g 
   

fiefficient markov network structure discovery using independence tests

base case  i       follows directly lemma        since g     
inductive step  let us assume statement true   m       g  
 x  y   sg gm  y    

   

need prove true       
 x  y   sg gm    y    

   

observation   grow test gm results independence 
 xgm   z   z sg  
strong union axiom become 
 xgm   z  y     z sg

   

 xgm    z  y     y     z sg  

   

equivalently
since z sg sg gm   z  y   sg gm   eq     
lemma   get desired relation 
 x  y    sg gm  y    gm      x  y   sg gm    y    

finally  prove x dependent every variable ss given universe
v  x    
lemma    ss    x  y   v  x     
proof  lemma   
 x  y   sg g  y   
suffices prove sg g  y     v  x     loop    gsmn  
queue x populated elements v  x   then  line     removed
x   grow phase partitions x variables dependent x  set sg  
independent x  set g  
corollary    ss  x  y   v  x     
proof  follows directly lemmas     
corollary immediately show graph returned
connecting x member bx   ss exactly markov network domain
using following theorem  first published pearl paz        
theorem     pearl   paz        every dependence model satisfying symmetry  decomposition  intersection  eqs       unique markov network g    v  e  produced
deleting complete graph every edge  x     xy   v  x     holds
  i e  
 x   
  e  xy   v  x      
   

fibromberg  margaritis    honavar

appendix b  correctness gsimn
gsimn algorithm differs gsmn use test subroutine igsimn
instead igsmn  algorithms      respectively   turn differs number
additional inferences conducted obtain independencies  lines       
inferences direct applications strong union axiom  which holds assumption 
triangle theorem  which proven hold theorem     using correctness
gsmn  proven appendix a  therefore conclude gsimn algorithm
correct 

references
abbeel  p   koller  d     ng  a  y          learning factor graphs polynomial time
sample complexity  journal machine learning research              
acid  s     de campos  l  m          searching bayesian network structures
space restricted acyclic partially directed graphs  journal artificial intelligence
research             
agresti  a          categorical data analysis   nd edition   wiley 
aliferis  c  f   tsamardinos  i     statnikov  a          hiton  novel markov blanket
algorithm optimal variable selection  proceedings american medical
informatics association  amia  fall symposium 
anguelov  d   taskar  b   chatalbashev  v   koller  d   gupta  d   heitz  g     ng  a 
        discriminative learning markov random fields segmentation  d range
data  proceedings conference computer vision pattern recognition
 cvpr  
barahona  f          computational complexity ising spin glass models  journal
physics a  mathematical general                    
besag  j          spacial interaction statistical analysis lattice systems  journal
royal statistical society  series b             
besag  j   york  j     mollie  a          bayesian image restoration two applications
spatial statistics   annals institute statistical mathematics          
bromberg  f   margaritis  d     honavar  v          efficient markov network structure discovery independence tests  proceedings siam international conference
data mining 
buntine  w  l          operations learning graphical models  journal artificial
intelligence research            
castelo  r     roverato  a          robust procedure gaussian graphical model search
microarray data p larger n  journal machine learning research 
            
chow  c     liu  c          approximating discrete probability distributions dependence trees  ieee transactions information theory                  

   

fiefficient markov network structure discovery using independence tests

cochran  w  g          methods strengthening common   tests  biometrics 
           
della pietra  s   della pietra  v     lafferty  j          inducing features random fields 
ieee transactions pattern analysis machine intelligence                 
dobra  a   hans  c   jones  b   nevins  j  r   yao  g     west  m          sparse graphical
models exploring gene expression data  journal multivariate analysis         
    
edwards  d          introduction graphical modelling   nd edition   springer  new
york 
friedman  n   linial  m   nachman  i     peer  d          using bayesian networks
analyze expression data  computational biology            
geman  s     geman  d          stochastic relaxation  gibbs distributions  bayesian
relation images   ieee transactions pattern analysis machine intelligence 
          
heckerman  d          tutorial learning bayesian networks  tech  rep  msr tr       
microsoft research 
heckerman  d   geiger  d     chickering  d  m          learning bayesian networks 
combination knowledge statistical data  machine learning             
henrion  m          propagation uncertainty probabilistic logic sampling bayes
networks  lemmer  j  f     kanal  l  n   eds    uncertainty artificial intelligence    elsevier science publishers b v   north holland  
hofmann  r     tresp  v          nonlinear markov networks continuous variables 
neural information processing systems  vol      pp         
isard  m          pampas  real valued graphical models computer vision  ieee
conference computer vision pattern recognition  vol     pp         
jerrum  m     sinclair  a          polynomial time approximation algorithms ising
model  siam journal computing               
kearns  m  j     vazirani  u  v          introduction computational learning theory 
mit press  cambridge  ma 
koller  d     sahami  m          toward optimal feature selection  international conference machine learning  pp         
lam  w     bacchus  f          learning bayesian belief networks  approach based
mdl principle  computational intelligence             
lauritzen  s  l          graphical models  oxford university press 
margaritis  d     thrun  s          bayesian network induction via local neighborhoods 
solla  s   leen  t     muller  k  r   eds    advances neural information processing
systems     pp          mit press 
mccallum  a          efficiently inducing features conditional random fields  proceedings uncertainty artificial intelligence  uai  

   

fibromberg  margaritis    honavar

newman  d  j   hettich  s   blake  c  l     merz  c  j          uci repository machine
learning databases  tech  rep   university california  irvine  dept  information
computer sciences 
pena  j  m          learning gaussian graphical models gene networks false discovery rate control  proceedings  th european conference evolutionary
computation  machine learning data mining bioinformatics  pp         
pearl  j          probabilistic reasoning intelligent systems  networks plausible inference  morgan kaufmann publishers  inc 
pearl  j     paz  a          graphoids  graph based logic reasoning releveance
relations  tech  rep          r    l   cognitive systems laboratory  university
california 
rebane  g     pearl  j          recovery causal poly trees statistical data 
kanal  l  n   levitt  t  s     lemmer  j  f   eds    uncertainty artificial
intelligence    pp          amsterdam  north holland 
schafer  j     strimmer  k          empirical bayes approach inferring large scale
gene association networks  bioinformatics             
scott  d  w          multivariate density estimation  wiley series probability
mathematical statistics  john wiley   sons 
shekhar  s   zhang  p   huang  y     vatsavai  r  r         kargupta  h   joshi  a  
sivakumar  k     yesha  y   eds    trends spatial data mining  chap      pp 
        aaai press   mit press 
spirtes  p   glymour  c     scheines  r          causation  prediction  search   nd
edition   adaptive computation machine learning series  mit press 
srebro  n     karger  d          learning markov networks  maximum bounded tree width
graphs  acm siam symposium discrete algorithms 
tsamardinos  i   aliferis  c  f     statnikov  a       a   algorithms large scale markov
blanket discovery  proceedings   th international flairs conference  pp 
       
tsamardinos  i   aliferis  c  f     statnikov  a       b   time sample efficient discovery markov blankets direct causal relations  proceedings  th acm
sigkdd international conference knowledge discovery data mining  pp 
       
tsamardinos  i   brown  l  e     aliferis  c  f          max min hill climbing bayesian
network structure learning algorithm  machine learning           
whittaker  j          graphical models applied multivariate statistics  john wiley  
sons  new york 

   


