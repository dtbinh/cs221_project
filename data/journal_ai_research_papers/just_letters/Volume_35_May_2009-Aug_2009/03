journal artificial intelligence research                  

submitted        published      

optimal value information graphical models
andreas krause

krausea   caltech   edu

california institute technology 
     e california blvd  
pasadena  ca       usa

carlos guestrin

guestrin   cs   cmu   edu

carnegie mellon university 
     forbes ave  
pittsburgh  pa       usa

abstract
many real world decision making tasks require us choose among several expensive observations  sensor network  example  important select subset sensors
expected provide strongest reduction uncertainty  medical decision making tasks  one
needs select tests administer deciding effective treatment 
general practice use heuristic guided procedures selecting observations  paper 
present first efficient optimal algorithms selecting observations class probabilistic
graphical models  example  algorithms allow optimally label hidden variables hidden
markov models  hmms   provide results selecting optimal subset observations 
obtaining optimal conditional observation plan 
furthermore prove surprising result  graphical models tasks  one designs
efficient algorithm chain graphs  hmms  procedure generalized polytree graphical models  prove optimizing value information nppp  hard even
polytrees  follows results computing decision theoretic value information objective functions  commonly used practice   p complete problem even
naive bayes models  a simple special case polytrees  
addition  consider several extensions  using algorithms scheduling observation selection multiple sensors  demonstrate effectiveness approach several
real world datasets  including prototype sensor network deployment energy conservation
buildings 

   introduction
probabilistic reasoning  one choose among several possible expensive observations 
often central issue decide variables observe order effectively increase
expected utility  howard        howard   matheson        mookerjee   mannino       
lindley         medical expert system  example  multiple tests available  test
different cost  turney        heckerman  horvitz    middleton         systems 
thus important decide tests perform order become certain
patients condition  minimum cost  occasionally  cost testing even exceed value
information possible outcome  suggesting discontinue testing 
following running example motivates research empirically evaluated section   
consider temperature monitoring task  wireless temperature sensors distributed across

c
    
ai access foundation  rights reserved 

fik rause   g uestrin

building  task become certain temperature distribution  whilst minimizing
energy expenditure  critically constrained resource  deshpande  guestrin  madden  hellerstein   
hong         fine grained building monitoring required obtain significant energy savings
 singhvi  krause  guestrin  garrett    matthews        
many researchers suggested use myopic  greedy  approaches select observations  scheffer  decomain    wrobel        van der gaag   wessels        dittmer   jensen 
      bayer zubek        kapoor  horvitz    basu         unfortunately  general  heuristic provide performance guarantees  paper  present efficient algorithms 
guarantee optimal nonmyopic value information chain graphical models  example 
algorithms used optimal active labeling hidden states hidden markov models  hmms  baum   petrie         address two settings  subset selection  optimal
subset observations obtained open loop fashion  conditional plans  sequential 
closed loop plan observation strategy depends actual value observed variables  c f   figure     knowledge  first optimal efficient algorithms
observation selection diagnostic planning based value information class graphical models  settings  address filtering smoothing versions  filtering
important online decision making  decisions utilize observations made
past  smoothing arises example structured classification tasks  temporal
dimension data  hence observations taken account  call approach
vo idp algorithms use dynamic programming optimize value information  evaluate vo idp algorithms empirically three real world datasets  show
well suited interactive classification sequential data 
inference problems graphical models  computing marginal distributions
finding probable explanation  solved efficiently chain structured graphs 
solved efficiently polytrees  prove problem selecting best k
observations maximizing decision theoretic value information nppp  complete even
discrete polytree graphical models  giving complexity theoretic classification core artificial
intelligence problem  nppp  complete problems believed significantly harder npcomplete even  p complete problems commonly arising context graphical models 
furthermore prove evaluating decision theoretic value information objective functions
 p complete even case naive bayes models  simple special case polytree graphical
models frequently used practice  c f   domingos   pazzani        
unfortunately  hardness results show that  problem scheduling single sensor optimally solved using algorithms  problem scheduling multiple  correlated
sensors wildly intractable  nevertheless  show vo idp algorithms single sensor
scheduling used approximately optimize multi sensor schedule  demonstrate
effectiveness approach real sensor network testbed building management 
summary  provide following contributions 
present first optimal algorithms nonmyopically computing optimizing value
information chain graphical models 
show optimizing decision theoretic value information nppp  hard discrete
polytree graphical models  computing decision theoretic value information  phard even naive bayes models 

   

fio ptimal value nformation g raphical odels



tmorn  high 

tnoon  high 

yes

teve  high 

figure    example conditional plan 
present several extensions algorithms  e g   tree graphical models
leaves  multiple correlated chains  for multi sensor scheduling  
extensively evaluate algorithms several real world problems  including sensor
scheduling real sensor testbed active labeling bioinformatics natural language processing 

   problem statement
assume state world described collection random variables
xv    x            xn    v index set  example  v could denote set locations  xi
models temperature reading sensor placed location v  subset
   i            ik   v  use notation xa refer random vector xa    xi            xik   
algorithms extend continuous distributions  generally assume variables xv discrete  take bayesian approach  assume prior probability distribution
p  xv   outcomes variables  suppose select subset variables  xa  for
v   observe xa   xa   example  set locations place sensors 
set medical tests decide perform  observing realization variables
xa   xa   compute posterior distribution variables p  xv   xa   xa    based
posterior probability obtain reward r p  xv   xa   xa     example  reward function could depend uncertainty  e g   measured entropy  distribution
p  xv   xa   xa    describe several examples detail below 
general  selecting observation  know ahead time observations
make  instead  distribution possible observations  hence 
interested expected reward  take expectation possible observations 
optimizing selection variables  consider different settings  subset selection  goal pick subset v variables  maximizing
x
  argmax
p  xa   xa  r p  xv   xa   xa    
   


xa

impose constraints set allowed pick  e g   number
variables selected  etc    subset selection setting  commit selection
variables get see realization 
instead  sequentially select one variable other  letting choice depend
observations made past  setting  would find conditional plan

   

fik rause   g uestrin

maximizes
  argmax


x

p  xv  r p  xv   x xv     x xv      

   

xv

hereby  conditional plan select different set variables possible state
world xv   use notation  xv   v refer subset variables selected
conditional plan state xv   xv   figure   presents example conditional plan
temperature monitoring example  define notion conditional planning formally
section     
general setup selecting observations goes back decision analysis literature
notion value information howard        statistical literature notion
bayesian experimental design lindley         paper  refer problems    
    problems optimizing value information 
paper  show complexity solving value information problems depend properties probability distribution p   give first algorithms optimally
solving value information interesting challenging class distributions including hidden markov models  present hardness results showing optimizing value information
wildly intractable  nppp  complete  even probability distributions efficient inference possible  even naive bayes models discrete polytrees  
    optimization criteria
paper  consider class local reward  functions ri   defined
marginal probability distributions variables xi   class computational advantage
local rewards evaluated using probabilistic inference techniques  total reward
sum local rewards 
let subset v  p  xj   xa   xa   denotes marginal distribution variable xj conditioned observations xa   xa   example  temperature monitoring
application  xj models temperature location j v  conditional marginal distribution
p  xj   xj   xa   xa   models conditional distribution temperature location j
observing temperature locations v 
classification purposes  appropriate consider max marginals
p max  xj   xj   xa   xa     max p  xv   xv   xj   xj   xa   xa   
xv

is  xj set value xj   probability probable assignment xv   xv
random variables  including xj simplicity notation  conditioned observations
xa   xa  
local reward rj functional probability distribution p p max xj  
is  rj takes entire distribution variable xj maps reward value  typically 
reward functions chosen certain peaked distributions obtain higher reward 
simplify notation  write
rj  xj   xa     rj  p  xj   xa   xa   
   local reward functions widely used additively independent utility models   c f   keeney   raiffa        

   

fio ptimal value nformation g raphical odels

denote reward variable xj upon observing xa   xa  
x
rj  xj   xa    
p  xa   xa  rj  xj   xa  
xa

refer expected local rewards  expectation taken assignments xa
observations a  important local reward functions include 
residual entropy  set
rj  xj   xa     h xj   xa    

x

p  xj   xa   log  p  xj   xa   

xj

objective optimization problem becomes minimize sum residual entropies  optimizing reward function attempts reduce uncertainty predicting marginals xi  
choose reward function running example measure uncertainty temperature distribution 
p
joint entropy  instead minimizing sum residual entropies h xi    attempt minimize joint entropy entire distribution 
x
h xv    
p  xv   log  p  xv   
xv

note  joint entropy depends full probability distribution p  xv    rather
marginals p  xi    hence local  nevertheless  exploit chain rule joint
entropy h xb   set random variables b               m   c f   cover   thomas        
h xb     h x      h x    x      h x    x    x        h xm   x            xm    
hence  choose local reward functions rj  xj   xa     h xj   x            xj    xa   
optimize non local reward function  the joint entropy  using local reward functions 
decision theoretic value information  concept local reward functions includes
concept decision theoretic value information  notion value information widely
used  c f   howard        lindley        heckerman et al          formalized  e g  
context influence diagrams  howard   matheson        partially observable markov decision processes  pomdps  smallwood   sondik         variable xj   let aj finite set
actions  also  let uj   aj dom xj r utility function mapping action aj
outcome x dom xj real number  maximum expected utility principle states actions
selected maximize expected utility 
x
euj  a   xa   xa    
p  xj   xa  uj  a  xj   
xj

certain xj   economically choose action  idea
captured notion value information  choose local reward function
rj  xj   xa     max euj  a   xa   


   

fik rause   g uestrin

margin structured prediction  consider margin confidence 
rj  xj   xa     p max  xj   xa   p max  xj   xa   

xj   argmax p max  xj   xa   xj   argmax p max  xj   xa   
xj   xj

xj

describes margin likely outcome closest runner up  reward
function useful structured classification purposes  shown section   
weighted mean squared error  variables continuous  might want minimize
mean squared error prediction  choosing
rj  xj   xa     wj var xj   xa   

z
var xj   xa    


p  xj   xa   xj

z

x j p  x j

 

xa  dx j

 
dxj

conditional variance xj given xa   xa   wj weight indicating importance
variable xj  
monitoring critical regions  hotspot sampling   suppose want use sensors detecting fire  generally  want detect  j  whether xj cj   cj dom xj
critical region variable xj   local reward function
rj  xj   xa     p  xj cj   xa  
favors observations maximize probability detecting critical regions 
function optimization  correlated bandits   consider setting collection
random variables xv taking numerical
p values interval  m  m   and  selecting
variables  get reward xi   setting arises want optimize unknown
 random  function  evaluating function expensive  setting  encouraged
evaluate function likely obtain high values  maximize expected
total reward choose local reward function
z
rj  xj   xa     xj p  xj   xa  dxj  
i e   expectation variable xj given observations xa   setting optimizing random
function considered version classical k armed bandit problem correlated
arms  details relationship bandit problems given section   
examples demonstrate generality notion local reward  note examples apply continuous distributions well discrete distributions 

   

fio ptimal value nformation g raphical odels

    cost selecting observations
want capture constraint observations expensive  mean
observation xj associated positive penalty cj effectively decreases reward 
example  might interested trading accuracy sensing energy expenditure  alternatively  possible define budget b selecting observations  one associated
integer cost j   here  want select observations whose sum cost within budget 
costs decrease reward  running example  sensors could powered
solar power  regain certain amount energy per day  allows certain amount
sensing  formulation optimization
penalties budgets 
p problems allows p
simplify notation write c a    ja cj  a    ja j extend c sets 
instead fixed penalties costs per observation  depend state
world  example  medical domain  applying particular diagnostic test bear different
risks health patient  depending patients illness  algorithms develop
adapted accommodate dependencies straight forward manner 
present details conditional planning algorithm section     

   decomposing rewards
section  present key observation allows us develop efficient algorithms
nonmyopically optimizing value information class chain graphical models 
algorithms presented section   
set random variables xv    x            xn   forms chain graphical model  a chain  
xi conditionally independent xv  i  i i    given xi  xi     without loss generality
assume joint distribution specified prior p  x    variable x 
conditional probability distributions p  xi     xi    time series model temperature
measured one sensor example formulated chain graphical model  note
transition probabilities p  xi     xi   allowed depend index  i e   chain models
allowed nonstationary   chain graphical models extensively used machine
learning signal processing 
consider example hidden markov model unrolled n time steps  i e   v partitioned hidden variables  x            xn   emission variables  y            yn    hmms 
yi always observed variables xi form chain  many applications 
discussed section    observe hidden variables xi well  e g   asking
expert  addition observing emission variables  cases  problem selecting
expert labels belongs class chain graphical models addressed paper  since
variables xi form chain conditional observed values emission variables yi   idea
generalized class dynamic bayesian networks separators time
slices size one  separators selected observation  formulation
includes certain conditional random fields  lafferty  mccallum    pereira        form
chains  conditional emission variables  the features  
chain graphical models originating time series additional  specific properties 
system online decision making  observations past present time steps
taken account  observations made future  generally referred
filtering problem  setting  notation p  xi   xa   refer distribution
xi conditional observations xa prior including time i  structured classification
   

fik rause   g uestrin

figure    illustration decomposing rewards idea  reward chain     observing
variables x    x  x  decomposes sum chain     plus reward chain
    plus immediate reward observing xp
  minus cost observing x    hereby
brevity use notation rew a   b    bj a rj  xj   x    x    x    

problems discussed section    general observations made anywhere chain must
taken account  situation usually referred smoothing problem  provide
algorithms filtering smoothing 
describe key insight  allows efficient optimization chains  consider
set observations v  j variable observed  i e   j a  local reward
simply r xj   xa     r xj   xj    consider j
  a  let aj subset
containing closest ancestor  and smoothing problem closest descendant  xj
xa   conditional independence property graphical model implies that  given xaj   xj
independent rest observed variables  i e   p  xj   xa     p  xj   xaj    thus 
follows r xj   xa     r xj   xaj   
observations imply expected reward set observations decomposes
along chain  simplicity notation  add two independent dummy variables x  xn    
r    c        rn     cn     n        let    i          p
  im     il   il    
i      im     n      using notation  total reward r a    j rj  xj   xa  
smoothing case given by 


iv    

x
x
riv  xiv   xiv   civ  
rj  xj   xiv   xiv      
v  

j iv   

filtering settings  simply replace rj  xj   xiv   xiv     rj  xj   xiv    figure   illustrates
decomposition 

   efficient algorithms optimizing value information
section  present algorithms efficiently nonmyopically optimizing value information chain graphical models 
    efficient algorithms optimal subset selection chain models
subset selection problem  want find informative subset variables observe
advance  i e   observations made  running example  would 
deploying sensors  identify k time points expected provide informative
sensor readings according model 

   

fio ptimal value nformation g raphical odels

first  define objective function l subsets v
l a   

n
x

rj  xj   xa   c a  

   

j  

subset selection problem find optimal subset
 

argmax l a 
av  a b

maximizing sum expected local rewards minus penalties  subject constraint
total cost must exceed budget b 
solve optimization problem using dynamic programming algorithm  chain
broken sub chains using insight section    consider sub chain variable xa
xb   define lsm
a b  k  represent expected total reward sub chain xa           xb  
lt
smoothing setting xa xb observed  budget level k  lfa b
 k  represents
expected reward filtering setting xa observed  formally 
lt
 k 
lfa b

 

b 
x

max

a a     b  
j a  
 a k

rj  xj   xa   xa   c a  

filtering version 
lsm
a b  k   

b 
x

max

rj  xj   xa   xa   xb   c a  

a a     b  
j a  
 a k

smoothing version  note cases  l  n    b    maxa  a b l a   equation      i e   computing values la b  k   compute maximum expected total reward
entire chain 
f lt
compute lsm
a b  k  la b  k  using dynamic programming  base case simply 
lt
lfa b
     

b 
x

rj  xj   xa   

j a  

filtering 
b 
x

lsm
a b      

rj  xj   xa   xb   

j a  

smoothing  recursion la b  k  two cases  choose spend
budget  reaching base case  break chain two sub chains  selecting optimal
observation xj     j   b  filtering smoothing


la b  k    max la b     
max
 rj  xj   xj   cj   la j       lj b  k j     
j a j b j k

   

fik rause   g uestrin

input  budget b  rewards rj   costs j penalties cj
output  optimal selection observation times
begin
    b n     compute la b     
k     b
    b n    
sel       la b     
j       b   sel j     rj  xj   xj   cj   la j       lj b  k j   
la b  k    maxj a       b     sel j  
a b  k    argmaxj a       b     sel j  
end
end
      b    n      k    b      
repeat
j    a b  k  
j       j      j  k    k j  
j      
end
algorithm    vo idp algorithm optimal subset selection  for filtering smoothing  
first  may seem recursion consider optimal split budget
two sub chains  however  since subset problem open loop order observations
irrelevant  need consider split points first sub chain receives zero budget 
pseudo code implementation dynamic programming approach  call vo idp
subset selection given algorithm    algorithm fills dynamic programming tables
two loops  inner loop ranging pairs  a  b     b  outer loop increasing k  within
inner loop  computing best reward sub chain b  fills table sel 
sel j  reward could obtained making observation j  sel   
reward observation made 
addition computing optimal rewards la b  k  could achieved sub chain   b
budget k  algorithm stores choices a b  k  realize maximum score  here 
a b  k  index next variable selected sub chain   b budget
k    variable selected  order recover optimal subset budget k 
algorithm   uses quantities a b recover optimal subset tracing maximal values
occurring dynamic programming equations  using induction proof  obtain 
theorem    subset selection   dynamic programming algorithm described computes
optimal subset budget b      n    o n    b evaluations expected local rewards 
note consider different costs variable  would simply choose j  
  variables compute la b  n    note variables xi continuous 
algorithm still applicable integrations inferences necessary computing
expected rewards performed efficiently  case  example  gaussian linear
model  i e   variables xi normally distributed  local reward functions residual
entropies residual variances variable 

   

fio ptimal value nformation g raphical odels

    efficient algorithms optimal conditional planning chain models
conditional plan problem  want compute optimal sequential querying policy  
observe variable  pay penalty  depending values observed past  select next
query  proceeding long budget suffices  objective find plan highest
expected reward  where  possible sequence observations  budget b exceeded 
filtering  select observations future  whereas smoothing case  next
observation anywhere chain  running example  filtering algorithm would
appropriate  sensors would sequentially follow conditional plan  deciding
informative times sense based previous observations  figure   shows example
conditional plan 
      f rom ubset election c onditional p lanning
note contrast subset selection setting considered section      conditional planning  set variables depends state world xv   xv   hence 
state  conditional plan could select different set variables   xv   v  example  consider figure    set possible observations v    morn  noon  eve  
xv    tmorn   tnoon   teve    world state xv    high  low  high   conditional
plan presented figure   would select  xv      morn  eve   whereas 
xv    low  low  high   would select  xv      morn  noon   since conditional plan
function  random  state world  set valued random variable  order optimize
problem      define objective function 
j    

x

p  xv  

xv

n
x



rj  xj   x xv     c  xv     

j  

i e   expected sum local rewards given observations made plan  xv   state xv   xv
minus penalties selected variables  expectation taken respect
distribution p  xv    addition defining value policy j    define cost   
     max   xv    
xv

maximum cost  a   as defined section      set    xv   could selected
policy   state world xv   xv  
based notation  goal find policy
  argmax j      b 


i e   policy maximum value  guaranteed never cost exceeding budget
b  hereby class sequential policies  i e   those  observations chosen
sequentially  based observations previously made  
useful introduce following notation 
j xa   k    max j    xa   xa      k 


   

   recall that  filtering setting  r xj   x xv       r xj   xa     a     t  xv   s t  j   i e  
observations past taken account 

   

fik rause   g uestrin


j    xa   xa    

x

n
x


p  xv   xa   xa  
rj  xj   x xv     c  xv     

xv

j  

hence  j xa   k  best possible reward achieved sequential policy cost
k  observing xa   xa   using notation  goal find optimal plan
reward j   b  
value function j satisfies following recursion  base case considers exhausted
budget 
x
j xa       
rj  xj   xa   c a  
jv

recursion  holds





x
j xa   k    max j xa       max
 
p  xj   xa  j xa   xj   k j  


j
 

   

xj

i e   best one state xa   xa budget k either stop selecting variables 
chose best next variable act optimally thereupon 
note easily allow cost j depend state xj variable xj   case 
would simply replace j j  xj    define j xa   r    whenever r      equivalently 
let penalty c a  depend state replacing c a  c xa   
relationship finite horizon markov decision processes  mdps   note function
j xa   k  defined     analogous concept value function markov decision processes  c f   bellman         finite horizon mdps  value function v  s  k  models maximum expected reward obtainable starting state performing k actions  value
function holds
x
v  s  k    r s  k    max
p  s    s  a v  s    k    


s 

p  s    s  a  probability transiting state s  performing action state s 
r s  k  immediate reward obtained state k steps still left  recursion 
similar eq       exploited value iteration algorithm solving mdps 
conditional planning problem unit observation cost  i e    a     a   could modeled
finite horizon mdp  states correspond observed evidence xa   xa   actions correspond
observing variables  or making observation  transition probabilities given
probability observing particular instantiation selected variable  immediate reward
r s  k      k      r s     expected reward  in value information problem 
observing assignment  i e   r p  xv   s   c s    observations unit cost 
mdp  holds v  xa   k    j xa   k   unfortunately  conditional planning problem  since
state mdp uniquely determined observed evidence xa   xa   state space
exponentially large  hence  existing algorithms solving mdps exactly  such value iteration 
cannot applied solve large value information problems  section        develop
efficient dynamic programming algorithm conditional planning chain graphical models
avoids exponential increase complexity 
   

fio ptimal value nformation g raphical odels

      dynamic p rogramming ptimal c onditional p lanning c hains
propose dynamic programming algorithm obtaining optimal conditional plan
similar subset algorithm presented section      again  utilize decomposition
rewards described section    difference observation selection budget
allocation depend actual values observations  order compute value
function j xa   k  entire chain  compute value functions ja b  xa   k  subchains xa           xb  
base case dynamic programming approach deals zero budget setting 
f lt
ja b
 xa     

 

b 
x

rj  xj   xa   xa   

j a  

filtering 
sm
 xa   xb       
ja b

b 
x

rj  xj   xa   xa   xb   xb   

j a  

smoothing  recursion defines ja b  xa   k   or ja b  xa   xb   k  smoothing   expected
reward problem restricted sub chain xa           xb conditioned values xa   xa
 and xb   xb smoothing   budget limited k  compute quantity 
iterate possible split points j    j   b  observe notable difference
filtering smoothing case  smoothing  must consider possible
splits budget two resulting sub chains  since observation time j might
require us make additional  earlier observation 
x

n
sm
sm
p  xj   xj   xa   xa   xb   xb  
ja b  xa  xb   k    max ja b  xa   xb       max
a j b

rj  xj   xj   cj  xj    

max
 lkj  xj  

xj



sm
ja j
 xa   xj   l 

 

sm
 xj   xb   k
jj b



 
l j  xj   

looking back time possible filtering case  hence recursion simplifies

x
n
f lt
f lt
ja b  xa   k    max ja b  xa      
max
p  xj   xj   xa   xa  
a j b j  xj  k

rj  xj   xj   cj  xj    

xj

f lt
ja j
 xa     

 

f lt
jj b
 xj   k


j  xj   
 

j f lt j sm   optimal reward obtained j  n      b    j   b    j    
algorithm   presents pseudo code implementation smoothing version filtering case
straight forward modification  call algorithm   vo idp algorithm conditional planning  algorithm fill dynamic programming tables using three loops  inner loop
ranging assignments xa   xb   middle loop ranging pairs  a  b    b 
outer loop covers increasing values k b  within innermost loop  algorithm
computes table sel sel j  optimal reward achievable selecting variable j next 
   

fik rause   g uestrin

value expectation possible observation variable xj make  note
every possible instantiation xj   xj different allocation remaining budget k j  xj  
left right sub chain  a   j j   b respectively  chosen  quantity  j  xj  
tracks optimal budget allocation 
input  budget b  rewards rj   costs j penalties cj
output  optimal conditional plan  a b   a b  
begin
sm  x   x      
    b n      xa dom xa   xb dom xb compute ja b
b
k     b
    b n    xa dom xa   xb dom xb
sm     
sel       ja b
  j   b
sel j       
xj dom xj
  l k j  xj  
sm  x   x   l    j sm  x   x   k l  x    
bd l     ja j
j
j j
j
b
j b
end
sel j     sel j    p  xj   xa   xb    rj  xj   xj   cj  xj     maxl bd j   
 j  xj     argmaxl bd j  
end
end
sm  k    max
ja b
j a       b     sel j  
a b  xa   xb   k    argmaxj a       b     sel j  
xj dom xa b  k  a b  xa   xb   xj   k     a b  k   xj   
end
end
end
algorithm    vo idp algorithm computating optimal conditional plan  for smoothing
setting  
input  budget k  observations xa   xa   xb   xb    
begin
j    a b  xa   xb   k  
j  
observe xj   xj  
l    a b  xa   xb   xj   k  
recurse k    l  xa   xa xj   xj instead xb   xb  
recurse k    k l j   xj   xj instead xa   xa   xb   xb  
end
end
algorithm    observation selection using conditional planning 
plan compactly encoded quantities a b a b   hereby  a b  xa   xb   k 
determines next variable query observing xa   xa xb   xb   remaining budget k  a b  xa   xb   xj   k  determines allocation budget new observation
xj   xj made  considering exponential number possible sequences observations 
   

fio ptimal value nformation g raphical odels

remarkable optimal plan represented using polynomial space  algorithm  
indicates computed plan executed  procedure recursive  requiring parameters       xa       b    n      xb      k    b initial call  temperature
monitoring example  could first collect temperature timeseries training data 
learn chain model data  offline  would compute conditional plan  for
filtering setting   encode quantities a b a b   would deploy computed
plan actual sensor node  together implementation algorithm    computation optimal plan  algorithm    fairly computationally expensive  execution plan
 algorithm    efficient  selecting next timestep observation requires single lookup
a b a b tables  hence well suited deployment small  embedded device 
summarize analysis following theorem 
theorem    conditional planning   algorithm smoothing presented computes
optimal conditional plan d  b        n    o n     evaluations local rewards 
maximum domain size random variables x            xn   filtering case  optimal plan computed using d  b      n    o n     evaluations  or  budget used 
d       n    o n     evaluations 
faster computation filtering   no budget case obtained observing
require third maximum computation  distributes budget sub chains 
also  note contrary algorithm computing optimal subsets section      algorithm   requires evaluations form r xj   xa   xa    general computed
d  times faster expectations r xj   xa    consideration  subset selection
algorithm general factor b faster  even though conditional planning algorithm
nested loops 
    efficient algorithms trees leaves
sections         presented dynamic programming based algorithms optimize value information chain graphical models  fact  key observations section  
local rewards decompose along chains holds chain graphical models  trees 
formally  tree graphical model joint probability distribution p  xv   collection
random variables xv p  xv   factors
p  xv    

 
i j  xi   xj   
z
 i j e

i j nonnegative potential function  mapping assignments xi xj nonnegative
real numbers  e v v set edges form undirected tree index set v  z
normalization constant enforcing valid probability distribution 
dynamic programming algorithms presented previous sections extended
tree models straightforward manner  instead identifying optimal subsets conditional
plans sub chains  algorithms would select optimal subsets plans sub trees
increasing size  note however number sub trees grow exponentially number
leaves tree  star n leaves example number subtrees exponential
n  fact  counting number subtrees arbitrary tree n vertices believed
intractable   p complete  goldberg   jerrum         however  trees contain
   

fik rause   g uestrin

small  constant  number leaves  number subtrees polynomial  optimal subset
conditional plans computed polynomial time 

   theoretical limits
many problems solved efficiently discrete chain graphical models efficiently solved discrete polytrees    examples include probabilistic inference probable explanation  mpe  
section     however seen complexity dynamic programming algorithms chains increases dramatically extended trees  complexity increases exponentially number leafs tree 
prove that  perhaps surprisingly  problem optimizing value information 
exponential increase complexity cannot avoided  reasonable complexity theoretic assumptions  making statement formal  briefly review complexity classes
used results 
    brief review relevant computational complexity classes
briefly review complexity classes used following statements presenting complete
problem class  details see  e g   references papadimitriou       
littman  goldsmith  mundhenk         class np contains decision problems
polynomial time verifiable proofs  well known complete problem  sat
instances boolean formulas conjunctive normal form containing three literals per
clause   cnf form   complexity class  p contains counting problems  complete problem
class  p   sat counts number satisfying instances  cnf formula 
pp decision version class  p  complete problem ajsat   decides
whether given  cnf formula satisfied majority  i e   half
possible assignments  b turing machine based complexity classes  ab
complexity class derived allowing turing machines deciding instances oracle calls
turing machines b  intuitively think problems class ab
solved turing machine class a  special command solves problem b 
pp similar  p ppp   p p   i e   allow deterministic polynomial time turing
machine access counting oracle  cannot solve complex problems give
access majority oracle  combining ideas  class nppp class problems
solved nondeterministic polynomial time turing machines access majority
 or counting  oracle  complete problem nppp em ajsat which  given  cnf
variables x            x n   decides whether exists assignment x            xn
satisfied majority assignments xn             x n   nppp introduced
found natural class modeling ai planning problems seminal work littman et al 
        example  map assignment problem nppp  complete general graphical
models  shown park darwiche        
complexity classes satisfy following set inclusions  where inclusions assumed 
known strict  
p np pp ppp   p p nppp  
   polytrees bayesian networks form trees edge directions dropped 

   

fio ptimal value nformation g raphical odels

    complexity computing optimizing value information
order solve optimization problems  likely evaluate objective
function  i e   expected local rewards  first result states that  even specialize decision theoretic value information objective functions defined section      problem
intractable even naive bayes models  special case discrete polytrees  naive bayes models
often used classification tasks  c f   domingos   pazzani         class variable
predicted noisy observations  features   assumed conditionally independent given
class variable  sense  naive bayes models next simplest  from perspective
inference  class bayesian networks chains  note naive bayes models correspond
stars referred section      number subtrees exponential number
variables 
theorem    hardness computation naive bayes models   computation decision
theoretic value information functions  p complete even naive bayes models 
hard approximate factor unless p   np 
immediate corollary subset selection problem pp hard naive bayes
models 
corollary    hardness subset selection naive bayes models   problem determining 
given naive bayes model  constants c b  cost function set decision theoretic value
information objective functions ri   whether subset variables v
l a  c  a  b pp hard 
fact  show subset selection arbitrary discrete polytrees  that general
naive bayes models  inference still tractable  even nppp  complete  complexity
class containing problems believed significantly harder np  p complete
problems  result provides complexity theoretic classification value information  core
ai problem 
theorem    hardness subset selection computation polytrees   problem determining  given discrete polytree  constants c b  cost function set decision theoretic
value information objective functions ri   whether subset variables v
l a  c  a  b nppp  complete 
running example  implies generalized problem optimally selecting k sensors
network correlated sensors likely computationally intractable without resorting
heuristics  corollary extends hardness subset selection hardness conditional plans 
corollary    hardness conditional planning computation polytrees   computing conditional plans pp hard naive bayes models nppp  hard discrete polytrees 
proofs results section stated appendix  rely reductions complete
problems np   p nppp involving boolean formulae problems computing   optimizing value information  reductions inspired works littman et al         park
darwiche         require development novel techniques  new reductions
boolean formulae naive bayes polytree graphical models associated appropriate reward
functions  ensuring observation selections lead feasible assignments boolean formulae 
   

fik rause   g uestrin

percent improvement

  

 

optimal conditional plan

 

mean margin optimal subset
mean margin greedy heuristic

 
    

   

mean f  score

    

 

    

   

 
 

   
optimal subset
greedy heuristic

 
 

 

 
  
  
  
number observations

  

 a  sensor scheduling

   

   

mean accuracy
greedy heuristic
 

 
 
 
 
number observations

 b  cpg island detection

mean margin

    

mean accuracy
optimal subset

 

    
    
 

  
  
  
  
number observations

  

 c  part speech tagging

figure    experimental results   a  temperature data  improvement uniform spacing
heuristic   b  cpg island data set  effect increasing number observations
margin classification accuracy   c  part of speech tagging data set  effect increasing number observations margin f  score 

   experiments
section  evaluate algorithms several real world data sets  special focus
comparison optimal methods greedy heuristic heuristic methods selecting observations  algorithms used interactive structured classification 
    temperature time series
first data set consists temperature time series collected sensor network deployed
intel research berkeley  deshpande et al         described running example  data
continuously collected    days  linear interpolation used case missing samples 
temperature measured every    minutes  discretized    bins   degrees
kelvin  avoid overfitting  used pseudo counts       learning model  using
parameter sharing  learned four sets transition probabilities        am         pm 
   pm     pm   pm      am  combining data three adjacent sensors  got    sample
time series 
goal task select k    time points day  sensor
readings informative  experiment designed compare performance
optimal algorithms  greedy heuristic  uniform spacing heuristic  distributed k
observations uniformly day  figure   a  shows relative improvement optimal algorithms greedy heuristic uniform spacing heuristic  performance measured
decrease expected entropy  zero observations baseline  seen k less
half possible observations  optimal algorithms decreased expected uncertainty several percent heuristics  improvement gained optimal plan
subset selection algorithms appears become drastic large number observations
 over half possible observations  allowed  furthermore  large number observations 
optimal subset subset selected greedy heuristic almost identical 

   

fio ptimal value nformation g raphical odels

    cpg island detection
studied bioinformatics problem finding cpg islands dna sequences  cpg islands
regions genome high concentration cytosine guanine sequence  areas
believed mainly located around promoters genes  frequently expressed
cell  experiment  considered gene loci hs   k    af       al       
genbank annotation listed three  two one cpg islands each  ran algorithm
   base window beginning end island  using transition emission
probabilities durbin  eddy  krogh  mitchison        hidden markov model 
used sum margins reward function 
goal experiment locate beginning ending cpg islands
precisely asking experts  whether certain bases belong cpg region not  figure   b  shows mean classification accuracy mean margin scores increasing number
observations  results indicate that  although expected margin scores similar
optimal algorithm greedy heuristic  mean classification performance optimal algorithm still better performance greedy heuristic  example  making  
observations  mean classification error obtained optimal algorithm     lower
error obtained greedy heuristic 
    part of speech tagging
third experiment  investigated structured classification task part of speech  pos 
tagging  conll         problem instances sequences words  sentences   word
part entity  e g   european union   entity belongs one five categories 
location  miscellaneous  organization  person other  imagine application  automatic
information extraction guided expert  algorithms compute optimal conditional plan
asking expert  trying optimize classification performance requiring little expert
interaction possible 
used conditional random field structured classification task  node corresponds word  joint distribution described node potentials edge potentials 
sum margins used reward function  measure classification performance f 
score  geometric mean precision recall  goal experiment analyze
addition expert labels increases classification performance  indirect  decomposing reward function used algorithms corresponds real world classification performance 
figure   c  shows increase mean expected margin f  score increasing number observations  summarized ten    word sequences  seen classification
performance effectively enhanced optimally incorporating expert labels  requesting
three    labels increased mean f  score five percent  following
example illustrates effect  one scenario words entity  sportsman p  simmons 
classified incorrectly p  simmons miscellaneous  first request
optimal conditional plan label simmons  upon labeling word correctly  word p 
automatically labeled correctly also  resulting f  score     percent 

   

fik rause   g uestrin

   applying chain algorithms general graphical models
section   seen algorithms used schedule single sensor  assuming time
series sensor readings  e g   temperature  form markov chain  natural assumption
sensor networks  deshpande et al          deploying sensor networks however  multiple
sensors need scheduled  time series sensors independent  could use
algorithms schedule sensors independently other  however  practice 
measurements correlated across different sensors fact  dependence essential
allow generalization measurements locations sensor placed  following  describe approach using single sensor scheduling algorithm coordinate
multiple sensors 
formally  interested monitoring spatiotemporal phenomenon set locations               m   time steps                  locationtime pair s  t 
associate random variable xs t describes state phenomenon location
time  random vector xs t fully describes relevant state world vector xs t
describes state particular time step t  before  make markov assumption  assuming
conditional independence xs t xs t  given xs t  t      
similarly single chain case  consider reward functions rs t associated
variable xs t   goal select  timestep  set sensors activate 
order maximize sum expected rewards  letting a  t   a    expected total
reward given
x
rs t  xs t   xa  t  
s t

filtering setting  i e   observations past taken account evaluating
rewards  
x
rs t  xs t   xa  t  
s t

smoothing setting  where observations taken account   generalization
conditional planning done described section   
note case single sensor          problem optimal sensor scheduling
solved using algorithm    unfortunately  optimization problem wildly intractable even
case two sensors        
corollary    hardness sensor selection two chains   given model two dependent
chains  constants c b  cost function set decision theoretic value information
functions rs t   nppp  complete determine whether subset a  t variables
l a  t   c  a  t   b 
following  develop approximate algorithm uses optimal single chain algorithms performs well practice 
    approximate sensor scheduling lower bound maximization
reason sudden increase complexity case multiple chains decomposition rewards along sub chains  as described section    extend case multiple

   

fio ptimal value nformation g raphical odels

s    

s    

s    

s    

s    

s    

s    

s    

s    

s    

figure    scheduling multiple correlated sensors dynamic processes 
sensors  since influence flow across chains  figure   visualizes problem there  distri   
   
   
bution sensor     depends three observations s  s  sensor     s 
sensor     
address complexity issue using  approximate  extension decomposition approach used single chains  focus decision theoretic value information objective  as described section       local reward functions  residual entropy 
used well 
considering recent observations  first approximation  allow sensor take
account recent observations  intuitively  appears reasonable approximation 
especially potential scheduling times reasonably far apart  formally  evaluating
local rewards time t  replace set observations time t  a  t subset
a   t a  t


a   t    s  t  a  t   t   s  t    a  t  
i e  sensor s  last observation  with largest time index t  kept 
approximate rs t  xs t   a  t   rs t  xs t   a   t    figure   example  a     
  s         s         s         total expected utility time t  would computed using observations a         s         s         i e   using time t  sensor one  time t  sensor two 
   
ignoring influence originating observation s  flowing chains indicated
dashed arrow  following proposition proves approximation lower bound
true value information 
proposition    monotonicity value information   decision theoretic value information rs t  a  set sensors monotonic a 
rs t  a    rs t  a 
a  a 
proposition   proves conditioning recent observations decrease
objective function  hence maximizing approximate objective implies maximizing lower bound
true objective 
coordinate ascent approach  propose following heuristic maximizing lower
bound expected utility  instead jointly optimizing schedules  timesteps selected
sensor   algorithm repeatedly iterate sensors  sensors s 
optimize selected observations as  t   holding schedules sensors fixed 
   

fik rause   g uestrin

procedure resembles coordinate ascent approach  coordinate ranges possible
schedules fixed sensor s 
optimizing sensor s  algorithm finds schedule as  t


 
x
as  t   argmax
rs t xs t   xa   t
xa s   as  t   b 
   
a  t

s    s

s t

  t

i e   maximizes  schedules a  t   sum expected rewards time steps
 
sensors  given schedules as  t non selected sensors s   
solving single chain optimization problem  order solve maximization problem
    individual sensors  use dynamic programming approach introduced
lt
section    recursive case lfa b
 k  k     exactly same  however  base case
computed
b  x


x
 
f lt
la b      
rs j xs j   xa
xa s   
s    s

j a  

  j

i e   takes account recent observation non selected sensors s   
lt
     first all 
several remarks need made computation base case lfa b
naive implementation  computation expected utility


 
rs j xs j   xa
xa s 
s    s

  j

requires time exponential number chains  case since  order compute
reward rs t   chain  possible observations xa s
  xa s
could made need
  t
  t
taken account  computation requires computing expectation joint distribution
p  xa   t    exponential size  increase complexity avoided using sampling
approximation  hoeffdings inequality used derive polynomial bounds sample complexity approximating value information arbitrarily small additive error   similarly
done approach krause guestrin      a     practice  small number samples
appears provide reasonable performance  secondly  inference becomes intractable
increasing number sensors  approximate inference algorithms algorithm proposed
boyen koller        provide viable way around problem 
analysis  since sensors maximize global objective l a  t    coordinated ascent
approach guaranteed monotonically increase global objective every iteration  ignoring
possible errors due sampling approximate inference   hence must converge  to local
optimum  finite number steps  procedure formalized algorithm   
although cannot general provide performance guarantees procedure  building algorithm provides optimal schedule sensor isolation 
benefit observations provided remaining sensors  also  note sensors
independent  algorithm   obtain optimal solution  even sensors correlated 
obtained solution least good solution obtained scheduling sensors independently other  algorithm   always converge  always compute lower bound
   absolute error evaluating reward rs t accumulate total error  t   s 
variables hence error optimal schedule 

   

fio ptimal value nformation g raphical odels

input  budget b
output  selection a            a  observation times sensor
begin
select ai       random 
repeat
     
use algorithm   select observations ai sensor i  conditioning current
sensor scheduling aj   j    i  remaining sensors 
end
compute improvement total expected utility 
small enough  
end
algorithm    multi sensor scheduling 
expected total utility  considering intractability general problem even two chains
 c f     corollary     properties reassuring  experiments  coordinated sensor
scheduling performed well  discussed section     
    proof concept study real deployment
work singhvi et al          presented approach optimizing light control
buildings  purpose satisfying building occupants preferences lighting conditions 
simultaneously minimizing energy consumption  approach  wireless sensor network
deployed monitors building environmental conditions  such sunlight intensity
etc    sensors feed measurements building controller actuates lighting system
 lamps  blinds  etc   accordingly  every timestep   building controller choose
action affects lighting conditions locations building  utility functions
ut  a  xs t   specified map chosen actions current lighting levels utility
value  utility chosen capture users preferences light levels  well
energy consumption lighting system  details utility functions described detail
singhvi et al  
evaluated multi sensor scheduling approach real building controller testbed 
described detail singhvi et al   experiments  used algorithm   schedule three
sensors  allowing sensor choose subset ten time steps  in one hour intervals
daytime   varied number timesteps sensor activated  computed
total energy consumption total user utility  as defined singhvi et al    figure   a  shows
mean user utility energy savings achieved  number observations varying
observations continuous sensing     observations discretization     results imply
using predictive model active sensing strategy  even small number observations
achieves results approximately good results achieved continuous sensing 
figure   b  presents mean total utility achieved using observations  one observation ten
observations per sensor day  seen even single observation per sensor increases
total utility close level achieved continuous sensing  figure   c  shows mean energy
   note figure   a   energy cost utility plotted different units directly compared 

   

fik rause   g uestrin

 

  
energy cost

  

  observ  
sensor

  

   observ  
sensor
energy cost

 

total utility

user utility energy cost

  

 
observ 

 

 

observ 

  

 

 

  observ  
sensor

measured user utility
 

 

 

   
number observations

  

 

 a  sensing scheduling evaluation

  

  
  
hour day

  

  

 

 b  total utility

  

   observ  
sensor

  
  
hour day

  

  

 c  energy cost

figure    active sensing results 
consumption required experiment  here  single sensor observation strategy comes
even closer power savings achieved continuous sensing 
since sensor network battery lifetime general inversely proportional amount
power expended sensing communication  conclude sensor scheduling strategy
promises lead drastic increases sensor network lifetime  deployment permanence reduced maintenance cost  testbed  network lifetime could increased factor  
without significant reduction user utility increase energy cost 

   related work
section  review related work number different areas 
    optimal experimental design
optimal experimental design general methodology selecting informative experiments infer
aspects state world  such parameters particular nonlinear function 
etc    large literature different approaches experimental design  c f   chaloner  
verdinelli        krause  singh    guestrin        
bayesian experimental design  prior distribution possible states world assumed  experiments chosen  e g   reduce uncertainty posterior distribution 
general form  bayesian experimental design pioneered lindley         users encode
preferences utility function u  p          first argument  p     distribution
states world  i e   parameters  second argument      true state
world  observations xa collected  change expected utility prior p   
posterior p     xa   xa   used design criterion  sense  value observation problems considered paper considered instances bayesian experimental design
problems  typically  bayesian experimental design employed continuous distributions  often
multivariate normal distribution  choosing different utility functions  different notions
optimality defined  including a  d  optimality developed  chaloner   verdinelli 
       posterior covariance matrix  a   whose
maximum

eigenvalue max  
bayesian a   d   e  optimality minimizes tr  a   det  a   max  a   respectively  terminology section      d optimality corresponds choosing total entropy 
a optimality corresponds  weighted  mean squared error criteria 

   

fio ptimal value nformation g raphical odels

even multivariate normal distributions  optimal bayesian experimental design np hard
 ko  lee    queyranne         applications experimental design  number experiments selected often large compared number design choices  cases  one
find fractional design  i e   non integral solution defining proportions experiments
performed   round fractional solutions  fractional formulation  a   d  
e optimality criteria solved exactly using semi definite program  boyd   vandenberghe 
       however known bounds integrality gap  i e   loss incurred
rounding process 
algorithms presented section     used optimally solve non fractional bayesian
experimental design problems chain graphical models  even continuous distributions 
long inference distributions tractable  such normal distributions   paper hence
provides new class combinatorial algorithms interesting class bayesian experimental
design problems 
    value information graphical models
decision theoretic value information frequently used principled information gathering  c f   howard        lindley        heckerman et al          popularized decision
analysis context influence diagrams  howard   matheson         sense  value
information problems special cases bayesian experimental design problems  prior
distribution particular structure  typically given graphical model considered
paper 
several researchers  scheffer et al         van der gaag   wessels        dittmer   jensen 
      kapoor et al         suggested myopic  i e   greedy approaches selectively gathering
evidence graphical models  considered paper  which  unlike algorithms presented
paper  algorithms applicable much general graphical models 
theoretical guarantees  heckerman et al         propose method compute
maximum expected utility specific sets observations  work considers general
graphical models paper  naive bayes models certain extensions   provide
large sample guarantees evaluation given sequence observations  use heuristic
without guarantees select sequences  bilgic getoor        present branch bound
approach towards exactly optimizing value information complex probabilistic models 
contrast algorithms described paper however  approach running time
worst case exponential  munie shoham        present algorithms hardness results
optimizing special class value information objective functions motivated optimal
educational testing problems  algorithms apply different class graphical models
chains  apply specific objective functions  rather general local reward functions
considered paper  radovilsky  shattah  shimony        extended previous version
paper  krause   guestrin      a  obtain approximation algorithms guarantees
case noisy observations  i e   selecting subset emission variables observe  rather
selecting among hidden variables considered paper  
    bandit problems exploration   exploitation
important class sequential value information problems class bandit problems 
classical k armed bandit problem  formalized robbins         slot machine given
   

fik rause   g uestrin

k arms  draw arm results reward success probability pi fixed
arm  different  and independent  across arm  selecting arms pull  important
problem trade exploration  i e   estimation success probabilities arms 
exploitation  i e   repeatedly pulling best arm known far   celebrated result gittins
jones        shows fixed number draws  optimal strategy computed
polynomial time  using dynamic programming based algorithm  similar sense
optimal sequential strategy computed polynomial time  gittins algorithm however
different structure dynamic programming algorithms presented paper 
note using function optimization objective function described section     
approach used solve particular instance bandit problems  arms
required independent  but  contrary classical notion bandit problems 
chosen repeatedly 
    probabilistic planning
optimized information gathering extensively studied planning community 
bayer zubek        example proposed heuristic method based markov decision process framework  however  approach makes approximations without theoretical guarantees 
problem optimizing decision theoretic value information naturally formalized
 finite horizon  partially observable markov decision process  pomdp  smallwood   sondik 
       hence  principle  algorithms planning pomdps  anytime algorithm
pineau  gordon  thrun         employed optimizing value information  unfortunately  state space grows exponentially number variables considered
selection problem  addition  complexity planning pomdps grows exponentially
cardinality state space  hence doubly exponentially number variables selection  steep increase complexity makes application black box pomdp solvers infeasible 
recently  ji  parr  carin        demonstrated use pomdp planning multi sensor
scheduling problem  presenting promising empirical results  approach however uses
approximate pomdp planning techniques without theoretical guarantees 
robotics literature  stachniss  grisetti  burgard         sim roy       
kollar roy        presented approaches information gathering context simultaneous localization mapping  slam   none approaches however provide guarantees
quality obtained solutions  singh  krause  guestrin  kaiser  batalin       
present approximation algorithm theoretical guarantees problem planning informative path environmental monitoring using gaussian process models  contrast
algorithms presented paper  dealing complex probabilistic models
complex cost functions arising path planning  approach requires submodular objective
functions  a property hold value information show proposition    
    sensor selection scheduling
context wireless sensor networks  sensor nodes limited battery hence
enable small number measurements  optimizing value information selected
sensors plays key role  problem deciding selectively turn sensors order
conserve power first discussed slijepcevic potkonjak        zhao  shin  reich
        typically  assumed sensors associated fixed sensing region  spatial
   

fio ptimal value nformation g raphical odels

domain needs covered regions associated selected sensors  abrams  goel 
plotkin        present efficient approximation algorithm theoretical guarantees
problem  deshpande  khuller  malekian  toossi        present approach problem
based semidefinite programming  sdp   handling general constraints providing tighter
approximations  approaches described apply problem optimizing sensor schedules complex utility functions as  e g   increase prediction accuracy
objectives considered paper  address shortcomings  koushanfary  taft 
potkonjak        developed approach sensor scheduling guarantees specified prediction accuracy based regression model  however  approach relies solution
mixed integer program  intractable general  zhao et al         proposed heuristics
selectively querying nodes sensor network order reduce entropy prediction  unlike algorithms presented paper  approaches performance
guarantees 
    relationship machine learning
decision trees  quinlan        popularized value information criterion creating
conditional plans  unfortunately  guarantees performance greedy method 
subset selection problem instance feature selection central issue machine
learning  vast amount literature  see molina  belanche    nebot       survey  
however  aware work providing similarly strong performance guarantees
algorithms considered paper 
problem choosing observations strong connection field active learning
 c f   cohn  gharamani    jordan        tong   koller        learning system designs
experiments based observations  sample complexity bounds derived
active learning problems  c f   dasgupta        balcan  beygelzimer    langford        
aware active learning algorithms perform provably optimal  even restricted
classes problem instances  
    previous work authors
previous version paper appeared work krause guestrin      b  
contents section   appeared part work singhvi et al          present version
much extended  new algorithmic hardness results detailed discussions 
light negative results presented section    cannot expect able optimize value information complex models chains  however  instead attempting
solve optimal solution  one might wonder whether possible obtain good approximations  authors showed  krause   guestrin      a  krause et al         krause  leskovec 
guestrin  vanbriesen    faloutsos        large number practical objective functions satisfy intuitive diminishing returns property  adding new observation helps
observations far  less already made many observations  intuition formalized using combinatorial concept called submodularity  fundamental result nemhauser
et al  proves optimizing submodular utility function  myopic greedy algorithm
fact provides near optimal solution  within constant factor     e      optimal 
unfortunately  decision theoretic value information satisfy submodularity 

   

fik rause   g uestrin

proposition    non submodularity value information   decision theoretic value information submodular  even naive bayes models 
intuitively  value information non submodular  need make several observations
order convince need change action 

   conclusions
described novel efficient algorithms optimal subset selection conditional plan computation chain graphical models  and trees leaves   including hmms  empirical
evaluation indicates algorithms improve upon commonly used heuristics decreasing expected uncertainty  algorithms effectively enhance performance interactive
structured classification tasks 
unfortunately  optimization problems become wildly intractable even slight generalization chains  presented surprising theoretical limits  indicate even class
decision theoretic value information functions  as widely used  e g   influence diagrams
pomdps  cannot efficiently computed even naive bayes models  identified optimization value information new class problems intractable  nppp  complete 
polytrees 
hardness results  along recent results polytree graphical models  npcompleteness maximum posteriori assignment  park   darwiche        np hardness
inference conditional linear gaussian models  lerner   parr         suggest possibility
developing generalized complexity characterization problems hard polytree graphical
models 
light theoretical limits computing optimal solutions  natural question ask
whether approximation algorithms non trivial performance guarantees found  recent
results krause guestrin      a   radovilsky et al         krause et al         show
case interesting classes value information problems 

acknowledgments
would thank ben taskar providing part of speech tagging model  reuters
making news archive available  would thank brigham anderson andrew moore helpful comments discussions  work partially supported nsf
grants no  cns          cns          aro muri w   nf        gift intel 
carlos guestrin partly supported alfred p  sloan fellowship  ibm faculty fellowship onr young investigator award n                             andreas krause
partially supported microsoft research graduate fellowship 

appendix
proof theorem    membership  p arbitrary discrete polytrees straightforward since
inference models p  let instance   sat   count
number assignments x            xn satisfying   let c    c            cm   set
clauses  create bayesian network  n     variables  x            xn   u            un y 
xi conditionally independent given y  let uniformly distributed values
   

fio ptimal value nformation g raphical odels


u 

u 

un


x 

x 

xn

figure    graphical model used proof theorem   
 n   n                              m   ui bernoulli prior p        let
observed variables xi cpts defined following way 

   xi   u satisfies clause cj  
xi    y    j  ui   u 
   otherwise 

     j 
xi    y   j  ui   u 
u  otherwise 
model  presented figure    holds x    x      xn     iff u            un
encode satisfying assignment        hence  observe x    x      xn     
know     certainty  furthermore  least one xi      know
p  y       x   x       let nodes zero reward  except y  assigned
reward function following properties  we show model local
reward function using decision theoretic value information  
 n m  n
  p  y       xa   xa       

r y   xa   xa    
  
otherwise 
argument  expected reward
x
r y   x            xn    
p  y   y p  u   u p  x  u r y   x   x 
u y x

 

x

p  y     p  u 

u sat

x
 n   m  n
 
 

u sat

exactly number satisfying assignments   note model defined yet
naive bayes model  however  easily turned one marginalizing u 
show realize reward function properties maximum expected utility sense  let    d    d    set two decisions  define utility function
property 

 n m  n

 
  d      


 n m   n  
u y  d   
    d      

   n
otherwise 
reward r y   xa   given decision theoretic value information 
x
x
r y   xa    
p  xa   max
p  y   xa  u y  d  
xa



   



fik rause   g uestrin

figure    graphical model used proof theorem   
utility function u based following consideration  upon observing particular instantiation variables x            xn make decision variable y  goal achieve
number times action d  chosen exactly corresponds number satisfying assignments   accomplished following way  xi    know ui
encoded satisfying assignment      probability    case  action d  chosen 
need make sure whenever least one xi      which indicates either    
u satisfying assignment  decision d  chosen  now  least one xi      either
  j     clause j satisfied       utilities designed unless
n
p  y       xa   xa     n  m   action d  gives higher expected reward    hereby 
n n
 m lower bound probability misclassification p  y       xa   xa   
note construction immediately proves hardness approximation  suppose
polynomial time algorithm computes approximation r within
factor      which depend problem instance  r   r y   x            xn    r    
implies r      r     implies r      hence  approximation r used
decide whether satisfiable not  implying p   np 
proof corollary    let  cnf formula  convert naive bayes model variables x            xn construction theorem    function l v 
v               n  set variables xi counts number satisfying assignments  
note function l a  v               n  monotonic  i e   l a  l v 
v  shown proposition    hence majority assignments satisfies
l v     n   
proof theorem    membership follows fact inference polytrees p discrete polytrees  nondeterministic turing machine  p oracle first guess selection
variables  compute value information using theorem    since computation
 p complete arbitrary discrete polytrees   compare constant c 
show hardness  let instance em ajsat   find instantiation
x            xn  x            x n   true majority assignments xn             x n  
let c    c            cm   set  cnf clauses  create bayesian network shown figure   
nodes ui   uniform bernoulli prior  add bivariate variables yi    seli   pari   
   n  seli takes values             m  pari parity bit  cpts yi

   

fio ptimal value nformation g raphical odels

defined as  sel  uniformly varies             m   par       y            y n  

   j      ui satisfies cj  
seli    seli    j  ui   ui  
j  otherwise 
pari    pari    bi    ui   bi  ui  
denotes parity  xor  operator  add variables zit zif   n
let

uniform          ui     

zi    ui   ui  
  
otherwise 
uniform denotes uniform distribution  similarly  let

uniform          ui     
zif    ui   ui  
  
otherwise 
intuitively  zit     guarantees us ui      whereas zit     leaves us uncertain ui  
case zif symmetric 
use subset selection algorithm choose zi encode solution em ajsat  
zit chosen  indicate xi set true  similarly zif indicates false assignment
xi   parity function going used ensure exactly one  zit   zif   observed
i 
first assign penalties nodes except zit   zif   n  uj
n     j  n  assigned zero penalty  let nodes zero reward  except
y n   assigned following reward 
n
    p  sel n       xa   xa      
 p  par n       xa   xa       p  par n       xa   xa        
r y n   xa   xa    

  
otherwise 
note sel n     probability   iff u            u n encode satisfying assignment   furthermore  get positive reward certain sel n      i e   chosen observation
set must contain proof satisfied  certain par n   parity certainty
occur certain assignment u            u n   possible infer
value ui certainty observing one ui   zit zif   since               n  cost
observing ui   receive reward must observe least one zit zif   assume
compute optimal subset budget  n  receive positive reward
observing exactly one zit zif  
interpret selection zit zif assignment first n variables em ajsat  
let r   r y n   o   claim em ajsat r        first let
em ajsat   assignment x            xn first n variables  add un             u n
add zit iff xi     zif iff xi      selection guarantees r       
assume r        call assignment u            u n consistent   n 
zit o  ui     zif ui      consistent assignment  chance
observations zi prove consistency  n   hence r       implies majority
provably consistent assignments satisfy hence em ajsat   proves subset
selection nppp complete 
note realize local reward function r sense maximum expected utility
similarly described proof theorem   
   

fik rause   g uestrin

proof corollary    constructions proof theorem   theorem   prove
computing conditional plans pp hard nppp  hard respectively  since  instances 
plan positive reward must observe variables corresponding valid instantiations  i e  
x            xn corollary    un             u n one z            zn satisfy
parity condition theorem     cases  order selection irrelevant  and  hence 
conditional plan effectively performs subset selection 
proof corollary    proof follows observation polytree construction
proof theorem   arranged two dependent chains  transformation  revert
arc zit ui applying bayes rule  make sure number
nodes sensor timeslice  triple variables yi   calling copies yi  yi    
conditional probability tables given equality constraints  yi    yi yi     yi   
transformation  variables associated timesteps  i    for    given sets
     z    timesteps  i   associated sets  u      timesteps  i associated
 yi 



 zif   yi    
proof proposition    bound follows fact maximization convex 
application jensens inequality  using induction argument  simply need show
l a  l   
 
x
x
l a   
p  xa   xa  
max eu  a  t  x   xa  t   xa  t  
xa



tv

 


x
tv

 

x
tv

max


x

p  xa   xa  eu  a  t  x   xa  t   xa  t  

xa

max eu  a  t  x    l  



eu  a  t  x   xa  t   xa  t    

x

p  xt   xa  t   xa  t  ut  a  xt  

xt

expected utility action time observing xa  t   xa  t  
proof proposition    consider following binary classification problem assymetric cost 
one bernoulli random variable  the class label  p  y           
p  y             two noisy observations x    x    conditionally independent given y  let p  xi   y         i e   observations agree class label
probability      disagree probability      three actions  a   classifying    
a   classifying     a   not assigning label   define utility functon u
gain utility   assign label correctly  u  a         u  a               misassign
label  u  a         u  a               choose a    i e   assign label  now 
 
 
 
    
verify l     l  x       l  x          l  x    x                   
hence  adding x  x  increases utility adding x  empty set  contradicting
submodularity 

   

fio ptimal value nformation g raphical odels

references
abrams  z   goel  a     plotkin  s          set k cover algorithms energy efficient monitoring
wireless sensor networks   ipsn 
balcan  n   beygelzimer  a     langford  j          agnostic active learning  icml 
baum  l  e     petrie  t          statistical inference probabilistic functions finite state
markov chains  ann  math  stat               
bayer zubek  v          learning diagnostic policies examples systematic search  uai 
bellman  r          markovian decision process  journal mathematics mechanics    
bilgic  m     getoor  l          voila  efficient feature value acquisition classification 
twenty second conference artificial intelligence  aaai  
boyd  s     vandenberghe  l          convex optimization  cambridge up 
boyen  x     koller  d          tractable inference complex stochastic processes  uncertainty artificial intelligence  uai  
chaloner  k     verdinelli  i          bayesian experimental design  review  statistical science 
              
cohn  d  a   gharamani  z     jordan  m  i          active learning statistical models  j ai
research            
conll        
conference computational natural language learning shared task 
http   cnts uia ac be conll     ner  
cover  t  m     thomas  j  a          elements information theory  wiley interscience 
dasgupta  s          coarse sample complexity bounds active learning  nips 
deshpande  a   guestrin  c   madden  s   hellerstein  j     hong  w          model driven data
acquisition sensor networks  vldb 
deshpande  a   khuller  s   malekian  a     toossi  m          energy efficient monitoring
sensor networks  latin 
dittmer  s     jensen  f          myopic value information influence diagrams  uai  pp 
        san francisco 
domingos  p     pazzani  m          optimality simple bayesian classifier
zero one loss  machine learning             
durbin  r   eddy  s  r   krogh  a     mitchison  g          biological sequence analysis   probabilistic models proteins nucleic acids  cambridge university press 
gittins  j  c     jones  d  m          dynamic allocation index discounted multiarmed
bandit problem  biometrika                
goldberg  l  a     jerrum  m          counting unlabelled subtrees tree  p complete  lms
j comput  math             
heckerman  d   horvitz  e     middleton  b          approximate nonmyopic computation
value information  ieee trans  pattern analysis machine intelligence             

   

fik rause   g uestrin

howard  r  a          information value theory  ieee transactions systems science
cybernetics  ssc    
howard  r  a     matheson  j          readings principles applications decision
analysis ii  chap  influence diagrams  pp          strategic decision group  menlo park 
reprinted      decision analysis              
ji  s   parr  r     carin  l          non myopic multi aspect sensing partially observable
markov decision processes  ieee transactions signal processing                  
kapoor  a   horvitz  e     basu  s          selective supervision  guiding supervised learning
decision theoretic active learning  international joint conference artificial intelligence
 ijcai  
keeney  r  l     raiffa  h          decisions multiple objectives  preferences value
trade offs  wiley 
ko  c   lee  j     queyranne  m          exact algorithm maximum entropy sampling 
operations research                
kollar  t     roy  n          efficient optimization information theoretic exploration slam 
aaai 
koushanfary  f   taft  n     potkonjak  m          sleeping coordination comprehensive sensing
using isotonic regression domatic partitions  infocom 
krause  a     guestrin  c       a   near optimal nonmyopic value information graphical
models  proc  uncertainty artificial intelligence  uai  
krause  a     guestrin  c       b   optimal nonmyopic value information graphical models
  efficient algorithms theoretical limits  proc  ijcai 
krause  a   leskovec  j   guestrin  c   vanbriesen  j     faloutsos  c          efficient sensor
placement optimization securing large water distribution networks  journal water resources planning management         
krause  a   singh  a     guestrin  c          near optimal sensor placements gaussian processes  theory  efficient algorithms empirical studies  jmlr 
lafferty  j   mccallum  a     pereira  f          conditional random fields  probabilistic models
segmenting labeling sequence data  icml 
lerner  u     parr  r          inference hybrid networks  theoretical limits practical algorithms  uai 
lindley  d  v          measure information provided experiment  annals
mathematical statistics              
littman  m   goldsmith  j     mundhenk  m          computational complexity probabilistic
planning  journal artificial intelligence research         
molina  l   belanche  l     nebot  a          feature selection algorithms  survey experimental evaluation  icdm 
mookerjee  v  s     mannino  m  v          sequential decision models expert system optimization  ieee trans  knowl  data eng                

   

fio ptimal value nformation g raphical odels

munie  m     shoham  y          optimal testing structured knowledge  twenty third conference artificial intelligence  aaai  
papadimitriou  c  h          computational complexity  addison wesley 
park  j  d     darwiche  a          complexity results approximation strategies map
explanations  journal aritificial intelligence research             
pineau  j   gordon  g     thrun  s          anytime point based approximations large pomdps 
jair             
quinlan  j  r          induction decision trees  machine learning           
radovilsky  y   shattah  g     shimony  s  e          efficient deterministic approximation algorithms non myopic value information graphical models  ieee international
conference systems  man cybernetics  smc   vol     pp           
robbins  h          aspects sequential design experiments  bulletin american
mathematical society             
scheffer  t   decomain  c     wrobel  s          active learning partially hidden markov models
information extraction  ecml pkdd workshop instance selection 
sim  r     roy  n          global a optimal robot exploration slam  ieee international
conference robotics automation  icra  
singh  a   krause  a   guestrin  c   kaiser  w  j     batalin  m  a          efficient planning
informative paths multiple robots  international joint conference artificial intelligence  ijcai   pp            hyderabad  india 
singhvi  v   krause  a   guestrin  c   garrett  j     matthews  h          intelligent light control
using sensor networks  proc   rd acm conference embedded networked sensor
systems  sensys  
slijepcevic  s     potkonjak  m          power efficient organization wireless sensor networks 
icc 
smallwood  r     sondik  e          optimal control partially observable markov decision
processes finite horizon  operations research               
stachniss  c   grisetti  g     burgard  w          information gain based exploration using raoblackwellized particle filters  robotics science systems  rss  
tong  s     koller  d          active learning parameter estimation bayesian networks 
nips 
turney  p  d          cost sensitive classification  empirical evaluation hybrid genetic decision
tree induction algorithm  journal artificial intelligence research            
van der gaag  l     wessels  m          selective evidence gathering diagnostic belief networks 
aisb quart            
zhao  f   shin  j     reich  j          information driven dynamic sensor collaboration tracking
applications  ieee signal processing              

   


