journal of artificial intelligence research                 

submitted        published      

message based web service composition  integrity constraints  and
planning under uncertainty  a new connection
jorg hoffmann

joe   hoffmann   sap  com

sap research
karlsruhe  germany

piergiorgio bertoli

bertoli   fbk   eu

fondazione bruno kessler
trento  italy

malte helmert

helmert   informatik   uni   freiburg   de

albert ludwigs universitat freiburg
freiburg  germany

marco pistore

pistore   fbk   eu

fondazione bruno kessler
trento  italy

abstract
thanks to recent advances  ai planning has become the underlying technique for several applications  figuring prominently among these is automated web service composition  wsc  at
the capability level  where services are described in terms of preconditions and effects over ontological concepts  a key issue in addressing wsc as planning is that ontologies are not only formal
vocabularies  they also axiomatize the possible relationships between concepts  such axioms correspond to what has been termed integrity constraints in the actions and change literature  and
applying a web service is essentially a belief update operation  the reasoning required for belief
update is known to be harder than reasoning in the ontology itself  the support for belief update is
severely limited in current planning tools 
our first contribution consists in identifying an interesting special case of wsc which is both
significant and more tractable  the special case  which we term forward effects  is characterized
by the fact that every ramification of a web service application involves at least one new constant
generated as output by the web service  we show that  in this setting  the reasoning required for
belief update simplifies to standard reasoning in the ontology itself  this relates to  and extends 
current notions of message based wsc  where the need for belief update is removed by a strong
 often implicit or informal  assumption of locality of the individual messages  we clarify the
computational properties of the forward effects case  and point out a strong relation to standard notions of planning under uncertainty  suggesting that effective tools for the latter can be successfully
adapted to address the former 
furthermore  we identify a significant sub case  named strictly forward effects  where an actual
compilation into planning under uncertainty exists  this enables us to exploit off the shelf planning tools to solve message based wsc in a general form that involves powerful ontologies  and
requires reasoning about partial matches between concepts  we provide empirical evidence that
this approach may be quite effective  using conformant ff as the underlying planner 

c
    
ai access foundation  all rights reserved 

fih offmann   b ertoli   h elmert   p istore

   introduction
since the mid nineties  ai planning tools have become several orders of magnitude more scalable 
through the invention of automatically generated heuristic functions and other search techniques
 see mcdermott        bonet   geffner        hoffmann   nebel        gerevini  saetti   
serina        helmert        chen  wah    hsu         this has paved the way to the adoption
of planning as the underlying technology for several applications  one such application area is
web service composition  wsc   by which in this paper we mean the automated composition of
semantic web services  sws   sws are pieces of software advertised with a formal description
of what they do  composing sws means to link them together so that their aggregate behavior
satisfies a complex user requirement  the ability to automatically compose web services is the key
to reducing human effort and time to market when constructing integrated enterprise applications 
as a result  there is a widely recognized economic potential for wsc 
in the wide spread sws frameworks owl s  and wsmo    sws are described at two distinct
levels  one of these addresses the overall functionality of the sws  and the other details precisely
how to interact with the sws  at the former level  called service profile in owl s and service
capability in wsmo  sws are described akin to planning operators  with preconditions and effects  therefore  planning is a prime candidate for realizing wsc at this level  this is the approach
we follow in our paper 
in such a setting  a key aspect is that sws preconditions and effects are described relative to
an ontology which defines the formal  logical  vocabulary  indeed  ontologies are much more than
just formal vocabularies introducing a set of logical concepts  they also define axioms which constrain the behavior of the domain  for instance  an ontology may define a subsumption relationship
between two concepts a and b  stating that all members of a are necessarily members of b  the
natural interpretation of such an axiom  in the context of wsc  is that every state that can be encountered  every possible configuration of domain entities  must satisfy the axiom  in that sense 
ontology axioms correspond to integrity constraints as discussed in the actions and change literature
 ginsberg   smith        eiter   gottlob        brewka   hertzberg        lin   reiter       
mccain   turner        herzig   rifi          hence wsc as considered here is like planning in
the presence of integrity constraints  since the constraints affect the outcome of action executions 
we are facing the frame and ramification problems  and execution of actions corresponds closely to
complex notions such as belief update  lutz   sattler        herzig  lang  marquis    polacsek 
       unsurprisingly  providing such support for integrity constraints in the modern scalable planning tools mentioned above poses serious challenges  to the best of our knowledge  it has yet to be
attempted at all 
regarding the existing wsc tools  or planning tools employed for solving wsc problems  the
situation isnt much better  most tools ignore the ontology  i e   they act as if no constraints on the
domain behavior were given  ponnekanti   fox        srivastava        narayanan   mcilraith 
      sheshagiri  desjardins    finin        pistore  traverso    bertoli      b  pistore  marconi  bertoli    traverso      a  agarwal  chafle  dasgupta  karnik  kumar  mittal    srivastava 
    a   other approaches tackle the full generality of belief update by using general reasoners  and
   for example  see the work of ankolekar et al         and burstein et al         
   for example  see the work of roman et al         and fensel et al         
   integrity constraints are sometimes also called state constraints or domain constraints 

  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

suffer from the inevitable performance deficiencies  eiter  faber  leone  pfeifer    polleres       
giunchiglia  lee  lifschitz  mccain    turner        
is a planningbased
formalization of

wsc formalism

is a variant of

is a restriction of
is a rich version of

forward effects

wsc

is a planningbased
formalization of

messagebased wsc

is a restriction of

conformant planning

can be
tackled by

strictly forward effects

figure    an overview of the planning and wsc frameworks addressed in this paper  special cases
identified herein shown in red   boldface 
our work addresses the middle ground between these two extremes  i e   the trade off between
expressivity and scalability in wsc  we do so via the identification of special cases that can be
tackled more efficiently  figure   gives an overview of the wsc and planning frameworks involved 
in brief  the forward effects case requires that every effect and ramification of a web service
affects at least one new constant that was generated as the web services output  in this situation 
the frame problem trivializes  making the planning problem more similar to common notions of
conformant planning  smith   weld        bonet   geffner        cimatti  roveri    bertoli 
      hoffmann   brafman         we will discuss how existing tools for the latter  in particular
conformant ff  hoffmann   brafman         can be extended to deal with wsc under forward
effects  with strictly forward effects  where action effects are required to affect only outputs  we
devise an actual compilation into conformant planning  we thus obtain a scalable tool for interesting
wsc problems with integrity constraints  in particular we are able to exploit  some of  the heuristic
techniques mentioned above  hoffmann   nebel        hoffmann   brafman        
in what follows  we will explain the various parts of figure   in a little more detail  our starting
point is a wsc formalism  addressing wsc in terms of planning in the presence of integrity constraints  as discussed above  the formalism is essentially an enriched form of conformant planning 
its distinguishing aspects are 
 the initial state description is a conjunction of literals  possibly not mentioning some of the
logical facts in the task  and hence introducing uncertainty  
 actions have a conditional effects semantics  meaning they can be executed in any state  but
have an effect only if they are applicable 
 actions may have output variables  i e   they may create new constants 
 there is a set of integrity constraints  each of which is a universally quantified clause 
 the semantics of action execution is defined in terms of a belief update operation 
section   below provides more details on these choices  and motivates them with an example and
results from the literature  as we will show  planning in the formalism is very hard  particularly 
  

fih offmann   b ertoli   h elmert   p istore

even just testing whether a given action sequence is a plan is p   complete  this is in contrast to the
more common notions of conformant planning  where plan testing is only conp complete 
as we will see  forward effects remove the additional complexity  intuitively  the forward effects
case covers the situation where a web service outputs some new constants  sets their characteristic
properties relative to the inputs  and relies on the ontology axioms to describe any ramifications
concerning the new constants  this case is syntactically characterized as follows 
    every effect literal contains at least one output variable 
    within each integrity constraint  every literal has the same set of variables in its arguments 
this definition is best understood with an example  consider the following variant of the widespread virtual travel agency  vta   web services that book travel and accommodation must
be linked  these web services generate new constants corresponding to tickets and reservations 
for example  there are integrity constraints stating subsumption  such as z   trainticket z  
ticket z   a web service bookticket may have the input variable x  the precondition train x    the
output variable y  and the effect trainticket y   ticketfor  y  x   this is a forward effects task 
every effect literal contains the output variable y  and the integrity constraint has the single variable
z which provides the arguments of all literals in the constraint  say one instantiates the input of
bookticket with a constant c and its output with a new constant d  when applying the resulting
ground action to a state where train c  holds true  the constant d gets created  and its characteristic
properties relative to the inputs  trainticket d   ticketfor  d  c   are set directly by the action 
the integrity constraint takes care of the ramification  establishing that ticket d  holds  note that
the status of c  apart from its relation to d  is not affected in any way   
the forward effects case is closely related to a wide spread notion of wsc problems  which
we refer to as message based wsc  in such approaches  the composition semantics is based on
chaining over input and output messages of web services  in one or the other sense  inferences from
ontology axioms can be made in many of these approaches  but only in a restricted way limited by an
assumption of locality of the individual messages  where the interferences affect only a particular
message transfer  and any implications for other transfers are ignored  this locality assumption is
usually made in an informal way  and often not stated explicitly at all  one contribution of our work
is to shed some light on this issue  via the identification of the forward effects case which lies in
between message based wsc and a full planning framework with belief update semantics 
both message based wsc and the forward effects case share the focus on output constants 
there are two important differences  first  the forward effects case is more restricted than messagebased wsc in terms of the ontology axioms allowed  essentially  forward effects correspond to
a special case of wsc where the locality assumption of message based wsc is actually justified 
within a full planning framework  second  that full framework comes with the benefit of increased
flexibility in the combination of services  because locality is not enforced  e g  the output of one
service may be reused at several points in a plan  
from a computational point of view  the key property of the forward effects case is that it
removes the need for belief update  in a nutshell  the reason is that actions affect only new propositions  i e   propositions involving at least one output constant   recall here the point made about
   the latter would not be the case if the effect of bookticket included a literal affecting only x  example 
train x    or if there was an integrity constraint capable of mixing old and new constants  example  x  y  
trainticket y   train x   

  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

the unchanged status of c  in the vta example above   the output constant  d  in the example  does
not exist prior to the application of the action  and hence the previous belief carries no knowledge
about it and need not be revised  consider the characterization of forward effects  as given above 
condition     ensures that the immediate effect of the action affects only new propositions  condition     ensures that any changes on new propositions only propagate to new propositions  since
all literals in a constraint share the same variables  the output constant in question is copied to all of
them  as we will see  by virtue of these properties the complexity of plan testing is conp complete 
rather than p   complete  in the forward effects case 
this complexity reduction is critical because the reduced complexity is the same as in the more
common notions of conformant planning under initial state uncertainty  therefore it should be feasible to adapt conformant planning tools to address wsc with forward effects  scalable planning
tools for conformant planning have already been developed  cimatti et al         bryce  kambhampati    smith        hoffmann   brafman        palacios   geffner         hence this is a
promising line of research  as an example  we will focus on the conformant ff tool  hoffmann
  brafman         short cff  and outline the main steps that need to be taken in adapting cff to
handle wsc with forward effects 
we then identify a case where an actual compilation into conformant planning under initial
state uncertainty exists  for that  one must fix a set of constants a priori  in a manner that is
fairly standard  see  e g   the settlers domain of long   fox         we simply include in that set
a subset of potential constants that can be used to instantiate outputs  the more subtle idea we
put forward is to identify a condition on the actions under which we can predict which properties
will be assigned to which potential constants  in case they are created  this enables us to design
a compilation that moves all action effects into the initial state formula  and uses actions only to
modify the set of constants that already exist  in this way  reasoning about the initial state formula
in the compiled task is the same as reasoning about output constants in the original task  and the
reasoning mechanisms included in tools such as cff can be naturally used to implement the latter 
our trick for predicting output properties is to require that all actions are compatible in the sense
that they either produce different outputs  or have the same effects  it turns out that this condition is
naturally given in a restriction of forward effects  which we call strictly forward effects  where the
web service effects concern only new constants 
clearly  not being able to reference the inputs is a limitation  for example  we can no longer
say  in the above vta example  that the output y is a ticket for the input x  still  the strictly forward
effects case describes an interesting class of wsc problems  that class corresponds to web services
modeled as in the early versions of owl s  for example  where there was no logical connection
between inputs and outputs  further  this class of wsc problems allows powerful ontologies 
universally quantified clauses  and makes it possible to combine services very flexibly  using
our compilation  this class of problems can be solved by off the shelf tools for planning under
uncertainty 
we validate the compilation approach empirically by running a number of tests using cff as
the underlying planner  we use two test scenarios  both of which are scalable in a variety of parameters  covering a range of different problem structures  we examine how cff reacts to the various
parameters  viewed in isolation  these results demonstrate that large and complex wsc instances
can be comfortably solved using modern planning heuristics 
a comparison to alternative wsc tools is problematic due to the widely disparate nature of
what kinds of problems these tools can solve  what kinds of input languages they understand  and
  

fih offmann   b ertoli   h elmert   p istore

what purpose the respective developers had in mind  to nevertheless provide some assessment of
the comparative benefits of our approach we run tests with the dlvk tool by eiter et al        
and eiter  faber  leone  pfeifer  and polleres         dlvk is one of the few planning tools that
deals with ontology axioms  called static causal rules  directly  without the need to restrict
to forward effects and without the need for a compilation  since  in the context of our work  the
main characteristic of wsc is the presence of ontology axioms  this means that dlvk is one of
the few existing native wsc tools  by comparison  our forward effects compilation approach
solves a similar problem  but sacrifices some expressivity  the question is  can we in principle
gain anything from this sacrifice  absolutely  the answer is yes  dlvk is much slower than
compilation cff  solving only a small fraction of our test instances even when always provided
with the correct plan length bound  we emphasize that we do not wish to over state these results 
due to the above mentioned differences between the tools  the only conclusion we draw is that the
trade off between expressivity and scalability in wsc is important  and that the forward effects case
seems to constitute an interesting point in that trade off 
the paper is organized as follows  first  section   provides some further background necessary
to understand the context and contribution of our work  section   introduces our wsc planning
formalism  section   defines and discusses forward effects  section   introduces our compilation to
planning under uncertainty  and section   presents empirical results  we discuss the most closely
related work at the relevant points during the text  and section   provides a more complete overview 
finally  section   concludes and discusses future work  to improve readability  most proofs are
moved into appendix a and replaced in the text by proof sketches 

   background
the context of our work is rather intricate  wsc as such is a very new topic posing many different
challenges to existing techniques  with the effect that the field is populated by disparate works differing considerably in their underlying purpose and scope  in other words  the common ground is
fairly thin in this area  further  our work actually involves three fields of research  wsc  planning 
and reasoning about actions and change  which are all relevant to understanding our contribution 
for these reasons  we now explain this background in some detail  we first discuss wsc in general 
and wsc as planning in particular  we then state the relevant facts about belief update  we finally
consider message based wsc 
    wsc  and wsc as planning
composition of semantic web services has received considerable attention in the last few years  a
general formulation of the problem  shared by a large variety of works  focuses on the capability
level  where each web service is conceived as an atomic operator that transforms concepts  more
specifically  a service is defined via an iope description  the service receives as input a set i
of typed objects  and  provided some precondition p on i holds  produces as output a set o of
typed objects for which some effect e is guaranteed to hold  the typing of the objects exchanged
by the services is given in terms of their membership in concepts  concepts are classes defined
within ontologies  which exploit description logics  dl   or some other form of logic  to formally
define the universe of concepts admitted in the discourse  an ontology can express complex relationships among concepts  like a subsumption hierarchy  or the way objects belonging to a concept
are structured into parts referring to other concepts 
  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

this general setting can be instantiated in various ways depending on the kind of conditions
admitted as preconditions effects of services  and on the kind of logics underlying the ontology
definitions  independent of this  the problem of semantic web service composition can be stated
as one of linking appropriately a set of existing services so that their aggregate behavior is that
of a desired service  the goal   to illustrate this problem  consider the following example  which
is inspired by the work of thakkar  ambite  and knoblock        on e services for bioinformatics
 and relies on the actual structure of proteins  see for example petsko   ringe        branden  
tooze        chasman        fersht        
example   say we want to compose a web service that provides information about different classes
of proteins  the ontology states which classes of proteins exist  and which structural characteristics
may occur  we have available an information service for every structural characteristic  and a
presentation service that combines a range of information  given a particular protein class  the
composed web service should run the relevant information services  and present their output 
concretely  classes of proteins are distinguished by their location  cell  membrane  intermembrane           this is modeled by predicates protein x   cellprotein x   membraneprotein x  
intermembraneprotein x   along with sub concept relations such as x   cellprotein x  
protein x   an individual protein is characterized by the following four kinds of structures 
   the primary structure states the proteins sequence of amino acids  e g    kw  x   a protein called glyoxalase  and  n   x   a protein called triosephosphate isomerase  
   the secondary structure states the proteins external shape in terms of a dssp  dictionary of secondary structure for proteins  code  admitting a limited set of possible values 
for example  g indicates a   turn helix  b a  sheet  and so on  the total set of values is
g h i t e b s 
   the tertiary structure categorizes the proteins   d shape 
   for a subset of the proteins  a quaternary structure categorizes the proteins shape when
combined in complexes of proteins  amounting to about      different shapes  see for example
 dcomplex org        
there are various axioms that constrain this domain  apart from the mentioned subconcept
relations  first  some obvious axioms specify that each protein has a value in each of the four
kinds of structures  i e   the protein has a sequence of amino acids  an external shape  etc   however 
there are also more complex axioms  particular kinds of proteins come only with particular structure
values  this is modeled by axioms such as 
x   cellprotein x   g x    n   x 
x   cellprotein x   b x    kw  x   complexbarrel x 
for each dssp code z there is an information service  named getinfodsspz   whose precondition
is z x  and whose effect is infodssp y  where y is an output of the service  similarly  we have information services for amino acids    d shapes  and shapes in complexes  the presentation service 
named combineinfo  requires that information on all four kinds of structures has been created  and
has the effect combinedpresentation y   where y is an output of combineinfo  
  

fih offmann   b ertoli   h elmert   p istore

the input to the composed web service is a protein c  a logical constant  and its class  the
goal is x   combinedpresentation x   a solution is to reason about which characteristics may
occur  to apply the respective information services  and then to run combineinfo  in a variant of
the problem  an additional requestinfo service is used to initiate the information request  i e   the
output of requestinfo is the protein c and its class 
this example shows how ontology axioms play a crucial role in our form of wsc  formulating
complex dependencies between different concepts  note that applying a web service may have indirect consequences implied by the ontology axioms  in the example  the output of the requestinfo
service has implications for which kinds of information services are required 
another interesting aspect of example   is that it requires what the sws community calls partial matches  as opposed to plug in matches  paolucci  kawamura  payne    sycara        li
  horrocks        kumar  neogi  pragallapati    ram          consider the situation where one
wants to connect a web service w to another web service w   that is  w will be executed prior to
w   and the output of w will be used to instantiate the input of w   then w and w are said to have
a partial match if  given the ontology axioms  the output of w sometimes suffices to provide the
necessary input for w   by contrast  w and w are said to have a plug in match if  given the ontology
axioms  the output of w always suffices to provide the necessary input for w  
plug in matches are tackled by many approaches to wsc  whereas partial matches are tackled
only by few  part of the reason probably is that plug in matches are easier to handle  in many types
of wsc algorithms  indeed most existing wsc tools support plug in matches only  see a detailed
discussion of wsc tools in section     example   cannot be solved with plug in matches because
each of the information services provides the necessary input for the combineinfo service only in
some particular cases 
we base our work on a planning formalism that allows to specify web services  i e   actions 
with outputs  and that allows to specify ontology axioms  the axioms are interpreted as integrity
constraints  and the resulting semantics corresponds closely to the common intuitions behind wsc 
as well as to the existing formal definitions related to wsc  lutz   sattler        baader  lutz 
milicic  sattler    wolter        liu  lutz  milicic    wolter      b      a  de giacomo  lenzerini  poggi    rosati         since one of our main aims is to be able to exploit existing planning
techniques  we consider a particular form of ontology axioms  in correspondence with the representations that are used by most of the existing tools for planning under uncertainty  namely  the axioms
are universally quantified clauses  an example is the subsumption relation x   trainticket x  
ticket x  mentioned above  where as usual a  b is an abbreviation for a  b  a planning task
specifies a set of such clauses  interpreted as the conjunction of the clauses  note that this provides
significant modeling power  the meaning of the universal quantification in the clauses is that the
clauses hold for all planning objects  logical constants  that are known to exist  in that sense  the
interpretation of formulas is closed world as is customary in planning tools  however  in contrast
to most standard planning formalisms including pddl  we do not assume a fixed set of constants 
rather  the specification of actions with outputs enables the dynamic creation of new constants  the
quantifiers in the ontology axioms range over all constants that exist in the respective world  in a
similar fashion  the planning goal may contain variables  which are existentially quantified  the
constants used to instantiate the goal may have pre existed  or they may have been generated as
   the terminology in these works is slightly different from what we use here  and they also describe additional kinds
of matches  some details are given in section   

  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

the outputs of some of the web services that were applied on the path to the world  consider for
illustration the goal x   combinedpresentation x  in example    where the goal variable x will
have to be instantiated with an output created by the combineinfo service 
another important aspect of our planning formalism is that we allow incomplete initial state
descriptions  the initial state corresponds to the input that the user provides to the composed web
service  certainly we cannot assume that this contains complete information about every aspect
of the world   in example    the initial state tells us which class of proteins we are interested
in  but leaves open what the consequences are regarding the possible structural characteristics  
we consider the case where there is no observability  i e   conformant planning  the outcome of
wsc is a sequence of web services that satisfies the user goal in all possible situations   as is
customary in conformant planning  the actions have a conditional effects semantics  i e   they fire if
their precondition holds true  and otherwise they do nothing  note that  this way  we obtain a notion
of partial matches  the solution employs different actions depending on the situation 
the main difference between our planning formalism and the formalisms underlying most current planning tools is the presence of integrity constraints  and its effect on the semantics of executing actions  that semantics is defined as a belief update operation 
    belief update
the correspondence of web service applications to belief update was first observed by lutz and
sattler         and followed by baader et al          liu et al       b      a  and de giacomo
et al          in the original statement of the belief update problem  we are given a belief  
i e   a logical formula defining the worlds considered possible  we are further given a formula  
the update  intuitively   corresponds to some observation telling us that the world has changed
in a way so that  now   is true  we want to obtain a formula  defining the worlds which are
possible given this update  certainly  we need to have that       ensuring this corresponds
to the well known ramification problem  at the same time  however  the world should not change
unnecessarily  that is  we want  to be as close as possible to   among the formulas which
satisfy   this corresponds to the frame problem 
say we want to apply an action a in the presence of integrity constraints   describes the
worlds that are possible prior to the application of a   is the resulting set of possible worlds 
the integrity constraints correspond to a formula ic which holds in   and which we require to
hold in    the update formula  is given as the conjunction of the action effect with ic   i e  
we have    effa  ic   this means that we update our previous belief with the information that 
after a  effa is a new formula required to hold  and ic is still true  for example  we may have an
action effect a c  and a subsumption relation between concepts a and b  formulated as a clause
x   a x   b x   then the update formula a c   x   a x   b x  ensures that b c  is true
in   
belief update has been widely considered in the literature on ai and databases  see for example
fagin  kuper  ullman    vardi        ginsberg   smith        winslett              katzuno
  mendelzon        herzig        herzig   rifi        liu et al       b  de giacomo et al  
       the various approaches differ in exactly how  should be defined  the best consensus is
that there is no one approach that is most adequate in every application context  all approaches
   of course  more generally  observability is partial and web service effects are also uncertain  we do not consider
these generalizations here  extending our notions accordingly should be straightforward  and is future work 

  

fih offmann   b ertoli   h elmert   p istore

agree that  should hold in the updated state of affairs        major differences lie in what
exactly it should be taken to mean that  should be as close as possible to   various authors  for
example brewka and hertzberg         mccain and turner         herzig         and giunchiglia
and lifschitz         argue that a notion of causality is needed  in addition to  or even instead of  a
notion of integrity constraints  to model domain behavior in a natural way  we do not counter these
arguments  but neither do we follow a causal approach in our work  the reason is that ontologies
in the context of wsc  for example ontologies formulated in the web ontology language owl
 mcguinness   van harmelen         do not incorporate a notion of causality  all we are given is a
set of axioms  made with the intention to describe the behavior of the domain itself  rather than the
behavior it exhibits when changed by some particular web services  our idea in this work is to try
to leverage on what we have  or what we are reasonably close to having   consideration of causal
approaches in wsc is left for future work 
belief update is a computationally very hard problem  eiter and gottlob        and liberatore
       show that  for the non causal approaches to defining    reasoning about  is typically
harder than reasoning in the class of formulas used for formulating  and   specifically  deciding
whether or not a particular literal is true in  is  p  hard even if  is a complete conjunction of
literals  corresponding to a single world state  and  is a propositional cnf formula  the same
problem is conp hard even if  is a single world state and  is a propositional horn formula  we
use these results to show that  in our planning formalism  checking a plan  testing whether or
not a given action sequence is a plan  is  p  complete  and deciding polynomially bounded plan
existence is  p  complete 
given this complexity  it is perhaps unsurprising that the support for integrity constraints in current planning tools is severely limited  the only existing planning tools that do support integrity
constraints  namely those by eiter et al         and giunchiglia et al          are based on generic
deduction  like satisfiability testing or answer set programming  they hence lack the planningspecific heuristic and search techniques that are the key to scalability in the modern planning tools
developed since the mid nineties  it has not even been investigated yet if and how integrity constraints could be handled in the latter tools  the only existing approach that ventures in this direction implements so called derived predicates in some of the modern planning tools  thiebaux 
hoffmann    nebel        gerevini  saetti  serina    toninelli        chen et al          this
approach postulates a strict distinction between basic predicates that may be affected by actions 
and derived predicates that may be affected by integrity constraints taking the form of logic programming rules  if a predicate appears in an action effect  then it is not allowed to appear in the
head of a rule  this is not a desirable restriction in the context of wsc  where web services are
bound to affect properties that are constrained by ontology axioms 
the existing work connecting wsc with belief update  lutz   sattler        baader et al  
      liu et al       b      a  de giacomo et al         is of a theoretical nature  the actual implemented wsc tools make severe simplifying assumptions  most often  that assumption is to ignore
the ontology axioms  ponnekanti   fox        srivastava        mcilraith   son        sheshagiri
et al         sirin  parsia  wu  hendler    nau        pistore et al       b      a   sometimes 
the ontology constraints are restricted to subsumption hierarchies  which makes the update problem
easy  constantinescu   faltings        constantinescu  faltings    binder      b      a   sirin
and parsia        and sirin  parsia  and hendler        discuss the problem of dealing with ontology axioms in wsc  but do not make a connection to belief update  and describe no alternative
solution  finally  some authors  for example meyer and weske         do deal with ontology ax  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

ioms during composition  but do not provide a formal semantics and do not specify exactly how
action applications are handled  it seems that these not fully formalized wsc approaches implicitly
assume a message based framework  those frameworks are closely related to the forward effects
special case identified herein 
    message based wsc
in message based approaches to wsc  the composition semantics is based on chaining over input
and output messages of web services  the word message is not a standard term in this context 
most authors use their own individual vocabulary  as far as we are aware  the first appearance of the
word message in a wsc paper title is in the work by liu  ranganathan  and riabov         this
work describes message based wsc as follows  a solution is a directed acyclic graph  dag  of
web services  where the input needed for web service  dag graph node  w must be provided by the
outputs of the predecessors of w in the graph  that is  the plan determines fixed connections between
the actions  reasoning  then  only takes place within these connections  any two connections
between different output and input messages  i e   any two graph edges ending in a different node 
are assumed to be mutually independent  consider the following example for illustration  say a web
service w has the effect hasattributea c  d  where d is an output constant and c is an input  i e   c
existed already prior to application of w   say there is an axiom x  y   hasattributea x  y  
conceptb x  expressing an attribute domain restriction  if x has y as a value of attribute a 
then x must be of concept b  given this  ws effect implies conceptb c   now  suppose that our
belief prior to applying w did not constrain c to be of concept b  then applying w leads to new
knowledge about c  hence we need a non trivial belief update taking into account the changed
status of c  and any implications that may have  message based wsc simply acts as if the latter is
not the case  it only checks whether w correctly supplies the inputs of the web services w that w
is connected to  that is  the new fact hasattributea c  d  may be taken as part of a proof that the
effect of w implies the precondition of a connected web service w   but it is not considered at all
what implications hasattributea c  d  may have with respect to the previous state of affairs  in
that sense  message based wsc ignores the need for belief update 
the intuitions underlying message based wsc are fairly wide spread  many papers use them
in a more or less direct way  there are many approaches that explicitly define wsc solutions to be
dags with local input output connections as above  zhan  arpinar    aleman meza        lecue
  leger        lecue   delteil        kona  bansal  gupta    hite        liu et al         ambite
  kapoor         in various other works  constantinescu   faltings        constantinescu et al  
    b      a  meyer   weske         the message based assumptions are more implicit  they
manifest themselves mainly in the sense that ontology axioms are only used to infer the properties
of output messages  and often only for checking whether the inferences imply that a desired input
message is definitely given 
previous work on message based wsc does not address at all how message based wsc relates
to the various notions  like belief update  considered in the literature  one contribution of our work
is to shed some light on this issue  via the identification of the forward effects case which lies in
between message based wsc and a full planning framework with belief update semantics 
both message based wsc and the forward effects case share the focus on outputs  indeed 
the output constants generated by our actions can be viewed as messages  an output constant
represents an information object which is created by one web service  and which will form the

  

fih offmann   b ertoli   h elmert   p istore

input of some other web service  in the forward effects case  due to the restriction on axioms  the
individual messages do not interact  this is much like message based wsc  the main difference is
this  while message based wsc ignores any possible interactions  in forward effects there actually
arent any interactions  according to a formal planning based execution semantics  in that sense 
forward effects correspond to a special case of wsc where the assumptions of message based wsc
are justified 
reconsider our example from above  featuring a web service w with an effect implying that
conceptb c  where c is a pre existing constant  as explained above  message based wsc will
simply ignore the need for updating the knowledge about c  in contrast  the forward effects case
disallows the axiom x  y   hasattributea x  y   conceptb x  because it may lead to new
conclusions about the old belief  note that the literals in the axiom refer to different sets of variables  
the forward effects case also differs significantly from most approaches to message based wsc
in terms of the flexibility with which it allows to combine actions into plans  in the messagebased approach using dags  a solution dag ensures that the inputs of each service w can always
be provided by ws predecessors  that is  we have a plug in match between the set w of ws
predecessors in the dag  and w itself  note that this is slightly more general than the usual notion
of plug in matches  in that  w   may be greater than    and hence each single service in w may have
only a partial match with w  this is the notion used  amongst others  by liu et al          other
authors  for example lecue and leger        and lecue and delteil         are more restrictive in
that they consider every individual input x of w in turn and require that there exists a w  w so that
w has a plug in match with x  i e   w guarantees to always provide x   even in the more generous
of these two definitions  partial matches are restricted to appear locally  on dag links  every
action web service is required to be always executable at the point where it is applied  in other
words  the services are used in a fixed manner  not considering the dynamics of actual execution 
in example    this would mean using the same information services regardless of the class of the
protein  hence completely ignoring what is relevant and what is not 
the forward effects case incorporates a much more general notion of partial matches  this happens in a straightforward way  exploiting the existing notions from planning  in the form of a conditional effects semantics  the standard notion of a conformant solution defines how partial matches
must work together on a global level  to accomplish the goal  to the best of our knowledge  there
is only one other line of work on wsc  by constantinescu et al   constantinescu   faltings       
constantinescu et al       b      a   that incorporates a comparable notion of partial matches  in
that work  web services are characterized in terms of input and output types  to handle partial
matches  so called switches combine several web services in a way that ascertains all relevant
cases can be covered  the switches are designed relative to a subsumption hierarchy over the types 
note that subsumption hierarchies are a special case of the much more general integrity constraints
 universally quantified clauses  that we consider in our work 

   formalizing wsc
as a solid basis for addressing wsc  we define a planning formalism featuring integrity constraints 
on the fly creation of output constants  incomplete initial state descriptions  and actions with a conditional effects semantics  the application of actions is defined as a belief update operation  following the possible models approach by winslett         that definition of belief update is somewhat
canonical in that it is very widely used and discussed  in particular it underlies all the recent work

  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

relating to formalizations of wsc  lutz   sattler        baader et al         liu et al       b 
    a  de giacomo et al         de giacomo  lenzerini  poggi    rosati         as we will show
further below  section       most belief update operations are equivalent anyway as soon as we
are in the forward effects case  recall here that the forward effects case is the central object of
investigation in this paper 
we first give the syntax of our formalism  which we denote with wsc  then we give its semantics  we conclude with an analysis of its main computational properties 
    syntax
we denote predicates with g  h  i  variables with x  y  z  and constants with c  d  e  literals are possibly negated predicates whose arguments are variables or constants  if all arguments are constants 
the literal is ground  we refer to positive ground literals as propositions  given a set p of predicates
and a set c of constants  we denote by p c the set of all propositions that can be formed from p
and c  given a set x of variables  we denote by lx the set of all literals l which use only variables
from x  note here that l may use arbitrary predicates and constants   if l is a literal  we write
l x  to indicate that l has the variable arguments x  if x    x            xk   and c    c            ck   
then by l c            ck  x            xk   we denote the respective substitution  abbreviated as l c   in the
same way  we use the substitution notation for any construct involving variables  slightly abusing
notation  we use a vector of constants also to denote the set of constants appearing in it  further  if
a function a assigns constants to the variables x  then by l a x  we denote the substitution where
each argument x  x was replaced with a x   we are only concerned with first order logic  that is 
whenever we write formula we mean a first order formula  we denote true as   and false as   
a clause  or integrity constraint  is a disjunction of literals with universal quantification on the
outside  the variables quantified over are exactly those that appear in at least one of the literals  for
example  x  y   g x  y   h x  is an integrity constraint but x  y  z   g x  y   h x  and x  
g x  y h x  are not  an operator o is a tuple  xo   preo   yo   effo    where xo   yo are sets of variables  preo is a conjunction of literals from lxo   and effo is a conjunction of literals from lxo yo   
the intended meaning is that xo are the inputs and yo the outputs  i e   the new constants created by
the operator  for an operator o  an action a is given by  prea   effa     preo   effo   ca  xo   ea  yo  
where ca and ea are vectors of constants  for ea we require that the constants are pairwise different  it makes no sense to output the same new constant twice  given an action a  we will refer
to as inputs and outputs by ca and ea   respectively  we will also use the notations prea   effa with
the obvious meaning 
a wsc task  or planning task  is a tuple  p  ic   o  c        g    here  p is a set of predicates 
ic is a set of integrity constraints  o is a set of operators and c  is a set of constants  the initial
constants supply    is a conjunction of ground literals  describing the possible initial states  g
is a conjunction of literals with existential quantification on the outside  describing the goal states 
e g   x  y   g x   h y   all predicates are taken from p  and all constants are taken from c   
all constructs  e g   sets and conjunctions  are finite  we will sometimes identify ic with the
conjunction of the clauses it contains  note that the existential quantification of the goal variables
   one could of course introduce more general notations for logical constructs using some set of predicates or constants 
however  herein the two notations just given will suffice 
   as stated  we do not address disjunctive or non deterministic effects  this is a topic for future work 

  

fih offmann   b ertoli   h elmert   p istore

provides the option to instantiate the goal with constants created during planning  obtaining objects
as requested by the goal may be possible only through the use of outputs 
the various formulas occurring in  p  ic   o  c        g   may make use of constants from c   
specifically  this is the case for clauses in ic and for the goal formula g   allowing such use of
constants does not have any effect on our complexity or algorithmic results  it is conceivable that
the feature may be useful  as a simple example  in the vta domain the user may wish to select a
particular train  say the train company provides a table of trains with their itineraries  that table can
be represented in     possibly with help from ic stating constraints that hold for particular trains 
the user can then select a train  say ice     and pose as a goal that y   ticketfor  y  ice     
constraining the produced ticket in this way would not be possible without the use of pre existing
constants  or would at least require a rather dirty hack  e g   encoding the desired train in terms of a
special predicate  
operator descriptions  that is  preconditions and effects  may also use constants from c    the
value of this is more benign than for ic and g because one can always replace a constant c in
the precondition effect with a new input output variable x  and instantiate x  during planning  with
c  note  however  that this would give the planner the option to  uselessly  instantiate x with some
other constant  and may hence affect planning performance  in our above example  there might be a
special operator booking a ticket for ice     e g   if that train has particular ticketing regulations  
the correspondence of a wsc task to a web service composition task is fairly obvious  the
set p of predicates is the formal vocabulary used in the underlying ontology  the set ic of
integrity constraints is the set of axioms specified by the ontology  i e   domain constraints such as
subsumption relations  the set o of operators is the set of web services  note that our formalization
corresponds very closely to the notion of iope descriptions  inputs  outputs  preconditions  and
effects  ankolekar et al         burstein et al          an action corresponds to a web service call 
where the web services parameters are instantiated with the call arguments 
the constructs c        and g are extracted from the user requirement on the composition 
we assume that such requirements also take the form of iope descriptions  then  c  are the
user requirement inputs  and   is the user requirement precondition  in other words  c  and  
describe the input given to the composition by the user  similarly  g is the user requirement effect
 the condition that the user wants to be accomplished  and the user requirement outputs are the
 existentially quantified  variables in g  
    semantics
in what follows  assume we are given a wsc task  p  ic   o  c        g    to be able to model
the creation of constants  states  also called world states  in our formalism are enriched with the set
of constants that exist in them  a state s is a pair  cs   is   where cs is a set of constants  and is is a
cs  interpretation  i e   a truth value assignment is   p cs           quantifiers are taken to range
over the constants that exist in a state  that is  if i is a c interpretation and  is a formula  then by
writing i     we mean that i    c where c is the same as  except that all quantifiers were
restricted to range over c  to avoid clumsy notation  we will sometimes write s     to abbreviate
is     
the core definition specifies how the application of an action affects a state  this is defined
through a form of belief update  let us first define the latter  assume a state s  a set of constants
c   cs   and a formula   we define update s  c      to be the set of interpretations that result

  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

from creating the constants c    cs   and updating s with  according to the semantics proposed by
winslett        
say i  and i  are c   interpretations  we define a partial order over such interpretations  by
setting i   s i  if and only if
 p  p cs   i   p     is  p     p  p cs   i   p     is  p   

   

in words  i  is ordered before i  iff it differs from s in a proper subset of values  given this  we can
now formally define update s  c       let i be an arbitrary c   interpretation  we define
i  update s  c        i     and  i    i       i   s i     

   

hence  update s  c      is defined to be the set of all c   interpretations which satisfy   and which
are minimal with respect to the partial order  s   put in different terms  update s  c      contains
all interpretations that differ from s in a set inclusion minimal set of values 
now  assume an action a  we say that a is applicable in s  short appl s  a   if s    prea  
ca  cs   and ea  cs     that is  on top of the usual precondition satisfaction we require that
as inputs exist and that as outputs do not yet exist  the result of executing a in s is 

  c    i      c    cs  ea   i   update s  c    ic  effa    appl s  a 
   
res s  a    
 s 
otherwise
note that a can be executed in s even if it is not applicable  in that case  the outcome is the singleton
set containing s itself  i e   the action does not affect the state  this is an important aspect of our
formalism  which we get back to below  if ic  effa is unsatisfiable  then obviously we get
res s  a      we say in this case that a is inconsistent  
the overall semantics of wsc tasks is now easily defined via a standard notion of beliefs  these
model our uncertainty about the true state of the world  a belief b is the set of world states that are
possible at a given point in time  the initial belief is
b      s   cs   c    s    ic      

   

an action a is inconsistent with a belief b if it is inconsistent with at least one s  b  in the latter
case  res b  a  is undefined  otherwise  it is defined by
 
res s  a  
   
res b  a    
sb

this is extended to action sequences in the obvious way  a plan is a sequence ha            an i so that
s  res b    ha            an i    s    g  

   

for illustration  consider the formalization of our example from section   
example   reconsider example    for the sake of conciseness  we formalize only a part of the
example  with simplified axioms  the wsc task is defined as follows 
   unless ic mentions any constants  if a is based on operator o and a is inconsistent  then any action based on o is
inconsistent  such operators can  in principle  be filtered out in a pre process to planning 

  

fih offmann   b ertoli   h elmert   p istore

 p    protein  cellprotein  g  h  i   n     kw   infodssp  info d  combinedpresentation  
where all the predicates are unary 
 ic consists of the clauses 
 x   cellprotein x   protein x    subsumption 
 x   protein x   g x   h x   i x    at least one dssp value 
 x   protein x    n   x    kw  x    at least one   d shape 
 x   cellprotein x   g x    n   x    dependency 
 x   cellprotein x   h x    n   x    dependency 
 o consists of the operators 
 getinfodsspg     x   g x    y   infodssp y  
 getinfodssph     x   h x    y   infodssp y  
 getinfodsspi     x   i x    y   infodssp y  
 getinfo d n       x    n   x    y   info d y  
 getinfo d kw      x    kw  x    y   info d y  
 combineinfo    x    x     infodssp x     info d x      y   combinedpresentation y  
 c     c       cellprotein c 
 g   x   combinedpresentation x 
to illustrate the formalism  we now consider a plan for this example task 
the initial belief b  consists of all states s where cs    c  and s    ic  cellprotein c   say
we apply the following sequence of actions 
   apply getinfodsspg  c  d  to b    then we get to the belief b  which is the same as b  except
that  from all s  b  where s    g c   new states are generated that have the constant d and
infodssp d  
   apply getinfodssph  c  d  to b    we get the belief b  where new states with d and
infodssp d  are generated from all s  b  where s    h c  
   apply getinfo d n    c  e  to b    yielding b   
   apply getinfo d kw   c  e  to b    this yields b    where we get e and info d e  from all s  b 
where s     n   c  or s     kw  c  
   apply combineinfo d  e  f   to b    this brings us to b  which is like b  except that from all
s  b  where d  e  cs new states are generated that have f and combinedpresentation f   
from the dependencies in ic  the last two clauses   we get that any s  b  satisfies either g c  or
h c   from the subsumption clause and the clause regarding   d shapes  first and third clauses 
we get that any s  b  satisfies either  n   c  or  kw  c   hence  as is easy to verify  b    
g and so hgetinfodsspg  c  d   getinfodssph  c  d   getinfo d n    c  e   getinfo d kw   c  e  
combineinfo d  e  f  i is a plan 
  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

note that this plan does not make use of getinfodsspi  c  d   to obtain a plan  in this domain
one can always just apply all information services  however  this plan is trivial and does not take
into account what is relevant and what is not  reasoning over ic enables us to find better plans 
our semantics for executing non applicable actions is vital for the workings of example    as
pointed out above  below the definition of res s  a   equation       a can be executed in s even if it
is not applicable  this realizes partial matches  a web service can be called as soon as it might match
one of the possible situations  in planning terms  our actions have a conditional effects semantics   
the contrasting notion would be to enforce preconditions  i e   to say that res s  a  is undefined if
a is not applicable to s  this would correspond to plug in matches 
in example    the partial match semantics is necessary in order to be able to apply actions that
cover only particular cases  for example  consider the action getinfodsspg  c  d   which is applied
to the initial belief in the example plan  the precondition of that action is g c   however  there
are states in the initial belief which do not satisfy that precondition  the initial belief allows any
interpretation satisfying ic     cf  equation       and some of these interpretations satisfy h c 
rather than g c   due to the partial match semantics  getinfodsspg  c  d  does not affect such
states  its match with the initial belief is partial 
clarification is also in order regarding our understanding of constants  first  like every pddllike planning formalism  we are aware of   we make a unique name assumption  i e   different
constants refer to different objects  second  our understanding of web services is that any output
they create is a separate individual  i e   a separate information object 
the latter directly raises the question why we allow actions to share output constants  the
answer is that we allow the planner to treat two objects as if they were the same  this makes
sense if the two objects play the same role in the plan  consider again example    the actions
getinfodsspg  c  d  and getinfodssph  c  d  share the same output constant  d  this means that
d is one name for two separate information objects  these two objects have the same properties 
derived from infodssp d   the only difference between them is that they are created in different
cases  namely from states that satisfy g c  and h c  respectively  having a single name for the
two objects is useful because we can take that name as a parameter of actions that do not need to
distinguish between the different cases  in the example  combineinfo d  e  f   is such an action 
as hinted  the cases in the above correspond to different classes of concrete execution traces 
importantly  on any particular execution trace  each output constant is created at most once  to see
this  consider an execution trace s    a    s    a            ak   sk     i e   an alternating sequence of states
and actions where s   b    and si    res si   ai   for all    i  k  say that ai and aj share
an output constant  d  say further that ai is applicable in si   and hence d  csi     then  quite
obviously  we have d  csl for all i      l  k      in particular  aj is not applicable in sj   the
intersection of its output constants with csj is non empty  cf  the definition of appl s  a    so  due
to our definition of action applicability  it can never happen that the same constant is created twice 
in other words  there can never be a reachable state where a single constant name refers to more
than one individual information object  in that sense  the use of one name for several objects occurs
only at planning time  when the actual execution trace  the actual case which will occur  is not
known  for illustration  consider getinfodsspg  c  d  and getinfodssph  c  d   and their shared
    an obvious generalization is to allow several conditional effects per action  in the style of the adl language  pednault         we omit this here for the sake of simplifying the discussion  an extension in this direction is straightforward 

  

fih offmann   b ertoli   h elmert   p istore

output d  in example    even if the concrete state s   b  in which the execution starts satisfies
both g c  and h c   only one of the actions will fire  namely the one that comes first 
we remark that we initially experimented with a definition where actions instantiate only their
inputs  and when they are applied to a state s their outputs are  by virtue of the execution semantics 
instantiated to constants outside of cs   in such a framework  one can never choose to share output
constants  i e   to use the same name for two different outputs  the notion we have settled for is
strictly richer  the planner can always choose to instantiate the outputs with constants outside of cs  
the question is  when does it make sense to share outputs  answering this question in a domainindependent planner may turn out to be quite non trivial  we get back to this when we discuss a
possible adaptation of cff in section      in the experiments reported herein  section     we use a
simple heuristic  outputs are shared iff the operator effects are identical  giving an indication that
the respective outputs may indeed play the same role in the plan  
we conclude this sub section with a final interesting observation regarding modeling in our
framework  negative effects are not an essential part of the wsc formalism  they can be compiled
away  we simply replace any negative effect g x            xk   with notg x            xk    introducing
a new predicate  and state in the integrity constraints that the two are equivalent  that is  we introduce the two new clauses x            xk   g x            xk    notg x            xk   and x            xk  
g x            xk    notg x            xk    while this is a simple compilation technique  the formal
details are a little intricate  and are moved to appendix a  if a is an action in the original task  then
a  denotes the corresponding action in the compiled task  and vice versa  similarly  if s is an action
in the original task  then s  denotes the corresponding state in the compiled task  we get 
proposition    compilation of negative effects in wsc  assume a wsc task  p  ic   o  c   
 
    g    let  p      
ic   o   c        g   be the same task but with negative effects compiled away 
assume an action sequence ha            an i  let b be the result of executing ha            an i in  p  ic  
 
 
 
 
o  c        g    and let b  be the result of executing ha 
            an i in  p   ic   o   c        g   
then  for any state s  we have that s  b iff s   b   
this can be proved by straightforward application of the relevant definitions  the most important aspect of the result is that the new clauses introduced are allowed in the forward effects and
strictly forward effects special cases identified later  hence  any hardness results transfer directly to
tasks without negative effects and dropping negative effects cannot make the algorithms any easier 
    computational properties
we now perform a brief complexity analysis of the wsc formalism in its most general form as
introduced above  in line with many related works of this kind  eiter   gottlob        bylander 
      liberatore        eiter et al          we consider the propositional case  in our context  this
means that we assume a fixed upper bound on the arity of predicates  on the number of input output
parameters of each operator  on the number of variables appearing in the goal  and on the number of
variables in any clause  we will refer to wsc tasks restricted in this way as wsc tasks with fixed
arity 
we consider the problems of checking plans  testing whether or not a given action sequence is a
plan  and of deciding plan existence  for the latter  we distinguish between polynomially bounded
plan existence  and unbounded plan existence  we deem these to be particularly relevant decision
problems in the context of plan generation  certainly  plan checks are an integral part of plan gen  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

eration  indeed  if a planning tool is based on state space search  then the tool either performs such
checks explicitly for  potentially many  plan candidates generated during search  or this complexity
is inherent in the effort that underlies the computation of state transitions  polynomially bounded
plan existence is relevant because  in most commonly used planning benchmark domains  plans are
of polynomial length  it is also a very wide spread intuition in the sws community that composed
web services will not contain exceedingly large numbers of web services   finally  unbounded plan
existence is the most general decision problem involved  and thus is of generic interest 
all the problems turn out to be very hard  to prove this  we reuse and adapt various results from
the literature  we start with the complexity of plan checking  for which hardness follows from a
long established result  eiter   gottlob        regarding the complexity of belief update  for all
the results  detailed proofs are available in appendix a 
theorem    plan checking in wsc  assume a wsc task with fixed arity  and a sequence
ha            an i of actions  it is p   complete to decide whether ha            an i is a plan 
proof sketch  membership can be shown by a guess and check argument  guess the proposition
values along ha            an i  then check whether these values comply with res  and lead to an
inconsistent action  or to a final state that does not satisfy the goal  ha            an i is a plan iff this is
not the case for any guess of proposition values  checking goal satisfaction is polynomial  checking
compliance with res is in conp  checking consistency is in np 
hardness follows by a simple adaptation of the proof of lemma     from eiter and gottlob
        that proof uses a reduction from checking validity of a qbf formula x y  x  y    the
lemma considers the case where a propositional belief  is updated with an arbitrary  propositional 
formula   and the decision problem is to ask whether some other formula  is implied by the
updated belief  in the proof   is a complete conjunction of literals  i e    corresponds to a single
world state   is a single propositional fact r which is true in   the semantics of x y  x  y  
are encoded in a complicated construction defining the update   in a nutshell   is a cnf telling
us that for every assignment to x  which will yield a world state s in the updated belief   we either
have to find an assignment to y so that  x  y   holds  completing s    or we have to falsify r 
the difference in our setting lies in our very restricted update formulas  action effects  and
in the fact that the integrity constraints are supposed to hold in every belief  we adapt the above
proof by  first  taking the integrity constraints to be the clauses in eiter and gottlobs cnf formula
  we then modify the constraints so that they need only be true if a new fact t holds  i e   we insert
t into every clause  the initial belief has t false  and otherwise corresponds exactly to  as above 
the only action of the plan makes t true  the goal is eiter and gottlobs fact r 
 
we remark that membership in theorem   remains valid when allowing actions with multiple
conditional effects  when allowing parallel actions  and even when allowing their combination  on
the other hand  by virtue of the proof argument as outlined  hardness holds even if the initial state
literals   are complete  describe a single world state   the plan consists of a single action with a
single positive effect literal  and the goal is a single propositional fact that is initially true 
we next consider polynomially bounded plan existence  for this  membership follows directly
from theorem    to prove hardness  we construct a planning task that extends eiter and gottlobs
construction from above with actions that allow to choose a valuation for a third  existentially quantified  set of variables  and hence reduces validity checking of a qbf formula x y z  x  y  z  

  

fih offmann   b ertoli   h elmert   p istore

theorem    polynomially bounded plan existence in wsc  assume a wsc task with fixed arity  and a natural number b in unary representation  it is p   complete to decide whether there exists
a plan of length at most b 
proof  for membership  guess a sequence of at most b actions  by theorem    we can check with
a p  oracle whether the sequence is a plan 
for hardness  validity of a qbf formula x y z  x  y  z   where  is in cnf  is reduced
to testing plan existence  say x    x            xn    in the planning task  there are n actions  operators
with empty input output parameters  oxi and oxi of which the former sets xi to true and the latter
sets xi to false  further  there is an action ot which corresponds to the action used in the hardness
proof of theorem    the actions are equipped with preconditions and effects ensuring that any
plan must first apply  for all    i  n  either oxi or oxi   and thereafter must apply ot  of course
enforcing the latter also requires a new goal fact that can be achieved only by ot    hence  choosing
a plan candidate in this task is the same as choosing a value assignment ax for the variables x 
in our construction  after all the oxi and oxi actions have been executed  one ends up in a belief
that contains a single world state  where the value assignment ax for the variables x corresponds
to the chosen actions  this world state basically corresponds to the belief  as in the hardness proof
of theorem    the only difference is that the construction has been extended to cater for the third
set of variables  this is straightforward  then  the belief that results from executing ot satisfies the
goal iff eiter and gottlobs fact r holds in all its world states  by virtue of similar arguments to
those of eiter and gottlob  the latter is the case iff y z  ax  x  y  z   i e   the substitution of
x y z  x  y  z  with ax   is valid  from this  the claim follows 
 
our final result regards unbounded plan existence in wsc  the result is relatively easy to
obtain from the generic reduction described by bylander        to prove pspace hardness of plan
existence in strips  somewhat shockingly  it turns out that plan existence in wsc is undecidable
even without any integrity constraints  and with a complete initial state description  the source of
undecidability is  of course  the ability to generate new constants on the fly 
theorem    unbounded plan existence in wsc  assume a wsc task  the decision problem
asking whether a plan exists is undecidable 
proof sketch  by a modification of the proof by bylander        that plan existence in propositional strips planning is pspace hard  the original proof proceeds by a generic reduction 
constructing a strips task for a turing machine with polynomially bounded space  the latter restriction is necessary to model the machines tape  tape cells are pre created for all positions within
the bound  exploiting the ability to create constants on the fly  we can instead introduce simple
operators that allow to extend the tape  at both ends 
 
not being able to decide plan existence is  of course  a significant limitation in principle  however  this limitation is probably of marginal importance in practice  because most planning tools
just assume that there is a plan  and they try to find it  rather than trying to prove that there is
no plan  in that sense  most planning tools are  by their nature  semi decision procedures anyway 
what matters more than decidability in such a setting is the question whether one can find a plan

  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

quickly enough  i e   before exhausting time or memory    this is also the most relevant question in
web service composition 

   forward effects
the high complexity of planning in wsc motivates the search for interesting special cases  we
define a special case  called forward effects  where every change an action makes to the state involves
a newly generated constant 
we start the section by defining the forward effects case and making a core observation about
its semantics  we then discuss the modeling power of this special case  next  we discuss forward effects from a more general perspective of belief update  we analyze the main computational
properties of forward effects  and we conclude the section with an assessment of how an existing
planning tool could be adapted to handle forward effects 
    wsc f wd and its semantics
the forward effects special case of wsc is defined as follows 
definition   assume a wsc task  p  ic   o  c        g    the task has forward effects iff 
   for all o  o  and for all l x   effo   we have x  yo     
   for all clauses   ic   where    x            xk   l   x         ln  xn    we have x   
     xn  
the set of all wsc tasks with forward effects is denoted with wsc f wd  
the first condition says that the variables of every effect literal contain at least one output variable  this implies that every ground effect literal of an action contains at least one new constant  the
second condition says that  within every integrity constraint  all literals share the same arguments 
this implies that effects involving new constants can only affect literals involving new constants 
note that  since x            xk are by definition exactly the variables occurring in any of the literals 
for each xi we have xi   x            xk   note further that we may have k      i e   the literals
in the clause may be ground  this is intentional  the constants mentioned in the clause must be
taken from c    cf  the discussion in section      therefore  such clauses have no interaction with
statements about the new constants generated by a wsc f wd action 
we will discuss the modeling power of wsc f wd below  section       first  we observe that
the semantics of wsc f wd is much simpler than that of general wsc  one no longer needs the
notion of minimal change with respect to the previous state  to state this more precisely  assume a

wsc task with predicates p  say i  is an interpretation over p c   where c  is a set of constants 
say that c  c    we denote by i   c the restriction of i  to p c   i e   the interpretation of p c that
coincides with i  on all these propositions  given a state s and an action a  we define 

  c    i      c    cs  ea   i   cs   is   i     ic  effa   appl s  a 
   
res f wd  s  a    
 s 
otherwise
    indeed the planning community is generally rather unconcerned by undecidability  cf  the numeric track of the international planning competitions  and helmerts        results on the decidability of numerical planning problems 

  

fih offmann   b ertoli   h elmert   p istore

compare this to equation      where i  is defined to be a member of update s  c    ic  effa   
which returns all interpretations that satisfy ic  effa and that differ minimally from is   in equation      i  is simply set to be identical to is   on the constants  on the propositions over the constants 
that existed beforehand  in other words  the set of new states we get is the cross product of the old
state with all satisfying assignments to ic  effa  
lemma    semantics of wsc f wd   assume a wsc f wd task  a reachable state s  and an action
a  then res s  a    res f wd  s  a  
proof sketch  in wsc f wd   if s differs minimally from s  then it follows that s agrees totally with
s  on the set of propositions p cs interpreted by s  to see this  denote as before with p cs  ea the
set of all propositions with arguments in cs  ea   and with at least one argument in ea   and denote
with ic  cs   ea   the instantiation of ic with all constants from cs  ea   where in each clause
at least one variable is instantiated from ea   the key argument is that s    ic  effa is equivalent
to s    ic  cs  ea    effa   which in turn is equivalent to s    ic  cs    ic  cs   ea    effa  
in the last formula  ic  cs   only uses the propositions p cs   whereas ic  cs   ea    effa only
uses the propositions p cs  ea   since s is reachable  we have s    ic  cs    therefore  to satisfy
ic  effa   there is no need to change any of the values assigned by s 
 
    modeling power
intuitively  wsc f wd covers the situation where a web service outputs some new constants  sets
their characteristic properties relative to the inputs  and relies on the ontology axioms to describe any
ramifications concerning the new constants  as was detailed in section    this closely corresponds
to the various notions of message based wsc explored in the literature  in that sense  the modeling
power of wsc f wd is comparable to that of message based wsc  one of the most widespread
approaches in the area 
a simple concrete way of assessing the modeling power of wsc f wd is to consider the allowed
and disallowed axioms  examples of axioms that are not allowed by wsc f wd are  attribute domain
restrictions  taking the form x  y   g x  y   h x   attribute range restrictions  taking the form
x  y   g x  y   h y   and relation transitivity  taking the form x  y  z   g x  y   g y  z  
g x  z   note that  for all these axioms  it is easy to construct a case where an action effect  even
though it involves a new constant  affects the old belief  for example  if constants c and e existed
beforehand  and an action outputs d and sets g c  d   g d  e   then the axiom x  y   g x  y  
g y  z   g x  z  infers that g c  e   a statement that does not involve the new constant d 
typical ontology axioms that are allowed by wsc f wd are  subsumption relations  taking
the form x   g x   h y   mutual exclusion  taking the form x   g x   h y   relation reflexivity  taking the form x   g x  x   and relation symmetry  taking the form x  y  
g x  y   g y  x   we can also express that a concept g is contained in the union of concepts
h            hn   and more generally we can express any complex dependencies between concepts  taking the form of clausal constraints on the allowed combinations of concept memberships 
one example where complex dependencies are important is the domain of proteins as illustrated
in example    capturing the dependencies is important here in order to be able to select the correct web services  similar situations arise in many domains that involve complex interdependencies
and or complex regulations  an example for the latter is the virtual travel agency which we discussed before  for example  in the german rail system there are all kinds of regulations regarding
  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

which train may be booked with which kind of discount under which conditions  modeling these
regulations would enable a wsc algorithm to select the appropriate booking services  another interesting case is the hospital domain described by de jonge  van der linden  and willems        
there  the problem of hospital asset tracking is handled by means of a set of tracking  logging and
filter services  which transform logs to extract various kinds of information  in this setting  it would
make sense to model complex dependencies so that the web service composer may determine which
hospital assets need to be tracked and retrieved  namely  the latter depends on the type of operation
in question  and on the kind of examinations which that operation requires  accordingly  what we
need to model is a categorization of operations  their mapping to sets of required examinations  and
how those examinations are associated with hospital assets  further complications arise since the
required examinations assets may depend on particular circumstances  clearly  we can express the
categorization and dependencies in terms of clauses  while this of course captures only a fraction
of what is relevant in a hospital  it is considerably more informed than a composer which always
just tracks all the assets 
the main weakness of wsc f wd is that it does not allow us to express changes regarding preexisting objects  this is best illustrated when considering the case of negative effects    in the
planning community  these are commonly used to model how previous properties of objects are
invalidated by an action  for illustration  reconsider example    say there is an additional operator
dropcoffeein dmachine  with effect info d y   one would normally expect that  when this
operator is applied  the fact info d y  is deleted and must be re established  this is not so in
wsc f wd   according to the restrictions this special case imposes  the variable y in info d y 
must be an output of dropcoffeein dmachine  that is  dropping coffee into the machine creates a
new object  whose characteristic property happens to be info d y  rather than info d y   clearly 
this is not the intended semantics of the operator 
to model the intended semantics  we would need to instantiate y with a pre existing constant 
say that  as in belief b  in example    a constant e with info d e  was previously created by
getinfo d n    c  e   then wsc f wd does allow us to instantiate dropcoffeein dmachine with
e  so that we have the effect info d e   however  by virtue of the definition of action applicability 
that action will be applicable only in states where e does not yet exist  corresponding to execution
paths where getinfo d n    c  e  was not executed  hence the property info d e  does not get
deleted from any state  and e as used by dropcoffeein dmachine is still regarded as a newly
created object whose characteristic property is info d y   the only difference the new action
makes is that  now  the plan uses the same name  e  to refer to two different information objects
 output of getinfo d n    c  e  vs  output of dropcoffeein dmachine  that do not play the same
role in the plan  cf  the discussion in section     
an interesting workaround is to let the operators output time steps  in a spirit reminiscent of
the situation calculus  mccarthy   hayes        reiter         every operator obtains an extra
output variable t  which is included into every effect literal  the new time step t is stated to stand
in some relation to the previous time steps  e g   next tprev  t  where tprev is an input variable
instantiated to the previous time step  in such a setting  we can state how the world changes over
time  in particular we can state that some object property is different in t than in tprev  for
example  if an action moves a file f from raedme to readme then we could state that
name f  raedme  tprev  and name f  readme  t   the problem with such a construction
    or  in wsc  positive effects triggering negative effects via ic   cf  proposition   

  

fih offmann   b ertoli   h elmert   p istore

is that the time steps have no special interpretation  they are just ordinary objects    this causes at
least two difficulties      if we want to refer to an object property  we have to know the time step
in the first place  that is  we have to know whether the actual time step is t or tprev  note here
that we cannot maintain a predicate actualtime x  because this would require us to invalidate a
property of tprev      there is no solution to the frame problem  the operators must explicitly state
every relevant property of the previous time step  and how each property is changed in the new time
step   
to conclude this sub section  let us consider how wsc f wd can be generalized without losing
lemma    most importantly  instead of requiring that every effect literal involves a new constant 
one can postulate this only for literals that may actually be affected by the integrity constraints  in
particular  if a predicate does not appear in any of the clauses  then certainly an effect literal on
that predicate is not harmful even if it does not involve an output constant  one obtains a potentially stronger notion by considering ground literals  rather than predicates  note that this kind of
generalization solves difficulty     of the time step construction  presuming that time steps are not
constrained by the clauses   the frame problem  however  persists  
another possibility  deviating somewhat from the way wsc and wsc f wd are currently defined  is to define the integrity constraints in terms of logic programming style rules  along the lines
of eiter et al                the requirement on wsc f wd can then be relaxed to postulate that the
effect literals without new constants do not appear in the rule heads 
we remark that the latter observation suggests a certain strategic similarity with the aforementioned derived predicates  thiebaux et al         previously used in ai planning to manage the
complexity of integrity constraints  there  the integrity constraints take the form of stratified logic
programming style derivation rules  and the predicates appearing in rule heads are not allowed to
appear in operator effects  this is an overly restricted solution  in the wsc context  the effects
of web services are indeed very likely to affect concepts and relations appearing in the ontology
axioms  they may do so in wsc f wd   as long as output constants are involved 
    belief update
lemma   is specific to the possible models approach  winslett        that underlies our semantics
of action applications  it is interesting to consider the semantics of wsc f wd from a more general
perspective of belief update  recall that such an update involves a formula characterizing the current
belief  and a formula describing the update  we seek a formula that characterizes the updated belief 
a wide variety of definitions has been proposed as to how the updated belief should be defined 
however  some common ground exists  katzuno and mendelzon        suggest eight postulates 
named  u          u    which every sensible belief update operation should satisfy  herzig and rifi
       discuss in detail to what degree the postulates are satisfied by a wide range of alternative
belief update operators  in particular they call a postulate uncontroversial if all update operators
under investigation satisfy them  we will take up these results in the following  we examine to what
extent we can draw conclusions about the updated belief     in the setting of the forward effects
case  when relying only on herzig and rifis uncontroversial postulates 
    note that here the similarity to the situation calculus ends  whereas time steps are assigned a specific role in the
formulas used in the situation calculus  here they are just ordinary objects handled by actions  as if they were packages
or blocks 
    despite these difficulties  theorem   below shows that a time step construction can be used to simulate an abacus
machine  and hence to prove undecidability of plan existence in wsc f wd  

  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

we assume that a planning task with predicates p is given  we need the following notations 
 if  and  are formulas  then    denotes the formula that results from updating the belief
 with the update   under some semantics for the belief update operator  
 given disjoint sets of constants c and e  p c e denotes the set of all propositions formed
from predicates in p  where all arguments are contained in c  e and there exists at least one
argument contained in e   recall that p c denotes the set of all propositions formed from
predicates in p and arguments from c  
 given a set of constants c  ic  c  denotes the instantiation of ic with c  that is  ic  c 
is the conjunction of all clauses that result from replacing the variables of a clause   ic  
   x            xk   l   x         ln  xn    with a tuple  c            ck   of constants in c 
 given disjoint sets of constants c and e  ic  c   e  is the conjunction of all clauses that
result from replacing the variables of a clause   ic      x            xk   l   x        
ln  xn    with a tuple  c            ck   of constants in c  e  where at least one constant is taken
from e   
 if  is a ground formula then by p    we denote the set of propositions occurring in  
we will denote the current belief by  and the update by   as another convention  given a set of
constants c  by writing  c we indicate that p     p c   similarly  given disjoint sets of constants
c and e  by writing  c e we indicate that p     p c e   if s is a state  then by s we denote
the conjunction of literals satisfied by s 
we first consider the case where  similar to the claim of lemma     corresponds to a single
concrete world state s  we want to apply an action a  we wish to characterize the set of states
res s  a   i e   we wish to construct the formula     for simplicity of notation  denote c    cs
and e    ea   if a is not applicable to s  there is nothing to do  otherwise  we have that 
 i    ic  c    c where p   c    p c  
for example  we can set  c    s   since s    ic   we get the desired equivalence  further  we
have that 
 iia    ic  c   ic  c   e   effa  
 iib  p  ic  c   e    p c e and p  effa    p c e  
 iia  holds trivially   is defined as ic  effa   which is equivalent to ic  c  e   effa which is
equivalent to ic  c   ic  c   e   effa   as for  iib   this is a consequence of the forward effects
case  every effect literal contains at least one output constant  hence effa contains only propositions
from p c e   for ic  c   e   we have that at least one variable in each clause is instantiated with
a constant e  e  since  by definition  all literals in the clause share the same variables  e appears
in every literal and therefore ic  c   e  contains only propositions from p c e  
as an illustration  consider our simple vta example  there are four predicates  train x  
ticket x   trainticket x   and ticketfor  x  y   the set of integrity constraints ic consists of
    if no clause in ic contains any variable  then ic  c   e  is empty  as is customary  an empty conjunction is
taken to be true  i e     

  

fih offmann   b ertoli   h elmert   p istore

the single axiom x   trainticket x   ticket x   in our current state s  we have cs    c  
and is sets all propositions to   except for train c   we consider the application of the action
a   bookticket c  d   whose precondition is train c   whose set e of output constants is  d  
and whose effect effa is trainticket d   ticketfor  d  c   in this setting  we have  ic  c   
 trainticket c   ticket c     c    train c ticket c trainticket c ticketfor  c  c   
and ic  c   e     trainticket d   ticket d   
we will derive in the following that 
 iii       ic  c    c     ic  c   e   effa   
that is  we can characterize the updated belief simply by the conjunction of the previous belief
with the action effect and the extended instantiation of the ontology axioms  this corresponds
exactly to lemma    to illustrate  we will continue the vta example  the left hand side of  iii 
refers to the four propositions based only on c  and sets them according to s  the right hand side
refers to propositions based only on d  trainticket d  and ticket d   as well as the proposition
ticketfor  d  c  which links c and d 
as one prerequisite of our derivation of  iii   we have to make an assumption which  to the best
of our knowledge  is not discussed anywhere in the belief update literature 
 iv  let               be formulas where p       p          p       p          p      
p          and p       p          then                                    
this assumption postulates that formulas talking about disjoint sets of variables can be updated
separately  since formulas with disjoint variables essentially speak about different aspects of the
world  this seems a reasonable assumption 
now  we start from the formula     we make replacements according to  i  and  iia   leading
to the equivalent formula  ic  c    c     ic  c   ic  c   e   effa    we can map this
formula onto  iv  by taking   to be ic  c    c     to be      to be ic  c   and   to be
ic  c   e   effa   hence  we can separate our update into two parts as follows 
 a      c     ic  c    c    ic  c 
 b      c e        ic  c   e   effa  
according to  iv   we then obtain our desired formula    by         c      c e  
illustrating this with the vta example  we simply separate the parts of the update that talk
only about c from those that talk only about d or the combination of both constants  the  a  part
of the update is trainticket c   ticket c  conjoined with s   updated with trainticket c  
ticket c   the  b  part of the update is    representing the  empty  statement that the previous state
s makes about d  updated with  trainticket d   ticket d    trainticket d   ticketfor  d  c  
it remains to examine     c and     c e   we need to prove that 
 c      c  ic  c    c   and
 d      c e  ic  c   e   effa  
essentially  this means to prove that   c  updating a formula with something it already implies does
not incur any changes   d  updating   with some formula yields a belief equivalent to that formula 
to see this  compare  a  with  c  and  b  with  d  
  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

while these two statements may sound quite trivial  it is in fact far from trivial to prove them for
the wide variety of  partly rather complex  belief update operations in the literature  here we build
on the works by katzuno and mendelzon        and herzig and rifi         we need two of the
postulates made by katzuno and mendelzon         namely 
 u   for any   and                 
 u   for any   and     if      then             
herzig and rifi        prove that  u   is uncontroversial  meaning it is satisfied by all belief
update operators they investigated  cf  above   they also prove that  u   is equivalent to the conjunction of two weaker statements  of which only one is uncontroversial  namely 
 u a  for any   and                      
the other statement is not uncontroversial  however  it is proved to be satisfied by all non causal
update operators under investigation  except the so called winsletts standard semantics  winslett 
       the latter semantics is not useful in our context anyway  the only restriction it makes on
the states in res s  a  is that they differ from s only on the propositions mentioned in the update
formula  in our case  these include all propositions appearing in ic  c  e   which is bound to be
quite a lot  so  if we were to use winsletts standard semantics  then res s  a  would be likely to
retain hardly any information from s 
consider now the formula     c as specified in  a       c    ic  c    c    ic  c  
we will now prove  c   this is indeed quite simple  we have that  ic  c    c    ic  c  
so we can instantiate   in  u   with ic  c    c   and   in  u   with ic  c   we obtain
 ic  c    c    ic  ic  c    c   and hence     c  ic  c    c as desired  with
what was said above  this result is not uncontroversial  but holds for all non causal update operators
 except winsletts standard semantics  investigated by herzig and rifi         in terms of the vta
example   u   allowed us to conclude that the update trainticket c   ticket c  does not make
any change to the previous belief  which already contains that property 
next  consider the formula   c e as specified in  b     c e     ic  c  e effa   
we now prove  d   by postulate  u    we get that     c e  ic  c   e   effa   because
ic  c  e effa is the update formula     for the other direction  we exploit  u a   we instantiate
  in  u a  with    and get that     ic  c   e   effa        ic  c   e   effa    which is the
same as     ic  c   e   effa        c e   which is equivalent to ic  c   e   effa 
    c e   this proves the claim  note that we have used only postulates that are uncontroversial
according to herzig and rifi         reconsidering the vta example  we have ic  c  e effa  
 trainticket d   ticket d    trainticket d   ticketfor  d  c   the previous state does not say
anything about these propositions  and is thus represented as    the postulates allow us to conclude
that  for all belief update operators investigated by herzig   rifi        the resulting belief will be
equivalent to  trainticket d   ticket d    trainticket d   ticketfor  d  c  
so far  we were restricted to the case where   the belief to be updated  corresponds to a single
world state s  consider now the more general case where  characterizes a belief b  and we want
to characterize the set of states res b  a   at first glance  it seems that not much changes  because
katzuno and mendelzon        also make this following postulate 
 u   for any         and                           

  

fih offmann   b ertoli   h elmert   p istore

this means that  if  consists of two alternate parts  then updating  is the same as taking the union
of the updated parts  in other words  we can compute the update on a state by state basis  the
statement  i  from above is still true  its just that now  c is the disjunction over s for all states
s  b  rather than only the single s   the rest of the argumentation stays exactly the same  herzig
and rifi        prove that  u   is uncontroversial and leave it at that 
however  matters are not that simple  the source of complications is our use of a partial
matches conditional effects semantics  the update formula  is different for the individual states
s  b  hence we cannot directly apply  u    obviously  states s   b where a is applicable are updated differently from states s   b where a is not applicable  the latter are not updated at all    a
somewhat more subtle distinction between states in b is which constants exist in them  for different
sets of constants  the integrity constraints in the update are different  hence  to obtain a generic
update of   we have to split  into equivalence classes             n where the states within each
i cannot be distinguished based on prea and based on the existing constants  then   u   and the
argumentation from above can be used to show the equivalent of  iii  for each i   the last step 
defining the final    to be the disjunction of the individual i  i   appears sensible  but it does
not follow immediately from katzuno and mendelzon        
for illustration  consider a variant of the vta example where we have two preceding states  one
state s where we have train c  as before  and a new state s where we have ticket c  instead  in s  
bookticket c  d  is not applicable  and hence the update is different for s and s   the s part is as
above  yielding the result s   trainticket d   ticket d    trainticket d   ticketfor  d  c  
the update to s is trivial  and yields s as its result  the final outcome is the disjunction of these
two beliefs 
we point out that the situation is much easier if we consider plug in matches  i e   forced preconditions  instead of partial matches  there  a is applicable to all states  and it is also easy to
see that every state in b has the same constants  therefore  for plug in matches   iii  follows immediately with  u    in the above vta example  an update would not be computed at all since
bookticket c  d  would not be considered to be applicable to the preceding belief  if s satisfies
train c  but disagrees in some other aspect  e g   quite nonsensically  that also ticket c  holds  then
the updated belief is equivalent to  s  s     trainticket d   ticket d    trainticket d  
ticketfor  d  c  
    computational properties
paralleling our analysis for general wsc from section      we now perform a brief complexity
analysis of the wsc f wd special case  as before  we consider the propositional case which
assumes a fixed upper bound on the arity of predicates  on the number of input output parameters
of each operator  on the number of variables appearing in the goal  and on the number of variables
in any clause  also as before  we consider the decision problems of checking plans  of deciding
polynomially bounded plan existence  and of deciding unbounded plan existence  in that order 
in contrast to before  we cannot reuse results from the literature as much because  of course  the
particular circumstances of wsc f wd have not been investigated before  we include proof sketches
here  and refer to appendix a for the detailed proofs 
    one might speculate that the common update would be prea    but that is not the case  for example  under the
possible models approach that we adopt in wsc  updating s where s    prea with prea   gives rise to result
states that change s to violate prea instead of changing it to satisfy  

  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

thanks to the simpler semantics as per lemma    plan checking is much easier in wsc f wd
than in wsc 
theorem    plan checking in wsc f wd   assume a wsc f wd task with fixed arity  and a sequence ha            an i of actions  it is conp complete to decide whether ha            an i is a plan 
proof sketch  hardness is obvious  considering an empty sequence  membership can be shown
by a guess and check argument  say c is the union of c  and all output constants appearing in
ha            an i  we guess an interpretation i of all propositions over p and c  further  for each
   t  n  we guess a set ct of constants  i needs not be time stamped because  once an action has
generated its outputs  the properties of the respective propositions remain fixed forever  thanks to
lemma    we can check in polynomial time whether  a  i and the ct correspond to an execution of
ha            an i  also  we can check in polynomial time whether  b  i and cn satisfy g   ha            an i
is a plan iff there is no guess where the answer to  a  is yes and the answer to  b  is no 
 
membership in theorem   remains valid when allowing parallel actions and multiple conditional effects  provided one imposes restrictions ensuring that the effects actions applied simultaneously  in one step  can never be self contradictory  otherwise  checking plans also involves a
consistency test for each plan step  which is an np complete problem  note that it is quite reasonable to demand that simultaneous actions effects do not contradict each other  widely used
restrictions imposed to ensure this are mutually exclusive effect conditions  and or non conflicting
sets of effect literals 
we next consider polynomially bounded plan existence  membership follows directly from theorem    to prove hardness  we reduce from validity checking of a qbf formula x y  x  y   
the constructed planning task allows to choose values for x  and thereafter to apply actions evaluating  for arbitrary values of y   the goal is accomplished iff a setting for x exists that works for
all y  
theorem    polynomially bounded plan existence in wsc f wd   assume a wsc f wd task with
fixed arity  and a natural number b in unary representation  it is p   complete to decide whether
there exists a plan of length at most b 
proof sketch  for membership  guess a sequence of at most b actions  by theorem    we can
check with an np oracle whether the sequence is a plan 
hardness can be proved by reduction from
wk validity checking of a qbf formula x y  x  y  
where  is in dnf normal form  i e      j   j   the key idea is to use outputs for the creation
of time steps  and hence ensure that the operators adhere to the restrictions of wsc f wd   setting
xi is allowed only at time step i  that is  for each xi we have operators oxi   and oxi     these take
as input a set of time steps  t            ti    which are required to be successive  by the precondition
start t    next t    t         next ti    ti     they output a new time step ti which they attach
as a successor of ti    and they set xi to   and    respectively  at time step i  that is  they have an
effect literal of the form xi  ti   and xi  ti    respectively  the rest of the planning task consists of 
operators ot that allow extending a sequence of time steps until step b  for a suitable value b  see
below   and of operators oj which allow achieving the goal  given j is true at the end of a time
step sequence of length b  there are no integrity constraints  ic is empty   the values of the yi
are not specified  i e   those variables can take on any value in the initial belief 

  

fih offmann   b ertoli   h elmert   p istore

if x y  x  y   is valid then obviously one can construct a plan for the task simply by setting
the xi accordingly  using the ot for stepping on to time b  and applying all the oj   what necessitates
our complicated construction is the other direction of the proof  namely  the plan may cheat by
setting a xi to both   and    the construction ensures that this is costly  because such a plan is
forced to maintain two parallel sequences of time steps  starting from the faulty xi   we can choose a
sufficiently large value for b  together with a sufficiently small plan length bound b  so that cheating
is not possible 
 
our final result regards unbounded plan existence  somewhat surprisingly  it turns out that this
is still undecidable in wsc f wd   similar to the above  the key idea again is to let actions output a
new time step  thereby ensuring membership of the constructed task in wsc f wd  
theorem    unbounded plan existence in wsc f wd   assume a wsc f wd task  the decision
problem asking whether a plan exists is undecidable 
proof sketch  by reduction from the halting problem for abacus machines  which is undecidable 
an abacus machine consists of a tuple of integer variables v            vk  ranging over all positive
integers including     and a tuple of instructions i            in   a state is given by the content of
v            vk plus the index pc of the active instruction  the machine stops iff it reaches a state where
pc   n  all vi are initially    and pc is initially    the instructions either increment a variable
and jump to another instruction  or they decrement a variable and jump to different instructions
depending on whether or not the variable was already   
it is not difficult to encode an abacus machine as a wsc f wd task  the two key ideas are     
design an operator that outputs the next successor to an integer      design operators simulating
the instructions  by stepping to successors or predecessors of integer values  in the latter kind of
operators  membership in wsc f wd is ensured by letting the operators output a new time step to
which the new variable values are associated  the goal asks for the existence of a time step where
the active instruction is in  
 
as argued at the end of section     already  we dont deem undecidability of unbounded plan
existence a critical issue in practice  most planning tools are by nature semi decision procedures 
anyway  in particular  web service composition is typically expected to occur in a real time setting 
where severe time outs apply 
    issues in adapting cff
in our view  the most crucial observation about wsc f wd is that we can now test plans in conp 
rather than in p  as for general wsc  standard notions of planning under uncertainty have the same
complexity of plan testing  and research has already resulted in a sizable number of approaches and
 comparatively  scalable tools  cimatti et al         bryce et al         hoffmann   brafman 
      palacios   geffner         we will show in the next section that  under certain additional
restrictions on wsc f wd   these tools can be applied off the shelf  regarding general wsc f wd   the
match in the complexity of plan testing suggests that the underlying techniques can be successfully
adapted  in the following  we consider in some detail the cff tool  hoffmann   brafman        
other promising options would be to extend mbp  cimatti et al         or pond  bryce et al  
       or to look into the compilation techniques investigated by palacios and geffner        
cff can be characterized as follows 
  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

    search is performed forward in the space of action sequences 
    for each sequence a  a cnf formula  a  is generated that encodes the semantics of a  and
sat reasoning over  a  checks whether a is a plan 
    some reasoning results  namely the literals that are always true after executing a  are cached
to speed up future tests 
    search is guided by an adaptation of ffs  hoffmann   nebel        relaxed plan heuristic 
    relaxed planning makes use of a strengthened variant of the cnf formulas  a  used for
reasoning about action sequences  where most of the clauses are projected onto only   of
their literals  i e   all but   of the literals are removed from each respective clause  
all of these techniques should be self explanatory  except possibly the last one  projecting the cnf
formulas ensures that the relaxed planning remains an over approximation of the real planning 
because the projected formulas allow us to draw more conclusions  at the same time  the projected
formulas can be handled sufficiently runtime efficiently    the method for   projecting most of
the clauses is  in a nutshell  to ignore all but one of the condition literals of each conditional effect
in the relaxed planning graph 
it is fairly obvious that the basic answers given by cff  i e   the techniques           also apply
in wsc f wd   note that  indeed  the main enabling factor here is that we can check plans in conp 
rather than in p  as for general wsc  this enables us to design the desired cnf formulas  a 
in a straightforward fashion  if plan checking is p   hard  then we either need to replace the cnf
formulas with qbf formulas  or we have to create worst case exponentially large cnf formulas 
both are  at the least  technically quite challenging 
the adaptation of cff to wsc f wd is of more immediate promise  but is not trivial  it involves
technical challenges regarding the on the fly creation of constants as well as the computation of
the heuristic function  the latter also brings significant new opportunities in the wsc context 
pertaining to the exploitation of typical forms of ontology axioms  let us consider these issues in a
little detail 
first  like most of todays planning tools  cff pre instantiates pddl into a purely propositional
representation  based on which the core planning algorithms are implemented  if one allows on thefly creation of constants  then pre instantiation is no longer possible  and hence the adaptation to
wsc f wd involves re implementing the entire tool  while this is a challenge in itself  there are
more difficult obstacles to overcome  a sloppy formulation of the key question is  how many
constants should we create  one can  of course  create a new tuple of constants for  the outputs of 
each and every new action application  however  it seems likely that such an approach would blow
up the representation size very quickly  and would hence be infeasible  so one should instead share
output constants where reasonable  but how does one recognize the reasonable points  this issue
is especially urgent inside the heuristic function  namely  it is easy to see that  in the worst case 
the relaxed planning graph grows exponentially in the number of layers  just imagine an example
where web service w  takes an input of type a and generates an output of type b  whereas w  takes
an input of type b and generates an output of type a  starting with one constant of type a and
one of type b  we get   constants of each type in the next graph layer  then  each of w  and w 
    inside the heuristic function  the formulas come from relaxed planning graphs which can be quite big  so handling
them without further approximations seems hopeless  this is discussed in detail by hoffmann and brafman        

  

fih offmann   b ertoli   h elmert   p istore

can be applied two times  and we get   constants of each type in the next graph layer  and so forth 
this dilemma probably cannot be handled without making further approximations in the relaxed
planning graph 
one a more positive note  it seems possible to exploit the most typical structures of ontologies
in practice  in particular  most practical ontologies make extensive use of subsumption relations 
structuring the domain of interest into a concept hierarchy  additional ontology axioms often come
in the form of constraints on relations  reflexivity  symmetry  transitivity  or on the typing or number
of relation arguments  it may make sense to exploit some of these structures for optimizing the
formulas  a  and the associated sat reasoning  certainly  it makes sense to exploit these structures
inside the heuristic function  one can include specialized analysis and sub solver techniques that
recognize these structures and solve them separately in order to obtain more precise relaxed plans 
one can even try to take into account only these structures inside the relaxed planning  and hence
 potentially  obtain a very fast heuristic function 

   compilation to initial state uncertainty
we now show that  under certain additional restrictions  off the shelf scalable tools for planning
under uncertainty can be exploited to solve wsc f wd   the main limiting factors are      these
tools do not allow the generation of new constants      these tools allow the specification of a
clausal formula only for the initial state  not for all states  our approach to deal with     considers
a set of constants fixed a priori  namely the initially available constants plus additional potential
constants that can be used to instantiate outputs  our more subtle observation is that  within a special
case of wsc f wd   where the dynamics of states become predictable a priori  one can also deal with
    in a natural way 
in what follows  we first introduce our core observation of a case where the state space becomes
predictable  in a certain sense  we then observe that predictability is naturally given in a special
case of forward effects  which we term strictly forward effects  we discuss the strengths and limitations of this new special case  we finally provide a compilation of strictly forward effects into
planning under initial state uncertainty 
    predictable state spaces
our core observation is based on a notion of compatible actions  assume a wsc f wd task  p  ic  
o  c        g    two actions a  a are compatible if either ea  ea     or effa   effa   that is 
a and a either have disjunct outputs  and hence affect disjunct sets of literals since we are in
wsc f wd  or their effects agree completely  a set a of actions is compatible if ea  c     for
all a  a  and every pair of actions in a is compatible 
lemma   states that  given the used actions are compatible  every state that can ever be reached
satisfies all action effects  modulo the existing constants 
lemma    predictable state spaces in wsc f wd   assume a wsc f wd task  a compatible set of
actions a  and a state s that can be reached with actions from a  then s      and  for all a  a 
if ea  cs then s    effa  
proof  the proof is by induction  in the base case  for s  b    the claim holds by definition since
cs  ea    for all a  a  say s is reached from s by an action a  a  if a is not applicable

  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

to s  with induction assumption there is nothing to prove  otherwise  because we are in wsc f wd  
by lemma   we have that res s  a      c    i      c    cs  ea   i   cs   is   s    ic  effa   
with v
induction assumption applied to s  we have res s  a      c    i      c    cs  ea   s   
   a a e  cs effa  ic  effa    now  if any a  a has ea  cs  ea but ea   cs   then
a
 
we have ea  ea     and hence effa   effa by prerequisite  this concludes the argument 
by virtue of this lemma  the possible configurations of all constants
that can be generated by
v
actions from a are characterized by the formula ic     aa effa   since all parts of this
formula are known prior to planning  the set of possible configurations is predictable  before
we even begin to plan  we already know how the constants will behave if they are generated  so
we can list the possible behaviors of all potential constants in our initial belief  and let the actions
affect only those constants which actually exist  in other words  we can compile into initial state
uncertainty  we will detail this further below  first  we need to identify a setting in which lemma  
can actually be applied 
    strictly forward effects
given a wsc f wd task  we must settle for a finite set a of compatible actions that the planner
should try to compose the plan from  one option is to simply require every action to have its own
unique output constants  this appears undesirable since planning tasks often contain many actions 
and so the set of potential constants would be huge  further  to enable chaining over several actions 
the potential constants should be allowed to instantiate the input parameters of every operator  hence
necessitating the creation of a new action and  with that  more new potential constants  it is unclear
where to break this recursion  in a sensible way 
herein  we focus instead on a restriction of wsc f wd where it suffices to assign unique output
constants to individual operators  rather than to individual actions 
definition   assume a wsc task  p  ic   o  c        g    the task has strictly forward effects
iff 
   for all o  o  and for all l x   effo   we have  x      and x  yo  
   for all clauses   ic   where    x            xk   l   x         ln  xn    we have x   
     xn  
the set of all wsc tasks with strictly forward effects is denoted with wsc sf wd  
the second condition is identical to the corresponding condition for wsc f wd   the first condition is strictly stronger  while wsc f wd requires that at least one effect literal variable is taken
from the outputs  wsc sf wd requires that all these variables are taken from the outputs  therefore 
obviously  wsc sf wd  wsc f wd   note that the wsc task formulated in example   is a member
of wsc sf wd  
the key property of wsc sf wd is that  without input variables in the effect  all actions based
on the operator will have the same effect  so  for the action set to be compatible  all we need is to
choose a set of unique output constants for every operator  indeed  we can do so for every set of
operators whose effects are pairwise identical  we can also choose several sets of output constants
for each such group of operators 
  

fih offmann   b ertoli   h elmert   p istore

    modeling power
the limitations of wsc f wd   discussed in section      are naturally inherited by wsc sf wd   moreover  unlike wsc f wd   we cannot state any properties in the effect that connect the inputs to the
outputs  this is a serious limitation  for illustration  consider the small vta example we have
been using  the operator bookticket has an effect ticketfor  y  x   relating the produced ticket y
to the train x given as input  clearly  the notion of a ticket is rather weak if we cannot state
what the ticket is actually valid for  another interesting case is the one where we extend example   by considering two proteins rather than just one  that is  we set c     c  c       
cellprotein c   cellprotein c    we wish to encode that we need the combined presentation for both
of those  i e   g   y   combinedpresentation y  c   combinedpresentation y  c    in wsc f wd  
we can solve this by including  for every information providing operator  the input variable x into
the effect literal  for example  we set getinfo d n        x    n   x    y   info d y  x    this is
not possible in wsc sf wd  
to some extent  these difficulties can be overcome by encoding the relevant inputs into predicate names  to handle composition for the two proteins c and c   this would essentially mean
making a copy of the entire model and renaming the part for c   the goal would be g   y  y   
combinedpresentation y   combinedpresentation  y     and the operator preconditions would make
sure that combinedpresentation y  is generated as before  while combinedpresentation  y    is generated using the new operators  note that this a rather dirty hack  and that it depends on knowing the
number of copies needed  prior to planning  the equivalent solution for the vta would introduce
a separate ticketfor x predicate for every entity x for which a ticket may be bought  at the very
least  this would result in a rather oversized and unreadable model  a yet more troublesome case is
the time step construction outlined in section      where we added a new output variable t into each
effect and related that via an effect literal next prevt  t  to a previous time step prevt provided as
input  in wsc sf wd   we can no longer relate t to prevt so there is no way of stating which time
step happens after which other one  trying to encode this information into predicate names  we
would have to include one predicate per possible time step  this necessitates assuming a bound on
the number of time steps  a clear limitation with respect to the more natural encoding 
despite the above  wsc sf wd is far from a pathological and irrelevant special case  an example
where it applies is the domain of proteins as shown in example    similarly  the hospital domain
discussed in section     can be naturally modeled in wsc sf wd   more generally  there is in fact a
wealth of wsc formalisms which do not encode any connections between inputs and outputs  for
example  that category contains all formalisms which rely exclusively on specifying the types of
input and output parameters  the information modeled with such types is only what kind of input
a service requires  and what kind of output it produces  for example  input is a train and output
is a ticket  examples of such formalisms are various notions of message based composition  zhan
et al         constantinescu et al       a  lecue   leger        lecue   delteil        kona
et al         liu et al          in fact  the early versions of owl s regarded inputs and outputs as
independent semantic entities  using a description logic formalization of their types 
thus  the existence of a compilation from wsc sf wd into planning under uncertainty is quite
interesting  it shows how a composition model similar to the early versions of owl s  in a general
form with partial matches and powerful background ontologies  can be attacked by off the shelf
planning techniques  this opens up a new connection between wsc and planning 

  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

    compilation
we compile a wsc sf wd task into a task of conformant planning under initial state uncertainty 
which takes the form  p  a      g    p is the finite set of propositions used  a is a finite set of
actions  where each a  a takes the form  pre a   eff a   of a pair of sets of literals over p    is
a cnf formula over p  g is a conjunction of literals over p  these notions are given a standard
belief state semantics  a state is a truth value assignment to p  the initial belief is the set of states
satisfying     the result of executing an action a in a state s is res s  a     s if s     pre a     and
otherwise res s  a      sadd a   del a   here we use the standard notation that gives s in terms
of the set of propositions that it makes true  uses add a  to denote the positive literals in eff a   and
del a  to denote the negative literals in eff a   extension of res to beliefs and the definition of a
plan remain unchanged 
assume a wsc sf wd task  p  ic   o  c        g    the compiled task  p    a      g   makes
use of a new unary predicate ex that expresses which constants have yet been brought into existence  the compilation is obtained as follows  for each operator o  o  with outputs
yo  
s
 y            yk    we create a set of new constants eo    e            ek    then  c    c   oo eo will
be the set of constants fixed a priori  initialize a      for each operator o  o 
v include into a
the
preo    xxo ex x   
v
v set of actions resulting from using c to instantiate the precondition
  eeo ex e    give each of these actions the same effect  eeo ex e   in words  we instantiate os outputs with eo   we enrich os precondition by saying that all inputs exist and that all
outputs do not yet exist  and we replace os effect with a statement simply bringing the outputs into
existence 
replacing the effects in this way  where do the original effects go  they are included into the
initial state formula  that is  we initialize   as the conjunction of effo  eo  yo   for all operators
o  o  then  we instantiate all clauses invic with c andv
conjoin this with     we obtain our final
  by further conjoining this with      cc  ex c    cc c  ex c    goal  here  goal
is a new proposition  it serves to model the goal  namely  we have to introduce a set of artificial
goal achievement actions  the goal has the form g   x       v
    xk   x            xk    the new actions
are obtained by instantiating the operator   x            xk      ki   ex xi      goal  with c  that
is  the goal achievement actions instantiate the existentially quantified variables in the goal with all
possible constants  those actions are added to the set a  the overall compiled task now takes the
form  p    a      goal   where p  is simply the set of mentioned propositions 
in summary  we compile a wsc sf wd task  p  ic   o  c        g   into a conformant planning
task  p    a      g   as follows 
 for each operator o  o  create a uniquesset of new constants eo    e            ek   where
yo    y            yk    we denote c    c   oo eo  

 p  contains all instantiations  with c  of p plus two new predicates  ex and goal  ex has
arity   and expresses which constants have yet been brought into existence  goal has arity  
and forms the new goal  i e   g   goal 

 the actions a are the instantiations of all o  o  where xv
o is instantiated with
vc  and yo is inex x  

 
stantiated with eo   the preconditions
are
enriched
with
 
eeo ex e   
xxo
v
the effects are replaced by eeo ex e  

    as before  we give the actions a conditional effects semantics  rather than the more usual distinction between forced
preconditions  and non forced effect conditions 

  

fih offmann   b ertoli   h elmert   p istore

 further  a contains goal achievement actions  achieving goal under preconditions instantiating g with c 
 the original action effects  i e   the conjunction of effo  eo  yo   for all operators
o  o  is
v
   further   contains    
moved
into

instantiated
with
c 
and
 
 
ic
 
 
cc  ex c   
v
cc c  ex c    goal 

in the terminology of section      this means that we choose the set a of actions as all actions
that can be obtained from an operator o  o by instantiating the inputs with constants from c 
and the outputs with eo   as suggested by lemma    the initial state formula   of the compiled
task describes the possible configurations of the constants c  and the only effect of applying an
action is to bring the respective output constants into existence  note that  although the effects of
the compiled actions are all positive  planning is still hard  conp complete  to be precise  due to the
uncertainty   if we allow wsc operators to also delete constants  then we have negative effects 
deleting constants  in the compiled task  
according to the above strategy  we create only one set of output constants per operator  and
we do not take into account sets of operators that have identical effects  this is only to simplify
the presentation  our results carry over immediately to more complicated strategies that create
more than one set of output constants per operator  as well as to strategies that share sets of output
constants between operators with identical effects  it should be noted  however  that operators
whose effects are not identical can not  in general  share their outputs  in particular  if the two
effects are in conflict  e g   infodssp d  and infodssp d   then the initial state formula   as
above is unsatisfiable  the compiled planning task is then trivially solved by the empty plan  and 
of course  does not encode solutions in the original problem 
example   re consider the planning task defined in example    we specify a compiled task  we set
c    c  d  e  f   where c is the only initially available constant  and d  e  f are potential constants
for operator outputs  the compiled planning task  p    a      g   is the following 
 p     protein  cellprotein  g  h  i   n     kw   combinedpresentation  infodssp 
info d  ex  goal   where all the predicates except goal are unary  have one argument  
 a consists of all instantiations of 
 getinfodsspg  d y     x   g x   ex x   ex d   ex d  
 getinfodssph  d y     x   h x   ex x   ex d   ex d  
 getinfodsspi  d y     x   i x   ex x   ex d   ex d  
 getinfo d n    e y     x    n   x   ex x   ex e   ex e  
 getinfo d kw   e y     x    kw  x   ex x   ex e   ex e  

 combineinfo f  y     x    x     infodssp x     info d x     ex x     ex x    
ex f    ex f   
 goalop    x   combinedpresentation x   ex x   goal 
   is the conjunction of 
 all instantiations of ic   consisting of the five axioms given in example   
  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

 cellprotein c       
 infodssp d  info d e  combinedpresentation f     original action effects 
 ex c  ex d  ex e  ex f     constants existence 
 goal   goal not yet achieved 
 g   goal
now consider again the plan for the original task  see example     hgetinfodsspg  c  d  
getinfodssph  c  d   getinfo d n    c  e   getinfo d kw   c  e   combineinfo d  e  f  i 
to illustrate  we now verify that this plan yields a plan for the compiled task  in that task 
the initial belief b  consists of all states s where c is the only existing constant  d  e  f satisfy the
respective effects  and s    ic  cellprotein c   now we apply the action sequence 
   apply getinfodsspg  c  d  to b    we get to the belief b  which is the same as b  except that 
in all s  b  where s    g c   d now also exists 
   apply getinfodssph  c  d  to b    we get to the belief b  which is the same as b  except that 
in all s  b  where s    h c   d exists 
   apply getinfo d n    c  e  to b    yielding b   
   apply getinfo d kw   c  e  to b    this brings us to b  where we have ex e  for all s  b 
with s     n   c  or s     kw  c  
   apply combineinfo d  e  f   to b    this brings us to b  which is like b  except that all s  b 
where both d and e exist now also have ex f   
   apply goalop f   to b    yielding b   
the same reasoning over ic used in example   to show that b  satisfies the original goal  can
now be used to show that goalop f   is applicable in all s  b  and hence the resulting belief b 
satisfies the goal  so we obtain a plan for the compiled task simply by attaching a goal achievement
action to the original plan 
to prove soundness and completeness of the compilation  we need to rule out inconsistent
operators  i e   operators whose effects are in conflict with the background theory  meaning that
ic  xo   yo   effo is unsatisfiable   for example  this is the case if x   a x   b x  is
contained in ic   and effo   a y   b y   in the presence of such an operator  the initial belief
of the compiled task is empty  making the task meaningless  note that inconsistent operators can
never be part of a plan  and hence can be filtered out as a pre process  note also that  in wsc sf wd  
an operator is inconsistent iff all actions based on it are inconsistent 
non goal achievement actions in a correspond to actions in the original task  in the obvious
way  with this connection  we can transform plans for the compiled task directly into plans for the
original task  and vice versa 
theorem    soundness of compilation  consider the wsc sf wd task  p  ic   o  c        g  
without inconsistent operators and a plan ha            an i for the compiled task  p    a      g   
then the sub sequence of non goal achievement actions in ha            an i is a plan for the task
 p  ic   o  c        g   
  

fih offmann   b ertoli   h elmert   p istore

proof sketch  for an arbitrary sequence of non goal achievement actions  denote by b the belief
after execution in the original task  and by b the belief after execution in the compiled task  for a
state ssin the original task  denote by  s  the class of all compiled task states s overvthe constants
c   oo eo so that  c
s   s ex c          cs   s cs   is   and s    ic     oo effo  eo   
one can prove that b   sb  s   the claim follows directly from that 
 

theorem    completeness of compilation  consider the wsc sf wd task  p  ic   o  c       
g   without inconsistent operators and a plan ha            an i where every operator o appears with at
most one instantiation eo of the outputs  then ha            an i can be extended with goal achievement
actions to form a plan for the compiled task  p    a      g   obtained using the outputs eo  
s
proof sketch  follows immediately from b   sb  s  as shown for the proof of theorem    say
one executes ha            an i in the compiled task  ending in a belief b  from there  a plan for the
compiled task can be obtained simply by attaching one goal achievement action for every tuple of
constants satisfying g in a world state from b 
 

the reader may have noticed that the number of instantiations of the goal achievement operator
is exponential in the arity of the goal  in the worst case  all these instantiations must be included
in the plan for the compiled task  in particular  this may happen in the plan constructed as per the
proof of theorem    however  for practical purposes it appears reasonable to assume a fixed upper
bound on the number of goal variables 
as indicated  the proofs of theorems   and   remain valid when allowing more than one eo
per operator  and or when operators with identical effects share output constants  note that operators have identical effects if several web services provide alternative ways of achieving something 
example   illustrates such a situation  cf  our earlier discussion in section       in our experiments
as described in the next section  all groups of operators with identical effects are assigned the same
output constants 

   empirical results
to show that the compilation approach has merits  we now report on a number of empirical experiments using cff as the underlying planner  we start with a discussion of the general experimental
setup and then discuss the results for two different test scenarios 
    experiments setup
we implemented the compilation from wsc sf wd into planning under uncertainty as described
above  and connected it to the cff tool  it should be noted here that  although the compiled planning
tasks do not have delete effects  they are not solved by cffs relaxed plan based heuristic function 
that function makes a further relaxation ignoring all but one of the conditions of each effect  see
the earlier discussion of cff in section       ignoring all but one condition significantly affects the
compiled tasks because their effects typically involve many conditions  particularly those conditions
stating that all inputs exist and all outputs do not yet exist 
one problematic point in evaluating planning based wsc is the choice of test cases  the field
is still rather immature  and due to the widely disparate nature of existing wsc tools  there is

  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

no common set of benchmarks    in fact  because web service composition is such a new topic
posing so many challenges to existing techniques  the different works differ widely in terms of both
their underlying purpose  and the specific aspect of wsc they address  a detailed discussion of
existing wsc tools is given below in section    the method we choose for evaluation is to design
two test scenarios that reflect what are intuitively relevant kinds of problem structures in potential
applications of planning based wsc  and that are scalable in a number of interesting parameters 
we test the reaction of our approach to these parameters 
while our test scenarios are artificial benchmarks  and cannot lead to broad conclusions of significance for practice  they do allow us to draw conclusions about planning behavior in differently
structured test problems  our solution method scales quite well in most of the tested cases  efficiently finding solutions that involve many web service calls  and that successfully employ only
those services that are really necessary  viewing these results in isolation  one can conclude that
representation techniques and heuristic functions from planning under uncertainty may be useful to
attack large and complex planning like wsc instances 
a comparison to alternative wsc tools is  again  problematic  due to the broad range of problems the tools can solve  the different kinds of solutions they find  and the different kinds of input
syntax language they read  to obtain at least some notion of empirical comparison to these tools 
in the following we consider only expressivity  how general is the input language of a tool   and
scalability  how quickly can the tool compose    each of the existing wsc tools constitutes a
separate point in the trade off between these two  the question then is whether our compilation
approach  restricting to wsc sf wd and using cff to solve the compiled tasks  is a sensible point in
that trade off 
in terms of expressivity  our approach is located in between very general planning methods  like
eiter et al               giunchiglia et al          inspired by the actions and change literature  and
the more restricted methods that have been applied to wsc so far  the question is whether we gain
scalability in comparison to the more expressive methods 
we confirm in our experiments that the answer is  as expected  yes  we run the dlvk tool
 eiter et al                which handles a powerful planning language based on logic programming 
that language in particular features static causal rules which are similar to the integrity constraints
in fully general wsc    in that sense  from our perspective dlvk is a native wsc tool that
handles ontology axioms directly rather than via restricting their expressivity and compiling them
away  in particular  we encoded our wsc test problems directly in dlvks input language  without
the compilation that we use for cff 
dlvk relies on answer set programming  instead of relaxed plan heuristics  to find plans  further  in the style of many reasoning based planners  dlvk requires as input a length bound on the
plan  and can hence be used to find optimal plans by running it several times with different bounds 
in all cases  we ran dlvk only once  with the bound corresponding to the optimal plan length 
even so  dlvk is much slower than cff  solving only a small fraction of our test instances  we do
not wish to over interpret these results  all we conclude is that wsc sf wd constitutes an interesting
point in the trade off between expressivity and scalability in wsc 
    while the vta example could be considered one such benchmark  essentially every individual approach defines its
own particular version of that example 
    the similarity lies in that both static causal rules and fully general integrity constraints can  as a side effect of applying
an action  yield ramifications affecting the properties inherited from the previous state 

  

fih offmann   b ertoli   h elmert   p istore

when running some first tests with the compilation approach  we noticed that the encoding as
per section     is unnecessarily generous about the set of initial states  observe that our compiled
tasks are always easier to solve if more propositions are true in the initial state  this is  simply  because all literals in operator preconditions  effects  and the goal are positive  hence  if a proposition
p does not appear positively in any initial state clause  then one can set p to   initially  and thereby
reduce the number of initial states  without introducing any new plans    setting a proposition to
  may cause unit propagations  setting other propositions to   or    we iterate these steps until a
fixpoint occurs  the resulting initial state description is stricter than before  and yields better performance both for cff and for dlvk  we use this optimized encoding in all the experiments reported
below 
we also experimented with another optimization  that optimization makes the assumption that
the constants requested by the goal will be generated in a step wise fashion  where each intermediate
constant is generated with certainty before generating the next constant  recallvthat in the encoding
as per section      the existence of the inputs of operators  i e   the condition xxo exists x   is
part of the operator precondition and is thus interpreted under a conditional effects semantics  however  both cff and dlvk offer a distinction between effect conditions and forced preconditions
that must hold in the entire belief
for the action to be applicable  we can exploit that distinction to
v
postulate that the condition xxo exists x  is forced  this reduces the state space  but may cut
out solutions  the reduction is quite beneficial both for cff and for dlvk  since the optimization
affects the set of plans  we switch it on only in part of the test cases  to point out the possible speedup  the tests where the optimization is switched on are discussed in the text  and indicated by the
keyword forced in the name of the test case 
we use two versions of cff  one is cffs default configuration which makes use of ffs enforced hill climbing search algorithm as well as its helpful actions pruning technique  hoffmann
  nebel         in the other configuration  cff helpful actions pruning is turned off  and the search
proceeds in standard greedy best first fashion  with an open queue ordered by increasing heuristic
values  we henceforth denote the former configuration with cff def and the latter configuration
with cff std 
all results were obtained on a    ghz pentium iv pc running linux  all tests were run with a
time out of     seconds cpu  and limiting memory usage to   gb 
    subsumption hierarchies
we first investigate how well our approach can deal with scaling subsumption hierarchies  and with
building chains of successively created entities  outputs   for that purpose  we design a test scenario
called sh  which demands the composition of web services realizing a chain of generation steps 
where every generation step has to deal with a subsumption hierarchy 
the scenario is depicted in figure    there are n top level concepts t l            t ln   depicted
with tl in figure    the goal input is t l    the goal output is t ln   beneath each t li   there
is a tree shaped hierarchy of sub concepts  more precisely  the tree is perfectly balanced with
branching factor b  and has depth d  the inner nodes of the tree are called intermediate level
 or simply intermediate  concepts  depicted with il in figure    the leaf nodes of the tree
are called basic level  or simply basic  concepts  depicted with bl in figure    for every
non leaf concept c in the tree  with children c            cb   we have the axioms x   ci  x   c x 
    of course  reducing the set of initial states does not invalidate any old plans  either 

  

fitl

w eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

il

il

bl

bl

bl

bl

bl

bl

sws

sws

sws

sws

sws

sws

tl

il

bl

il

bl

bl

bl

bl

bl

figure    schematic illustration of the sh scenario 
expressing subsumption  as well as an axiom x   c x   c   x       cb  x  expressing that the
parent is covered by its children 
the available web services are defined as follows  for each top level concept t li   and for
each leaf bli j of the corresponding tree structure  there is a web service available that takes
bli j as input and that outputs t li     the corresponding wsc operator takes the form oi j  
  x   bli j  x    y   t li    y    then  by applying  for each    i   n in order  all services oi j  
it is possible to make sure that a constant of concept t li   is created in all possible cases  hence 
sequencing all these steps is a plan  of length  n      bd   note here that  as we already stated
in section      in our experiments groups of operators with identical effects are assigned the same
output constants  for the sh scenario  this means that for each    i   n  all the oi j share the
same output constant  hence the total number of output constants generated  i e   the number of
potential constants in the initial state  is equal to the number of top level concepts  n 
although the sh scenario is of an abstract nature  it is representative for a variety of relevant
situations  specifically  the scenario can model situations where sets of different services must be
used to address a request which none of them can handle alone  the role of each single service is
then to handle some particular possible case  in our example  the set of different services is the
set of services oi j assembled for each t li   given a constant c which is a member of t li   i e  
t li  c  holds  the particular possible case handled by service oi j is the case where c happens to
be a member of leaf bli j  one of those cases must hold due to the coverage clauses in the tree 
  

fih offmann   b ertoli   h elmert   p istore

similar situations arise  e g   for geographically located  regional  services when the composition
request is not location specific or addresses locations at a higher  inter regional  level  a similar
pattern can also be found in e government scenarios where a clear cut classification of activities
leads to establishing several parallel services that serve different departmental areas 
orthogonal to this horizontal composition  the scenario can model vertical composition 
where one function has to be pursued by concatenating existing functions  this is the case for most
complex procedures in such diverse areas as e government or e commerce 
the scenario can be instantiated to study different aspects of the scalability of our approach 
our empirical tests measure scalability in both the horizontal and the vertical direction  further 
we consider two extreme cases of the possible shapes of the individual concept trees in the chain 
giving us instances with identical numbers of leaves  we set up the test scenario sh broad  where
d     and b scales over                  we set up the test scenario sh deep  where b     and d
scales over                in both scenarios  n scales from   to    
further  we designed a sh trap variant where a second chain of n concepts can be linked 
but is completely irrelevant for the goal service  this variant is suitable for testing to what extent
the composition techniques are affected by irrelevant information  finally  recall that the encoding
method
comes in two versions as explained above  where the default method treats input existence
v
 xxo exists x   by a conditional effects semantics  whereas the non default method  forced 
compromises completeness for efficiency by treating input existence as a forced precondition 
all in all  we have the following choices    different planners  cff def  cff std  dlvk  
  different encoding methods  sh with or without the trap  sh broad or sh deep  the crossproduct of these choices yields    experiments  within each of which there are    possible values
for n and   possible values for b or d  i e      test instances  for cff  we measured   performance
parameters  total runtime  number of search states inserted into the open queue  and number of
actions in the plan  for dlvk  we measured total runtime and number of actions in the plan  of
course  not all of this large amount of data is interesting  in what follows  we summarize the most
important observations  figure   shows the data we selected for this purpose  part  a  of the figure
shows cff std on sh broad   b  shows cff std on sh deep   c  shows cff def on sh forcedbroad   d  shows dlvk on sh broad and sh deep   e  shows dlvk on sh forced broad and
sh forced deep   f  shows dlvk and cff std on sh trap  the vertical axes all show log scaled
runtime  sec   the horizontal axes show n in  a    b  and  c   in  d    e  and  f   n is fixed to n    
and the horizontal axes show the number of leaves in the concept hierarchy 
consider first figure    a  and  b   these plots point out how efficiently cff can handle this
kind of wsc problem  even with no forced optimization  comparing the two plots points out the
difference between handling broad and deep concept hierarchies  in both plots  cff std runtime
is shown over n  the length of the chain to be built  in  a   we show   curves for the   different
values of b  the number of leaves in a hierarchy of depth     and in  b  we show   curves for the  
different values of d  the depth of a hierarchy with branching factor     in both cases  the scaling
behavior is fairly good  with small concept hierarchies  b     or d       chains of almost arbitrary
length can be built easily  as the hierarchies grow  runtime becomes exponentially worse  note 
however  that from one curve to the next the size of the hierarchies doubles  so that growth is itself
exponential  with concept hierarchies of    leaves  i e      alternative cases to be handled in each
step  we can still easily build chains of   steps  where the solution involves    web services  the
most interesting aspect of comparing the two plots   a  and  b   is that the underlying search spaces
are actually identical  the open queues are the same  the only difference in performance stems
  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

   

   

  

  

 

 

b  
b  
b  
b   
b   

   

    
 

 

 

 

  

  

  

d  
d  
d  
d  
d  

   

    

  

  

  

 

 

 

 a  cff std on sh broad

 

  

  

  

  

  

  

 b  cff std on sh deep
     
sh broad
sh deep
    

   

   

  
  

 
 

   
   

    
 

 

 

 

  

  

  

  

  

  

    
 

 

 c  cff def on sh forced broad

 

 

 

 d  dlvk on sh broad and sh deep

     

     
sh forced broad
sh forced deep

dlvk sh trap broad
dlvk sh forced trap broad
cff std sh trap broad

    

    

   

   

  

  

 

 

   

   

    

    
 

 

 

 

 

 

 

 

  

  

  

  

  

  

  

  

  

  

  

 

 e  dlvk on sh forced broad and sh forced deep

 

 

 

  

  

  

  

  

  

  

  

  

  

 f  dlvk and cff std on sh trap

figure    selected results for sh scenario  see detailed explanation in text 

  

  

  

fih offmann   b ertoli   h elmert   p istore

from an overhead in cffs reasoning techniques  which consume more runtime in the case of deep
concept hierarchies  hence the slightly worse behavior in  b  
if we run cff def on the test suites of figure    a  and  b   then we obtain much worse behavior 
for example  with b     we only get up to n      the reason seems to be that ffs helpful actions
pruning and enforced hill climbing are too greedy in this domain  a simple way to overcome this is
to use a standard heuristic search algorithm instead  as done by cff std shown in figure    a  and
 b   on the other hand  if the forced optimization is switched on  then helpful actions pruning and
enforced hill climbing work much better  and we obtain a significant performance boost when using
cff def  the latter is pointed out by figure    c   showing data for cff def on sh forced broad 
like figure    a  for cff std on sh broad  this plot shows   curves  one for each of the   values
of b  legend omitted from the plot because it would overlap the curves   we see that  in this case 
we can easily build arbitrarily long chains even for b       giving us a solution involving     web
services for n       even for b       we still get up to n     
figure    d  and  e  show what one gets when trying to solve the same examples  encoding them
directly for dlvk instead of using the compilation and solving them with cff  as expected  the
performance is much worse  since hardly any test instance is solved for n      we fixed n to its
minimum value   in these plots  unlike  a    b  and  c   each of  d  and  e  shows data for both the
broad and deep variants  showing the number of leaves on the horizontal axis  in order to obtain a
more fine grained view  for the broad variant we increase that number by steps of   rather than by
a multiplicative factor of   as before  we see that  without the forced optimization  figure    d  
performance is poor  and the largest case we can solve is n      b     where the solution involves
  web services  as we switch forced on  figure    e   performance is dramatically improved but
is still on a different level than what we obtain by compilation cff 
figure    f   finally  exemplifies the results we get in the trap scenario  we show data for
the broad version  on the default encoding with cff std  and on both the default and the forced
encoding with dlvk  dlvk is quite affected by the irrelevant chain of concepts  now solving only
the single instance n      b     for the default encoding  and getting up to n      b      for
the forced encoding  instead of n      b      without the trap  this behavior is expected since
dlvk does not make use of heuristic techniques that would be able to detect the irrelevance of the
second chain of concepts  the question then is whether cffs techniques are better at that  figure  
 f  shows that cff std is largely unaffected for n      one can see this by comparing that curve
with the points on the vertical axis in figure    a   however  for n     the performance of cff std
drastically degrades  the only instances solved are n      b     and n      b      the reason
seems to be that the additional actions yield a huge blow up in the open queue used by the global
heuristic search algorithm in cff std  indeed  the picture is very different when using cff def and
the forced encoding instead  the search spaces are then identical to those explored with no trap  and
the behavior we get is identical to that shown in figure    c  
all plans found in the sh scenario are optimal  i e   the plans returned contain only those web
services that are needed  the single exception is dlvk in trap  where the solutions include some
useless web services from the trap chain   
    note here that dlvks plans are parallel  their parallel length is optimal  because we provided the correct plan
length bound  cf  section      however  each parallel step may contain unnecessary actions  on top of the necessary
ones  thats what happens in trap 

  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

    complex concept dependencies
the two variants of the sh scenario feature tightly structured relationships between the involved
concepts  and allow the investigation of scalability issues by varying the size of the structure  we
now consider a more advanced scenario  where the way top level concepts are covered by lowerlevel concepts is subject to complex concept dependencies  similar to the axioms constraining protein classes and their characteristics in example    therefore we investigate how performance is
impacted by more complex concept structures than just subsumption hierarchies 
tl

tl

il

il

il

il

bl

bl

bl

bl

bl

bl

bl

bl

bl

bl

bl

bl

sws

sws

sws

sws

sws

sws

sws

sws

sws

sws

sws

sws

tl

tl

il

bl

bl

il

il

bl

bl

bl

bl

bl

bl

il

bl

bl

bl

bl

figure    schematic illustration of the cd scenario vs  the sh scenario 
our new scenario is called cd  for concept dependencies  figure   illustrates this scenario  and
contrasts it with the sh scenario  similarly to what we had in sh  we have top level concepts 
of which each one is associated to a set of basic sub concepts  there are b basic concepts for
every top level concept  there are n top level concepts t l            t ln   and the goal is to achieve
t ln starting from t l    as before  this is done through combining web services that cover all
possibilities  namely  for every top level concept t li and for every basic concept bli j associated
with it  we have the operator oi j      x   bli j  x    y   t li    y     
the difference lies in the connection between the basic concepts and the top level concepts 
in sh  this was rigidly given in terms of a tree structure of subsumption and coverage axioms
over intermediate concepts  every basic concept  i e   every operator oi j corresponding to such a
concept  had to be included in the plan in order to cover all possible cases  in cd  we use instead a
complex set of axioms to connect the basic concepts to the top level  each top level concept has m
intermediate concepts ili             ili m   for which as before we have axioms stating that each ili j
    note here again that  for the same i  all these operators are assigned the same output constant by our compilation
technique 

  

fih offmann   b ertoli   h elmert   p istore

is a sub concept of t li   as well as the axiom x   t li  x   ili    x       ili m  x  stating
that t li is covered by ili             ili m   for the connection between the intermediate concepts and
the basic concepts  complex dependencies are used  each intermediate subconcept is constrained to
be covered by some non empty set of combinations of the basic subconcepts  precisely  we create a
random dnf  of only positive literals  using the basic concepts as the predicates  we then take that
dnf to imply ili j   note here that  in the implication  the dnf is negated and hence becomes a
cnf  which we can directly encode into our formalism  we do this for every ili j  
in such a setting  it is interesting to control how many combinations are required to cover the
top level concept t li   this directly corresponds to the total number of random combinations  random dnf disjuncts  that are generated  for all of the intermediate concepts ili j taken together 
we control this via what we call the coverage factor  c  ranging in         from the  b    possible
combinations of basic concepts  we pick a random subset of size c    b      each such combination is associated to the dnf of a randomly chosen intermediate concept  note that the cnf
formulas generated this way may be enormous  to minimize the size of the encoding  we use the
formula minimization software espresso  brayton  hachtel  mcmullen    sangiovanni vincentelli 
      mcgeer  sanghavi  brayton    sangiovanni vincentelli        
if  hypothetically  c is set to   then the task is unsolvable  in the experiments reported below 
whenever we write c      this means that exactly one combination was selected  and associated
with every intermediate concept 
by escaping from the rigid schema of relationships presented by sh  the cd scenario is suitable to test whether the performance of our approach is tied to the specific structure of the sh
problem  moreover  the way cd is designed allows us to determine to what degree the planners
react intelligently to different concept structures  in particular  the scenario allows the analysis of 
   the ability of our approach  and in particular of the selected underlying planner cff  to identify plans that contain only relevant actions  especially when the coverage factor c is low 
some basic subconcepts may never appear in any partition of intermediate concepts  and thus 
the plan does not need to include the respective operators  still  due to the conditional effects partial matches semantics  plans that include those operators are valid plans  evaluating
plan length performance over varying c is therefore interesting 
   the ability of our approach to deal with complex axiomatizations  this can be measured in
terms of the impact of the coverage factor on runtime performance  the randomization of
the choice of combinations of basic factors  in different settings of c  may induce significant
differences in the cnf axiomatizations  and as a result  subject the underlying reasoning
engine to very different situations 
in summary  the cd scenario is representative for situations where complex dependencies must be
taken into account in order to select the correct services  examples of such domains were discussed
in sections     and      in particular  the cd scenario corresponds closely to  a scalable version of 
our protein domain example  the different values for the dssp code correspond to different basic
concepts  and the respective getinfodssp services are the operators taking them to an intermediate
concept  infodssp y   this is similar for amino acids    d shapes  and shapes in complexes  the
top level concept combinedpresentation y  can be achieved once constants for every intermediate
concept have been created  so  the only difference to cd lies in that  rather than having just a single
top level concept generated from its intermediates  cd has a sequence of top level concepts that
need to be generated in turn 
  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

as with the sh scenario  the total data of our experiments is extensive  even more so since we
now have   scenario parameters rather than   as before  and since individual instances now contain
a random element  in figure    we report selected results pointing out the main observations  part
 a   b  of the figure show cff std runtime plan length over n for m      b       c   d  show cffstd runtime search nodes over c for n      m      b       e  shows dlvk and cff std runtime
over b in cd for n      c          f  show the latter data for cff def and cd forced 
figure    a  and  b  consider the scalability and solution lengths of the test varying the size of
the scenario  and representing different coverage factors as different lines  we report data for cffstd  results are very similar for cd forced and cff def  i e   contrary to sh  in cd this setting of
options does not bring a significant performance gain  we see in figure    a  that cff scales up
pretty well  though not as well as in sh  being easily able to solve tasks with   top level concepts of
which each has   intermediate concepts and   basic concepts  tasks with minimum coverage factor 
c       are solved particularly effortlessly  for higher c values  one can observe somewhat of an
easy hard easy pattern  where  for example  the curve for c        lies significantly below the
curves for c       and c        we examine this easy hard easy pattern in more detail below 
in figure    b   an obvious and expected observation is that plan length grows linearly with
n  i e   with the number of top level concepts  a likewise obvious  but much more important 
observation is that plan length grows monotonically with the coverage factor c  as reported above 
a lower coverage factor opens up the opportunity to employ less basic services  namely only the
relevant ones  figure    b  clearly shows that cff std is effective at determining which of the
services are relevant and which are not 
let us get back to the intriguing observation from figure    a   the easy hard easy pattern over
growing c  figure    c  and  d  examine this phenomenon in more detail  both plots scale c on
the horizontal axis  for a fixed setting of n  m and b  runtime is shown in  c   while  d  shows
the number of search states inserted into the open queue  for each value of c  the plots give the
average and standard deviation of the results for    randomized instances  we clearly see the easyhard easy pattern in  c  for runtime  with high variance particularly for c        in  d   we
see that there is no such pattern for the number of search states  and that the variance is much less
pronounced  this shows that the easy hard easy pattern is not due to differences in the actual search
performed by cff  but due to the effort spent in the search nodes  we traced the behavior of cff
in detail  and found that the reason for the easy hard easy pattern lies in the runtime cff spends
in its sat reasoning for state transitions  i e   in the reasoning it uses to determine which facts
are definitely true false in each belief  for high but non     values of c  the cnf encodings of the
concept dependency structures take on a rather complex form  in the cases where cff takes a lot
of runtime  almost all of the runtime is spent within a single call to the sat solver  that is  it seems
that cffs sat solver exhibits a kind of heavy tailed behavior on these formulas  a phenomenon
well known in the sat and cp community  see for example the work of gomes  selman  crato  and
kautz         it should be noted here that  in typical planning benchmarks  the cnfs have a much
simpler structure  which motivates the use of a fairly naive sat solver in cff  using neither clause
learning nor restarts  in order to save overhead on formulas that are simple anyway  it seems likely
that the addition of advanced sat techniques to the solver could ameliorate the observed problem 
finally  figure    e  and  f  compare the performances of compilation cff and dlvk  with
no compilation   both plots fix n      i e   data is shown for only   top level concepts  the only
instances that dlvk solves for n     are the ones where the forced optimization is used and n     
m      b      further  in both plots c is fixed to c         the reason for this is that we did
  

fih offmann   b ertoli   h elmert   p istore

    

  

c   
c    
c    
c    
c    
c     

   

c   
c    
c    
c    
c    
c     

  

  

  

  

  

 

  
   
 

    

 
 

 

 

 

 

 

 

 

 a  cff std runtime over n

 

 

 

 

 b  cff std plan length over n
   

   

   

  

   

 

   

   

  

    

 
 

  

  

  

  

   

 

 c  cff std runtime over c
     

  

  

    

dlvk m  
dlvk m  
dlvk m  
cff std m  
cff std m  
cff std m  

    

  

  

   

 d  cff std plan length over c
dlvk m  
dlvk m  
dlvk m  
cff def m  
cff def m  
cff def m  

   

   
  
  
 
 

   
   

    

    
 

 

 

 

  

  

 

 e  dlvk and cff std runtime over b

 

 

 

  

  

 f  dlvk and cff def runtime over b

figure    selected results for cd scenario  see detailed explanation in text 
not find a significant difference in the performance of dlvk for different values of c  dlvk was
unable to exploit lower c for lower runtime  and neither did it show an easy hard easy pattern  we
speculate that dlvks answer set programming solver tends to perform exhaustive search anyway
and is accordingly not as affected by different structures as the heuristic techniques employed by
cff  however  like cff  dlvk was able to exploit lower coverage factors c for shorter plans 
  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

figure    e  shows the default setting without the forced optimization  we see that the performance of dlvk explodes quickly while cff does not experience as much trouble  cff fails at the
upper ends of its curves  both in figure    e  and  f   only because the problem files  i e   the cnfs
describing the complex concept dependencies  become too large to parse      mb   that notwithstanding  cffs runtime behavior is clearly exponential  note  however  that the actual encodings 
i e   the problem instances to be solved  also grow exponentially over c 
we can further observe that dlvk exhibits quite some variance  particularly across different
settings of m  the curves cross in figure    e   this is even more pronounced in figure    f   where
we can also observe  as before for sh  that the forced optimization brings a huge advantage for
dlvk  for m     and m     in figure    f   dlvk fails on the first unsolved problem instance
due to running out of memory shortly after parsing the problem 
concluding this section  we observe that the empirical behavior of cff in the sh and cd scenarios is promising  these results should not be over interpreted  though  while the test scenarios
do capture problem structure typical of a variety of potential applications of wsc technology  our
approach has yet to be put to the test of actual practice  the same  however  can be said of essentially
all current planning based wsc technology  since the field as a whole is still rather immature 

   related work
the relation of our work to the belief update literature has been covered in detail already in sections     and      as for the relation to planning  our formalism basically follows all the commonly
used frameworks  our notions of operators  actions  and conditional effects are exactly as used in
the pddl framework  mcdermott et al         bacchus        fox   long         except for the
extension with outputs  regarding the latter  it has been recognized for some time in the planning
community  for example by golden              and edelkamp         that on the fly creation of
constants is a relevant feature for certain kinds of planning problems  however  attempts to actually
address this feature in planning tools are scarce  in fact the only attempt we are aware of is the work
by golden              and golden  pand  nemani  and votava         part of the reason for this
situation is probably that almost all current state of the art tools employ pre processing procedures
that compile the pddl task into a fully grounded description  the core algorithms are then implemented based on a propositional representation  lifting such algorithms to a representation that
involves variables and on the fly instantiations requires a major  implementation  effort  in the work
herein  we circumvent that effort by using potential constants and feeding the resulting problem
to cff  which like most planners employs the said pre processing  extending cff for wsc f wd
will involve dealing with non propositional representations as a sub problem 
our notion of initial state uncertainty and conformant plans closely follows the related literature
from planning under uncertainty  smith   weld        cimatti et al         hoffmann   brafman 
       the formalization in terms of beliefs is adapted from the work by bonet and geffner        
there are some related works in planning which allow a domain axiomatization  i e   some form of
axioms constraining the possible world states  eiter et al         giunchiglia et al          to the
best of our knowledge  no work in planning exists  apart from the work presented herein  which
considers the combination of domain axioms and outputs 
a few words are in order regarding our notions of partial and plug in matches  this terminology originates from work on service discovery in the sws community  see for example paolucci
et al         li   horrocks        kumar et al          in service discovery  one is concerned with

  

fih offmann   b ertoli   h elmert   p istore

matching service advertisements against service requests  the discovery result is the set of services
whose advertisement matches the request  the descriptions of services and requests are similar to
the functional level service descriptions  i e   the planning operators that we use here  however  the
terminology in these works is slightly different from ours  and they also describe additional kinds
of matches  the notions given by li and horrocks        have the closest relation to ours  service
descriptions are defined in terms of constructed description logic concepts  say a is the concept
describing the advertisement  and r is the concept describing the request  then li and horrocks
say that a and r have  an exact match if a  r  a plug in match if a  r  a subsume match
if a  r  and an intersection match if a  r     to compare this to our setting  consider the
situation where a is the effect of action a  and r is the precondition of action r  exact matches
are a special case of plug in matches which we do not distinguish herein  intersection matches
correspond to what we call partial matches  concerning plug in and subsume matches  matters are
more subtle  the intuitive meaning of plug in match is that the advertisement fully suffices to
fulfill the request  in planning terms  this means that the effect of a implies the precondition of r 
however  in service discovery this is traditionally taken to mean that every requested entity is being
provided  i e   a  r  the latter notion  where the precondition of r implies the effect of a  is
not meaningful in planning  hence we use only one of the two notions  in correspondence to li and
horrockss subsume matches 
in contrast to the work of li and horrocks         and to our work  paolucci et al         and
kumar et al         define matches for individual input output parameters in service descriptions 
rather than for service descriptions on a more global level  precondition effect for us  constructed
concept for li   horrocks         on the level of individual parameters  paolucci et al        
suggest the same notions as li and horrocks        except that they do it in a less formal notation 
and they do not define intersection matches  the same is true of kumar et al          the latter
authors also define notions of contains and part of matches  relating to the building blocks of
constructed concepts  obviously  such notions do not make sense in our framework  where there
arent any constructed concepts  finally  kumar et al  define some ways of aggregating matches for
individual parameters to matches for entire service descriptions  again  this is not applicable in our
case since we work on a more global level in the first place 
a brief survey of the existing works on wsc is as follows  there is a variety of works that compile composition into more or less standard deterministic planning formalisms  ponnekanti   fox 
      srivastava        sheshagiri et al          some other works  agarwal  dasgupta  karnik 
kumar  kundu  mittal    srivastava      b  agarwal et al       a  additionally focus on end to end
integration of sws composition in the larger context  akkiraju  srivastava  anca andreea  goodwin  and syeda mahmood        investigate techniques to disambiguate concept names  mcilraith
and fadel        achieve composition with particular forms of non atomic services  by modeling the
latter as atomic actions that take the meaning of a kind of macro actions  narayanan and mcilraith
       obtain a composition ability as a side effect of verifying sws properties using petri nets 
kuter  sirin  nau  parsia  and hendler         au  kuter  and nau         and au and nau       
focus on information gathering at composition time  rather than at plan execution time  mcdermott
       treats the actual interaction  communication  with a web service as a planning problem 
mediratta and srivastava        design an approach to wsc based on conditional planning  i e  
a form of planning under uncertainty  while this suggests a close relation to our work  the focus
of mediratta and srivastavas work is actually quite different from ours  mediratta and srivastava
do not consider output variables  and neither do they consider any domain axiomatizations  the
  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

only overlap with our formalism lies in that they allow incomplete initial state descriptions  i e  
initial states that assign a value to only a subset of the propositions  they handle observation
actions which allow observing the value of any unspecified proposition  to ameliorate the need
for complete modeling  they consider a definition of user acceptable plans  where only a subset
of the plan branches  as specified by the user  are guaranteed to lead to the goal  the latter may be
an interesting option to look into when extending our framework to handle partial observability 
two approaches explore how to adapt formalisms from so called hand tailored planning for
sws composition  the approaches are based on golog  mcilraith   son        and htn planning  sirin et al          respectively  these frameworks enable the human user to provide control
information  however  non deterministic action choice is allowed  if no control information is
given  then planning is fully automatic  hence  in this sense  these frameworks are strictly more
powerful than planning without such control information  further  both approaches are capable of
handling advanced plan constructs such as loops and branches  in golog  the possible plans  the
possible composition solutions  are described in a kind of logic where high level instructions are
given by the programmer  and the planner will bind these instructions to concrete actions as part
of the execution  in htn  the programmer supplies the planning algorithm with a set of so called
decomposition methods  specifying how a certain task can be accomplished in terms of a combination of sub tasks  recursively  there are decomposition methods for those sub tasks  thus the
overall task can be decomposed in a step wise fashion  until atomic actions are reached  neither
mcilraith and son        nor sirin et al         are concerned with handling ontology axioms  as
we do in this paper  hence  combining the insights of both directions has synergetic potential  and
is an interesting topic for future work 
another approach capable of handling advanced plan constructs  loops  branches  is described
by pistore et al       b   pistore  traverso  bertoli  and marconi      c   pistore et al       a   and
bertoli  pistore  and traverso         in this work  process level composition is implemented  as
opposed to the profile capability level composition as addressed in this paper  at the process level 
the semantic descriptions detail precisely how to interact with the sws  rather than characterizing
them only in terms of preconditions and effects  pistore et al       b      c      a  and bertoli
et al         exploit bdd  binary decision diagram  based search techniques to obtain complex
solutions fully automatically  however  ontology axioms are not handled and input output types are
matched based on type names 
there are only a few approaches where ontology axioms are used and the requirements on the
matches are relaxed  one of those is described by sirin  hendler  and parsia         sirin  parsia 
and hendler         sirin and parsia         and sirin et al          in the first two papers of
this series  sirin et al                a sws composition support tool for human programmers is
proposed  at any stage during the composition process  the tool provides the user with a list of
matching services  the matches are found by examining the subconcept relation  an output a
is considered a match of input b if a  b  this corresponds to plug in matches  in later work
 sirin   parsia        sirin et al          the htn approach  sirin et al         mentioned above
is adapted to not work on the standard planning semantics  but on the description logics semantics
of owl s  the difficulties inherent in updating a belief are observed  but the connection to belief
update as studied in the literature is not made  and it remains unclear which solution is adopted 
as far as we are aware  all other methods with more relaxed matches follow what we have here
termed a message based approach to wsc  these approaches were already discussed in some depth
in section      next  we give a few more details on the ones most closely related to our work  the
  

fih offmann   b ertoli   h elmert   p istore

approach by liu et al         was discussed in sufficient detail already in section      so we do not
reconsider this here 
meyer and weske        handle ontology axioms in their wsc tool  but do not provide a
semantics for action applications  reasoning is only used to determine whether a particular output
can be used to establish a particular input  so the approach can be classified as message based 
in our terms  the kind of matches handled is said to be plug in  to the best of our knowledge 
this tool is the only existing wsc tool that employs a relaxed plan based heuristic function  like
cff  however  through various design decisions  the authors sacrifice scalability  they explicitly
enumerate all world states in every belief  and hence suffer from exponentially large beliefs  they
search forward with parallel actions and consequently suffer from a huge branching factor  they
take their heuristic to be relaxed planning graph length  rather than relaxed plan length  and thus
suffer from the fact that  most of the time  hmax is a much less informative heuristic than h   bonet
  geffner        hoffmann        
an approach rather closely related to ours  in that it can handle partial matches  is described
by constantinescu and faltings        and constantinescu et al       a      b   in this work the
ontology is assumed to take the form of a tree of concepts  where edges indicate the subconcept
relation  such a tree is compiled into intervals  where each interval represents a concept and the
contents are arranged to correspond to the tree  the intervals are used for efficient implementation
of indexing in service lookup  discovery   as well as for matching during composition  the latter
searches forward in a space of switches  starting at the initial input  if the current input is of type
a  then a service with input ai matches if a  ai      such services are collected until the set
of the collected ai covers a  that is  until the union of the intervals for the various ai contains the
interval for a   the collected services form a switch  and in the next step of the search  each of their
outputs becomes a new input that must be treated  i e   the switch is an and node   composition
is interleaved with discovery  i e   in every search state discovery is called to find the services that
match this state  the search proceeds in a depth first fashion  major differences to our work are
the following  first  the formalization is very different  using intervals vs  using standard notions
from planning based on logics  second  the approach interleaves discovery and composition  which
are separate steps in our framework  web service discovery is needed to determine the operators
of a wsc task   third  the approach considers concept trees vs  clausal integrity constraints  last 
the approach uses depth first search  whereas one of the main points we are making is that one can
exploit the heuristic techniques implemented in standard planning tools for scalable wsc 
finally  an interesting approach related to planning is described by ambite and kapoor        
to capture the dependencies between different input variables of a web service  the input is described in terms of a relation between those variables  the same is done for the outputs  the
relations are formulated in terms of logical formulas relative to an ontology  the underlying formalism is first order logic  so the modeling language is quite expressive    reasoning is performed
in order to establish links  messages  in our terms  between inputs and outputs  the algorithmic
framework in which that happens is inspired by partial order planning  penberthy   weld        
starting from the goal relation and maintaining a set of open links  the solution is a dag of web
services where links correspond to different kinds of data exchanges  selection  projection  join 
union   automatic insertion of mediator services  e g   for converting a set of standard formats  is
also supported 
    at the cost of undecidable reasoning  which according to the authors is not a major issue in practice 

   

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

to some extent  our preconditions effects and clausal integrity constraints can be used to model
relations in the sense of ambite and kapoor         say r is a k ary relation with definition
  describing the input of a web service  we set the corresponding operators precondition to
r x            xk    and we transform  into a set of universally quantified clauses  as long as the
latter can be done  and as long as the ontology axioms can be likewise transformed  we obtain a
model equivalent to that of ambite and kapoor  in that sense  the main modeling advantage of the
approach of ambite and kapoor over wsc f wd is existential quantification  it is an open question
whether such quantification can be accommodated in our framework  insertion of mediator services
can be supported in wsc f wd   but only in the limited sense of recognizing  via particular preconditions  that a particular kind of mediator is required  modeling the actual data flow is bound to be
awkward  in summary  the work of ambite and kapoor is more advanced than ours from a data description and transformation point of view  on the other hand  ambite and kapoor neither consider
belief update  nor do they place their work in the context of a fully fledged planning formalism  and
they are less concerned with exploiting the heuristic technologies of recent planners  combining
the virtues of both approaches  within either framework  is an interesting direction for further
research 

   discussion
we have suggested a natural planning formalism for a significant notion of web service composition
at the profile   capability level  incorporating on the fly creation of constants to model outputs  incomplete initial states to model incomplete user input  conditional effects semantics to model partial
matches  and  most importantly  clausal integrity constraints to model ontology axioms  we have
identified an interesting special case  forward effects  where the semantics of action applications
is simpler than in the general case  we have demonstrated how this relates to the belief update
literature  and we have shown how it results in reduced computational complexity  forward effects
relate closely to message based wsc  and our results serve both to put this form of wsc into context  and to extend it towards a more general notion of partial matches  further  we have identified a
compilation into planning under  initial state  uncertainty  opening up an interesting new connection
between the planning and wsc areas 
our empirical results are encouraging  but should not be over interpreted  while our test scenarios serve to capture some structural properties that are likely to appear in applications of wsc
technology  our approach has yet to be put to the test of actual practice  the same  however  can be
said of essentially all current planning based wsc technology  since that field is still rather immature  in that sense  a more thorough evaluation of our approach  and of planning based wsc as a
whole  is a challenge for the future 
apart from such evaluation  there are several directions for research improving and extending
the technology introduced herein  a line of research that we find particularly interesting is to adapt
modern planning tools for wsc  starting from our special cases  where the complications incurred
by integrity constraints are more manageable  we have already outlined a few ideas for adapting
cff  and pointed out that new challenges arise  it appears particularly promising to tailor generic
heuristic functions  originating in planning  to exploit the typical forms of ontology axioms as occur
in practice  considering the wealth of heuristic functions available by now  this topic alone provides
material for a whole family of subsequent work 

   

fih offmann   b ertoli   h elmert   p istore

acknowledgments
we thank the anonymous reviewers  as well as the managing editor derek long  for their comments 
which were of significant help for improving the paper 
jorg hoffmann performed part of this work while being employed at the university of innsbruck  austria  his work was partly funded through the european unions  th framework programme under the super project  ist fp          http   www ip super org  
piergiorgio bertolis and marco pistores work was partly supported by the project software
methodology and technology for peer to peer systems  stamps  
malte helmerts work was partly supported by the german research council  dfg  as part of
the transregional collaborative research center automatic verification and analysis of complex
systems  sfb tr    avacs   see www avacs org for more information 

appendix a  proofs
we first formally prove proposition    stating that negative effects can be compiled away in wsc 
before we do so  we first need to introduce the compilation formally  assume a wsc task  p 
 
ic   o  c        g    we construct a second wsc task  p      
ic   o   c        g    where initially
 
p     ic and o  are the same as p  ic and o  respectively  we proceed as follows  let g 
p be a predicate with arity k  so that there exists o  o  o    xo   preo   yo   effo   where effo
contains a negative literal g x            xk    we introduce a new predicate notg into p     and we
introduce the two new clauses x            xk   g x            xk    notg x            xk   and x            xk  
g x            xk    notg x            xk    for every operator o whose effect contains a negation of
g  we replace  in effo   g a            ak   with notg a            ak      we continue doing so until no
negative effect literals remain in o   
if a is an action in  p  ic   o  c        g   then we denote by a  the corresponding action
 
 
in  p      
ic   o   c        g    we also use this notation vice versa  i e   if a is an action in
 
 
 
 p   ic   o   c        g   then a denotes the corresponding action in  p  ic   o  c        g    if
s    cs   is   is a state using the predicates p  then we denote by s  a state using the predicates p    
with the following properties  cs    cs   for all p  p cs we have is   p    is  p   for all notp
where p  p cs we have is   notp      iff is  p       since there is  obviously  exactly one such
s    we will also use this correspondence vice versa 
 
proposition   assume a wsc task  p  ic   o  c        g    let  p      
ic   o   c        g   be
the same task but with negative effects compiled away  assume an action sequence ha            an i 
let b be the result of executing ha            an i in  p  ic   o  c        g    and b  is the result of
 
 
 
 
executing ha 
            an i in  p   ic   o   c        g    then  for any state s  we have that s  b iff
s   b   

proof  by induction over the length of the action sequence in question  if the sequence is empty 
then we have to consider the initial beliefs of the two tasks  for which the claim follows directly by
definition  for the inductive step  say that the claim holds for b and b    and a is an action  we need
to show that  for any state s  we have that s  res b  a  iff s   res b    a    
for the direction from right to left  say s   res b    a     by definition we have s  
 
 
 
res s 
    a   for a state s   b   by induction hypothesis  s   b  it therefore suffices to show that
    the arguments ai here may be either variables or constants 

   

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

s  res s    a   we need to show that     s    ic  effa and     s differs from s  in a set inclusion
minimal set of values      is obvious from the definitions  assume to the contrary of     that there
exists s  so that s     ic  effa and s  is identical to s except that there exists at least one propo 
sition p where s   p    s   p  but s p     s   p   by definition  we get that s 
     ic  effa   
 
 
 
 
further  we get that s   p    s   p  but s   p     s   p   and altogether that s   s  s    this is a
 
contradiction to s   res s    a     and hence proves that s  res s    a  as desired 
the direction from left to right proceeds in the same fashion  say s  res b  a   by definition
 
we have s  res s    a  for a state s   b  by induction hypothesis  s 
   b   it then suffices to
 
 
 
 
 
show that s   res s 
    a    we need to show that     s    ic  effa and     s differs from s 
in a set inclusion minimal set of values      is obvious from the definitions  assume to the contrary
 
 
 
 
of     that there exists s 
  so that s     ic  effa  and s  is identical to s except that there
 
 
 
exists at least one proposition p where s 
   p    s   p  but s  p     s   p   by definition  we get
c
s
that s     ic  effa   further  if p  p   then we get that s   p    s   p  but s p     s   p   if
p   notq   p cs  then we get the same property for q  altogether  we get that s   s  s  this is a
 
contradiction to s  res s  a   and hence proves that s   res s 
 
    a   as desired 
theorem   assume a wsc task with fixed arity  and a sequence ha            an i of actions  it is
p   complete to decide whether ha            an i is a plan 
proof  membership is proved by a guess and check argument  first  observe that  for arbitrary s  s  
and a  we can decide within conp whether s  res s  a   guess a state s where cs   cs  ea  
check whether s    ic  effa   check whether is  s is   then s  res s  a  iff no guess
succeeds  further  for an action a  deciding whether a is inconsistent is  obviously  equivalent
to a satisfiability test  so this is contained in np  with these instruments at hand  we can design
a guess and check procedure to decide whether ha            an i is a plan  we guess the proposition
values along ha            an i  we then check whether these values comply with res  and lead to an
inconsistent action  or to a final state that does not satisfy the goal  in detail  the checking proceeds
as follows  first  check whether the initial proposition values satisfy ic      if not  stop without
success  otherwise  iteratively consider each action ai   with pre state s and post state s   check
with an np oracle whether a is inconsistent  if yes  stop with success  if not  test with an np oracle
whether s  res s  a   if not  stop without success  otherwise  if i   n  then go on to ai     if
i   n  then test whether s    g   stop with success if s     g   stop without success if s    g  
ha            an i is a plan iff no guess of proposition values is successful 
hardness follows by the following adaptation of the proof of lemma     from eiter and gottlob
        validity of a qbf formula x y  x  y    where  is in cnf  is reduced to plan testing
for a single action a  we use the   ary predicates x    x            xm    y    y            yn    and
new   ary predicates  z            zm   r  t   the set of operators contains the single operator o with
empty in out parameters  empty precondition  and effect t  the initial constants are empty    is the
conjunction of all xi   all yi   all zi   r  and t  g is r  the theory is 
 

m
 

i  

 t  xi  zi      

m
 

 t  xi  zi      

i  

 

 t  r  c     

c

   

n
 

i  

 t  yi  r  

fih offmann   b ertoli   h elmert   p istore

where  is viewed as a set of clauses c  more readably  the theory is equivalent to 
t    

m
 

xi  zi     r       

n
 

yi    r  

i  

i  

we refer to the initial belief as b  the plan to test contains the single action a based on  equal to  in
fact  o  we refer to the resulting belief as b   obviously  b contains a single state s where everything
except t is true  also  a is consistent  any interpretation that sets r and all yi to   satisfies ic effa  
the theory conjuncts xi  zi make sure that each w  b makes exactly one of xi   zi true 
in particular  the different assignments to x are incomparable with respect to set inclusion  hence 
we have that for every assignment ax of truth values to x  there exists a state s  b that complies
with ax   ax is satisfiable together with ic  effa   and any other assignment ax is more distant
from s in at least one variable  e g   if ax  xi       and ax  xi       then ax is closer to s than ax
regarding the interpretation of zi   
we now prove that  if a is a plan  then x y  x  y   is valid  let ax be a truth value assignment to x  with the above  we have a state s  b that complies with ax   since a is a plan  we
have s    r  therefore  due to the theory conjunct r    we have s      obviously  the values
assigned to y by s satisfy  for ax  
for the other direction  say x y  x  y   is valid  assume that  contrary to thew
claim  a is not
a plan  then we have s  b so that s     r  but then  due to the theory conjunct   ni   yi    r 
we have that s sets all yi to false  now  because x y  x  y   is valid  there exists a truth value
assignment ay to y that complies with the setting of all xi and zi in s  obtain s by modifying s
to comply with ay   and setting r to    we have that s    ic  effa   but then  s is closer to s
than s   and hence s   b in contradiction  this concludes the argument 
 
theorem    assume a wsc task with fixed arity  and a natural number b in unary representation 
it is p   complete to decide whether there exists a plan of length at most b 
proof  for membership  guess a sequences of actions containing at most b actions  note that the
size of such a sequence is polynomial in the size of the input representation   by theorem    we
can check with a p  oracle whether the sequence is a plan 
hardness follows by an extension of the proof of lemma     of eiter and gottlob         validity of a qbf formula x y z  x  y  z   where  is in cnf  is reduced to testing plan existence 
we use the   ary predicates x    x            xn    y    y            ym    z    z            zk    and new
  ary predicates  q            qm   r  t  f          fn   h  g   the set of operators is composed of 
 ot       f       fn  h    t  g  h 
 for    i  n  oxi       h    xi  fi  
 for    i  n  oxi       h    xi  fi  
the initial constants are empty  the initial literal conjunction   is composed of all yi   all zi   all qi  
r  t  all fi   h  and g  that is  the yi   zi   and qi as well as r and h are true  while the fi as well
as t and g are false  no value is specified  only  for the xi   the goal g is r  g  the theory is 
 

m
 

i  

 t  yi  qi      

m
 

 t  yi  qi      

i  

 

 t  r  c     

c

   

n
 

i  

 t  zi  r  

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

where  is viewed as a set of clauses c  more readably  the theory is equivalent to 
t    

m
 

yi  qi     r       

i  

n
 

zi    r  

i  

first  note a few obvious things about this construction 
 ot must be included in any plan 
 once ot is applied  no action can be applied anymore 
 before ot is applied  either oxi or oxi must be applied  for every    i  n 
 the theory is switched off  i e   made irrelevant because t is false  up to the point where ot
is applied 
that is  any plan for this task must first apply oxi or oxi   for every    i  n  thereby choosing a
value for every xi   then  ot must be applied and the plan must stop  before applying ot   no changes
are made to the states except that the values of xi are set and that the fi are made true one after
the other  hence  the belief b in which ot is applied contains a single state s which corresponds to
an extension of   with a value assignment for x  where the values of the fi have been flipped 
we denote the value assignment for x in s with ax   we further denote b    res b  ot    note that
ot is consistent  any interpretation that sets r and all zi to    besides setting the immediate effects
t  g  h  satisfies ic  effot   obviously  all of the applications of oxi and oxi are consistent as
well 
the theory conjuncts yi  qi make sure that each w  b makes exactly one of yi   qi true  in
particular  the different assignments to y are incomparable with respect to set inclusion  hence  we
have that for every assignment ay of truth values to y   there exists a state s  b that complies with
ay   ay is satisfiable together with ic  effot   and any other assignment ay is more distant from s
in at least one variable  e g   if ay  yi       and ay  yi       then ay is closer to s than ay regarding
the interpretation of qi   
we now prove that  if there exists a plan  a yielding assignment ax   then x y z  x  y  z 
is valid  let ay be an arbitrary truth value assignment to y   then we have a state s  b that
complies with ax and ay   ax and ay are satisfiable together with ic  effot   with the above  any
other assignment ay is more distant from s in at least one variable  and  of course  if one deviates
from ax then one is more distant from s in the respective variable  since  a is a plan  we have s    r 
therefore  due to the theory conjunct r    we have s      obviously  the values assigned to z
by s satisfy  for ax and ay   this proves the claim because ay can be chosen arbitrarily 
for the other direction  say x y z  x  y  z  is valid  let ax be an assignment to x so that
y z  ax  x  y  z  is valid  let  a be the corresponding plan  i e    a first applies  for    i  n 
either oxi or oxi according to ax   thereafter   a applies ot   assume
wn that  a is not a plan  then we



have s  b so that s     r  but then  due to the theory conjunct   i   zi    r  we have that s sets
all zi to false  now  because y z  ax  x  y  z  is valid  there exists a truth value assignment
az to z that complies with the setting of all xi   yi   and qi in s  obtain s by modifying s to comply
with az   and setting r to    we have that s    ic  effot   but then  s is closer to s than s   and
hence s   b in contradiction  this concludes the argument 
 

   

fih offmann   b ertoli   h elmert   p istore

theorem    assume a wsc task  the decision problem asking whether there exists a plan is
undecidable 
proof  this result holds even with an empty background theory  a complete specification of the
initial state  predicates of arity at most    operators of arity at most    a goal with no variables at all
 arity     and only positive literals in preconditions and the goal  the result follows with a minor
modification of tom bylanders proof  bylander        that plan existence in propositional strips
planning is pspace complete    the original proof proceeds by a generic reduction  constructing a
strips task for a turing machine  tm  with polynomially bounded space  the latter restriction is
necessary to model the machines tape  tape cells are pre created for all positions within the bound 
what makes the difference between pspace membership and undecidability is the ability to create
constants  we can introduce simple operators that allow us to extend the tape  at both ends 
in detail  say the tm has  a finite number of  states q and tape alphabet symbols a  where b is the
blank    is the transition function  q  is the initial state  and f is the set of accepting states   is the
input word  our planning encoding contains the following predicates  state q  indicates that the
current tm state is q  in a  c  indicates that the current content of tape cell c is a  n eighbors c  c  
is true iff c is the  immediate  right neighbor of c  at c  indicates that the current position of the
tm head is c  rightmost c   lef tmost c   is true iff c currently has no right  left  neighbor  the
set of initial constants contains all states q  all alphabet symbols a  and tape cells c corresponding
to   by the initial literals  all the propositions over these constants are assigned truth values as
obvious  for every transition  q  a  q    a   r    we include an operator 
  x  x    state q   in x  a   n eighbors x  x    at x  
  state q     state q   in x  a    in x  a   at x    at x   
obviously  this encodes exactly that transition  we do likewise for transitions  q  a  q    a   l    
to model the final states  we introduce a   ary predicate g  and include for each q  f an operator 
   state q     g 
we finally include the operators 
  x   rightmost x    x    n eighbors x  x    in b  x    rightmost x    rightmost x  
and
  x    lef tmost x     x   n eighbors x  x    in b  x   lef tmost x   lef tmost x   
with these definitions  it is easy to verify that there exists a plan iff the tm can reach an accepting
state on  
 
lemma    assume a wsc f wd task  a reachable state s  and an action a  then res s  a   
res f wd  s  a  
    propositional strips is like our framework  but with an empty background theory  a complete specification of
the initial state  a goal with no variables  only positive literals in preconditions and the goal  and with no output
parameters in the operators 

   

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

proof  if a is not applicable to s  then the claim holds trivially  consider the other case  by
equation    res s  a  is defined as

  c    i      c    cs  ea   i   min s  c    ic  effa    appl s  a 
res s  a    
 s 
otherwise
where min s  c    ic  effa   is the set of all c   interpretations that satisfy ic  effa and that are
minimal with respect to the partial order defined by i  s i   iff for all propositions p over cs   if
i   p    is  p  then i   p    is  p  
it is obvious that res f wd  s  a   res s  a   if is satisfies ic  effa and is is identical to is
on the propositions over cs   then in particular is is minimal according to s  
for the other direction  let s  res s  a   assume that is  p     is  p  for some proposition p
over cs   define s to be equal to s except that is  p     is  p   obviously  is  s i    it now
suffices to show that s    ic  effa   then  we get is   min s  c    ic  effa   in contradiction 
hence is agrees with is on all propositions p over cs   hence s  res f wd  s  a  
as before  denote with p cs  ea the set of all propositions with arguments in cs  ea   and
with at least one argument in e  and denote with ic  cs   ea   the instantiation of ic with all
constants from cs  ea   where in each clause at least one variable is instantiated from ea   to see
that s    ic  effa   consider first that this is equivalent to s    ic  cs  ea    effa   which in
turn is equivalent to s    ic  cs    ic  cs   ea    effa   in the last formula  because the task is in
wsc f wd   ic  cs   speaks only over the propositions p cs   whereas ic  cs  ea  effa speaks only
over the propositions p cs  ea   so we can treat these two parts separately  we have s    ic  cs  
because s    ic  cs   by prerequisite since s is reachable  we have s    ic  cs   ea    effa by
definition  this concludes the argument 
 
theorem    assume a wsc f wd task with fixed arity  and a sequence ha            an i of actions  it
is conp complete to decide whether ha            an i is a plan 
proof  hardness is obvious  considering an empty sequence  membership can be shown by the
following guess and check argument  say c is the union of c  and all output constants appearing
in ha            an i  we guess an interpretation i of all propositions over p and c  further  for each
   t  n  we guess a set ct of constants  we can then check in polynomial time whether i
and the ct correspond to an execution of ha            an i  for    t  n and a  at   say that a
is applicable if i    prea   ca  ct   and ea  ct     first  we assert i    ic   second  for
all t and for all a  at   assert that  if a is applicable  then i    effa   third  assert that ct    
ct   ea   a  at   a is applicable   using lemma    it is easy to see that i and the ct correspond
to an execution iff all three assertions hold  note that i needs not be time stamped because once
an action has generated its outputs then the properties of the respective propositions remain fixed
forever  the claim follows because  with fixed arity  we can also test in polynomial time whether i
and cn satisfy g   a guess of i and ct is successful if it corresponds to an execution and does not
satisfy g   obviously  ha            an i is a plan iff there is no such guess of i and ct  
 
theorem    assume a wsc f wd task with fixed arity  and a natural number b in unary representation  it is p   complete to decide whether there exists a plan of length at most b 
proof  for membership  guess a sequence of at most b actions  by theorem    we can check with
a p  oracle whether the sequence is a plan 
   

fih offmann   b ertoli   h elmert   p istore

to prove hardness  assume a qbf formula x y  x  y   where  is in dnf normal form 
 this formula class is complete for p     say x   x            xn   y   y            ym   and          
k   we design a wsc f wd task which has a plan iff x y  x  y   is true  the key construction
is to use outputs for the creation of time steps  and to allow setting xi only at time step i  the
yi can take on arbitrary values  once all xi are set  one operator per k allows to achieve the goal
given k is true  the main property we need to ensure in the construction is that each xi can be set
at most once  i e   either to   or to    then there is a plan for the task iff one can set x so that  for
all y   at least one i is true  which is the case iff x y  x  y   is true 
the predicates for our task are p    x               xn      y              ym     time     start    
next      goal      we indicate predicate arity here by the number of points in the parentheses 
for example  the predicate next     has arity    the theory ic is empty  the initial constants are
c     t     the initial literals are     time t     the goal is y goal y   the operators are as
follows 
 for all    i  n  we have  oxi       t            ti     start t    next t    t        
next ti    ti      ti    time ti  next ti    ti  xi  ti     such an operator allows generating
time step i  and setting xi to   at that step 
 for all    i  n  we have  oxi       t            ti     start t    next t    t        
next ti    ti      ti    time ti    next ti    ti    xi  ti     such an operator allows generating time step i  and setting xi to   at that step 
 we will define a value b below  for all n  j   n   b  we have  otj     t            tj    
start t    next t    t         next tj    tj      tj    time tj    next tj    tj     these
operators allow increasing the time step from n to n   b 
 for    i  k  say i   xlxj       xlxjxn  ylyj       ylyjyn where xlj   xj   xj  
and ylj   yj   yj    we have  oi     t            tn b    start t    next t    t        
next tn b    tn b   xlxj   txj         xlxjxn  txjxn   ylyj          ylyjyn      c  
goal c    such an operator allows to achieve the goal after time step n   b  provided the
respective i is true  note here that the xj precondition literals refer to time step tj   i e   the
value set for xj at an earlier time step  while the yj precondition literals have no arguments
and refer to the initial values of yj   which are arbitrary 
assume we choose any value for b  polynomial in the input size   if x y  x  y   is true 
then  obviously  we can find a plan of size n   b   k  we apply an oxi   or oxi   operator for each xi  
depending on whether xi must be set to   or    we apply b operators otj   we apply all operators
oi   the respective input parameter instantiations are all obvious 
the opposite direction  proving truth of x y  x  y   based on a plan  is more problematic 
the plan might cheat by setting some xi to both   and    the reason why our construction is so
complicated is to be able to avoid precisely this case  based on specifying a strict enough plan length
bound b  the key property is that  in order to cheat for xi   the plan has to generate two sequences
of time steps ti           tn b   therefore  a lower bound on the length for a cheating plan is n    b 
as we have already seen  an upper bound on the length of a non cheating plan is n   b   k  to
determine our plan length bound b  we now simply choose a b so that any cheating plan will have to
use too many steps  n  b   n b  k is the case iff b   k  so we can set b    k     and obtain
b    n    k      with this bound b  any plan will proceed by setting each xi to a value  n actions  
   

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

increasing the time step to n   b   n   k      k     actions   and applying a sufficient subset of the
oi  at most k actions   if the plan cheats  then it needs to apply at least n  b   n  k    actions
before being able to apply oi actions exploiting different value settings for a xi   this concludes
the argument 
 
theorem    assume a wsc f wd task  the decision problem asking whether there exists a plan is
undecidable 
proof  we reduce from the halting problem for abacus machines  which is undecidable  an abacus machine consists of a tuple of integer variables v            vk  ranging over all positive integers
including     and a tuple of instructions i            in   a state is given by the content of v            vk plus
the index pc of the active instruction  the machine stops iff it reaches a state where pc   n  all vi
are initially    and pc is initially    there are two kinds of instructions  ii   inc j  goto ii increments the value of vj and jumps to pc   i   ii   dec j  branch ii   ii  asks whether vj      if
so  it jumps to pc   i    otherwise  it decrements the value of vj and jumps to pc   i   
we map an arbitrary abacus program to a wsc f wd instance as follows 
 predicates  number v   zero v   succ v    v   value   v  t           valuek  v  t   instruction   t  
        instructionn  t 
 background theory  none  i e   the trivial theory 
 operators 
 an operator h v    number v     v      number v     succ v    v  i
 for instructions of the form ii   inc j  goto ii   the operator
h v            vk   t  
 instructioni  t   value   v    t           valuek  vk   t   succ v    vj    
 t   
 instructioni  t    value   v    t            valuej   vj    t    valuej  v    t   
valuej    vj     t            valuek  vk   t   i 
 for instructions of the form ii   dec j  branch ii   ii    the operators
h v            vk   t  
 instructioni  t   value   v    t           valuek  vk   t   succ vj   v     
 t   
 instructioni   t    value   v    t            valuej   vj    t    valuej  v    t   
valuej    vj     t            valuek  vk   t   i 
and
h v            vk   t  
 instructioni  t   value   v    t           valuek  vk   t   zero vj    
 t   
 instructioni   t    value   v    t            valuej   vj    t    valuej  vj   t   
valuej    vj     t            valuek  vk   t   i 
   

fih offmann   b ertoli   h elmert   p istore

 initial constants  v    t 
 initial literals  number v   zero v   value   v    t     valuek  v    t   instruction   t   
 goal condition  t instructionn  t 
we now describe the intuitive meaning of the constants and predicates  there are two kinds of constants  numbers  which represent natural numbers  including     and time points  which represent
computation steps of the abacus machine  variables that refer to time points are denoted as t or t
above  all other variables represent numbers 
three predicates refer to numbers exclusively  number v  is true iff v encodes a natural number
 and not a time point   zero v  is true iff v encodes the number    and succ v    v  is true iff v 
encodes the number that is one larger than the number encoded by v  the reduction does not
enforce that every number is uniquely represented  e g   there may be several representations of the
number     but such a unique representation is not necessary  it is guaranteed that the number   is
uniquely represented  though 
the remaining predicates encode configurations of the abacus machine  valuei  v  t  is true iff 
at time point t  the i th abacus variable holds the number represented by v  and instructionj  t  is
true iff the current instruction at time point t is ij  
obviously  from an accepting run of the abacus machine we can extract a plan for the task  and
vice versa  this proves the claim 
 
to prove theorems   and    we first establish a core lemma from which both theorems follow
relatively easily  we need a few notations  we denote beliefs  states  in  p  ic   o  c        g  
with b  s   and we denote beliefs  states  in  p    a      g   with b  s   assume a sequence ha           
ai i of non goal achievement actions  then we denote b    res b    ha            ai i  and b    res b   
ha            ai i   note here that we overload the res function to also denote state transitions in the
compiled task formalism  further  for a state s  by c s      c   s ex c        we denote
the constants that exist in s  we denote by c the relation over states s and s that is true iff
c s    c s   and s c s    s  c s    c is an equivalence relation  where equivalent states agree on
which constants exist and howvthey are interpreted  note that every state s reachable
in the compiled
v
task satisfies s    ic     oo effo  eo    note further that ic     oo effo  eo   is actually
satisfiable be prerequisite  unless ic    is unsatisfiable  because the outputs are instantiated with
unique constants and the operators are consistent  for a state s  we define  s    
 
 
 s   s defined over c  
effo  eo   
eo   c s    cs   s cs   is   s    ic    
oo

oo

that is   s  is the equivalence class of states s reachable in the compiled task that agree with s on
which constants exist and how they are interpreted 
lemma   assume a wsc sf wd task without inconsistent operators  let ha            ai i consist of
non goal achievement
actions  and let b    res b    ha            ai i  and b    res b    ha            ai i  
s
then b   sb  s  

proof  the proof is by induction over i  in the base case  we have i      i e   b   b  and b   b   
we have b   
 s   cs   c    is    ic     
   

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

on the other hand  we have b   
 s   c s    c    s    ic    

 

effo  eo   

oo

obviously  the latter is comprised of one equivalence class for each possibility to assign the propositions over c  in a way compliant with ic      this is exactly the claim 
in thes
inductive case  say we add another action as
to ha            ai i  by induction assumption  we
have b   sb  s   we need to prove that res b  a    s res b a   s    obviously  it suffices to prove
s
that  for every s  b  we have res  s   a    s res s a   s    first  say a is not applicable to s  then
s
s is neither applicable in any s   s   and we have res  s   a     s    s res s a   s    second  say
a is applicable to s  then by lemma   we have res s  a   
  cs  ea   i      i   cs   is   i     ic  effa  
on the other hand  we have res  s   a   
 s   ex  s   s   c s     c s   ea   s  c s    s  s    ic    

 

effo  eo   

oo

we can re write the latter into
 s   c s     cs  ea   s  cs   is   s    ic    

 

effo  eo   

oo

obviously  as desired  the latter set is comprised of one equivalence class for each possibility to
assign the propositions over cs  ea in a way compliant with s and ic  effa   this concludes the
argument 
 
theorem    assume a wsc sf wd task  p  ic   o  c        g   without inconsistent operators 
and a plan ha            an i for the compiled task  p    a      g    then the sub sequence of non goal
achievement actions in ha            an i is a plan for  p  ic   o  c        g   
proof  if ic    is unsatisfiable  there is nothing to prove  because the start belief of the original
task is empty  for the non trivial case  first note that  in any plan for the compiled task  the goal
achievement actions can be moved to the back of the plan  hence  without loss of generality  we can
assume that ha            ai i consist entirely of non goal achievement actions  and hai             ai i consist
entirely of goal achievement actions 
s denote b    res b    ha            ai i  and b    res b    ha           
ai i   by lemma    we have b   sb  s   since ha        s
    an i is a plan for the compiled task  every
s  b has a tuple of constants satisfying g   with b   sb  s   it follows that every s  b satisfies
g  
 
theorem    assume a wsc sf wd task  p  ic   o  c        g   without inconsistent operators 
and a plan ha            an i where every operator o appears with at most one instantiation eo of the
outputs  then ha            an i can be extended with goal achievement actions to form a plan for the
compiled task  p    a      g   obtained using the outputs eo  
proof 
s denote b    res b    ha            an i  and b    res b    ha            an i   by
s lemma    we have
b   sb  s   since ha            an i is a plan  every s  b satisfies g   with b   sb  s   it follows that
every s  b has a tuple of constants satisfying g   attaching all the respective goal achievement
actions yields a plan for the compiled task 
 
   

fih offmann   b ertoli   h elmert   p istore

references
 dcomplex org         a web server to browse protein complexes of known  d structures 
http   supfam mrc lmb cam ac uk elevy  dcomplex data hierarchy   root html 
agarwal  v   chafle  g   dasgupta  k   karnik  n   kumar  a   mittal  s     srivastava  b       a  
synthy  a system for end to end composition of web services  journal of web semantics 
     
agarwal  v   dasgupta  k   karnik  n   kumar  a   kundu  a   mittal  s     srivastava  b       b  
a service creation environment based on end to end composition of web services  in   th
international conference on the world wide web  www     pp         
akkiraju  r   srivastava  b   anca andreea  i   goodwin  r     syeda mahmood  t          semaplan  combining planning with semantic matching to achieve web service composition  in
 th international conference on web services  icws    
ambite  j     kapoor  d          automatically composing data workflows with relational descriptions and shim services  in  th international semantic web conference  iswc    
ankolekar  a   burstein  m   hobbs  j   lassila  o   martin  d   mcdermott  d   mcilraith  s  
narayanan  s   paolucci  m   payne  t     sycara  k          daml s  web service description for the semantic web  in  st international semantic web conference  iswc    
au  t  c   kuter  u     nau  d          web service composition with volatile information  in  th
international semantic web conference  iswc    
au  t  c     nau  d          the incompleteness of planning with volatile external information  in
  th european conference on artificial intelligence  ecai    
baader  f   lutz  c   milicic  m   sattler  u     wolter  f          integrating description logics
and action formalisms  first results  in   th national conference on artificial intelligence
 aaai    
bacchus  f          subset of pddl for the aips     planning competition  the aips    planning competition committee 
bertoli  p   pistore  m     traverso  p          automated web service composition by on the fly
belief space search  in   th international conference on automated planning and scheduling
 icaps    
bonet  b     geffner  h          planning with incomplete information as heuristic search in belief
space  in  th international conference on artificial intelligence planning systems  aips    
pp       
bonet  b     geffner  h          planning as heuristic search  artificial intelligence          
    
branden  c     tooze  j          introduction to protein structure  second edition  garland publishing company  new york  isbn            
brayton  r   hachtel  g   mcmullen  c     sangiovanni vincentelli  a          logic minimization
algorithms for vlsi synthesis  kluwer academic publishers 
brewka  g     hertzberg  j          how to do things with worlds  on formalizing actions and
plans  j  logic and computation               
   

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

bryce  d   kambhampati  s     smith  d  e          planning graph heuristics for belief space
search  journal of artificial intelligence research           
burstein  m   hobbs  j   lassila  o   mcdermott  d   mcilraith  s   narayanan  s   paolucci  m  
parsia  b   payne  t   sirin  e   srinivasan  n   sycara  k     martin  d          owl s 
semantic markup for web services  owl s      http   www daml org services owl s      
version     
bylander  t          the computational complexity of propositional strips planning  artificial
intelligence                 
chasman  d   ed            protein structure determination  analysis and applications for drug
discovery  marcel dekker ltd                
chen  y   wah  b     hsu  c          temporal planning using subgoal partitioning and resolution
in sgplan  journal of artificial intelligence research             
cimatti  a   roveri  m     bertoli  p          conformant planning via symbolic model checking
and heuristic search  artificial intelligence                  
constantinescu  i     faltings  b          efficient matchmaking and directory services  in  nd
international conference on web intelligence  wi    
constantinescu  i   faltings  b     binder  w       a   large scale  type compatible service composition  in  nd international conference on web services  icws    
constantinescu  i   faltings  b     binder  w       b   typed based service composition  in   th
international conference on the world wide web  www    
de giacomo  g   lenzerini  m   poggi  a     rosati  r          on the update of description
logic ontologies at the instance level  in   st national conference on artificial intelligence
 aaai    
de giacomo  g   lenzerini  m   poggi  a     rosati  r          on the approximation of instance
level update and erasure in description logics  in   nd national conference of the american
association for artificial intelligence  aaai    
de jonge  m   van der linden  w     willems  r          eservices for hospital equipment  in  th
international conference on service oriented computing  icsoc     pp         
edelkamp  s          promela planning  in   th international spin workshop on model checking
of software  spin    
eiter  t   faber  w   leone  n   pfeifer  g     polleres  a          a logic programming approach to
knowledge state planning  ii  the dlvk system  artificial intelligence                   
eiter  t   faber  w   leone  n   pfeifer  g     polleres  a          a logic programming approach to
knowledge state planning  semantics and complexity  transactions on computational logic 
             
eiter  t     gottlob  g          on the complexity of propositional knowledge base revision  updates  and counterfactuals  artificial intelligence                  
fagin  r   kuper  g   ullman  j     vardi  m          updating logical databases  advances in
computing research         

   

fih offmann   b ertoli   h elmert   p istore

fensel  d   lausen  h   polleres  a   de bruijn  j   stollberg  m   roman  d     domingue  j         
enabling semantic web services  the web service modeling ontology  springer verlag 
fersht  a          structure and mechanism in protein science  a guide to enzyme catalysis and
protein folding  mps  isbn                  
fox  m     long  d          pddl     an extension to pddl for expressing temporal planning
domains  journal of artificial intelligence research            
gerevini  a   saetti  a     serina  i          planning through stochastic local search and temporal
action graphs  journal of artificial intelligence research             
gerevini  a   saetti  a   serina  i     toninelli  p          fast planning in domains with derived
predicates  an approach based on rule action graphs and local search  in   th national conference of the american association for artificial intelligence  aaai    
ginsberg  m     smith  d          reasoning about action i  a possible worlds approach  artificial
intelligence                
giunchiglia  e   lee  j   lifschitz  v   mccain  n     turner  h          nonmonotonic causal
theories  artificial intelligence                  
giunchiglia  e     lifschitz  v          an action language based on causal explanation  preliminary report  in   th national conference on artificial intelligence  aaai    
golden  k          dpadl  an action language for data processing domains  in proc  of the  rd
international nasa planning and scheduling workshop 
golden  k          a domain description language for data processing  in proc  of the workshop
on the future of pddl at icaps   
golden  k   pand  w   nemani  r     votava  p          automating the processing of earth observation data  in proceedings of the  th international symposium on artificial intelligence 
robotics and automation for space 
gomes  c   selman  b   crato  n     kautz  h          heavy tailed phenomena in satisfiability
and constraint satisfaction problems  journal of automated reasoning                 
helmert  m          decidability and undecidability results for planning with numerical state variables  in  th international conference on artificial intelligence planning systems  aips    
helmert  m          the fast downward planning system  journal of artificial intelligence research             
herzig  a          the pma revisited  in  th international conference on principles of knowledge
representation and reasoning  kr    
herzig  a   lang  j   marquis  p     polacsek  t          updates  actions  and planning  in   th
international joint conference on artificial intelligence  ijcai     pp         
herzig  a     rifi  o          propositional belief base update and minimal change  artificial
intelligence                 
hoffmann  j          where ignoring delete lists works  local search topology in planning benchmarks  journal of artificial intelligence research             
hoffmann  j     brafman  r          conformant planning via heuristic forward search  a new
approach  artificial intelligence                  
   

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

hoffmann  j     nebel  b          the ff planning system  fast plan generation through heuristic
search  journal of artificial intelligence research             
katzuno  h     mendelzon  a          on the difference between updating a knowledge base and
revising it  in  nd international conference on principles of knowledge representation and
reasoning  kr    
kona  s   bansal  a   gupta  g     hite  d          automatic composition of semantic web services  in  th international conference on web services  icws    
kumar  a   neogi  a   pragallapati  s     ram  j          raising programming abstraction from
objects to services  in  th international conference on web services  icws    
kuter  u   sirin  e   nau  d   parsia  b     hendler  j          information gathering during planning
for web service composition  journal of web semantics                 
lecue  f     delteil  a          making the difference in semantic web service composition  in
  nd national conference of the american association for artificial intelligence  aaai    
lecue  f     leger  a          a formal model for semantic web service composition  in  th
international semantic web conference  iswc    
li  l     horrocks  i          a software framework for matchmaking based on semantic web
technology  in   th international conference on the world wide web  www    
liberatore  p          the complexity of belief update  artificial intelligence                   
lin  f     reiter  r          state constraints revisited  journal of logic and computation       
       
liu  h   lutz  c   milicic  m     wolter  f       a   reasoning about actions using description
logics with general tboxes  in   th european conference on logics in artificial intelligence
 jelia       
liu  h   lutz  c   milicic  m     wolter  f       b   updating description logic aboxes  in   th international conference on principles of knowledge representation and reasoning  kr    
liu  z   ranganathan  a     riabov  a          a planning approach for message oriented semantic
web service composition  in   nd national conference of the american association for
artificial intelligence  aaai    
long  d     fox  m          the  rd international planning competition  results and analysis 
journal of artificial intelligence research          
lutz  c     sattler  u          a proposal for describing services with dls  in international
workshop on description logics       dl    
mccain  n     turner  h          a causal theory of ramifications and qualifications  in   th
international joint conference on artificial intelligence  ijcai      pp           
mccarthy  j     hayes  p          some philosophical problems from the standpoint of artificial
intelligence  machine intelligence            
mcdermott  d          estimated regression planning for interactions with web services  in  th
international conference on artificial intelligence planning systems  aips    
mcdermott  d   et al          the pddl planning domain definition language  the aips   
planning competition committee 
   

fih offmann   b ertoli   h elmert   p istore

mcdermott  d  v          using regression match graphs to control search in planning  artificial
intelligence                   
mcgeer  p   sanghavi  j   brayton  r  k     sangiovanni vincentelli  a          espressosignature  a new exact minimizer for logic functions  in proceedings of the   th acm ieee
design automation conference  dac     
mcguinness  d  l     van harmelen  f          owl web ontology language overview  w c
recommendation   online at http   www w  org tr owl features  
mcilraith  s     fadel  r          planning with complex actions  in  th international workshop
on non monotonic reasoning  nmr     pp         
mcilraith  s     son  t  c          adapting golog for composition of semantic web services  in
 th international conference on the principles of knowledge representation and reasoning
 kr    
mediratta  a     srivastava  b          applying planning in composition of web services with a
user driven contingent planner  tech  rep  ri        ibm research 
meyer  h     weske  m          automated service composition using heuristic search  in  th
international conference on business process management  bpm    
narayanan  s     mcilraith  s          simulation  verification and automated composition of web
services  in   th international conference on the world wide web  www    
palacios  h     geffner  h          from conformant into classical planning  efficient translations
that may be complete too  in   th international conference on automated planning and
scheduling  icaps    
paolucci  m   kawamura  t   payne  t     sycara  k          semantic matching of web services
capabilities  in  st international semantic web conference  iswc    
pednault  e  p          adl  exploring the middle ground between strips and the situation
calculus  in  st international conference on the principles of knowledge representation and
reasoning  kr    
penberthy  j     weld  d          ucpop  a sound  complete  partial order planner for adl  in
 rd international conference on the principles of knowledge representation and reasoning
 kr     pp         
petsko  g  a     ringe  d          protein structure and function  new science press  isbn
                          
pistore  m   marconi  a   bertoli  p     traverso  p       a   automated composition of web services by planning at the knowledge level  in   th international joint conference on artificial
intelligence  ijcai    
pistore  m   traverso  p     bertoli  p       b   automated composition of web services by planning
in asynchronous domains  in   th international conference on automated planning and
scheduling  icaps    
pistore  m   traverso  p   bertoli  p     marconi  a       c   automated synthesis of composite
bpel ws web services  in  rd international conference on web services  icws    

   

fiw eb s ervice c omposition and p lanning under u ncertainty  a n ew c onnection

ponnekanti  s     fox  a          sword  a developer toolkit for web services composition  in
  th international conference on the world wide web  www    
reiter  r          the frame problem in the situation calculus  a simple solution  sometimes  and a
completeness result for goal regression  in artificial intelligence and mathematical theory of
computation  papers in honour of john mccarthy  pp         
roman  d   keller  u   lausen  h   de bruijn  j   lara  r   stollberg  m   polleres  a   feier  c  
bussler  c     fensel  d          web service modeling ontology  applied ontology       
      
sheshagiri  m   desjardins  m     finin  t          a planner for composing services described in
daml s  in third symposium on adaptive agents and multi agent systems  aamas    
sirin  e   parsia  b   wu  d   hendler  j     nau  d          htn planning for web service composition using shop   journal of web semantics       
sirin  e   hendler  j     parsia  b          semi automatic composition of web services using
semantic descriptions  in workshop web services at iceis   
sirin  e     parsia  b          planning for semantic web services  in workshop semantic web
services at iswc   
sirin  e   parsia  b     hendler  j          composition driven filtering and selection of semantic
web services  in aaai fall symposium on semantic web services 
sirin  e   parsia  b     hendler  j          template based composition of semantic web services 
in aaai fall symposium on agents and search 
smith  d  e     weld  d          conformant graphplan  in   th national conference of the
american association for artificial intelligence  aaai     
srivastava  b          automatic web services composition using planning  in knowledge based
computer systems  kbcs     pp         
thakkar  s   ambite  j  l     knoblock  c          composing  optimizing  and executing plans for
bioinformatics web services  vldb journal  special issue on data management  analysis
and mining for life sciences                
thiebaux  s   hoffmann  j     nebel  b          in defense of pddl axioms  artificial intelligence 
              
winslett  m          reasoning about actions using a possible models approach  in  th national
conference of the american association for artificial intelligence  aaai    
winslett  m          updating logical databases  cambridge university press 
zhan  r   arpinar  b     aleman meza  b          automatic composition of semantic web services  in  st international conference on web services  icws    

   

fi