journal artificial intelligence research               

submitted        published      

complex question answering  unsupervised learning
approaches experiments
yllias chali

chali cs uleth ca

university lethbridge
lethbridge  ab  canada  t k  m 

shafiq r  joty

rjoty cs ubc ca

university british columbia
vancouver  bc  canada  v t  z 

sadid a  hasan

hasan cs uleth ca

university lethbridge
lethbridge  ab  canada  t k  m 

abstract
complex questions require inferencing synthesizing information multiple
documents seen kind topic oriented  informative multi document summarization goal produce single text compressed version set
documents minimum loss relevant information  paper  experiment
one empirical method two unsupervised statistical machine learning techniques 
k means expectation maximization  em   computing relative importance
sentences  compare results approaches  experiments show
empirical approach outperforms two techniques em performs better
k means  however  performance approaches depends entirely feature
set used weighting features  order measure importance
relevance user query extract different kinds features  i e  lexical  lexical semantic  cosine similarity  basic element  tree kernel based syntactic shallow semantic 
document sentences  use local search technique learn weights
features  best knowledge  study used tree kernel functions
encode syntactic semantic information complex tasks computing
relatedness query sentences document sentences order generate
query focused summaries  or answers complex questions   methods
generating summaries  i e  empirical  k means em  show effects syntactic
shallow semantic features bag of words  bow  features 

   introduction
vast increase amount online text available demand access different types information led renewed interest broad range information
retrieval  ir  related areas go beyond simple document retrieval  areas
include question answering  topic detection tracking  summarization  multimedia retrieval  chemical biological informatics  text structuring  text mining  genomics  etc 
automated question answering  qa the ability machine answer questions  simple
complex  posed ordinary human languageis perhaps exciting technological development past six seven years  strzalkowski   harabagiu        
c
    
ai access foundation  rights reserved 

fichali  joty    hasan

expectations already tremendous  reaching beyond discipline  a subfield natural
language processing  nlp   itself 
tool finding documents web  search engines proven adequate 
although limitation expressiveness user terms query formulation  certain limitations exist search engine query  complex
question answering tasks require multi document summarization aggregated
search  faceted search  represents information need cannot answered
single document  example  look comparison average number
years marriage first birth women u s   asia  europe  answer
likely contained multiple documents  multi document summarization useful
type query currently tool market designed meet
kind information need 
qa research attempts deal wide range question types including  fact  list 
definition  how  why  hypothetical  semantically constrained  cross lingual questions 
questions  call simple questions  easier answer  example 
question  president bangladesh  asks persons name  type
question  i e  factoid  requires small snippets text answer  again  question 
countries pope john paul ii visited  sample list question  asking
list small snippets text 
made substantial headway factoid list questions  researchers
turned attention complex information needs cannot answered
simply extracting named entities  persons  organizations  locations  dates  etc   documents  unlike informationally simple factoid questions  complex questions often seek multiple different types information simultaneously presuppose one single
answer meet information needs  example  factoid question like 
accurate hiv tests  safely assumed submitter question looking number range numbers  however  complex questions like 
causes aids  wider focus question suggests submitter
may single well defined information need therefore may amenable
receiving additional supporting information relevant  as yet  undefined informational goal  harabagiu  lacatusu    hickl         questions require inferencing
synthesizing information multiple documents 
well known qa systems korean navers knowledge search   
pioneers community qa  tool allows users ask question get
answers users  navers knowledge roughly    times entries
wikipedia  used millions korean web users given day  people
say koreans addicted internet naver  january      knowledge search database included    million pages user generated information 
another popular answer service yahoo  answers community driven knowledge market website launched yahoo   allows users submit questions
answered answer questions users  people vote best answer  site
gives members chance earn points way encourage participation based
naver model  december       yahoo  answers    million users   
   http   kin naver com 

 

ficomplex question answering  unsupervised approaches

million answers  google qa system  based paid editors launched
april      fully closed december      
however  computational linguistics point view information synthesis
seen kind topic oriented informative multi document summarization  goal
produce single text compressed version set documents minimum loss
relevant information  unlike indicative summaries  which help determine whether
document relevant particular topic   informative summaries must attempt find
answers 
paper  focus extractive approach summarization subset
sentences original documents chosen  contrasts abstractive summarization information text rephrased  although summaries produced
humans typically extractive  state art summarization systems
based extraction achieve better results automated abstraction  here 
experimented one empirical two well known unsupervised statistical machine
learning techniques  k means em evaluated performance generating topicoriented summaries  however  performance approaches depends entirely
feature set used weighting features  order measure importance
relevance user query extract different kinds features  i e  lexical  lexical
semantic  cosine similarity  basic element  tree kernel based syntactic shallow semantic 
document sentences  used gradient descent local search technique
learn weights features 
traditionally  information extraction techniques based bow approach augmented language modeling  task requires use complex semantics  approaches based bow often inadequate perform fine level textual
analysis  improvements bow given use dependency trees syntactic parse trees  hirao    suzuki  isozaki    maeda        punyakanok  roth    yih       
zhang   lee      b   adequate dealing complex questions
whose answers expressed long articulated sentences even paragraphs  shallow
semantic representations  bearing compact information  could prevent sparseness
deep structural approaches weakness bow models  moschitti  quarteroni 
basili    manandhar         pinpointing answer question relies deep understanding semantics both  attempting application syntactic semantic
information complex qa seems natural  best knowledge  study used
tree kernel functions encode syntactic semantic information complex tasks
computing relatedness query sentences document sentences
order generate query focused summaries  or answers complex questions  
methods generating summaries  i e  empirical  k means em  show effects
syntactic shallow semantic features bow features 
past three years  complex questions focus much attention
automatic question answering multi document summarization  mds  communities  typically  current complex qa evaluations including      aquaint
relationship qa pilot       text retrieval conference  trec  relationship qa task 
trec definition  and others  require systems return unstructured lists can   http   answers google com 

 

fichali  joty    hasan

didate answers response complex question  however recently  mds evaluations  including                 document understanding conference  duc   tasked
systems returning paragraph length answers complex questions responsive 
relevant  coherent 
experiments based duc      data show including syntactic semantic features improves performance  comparison among approaches shown 
comparing duc      participants  systems achieve top scores
statistically significant difference results system results duc
     best system 
paper organized follows  section   focuses related work  section  
gives brief description intended final model  section   describes features
extracted  section   discusses learning issues presents learning approaches 
section   discusses remove redundant sentences adding final
summary  section   describes experimental study  conclude discuss future
directions section   

   related work
researchers world working query based summarization trying different
directions see methods provide best results 
number sentence retrieval systems based ir  information retrieval 
techniques  systems typically dont use lot linguistic information  still
deserve special attention  murdock croft        propose translation model specifically
monolingual data  show significantly improves sentence retrieval query
likelihood  translation models train parallel corpus used corpus question answer pairs  losada        presents comparison multiple bernoulli models
multinomial models context sentence retrieval task shows multivariate bernoulli model really outperform popular multinomial models retrieving
relevant sentences  losada fernandez        propose novel sentence retrieval method
based extracting highly frequent terms top retrieved documents  results reinforce idea top retrieved data valuable source enhance retrieval systems 
specially true short queries usually query sentence matching terms  argue method improves significantly precision top ranks
handling poorly specified information needs 
lexrank method addressed erkan radev        successful
generic multi document summarization  topic sensitive lexrank proposed otterbacher  erkan  radev         lexrank  set sentences document cluster
represented graph nodes sentences  links nodes induced similarity relation sentences  system ranks sentences
according random walk model defined terms inter sentence similarities
similarities sentences topic description question 
concepts coherence cohesion enable us capture theme text  coherence represents overall structure multi sentence text terms macro level
relations clauses sentences  halliday   hasan         cohesion  defined
halliday hasan         property holding text together one single grammat 

ficomplex question answering  unsupervised approaches

ical unit based relations  i e  ellipsis  conjunction  substitution  reference  lexical
cohesion  various elements text  lexical cohesion defined cohesion
arises semantic relations  collocation  repetition  synonym  hypernym  hyponym  holonym  meronym  etc   words text  morris   hirst        
lexical cohesion among words represented lexical chains sequences
semantically related words  summarization methods based lexical chain first extract nouns  compound nouns named entities candidate words  li  sun  kit   
webster         using wordnet  systems find semantic similarity
nouns compound nouns  lexical chains built two steps 
   building single document strong chains disambiguating senses words 
   building multi chain merging strongest chains single documents
one chain 
systems rank sentences using formula involves a  lexical chain  b  keywords query c  named entities  example  li et al         uses following
formula 
score   p  chain    p  query    p  namedentity 
p  chain  sum scores chains whose words come
candidate sentence  p  query  sum co occurrences key words topic
sentence  p  namedentity  number name entities existing topic
sentence  three coefficients   set empirically  top ranked
sentences selected form summary 
harabagiu et al         introduce new paradigm processing complex questions
relies combination  a  question decompositions   b  factoid qa techniques 
 c  multi document summarization  mds  techniques  question decomposition
procedure operates markov chain  is  following random walk mixture
model bipartite graph relations established concepts related topic
complex question subquestions derived topic relevant passages manifest
relations  decomposed questions submitted state of the art qa system
order retrieve set passages later merged comprehensive answer mds system  show question decompositions using method
significantly enhance relevance comprehensiveness summary length answers
complex questions 
approaches based probabilistic models  pingali  k     varma 
      toutanova  brockett  gamon  jagarlamudi  suzuki    vanderwende         pingali
et al         rank sentences based mixture model component
model statistical model 
score s    qiscore s         qf ocus s  q 

   

   wordnet  http   wordnet princeton edu   widely used semantic lexicon english language 
groups english words  i e  nouns  verbs  adjectives adverbs  sets synonyms called synsets 
provides short  general definitions  i e  gloss definition   records various semantic relations
synonym sets 

 

fichali  joty    hasan

score s  score sentence s  query independent score  qiscore 
query dependent score  qfocus  calculated based probabilistic models  toutanova
et al         learns log linear sentence ranking model maximizing three metrics
sentence goodness   a  rouge oracle   b  pyramid derived   c  model frequency 
scoring function learned fitting weights set feature functions sentences
document set trained optimize sentence pair wise ranking criterion 
scoring function adapted apply summaries rather sentences take
account redundancy among sentences 
pingali et al         reduce document sentences dropping words
contain important information  toutanova et al          vanderwende  suzuki 
brockett         zajic  lin  dorr  schwartz        heuristically decompose
document sentences smaller units  apply small set heuristics parse
tree create alternatives original sentence  possibly multiple 
simplified versions available selection 
approaches multi document summarization try cluster sentences
together  guo stylios        use verb arguments  i e  subjects  times  locations
actions  clustering  sentence method establishes indices information
based verb arguments  subject first index  time second  location third
action fourth   sentences closest subjects index put
cluster sorted according temporal sequence earliest
latest  sentences spaces locations index value cluster
marked out  clusters ranked based sizes top    clusters
chosen  then  applying cluster reduction module system generates compressed
extract summaries 
approaches recognizing textual entailment  sentence alignment 
question answering use syntactic and or semantic information order measure
similarity two textual units  indeed motivated us include syntactic
semantic features get structural similarity document sentence query
sentence  discussed section       maccartney  grenager  de marneffe  cer  manning
       use typed dependency graphs  same dependency trees  represent text
hypothesis  try find good partial alignment typed dependency
graphs representing hypothesis  contains n nodes  text  graph contains
nodes  search space o  m     n   use incremental beam search combined
node ordering heuristic approximate global search space possible
alignments  locally decomposable scoring function chosen score
alignment sum local node edge alignment scores  scoring measure
designed favor alignments align semantically similar subgraphs  irrespective
polarity  reason  nodes receive high alignment scores words represent
semantically similar  synonyms antonyms receive highest score unrelated
words receive lowest  alignment scores incorporate local edge scores based
shape paths nodes text graph correspond adjacent
nodes hypothesis graph  final step make decision whether
hypothesis entailed text conditioned typed dependency graphs
well best alignment them  make decision use supervised
 

ficomplex question answering  unsupervised approaches

statistical logistic regression classifier  with feature space    features  gaussian
prior parameter regularization 
hirao et al         represent sentences using dependency tree path  dtp  incorporate syntactic information  apply string subsequence kernel  ssk  measure
similarity dtps two sentences  introduce extended string
subsequence kernel  esk  incorporate semantics dtps  kouylekov magnini
       use tree edit distance algorithms dependency trees text
hypothesis recognize textual entailment  according approach  text entails
hypothesis h exists sequence transformations  i e  deletion  insertion
substitution  applied obtain h overall cost certain
threshold  punyakanok et al         represent question sentence containing
answer dependency trees  add semantic information  i e  named entity 
synonyms related words  dependency trees  apply approximate
tree matching order decide similar given pair trees are  use
edit distance matching criteria approximate tree matching  methods
show improvement bow scoring methods 

   approach
accomplish task answering complex questions extract various important features sentences document collection measure relevance
query  sentences document collection analyzed various levels
document sentences represented vector feature values  feature set
includes lexical  lexical semantic  statistical similarity  syntactic semantic features 
graph based similarity measures  chali   joty      b   reimplemented many
features successfully applied many related fields nlp 
use simple local search technique fine tune feature weights  use
statistical clustering algorithms  em k means select relevant sentences
summary generation  experimental results show systems perform better
include tree kernel based syntactic semantic features though summaries based
syntactic semantic feature achieve good results  graph based cosine
similarity lexical semantic features important selecting relevant sentences 
find local search technique outperforms two em performs
better k means based learning  later sections describe subparts
systems details 

   feature extraction
section  describe features used score sentences 
provide detailed examples  show get feature values  first describe
syntactic semantic features introducing work  follow
detailed description features commonly used question answering
summarization communities 
   query document sentences used examples taken duc      collection 

 

fichali  joty    hasan

    syntactic shallow semantic features
task query based summarization requires use complex syntactic
semantics  approaches bow often inadequate perform fine level
textual analysis  importance syntactic semantic features context
described zhang lee      a   moschitti et al          bloehdorn moschitti
     a   moschitti basili        bloehdorn moschitti      b  
effective way integrate syntactic semantic structures machine learning algorithms use tree kernel functions  collins   duffy        moschitti   quarteroni 
      successfully applied question classification  zhang   lee      a 
moschitti   basili         syntactic semantic information used effectively measure similarity two textual units maccartney et al          best
knowledge  study used tree kernel functions encode syntactic semantic
information complex tasks computing relatedness query
sentences document sentences  another good way encode shallow syntactic
information use basic elements  be   hovy  lin  zhou    fukumoto       
uses dependency relations  experiments show including syntactic semantic
features improves performance sentence selection complex question answering
task  chali   joty      a  
      encoding syntactic structures
basic element  be  overlap measure shallow syntactic information based dependency relations proved effective finding similarity two textual
units  hirao et al          incorporate information using basic elements
defined follows  hovy et al         
head major syntactic constituent  noun  verb  adjective adverbial phrases  
expressed single item 
relation head be single dependent  expressed triple 
 head modifier relation  
triples encode syntactic information one decide whether two units
match not  easily longer units  hovy et al          extracted bes
sentences  or query  using package distributed isi   
get bes sentence  computed likelihood ratio  lr 
following zhou  lin  hovy         sorting bes according lr scores produced
be ranked list  goal generate summary answer users questions 
ranked list bes way contains important bes top may may
relevant users questions  filter bes checking whether contain
word query word queryrelatedwords  defined section      
example  consider following sentence get score         
query  describe steps taken worldwide reaction prior introduction euro
january          include predictions expectations reported press 
   website http   www isi edu  cyl be

 

ficomplex question answering  unsupervised approaches

sentence  frankfurt based body said annual report released today
decided two themes new currency  history european civilization
abstract concrete paintings 
score         
here  decided themes obj considered contain word
query words query relevant words report annual mod taken
contains query word report  way  filter bes related
query  score sentence sum scores divided number bes
sentence  limiting number top bes contribute calculation
sentence scores remove bes little importance sentences
fewer important bes  set threshold     topmost     bes ranked
list contribute normalized sentence score computation  paper 
set threshold took bes counted calculating scores
sentences 
tree kernels approach order calculate syntactic similarity query
sentence first parse sentence well query syntactic tree
 moschitti        using parser charniak         calculate similarity
two trees using tree kernel  reimplemented tree kernel model
proposed moschitti et al         
build trees  next task measure similarity trees 
this  every tree represented dimensional vector v t      v   t    v   t    vm  t    
i th element vi  t   number occurrences i th tree fragment tree
  tree fragments tree sub trees include least one production
restriction production rules broken incomplete parts  moschitti
et al          figure   shows example tree portion subtrees 

figure     a  example tree  b  sub trees np covering press 
implicitly enumerate possible tree fragments         m  fragments
axis m dimensional space  note could done implicitly since
number extremely large  this  collins duffy        define tree
kernel algorithm whose computational complexity depend m 
tree kernel two trees t  t  actually inner product v t    v t    
 

fichali  joty    hasan

k t    t      v t    v t   

   

define indicator function ii  n    sub tree seen rooted node n
  otherwise  follows 

vi  t     

x

x

ii  n     vi  t     

n  n 

ii  n   

   

n  n 

n  n  set nodes t  t  respectively  so  derive 
k t    t      v t    v t     

x

vi  t   vi  t   



x

 

x x

n  n  n  n 

x

 

x

ii  n   ii  n   



c n    n   

   

n  n  n  n 

define c n    n      ii  n   ii  n     next  note c n    n   
computed polynomial time due following recursive definition 
p

   productions n  n  different c n    n       
   productions n  n  same  n  n  pre terminals 
c n    n       
   else productions n  n  pre terminals 
nc n   

c n    n     



     c ch n    j   ch n    j   

   

j  

nc n    number children n  tree  productions n 
n  nc n      nc n     i th child node n  ch n    i  
cases query composed two sentences compute similarity
document sentence  s  query sentences  qi   take
average scores syntactic feature value 
syntactic similarity value  

pn

i   k qi   s 

n

n number sentences query q sentence consideration  tk similarity value  tree kernel  sentence query
sentence q based syntactic structure  example  following sentence
query q get score 
  

ficomplex question answering  unsupervised approaches

figure    example semantic trees
query  q   describe steps taken worldwide reaction prior introduction euro
january          include predictions expectations reported press 
sentence  s   europes new currency  euro  rival u s  dollar international
currency long term  der spiegel magazine reported sunday 
scores        
average score      
      semantic features
though introducing syntactic information gives improvement bow  use
syntactic parses  adequate dealing complex questions whose answers expressed long articulated sentences even paragraphs  shallow semantic
representations  bearing compact information  could prevent sparseness deep
structural approaches weakness bow models  maccartney et al         moschitti
et al         
initiatives propbank  pb   kingsbury   palmer        made design
accurate automatic semantic role labeling  srl  systems assert  hacioglu  pradhan  ward  martin    jurafsky        possible  hence  attempting application srl
qa seems natural pinpointing answer question relies deep understanding
semantics both  example  consider pb annotation 
 arg  all  target use  arg  french franc  arg  currency 
annotation used design shallow semantic representation
matched semantically similar sentences  e g 
 arg  vatican  target use  arg  italian lira  arg  currency 
order calculate semantic similarity sentences first represent
annotated sentence  or query  using tree structures figure   called semantic tree
 st  proposed moschitti et al          semantic tree arguments replaced
important wordoften referred semantic head  look noun
first  verb  adjective  adverb find semantic head argument 
none present take first word argument semantic head 
  

fichali  joty    hasan

figure    two sts composing stn
however  sentences rarely contain single predicate  rather typically propositions contain one subordinate clauses  instance  let us consider slight modification
second sentence  vatican  located wholly within italy uses italian lira
currency  here  main predicate uses subordinate predicate located 
srl system outputs following two annotations 
     arg  vatican located wholly within italy  target uses  arg  italian
lira  arg  currency 
     arg  vatican  target located   argm loc wholly  argm loc within
italy  uses italian lira currency
giving sts figure    see figure   a   argument node
corresponds entire subordinate clause label leaf st  e g  leaf
arg    st node actually root subordinate clause figure   b  
taken separately  sts express whole meaning sentence  hence 
accurate define single structure encoding dependency two
predicates figure   c   refer kind nested sts stns 
note tree kernel  tk  function defined section       computes number
common subtrees two trees  subtrees subject constraint
nodes taken none children original tree  though
definition subtrees makes tk function appropriate syntactic trees  well
suited semantic trees  st   instance  although two sts figure   share
subtrees rooted st node  kernel defined computes match 
critical aspect steps               tk function productions
two evaluated nodes identical allow match descendants 
means common substructures cannot composed node
children effective st representation would require  moschitti et al         solve
problem designing shallow semantic tree kernel  sstk  allows portions
st match 
shallow semantic tree kernel  sstk  reimplemented sstk according
model given moschitti et al          sstk based two ideas  first  changes
  

ficomplex question answering  unsupervised approaches

st  shown figure   adding slot nodes  accommodate argument labels
specific order fixed number slots  possibly filled null arguments
encode possible predicate arguments  leaf nodes filled wildcard character  
may alternatively accommodate additional information  slot nodes used
way adopted tk function generate fragments containing one
children example shown frames  b   c  figure    previously
pointed out  arguments directly attached root node kernel function
would generate structure children  or structure children  i e 
empty   moschitti et al         

figure    semantic tree fragments
second  original tree kernel would generate many matches slots filled
null label set new step   tk calculation 
    n   or n    pre terminal node child label null  c n    n        
subtract one unit c n    n     step   
nc n   

    c n    n     



j  

     c ch n    j   ch n    j     

   

changes generate new c which  substituted  in place original c  
eq     gives new sstk 
example  following sentence query q get semantic score 
query  q   describe steps taken worldwide reaction prior introduction euro
january          include predictions expectations reported press 
sentence  s   frankfurt based body said annual report released today
decided two themes new currency history european civilization
abstract concrete paintings 
scores       
average score   
  

fichali  joty    hasan

    lexical features
here  discuss lexical features commonly used qa
summarization communities  reimplemented research 
      n gram overlap
n gram overlap measures overlapping word sequences candidate document
sentence query sentence  view measure overlap scores  query pool
sentence pool created  order create query  or sentence  pool  took
query  or document  sentence created set related sentences replacing
content words  first sense synonyms using wordnet  example  given stemmed
document sentence  john write poem  sentence pool contains  john compose
poem  john write verse form along given sentence 
measured recall based n gram scores sentence p using following formula 
n gramscore p     maxi  maxj n gram si   qj   
p
gram countmatch  gramn  
n gram s  q    p n
gramn count  gramn  

   
   

n stands length n gram  n                countmatch  gramn  
number n grams co occurring query candidate sentence  qj j th
sentence query pool  si i th sentence sentence pool sentence p  
  gram overlap measure
  gram overlap score measures number words common sentence hand
query related words  computed follows 
 gram overlap score  

p

countmatch  w   
w  count  w   

w 

p

   

set content words candidate sentence countmatch
number matches sentence content words query related words  count  gramn  
number w   
note order measure   gram score took query related words instead
exact query words  motivation behind sentence word s 
exactly query words synonyms  hypernyms  hyponym gloss
words  get counted 
example 
query describe steps taken worldwide reaction prior introduction euro
january          include predictions expectations reported press 
sentence frankfurt based body said annual study released today
decided two themes new currency  history european civilization
abstract concrete paintings 
   hence forth content words nouns  verbs  adverbs adjectives 

  

ficomplex question answering  unsupervised approaches

  gram score          after normalization    
note sentence   gram overlap score         even though
exact word common query words  got score sentence word
study synonym query word report 
n gram overlap measures
above  calculate n gram overlap scores  example  considering
following query sentence document sentence  from duc      collection    
matching   grams          of euro  january january     hence  employing
formula given above  get following   gram score normalization    gram score
found accordingly 
query sentence  describe steps taken worldwide reaction prior introduction
euro january          include predictions expectations reported
press 
document sentence  despite skepticism actual realization single european currency scheduled january          preparations design
euro note already begun 
  gram         
  gram        
      lcs wlcs
sequence w    w    w         wn   subsequence another sequence x    x    x         xm    
exists strict increasing sequence  i    i           indices x
j              n xij   wj  cormen  leiserson    rivest         given two sequences
s  s    longest common subsequence  lcs  s  s  common subsequence
maximum length  lin        
longer lcs two sentences is  similar two sentences are 
used lcs based f measure estimate similarity document sentence
length query sentence q length n follows 
lcs s  q 

lcs s  q 
plcs  s  q   
n
flcs  s  q         plcs  s  q    rlcs  s  q 
rlcs  s  q   

    
    
    

lcs s  q  length longest common subsequence q 
constant determines importance precision recall  computing
lcs measure document sentence query sentence viewed sequence words 
   normalize feature values corresponding sentence respect entire context
particular document 

  

fichali  joty    hasan

intuition longer lcs two similar are 
recall  rlcs  s  q   ratio length longest common subsequence
q document sentence length measures completeness  whereas precision
 plcs  s  q   ratio length longest common subsequence q
query sentence length measure exactness  obtain equal importance
precision recall set value      equation    called lcs based
f measure  notice flcs   when  s q  flcs   nothing
common q 
one advantage using lcs require consecutive matches insequence matches reflect sentence level word order n grams  advantage
automatically includes longest in sequence common n grams  therefore  predefined n gram length necessary  moreover  property value less
equal minimum unigram  i e    gram  f measure q  unigram recall
reflects proportion words present q  unigram precision
proportion words q s  unigram recall precision count
co occurring words regardless orders  lcs counts in sequence co occurrences 
awarding credit in sequence unigram matches  lcs measure captures
sentence level structure natural way  consider following example 
s  john shot thief
s  john shot thief
s  thief shot john
using s  reference sentence  s  s  sentences consideration s 
s  would   gram score since one bigram  i e  thief 
common s   however  s  s  different meanings  case lcs s 
score          s  score                therefore  s  better
s  according lcs 
however  lcs suffers one disadvantage counts main in sequence
words  therefore  alternative lcses shorter sequences reflected
final score  example  given following candidate sentence 
s  thief john shot
using s  reference  lcs counts either thief john shot both 
therefore  s  lcs score s    gram would prefer s  s  
order measure lcs score sentence took similar approach previous section using wordnet  i e  creation sentence pool query pool   calculated
lcs score using following formula 

lcs score   maxi  maxj flcs  si   qj   

    

qj j th sentence query pool  si i th sentence
sentence pool 
  

ficomplex question answering  unsupervised approaches

basic lcs problem differentiate lcses different spatial
relations within embedding sequences  lin         example  given reference
sequence two candidate sequences y  y  follows 
s  b c e f g
y    b c h k
y    h b k c
y  y  lcs score  however  y  better choice y 
y  consecutive matches  improve basic lcs method store length
consecutive matches encountered far regular two dimensional dynamic program table
computing lcs  call weighted lcs  wlcs  use k indicate length
current consecutive matches ending words xi yj   given two sentences x y 
wlcs score x computed using similar dynamic programming
procedure stated lin         use wlcs advantage measuring
similarity taking words higher dimension string kernels indeed
reduces time complexity  before  computed wlcs based f measure
way using query pool sentence pool 
w lcs score   maxi  maxj fwlcs  si   qj   

    

example 
query sentence  describe steps taken worldwide reaction prior introduction
euro january          include predictions expectations reported
press 
document sentence  despite skepticism actual realization single european currency scheduled january          preparations design
euro note already begun 
find   matching strings   of   euro      january  longest common
subsequence considering sentence related sentences  wlcs set
weight      normalization  get following lcs wlcs scores
sentence applying formula 
lcs score         
wlcs score         
      skip bigram measure
skip bigram pair words sentence order allowing arbitrary gaps  skipbigram measures overlap skip bigrams candidate sentence query
sentence  lin         rely query pool sentence pool using
wordnet  considering following sentences 
  

fichali  joty    hasan

s  john shot thief
s  john shoot thief
s  thief shoot john
s  thief john shot
get sentence c        skip bigrams    example  s  following
skip bigrams   john shot  john the  john thief  shot the  shot thief
thief  s  three skip bi gram matches s   john the  john thief  thief  
s  one skip bi gram match s   the thief   s  two skip bi gram matches
s   john shot  thief  
skip bi gram score document sentence length query
sentence q length n computed follows 
skip   s  q 
c m    
skip   s  q 
pskip   s  q   
c n    
fskip   s  q         pskip   s  q    rskip   s  q 
rskip   s  q   

    
    
    

skip   s  q  number skip bi gram matches q 
constant determines importance precision recall  set value
    associate equal importance precision recall  c combination
function  call equation    skip bigram based f measure  computed skip
bigram based f measure using formula 

skip bigram   maxi  maxj fskip   si   qj   

    

example  given following query sentence  get   skip bigrams   on   
january    january       euro                january on  
applying equations above  get skip bi gram score         normalization 
query describe steps taken worldwide reaction prior introduction euro
january          include predictions expectations reported press 
sentence despite skepticism actual realization single european currency
scheduled january          preparations design euro note
already begun 
skip bi gram score         
   c n  r   

n 
r  nr  

  

ficomplex question answering  unsupervised approaches

note skip bi gram counts in order matching word pairs lcs counts
one longest common subsequence  put constraint maximum skip distance 
dskip   two in order words form skip bi gram avoids spurious matches
from  example  set dskip   equivalent bi gram
overlap measure  lin         set dskip   word pairs   words
apart form skip bi grams  experiment set dskip     order ponder
  words apart get skip bi grams 
modifying equations             allow maximum skip distance limit
straightforward  following lin        count skip bi gram matches  skip   s  q  
within maximum skip distance replace denominators equations
actual numbers within distance skip bi grams reference sentence
candidate sentence respectively 
      head head related words overlap
number head words common two sentences indicate much
relevant other  order extract heads sentence  or query  
sentence  or query  parsed minipar  dependency tree extract
heads call exact head words  example  head word sentence  john
eats rice eat 
take synonyms  hyponyms  hypernyms   query head words
sentence head words form set words call head related words 
measured exact head score head related score follows 
p

w  headset countmatch  w   

    

w  headrelset countmatch  w   

    

exactheadscore  
headrelatedscore  

p

p

p

w  headset count  w   

w  headrelset count  w   

headset set head words sentence countmatch number
matches headset query sentence  headrelset set
synonyms  hyponyms  hypernyms head words sentence countmatch
number matches head related words query sentence 
example  list head words query sentence measures 
query  describe steps taken worldwide reaction prior introduction euro
january          include predictions expectations reported press 
heads query  include  reaction  step  take  describe  report  euro  introduction  press 
prediction        expectation
sentence  frankfurt based body said annual report released today
decided two themes new currency  history european civilization
abstract concrete paintings 
   http   www cs ualberta ca  lindek minipar htm
    hypernym hyponym levels restricted     respectively 

  

fichali  joty    hasan

heads sentence  history  release  currency  body  report painting  say  civilization 
theme  decide 
exact head score 

 
  

      

head related score   
    lexical semantic features
form set words call queryrelatedwords taking content words
query  first sense synonyms  nouns hypernyms hyponyms  nouns
gloss definitions using wordnet 
      synonym overlap
synonym overlap measure overlap list synonyms content
words extracted candidate sentence query related words  computed
follows 
synonym overlap score  

p

w  synset countmatch  w   
w  synset count  w   

p

    

synset synonym set content words sentence countmatch
number matches synset query related words 
      hypernym hyponym overlap
hypernym hyponym overlap measure overlap list hypernyms  level
   hyponyms  level    nouns extracted sentence consideration
query related words  computed follows 
hypernym hyponym overlap score  

p

h  hypset countmatch  h   
h  hypset count  h   

p

    

hypset hyponym hyponym set nouns sentence countmatch
number matches hypset query related words 
      gloss overlap
gloss overlap measure overlap list content words extracted
gloss definition nouns sentence consideration query related
words  computed follows 
gloss overlap score  

p

g  glossset countmatch  g   

p

g  glossset count  g   

    

glossset set content words  i e  nouns  verbs adjectives  taken
gloss definition nouns sentence countmatch number matches
glossset query related words 
  

ficomplex question answering  unsupervised approaches

example 
example  given query following sentence gets synonym overlap score
         hypernym hyponym overlap score           gloss overlap score           
query describe steps taken worldwide reaction prior introduction euro
january          include predictions expectations reported press 
sentence frankfurt based body said annual report released today
decided two themes new currency  history european civilization
abstract concrete paintings 
synonym overlap score         
hypernym hyponym overlap score           
gloss overlap score           
    statistical similarity measures
statistical similarity measures based co occurrence similar words corpus 
two words termed similar belong context  used thesaurus
provided dr  dekang lin   purpose  used two statistical similarity
measures 
dependency based similarity measure
method uses dependency relations among words order measure similarity  lin      b   extracts dependency triples uses statistical approach
measure similarity  using given corpus one retrieve similar words
given word  similar words grouped clusters 
note word one cluster  cluster represents
sense word similar words sense  so  selecting right cluster
word problem  goals are  i  create bag similar words query
words ii  get bag similar words  dependency based  query words
measure overlap score sentence words 
creating bag similar words 
query word extract clusters thesaurus  order
determine right cluster query word measure overlap score
query related words  i e  exact words  synonyms  hypernyms hyponyms gloss 
clusters  hypothesis cluster words common query
related words right cluster assumption first synonym correct
sense  choose cluster word highest overlap score 

overlap scorei  

p

w  queryrelatedw ords countmatch  w   

    

cluster   argmaxi  overlap scorei  

    

w  queryrelatedw ords count  w   

p

    http   www cs ualberta ca  lindek downloads htm

  

fichali  joty    hasan

queryrelatedwords set exact words  synonyms  hyponyms hypernyms 
gloss words words query  i e query words  countmatch number
matches query related words ith cluster similar words 
measuring overlap score 
get clusters query words measured overlap
cluster words sentence words call dependency based similarity measure 

dependencym easure  

w  senw ords countmatch  w   

p

p

w  senw ords count  w   

    

senwords set words sentence countmatch number
matches sentence words cluster similar words 
proximity based similarity measure
similarity computed based linear proximity relationship words
 lin      a   uses information theoretic definition similarity measure
similarity  similar words grouped clusters  took similar approach
measure feature previous section except used different thesaurus 
example 
considering following query sentence get following measures 
query  describe steps taken worldwide reaction prior introduction euro
january          include predictions expectations reported press 
sentence  frankfurt based body said annual report released today
decided two themes new currency  history european civilization
abstract concrete paintings 
dependency based similarity score           
proximity based similarity score            
    graph based similarity measure
erkan radev        used concept graph based centrality rank set sentences
producing generic multi document summaries  similarity graph produced
sentences document collection  graph node represents sentence 
edges nodes measure cosine similarity respective pair sentences 
degree given node indication important sentence is  figure  
shows example similarity graph   sentences 
similarity graph constructed  sentences ranked according
eigenvector centrality  lexrank performed well context generic summarization  apply lexrank query focused context topic sensitive version lexrank
proposed otterbacher et al          followed similar approach order calculate
feature  score sentence determined mixture model relevance
sentence query similarity sentence high scoring sentences 
  

ficomplex question answering  unsupervised approaches

figure    lexrank similarity
relevance question
first stem sentences collection compute word idfs  inverse
document frequency  using following formula 
n   
idfw   log
      sfw




    

n total number sentences cluster  sfw number
sentences word w appears in 
stem questions remove stop words  relevance sentence
question q computed by 
rel s q   

x

wq

log  tfw s      log  tfw q      idfw

    

tfw s tfw q number times w appears q  respectively 
mixture model
previous section measured relevance sentence question
sentence similar high scoring sentences cluster high
score  instance  sentence gets high score based question relevance
model likely contain answer question related sentence  may
similar question itself  likely contain answer  otterbacher et al  
      
capture idea following mixture model 

p s q   

 

x
sim s  v 
rel s q 
p
     d 
p
zc rel z q 
zc sim z  v 
vc

 

p v q 

    

p s q   score sentence given question q  determined sum
relevance question similarity sentences collection 
c set sentences collection  value parameter call
  

fichali  joty    hasan

bias trade off two terms equation set empirically  higher
values prefer relevance question similarity sentences 
denominators terms normalization  although computationally
expensive  equation    calculates sum entire collection since required
model sense global impact voting sentences  measure
cosine similarity weighted word idfs similarity two sentences cluster 

sim x  y    qp

p

wx y

tfw x tfw y  idfw   

 
xi x  tfxi  x idfxi  

qp

 
yi  tfyi  y idfyi  

    

equation    written matrix notation follows 
p    da      d b t p

    

square matrix given index i  elements i th column
proportional rel i q   b square matrix entry b i j 
proportional sim i j   matrices normalized row sums add   
note result normalization rows resulting square matrix q  
 da      d b  add    matrix called stochastic defines markov
chain  view sentence state markov chain q i j  specifies
transition probability state state j corresponding markov chain 
vector p looking eq     stationary distribution markov chain 
intuitive interpretation stationary distribution understood concept
random walk graph representation markov chain  probability
transition made current node nodes similar query 
probability    d  transition made nodes lexically similar current
node  every transition weighted according similarity distributions  element
vector p gives asymptotic probability ending corresponding state
long run regardless starting state  stationary distribution markov chain
computed simple iterative algorithm called power method  erkan   radev 
       starts uniform distribution  iteration eigenvector updated
multiplying transpose stochastic matrix  since markov chain
irreducible aperiodic algorithm guaranteed terminate 

   ranking sentences
use several methods order rank sentences generate summaries applying
features described section    section describe systems detail 
    learning feature weights  local search strategy
order fine tune weights features  used local search technique  initially set feature weights  w      wn   equal values  i e        see algorithm    
train weights using duc      data set  based current weights
score sentences generate summaries accordingly  evaluate summaries using
  

ficomplex question answering  unsupervised approaches

input  stepsize l  weight initial value v
output  vector w
  learned weights
initialize weight values wi v 
  n
rg    rg    prev    
 true 
scoresentences w 
 
generatesummaries  
rg    evaluaterouge  
rg  rg 
prev   wi
wi     l
rg    rg 
else
break
end
end
end
return w
 
algorithm    tuning weights using local search technique
automatic evaluation tool rouge  lin         described section    rouge
value works feedback learning loop  learning system tries maximize
rouge score every step changing weights individually specific step size  i e 
       means  learn weight wi change value wi keeping weight
values  wj j  i   stagnant  weight wi algorithm achieves local maximum
 i e  hill climbing  rouge value 
learned feature weights compute final scores sentences
using formula 
scorei   x i  w
 

    

x i feature vector i th sentence  w
  weight vector  scorei
score i th sentence 
    statistical machine learning approaches
experimented two unsupervised statistical learning techniques features
extracted previous section sentence selection problem 
   k means learning
   expectation maximization  em  learning
      k means learning
k means hard clustering algorithm defines clusters center mass
members  start set initial cluster centers chosen randomly go
  

fichali  joty    hasan

several iterations assigning object cluster whose center closest 
objects assigned recompute center cluster centroid
  members  distance function use squared euclidean distance
mean  
instead true euclidean distance 
since square root monotonically growing function squared euclidean distance
result true euclidean distance computation overload smaller
square root dropped 
learned means clusters using k means algorithm next
task rank sentences according probability model  used bayesian
model order so  bayes law says 

x qk    p  qk   
p x
x  
p x
x qk    p  qk   
p x
pk
x qk    p qk   
k   p x

x     
p  qk  x
 

    

qk cluster  x feature vector representing sentence  parameter
set class models  set weights clusters equiprobable  i e  p  qk     
x qk     using gaussian probability distribution  gaussian
  k   calculated p x
probability density function  pdf  d dimensional random variable x given by 

x   
p  
   x

e

 
x
 t    x
x
 
 x
 

dp
 
  det 

    

  mean vector    covariance matrix  parameters
  k means algorithm calculate
gaussian distribution  get means  
covariance matrix using unbiased covariance estimation procedure 

j  

n
  x
xi j   x
x j  t
 x
n   i  

    

      em learning
em algorithm gaussian mixture models well known method cluster analysis 
useful outcome model produces likelihood value clustering model
likelihood values used select best model number different
models providing number parameters  i e  number
clusters  
  

ficomplex question answering  unsupervised approaches

x  represented feature vector length
input  sample n data points  x
l
input  number clusters k
output  array k means based scores
data  array dnk   k   k
data  array c k   nk
randomly choose k data points k initial means  k   k        k 
repeat
  n
j   k
xi j k     x
xi j  t  x
xi j  
ij   kx
end
ik   il   l    k
assign x c k  
end
end
p
  k
c
x c

xj

j

 
c i 
 c
end
change occurs  
   calculating covariances cluster
  k
c i 
   c
j  
c ij    c
c ij  t
     c
end
      m    
end
   calculating scores sentences
  n
j   k
 
 


   x j   j  x j  
yij   e

 

  

  

j  
det 

end
j   k
p
   where  wj     k
zij    yij wj    k
j   yij wj  
end
k   k
  max 
push zim
end
return
algorithm    computing k means based similarity measure

  

fichali  joty    hasan

significant problem em algorithm converges local maximum
likelihood function hence quality result depends initialization 
problem along method improving initialization discussed later
section 
em soft version k means algorithm described above  k means
start set random cluster centers c  ck   iteration soft assignment
data points every cluster calculating membership probabilities  em
iterative two step procedure     expectation step    maximization step 
expectation step compute expected values hidden variables hi j cluster
membership probabilities  given current parameters compute likely
object belongs clusters  maximization step computes likely
parameters model given cluster membership probabilities 
data points considered generated mixture model k gaussians
form 

p  x   

k
x

p  c   i p  x c   i   

i  

k
x

   
p  c   i p  x 

    

i  

total likelihood model k components  given observed data points
x   x       x n   is 

l  x 

 


n x
k


i   j  

x  j    
p  c   j p  x

n
x

k
x

i  

log

j  

n x
k


i   j  

xi  
j   j  
wj p  x

xi  
j   j     taking log likelihood  
wj p  x

    
    

p probability density function  i e  eq      j j mean
covariance matrix component j  respectively  component contributes proportion 
p
wj   total population that  k
j   wj     
log likelihood used instead likelihood turns product sum 
describe em algorithm estimating gaussian mixture 
singularities covariance matrix must non singular invertible 
em algorithm may converge position covariance matrix becomes singular
       close singular  means invertible anymore  covariance
  
matrix becomes singular close singular em may result wrong clusters 
restrict covariance matrices become singular testing cases iteration
algorithm follows 
q

     e    update
   
else update
  

ficomplex question answering  unsupervised approaches

discussion  starting values em algorithm
convergence rate success clustering using em algorithm degraded
poor choice starting values means  covariances  weights components  experimented one summary  for document number d    a duc
      order test impact initial values em algorithm  cluster
means initialized
p heuristic spreads randomly around ean dat a 
standard deviation cov dat a      initial covariance set cov dat a 
initial values weights wj     k k number clusters 
is  d dimensional data points parameters j th component follows 

 j   rand      d 
j    dat a 
wj

q

 dat a         dat a 

    k

highly variable nature results tests reflected inconsistent values total log likelihood results repeated experiments indicated
using random starting values initial estimates means frequently gave poor
results  two possible solutions problem  order get good results
using random starting values  as specified algorithm  run em algorithm several times choose initial configuration get maximum
log likelihood among configurations  choosing best one among several runs
computer intensive process  so  improve outcome em algorithm gaussian
mixture models  necessary find better method estimating initial means
components 
best starting position em algorithm  regard estimates means 
would one estimated mean per cluster closer true mean
cluster 
achieve aim explored widely used k means algorithm cluster
 means  finding method  is  means found k means clustering
utilized initial means em calculate initial covariance matrices
using unbiased covariance estimation procedure  equation     
ranking sentences
sentences clustered em algorithm  identify sentences
xi     qr denotes clusare question relevant checking probabilities  p  qr  x
x           x considered
ter question relevant  sentence x   p  qr  x
question relevant  cluster mean values greater one
considered question relevant cluster 
next task rank question relevant sentences order include
summary  done easily multiplying feature vector x i weight
vector w
  learned applying local search technique  equation     
  

fichali  joty    hasan

input  sample n data points   x   represented feature vector length
l
input  number clusters k
output  array em based scores
k   k   k        k  equal priors set
start k initial gaussian models  n  
p  qk       k 
repeat
 i 
x j    i   
   estimation step  compute probability p  qk  x
 i 

data point xj   j        n  belong class qk
j   n
k   k
 i 
x j    i     
p  qk  x

 i 

 i 

xj  qk    i   
p  qk   i   p x
xj   i   
p x
 i 

 
end
end
   maximization step 
k   k
j   n
   update means 
i  
k

 

 i 

 i 

x j  
k   k  
p  qk   i   p x

 i 
 i 
 i 
x j  
 i 
k   p  qk    p x
k   k  

pk

  

 

   update variances 
 i   
k

  

 i 
xj    i   
j   x j p  qk  x
pn
 i 
xj    i   
j   p  qk  x

pn

 i 
xj
xj    i    x
xj  i   
  x
j   p  qk  x
k
pn
 i 
xj    i   
j   p  qk  x

pn

 i   
 

k

   update priors 

p  qk  i       i       

n
 x
 i 
x j    i   
p  qk  x
n j  

end
end
total likelihood increase falls desired threshold  
return
algorithm    computing em based similarity measure

  

ficomplex question answering  unsupervised approaches

   redundancy checking generating summary
sentences scored easiest way create summaries output
topmost n sentences required summary length reached  case 
ignoring factors  redundancy coherence 
know text summarization clearly entails selecting salient information putting together coherent summary  answer summary consists
multiple separately extracted sentences different documents  obviously 
selected text snippets individually important  however  many competing sentences included summary issue information overlap parts
output comes mechanism addressing redundancy needed  therefore 
summarization systems employ two levels analysis  first content level every
sentence scored according features concepts covers  second textual level 
when  added final output  sentences deemed important
compared similar candidates included final answer summary  goldstein  kantrowitz  mittal  carbonell       
observed authors called maximum marginal relevance  mmr   following hovy et al         modeled overlap intermediate summary
to be added candidate summary sentence 
call overlap ratio r  r     inclusively  setting r      
means candidate summary sentence  s  added intermediate summary 
s  sentence overlap ratio less equal     

   experimental evaluation
section describes results experiments conducted using duc        dataset
provided nist      questions experiments address include 
different features affect behavior summarizer system 
one algorithms  k means  em local search  performs better
particular problem 
used main task duc      evaluation  task was 
given complex question  topic description  collection relevant documents 
task synthesize fluent  well organized     word summary documents
answers question s  topic 
documents duc      came aquaint corpus comprising newswire
articles associated press new york times             xinhua news
agency              nist assessors developed topics interest choose set
   documents relevant  document cluster  topic  topic document
cluster given   different nist assessors including developer topic 
assessor created     word summary document cluster satisfies information
    http   www nlpir nist gov projects duc 
    national institute standards technology

  

fichali  joty    hasan

need expressed topic statement  multiple reference summaries used
evaluation summary content 
purpose experiments study impact different features  accomplish generated summaries    topics duc      seven
systems defined below 
lex system generates summaries based lexical features  section      
n gram  n           lcs  wlcs  skip bi gram  head  head synonym overlap 
lexsem system considers lexical semantic features  section       synonym  hypernym hyponym  gloss  dependency based proximity based similarity 
syn system generates summary based syntactic feature  section        
cos system generates summary based graph based method  section      
sys  system considers features except syntactic semantic features
 all features except section      
sys  system considers features except semantic feature  all features
except section       
system generates summaries taking features  section    account 
    automatic evaluation
rouge carried automatic evaluation summaries using rouge  lin 
      toolkit  widely adopted duc automatic summarization evaluation  rouge stands recall oriented understudy gisting evaluation 
collection measures determines quality summary comparing reference summaries created humans  measures count number overlapping units
n gram  word sequences  word pairs system generated summary
evaluated ideal summaries created humans  available rouge measures
are  rouge n  n           rouge l  rouge w rouge s  rouge n n gram
recall candidate summary set reference summaries  rouge l measures
longest common subsequence  lcs  takes account sentence level structure
similarity naturally identifies longest co occurring insequence n grams automatically 
rouge w measures weighted longest common subsequence  wlcs  providing improvement basic lcs method computation credit sentences
consecutive matches words  rouge s overlap skip bigrams candidate summary set reference summaries skip bigram pair words
sentence order allowing arbitrary gaps  rouge measures
applied automatic evaluation summarization systems achieved promising
results  lin        
systems  report widely accepted important metrics  rouge  
rouge su  present rouge   scores since never shown
correlate human judgement  rouge measures calculated running
  

ficomplex question answering  unsupervised approaches

rouge       stemming removal stopwords  rouge run time parameters
set duc      evaluation setup  are 
rouge       pl        u  r       t    n    w      m  l      a
show     confidence interval important evaluation metrics systems
report significance meaningful comparison  use rouge tool
purpose  rouge uses randomized method named bootstrap resampling compute
confidence interval  used      sampling points bootstrap resampling 
report evaluation scores one baseline system  the base column 
tables order show level improvement systems achieve  baseline
system generates summaries returning leading sentences  up     words 
ht ext field recent document s  
presenting results highlight top two f scores bottom one f score
indicate significance glance 
      results discussion
k means learning table   shows rouge   scores different combinations
features k means learning  noticeable k means performs best
graph based cosine similarity feature  note including syntactic feature
improve score  also  including syntactic semantic features increases score
significant amount  summaries based lexical features give us good
rouge   evaluation 
scores
recall
precision
f score

lex
     
     
     

lexsem
     
     
     

syn
     
     
     

cos
     
     
     

sys 
     
     
     

sys 
     
     
     


     
     
     

base
     
     
     

table    rouge   measures k means learning

table   shows rouge   scores different combinations features k means
learning  rouge   graph based cosine similarity feature performs well here 
get significant improvement rouge   score include syntactic feature
features  semantic features affect score much  lexical semantic features
perform well here 

scores
recall
precision
f score

lex
     
     
     

lexsem
     
     
     

syn
     
     
     

cos
     
     
     

sys 
     
     
     

sys 
     
     
     


     
     
     

table    rouge   measures k means learning

  

base
     
     
     

fichali  joty    hasan

table   shows  rouge su scores best features without syntactic
semantic  including syntactic semantic features features degrades scores 
summaries based lexical features achieve good scores 
scores
recall
precision
f score

lex
     
     
     

lexsem
     
     
     

syn
     
     
     

cos
     
     
     

sys 
     
     
     

sys 
     
     
     


     
     
     

base
     
     
     

table    rouge su measures k means learning
table   shows     confidence interval  for f measures k means learning 
important rouge evaluation metrics systems comparison confidence
interval baseline system  seen systems performed significantly
better baseline system cases 
systems
baseline
lex
lexsem
syn
cos
sys 
sys 


rouge  
                   
                   
                   
                   
                   
                   
                   
                   

rouge  
                   
                   
                   
                   
                   
                   
                   
                   

rouge su
                   
                   
                   
                   
                   
                   
                   
                   

table        confidence intervals k means system

em learning table   table   show different rouge measures feature
combinations context em learning  easily noticed
measures get significant amount improvement rouge scores include
syntactic semantic features along features  get       improvement
sys  f score include syntactic feature       improvement include
syntactic semantic features  cosine similarity measure perform well
k means experiments  summaries considering lexical features achieve
good results 
table   shows     confidence interval  for f measures em learning  important rouge evaluation metrics systems comparison confidence
interval baseline system  see systems performed significantly
better baseline system cases 
local search technique rouge scores based feature combinations
given table   table     summaries generated including features perform
  

ficomplex question answering  unsupervised approaches

scores
recall
precision
f score

lex
     
     
     

lexsem
     
     
     

syn
     
     
     

cos
     
     
     

sys 
     
     
     

sys 
     
     
     


     
     
     

base
     
     
     


     
     
     

base
     
     
     


     
     
     

base
     
     
     

table    rouge   measures em learning

scores
recall
precision
f score

lex
     
     
     

lexsem
     
     
     

syn
     
     
     

cos
     
     
     

sys 
     
     
     

sys 
     
     
     

table    rouge   measures em learning

scores
recall
precision
f score

lex
     
     
     

lexsem
     
     
     

syn
     
     
     

cos
     
     
     

sys 
     
     
     

sys 
     
     
     

table    rouge su measures em learning

systems
baseline
lex
lexsem
syn
cos
sys 
sys 


rouge  
                   
                   
                   
                   
                   
                   
                   
                   

rouge  
                   
                   
                   
                   
                   
                   
                   
                   

rouge su
                   
                   
                   
                   
                   
                   
                   
                   

table        confidence intervals em system

  

fichali  joty    hasan

best scores measures  get       improvement sys  f score
include syntactic feature       improvement sys  f score include
syntactic semantic features  case lexical features  lex  perform well
better features  all  
scores
recall
precision
f score

lex
     
     
     

lexsem
     
     
     

syn
     
     
     

cos
     
     
     

sys 
     
     
     

sys 
     
     
     


     
     
     

base
     
     
     

table    rouge   measures local search technique

scores
recall
precision
f score

lex
     
     
     

lexsem
     
     
     

syn
     
     
     

cos
     
     
     

sys 
     
     
     

sys 
     
     
     


     
     
     

base
     
     
     

table     rouge   measures local search technique

scores
recall
precision
f score

lex
     
     
     

lexsem
     
     
     

syn
     
     
     

cos
     
     
     

sys 
     
     
     

sys 
     
     
     


     
     
     

base
     
     
     

table     rouge su measures local search technique
table    shows     confidence interval  for f measures local search technique 
important rouge evaluation metrics systems comparison confidence interval baseline system  find systems performed significantly
better baseline system cases 
      comparison
results reported see three algorithms systems clearly outperform baseline system  table    shows f scores reported rouge measures
table    reports     confidence intervals baseline system  best system
duc       three techniques taking features  all  consideration 
see method based local search technique outperforms two
em algorithm performs better k means algorithm  analyze deeply  find
cases rouge su local search confidence intervals overlap
best duc      system 
  

ficomplex question answering  unsupervised approaches

systems
baseline
lex
lexsem
syn
cos
sys 
sys 


rouge  
                   
                   
                   
                   
                   
                   
                   
                   

rouge  
                   
                   
                   
                   
                   
                   
                   
                   

rouge su
                   
                   
                   
                   
                   
                   
                   
                   

table         confidence intervals local search system

algorithms
baseline
best system
k means
em
local search

rouge  
     
     
     
     
     

rouge  
     
     
     
     
     

rouge su
     
     
     
     
     

table     rouge f scores different systems

algorithms
baseline
best system
k means
em
local search

rouge  
                   
                   
                   
                   
                   

rouge  
                   
                   
                   
                   
                   

rouge su
                   
                   
                   
                   
                   

table         confidence intervals different systems

  

fichali  joty    hasan

    manual evaluation
sample     summaries   drawn different systems generated summaries
conduct extensive manual evaluation order analyze effectiveness
approaches  manual evaluation comprised pyramid based evaluation contents
user evaluation get assessment linguistic quality overall responsiveness 
      pyramid evaluation
duc      main task     topics selected optional community based
pyramid evaluation  volunteers    different sites created pyramids annotated
peer summaries duc main task using given guidelines       sites among
created pyramids  used pyramids annotate peer summaries
compute modified pyramid scores     used ducview jar   annotation tool
purpose  table    table    show modified pyramid scores systems
three algorithms  baseline systems score reported  peer summaries
baseline system generated returning leading sentences  up     words 
ht ext field recent document s   results see
systems perform better baseline system inclusion syntactic semantic
features yields better scores  three algorithms notice lexical
semantic features best terms modified pyramid scores 
      user evaluation
   university graduate students judged summaries linguistic quality overall
responsiveness  given score integer    very poor     very good 
guided consideration following factors     grammaticality     non redundancy 
   referential clarity     focus    structure coherence  assigned
content responsiveness score automatic summaries  content score
integer    very poor     very good  based amount information
summary helps satisfy information need expressed topic narrative 
measures used duc       table    table    present average linguistic
quality overall responsive scores systems three algorithms 
baseline systems scores given meaningful comparison  closer look
results  find systems perform worse baseline system terms
linguistic quality achieve good scores case overall responsiveness 
obvious tables exclusion syntactic semantic features often causes
lower scores  hand  lexical lexical semantic features show good overall
responsiveness scores three algorithms 
      systems   algorithms  cumulatively    systems  randomly chose
  summaries    systems 
    http   www  cs columbia edu  becky duc          pyramid guidelines html
    equals sum weights summary content units  scus  peer summary matches 
normalized weight ideally informative summary consisting number contributors
peer 
    http   www  cs columbia edu  ani duc     tool html

  

ficomplex question answering  unsupervised approaches

systems
baseline
lex
lexsem
syn
cos
sys 
sys 


modified pyramid scores
       
       
       
       
       
       
       
       

table     modified pyramid scores k means system

systems
baseline
lex
lexsem
syn
cos
sys 
sys 


modified pyramid scores
       
       
       
       
       
       
       
       

table     modified pyramid scores em system

systems
baseline
lex
lexsem
syn
cos
sys 
sys 


modified pyramid scores
       
       
       
       
       
       
       
       

table     modified pyramid scores local search system

  

fichali  joty    hasan

systems
baseline
lex
lexsem
syn
cos
sys 
sys 


linguistic quality
    
    
    
    
    
    
    
    

overall responsiveness
    
    
    
    
    
    
    
    

table     linguistic quality responsive scores k means system

systems
baseline
lex
lexsem
syn
cos
sys 
sys 


linguistic quality
    
    
    
    
    
    
    
    

overall responsiveness
    
    
    
    
    
    
    
    

table     linguistic quality responsive scores em system

systems
baseline
lex
lexsem
syn
cos
sys 
sys 


linguistic quality
    
    
    
    
    
    
    
    

overall responsiveness
    
    
    
    
    
    
    
    

table     linguistic quality responsive scores local search system

  

ficomplex question answering  unsupervised approaches

   conclusion future work
paper presented works answering complex questions  extracted eighteen important features sentences document collection  later used
simple local search technique fine tune feature weights  weight  wi  
algorithm achieves local maximum rouge value  way  learn
weights rank sentences multiplying feature vector weight vector 
experimented two unsupervised learning techniques     em    k means
features extracted  assume two clusters sentences     queryrelevant    query irrelevant  learned means clusters using k means
algorithm used bayesian model order rank sentences  learned means
k means algorithm used initial means em algorithm  applied em algorithm cluster sentences two classes      query relevant   
query irrelevant  take query relevant sentences rank using learned
weights  i e  local search   methods generating summaries filter
redundant sentences using redundancy checking module generate summaries
taking top n sentences 
experimented effects different kinds features  evaluated
systems automatically using rouge report significance results
    confidence intervals  conducted two types manual evaluation     pyramid
   user evaluation analyze performance systems  experimental
results mostly show following   a  approaches achieve promising results   b 
empirical approach based local search technique outperforms two learning
techniques em performs better k means algorithm   c  systems achieve
better results include tree kernel based syntactic semantic features 
 d  cases rouge su local search confidence intervals overlap
best duc      system 
experimenting supervised learning techniques  i e  svm  maxent  crf etc  analyzing perform problem  prior that  produced huge amount labeled data automatically using similarity measures rouge
 toutanova et al         
future plan decompose complex questions several simple questions
measuring similarity document sentence query sentence 
certainly serve create limited trees subsequences might increase
precision  thus  expect decomposing complex questions sets
subquestions entail systems improve average quality answers returned
achieve better coverage question whole 

acknowledgments
thank anonymous reviewers useful comments earliest version
paper  special thanks go colleagues proofreading paper  grateful
graduate students took part user evaluation process  research
reported supported natural sciences engineering research council
 nserc  research grant university lethbridge 
  

fichali  joty    hasan

appendix a  stop word list

reuters
may
nov
tue

accordingly

alone

another
anyway
appropriate
ask
awfully
becomes

better

cant
certainly
comes
containing
currently
didnt
dont

else
etc
everyone
except
followed
forth
get
goes
h
hasnt

ap
jun
dec
wed

across
aint
along
amid

anyways

asking
b
becoming
believe

c
cannot
changes
concerning
contains

different
done
edu
elsewhere
etc 
everything
f
following
four
gets
going



jan
jul
tech
thu
able
actually

already
among
anybody
anywhere
arent
associated



beyond
cmon
cant
clearly
consequently
corresponding
definitely


eg
enough
even
everywhere
far
follows

getting
gone
hadnt
havent

  

feb
aug
news
fri


allow

amongst
anyhow
apart
around

became

beside

cs
cause
co
consider
could
described

downwards
e g 
entirely
ever
ex



given
got
happens


mar
sep
index
sat

afterwards
allows
although

anyone
appear

available

beforehand
besides
brief
came
causes
com
considering
couldnt
despite
doesnt

eight
especially
every
exactly
fifth
former
furthermore
gives
gotten
hardly


apr
oct
mon

according

almost
always

anything
appreciate
aside
away
become
behind
best


certain
come
contain
course


e
either
et
everybody
example
five
formerly
g
go
greetings

hes

ficomplex question answering  unsupervised approaches

hello
hereafter
hi

im
immediate
indicated
inward

keep
l
less
likely

mean


nearly
nevertheless
non
nothing

old
onto

overall
perhaps
probably
r
regarding

help
hereby

howbeit
ive

indicates


keeps
lately
lest
little
mainly
meanwhile
mostly

necessary
new
none
novel





placed
provides
rather
regardless

hence
herein

however
ie
inasmuch
inner
isnt

kept
later
let
look
many
merely
mr 
n
need
next
noone

often



p
please
q
rd
regards


hereupon


i e 
inc
insofar

j
know
latter
lets
looking
may
might
ms 
namely
needs
nine

nowhere
oh
one
others

particular
plus
que

relatively

  



hither
id

indeed
instead
itd

knows
latterly

looks
maybe

much
nd
neither

normally

ok
ones
otherwise
outside
particularly
possible
quite
really
respectively

heres

hopefully
ill
ignored
indicate

itll
k
known
least
liked
ltd

moreover
must
near
never
nobody

obviously
okay

ought

per
presumably
qv
reasonably
right

fichali  joty    hasan


says
seemed
sensible
shall

sometime
specified
sup
tell
thanx

theres
thereupon
theyve

thus
towards
twice
unless
us
usually
via


werent
whenever
wherein
whither
whose
within
wouldnt
youd


said
second
seeming
sent


sometimes
specify
sure
tends


thereafter

think
though

tried
two
unlikely
use
uucp
viz
wasnt
weve


whereupon


without
x
youll



secondly
seems
serious

somebody
somewhat
specifying

th
thats

thereby

third
three
together
tries
u

used
v
vs
way
welcome
whats
wheres
wherever
whos

wont

youre
z

saw
see
seen
seriously
shouldnt
somehow
somewhere
still
ts

thats

therefore
theyd



truly
un
unto
useful
value
w

well
whatever
whereafter
whether
whoever
willing
wonder
yes
youve
zero

  

say
seeing
self
seven
since
someone
soon
sub
take
thank

thence
therein
theyll
thorough
throughout
took
try


uses
various
want
wed
went

whereas

whole
wish
would
yet


saying
seem
selves
several
six
something
sorry

taken
thanks


theres
theyre
thoroughly
thru
toward
trying
unfortunately
upon
using

wants
well

whence
whereby



would



ficomplex question answering  unsupervised approaches

references
bloehdorn  s     moschitti  a       a   combined syntactic semantic kernels text
classification    th european conference ir research  ecir       pp        
rome  italy 
bloehdorn  s     moschitti  a       b   structure semantics expressive text kernels 
cikm       pp         
chali  y     joty  s  r       a   improving performance random walk model
answering complex questions   proceedings   th annual meeting
acl hlt  short paper section  pp      oh  usa 
chali  y     joty  s  r       b   selecting sentences answering complex questions 
proceedings emnlp  pp         hawaii  usa 
charniak  e          maximum entropy inspired parser  technical report cs      
brown university  computer science department 
collins  m     duffy  n          convolution kernels natural language  proceedings
neural information processing systems  pp         vancouver  canada 
cormen  t  r   leiserson  c  e     rivest  r  l          introduction algorithms 
mit press 
erkan  g     radev  d  r          lexrank  graph based lexical centrality salience
text summarization  journal artificial intelligence research             
goldstein  j   kantrowitz  m   mittal  v     carbonell  j          summarizing text documents  sentence selection evaluation metrics  proceedings   nd international acm conference research development information retrieval 
sigir  pp         berkeley  ca 
guo  y     stylios  g          new multi document summarization system  proceedings document understanding conference  nist 
hacioglu  k   pradhan  s   ward  w   martin  j  h     jurafsky  d          shallow
semantic parsing using support vector machines  technical report tr cslr        university colorado 
halliday  m     hasan  r          cohesion english  longman  london 
harabagiu  s   lacatusu  f     hickl  a          answering complex questions random
walk models  proceedings   th annual international acm sigir conference
research development information retrieval  pp           acm 
hirao  t     suzuki  j   isozaki  h     maeda  e          dependency based sentence
alignment multiple document summarization  proceedings coling       pp 
       geneva  switzerland  coling 
  

fichali  joty    hasan

hovy  e   lin  c  y   zhou  l     fukumoto  j          automated summarization evaluation basic elements  proceedings fifth conference language
resources evaluation genoa  italy 
kingsbury  p     palmer  m          treebank propbank  proceedings
international conference language resources evaluation las palmas  spain 
kouylekov  m     magnini  b          recognizing textual entailment tree edit distance
algorithms  proceedings pascal challenges workshop  recognising textual
entailment challenge 
li  j   sun  l   kit  c     webster  j          query focused multi document summarizer based lexical chains  proceedings document understanding
conference rochester  nist 
lin  c  y          rouge  package automatic evaluation summaries  proceedings workshop text summarization branches out  post conference workshop
association computational linguistics  pp       barcelona  spain 
lin  d       a   information theoretic definition similarity  proceedings
international conference machine learning  pp         madison  wisconsin 
lin  d       b   automatic retrieval clustering similar words  proceedings
international conference computational linguistics association
computational linguistics  pp         montreal  canada 
losada  d          language modeling sentence retrieval  comparison
multiple bernoulli models multinomial models  information retrieval theory workshop glasgow  uk 
losada  d     fernandez  r  t          highly frequent terms sentence retrieval 
proc    th string processing information retrieval symposium  spire    pp 
       santiago de chile 
maccartney  b   grenager  t   de marneffe  m   cer  d     manning  c  d          learning recognize features valid textual entailments  proceedings human
language technology conference north american chapter acl  p      
new york  usa 
morris  j     hirst  g          lexical cohesion computed thesaural relations
indicator structure text  computational linguistics               
moschitti  a          efficient convolution kernels dependency constituent syntactic
trees  proceedings   th european conference machine learning berlin 
germany 
moschitti  a     basili  r          tree kernel approach question answer classification question answering systems  proceedings  th international
conference language resources evaluation genoa  italy 
  

ficomplex question answering  unsupervised approaches

moschitti  a     quarteroni  s          kernels linguistic structures answer extraction  proceedings   th conference association computational
linguistics  acl     short paper section columbus  oh  usa 
moschitti  a   quarteroni  s   basili  r     manandhar  s          exploiting syntactic
shallow semantic kernels question answer classificaion  proceedings
  th annual meeting association computational linguistics  pp        
prague  czech republic  acl 
murdock  v     croft  w  b          translation model sentence retrieval  hlt    
proceedings conference human language technology empirical methods
natural language processing  pp         morristown  nj  usa  acl 
otterbacher  j   erkan  g     radev  d  r          using random walks questionfocused sentence retrieval  proceedings human language technology conference
conference empirical methods natural language processing  pp        
vancouver  canada 
pingali  p   k   r     varma  v          iiit hyderabad duc       proceedings
document understanding conference rochester  nist 
punyakanok  v   roth  d     yih  w          mapping dependencies trees  application
question answering  proceedings ai   math florida  usa 
strzalkowski  t     harabagiu  s          advances open domain question answering 
springer 
toutanova  k   brockett  c   gamon  m   jagarlamudi  j   suzuki  h     vanderwende 
l          pythy summarization system  microsoft research duc      
proceedings document understanding conference rochester  nist 
vanderwende  l   suzuki  h     brockett  c          microsoft research duc     
task focused summarization sentence simplification lexical expansion 
proceedings document understanding conference rochester  nist 
zajic  d  m   lin  j   dorr  b  j     schwartz  r          sentence compression component multi document summarization system  proceedings document
understanding conference rochester  nist 
zhang  a     lee  w       a   question classification using support vector machines 
proceedings special interest group information retrieval  pp       toronto 
canada  acm 
zhang  d     lee  w  s       b   language modeling approach passage question
answering  proceedings twelfth text retreival conference  pp        
gaithersburg  maryland 
zhou  l   lin  c  y     hovy  e          be based multi dccument summarizer
query interpretation  proceedings document understanding conference vancouver  b c   canada 

  


