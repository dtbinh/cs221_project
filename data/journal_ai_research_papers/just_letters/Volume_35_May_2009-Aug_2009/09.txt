journal of artificial intelligence research                  

submitted       published     

efficient markov network structure discovery
using independence tests
facundo bromberg

fbromberg frm utn edu ar

departamento de sistemas de informacion 
universidad tecnologica nacional 
mendoza  argentina

dimitris margaritis
vasant honavar

dmarg cs iastate edu
honavar cs iastate edu

dept  of computer science 
iowa state university 
ames  ia      

abstract
we present two algorithms for learning the structure of a markov network from data 
gsmn and gsimn  both algorithms use statistical independence tests to infer the structure by successively constraining the set of structures consistent with the results of these
tests  until very recently  algorithms for structure learning were based on maximum likelihood estimation  which has been proved to be np hard for markov networks due to the
difficulty of estimating the parameters of the network  needed for the computation of the
data likelihood  the independence based approach does not require the computation of
the likelihood  and thus both gsmn and gsimn can compute the structure efficiently
 as shown in our experiments   gsmn is an adaptation of the grow shrink algorithm
of margaritis and thrun for learning the structure of bayesian networks  gsimn extends gsmn by additionally exploiting pearls well known properties of the conditional
independence relation to infer novel independences from known ones  thus avoiding the performance of statistical tests to estimate them  to accomplish this efficiently gsimn uses
the triangle theorem  also introduced in this work  which is a simplified version of the set
of markov axioms  experimental comparisons on artificial and real world data sets show
gsimn can yield significant savings with respect to gsmn   while generating a markov
network with comparable or in some cases improved quality  we also compare gsimn to
a forward chaining implementation  called gsimn fch  that produces all possible conditional independences resulting from repeatedly applying pearls theorems on the known
conditional independence tests  the results of this comparison show that gsimn  by the
sole use of the triangle theorem  is nearly optimal in terms of the set of independences
tests that it infers 

   introduction
graphical models  bayesian and markov networks  are an important subclass of statistical models that possess advantages that include clear semantics and a sound and widely
accepted theoretical foundation  probability theory   graphical models can be used to
represent efficiently the joint probability distribution of a domain  they have been used
in numerous application domains  ranging from discovering gene expression pathways in
bioinformatics  friedman  linial  nachman    peer        to computer vision  e g  geman

c
    
ai access foundation  all rights reserved 

fibromberg  margaritis    honavar

figure    example markov network  the nodes represent variables in the domain v  
                         

  geman        besag  york    mollie        isard        anguelov  taskar  chatalbashev 
koller  gupta  heitz    ng         one problem that naturally arises is the construction of
such models from data  heckerman  geiger    chickering        buntine         a solution
to this problem  besides being theoretically interesting in itself  also holds the potential of
advancing the state of the art in application domains where such models are used 
in this paper we focus on the task of learning markov networks  mns  from data in
domains in which all variables are either discrete or continuous and distributed according
to a multidimensional gaussian distribution  mns are graphical models that consist of two
parts  an undirected graph  the model structure   and a set of parameters  an example
markov network is shown in figure    learning such models from data consists of two interdependent tasks  learning the structure of the network  and  given the learned structure 
learning the parameters  in this work we focus on the problem of learning the structure of
the mn of a domain from data 
we present two algorithms for mn structure learning from data  gsmn  grow shrink
markov network learning algorithm  and gsimn  grow shrink inference based markov
network learning algorithm   the gsmn algorithm is an adaptation to markov networks
of the gs algorithm by margaritis and thrun         originally developed for learning the
structure of bayesian networks  gsmn works by first learning the local neighborhood
of each variable in the domain  also called the markov blanket of the variable   and then
using this information in subsequent steps to improve efficiency  although interesting and
useful in itself  we use gsmn as a point of reference of the performance with regard to
time complexity and accuracy achieved by gsimn  which is the main result of this work 
the gsimn algorithm extends gsmn by using pearls theorems on the properties of the
conditional independence relation  pearl        to infer additional independences from a
set of independences resulting from statistical tests and previous inferences  thus avoiding
the execution of these tests on data  this allows savings in execution time and  when data
are distributed  communication bandwidth 
the rest of the paper is organized as follows  in the next section we present previous
research related to the problem  section   introduces notation  definitions and presents
some intuition behind the two algorithms  section   contains the main algorithms  gsmn
and gsimn  as well as concepts and practical details related to their operation  we evaluate
gsmn and gsimn and present our results in section    followed by a summary of our
   

fiefficient markov network structure discovery using independence tests

work and possible directions of future research in section    appendices a and b contain
proofs of correctness of gsmn and gsimn 

   related work
markov networks have been used in the physics and computer vision communities  geman  
geman        besag et al         anguelov et al         where they have been historically
called markov random fields  recently there has been interest in their use for spatial
data mining  which has applications in geography  transportation  agriculture  climatology 
ecology and others  shekhar  zhang  huang    vatsavai        
one broad and popular class of algorithms for learning the structure of graphical models
is the score based approach  exemplified for markov networks by della pietra  della pietra 
and lafferty         and mccallum         score based approaches conduct a search in the
space of legal structures in an attempt to discover a model structure of maximum score 
due to the intractable size of the search space i e   the space of all legal graphs  which is
super exponential in size  score based algorithms must usually resort to heuristic search 
at each step of the structure search  a probabilistic inference step is necessary to evaluate
the score  e g   maximum likelihood  minimum description length  lam   bacchus        or
pseudo likelihood  besag         for bayesian networks this inference step is tractable and
therefore several practical score based algorithms for structure learning have been developed
 lam   bacchus        heckerman        acid   de campos         for markov networks
however  probabilistic inference requires the calculation of a normalizing constant  also
known as partition function   a problem known to be np hard  jerrum   sinclair       
barahona         a number of approaches have considered a restricted class of graphical
models  e g  chow   liu        rebane   pearl        srebro   karger         however 
srebro and karger        prove that finding the maximum likelihood network is np hard
for markov networks of tree width greater than   
some work in the area of structure learning of undirected graphical models has concentrated on the learning of decomposable  also called chordal  mns  srebro   karger 
       an example of learning non decomposable mns is presented in the work of hofmann and tresp         which is an approach for learning structure in continuous domains
with non linear relationships among the domain attributes  their algorithm removes edges
greedily based on a leave one out cross validation log likelihood score  a non score based
approach is in the work of abbeel  koller  and ng         which introduces a new class of efficient algorithms for structure and parameter learning of factor graphs  a class of graphical
models that subsumes markov and bayesian networks  their approach is based on a new
parameterization of the gibbs distribution in which the potential functions are forced to be
probability distributions  and is supported by a generalization of the hammersley clifford
theorem for factor graphs  it is a promising and theoretically sound approach that may
lead in the future to practical and efficient algorithms for undirected structure learning 
in this work we present algorithms that belong to the independence based or constraintbased approach  spirtes  glymour    scheines         independence based algorithms exploit the fact that a graphical model implies that a set of independences exist in the distribution of the domain  and therefore in the data set provided as input to the algorithm  under
assumptions  see next section   they work by conducting a set of conditional independence
   

fibromberg  margaritis    honavar

tests on data  successively restricting the number of possible structures consistent with the
results of those tests to a singleton  if possible   and inferring that structure as the only
possible one  a desirable characteristic of independence based approaches is the fact that
they do not require the use of probabilistic inference during the discovery of the structure 
also  such algorithms are amenable to proofs of correctness  under assumptions  
for bayesian networks  the independence based approach has been mainly exemplified
by the sgs  spirtes et al          pc  spirtes et al          and algorithms that learn the
markov blanket as a step in learning the bayesian network structure such as grow shrink
 gs  algorithm  margaritis   thrun         iamb and its variants  tsamardinos  aliferis 
  statnikov      a   hiton pc and hiton mb  aliferis  tsamardinos    statnikov 
       mmpc and mmmb  tsamardinos  aliferis    statnikov      b   and max min hill
climbing  mmhc   tsamardinos  brown    aliferis         all of which are widely used in
the field  algorithms for restricted classes such as trees  chow   liu        and polytrees
 rebane   pearl        also exist 
for learning markov networks previous work has mainly focused on learning gaussian
graphical models  where the assumption of a continuous multivariate gaussian distribution
is made  this results in linear dependences among the variables with gaussian noise  whittaker        edwards         more recent approaches are included in the works of dobra 
hans  jones  nevins  yao  and west          castelo   roverato         pena         and
schafer and strimmer         that focus on applications of gaussian graphical models in
bioinformatics  while we do not make the assumption of continuous gaussian variables
in this paper  all algorithms we present are applicable to such domains with the use of
an appropriate conditional independence test  such as partial correlation   the gsmn
and gsimn algorithms presented apply to any case where an arbitrary faithful distribution can be assumed and a probabilistic conditional independence test for that distribution
is available  the algorithms were first introduced by bromberg  margaritis  and honavar
        the contributions of the present paper include extending these results by conducting
an extensive evaluation of their experimental and theoretical properties  more specifically 
the contributions include an extensive and systematic experimental evaluation of the proposed algorithms on  a  data sets sampled from artificially generated networks of varying
complexity and strength of dependences  as well as  b  data sets sampled from networks
representing real world domains  and  c  formal proofs of correctness that guarantee that
the proposed algorithms will compute the correct markov network structure of the domain 
under the stated assumptions 

   notation and preliminaries
we denote random variables with capitals  e g   x  y  z  and sets of variables with bold
capitals  e g   x  y  z   in particular  we denote by v               n     the set of all n
variables in the domain  we name the variables by their indices in v  for instance  we
refer to the third variable in v simply by    we denote the data set as d and its size
 number of data points  by  d  or n   we use the notation  xy   z  to denote the
proposition that x is independent of y conditioned on z  for disjoint sets of variables x 
y  and z   x  y   z  denotes conditional dependence  we use  xy   z  as shorthand
for   x  y     z  to improve readability 
   

fiefficient markov network structure discovery using independence tests

a markov network is an undirected graphical model that represents the joint probability
distribution over v  each node in the graph represents one of the random variables in
the domain  and absences of edges encode conditional independences among them  we
assume the underlying probability distribution to be graph isomorph  pearl        or faithful
 spirtes et al          which means that it has a faithful undirected graph  a graph g is
said to be faithful to some distribution if its graph connectivity represents exactly those
dependencies and independences existent in the distribution  in detail  this means that that
for all disjoint sets x  y  z  v  x is independent of y given z if and only if the set of
vertices z separates the set of vertices x from the set of vertices y in the graph g  this is
sometimes called the global markov property  lauritzen         in other words  this means
that  after removing all vertices in z from g  including all edges incident to each of them  
there exists no  undirected  path in the remaining graph between any variable in x to some
variable in y  for example  in figure    the set of variables        separates set        from
set      more generally  it has been shown  pearl        theorem    page    and definition
of graph isomorphism  page     that a necessary and sufficient condition for a distribution
to be graph isomorph is for its set of independence relations to satisfy the following axioms
for all disjoint sets of variables x  y  z  w and individual variable  

 symmetry 
 decomposition 
 intersection 
 strong union 
 transitivity 

 xy   z 
 xy  w   z 
 xy   z  w 
  xw   z  y 
 xy   z 
 xy   z 




 yx   z 
 xy   z    xw   z 

 
 
 

 xy  w   z 
 xy   z  w 
 x   z    y   z 

   

for the operation of the algorithms we also assume the existence of an oracle that can
answer statistical independence queries  these are standard assumptions that are needed
for formally proving the correctness of independence based structure learning algorithms
 spirtes et al         
    independence based approach to structure learning
gsmn and gsimn are independence based algorithms for learning the structure of the
markov network of a domain  this approach works by evaluating a number of statistical
independence statements  reducing the set of structures consistent with the results of these
tests to a singleton  if possible   and inferring that structure as the only possible one 
as mentioned above  in theory we assume the existence of an independence query oracle
that can provide information about conditional independences among the domain variables 
this can be viewed as an instance of a statistical query oracle  kearns   vazirani        
in practice such an oracle does not exist  however  it can be implemented approximately
by a statistical test evaluated on the data set d  for example  for discrete data this
can be pearsons conditional independence chi square      test  agresti         a mutual
information test etc  for continuous gaussian data a statistical test that can be used to
measure conditional independence is partial correlation  spirtes et al          to determine
conditional independence between two variables x and y given a set z from data  the
   

fibromberg  margaritis    honavar

statistical test returns a p value  the p value of a test equals the probability of obtaining a
value for the test statistic that is at least as extreme as the one that was actually observed
given that the null hypothesis is true  which corresponds to conditional independence in
our case  assuming that the p value of a test is p x  y   z   the statistical test concludes
dependence if and only if p x  y   z  is less than or equal to a threshold  i e  
 x  y   z   p x  y   z    
the quantity     is sometimes referred to as the tests confidence threshold  we use
the standard value of         in all our experiments  which corresponds to a confidence
threshold of     
in a faithful domain  it can be shown  pearl   paz        that an edge exists between
two variables x    y  v in the markov network of that domain if an only if they are
dependent conditioned on all remaining variables in the domain  i e  
 x  y   is an edge iff  x  y   v   x  y    
thus  to learn the structure  theoretically it suffices to perform only n n       tests i e  
one test  x  y   v   x  y    for each pair of variables x  y  v  x    y   unfortunately 
in non trivial domains this usually involves a test that conditions on a large number of
variables  large conditioning sets produce sparse contingency tables  count histograms 
which result in unreliable tests  this is because the number of possible configurations of
the variables grows exponentially with the size of the conditioning setfor example  there
are  n cells in a test involving n binary variables  and to fill such a table with one data point
per cell we would need a data set of at least exponential size i e   n   n   exacerbating
this problem  more than one data point per cell is typically necessary for a reliable test  as
recommended by cochran         if more than     of the cells of the contingency table
have less than   data points the test is deemed unreliable  therefore both gsmn and
gsimn algorithms  presented below  attempt to minimize the conditioning set size  they
do that by choosing an order of examining the variables such that irrelevant variables are
examined last 

   algorithms and related concepts
in this section we present our main algorithms  gsmn and gsimn  and supporting concepts required for their description  for the purpose of aiding the understanding of the
reader  before discussing these we first describe the abstract gsmn algorithm in the next
section  this helps in showing the intuition behind the algorithms and laying the foundation
for them 
    the abstract gsmn algorithm
for the sake of clarity of exposition  before discussing our first algorithm gsmn   we
describe the intuition behind it by describing its general structure using the abstract gsmn
algorithm which deliberately leaves a number of details unspecified  these are filled in in the
concrete gsmn algorithm  presented in the next section  note that the choices for these

   

fiefficient markov network structure discovery using independence tests

algorithm   gsmn algorithm outline  g   gsmn  v  d  
   initialize g to the empty graph 
   for all variables x in the domain v do
  
   learn the markov blanket bx of x using the gs algorithm    
  
bx  gs  x  v  d 
  
add an undirected edge in g between x and each variable y  bx  
   return g

algorithm   gs algorithm  returns the markov blanket bx of variable x  v  bx  
gs  x  v  d  
  
  
  
  
  
  
  
  
  
   
   
   

bx  
   grow phase    
for each variable y in v   x  do
if  x  y   bx    estimated using data d  then
bx  bx   y  
goto      restart grow loop    
   shrink phase    
for each variable y in bx do
if  xy   bx   y     estimated using data d  then
bx  bx   y  
goto      restart shrink loop    
return bx

details are a source of optimizations that can reduce the algorithms computational cost 
we make these explicit when we discuss the concrete gsmn and gsimn algorithms 
the abstract gsmn algorithm is shown in algorithm    given as input a data set d
and a set of variables v  gsmn computes the set of nodes  variables  bx that are adjacent
to each variable x  v  these completely determine the structure of the domain mn  the
algorithm consists of a main loop in which it learns the markov blanket bx of each node
 variable  x in the domain using the gs algorithm  it then constructs the markov network
structure by connecting x with each variable in bx  
the gs algorithm was first proposed by margaritis and thrun        and is shown in
algorithm    it consists of two phases  a grow phase and a shrink phase  the grow phase
of x proceeds by attempting to add each variable y to the current set of hypothesized
neighbors of x  contained in bx   which is initially empty  bx grows by some variable y
during each iteration of the grow loop of x if and only if y is found dependent with x
given the current set of hypothesized neighbors bx   due to the  unspecified  ordering that
the variables are examined  this is explicitly specified in the concrete gsmn algorithm 
presented in the next section   at the end of the grow phase some of the variables in bx
might not be true neighbors of x in the underlying mnthese are called false positives 
this justifies the shrink phase of the algorithm  which removes each false positive y in bx
by testing for independence with x conditioned on bx   y    if y is found independent of
x during the shrink phase  it cannot be a true neighbor  i e   there cannot be an edge x y   
and gsmn removes it from bx   assuming faithfulness and correctness of the independence
query results  by the end of the shrink phase bx contains exactly the neighbors of x in
the underlying markov network 
   

fibromberg  margaritis    honavar

in the next section we present a concrete implementation of gsmn  called gsmn  
this augments gsmn by specifying a concrete ordering that the variables x are examined
in the main loop of gsmn  lines    in algorithm     as well as a concrete order that the
variables y are examined in the grow and shrink phases of the gs algorithm  lines    and
    in algorithm    respectively  
    the concrete gsmn algorithm
in this section we discuss our first algorithm  gsmn  grow shrink markov network
learning algorithm   for learning the structure of the markov network of a domain  note
that the reason for introducing gsmn in addition to our main contribution  the gsimn
algorithm  presented later in section       is for comparison reasons  in particular  gsimn
and gsmn have identical structure  following the same order of examination of variables 
with their only difference being the use of inference by gsimn  see details in subsequent
sections   introducing gsmn therefore makes it possible to measure precisely  through
our experimental results in section    the benefits of the use of inference on performance 
the gsmn algorithm is shown in algorithm    its structure is similar to the abstract
gsmn algorithm  one notable difference is that the order that variables are examined is
now specified  this is done in the initialization phase where the so called examination order
 and grow order x of each variable x  v is determined   and all x are priority
queues and each is initially a permutation of v  x is a permutation of v   x   such
that the position of a variable in the queue denotes its priority e g                means that
variable   has the highest priority  will be examined first   followed by   and finally by   
similarly  the position of a variable in x determines the order it will be examined during
the grow phase of x 
during the initialization phase the algorithm computes the strength of unconditional
dependence between each pair of variable x and y   as given by the unconditional p value
p x  y     of an independence test between each pair of variables x    y   denoted by
pxy in the algorithm   in practice the logarithm of the p values is computed  which allows
greater precision in domains where some dependencies may be very strong or very weak  
in particular  the algorithm gives higher priority to  examines earlier  those variables with
a lower average log p value  line     indicating stronger dependence  this average is defined
as 
x
 
avg log pxy    
log pxy   
 v    
y
y   x

for the grow order x of variable x  the algorithm gives higher priority to those variables
y whose p value  or equivalently the log of the p value  with variable x is small  line    
this ordering is due to the intuition behind the folk theorem  as koller   sahami       
puts it  that states that probabilistic influence or association between attributes tends to
attenuate over distance in a graphical model  this suggests that a pair of variables x and y
with high unconditional p value are less likely to be directly linked  note that this ordering
is a heuristic and is not guaranteed to hold in general  for example  it may not hold
if the underlying domain is a bayesian network e g   two spouses may be independent
unconditionally but dependent conditional on a common child  note however that this
example does not apply to faithful domains i e   graph isomorph to a markov network  also
   

fiefficient markov network structure discovery using independence tests

algorithm   gsmn   a concrete implementation of gsmn  g   gsmn   v  d  
  
  
  
  
  

initialize g to the empty graph 
   initialization    
for all x  y  v  x    y do
pxy  p x  y    


initialize  such that i  i              n      i   i  avg log pi j     avg log pi j    
j

j

   for all x  v do
  
bx  


  
initialize x such that j  j               n      j   j   pxx   pxx  
j

j

  
remove x from x  
       main loop    
    while  is not empty do
   
x  dequeue  
   
   propagation phase    
   
t   y   y was examined and x  by  
   
f   y   y was examined and x 
  by  
   
for all y  t  move y to the end of x  
   
for all y  f  move y to the end of x  
   
   grow phase    
   
s
   
while x not empty do
   
y  dequeue x  
   
if pxy   then
   
if igsmn  x  y  s  f  t  then
   
s  s   y  
   
   change grow order of y     
   
move x to the beginning of y  
   
for w   s s   to s  do
   
move w to the beginning of y  
   
   change examination order    
   
for w   s s   to s  do
   
if w   then
   
move w to the beginning of  
   
break to line   
   
   shrink phase    
   
for y   s s   to s  do
   
if igsmn  x  y  s   y     f  t  then
   
s  s   y  
   
bx  s
   
add an undirected edge in g between x and each variable y  bx  
    return g

note that the correctness of all algorithms we present does not depend on it holding i e  
as we prove in appendices a and b  both gsmn and gsimn are guaranteed to return
the correct structure under the assumptions stated in section   above  also note that the
computational cost for the calculation of pxy is low due to the empty conditioning set 
the remaining of the gsmn algorithm contains the main loop  lines       in which
each variable in v is examined according to the examination order   determined during
   

fibromberg  margaritis    honavar

algorithm   igsmn  x  y  s  f  t   calculate independence test  x  y   s  by propagation  if possible  otherwise run a statistical test on data 
  
  
  
  
  
  
  
  
  

   attempt to infer dependence by propagation    
if y  t then
return false
   attempt to infer independence by propagation    
if y  f then
return true
   else do statistical test on data    
t    p x y  z       t   true iff p value of statistical test  x  y   s        
return t

the initialization phase  the main loop includes three phases  the propagation phase  lines
       the grow phase  lines        and the shrink phase  lines        the propagation
phase is an optimization in which all variables y for which by has already been computed
 i e   all variables y already examined  are collected in two sets f and t  set f  t  contains
all variables y such that x 
  by  x  by    both sets are passed to the independence
procedure igsmn   shown in algorithm    for the purpose of avoiding the execution of any
tests between x and y by the algorithm  this is justified by the fact that  in undirected
graphs  y is in the markov blanket of x if and only if x is in the markov blanket of y  
variables y already found not to contain x in their blanket by  set f  cannot be members
of bx because there exists some set of variables that has rendered them conditionally
independent of x in a previous step  and independence can therefore be inferred easily 
note that in the experiments section of the paper  section    we evaluate gsmn with
and without the propagation phase  in order to measure the effect that this propagation
optimization has on performance  turning off propagation is accomplished simply by setting
sets t and f  as computed in lines    and     respectively  to the empty set 
another difference of gsmn from the abstract gsmn algorithm is in the use of condition pxy    line      this is an additional optimization that avoids an independence test
in the case that x and y were found  unconditionally  independent during the initialization
phase  since in that case this would imply x and y are independent given any conditioning
set by the axiom of strong union 
a crucial difference between gsmn and the abstract gsmn algorithm is that gsmn
changes the examination order  and the grow order y of every variable y  x    since
x 
  x   this excludes the grow order of x itself   these changes in ordering proceed as
follows  after the end of the grow phase of variable x  the new examination order   set
in lines       dictates that the next variable w to be examined after x is the last to be
added to s during the growing phase that has not yet been examined  i e   w is still in   
the grow order y of all variables y found dependent with x is also changed  this is done
to maximize the number of optimizations by the gsimn algorithm  our main contribution
in this paper  which shares the algorithm structure of gsmn   the changes in grow order
are therefore explained in detail in section     when gsimn is presented 
a final difference between gsmn and the abstract gsmn algorithm is the restart
actions of the grow and shrink phases of gsmn whenever the current markov blanket is
modified  lines   and    of algorithm     which are not present in gsmn   the restarting

   

fiefficient markov network structure discovery using independence tests

figure    illustration of the operation of gsmn using an independence graph  the figure
shows the growing phase of variable    variables are examined according to its
grow order                           

of the loops was necessary in the gs algorithm due to its original usage in learning the
structure of bayesian networks  in that task  it was possible for a true member y of the
blanket of x to be found initially independent during the grow loop when conditioning on
some set s but to be found dependent later when conditioned on a superset s  s  this
could happen if y was an unshielded spouse of x i e   if y had one or more common
children with x but there existed no direct link between y and x in the underlying bayesian
network  however  this behavior is impossible in a domain that has a distribution faithful
to a markov network  one of our assumptions   any independence between x and y given
s must hold for any superset s of s by the axiom of strong union  see eqs        the
restart of the grow and shrink loops is therefore omitted from gsmn in order to save
unnecessary tests  note that  even though it is possible that this behavior is impossible in
faithful domains  it is possible in unfaithful ones  so we also experimentally evaluated our
algorithms in real world domains in which the assumption of markov faithfulness may not
necessarily hold  section    
a proof of correctness of gsmn is presented in appendix a 
    independence graphs
we can demonstrate the operation of gsmn graphically by the concept of the independence
graph  which we now introduce  we define an independence graph to be an undirected
graph in which conditional independences and dependencies between single variables are
represented by one or more annotated edges between them  a solid  dotted  edge between
variables x and y annotated by z represents the fact that x and y have been found
dependent  independent  given z  if the conditioning set z is enclosed in parentheses then
this edge represents an independence or dependence that was inferred from eqs       as
opposed to computed from statistical tests   shown graphically 
   

fibromberg  margaritis    honavar

x
x
x
x

z
y

 x  y   z 

y

 xy   z 

y

 x  y   z   inferred 

y

 xy   z   inferred 

z
 z 
 z 

for instance  in figure    the dotted edge between   and   annotated with      represents
the fact that                the absence of an edge between two variables indicates the
absence of information about the independence or dependence between these variables under
any conditioning set 
example    figure   illustrates the operation of gsmn using an independence graph in
the domain whose underlying markov network is shown in figure    the figure shows the
independence graph at the end of the grow phase of the variable    the first in the examination
order    we do not discuss in this example the initialization phase of gsmn   instead  we
assume that the examination    and grow    orders are as shown in the figure   according
to vertex separation on the underlying network  figure     variables          and   are found
dependent with   during the growing phase i e  
i          
i             
i                
i                  
and are therefore connected to   in the independence graph by solid edges annotated by sets
              and           respectively  variables       and   are found independent i e  
i                
i                   
i                     
and are thus connected to   by dotted edges annotated by                   and             
respectively 
    the triangle theorem
in this section we present and prove a theorem that is used in the subsequent gsimn
algorithm  as will be seen  the main idea behind the gsimn algorithm is to attempt to decrease the number of tests done by exploiting the properties of the conditional independence
relation in faithful domains i e   eqs       these properties can be seen as inference rules
that can be used to derive new independences from ones that we know to be true  a careful
study of these axioms suggests that only two simple inference rules  stated in the triangle
theorem below  are sufficient for inferring most of the useful independence information that
can be inferred by a systematic application of the inference rules  this is confirmed in our
experiments in section   
   

fiefficient markov network structure discovery using independence tests

figure    independence graph depicting the triangle theorem  edges in the graph are
labeled by sets and represent conditional independences or dependencies  a solid
 dotted  edge between x and y labeled by z means that x and y are dependent
 independent  given z  a set label enclosed in parentheses means the edge was
inferred by the theorem 

theorem    triangle theorem   given eqs       for every variable x  y   w and sets z 
and z  such that  x  y  w    z     x  y  w    z     
 x  w   z      w  y   z   

 

 x  y   z   z   

 xw   z      w  y   z   z   

 

 xy   z    

we call the first relation the d triangle rule and the second the i triangle rule 
proof  we are using the strong union and transitivity of eqs      as shown or in contrapositive form 
 proof of d triangle rule  
 from strong union and  x  w   z    we get  x  w   z   z    
 from strong union and  w  y   z    we get  w  y   z   z    
 from transitivity   x  w   z  z     and  w  y   z  z     we get  x  y   z  z    
 proof of i triangle rule  
 from strong union and  w  y   z   z    we get  w  y   z    
 from transitivity   xw   z    and  w  y   z    we get  xy   z    

we can represent the triangle theorem graphically using the independence graph construct of section      figure   depicts the two rules of the triangle theorem using two
independence graphs 
the triangle theorem can be used to infer additional conditional independences from
tests conducted during the operation of gsmn   an example of this is shown in figure    which illustrates the application of the triangle theorem to the example presented
in figure    the independence information inferred from the triangle theorem is shown by
curved edges  note that the conditioning set of each such edge is enclosed in parentheses  

   

fibromberg  margaritis    honavar

figure    illustration of the use of the triangle theorem on the example of figure    the set
of variables enclosed in parentheses correspond to tests inferred by the triangle
theorem using the two adjacent edges as antecedents  for example  the result
               is inferred from the i triangle rule  independence              
and dependence                    

for example  independence edge        can be inferred by the d triangle rule from the adjacent edges        and         annotated by     and           respectively  the annotation
for this inferred edge is      which is the intersection of the annotations     and           
an example application of the i triangle rule is edge         which is inferred from edges
       and        with annotations        and           respectively  the annotation for this
inferred edge is         which is the intersection of the annotations           and        
    the gsimn algorithm
in the previous section we saw the possibility of using the two rules of the triangle
theorem to infer the result of novel tests during the grow phase  the gsimn algorithm
 grow shrink inference based markov network learning algorithm   introduced in this section  uses the triangle theorem in a similar fashion to extend gsmn by inferring the value
of a number of tests that gsmn executes  making their evaluation unnecessary  gsimn
and gsmn work in exactly the same way  and thus the gsimn algorithm shares exactly
the same algorithmic description i e   both follow algorithm     with all differences between
them concentrated in the independence procedure they use  instead of using independence
procedure igsmn of gsmn   gsimn uses procedure igsimn   shown in algorithm    procedure igsimn   in addition to attempting to propagate the blanket information obtained
from the examination of previous variables  as igsmn does   also attempts to infer the
value of the independence test that is provided as its input by either the strong union
axiom  listed in eqs       or the triangle theorem  if this attempt is successful  igsimn
returns the value inferred  true or false   otherwise it defaults to a statistical test on the
data set  as igsmn does   for the purpose of assisting in the inference process  gsimn and
   

fiefficient markov network structure discovery using independence tests

algorithm   igsimn  x  y  s  f  t   calculate independence test result by inference  including propagation   if possible  record test result in the knowledge base 
  
  
  
  
  
  
  
  
  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   attempt to infer dependence by propagation    
if y  t then
return false
   attempt to infer independence by propagation    
if y  f then
return true
   attempt to infer dependence by strong union    
if   a  false   kxy such that a  s then
return false
   attempt to infer dependence by the d triangle rule    
for all w  s do
if   a  false   kxw such that a  s    b  false   kw y such that b  s then
add  a  b  false  to kxy and ky x  
return false
   attempt to infer independence by strong union    
if   a  true   kxy such that a  s then
return true
   attempt to infer independence by the i triangle rule    
for all w  s do
if   a  true   kxw s t  a  s    b  false   kw y s t  b  a then
add  a  true  to kxy and ky x  
return true
   else do statistical test on data    
t    p x y  z       t   true iff p value of statistical test  x  y   s        
add  s  t  to kxy and ky x  
return t

igsimn maintain a knowledge base kxy for each pair of variables x and y   containing the
outcomes of all tests evaluated so far between x and y  either from data or inferred   each
of these knowledge bases is empty at the beginning of the gsimn algorithm  the initialization step is not shown in the algorithm since gsmn does not use it   and is maintained
within the test procedure igsimn  
we now explain igsimn  algorithm    in detail  igsimn attempts to infer the independence value of its input triplet  x  y   s  by applying a single step of backward
chaining using the strong union and triangle rules i e   it searches the knowledge base
k    kxy   x  y  v  for antecedents of instances of rules that have the input triplet
 x  y   s  as consequent  the strong union rule is used in its direct from as shown in
eqs      and also in its contrapositive form  the direct form can be used to infer independences  and therefore we refer to it as the i su rule from here on  in its contrapositive form 
the i su rule becomes  x  y   s  w     x  y   s   referred to as the d su rule
since it can be used to infer dependencies  according to the d triangle and d su rules 
the dependence  x  y   s  can be inferred if the knowledge base k contains
   a test  x  y   a  with a  s  or
   tests  x  w   a  and  w  y   b  for some variable w   with a  s and b  s 
   

fibromberg  margaritis    honavar

figure    illustration of the operation of gsimn  the figure shows the grow phase of two
consecutively examined variables   and    the figure shows how the variable
examined second is not   but    according to the change in the examination order
 in lines      of algorithm    the set of variables enclosed in parentheses
correspond to tests inferred by the triangle theorem using two adjacent edges as
antecedents  the results                                           and        
           in  b   shown highlighted  were not executed but inferred from the tests
done in  a  

respectively  according to the i triangle and i su rules  the independence  xy   s  can
be inferred if the knowledge base contains
   a test  xy   a  with a  s  or
   tests  xw   a  and  w  y   b  for some variable w   with a  s and b  a 
respectively 
the changes to the grow orders of some variables occur inside the grow phase of the
currently examined variable x  lines      of gsimn i e   algorithm   with igsmn replaced by igsimn     in particular  if  for some variable y   the algorithm reaches line    
i e   pxy   and igsimn  x  y  s    false  then x and all the variables that were found
dependent with x before y  i e   all variables currently in s  are promoted to the beginning
of the grow order y   this is illustrated in figure   for variable    which depicts the grow
phase of two consecutively examined variables   and    in this figure  the curved edges
show the tests that are inferred by igsimn during the grow phase of variable    the grow
order of   changes from                           to                           after the grow phase
of variable   is complete because the variables         and   were promoted  in that order 
to the beginning of the queue  the rationale for this is the observation that this increases
the number of tests inferred by gsimn at the next step  the change in the examination
and grow orders described above was chosen so that the inferred tests while learning the
blanket of variable   match exactly those required by the algorithm in some future step  in
   

fiefficient markov network structure discovery using independence tests

particular  note that in the example the set of inferred dependencies between each variable
found dependent with   before   are exactly those required during initial part of the grow
phase of variable    shown highlighted in figure   b   the first four dependencies   these
independence tests were inferred  not conducted   resulting in computational savings  in
general  the last dependent variable of the grow phase of x has the maximum number of
dependences and independences inferred and this provides the rationale for its change in
grow order and its selection by the algorithm to be examined next 
it can be shown that under the same assumptions as gsmn   the structure returned
by gsimn is the correct one i e   each set bx computed by the gsimn algorithm equals
exactly the neighbors of x  the proof of correctness of gsimn is based on correctness of
gsmn and is presented in appendix b 
    gsimn technical implementation details
in this section we discuss a number of practical issues that subtly influence the accuracy and
efficiency of an implementation of gsimn  one is the order of application of the i su  d su 
i triangle and d triangle rules within the function igsimn   given an independence query
oracle  the order of application should not matterassuming there are more than one rules
for inferring the value of an independence  all of them are guaranteed to produce the same
value due to the soundness of the axioms of eqs       pearl         in practice however 
the oracle is implemented by statistical tests conducted on data which can be incorrect  as
previously mentioned  of particular importance is the observation that false independences
are more likely to occur than false dependencies  one example of this is the case where the
domain dependencies are weakin this case any pair of variables connected  dependent  in
the underlying true network structure may be incorrectly deemed independent if all paths
between them are long enough  on the other hand  false dependencies are much more rare
the confidence threshold of            of a statistical test tells us that the probability of a
false dependence by chance alone is only     assuming i i d  data for each test  the chance
of multiple false dependencies is even lower  decreasing exponentially fast  this practical
observation i e   that dependencies are typically more reliable than independences  provide
the rationale for the way the igsimn algorithm works  in particular  igsimn prioritizes the
application of rules whose antecedents contain dependencies first i e   the d triangle and
d su rules  followed by the i triangle and i su rules  in effect  this uses statistical results
that are typically known with greater confidence before ones that are usually less reliable 
the second practical issue concerns efficient inference  the gsimn algorithm uses a onestep inference procedure  shown in algorithm    that utilizes a knowledge base k    kxy  
containing known independences and dependences for each pair of variables x and y   to
implement this inference efficiently we utilize a data structure for k for the purpose of
storing and retrieving independence facts in constant time  it consists of two  d arrays 
one for dependencies and another for independencies  each array is of n  n size  where n
is the number of variables in the domain  each cell in this array corresponds to a pair of
variables  x  y    and stores the known independences  dependences  between x and y in
the form of a list of conditioning sets  for each conditioning set z in the list  the knowledge
base kxy represents a known independence  xy   z   dependence  x  y   z    it is
important to note that the length of each list is at most    as there are no more than two

   

fibromberg  margaritis    honavar

tests done between any variable x and y during the execution of gsimn  done during
the growing and shrinking phases   thus  it always takes a constant time to retrieve store
an independence  dependence   and therefore all inferences using the knowledge base are
constant time as well  also note that all uses of the strong union axion by the igsimn
algorithm are constant time as well  as they can be accomplished by testing the  at most
two  sets stored in kxy for subset or superset inclusion 

   experimental results
we evaluated the gsmn and gsimn algorithms on both artificial and real world data sets 
through the experimental results presented below we show that the simple application of
pearls inference rules in gsimn algorithm results in a significant reduction in the number
of tests performed when compared to gsmn without adversely affecting the quality of the
output network  in particular we report the following quantities 
 weighted number of tests  the weighted number of tests is computed by the
summation of the weight of each test executed  where the weight of test  x  y   z  is
defined as    z   this quantity reflects the time complexity of the algorithm  gsmn
or gsimn  and can be used to assess the benefit in gsimn of using inference instead
of executing statistical tests on data  this is the standard method of comparison of
independence based algorithms and it is justified by the observation that the running
time of a statistical test on triplet  x  y   z  is proportional to the size n of the data
set and the number of variables involved in it i e   o n   z       and is not exponential
in the number of variables involved as a nave implementation might assume   this
is because one can construct all non zero entries in the contingency table used by the
test by examining each data point in the data set exactly once  in time proportional to
the number of variables involved in the test i e   proportional to   x  y  z       z  
 execution time  in order to assess the impact of inference in the running time
 in addition to the impact of statistical tests   we report the execution time of the
algorithm 
 quality of the resulting network  we measure quality in two ways 
 normalized hamming distance  the hamming distance between the output
network and the structure of the underlying model is another measure of the
quality of the output network  when the actual network that was used to generate
the data is known  the hamming distance is defined as the number of reversed
edges between these two network structures  i e   the number of times an actual
edge in the true network is missing in the returned network or an edge absent
from the true network exists in the algorithms output network  a value zero
means that the output network has the correct structure  to be able to compare
domains
of different dimensionalities  number of variables n  we normalize it by

n
    the total number of node pairs in the corresponding domain 
 accuracy  for real world data sets where the underlying network is unknown 
no hamming distance calculation is possible  in this case it is impossible to know
the true value of any independence  we therefore approximate it by a statistical
test on the entire data set  and use a limited  randomly chosen subset      of
the data set  to learn the network  to measure accuracy we compare the result
   

fiefficient markov network structure discovery using independence tests

 true or false  of a number of conditional independence tests on the network
output  using vertex separation   to the same tests performed on the full data
set 
in all experiments involving data sets we used the   statistical test for estimation of
conditional independences  as mentioned above  rules of thumb exist that deem certain
tests as potentially unreliable depending on the counts of the contingency table involved 
for example  one such rule cochran        deems a test unreliable if more than     of the
cells of the contingency table have less than   data points the test  due to the requirement
that an answer must be obtained by an independence algorithm conducting a test  we used
the outcomes of such tests as well in our experiments  the effect of these possibly unreliable
tests on the quality of the resulting network is measured by our accuracy measures  listed
above 
in the next section we present results for domains in which the underlying probabilistic
model is known  this is followed by real world data experiments where no model structure
is available 
    known model experiments
in the first set of experiments the underlying model  called the true model or true network  is
a known markov network  the purpose of this set of experiments is to conduct a controlled
evaluation of the quality of the output network through a systematic study of the algorithms
behavior under varying conditions of domain size  number of variables  and amount of
dependencies  average node degree in the network  
each true network that contains n variables was generated randomly as follows  the
network was initialized with n nodes and no edges  a user specified parameter of the
network structure is the average node degree  that equals the average number of neighbors
per node  given    for every node its set of neighbors was determined randomly and
uniformly by selecting the first  n  pairs in a random permutation of all possible pairs  the
factor     is necessary because each edge contributes to the degree of two nodes 
we conducted two types of experiments using known network structure  exact learning
experiments and sample based experiments 
      exact learning experiments
in this set of known model experiments  we assume that the result of all statistical queries
asked by the gsmn and gsimn algorithms were available  which assumes the existence
of an oracle that can answer independence queries  when the underlying model is known 
this oracle can be implemented through vertex separation  the benefits of querying the
true network for independence are two  first  it ensures faithfulness and correctness of
the independence query results  which allows the evaluation of the algorithms under their
assumptions for correctness  second  these tests can be performed much faster than actual
statistical tests on data  this allowed us to evaluate our algorithms in large networkswe
were able to conduct experiments of domains containing up to     variables 
we first report the weighted number of tests executed by gsmn with and without
propagation and gsimn  our results are summarized in figure    which shows the ratio
between the weighted number of tests of gsimn and the two versions of gsmn   one
   

fiwc gsimn    wc gsmn  with propagation 

ratio of weighted cost of gsimn vs  gsmn  without propagation
 
   
   
   
   
  
  
  
  

   
   
   
   
   
 
 

  

  

  
  
  
  
  
  
domain size  number of variables 

  

   

wc gsimn    wc gsmn  without propagation 

bromberg  margaritis    honavar

ratio of weighted cost of gsimn vs  gsmn  with propagation
 
   
   
   
   
   
   
  
  
  
  

   
   
   
 
 

  

  

  
  
  
  
  
  
domain size  number of variables 

  

   

figure    ratio of the weighted number of tests of gsimn over gsmn without propagation  left plot  and with propagation  right plot  for network sizes  number of
nodes  up to n       of average degree             and   
algorithm   ifch  x  y  s  f  t   forward chaining implementation of independence test
igsimn  x  y  s  f  t  
  
  
  
  
  
  
  

   query knowledge base    
if   s  t   kxy then
return t
t  result of test  x  y   s     t   true iff test  x  y   s  returns independence    
add  s  t  to kxy and ky x  
run forward chaining inference algorithm on k  update k 
return t

hundred true networks were generated randomly for each pair  n      and the figure shows
the mean value  we can see that the limiting reduction  as n grows large  in weighted
number of tests depends primarily on the average degree parameter    the reduction of
gsimn for large n and dense networks        is approximately     compared to gsmn
with propagation and     compared to gsmn without the propagation optimization 
demonstrating the benefit of gsimn vs  gsmn in terms of number of tests executed 
one reasonable question about the performance of gsimn is to what extent its inference
procedure is complete i e   from all those tests that gsimn needs during its operation  how
does the number of tests that it infers  by applying a single step of backward chaining on
the strong union axiom and the triangle theorem  rather than executing a statistical test
on data  compare to the number of tests that can be inferred  for example using a complete
automated theorem prover on eqs        to measure this  we compared the number of tests
done by gsimn with the number done by an alternative algorithm  which we call gsimnfch  gsimn with forward chaining   gsimn fch differs from gsimn in function
ifch   shown in algorithm    which replaces function igsimn of gsimn  ifch exhaustively
produces all independence statements that can be inferred through the properties of eqs     
using a forward chaining procedure  this process iteratively builds a knowledge base k
containing the truth value of conditional independence predicates  whenever the outcome
of a test is required  k is queried  line   of ifch in algorithm     if the value of the test is
   

fiefficient markov network structure discovery using independence tests

ratio of number of tests gsimn fch and gsimn

  
  
  
  

   
   

ratio

 
   
   
   
   
 

 

 

 

 

 

 

 

 

        

number of variables  n 

figure    ratio of number of tests of gsimn fch over gsimn for network sizes  number
of variables  n     to n      and average degrees             and   

found in k  it is returned  line     if not  gsimn fch performs the test and uses the result
in a standard forward chaining automatic theorem prover subroutine  line    to produce all
independence statements that can be inferred by the test result and k  adding these new
facts to k 
a comparison of the number of tests executed by gsimn vs  gsimn fch is presented
in figure    which shows the ratio of the number of tests of gsimn over gsimn fch 
the figure shows the mean value over four runs  each corresponding to a network generated
randomly for each pair  n      for            and   and n up to     unfortunately  after two
days of execution gsimn fch was unable to complete execution on domains containing
   variables or more  we therefore present results for domain sizes up to    only  the
figure shows that for n     and every  the ratio is exactly   i e   all tests inferable were
produced by the use of the triangle theorem in gsimn  for smaller domains  the ratio is
above      with the exception of a single case   n            
      sample based experiments
in this set of experiments we evaluate gsmn  with and without propagation  and gsimn
on data sampled from the true model  this allows a more realistic assessment of the
performance of our algorithms  the data were sampled from the true  known  markov
network using gibbs sampling 
in the exact learning experiments of the previous section only the structure of the true
network was required  generated randomly in the fashion described above  to sample data
from a known structure however  one also needs to specify the network parameters  for each
random network  the parameters determine the strength of dependencies among connected
variables in the graph  following agresti         we used the log odds ratio as a measure of
the strength of the probabilistic influence between two binary variables x and y   defined
as
pr x      y      pr x      y     
xy   log
 
pr x      y      pr x      y     

   

fibromberg  margaritis    honavar

hamming distance for sampled data
n                   

gsmn  without propagation
gsmn  with propagation
gsimn

   
   
   
 
 

 

 

  

  

  

  

  

   
   
 

  

 

 

data set size  thousands of data points 

gsmn  without propagation
gsmn  with propagation
gsimn

   
   
   
 
 

 

 

 

  

  

  

  

  

   
 
  

  

  

  

  

 

   
 
  

  

  

  

  

data set size  thousands of data points 

 

  

  

  

  

  

   
 
 

 

  

  

  

  

  

  

   
   
 
 

 

 

  

  

  

  

  

data set size  thousands of data points 

  

  

  

   
   
 
 

 

 

 

 

  

  

  

  

  

  

 
gsmn  without propagation
gsmn  with propagation
gsimn

   
   
   
   
 
 

 

 

 

 

  

  

  

  

  

  

hamming distance for sampled data
n                   

   

 

  

data set size  thousands of data points 

gsmn  without propagation
gsmn  with propagation
gsimn

 

  

   

  

 
   

  

hamming distance for sampled data
n                   

   

 

 

data set size  thousands of data points 

   

 

 

gsmn  without propagation
gsmn  with propagation
gsimn

   

  

gsmn  without propagation
gsmn  with propagation
gsimn

   

 

normalized hamming distance

normalized hamming distance

   

 

 

 

 

hamming distance for sampled data
n                   

   

 

 

data set size  thousands of data points 

gsmn  without propagation
gsmn  with propagation
gsimn

 

 

 

hamming distance for sampled data
n                   

 

 

hamming distance for sampled data
n                   

 

  

 

 

 

data set size  thousands of data points 

   

data set size  thousands of data points 

   

   

  

   

 

normalized hamming distance

normalized hamming distance

   

 

  

   

hamming distance for sampled data
n                   

   

 

  

   

data set size  thousands of data points 

gsmn  without propagation
gsmn  with propagation
gsimn

 

  

   

hamming distance for sampled data
n                   

 

  

gsmn  without propagation
gsmn  with propagation
gsimn

   

  

 

 

  

 

data set size  thousands of data points 

   

 

gsmn  without propagation
gsmn  with propagation
gsimn

   

hamming distance for sampled data
n                   
normalized hamming distance

normalized hamming distance

hamming distance for sampled data
n                   

 

 

 

data set size  thousands of data points 

 
   

 

normalized hamming distance

 

   

normalized hamming distance

 

gsmn  without propagation
gsmn  with propagation
gsimn

   

normalized hamming distance

   

hamming distance for sampled data
n                   

 

normalized hamming distance

normalized hamming distance

normalized hamming distance

hamming distance for sampled data
n                   
 

  

 
gsmn  without propagation
gsmn  with propagation
gsimn

   
   
   
   
 
 

 

 

 

 

  

  

  

  

  

  

data set size  thousands of data points 

figure    normalized hamming distances between the true network and the network output
by gsmn  with and without propagation  and gsimn for domain size n     
and average degrees               

the network parameters were generated randomly so that the log odds ratio between every
pair of variables connected by an edge in the graph has a specified value  in this set of
experiments  we used values of              and      for every such pair of variables in
the network 
figures   and   show plots of the normalized hamming distance between the true
network and that output by the gsmn  with and without propagation  and gsimn for
domain sizes of n      and n      variables  respectively  these plots show that the
hamming distance of gsimn is comparable to the ones of the gsmn algorithms for both

   

fiefficient markov network structure discovery using independence tests

hamming distance for sampled data
n                   

gsmn  without propagation
gsmn  with propagation
gsimn

   
   
   
 
 

 

 

  

  

  

  

  

   
   
 

  

 

 

data set size  thousands of data points 

gsmn  without propagation
gsmn  with propagation
gsimn

   
   
   
 
 

 

 

 

  

  

  

  

  

   
 
  

  

  

  

  

 

   
 
  

  

  

  

  

data set size  thousands of data points 

 

  

  

  

  

  

   
 
 

 

  

  

  

  

  

  

   
   
 
 

 

 

  

  

  

  

  

data set size  thousands of data points 

  

  

  

   
   
 
 

 

 

 

 

  

  

  

  

  

  

 
gsmn  without propagation
gsmn  with propagation
gsimn

   
   
   
   
 
 

 

 

 

 

  

  

  

  

  

  

hamming distance for sampled data
n                   

   

 

  

data set size  thousands of data points 

gsmn  without propagation
gsmn  with propagation
gsimn

 

  

   

  

 
   

  

hamming distance for sampled data
n                   

   

 

 

data set size  thousands of data points 

   

 

 

gsmn  without propagation
gsmn  with propagation
gsimn

   

  

gsmn  without propagation
gsmn  with propagation
gsimn

   

 

normalized hamming distance

normalized hamming distance

   

 

 

 

 

hamming distance for sampled data
n                   

   

 

 

data set size  thousands of data points 

gsmn  without propagation
gsmn  with propagation
gsimn

 

 

 

hamming distance for sampled data
n                   

 

 

hamming distance for sampled data
n                   

 

  

 

 

 

data set size  thousands of data points 

   

data set size  thousands of data points 

   

   

  

   

 

normalized hamming distance

normalized hamming distance

   

 

  

   

hamming distance for sampled data
n                   

   

 

  

   

data set size  thousands of data points 

gsmn  without propagation
gsmn  with propagation
gsimn

 

  

   

hamming distance for sampled data
n                   

 

  

gsmn  without propagation
gsmn  with propagation
gsimn

   

  

 

 

  

 

data set size  thousands of data points 

   

 

gsmn  without propagation
gsmn  with propagation
gsimn

   

hamming distance for sampled data
n                   
normalized hamming distance

normalized hamming distance

hamming distance for sampled data
n                   

 

 

 

data set size  thousands of data points 

 
   

 

normalized hamming distance

 

   

normalized hamming distance

 

gsmn  without propagation
gsmn  with propagation
gsimn

   

normalized hamming distance

   

hamming distance for sampled data
n                   

 

normalized hamming distance

normalized hamming distance

normalized hamming distance

hamming distance for sampled data
n                   
 

  

 
gsmn  without propagation
gsmn  with propagation
gsimn

   
   
   
   
 
 

 

 

 

 

  

  

  

  

  

  

data set size  thousands of data points 

figure    normalized hamming distance results as in figure   but for domain size n      

domain sizes n      and n       all average degrees               and log odds ratios      
       and       this reinforces the claim that inference done by gsimn has a small
impact on the quality of the output networks 
figure    shows the weighted number of tests of gsimn vs  gsmn  with and without
propagation  for a sampled data set of        points for domains n       and n      
average degree parameters             and   and log odds ratios           and    gsimn
shows a reduced weighted number of tests with respect to gsmn without propagation in
all cases and compared to gsmn with propagation in most cases  with the only exceptions
of              and                  for sparse networks and weak dependences i e  
      this reduction is larger than     for both domain sizes  a reduction much larger

   

fibromberg  margaritis    honavar

weighted cost for sampled data
                     data points

weighted cost for sampled data
                     data points

      
      
      
     
 

      

gsmn  without propagation
gsmn  with propagation
gsimn

      
      
      
     
 

  

  

weighted cost for sampled data
                     data points

      
     
 

      

gsmn  without propagation
gsmn  with propagation
gsimn

      
      
      
     

  

gsmn  without propagation
gsmn  with propagation
gsimn

      
      
      
     

  

  

weighted cost for sampled data
                     data points

weighted cost for sampled data
                     data points

      
      
      
     
 

      

      
gsmn  without propagation
gsmn  with propagation
gsimn

weighted number of tests

gsmn  without propagation
gsmn  with propagation
gsimn

      
      
      
     
 

  

      

gsmn  without propagation
gsmn  with propagation
gsimn

      
      
      
     
 

  

number of variables

  

  

number of variables

weighted cost for sampled data
                     data points

weighted cost for sampled data
                     data points

      
weighted number of tests

gsmn  without propagation
gsmn  with propagation
gsimn

      
      
      
     
 

      

      
gsmn  without propagation
gsmn  with propagation
gsimn

      
      
      
     
 

  

  
number of variables

weighted cost for sampled data
                     data points

      

  
number of variables

      
weighted number of tests

weighted number of tests

      

number of variables

weighted cost for sampled data
                     data points

weighted number of tests

weighted cost for sampled data
                     data points

 
  

      

  

      

number of variables

number of variables

     

number of variables

 

  

      

  

weighted number of tests

      

      

      

weighted cost for sampled data
                     data points
weighted number of tests

weighted number of tests

gsmn  without propagation
gsmn  with propagation
gsimn

  

      

  

      

      

      

gsmn  without propagation
gsmn  with propagation
gsimn

number of variables

      

  

      

 
  

number of variables

      

weighted number of tests

gsmn  without propagation
gsmn  with propagation
gsimn

      

weighted number of tests

      

weighted cost for sampled data
                     data points

      
weighted number of tests

weighted number of tests

      

      

gsmn  without propagation
gsmn  with propagation
gsimn

      
      
      
     
 

  

  
number of variables

  

  
number of variables

figure     weighted number of tests executed by gsmn  with and without propagation 
and gsimn for  d             for domains sizes n      and     average degree
parameters             and    and log odds ratios             and   

than the one observed for the exact learning experiments  the actual execution times for
various data set sizes and network densities are shown in figure    for the largest domain
of n       and       verifying the reduction in cost of gsimn for various data set sizes 
note that the reduction is proportional to the number of data points  this is reasonable as
each test executed must go over the entire data set once to construct the contingency table 
this confirms our claim that the cost of inference of gsimn is small  constant time per
test  see discussion in section      compared to the execution time of the tests themselves 
and indicates increasing cost benefits of the use of gsimn for even large data sets 

   

fiefficient markov network structure discovery using independence tests

execution times for sampled data sets
n      variables            

execution times for sampled data sets
n      variables            

   

   
gsmn  without propagation
gsmn  with propagation
gsimn

gsmn  without propagation
gsmn  with propagation
gsimn

   
execution time  sec 

execution time  sec 

   
   
   
   
  

   
   
   
  

 

 
 

                                                       

 

execution times for sampled data sets
n      variables            

execution times for sampled data sets
n      variables            

   

   
gsmn  without propagation
gsmn  with propagation
gsimn

gsmn  without propagation
gsmn  with propagation
gsimn

   
execution time  sec 

   
execution time  sec 

                                                       

   
   
   
  

   
   
   
  

 

 
 

                                                       

 

                                                       

figure     execution times for sampled data experiments for                top row 
and          bottom row  for a domain of n      variables 

      real world network sampled data experiments
we also conducted sampled data experiments on well known real world networks  as there
is no known repository of markov networks drawn from real world domains  we instead
utilized well known bayesian networks that are widely used in bayesian network research
and are available from a number of repositories   to generate markov networks from these
bayesian network structures we used the process of moralization  lauritzen        that
consists of two steps   a  connect each pair of nodes in the bayesian network that have
a common child with an undirected edge and  b  remove directions of all edges  this
results in a markov network in which the local markov property is valid i e   each node is
conditionally independent of all other nodes in the domain given its direct neighbors  during
this procedure some conditional independences may be lost  this  however  does not affect
the accuracy results because we compare the independencies of the output network with
those of the moralized markov network  as opposed to the bayesian network  
we conducted experiments using   real world domains  hailfinder  insurance  alarm 
mildew  and water  for each domain we sampled a varying number of data points from its
corresponding bayesian network using logic sampling  henrion         and used it as input
to the gsmn  with and without propagation  and gsimn algorithms  we then compared
the network output from each of these algorithms to the original moralized network using
the normalized hamming distance metric previously described  the results are shown in
   we used http   compbio cs huji ac il repository   accessed on december         

   

fibromberg  margaritis    honavar

hamming distance for hailfinder data set

hamming distance for insurance data set

hamming distance for alarm data set

gsmn  without propagation
gsmn  with propagation
gsimn

   
   
   
   
   
   
   
   
   
 
 

 

 

 

 

           

 
gsmn  without propagation
gsmn  with propagation
gsimn

   
   
   
   
   
   
   
   
   
 

        

 

data set size  thousands of data points 

 

 

 

 

           

normalized hamming distance

normalized hamming distance

   
   
   
   
   
   
   
 

        

 

 

 

 

 

           

        

data set size  thousands of data points 

hamming distance for water data set

gsmn  without propagation
gsmn  with propagation
gsimn

   
   

gsmn  without propagation
gsmn  with propagation
gsimn

   
   

data set size  thousands of data points 

hamming distance for mildew data set
 
   

normalized hamming distance

 
normalized hamming distance

normalized hamming distance

 

   
   
   
   
   
   
 

 
   

gsmn  without propagation
gsmn  with propagation
gsimn

   
   
   
   
   
   
   
   
 

 

 

 

 

 

           

        

 

data set size  thousands of data points 

 

 

 

 

           

        

data set size  thousands of data points 

figure     normalized hamming distance of the network output by gsmn  with and
without propagation  and gsimn with the true markov networks network using
varying data set sizes sampled from markov networks for various real world
domains modeled by bayesian networks 

fig     and indicate that the distances produced from the three algorithms are similar 
in some cases  e g   water and hailfinder  the network resulting from the use of gsimn
is actually better  of smaller hamming distance  than the ones output by the gsmn
algorithms 
we also measured the weighted cost of the three algorithms for each of these domains 
shown in fig      the plots show a significant decrease in the weighted number of tests for
gsimn with respect to both gsmn algorithms  the cost of gsimn is     of the cost of
gsmn without propagation on average  a savings of      while the cost of gsimn is    
of the cost of gsmn without propagation on average  a savings of     
    real world data experiments
while the artificial data set studies of the previous section have the advantage of allowing a
more controlled and systematic study of the performance of the algorithms  experiments on
real world data are necessary for a more realistic assessment of their performance  real data
are more challenging because they may come from non random topologies  e g   a possibly
irregular lattice in many cases of spatial data  and the underlying probability distribution
may not be faithful 
we conducted experiments on a number of data sets obtained from the uci machine
learning data set repository  newman  hettich  blake    merz         continuous variables
in the data sets were discretized using a method widely recommended in introductory statistics texts  scott         it dictates that the optimal number of equally spaced discretization
bins for each continuous variable is k       log  n   where n is the number of points in the
   

fiefficient markov network structure discovery using independence tests

gsmn  without propagation
gsmn  with propagation
gsimn

     
     
     
     
     
     

gsmn  without propagation
gsmn  with propagation
gsimn

    

weighted cost of tests

weighted cost of tests

     

weighted cost of tests for insurance data set
    
    
    
    
    
    

     
     
     
    
    
    
    

    

 

 
 

 

  

  

  

 
 

 

data set size  thousands of data points 

  

  

 

 

  

  

gsmn  without propagation
gsmn  with propagation
gsimn

     

    
    
    
    

     
     
     
    

 

 
 

 

  

  

  

 

data set size  thousands of data points 

 

  

  

  

data set size  thousands of data points 

figure     weighted cost of tests conducted by the gsmn  with and without propagation 
and gsimn algorithms for various real world domains modeled by bayesian
networks 
weighted cost and accuracy for real world data sets
 
acc gsimn    acc gsmn  without propagation 
acc gsimn    acc gsmn  with propagation 
wc gsimn    wc gsmn  without propagation 
wc gsimn    wc gsmn  with propagation 

   
   
   
   
   
   
   
   
   
 
    
    

    

      

      

         
data set index

                  

figure     ratio of the weighted number of tests of gsimn versus gsmn and difference
between the accuracy of gsimn and gsmn on real data sets  ratios smaller
that   and positive bars indicate an advantage of gsimn over gsmn   the
numbers in the x axis are indices of the data sets as shown in table   

data set  for each data set and each algorithm  we report the weighted number of conditional independence tests conducted to discover the network and the accuracy  as defined
below 

   

  

data set size  thousands of data points 

weighted cost of tests for water data set

gsmn  without propagation
gsmn  with propagation
gsimn

weighted cost of tests

weighted cost of tests

  

data set size  thousands of data points 

weighted cost of tests for mildew data set
     

gsmn  without propagation
gsmn  with propagation
gsimn

     

    

     

weighted cost of tests for alarm data set

weighted cost of tests

weighted cost of tests for hailfinder data set

fibromberg  margaritis    honavar

table    weighted number of tests and accuracy for several real world data sets  for each
evaluation measure  the best performance between gsmn  with and without
propagation  and gsimn is indicated in bold  the number of variables in the
domain is denoted by n and the number of data points in each data set by n  

 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  

data set
name
echocardiogram
ecoli
lenses
hayes roth
hepatitis
cmc
balance scale
baloons
flag
tic tac toe
bridges
car
monks  
haberman
nursery
crx
imports   
dermatology
adult

n

n

  
 
 
 
  
  
 
 
  
  
  
 
 
 
 
  
  
  
  

  
   
  
   
  
    
   
  
   
   
  
    
   
   
     
   
   
   
     

weighted number of tests
gsmn
gsmn
gsimn
 w o prop    w  prop  
    
    
   
   
   
   
  
  
  
   
  
  
    
   
   
   
   
   
  
  
  
  
  
  
    
    
   
   
   
   
   
   
   
   
   
  
   
  
  
  
  
  
   
   
   
    
   
   
    
    
    
    
    
    
   
   
   

gsmn
 w o prop  
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

accuracy
gsmn
 w  prop  
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

gsimn
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

because for real world data the structure of the underlying bayesian network  if any 
is unknown  it is impossible to measure the hamming distance of the resulting network
structure  instead  we measured the estimated accuracy of a network produced by gsmn
or gsimn by comparing the result  true or false  of a number of conditional independence
tests on the network learned by them  using vertex separation  to the result of the same tests
performed on the data set  using a   test   this approach is similar to estimating accuracy
in a classification task over unseen instances but with inputs here being triplets  x  y  z 
and the class attribute being the value of the corresponding conditional independence test 
we used     of the real world data set  randomly sampled  as input to gsmn and gsimn
and the entire data set to the   test  this corresponds to the hypothetical scenario that
a much smaller data set is available to the researcher  and approximates the true value
of the test by its outcome on the entire data set  since the number of possible tests is
exponential  we estimated the independence accuracy by sampling        triplets  x  y  z 
randomly  evenly distributed among all possible conditioning set sizes m              n    
 i e          n     tests for each m   each of these triplets was constructed as follows 
first  two variables x and y were drawn randomly from v  second  the conditioning set
was determined by picking the first m variables from a random permutation of v   x  y   
denoting by t this set of        triplets  by t  t a triplet  by idata  t  the result of a test
performed on the entire data set and by inetwork  t  the result of a test performed on the

   

fiefficient markov network structure discovery using independence tests

network output by either gsmn or gsimn  the estimated accuracy is defined as 
fi
ofifi
  fifin
t  t   inetwork  t    idata  t  fifi 
accuracy
   
 t   fi

for each of the data sets  table   shows the detailed results for accuracy and the weighted
number of tests for the gsmn and gsimn algorithms  these results are also plotted in
figure     with the horizontal axis indicating the data set index appearing in the first column
of table    figure    plots two quantities in the same graph for these real world data sets 
the ratio of the weighted number of tests of gsimn versus the two gsmn algorithms
and the difference of their accuracies  for each data set  an improvement of gsimn over
gsmn corresponds to a number smaller than   for the ratios and a positive histogram bar
for the accuracy differences  we can observe that gsimn reduced the weighted number of
tests on every data set  with maximum savings of     over gsmn without propagation
 for the crx data set  and     over gsmn with propagation  for the crx data set as
well   moreover  in    out of    data sets gsimn resulted in improved accuracy    in a tie
and only   in somewhat reduced accuracy compared to gsmn with propagation  for the
nursery and balance scale data sets  

   conclusions and future research
in this paper we presented two algorithms  gsmn and gsimn  for learning efficiently
the structure of a markov network of a domain from data using the independence based
approach  as opposed to np hard algorithms based on maximum likelihood estimation  we
evaluated their performance through measurement of the weighted number of tests they
require to learn the structure of the network and the quality of the networks learned from
both artificial and real world data sets  gsimn showed a decrease in the vast majority
of artificial and real world domains in an output network quality comparable to that of
gsmn   with some cases showing improvement  in addition  gsimn was shown to be
nearly optimal in the number of tests executed compared to gsimn fch  which uses an
exhaustive search to produce all independence information that can inferred from pearls
axioms  some directions of future research include an investigation into the way the topology
of the underlying markov network affects the number of tests required and quality of the
resulting network  especially for commonly occurring topologies such as grids  another
research topic is the impact on number of tests of other examination and grow orderings of
the variables 

acknowledgments
we thank adrian silvescu for insightful comments on accuracy measures and general advice
on the theory of undirected graphical models 

appendix a  correctness of gsmn
for each variable x  v examined during the main loop of the gsmn algorithm  lines
       the set bx of variable x  v is constructed by growing and shrinking a set s 
   

fibromberg  margaritis    honavar

starting from the empty set  x is then connected to each member of bx to produce the
structure of a markov network  we prove that this procedure returns the actual markov
network structure of the domain 
for the proof of correctness we make the following assumptions 
 the axioms of eqs      hold 
 the probability distribution of the domain is strictly positive  required for intersection
axiom to hold  
 tests are conducted by querying an oracle  which returns its true value in the underlying model 
the algorithm examines every variable y  x for inclusion to s  and thus to bx  
during the grow phase  lines    to     and  if y was added to s during the grow phase  it
considers it for removal during the shrinking phase  lines    to      note that there is only
one test executed between x and y during the growing phase of x  we call this the grow
test of y on x  line      similarly  there is one or no tests executed between x and y
during the shrinking phase  this test  if executed  is called the shrink test of y on x  line
    
the general idea behind the proof is to show that  while learning the blanket of x 
variable y is in s by the end of the shrinking phase if and only if the dependence  x  y  
v   x  y    between x and y holds  which  according to theorem   at the end of the
appendix  implies there is an edge between x and y    we can immediately prove one
direction 
lemma    if y 
  s at the end of the shrink phase  then  xy   v   x  y    
proof  let us assume that y 
  s by the end of the shrink phase  then  either y was
not added to set s during the grow phase  i e   line    was never reached   or removed
from it during the shrink phase  i e   line    was reached   the former can only be true if
 pxy     in line     indicating x and y are unconditionally independent  or y was found
independent of x in line     the latter can only be true if y was found independent of x
in line     in all cases then a  v   x  y   such that  xy   a   and by strong union
then  xy   v   x  y    
the opposite direction is proved in lemma   below  however  its proof is more involved 
requiring a few auxiliary lemmas  observations  and definitions  the two main auxiliary
lemmas are   and    both use the lemma presented next  lemma    inductively to extend
the conditioning set of dependencies found by the grow and shrink tests between x and y  
to all the remaining variables v x  y    the lemma shows that  if a certain independence
holds  the conditioning set of a dependence can be increased by one variable 
lemma    let x  y  v  z  v   x  y    and z  z  then  w  v 
 x  y   z  and  xw   z   y       x  y   z   w    

   

fiefficient markov network structure discovery using independence tests

proof  we prove by contradiction  and make use of the axioms of intersection  i   strong
union  su   and decomposition  d   let us assume that  x  y   z  and  xw   z  y   
but  xy   z   w     then
 xy   z   w    and  xw   z   y   
su

 

 xy   z   w    and  xw   z   y   

i

 x y  w     z 

d

 

 xy   z    xw   z 

 

 xy   z  

 

this contradicts the assumption  x  y   z  
we now introduce notation and definitions and prove auxiliary lemmas 
we denote by sg the value of s at the end of the grow phase  line     i e   the set of
variables found dependent of x during the grow phase  and by ss the value of s at the end
of the shrink phase  line      we also denote by g the set of variables found independent
of x during the grow phase and by u    u            uk   the sequence of variables shrunk from
bx   i e   found independent of x during the shrink phase  the sequence u is assumed
ordered as follows  if i   j then variable ui was found independent from x before uj
during the shrinking phase  a prefix of the first i variables  u            ui    of u is denoted
by ui   for some test t performed during the algorithm  we define k t  as the integer such
that uk t  is the prefix of u containing the variables that were found independent of x in
this loop before t  furthermore  we abbreviate uk t  by ut  
from the definition of u and the fact that in the grow phase the conditioning set
increases by dependent variables only  we can immediately make the following observation 
observation    for some variable ui  u  if t denotes the shrink test performed between
x and ui then ut   ui   
we can then relate the conditioning set of the shrink test t with ut as follows 
lemma    if y  ss and t    x  y   z  is the shrink test of y   then z   sg  ut   y   
proof  according to line    of the algorithm  z   s   y    at the beginning of the shrink
phase  line     s   sg   but variables found independent afterward and until t is conducted
are removed from s in line     thus  by the time t is performed  s   sg  ut and the
conditioning set becomes sg  ut   y   
corollary     xui   sg  ui   
proof  the proof follows immediately from lemma    observation    and the fact that
ui   ui    ui   
the following two lemmas use lemma   inductively to extend the conditioning set of
the dependence between x and a variable y in ss   the first lemma starts from the shrink
test between x and y  a dependence   and extends its conditioning set from ss   y    or
equivalently sg   y    ut according to lemma    to sg   y   
   

fibromberg  margaritis    honavar

lemma    if y  ss and t is the shrink test of y   then  x  y   sg   y    
proof  the proof proceeds by proving
 x  y   sg   y    ui  
by induction on decreasing values of i  for i                 k t    starting at i   k t   the
lemma then follows for i     by noticing that u     
 base case  i   k t    from lemma    t    x  y   sg   y    ut    which equals
 x  y   sg   y    uk t    by definition of ut   since y  ss   it must be the case that
t was found dependent  i e    x  y   sg   y    uk t    
 inductive step  let us assume that the statement is true for i   m      m  k t   
 x  y   sg   y    um   

   

we need to prove that this is also true for i   m    
 x  y   sg   y    um    
by corollary    we have
 xum   sg  um  
and by strong union 
 xum    sg  um     y   
or
 xum    sg  um   y      y    

   

from eqs           and lemma   we get the desired relation 
 x  y    sg   y    um     um       x  y   sg   y    um    

observation    by definition of sg   we have that for every test t    x  y   z  performed
during the grow phase  z  sg  
the following lemma completes the extension of the conditioning set of the dependence
between x and y  ss into the universe of variables v   x  y    starting from sg   y  
 where lemma   left off  and extending it to sg  g   y   
lemma    if y  ss   then  x  y   sg  g   y    
proof  the proof proceeds by proving
 x  y   sg  gi   y   
by induction on increasing values of i from   to  g   where gi denotes the first i elements
of an arbitrary ordering of set g 
   

fiefficient markov network structure discovery using independence tests

 base case  i       follows directly from lemma   for i      since g     
 inductive step  let us assume that the statement is true for i   m     m    g  
 x  y   sg  gm   y    

   

we need to prove that it is also true for i   m     
 x  y   sg  gm     y    

   

from observation   the grow test of gm results in the independence 
 xgm   z   where z  sg  
by the strong union axiom this can become 
 xgm   z   y     where z  sg

   

 xgm    z   y      y     where z  sg  

   

or equivalently
since z  sg  sg  gm   we have that z   y    sg  gm   and so from eq     
and lemma   we get the desired relation 
 x  y    sg  gm   y     gm      x  y   sg  gm     y    

finally  we can prove that x is dependent with every variable y  ss given the universe
v   x  y   
lemma    if y  ss   then  x  y   v   x  y    
proof  from lemma   
 x  y   sg  g   y   
it suffices then to prove that sg  g   y     v   x  y    in loop    of gsmn   the
queue x is populated with all elements in v   x   and then  in line     y is removed
from x   the grow phase then partitions x into variables dependent of x  set sg   and
independent of x  set g  
corollary    y  ss   x  y   v   x  y    
proof  follows directly from lemmas   and   
from the above corollary we can now immediately show that the graph returned by
connecting x to each member of bx   ss is exactly the markov network of the domain
using the following theorem  first published by pearl and paz        
theorem     pearl   paz        every dependence model m satisfying symmetry  decomposition  and intersection  eqs       has a unique markov network g    v  e  produced by
deleting from the complete graph every edge  x  y   for which  xy   v   x  y    holds
in m   i e  
 x  y   
  e   xy   v   x  y    in m  
   

fibromberg  margaritis    honavar

appendix b  correctness of gsimn
the gsimn algorithm differs from gsmn only by the use of test subroutine igsimn
instead of igsmn  algorithms   and    respectively   which in turn differs by a number
of additional inferences conducted to obtain the independencies  lines   to      these
inferences are direct applications of the strong union axiom  which holds by assumption 
and the triangle theorem  which was proven to hold in theorem     using the correctness
of gsmn  proven in appendix a  we can therefore conclude that the gsimn algorithm
is correct 

references
abbeel  p   koller  d     ng  a  y          learning factor graphs in polynomial time and
sample complexity  journal of machine learning research              
acid  s     de campos  l  m          searching for bayesian network structures in the
space of restricted acyclic partially directed graphs  journal of artificial intelligence
research             
agresti  a          categorical data analysis   nd edition   wiley 
aliferis  c  f   tsamardinos  i     statnikov  a          hiton  a novel markov blanket
algorithm for optimal variable selection  in proceedings of the american medical
informatics association  amia  fall symposium 
anguelov  d   taskar  b   chatalbashev  v   koller  d   gupta  d   heitz  g     ng  a 
        discriminative learning of markov random fields for segmentation of  d range
data  in proceedings of the conference on computer vision and pattern recognition
 cvpr  
barahona  f          on the computational complexity of ising spin glass models  journal
of physics a  mathematical and general                    
besag  j          spacial interaction and the statistical analysis of lattice systems  journal
of the royal statistical society  series b             
besag  j   york  j     mollie  a          bayesian image restoration with two applications
in spatial statistics   annals of the institute of statistical mathematics          
bromberg  f   margaritis  d     honavar  v          efficient markov network structure discovery from independence tests  in proceedings of the siam international conference
on data mining 
buntine  w  l          operations for learning with graphical models  journal of artificial
intelligence research            
castelo  r     roverato  a          a robust procedure for gaussian graphical model search
from microarray data with p larger than n  journal of machine learning research 
            
chow  c     liu  c          approximating discrete probability distributions with dependence trees  ieee transactions on information theory                   

   

fiefficient markov network structure discovery using independence tests

cochran  w  g          some methods of strengthening the common   tests  biometrics 
           
della pietra  s   della pietra  v     lafferty  j          inducing features of random fields 
ieee transactions on pattern analysis and machine intelligence                 
dobra  a   hans  c   jones  b   nevins  j  r   yao  g     west  m          sparse graphical
models for exploring gene expression data  journal of multivariate analysis         
    
edwards  d          introduction to graphical modelling   nd edition   springer  new
york 
friedman  n   linial  m   nachman  i     peer  d          using bayesian networks to
analyze expression data  computational biology            
geman  s     geman  d          stochastic relaxation  gibbs distributions  and the bayesian
relation of images   ieee transactions on pattern analysis and machine intelligence 
          
heckerman  d          a tutorial on learning bayesian networks  tech  rep  msr tr       
microsoft research 
heckerman  d   geiger  d     chickering  d  m          learning bayesian networks  the
combination of knowledge and statistical data  machine learning             
henrion  m          propagation of uncertainty by probabilistic logic sampling in bayes
networks  in lemmer  j  f     kanal  l  n   eds    uncertainty in artificial intelligence    elsevier science publishers b v   north holland  
hofmann  r     tresp  v          nonlinear markov networks for continuous variables  in
neural information processing systems  vol      pp         
isard  m          pampas  real valued graphical models for computer vision  in ieee
conference on computer vision and pattern recognition  vol     pp         
jerrum  m     sinclair  a          polynomial time approximation algorithms for the ising
model  siam journal on computing               
kearns  m  j     vazirani  u  v          an introduction to computational learning theory 
mit press  cambridge  ma 
koller  d     sahami  m          toward optimal feature selection  in international conference on machine learning  pp         
lam  w     bacchus  f          learning bayesian belief networks  an approach based on
the mdl principle  computational intelligence             
lauritzen  s  l          graphical models  oxford university press 
margaritis  d     thrun  s          bayesian network induction via local neighborhoods  in
solla  s   leen  t     muller  k  r   eds    advances in neural information processing
systems     pp          mit press 
mccallum  a          efficiently inducing features of conditional random fields  in proceedings of uncertainty in artificial intelligence  uai  

   

fibromberg  margaritis    honavar

newman  d  j   hettich  s   blake  c  l     merz  c  j          uci repository of machine
learning databases  tech  rep   university of california  irvine  dept  of information
and computer sciences 
pena  j  m          learning gaussian graphical models of gene networks with false discovery rate control  in proceedings of the  th european conference on evolutionary
computation  machine learning and data mining in bioinformatics  pp         
pearl  j          probabilistic reasoning in intelligent systems  networks of plausible inference  morgan kaufmann publishers  inc 
pearl  j     paz  a          graphoids  a graph based logic for reasoning about releveance
relations  tech  rep          r    l   cognitive systems laboratory  university of
california 
rebane  g     pearl  j          the recovery of causal poly trees from statistical data 
in kanal  l  n   levitt  t  s     lemmer  j  f   eds    uncertainty in artificial
intelligence    pp          amsterdam  north holland 
schafer  j     strimmer  k          an empirical bayes approach to inferring large scale
gene association networks  bioinformatics             
scott  d  w          multivariate density estimation  wiley series in probability and
mathematical statistics  john wiley   sons 
shekhar  s   zhang  p   huang  y     vatsavai  r  r         in kargupta  h   joshi  a  
sivakumar  k     yesha  y   eds    trends in spatial data mining  chap      pp 
        aaai press   the mit press 
spirtes  p   glymour  c     scheines  r          causation  prediction  and search   nd
edition   adaptive computation and machine learning series  mit press 
srebro  n     karger  d          learning markov networks  maximum bounded tree width
graphs  in acm siam symposium on discrete algorithms 
tsamardinos  i   aliferis  c  f     statnikov  a       a   algorithms for large scale markov
blanket discovery  in proceedings of the   th international flairs conference  pp 
       
tsamardinos  i   aliferis  c  f     statnikov  a       b   time and sample efficient discovery of markov blankets and direct causal relations  in proceedings of the  th acm
sigkdd international conference on knowledge discovery and data mining  pp 
       
tsamardinos  i   brown  l  e     aliferis  c  f          the max min hill climbing bayesian
network structure learning algorithm  machine learning           
whittaker  j          graphical models in applied multivariate statistics  john wiley  
sons  new york 

   

fi