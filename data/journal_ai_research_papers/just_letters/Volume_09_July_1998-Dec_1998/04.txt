journal of artificial intelligence research                 

submitted       published      

probabilistic inference from arbitrary uncertainty
using mixtures of factorized generalized gaussians
alberto ruiz
pedro e  lpez de teruel
m  carmen garrido
universidad de murcia  facultad de informtica 
campus de espinardo         murcia  spain

aruiz dif um es
pedroe ditec um es
mgarrido dif um es

abstract
this paper presents a general and efficient framework for probabilistic inference and learning from
arbitrary uncertain information  it exploits the calculation properties of finite mixture models  conjugate families and factorization  both the joint probability density of the variables and the likelihood
function of the  objective or subjective  observation are approximated by a special mixture model  in
such a way that any desired conditional distribution can be directly obtained without numerical integration  we have developed an extended version of the expectation maximization  em  algorithm to
estimate the parameters of mixture models from uncertain training examples  indirect observations   as
a consequence  any piece of exact or uncertain information about both input and output values is consistently handled in the inference and learning stages  this ability  extremely useful in certain situations  is not found in most alternative methods  the proposed framework is formally justified from
standard probabilistic principles and illustrative examples are provided in the fields of nonparametric
pattern classification  nonlinear regression and pattern completion  finally  experiments on a real application and comparative results over standard databases provide empirical evidence of the utility of the
method in a wide range of applications 

   introduction
the estimation of unknown magnitudes from available information  in the form of sensor measurements or subjective judgments  is a central problem in many fields of science and engineering 
to solve this task  the domain must be accurately described by a model able to support the desired
range of inferences  when satisfactory models cannot be derived from first principles  approximations must be obtained from empirical data in a learning stage 
consider a domain z composed by a collection of objects z   z   z        zn   represented by
vectors of n attributes  given some partial knowledge s  expressed in a general form explained
later  about a certain object z  we are interested in computing a good estimate z   s     close to the
true z  we allow heterogeneous descriptions  any attribute zi may be continuous  discrete  or symbolic valued  including mixed types  if there is a specific subset of unknown or uncertain attributes
to be estimated  the attribute vector can be partitioned as z    x  y   where y  z denotes the target
or output attributes  the target attributes can be different for different objects z  this scenario includes several usual inference paradigms  for instance  when there is a specific target symbolic
attribute  the task is called pattern recognition or classification  when the target attribute is continuous  the inference task is called regression or function approximation  in general  we are interested in a general framework for pattern completion from partially known objects 
example    to illustrate this setting  assume that the preprocessor of a hypothetical computer
vision system obtains features of a segmented object  the instances of the domain are described

     ai access foundation and morgan kaufmann publishers  all rights reserved 

firuiz  lpez de teruel   garrido

by the following n   attributes  area  z     color  z    white  black  red        distance 
z     shape  z    circular  rectangular  triangular        texture  z    soft  rough       
objecttype  z    door  window       and angle  z     a typical instance may be z       
blue       triangular  soft  window       if the object is partially occluded or   dimensional 
some attributes will be missing or uncertain  for instance  the available information s about z
could be expressed as       blue or black       triangular     window     door         
where z   z   z  are uncertain  z   z  are exact and z   z  are missing  in this case we could be interested in estimates for y    z   z   z   and even in improving our knowledge on z  and z  

the non deterministic nature of many real world domains suggests a probabilistic approach 
where attributes are considered as random variables  objects are assumed to be drawn independently and identically distributed from p z    p z        zn    p x  y   the multivariate joint probability
density function of the attributes  which completely characterizes the n dimensional random variable z  to simplify notation  we will use the same function symbol p   to denote different p d f s if
they can be identified without risk of confusion 
according to statistical decision theory  berger        optimum estimators for the desired attributes are obtained through minimization of a suitable expected loss function 

y opt   s     argmin y e  l  y   y    s 


where l y  y   is the loss incurred when the true y is estimated by y   estimators are always features of the conditional or posterior distribution p y s  of the target variables given the available
information  for instance  the minimum squared error  mse  estimator is the posterior mean  the
minimum linear loss estimator is the posterior median and the minimum error probability  ep     
loss  estimator is the posterior mode 
example    a typical problem is the prediction of an unknown attribute y from the observed attributes x  in this case the available information can be written as s    x      if y is continuous  it
is reasonable to use the mse estimator  y mse   s     e  y   x    the general regression function  if y
is symbolic and the same loss is associated to all errors  the ep estimator is adequate 
y ep   s     argmaxy p y x    argmaxy p x y p y   it corresponds to the maximum a posteriori rule
or bayes test  widely used in statistical pattern recognition 

the joint density p z    p x  y  plays an essential role in the inference process  it implicitly
includes complete information about attribute dependences  in principle  any desired conditional
distribution or estimator can be computed from the joint density by adequate integration  probabilistic inference is the process of computing the desired conditional probabilities from a  possibly
implicit  joint distribution  from p z   the prior  model of the domain  comprising implications  and
s  a known event  somewhat related to a certain z   we could obtain the posterior p z s  and the
desired target marginal p y s   the probabilistic consequent  
example    if we observe an exact value xo in attribute x  i e  s     x   xo   we have 

p  y  s    p y  xo   



y

p   xo   y  
p  xo   y  dy

if we know that instance z is in a certain region r in the attribute space  i e  s    z  r   we
compute the marginal density of y from the joint p z    p x  y  restricted to region r  fig     

   

fiprobabilistic inference from uncertain data using mixtures

p  y  s    



x

p  x  y    x   y    r   dx  

 p  x  y  dx
 p  x  y  dxdy
r

r

more general types of uncertain information s about z will be discussed later 

y
p y r 
r

p x y 
x

figure    the conditional probability density of y  assuming z    x y   r 

in summary  from the joint density p z  of a multivariate random variable  any subset of variables y  z may be  in principle  estimated given the available information s about the whole z    x 
y   in practical situations  two steps are required to solve the inference problem  first  a good model
of the true joint density p z  must be obtained  second  the available information s must be efficiently processed to improve our knowledge about future  partially specified instances z  these two
complementary aspects  learning and inference  are approached from many scientific fields  providing different methodologies to solve practical applications 
from the point of view of computer science  the essential goal of inductive inference is to
find an approximate intensional definition  properties  of an unknown concept  subset of the domain  from an incomplete extensional definition  finite sample   machine learning techniques
 michalski  carbonell   mitchell             hutchinson       provide practical solutions  e g 
automatic construction of decision trees  to solve many situations where explicit programming
must be avoided  computational learning theory  valiant       wolpert       vapnik      
studies the feasibility of induction in terms of generalization ability and resource requirements of
different learning paradigms 
under the general setting of statistical decision theory  modeling techniques and the operational aspects of inference  based in numerical integration  monte carlo simulation  analytic approximations  etc   are extensively studied from the bayesian perspective  berger       bernardo
  smith        in the more specific field of statistical pattern recognition  duda   hart      
fukunaga        standard parametric or nonparametric density approximation techniques  izenman
      are used to learn from training data the class conditional p d f s required by the optimum
decision rule  for instance  if the class conditional densities p x y  are gaussian  the required parameters are the mean vector and covariance matrix of the feature vector in each class and the decision regions for y in x have quadratic boundaries  among the nonparametric classification techniques  the parzen method and the k n nearest neighbors rule must be mentioned  analogously  if
the target attribute is continuous and the statistical dependence between input and output variables
p x y  can be properly modeled by joint normality  we get multivariate linear regression  y mse x   
a x   b  where the required parameters are the mean values and the covariance matrix of the attrib 

   

firuiz  lpez de teruel   garrido

utes  nonlinear regression curves can be also derived from nonparametric approximation techniques  nonparametric methods present slower convergence rates  requiring significantly larger
sample sizes to obtain satisfactory approximations  they are also strongly affected by the dimensionality of the data and the selection of the smoothing parameter is a crucial step  in contrast  they
only require some kind of smoothness assumption on the target density 
neural networks  hertz et  al       are computational models trainable from empirical data
that have been proposed to solve more complex situations  their intrinsic parallel architecture is
especially efficient in the inference stage  one of the most widely used neural models is the multilayer perceptron  a universal function approximator  hornik et al        that breaks the limitations
of linear decision functions  the backpropagation learning algorithm  rumelhart et al        can 
in principle  adjust the network weights to implement arbitrary mappings  and the network outputs
show desirable probabilistic properties  wan       rojas        there are also unsupervised networks for probability density function approximation  kohonen        however  neural models
usually contain a large number of adjustable parameters  which is not convenient for generalization
and  frequently  long times are required for training in relatively easy tasks  the input   output role
of attributes cannot be changed in runtime and missing and uncertain values are poorly supported 
bayesian networks  based in the concept of conditional independence  are among the most
relevant probabilistic inference technologies  pearl       heckerman   wellman        the joint
density of the variables is modeled by a directed graph which explicitly represents dependence
statements  a wide range of inferences can be performed under this framework  chang   fung
      lauritzen   spiegelhalter       and there are significant results on inductive learning of
network structures  bouckaert       cooper   herskovits       valiveti   oomen        this
approach is adequate when there is a large number of variables showing explicit dependences and
simple cause effect relations  nevertheless  solving arbitrary queries is np complete  automatic
learning algorithms are time consuming and the allowed dependences between variables are relatively simple 
in an attempt to mitigate some of the above drawbacks  we have developed a general and efficient inference and learning framework based on the following considerations  it is well known
 titterington et al        mclachlan   basford       dalal   hall       bernardo   smith      
xu   jordan       that any reasonable probability density function p z  can be approximated up to
the desired degree of accuracy by a finite mixture of simple components ci  i      l 

p  z     p ci   p  z  ci  

   

i

the superposition of simple densities is extensively used to approximate arbitrary data dependences  fig      maximum likelihood estimators of the mixture parameters can be efficiently obtained from samples by the expectation maximization  em  algorithm  dempster  laird   rubin
      redner   walker        see section    

   

fiprobabilistic inference from uncertain data using mixtures

 a 
 b 
 c 
figure    illustrative example of density approximation using a mixture model   a  samples from a p d f  p x y  showing a nonlinear dependence   b  mixture model for p x y 
with   gaussian components obtained by the standard em algorithm   c  location of
components 

the decomposition of probability distributions using mixtures has been frequently applied to
unsupervised learning tasks  especially cluster analysis  mclachlan   basford       duda  
hart       fukunaga        the a posteriori probabilities of each postulated category are computed
for all the examples  which are labeled according to the most probable source density  however 
mixture models are specially useful in nonparametric supervised learning situations  for instance 
the class conditional densities required in statistical pattern recognition were individually approximated in  priebe   marchette       traven       by finite mixtures  hierarchical mixtures of
linear models were proposed in  jordan   jacobs       peng et  al        mixtures of factor analyzers have been developed in  ghahramani   hinton       hinton  dayan    revow       and
mixture models have been also useful for feature selection  pudil et al         mixture modeling is
a growing semiparametric probabilistic learning methodology with applications in many research
areas  weiss   adelson       fan et al        moghaddam   pentland       
this paper introduces a framework for probabilistic inference and learning from arbitrary uncertain data  any piece of exact or uncertain information about both input and output values is consistently handled in the inference and learning stages  we approximate both the joint density p z 
 model of the domain  and the relative likelihood function p s z   describing the available information  by a specific mixture model with factorized conjugate components  in such a way that numerical integration is avoided in the computation of any desired estimator  marginal or conditional
density 
the advantages of modeling arbitrary densities using mixtures of natural conjugate components were already shown in  dalal   hall        and  recently  inference procedures based in a
similar idea have been proposed in  ghahramani   jordan       cohn et al        peng et al       
palm        however  our method efficiently handles uncertain data using explicit likelihood
functions  which has not been extensively used before in machine learning  pattern recognition or
related areas  we will follow standard probabilistic principles  providing natural statistical validation procedures 
the organization of the paper is as follows  section   reviews some elementary results and
concepts used in the proposed framework  section   addresses the inference stage  section   is
concerned with learning  extending the em algorithm to manage uncertain information  section  
discusses the method in relation to alternative techniques and presents experimental evaluation 
the last section summarizes the conclusions and future directions of this work 

   

firuiz  lpez de teruel   garrido

   preliminaries
    a calculus of generalized normals
in many applications  the instances of the domain are represented simultaneously by continuous
and symbolic or discrete variables  as in wilson   martinez        to simplify notation  we will
denote both probability impulses and gaussian densities by means of a common formalism  the
generalized normal
 x    denotes a probability density function with the following properties 

t

t x    

if      

    x      
 
exp

      

t x      t x      t x     x 

if      

tzero  x   
is a gaussian density with mean  and standard deviation      when the dispersion is
t reduces
to a diracs delta function located at   in both cases t is a proper p d f  
t x       
 t x    dx    
x

the product of generalized normals can be elegantly expressed  papoulis      pp       berger
      by 
for        

t x       t x        t x     t               

   

where the mean  and dispersion  of the new normal are given by 

          
 
       

    
    
      
 

this relation is useful for computing the integral of the product of two generalized normals 
for        



x

t x       t x      dx   t                

   

and  for consistency  we define



for           

x

t x    t x    dx   t       i     

where i predicate      if predicate is true and zero otherwise  virtually any reasonable univariate
probability distribution or likelihood function can be accurately modeled by an appropriate mixture
of generalized normals  in particular  p d f s over symbolic variables are mixtures of impulses 
without loss of generality  symbols may be arbitrarily mapped to specific numbers and represented
over numeric axes  integrals over discrete domains become sums 
example    let us approximate the p d f  p x  of a mixed continuous and symbolic valued random variable x by a mixture of generalized normals  assume that x takes with probability    
the exact value     with a special meaning   and with probability     a random value continuously distributed following the triangular shape shown in fig     the density p x  can be accurately approximated  see section    using   generalized normals 

t x          t x               t x               t x         

p x      

   

fiprobabilistic inference from uncertain data using mixtures

figure    the p d f  of a mixed random variable approximated by a mixture of generalized normals 

    modeling uncertainty  the likelihood principle
assume that the value of a random variable z must be inferred from a certain observation or subjective information s  if z has been drawn from p z  and the measurement or judgment process is
characterized by a conditional p s z   our knowledge on z is updated according to p z s  p z  p s z 
  p s   where p s    z p s z  p z  dz  see fig     
the likelihood function fs z   p s z  is the probability density ascribed to s by each possible
z  it is an arbitrary nonnegative function over z that can be interpreted in two alternative ways  it
can be the objective conditional distribution p s z  of a physical measurement process  e g  a
model of sensor noise  specifying bias and variance of the observable s for every possible true
value z   also known as error model  it can also be a subjective judgment about the chance of the
different z values  e g  intervals  more likely regions  etc    based on vague or difficult to formalize
information  the dispersion of fs z  is directly related to the uncertainty associated to the measurement process  following the likelihood principle  berger        we explicitly assume that all the
experimental information required to perform probabilistic inference is contained in the likelihood
function fs z  

p z 
z 

z 

z 

z

p s z  

p s z  

p s z  
s

so

prior
model of
measurement

p s 
s
fso z   p so z 

z

observable

likelihood of
observation so

p z so 
posterior
z
figure    illustration of the elementary bayesian univariate inference process 

   

firuiz  lpez de teruel   garrido

    inference using mixtures of conjugate densities
the computation of p z s  may be hard  unless p z  and p s z  belong to special  conjugate  families  berger       bernardo   smith        in this case the posterior density can be analytically
obtained from the parameters of the prior and the likelihood  avoiding numeric integration  the
prior  the likelihood and the posterior are in the same mathematical family  the belief structure is
closed under the inference process 

t

example    in the univariate case  assume that z is known to be normally distributed around r
with dispersion r  i e  p z     z  r  r   assume also that our measurement device has gaussian noise  so the observed values are distributed according to p s z   
 s  z  s   therefore 
if we observe a certain value so  from the property of the product of generalized normals in eq 
     the posterior knowledge on z becomes another normal
 z       the new expected location of z can be expressed as a weighted average of r and so      so       r and the uncertainty is reduced to      s    the coefficient      r       r   s    quantifies the relative im 

t

t

portance of the experiment with respect to the prior 

this computational advantage can be extended to the general case by using mixtures of conjugate families  dalal   hall       to approximate the desired joint probability distribution and the
likelihood function 
example    if the domain and the likelihood are modeled respectively by

p z  


i

pi

t z       
i

p so z  

i



r

r

t z       
r

r

 where  r   r and  r depend explicitly on the observed so   then the posterior can be also
written as the following mixture 



p z so  

i  r

from properties     and      the parameters

i   r

i  r

t z  

   i  r  

   

i  r and  i  r and the weights i  r are given by 

i  r     r  i
 
i     r 

 i  r 

i  r

 i  r  

i  r
 i     r 

pi  r t  i   r   i     r   

p 
k

l

t  k   l    k     l  

k  l

    the role of factorization
given a multivariate observation z partitioned into two subvectors  z    x  y   assume that we are
interested in inferring the value of the unknown attributes y from the observed attributes x  note
that if x and y are statistically independent  the joint density is factorizable  p z    p x  y    p x 
p y  and  therefore  the posterior p y x  equals the prior marginal p y   the observed x carries no
predictive information about y and the optimum estimators do no depend on x  for instance 
y mse   x     e y x    e y  and y ep   x     argmax y p  y     this is the simplest estimation task  no
runtime computations are required for the optimum solution  which may be precalculated 

   

fiprobabilistic inference from uncertain data using mixtures

in realistic situations the variables are statistically dependent  in general  the joint density cannot be factorized and the required marginal densities may be hard to compute  however  interesting
consequences arise if the joint density is expressed as a finite mixture of factorized  with independent variables  components c   c        cl  

p z    p  z         z n      p ci   p  z  ci      p ci    p  z j   ci  
i

i

   

j

this structure is convenient for inference purposes  in particular  in terms of the desired partition of z    x  y  

p z    p x  y   

 p c   p  x  c   p  y  c  
i

i

i

i

so the marginal densities are mixtures of the marginal components 

p  x      p  x   y  dy    p ci   p  x  ci  
y

i

p  y      p ci   p  y  ci  
i

and the desired conditional densities are also mixtures of the marginal components 

p  y  x       i   x   p  y  ci  

   

i

where the weights  i  x  are the probabilities that the observed x has been generated by each component ci  

i   x   

p ci   p  x ci  

 p c   p  x c  
j

  p ci   x 

j

j

the p d f  approximation capabilities of mixture models with factorized components remain
unchanged  at the cost of a possibly higher number of components to obtain the desired degree of
accuracy  avoiding artifacts  see fig      section     discusses the implications of factorization in
relation with alternative model structures 

 a 
 b 
figure     a  density approximation for the data in fig     using a mixture with   factorized components   b  location of components  note how an arbitrary dependence can be
represented as a mixture of components which itself have independent variables  observe
that a somewhat smoother solution could be obtained increasing the number of components  

   

firuiz  lpez de teruel   garrido

   the mfgn framework
the previous concepts will be integrated in a general probabilistic inference framework that we call
mfgn  mixtures of factorized generalized normals   fig    shows the abstract dependence relations among attributes in a generic domain  upper section of the figure  and between the attributes
and the observed information  lower section   in the mfgn framework  both relations are modeled
by finite mixtures of products of generalized normals  the key idea is using factorization to cope
with multivariate domains and heterogeneous attribute vectors  and conjugate densities to efficiently perform inferences given arbitrary uncertain information  in this section  we will derive the
main inference expressions  the learning stage will be described in section   

p z 
z 

zn
z 

model of
domain
p z 

zj
model of
measurement
p s z 

s

figure    generic dependences in the inference process 

    modeling attribute dependences in the domain
in the mfgn framework the attribute dependencies in the domain are modeled by a joint density in
the form of a finite mixture of factored components  as in expression      where the component
marginals p  z j   ci    t  z j    ij    ij   are generalized normals 

p  z      pi
i

 t  z

j

   ij    ij  

i    l  j    n 

   

j

if desired  the terms associated to the pure symbolic attributes z j  with all the  ij      can be
collected in such a way that the component marginals are expressed as mixtures of impulses 

p  z j   ci     ti j  t  z j     

   



where ti j   p z j     ci   is the probability that z j takes its  th value in component ci   this
manipulation reduces the number l of global components in the mixture  the adjustable parameters
of the model are the proportions pi   p ci   and the mean value  ij and dispersion  ij of the j th
j
attribute in the i th component  or  for the symbolic attributes  the probabilities ti      while the

structure     will be explicitly used for symbolic attributes in applications and illustrative examples  most of the mathematical derivations will be made over the concise expression     

   

fiprobabilistic inference from uncertain data using mixtures

when all variables are continuous  the mfgn architecture reduces to a mixture of gaussians
with diagonal covariance matrices  the proposed factorized structure extends the properties of
diagonal covariance matrices to heterogeneous attribute vectors  we are interested in joint models 
which support inferences from partial information about any subset of variables  note that there is
not an easy way to define a measure of statistical depencence between symbolic and continuous
attributes  to be used as a parameter of some probability density function   the required  hetereogeneous  dependence model can be conveniently captured by superposition of simple factorized
 with independent variables  densities 
example    figure   shows an illustrative   attribute data set  x and y are continuous and z is
symbolic  and the components of the mfgn approximation obtained by the em algorithm
 see section    for their joint density  the parameters of the mixture are shown in table   
note that  because of the overlapped structure of the data  some components    and    are assigned to both values of the symbolic attribute z 

 a 
 b 
figure     a  simple data set with two continuous and one symbolic attribute 
 b  location of the mixture components 

i

pi

 ix

 ix

 iy

 iy

tiz white

 
   
    
   
    
   
 
 
   
    
   
    
   
 
 
   
   
   
   
   
 
 
   
    
   
   
   
 
 
   
   
   
    
   
   
 
   
    
   
   
   
   
 
   
   
   
    
   
 
table    parameters of the mixture model for the data set in fig    

tiz black
 
 
 
 
   
   
 

    modeling arbitrary information about instances
the available information about a particular instance z is denoted by s  following the likelihood
principle  we are not concerned with the true nature of s  whether it is some kind of physical meas 

for this reason  in pattern classification tasks separate models are typically built for each class conditional density 

   

firuiz  lpez de teruel   garrido

urement or a more subjective judgment about the location of z in the attribute space  all we need to
update our knowledge about z  in the form of the posterior p z s   is the relative likelihood function
p s z  of the observed s  in general  p s z  can be any nonnegative multivariable function fs z  over
the domain  in the objective case  statistical studies of the measurement process can be used to
determine the likelihood function  in the subjective case  it may be obtained by standard distribution elicitation techniques  berger        in either case  under the mfgn framework  the likelihood function of the available information to be used in the inference process will be approximated  up to the desired degree of accuracy  by a sum of products of generalized normals 

p  s  z      p s   s r   p  s r   z      p s   s r   p  srj   z j  
r

r

 

   t  z
r

j

j

  srj    rj  

   

j

r

without loss of generality  the available knowledge is structured as a weighted disjunction s  
 
 
  s  or  s      or  r s r   of conjunctions sr     sr and sr     and srn   of elementary uncertain
observations in the form of generalized normal likelihoods t  z j   srj    rj   centered at srj with
uncertainty  rj   the measurement process can be interpreted as the result of r  objective or subjective  sensors sr   providing conditionally independent information p  srj   z j   about the attributes
 each srj only depends on z j   with relative strength  r   note that any complex uncertain information about an instance z  expressed as a nested combination of elementary uncertain beliefs srj
about z j using probabilistic connectives  can be ultimately expressed by structure      or translates to addition  and translates to product and the product of two generalized normals over the
same attribute becomes a single  weighted normal  
example    consider the hypothetical computer vision domain in example    assume that the
information about an object z is the following  area is around a and distance is around b or 
more likely  shape is surely triangular or else circular and area is around c and angle is
around d or equal to e  this structured piece of information can be formalized as 

t z   a     t z   b     
          t z  triang    t z  circ   t z   c      t z   d     t z  e    

p s z        

 

 

a

 

b

 

 

 

c

 

d

which  expanded  becomes the mixture of   factorized components operationally represented by
the parameters shown in table   
in a simpler situation  the available information about z could be a conjunction of uncertain attributes similar to  color   red     green      and  area          and  shape   rectangular    
circular     triangular       the likelihood of shape values can be obtained from the output of a
simple pattern classifier  e g  k n nearest neighbors  over moment invariants  while attributes
as color and area are directly extracted from the image  in this case we could be interested in
the distribution of values for other attributes as texture and objecttype  alternatively  we
could start from  objecttype   door     window      and  texture   rough  in order to determine the probabilities of color and angle values for selecting a promising search region 

   

fiprobabilistic inference from uncertain data using mixtures

r

sr     r

sr     r 

sr     r

sr     r

sr     r

sr     r

sr     r

   
a  a
   
b  b
   
   
   
   
triang   
c  c
   
   
   
   
   
triang   
c  c
   
   
   
   
   
circ   
c  c
   
   
   
   
   
circ
 
 
c  c
   
   
   
   
table    parameters of the uncertain information model in example   

   
d d
e   
d d
e   

    the joint model observation density
the generic dependence structure in fig    is implemented by the mfgn framework as shown in
fig     the upper section of the figure is the model of nature  obtained in a previous learning stage
and used for inference without further changes  dependences among attributes are conducted
through an intermediary hidden or latent component ci  the lower section represents the available
uncertain information  measurement model or query structure associated to each particular inference operation 

ci

p  z     ci  
 

domain 

p  z    p ci   p  z j   ci  
 

z

z

s  

s  

   

zn

   

s n

j

i

p  s     z   
s  

s  

   

s n

s 

   

s r

sr 

   

srn

sr

s 
p s s  

measurement 
p  s  z     p s   s r   p  srj   z j  

s

r

j

figure    structure of the mfgn model  the attributes are conditionally independent 
the measurement process is modeled by a collection of independent virtual sensors
p  srj   z j    

the joint density of the relevant variables becomes 

p ci   z   sr   s     p s   s r   p  s r   z   p  z  ci   p ci  

 p  s

  p ci   p s   s r  

j
r

  z j   p  z j   ci  

j

  pi  r

 t  z

j

  srj    rj   t  z j    ij    ij  

j

   

    

firuiz  lpez de teruel   garrido

now we will derive an alternative expression for eq       which is more convenient for computing the marginal densities of any desired variable  using the following relation 

p  srj   z j   p  z j   ci     p  z j   srj   ci     p  z j   srj   ci   p  srj   ci  
and properties     and      we define the dual densities of the model 

 ij r  p  srj   ci    



zj

p  srj   z j   p  z j   ci   dz j  t  srj    ij    ij r  

    

p  srj   z j  
p  z j   ci     t  z j   ij r    ji  r  
p  srj   ci  

    

 ij r   z j    p  z j   srj   ci    

where the parameters ij r   ij r and  ji  r are given by 

 ij r    ij         rj    
   ij     srj      rj      ij
 
 ij r    
j
i  r



j
i  r

 ij  rj
 j
i  r

ij r is the likelihood of the r th elementary observation srj of the j th attribute z j in each
component ci and  ij r   z j   is the effect of the r th elementary information srj about the j th attribute z j over the marginal component p  z j   ci   in each component ci   using the above notation  the mfgn model structure can be conveniently written as 

p ci   z   sr   s     pi  r  ij r  ij r   z j  

    

j

    the posterior density
in the inference process the available information is combined with the model of the domain to
update our knowledge about a particular object  given a new piece of information s we must compute the posterior distribution p  y  s   of the desired target attributes y  z  then  estimators
y   s    y can be obtained from p  y  s   to minimize any suitable average loss function  this is
efficiently supported under the mfgn framework regardless of the complexity of the domain p z 
and the structure of the available information s     r sr    
the attributes are partitioned into two subvectors z    x  y   where y     z d   are the desired
target attributes and x     z o   are the rest of attributes  accordingly  each component sr of the
available information s is partitioned as s r     s rx   s ry     the information about the target attributes
y in the r th observation  independent from the model p z   is denoted by sry  often y is just missing
and there are no such pieces of information  and srx represents the information about the rest of
attributes x  using this convention we can write 

   

fiprobabilistic inference from uncertain data using mixtures

p  z   s     p  x   y   s      pi  r i  r i  r   x   i  r   y  
i  r

where i  r is the likelihood of the r th conjunction sr of s in component ci  

i  r   ij r

    

j

and the terms  ij r   z j   are grouped according to the partition of z    x  y  

i  r   x      io r   z o  

i  r   y      id r   z d  

o

d

the desired posterior p y s    p y s    p s  can be computed from the joint p z s  by marginalization  along x to obtain p y s  and along all z to obtain p s   note that each univariate marginalization of p z s  along attribute z j eliminates all the terms  ij r   z j   in the sum      

p  y   s      p  x   y   s   dx    pi  r i  r i  r   y  
x

i  r

p  s      p  z   s   dz    pi  r i  r
z

i  r

therefore  the posterior density can be compactly written as 

p  y  s       i  r i  r   y  

    

i  r

where  i  r is the probability that the object z has been generated by component ci and the elementary information sr is true  given the total information s 

 i  r  p ci   sr   s   

pi  r i  r

p

k

 l  k  l

    

k  l

and i  r   y     p  y  sry   ci   is the marginal density p  y  ci   of the desired attributes in the i th
component  modified by the contribution of all the associated sry   since p  y  sry   ci    
p  y  sr   ci     the expression      also follows from the expansion 

p  y  s      p  y  sr   ci   p ci   sr   s 
i  r

in summary  when the joint density and the likelihood function are approximated by mixture
models with the proposed structure  the computation of conditional densities given events of arbitrary geometry is notably simplified  factorized components reduce multidimensional integration
to simple combination of univariate integrals and conjugate families avoid numeric integration 
this property is illustrated in fig    

   

firuiz  lpez de teruel   garrido

s

y

p y c  

 y

e y s 

c 

p y x  x  

  y

c 

p y c  
p x  c  

p y s 
p x  c  

         
 

p x  c  

    

x 

p x  c  

    

    

s 
s 

x

 

s

figure    graphical illustration of the essential property of the mfgn framework  consider the mse estimate for y  conditioned to the event that  y  x   x   is in the cylindrical
region s  the required multidimensional integrations are computed analytically in terms
of the marginal likelihoods ji r associated to each attribute and each pair of components
ci and sr of the models for p y  x   x   and for s  respectively  in this case i r y  p y ci 
because no information about y is supplied in s 

example    fig     a shows the joint density of two continuous variables x and y  it is modeled
as a mixture with    factorized generalized normals  fig     b shows the likelihood function
of the event s      x  y or x   y  and y     fig     c shows the posterior joint density
p x y s    fig     d shows the likelihood function of the event s      x y         or x   
fig     e shows the posterior joint density p x y s    fig     f and    g show respectively the
posterior marginal density p x s   and p y s    these complex inferences are analytically computed under the mfgn framework  without any numeric integration 

   

fiprobabilistic inference from uncertain data using mixtures

 a 

 b 

 c 

 d 
 e 
figure     illustrative examples of probabilistic inference from arbitrary uncertain information in the mfgn framework  see example    

   

firuiz  lpez de teruel   garrido

p y s 

   

   

   

   

   

   

   

p x s 

   

y

x

  

  

 

 

  

  

 

 f 
figure      cont   

 

 g 

    expressions for the estimators
approximations to the optimum estimators can be easily obtained by taking advantage of the
mathematically convenient structure of the posterior density  under the mfgn framework  the
conditional expected value of any function g y  becomes a linear combination of constants 

e g   y    s     g   y   p  y  s   dy  
y

 

 
i  r

y

i  r

g   y   i  r   y   dy  



i  r

e i  r  g   y   

    

i  r

where e i  r  g   y     e g   y    s ry   ci   is the expected value of g y  in the i th component  modified  by the r th observation sry  

e i  r  g   y    



y

g   y    t  z d    id r   di  r   dy
d

we can now analytically compute the desired optimum estimators  for instance  the mse estimator for a single continuous attribute y   z d requires the mean values e i  r  z d      id r  

y mse   s     e  y  s      i  r  id r
i  r

from our explicit expression for p y s  we can also compute the conditional cost 

 

 

e  mse   s     e   y  y mse   s       s   e y     s   y  mse   s    
 


i  r

i  r

 

   

d
i  r

       
 

d
i  r

 

 

 



    i  r  id r 
 i  r


 

note that computing the conditional expected value of an arbitrary function g y  of several variables may be difficult  in
general g y  can be expanded as a power series to obtain e g y  s  in terms of moments of p y s  
 
when s is just sx  there is no information about the target attributes  the constants ei r g y   can be precomputed from
the model of nature p z  after the learning stage 

   

fiprobabilistic inference from uncertain data using mixtures

therefore  given s  from tchevichev inequality we can answer y  y mse   s     e mse   s  
with a confidence level above      when the shape of p y s  is complex it must be reported explicitly  the point estimator y  y mse   s   only makes sense if p y s  is unimodal  
example     nonlinear regression  fig     shows the mixture components and regression
lines  with a confidence band of two standard deviations  obtained in a simple example of
nonlinear dependence between two variables  in this case the joint density can be adequately
approximated by   or   components  mse    component           mse    comp           
mse    comp            mse    comp           

 a 
 b 
figure     nonlinear regression example   a    components   b    components 

when the target y is symbolic we must compute the posterior probability of each value  in this
case all the di  r     and the id r    id are the possible values  taken by y   z d   collecting together all the id r     as in      eq       can be written as 

p   y  s        i  r   t   y     


i  r

where  i  r   are the coefficients of the impulses located at   the posterior probability of each
value is 

q   p  y     s      i  r  
i  r

for instance  the minimum error probability estimator  ep  is 

y ep   s     argmax



q

and any desired rejection threshold can be easily established  we can reject the decision if the entropy of the posterior  h    q log q  or the estimated error probability  e      max q  are too
high 
example     nonparametric pattern recognition  fig     shows a bivariate data set with elements from two different categories  represented as the value of an additional symbolic attribute  the joint density can be satisfactorily approximated by a   component mixture  fig 
   a   the decision regions when the rejection threshold was set to     are shown in fig     b 

   

firuiz  lpez de teruel   garrido

note that statistical pattern classification usually start from an  implicit or explicit  approximation to the class conditional densities  in contrast  we start from the joint density  from
which the class conditional densities can be easily derived  fig     c  

 a 
 b 
 c 
figure     simple nonparametric  feature pattern recognition task and its  attribute
joint mixture model   a  feature space and mixture components   b  decision boundary 
 c  one of the class conditional densities 

the computation of the optimum estimators for other loss functions is straightforward  observe that the estimators are based on the combination of different rules  weighted by their degree
of applicability  this is a typical structure used by many other decision methods  in our case  since
the components of the joint density have independent variables the rules reduce to constants  the
simplest type of rule 
    examples of elementary pieces of information
some important types of elementary observations srj about z j are shown  including the corresponding likelihoods ij r and modified marginals  ij r   z j    j d  required in expression      
exact information  srj   z j   the observation is modeled by an impulse 

p  srj   z j    

t  srj   z j       srj  z j     therefore 
 ij r   t  srj    ij    ij  
 ij r   z j     t  z j   srj  
the contribution ij r of exact information about the input attribute z j is the standard likelihood p  z j   ci   of the observed value z j in each component  on the other hand  if we acquire
exact information about a target attribute z j  when there is only one  r    elementary observation
and s j   z j   then the inference process is trivially not required  p  z j   s       z j  s j    
gaussian noise with bias rj and standard deviation  rj   the observation is modeled by a  component mixture  p  srj   z j     t  srj   z j   rj    rj     which can also be expressed as a     confidence interval z j  srj   rj    rj   from property       

   

fiprobabilistic inference from uncertain data using mixtures

 ij r   t  srj    ij  rj     ij          rj      
the effect of a noisy input z j  srj    rj is equivalent to the effect of an exact input z j   srj
in a mixture with components of larger variance   ij    ij          rj       uncertainty spreads the
effect of the observation  increasing the contribution of distant components 
example     fig     a shows a simple two attribute domain approximated by a   component
mixture  we are interested in the marginal density of attribute x given different degrees of uncertainty  in the input attribute y         modeled by

p  s y   y     t  s y           if     

we have the sharpest density  a  in fig     b  providing x      if        we obtain density
 b  and x      finally  if       we obtain density  c  and x      obviously  as the uncertainty in y increases  so does the uncertainty in x  the expected value of x moves towards
more distant components  which become more likely as the probability distribution of y expands  in this situation an interesting effect appears  the mode of the marginal density does not
change at the same rate than the mean  uncertainty in y skews p x   this effect suggests that
the optimum estimators for different loss functions are not equally robust against uncertainty 

a
b
c

 a 
 b 
figure     effect of the amount of uncertainty  see text    a  data set and   component
model   b  p x   uncertain ys around      

j
j
for the output role   i  r   z   becomes the original marginal  modified in location and disper 
 
 
sion towards srj according to the factor      ij        ij        rj       which quantifies the relative
importance of the observation 

 ij r   z j     t z j      srj   rj            ij          rj  
missing data  when there is no information about the j th attribute  srj    z j        the observation can be modeled by p  srj   z j     constant or  equivalently  p  srj   z j     t  srj   a   b  with a
arbitrary and b    all the components contribute with the same weight 

 ij r   p  z j   anything  ci    constant   
if the target is missing the  ij r   z j   reduce to the original marginal components 

   

firuiz  lpez de teruel   garrido

 ij r   z j     t  z j    ij    ij     p  z j   ci  
arbitrary uncertainty  in general  any unidimensional relative likelihood function can be approximated by a mixture of generalized normals  as shown in example    where ij r and  ij r   z j  
are given respectively by eqs       and      
intervals  some useful functions cannot be accurately approximated by a small number of normal
components  a typical example is the indicator function of an interval  used to model an uncertain
observation where all the values are equally likely  srj    z j   a  b     if z j is only considered as
input  we can use the shortcut  ij r   fi j  b   fi j  a     where fi j   z j   is the cumulative distribution of the normal marginal component p  z j   ci     unfortunately  the expression for  ij r   z j    
required for z j considered as output  may not be so useful for computing certain optimum estimaj
j
j
tors   i  r   z   is the restriction of p z j ci  to the interval  a b  and normalized by i  r  
disjunction and conjunction of events  finally  standard probability rules are used to build
structured information from simple observations  if from subjective judgments or objective evidence we ascribe relative degrees of credibility  rj to several observations srj about z j   the overall
j
j j
likelihood becomes  i    r  r  i  r   in particular  if s j    z j      or z j        and the two

possibilities are equiprobable then  ij   p      ci     p      ci     analogously  conjunctions of
events translate to multiplication of likelihood functions 
    summary of the inference procedure
once the domain p z  has been adequately modeled in the learning process  as explained in section
    the system enters the inference stage over new  partially specified objects  from the parameters
of the domain p z    pi    ij and  ij   and the parameters of the model of the observation p s z 
   r   srj and  rj    we must obtain the parameters  ij r    id r and di  r of the desired marginal posterior densities and estimators  the inference procedure comprises the following steps 


compute the elementary likelihoods ij r   using eq       



obtain the product i  r for each conjunction sr and component ci   using eq       



normalize pi  r  i  r to obtain the coefficients  i  r of the posterior  using eq       



choose the desired target attributes y     z d   and compute the parameters id r   and

di  r of the modified component marginal densities  id r   z d   using eq       


report the joint posterior density of y  show graphs of the posterior marginal densities
of the desired attributes z d using eq        provide optimum  point  interval  etc   estimators using eq       

example     iris data  the inference procedure is illustrated over the well known iris benchmark      objects represented by four numeric features  x  y  z and w  and one symbolic category u   u   setosa   u   versicolor   u   virginica    the whole data set was divided into
two disjoints subsets for training and validation  the joint density can be satisfactorily approxi 

   

fiprobabilistic inference from uncertain data using mixtures

mated  see section    by a  component mixture  the error rate classifying u in the validation
set without rejection was         fig     shows two projections of the     examples and the location of the mixture components learned from the training subset  the parameters of the mixture are shown in table   

 a 
 b 
figure     two views of the iris examples and the components of the joint density mixture model  u   white  u   black  u   gray   a  attributes x  y  b  attributes z  w 

i

pi

 
 
 
 
 
 

    
    
    
    
    
    

 ix

 ix

 iy

 iy

 iz

 iz

                             
                             
                             
                             
                             
                             
table    parameters of the iris data joint density model

 iw

 iw

    
    
    
    
    
    

    
    
    
    
    
    

p u  ci  p u  ci  p u  ci 

 
 
 
 
 
 

 
    
 
 
 
    

table   shows the results of the inference process in the following illustrative situations 
case    attribute z is known  s    z      
case    attributes x and u are known  s     x        and  u u    
case    attribute x is uncertain  s    x      
case    attributes x and w are uncertain  s     x      and  w          note that uncertainty decreases when more information is supplied  compare with case    
case    structured query expressed in terms of logical connectives over uncertain elementary
events  s      z      or  z       and   u   u   or  u   u     

   

 
    
    
 
 
 

firuiz  lpez de teruel   garrido

case
 
 
 
 
 

input
output
input
output
input
output
input
output
input
or

output

x
 
      
   
   
  
      
  
      
 
 

y
 
      
 
      
 

z
   
   
 
      
 

w
 
      
 
      
 

      
 

      
 

      
 
 

      
    
      
 
 

      

      

      
  
  
  

 approx 
unimodal 

 unimodal 

 bimodal 

    

u
 

u       u      
u       
u       
 
u       u      
 
u       u     
u       u      
u       u      
u       u      

 bimodal 

table    some inference results over the iris domain

the consistency of the results can be visually checked in fig      finally  table   shows the
elementary likelihoods

i
 
 
 
 
 
 

ix  

iy  

i j r of case    illustrating the essence of the method 

iz  

iw  

ui   

i   

ix  

 
 
 
 
 
    
 
 
 
 
 
    
       
 
 
 
 
    
       
 
 
 
 
    
       
 
 
 
 
    
       
 
 
 
 
 
    
 
table    elementary likelihoods in case   from table   

iy  
 
 
 
 
 
 

iz  

iw  

ui   

i   

    
    
    
 e  
 e  
    

 
 
 
 
 
 

 
   
   
   
   
 

 
   
   
   
   
 

    independent measurements
one of the key features of the mfgn framework is the ability to infer over arbitrary relational
knowledge about the attributes  in the form of a likelihood function adequately approximated by a
mixture model with the structure of eq       for instance  we could answer questions as  what happens to z d when z i tends to be less than z j    i e   when p s z  is high in the region z i  z j       
however  there are situations where the observations over each single attribute z j are statistically
independent  we have information about attributes  e g  z i is around a and z j is around b  but not
about attribute relations  we will pay attention to this particular case because it illustrates the role
of the main mfgn framework elements  furthermore  many practical applications can be satisfactorily solved under the assumption of independent measurements or judgments  in this case  the
likelihood of the available information can be expressed as the conjunction of n marginal observations s j about z j  

p   s   z      p  s j   z j  
j

   

    

fiprobabilistic inference from uncertain data using mixtures

this means that the sum of products in equation     is complete  i e   it includes all the elements
in the n fold cartesian product of attributes 

p  s   z        rj t  z j   srj    rj  
j

r

where  j  rj    r   this factored likelihood function can be considered also as a   component
mixture  with r   in     and s j  s j   where the marginal observation models are allowed to be
mixtures of generalized normals  p  s j   z j      r    rj  t  z j   srj     rj       in this case we can even think
 
 
of function valued attributes z    f   z        f n   z n      where f j   z j    p  s j   z j   models the

range and relative likelihood of the values of z j   loosely speaking  attributes with concentrated
f j   z j   may be considered as inputs  and attributes with high dispersion play the role of outputs 
since y is conditionally independent of s x given ci   the posterior can be obtained from the expansion 

p  y  s      p  y  s   ci   p ci   s     p  y  ci   s y   p ci   s 
i

    

i

the interpretation of      is straightforward  the effect of sx over y    zd  must be computed
through x    zo  and the components ci   then  a simple bayesian update of p  y  s x   as a new
prior is made using s y  see fig      

ci
z 

i

p  z j   ci  

zd
 id   z d  

s 

   


   

sd

j
i

zj
p  s j   z j  

sj

   

figure     structure of the mfgn inference process from independent pieces of information  in this case  the likelihood function is also factorizable  the data flow in the inference process is shown by dotted arrows 

   

firuiz  lpez de teruel   garrido

   learning from uncertain information
in the previous section  we have described the inference process from uncertain information under
the mfgn framework  now we will develop a learning algorithm for the model of the domain 
where the training examples will be also uncertain  specifically  we must find the parameters pi  

 ij    ij  or ti j    of a mixture with structure     to approximate the true joint density p z  from a
training i i d  random sample  z k    k    m  partially known through the associated likelihood
functions  s k   with structure     
    overview of the em algorithm
maximum likelihood estimates for the parameters of mixture models are usually computed by the
well known expectation maximization  em  algorithm  dempster  laird and rubin       redner
and walker       tanner        based in the following idea  in principle  the maximization of the
training sample likelihood j    k p  z   k     is a mathematically complex task due to the product of
sums structure  however  note that j could be conveniently expressed for maximization if the components that generated each example were known  this is called complete data in em terminology   the underlying credit assignment problem disappears and the estimation task reduces to several uncoupled simple maximizations  the key idea of em is the following  instead of maximizing
the complete data likelihood  which is unknown   we can iteratively maximize its expected value
given the training sample and the current mixture parameters  it can be shown that this process
eventually achieves a local maximum of j 
instead of a rigorous derivation of the em algorithm  to be found in the references  see especially mclachlan and krishnan         we will present here a more heuristic justification which
provides insight for generalizing the em algorithm to accept uncertain examples  we will review
first the simplest case  where no missing or uncertain values are allowed in the training set  the
parameters of the mixture are conditional expectations 

e z  ci  g   z    ci       g   z   p  z  ci   dz
z

    

 
 
in particular   ij   e z j   ci       ij     e   z j   ij     ci   and ti j    e i  z j      ci     the

mixture proportions are pi   e  p ci   z     
we rewrite the conditional expectation      using bayes theorem in the form of an unconditional expectation 

e z  ci  g   z    ci       g   z   p ci   z  p  z     p ci   dz  

    

  e z  g   z   p ci   z     pi

    

z

the em algorithm can be interpreted as a method to iteratively update the mixture parameters
using expression      in the form of an empirical average over the training data   starting from a
tentative  randomly chosen set of parameters  the following e and m steps are repeated until the
 

expression      can be also used for iterative approximation of explicit functions which are not indirectly known by
i i d  sampling  e g   subjective likelihood functions sketched by the human user  as in example     in this case p z  is set
to the target function and p ci  z  is computed from the current mixture model 

   

fiprobabilistic inference from uncertain data using mixtures

total likelihood j no longer improves  the notation  expression  k  means that  expression  is computed with the parameters of example z k   
   
   
 e  expectation step  compute the probabilities qi k  p ci   z k   that the k th example has been
generated by the i th component of the mixture 

qi  k    p  z   k     ci   p ci     p  z   k    
 m  maximization step  update the parameters of each component using all the examples  weighted
by their probabilities qi  k     first  the a priori probabilities of each component 

pi 

 
 q  k  
m k i

then  for continuous variables  the mean values and standard deviations in each component 

 
mpi

 ij 
  ij     

 
mpi

  q

z j    k  

i

k

  q

  z j        k      ij    

i

    

k

and for symbolic variables  the probabilities of each value 

ti j  

 
mpi

  q

i  z j       k  

i

k

    extension to uncertain values
in general  in the mfgn framework we do not know the true values z j of the attributes in the
training examples  required to compute g   z   p ci   z  in the  empirical  expectation       instead 
we will start from uncertain observations s   k   about the true training examples z
likelihood functions expressed as mixtures of generalized normals 

 k  

  in the form of

p  s   k   z   k        p s   k   sr  k     p  sr  k   z   k    
r

therefore  we must express the expectation      over p z  as an unconditional expectation
over p s   the distribution which generates the available information about the training set  this
can be easily done by expanding p  z  ci   in terms of s 

e z  ci  g   z    ci       g   z   p  z  ci   dz
z

   p   z   s   c   p   s   c   ds   dz
     g   z   p  z  s   c   dz   p c   s  p  s   ds   p c  
 

s

z



z

g   z 

i

s

i

i

i

if we define

   

i

    

firuiz  lpez de teruel   garrido

i   s    e z  s  ci  g   z    s   ci       g   z   p  z  ci   p  s   z   dz   p  s   ci  
z

then the parameters of p z  can be finally written  as an unconditional expectation over the observable p s  in a form similar to eq       

e z  ci  g   z    ci      e s  i   s   p ci   s     pi

    

this expression justifies an extended form of the em algorithm to iteratively update the parameters of p z  by averaging i   s   p ci   s  over the available training information   s   k    
drawn from p s   this can be considered as a numerical statistical method for solving p z  in the
integral equation 



z

p   s   z   p   z   dz   p   s  

note that we cannot approximate p s  as a fixed mixture in terms of p  s   ci   and then computing back the corresponding p  z  ci   because  in general  p  s   z   will be different for the different training examples  for the same reason  elementary deconvolution methods are not directly
applicable 
this kind of problem is addressed by vapnik              to perform inference from the result of
indirect measurements  this is an ill posed problem  requiring regularization techniques  the
proposed extended em algorithm can be considered as a method for empirical regularization  in
which the solution is restricted to be in the family of mixtures of  generalized  gaussians  em is
also proposed by you and kaveh        for regularization in the context of image restoration 
the interpretation of      is straightforward  since we do not know the exact z required to
approximate the parameters of p z  by empirically averaging g   z   p ci   z    we obtain the same
result by averaging the corresponding i   s   p ci   s  in the s domain  where i   s   plays the
role of g z  in       as z is uncertain  g z  is replaced by its expected value in each component
given the information about s  in particular  if there is exact knowledge about the training set at   
tributes   s   k     z k   i e   r     and the marginal likelihoods are impulses  then      reduces to
      fig      illustrates the approximation process performed by the extended version of the em
algorithm in a simple univariate situation 
it is convenient to develop a version of the proposed extended em algorithm for uncertain
training sets  structured as tables of  sub cases   uncertainly valued  variables  see fig      
first  let us write eq       expanding s in terms of its components sr 

p  z  ci   s   p ci   s     p  z   ci   s  
 

 p  z  c   s   p s   s     p  z  c   s   p c   s   p s   s 
i

r

r

r

i

r

i

r

r

r

therefore

i   s   p ci   s     i  r   s r   p ci   s r   s 
r

this result can be also obtained from the relation ez w z     es  ez s w z   s    for w z   g z  p ci z  and bayes theorem 

 

   

fiprobabilistic inference from uncertain data using mixtures

 a 

 b 
 d 

 c 

figure     the extended em algorithm iteratively reduces the  large  difference between
 a  the true density p z   and  b  the mixture model p   z     indirectly through the  small 
discrepancies between  c  the true observation density p s  and  d  the modeled observation density p   s     in real cases p s  must be estimated from a finite i i d  sample
 s k   

s   

s    
s    
s   
s    
s    
s    
   

  
  
 
  
  
  
   

s   
s   

  r k  

s r 

  sr      r     k  

  srj    rj     k  

   

   

   
   
figure     structure of the uncertain training information for the extended em algorithm  the coefficients

  r k   are normalized for easy detection of the rows included in

each uncertain example  when z



 k  

    and all the

 k 

 k 

is not uncertain  s

reduces to a single row with

     
j

using the notation introduced in      

i  r   s r    e z  sr  ci  g   z    s r   ci    



z

g   z   p  z  s r   ci   dz  



z

p ci   s r   s    p ci   s r   p s r   s     i  r
we can write      as 

   

g   z    ij r   z j   dz
j

firuiz  lpez de teruel   garrido



e z ci  g  z     ci     e s   i  r  g  z    ij r   z j  dz   pi
z
j
 r

in the mfgn framework the contributions i  r   s r   p ci   s r   s  to the empirical expected
values required by the extended em algorithm can be obtained again without numeric integration 
we only need to consider the case g z    z j to compute the means  ij and probabilities ti j    and
g z      z j    for the deviations  ij   from      we already know an explicit expression for the parameters of  ij r   z j     t  z j    ij r    ji  r     hence 



z

z j i  r   z   dz  

  z
z

j



z

z j  ij r   z j  dz j    ij r

    i  r   z   dz      ij r          ji  r    

in conclusion  the steps of the extended em algorithm are as follows 
 e  expectation step  compute all the elementary likelihoods of the training set 

 

 ij r  k     t srj    ij     ij          rj    

 

 k  

    

   
   
obtain the likelihood of each conjunction sr k of example s k in component ci 

 i  kr      ij r  k  
j

   
obtain the total likelihood of example s k  

   k    p  s   k         pi  r  k    i  kr  
i

r

   
   
   
compute the probabilities qi  kr  p ci   s r k   s k   that the r th component of the k th exam 

ple has been generated by the i th component of the mixture 

qi  kr     i  kr    pi  r   k    i  kr       k  
 m  maximization step  update the parameters of each component ci using all the components

s r  k   of all the examples weighted with their probabilities qi  kr     first  the prior probabilities of each
component 

pi 

 
m

 q
k

 k  
i  r

r

then the mean value and standard deviation in each component 

 ij 

 
mpi

   q
k

r

   

i  r

ij r

 

 k  

fiprobabilistic inference from uncertain data using mixtures

  ij     

 
mpi

   q
k

i  r

 

   ij r         ij  r      

r

 k  

   ij    

    

for symbolic variables under representation     we may use 

ij r  k      p srj      k   ti j 

    



ti j  

 
mpi

   q
k

i  r

p srj     t i j     ij r

r

 

 k  

    

consider the particular case in which the attributes in the training examples are contaminated
with unbiased gaussian noise  the likelihood of the uncertain observations is modeled by     
   
   
   
   
     
component mixtures  p  s k   z k      j t  z j k   s j k    j k     where   j k   is the variance of
the measurement process over z j   k   which obtains the observed value s j   k     this can be also ex   
   
   
pressed as a confidence interval z j k  s j k    j k   in this case  the basic em algorithm     
   
can be easily modified to take into account the effect of the uncertainties  j k   in the e step  com   
pute qi k using the following deviations 

 ij    ij          j   k      
and  in the m step  apply the substitution 

z j   k     s j   k             ij

 

  z j   k         s j   k            ij

        
 

j  k    

where

  ij    
  j  
  i        j   k      
measures the relative importance of the observed s j k for computing the new  ij and  ij  
the previous situation illustrates how missing values must be processed in the learning stage 
   
j  k  
if z
is exact then  j k     and       so the original algorithm      is not changed  in the other
   
extreme  if z j   k   is missing  which can be modeled by  j k    we get      and therefore the
   
observation s j k does not contribute to the new parameters at all  the correct procedure to deal
with missing values in the mfgn framework is simply omitting them in the empirical averages 
note that this fact arises from the factorized structure of the mixture components  providing conditionally independent attributes  alternative learning methods require a careful management of
missing data to avoid biased estimators  ghahramani   jordan       
   

    evaluation of the extended em algorithm
we have studied the improvement of the parameter estimations when the uncertainty of the observations  modeled by likelihood functions  is explicitly taken into account  the proposed extended

   

firuiz  lpez de teruel   garrido

em is compared with the em algorithm over the  raw  observations  basic em   which ignores the
likelihood function and typically uses just its average value  e g   given x    basic em uses
x     we considered a synthetic   attribute domain with the following joint density 
p x y w         x       y       w white 
       x       y       w black 

tt

tt

tt

different learning experiments were performed with varying degrees of uncertainty  in all
cases the training sample size was      all trained models had the same structure as the true density    components   since the goal of this experiment is to measure the quality of the estimation
with respect to the amount of uncertainty  without regard of other sources of variability such as
local minima  alternative solutions  etc   which are empirically studied in section    table   shows
the mixture parameters obtained by the learning algorithms  fig     graphically shows the difference between extended and basic em in some illustrative cases 
case    exact data  fig     a  
cases m    results of the extended em learning algorithm when there is a m   rate of missing
values in the training data 
case    basic em when attribute y is biased    units with probability      case    extended em
algorithm over case    see fig     b   here  the observed value is sy y   in     of the samples
and sy y in the rest  in all samples  basic em uses the observed value sy and extended em uses
the explicit likelihood function f y         ysy         y sy    
case    basic em when attributes x and y have gaussian noise with        and w is changed with
probability      case    extended em algorithm over case   
case    basic em when x and y have gaussian noise with      and w is changed with probability
     case    extended em algorithm over case    see fig     c  
case    basic em when x and y have gaussian noise with      and w is changed with probability
     case    extended em algorithm over case    see fig     d  

t

case    extended em when values y   are missing  censoring   case     extended em over case
  when the missing y values are assumed to be distributed as
 y         providing some additional information on the data generation mechanism 
table   and fig     confirm that for small amounts of deterioration in relation to the sample
size  the estimates computed by the basic em algorithm over the raw observed data are similar
to those obtained by the extended em algorithm  e g   cases   and     however  when the data sets
are moderately deteriorated the true joint density can be correctly recovered by extended em using
the likelihood functions of the attributes instead of the raw observed data  e g   cases   and    fig 
   c   finally  when there is a very large amount of uncertainty with respect to the training sample
size the true joint density cannot be adequately recovered  e g   cases   and    fig     d  

   

fiprobabilistic inference from uncertain data using mixtures

 a 

 b 

 c 
 d 
figure     illustration of the advantages of the extended em algorithm  see text    a 
case    exact data    b  cases   and    biased data    c  cases   and    moderated noise  
 d  cases   and    large noise   all figures show the true mixture components  gray ellipses   the available raw observations  black and white squares   the components estimated by basic em from the raw observations  dotted ellipses  and the components estimated by extended em taking into account the likelihood functions of the uncertain
values  black ellipses  

note that the ability to learn from uncertain information suggests a method to manage non
random missing attributes  e g   censoring   ghahramani   jordan       and other complex
mechanisms of uncertain data generation  as illustrated in case    if the missing data generation
mechanism depends on the value of the hidden attribute  it is not correct to assign equal likelihood
to all components  in principle  statistical studies or other kind of knowledge may help to ascertain
the likelihood of the true values as a function of the available observations  for instance  in case   
we replaced the missing attributes of case   by normal likelihoods y      i e  y is high   improving the estimates of the mixture parameters 

   

firuiz  lpez de teruel   garrido

case

p 

 x

 y

 x

 y

t  wwhite

  x

  y

true
  
 
 
 
 
 
 
 
 
                                     
   
                                     
   
                                    
   
                                    
   
                                     
 
       
                            
 
                                    
 
       
                           
 
                     
             
 
                                     
 
                                     
 
                                    
 
       
                           
 
                                     
  
       
                            
table    parameter estimates from uncertain information  see text 

  x

  y

t  wblack

 
    
   
   
    
    
   
    
    
    
    
   
    
    
    
    

 
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

 
    
    
    
    
    
    
    
    
    
   
   
   
   
    
    

example     learning from examples with missing attributes has been performed over the iris
domain to illustrate the behavior of the mfgn framework  the whole data set was randomly
divided into two subsets of equal size for training and testing    component mixture models
were obtained and evaluated  combining missing data proportions of    and      the error
prediction on attribute u  plant class  was the following 

missing attributes

training set
  
  
   
   

test set
  
   
  
   

prediction error
    
     
    
     

in the relatively simple iris domain  the performance degradation due to     missing attributes is much greater in inference than in learning stage  the extended em algorithm is able
to correctly recover the overall structure of the domain from the available information 

    comments
convergence of the em algorithm is very fast  requiring no adjustable parameters such as learning
rates  the algorithm is robust with respect to the random initial mixture parameters  bad local
maxima are not frequent and alternative solutions are usually equally acceptable  all the examples
contribute to all the components  which are never wasted by unfortunate initialization  for a fixed
number of components  the algorithm progressively increases the likelihood j of the training data
until a maximum is reached  when the number of components is incremented the maximum j also
increases  until a limit value is obtained that cannot be improved using extra components  fukunaga        some simple heuristics can be incorporated to the standard expectation maximization
scheme to control the value of certain parameters  e g   lower bounds can be established for variances  or the quality of the model  e g   mixture components can be eliminated if their proportions
are too small  
in our case  factorized components are specially convenient because matrix inversions are not
required and  what is more important  uncertain and missing values can be correctly handled in a

   

fiprobabilistic inference from uncertain data using mixtures

simple and unified way  for heterogeneous attribute sets  it is not necessary to provide models for
uncertain attribute correlations since no covariance parameters must be estimated  finally  the
training sample size must be large enough in relation to both the degree of uncertainty of the examples and the complexity of the joint density model in order to obtain satisfactory approximations 
on the other hand  the number of mixture components required for a satisfactory approximation to the joint density must be specified  a pragmatic option is the minimization of the experimental estimation cost over the main inference task  if it exists  for instance  in regression we
could increase the number of components until an acceptable estimation error is obtained over an
independent data set  cross validation   the same idea applies to pattern classification  use the
number of components that minimizes the error rate over an independent test set  however  one of
the main advantages of the proposed method is the independence between the learning stage and
the inference stage  where we can freely choose and dynamically modify the input and output role
of the attributes  therefore  a global validity criterion is desirable  some typical validation methods
for mixture models are reviewed in mclachlan   basford         the standard approach is based
on likelihood ratio tests on the number of components  unfortunately  this method does not validate
the mixture itself  only selects the best number of components  desoete       
since the mfgn framework provides an explicit expression for the model p z   we can apply
statistical tests of hypothesis over an independent sample t taken from the true density  e g  a subset of the examples reserved for testing  to find out if the obtained approximation is compatible
with test data  if the hypothesis h    t comes from p z   is rejected  then the learning process must
continue  possibly increasing the number of components  it is not difficult to build some statistical
tests  e g  over moments of p z   because their sample means and variances can be directly obtained 
however  as data sets usually include symbolic and numeric variables  we have also developed a
test on the expected likelihood of the test sample  which measures how well p z  covers the examples  the mean and variance of p z  can be easily obtained using the properties of generalized
normals  some experiments over simple univariate continuous densities show that this test is not
very powerful for small sample sizes  i e  incompatibility is not always detected  while other standard tests significantly evidence rejection  nevertheless  clearly inaccurate approximations are
detected  results improve as the sample size increases and the test is valid for data sets with uncertain values 
the minimum description length  li   vitnyi       principle can be also invoked to select
the optimum number of components by trading off the complexity of the model and the accuracy in
the description of the data 

   

firuiz  lpez de teruel   garrido

   discussion and experimental results
    advantages of joint models
most inductive inference methods compute a direct approximation of the conditional densities of
interest  or even obtain empirical decision rules without explicit models of the underlying conditional densities  in these cases  both the model and the learning stage depend on the selected input  
output role of the variables  in contrast  we have presented an inference and learning method based
on an approximation to the joint probability density function of the attributes by a convenient
parametric family  a special mixture model   the mfgn framework works as a pattern completion
machine operating over possibly uncertain information  for example  given a pattern classification
problem  the same learning stage suffices for predicting class labels from feature vectors and for
estimating the value of missing features from the observed information in incomplete patterns  the
joint density approach finds the regions occupied by the training examples in the whole attribute
space  the attribute dependences are captured at a higher abstraction level than the one provided by
strictly empirical rules for pre established target variables  this property is extremely useful in
many situations  as shown in the following examples 
example     hints can be provided for inference over multivalued relations  given the data
set and model from example     assume that we are interested in the value of x for y      we
obtain the bimodal marginal density shown in fig     a and the corresponding estimator x 
         which is  in some sense  meaningless  however  if we specify the branch of interest
of the model  inferring from y     and x       i e   x is small   we obtain the unimodal
marginal density in fig     b and the reasonable estimator x         

 a 
 b 
figure     the desired branch in multivalued relations can be selected by providing
some information about the output values   a  bimodal posterior density inferred from
y     b  unimodal posterior density inferred from y     and the hint x is small 

example     image processing  the advantages of a joint model supporting inferences from
partial information on both inputs and outputs can be illustrated in the following application
with natural data  see fig       the image in fig     a is characterized by a   attribute density
 x  y  r  g  b  describing position and color of the pixels  a random sample of      pixels was
used to build a     component mixture model  we are interested in the location of certain ob 

   

fiprobabilistic inference from uncertain data using mixtures

jects in the image  figs     b f show the posterior density of the desired attributes given the following queries 


 something light green   fig     b  two groups can be easily identified in the posterior
density  corresponding to the upper faces of the green objects   s   c   x  y unknown 
r        g        b        



 something light green or dark red   fig     c  we find the same groups as above and an
additional  more scattered group  corresponding to the red object  this greater dispersion
arises from the larger size of the red object and also from the fact that the r component of
dark red is more disperse than the g component of light green  s  two equiprobable components with c  as above and c   x  y unknown  r        g b       



 something light green on the right   fig     d  here we provide partial information on the
output  s   c   x        y unknown  r        g        b       



 something white   fig     e  s   c   x y unknown  r        g        b       



 something white  in the lower left region  under the main diagonal  y     x    fig     f 
here we provide relational information on the attributes that can be modeled by s     equiprobable components  note that in this case the posterior distribution contains     components  but it is still computationally manageable   

 x       y        r        g        b        
 x       y        r        g        b        
 x       y       r        g        b        
 x        y        r        g        b        
 x        y       r        g        b        
 x        y       r        g        b       
in all cases  the posterior density is consistent with the structure of the original image  the time
required to compute the posterior distribution is always lower than one second  learning time
was of order of hours in a pentium     system  simpler models     component  obtained from
     random pixels  produced also acceptable results with much lower learning time  furthermore  the em algorithm can be efficiently parallelized 

on the other hand  when there is a large number of irrelevant attributes  the joint model strategy wastes resources to capture a proper probability density function along unnecessary dimensions   this problem does not arise in the specification of a likelihood function  since only the relevant attributes explicitly appear in the model   joint modeling is appropriate for domains with a
moderated number of  meaningful  variables without fixed input   output roles 

 

note that a sharp peak  a component with small dispersion  was obtained in the learning process  which also  transmits 
to the posterior density  this kind of artifacts are inocuous and can be easily removed by post processing 

   

firuiz  lpez de teruel   garrido

 a 

 b 

 c 

 d 

 e 
 f 
figure     inference results for the image domain in example      a  source image 
 b  posterior density of attributes x y given  something light green    c  the same for
 something light green or dark red    d  for  something light green on the right  
 e  for  something white    f  for  something white  in the lower left region  under the
main diagonal of the image  y     x  

    advantages of factorization
the proposed methodology is supported by the general density approximation property of mixture
models  we use components with independent variables in order to make computations feasible in

   

fiprobabilistic inference from uncertain data using mixtures

the inference and learning stage  factorized components can be imposed to a mixture model without loss of generality  any statistical dependence between variables can be still captured  at the cost
of a possibly larger number of components in the mixture to achieve the required accuracy in the
approximation 
the simplicity of the building block structure is entirely compensated by an important saving in computation time  high dimensional integrals are analytically computed from univariate
integrals and matrix inversions are avoided in the learning stage  additionally  high dimensional
domains can be easily modeled using a small number of parameters in each mixture component 
from the viewpoint of computational learning theory  vapnik        models with a small number of adjustable parameters  actually  with low expressive power  have favorable consequences
for generalization 
mixtures of factorized components are also used in latent class analysis  desoete        a
well known unsupervised classification technique  it is assumed that the statistical dependences
between attributes can be fully explained by a hidden variable specifying the latent class of each
example  this method is similar to the gaussian decomposition clustering algorithm mentioned in
section    constrained to component conditional attribute independence  however  our goal is not
unsupervised classification but obtaining an accurate and mathematically convenient expression for
the joint density of the variables  required to derive the desired estimators  the meaning of the
components is irrelevant  as long as the whole mixture is a good approximation to the joint density 
more expressive architectures  which combine mixture models with local dimensionality reduction  have been also considered  mixtures of linear experts  jordan   jacobs        mixtures
of principal component analyzers  sung   poggio       or mixtures of factor analyzers  ghahramani   hinton       hinton  dayan    revow        unfortunately  the general kind of inference and learning from uncertain data considered in this work cannot be directly incorporated into
these architectures with the computational advantages demonstrated by the mfgn model 
the restriction to factorized components may produce undesirable artifacts in the approximations of certain domains learned from small training samples  nevertheless  this problem always
occurs to any approximator when the structure of the building block does not match the shape of
the target function  in this case  many terms  or components  units  etc   are required for a good
approximation and the associated parameters can be correctly adjusted only from a large training
sample  however  note that the complexity of the model should not be measured uniquely in terms
of the number of mixture components  the number of adjustable parameters is probably a better
measure of complexity  for instance  full covariance models show a quadratic growth of the number of free parameters with respect to the dimension of the attribute vector  for factorized components the growth is linear  so the amount of training data need not be unreasonably high even if the
number of mixture components is large 
in real applications  the nature of the target function is unknown  so little can be said a priori
about the best building block structure to be used by a universal approximator  we have chosen a
very simple component structure to make inference and learning feasible from uncertain information  section     provides experimental evidence that in realistic problems the proposed model is
not inferior to other popular approaches 
    qualitative comparison with alternative approaches
instead of the proposed methodology  based on mixture models and the em algorithm  other alternative nonparametric density approximation methods could also be used  either for the joint density
or for specific conditional densities   for instance  the nearest neighbor rule locally approximates
the target density using a certain number of training samples near to the point of interest  symbolic

   

firuiz  lpez de teruel   garrido

attributes are directly estimated by a voting scheme and continuous attributes can be also estimated
by averaging the observed values of training instances which are near  in the subspace of observed
attributes  to the point of interest  however  for small sample sizes  the above estimators are not
smooth and show strong sensitivity to random fluctuations in the training set  which penalizes the
estimation cost  for large sample size  the time required to find the nearest neighbors becomes very
long  as an example  consider the regression problem in example     section      fig     b shows
the mfgn solution with   components and mse        fig     a shows the regression line obtained by   nearest neighbors average  with a higher mse       
parzen windows and similar kernel approximation methods are used to smooth the results of
the simple nearest neighbors rule  duda   hart       izenman        they are actually mixtures
of simple conventional densities located at the training samples  in principle  the properties of the
mfgn framework could be adapted to that kind of approximation  ruiz et al         learning
becomes trivial  but strong run time computation effort is required since a concise model of the
domain is not extracted from the training set  this kind of rote learning has also negative consequences on generalization according to the occam razor principle  li   vitnyi        an adequately cross validated mixture model with a small number of components in relation to the training sample size reasonably guarantees that probably the true attribute dependencies are correctly
captured 

c 

c 
c 

c 

 a 
 b 
 c 
figure     alternative solutions in regression and classification  see text for details  

the nature of the solutions obtained by backpropagation multilayer perceptrons  rumelhart et
al        in pattern classification is also illustrative  in general  each decision region can be geometrically expressed as the union of intersections of several halfspaces defined by the units in the
first hidden layer  however  backprop networks often require very long learning times  many adjustable parameters and  what is worse  apparently simple distributions of patterns are hard to learn 
for instance  the solution to the circle ring classification problem in fig     b  obtained by a network with   hidden units requires hundreds of standard backprop epochs  the decision regions are
not very satisfactory  even though the network has extra flexibility for this task    hidden units
suffice to separate the training examples   better solutions exist using all the resources in the network architecture  but backprop learning does not find them  in contrast  the solution obtained by
the mfgn approach using   components  fig     c  requires a learning time orders of magnitude
shorter than backprop optimization  all the components in the mixture contribute to synthesize
reliable decision regions and acceptable solutions can be also obtained with a smaller number of
components 

   

fiprobabilistic inference from uncertain data using mixtures

the proposed approach is closely related to a well known family of approximation techniques
which  essentially  distribute  using some kind of clustering or self organizing algorithm  detectors over the relevant regions of the input space and then combine their responses for computing
the desired outputs  this is the case of radial basis functions  rbf   hertz et al    the classification and regression trees proposed in  breiman et al        and the topological maps used in  cherkassky   najafi       to locate the knots required for piecewise linear regression 
a relevant methodology is proposed in  jordan   jacobs       peng et al         where the
em algorithm is used to learn hierarchical mixtures of experts in the form of linear rules in such a
way that the desired posterior densities can be explicitly obtained  the properties of the em algorithm are also satisfactorily used in  ghahramani   jordan       to obtain unbiased approximations from missing data in a mixture based framework similar to ours  our framework extends this
successful approach by exploiting the conjugate properties of the chosen universal approximation
model  uncertain information of arbitrary complexity can be efficiently processed in the inference
and learning stages 
the mfgn framework is appropriate for a moderated number of variables showing relatively
complex dependencies  in contrast  bayesian networks satisfactorily addresses the case of a large
number of variables with clear conditional independence relations  there are situations in which a
certain subset of the variables in a bayesian network shows no explicit causal structure  this subdomain could be empirically modeled by a mixture model in order to be considered later as a composite node embedded in the whole network  if the subdomain can be conditionally isolated from
the rest of variables through a set of communication nodes  the mfgn framework can be used to
perform the required inferences 
finally  mixture models are typically used for unsupervised classification  the examples are
labeled with the index of the component with highest posterior probability  in fact  the mfgn
framework explicitly finds clusters in the training set  furthermore  continuous and symbolic attributes are allowed in the joint density  so the examples are clustered using an implicit probabilistic metric which automatically weighs all the  heterogeneous  attributes  even with missing and
uncertain values  however  this method is effective only when the groups of interest have the same
structure as the component densities  in order to simplify inference the mixture components have
been selected with constraints  gaussian  independent variables  which are not necessarily verified
by the natural groups found in real applications 
a tentative possibility  inspired in a common heuristic clustering technique  consists of joining overlapping components  e g   according to the battachariya distance  a well known bound on
the bayes error used in statistical pattern recognition  fukunaga         unfortunately  our experiments indicate that the overlapping threshold is a free parameter that strongly determines the
quality of the results  a universal threshold  independent of the application  does not seem to exist 
in principle  clusters of arbitrary geometry may be discovered  but this cannot be easily automated 
therefore  other nonparametric cluster analysis methods  e g  density valley seeking  are suggested
for labeling complex groups 
    experimental evaluation
the mfgn method has been evaluated on standard benchmarks from the machine learning database repository at the university of california  irvine  merz and murphy        it contains inductive learning problems which are representative of real world situations  we have experimented
with the following databases  ionosphere  pima indians  monk s problems  and horse colic  which
illustrate different properties of the proposed methodology  in most cases mfgn has been compared to alternative learning methods with respect to the inference task considered of interest in

   

firuiz  lpez de teruel   garrido

each problem  typically  prediction of a specific attribute given the rest of them   we usually give
the error rate over both the training and the test set to indicate the amount of overfitting obtained by
the learning algorithms 

 a  ionosphere
 b  pima indians
figure     most discriminant  d projections of two representative databases 

      ionosphere database
two classes of radar returns from the ionosphere must be discriminated from vectors of    continuous attributes   there are     examples  randomly partitioned into two disjoint subsets of approximately equal size for training and testing  the prevalence of the minoritary class  random
prediction rate  is      figure    a and table   show that this is a typical statistical pattern recognition problem  easily solvable by standard methods  the results suggest that the bayes  optimum 
error probability is around    
error rate
pe
 training set 
method
 test set 
linear mse  pseudoinverse 
    nearest neighbor
    nearest neighbor
parzen model
backprop multilayer perceptron   hidden units
support vector machine  rbf kernel  width         s v  
support vector machine  rbf kernel  width        s v  
support vector machine  polinomial kernel  order        s v  
support vector machine  polinomial kernel  order        s v  
support vector machine  polinomial kernel  order        s v  
full covariance gaussian mixture    component class
full covariance gaussian mixture    component class
full covariance gaussian mixture    component class
mfgn   components  average 
mfgn   components  average 
mfgn    components  average 
mfgn  best result by cross validation    components 
table    ionosphere database results

 

   

   
   

   
   
    
      
      
      
   

originally the database contains    attributes  two of them  meaningless or ill behaved  were eliminated 

   

   
   
   
   
   
   
   
   
   
   
   
   
   
      
      
      
   

fiprobabilistic inference from uncertain data using mixtures

in this problem  the plain mfgn method  without special heuristics in the learning stage  is
comparable in average to the alternative methods  the best solution on the training set  crossvalidation  is entirely satisfactory 
for the ionosphere database we also present an exhaustive study of performance given varying
proportions of missing values in the training and testing examples  a value of x   means that in all
training or test examples the value of each attribute is deleted with probability x  the basic experiment consists of learning a mfgn model with the prescribed number of components       and    
and computing the error rate on the training and test sets  table   shows the mean value    standard deviations of the error rates obtained in    repetitions of the basic experiment in each configuration  column m contains the error rate of each configuration over its own training set  the training test partition is kept fixed to analyze the variability of the solutions due to random initialization
of the em 
learning
m

  

inference
   
   

   

  comp      
  comp      
   comp      

      
     
     

     
     
     

     
     
     

     
     
     

     
     
     

  comp       
  comp       
   comp       

      
     
     

      
     
     

      
     
     

      
     
     

      
     
     

  comp       
  comp       
   comp       

     
     
  

     
      
     

     
     
      

     
     
     

     
     
     

  comp       
                                  
  comp       
                                  
   comp             
                           
table    evaluation of mfgn on ionosphere database given
different proportions of missing data in the training and testing subsets 

as expected  the mfgn model is robust with respect to large proportions of missing values in
the test patterns  and to moderated proportions of missing data in the training set  we have compared the above behavior with a standard algorithm for decision tree construction inspired in
 quinlan        which is also able to support missing values   table   shows the error rates of the
decision trees for the same experimental setting as in table    this kind of decision tree obtains
error rates that are better than the averages obtained by mfgn  however  mfgn s best solutions
 selected by cross validation  are better than the ones obtained by decision tree  furthermore 
decision tree performance degrades faster than mfgn  especially with respect to the proportion
of missing values in the inference stage 

 

essentially  missing values are handled as follows  in the learning stage  when an attribute is selected  examples with
missing values are sent to all the partitions with appropriate weights  in the inference stage  if a node asks for a missing
value  it follows all the branches with appropriate weights and finally the outputs are combined 

   

firuiz  lpez de teruel   garrido

learning
m

  

inference
    
    

    

  
  
  
    
    
    
  
    
    
    
    
    
  
    
    
    
    
    
  
    
    
    
    
    
table    evaluation of basic decision tree on ionosphere database given different proportions of missing data in the training and testing subsets 

      pima indians database
in this problem we must discriminate between two possible results of a diabetes test given to pima
indians  there are   continuous attributes  and     examples  randomly partitioned into two disjoint subsets of equal size for training and testing  the prevalence of the minority class is      the
attribute vector has been normalized  table    presents comparative results 
error rate
pe
 training set 
method
 test set 
linear mse  pseudoinverse 
oblique decision tree   decision nodes
    nearest neighbor
    nearest neighbor
full covariance gaussian mixture    component class
full covariance gaussian mixture    component class
full covariance gaussian mixture    component class
full covariance gaussian mixture    component class
backprop multilayer perceptron   hidden units
backprop multilayer perceptron   hidden units
backprop multilayer perceptron   hidden units
support vector machine  rbf kernel  width        s v  
support vector machine  rbf kernel  width        s v  
support vector machine  polynomial kernel  order        s v  
support vector machine  polynomial kernel  order        s v  
mfgn   components
mfgn   components
mfgn   components
table     pima indians database results

   
   

   
   
   
   
   
   
   

   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

despite of low dimensionality and large number of examples  this classification problem is
hard  see figure    b   even sophisticated learners such as backpropagation networks  decision
trees or support vector machines  which are able to store a reasonable proportion of the training set 
do not achieve significant generalization  mfgn shows a similar behavior  although it is slightly
less prone to overfitting  the error rate on the training set is not misleading  
      horse colic database
this database contains a classification task from a heterogeneous attribute vector including symbolic  discrete and continuous variables  with     missing values  it illustrates the problem of

   

fiprobabilistic inference from uncertain data using mixtures

feature selection in the context of joint modeling  mentioned in section      table    shows the
error rates obtained by mfgn using different attribute subsets   to take advantage of its general
inference properties  the mfgn model must be applied to the attribute subset of interest  if the
inference task is fixed and the number of attributes is very large  alternative methods should be
used 
method

pe

pe

 distribution     initializations 

 best 

  selected attributes
mfgn   components
mfgn   components
mfgn   components
mfgn   components
mfgn   components
mfgn   components
mfgn    components
mfgn    components
mfgn    components

      
      
      
      
      
      
      
      
      

   
   
   
   
   
   
   
   
   

      
      
      
      
      
      

   
   
   
   
   
   

      
      
      
      

   
   
   
   

  selected attributes
mfgn   components
mfgn   components
mfgn   components
mfgn    components
mfgn    components
mfgn    components

   selected attributes
mfgn   components
mfgn   components
mfgn    components
mfgn    components
table     horse colic database results  random rate      

      monk s problems
the monk s problems are three concept learning tasks from   symbolic attributes  widely used as
benchmarks for inductive learning algorithms  thrun et al         as seen in table     mfgn fails
on monk   where acceptable generalization is not obtained  and monk   where the training
examples cannot even be stored   in contrast  mfgn correctly solves monk   this behavior is
related to the fact that the monk s problems are based on deterministic or abstract concepts which
may lack the kind of geometric regularities in the attribute space required by probabilistic models   
 

features were individually selected using a simple discrimination index related to the kolmogorov smirnov statistic
 ruiz       
  
a typical example is the parity problem  acceptable off training set generalization cannot be achieved if the inductive
bias of the learning machine is biased towards  smooth  solutions 

   

firuiz  lpez de teruel   garrido

fig     shows the most discriminant  d projections of the datasets and illustrates the fact that
monk  cannot be easily captured by statistic techniques  in this benchmark  mfgn performance
is similar to that of other popular probabilistic methods  thrun et al        

 a  monk 
 b  monk 
figure     most discriminant  d projections of the monk s datasets 

 c  monk 

error rate
 training set 

method

pe
 test set 

monk   random rate      
linear mse  pseudoinverse 
    nearest neighbor
support vector machine  rbf kernel  width       s v  
cascade correlation
mfgn   components
mfgn   components

   

   
   

   
   
   
 
   
   

monk   random rate     
linear mse  pseudoinverse 
    nearest neighbor
support vector machine  rbf kernel  width        s v  
cascade correlation
mfgn   components
mfgn   components
mfgn    components

   

   
   
   

   
   
   
 
   
   
   

monk   random rate     
linear mse  pseudoinverse 
    nearest neighbor
support vector machine  rbf kernel  width       s v  
cascade correlation
mfgn   components
mfgn   components
mfgn   components
table     monk s problems results

   

   

   
   
   

   
   
   
   
   
   
   

fiprobabilistic inference from uncertain data using mixtures

      comments
the above experiments demonstrate that the mfgn model is able to obtain acceptable results on
many real world applications  in particular  the error rates obtained in standard classification tasks
are comparable to those obtained by other popular learners  additionally  mfgn is able to perform
inferences over any other attribute given uncertain or partial information  which is not possible for
most of the alternative methods  this property makes mfgn a very attractive alternative for many
inference problems such as the one illustrated in example     the experiments have also contributed to characterize the kind of problems for which the mfgn model is best suited  essentially  the
relationship among attributes must be of a true probabilistic nature  and the attribute vector must be
of a moderated size containing  relevant  variables  a previous feature selection   accommodation
stage is recommended in certain applications 

   conclusions
we have developed an efficient methodology for probabilistic inference and learning from uncertain information  under the proposed mfgn framework  the joint probability density function of
the attributes and the likelihood function of the available information are approximated by mixtures of factorized generalized normals  this mathematical structure allows efficient computation 
without numerical integration  of posterior densities and expectations of the desired variables given
events of arbitrary geometry  an extended version of the em learning algorithm has been developed to estimate the parameters of the required mixture models from uncertain training examples 
different paradigms as pattern recognition  regression or pattern completion are subsumed under a
common framework 
a comprehensive collection of examples illustrates the methodology  which has been critically
compared with alternative techniques  the extended em algorithm is able to learn satisfactory
domain models from a reasonable number of examples with uncertain values  taking into account
the explicit likelihood functions of the available information  results are satisfactory whenever the
sample size is large in relation to the amount of  known  degradation of the training set  the experiments also characterized the kind of situations that the model manages better  domains described by a moderate number of heterogeneous attributes with complex probabilistic dependences 
problems in which the output variables are not necessarily known in the learning stage  i e  pattern
completion   and  finally  problems in which an explicit management of uncertainty is needed  either in the learning or in the inference stage  or even in both   the mfgn framework has obtained
a very favorable trade off between useful features and model complexity in the solutions to different applications and benchmarks 
future developments of our work include improving the learning stage with some heuristic
steps that are combined with the standard e and m steps to control the adequacy of the acquired
models  additional studies are required on validation tests  generalization  scalability  robustness
and data preprocessing  the essential idea of working with explicit likelihood functions will be
incorporated into the parzen approximation scheme and we are also interested in more expressive
model structures such as mixtures of factor analyzers  principal component analyzers or linear experts  finally  the methodology can be developed in a pure bayesian framework or subsumed under
the dempster shafer evidence theory 

   

firuiz  lpez de teruel   garrido

acknowledgments
the authors would like to thank the anonymous reviewers for their careful reading and helpful suggestions  this work has been supported by the spanish cicyt grants tic         tic       c      and tic        c      

references
berger  j           statistical decision theory and bayesian analysis  springer verlag 
bernardo  j m   smith  a f m          bayesian theory  wiley 
bouckaert  r r          properties of bayesian belief network learning algorithms  proceedings of uncertainty in ai  pp          
breiman  l   friedman  j h   olshen  r a   and stone  c j          classification and regression
trees  wadsworth international group  belmont  ca 
chang  k    fung  r          symbolic probabilistic inference with both discrete and continuous variables  ieee tran  on systems  man  and cybernetics  vol      no     june  pp         
cherkassky  v  and lari najafi  h          nonparametric regression analyisis using selforganizing topological maps in h  wechsler  ed    neural networks for perception  vol   
computation  learning and architectures  san diego  academic press 
cohn  d a   ghahramani  z    jordan  m i          active learning with statistical models 
journal of artificial intelligence research    pp          
dalal  s r    hall  w j          approximating priors by mixtures of natural conjugate priors 
j  r  statist  soc  b  vol      no     pp          
de soete  g          using latent class analysis in categorization research in i  v  mechelen 
j  hampton  r s  michalski  p  theuns  eds    categories and concepts  theoretical views
and inductive data analysis  san diego  academic press 
dempster  a p   laird  n m   rubin  d b           maximum likelihood estimation from incomplete data via the em algorithm  journal of the royal statistical society  series b  vol     
pp       
duda  r o  and hart  p e          pattern classification and scene analysis  john wiley   sons 
fan  c m   namazi  n m  and penafiel  p b          a new image motion estimation algorithm
based on the em technique  ieee transactions on pattern analisys and machine intelligence  vol     no    march  pp          
fukunaga  k          introduction to statistical pattern recognition  academic press 
ghahramani  z  and jordan  m i         supervised learning from incomplete data via an em
approach in cowan  j d   tesauro  g   and alspector  j   eds    advances in neural information processing systems    morgan kauffman
ghahramani  z  and hinton  g e         the em algorithm for mixtures of factor analyzers 
tech  rep  univ  toronto  crg tr      

   

fiprobabilistic inference from uncertain data using mixtures

heckerman  d    wellman  m p          bayesian networks  communications of the acm  vol 
    no    pp         march 
hertz  j   krogh  a   palmer  r g           introduction to the theory of neural computation 
addison wesley 
hinton  g e   dayan  p  and revow  m          modeling the manifold of images of handwritten
digits  ieee t  on neural networks    pp        
hornik  k   stinchcombe  m   white  h           multilayer feedforward networks are universal
approximators  neural networks  no   
hutchinson  a          algorithmic learning  new york  oxford univ  press 
izenman  a j          recent developments in nonparametric density estimation  j  amer  statist  assoc  vol      no       pp          
jordan  m i   jacobs  r a           hierarchical mixtures of experts and the em algorithm 
neural computation     pp          
kohonen  t           self organization and associative memory  springer verlag 
lauritzen  s l    spiegelhalter  d  j          local computations with probabilities on graphical
structures and their application to expert systems  j  r  statist  soc  b      no     pp         
li  m  and vitnyi  p          an introduction to kolmogorov complexity and its applications 
new york  springer verlag 
mclachlan  g j   basford  k e           mixture models  new york  marcel dekker 
mclachlan  g j  and krishnan  t          the em algorithm and extensions  john wiley and
sons 
michalski  r s   carbonell  j  and mitchell  t m   eds          machine learning  an artificial
intelligence approach  palo alto  ca  tioga press  also reprinted by morgan kaufmann
 los altos  ca  
michalski  r s   carbonell  j  and mitchell  t m   eds          machine learning  an artificial
intelligence approach  vol  ii  los altos  ca  morgan kaufmann 
mohgaddam  b  and pentland  a          probabilistic visual learning for object representation  ieee t pami  vol      no         pp       
merz  c j  and murphy  p m          uci repository of machine learning databases 
 http   www ics uci edu  mlearn mlrepository html   irvine  ca  university of california 
department of information and computer science 
palm  h c          new method for generating statistical classifiers assuming linear mixtures of
gaussian densities  proceedings   th iapr international conference on pattern recognition  jerusalem  october              vol    ieee  piscataway  nj  usa 
papoulis  a           probability  random variables and stochastic processes  mcgraw hill 
pearl  j           probabilistic reasoning in intelligent systems  networks of plausible inference 
morgan kaufmann 

   

firuiz  lpez de teruel   garrido

peng  f   jacobs  r a   tanner  m a          bayesian inference in mixtures of experts and hierchical mixtures of experts models with an application to speech recognition  accepted
in the journal of the american statistical association 
priebe  c e   marchette  d j           adaptive mixtures  recursive nonparametric pattern recognition  pattern recognition  v   n    pp            
pudil  p   novovicova  j   choakjarernwanit  n   kittler  j          feature selection based on the
approximation of class densities by finite mixtures of special type  pattern recognition
vol    no   pp            
quinlan  j r           c     programs for machine learning  san mateo  ca  morgan kaufmann 
redner  r a   walker  h f           mixture densities  maximum likelihood estimation and the
em algorithm  siam review  vol      pp          
rojas  r          a short proof of the posterior probability property of classifier neural networks  neural computation vol   issue    january 
rumelhart  d e   hinton  g  e  and williams  r          learning internal representations by
error propagation in rumelhart  mcclelland   the pdp group         pp          
rumelhart  d e   mcclelland  j l    the pdp research group          parallel distributed processing  explorations in the microstructure of cognition  vol     foundations  cambrigde
ma  bradford books mit press 
ruiz  a          a nonparametric bound for the bayes error  pattern recognition  vol      no 
   pp          
ruiz  a   lpez de teruel  p e  and garrido  m c          kernel density estimation from indirect observations  in preparation 
sung  k  k  and poggio  t            example based learning for view based human face detection   ieee trans  pattern analyisis and machine intelligence  vol     n    january  pp 
      
tanner  m a          tools for statistical inference    rd ed    springer 
thrun  s  et al          the monk s problems  a performance comparison of different learning
algorithms   technical report cmu cs        
titterington  d m   a f m  smith and u e  makov         statistical analysis of finite mixture
distributions  wiley  new york 
traven  h g c           a neural network approach to statistical pattern classification by
 semiparametric  estimation of prob  den  func   ieee t neural networks  v  n  
valiant  l g          a view of computational learning theory in meyrowitz and chipman 
eds          foundations of knowledge acquisition  machine learning  kluwer acad  pub 
valiveti  r s   oommen  b j           on using the chi squared metric for determining stochastic
dependence  pattern recognition  v   n   pp            
vapnik  v n          learning dependencies based on empirical data  springer  new york 
vapnik  v n          the nature of statistical learning theory  springer  new york 

   

fiprobabilistic inference from uncertain data using mixtures

wan  e a           neural networks classification  a bayesian interpretation  ieee trans  on
neural networks  v  n  
weiss  y  and adelson e h          perceptually organized em  a framework for motion segmentation that combines information about form and motion  tr mit mlpcs tr     
wilson  d r  and martinez  t r          improved heterogeneous distance functions  jair  v  
pp       
wolpert  d h   ed       a   the mathematics of generalization  proc  sfi clns workshop on
formal approaches to supervised learning 
xu  l    jordan  m i          on the convergence properties of the em algorithm for gaussian
mixtures  neural computation vol   issue    january 
you  y l  and kaveh  m          a regularization approach to joint blur identification and image
restoration  ieee t on image processing  vol   no    pp          

   

fi