journal artificial intelligence research                 

submitted       published      

probabilistic inference arbitrary uncertainty
using mixtures factorized generalized gaussians
alberto ruiz
pedro e  lpez de teruel
m  carmen garrido
universidad de murcia  facultad de informtica 
campus de espinardo         murcia  spain

aruiz dif um es
pedroe ditec um es
mgarrido dif um es

abstract
paper presents general efficient framework probabilistic inference learning
arbitrary uncertain information  exploits calculation properties finite mixture models  conjugate families factorization  joint probability density variables likelihood
function  objective subjective  observation approximated special mixture model 
way desired conditional distribution directly obtained without numerical integration  developed extended version expectation maximization  em  algorithm
estimate parameters mixture models uncertain training examples  indirect observations  
consequence  piece exact uncertain information input output values consistently handled inference learning stages  ability  extremely useful certain situations  found alternative methods  proposed framework formally justified
standard probabilistic principles illustrative examples provided fields nonparametric
pattern classification  nonlinear regression pattern completion  finally  experiments real application comparative results standard databases provide empirical evidence utility
method wide range applications 

   introduction
estimation unknown magnitudes available information  form sensor measurements subjective judgments  central problem many fields science engineering 
solve task  domain must accurately described model able support desired
range inferences  satisfactory models cannot derived first principles  approximations must obtained empirical data learning stage 
consider domain z composed collection objects z   z   z        zn   represented
vectors n attributes  given partial knowledge  expressed general form explained
later  certain object z  interested computing good estimate z       close
true z  allow heterogeneous descriptions  attribute zi may continuous  discrete  symbolic valued  including mixed types  specific subset unknown uncertain attributes
estimated  attribute vector partitioned z    x  y   z denotes target
output attributes  target attributes different different objects z  scenario includes several usual inference paradigms  instance  specific target symbolic
attribute  task called pattern recognition classification  target attribute continuous  inference task called regression function approximation  general  interested general framework pattern completion partially known objects 
example    illustrate setting  assume preprocessor hypothetical computer
vision system obtains features segmented object  instances domain described

     ai access foundation morgan kaufmann publishers  rights reserved 

firuiz  lpez de teruel   garrido

following n   attributes  area  z    color  z   white  black  red        distance 
z    shape  z   circular  rectangular  triangular        texture  z   soft  rough       
objecttype  z   door  window       angle  z    typical instance may z       
blue       triangular  soft  window       object partially occluded   dimensional 
attributes missing uncertain  instance  available information z
could expressed       blue black       triangular     window     door         
z   z   z  uncertain  z   z  exact z   z  missing  case could interested estimates    z   z   z   even improving knowledge z  z  

non deterministic nature many real world domains suggests probabilistic approach 
attributes considered random variables  objects assumed drawn independently identically distributed p z    p z        zn    p x  y   multivariate joint probability
density function attributes  completely characterizes n dimensional random variable z  simplify notation  use function symbol p   denote different p d f s
identified without risk confusion 
according statistical decision theory  berger        optimum estimators desired attributes obtained minimization suitable expected loss function 

opt       argmin e  l       s 


l y    loss incurred true estimated   estimators always features conditional posterior distribution p y s  target variables given available
information  instance  minimum squared error  mse  estimator posterior mean 
minimum linear loss estimator posterior median minimum error probability  ep     
loss  estimator posterior mode 
example    typical problem prediction unknown attribute observed attributes x  case available information written    x      continuous 
reasonable use mse estimator  mse       e    x    general regression function 
symbolic loss associated errors  ep estimator adequate 
ep       argmaxy p y x    argmaxy p x y p y   corresponds maximum posteriori rule
bayes test  widely used statistical pattern recognition 

joint density p z    p x  y  plays essential role inference process  implicitly
includes complete information attribute dependences  principle  desired conditional
distribution estimator computed joint density adequate integration  probabilistic inference process computing desired conditional probabilities  possibly
implicit  joint distribution  p z   the prior  model domain  comprising implications 
 a known event  somewhat related certain z   could obtain posterior p z s 
desired target marginal p y s   the probabilistic consequent  
example    observe exact value xo attribute x  i e      x   xo   have 

p  y    p y  xo   





p   xo    
p  xo    dy

know instance z certain region r attribute space  i e     z r  
compute marginal density joint p z    p x  y  restricted region r  fig     

   

fiprobabilistic inference uncertain data using mixtures

p  y     



x

p  x  y    x     r   dx  

p  x  y  dx
p  x  y  dxdy
r

r

general types uncertain information z discussed later 


p y r 
r

p x y 
x

figure    conditional probability density y  assuming z    x y  r 

summary  joint density p z  multivariate random variable  subset variables z may be  principle  estimated given available information whole z    x 
y   practical situations  two steps required solve inference problem  first  good model
true joint density p z  must obtained  second  available information must efficiently processed improve knowledge future  partially specified instances z  two
complementary aspects  learning inference  approached many scientific fields  providing different methodologies solve practical applications 
point view computer science  essential goal inductive inference
find approximate intensional definition  properties  unknown concept  subset domain  incomplete extensional definition  finite sample   machine learning techniques
 michalski  carbonell   mitchell             hutchinson       provide practical solutions  e g 
automatic construction decision trees  solve many situations explicit programming
must avoided  computational learning theory  valiant       wolpert       vapnik      
studies feasibility induction terms generalization ability resource requirements
different learning paradigms 
general setting statistical decision theory  modeling techniques operational aspects inference  based numerical integration  monte carlo simulation  analytic approximations  etc   extensively studied bayesian perspective  berger       bernardo
  smith        specific field statistical pattern recognition  duda   hart      
fukunaga        standard parametric nonparametric density approximation techniques  izenman
      used learn training data class conditional p d f s required optimum
decision rule  instance  class conditional densities p x y  gaussian  required parameters mean vector covariance matrix feature vector class decision regions x quadratic boundaries  among nonparametric classification techniques  parzen method k n nearest neighbors rule must mentioned  analogously 
target attribute continuous statistical dependence input output variables
p x y  properly modeled joint normality  get multivariate linear regression  mse x   
x   b  required parameters mean values covariance matrix attrib 

   

firuiz  lpez de teruel   garrido

utes  nonlinear regression curves derived nonparametric approximation techniques  nonparametric methods present slower convergence rates  requiring significantly larger
sample sizes obtain satisfactory approximations  strongly affected dimensionality data selection smoothing parameter crucial step  contrast 
require kind smoothness assumption target density 
neural networks  hertz et  al       computational models trainable empirical data
proposed solve complex situations  intrinsic parallel architecture
especially efficient inference stage  one widely used neural models multilayer perceptron  universal function approximator  hornik et al        breaks limitations
linear decision functions  backpropagation learning algorithm  rumelhart et al        can 
principle  adjust network weights implement arbitrary mappings  network outputs
show desirable probabilistic properties  wan       rojas        unsupervised networks probability density function approximation  kohonen        however  neural models
usually contain large number adjustable parameters  convenient generalization
and  frequently  long times required training relatively easy tasks  input   output role
attributes cannot changed runtime missing uncertain values poorly supported 
bayesian networks  based concept conditional independence  among
relevant probabilistic inference technologies  pearl       heckerman   wellman        joint
density variables modeled directed graph explicitly represents dependence
statements  wide range inferences performed framework  chang   fung
      lauritzen   spiegelhalter       significant results inductive learning
network structures  bouckaert       cooper   herskovits       valiveti   oomen       
approach adequate large number variables showing explicit dependences
simple cause effect relations  nevertheless  solving arbitrary queries np complete  automatic
learning algorithms time consuming allowed dependences variables relatively simple 
attempt mitigate drawbacks  developed general efficient inference learning framework based following considerations  well known
 titterington et al        mclachlan   basford       dalal   hall       bernardo   smith      
xu   jordan       reasonable probability density function p z  approximated
desired degree accuracy finite mixture simple components ci       l 

p  z   p ci   p  z  ci  

   



superposition simple densities extensively used approximate arbitrary data dependences  fig      maximum likelihood estimators mixture parameters efficiently obtained samples expectation maximization  em  algorithm  dempster  laird   rubin
      redner   walker        see section    

   

fiprobabilistic inference uncertain data using mixtures

 a 
 b 
 c 
figure    illustrative example density approximation using mixture model   a  samples p d f  p x y  showing nonlinear dependence   b  mixture model p x y 
  gaussian components obtained standard em algorithm   c  location
components 

decomposition probability distributions using mixtures frequently applied
unsupervised learning tasks  especially cluster analysis  mclachlan   basford       duda  
hart       fukunaga        posteriori probabilities postulated category computed
examples  labeled according probable source density  however 
mixture models specially useful nonparametric supervised learning situations  instance 
class conditional densities required statistical pattern recognition individually approximated  priebe   marchette       traven       finite mixtures  hierarchical mixtures
linear models proposed  jordan   jacobs       peng et  al        mixtures factor analyzers developed  ghahramani   hinton       hinton  dayan    revow      
mixture models useful feature selection  pudil et al         mixture modeling
growing semiparametric probabilistic learning methodology applications many research
areas  weiss   adelson       fan et al        moghaddam   pentland       
paper introduces framework probabilistic inference learning arbitrary uncertain data  piece exact uncertain information input output values consistently handled inference learning stages  approximate joint density p z 
 model domain  relative likelihood function p s z   describing available information  specific mixture model factorized conjugate components  way numerical integration avoided computation desired estimator  marginal conditional
density 
advantages modeling arbitrary densities using mixtures natural conjugate components already shown  dalal   hall        and  recently  inference procedures based
similar idea proposed  ghahramani   jordan       cohn et al        peng et al       
palm        however  method efficiently handles uncertain data using explicit likelihood
functions  extensively used machine learning  pattern recognition
related areas  follow standard probabilistic principles  providing natural statistical validation procedures 
organization paper follows  section   reviews elementary results
concepts used proposed framework  section   addresses inference stage  section  
concerned learning  extending em algorithm manage uncertain information  section  
discusses method relation alternative techniques presents experimental evaluation 
last section summarizes conclusions future directions work 

   

firuiz  lpez de teruel   garrido

   preliminaries
    calculus generalized normals
many applications  instances domain represented simultaneously continuous
symbolic discrete variables  as wilson   martinez        simplify notation 
denote probability impulses gaussian densities means common formalism 
generalized normal
 x    denotes probability density function following properties 



t x   

    

  x    
 
exp

    

t x      t x     t x    x 

    

tzero  x   
gaussian density mean standard deviation    dispersion
reduces
diracs delta function located   cases proper p d f  
t x       
t x    dx    
x

product generalized normals elegantly expressed  papoulis      pp       berger
      by 
       

t x      t x        t x    t               

   

mean dispersion new normal given by 

          
 
       

    
   
      
 

relation useful computing integral product two generalized normals 
       



x

t x      t x      dx   t                

   

and  consistency  define



          

x

t x    t x    dx   t      i     

i predicate      predicate true zero otherwise  virtually reasonable univariate
probability distribution likelihood function accurately modeled appropriate mixture
generalized normals  particular  p d f s symbolic variables mixtures impulses 
without loss generality  symbols may arbitrarily mapped specific numbers represented
numeric axes  integrals discrete domains become sums 
example    let us approximate p d f  p x  mixed continuous symbolic valued random variable x mixture generalized normals  assume x takes probability    
exact value     with special meaning   probability     random value continuously distributed following triangular shape shown fig     density p x  accurately approximated  see section    using   generalized normals 

t x          t x               t x               t x         

p x     

   

fiprobabilistic inference uncertain data using mixtures

figure    p d f  mixed random variable approximated mixture generalized normals 

    modeling uncertainty  likelihood principle
assume value random variable z must inferred certain observation subjective information s  z drawn p z  measurement judgment process
characterized conditional p s z   knowledge z updated according p z s  p z  p s z 
  p s   p s    z p s z  p z  dz  see fig     
likelihood function fs z  p s z  probability density ascribed possible
z  arbitrary nonnegative function z interpreted two alternative ways 
objective conditional distribution p s z  physical measurement process  e g 
model sensor noise  specifying bias variance observable every possible true
value z   known error model  subjective judgment chance
different z values  e g  intervals  likely regions  etc    based vague difficult formalize
information  dispersion fs z  directly related uncertainty associated measurement process  following likelihood principle  berger        explicitly assume
experimental information required perform probabilistic inference contained likelihood
function fs z  

p z 
z 

z 

z 

z

p s z  

p s z  

p s z  




prior
model
measurement

p s 

fso z   p so z 

z

observable

likelihood
observation

p z so 
posterior
z
figure    illustration elementary bayesian univariate inference process 

   

firuiz  lpez de teruel   garrido

    inference using mixtures conjugate densities
computation p z s  may hard  unless p z  p s z  belong special  conjugate  families  berger       bernardo   smith        case posterior density analytically
obtained parameters prior likelihood  avoiding numeric integration 
prior  likelihood posterior mathematical family  belief structure
closed inference process 



example    univariate case  assume z known normally distributed around r
dispersion r  i e  p z     z  r  r   assume measurement device gaussian noise  observed values distributed according p s z   
 s  z  s   therefore 
observe certain value so  property product generalized normals eq 
     posterior knowledge z becomes another normal
 z       new expected location z expressed weighted average r so          r uncertainty reduced     s    coefficient    r      r   s    quantifies relative im 





portance experiment respect prior 

computational advantage extended general case using mixtures conjugate families  dalal   hall       approximate desired joint probability distribution
likelihood function 
example    domain likelihood modeled respectively

p z 




pi

t z     


p so z 





r

r

t z     
r

r

 where r   r r depend explicitly observed so   posterior
written following mixture 



p z so 

i  r

properties          parameters

  r

 r

t z 

   r  

   

 r  r weights  r given by 

i  r    r
 
i    r 

 r

 r

 r  

r
i    r 

pi r t    r   i    r   

p
k

l

t  k   l    k    l  

k  l

    role factorization
given multivariate observation z partitioned two subvectors  z    x  y   assume
interested inferring value unknown attributes observed attributes x  note
x statistically independent  joint density factorizable  p z    p x  y    p x 
p y  and  therefore  posterior p y x  equals prior marginal p y   observed x carries
predictive information optimum estimators depend x  instance 
mse   x     e y x    e y  ep   x     argmax p      simplest estimation task 
runtime computations required optimum solution  may precalculated 

   

fiprobabilistic inference uncertain data using mixtures

realistic situations variables statistically dependent  general  joint density cannot factorized required marginal densities may hard compute  however  interesting
consequences arise joint density expressed finite mixture factorized  with independent variables  components c   c        cl  

p z    p  z         z n     p ci   p  z  ci     p ci   p  z j   ci  




   

j

structure convenient inference purposes  particular  terms desired partition z    x  y  

p z    p x  y   

p c   p  x  c   p  y  c  








marginal densities mixtures marginal components 

p  x     p  x    dy   p ci   p  x  ci  




p      p ci   p  y  ci  


desired conditional densities mixtures marginal components 

p  y  x       x   p  y  ci  

   



weights  x  probabilities observed x generated component ci  

  x   

p ci   p  x ci  

p c   p  x c  
j

  p ci   x 

j

j

p d f  approximation capabilities mixture models factorized components remain
unchanged  cost possibly higher number components obtain desired degree
accuracy  avoiding artifacts  see fig      section     discusses implications factorization
relation alternative model structures 

 a 
 b 
figure     a  density approximation data fig     using mixture   factorized components   b  location components  note arbitrary dependence
represented mixture components independent variables  observe
somewhat smoother solution could obtained increasing number components  

   

firuiz  lpez de teruel   garrido

   mfgn framework
previous concepts integrated general probabilistic inference framework call
mfgn  mixtures factorized generalized normals   fig    shows abstract dependence relations among attributes generic domain  upper section figure  attributes
observed information  lower section   mfgn framework  relations modeled
finite mixtures products generalized normals  key idea using factorization cope
multivariate domains heterogeneous attribute vectors  conjugate densities efficiently perform inferences given arbitrary uncertain information  section  derive
main inference expressions  learning stage described section   

p z 
z 

zn
z 

model
domain
p z 

zj
model
measurement
p s z 



figure    generic dependences inference process 

    modeling attribute dependences domain
mfgn framework attribute dependencies domain modeled joint density
form finite mixture factored components  expression      component
marginals p  z j   ci   t  z j   ij   ij   generalized normals 

p  z     pi


t  z

j

  ij   ij  

i    l  j    n 

   

j

desired  terms associated pure symbolic attributes z j  with ij     
collected way component marginals expressed mixtures impulses 

p  z j   ci   ti j  t  z j    

   



ti j  p z j     ci   probability z j takes  th value component ci  
manipulation reduces number l global components mixture  adjustable parameters
model proportions pi   p ci   mean value ij dispersion ij j th
j
attribute i th component  or  symbolic attributes  probabilities ti     

structure     explicitly used symbolic attributes applications illustrative examples  mathematical derivations made concise expression     

   

fiprobabilistic inference uncertain data using mixtures

variables continuous  mfgn architecture reduces mixture gaussians
diagonal covariance matrices  proposed factorized structure extends properties
diagonal covariance matrices heterogeneous attribute vectors  interested joint models 
support inferences partial information subset variables  note
easy way define measure statistical depencence symbolic continuous
attributes  used parameter probability density function   required  hetereogeneous  dependence model conveniently captured superposition simple factorized
 with independent variables  densities 
example    figure   shows illustrative   attribute data set  x continuous z
symbolic  components mfgn approximation obtained em algorithm
 see section    joint density  parameters mixture shown table   
note that  overlapped structure data  components       assigned values symbolic attribute z 

 a 
 b 
figure     a  simple data set two continuous one symbolic attribute 
 b  location mixture components 



pi

ix

ix

iy

iy

tiz white

 
   
    
   
    
   
 
 
   
    
   
    
   
 
 
   
   
   
   
   
 
 
   
    
   
   
   
 
 
   
   
   
    
   
   
 
   
    
   
   
   
   
 
   
   
   
    
   
 
table    parameters mixture model data set fig    

tiz black
 
 
 
 
   
   
 

    modeling arbitrary information instances
available information particular instance z denoted s  following likelihood
principle  concerned true nature s  whether kind physical meas 

reason  pattern classification tasks separate models typically built class conditional density 

   

firuiz  lpez de teruel   garrido

urement subjective judgment location z attribute space  need
update knowledge z  form posterior p z s   relative likelihood function
p s z  observed s  general  p s z  nonnegative multivariable function fs z 
domain  objective case  statistical studies measurement process used
determine likelihood function  subjective case  may obtained standard distribution elicitation techniques  berger        either case  mfgn framework  likelihood function available information used inference process approximated  desired degree accuracy  sum products generalized normals 

p  s  z     p s   r   p  r   z     p s   r   p  srj   z j  
r

r

 

t  z
r

j

j

  srj   rj  

   

j

r

without loss generality  available knowledge structured weighted disjunction  
 
 
  s   s      r r   conjunctions sr     sr sr     srn   elementary uncertain
observations form generalized normal likelihoods t  z j   srj   rj   centered srj
uncertainty rj   measurement process interpreted result r  objective subjective  sensors sr   providing conditionally independent information p  srj   z j   attributes
 each srj depends z j   relative strength r   note complex uncertain information instance z  expressed nested combination elementary uncertain beliefs srj
z j using probabilistic connectives  ultimately expressed structure      or translates addition  translates product product two generalized normals
attribute becomes single  weighted normal  
example    consider hypothetical computer vision domain example    assume
information object z following  area around distance around b or 
likely  shape surely triangular else circular area around c angle
around equal e  structured piece information formalized as 

t z   a    t z   b    
          t z  triang    t z  circ   t z   c     t z   d    t z  e    

p s z        

 

 



 

b

 

 

 

c

 



which  expanded  becomes mixture   factorized components operationally represented
parameters shown table   
simpler situation  available information z could conjunction uncertain attributes similar  color   red     green       area          shape   rectangular    
circular     triangular       likelihood shape values obtained output
simple pattern classifier  e g  k n nearest neighbors  moment invariants  attributes
color area directly extracted image  case could interested
distribution values attributes texture objecttype  alternatively 
could start  objecttype   door     window       texture   rough  order determine probabilities color angle values selecting promising search region 

   

fiprobabilistic inference uncertain data using mixtures

r

sr     r

sr    r 

sr     r

sr     r

sr     r

sr     r

sr     r

   
a 
  
b  b
  
  
  
   
triang   
c  c
  
  
  
  
   
triang   
c  c
  
  
  
  
   
circ   
c  c
  
  
  
  
   
circ
 
 
c  c
  
  
  
  
table    parameters uncertain information model example   

  
d d
e   
d d
e   

    joint model observation density
generic dependence structure fig    implemented mfgn framework shown
fig     upper section figure model nature  obtained previous learning stage
used inference without changes  dependences among attributes conducted
intermediary hidden latent component ci  lower section represents available
uncertain information  measurement model query structure associated particular inference operation 

ci

p  z     ci  
 

domain 

p  z  p ci   p  z j   ci  
 

z

z

s  

s  

   

zn

   

s n

j



p  s     z   
s  

s  

   

s n

s 

   

s r

sr 

   

srn

sr

s 
p s s  

measurement 
p  s  z   p s   r   p  srj   z j  



r

j

figure    structure mfgn model  attributes conditionally independent 
measurement process modeled collection independent virtual sensors
p  srj   z j    

joint density relevant variables becomes 

p ci   z   sr       p s   r   p  r   z   p  z  ci   p ci  

p 

  p ci   p s   r  

j
r

  z j   p  z j   ci  

j

  pi r

t  z

j

  srj   rj   t  z j   ij   ij  

j

   

    

firuiz  lpez de teruel   garrido

derive alternative expression eq       convenient computing marginal densities desired variable  using following relation 

p  srj   z j   p  z j   ci     p  z j   srj   ci     p  z j   srj   ci   p  srj   ci  
properties          define dual densities model 

ij r p  srj   ci    



zj

p  srj   z j   p  z j   ci   dz j  t  srj   ij   ij r  

    

p  srj   z j  
p  z j   ci     t  z j   ij r   ji  r  
p  srj   ci  

    

ij r   z j   p  z j   srj   ci    

parameters ij r   ij r ji  r given by 

ij r   ij         rj    
  ij     srj     rj     ij

 ij r    
j
 r



j
 r

ij rj
j
 r

ij r likelihood r th elementary observation srj j th attribute z j
component ci ij r   z j   effect r th elementary information srj j th attribute z j marginal component p  z j   ci   component ci   using notation  mfgn model structure conveniently written as 

p ci   z   sr       pi r ij r ij r   z j  

    

j

    posterior density
inference process available information combined model domain
update knowledge particular object  given new piece information must compute posterior distribution p  y    desired target attributes z  then  estimators
    obtained p  y    minimize suitable average loss function 
efficiently supported mfgn framework regardless complexity domain p z 
structure available information     r sr    
attributes partitioned two subvectors z    x  y       z   desired
target attributes x     z   rest attributes  accordingly  component sr
available information partitioned r     rx   ry     information target attributes
r th observation  independent model p z   denoted sry  often missing
pieces information  srx represents information rest
attributes x  using convention write 

   

fiprobabilistic inference uncertain data using mixtures

p  z       p  x         pi r  r  r   x    r    
 r

 r likelihood r th conjunction sr component ci  

 r ij r

    

j

terms ij r   z j   grouped according partition z    x  y  

 r   x   io r   z  

 r     id r   z  





desired posterior p y s    p y s    p s  computed joint p z s  marginalization  along x obtain p y s  along z obtain p s   note univariate marginalization p z s  along attribute z j eliminates terms ij r   z j   sum      

p        p  x       dx   pi r  r  r    
x

 r

p      p  z     dz   pi r  r
z

 r

therefore  posterior density compactly written as 

p  y       r  r    

    

 r

 r probability object z generated component ci elementary information sr true  given total information s 

 r p ci   sr   s   

pi r  r

p

k

l k  l

    

k  l

 r       p  y  sry   ci   marginal density p  y  ci   desired attributes i th
component  modified contribution associated sry   since p  y  sry   ci    
p  y  sr   ci     expression      follows expansion 

p  y      p  y  sr   ci   p ci   sr   s 
 r

summary  joint density likelihood function approximated mixture
models proposed structure  computation conditional densities given events arbitrary geometry notably simplified  factorized components reduce multidimensional integration
simple combination univariate integrals conjugate families avoid numeric integration 
property illustrated fig    

   

firuiz  lpez de teruel   garrido





p y c  

 y

e y s 

c 

p y x  x  

 y

c 

p y c  
p x  c  

p y s 
p x  c  

         
 

p x  c  

    

x 

p x  c  

    

    

s 
s 

x

 



figure    graphical illustration essential property mfgn framework  consider mse estimate y  conditioned event  y  x   x   cylindrical
region s  required multidimensional integrations computed analytically terms
marginal likelihoods ji r associated attribute pair components
ci sr models p y  x   x   s  respectively  case i r y  p y ci 
information supplied s 

example    fig     a shows joint density two continuous variables x y  modeled
mixture    factorized generalized normals  fig     b shows likelihood function
event s      x x  y  y     fig     c shows posterior joint density
p x y s    fig     d shows likelihood function event s      x y        x   
fig     e shows posterior joint density p x y s    fig     f    g show respectively
posterior marginal density p x s   p y s    complex inferences analytically computed mfgn framework  without numeric integration 

   

fiprobabilistic inference uncertain data using mixtures

 a 

 b 

 c 

 d 
 e 
figure     illustrative examples probabilistic inference arbitrary uncertain information mfgn framework  see example    

   

firuiz  lpez de teruel   garrido

p y s 

   

   

   

   

   

   

   

p x s 

   



x

  

  

 

 

  

  

 

 f 
figure      cont   

 

 g 

    expressions estimators
approximations optimum estimators easily obtained taking advantage
mathematically convenient structure posterior density  mfgn framework 
conditional expected value function g y  becomes linear combination constants 

e g      s    g     p  y    dy  


 


 r



 r

g      r     dy  



 r

e  r  g     

    

 r

e  r  g      e g      ry   ci   expected value g y  i th component  modified  r th observation sry  

e  r  g     





g     t  z   id r   di  r   dy


analytically compute desired optimum estimators  instance  mse estimator single continuous attribute   z requires mean values e  r  z     id r  

mse       e  y  s     r id r
 r

explicit expression p y s  compute conditional cost 

 

 

e  mse       e   mse           e y     s   mse      
 


 r

 r

 

  


 r

       
 


 r

 

 

 



 r id r
 r


 

note computing conditional expected value arbitrary function g y  several variables may difficult 
general g y  expanded power series obtain e g y  s  terms moments p y s  
 
sx  there information target attributes  constants ei r g y   precomputed
model nature p z  learning stage 

   

fiprobabilistic inference uncertain data using mixtures

therefore  given s  tchevichev inequality answer mse      e mse    
confidence level      shape p y s  complex must reported explicitly  the point estimator mse     makes sense p y s  unimodal  
example     nonlinear regression  fig     shows mixture components regression
lines  with confidence band two standard deviations  obtained simple example
nonlinear dependence two variables  case joint density adequately
approximated     components  mse    component           mse    comp           
mse    comp            mse    comp           

 a 
 b 
figure     nonlinear regression example   a    components   b    components 

target symbolic must compute posterior probability value 
case di  r     id r   id possible values taken   z   collecting together id r          eq       written as 

p   y       r        


 r

 r   coefficients impulses located   posterior probability
value is 

q p      s     r  
 r

instance  minimum error probability estimator  ep  is 

ep       argmax



q

desired rejection threshold easily established  reject decision entropy posterior  h   q log q  estimated error probability  e      max q 
high 
example     nonparametric pattern recognition  fig     shows bivariate data set elements two different categories  represented value additional symbolic attribute  joint density satisfactorily approximated   component mixture  fig 
   a   decision regions rejection threshold set     shown fig     b 

   

firuiz  lpez de teruel   garrido

note statistical pattern classification usually start  implicit explicit  approximation class conditional densities  contrast  start joint density 
class conditional densities easily derived  fig     c  

 a 
 b 
 c 
figure     simple nonparametric  feature pattern recognition task  attribute
joint mixture model   a  feature space mixture components   b  decision boundary 
 c  one class conditional densities 

computation optimum estimators loss functions straightforward  observe estimators based combination different rules  weighted degree
applicability  typical structure used many decision methods  case  since
components joint density independent variables rules reduce constants 
simplest type rule 
    examples elementary pieces information
important types elementary observations srj z j shown  including corresponding likelihoods ij r modified marginals ij r   z j    j d  required expression      
exact information  srj   z j   observation modeled impulse 

p  srj   z j    

t  srj   z j       srj z j     therefore 
ij r   t  srj   ij   ij  
ij r   z j     t  z j   srj  
contribution ij r exact information input attribute z j standard likelihood p  z j   ci   observed value z j component  hand  acquire
exact information target attribute z j  when one  r    elementary observation
j   z j   inference process trivially required  p  z j         z j j    
gaussian noise bias rj standard deviation rj   observation modeled  component mixture  p  srj   z j     t  srj   z j   rj   rj     expressed     confidence interval z j srj   rj   rj   property       

   

fiprobabilistic inference uncertain data using mixtures

ij r   t  srj   ij rj     ij         rj      
effect noisy input z j srj   rj equivalent effect exact input z j   srj
mixture components larger variance  ij   ij         rj       uncertainty spreads
effect observation  increasing contribution distant components 
example     fig     a shows simple two attribute domain approximated   component
mixture  interested marginal density attribute x given different degrees uncertainty input attribute       modeled

p        t              

sharpest density  a  fig     b  providing x            obtain density
 b  x      finally       obtain density  c  x      obviously  uncertainty increases  uncertainty x  expected value x moves towards
distant components  become likely probability distribution expands  situation interesting effect appears  mode marginal density
change rate mean  uncertainty skews p x   effect suggests
optimum estimators different loss functions equally robust uncertainty 


b
c

 a 
 b 
figure     effect amount uncertainty  see text    a  data set   component
model   b  p x   uncertain ys around      

j
j
output role   r   z   becomes original marginal  modified location disper 
 
 
sion towards srj according factor     ij        ij       rj       quantifies relative
importance observation 

ij r   z j     t z j     srj rj          ij        rj  
missing data  information j th attribute  srj    z j        observation modeled p  srj   z j     constant or  equivalently  p  srj   z j     t  srj     b 
arbitrary b   components contribute weight 

ij r   p  z j   anything  ci   constant  
target missing ij r   z j   reduce original marginal components 

   

firuiz  lpez de teruel   garrido

ij r   z j     t  z j   ij   ij     p  z j   ci  
arbitrary uncertainty  general  unidimensional relative likelihood function approximated mixture generalized normals  shown example    ij r ij r   z j  
given respectively eqs            
intervals  useful functions cannot accurately approximated small number normal
components  typical example indicator function interval  used model uncertain
observation values equally likely  srj    z j  a  b     z j considered
input  use shortcut ij r   j  b  j  a     j   z j   cumulative distribution normal marginal component p  z j   ci     unfortunately  expression ij r   z j    
required z j considered output  may useful computing certain optimum estimaj
j
j
tors   r   z   restriction p z j ci  interval  a b  normalized  r  
disjunction conjunction events  finally  standard probability rules used build
structured information simple observations  subjective judgments objective evidence ascribe relative degrees credibility rj several observations srj z j   overall
j
j j
likelihood becomes   r r  r   particular  j    z j     z j       two

possibilities equiprobable ij   p      ci     p      ci     analogously  conjunctions
events translate multiplication likelihood functions 
    summary inference procedure
domain p z  adequately modeled learning process  as explained section
    system enters inference stage new  partially specified objects  parameters
domain p z    pi   ij ij   parameters model observation p s z 
  r   srj rj    must obtain parameters ij r   id r di  r desired marginal posterior densities estimators  inference procedure comprises following steps 


compute elementary likelihoods ij r   using eq       



obtain product  r conjunction sr component ci   using eq       



normalize pi r  r obtain coefficients  r posterior  using eq       



choose desired target attributes     z   compute parameters id r  

di  r modified component marginal densities id r   z   using eq       


report joint posterior density y  show graphs posterior marginal densities
desired attributes z using eq        provide optimum  point  interval  etc   estimators using eq       

example     iris data  inference procedure illustrated well known iris benchmark      objects represented four numeric features  x  y  z w  one symbolic category u  u   setosa   u   versicolor   u   virginica    whole data set divided
two disjoints subsets training validation  joint density satisfactorily approxi 

   

fiprobabilistic inference uncertain data using mixtures

mated  see section     component mixture  the error rate classifying u validation
set without rejection         fig     shows two projections     examples location mixture components learned training subset  parameters mixture shown table   

 a 
 b 
figure     two views iris examples components joint density mixture model  u   white  u   black  u   gray   a  attributes x   b  attributes z  w 



pi

 
 
 
 
 
 

    
    
    
    
    
    

ix

ix

iy

iy

iz

iz

                             
                             
                             
                             
                             
                             
table    parameters iris data joint density model

iw

iw

    
    
    
    
    
    

    
    
    
    
    
    

p u  ci  p u  ci  p u  ci 

 
 
 
 
 
 

 
    
 
 
 
    

table   shows results inference process following illustrative situations 
case    attribute z known     z      
case    attributes x u known      x         u u    
case    attribute x uncertain     x     
case    attributes x w uncertain      x      w         note uncertainty decreases information supplied  compare case    
case    structured query expressed terms logical connectives uncertain elementary
events       z      z        u   u    u   u     

   

 
    
    
 
 
 

firuiz  lpez de teruel   garrido

case
 
 
 
 
 

input
output
input
output
input
output
input
output
input


output

x
 
      
   
   
  
      
  
      
 
 


 
      
 
      
 

z
   
   
 
      
 

w
 
      
 
      
 

      
 

      
 

      
 
 

      
    
      
 
 

      

      

      
  
  
  

 approx 
unimodal 

 unimodal 

 bimodal 

    

u
 

u       u      
u       
u       
 
u       u      
 
u       u     
u       u      
u       u      
u       u      

 bimodal 

table    inference results iris domain

consistency results visually checked fig      finally  table   shows
elementary likelihoods


 
 
 
 
 
 

ix  

iy  

i j r case    illustrating essence method 

iz  

iw  

ui   

  

ix  

 
 
 
 
 
    
 
 
 
 
 
    
       
 
 
 
 
    
       
 
 
 
 
    
       
 
 
 
 
    
       
 
 
 
 
 
    
 
table    elementary likelihoods case   table   

iy  
 
 
 
 
 
 

iz  

iw  

ui   

  

    
    
    
 e  
 e  
    

 
 
 
 
 
 

 
   
   
   
   
 

 
   
   
   
   
 

    independent measurements
one key features mfgn framework ability infer arbitrary relational
knowledge attributes  form likelihood function adequately approximated
mixture model structure eq       instance  could answer questions as  happens z z tends less z j    i e   p s z  high region z z j       
however  situations observations single attribute z j statistically
independent  information attributes  e g  z around z j around b 
attribute relations  pay attention particular case illustrates role
main mfgn framework elements  furthermore  many practical applications satisfactorily solved assumption independent measurements judgments  case 
likelihood available information expressed conjunction n marginal observations j z j  

p     z     p  j   z j  
j

   

    

fiprobabilistic inference uncertain data using mixtures

means sum products equation     complete  i e   includes elements
n fold cartesian product attributes 

p    z     rj t  z j   srj   rj  
j

r

j rj   r   factored likelihood function considered   component
mixture  with r       j s j   marginal observation models allowed
mixtures generalized normals  p  j   z j     r   rj  t  z j   srj    rj       case even think
 
 
function valued attributes z   f   z        f n   z n      f j   z j   p  j   z j   models

range relative likelihood values z j   loosely speaking  attributes concentrated
f j   z j   may considered inputs  attributes high dispersion play role outputs 
since conditionally independent x given ci   posterior obtained expansion 

p  y      p  y    ci   p ci   s    p  y  ci     p ci   s 


    



interpretation      straightforward  effect sx    zd  must computed
x    zo  components ci   then  simple bayesian update p  y  x   new
prior made using  see fig      

ci
z 



p  z j   ci  

zd
id   z  

s 

   


   

sd

j


zj
p  j   z j  

sj

   

figure     structure mfgn inference process independent pieces information  case  likelihood function factorizable  data flow inference process shown dotted arrows 

   

firuiz  lpez de teruel   garrido

   learning uncertain information
previous section  described inference process uncertain information
mfgn framework  develop learning algorithm model domain 
training examples uncertain  specifically  must find parameters pi  

ij   ij  or ti j    mixture structure     approximate true joint density p z 
training i i d  random sample  z k    k    m  partially known associated likelihood
functions  s k   structure     
    overview em algorithm
maximum likelihood estimates parameters mixture models usually computed
well known expectation maximization  em  algorithm  dempster  laird rubin       redner
walker       tanner        based following idea  principle  maximization
training sample likelihood j   k p  z   k     mathematically complex task due product
sums structure  however  note j could conveniently expressed maximization components generated example known  this called complete data em terminology   underlying credit assignment problem disappears estimation task reduces several uncoupled simple maximizations  key idea em following  instead maximizing
complete data likelihood  which unknown   iteratively maximize expected value
given training sample current mixture parameters  shown process
eventually achieves local maximum j 
instead rigorous derivation em algorithm  found references  see especially mclachlan krishnan         present heuristic justification
provides insight generalizing em algorithm accept uncertain examples  review
first simplest case  missing uncertain values allowed training set 
parameters mixture conditional expectations 

e z  ci  g   z    ci      g   z   p  z  ci   dz
z

    

 
 
particular  ij   e z j   ci       ij     e   z j ij     ci   ti j    e i  z j      ci    

mixture proportions pi   e  p ci   z     
rewrite conditional expectation      using bayes theorem form unconditional expectation 

e z  ci  g   z    ci      g   z   p ci   z  p  z     p ci   dz  

    

  e z  g   z   p ci   z     pi

    

z

em algorithm interpreted method iteratively update mixture parameters
using expression      form empirical average training data   starting
tentative  randomly chosen set parameters  following e steps repeated
 

expression      used iterative approximation explicit functions indirectly known
i i d  sampling  e g   subjective likelihood functions sketched human user  example     case p z  set
target function p ci  z  computed current mixture model 

   

fiprobabilistic inference uncertain data using mixtures

total likelihood j longer improves  the notation  expression  k  means  expression  computed parameters example z k   
   
   
 e  expectation step  compute probabilities qi k p ci   z k   k th example
generated i th component mixture 

qi  k   p  z   k     ci   p ci     p  z   k    
 m  maximization step  update parameters component using examples  weighted
probabilities qi  k     first  priori probabilities component 

pi

 
q  k  
k

then  continuous variables  mean values standard deviations component 

 
mpi

ij
  ij    

 
mpi

 q

z j    k  



k

 q

  z j        k     ij    



    

k

symbolic variables  probabilities value 

ti j 

 
mpi

 q

 z j       k  



k

    extension uncertain values
general  mfgn framework know true values z j attributes
training examples  required compute g   z   p ci   z   empirical  expectation       instead 
start uncertain observations   k   true training examples z
likelihood functions expressed mixtures generalized normals 

 k  

  form

p    k   z   k       p s   k   sr  k     p  sr  k   z   k    
r

therefore  must express expectation      p z  unconditional expectation
p s   distribution generates available information training set 
easily done expanding p  z  ci   terms s 

e z  ci  g   z    ci      g   z   p  z  ci   dz
z

  p   z     c   p     c   ds   dz
    g   z   p  z    c   dz   p c   s  p    ds   p c  
 



z



z

g   z 











define

   



    

firuiz  lpez de teruel   garrido

    e z  s  ci  g   z      ci      g   z   p  z  ci   p    z   dz   p    ci  
z

parameters p z  finally written  unconditional expectation observable p s  form similar eq       

e z  ci  g   z    ci      e  i     p ci   s     pi

    

expression justifies extended form em algorithm iteratively update parameters p z  averaging     p ci   s  available training information     k    
drawn p s   considered numerical statistical method solving p z 
integral equation 



z

p     z   p   z   dz   p    

note cannot approximate p s  fixed mixture terms p    ci   computing back corresponding p  z  ci   because  general  p    z   different different training examples  reason  elementary deconvolution methods directly
applicable 
kind problem addressed vapnik              perform inference result
indirect measurements  ill posed problem  requiring regularization techniques 
proposed extended em algorithm considered method empirical regularization 
solution restricted family mixtures  generalized  gaussians  em
proposed kaveh        regularization context image restoration 
interpretation      straightforward  since know exact z required
approximate parameters p z  empirically averaging g   z   p ci   z    obtain
result averaging corresponding     p ci   s  domain      plays
role g z        z uncertain  g z  replaced expected value component
given information s  particular  exact knowledge training set at   
tributes     k     z k   i e   r     marginal likelihoods impulses       reduces
      fig      illustrates approximation process performed extended version em
algorithm simple univariate situation 
convenient develop version proposed extended em algorithm uncertain
training sets  structured tables  sub cases  uncertainly valued  variables  see fig      
first  let us write eq       expanding terms components sr 

p  z  ci     p ci       p  z   ci    
 

p  z  c     p s   s    p  z  c     p c     p s   s 


r

r

r



r



r

r

r

therefore

    p ci   s     r   r   p ci   r   s 
r

result obtained relation ez w z     es  ez s w z   s    w z  g z  p ci z  bayes theorem 

 

   

fiprobabilistic inference uncertain data using mixtures

 a 

 b 
 d 

 c 

figure     extended em algorithm iteratively reduces  large  difference
 a  true density p z    b  mixture model p   z     indirectly  small 
discrepancies  c  true observation density p s   d  modeled observation density p       real cases p s  must estimated finite i i d  sample
 s k   

s   

s    
s    
s   
s    
s    
s    
   

  
  
 
  
  
  
   

s   
s   

 r k  

s r 

  sr     r     k  

  srj   rj     k  

   

   

   
   
figure     structure uncertain training information extended em algorithm  coefficients

 r k   normalized easy detection rows included

uncertain example  z



 k  

   

 k 

 k 

uncertain 

reduces single row

    
j

using notation introduced      

 r   r   e z  sr  ci  g   z    r   ci    



z

g   z   p  z  r   ci   dz  



z

p ci   r   s    p ci   r   p s r   s     r
write      as 

   

g   z   ij r   z j   dz
j

firuiz  lpez de teruel   garrido



e z ci  g  z     ci     e  r g  z   ij r   z j  dz   pi
z
j
r

mfgn framework contributions  r   r   p ci   r   s  empirical expected
values required extended em algorithm obtained without numeric integration 
need consider case g z    z j compute means ij probabilities ti j   
g z      z j    deviations ij        already know explicit expression parameters ij r   z j     t  z j   ij r   ji  r     hence 



z

z j  r   z   dz  

 z
z

j



z

z j ij r   z j  dz j   ij r

     r   z   dz     ij r         ji  r    

conclusion  steps extended em algorithm follows 
 e  expectation step  compute elementary likelihoods training set 

 

ij r  k     srj   ij     ij         rj    

 

 k  

    

   
   
obtain likelihood conjunction sr k example k component ci 

i  kr    ij r  k  
j

   
obtain total likelihood example k  

  k   p    k       pi r  k   i  kr  


r

   
   
   
compute probabilities qi  kr p ci   r k   k   r th component k th exam 

ple generated i th component mixture 

qi  kr   i  kr    pi r   k   i  kr      k  
 m  maximization step  update parameters component ci using components

r  k   examples weighted probabilities qi  kr     first  prior probabilities
component 

pi

 


q
k

 k  
 r

r

mean value standard deviation component 

ij

 
mpi

 q
k

r

   

 r

ij r

 

 k  

fiprobabilistic inference uncertain data using mixtures

  ij    

 
mpi

 q
k

 r

 

   ij r         ij  r      

r

 k  

  ij    

    

symbolic variables representation     may use 

ij r  k     p srj      k   ti j 

    



ti j 

 
mpi

 q
k

 r

p srj     j    ij r

r

 

 k  

    

consider particular case attributes training examples contaminated
unbiased gaussian noise  likelihood uncertain observations modeled     
   
   
   
   
     
component mixtures  p  k   z k     j t  z j k   j k   j k       j k   variance
measurement process z j   k   obtains observed value j   k     ex   
   
   
pressed confidence interval z j k j k   j k   case  basic em algorithm     
   
easily modified take account effect uncertainties j k   e step  com   
pute qi k using following deviations 

ij   ij         j   k      
and  step  apply substitution 

z j   k   j   k          ij

 

  z j   k       j   k          ij

       
 

j  k    



  ij    
  j  
        j   k      
measures relative importance observed j k computing new ij ij  
previous situation illustrates missing values must processed learning stage 
   
j  k  
z
exact j k          original algorithm      changed 
   
extreme  z j   k   missing  modeled j k   get     therefore
   
observation j k contribute new parameters all  correct procedure deal
missing values mfgn framework simply omitting empirical averages 
note fact arises factorized structure mixture components  providing conditionally independent attributes  alternative learning methods require careful management
missing data avoid biased estimators  ghahramani   jordan       
   

    evaluation extended em algorithm
studied improvement parameter estimations uncertainty observations  modeled likelihood functions  explicitly taken account  proposed extended

   

firuiz  lpez de teruel   garrido

em compared em algorithm  raw  observations  basic em   ignores
likelihood function typically uses average value  e g   given x    basic em uses
x     considered synthetic   attribute domain following joint density 
p x y w         x       y       w white 
       x       y       w black 

tt

tt

tt

different learning experiments performed varying degrees uncertainty 
cases training sample size      trained models structure true density    components   since goal experiment measure quality estimation
respect amount uncertainty  without regard sources variability
local minima  alternative solutions  etc   empirically studied section    table   shows
mixture parameters obtained learning algorithms  fig     graphically shows difference extended basic em illustrative cases 
case    exact data  fig     a  
cases    results extended em learning algorithm   rate missing
values training data 
case    basic em attribute biased    units probability      case    extended em
algorithm case    see fig     b   here  observed value sy y       samples
sy y rest  samples  basic em uses observed value sy extended em uses
explicit likelihood function f y         ysy         y sy    
case    basic em attributes x gaussian noise       w changed
probability      case    extended em algorithm case   
case    basic em x gaussian noise     w changed probability
     case    extended em algorithm case    see fig     c  
case    basic em x gaussian noise     w changed probability
     case    extended em algorithm case    see fig     d  



case    extended em values y   missing  censoring   case     extended em case
  missing values assumed distributed
 y         providing additional information data generation mechanism 
table   fig     confirm small amounts deterioration relation sample
size  estimates computed basic em algorithm raw observed data similar
obtained extended em algorithm  e g   cases       however  data sets
moderately deteriorated true joint density correctly recovered extended em using
likelihood functions attributes instead raw observed data  e g   cases      fig 
   c   finally  large amount uncertainty respect training sample
size true joint density cannot adequately recovered  e g   cases      fig     d  

   

fiprobabilistic inference uncertain data using mixtures

 a 

 b 

 c 
 d 
figure     illustration advantages extended em algorithm  see text    a 
case    exact data    b  cases      biased data    c  cases      moderated noise  
 d  cases      large noise   figures show true mixture components  gray ellipses   available raw observations  black white squares   components estimated basic em raw observations  dotted ellipses  components estimated extended em taking account likelihood functions uncertain
values  black ellipses  

note ability learn uncertain information suggests method manage non
random missing attributes  e g   censoring   ghahramani   jordan       complex
mechanisms uncertain data generation  illustrated case    missing data generation
mechanism depends value hidden attribute  correct assign equal likelihood
components  principle  statistical studies kind knowledge may help ascertain
likelihood true values function available observations  instance  case   
replaced missing attributes case   normal likelihoods     i e  high   improving estimates mixture parameters 

   

firuiz  lpez de teruel   garrido

case

p 

 x

 y

 x

 y

t  wwhite

 x

 y

true
  
 
 
 
 
 
 
 
 
                                     
   
                                     
   
                                    
   
                                    
   
                                     
 
       
                            
 
                                    
 
       
                           
 
                     
             
 
                                     
 
                                     
 
                                    
 
       
                           
 
                                     
  
       
                            
table    parameter estimates uncertain information  see text 

 x

 y

t  wblack

 
    
   
   
    
    
   
    
    
    
    
   
    
    
    
    

 
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

 
    
    
    
    
    
    
    
    
    
   
   
   
   
    
    

example     learning examples missing attributes performed iris
domain illustrate behavior mfgn framework  whole data set randomly
divided two subsets equal size training testing    component mixture models
obtained evaluated  combining missing data proportions         error
prediction attribute u  plant class  following 

missing attributes

training set
  
  
   
   

test set
  
   
  
   

prediction error
    
     
    
     

relatively simple iris domain  performance degradation due     missing attributes much greater inference learning stage  extended em algorithm able
correctly recover overall structure domain available information 

    comments
convergence em algorithm fast  requiring adjustable parameters learning
rates  algorithm robust respect random initial mixture parameters  bad local
maxima frequent alternative solutions usually equally acceptable  examples
contribute components  never wasted unfortunate initialization  fixed
number components  algorithm progressively increases likelihood j training data
maximum reached  number components incremented maximum j
increases  limit value obtained cannot improved using extra components  fukunaga        simple heuristics incorporated standard expectation maximization
scheme control value certain parameters  e g   lower bounds established variances  quality model  e g   mixture components eliminated proportions
small  
case  factorized components specially convenient matrix inversions
required and  important  uncertain missing values correctly handled

   

fiprobabilistic inference uncertain data using mixtures

simple unified way  heterogeneous attribute sets  necessary provide models
uncertain attribute correlations since covariance parameters must estimated  finally 
training sample size must large enough relation degree uncertainty examples complexity joint density model order obtain satisfactory approximations 
hand  number mixture components required satisfactory approximation joint density must specified  pragmatic option minimization experimental estimation cost main inference task  exists  instance  regression
could increase number components acceptable estimation error obtained
independent data set  cross validation   idea applies pattern classification  use
number components minimizes error rate independent test set  however  one
main advantages proposed method independence learning stage
inference stage  freely choose dynamically modify input output role
attributes  therefore  global validity criterion desirable  typical validation methods
mixture models reviewed mclachlan   basford         standard approach based
likelihood ratio tests number components  unfortunately  method validate
mixture itself  selects best number components  desoete       
since mfgn framework provides explicit expression model p z   apply
statistical tests hypothesis independent sample taken true density  e g  subset examples reserved testing  find obtained approximation compatible
test data  hypothesis h    t comes p z   rejected  learning process must
continue  possibly increasing number components  difficult build statistical
tests  e g  moments p z   sample means variances directly obtained 
however  data sets usually include symbolic numeric variables  developed
test expected likelihood test sample  measures well p z  covers examples  mean variance p z  easily obtained using properties generalized
normals  experiments simple univariate continuous densities show test
powerful small sample sizes  i e  incompatibility always detected  standard tests significantly evidence rejection  nevertheless  clearly inaccurate approximations
detected  results improve sample size increases test valid data sets uncertain values 
minimum description length  li   vitnyi       principle invoked select
optimum number components trading off complexity model accuracy
description data 

   

firuiz  lpez de teruel   garrido

   discussion experimental results
    advantages joint models
inductive inference methods compute direct approximation conditional densities
interest  even obtain empirical decision rules without explicit models underlying conditional densities  cases  model learning stage depend selected input  
output role variables  contrast  presented inference learning method based
approximation joint probability density function attributes convenient
parametric family  a special mixture model   mfgn framework works pattern completion
machine operating possibly uncertain information  example  given pattern classification
problem  learning stage suffices predicting class labels feature vectors
estimating value missing features observed information incomplete patterns 
joint density approach finds regions occupied training examples whole attribute
space  attribute dependences captured higher abstraction level one provided
strictly empirical rules pre established target variables  property extremely useful
many situations  shown following examples 
example     hints provided inference multivalued relations  given data
set model example     assume interested value x     
obtain bimodal marginal density shown fig     a corresponding estimator x
        is  sense  meaningless  however  specify branch interest
model  inferring     x      i e   x small   obtain unimodal
marginal density fig     b reasonable estimator x        

 a 
 b 
figure     desired branch multivalued relations selected providing
information output values   a  bimodal posterior density inferred
y     b  unimodal posterior density inferred     hint x small 

example     image processing  advantages joint model supporting inferences
partial information inputs outputs illustrated following application
natural data  see fig       image fig     a characterized   attribute density
 x  y  r  g  b  describing position color pixels  random sample      pixels
used build     component mixture model  interested location certain ob 

   

fiprobabilistic inference uncertain data using mixtures

jects image  figs     b f show posterior density desired attributes given following queries 


 something light green   fig     b  two groups easily identified posterior
density  corresponding upper faces green objects     c   x  unknown 
r        g        b        



 something light green dark red   fig     c  find groups
additional  scattered group  corresponding red object  greater dispersion
arises larger size red object fact r component
dark red disperse g component light green   two equiprobable components c  c   x  unknown  r        g b       



 something light green right   fig     d  provide partial information
output    c   x        unknown  r        g        b       



 something white   fig     e    c   x y unknown  r        g        b       



 something white  lower left region  main diagonal  y     x    fig     f 
provide relational information attributes modeled     equiprobable components  note case posterior distribution contains     components  still computationally manageable   

 x       y        r        g        b        
 x       y        r        g        b        
 x       y       r        g        b        
 x        y        r        g        b        
 x        y       r        g        b        
 x        y       r        g        b       
cases  posterior density consistent structure original image  time
required compute posterior distribution always lower one second  learning time
order hours pentium     system  simpler models     component  obtained
     random pixels  produced acceptable results much lower learning time  furthermore  em algorithm efficiently parallelized 

hand  large number irrelevant attributes  joint model strategy wastes resources capture proper probability density function along unnecessary dimensions   this problem arise specification likelihood function  since relevant attributes explicitly appear model   joint modeling appropriate domains
moderated number  meaningful  variables without fixed input   output roles 

 

note sharp peak  a component small dispersion  obtained learning process   transmits 
posterior density  kind artifacts inocuous easily removed post processing 

   

firuiz  lpez de teruel   garrido

 a 

 b 

 c 

 d 

 e 
 f 
figure     inference results image domain example      a  source image 
 b  posterior density attributes x y given  something light green    c 
 something light green dark red    d   something light green right  
 e   something white    f   something white  lower left region 
main diagonal image  y     x  

    advantages factorization
proposed methodology supported general density approximation property mixture
models  use components independent variables order make computations feasible

   

fiprobabilistic inference uncertain data using mixtures

inference learning stage  factorized components imposed mixture model without loss generality  statistical dependence variables still captured  cost
possibly larger number components mixture achieve required accuracy
approximation 
simplicity building block structure entirely compensated important saving computation time  high dimensional integrals analytically computed univariate
integrals matrix inversions avoided learning stage  additionally  high dimensional
domains easily modeled using small number parameters mixture component 
viewpoint computational learning theory  vapnik        models small number adjustable parameters  actually  low expressive power  favorable consequences
generalization 
mixtures factorized components used latent class analysis  desoete       
well known unsupervised classification technique  assumed statistical dependences
attributes fully explained hidden variable specifying latent class
example  method similar gaussian decomposition clustering algorithm mentioned
section    constrained component conditional attribute independence  however  goal
unsupervised classification obtaining accurate mathematically convenient expression
joint density variables  required derive desired estimators  meaning
components irrelevant  long whole mixture good approximation joint density 
expressive architectures  combine mixture models local dimensionality reduction  considered  mixtures linear experts  jordan   jacobs        mixtures
principal component analyzers  sung   poggio       mixtures factor analyzers  ghahramani   hinton       hinton  dayan    revow        unfortunately  general kind inference learning uncertain data considered work cannot directly incorporated
architectures computational advantages demonstrated mfgn model 
restriction factorized components may produce undesirable artifacts approximations certain domains learned small training samples  nevertheless  problem always
occurs approximator structure building block match shape
target function  case  many terms  or components  units  etc   required good
approximation associated parameters correctly adjusted large training
sample  however  note complexity model measured uniquely terms
number mixture components  number adjustable parameters probably better
measure complexity  instance  full covariance models show quadratic growth number free parameters respect dimension attribute vector  factorized components growth linear  amount training data need unreasonably high even
number mixture components large 
real applications  nature target function unknown  little said priori
best building block structure used universal approximator  chosen
simple component structure make inference learning feasible uncertain information  section     provides experimental evidence realistic problems proposed model
inferior popular approaches 
    qualitative comparison alternative approaches
instead proposed methodology  based mixture models em algorithm  alternative nonparametric density approximation methods could used  either joint density
specific conditional densities   instance  nearest neighbor rule locally approximates
target density using certain number training samples near point interest  symbolic

   

firuiz  lpez de teruel   garrido

attributes directly estimated voting scheme continuous attributes estimated
averaging observed values training instances near  subspace observed
attributes  point interest  however  small sample sizes  estimators
smooth show strong sensitivity random fluctuations training set  penalizes
estimation cost  large sample size  time required find nearest neighbors becomes
long  example  consider regression problem example     section      fig     b shows
mfgn solution   components mse        fig     a shows regression line obtained   nearest neighbors average  higher mse       
parzen windows similar kernel approximation methods used smooth results
simple nearest neighbors rule  duda   hart       izenman        actually mixtures
simple conventional densities located training samples  principle  properties
mfgn framework could adapted kind approximation  ruiz et al         learning
becomes trivial  strong run time computation effort required since concise model
domain extracted training set  kind rote learning negative consequences generalization according occam razor principle  li   vitnyi        adequately cross validated mixture model small number components relation training sample size reasonably guarantees probably true attribute dependencies correctly
captured 

c 

c 
c 

c 

 a 
 b 
 c 
figure     alternative solutions regression classification  see text details  

nature solutions obtained backpropagation multilayer perceptrons  rumelhart et
al        pattern classification illustrative  general  decision region geometrically expressed union intersections several halfspaces defined units
first hidden layer  however  backprop networks often require long learning times  many adjustable parameters and  worse  apparently simple distributions patterns hard learn 
instance  solution circle ring classification problem fig     b  obtained network   hidden units requires hundreds standard backprop epochs  decision regions
satisfactory  even though network extra flexibility task    hidden units
suffice separate training examples   better solutions exist using resources network architecture  backprop learning find them  contrast  solution obtained
mfgn approach using   components  fig     c  requires learning time orders magnitude
shorter backprop optimization  components mixture contribute synthesize
reliable decision regions acceptable solutions obtained smaller number
components 

   

fiprobabilistic inference uncertain data using mixtures

proposed approach closely related well known family approximation techniques
which  essentially  distribute  using kind clustering self organizing algorithm  detectors relevant regions input space combine responses computing
desired outputs  case radial basis functions  rbf   hertz et al    classification regression trees proposed  breiman et al        topological maps used  cherkassky   najafi       locate knots required piecewise linear regression 
relevant methodology proposed  jordan   jacobs       peng et al        
em algorithm used learn hierarchical mixtures experts form linear rules
way desired posterior densities explicitly obtained  properties em algorithm satisfactorily used  ghahramani   jordan       obtain unbiased approximations missing data mixture based framework similar ours  framework extends
successful approach exploiting conjugate properties chosen universal approximation
model  uncertain information arbitrary complexity efficiently processed inference
learning stages 
mfgn framework appropriate moderated number variables showing relatively
complex dependencies  contrast  bayesian networks satisfactorily addresses case large
number variables clear conditional independence relations  situations
certain subset variables bayesian network shows explicit causal structure  subdomain could empirically modeled mixture model order considered later composite node embedded whole network  subdomain conditionally isolated
rest variables set communication nodes  mfgn framework used
perform required inferences 
finally  mixture models typically used unsupervised classification  examples
labeled index component highest posterior probability  fact  mfgn
framework explicitly finds clusters training set  furthermore  continuous symbolic attributes allowed joint density  examples clustered using implicit probabilistic metric automatically weighs  heterogeneous  attributes  even missing
uncertain values  however  method effective groups interest
structure component densities  order simplify inference mixture components
selected constraints  gaussian  independent variables  necessarily verified
natural groups found real applications 
tentative possibility  inspired common heuristic clustering technique  consists joining overlapping components  e g   according battachariya distance  well known bound
bayes error used statistical pattern recognition  fukunaga         unfortunately  experiments indicate overlapping threshold free parameter strongly determines
quality results  universal threshold  independent application  seem exist 
principle  clusters arbitrary geometry may discovered  cannot easily automated 
therefore  nonparametric cluster analysis methods  e g  density valley seeking  suggested
labeling complex groups 
    experimental evaluation
mfgn method evaluated standard benchmarks machine learning database repository university california  irvine  merz murphy        contains inductive learning problems representative real world situations  experimented
following databases  ionosphere  pima indians  monk s problems  horse colic 
illustrate different properties proposed methodology  cases mfgn compared alternative learning methods respect inference task considered interest

   

firuiz  lpez de teruel   garrido

problem  typically  prediction specific attribute given rest them   usually give
error rate training test set indicate amount overfitting obtained
learning algorithms 

 a  ionosphere
 b  pima indians
figure     discriminant  d projections two representative databases 

      ionosphere database
two classes radar returns ionosphere must discriminated vectors    continuous attributes       examples  randomly partitioned two disjoint subsets approximately equal size training testing  prevalence minoritary class  random
prediction rate       figure    a table   show typical statistical pattern recognition problem  easily solvable standard methods  results suggest bayes  optimum 
error probability around    
error rate
pe
 training set 
method
 test set 
linear mse  pseudoinverse 
    nearest neighbor
    nearest neighbor
parzen model
backprop multilayer perceptron   hidden units
support vector machine  rbf kernel  width         s v  
support vector machine  rbf kernel  width        s v  
support vector machine  polinomial kernel  order        s v  
support vector machine  polinomial kernel  order        s v  
support vector machine  polinomial kernel  order        s v  
full covariance gaussian mixture    component class
full covariance gaussian mixture    component class
full covariance gaussian mixture    component class
mfgn   components  average 
mfgn   components  average 
mfgn    components  average 
mfgn  best result cross validation    components 
table    ionosphere database results

 

   

   
   

   
   
    
      
      
      
   

originally database contains    attributes  two them  meaningless ill behaved  eliminated 

   

   
   
   
   
   
   
   
   
   
   
   
   
   
      
      
      
   

fiprobabilistic inference uncertain data using mixtures

problem  plain mfgn method  without special heuristics learning stage 
comparable average alternative methods  best solution training set  crossvalidation  entirely satisfactory 
ionosphere database present exhaustive study performance given varying
proportions missing values training testing examples  value x   means
training test examples value attribute deleted probability x  basic experiment consists learning mfgn model prescribed number components          
computing error rate training test sets  table   shows mean value   standard deviations error rates obtained    repetitions basic experiment configuration  column contains error rate configuration training set  training test partition kept fixed analyze variability solutions due random initialization
em 
learning


  

inference
   
   

   

  comp      
  comp      
   comp      

     
    
    

    
    
    

    
    
    

    
    
    

    
    
    

  comp       
  comp       
   comp       

     
    
    

     
    
    

     
    
    

     
    
    

     
    
    

  comp       
  comp       
   comp       

    
    
  

    
     
    

    
    
     

    
    
    

    
    
    

  comp       
                             
  comp       
                             
   comp            
                       
table    evaluation mfgn ionosphere database given
different proportions missing data training testing subsets 

expected  mfgn model robust respect large proportions missing values
test patterns  moderated proportions missing data training set  compared behavior standard algorithm decision tree construction inspired
 quinlan        able support missing values   table   shows error rates
decision trees experimental setting table    kind decision tree obtains
error rates better averages obtained mfgn  however  mfgn s best solutions
 selected cross validation  better ones obtained decision tree  furthermore 
decision tree performance degrades faster mfgn  especially respect proportion
missing values inference stage 

 

essentially  missing values handled follows  learning stage  attribute selected  examples
missing values sent partitions appropriate weights  inference stage  node asks missing
value  follows branches appropriate weights finally outputs combined 

   

firuiz  lpez de teruel   garrido

learning


  

inference
    
    

    

  
  
  
    
    
    
  
    
    
    
    
    
  
    
    
    
    
    
  
    
    
    
    
    
table    evaluation basic decision tree ionosphere database given different proportions missing data training testing subsets 

      pima indians database
problem must discriminate two possible results diabetes test given pima
indians    continuous attributes      examples  randomly partitioned two disjoint subsets equal size training testing  prevalence minority class     
attribute vector normalized  table    presents comparative results 
error rate
pe
 training set 
method
 test set 
linear mse  pseudoinverse 
oblique decision tree   decision nodes
    nearest neighbor
    nearest neighbor
full covariance gaussian mixture    component class
full covariance gaussian mixture    component class
full covariance gaussian mixture    component class
full covariance gaussian mixture    component class
backprop multilayer perceptron   hidden units
backprop multilayer perceptron   hidden units
backprop multilayer perceptron   hidden units
support vector machine  rbf kernel  width        s v  
support vector machine  rbf kernel  width        s v  
support vector machine  polynomial kernel  order        s v  
support vector machine  polynomial kernel  order        s v  
mfgn   components
mfgn   components
mfgn   components
table     pima indians database results

   
   

   
   
   
   
   
   
   

   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

despite low dimensionality large number examples  classification problem
hard  see figure    b   even sophisticated learners backpropagation networks  decision
trees support vector machines  able store reasonable proportion training set 
achieve significant generalization  mfgn shows similar behavior  although slightly
less prone overfitting  the error rate training set misleading  
      horse colic database
database contains classification task heterogeneous attribute vector including symbolic  discrete continuous variables      missing values  illustrates problem

   

fiprobabilistic inference uncertain data using mixtures

feature selection context joint modeling  mentioned section      table    shows
error rates obtained mfgn using different attribute subsets   take advantage general
inference properties  mfgn model must applied attribute subset interest 
inference task fixed number attributes large  alternative methods
used 
method

pe

pe

 distribution     initializations 

 best 

  selected attributes
mfgn   components
mfgn   components
mfgn   components
mfgn   components
mfgn   components
mfgn   components
mfgn    components
mfgn    components
mfgn    components

      
      
      
      
      
      
      
      
      

   
   
   
   
   
   
   
   
   

      
      
      
      
      
      

   
   
   
   
   
   

      
      
      
      

   
   
   
   

  selected attributes
mfgn   components
mfgn   components
mfgn   components
mfgn    components
mfgn    components
mfgn    components

   selected attributes
mfgn   components
mfgn   components
mfgn    components
mfgn    components
table     horse colic database results  random rate      

      monk s problems
monk s problems three concept learning tasks   symbolic attributes  widely used
benchmarks inductive learning algorithms  thrun et al         seen table     mfgn fails
monk   where acceptable generalization obtained  monk   where training
examples cannot even stored   contrast  mfgn correctly solves monk   behavior
related fact monk s problems based deterministic abstract concepts
may lack kind geometric regularities attribute space required probabilistic models   
 

features individually selected using simple discrimination index related kolmogorov smirnov statistic
 ruiz       
  
typical example parity problem  acceptable off training set generalization cannot achieved inductive
bias learning machine biased towards  smooth  solutions 

   

firuiz  lpez de teruel   garrido

fig     shows discriminant  d projections datasets illustrates fact
monk  cannot easily captured statistic techniques  benchmark  mfgn performance
similar popular probabilistic methods  thrun et al        

 a  monk 
 b  monk 
figure     discriminant  d projections monk s datasets 

 c  monk 

error rate
 training set 

method

pe
 test set 

monk   random rate      
linear mse  pseudoinverse 
    nearest neighbor
support vector machine  rbf kernel  width       s v  
cascade correlation
mfgn   components
mfgn   components

   

   
   

   
   
   
 
   
   

monk   random rate    
linear mse  pseudoinverse 
    nearest neighbor
support vector machine  rbf kernel  width        s v  
cascade correlation
mfgn   components
mfgn   components
mfgn    components

   

   
   
   

   
   
   
 
   
   
   

monk   random rate    
linear mse  pseudoinverse 
    nearest neighbor
support vector machine  rbf kernel  width       s v  
cascade correlation
mfgn   components
mfgn   components
mfgn   components
table     monk s problems results

   

   

   
   
   

   
   
   
   
   
   
   

fiprobabilistic inference uncertain data using mixtures

      comments
experiments demonstrate mfgn model able obtain acceptable results
many real world applications  particular  error rates obtained standard classification tasks
comparable obtained popular learners  additionally  mfgn able perform
inferences attribute given uncertain partial information  possible
alternative methods  property makes mfgn attractive alternative many
inference problems one illustrated example     experiments contributed characterize kind problems mfgn model best suited  essentially 
relationship among attributes must true probabilistic nature  attribute vector must
moderated size containing  relevant  variables  previous feature selection   accommodation
stage recommended certain applications 

   conclusions
developed efficient methodology probabilistic inference learning uncertain information  proposed mfgn framework  joint probability density function
attributes likelihood function available information approximated mixtures factorized generalized normals  mathematical structure allows efficient computation 
without numerical integration  posterior densities expectations desired variables given
events arbitrary geometry  extended version em learning algorithm developed estimate parameters required mixture models uncertain training examples 
different paradigms pattern recognition  regression pattern completion subsumed
common framework 
comprehensive collection examples illustrates methodology  critically
compared alternative techniques  extended em algorithm able learn satisfactory
domain models reasonable number examples uncertain values  taking account
explicit likelihood functions available information  results satisfactory whenever
sample size large relation amount  known  degradation training set  experiments characterized kind situations model manages better  domains described moderate number heterogeneous attributes complex probabilistic dependences 
problems output variables necessarily known learning stage  i e  pattern
completion   and  finally  problems explicit management uncertainty needed  either learning inference stage  or even both   mfgn framework obtained
favorable trade off useful features model complexity solutions different applications benchmarks 
future developments work include improving learning stage heuristic
steps combined standard e steps control adequacy acquired
models  additional studies required validation tests  generalization  scalability  robustness
data preprocessing  essential idea working explicit likelihood functions
incorporated parzen approximation scheme interested expressive
model structures mixtures factor analyzers  principal component analyzers linear experts  finally  methodology developed pure bayesian framework subsumed
dempster shafer evidence theory 

   

firuiz  lpez de teruel   garrido

acknowledgments
authors would thank anonymous reviewers careful reading helpful suggestions  work supported spanish cicyt grants tic         tic       c      tic        c      

references
berger  j           statistical decision theory bayesian analysis  springer verlag 
bernardo  j m   smith  a f m          bayesian theory  wiley 
bouckaert  r r          properties bayesian belief network learning algorithms  proceedings uncertainty ai  pp          
breiman  l   friedman  j h   olshen  r a   stone  c j          classification regression
trees  wadsworth international group  belmont  ca 
chang  k    fung  r          symbolic probabilistic inference discrete continuous variables  ieee tran  systems  man  cybernetics  vol      no     june  pp         
cherkassky  v  lari najafi  h          nonparametric regression analyisis using selforganizing topological maps h  wechsler  ed    neural networks perception  vol   
computation  learning architectures  san diego  academic press 
cohn  d a   ghahramani  z    jordan  m i          active learning statistical models 
journal artificial intelligence research    pp          
dalal  s r    hall  w j          approximating priors mixtures natural conjugate priors 
j  r  statist  soc  b  vol      no     pp          
de soete  g          using latent class analysis categorization research i  v  mechelen 
j  hampton  r s  michalski  p  theuns  eds    categories concepts  theoretical views
inductive data analysis  san diego  academic press 
dempster  a p   laird  n m   rubin  d b           maximum likelihood estimation incomplete data via em algorithm  journal royal statistical society  series b  vol     
pp       
duda  r o  hart  p e          pattern classification scene analysis  john wiley   sons 
fan  c m   namazi  n m  penafiel  p b          new image motion estimation algorithm
based em technique  ieee transactions pattern analisys machine intelligence  vol     no    march  pp          
fukunaga  k          introduction statistical pattern recognition  academic press 
ghahramani  z  jordan  m i         supervised learning incomplete data via em
approach cowan  j d   tesauro  g   alspector  j   eds    advances neural information processing systems    morgan kauffman
ghahramani  z  hinton  g e         em algorithm mixtures factor analyzers 
tech  rep  univ  toronto  crg tr      

   

fiprobabilistic inference uncertain data using mixtures

heckerman  d    wellman  m p          bayesian networks  communications acm  vol 
    no    pp         march 
hertz  j   krogh  a   palmer  r g           introduction theory neural computation 
addison wesley 
hinton  g e   dayan  p  revow  m          modeling manifold images handwritten
digits  ieee t  neural networks    pp        
hornik  k   stinchcombe  m   white  h           multilayer feedforward networks universal
approximators  neural networks  no   
hutchinson  a          algorithmic learning  new york  oxford univ  press 
izenman  a j          recent developments nonparametric density estimation  j  amer  statist  assoc  vol      no       pp          
jordan  m i   jacobs  r a           hierarchical mixtures experts em algorithm 
neural computation     pp          
kohonen  t           self organization associative memory  springer verlag 
lauritzen  s l    spiegelhalter  d  j          local computations probabilities graphical
structures application expert systems  j  r  statist  soc  b      no     pp         
li  m  vitnyi  p          introduction kolmogorov complexity applications 
new york  springer verlag 
mclachlan  g j   basford  k e           mixture models  new york  marcel dekker 
mclachlan  g j  krishnan  t          em algorithm extensions  john wiley
sons 
michalski  r s   carbonell  j  mitchell  t m   eds          machine learning  artificial
intelligence approach  palo alto  ca  tioga press  reprinted morgan kaufmann
 los altos  ca  
michalski  r s   carbonell  j  mitchell  t m   eds          machine learning  artificial
intelligence approach  vol  ii  los altos  ca  morgan kaufmann 
mohgaddam  b  pentland  a          probabilistic visual learning object representation  ieee pami  vol      no         pp       
merz  c j  murphy  p m          uci repository machine learning databases 
 http   www ics uci edu  mlearn mlrepository html   irvine  ca  university california 
department information computer science 
palm  h c          new method generating statistical classifiers assuming linear mixtures
gaussian densities  proceedings   th iapr international conference pattern recognition  jerusalem  october              vol    ieee  piscataway  nj  usa 
papoulis  a           probability  random variables stochastic processes  mcgraw hill 
pearl  j           probabilistic reasoning intelligent systems  networks plausible inference 
morgan kaufmann 

   

firuiz  lpez de teruel   garrido

peng  f   jacobs  r a   tanner  m a          bayesian inference mixtures of experts hierchical mixtures of experts models application speech recognition  accepted
journal american statistical association 
priebe  c e   marchette  d j           adaptive mixtures  recursive nonparametric pattern recognition  pattern recognition  v   n    pp            
pudil  p   novovicova  j   choakjarernwanit  n   kittler  j          feature selection based
approximation class densities finite mixtures special type  pattern recognition
vol    no   pp            
quinlan  j r           c     programs machine learning  san mateo  ca  morgan kaufmann 
redner  r a   walker  h f           mixture densities  maximum likelihood estimation
em algorithm  siam review  vol      pp          
rojas  r          short proof posterior probability property classifier neural networks  neural computation vol   issue    january 
rumelhart  d e   hinton  g  e  williams  r          learning internal representations
error propagation rumelhart  mcclelland   pdp group         pp          
rumelhart  d e   mcclelland  j l    pdp research group          parallel distributed processing  explorations microstructure cognition  vol     foundations  cambrigde
ma  bradford books mit press 
ruiz  a          nonparametric bound bayes error  pattern recognition  vol      no 
   pp          
ruiz  a   lpez de teruel  p e  garrido  m c          kernel density estimation indirect observations  preparation 
sung  k  k  poggio  t            example based learning view based human face detection   ieee trans  pattern analyisis machine intelligence  vol     n    january  pp 
      
tanner  m a          tools statistical inference    rd ed    springer 
thrun  s  et al          the monk s problems  performance comparison different learning
algorithms   technical report cmu cs        
titterington  d m   a f m  smith u e  makov         statistical analysis finite mixture
distributions  wiley  new york 
traven  h g c           neural network approach statistical pattern classification
 semiparametric  estimation prob  den  func   ieee neural networks  v  n  
valiant  l g          view computational learning theory meyrowitz chipman 
eds          foundations knowledge acquisition  machine learning  kluwer acad  pub 
valiveti  r s   oommen  b j           using chi squared metric determining stochastic
dependence  pattern recognition  v   n   pp            
vapnik  v n          learning dependencies based empirical data  springer  new york 
vapnik  v n          nature statistical learning theory  springer  new york 

   

fiprobabilistic inference uncertain data using mixtures

wan  e a           neural networks classification  bayesian interpretation  ieee trans 
neural networks  v  n  
weiss  y  adelson e h          perceptually organized em  framework motion segmentation combines information form motion  tr mit mlpcs tr     
wilson  d r  martinez  t r          improved heterogeneous distance functions  jair  v  
pp       
wolpert  d h   ed       a   mathematics generalization  proc  sfi clns workshop
formal approaches supervised learning 
xu  l    jordan  m i          convergence properties em algorithm gaussian
mixtures  neural computation vol   issue    january 
you  y l  kaveh  m          regularization approach joint blur identification image
restoration  ieee image processing  vol   no    pp          

   


