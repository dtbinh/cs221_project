journal of artificial intelligence research                  

submitted       published      

safe exploration of state and action spaces in
reinforcement learning
javier garca
fernando fernandez

fjgpolo inf uc m es
ffernand inf uc m es

universidad carlos iii de madrid 
avenida de la universidad    
      leganes  madrid  spain

abstract
in this paper  we consider the important problem of safe exploration in reinforcement
learning  while reinforcement learning is well suited to domains with complex transition
dynamics and high dimensional state action spaces  an additional challenge is posed by
the need for safe and efficient exploration  traditional exploration techniques are not
particularly useful for solving dangerous tasks  where the trial and error process may lead
to the selection of actions whose execution in some states may result in damage to the
learning system  or any other system   consequently  when an agent begins an interaction
with a dangerous and high dimensional state action space  an important question arises 
namely  that of how to avoid  or at least minimize  damage caused by the exploration of the
state action space  we introduce the pi srl algorithm which safely improves suboptimal
albeit robust behaviors for continuous state and action control tasks and which efficiently
learns from the experience gained from the environment  we evaluate the proposed method
in four complex tasks  automatic car parking  pole balancing  helicopter hovering  and
business management 

   introduction
reinforcement learning  rl   sutton   barto        is a type of machine learning whose
main goal is that of finding a policy that moves an agent optimally in an environment  generally formulated as a markov decision process  mdp   many rl methods are being used
in important and complex tasks  e g   robot control see smart   kaelbling        hester 
quinlan    stone        stochastic games see mannor        konen   bartz beielstein 
     and control optimization of complex dynamical systems see salkham  cunningham 
garg    cahill         while most rl tasks are focused on maximizing a long term cumulative reward  rl researchers are paying increasing attention not only to long term
reward maximization  but also to the safety of approaches to sequential decision problems
 sdps   mihatsch   neuneier        hans  schneegass  schafer    udluft        martn h 
  lope        koppejan   whiteson         well written reviews of these matters can also
be found  geibel   wysotzki        defourny  ernst    wehenkel         nevertheless 
while it is important to ensure reasonable system performance and consider the safety of
the agent  e g   avoiding collisions  crashes  etc   in the application of rl to dangerous
tasks  most exploration techniques in rl offer no guarantees on both issues  thus  when
using rl techniques in dangerous control tasks  an important question arises  namely  how
can we ensure that the exploration of the state action space will not cause damage or injury
c
    
ai access foundation  all rights reserved 

figarca   fernandez

while  at the same time  learning  near  optimal policies  the matter  in other words  is
one of ensuring that the agent be able to explore a dangerous environment both safely and
efficiently  there are many domains where the exploration exploitation process may lead
to catastrophic states or actions for the learning agent  geibel   wysotzki         the
helicopter hovering control task is one such case involving high risk  since some policies
can crash the helicopter  incurring catastrophic negative reward  exploration exploitation
strategies such as greedy may even result in constant helicopter crashes  especially where
there is a high probability of random action selection   another example can be found in
portfolio theory where analysts are expected to find a portfolio that maximizes profit while
avoiding risks of considerable losses  luenberger         since the maximization of expected
returns does not necessarily prevent rare occurrences of large negative outcomes  a different
criteria for safe exploration is needed  the exploration process in which new policies are
evaluated must be conducted with extreme care  indeed  for such environments  a method is
required which not only explores the state action space  but which does so in a safe manner 
in this paper  we propose the policy improvement through safe reinforcement learning
 pi srl  algorithm for safe exploration in dangerous and continuous control tasks  such a
method requires a predefined  and safe  baseline policy which is assumed to be suboptimal
 otherwise  learning would be pointless   predefined baseline policies have been used in
different ways by other approaches  in the work of koppejan and whiteson         singlelayers perceptrons are evolved  albeit starting from a prototype network whose weights correspond to a baseline policy provided by helicopter control task competition software  abbeel 
coates  hunter    ng         this approach can be viewed as a simple form of population seeding which has proven to be advantageous in numerous evolutionary methods
 e g  see hernandez daz  coello  perez  caballero  luque    santana quintero        poli
  cagnoni         in the work of martn and de lope         the weights of neural networks are also evolved by inserting several baseline policies  including that provided in the
helicopter control task competition software  into the initial population  to minimize the
possibility of evaluating unsafe policies  their approach prevents crossover and mutation
operators from permitting anything more than tiny changes to the initial baseline policies 
in this paper  we present the pi srl algorithm  a novel approach to improving baseline
policies in dangerous domains using rl  the pi srl algorithm is composed of two different steps  in the first  baseline behavior  robust albeit suboptimal  is approximated
using behavioral cloning techniques  anderson  draper    peterson        abbott        
in order to achieve this goal  case based reasoning  cbr  techniques  aamodt   plaza 
      bartsch sprl  lenz    hbner        were used which have been successfully applied
to imitation tasks in the past  floyd   esfandiari        floyd  esfandiari    lam        
in the second step  the pi srl algorithm attempts to safely explore the state action space
in order to build a more accurate policy from previously learned behavior  thus  the set
of cases  i e   state action pairs  obtained in the previous phase is improved through the
safe exploration of the state action space  to perform this exploration  small amounts of
gaussian noise are randomly added to the greedy actions of the baseline policy approach 
the exploration strategy has been used successfully in previous works  argall  chernova 
veloso    browning        van hasselt   wiering        
the novelty of the present study is in the use of two new  main components   i  a risk
function to determine the degree of risk of a particular state and  ii  a baseline behavior
   

fisafe exploration of state and action spaces in reinforcement learning

capable of producing safe actions in supposedly risky states  i e   states that can lead to
damage or injury   in addition  we present a new definition of risk based on what for the
agent is unknown and known space  as will be described in section   in greater detail  this
new definition is completely different from traditional definitions of risk found in the literature  geibel        mihatsch   neuneier        geibel   wysotzki         the paper also
reports the experimental results obtained from the application of the new approach in four
different domains   i  automatic car parking  lee   lee          ii  pole balancing  sutton
  barto          iii       rl competition helicopter hovering  ng  kim  jordan    sastry 
      and  iv  business management  borrajo  bueno  de pablo  santos  fernandez  garca 
  sagredo         in each domain  we propose the learning of a near optimal policy which 
in the learning phase  will minimize car crashes  pole disequilibrium  helicopter crashes and
company bankruptcies  respectively  it is important to note that the comparison of our
approach with an agent with an optimal exploration policy is not possible since  in the
proposed domains  each with a high dimensional and continuous state and action space  as
well as complex stochastic dynamics   we do not know what the optimal exploration policy
is 
regarding the organization of the remainder of the paper  section   introduces key
definitions  while section   describes in detail the learning approach proposed  in section   
the evaluation performed in the four above mentioned domains is presented  section  
discusses related work and section   summarizes the main conclusions of our study  in
these sections 
the term return is used to refer to the expected cumulative future discounted
p
reward r   t    t rt   while the term reward is used to refer to a single real value used to
evaluate the selection of an action in a particular state and it is denoted by r 

   definitions
to illustrate the concept of safety used in our approach  a navigation problem is presented
below in figure    in the navigation problem presented in figure    a control policy must
be learned to get from a particular start state to a goal state  given a set of demonstration
trajectories  in this environment  we assume the task to be difficult due to a stochastic
and complex dynamic of the environment  e g   an extremely irregular surface in the case
of a robot navigation domain or wind effects in the case of the helicopter hover task   this
stochasticity makes it impossible to complete the task using exactly the same trajectory
every time  additionally  the problem supposes that a set of demonstrations from a baseline
controller performing the task  the continuous black lines  are also given  this set of
demonstrations is composed of different trajectories covering a well defined region of the
state space  the region within the rectangle  
our approach is based on the addition of small amounts of gaussian noise or perturbations to the baseline trajectories in order to find new and better ways of completing the
task  this noise will affect the baseline trajectories in different ways  depending on the
amount of noise added which  in turn  depends on the amount of risk to be taken  if no risk
is desired  the noise added to the baseline trajectories will be   and  consequently  no new
or improved behavior will be discovered  nevertheless  the robot will never fall off the cliff
and the helicopter will never crash   if  however  an intermediate level of risk is desired 
small amounts of noise will be added to the baseline trajectories and new trajectories  the
   

figarca   fernandez

figure    exploration strategy based on the addition of small amounts of noise to baseline
policy behavior  continuous lines represent the baseline behavior  while newly
explored behaviors are indicated by the dotted and dashed lines 
dotted blue lines  to complete the task are discovered  in some cases  the exploration of new
trajectories leads the robot to unknown regions of the state space  the dashed red lines  
the robot is assumed to be able to detect such situations with a risk function and use the
baseline behavior to return to safe  known states  if  instead  a very high risk is desired 
large amounts of noise will be added to the baseline trajectories  leading to the discovery
of new trajectories  but also to a higher probability that the robot gets damaged   the
iteration of this process leads the robot to progressively and safely explore the state and
action spaces in order to find new and improved ways to complete the task  the degree of
safety in the exploration  however  will depend on the risk taken 
    error and non error states
in this paper  we follow as far we can the notation presented in geibel et al         for
the definition of our concept of risk  in their study  geibel et al  associate risk with error
states and non error states  with the former understood as a state in which it is considered
undesirable or dangerous to enter 
definition   error and non error states  let s be a set of states and   s the set
of error states  a state s   is an undesirable terminal state where the control of the
agent ends when s is reached with damage or injury to the agent  the learning system or
any external entities  the set   s is considered a set of non error terminal states with
      and where the control of the agent ends normally without damage or injury 
in terms of rl  if the agent enters an error state  the current episode ends with damage
to the learning system  or other systems   whereas if it enters a non error state  the episode
ends normally and without damage  thus  geibel et al  define the risk of s with respect
to policy     s   as the probability that the state sequence  si  i  with s    s  generated
by the execution of policy   terminates in an error state s     by definition    s     
if s    if s    then   s      because        for states s 
      the risk
taken depends on the actions selected by the policy   with these definitions  we have the
   

fisafe exploration of state and action spaces in reinforcement learning

theoretical framework with which to introduce our own definition of the risk associated with
known and unknown states 
    known and unknown states in continuous action and state spaces
we assume a continuous  n dimensional state space s   n where each state s    s    s           
sn    s is a vector of real numbers and each dimension has an individual domain dis    
similarly  we assume a continuous and m dimensional action space a   m where each
action a    a    a            am    a is a vector of real numbers and each dimension has an
individual domain dia     additionally  the agent considered here is endowed with a
memory  or case base b  of the size   each memory element represents a state action pair 
or case  the agent has experienced before 
definition    case base   a case base is a set of cases b    c          c    every case
ci consists of a state action pair  si   ai   the agent has experienced in the past and with an
associated value v  si    thus  ci    si   ai   v  si      where the first element represents the
cases problem part and corresponds to the state si   the following element ai depicts the case
solution  i e   the action expected when the agent is in the state si   and the final element
v  si   is the value function associated with the state si   each state si is composed of n
continuous state variables and each action ai is composed of m continuous action variables 
when the agent receives a new state sq   it first retrieves the nearest neighbor of sq in
b according to a given similarity metric and then performs the associated action  in this
paper  we use euclidean distance as our similarity metric  equation    
v
ux
u n
d sq   si     t  sq j  si j   

   

j  

the euclidean distance metric is useful when the value function is expected to be continuous and smooth throughout the state space  santamara  sutton    ram         however 
since the value function is unknown a priori and the euclidean distance metric is not particularly suitable for many problems  many researchers have begun to ask how the distance
metric itself can learn or adapt in order to achieve better results  taylor  kulis    sha 
       while the use of distance metric learning techniques would certainly be desirable in
order to induce a more powerful distance metric for a specific domain  such a consideration
lies outside the scope of the present study  in this paper  therefore  we have focused only on
domains in which euclidean distance has been proven successful  i e   it has been successfully applied to car parking  cichosz         pole balancing  martin h   de lope        
helicopter hovering control  martin h   de lope        and simba  borrajo et al         
traditionally  case based approaches use a density threshold  in order to determine when
a new case should be added to the memory  when the distance of the nearest neighbor to
sq is greater than   a new case is added  in this sense  the parameter  defines the size
of the classification region for each case in b  figure     if a new case sq is within the
classification region of a case ci   it is considered to be a known state  hence  the cases in

 and its associated value function v b
b describe a case based policy of the agent b
 
   

figarca   fernandez

figure    known and unknown states 
definition    known unknown states   given a case base b    c          c   composed
of cases ci    si   ai   v  si    and a density threshold   a state sq is considered known when
min i d sq   si     and unknown in all other cases  formally    s is the set of known
states  while   s is the set of unknown states with       and      s 
with definition    states can be identified as known or unknown  when the agent
receives a new state s    it performs the action ai of the case ci for which d s  si    
min j d s  sj    however  if the agent receives a state s   where  by definition  the
distance to any state in b is larger than   no case is retrieved  consequently  the action
to be performed from that state is unknown to the agent 
definition    case based risk function   given a case base b    c          c   composed
of cases ci    si   ai   v  si     the risk for each state s is defined as equation   

b

 


 s   

 
 

if min j d s  sj     
otherwise

   



thus   b  s      holds if s    i e   s is unknown   such that the state s is not
associated with any case and  hence  the action to be performed in the given situation is

unknown  if s    then  b  s      
 derived from a casedefinition    safe case based policy   the case based policy b
base b    c            c   is safe when  from any initial known state s  with respect to b  the
 always produces known non error states with respect to b 
execution of b






b
s     b  s         then  si  i  
 b  si      

   

additionally  it is assumed here that the probability that the state sequence  si  i  from
   terminates in an error state
any known state s     generated by executing policy b

s   is b  s         i e          
   

fisafe exploration of state and action spaces in reinforcement learning

definition    safe case based coverage   the coverage of a single state s with respect
to a safe case base b    c            c   is defined as the state si for which min i d s  si     
therefore  we assume that the safe case based does not provide actions for the entire state
space  but rather only for known states s   
figure   graphically represents the relationship between known unknown and error non learnt  an
error states  the green area in the image denotes the safe case based policy b
area of the state space corresponding to the initial known space  an agent following the
 will always be in the green area and all resulting episodes will end without
policy b
damages  consequently  a subset of non error states will also form part of the known space 
formally  let  and  be subsets of non error states belonging to the known and unknown
spaces  respectively  with        then     the yellow area in the figure  by
contrast  represents the unknown space   in this space will be found all error states  as
well as a subset of remaining non error states  formally     and    
understood in this way  the pi srl algorithm can be summed up as follows 
 
 as a first step  learn the known space  green area  from the safe case based policy b

 as a second step  adjust the known space  green area  and unknown space  yellow
area  in order to explore new and improved behaviors while avoiding error states  red
area   during this process of adjusting the known space to the space used for safe and
better policies  the algorithm can forget ineffectual known states  as will be shown
in section   

figure    known unknown and error non error states given the case base b 

    the advantages of using prior knowledge and predetermined
exploration policies
in the present subsection  the advantages of using teacher knowledge in rl  namely  i  to
provide initial knowledge about the task to be learned and  ii  to support the exploration
process  are highlighted  furthermore  we explain why we believe this knowledge to be
   

figarca   fernandez

indispensable in rl for tackling highly complex and realistic problems with large  continuous
state and action spaces and in which a particular action may result in an undesirable
consequence 
      providing initial knowledge about the task
most rl algorithms begin learning without any previous knowledge about the task to be
learnt  in such cases  exploration strategies such as   greedy are used  the application
of this strategy results in the random exploration of the state and action spaces to gather
knowledge about the task  only when enough information is discovered from the environment does the algorithms behavior improve  such random exploration policies  however 
waste a significant amount of time exploring irrelevant regions of the state and action
spaces in which the optimal policy will never be encountered  this problem is compounded
in domains with extremely large and continuous state and action spaces in which random
exploration will never likely visit the regions of the spaces necessary to learn  near  optimal
policies  additionally  in many real rl tasks with real robots  a random exploration to
gather information from the environment cannot even be applied  with real robots  what
is considered to be sufficient information can be much more information than a real robot
can gather from the environment  finally  as it is impossible to avoid undesirable situations
in high risk environments without a certain amount of prior knowledge about the task  the
use of random exploration would require that an undesirable state be visited before it can
be labeled as undesirable  however  such visits to undesirable states may result in damage
or injury to the agent  the learning system or external entities  consequently  visits to these
states should be avoided from the earliest steps of the learning process 
mitigating the difficulties described above  finite sets of teacher provided examples or
demonstrations can be used to incorporate prior knowledge into the learning algorithm 
this teacher knowledge can be used in two general ways  either  i  to bootstrap the learning algorithm  i e   a sort of initialization procedure  or  ii  to derive a policy from such
examples  in the first case  the learning algorithm is provided with examples or demonstrations from which to bootstrap the value function approximation and lead the agent through
the more relevant regions of the space  the second way in which teacher knowledge can
be used refers to learning from demonstration  lfd  approaches in which a policy is derived from a finite set of demonstrations provided by a teacher  the principal drawback
to this approach  however  is that the performance of the derived policy is heavily limited
by teacher ability  while one way to circumvent the difficulty and improve performance is
by exploring beyond what is provided in the teacher demonstrations  this again raises the
question of how the agent should act when it encounters a state for which no demonstration
exists  an unknown state  
      supporting the exploration process
while furnishing the agent with initial knowledge helps mitigate the problems associated
with random exploration  this alone is not sufficient to prevent the undesirable situations
that arise in the subsequent explorations undertaken to improve learner ability  an additional mechanism is necessary to guide this subsequent exploration process in such a way
that the agent may be kept far away from catastrophic states  in this paper  a teacher 
   

fisafe exploration of state and action spaces in reinforcement learning

rather than the policy derived from the current value function approximation is used for
the selection of actions in unknown states  one way to prevent the agent from encountering
unknown states during the exploration process would be by requesting from the beginning
a teacher demonstration for every state in the state space  however  such a strategy is not
possible due to  i  its computational infeasibility given the extremely large number of states
in the state space and  ii  the fact that the teacher should not be forced to give an action
for every state  given that many states will be ineffectual for learning the optimal policy 
consequently  pi srl requests teacher action only when such action is actually required
 i e   when the agent is in an unknown state  
as this paper supposes that such a teacher is available for the task to be learned  the
teacher is taken as the baseline behavior  although some studies have examined the use of
robotic teachers  hand written control policies and simulated planners  the great majority
to date have made use of human teachers  this paper uses suboptimal automatic controllers
as teachers  with t taken as the teachers policy 
definition    baseline behavior   policy t is considered the baseline behavior about
which three assumptions are made   i  it is able to provide safe demonstrations of the
task to be learnt from which prior knowledge can be extracted   ii  it is able to support the
subsequent exploration process  advising suboptimal actions in unknown states to reduce the
probability of entering into error states and return the system to a known situation  and
 iii  its performance is far from optimal 
while optimal baseline behaviors are certainly ideal to behave safely  non optimal behaviors are often easy  or easier  to implement or generate than optimal ones  the pi srl
algorithm uses the baseline behavior t in two different ways  first  it uses the safe demonstrations of t to provide prior knowledge about the task  in this step  the algorithm builds
 with the
the initial known space of the agent derived from the safe case based policy b

purpose of mimicking t through b   in the second step  pi srl uses t to support the
subsequent exploration process conducted to improve the abilities of the previously learnt
   as the exploration process continues  an action of  is requested only when required 
b
t
that is  when the agent is in an unknown state  figure     in this step  t acts as a backup
policy in the case of an unknown state with the intention of guiding the learning away from
catastrophic errors or  at least  reducing their frequency  it is important to note that the
baseline behavior cannot demonstrate the correct action for every possible state  however 
while the baseline behavior might not be able to indicate the best action in all cases  the
action it supplies should  at the very least  be safer than that obtained through random
exploration 
    the risk parameter
in order to maximize exploration safety  it seems advisable that movement through the
state space not be arbitrary  but rather that known space be expanded only gradually by
starting from a known state  such an exploration is carried out through the perturbation
   perturbation of the trajectories
of the state action trajectories generated by the policy b
is accomplished by the addition of gaussian random noise to the actions in b in order
to obtain new ways of completing the task  thus  the gaussian exploration takes place
   

figarca   fernandez

figure    the exploration process in pi srl requests actions of the baseline behavior  t  
when it is really required 
around the current approximation of the action ai for the current known state sc    with
ci    si   ai   v  si    and d sc   si     min j d s  sj    the action performed is sampled from
a gaussian distribution with the mean at the action output given by the instance selected
in b  when ai denotes the algorithm action output  the probability of selecting action a i  
 s  a i   is computed using equation   
 s  a i    

 
 
 
   e ai ai     
   

if        

   

the shape of the gaussian distribution depends on parameter   standard deviation  
in this study   is used as a width parameter  while large  values imply a wide bellshaped distribution  increasing the probability of selecting actions a i very different from
the current action ai   a small  value implies a narrow bell shaped distribution  increasing
the probability of selecting actions a i very similar to the current action ai   when        
we assume  s  ai        hence  the  value is directly related to the amount of perturbation
   higher  values imply
added to the state action trajectories generated by the policy b
greater perturbations  more gaussian noise  and a greater probability of visiting unknown
states 
definition    risk parameter   the parameter  is considered a risk parameter  large
values of  increase the probability of visiting distant unknown states and  hence  increase
the probability of reaching error states 
these exploratory actions drive the agent to the edge of the known space and force it
to go slightly beyond  into the unknown space  in search of better  safer behaviors  after
a period of time  the execution of these exploratory actions increases the known space
   the risk
and improves the abilities of the previously learned safe case based policy b
parameter   as well as   are design parameters that must be selected by the user  in
section      guidelines for this selection are offered 
it is important to note that the approach proposed in this study is based on two logical
assumptions in rl derived from the following generalization principles  kaelbling  littman 
  moore        sutton   barto        
   

fisafe exploration of state and action spaces in reinforcement learning

 i  nearby states have similar optimal actions  in continuous state spaces  it is
impossible for the agent to visit every state and store its value  or optimal action  in a
table  this is why generalization techniques are needed  in large  smooth state spaces 
similar states are expected to have similar values and similar optimal actions  therefore 
it is possible to use experience gathered from the environment with a limited subset of the
state space and produce a reliable approximation over a much larger subset  boyan  moore 
  sutton        hu  kostiadis  hunter    kalyviotis        fernandez   borrajo        
one must also note that  in the proposed domains  an optimal action is also considered to
be a safe action in the sense that it never produces error states  i e   no action is considered
optimal that leads the agent to a catastrophic situation  
 ii  similar actions in similar states tend to produce similar effects  considering a deterministic domain  the action at performed in state st always produces the same
state st     in a stochastic domain  it is understood intuitively that the execution of the
action at in state st will produce similar effects  i e   it produces states  s t     s t     s t           
where i  j i    j dist sit     sjt          additionally  the execution of the action a t  at
in a state s t  st produces states  s   t     s   t     s   t            where i  j dist s  it     sjt        
as explained earlier  the present study uses euclidean distance as a similarity metric  as
it has been proven successful in the proposed domains  as a result of this assumption 
approximation techniques can be used  such that actions that generate similar effects can
be grouped together as one action  jiang         in continuous action spaces  the need
for generalization techniques is even greater  kaelbling et al          in this paper  the
assumption also allows us to assume that low values of  increase the probability of visiting
known states and  hence  of exploring less and taking less risks  while greater values of 
increase the probability of reaching error states 

   the pi srl algorithm
the pi srl algorithm is composed of two main steps described in detail below 
    first step  modeling baseline behaviors by cbr
the first step of pi srl is an approach for behavioral cloning  using cbr to allow a software
agent to behave in a similar manner to a teacher policy  baseline behavior  t  floyd et al  
       whereas lfd approaches are named differently according to what is learned  argall et al          to prevent terminological inconsistencies here  we consider behavioral
cloning  also known as imitation learning  to be an area of lfd whose goal is the reproduction mimicking of the underlying teacher policy t  peters  tedrake  roy    morimoto 
      abbott        
when using cbr for behavioral cloning  a case can be built using the agents state
received from the environment  as well as the corresponding action command performed by
the teacher  in pi srl  the objective of the first step is to properly imitate the behavior of
t using the cases stored in a case base  at this point  an important question arises  namely 
how a case base b can be learnt using the sample trajectories provided by t such that  at
the end of the learning process  the resulting policy derived from b mimics the behavior
of t   baseline behavior is a function that maps states to actions t   s  a or  in other
   

figarca   fernandez

words  a function that  given a state si  s  provides the corresponding action ai  a  in
this paper  we want to build a policy b derived from a case base composed of cases  sj   aj  
such that  for a new state sq   the case with the minimum euclidean distance dist sq   sj   is
retrieved and the corresponding action aj is returned  intuitively  it can be assumed that
b can be built simply by storing all cases  si   ai   gathered from one interaction between
t and the environment during a limited number of episodes k  at the end of k episodes 
one expects the resulting b to be able to properly mimic the behavior of t   however 
informal experimentation in the helicopter hovering domain shows this not to be the case
 section       in helicopter hovering  after k       episodes and the prohibitive number
of         cases stored  the policy derived from the case base b is unable to correctly
imitate the baseline behavior t and  instead  continuously crashes the helicopter  indeed 
in order for b to mimic t in large continuous and stochastic domains  the approach
requires a larger number of episodes and  consequently  a prohibitive number of cases  in
fact  to perfectly mimic t in these domains  an infinite number of cases would be required 
figure   attempts to explain why we believe that this learning process does not work  in
it  the region of the space represented by simply storing cases derived from t in the form
c    s  a  is shown  each stored case  red circles  covers an area of the space and represents
the centroid of a voronoi region 

figure    effects of storing all training cases 
if the previously learned policy b is used when a new state sq is presented  the action aj is performed  corresponding to the case cj    sj   aj   where the euclidean distance
dist sq   sj   is less than that with all other stored cases  however  if we use the policy t to
provide an action in the situation sq   the action ai is provided which is different than aj  
at this point  the policy b can be said to classify the state sq as the obtained class aj  
while the policy t can be said to classify the state sq as the desired class ai  insofar as
t is the policy to be mimicked   with  ai  aj        furthermore   ai  aj   is understood
as the classification error  if the case base stored all the possible pairs  si   ai   that t
were able to generate in the domain  the actions aj and ai would always be identical  with
dist sq   sj       and  ai  aj        however  in a stochastic and large  continuous domain  it
is impossible to store all such cases  the sum of all such classification errors in an episode
   

fisafe exploration of state and action spaces in reinforcement learning

leads to the visiting of unexplored regions of the case space  i e   regions where the new
state sq received from the environment has a euclidean distance dist sq   sj       with
respect to the closest case cj    sj   aj   in b   when these unexplored regions are visited 
the difference between the obtained class derived from b and the desired class derived
from t is large  i e    ai  aj         and the probability that error states might be visited
greatly increases 
it may be concluded  therefore  that simply storing the pairs c    s  a  generated by t
is not sufficient to properly mimic its behavior  for this reason  the algorithm in figure  
below has been proposed 
cbr approach for behavioral cloning
  
  
  
  

given
given
given
   set

  
  
  
  

   repeat
set k    
while k   maxepisodelength do
compute the case   sc   ac       closest to the current state sk

  
  
  
  
  
  
  
  
  
  
  
  
  

the
the
the
the

baseline behavior t
density threshold 
maximum number of cases 
case base b   



if  b  sk       then    by equation  
set ak   ac
else
set ak using the baseline behavior t
create a new case cnew    sk   ak     
b    b  cnew
execute ak   and receive sk  
set k   k    
end while
if kbk    then
remove the   kbk least frequently used cases in b
until stop criterion becomes true

   return b performing the safe case based policy b

figure    cbr algorithm for behavioral cloning 


in the first step of the algorithm  the state value function v b  si   is initialized to    see

line      the value v b  si   for each case is computed in the second step of the algorithm
in section      additionally  this step uses the case based risk function  equation    to
determine whether a new state sk should be considered risky  line      if the new state is
not risky  i e   it is a known state sk     a   nearest neighbor strategy is followed  line
     otherwise  the algorithm performs the action ak using the baseline behavior t and a
new case cnew    sk   ak      is built and added to the case base b  line      starting with an
empty case base  the learning algorithm continuously increases its competence by storing
new experiences  however  there are a number of reasons why the inflow of new cases should
be limited  large case bases increase the time required to find the closest cases to a new
example  while this may be partially solved using techniques to reduce the retrieval time
 e g   k d trees that have been used in this work   they nevertheless do not reduce the storage
   

figarca   fernandez

requirements  several approaches to the removal of ineffectual cases during training exist 
including ahas ibx algorithms  aha        or any nearest prototype approach  fernandez
  isasi         when the number of cases stored in b exceeds a critical value kbk    such
that the realization of a retrieval within a certain amount of time cannot be guaranteed 
the removal of some cases is inevitable  an efficient approach to such a problem is through
the removal of the least frequently used elements of b  line     
the result of this step is a constrained case base b describing the safe case based policy

b that mimics the baseline behavior t   though perhaps with some deviation  line     
formally  let u  t   be an estimate of the utility of the baseline behavior t computed by
    u     
averaging the sum of rewards accumulated in each of nt trials  then  u  b
t
    second step  improving the learned baseline behavior
 learned in the previous
in this step of the pi srl algorithm  the safe case based policy b
step is improved by the safe exploration of the state action space  first  for each case ci 

b  the state value function v b  si   is computed following a monte carlo  mc  approach
 figure    
mc algorithm adapted to cbr
  
  
  
  

given the case base b
   initialize  for each ci  b
v  s   arbitrary
returns s   empty list

  
  
  
  
  
  

   while k   maxn umberepisodes

generate an episode using b
for each s appearing in the episode with   s  a  v  s     b
r  return following the first occurrence of s
append r to returns s 
v s   average returns s  

  
  

set k   k    
   return b

figure    monte carlo algorithm for the computation of state value function for each case 
this algorithm is similar in spirit to a first visit mc method for v   sutton   barto 
       adapted in this paper to work with a policy given by a case base  in the algorithm
shown in figure    all returns for each state si  b are accumulated and averaged  following
 derived by the case base b  see line      it is important to note that in the
the policy b
algorithm the term return following the first occurrence of s refers to the expected return of
s  i e   the expected cumulative future discounted reward starting from that state   whereas
returns refers to a list composed of each return of s in different episodes  one of the
principal reasons for using the mc method is that it allows us to quickly and easily estimate

state values v b  si   for each case ci  b  in addition  mc methods have been shown to
be successful in a wide variety of domains  sutton   barto         once the state value

function v b  si   is computed for each case ci  b  small amounts of gaussian noise are
 in order to obtain new and improved ways
randomly added to the actions of the policy b
   

fisafe exploration of state and action spaces in reinforcement learning

to complete the task  the algorithm used to improve the baseline behavior learned in the
previous step is depicted in figure    the algorithm is composed of four steps performed
in each episode 
   a  initialization step  the algorithm initializes the list used to store cases occurring
during an episode and sets the cumulative reward counter of the episode to   
   b  case generation  the algorithm builds a case for each step of an episode 
for each new state sk   the closest case   s  a  v  s    b is computed using the euclidean
distance metric from equation    see line    in algorithm of figure     in order to determine
the perceived degree of risk of the new state sk   the case based risk function is used  line

     if  b  sk        then sk    known state   in this case  the action ak performed is
computed using equation   and a new case cnew    s  ak   v  s    is built to be added to
the list of cases having occurred in the episode  line      it is important to note that the
new case   s  ak   v  s    is built replacing the action a corresponding to the closest case
in   s  a  v  s    b  with the new action ak resulting from the application of random
gaussian noise to a in the equation    thus  the algorithm only produces smooth changes

in the cases of b where ak  a  if  however   b  sk        the state sk    i e   unknown
state  line       in unknown states  the action ak performed is suggested by the baseline
behavior t which defines safe behavior  line      a new case   sk   ak       is built and
added to the list of cases in the episode and actions will be performed using t until the
agent is not in a known state  finally  the reward obtained in the episode is accumulated 
where r sk   ak   is the immediate reward obtained when action ak is performed in state sk
 line     
   c  computing the state value function for the unknown states  in this step 
the state value function of the states considered to be unknown in the previous step is
computed  in the previous step  line      the state value function for these states is set at
   the algorithm proceeds in a manner similar to the first visit mc algorithm in figure   
in this case  the return for each unknown state si is computed  but not averaged since only
one episode is considered  line    and      the return for each si is computed  taking into
account the first visit of the state si in the episode  each occurrence of a state in an episode
is called a visit to si    although the state si could appear multiple times in the rest of the
episode 
   d  updating the cases in b using experience gathered  updates in b are
made with the cases gathered from episodes with a cumulative reward similar to that of the
best episode found to that point using the threshold   line      in this way  good sequences
are provided for the updates since it has been shown that such sequences of experiences can
cause an adaptive agent to converge to a stable and useful policy  whereas bad sequences may
cause an agent to converge to an unstable or bad policy  wyatt         this also prevents
the degradation of the initial performance of b as computed in the first step of the algorithm
through the use of bad episodes  or episodes with errors  for updates  in this step  two types
of updates appear  namely  replacements and additions of new cases  again  the algorithm
iterates for each case ci    si   ai   v  si     listcasesepisode  line      if si is a known state
 line      we compute the case   si   a  v  si     b corresponding to the state si  line     
one should note that the case ci    si   ai   v  si     listcasesepisode was built in line    of
the algorithm  replacing the action a corresponding to the case   si   a  v  si     b with the
new action ai and resulting from the application of random gaussian noise to the action a
   

figarca   fernandez

policy improvement algorithm
  
  
  
  
  
  
  
  
  
  

given the case base b  and the maximum number of cases 
given the baseline behavior t
given the update threshold 
   set maxt otalrwepisode      the maximum cumulative reward reached in an episode
   repeat
 a  initialization step 
set k      listcasesepisode    totalrwepisode    
 b  case generation 
while k   maxepisodelength do
compute the case   s  a  v  s    b closest to the current state sk


  
  
  
  

if  b  sk       then    known state
chose an action ak using equation  
perform action ak
create a new instance cnew     s  ak   v  s  

  
  
  
  
  
  

else    unknown state
chose an action ak using t
perform action ak
create a new instance cnew     sk   ak     
totalrwepisode    totalrwepisode   r sk   ak  
listcasesepisode    listcasesepisode  cnew

  
  
  
  
  
  
  
  
  
  

set k   k    
 c  computing the state value function for the unknown states 
for each instance ci in listcasesepisode


if  b  si       then    unknown state
p
return si      kj n  jn r sj   aj      n is the first ocurrence of si in the episode
v  si      return si  
 d  updating the cases in b using the experience gathered  
if totalrwepisode    maxt otalrwepisode    then
maxt otalrwepisode    max  maxt otalrwepisode  totalrwepisode 
for each case ci    si   ai   v  si     in listcasesepisode


  
  
  

if  b  si       then    known state
compute the case   si   a  v  si     b corresponding to the state si
compute    r si   ai     v  si      v  si  

  
  
  
  
  

if      then
replace the case   si   a  v  si     b with the case   si   ai   v  si     listcasesepisode
v  si     v  si     
else    unknown state
b    b  ci

  
  
  
  

if kbk    then
remove the   kbk least frequently used cases in b
until stop criterion becomes true
   return b

figure    description of step two of pi srl algorithm 

by the equation    then  the temporal distance  td  error  is computed  line      if      
performing the action ai results in a positive change for the value of a state  the action  in
   

fisafe exploration of state and action spaces in reinforcement learning

turn  could potentially lead to a higher return and  thus  to a better policy  van hasselt and
wiering        also update the value function using only the actions that potentially lead
to a higher return  if the td error  is positive  ai is considered to be a good selection and is
reinforced  in the algorithm  this reinforcement is carried out by updating the output of the
case   si   a  v  si     b at ai  line      therefore  an update to the case base only occurs
when the td error is positive  this is similar to a linear reward inaction update for learning
automata  narendra   thathachar              in which the sign of the td error is used
as a measure of success  pi srl only updates the case base when actual improvements
have been observed  thus avoiding slow learning when there are plateaus in the value space
and td errors are small  it has been shown empirically that this procedure can result in
better policies than when step size depends on the size of the td error  van hasselt  
wiering         it is important to note that these replacements produce smooth changes in
the case base b since an action a is replaced only if ai results in a higher v  si   and ai  a 
this form of updating can be understood as a risk seeking approach  overweighting only
transitions to successor states that promise an above average return  mihatsch   neuneier 
       additionally  it prevents the degradation of b  ensuring that replacements are made
only when an action can potentially lead to a higher v  si   
if  instead  si is not a known state  the case ci is added to b  line      finally  the
algorithm removes cases from b if necessary  line      complex scoring metrics to calculate
which cases are to be removed for a given moment have been proposed by several authors 
forbes and andres        suggest the removal of cases that contribute least to the overall
approximation  while driessens and ramon        pursue a more error oriented view and
propose the deletion of cases that contribute most to the prediction error of other examples 
the principal drawback of these more sophisticated measures is their complexity  the
determination of the case s  to be removed involves the computation of a score value for
each ci  b  which in turn requires at least one retrieval and regression  respectively  for
each cj  b  j    i   such entire repeated sweeps through the case base entail an enormous
computational load  gabel and riedmiller        compute a different score metric for
each ci  b  requiring the computation of the set of the k nearest neighbors around ci  
such approaches are not well suited to systems learning with adjusted time requirements
and with a high dimensional state space  requiring the use of larger case bases than those
proposed here  rather  in this paper  we propose the removal of the least frequently used
cases  the idea seems intuitive insofar as the least frequently used cases usually contain
worse estimates of a corresponding states value  although the strategy might lead to a
function approximator that forgets some of the valuable experience made in the past
 e g   corner cases   despite this  pi srl performs successfully in all domains proposed
using the strategy  as demonstrated in section    thus  the ability to forget ineffectual
known states described in section   is a result of the algorithm removing kbk   cases
from the least frequently used cases of b 
    parameter setting design
one of the main difficulties of applying the pi srl algorithm to a given problem is to
decide on an appropriate set of parameter values for the threshold   the risk parameter  
the update threshold  and the maximum number of cases   an incorrect value for the
   

figarca   fernandez

parameter  can lead to mislabeling a state as known when it is really unknown  potentially
leading to damage or injury in the agent  in the case of the risk parameter   high values
can continuously result in damage or injury  while low values are safe  but do not allow for
exploration of the state action space sufficient for reaching a near optimal policy  unlike
 and   the parameter  is not related to risk  but instead is directly related to the
performance of the algorithm  parameter  is used to determine how good an episode
must be with respect to the best episode obtained  since only the best episodes are used to
update the case base b  if the  value is too large  bad episodes may be used to update b
 influencing the convergence and performance of the algorithm   if  instead   is too low 
the number of updates in b may be insufficient for improving the baseline behavior  finally 
a very high  value allows for large case bases  increasing the computational effort during
retrieval and degrading the efficiency of the system  by contrast  a very low  value might
excessively restrict the size of the case base and thus negatively affect the final performance
of the algorithm  in this subsection  a solid perspective is given on the automatic definition
of these parameters  the parameter setting proposed here are taken as a suitable set of
heuristics tested successfully in a wide variety of domains  section    
 parameter   the parameter is domain dependent and related to the average size
of the actions  in this paper  the value for this parameter has been established by
computing the mean distance between states during an execution of the baseline
behavior t   expressed in another way  the execution of the policy t provides a
state action sequence of the form s   a   s   a          sn   thus  the value of
 is computed using equation   

 

dist s    s              dist sn    sn  
n 

   

 parameter   several authors agree that it is impossible to completely avoid all
accidents  moldovan   abbeel        geibel   wysotzki         it is important to
note that pi srl is completely safe only if the first step of the algorithm is executed 
however  by proceeding in this way  the performance of the algorithm is heavily
limited by the abilities of the baseline behavior  the running of the subsequent
exploratory process is inevitable if learner performance is to be improved beyond that
of the baseline behavior  since the agent operates in a state of incomplete knowledge
of the domain and its dynamic  it is inevitable during the exploratory process that
unknown regions of the state space will be visited where the agent may reach an error
state  however  it is possible to adjust the risk parameter  to determine the level
of risk assumed during this exploratory process  in this paper  we start with low 
values  low risk  which we gradually increase  specifically  we propose beginning with
          and increasing this value iteratively until either an accurate policy is
obtained or the amount of damage or injury is high 
 parameter   the value of this parameter is set relative to the best episode obtained 
in this paper  the  value is set to    of the cumulative reward of the best episode
obtained 
   

fisafe exploration of state and action spaces in reinforcement learning

figure    trajectories generated by the baseline policy t in a deterministic  slightly
stochastic and highly stochastic domain 
 parameter   previously  we estimated the maximum number of cases  to be stored
in the case base as being the estimated maximum number of cases required to properly mimic the baseline behavior t   what follows is a description of how this value is
computed  figure   presents the trajectories  sequences of states  followed by the baseline policy t in three different domains  deterministic  slightly stochastic and highly
stochastic  for each domain  different sequences of the states produced by t are
represented  s     s     s             s n     s     s     s             s n           s     sm    sm            smn   
where sji is the i th state  s   the initial state and sjn the final state of the resulting
trajectory in episode j  in the deterministic domain  the m different executions of t
always result in the same trajectory  in this case  we set the maximum number of
cases to    n with all the cases computed in the episode being stored 
in the slightly stochastic domain  the trajectories produced in m different episodes
are different  but only slightly so  here  we suppose the case base at the beginning to
be empty  additionally  we assume that all states  s     s     s             s n   corresponding to the first trajectory produced in the domain will be stored in the case base 
furthermore  for each domain we execute m different episodes  obtaining m different
trajectories  following the execution of the m episodes  we compute the maximum distance between the i th state of the first trajectory  previously added to the case base 
and the i th state produced in the trajectory j such that max jm d s i   sji    in the
slightly stochastic domain  this maximum distance does not exceed the threshold  in
any case such that max jm d s i   sji       at this point  we assume the i th state in
trajectory j to have at least one neighbor with a distance less than   corresponding
to the state s i    thus  the i th state in j is not added to the case base 
by contrast  in a highly stochastic domain  this maximum distance greatly exceeds the
threshold  in all the cases such that max jm d s i   sji        in this domain  we
estimate the total number of cases that will be added to the case base in the following
   

figarca   fernandez

way  for each i th state in the sequence of the
j first trajectory k we estimate the number
max jm d s i  sji  
of cases to be added to the case base as
or  in other words  we

compute the number of intervals in the range     max jm d s i   sji    with a width of
  the threshold used to decide whether a new case is to be added or not to the casebase   consequently  the estimated number of cases addedjto the case base  taking
into
k
pn
max jm d s i  sji  
account all states in the sequence  is computed as i  
  finally 

the estimated maximum number of cases is computed as shown in equation   

  n 


n 
x
max jm d s i   sji  


i  

 
   

it is important to remember that in a deterministic domain  the summation in equation   is equal to   and that  therefore     n  the increase of the value of this element
is related to the increase of stochasticity of the environment  insofar as the greater
stochasticity of the environment increases the number of cases required  finally  if
the number of cases is very large or nearly infinite  the threshold  can be increased
to make more restrictive the addition of new cases to the case base  however  this
increase may also adversely affect the final performance of the algorithm 

   experimental results
this section presents the experimental results collected from the use of pi srl for policy
learning in four different domains presented in order of increasing complexity  i e   increasing number of variables describing states and actions   the car parking problem  lee  
lee         pole balancing  sutton   barto         helicopter hovering  ng et al        
and the business simulator simba  borrajo et al          for each of these domains 
we have proposed the learning of a near optimal policy which minimizes car accidents 
pole disequilibrium  helicopter crashes and company bankruptcies  respectively  during the
learning phase  all four of the domains are stochastic in our experimentation  while both
helicopter hovering and the business simulator simba are  in themselves  stochastic and 
additionally  generalized domains  we have made the car parking and pole balancing domains stochastic with the intentional addition of random gaussian noise to the actions and
reward function  the results of pi srl in the four domains are compared to those yielded
by two additional techniques  namely  the evolutionary rl approach selected winner of the
helicopter domain in the      rl competition  martn h    lope        and geibel and
wysotzkis risk sensitive rl approach  geibel   wysotzki         in the evolutionary approach  several neural networks cloning error free teacher policies are added to the initial
population  guaranteeing rapid convergence of the algorithm to a near optimal policy and 
indirectly  minimizing agent damage or injury   indeed  as the winner of the helicopter
domain is the agent with the highest cumulative reward  the winner must also indirectly
minimize helicopter crashes insofar as these incur large catastrophic negative rewards  on
the other hand  the risk sensitive approach defines risk as the probability   s  of reaching
a terminal error state  e g   a helicopter crash ending agent control   starting at some initial
   

fisafe exploration of state and action spaces in reinforcement learning

state s  in this case  a new value function with the weighted sum of the risk probability 
   and value function  v    is used  equation    
v  s    v   s     s 

   

the parameter     determines the influence of the v   s  values compared to the   s values  for       v corresponds to the computation of minimum risk policies  for large 
values  the original value function multiplied by  dominates the weighted criterion  while
geibel and wysotzki        consider only finite  discretized  action sets in their study 
their algorithm has been adapted here for continuous action sets  we use cbr for value
and risk function approximation and a gaussian exploration around the current action  in
the experiments  for each domain  three different  values are used  modifying the influence
of the v  values compared to the  values  in all cases  the goal is to improve the control
policy while  at the same time  minimizing the number of episodes with agent damage or
injury  in each domain  we establish different risk levels by modifying risk parameter 
values according to the procedure described in subsection      it is important to note that
one baseline behavior used to initialize the evolutionary rl approach is exactly the same as
that used subsequently in the first and second step of pi srl  furthermore  the case base
in the risk sensitive approach does not begin from scratch since it is initialized with the safe
   this makes the comparison of performances as fair as possible  but
case based policy b
taking into account that the different techniques make its own use of the baseline behaviors 
    car parking problem
the car parking problem is represented in figure    and originates from the rl literature  cichosz         a car  represented as the rectangle in figure     is initially located
inside a bounded area  represented by the dark solid lines  referred to as the driving area 
the goal for the learning agent is to navigate the car from its initial position into the garage 
such that the car is entirely inside  in a minimum number of steps  the car cannot move
outside of the driving area  figure     b  shows the two possible paths the car can take from
the starting point to the garage with an obstacle in between in order to correctly perform
the task  we consider the optimal policy for the domain to be that which reaches the goal
state in the shortest time and which  at the same time  is free of failures 
the state space of the domain is described by three continuous variables  namely  the
coordinates of the center of the car xt and yt and the angle t between the cars axis and
the x of the coordinate system  while the car can be modeled essentially with two control
inputs  speed v and steering angle   let us suppose here that the car is controlled only by the
steering angle  i e   it moves at a constant speed   thus  the action space is described by one
continuous variable at         corresponding to the turn radius  as used in the equations
below  the agent receives a positive reward value of r        dist pt   pg          where
pt    xt   yt   is the center of the car  pg    xg   yg   is the center of the garage  i e   the goal
position  and  is a normalizing function scaling the euclidean distance dist pt   pg   between
pt and pg to a range        when the car is inside the garage  i e   the reward value is greater
if the car is parked correctly in the center of the garage   the agent receives a reward of
   whenever it hits the wall or obstacle  all other steps receive a reward of       thus  the
difficulty of the problem lies not only in the reinforcement delay  but also in the fact that
   

figarca   fernandez

figure     car parking problem   a  model of the car parking problem   b  examples of
trajectories generated by the agent to park the car in the garage 
punishments are much more frequent than positive rewards  i e   it is much easier to hit
a wall than park the car correctly   the motion of the car is described by the following
equations  lee   lee       
t     t   v   l    tan   at   

   

xt     xt   v cos t     

   

yt     yt   v sin t     

    

where v is the linear velocity of the car  assumed to be a constant value    is the
maximum steering angle  i e   the car can change its position by a maximum angle of 
in both directions  and  is the simulation time step  gaussian noise was added to the
actions and rewards with a standard deviation of      since noisy interactions are inevitable
in most real world applications  adding this noise to the actuators and the environment 
we transform the deterministic domain into a stochastic domain  it is important to note
that the noise added to transform the domain into a stochastic domain is independent of
the gaussian noise with standard deviation   risk parameter  used to explore the state and
action space in the second step of the pi srl algorithm  in this case  the gaussian noise
with standard deviation  used for exploration will be added to the noise previously added
to the actuators  in this paper  l      m   v        m s            rad  and         s 
 the driving area and obstacle dimensions are detailed in figure     a    the initial position
of the car is fixed at xs        ys       and s         rad   while the goal position is
xg        and yg         for this domain  we have designed a baseline behavior t with
an average cumulative reward per trial of      
in order to perform the pi srl algorithm  the modeling baseline behavior step is exe learned from demonstrations
cuted  the result of this step is the safe case based policy b
provided by the baseline behavior t  see subsection        and  were computed following
the procedure described in subsection     with resulting values of      and      respectively 
   

fisafe exploration of state and action spaces in reinforcement learning

figure     car parking task modeling baseline behavior step   a  number of steps per
trial executed by case base b and the baseline behavior t    b  cumulative
reward per trial by the baseline behavior t   the learned safe case based policy
 and an ibl approach 
b
figure     a  graphically represents the execution of the modeling baseline behavior step  in
it  two different learning processes are presented and  for each one  the number of steps per
trial executed by the baseline behavior t  continuous red lines  and the cases in b  dashed
green lines  is shown  at the beginning of the learning process with an empty case base b 
all steps are performed using the baseline behavior t   as the learning process continues 
 is learned  at around the trials
new cases are added to b and the safe case based policy b
       practically all steps are performed using the cases in b and t is rarely used  that
means that a safe case based policy has been learned  in the two learning processes shown
in figure     a   the modeling baseline behavior step is performed without collisions with
the wall or the obstacle  in other words  the baseline behavior t is cloned safely without
errors  figure     b  shows the cumulative reward for three different execution processes 
the first  continuous red lines  corresponding to the performance of the baseline behavior
t   the second  dashed green lines  corresponding to the previously learned safe case based
  derived from b   and the third  dashed blue lines  corresponding to an instancepolicy b
based learning  ibl  approach consisting of storing cases in memory  in the ibl approach 
new items are classified by examining the cases stored in memory and determining the most
similar case s  given a particular similarity metric  euclidean distance is used in this paper   the classification of that nearest neighbor  or those nearest neighbors  is taken as the
classification of the new item using a   nearest neighbor strategy  aha   kibler         for
each approach  two different executions are carried out  in the ibl approach  the training
process is performed saving all training cases produced by the baseline behavior t during
   trials  so we consider this approach an ib  algorithm in the sense that it saves every
case during the training phase  see aha   kibler         figure     b  shows that the safe
case based policy b almost perfectly mimics the behavior of the baseline behavior t   in
the domain  the performance of the ib  approach is also similar 
figure     a  shows the results for different risk configurations obtained by the improving
the learned baseline behavior step  for each risk configuration  two different learning pro   

figarca   fernandez

figure     improving the learned baseline behavior step in car parking problem   a  cumulative reward per episode for different risk configurations    obtained by
pi srl   b  cumulative reward per episode by the evolutionary rl and risksensitive rl approaches  in all cases  any episode ending in failure is marked 
cesses are performed  all trials ending in failure  car hits the wall or obstacle  are marked
 blue triangles   the learning processes in figure     a  demonstrate that the number of
failures increases with an increase in the parameter   for a low level of risk              
although no failures are produced  the performance is nevertheless weak  around the baseline behavior t   and constant throughout the whole of the learning process  additional
experiments have demonstrated that increasing the  value above         increases the
number of failures without improving performance  figure     b  shows the results for the
evolutionary and risk sensitive rl approaches for different  values  regarding the former 
the number of failures is higher than that obtained by the pi srl approach  while its final
performance is similar  in the case of the latter  performance is higher when         value
maximization   yet the agent consistently crashes the car into the wall 
figure    shows the mean number of failures  i e   car collisions  and cumulative reward
for each approach over     trials with the red circles corresponding to the pi srl algorithm 
the black triangles to the risk sensitive approach and the blue square to the evolutionary
rl approach  additionally  figure    shows two asymptotes  the horizontal asymptote
is established according to the cumulative reward obtained by the highest  value  the
horizontal asymptote indicates that higher  values increase the number of failures without
improving the cumulative reward  which may  in fact  get worse   the vertical asymptote
at f ailures     indicates that reducing the risk parameter  does not reduce the number
of failures  figure    also shows the performance for two additional risk levels  a very
high level of risk              and very low level of risk         with respect to
figure     when using a very low level of risk       no additional random gaussian
noise is added to the actions and the algorithm is free of failures  although performance
 learned in the first step
does not improve with respect to the safe case based policy b
of the algorithm  pi srl with a medium level of risk              also is free of
failures  yet performance is also slightly improved  the pi srl algorithm with high level
of risk              obtains the highest cumulative reward           with a mean of
   

fisafe exploration of state and action spaces in reinforcement learning

figure     mean number of failures  car collisions  and cumulative reward over     trials
for each approach in car parking task  the means have been computed from   
different executions 

     failures  however  when using a very high level of risk               the number of
failures greatly increases and  consequently  the cumulative reward decreases  as shown in
figure     pi srl with high risk              and the evolutionary rl approach obtain
a similar performance  while pi srl demonstrates a faster convergence  thus  in figure    
the cumulative reward obtained by pi srl is higher   the pareto comparison criterion can
be used to compare the solutions in figure     using this principle  one solution y  strictly
dominates  or is preferred to  a solution y if each parameter of y  is not strictly worse
than the corresponding parameter of y and at least one parameter is strictly better  this
is written as y   y  indicating that y  strictly dominates y  in accordance with the pareto
principle  we can assume the points in figure    corresponding to the pi srl solutions 
save pi srl with very high level of risk  to be on the pareto frontier  since these points are
not strictly dominated by any other solution  i e   no other solution has  at the same time  a
higher cumulative reward and a lower number of failures than pi srl   in this domain  the
solution of the pi srl with a medium level of risk strictly dominates  or is preferred to 
the risk sensitive solutions  pi srl            risk sensitive  and the solution pi srl
with a high level of risk strictly dominates the solution of the evolutionary rl solution
 pi srl            evolutionary rl  
nevertheless  it is important to note that any ultimate decision about which approach
in figure    is best depends on the criteria of the researcher  if  for instance  the minimization of the number of failures is deemed the most important optimization criterion
 independently of the improvement obtained with respect to the baseline behavior t    the
best approach will be pi srl with a low level of risk               similarly  if the
maximization of the cumulative reward is instead judged to be the most important optimization criterion  independently of the number of failures generated   the best approach
will be pi srl with a high level of risk              
figure    shows the evolution of the cases in the case base b  known space  in different
trials for a high risk learning process  each graph presents the set of known states   green
   

figarca   fernandez

figure     car parking problem  evolution of the known space for different trials t    
 a   t       b   t        c  and t        d  in a high risk learning process
              each graph corresponds to the situation of the state space in
accordance with the case base b in trial t  
area   error states   red area   unknown states   yellow area  and non error states 
 orange circles   pi srl adapts the known space in order to find safer and better policies
to complete the task  figure     a  shows the initial situation of b  corresponding to the
    it is robust in the sense that it never results in
previously learned safe case based policy b
any collisions  but suboptimal  it selects the longest parking path driving around the upper
side of the obstacle   as the learning process progresses  figure     b    pi srl finds a
shorter path to park the car in the garage along the upper side of the obstacle  increasing the
performance   but which comes closer to the obstacle than before  increasing the probability
of collisions   in figure     c   pi srl finds a new and even shorter path  this time along
the lower side of the obstacle  however  there are still cases in the case base b corresponding
to the older path along the upper side of the obstacle  so figure     c  indicates two paths
to park the car   finally  in figure     d   the cases corresponding to the suboptimal path
along the upper side of the obstacle have been removed from b and replaced by new cases
corresponding to the safe and improved path along the lower side of the obstacle  in other
words  pi srl adapts the known space through the exploration of the unknown space in
order to find new and improved behaviors  during this process of adjusting the known space
   

fisafe exploration of state and action spaces in reinforcement learning

to safe and better policies  the algorithm forgets the previously learned  yet ineffective
known states 
in the following experiment  it becomes apparent that if the domain is noisy enough  even
when taking no risk at all  i e   no further noise added to the actuator for exploration   the
agent could nevertheless perform poorly and constantly produce collisions  the experiment
also serves to explain why domain noise can never be sufficient for the efficient exploration
of the space without action selection noise  in the experiment  we have intentionally added
more noise to the actuators and have performed second step of pi srl again  however this
time taking no risk  i e          in this test  we have added random gaussian noise with
a standard deviation of      rather than the standard deviation of     used previously  to
the actuators  figure    shows two executions of the second step  improving the learned
baseline policy  of the pi srl algorithm with the x axis indicating the number of trials 
the y axis the cumulative reward per episode and failures  i e   collisions  marked as blue
triangles  in the experiments in figure     b   the case based policy b with low level
of risk              never produces failures  in contrast  in the experiments shown in
figure     the same case based policy b continually collides with the wall although the
risk parameter is set to           furthermore  an increase in the performance can also be
detected 

figure     improving the learned baseline behavior step of car parking task  two learning processes for risk configuration      and an increase in the noise in the
actuators 

the increase of noise in the actuators in the second step of the algorithm with respect
to the first step  the case based policy b is learned in the first step using gaussian random
noise in the actuator with a standard deviation of      while the second step is performed
using gaussian random noise in the actuator with a standard deviation of      takes the
agent beyond the known space of the case base b learnt in the first step of pi srl and
allows it to find new trajectories for parking the car in the garage  in this new situation  the
exploration process is guided as follows  if a known state is reached  the agent performs the
action a retrieved from b without the addition of gaussian noise  since the risk parameter
      see line    in figure   algorithm   if an unknown state is reached  the agent performs
   

figarca   fernandez

the action a advised by the baseline behavior t  see line      using this exploration
process  if a new and better trajectory is found for parking the car in the garage  the
resulting cases in the episode corresponding to unknown states are added to the case base
 see line      slightly improving the performance in figure     it is important to note that
the replacements of cases  see line     does not change the actions in b  since these are
replaced by the same action previously retrieved from b plus a certain amount of gaussian
noise with standard deviation   see line      nevertheless  given that the risk parameter 
has been set to    the actions retrieved from the case base are not replaced  this exploration
process  however  with       i e   taking no risk  does not lead to optimal behavior since 
 the actions performed in unknown situations and added to the case base b are performed using the baseline behavior t which is supposed perform suboptimal actions
 see definition of baseline behavior  
 the actions in the cases of b are not replaced with improved actions  the gaussian
noise with standard deviation  is used to explore different and better actions than
those provided by b and t   however  in this case  the risk parameter is set to     
and new and better actions are not discovered 
additional experiments demonstrate that pi srl behaves much worse when a higher
value of noise is used in the actuators  with collisions in all episodes   we assume that taking
no risk  i e         implies always performing the same actions while not discovering any
newer or better actions than those provided by the learned case base b and the baseline
behavior t   in pi srl  the replacements in the case base are executed towards the more
promising action which  in our case  is that which guarantees a higher return  this is
why exploration is necessary in order to obtain  near  optimal behavior  since without
exploration  new and better actions are not discovered and pi srl performance is limited
by that of the case based policy learned in the first step b and the baseline behavior t
which  one must remember  is intended to perform suboptimal policies 
    pole balancing
as the name suggests  the objective in the pole balancing problem is to balance a pole
vertically on top of a moving cart  sutton   barto         the state description consists
of a four dimensional vector containing the angle   the radial speed     the cart position
x and the speed x    the action consists of a real valued force that is used to push the cart 
in this study  the reward is computed to encourage actions that keep the pole as upright as
possible on the cart and the cart as centered as possible on the track  thus  the reward in
step t is computed as rt        t      xt       where  and  are normalizing functions
scaling the angle t and the position xt to a range         an episode is composed of       
steps  although it may nevertheless end prematurely if the pole becomes unbalanced  i e  
if it has an inclination of more than twelve degrees in either direction  or the cart falls off
the track  i e   if it is more than    m from the center of the track   both of which being
considered failures  as in the car parking problem  gaussian noise was added to the actions
and rewards  this time with a standard deviation of       the pole balancing domain
becomes stochastic through the addition of this noise to the actuators and reward function 
   

fisafe exploration of state and action spaces in reinforcement learning

figure     modeling baseline behavior step in pole balancing task   a  number of steps per
trial executed by case base b and baseline behavior t    b  cumulative reward
 and an ibl approach 
per trial for t   the learned safe case based policy b
the hand made baseline behavior t demonstrates the execution of a safe  yet suboptimal
policy  with an average cumulative reward per episode trial of      
 is learnt
in the modeling baseline behavior step of pi srl  the safe case based policy b
from demonstrations provided by the baseline behavior t    and  were computed following
the procedure described in subsection      with values of      and        respectively 
figure     a  shows two different learning processes for the modeling baseline behavior step 
for each learning process  figure     a  shows the number of steps per trial executed by
baseline behavior t  continuous red lines  and by the case base b  dashed green lines   at
the beginning of the learning process  the case base b is empty and all steps are performed
using the baseline behavior t   as the learning process progresses  however  b is filled and
 is learnt  at the end of the learning process  after around
the safe case based policy b
      trials   almost all steps are performed using the cases in b and t is rarely used  it
is important to note that the modeling baseline behavior step has been performed without
failures  i e   pole disequilibrium or cart off the track  in each case  as with the previous
task  figure     b  represents three independent execution processes using the previously  derived from b and indicated with dashed green lines  
learned safe case based policy b
the baseline behavior t  indicated with continuous red lines  and an approach based on
ibl  indicated with dashed blue lines   aha   kibler         the average cumulative
 is       figure     b    while   almost perfectly clones    the
reward per episode in b
t
b
ib  approach which  in most cases  results in pole disequilibrium or the cart falling off the
track averages a cumulative reward per episode of      
figure     a  shows the results of pi srl for different risk configurations  for each
configuration  the learning curves are shown for two different learning processes performed 
additionally  any episode ending in failure is marked  blue triangles   while an increase in
risk increases the probability of failure  the policy obtained is nevertheless better in terms of
the cumulative reward  nevertheless  much greater risk values              produce more
failures without an accompanying increase in the cumulative reward  figure     b  shows
the results for the evolutionary and risk sensitive rl approaches  the former of which being
   

figarca   fernandez

figure     improving the learned baseline behavior step of pole balancing task   a  cumulative reward per episode for different risk configurations    obtained by pisrl   b  cumulative reward per episode obtained by the evolutionary and risksensitive rl approaches  in all cases  any episode ending in failure is marked 

clearly the algorithm with the greatest number of failures  in the risk sensitive approach 
for         value maximization   the agent selects actions that result in a higher value 
but also in a higher risk  on the contrary  for       risk minimization   when the agent
learns the risk function  at around episode        it selects actions with a lower risk  and a
lower number of failures   but also with considerably weak performance  the value       
produces an intermediate policy  consequently  it can be concluded that pi srl with a high
level of risk obtains better policies and less failures than the evolutionary or risk sensitive
rl approaches  figure    reinforces the previous conclusions 

figure     mean number of failures  pole disequilibrium or cart off the track  and cumulative reward during     trials for each approach in the pole balancing task  the
means have been computed from    different executions 
   

fisafe exploration of state and action spaces in reinforcement learning

in it  the mean number of failures and cumulative reward during        trials are shown 
with the red circles corresponding to pi srl  the black triangles corresponding to the risksensitive approach and the blue square corresponding to the evolutionary rl approach 
the figure also shows performance for two additional risk levels  a very high level of risk
             and very low level of risk         with respect to figure     the cumulative
reward and number of failures increase with the high level of risk               this
risk level represents an inflection point at which higher levels of risk produce more failures
without an accompanying improvement in the cumulative reward  in fact  the very high level
of risk              results in a reduction in the cumulative reward when compared with
the high level of risk               again  the pareto comparison criterion may be used
to compare the solutions from figure     in this domain  the solution from pi srl with
a low level of risk strictly dominates the risk sensitive solutions with        and        
such that pi srl            risk sensitive with        and         additionally  the
solution from pi srl with a high level of risk strictly dominates evolutionary rl solution 
such that pi srl            evolutionary rl 
lastly  figure    shows the evolution of the known space derived from the case base
b in different trials for a high risk learning process  for each graph  error states   red
area   the set of unknown states   yellow area   the set of known states   green area  and
the set of non error states   orange circles  are represented  the known space  in each
graph has been computed taking cases from b in the trials t                 and       for
each graph  non error states  have been computed from    different executions of b in
the trial t  the orange circles representing the terminal states for each of these executions  
the first graph  figure     a   presents the initial known space resulting from the modeling
baseline behavior step  the evolution in figure    demonstrates two different points  first 
pi srl progressively adapts the known space in order to encounter better behavior such
that the known space tends to be compressed toward the center of the coordinates  this
is so due to the fact that the reward is greater if the angle  of the pole and the cart
position x are    i e   the pole is as upright as possible on the cart and the cart is centered
on the track   second  the risk of failure in the pole balancing domain is greater during
early trials of the learning process  at the beginning of the learning process  figure     a   
t       some regions of the known space are close to the error space  in this situation 
slight modifications of the actions consistently produce visits to the states in   i e   pole
disequilibrium or cart falling off the track   as the learning process advances  figure   
 b    c  and  d    the known space is compressed toward the origin of coordinates and away
from the error space  consequently  the probability of visiting error states decreases  for
example  returning to figure     a   in the high risk learning processes      of the failures
      occur in the first      trials  while the remaining           occur in the last     
trials 
    helicopter hovering
as suggested by its name  the objective of this domain is to make a helicopter hover as close
as possible to a defined position for a duration established by an episode  the task is challenging for two main reasons  firstly  both the state and action spaces are high dimensional
and continuous  more specifically  the state space is    dimensional and the action space
   

figarca   fernandez

figure     pole balancing task  evolution of the known space for different trials t      a  
t         b   t         c  and t         d  in a high risk learning process
              each graph corresponds to the situation of the state space
according to the case base b in trial t  
  dimensional   secondly  it is a generalized domain whose behavior is modified by the wind
factor  a helicopter episode is composed of      steps  although it may end prematurely if
the helicopter crashes  the first step of pi srl is performed in order to imitate the baseline
behavior t    and  were computed following the procedure described in subection    
with values of     and        respectively  once this step has been performed  the resulting
 is able to properly imitate the baseline behavior   
safe case based policy b
t
figure     a  shows two learning processes of the modeling baseline behavior step 
similar to previous tasks  as the learning processes progress  the number of steps executed
by the baseline behavior t is reduced while the number of steps using the case base b
increases  by the end of the learning process  the case base b stores the safe case based
   figure     b  compares the performance  in terms of cumulative reward per
policy b
 and the ib  approach  regarding the
episode  of t   the learned case based policy b
first two  the average cumulative reward per episode of t is            while that obtained
 is            although the   does not perfectly mimic the baseline behavior   
by b
t
b
   

fisafe exploration of state and action spaces in reinforcement learning

figure     modeling baseline behavior step of helicopter hovering task   a  number of
steps per trial executed by case base b and baseline behavior t    b  cumula and an ibl
tive reward per trial by t   the learned safe case based policy b
approach 
it nevertheless performs a safe policy without crashing the helicopter  with regard to the
training process of the ib  approach  every case produced during    episodes by the baseline
behavior t is stored  figure     b  demonstrates that the ib  approach consistently results
in helicopter crashes  with a performance extremely far from that of the learned safe case   improvement of the policy   begins when the state action space is safely
based policy b
b
explored through the execution of step two of pi srl 
figure     a  shows the results for different risk levels  while pi srl low and medium
levels of risk levels do not produce helicopter crashes in pi srl  performance is nevertheless
quite weak 

figure     improving the learned baseline behavior step in helicopter hovering task   a 
cumulative reward per episode for different risk configurations obtained by pisrl   b  cumulative reward per episode obtained by evolutionary and risksensitive rl approaches  in all cases  any episode ending in failure is marked 
   

figarca   fernandez

conversely  the high level of risk established produces a near optimal policy with a
low number of collisions  extensive experimentation demonstrates that increasing the risk
parameter           also increases the number of crashes without an accompanying
improvement in the cumulative reward  figure     b  shows the results of the evolutionary
rl approach which  it should be remembered  was selected winner of the rl competition
     in the same domain  martn h    lope         as well as the risk sensitive rl algorithm
for different  values  a comparison of the results between the evolutionary rl approach
and pi srl shows a similar cumulative reward  while also a significantly higher number
of crashes from the former than from the latter  in the evolutionary approach  all crashes
occur in the early steps of the learning process  while in pi srl  accidents occur at more
advanced steps of the learning process  in the case of the risk sensitive rl algorithm  for
     and         the risk function is learned at around episode       at this point  the
agent selects lower risk actions and the number of crashes is considerably reduced  when
       and the agent selects actions resulting in higher values without taking risk into
account  performance improves  but at the expense of an increased number of accidents 
nevertheless and whatever the  value  the number of crashes is higher and the performance
is worse than with pi srl 

figure     mean number of failures  helicopter crashes  and cumulative reward during     
episodes for each approach to the helicopter hovering task  the means have been
computed from    different executions 
the information from figure     indicating the mean number of failures and cumulative
reward over      episodes for each approach  complements the conclusions made above 
the data has been computed from    independent executions of each approach  as in
previous domains  pi srl is indicated by red circles  the risk sensitive approach by the
black triangles and the evolutionary rl approach by the blue square  figure    also shows
the performance for two additional risk levels  a very high level of risk             
and a very low level risk         with respect to figure     figure    demonstrates
that the evolutionary rl approach obtains the highest cumulative reward              
followed closely by pi srl               the other approaches are far from these results 
regarding the number of failures  i e   helicopter crashes   as pi srl with a very low level
of risk         a low level of risk              and a medium level of risk             
   

fisafe exploration of state and action spaces in reinforcement learning

produces no collisions  the pi srl algorithm with medium risk is preferable inasmuch as
the cumulative reward is higher                using the pareto comparison criterion 
the pi srl solution with a high level of risk strictly dominates the solutions of the risksensitive approach  pi srl            risk sensitive   moreover  pi srl is not strictly
dominated by any other solution 

figure     evolution of the known space for different episodes in the helicopter hovering
task   a  example of representation of a single known state in a radar chart   b  
 c   and  d  known states in episodes t      t       and t         respectively 
in a high risk learning process               each graph corresponds to the
situation of the known space according to the case base b in episode t  
as with the pole balancing domain  figure    shows the evolution of the known space
according to the case base b in different episodes for a high risk learning process  in this
case  radar charts are used due to the high number of features describing the states  a
radar chart is a graphical method for displaying multivariate data two dimensionally  in the
figure  each axis represents one of the features of the state and  to preserve the simplicity of
the representation  the charts are generated normalizing the absolute values of the features
between   and    figure     a  is an example of a representation of a single known state 
   

figarca   fernandez

the value of each axis corresponds to the value of an individual feature in a state and a
line is drawn connecting the feature values for each axis  while the line in figure     a 
represents a single state  figures     b    c  and  d  show the known space according to the
case base b in episodes        and       respectively  these three charts do not represent
a single state  but rather all the states in b for the corresponding episode  thus  for each
graph  the set of known states is marked   green area   a state is considered an error state
if a single feature value for that state is greater than    the limits  marked by a red line in
the graphs  have been computed taking into account that the helicopter crashes if  i  the
velocity along any of the main axes exceeds   m s   ii  the position of the helicopter is off by
more than    m   iii  the angular rate around any of the main axes exceeds      rad s or
 iv  the orientation is more than    degrees from the target orientation  as with previous
tasks  figure    indicates two different matters  first  as the learning proceeds  the known
space derived from b is adjusted to the space used for better and safer policies  in the
helicopter domain  the agent tries to hover the helicopter as close as possible to a target
position  i e   the origin of coordinates   since the immediate rewards are greater the closer
the helicopter hovers to the origin  thus  the known space starts to expand  figure   
 b   and  progressively  is concentrated at the origin of coordinates  figure     c  and  d   
with regard to the second matter  the probability of crashing is very low since  from the
very beginning  the known space already appears concentrated at the origin and far from
the error space  figure     b    in other words  from the very beginning  all features of the
known space  i e   forward  sideways and downward velocities  x  y  and z coordinates  x 
y and z angular rates  and x  y and z quaternation  are very far from error space limits 
decreasing the probability of visiting an error state 
in the previous experiments  the second step of pi srl has been performed using an
initial case base b free of failures that is built into the first step of the algorithm  the
following experiments show the performance of the second step of pi srl when different
initial policies are used  figure     a  shows the performance of these policies used as initial
policies  the continuous black line indicates the performance of the initial safe case based
policy b   with an average cumulative reward per episode of             used in the previous
experiments prior to the execution of step two in the algorithm  the remaining lines in the
figure correspond to the performance of three different initializations of the case base b
used in the new experiments  prior to the execution of step two of the algorithm  using a
very poor initial policy  dashed green lines  with which the helicopter crashed in nearly all
of the episodes  the average cumulative reward per episode was calculated at             
using a different poor  albeit less poor  initial policy  continuous red lines  with which the
helicopter crashed occasionally  the average cumulative reward per episode was            
finally  a near optimal policy  dashed blue lines  whereby helicopter hovering is free of
failures yields an average cumulative reward per episode of           
the figure     b  shows performance in the second step  improving the baseline behavior
step  of pi srl  starting from a case base b corresponding to the very poor  poor and the
near optimal policies presented in figure     a   in figure     b   the dashed blue lines
correspond to the use of a case base b containing the near optimal policy  the continuous
red lines correspond to the use of a case base b containing the poor policy and the dashed
green lines correspond to the use of a case base b containing the very poor policy  all the
experiments in the figure have been conducted using a high level of risk in the domain
   

fisafe exploration of state and action spaces in reinforcement learning

figure      a  the performance of different initial policies in the helicopter hovering task 
 b  the performance of different executions of the second step of pi srl  each
starting from a case base b containing a policy of three different types  very
poor  poor and near optimal 

              the graph indicates that with the use of a near optimal policy for an initial
policy and a high level of risk level  the case base does not worsen performance which  in
fact  appears to improve slightly  the second step of pi srl prevents the degradation of
the initial performance of b  since no updates of cases in the case base are made using bad
episodes  in other words  the updates in b are made with the cases gathered from episodes
with a cumulative reward similar to that of the best episode found at a particular point and
using a threshold   whose value is set to    of the cumulative reward of the best episode  
for example  if the cumulative reward of the best episode is            only the episodes
with a cumulative reward higher than         are used to update the case base  discarding
the bad episodes or other episodes with failures   in this way  good sequences of experiences
are provided to the updates  since it has been proven that good sequences of experiences
can cause an adaptive agent to converge to a stable and useful policy  while bad sequences
may cause an agent to converge to an unstable or poor policy  wyatt         the solid red
lines in figure     b  show that using a poor policy with failures as initial policy produces
a higher number of failures than using an initial policy that is free of failures  however and
despite the poor initialization  pi srl is nevertheless able to learn a near optimal policy as
well as when a policy free of failures is used to initialize b  see lines corresponding to a high
level of risk              in figure     a    finally  the dashed green lines in figure   
 b  show that the use of a very poor initial policy with many failures results in decreased
performance and a higher number of failures produced  even though it is nevertheless able
to learn better behavior  in this case  the algorithm falls into a local minimum  probably
biased by the very poor initialization  in both cases with poor policies  the number of
failures is higher at the beginning of the learning process and decreases as the learning
process proceeds  while both the poor and very poor initial policies are very close to the
error space  this is in stark contrast to the initial policy shown in figure    which  from
the very beginning  already appears concentrated at the origin  far from the error space 
   

figarca   fernandez

as the learning process proceeds  the different policies are compressed away from the error
space and the number of failures decreases 
    simba
business simulators are powerful tools for improving management decision making processes  an example of such a tool is the simulator for business administration  simba 
 borrajo et al          simba is a competitive simulator  since agents can compete against
other agents through their management of different virtual companies  the simulator 
the result of over twenty years of experience both with university students and business
executives  emulates business realities using the same variables  relationships and events
present in the business world  its objective is to provide users with an integrated vision
of a company  using basic techniques of business management  simplifying complexity and
emphasizing the content and principles with the greatest educational value  borrajo et al  
       in the experiments performed here  the learning agent competes against five handcoded agents  borrajo et al          decision making in simba is an episodic task where
decisions are made sequentially  to make a business decision  the state must be studied
and    continuous decision variables  e g   selling price  advertising expenses  etc   must
be set  followed by the study of a state composed of    continuous variables  e g   material
costs  financial expenses  economic productivity  etc    borrajo et al          each episode
is composed of    steps  although it may prematurely if the company goes bankrupt  i e  
its losses are higher than     of its net assets  

figure     modeling baseline behavior step in simba task   a  number of steps per trial
executed by case base b and baseline behavior t    b  cumulative reward per
 and an ibl approach 
trial by t   the learned safe case based policy b
figure     a  shows the evolution of the number of steps executed by the baseline behavior t and the case base b during two learning processes performing the modeling baseline
behavior step   and  were computed following the procedure described in subsection    
and have values of        and      respectively  in few episodes  approximately      the
 is learned  figure     b  shows the performance of the previouslysafe case based policy b

learned b   t and the ib  approach  in this study  the mean profits per episode of t
   

fisafe exploration of state and action spaces in reinforcement learning

figure     improving the learned baseline behavior step in simba task   a  the mean
profits per episode for different risk configurations obtained by the pi srl agent
against five hand coded agents   b  the mean profits per episode obtained by
the evolutionary and risk sensitive rl agent against five hand coded agents  in
each cases  any episode ending in failure  bankruptcy  is noted 
 are      million euros  in the ib 
are      million euros  while those obtained for b
approach  all cases generated using the baseline behavior t during    episodes are stored 
the experiments demonstrate that in simba  in contrast with the previous domains  storing all cases is sufficient for obtaining a safe policy with a performance similar to that using
the modeling baseline behavior step  with mean profits per episode of      million euros  
 is learned  we execute the improving the learned baseline
once the safe case based policy b
behavior step 
similar to the findings in earlier tasks  figure     a  indicates that while low and medium
levels of risk do not produce bankruptcies  performance is nevertheless weak  the highest
level of risk produces a near optimal policy with a low number number of failures  by
contrast  figure     b  presents the results for the evolutionary and risk sensitive rl approaches  with the former being clearly that which yields the highest number of failures 
in the risk sensitive case  the number of bankruptcies in all cases is insufficient for learning the risk function   the comparative results in figure    show that pi srl with
          obtains better policies and less failures than the evolutionary or risk sensitive
rl approaches 
figure    shows a graphical representation of the different solutions in this domain  it
shows the mean number of failures and cumulative reward for the different approaches over
    episodes  with data computed from    independent executions of each approach  in the
figure  red circles correspond to the pi srl algorithm  black triangles correspond to the
risk sensitive approach and the blue square corresponds to the evolutionary rl approach 
figure    also shows the performance for two additional risk levels  very high             
and very low         with respect the figure     the experiments in figure    demonstrate
that pi srl with a high level of risk              obtains the highest cumulative reward 
         additionally  pi srl with a very low level of risk         a low level of risk
             and a medium level of risk              are the approaches with the lowest

   

figarca   fernandez

figure     mean number of failures  company bankruptcies  and the cumulative reward
over     episodes for each approach to the simba task  the means have been
computed from    different executions 
mean number of failures       however  pi srl with a medium level of risk is preferred
inasmuch as its performance is superior in terms of cumulative reward  pi srl with a very
high level risk              increases the number of failures and obtains a lower cumulative
reward when compared to pi srl with a high level of risk  using the pareto comparison
criterion  pi srl with a high level of risk strictly dominates all other solutions  pi srl
         risk sensitive and pi srl          evolutionary rl   while the approach
is not strictly dominated by any other solution 
due to the difficulty of representing the high dimensional state and action space of the
simba domain  no graphs are provided with the evolution of the known space 

   related work
reinforcement learning  rl  and case based reasoning  cbr  techniques have been combined in the literature in different ways  in the work of bianchi et al          a new approach
is presented permitting the use of cases as heuristics to speed up rl algorithms  additionally  sharma et al         use a combination of cbr and rl  called carl  to achieve
transfer while playing against the game ai across a variety of scenarios in madrts tm 
a commercial real time strategy game  cbr has also been used for state value function
approximation in rl  gabel   riedmiller         however  the present study is  to our
knowledge  the first time that cbr and rl have been used in conjunction for safe exploration in dangerous domains  in the field of safe reinforcement learning  three principal
trends can be observed   i  approaches based on return and its variance   ii  risk sensitive
approaches based on the definition of error states and  iii  approaches using teachers 
    approaches based on the return and its variance
in the literature  it has long been known that the optimal policy and the optimal expected
return of an mdp are quite sensitive to parameter variations  even an optimal policy may
   

fisafe exploration of state and action spaces in reinforcement learning

perform badly in some cases due to the stochastic nature of the problem   to mitigate this
problem  the agent can try to maximize the return associated with the worst case scenario 
even though the case may be highly
unlikely  thus  in this trend  the risk refers to the worst
p
t r or its variance  an example of such an approach
outcomes of the return r   

t
t  
is worst case control where the worst possible outcome of r is to be optimized  coraluppi
  marcus        heger         in worst case control strategies  the optimality criterion
is exclusively focused on risk avoiding policies  a policy is considered to be optimal if its
worst case return is superior  the approach  however  is too restrictive inasmuch as it takes
very rare scenarios fully into account 
the   value of the return m introduced by heger        can be seen as an extension
of the worst case control of mdps  this concept establishes that the returns r   m of
a policy that occur with a probability lower than  are neglected  the algorithm is less
pessimistic than pure worst case control  given that extremely rare scenarios have no effect
on the policy  in the work of heger et al   the idea of weighting return and risk  namely the
expected value variance criterion  is also introduced 
in risk sensitive control based on the use of exponential utility functions  the return r is
transformed to reflect a subjective measure of utility  instead of maximizing the expected
value of r  the objective here is to maximize u      loge er    where  is a parameter
and r is the usual return  it can be shown that  depending on the parameter   policies
with a high variance v  r  are penalized        or enforced         instead  neuneier
and mihatsch        consider the worst case outcomes of a policy   i e   risk related to the
variability of the return   in the study  the authors demonstrate that the learning algorithm
interpolates between risk neutral and the worst case criterion and has the same limiting
behavior as exponential utility functions  it should be noted that these approaches based
on the variability of the return or its worst possible outcomes are not suited for problems
where a policy with a small variance can produce a large risk  geibel   wysotzki        
our view of risk in the present study  however  is not concerned with the variance of the
return or its worst possible outcome  but instead with the fact that processes generally
possess unsafe states that should be avoided  consequently  we address a different class of
problems than those dealt with by approaches focusing on the variability of the return 
    risk sensitive approaches based on error states 
in this second trend of approaches  the concept of risk is based on the definition of error
states or fatal transitions  thus  geibel et al           for instance  establish the risk
function as the probability of entering in an error state  instead  hans et al        consider
a transition to be fatal if the corresponding reward is less than a given threshold    in
the first case and as demonstrated in section     is learned by td methods which require
that error states  i e   car collisions  pole balancing disequilibrium  helicopter crashes and
company bankruptcies  be visited repeatedly in order to approximate the risk function
and  subsequently  avoid dangerous situations  in the second case  the concept of risk is
again joined with that of reward  moreover  the above mentioned studies either  i  assume
that the system dynamics are known   ii  tolerate undesirable states during exploration
or  in contrast with our paper   iii  do not deal with problems with high dimensional and
continuous state action spaces  regarding the latter  while geibel et al  write that their
   

figarca   fernandez

approach can also be extended to continuous action sets  e g   by using an actor critic
method   they do not give any more details on how this may be done with entirely continuous
problems  in section    we present an approach that solves the problem 
    approaches using teachers
the last trend in the approaches is based on the use of teachers in three different ways 
 i  to bootstrap the learning algorithm  i e   as an initialization procedure    ii  to derive a
policy from a finite demonstration set and  iii  to guide the exploration process 
      bootstrapping the learning algorithm
in the work of driessens and szeroski         a bootstraping procedure is used for relational rl in which a finite set of demonstrations are recorded from a human expert and
later presented to a regression algorithm  this allows the regression algorithm to build a
partial q function which can later be used to guide further exploration of the state space
using a boltzmann exploration strategy  smart and kaelbling        also use examples 
training runs to bootstrap the q learning approach for their hedger algorithm  the
initial knowledge bootstrapped into the q learning approach allows the agent to learn more
effectively and helps reduce the time spent with random actions  teacher behaviors are
also used as a form of population seeding in neuroevolution approaches  yao        siebel  
sommer         evolutionary methods are used to optimize the weights of neural networks 
but starting from a prototype network whose weights correspond to a teacher  or baseline
policy   using this technique  rl competition helicopter hovering task winners martin et
al         developed an evolutionary rl algorithm in which several teachers are provided in
the initial population  the algorithm restricts crossover and mutation operators  allowing
only slight changes to the policies given by the teachers  consequently  the rapid convergence of the algorithm to a near optimal policy is ensured  as is the indirect minimization of
damage to the agent  however  the teachers included in the initial population resulting from
an ad hoc training regimen conducted before the competition  consequently  the proposed
approach seems somewhat ad hoc and not easily generalizable to arbitrary rl problems 
in the work of koppejan et al                neural networks are also evolved  beginning
with one whose weights corresponds to teacher behavior  while this approach has been
proven advantageous in numerous applications of evolutionary methods  hernandez daz
et al         koppejan   whiteson         koppejans algorithm nevertheless also seems
somewhat ad hoc and designed for a specialized set of environments 
      deriving a policy from a finite set of demonstrations
all approaches falling under this category are framed according to the field of learning from
demonstration  lfd   argall et al          highlighting the study by abbeel et al        
based on apprenticeship learning  the approach is composed of three distinct steps  in the
first  a teacher demonstrates the task to be learned and the state action trajectories of the
teachers demonstration are recorded  in the second step  all state action trajectories seen
to that point are used to learn a dynamics model for the system  for this model  a  near optimal policy is to be found using any reinforcement learning  rl  algorithm  finally  the
policy obtained should be tested by running it on the real system  in the work of tang et
   

fisafe exploration of state and action spaces in reinforcement learning

al          an algorithm based on apprenticeship learning is also presented for automaticallygenerating trajectories for difficult control tasks  the proposal is based on the learning of
parameterized versions of desired maneuvers from multiple expert demonstrations  despite
each approachs potential strengths and general interest  all are inherently linked to the
information provided in the demonstration dataset  as a result  learner performance is
heavily limited by the quality of the teachers demonstrations 
      guiding the exploration process
driessens and szeroski         in the context of relational rl  also use a given teachers
policy  rather than a policy derived from the current q function hypothesis  which is not
informative in the early learning stages   for the selection of actions  in this approach 
episodes performed by a teacher are interleaved with normal exploration episodes  this
mixture of teacher and normal exploration make it easier for the regression algorithm to
distinguish between beneficial and poor actions  in the context of lfd  there are other
approaches which include teacher advice  argall et al          this advice is used to improve
learner performance  offering information beyond that which is provided by a demonstration
dataset  in this approach  following an initial task demonstration by the teacher  the agent
directly requests additional demonstration from the teacher in very different states from
those previously demonstrated or in states in which a single action cannot be selected with
certainty  chernova   veloso              
in all works mentioned for this trend  no explicit definition of risk is ever given 

   conclusions
in this work  pi srl  an algorithm for policy improvement through safe reinforcement
learning in high risk tasks  is described  the main contributions of this algorithm are the
definitions of a novel case based risk function and a baseline behavior for the safe exploration
of the state action space  the use of the case based risk function presented is possible
inasmuch as the policy is stored as a case base  this represents a clear advantage over
other approaches  e g   evolutionary rl  martn h    lope        koppejan   whiteson 
      where the extraction of knowledge about the known space by the agent is impossible
using the weights of the neural networks  additionally  a completely different notion of
risk from others found in the literature is presented  according to our notion  risk is
independent of the variance of the return and the reward function  and does not require
the identification of error states or the learning of risk functions  rather  the concept of
risk described in this paper is based on the distance between the known and unknown
space and  therefore  is a domain independent parameter  in this sense  our proposal allows
for the application of a parameter setting method as described in subsection       while
koppejan et al         also use a function to identify dangerous states  in contrast with our
approach  the definition of their function requires strong previous knowledge of the domain 
furthermore  most of the approaches to risk found in the literature only tackle problems
that are not entirely continuous  geibel   wysotzki        or that only report results on
one continuous domain  koppejan   whiteson         consequently  it is difficult to know
for certain if these approaches from the literature generalize easily to arbitrary domains 
   

figarca   fernandez

this paper presents the pi srl algorithm in great detail and demonstrates its effectiveness in four entirely different continuous domains  the car parking problem  pole balancing 
helicopter hovering and business management  simba   the experiments presented in this
paper demonstrate different characteristics about the learning capabilities of the pi srl
algorithm 
 i  pi srl obtains higher quality solutions  the experiments in section   demonstrate
that  save in the helicopter hovering task  pi srl obtains in all cases the best cumulative
reward per episode and the least number of failures  additionally  using the pareto comparison criterion it can be said that  save the very high risk configuration in the car parking
problem  our approach is not strictly dominated by any other approach 
 ii  pi srl adjusts the initial known space to safe and better policies  the initial known
space resulting from the first step of pi srl  modeling baseline behavior  is adjusted and
improved in the second step of the algorithm  improving the learned baseline behavior 
additionally  the experiments demonstrate that the adjustment process can compress the
known space away from the error space  e g   pole balancing domain  subsection      and
helicopter hovering domain  subsection      or  on other occasions  can require the known
space to move closer to the error space  e g   car parking problem  subsection      in the
event that better policies are be found there 
 iii  pi srl works well in domains with differently structured state action spaces and
where the value function can vary sharply  although the car parking problem  the polebalancing domain  the helicopter hovering task and the business simulator all represent
very differently structured problems  experiments in the study nevertheless demonstrate
that pi srl performs well in each  furthermore  even in such domains as the car parking
problem in which the value function varies sharply due to the presence of an obstacle 
experimental results demonstrate that pi srl can nevertheless successfully handle this
difficulty  however  it is impossible to avoid all failures if the known space edge is the
same as the edge to error states the algorithm would often explore into error states 
 iv  the number of failures depends on the distance between the known space and the
error space  the experiments in the pole balancing and helicopter hovering domains demonstrate that the number of failures depends on how close the known space is to the error
space  due to the structure of these domains  the improving the learned baseline behavior
step in the algorithm tends to concentrate the known space at the origin of coordinates
away from the error space  the greater the distance between the known space and the error
space  the lower the number of failures  additionally  in helicopter hovering  the known
space is  from the beginning  far from the error space  consequently  the number of failures is also low from the beginning   therefore  the initial distribution of the known space
learned from the baseline policy t later influences the number of failures obtained by the
second step of pi srl 
 v  pi srl is completely safe if only the first step of the algorithm is executed  however 
by proceeding only in this way  algorithm performance would be heavily limited by the
capabilities of the baseline behavior  if learner performance is to be improved beyond
the performance of this baseline behavior  the subsequent exploratory process from the
second step of pi srl must be carried out  since complete knowledge of the domain and
its dynamic is not possessed  however  it is also inevitable that  during this exploratory
   

fisafe exploration of state and action spaces in reinforcement learning

process  unknown regions of the state space will be visited where the agent may reach error
states 
 vi  the risk parameter allows the user to configure the level of risk assumed  in our
algorithm  the user can gradually increase the value of the risk parameter  in order to
obtain better policies  but also assuming a greater likelihood of damage in the learning
system 
 vii  pi srl performs successfully even when a poor initial policy with failures is used 
the experiments in figure    from the helicopter hovering domain demonstrate that pi srl
is able to learn a near optimal policy despite poor initialization  just as it can when a policy
free of failures is used to initialize the case base b  however  the figure also shows that if
a very poor initial policy with many failures is used  pi srl decreases in performance and
produces a higher number of failures  although some better behavior is still learnt  in this
case  the algorithm falls into a local minimum  likely biased by the very poor initialization 
in what follows  the applicability of the method is discussed  allowing the reader to more
clearly understand the scenarios in which the proposed pi srl approach may be applicable 
this applicability is restricted to domains having the following characteristics 
 i  it is mandatory that the scenario satisfy the two assumptions described in section   
according to the first assumption  nearby states in the domain must necessarily have similar actions  according to the other  similar actions in similar states should produce similar
effects  this fact that similar actions lead to similar states assumes some degree of smoothness in the dynamic behavior of the system which  in certain environments  may not hold 
however  as we clearly explain in section    we consider both assumptions to be logical
assumptions derived from generalization principles in the rl literature  kaelbling et al  
      jiang        
 ii  the applicability of the method is limited by the size of the case base b required to
mimic the baseline behavior  it is not possible to apply the proposed approach to tasks when 
in the first step of the pi srl algorithm  modeling baseline behavior  a prohibitively large
number of cases are required to properly mimic complex baseline behaviors  in this case 
the threshold  can be increased to further restrict the addition of new cases to the casebase  however  this increase may adversely affect the final performance of the algorithm 
nevertheless  the experiments performed in section   demonstrate that relatively simple
baseline behaviors are mimicked almost perfectly using a manageable number of cases 
 iii  the pi srl algorithm requires the presence of a baseline behavior  the proposed
method requires the presence of a baseline behavior that safely demonstrates the task to
be learned  this baseline behavior can be conducted by a human teacher or a hand coded
agent  it is important to note  nevertheless  that the presence of such a baseline behavior
is not guaranteed in all domains 
finally  a logical continuation of the present study would take into account the automatic
graduation of the risk parameter along the learning process  for example  it would be
particularly interesting to exploit the fact that the known space is far away from the error
space in order to increase the risk parameter or  on the contrary  to reduce it when it is
close  other future work aims to deploy the algorithm in real environments  inasmuch
as the uncertainty of the real environments presents the biggest challenge to autonomous
robots  autonomous robotic controllers must deal with a large number of factors such
as the robotic mechanical system and electrical characteristics  as well as environmental
   

figarca   fernandez

complexity  however  the use of the pi srl algorithm  or other risk sensitive approaches 
for learning processes in real environments could reduce the amount of damage incurred
and  consequently  allow the lifespan of the robots to be extended  it might be worthwhile
add a mechanism to the algorithm to detect when a known state can lead directly to an
error state  all such problems are currently being investigated 

acknowledgments
this study has been partially supported by spanish miciin projects tin           c      tra          and ccg   uc m tic       we offer our gratitude and special thanks
to raquel fuentetaja pizan  assistant professor at universidad carlos iii de madrid in the
planning   learning group  plg   for her generous and invaluable comments during the
revision of this paper  we would also like to thank to jose antonio martn  assistant
professor at universidad complutense de madrid  for his invaluable comments regarding
his evolutionary rl algorithm 

references
aamodt  a     plaza  e          case based reasoning  foundational issues  methodological variations  and system approaches  ai communications              
abbeel  p   coates  a   hunter  t     ng  a  y          autonomous autorotation of an
rc helicopter  in iser  pp         
abbeel  p   coates  a     ng  a  y          autonomous helicopter aerobatics through
apprenticeship learning  i  j  robotic res                     
abbott  r  g          robocup       robot soccer world cup xi   chap  behavioral cloning
for simulator validation  pp          springer verlag  berlin  heidelberg 
aha  d  w          tolerating noisy  irrelevant and novel attributes in instance based
learning algorithms  international journal man machine studies                 
aha  d  w     kibler  d          instance based learning algorithms  in machine learning 
pp       
anderson  c  w   draper  b  a     peterson  d  a          behavioral cloning of student
pilots with modular neural networks  in proceedings of the seventeenth international
conference on machine learning  pp        morgan kaufmann 
argall  b   chernova  s   veloso  m     browning  b          a survey of robot learning
from demonstration  robotics and autonomous systems                 
bartsch sprl  b   lenz  m     hbner  a          case based reasoning  survey and future
directions   in puppe  f   ed    xps  vol       of lecture notes in computer science 
pp        springer 
bianchi  r   ros  r     de mantaras  r  l          improving reinforcement learning by
using case based heuristics   vol        pp        lecture notes in artificial intelligence  springer  lecture notes in artificial intelligence  springer 
   

fisafe exploration of state and action spaces in reinforcement learning

borrajo  f   bueno  y   de pablo  i   santos  b  n   fernandez  f   garca  j     sagredo  i 
        simba  a simulator for business education and research  decission support
systems                 
boyan  j   moore  a     sutton  r          proceedings of the workshop on value function
approximation  machine learning conference         technical report cmu cs       
chernova  s     veloso  m          confidence based policy learning from demonstration
using gaussian mixture models  in joint conference on autonomous agents and
multi agent systems 
chernova  s     veloso  m          multi thresholded approach to demonstration selection
for interactive robot learning  in proceedings of the  rd acm ieee international
conference on human robot interaction  hri     pp          new york  ny  usa 
acm 
cichosz  p          truncating temporal differences  on the efficient implementation of
td lambda  for reinforcement learning  journal of artificial intelligence research
 jair             
cichosz  p          truncated temporal differences with function approximation  successful examples using cmac  in proceedings of the thirteenth european symposium on
cybernetics and systems research  emcsr     
coraluppi  s  p     marcus  s  i          risk sensitive and minimax control of discretetime  finite state markov decision processes  automatica             
defourny  b   ernst  d     wehenkel  l          risk aware decision making and dynamic
programming  in nips      workshop on model uncertainty and risk in rl 
driessens  k     ramon  j          relational instance based regression for relational rl 
in international conference of machine learning  icml   pp         
driessens  k     dzeroski  s          integrating guidance into relational reinforcement
learning  machine learning                 
fernandez  f     isasi  p          local feature weighting in nearest prototype classification 
neural networks  ieee transactions on               
fernandez  f     borrajo  d          two steps reinforcement learning  international
journal of intelligent systems                 
floyd  m  w     esfandiari  b          toward a domain independent case based reasoning
approach for imitation  three case studies in gaming  in workshop on case based
reasoning for computer games at the   th international conference on case based
reasoning  iccbr   pp       
floyd  m  w   esfandiari  b     lam  k          a case based reasoning approach to
imitating robocup players  in proceedings of the   st international florida artificial
intelligence research society conference  pp         
forbes  j     andre  d          representations for learning control policies  in the
university of new south  pp      
   

figarca   fernandez

gabel  t     riedmiller  m          cbr for state value function approximation in reinforcement learning  in proceedings of the  th international conference on case based
reasoning  iccbr       pp          springer 
geibel  p          reinforcement learning with bounded risk  in proceedings of the   th
international conference on machine learning  pp          morgan kaufmann 
geibel  p     wysotzki  f          risk sensitive reinforcement learning applied to control
under constraints  journal of artificial intelligence research  jair             
hans  a   schneegass  d   schafer  a  m     udluft  s          safe exploration for reinforcement learning  in european symposium on artificial neural network  pp 
       
heger  m          consideration of risk in reinforcement learning  in   th international
conference on machine learning  pp         
hernandez daz  a  g   coello  c  a  c   perez  f   caballero  r   luque  j  m     santanaquintero  l  v          seeding the initial population of a multi objective evolutionary algorithm using gradient based information  in ieee congress on evolutionary
computation  pp            ieee 
hester  t   quinlan  m     stone  p          a real time model based reinforcement learning
architecture for robot control  tech  rep  arxiv e prints            arxiv 
hu  h   kostiadis  k   hunter  m     kalyviotis  n          essex wizards      team
description  in birk  a   coradeschi  s     tadokoro  s   eds    robocup  vol      
of lecture notes in computer science  pp          springer 
jiang  a  x          multiagent reinforcement learning in stochastic games with continuous
action spaces  
kaelbling  l   littman  m     moore  a          reinforcement learning  a survey  journal
of artificial intelligence research  jair             
konen  w     bartz beielstein  t          reinforcement learning for games  failures and
successes  in proceedings of the   th annual conference companion on genetic and
evolutionary computation conference  late breaking papers  gecco     pp      
      new york  ny  usa  acm 
koppejan  r     whiteson  s          neuroevolutionary reinforcement learning for generalized helicopter control  in gecco       proceedings of the genetic and evolutionary
computation conference  pp         
koppejan  r     whiteson  s          neuroevolutionary reinforcement learning for generalized control of simulated helicopters  evolutionary intelligence            
lee  j  y     lee  j  j          multiple designs of fuzzy controllers for car parking using
evolutionary algorithm  pp      no  may 
luenberger  d  g          investment science  oxford university press 
mannor  s          reinforcement learning for average reward zero sum games  in shawetaylor  j     singer  y   eds    colt  vol       of lecture notes in computer science 
pp        springer 
   

fisafe exploration of state and action spaces in reinforcement learning

martin h  j     de lope  j          exa  an effective algorithm for continuous actions
reinforcement learning problems  in industrial electronics        iecon       th
annual conference of ieee  pp            
martn h   j  a     lope  j          learning autonomous helicopter flight with evolutionary reinforcement learning  in   th international conference on computer
aided systems theory  eurocast   pp       
mihatsch  o     neuneier  r          risk sensitive reinforcement learning  machine learning                   
moldovan  t  m     abbeel  p          safe exploration in markov decision processes 
corr  abs           
narendra  k  s     thathachar  m  a  l          learning automata   a survey  ieee
transactions on systems man and cybernetics  smc               
narendra  k  s     thathachar  m  a  l          learning automata  an introduction 
prentice hall  inc   upper saddle river  nj  usa 
ng  a  y   kim  h  j   jordan  m  i     sastry  s          autonomous helicopter flight
via reinforcement learning  in thrun  s   saul  l  k     scholkopf  b   eds    nips 
mit press 
peters  j   tedrake  r   roy  n     morimoto  j          robot learning  in sammut  c  
  webb  g  i   eds    encyclopedia of machine learning  pp          springer 
poli  r     cagnoni  s          genetic programming with user driven selection  experiments on the evolution of algorithms for image enhancement  in genetic programming
      proceedings of the second annual conference  pp          morgan kaufmann 
salkham  a   cunningham  r   garg  a     cahill  v          a collaborative reinforcement learning approach to urban traffic control optimization  in web intelligence
and intelligent agent technology        wi iat     ieee wic acm international
conference on  vol     pp         
santamara  j  c   sutton  r  s     ram  a          experiments with reinforcement
learning in problems with continuous state and action spaces  adaptive behavior    
       
sharma  m   holmes  m   santamaria  j   irani  a   isbell  c     ram  a          transfer
learning in real time strategy games using hybrid cbr rl  in in proceedings of the
twentieth international joint conference on artificial intelligence 
siebel  n  t     sommer  g          evolutionary reinforcement learning of artificial neural
networks  international journal of hybrid intelligent systems            
smart  w  d     kaelbling  l  p          practical reinforcement learning in continuous
spaces  in artificial intelligence  pp          morgan kaufmann 
smart  w  d     kaelbling  l  p          effective reinforcement learning for mobile robots 
in icra  pp            ieee 
sutton  r  s     barto  a  g          reinforcement learning  an introduction  the mit
press 
   

figarca   fernandez

tang  j   singh  a   goehausen  n     abbeel  p          parameterized maneuver learning for autonomous helicopter flight  in international conference on robotics and
automation  icra  
taylor  m  e   kulis  b     sha  f          metric learning for reinforcement learning agents 
in proceedings of the international conference on autonomous agents and multiagent
systems  aamas  
van hasselt  h     wiering  m  a          reinforcement learning in continuous action
spaces  in approximate dynamic programming and reinforcement learning       
adprl       ieee international symposium on  pp         
wyatt  j          exploration and inference in learning from reinforcement  university of
edinburgh 
yao  x          evolving artificial neural networks  pieee  proceedings of the ieee     
         

   

fi