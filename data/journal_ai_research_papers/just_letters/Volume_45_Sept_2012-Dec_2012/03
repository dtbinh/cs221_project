journal artificial intelligence research                  

submitted       published      

safe exploration state action spaces
reinforcement learning
javier garca
fernando fernandez

fjgpolo inf uc m es
ffernand inf uc m es

universidad carlos iii de madrid 
avenida de la universidad    
      leganes  madrid  spain

abstract
paper  consider important problem safe exploration reinforcement
learning  reinforcement learning well suited domains complex transition
dynamics high dimensional state action spaces  additional challenge posed
need safe efficient exploration  traditional exploration techniques
particularly useful solving dangerous tasks  trial error process may lead
selection actions whose execution states may result damage
learning system  or system   consequently  agent begins interaction
dangerous high dimensional state action space  important question arises 
namely  avoid  or least minimize  damage caused exploration
state action space  introduce pi srl algorithm safely improves suboptimal
albeit robust behaviors continuous state action control tasks efficiently
learns experience gained environment  evaluate proposed method
four complex tasks  automatic car parking  pole balancing  helicopter hovering 
business management 

   introduction
reinforcement learning  rl   sutton   barto        type machine learning whose
main goal finding policy moves agent optimally environment  generally formulated markov decision process  mdp   many rl methods used
important complex tasks  e g   robot control see smart   kaelbling        hester 
quinlan    stone        stochastic games see mannor        konen   bartz beielstein 
     control optimization complex dynamical systems see salkham  cunningham 
garg    cahill         rl tasks focused maximizing long term cumulative reward  rl researchers paying increasing attention long term
reward maximization  safety approaches sequential decision problems
 sdps   mihatsch   neuneier        hans  schneegass  schafer    udluft        martn h 
  lope        koppejan   whiteson         well written reviews matters
found  geibel   wysotzki        defourny  ernst    wehenkel         nevertheless 
important ensure reasonable system performance consider safety
agent  e g   avoiding collisions  crashes  etc   application rl dangerous
tasks  exploration techniques rl offer guarantees issues  thus 
using rl techniques dangerous control tasks  important question arises  namely 
ensure exploration state action space cause damage injury
c
    
ai access foundation  rights reserved 

figarca   fernandez

while  time  learning  near  optimal policies  matter  words 
one ensuring agent able explore dangerous environment safely
efficiently  many domains exploration exploitation process may lead
catastrophic states actions learning agent  geibel   wysotzki        
helicopter hovering control task one case involving high risk  since policies
crash helicopter  incurring catastrophic negative reward  exploration exploitation
strategies greedy may even result constant helicopter crashes  especially
high probability random action selection   another example found
portfolio theory analysts expected find portfolio maximizes profit
avoiding risks considerable losses  luenberger         since maximization expected
returns necessarily prevent rare occurrences large negative outcomes  different
criteria safe exploration needed  exploration process new policies
evaluated must conducted extreme care  indeed  environments  method
required explores state action space  safe manner 
paper  propose policy improvement safe reinforcement learning
 pi srl  algorithm safe exploration dangerous continuous control tasks 
method requires predefined  and safe  baseline policy assumed suboptimal
 otherwise  learning would pointless   predefined baseline policies used
different ways approaches  work koppejan whiteson         singlelayers perceptrons evolved  albeit starting prototype network whose weights correspond baseline policy provided helicopter control task competition software  abbeel 
coates  hunter    ng         approach viewed simple form population seeding proven advantageous numerous evolutionary methods
 e g  see hernandez daz  coello  perez  caballero  luque    santana quintero        poli
  cagnoni         work martn de lope         weights neural networks evolved inserting several baseline policies  including provided
helicopter control task competition software  initial population  minimize
possibility evaluating unsafe policies  approach prevents crossover mutation
operators permitting anything tiny changes initial baseline policies 
paper  present pi srl algorithm  novel approach improving baseline
policies dangerous domains using rl  pi srl algorithm composed two different steps  first  baseline behavior  robust albeit suboptimal  approximated
using behavioral cloning techniques  anderson  draper    peterson        abbott        
order achieve goal  case based reasoning  cbr  techniques  aamodt   plaza 
      bartsch sprl  lenz    hbner        used successfully applied
imitation tasks past  floyd   esfandiari        floyd  esfandiari    lam        
second step  pi srl algorithm attempts safely explore state action space
order build accurate policy previously learned behavior  thus  set
cases  i e   state action pairs  obtained previous phase improved
safe exploration state action space  perform exploration  small amounts
gaussian noise randomly added greedy actions baseline policy approach 
exploration strategy used successfully previous works  argall  chernova 
veloso    browning        van hasselt   wiering        
novelty present study use two new  main components   i  risk
function determine degree risk particular state  ii  baseline behavior
   

fisafe exploration state action spaces reinforcement learning

capable producing safe actions supposedly risky states  i e   states lead
damage injury   addition  present new definition risk based
agent unknown known space  described section   greater detail 
new definition completely different traditional definitions risk found literature  geibel        mihatsch   neuneier        geibel   wysotzki         paper
reports experimental results obtained application new approach four
different domains   i  automatic car parking  lee   lee          ii  pole balancing  sutton
  barto          iii       rl competition helicopter hovering  ng  kim  jordan    sastry 
       iv  business management  borrajo  bueno  de pablo  santos  fernandez  garca 
  sagredo         domain  propose learning near optimal policy which 
learning phase  minimize car crashes  pole disequilibrium  helicopter crashes
company bankruptcies  respectively  important note comparison
approach agent optimal exploration policy possible since 
proposed domains  each high dimensional continuous state action space 
well complex stochastic dynamics   know optimal exploration policy
is 
regarding organization remainder paper  section   introduces key
definitions  section   describes detail learning approach proposed  section   
evaluation performed four mentioned domains presented  section  
discusses related work section   summarizes main conclusions study 
sections 
term return used refer expected cumulative future discounted
p
reward r   t   rt   term reward used refer single real value used
evaluate selection action particular state denoted r 

   definitions
illustrate concept safety used approach  navigation problem presented
figure    navigation problem presented figure    control policy must
learned get particular start state goal state  given set demonstration
trajectories  environment  assume task difficult due stochastic
complex dynamic environment  e g   extremely irregular surface case
robot navigation domain wind effects case helicopter hover task  
stochasticity makes impossible complete task using exactly trajectory
every time  additionally  problem supposes set demonstrations baseline
controller performing task  the continuous black lines  given  set
demonstrations composed different trajectories covering well defined region
state space  the region within rectangle  
approach based addition small amounts gaussian noise perturbations baseline trajectories order find new better ways completing
task  noise affect baseline trajectories different ways  depending
amount noise added which  turn  depends amount risk taken  risk
desired  noise added baseline trajectories   and  consequently  new
improved behavior discovered  nevertheless  robot never fall cliff
helicopter never crash   if  however  intermediate level risk desired 
small amounts noise added baseline trajectories new trajectories  the
   

figarca   fernandez

figure    exploration strategy based addition small amounts noise baseline
policy behavior  continuous lines represent baseline behavior  newly
explored behaviors indicated dotted dashed lines 
dotted blue lines  complete task discovered  cases  exploration new
trajectories leads robot unknown regions state space  the dashed red lines  
robot assumed able detect situations risk function use
baseline behavior return safe  known states  if  instead  high risk desired 
large amounts noise added baseline trajectories  leading discovery
new trajectories  but higher probability robot gets damaged  
iteration process leads robot progressively safely explore state
action spaces order find new improved ways complete task  degree
safety exploration  however  depend risk taken 
    error non error states
paper  follow far notation presented geibel et al        
definition concept risk  study  geibel et al  associate risk error
states non error states  former understood state considered
undesirable dangerous enter 
definition   error non error states  let set states set
error states  state undesirable terminal state control
agent ends reached damage injury agent  learning system
external entities  set considered set non error terminal states
  control agent ends normally without damage injury 
terms rl  agent enters error state  current episode ends damage
learning system  or systems   whereas enters non error state  episode
ends normally without damage  thus  geibel et al  define risk respect
policy    s   probability state sequence  si  i  s    s  generated
execution policy   terminates error state s    definition   s     
     s          states
    risk
taken depends actions selected policy   definitions 
   

fisafe exploration state action spaces reinforcement learning

theoretical framework introduce definition risk associated
known unknown states 
    known unknown states continuous action state spaces
assume continuous  n dimensional state space  n state    s    s           
sn   vector real numbers dimension individual domain dis   
similarly  assume continuous m dimensional action space  m
action    a    a              vector real numbers dimension
individual domain dia    additionally  agent considered endowed
memory  case base b  size   memory element represents state action pair 
case  agent experienced before 
definition    case base   case base set cases b    c          c    every case
ci consists state action pair  si   ai   agent experienced past
associated value v  si    thus  ci    si   ai   v  si      first element represents
cases problem part corresponds state si   following element ai depicts case
solution  i e   action expected agent state si   final element
v  si   value function associated state si   state si composed n
continuous state variables action ai composed continuous action variables 
agent receives new state sq   first retrieves nearest neighbor sq
b according given similarity metric performs associated action 
paper  use euclidean distance similarity metric  equation    
v
ux
u n
d sq   si      sq j si j   

   

j  

euclidean distance metric useful value function expected continuous smooth throughout state space  santamara  sutton    ram         however 
since value function unknown priori euclidean distance metric particularly suitable many problems  many researchers begun ask distance
metric learn adapt order achieve better results  taylor  kulis    sha 
       use distance metric learning techniques would certainly desirable
order induce powerful distance metric specific domain  consideration
lies outside scope present study  paper  therefore  focused
domains euclidean distance proven successful  i e   successfully applied car parking  cichosz         pole balancing  martin h   de lope        
helicopter hovering control  martin h   de lope        simba  borrajo et al         
traditionally  case based approaches use density threshold order determine
new case added memory  distance nearest neighbor
sq greater   new case added  sense  parameter defines size
classification region case b  figure     new case sq within
classification region case ci   considered known state  hence  cases

associated value function v b
b describe case based policy agent b
 
   

figarca   fernandez

figure    known unknown states 
definition    known unknown states   given case base b    c          c   composed
cases ci    si   ai   v  si    density threshold   state sq considered known
min i d sq   si   unknown cases  formally  set known
states  set unknown states     s 
definition    states identified known unknown  agent
receives new state   performs action ai case ci d s  si    
min j d s  sj    however  agent receives state where  definition 
distance state b larger   case retrieved  consequently  action
performed state unknown agent 
definition    case based risk function   given case base b    c          c   composed
cases ci    si   ai   v  si     risk state defined equation   

b

 


 s   

 
 

min j d s  sj    
otherwise

   



thus   b  s      holds  i e   unknown   state
associated case and  hence  action performed given situation

unknown     b  s      
derived casedefinition    safe case based policy   case based policy b
base b    c            c   safe when  initial known state s  respect b 
always produces known non error states respect b 
execution b






b
s     b  s          si  i  
 b  si      

   

additionally  assumed probability state sequence  si  i 
  terminates error state
known state s    generated executing policy b

b  s         i e       
   

fisafe exploration state action spaces reinforcement learning

definition    safe case based coverage   coverage single state respect
safe case base b    c            c   defined state si min i d s  si    
therefore  assume safe case based provide actions entire state
space  rather known states  
figure   graphically represents relationship known unknown error non learnt 
error states  green area image denotes safe case based policy b
area state space corresponding initial known space  agent following
always green area resulting episodes end without
policy b
damages  consequently  subset non error states form part known space 
formally  let subsets non error states belonging known unknown
spaces  respectively        yellow area figure 
contrast  represents unknown space   space found error states 
well subset remaining non error states  formally   
understood way  pi srl algorithm summed follows 
 
first step  learn known space  green area  safe case based policy b

second step  adjust known space  green area  unknown space  yellow
area  order explore new improved behaviors avoiding error states  red
area   process adjusting known space space used safe
better policies  algorithm forget ineffectual known states  shown
section   

figure    known unknown error non error states given case base b 

    advantages using prior knowledge predetermined
exploration policies
present subsection  advantages using teacher knowledge rl  namely  i 
provide initial knowledge task learned  ii  support exploration
process  highlighted  furthermore  explain believe knowledge
   

figarca   fernandez

indispensable rl tackling highly complex realistic problems large  continuous
state action spaces particular action may result undesirable
consequence 
      providing initial knowledge task
rl algorithms begin learning without previous knowledge task
learnt  cases  exploration strategies greedy used  application
strategy results random exploration state action spaces gather
knowledge task  enough information discovered environment algorithms behavior improve  random exploration policies  however 
waste significant amount time exploring irrelevant regions state action
spaces optimal policy never encountered  problem compounded
domains extremely large continuous state action spaces random
exploration never likely visit regions spaces necessary learn  near  optimal
policies  additionally  many real rl tasks real robots  random exploration
gather information environment cannot even applied  real robots 
considered sufficient information much information real robot
gather environment  finally  impossible avoid undesirable situations
high risk environments without certain amount prior knowledge task 
use random exploration would require undesirable state visited
labeled undesirable  however  visits undesirable states may result damage
injury agent  learning system external entities  consequently  visits
states avoided earliest steps learning process 
mitigating difficulties described above  finite sets teacher provided examples
demonstrations used incorporate prior knowledge learning algorithm 
teacher knowledge used two general ways  either  i  bootstrap learning algorithm  i e   sort initialization procedure   ii  derive policy
examples  first case  learning algorithm provided examples demonstrations bootstrap value function approximation lead agent
relevant regions space  second way teacher knowledge
used refers learning demonstration  lfd  approaches policy derived finite set demonstrations provided teacher  principal drawback
approach  however  performance derived policy heavily limited
teacher ability  one way circumvent difficulty improve performance
exploring beyond provided teacher demonstrations  raises
question agent act encounters state demonstration
exists  an unknown state  
      supporting exploration process
furnishing agent initial knowledge helps mitigate problems associated
random exploration  alone sufficient prevent undesirable situations
arise subsequent explorations undertaken improve learner ability  additional mechanism necessary guide subsequent exploration process way
agent may kept far away catastrophic states  paper  teacher 
   

fisafe exploration state action spaces reinforcement learning

rather policy derived current value function approximation used
selection actions unknown states  one way prevent agent encountering
unknown states exploration process would requesting beginning
teacher demonstration every state state space  however  strategy
possible due  i  computational infeasibility given extremely large number states
state space  ii  fact teacher forced give action
every state  given many states ineffectual learning optimal policy 
consequently  pi srl requests teacher action action actually required
 i e   agent unknown state  
paper supposes teacher available task learned 
teacher taken baseline behavior  although studies examined use
robotic teachers  hand written control policies simulated planners  great majority
date made use human teachers  paper uses suboptimal automatic controllers
teachers  taken teachers policy 
definition    baseline behavior   policy considered baseline behavior
three assumptions made   i  able provide safe demonstrations
task learnt prior knowledge extracted   ii  able support
subsequent exploration process  advising suboptimal actions unknown states reduce
probability entering error states return system known situation 
 iii  performance far optimal 
optimal baseline behaviors certainly ideal behave safely  non optimal behaviors often easy  or easier  implement generate optimal ones  pi srl
algorithm uses baseline behavior two different ways  first  uses safe demonstrations provide prior knowledge task  step  algorithm builds

initial known space agent derived safe case based policy b

purpose mimicking b   second step  pi srl uses support
subsequent exploration process conducted improve abilities previously learnt
  exploration process continues  action requested required 
b

is  agent unknown state  figure     step  acts backup
policy case unknown state intention guiding learning away
catastrophic errors or  least  reducing frequency  important note
baseline behavior cannot demonstrate correct action every possible state  however 
baseline behavior might able indicate best action cases 
action supplies should  least  safer obtained random
exploration 
    risk parameter
order maximize exploration safety  seems advisable movement
state space arbitrary  rather known space expanded gradually
starting known state  exploration carried perturbation
  perturbation trajectories
state action trajectories generated policy b
accomplished addition gaussian random noise actions b order
obtain new ways completing task  thus  gaussian exploration takes place
   

figarca   fernandez

figure    exploration process pi srl requests actions baseline behavior   
really required 
around current approximation action ai current known state sc  
ci    si   ai   v  si    d sc   si     min j d s  sj    action performed sampled
gaussian distribution mean action output given instance selected
b  ai denotes algorithm action output  probability selecting action a i  
 s  a i   computed using equation   
 s  a i    

 
 
 
  e ai ai     
   

      

   

shape gaussian distribution depends parameter  standard deviation  
study  used width parameter  large values imply wide bellshaped distribution  increasing probability selecting actions a i different
current action ai   small value implies narrow bell shaped distribution  increasing
probability selecting actions a i similar current action ai         
assume  s  ai        hence  value directly related amount perturbation
  higher values imply
added state action trajectories generated policy b
greater perturbations  more gaussian noise  greater probability visiting unknown
states 
definition    risk parameter   parameter considered risk parameter  large
values increase probability visiting distant unknown states and  hence  increase
probability reaching error states 
exploratory actions drive agent edge known space force
go slightly beyond  unknown space  search better  safer behaviors 
period time  execution exploratory actions increases known space
  risk
improves abilities previously learned safe case based policy b
parameter   well   design parameters must selected user 
section      guidelines selection offered 
important note approach proposed study based two logical
assumptions rl derived following generalization principles  kaelbling  littman 
  moore        sutton   barto        
   

fisafe exploration state action spaces reinforcement learning

 i  nearby states similar optimal actions  continuous state spaces 
impossible agent visit every state store value  or optimal action 
table  generalization techniques needed  large  smooth state spaces 
similar states expected similar values similar optimal actions  therefore 
possible use experience gathered environment limited subset
state space produce reliable approximation much larger subset  boyan  moore 
  sutton        hu  kostiadis  hunter    kalyviotis        fernandez   borrajo        
one must note that  proposed domains  optimal action considered
safe action sense never produces error states  i e   action considered
optimal leads agent catastrophic situation  
 ii  similar actions similar states tend produce similar effects  considering deterministic domain  action performed state st always produces
state st     stochastic domain  understood intuitively execution
action state st produce similar effects  i e   produces states  s t     s t     s t           
i  j    j dist sit     sjt         additionally  execution action a t
state s t st produces states  s   t     s   t     s   t            i  j dist s  it     sjt       
explained earlier  present study uses euclidean distance similarity metric 
proven successful proposed domains  result assumption 
approximation techniques used  actions generate similar effects
grouped together one action  jiang         continuous action spaces  need
generalization techniques even greater  kaelbling et al          paper 
assumption allows us assume low values increase probability visiting
known states and  hence  exploring less taking less risks  greater values
increase probability reaching error states 

   pi srl algorithm
pi srl algorithm composed two main steps described detail below 
    first step  modeling baseline behaviors cbr
first step pi srl approach behavioral cloning  using cbr allow software
agent behave similar manner teacher policy  baseline behavior   floyd et al  
       whereas lfd approaches named differently according learned  argall et al          prevent terminological inconsistencies here  consider behavioral
cloning  also known imitation learning  area lfd whose goal reproduction mimicking underlying teacher policy  peters  tedrake  roy    morimoto 
      abbott        
using cbr behavioral cloning  case built using agents state
received environment  well corresponding action command performed
teacher  pi srl  objective first step properly imitate behavior
using cases stored case base  point  important question arises  namely 
case base b learnt using sample trajectories provided that 
end learning process  resulting policy derived b mimics behavior
  baseline behavior function maps states actions   or 
   

figarca   fernandez

words  function that  given state si s  provides corresponding action ai a 
paper  want build policy b derived case base composed cases  sj   aj  
that  new state sq   case minimum euclidean distance dist sq   sj  
retrieved corresponding action aj returned  intuitively  assumed
b built simply storing cases  si   ai   gathered one interaction
environment limited number episodes k  end k episodes 
one expects resulting b able properly mimic behavior   however 
informal experimentation helicopter hovering domain shows case
 section       helicopter hovering  k       episodes prohibitive number
        cases stored  policy derived case base b unable correctly
imitate baseline behavior and  instead  continuously crashes helicopter  indeed 
order b mimic large continuous stochastic domains  approach
requires larger number episodes and  consequently  prohibitive number cases 
fact  perfectly mimic domains  infinite number cases would required 
figure   attempts explain believe learning process work 
it  region space represented simply storing cases derived form
c    s  a  shown  stored case  red circles  covers area space represents
centroid voronoi region 

figure    effects storing training cases 
previously learned policy b used new state sq presented  action aj performed  corresponding case cj    sj   aj   euclidean distance
dist sq   sj   less stored cases  however  use policy
provide action situation sq   action ai provided different aj  
point  policy b said classify state sq obtained class aj  
policy said classify state sq desired class ai  insofar
policy mimicked    ai aj        furthermore   ai aj   understood
classification error  case base stored possible pairs  si   ai  
able generate domain  actions aj ai would always identical 
dist sq   sj        ai aj        however  stochastic large  continuous domain 
impossible store cases  sum classification errors episode
   

fisafe exploration state action spaces reinforcement learning

leads visiting unexplored regions case space  i e   regions new
state sq received environment euclidean distance dist sq   sj     
respect closest case cj    sj   aj   b   unexplored regions visited 
difference obtained class derived b desired class derived
large  i e    ai aj         probability error states might visited
greatly increases 
may concluded  therefore  simply storing pairs c    s  a  generated
sufficient properly mimic behavior  reason  algorithm figure  
proposed 
cbr approach behavioral cloning
  
  
  
  

given
given
given
   set

  
  
  
  

   repeat
set k    
k   maxepisodelength
compute case   sc   ac       closest current state sk

  
  
  
  
  
  
  
  
  
  
  
  
  






baseline behavior
density threshold
maximum number cases
case base b  



 b  sk          equation  
set ak   ac
else
set ak using baseline behavior
create new case cnew    sk   ak     
b    b cnew
execute ak   receive sk  
set k   k    
end
kbk  
remove kbk least frequently used cases b
stop criterion becomes true

   return b performing safe case based policy b

figure    cbr algorithm behavioral cloning 


first step algorithm  state value function v b  si   initialized    see

line      value v b  si   case computed second step algorithm
section      additionally  step uses case based risk function  equation   
determine whether new state sk considered risky  line      new state
risky  i e   known state sk      nearest neighbor strategy followed  line
     otherwise  algorithm performs action ak using baseline behavior
new case cnew    sk   ak      built added case base b  line      starting
empty case base  learning algorithm continuously increases competence storing
new experiences  however  number reasons inflow new cases
limited  large case bases increase time required find closest cases new
example  may partially solved using techniques reduce retrieval time
 e g   k d trees used work   nevertheless reduce storage
   

figarca   fernandez

requirements  several approaches removal ineffectual cases training exist 
including ahas ibx algorithms  aha        nearest prototype approach  fernandez
  isasi         number cases stored b exceeds critical value kbk  
realization retrieval within certain amount time cannot guaranteed 
removal cases inevitable  efficient approach problem
removal least frequently used elements b  line     
result step constrained case base b describing safe case based policy

b mimics baseline behavior   though perhaps deviation  line     
formally  let u  t   estimate utility baseline behavior computed
  u     
averaging sum rewards accumulated nt trials  then  u  b

    second step  improving learned baseline behavior
learned previous
step pi srl algorithm  safe case based policy b
step improved safe exploration state action space  first  case ci

b  state value function v b  si   computed following monte carlo  mc  approach
 figure    
mc algorithm adapted cbr
  
  
  
  

given case base b
   initialize  ci b
v  s  arbitrary
returns s  empty list

  
  
  
  
  
  

   k   maxn umberepisodes

generate episode using b
appearing episode   s  a  v  s    b
r return following first occurrence
append r returns s 
v s  average returns s  

  
  

set k   k    
   return b

figure    monte carlo algorithm computation state value function case 
algorithm similar spirit first visit mc method v  sutton   barto 
       adapted paper work policy given case base  algorithm
shown figure    returns state si b accumulated averaged  following
derived case base b  see line      important note
policy b
algorithm term return following first occurrence refers expected return
 i e   expected cumulative future discounted reward starting state   whereas
returns refers list composed return different episodes  one
principal reasons using mc method allows us quickly easily estimate

state values v b  si   case ci b  addition  mc methods shown
successful wide variety domains  sutton   barto         state value

function v b  si   computed case ci b  small amounts gaussian noise
order obtain new improved ways
randomly added actions policy b
   

fisafe exploration state action spaces reinforcement learning

complete task  algorithm used improve baseline behavior learned
previous step depicted figure    algorithm composed four steps performed
episode 
   a  initialization step  algorithm initializes list used store cases occurring
episode sets cumulative reward counter episode   
   b  case generation  algorithm builds case step episode 
new state sk   closest case   s  a  v  s    b computed using euclidean
distance metric equation    see line    algorithm figure     order determine
perceived degree risk new state sk   case based risk function used  line

      b  sk        sk  known state   case  action ak performed
computed using equation   new case cnew    s  ak   v  s    built added
list cases occurred episode  line      important note
new case   s  ak   v  s    built replacing action corresponding closest case
  s  a  v  s    b  new action ak resulting application random
gaussian noise equation    thus  algorithm produces smooth changes

cases b ak a  if  however   b  sk        state sk  i e   unknown
state  line       unknown states  action ak performed suggested baseline
behavior defines safe behavior  line      new case   sk   ak       built
added list cases episode actions performed using
agent known state  finally  reward obtained episode accumulated 
r sk   ak   immediate reward obtained action ak performed state sk
 line     
   c  computing state value function unknown states  step 
state value function states considered unknown previous step
computed  previous step  line      state value function states set
   algorithm proceeds manner similar first visit mc algorithm figure   
case  return unknown state si computed  averaged since
one episode considered  line         return si computed  taking
account first visit state si episode  each occurrence state episode
called visit si    although state si could appear multiple times rest
episode 
   d  updating cases b using experience gathered  updates b
made cases gathered episodes cumulative reward similar
best episode found point using threshold  line      way  good sequences
provided updates since shown sequences experiences
cause adaptive agent converge stable useful policy  whereas bad sequences may
cause agent converge unstable bad policy  wyatt         prevents
degradation initial performance b computed first step algorithm
use bad episodes  episodes errors  updates  step  two types
updates appear  namely  replacements additions new cases  again  algorithm
iterates case ci    si   ai   v  si    listcasesepisode  line      si known state
 line      compute case   si   a  v  si     b corresponding state si  line     
one note case ci    si   ai   v  si    listcasesepisode built line   
algorithm  replacing action corresponding case   si   a  v  si     b
new action ai resulting application random gaussian noise action
   

figarca   fernandez

policy improvement algorithm
  
  
  
  
  
  
  
  
  
  

given case base b  maximum number cases
given baseline behavior
given update threshold
   set maxt otalrwepisode      maximum cumulative reward reached episode
   repeat
 a  initialization step 
set k      listcasesepisode   totalrwepisode    
 b  case generation 
k   maxepisodelength
compute case   s  a  v  s    b closest current state sk


  
  
  
  

 b  sk          known state
chose action ak using equation  
perform action ak
create new instance cnew     s  ak   v  s  

  
  
  
  
  
  

else    unknown state
chose action ak using
perform action ak
create new instance cnew     sk   ak     
totalrwepisode    totalrwepisode   r sk   ak  
listcasesepisode    listcasesepisode cnew

  
  
  
  
  
  
  
  
  
  

set k   k    
 c  computing state value function unknown states 
instance ci listcasesepisode


 b  si          unknown state
p
return si      kj n jn r sj   aj      n first ocurrence si episode
v  si      return si  
 d  updating cases b using experience gathered  
totalrwepisode    maxt otalrwepisode  
maxt otalrwepisode    max  maxt otalrwepisode  totalrwepisode 
case ci    si   ai   v  si     listcasesepisode


  
  
  

 b  si          known state
compute case   si   a  v  si     b corresponding state si
compute   r si   ai     v  si     v  si  

  
  
  
  
  

   
replace case   si   a  v  si     b case   si   ai   v  si     listcasesepisode
v  si     v  si    
else    unknown state
b    b ci

  
  
  
  

kbk  
remove kbk least frequently used cases b
stop criterion becomes true
   return b

figure    description step two pi srl algorithm 

equation    then  temporal distance  td  error computed  line          
performing action ai results positive change value state  action 
   

fisafe exploration state action spaces reinforcement learning

turn  could potentially lead higher return and  thus  better policy  van hasselt
wiering        update value function using actions potentially lead
higher return  td error positive  ai considered good selection
reinforced  algorithm  reinforcement carried updating output
case   si   a  v  si     b ai  line      therefore  update case base occurs
td error positive  similar linear reward inaction update learning
automata  narendra   thathachar              sign td error used
measure success  pi srl updates case base actual improvements
observed  thus avoiding slow learning plateaus value space
td errors small  shown empirically procedure result
better policies step size depends size td error  van hasselt  
wiering         important note replacements produce smooth changes
case base b since action replaced ai results higher v  si   ai a 
form updating understood risk seeking approach  overweighting
transitions successor states promise above average return  mihatsch   neuneier 
       additionally  prevents degradation b  ensuring replacements made
action potentially lead higher v  si   
if  instead  si known state  case ci added b  line      finally 
algorithm removes cases b necessary  line      complex scoring metrics calculate
cases removed given moment proposed several authors 
forbes andres        suggest removal cases contribute least overall
approximation  driessens ramon        pursue error oriented view
propose deletion cases contribute prediction error examples 
principal drawback sophisticated measures complexity 
determination case s  removed involves computation score value
ci b  turn requires least one retrieval regression  respectively 
cj b  j    i   entire repeated sweeps case base entail enormous
computational load  gabel riedmiller        compute different score metric
ci b  requiring computation set k nearest neighbors around ci  
approaches well suited systems learning adjusted time requirements
high dimensional state space  requiring use larger case bases
proposed here  rather  paper  propose removal least frequently used
cases  idea seems intuitive insofar least frequently used cases usually contain
worse estimates corresponding states value  although strategy might lead
function approximator forgets valuable experience made past
 e g   corner cases   despite this  pi srl performs successfully domains proposed
using strategy  demonstrated section    thus  ability forget ineffectual
known states described section   result algorithm removing kbk cases
least frequently used cases b 
    parameter setting design
one main difficulties applying pi srl algorithm given problem
decide appropriate set parameter values threshold   risk parameter  
update threshold maximum number cases   incorrect value
   

figarca   fernandez

parameter lead mislabeling state known really unknown  potentially
leading damage injury agent  case risk parameter   high values
continuously result damage injury  low values safe  allow
exploration state action space sufficient reaching near optimal policy  unlike
  parameter related risk  instead directly related
performance algorithm  parameter used determine good episode
must respect best episode obtained  since best episodes used
update case base b  value large  bad episodes may used update b
 influencing convergence performance algorithm   if  instead  low 
number updates b may insufficient improving baseline behavior  finally 
high value allows large case bases  increasing computational effort
retrieval degrading efficiency system  contrast  low value might
excessively restrict size case base thus negatively affect final performance
algorithm  subsection  solid perspective given automatic definition
parameters  parameter setting proposed taken suitable set
heuristics tested successfully wide variety domains  section    
parameter   parameter domain dependent related average size
actions  paper  value parameter established
computing mean distance states execution baseline
behavior   expressed another way  execution policy provides
state action sequence form s  a  s  a        sn   thus  value
computed using equation   

 

dist s    s              dist sn    sn  
n 

   

parameter   several authors agree impossible completely avoid
accidents  moldovan   abbeel        geibel   wysotzki         important
note pi srl completely safe first step algorithm executed 
however  proceeding way  performance algorithm heavily
limited abilities baseline behavior  running subsequent
exploratory process inevitable learner performance improved beyond
baseline behavior  since agent operates state incomplete knowledge
domain dynamic  inevitable exploratory process
unknown regions state space visited agent may reach error
state  however  possible adjust risk parameter determine level
risk assumed exploratory process  paper  start low
values  low risk  gradually increase  specifically  propose beginning
        increasing value iteratively either accurate policy
obtained amount damage injury high 
parameter   value parameter set relative best episode obtained 
paper  value set    cumulative reward best episode
obtained 
   

fisafe exploration state action spaces reinforcement learning

figure    trajectories generated baseline policy deterministic  slightly
stochastic highly stochastic domain 
parameter   previously  estimated maximum number cases stored
case base estimated maximum number cases required properly mimic baseline behavior   follows description value
computed  figure   presents trajectories  sequences states  followed baseline policy three different domains  deterministic  slightly stochastic highly
stochastic  domain  different sequences states produced
represented  s     s     s             s n     s     s     s             s n           s     sm    sm            smn   
sji i th state  s   initial state sjn final state resulting
trajectory episode j  deterministic domain  different executions
always result trajectory  case  set maximum number
cases   n cases computed episode stored 
slightly stochastic domain  trajectories produced different episodes
different  slightly so  here  suppose case base beginning
empty  additionally  assume states  s     s     s             s n   corresponding first trajectory produced domain stored case base 
furthermore  domain execute different episodes  obtaining different
trajectories  following execution episodes  compute maximum distance i th state first trajectory  previously added case base 
i th state produced trajectory j max jm d s i   sji   
slightly stochastic domain  maximum distance exceed threshold
case max jm d s i   sji       point  assume i th state
trajectory j least one neighbor distance less  corresponding
state s i    thus  i th state j added case base 
contrast  highly stochastic domain  maximum distance greatly exceeds
threshold cases max jm d s i   sji        domain 
estimate total number cases added case base following
   

figarca   fernandez

way  i th state sequence
j first trajectory k estimate number
max jm d s i  sji  
cases added case base
or  words 

compute number intervals range     max jm d s i   sji    width
 the threshold used decide whether new case added casebase   consequently  estimated number cases addedjto case base  taking

k
pn
max jm d s i  sji  
account states sequence  computed i  
  finally 

estimated maximum number cases computed shown equation   

 n 


n
x
max jm d s i   sji  


i  

 
   

important remember deterministic domain  summation equation   equal   that  therefore    n  increase value element
related increase stochasticity environment  insofar greater
stochasticity environment increases number cases required  finally 
number cases large nearly infinite  threshold increased
make restrictive addition new cases case base  however 
increase may adversely affect final performance algorithm 

   experimental results
section presents experimental results collected use pi srl policy
learning four different domains presented order increasing complexity  i e   increasing number variables describing states actions   car parking problem  lee  
lee         pole balancing  sutton   barto         helicopter hovering  ng et al        
business simulator simba  borrajo et al          domains 
proposed learning near optimal policy minimizes car accidents 
pole disequilibrium  helicopter crashes company bankruptcies  respectively 
learning phase  four domains stochastic experimentation 
helicopter hovering business simulator simba are  themselves  stochastic and 
additionally  generalized domains  made car parking pole balancing domains stochastic intentional addition random gaussian noise actions
reward function  results pi srl four domains compared yielded
two additional techniques  namely  evolutionary rl approach selected winner
helicopter domain      rl competition  martn h    lope        geibel
wysotzkis risk sensitive rl approach  geibel   wysotzki         evolutionary approach  several neural networks cloning error free teacher policies added initial
population  guaranteeing rapid convergence algorithm near optimal policy and 
indirectly  minimizing agent damage injury   indeed  winner helicopter
domain agent highest cumulative reward  winner must indirectly
minimize helicopter crashes insofar incur large catastrophic negative rewards 
hand  risk sensitive approach defines risk probability  s  reaching
terminal error state  e g   helicopter crash ending agent control   starting initial
   

fisafe exploration state action spaces reinforcement learning

state s  case  new value function weighted sum risk probability 
  value function  v   used  equation    
v  s    v  s   s 

   

parameter   determines influence v  s  values compared  s values       v corresponds computation minimum risk policies  large
values  original value function multiplied dominates weighted criterion 
geibel wysotzki        consider finite  discretized  action sets study 
algorithm adapted continuous action sets  use cbr value
risk function approximation gaussian exploration around current action 
experiments  domain  three different values used  modifying influence
v  values compared  values  cases  goal improve control
policy while  time  minimizing number episodes agent damage
injury  domain  establish different risk levels modifying risk parameter
values according procedure described subsection      important note
one baseline behavior used initialize evolutionary rl approach exactly
used subsequently first second step pi srl  furthermore  case base
risk sensitive approach begin scratch since initialized safe
  makes comparison performances fair possible 
case based policy b
taking account different techniques make use baseline behaviors 
    car parking problem
car parking problem represented figure    originates rl literature  cichosz         car  represented rectangle figure     initially located
inside bounded area  represented dark solid lines  referred driving area 
goal learning agent navigate car initial position garage 
car entirely inside  minimum number steps  car cannot move
outside driving area  figure     b  shows two possible paths car take
starting point garage obstacle order correctly perform
task  consider optimal policy domain reaches goal
state shortest time which  time  free failures 
state space domain described three continuous variables  namely 
coordinates center car xt yt angle cars axis
x coordinate system  car modeled essentially two control
inputs  speed v steering angle   let us suppose car controlled
steering angle  i e   moves constant speed   thus  action space described one
continuous variable        corresponding turn radius  used equations
below  agent receives positive reward value r       dist pt   pg        
pt    xt   yt   center car  pg    xg   yg   center garage  i e   goal
position  normalizing function scaling euclidean distance dist pt   pg  
pt pg range        car inside garage  i e   reward value greater
car parked correctly center garage   agent receives reward
   whenever hits wall obstacle  steps receive reward       thus 
difficulty problem lies reinforcement delay  fact
   

figarca   fernandez

figure     car parking problem   a  model car parking problem   b  examples
trajectories generated agent park car garage 
punishments much frequent positive rewards  i e   much easier hit
wall park car correctly   motion car described following
equations  lee   lee       
t       v   l    tan    

   

xt     xt   v cos t     

   

yt     yt   v sin t     

    

v linear velocity car  assumed constant value  
maximum steering angle  i e   car change position maximum angle
directions  simulation time step  gaussian noise added
actions rewards standard deviation      since noisy interactions inevitable
real world applications  adding noise actuators environment 
transform deterministic domain stochastic domain  important note
noise added transform domain stochastic domain independent
gaussian noise standard deviation  risk parameter  used explore state
action space second step pi srl algorithm  case  gaussian noise
standard deviation used exploration added noise previously added
actuators  paper  l      m   v        m s           rad         s 
 the driving area obstacle dimensions detailed figure     a    initial position
car fixed xs        ys               rad   goal position
xg        yg         domain  designed baseline behavior
average cumulative reward per trial      
order perform pi srl algorithm  modeling baseline behavior step exe learned demonstrations
cuted  result step safe case based policy b
provided baseline behavior  see subsection       computed following
procedure described subsection     resulting values           respectively 
   

fisafe exploration state action spaces reinforcement learning

figure     car parking task modeling baseline behavior step   a  number steps per
trial executed case base b baseline behavior    b  cumulative
reward per trial baseline behavior   learned safe case based policy
ibl approach 
b
figure     a  graphically represents execution modeling baseline behavior step 
it  two different learning processes presented and  one  number steps per
trial executed baseline behavior  continuous red lines  cases b  dashed
green lines  shown  beginning learning process empty case base b 
steps performed using baseline behavior   learning process continues 
learned  around trials
new cases added b safe case based policy b
       practically steps performed using cases b rarely used 
means safe case based policy learned  two learning processes shown
figure     a   modeling baseline behavior step performed without collisions
wall obstacle  words  baseline behavior cloned safely without
errors  figure     b  shows cumulative reward three different execution processes 
first  continuous red lines  corresponding performance baseline behavior
  second  dashed green lines  corresponding previously learned safe case based
 derived b   third  dashed blue lines  corresponding instancepolicy b
based learning  ibl  approach consisting storing cases memory  ibl approach 
new items classified examining cases stored memory determining
similar case s  given particular similarity metric  euclidean distance used paper   classification nearest neighbor  or nearest neighbors  taken
classification new item using   nearest neighbor strategy  aha   kibler        
approach  two different executions carried out  ibl approach  training
process performed saving training cases produced baseline behavior
   trials  so consider approach ib  algorithm sense saves every
case training phase  see aha   kibler         figure     b  shows safe
case based policy b almost perfectly mimics behavior baseline behavior  
domain  performance ib  approach similar 
figure     a  shows results different risk configurations obtained improving
learned baseline behavior step  risk configuration  two different learning pro   

figarca   fernandez

figure     improving learned baseline behavior step car parking problem   a  cumulative reward per episode different risk configurations    obtained
pi srl   b  cumulative reward per episode evolutionary rl risksensitive rl approaches  cases  episode ending failure marked 
cesses performed  trials ending failure  car hits wall obstacle  marked
 blue triangles   learning processes figure     a  demonstrate number
failures increases increase parameter   low level risk             
although failures produced  performance nevertheless weak  around baseline behavior   constant throughout whole learning process  additional
experiments demonstrated increasing value        increases
number failures without improving performance  figure     b  shows results
evolutionary risk sensitive rl approaches different values  regarding former 
number failures higher obtained pi srl approach  final
performance similar  case latter  performance higher        value
maximization   yet agent consistently crashes car wall 
figure    shows mean number failures  i e   car collisions  cumulative reward
approach     trials red circles corresponding pi srl algorithm 
black triangles risk sensitive approach blue square evolutionary
rl approach  additionally  figure    shows two asymptotes  horizontal asymptote
established according cumulative reward obtained highest value 
horizontal asymptote indicates higher values increase number failures without
improving cumulative reward  which may  fact  get worse   vertical asymptote
f ailures     indicates reducing risk parameter reduce number
failures  figure    shows performance two additional risk levels 
high level risk             low level risk         respect
figure     using low level risk      additional random gaussian
noise added actions algorithm free failures  although performance
learned first step
improve respect safe case based policy b
algorithm  pi srl medium level risk             free
failures  yet performance slightly improved  pi srl algorithm high level
risk             obtains highest cumulative reward           mean
   

fisafe exploration state action spaces reinforcement learning

figure     mean number failures  car collisions  cumulative reward     trials
approach car parking task  means computed   
different executions 

     failures  however  using high level risk              number
failures greatly increases and  consequently  cumulative reward decreases  shown
figure     pi srl high risk             evolutionary rl approach obtain
similar performance  pi srl demonstrates faster convergence  thus  figure    
cumulative reward obtained pi srl higher   pareto comparison criterion
used compare solutions figure     using principle  one solution strictly
dominates  or preferred to  solution parameter strictly worse
corresponding parameter least one parameter strictly better 
written y  indicating strictly dominates y  accordance pareto
principle  assume points figure    corresponding pi srl solutions 
save pi srl high level risk  pareto frontier  since points
strictly dominated solution  i e   solution has  time 
higher cumulative reward lower number failures pi srl   domain 
solution pi srl medium level risk strictly dominates  or preferred to 
risk sensitive solutions  pi srl         risk sensitive  solution pi srl
high level risk strictly dominates solution evolutionary rl solution
 pi srl         evolutionary rl  
nevertheless  important note ultimate decision approach
figure    best depends criteria researcher  if  instance  minimization number failures deemed important optimization criterion
 independently improvement obtained respect baseline behavior   
best approach pi srl low level risk              similarly 
maximization cumulative reward instead judged important optimization criterion  independently number failures generated   best approach
pi srl high level risk             
figure    shows evolution cases case base b  known space  different
trials high risk learning process  graph presents set known states  green
   

figarca   fernandez

figure     car parking problem  evolution known space different trials    
 a         b          c         d  high risk learning process
             graph corresponds situation state space
accordance case base b trial  
area   error states  red area   unknown states  yellow area  non error states
 orange circles   pi srl adapts known space order find safer better policies
complete task  figure     a  shows initial situation b  corresponding
   robust sense never results
previously learned safe case based policy b
collisions  suboptimal  it selects longest parking path driving around upper
side obstacle   learning process progresses  figure     b    pi srl finds
shorter path park car garage along upper side obstacle  increasing
performance   comes closer obstacle  increasing probability
collisions   figure     c   pi srl finds new even shorter path  time along
lower side obstacle  however  still cases case base b corresponding
older path along upper side obstacle  so figure     c  indicates two paths
park car   finally  figure     d   cases corresponding suboptimal path
along upper side obstacle removed b replaced new cases
corresponding safe improved path along lower side obstacle 
words  pi srl adapts known space exploration unknown space
order find new improved behaviors  process adjusting known space
   

fisafe exploration state action spaces reinforcement learning

safe better policies  algorithm forgets previously learned  yet ineffective
known states 
following experiment  becomes apparent domain noisy enough  even
taking risk  i e   noise added actuator exploration  
agent could nevertheless perform poorly constantly produce collisions  experiment
serves explain domain noise never sufficient efficient exploration
space without action selection noise  experiment  intentionally added
noise actuators performed second step pi srl again  however
time taking risk  i e         test  added random gaussian noise
standard deviation      rather standard deviation     used previously 
actuators  figure    shows two executions second step  improving learned
baseline policy  pi srl algorithm x axis indicating number trials 
y axis cumulative reward per episode failures  i e   collisions  marked blue
triangles  experiments figure     b   case based policy b low level
risk             never produces failures  contrast  experiments shown
figure     case based policy b continually collides wall although
risk parameter set           furthermore  increase performance
detected 

figure     improving learned baseline behavior step car parking task  two learning processes risk configuration     increase noise
actuators 

increase noise actuators second step algorithm respect
first step  the case based policy b learned first step using gaussian random
noise actuator standard deviation      second step performed
using gaussian random noise actuator standard deviation      takes
agent beyond known space case base b learnt first step pi srl
allows find new trajectories parking car garage  new situation 
exploration process guided follows  known state reached  agent performs
action retrieved b without addition gaussian noise  since risk parameter
     see line    figure   algorithm   unknown state reached  agent performs
   

figarca   fernandez

action advised baseline behavior  see line      using exploration
process  new better trajectory found parking car garage 
resulting cases episode corresponding unknown states added case base
 see line      slightly improving performance figure     important note
replacements cases  see line     change actions b  since
replaced action previously retrieved b plus certain amount gaussian
noise standard deviation  see line      nevertheless  given risk parameter
set    actions retrieved case base replaced  exploration
process  however       i e   taking risk  lead optimal behavior since 
actions performed unknown situations added case base b performed using baseline behavior supposed perform suboptimal actions
 see definition baseline behavior  
actions cases b replaced improved actions  gaussian
noise standard deviation used explore different better actions
provided b   however  case  risk parameter set    
new better actions discovered 
additional experiments demonstrate pi srl behaves much worse higher
value noise used actuators  with collisions episodes   assume taking
risk  i e        implies always performing actions discovering
newer better actions provided learned case base b baseline
behavior   pi srl  replacements case base executed towards
promising action which  case  guarantees higher return 
exploration necessary order obtain  near  optimal behavior  since without
exploration  new better actions discovered pi srl performance limited
case based policy learned first step b baseline behavior
which  one must remember  intended perform suboptimal policies 
    pole balancing
name suggests  objective pole balancing problem balance pole
vertically top moving cart  sutton   barto         state description consists
four dimensional vector containing angle   radial speed     cart position
x speed x    action consists real valued force used push cart 
study  reward computed encourage actions keep pole upright
possible cart cart centered possible track  thus  reward
step computed rt       t      xt       normalizing functions
scaling angle position xt range         episode composed       
steps  although may nevertheless end prematurely pole becomes unbalanced  i e  
inclination twelve degrees either direction  cart falls
track  i e      m center track  
considered failures  car parking problem  gaussian noise added actions
rewards  time standard deviation       pole balancing domain
becomes stochastic addition noise actuators reward function 
   

fisafe exploration state action spaces reinforcement learning

figure     modeling baseline behavior step pole balancing task   a  number steps per
trial executed case base b baseline behavior    b  cumulative reward
ibl approach 
per trial   learned safe case based policy b
hand made baseline behavior demonstrates execution safe  yet suboptimal
policy  average cumulative reward per episode trial      
learnt
modeling baseline behavior step pi srl  safe case based policy b
demonstrations provided baseline behavior   computed following
procedure described subsection      values             respectively 
figure     a  shows two different learning processes modeling baseline behavior step 
learning process  figure     a  shows number steps per trial executed
baseline behavior  continuous red lines  case base b  dashed green lines  
beginning learning process  case base b empty steps performed
using baseline behavior   learning process progresses  however  b filled
learnt  end learning process  after around
safe case based policy b
      trials   almost steps performed using cases b rarely used 
important note modeling baseline behavior step performed without
failures  i e   pole disequilibrium cart track  case  previous
task  figure     b  represents three independent execution processes using previously  derived b indicated dashed green lines  
learned safe case based policy b
baseline behavior  indicated continuous red lines  approach based
ibl  indicated dashed blue lines   aha   kibler         average cumulative
      figure     b    almost perfectly clones  
reward per episode b

b
ib  approach which  cases  results pole disequilibrium cart falling
track averages cumulative reward per episode      
figure     a  shows results pi srl different risk configurations 
configuration  learning curves shown two different learning processes performed 
additionally  episode ending failure marked  blue triangles   increase
risk increases probability failure  policy obtained nevertheless better terms
cumulative reward  nevertheless  much greater risk values             produce
failures without accompanying increase cumulative reward  figure     b  shows
results evolutionary risk sensitive rl approaches  former
   

figarca   fernandez

figure     improving learned baseline behavior step pole balancing task   a  cumulative reward per episode different risk configurations    obtained pisrl   b  cumulative reward per episode obtained evolutionary risksensitive rl approaches  cases  episode ending failure marked 

clearly algorithm greatest number failures  risk sensitive approach 
       value maximization   agent selects actions result higher value 
higher risk  contrary       risk minimization   agent
learns risk function  at around episode        selects actions lower risk  and
lower number failures   considerably weak performance  value      
produces intermediate policy  consequently  concluded pi srl high
level risk obtains better policies less failures evolutionary risk sensitive
rl approaches  figure    reinforces previous conclusions 

figure     mean number failures  pole disequilibrium cart track  cumulative reward     trials approach pole balancing task 
means computed    different executions 
   

fisafe exploration state action spaces reinforcement learning

it  mean number failures cumulative reward        trials shown 
red circles corresponding pi srl  black triangles corresponding risksensitive approach blue square corresponding evolutionary rl approach 
figure shows performance two additional risk levels  high level risk
            low level risk         respect figure     cumulative
reward number failures increase high level risk             
risk level represents inflection point higher levels risk produce failures
without accompanying improvement cumulative reward  fact  high level
risk             results reduction cumulative reward compared
high level risk              again  pareto comparison criterion may used
compare solutions figure     domain  solution pi srl
low level risk strictly dominates risk sensitive solutions             
pi srl         risk sensitive              additionally 
solution pi srl high level risk strictly dominates evolutionary rl solution 
pi srl         evolutionary rl 
lastly  figure    shows evolution known space derived case base
b different trials high risk learning process  graph  error states  red
area   set unknown states  yellow area   set known states  green area 
set non error states  orange circles  represented  known space
graph computed taking cases b trials                      
graph  non error states computed    different executions b
trial  the orange circles representing terminal states executions  
first graph  figure     a   presents initial known space resulting modeling
baseline behavior step  evolution figure    demonstrates two different points  first 
pi srl progressively adapts known space order encounter better behavior
known space tends compressed toward center coordinates 
due fact reward greater angle pole cart
position x    i e   pole upright possible cart cart centered
track   second  risk failure pole balancing domain greater
early trials learning process  beginning learning process  figure     a   
      regions known space close error space  situation 
slight modifications actions consistently produce visits states  i e   pole
disequilibrium cart falling track   learning process advances  figure   
 b    c   d    known space compressed toward origin coordinates away
error space  consequently  probability visiting error states decreases 
example  returning figure     a   high risk learning processes      failures
      occur first      trials  remaining           occur last     
trials 
    helicopter hovering
suggested name  objective domain make helicopter hover close
possible defined position duration established episode  task challenging two main reasons  firstly  state action spaces high dimensional
continuous  more specifically  state space    dimensional action space
   

figarca   fernandez

figure     pole balancing task  evolution known space different trials      a  
        b           c          d  high risk learning process
             graph corresponds situation state space
according case base b trial  
  dimensional   secondly  generalized domain whose behavior modified wind
factor  helicopter episode composed      steps  although may end prematurely
helicopter crashes  first step pi srl performed order imitate baseline
behavior   computed following procedure described subection    
values            respectively  step performed  resulting
able properly imitate baseline behavior  
safe case based policy b

figure     a  shows two learning processes modeling baseline behavior step 
similar previous tasks  learning processes progress  number steps executed
baseline behavior reduced number steps using case base b
increases  end learning process  case base b stores safe case based
  figure     b  compares performance  in terms cumulative reward per
policy b
ib  approach  regarding
episode    learned case based policy b
first two  average cumulative reward per episode            obtained
           although perfectly mimic baseline behavior  
b

b
   

fisafe exploration state action spaces reinforcement learning

figure     modeling baseline behavior step helicopter hovering task   a  number
steps per trial executed case base b baseline behavior    b  cumula ibl
tive reward per trial   learned safe case based policy b
approach 
nevertheless performs safe policy without crashing helicopter  regard
training process ib  approach  every case produced    episodes baseline
behavior stored  figure     b  demonstrates ib  approach consistently results
helicopter crashes  performance extremely far learned safe case   improvement policy begins state action space safely
based policy b
b
explored execution step two pi srl 
figure     a  shows results different risk levels  pi srl low medium
levels risk levels produce helicopter crashes pi srl  performance nevertheless
quite weak 

figure     improving learned baseline behavior step helicopter hovering task   a 
cumulative reward per episode different risk configurations obtained pisrl   b  cumulative reward per episode obtained evolutionary risksensitive rl approaches  cases  episode ending failure marked 
   

figarca   fernandez

conversely  high level risk established produces near optimal policy
low number collisions  extensive experimentation demonstrates increasing risk
parameter         increases number crashes without accompanying
improvement cumulative reward  figure     b  shows results evolutionary
rl approach which  remembered  selected winner rl competition
     domain  martn h    lope         well risk sensitive rl algorithm
different values  comparison results evolutionary rl approach
pi srl shows similar cumulative reward  significantly higher number
crashes former latter  evolutionary approach  crashes
occur early steps learning process  pi srl  accidents occur
advanced steps learning process  case risk sensitive rl algorithm 
           risk function learned around episode       point 
agent selects lower risk actions number crashes considerably reduced 
      agent selects actions resulting higher values without taking risk
account  performance improves  expense increased number accidents 
nevertheless whatever value  number crashes higher performance
worse pi srl 

figure     mean number failures  helicopter crashes  cumulative reward     
episodes approach helicopter hovering task  means
computed    different executions 
information figure     indicating mean number failures cumulative
reward      episodes approach  complements conclusions made above 
data computed    independent executions approach 
previous domains  pi srl indicated red circles  risk sensitive approach
black triangles evolutionary rl approach blue square  figure    shows
performance two additional risk levels  high level risk            
low level risk         respect figure     figure    demonstrates
evolutionary rl approach obtains highest cumulative reward             
followed closely pi srl              approaches far results 
regarding number failures  i e   helicopter crashes   pi srl low level
risk         low level risk             medium level risk            
   

fisafe exploration state action spaces reinforcement learning

produces collisions  pi srl algorithm medium risk preferable inasmuch
cumulative reward higher               using pareto comparison criterion 
pi srl solution high level risk strictly dominates solutions risksensitive approach  pi srl         risk sensitive   moreover  pi srl strictly
dominated solution 

figure     evolution known space different episodes helicopter hovering
task   a  example representation single known state radar chart   b  
 c    d  known states episodes                    respectively 
high risk learning process              graph corresponds
situation known space according case base b episode  
pole balancing domain  figure    shows evolution known space
according case base b different episodes high risk learning process 
case  radar charts used due high number features describing states 
radar chart graphical method displaying multivariate data two dimensionally 
figure  axis represents one features state and  preserve simplicity
representation  charts generated normalizing absolute values features
     figure     a  example representation single known state 
   

figarca   fernandez

value axis corresponds value individual feature state
line drawn connecting feature values axis  line figure     a 
represents single state  figures     b    c   d  show known space according
case base b episodes              respectively  three charts represent
single state  rather states b corresponding episode  thus 
graph  set known states marked  green area   state considered error state
single feature value state greater    limits  marked red line
graphs  computed taking account helicopter crashes  i 
velocity along main axes exceeds   m s   ii  position helicopter
   m   iii  angular rate around main axes exceeds     rad s
 iv  orientation    degrees target orientation  previous
tasks  figure    indicates two different matters  first  learning proceeds  known
space derived b adjusted space used better safer policies 
helicopter domain  agent tries hover helicopter close possible target
position  i e   origin coordinates   since immediate rewards greater closer
helicopter hovers origin  thus  known space starts expand  figure   
 b   and  progressively  concentrated origin coordinates  figure     c   d   
regard second matter  probability crashing low since 
beginning  known space already appears concentrated origin far
error space  figure     b    words  beginning  features
known space  i e   forward  sideways downward velocities  x  y  z coordinates  x 
z angular rates  x  z quaternation  far error space limits 
decreasing probability visiting error state 
previous experiments  second step pi srl performed using
initial case base b free failures built first step algorithm 
following experiments show performance second step pi srl different
initial policies used  figure     a  shows performance policies used initial
policies  continuous black line indicates performance initial safe case based
policy b   average cumulative reward per episode             used previous
experiments prior execution step two algorithm  remaining lines
figure correspond performance three different initializations case base b
used new experiments  prior execution step two algorithm  using
poor initial policy  dashed green lines  helicopter crashed nearly
episodes  average cumulative reward per episode calculated             
using different poor  albeit less poor  initial policy  continuous red lines 
helicopter crashed occasionally  average cumulative reward per episode            
finally  near optimal policy  dashed blue lines  whereby helicopter hovering free
failures yields average cumulative reward per episode           
figure     b  shows performance second step  improving baseline behavior
step  pi srl  starting case base b corresponding poor  poor
near optimal policies presented figure     a   figure     b   dashed blue lines
correspond use case base b containing near optimal policy  continuous
red lines correspond use case base b containing poor policy dashed
green lines correspond use case base b containing poor policy 
experiments figure conducted using high level risk domain
   

fisafe exploration state action spaces reinforcement learning

figure      a  performance different initial policies helicopter hovering task 
 b  performance different executions second step pi srl 
starting case base b containing policy three different types 
poor  poor near optimal 

             graph indicates use near optimal policy initial
policy high level risk level  case base worsen performance which 
fact  appears improve slightly  second step pi srl prevents degradation
initial performance b  since updates cases case base made using bad
episodes  words  updates b made cases gathered episodes
cumulative reward similar best episode found particular point
using threshold  whose value set    cumulative reward best episode  
example  cumulative reward best episode            episodes
cumulative reward higher         used update case base  discarding
bad episodes episodes failures   way  good sequences experiences
provided updates  since proven good sequences experiences
cause adaptive agent converge stable useful policy  bad sequences
may cause agent converge unstable poor policy  wyatt         solid red
lines figure     b  show using poor policy failures initial policy produces
higher number failures using initial policy free failures  however
despite poor initialization  pi srl nevertheless able learn near optimal policy
well policy free failures used initialize b  see lines corresponding high
level risk            figure     a    finally  dashed green lines figure   
 b  show use poor initial policy many failures results decreased
performance higher number failures produced  even though nevertheless able
learn better behavior  case  algorithm falls local minimum  probably
biased poor initialization  cases poor policies  number
failures higher beginning learning process decreases learning
process proceeds  poor poor initial policies close
error space  stark contrast initial policy shown figure    which 
beginning  already appears concentrated origin  far error space 
   

figarca   fernandez

learning process proceeds  different policies compressed away error
space number failures decreases 
    simba
business simulators powerful tools improving management decision making processes  example tool simulator business administration  simba 
 borrajo et al          simba competitive simulator  since agents compete
agents management different virtual companies  simulator 
result twenty years experience university students business
executives  emulates business realities using variables  relationships events
present business world  objective provide users integrated vision
company  using basic techniques business management  simplifying complexity
emphasizing content principles greatest educational value  borrajo et al  
       experiments performed here  learning agent competes five handcoded agents  borrajo et al          decision making simba episodic task
decisions made sequentially  make business decision  state must studied
   continuous decision variables  e g   selling price  advertising expenses  etc   must
set  followed study state composed    continuous variables  e g   material
costs  financial expenses  economic productivity  etc    borrajo et al          episode
composed    steps  although may prematurely company goes bankrupt  i e  
losses higher     net assets  

figure     modeling baseline behavior step simba task   a  number steps per trial
executed case base b baseline behavior    b  cumulative reward per
ibl approach 
trial   learned safe case based policy b
figure     a  shows evolution number steps executed baseline behavior case base b two learning processes performing modeling baseline
behavior step  computed following procedure described subsection    
values            respectively  episodes  approximately     
learned  figure     b  shows performance previouslysafe case based policy b

learned b   ib  approach  study  mean profits per episode
   

fisafe exploration state action spaces reinforcement learning

figure     improving learned baseline behavior step simba task   a  mean
profits per episode different risk configurations obtained pi srl agent
five hand coded agents   b  mean profits per episode obtained
evolutionary risk sensitive rl agent five hand coded agents 
cases  episode ending failure  bankruptcy  noted 
     million euros  ib 
     million euros  obtained b
approach  cases generated using baseline behavior    episodes stored 
experiments demonstrate simba  contrast previous domains  storing cases sufficient obtaining safe policy performance similar using
modeling baseline behavior step  with mean profits per episode      million euros  
learned  execute improving learned baseline
safe case based policy b
behavior step 
similar findings earlier tasks  figure     a  indicates low medium
levels risk produce bankruptcies  performance nevertheless weak  highest
level risk produces near optimal policy low number number failures 
contrast  figure     b  presents results evolutionary risk sensitive rl approaches  former clearly yields highest number failures 
risk sensitive case  number bankruptcies cases insufficient learning risk function   comparative results figure    show pi srl
        obtains better policies less failures evolutionary risk sensitive
rl approaches 
figure    shows graphical representation different solutions domain 
shows mean number failures cumulative reward different approaches
    episodes  data computed    independent executions approach 
figure  red circles correspond pi srl algorithm  black triangles correspond
risk sensitive approach blue square corresponds evolutionary rl approach 
figure    shows performance two additional risk levels  high            
low         respect figure     experiments figure    demonstrate
pi srl high level risk             obtains highest cumulative reward 
         additionally  pi srl low level risk         low level risk
            medium level risk             approaches lowest

   

figarca   fernandez

figure     mean number failures  company bankruptcies  cumulative reward
    episodes approach simba task  means
computed    different executions 
mean number failures       however  pi srl medium level risk preferred
inasmuch performance superior terms cumulative reward  pi srl
high level risk             increases number failures obtains lower cumulative
reward compared pi srl high level risk  using pareto comparison
criterion  pi srl high level risk strictly dominates solutions  pi srl
       risk sensitive pi srl        evolutionary rl   approach
strictly dominated solution 
due difficulty representing high dimensional state action space
simba domain  graphs provided evolution known space 

   related work
reinforcement learning  rl  case based reasoning  cbr  techniques combined literature different ways  work bianchi et al          new approach
presented permitting use cases heuristics speed rl algorithms  additionally  sharma et al         use combination cbr rl  called carl  achieve
transfer playing game ai across variety scenarios madrts tm 
commercial real time strategy game  cbr used state value function
approximation rl  gabel   riedmiller         however  present study is 
knowledge  first time cbr rl used conjunction safe exploration dangerous domains  field safe reinforcement learning  three principal
trends observed   i  approaches based return variance   ii  risk sensitive
approaches based definition error states  iii  approaches using teachers 
    approaches based return variance
literature  long known optimal policy optimal expected
return mdp quite sensitive parameter variations  even optimal policy may
   

fisafe exploration state action spaces reinforcement learning

perform badly cases due stochastic nature problem   mitigate
problem  agent try maximize return associated worst case scenario 
even though case may highly
unlikely  thus  trend  risk refers worst
p
r variance  example approach
outcomes return r  


t  
worst case control worst possible outcome r optimized  coraluppi
  marcus        heger         worst case control strategies  optimality criterion
exclusively focused risk avoiding policies  policy considered optimal
worst case return superior  approach  however  restrictive inasmuch takes
rare scenarios fully account 
value return introduced heger        seen extension
worst case control mdps  concept establishes returns r  
policy occur probability lower neglected  algorithm less
pessimistic pure worst case control  given extremely rare scenarios effect
policy  work heger et al   idea weighting return risk  namely
expected value variance criterion  introduced 
risk sensitive control based use exponential utility functions  return r
transformed reflect subjective measure utility  instead maximizing expected
value r  objective maximize u     loge er    parameter
r usual return  shown that  depending parameter   policies
high variance v  r  penalized        enforced         instead  neuneier
mihatsch        consider worst case outcomes policy   i e   risk related
variability return   study  authors demonstrate learning algorithm
interpolates risk neutral worst case criterion limiting
behavior exponential utility functions  noted approaches based
variability return worst possible outcomes suited problems
policy small variance produce large risk  geibel   wysotzki        
view risk present study  however  concerned variance
return worst possible outcome  instead fact processes generally
possess unsafe states avoided  consequently  address different class
problems dealt approaches focusing variability return 
    risk sensitive approaches based error states 
second trend approaches  concept risk based definition error
states fatal transitions  thus  geibel et al           instance  establish risk
function probability entering error state  instead  hans et al        consider
transition fatal corresponding reward less given threshold  
first case demonstrated section    learned td methods require
error states  i e   car collisions  pole balancing disequilibrium  helicopter crashes
company bankruptcies  visited repeatedly order approximate risk function
and  subsequently  avoid dangerous situations  second case  concept risk
joined reward  moreover  mentioned studies either  i  assume
system dynamics known   ii  tolerate undesirable states exploration
or  contrast paper   iii  deal problems high dimensional
continuous state action spaces  regarding latter  geibel et al  write
   

figarca   fernandez

approach extended continuous action sets  e g   using actor critic
method   give details may done entirely continuous
problems  section    present approach solves problem 
    approaches using teachers
last trend approaches based use teachers three different ways 
 i  bootstrap learning algorithm  i e   initialization procedure    ii  derive
policy finite demonstration set  iii  guide exploration process 
      bootstrapping learning algorithm
work driessens szeroski         bootstraping procedure used relational rl finite set demonstrations recorded human expert
later presented regression algorithm  allows regression algorithm build
partial q function later used guide exploration state space
using boltzmann exploration strategy  smart kaelbling        use examples 
training runs bootstrap q learning approach hedger algorithm 
initial knowledge bootstrapped q learning approach allows agent learn
effectively helps reduce time spent random actions  teacher behaviors
used form population seeding neuroevolution approaches  yao        siebel  
sommer         evolutionary methods used optimize weights neural networks 
starting prototype network whose weights correspond teacher  or baseline
policy   using technique  rl competition helicopter hovering task winners martin et
al         developed evolutionary rl algorithm several teachers provided
initial population  algorithm restricts crossover mutation operators  allowing
slight changes policies given teachers  consequently  rapid convergence algorithm near optimal policy ensured  indirect minimization
damage agent  however  teachers included initial population resulting
ad hoc training regimen conducted competition  consequently  proposed
approach seems somewhat ad hoc easily generalizable arbitrary rl problems 
work koppejan et al                neural networks evolved  beginning
one whose weights corresponds teacher behavior  approach
proven advantageous numerous applications evolutionary methods  hernandez daz
et al         koppejan   whiteson         koppejans algorithm nevertheless seems
somewhat ad hoc designed specialized set environments 
      deriving policy finite set demonstrations
approaches falling category framed according field learning
demonstration  lfd   argall et al          highlighting study abbeel et al        
based apprenticeship learning  approach composed three distinct steps 
first  teacher demonstrates task learned state action trajectories
teachers demonstration recorded  second step  state action trajectories seen
point used learn dynamics model system  model   near optimal policy found using reinforcement learning  rl  algorithm  finally 
policy obtained tested running real system  work tang et
   

fisafe exploration state action spaces reinforcement learning

al          algorithm based apprenticeship learning presented automaticallygenerating trajectories difficult control tasks  proposal based learning
parameterized versions desired maneuvers multiple expert demonstrations  despite
approachs potential strengths general interest  inherently linked
information provided demonstration dataset  result  learner performance
heavily limited quality teachers demonstrations 
      guiding exploration process
driessens szeroski         context relational rl  use given teachers
policy  rather policy derived current q function hypothesis  which
informative early learning stages   selection actions  approach 
episodes performed teacher interleaved normal exploration episodes 
mixture teacher normal exploration make easier regression algorithm
distinguish beneficial poor actions  context lfd 
approaches include teacher advice  argall et al          advice used improve
learner performance  offering information beyond provided demonstration
dataset  approach  following initial task demonstration teacher  agent
directly requests additional demonstration teacher different states
previously demonstrated states single action cannot selected
certainty  chernova   veloso              
works mentioned trend  explicit definition risk ever given 

   conclusions
work  pi srl  algorithm policy improvement safe reinforcement
learning high risk tasks  described  main contributions algorithm
definitions novel case based risk function baseline behavior safe exploration
state action space  use case based risk function presented possible
inasmuch policy stored case base  represents clear advantage
approaches  e g   evolutionary rl  martn h    lope        koppejan   whiteson 
      extraction knowledge known space agent impossible
using weights neural networks  additionally  completely different notion
risk others found literature presented  according notion  risk
independent variance return reward function  require
identification error states learning risk functions  rather  concept
risk described paper based distance known unknown
space and  therefore  domain independent parameter  in sense  proposal allows
application parameter setting method described subsection      
koppejan et al         use function identify dangerous states  contrast
approach  definition function requires strong previous knowledge domain 
furthermore  approaches risk found literature tackle problems
entirely continuous  geibel   wysotzki        report results
one continuous domain  koppejan   whiteson         consequently  difficult know
certain approaches literature generalize easily arbitrary domains 
   

figarca   fernandez

paper presents pi srl algorithm great detail demonstrates effectiveness four entirely different continuous domains  car parking problem  pole balancing 
helicopter hovering business management  simba   experiments presented
paper demonstrate different characteristics learning capabilities pi srl
algorithm 
 i  pi srl obtains higher quality solutions  experiments section   demonstrate
that  save helicopter hovering task  pi srl obtains cases best cumulative
reward per episode least number failures  additionally  using pareto comparison criterion said that  save high risk configuration car parking
problem  approach strictly dominated approach 
 ii  pi srl adjusts initial known space safe better policies  initial known
space resulting first step pi srl  modeling baseline behavior  adjusted
improved second step algorithm  improving learned baseline behavior 
additionally  experiments demonstrate adjustment process compress
known space away error space  e g   pole balancing domain  subsection     
helicopter hovering domain  subsection      or  occasions  require known
space move closer error space  e g   car parking problem  subsection     
event better policies found there 
 iii  pi srl works well domains differently structured state action spaces
value function vary sharply  although car parking problem  polebalancing domain  helicopter hovering task business simulator represent
differently structured problems  experiments study nevertheless demonstrate
pi srl performs well each  furthermore  even domains car parking
problem value function varies sharply due presence obstacle 
experimental results demonstrate pi srl nevertheless successfully handle
difficulty  however  impossible avoid failures known space edge
edge error states algorithm would often explore error states 
 iv  number failures depends distance known space
error space  experiments pole balancing helicopter hovering domains demonstrate number failures depends close known space error
space  due structure domains  improving learned baseline behavior
step algorithm tends concentrate known space origin coordinates
away error space  greater distance known space error
space  lower number failures  additionally  helicopter hovering  known
space is  beginning  far error space  consequently  number failures low beginning   therefore  initial distribution known space
learned baseline policy later influences number failures obtained
second step pi srl 
 v  pi srl completely safe first step algorithm executed  however 
proceeding way  algorithm performance would heavily limited
capabilities baseline behavior  learner performance improved beyond
performance baseline behavior  subsequent exploratory process
second step pi srl must carried out  since complete knowledge domain
dynamic possessed  however  inevitable that  exploratory
   

fisafe exploration state action spaces reinforcement learning

process  unknown regions state space visited agent may reach error
states 
 vi  risk parameter allows user configure level risk assumed 
algorithm  user gradually increase value risk parameter order
obtain better policies  assuming greater likelihood damage learning
system 
 vii  pi srl performs successfully even poor initial policy failures used 
experiments figure    helicopter hovering domain demonstrate pi srl
able learn near optimal policy despite poor initialization  policy
free failures used initialize case base b  however  figure shows
poor initial policy many failures used  pi srl decreases performance
produces higher number failures  although better behavior still learnt 
case  algorithm falls local minimum  likely biased poor initialization 
follows  applicability method discussed  allowing reader
clearly understand scenarios proposed pi srl approach may applicable 
applicability restricted domains following characteristics 
 i  mandatory scenario satisfy two assumptions described section   
according first assumption  nearby states domain must necessarily similar actions  according other  similar actions similar states produce similar
effects  fact similar actions lead similar states assumes degree smoothness dynamic behavior system which  certain environments  may hold 
however  clearly explain section    consider assumptions logical
assumptions derived generalization principles rl literature  kaelbling et al  
      jiang        
 ii  applicability method limited size case base b required
mimic baseline behavior  possible apply proposed approach tasks when 
first step pi srl algorithm  modeling baseline behavior  prohibitively large
number cases required properly mimic complex baseline behaviors  case 
threshold increased restrict addition new cases casebase  however  increase may adversely affect final performance algorithm 
nevertheless  experiments performed section   demonstrate relatively simple
baseline behaviors mimicked almost perfectly using manageable number cases 
 iii  pi srl algorithm requires presence baseline behavior  proposed
method requires presence baseline behavior safely demonstrates task
learned  baseline behavior conducted human teacher hand coded
agent  important note  nevertheless  presence baseline behavior
guaranteed domains 
finally  logical continuation present study would take account automatic
graduation risk parameter along learning process  example  would
particularly interesting exploit fact known space far away error
space order increase risk parameter or  contrary  reduce
close  future work aims deploy algorithm real environments  inasmuch
uncertainty real environments presents biggest challenge autonomous
robots  autonomous robotic controllers must deal large number factors
robotic mechanical system electrical characteristics  well environmental
   

figarca   fernandez

complexity  however  use pi srl algorithm  or risk sensitive approaches 
learning processes real environments could reduce amount damage incurred
and  consequently  allow lifespan robots extended  might worthwhile
add mechanism algorithm detect known state lead directly
error state  problems currently investigated 

acknowledgments
study partially supported spanish miciin projects tin           c      tra          ccg   uc m tic       offer gratitude special thanks
raquel fuentetaja pizan  assistant professor universidad carlos iii de madrid
planning   learning group  plg   generous invaluable comments
revision paper  would thank jose antonio martn  assistant
professor universidad complutense de madrid  invaluable comments regarding
evolutionary rl algorithm 

references
aamodt  a     plaza  e          case based reasoning  foundational issues  methodological variations  system approaches  ai communications              
abbeel  p   coates  a   hunter  t     ng  a  y          autonomous autorotation
rc helicopter  iser  pp         
abbeel  p   coates  a     ng  a  y          autonomous helicopter aerobatics
apprenticeship learning  i  j  robotic res                     
abbott  r  g          robocup       robot soccer world cup xi   chap  behavioral cloning
simulator validation  pp          springer verlag  berlin  heidelberg 
aha  d  w          tolerating noisy  irrelevant novel attributes instance based
learning algorithms  international journal man machine studies                 
aha  d  w     kibler  d          instance based learning algorithms  machine learning 
pp       
anderson  c  w   draper  b  a     peterson  d  a          behavioral cloning student
pilots modular neural networks  proceedings seventeenth international
conference machine learning  pp        morgan kaufmann 
argall  b   chernova  s   veloso  m     browning  b          survey robot learning
demonstration  robotics autonomous systems                 
bartsch sprl  b   lenz  m     hbner  a          case based reasoning  survey future
directions   puppe  f   ed    xps  vol       lecture notes computer science 
pp        springer 
bianchi  r   ros  r     de mantaras  r  l          improving reinforcement learning
using case based heuristics   vol        pp        lecture notes artificial intelligence  springer  lecture notes artificial intelligence  springer 
   

fisafe exploration state action spaces reinforcement learning

borrajo  f   bueno  y   de pablo  i   santos  b  n   fernandez  f   garca  j     sagredo  i 
        simba  simulator business education research  decission support
systems                 
boyan  j   moore  a     sutton  r          proceedings workshop value function
approximation  machine learning conference         technical report cmu cs       
chernova  s     veloso  m          confidence based policy learning demonstration
using gaussian mixture models  joint conference autonomous agents
multi agent systems 
chernova  s     veloso  m          multi thresholded approach demonstration selection
interactive robot learning  proceedings  rd acm ieee international
conference human robot interaction  hri     pp          new york  ny  usa 
acm 
cichosz  p          truncating temporal differences  efficient implementation
td lambda  reinforcement learning  journal artificial intelligence research
 jair             
cichosz  p          truncated temporal differences function approximation  successful examples using cmac  proceedings thirteenth european symposium
cybernetics systems research  emcsr     
coraluppi  s  p     marcus  s  i          risk sensitive minimax control discretetime  finite state markov decision processes  automatica             
defourny  b   ernst  d     wehenkel  l          risk aware decision making dynamic
programming  nips      workshop model uncertainty risk rl 
driessens  k     ramon  j          relational instance based regression relational rl 
international conference machine learning  icml   pp         
driessens  k     dzeroski  s          integrating guidance relational reinforcement
learning  machine learning                 
fernandez  f     isasi  p          local feature weighting nearest prototype classification 
neural networks  ieee transactions on               
fernandez  f     borrajo  d          two steps reinforcement learning  international
journal intelligent systems                 
floyd  m  w     esfandiari  b          toward domain independent case based reasoning
approach imitation  three case studies gaming  workshop case based
reasoning computer games   th international conference case based
reasoning  iccbr   pp       
floyd  m  w   esfandiari  b     lam  k          case based reasoning approach
imitating robocup players  proceedings   st international florida artificial
intelligence research society conference  pp         
forbes  j     andre  d          representations learning control policies 
university new south  pp      
   

figarca   fernandez

gabel  t     riedmiller  m          cbr state value function approximation reinforcement learning  proceedings  th international conference case based
reasoning  iccbr       pp          springer 
geibel  p          reinforcement learning bounded risk  proceedings   th
international conference machine learning  pp          morgan kaufmann 
geibel  p     wysotzki  f          risk sensitive reinforcement learning applied control
constraints  journal artificial intelligence research  jair             
hans  a   schneegass  d   schafer  a  m     udluft  s          safe exploration reinforcement learning  european symposium artificial neural network  pp 
       
heger  m          consideration risk reinforcement learning    th international
conference machine learning  pp         
hernandez daz  a  g   coello  c  a  c   perez  f   caballero  r   luque  j  m     santanaquintero  l  v          seeding initial population multi objective evolutionary algorithm using gradient based information  ieee congress evolutionary
computation  pp            ieee 
hester  t   quinlan  m     stone  p          real time model based reinforcement learning
architecture robot control  tech  rep  arxiv e prints            arxiv 
hu  h   kostiadis  k   hunter  m     kalyviotis  n          essex wizards      team
description  birk  a   coradeschi  s     tadokoro  s   eds    robocup  vol      
lecture notes computer science  pp          springer 
jiang  a  x          multiagent reinforcement learning stochastic games continuous
action spaces  
kaelbling  l   littman  m     moore  a          reinforcement learning  survey  journal
artificial intelligence research  jair             
konen  w     bartz beielstein  t          reinforcement learning games  failures
successes  proceedings   th annual conference companion genetic
evolutionary computation conference  late breaking papers  gecco     pp      
      new york  ny  usa  acm 
koppejan  r     whiteson  s          neuroevolutionary reinforcement learning generalized helicopter control  gecco       proceedings genetic evolutionary
computation conference  pp         
koppejan  r     whiteson  s          neuroevolutionary reinforcement learning generalized control simulated helicopters  evolutionary intelligence            
lee  j  y     lee  j  j          multiple designs fuzzy controllers car parking using
evolutionary algorithm  pp      no  may 
luenberger  d  g          investment science  oxford university press 
mannor  s          reinforcement learning average reward zero sum games  shawetaylor  j     singer  y   eds    colt  vol       lecture notes computer science 
pp        springer 
   

fisafe exploration state action spaces reinforcement learning

martin h  j     de lope  j          exa  effective algorithm continuous actions
reinforcement learning problems  industrial electronics        iecon       th
annual conference ieee  pp            
martn h   j  a     lope  j          learning autonomous helicopter flight evolutionary reinforcement learning    th international conference computer
aided systems theory  eurocast   pp       
mihatsch  o     neuneier  r          risk sensitive reinforcement learning  machine learning                   
moldovan  t  m     abbeel  p          safe exploration markov decision processes 
corr  abs           
narendra  k  s     thathachar  m  a  l          learning automata   survey  ieee
transactions systems man cybernetics  smc               
narendra  k  s     thathachar  m  a  l          learning automata  introduction 
prentice hall  inc   upper saddle river  nj  usa 
ng  a  y   kim  h  j   jordan  m  i     sastry  s          autonomous helicopter flight
via reinforcement learning  thrun  s   saul  l  k     scholkopf  b   eds    nips 
mit press 
peters  j   tedrake  r   roy  n     morimoto  j          robot learning  sammut  c  
  webb  g  i   eds    encyclopedia machine learning  pp          springer 
poli  r     cagnoni  s          genetic programming user driven selection  experiments evolution algorithms image enhancement  genetic programming
      proceedings second annual conference  pp          morgan kaufmann 
salkham  a   cunningham  r   garg  a     cahill  v          collaborative reinforcement learning approach urban traffic control optimization  web intelligence
intelligent agent technology        wi iat     ieee wic acm international
conference on  vol     pp         
santamara  j  c   sutton  r  s     ram  a          experiments reinforcement
learning problems continuous state action spaces  adaptive behavior    
       
sharma  m   holmes  m   santamaria  j   irani  a   isbell  c     ram  a          transfer
learning real time strategy games using hybrid cbr rl  proceedings
twentieth international joint conference artificial intelligence 
siebel  n  t     sommer  g          evolutionary reinforcement learning artificial neural
networks  international journal hybrid intelligent systems            
smart  w  d     kaelbling  l  p          practical reinforcement learning continuous
spaces  artificial intelligence  pp          morgan kaufmann 
smart  w  d     kaelbling  l  p          effective reinforcement learning mobile robots 
icra  pp            ieee 
sutton  r  s     barto  a  g          reinforcement learning  introduction  mit
press 
   

figarca   fernandez

tang  j   singh  a   goehausen  n     abbeel  p          parameterized maneuver learning autonomous helicopter flight  international conference robotics
automation  icra  
taylor  m  e   kulis  b     sha  f          metric learning reinforcement learning agents 
proceedings international conference autonomous agents multiagent
systems  aamas  
van hasselt  h     wiering  m  a          reinforcement learning continuous action
spaces  approximate dynamic programming reinforcement learning       
adprl       ieee international symposium on  pp         
wyatt  j          exploration inference learning reinforcement  university
edinburgh 
yao  x          evolving artificial neural networks  pieee  proceedings ieee     
         

   


