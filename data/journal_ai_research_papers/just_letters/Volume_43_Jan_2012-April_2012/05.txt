journal of artificial intelligence research                  

submitted        published      

learning to win by reading manuals
in a monte carlo framework
s r k  branavan

branavan csail mit edu

computer science and artificial intelligence laboratory
massachusetts institute of technology

david silver

d silver cs ucl ac uk

department of computer science
university college london

regina barzilay

regina csail mit edu

computer science and artificial intelligence laboratory
massachusetts institute of technology

abstract
domain knowledge is crucial for effective performance in autonomous control systems 
typically  human effort is required to encode this knowledge into a control algorithm  in
this paper  we present an approach to language grounding which automatically interprets
text in the context of a complex control application  such as a game  and uses domain
knowledge extracted from the text to improve control performance  both text analysis and
control strategies are learned jointly using only a feedback signal inherent to the application 
to effectively leverage textual information  our method automatically extracts the text
segment most relevant to the current game state  and labels it with a task centric predicate
structure  this labeled text is then used to bias an action selection policy for the game 
guiding it towards promising regions of the action space  we encode our model for text
analysis and game playing in a multi layer neural network  representing linguistic decisions
via latent variables in the hidden layers  and game action quality via the output layer 
operating within the monte carlo search framework  we estimate model parameters using
feedback from simulated games  we apply our approach to the complex strategy game
civilization ii using the official game manual as the text guide  our results show that a
linguistically informed game playing agent significantly outperforms its language unaware
counterpart  yielding a     absolute improvement and winning over     of games when
playing against the built in ai of civilization 

   introduction
in this paper  we study the task of grounding document content in control applications
such as computer games  in these applications  an agent attempts to optimize a utility
function  e g   game score  by learning to select situation appropriate actions  in complex
domains  finding a winning strategy is challenging even for humans  therefore  human
players typically rely on manuals and guides that describe promising tactics and provide
general advice about the underlying task  surprisingly  such textual information has never
been utilized in control algorithms despite its potential to greatly improve performance 
our goal  therefore  is to develop methods that can achieve this in an automatic fashion 
c
    
ai access foundation  all rights reserved 

fibranavan  silver    barzilay

the natural resources available where a population settles aects its ability to produce food
and goods  cities built on or near water sources can irrigate to increase their crop yields  and
cities near mineral resources can mine for raw materials  build your city on a plains or grassland
square with a river running through it if possible 

figure    an excerpt from the user manual of the game civilization ii 
we explore this question in the context of strategy games  a challenging class of large scale
adversarial planning problems 
consider for instance the text shown in figure    this is an excerpt from the user
manual of the game civilization ii   this text describes game locations where the action
build city can be effectively applied  a stochastic player that does not have access to this
text would have to gain this knowledge the hard way  it would repeatedly attempt this
action in a myriad of states  thereby learning the characterization of promising state action
pairs based on observed game outcomes  in games with large state spaces  long planning
horizons  and high branching factors  this approach can be prohibitively slow and ineffective 
an algorithm with access to the text  however  could learn correlations between words in
the text and game attributes  e g   the word river and places with rivers in the game 
thus leveraging strategies described in text to select better actions 
to improve the performance of control applications using domain knowledge automatically extracted from text  we need to address the following challenges 
 grounding text in the state action space of a control application text
guides provide a wealth of information about effective control strategies  including
situation specific advice as well as general background knowledge  to benefit from
this information  an algorithm has to learn the mapping between the text of the
guide  and the states and actions of the control application  this mapping allows
the algorithm to find state specific advice by matching state attributes to their verbal
descriptions  furthermore  once a relevant sentence is found  the mapping biases the
algorithm to select the action proposed in the guide document  while this mapping
can be modeled at the word level  ideally we would also use information encoded in
the structure of the sentence  such as the predicate argument structure  for instance 
the algorithm can explicitly identify predicates and state attribute descriptions  and
map them directly to structures inherent in the control application 
 annotation free parameter estimation while the above text analysis tasks relate
to well known methods in information extraction  prior work has primarily focused on
supervised methods  in our setup  text analysis is state dependent  therefore annotations need to be representative of the entire state space  given an enormous state
space that continually changes as the game progresses  collecting such annotations is
impractical  instead  we propose to learn text analysis based on a feedback signal
inherent to the control application  e g   the game score  this feedback is computed
automatically at each step of the game  thereby allowing the algorithm to continuously
adapt to the local  observed game context 
   http   en wikipedia org wiki civilization ii

   

filearning to win by reading manuals in a monte carlo framework

 effective integration of extracted text information into the control application most text guides do not provide complete  step by step advice for all situations that a player may encounter  even when such advice is available  the learned
mapping may be noisy  resulting in suboptimal choices  therefore  we need to design a method which can achieve effective control in the absence of textual advice 
while robustly integrating automatically extracted information when available  we
address this challenge by incorporating language analysis into monte carlo search  a
state of the art framework for playing complex games  traditionally this framework
operates only over state and action features  by extending monte carlo search to
include textual features  we integrate these two sources of information in a principled
fashion 
    summary of the approach
we address the above challenges in a unified framework based on markov decision processes
 mdp   a formulation commonly used for game playing algorithms  this setup consists of
a game in a stochastic environment  where the goal of the player is to maximize a given
utility function r s  at state s  the players behavior is determined by an action value
function q s  a  that assesses the goodness of action a at state s based on the attributes of
s and a 
to incorporate linguistic information into the mdp formulation  we expand the action
value function to include linguistic features  while state and action features are known
at each point of computation  relevant words and their semantic roles are not observed 
therefore  we model text relevance as a hidden variable  similarly  we use hidden variables
to discriminate the words that describe actions and those that describe state attributes from
the rest of the sentence  to incorporate these hidden variables in our action value function 
we model q s  a  as a non linear function approximation using a multi layer neural network 
despite the added complexity  all the parameters of our non linear model can be effectively learned in the monte carlo search framework  in monte carlo search  the actionvalue function is estimated by playing multiple simulated games starting at the current
game state  we use the observed reward from these simulations to update the parameters
of our neural network via backpropagation  this focuses learning on the current game state 
allowing our method to learn language analysis and game play appropriate to the observed
game context 
    evaluation
we test our method on the strategy game civilization ii  a notoriously challenging game
with an immense action space   as a source of knowledge for guiding our model  we use the
official game manual  as a baseline  we employ a similar monte carlo search based player
which does not have access to textual information  we demonstrate that the linguisticallyinformed player significantly outperforms the baseline in terms of the number of games
won  moreover  we show that modeling the deeper linguistic structure of sentences further improves performance  in full length games  our algorithm yields a     improve   civilization ii was    in igns      list of top video games of all time 
 http   top    ign com      ign top game   html 

   

fibranavan  silver    barzilay

ment over a language unaware baseline and wins over     of games against the built in 
hand crafted ai of civilization ii  a video of our method playing the game is available at
http   groups csail mit edu rbg code civ video  the code and data for this work  along
with a complete experimental setup and a preconfigured environment in a virtual machine
are available at http   groups csail mit edu rbg code civ 
    roadmap
in section    we provide intuition about the benefits of integrating textual information into
learning algorithms for control  section   describes prior work on language grounding  emphasizing the unique challenges and opportunities of our setup  this section also positions
our work in a large body of research on monte carlo based players  section   presents
background on monte carlo search as applied to game playing  in section   we present a
multi layer neural network formulation for the action value function that combines information from the text and the control application  next  we present a monte carlo method
for estimating the parameters of this non linear function  sections   and   focus on the application of our algorithm to the game civilization ii  in section   we compare our method
against a range of competitive game playing baselines  and empirically analyze the properties of the algorithm  finally  in section   we discuss the implications of this research  and
conclude 

   learning game play from text
in this section  we provide an intuitive explanation of how textual information can help improve action selection in a complex game  for clarity  we first discuss the benefits of textual
information in the supervised scenario  thereby decoupling questions concerning modeling
and representation from those related to parameter estimation  assume that every state s
is represented by a set of n features  s    s            sn    given a state s  our goal is to select the
best possible action aj from a fixed set a  we can model this task as multiclass classification  where each choice aj is represented by a feature vector   s    aj     s    aj             sn   aj    
here   si   aj    i      n  represents a feature created by taking the cartesian product between  s    s            sn   and aj   to learn this classifier effectively  we need a training set that
sufficiently covers the possible combinations of state features and actions  however  in domains with complex state spaces and a large number of possible actions  many instances of
state action feature values will be unobserved in training 
now we show how the generalization power of the classifier can be improved using textual information  assume that each training example  in addition to a state action pair 
contains a sentence that may describe the action to be taken given the state attributes  intuitively  we want to enrich our basic classifier with features that capture the correspondence
between states and actions  and words that describe them  given a sentence w composed
of word types w    w            wm   these features can be of the form  si   wk   and  aj   wk   for
every i      n   k      m  and aj  a  assuming that an action is described using similar words throughout the guide  we expect that a text enriched classifier would be able to
learn this correspondence via the features  aj   wk    a similar intuition holds for learning
the correspondence between state attributes and their descriptions represented by features
 si   wk    through these features  the classifier can connect state s and action aj based
   

filearning to win by reading manuals in a monte carlo framework

on the evidence provided in the guiding sentence and their occurrences in other contexts
throughout the training data  a text free classifier may not support such an association if
the action does not appear in a similar state context in a training set 
the benefits of textual information extend to models that are trained using control
feedback rather than supervised data  in this training scenario  the algorithm assesses the
goodness of a given state action combination by simulating a limited number of game turns
after the action is taken and observing the control feedback provided by the underlying
application  the algorithm has a built in mechanism  see section    that employs the
observed feedback to learn feature weights  and intelligently samples the space in search for
promising state action pairs  when the algorithm has access to a collection of sentences  a
similar feedback based mechanism can be used to find sentences that match a given stateaction pair  section       through the state  and action description features  si   wk   and
 aj   wk    the algorithm jointly learns to identify relevant sentences and to map actions and
states to their descriptions  note that while we have used classification as the basis of
discussion in this section  in reality our methods will learn a regression function 

   related work
in this section  we first discuss prior work in the field of grounded language acquisition 
subsequently we look are two areas specific to our application domain  i e   natural language
analysis in the context of games  and monte carlo search applied to game playing 
    grounded language acquisition
our work fits into the broad area of research on grounded language acquisition where
the goal is to learn linguistic analysis from a non linguistic situated context  oates       
barnard   forsyth        siskind        roy   pentland        yu   ballard        chen
  mooney        zettlemoyer   collins        liang  jordan    klein        branavan 
chen  zettlemoyer    barzilay        branavan  zettlemoyer    barzilay        vogel   jurafsky        clarke  goldwasser  chang    roth        tellex  kollar  dickerson  walter 
banerjee  teller    roy        chen   mooney        liang  jordan    klein        goldwasser  reichart  clarke    roth         the appeal of this formulation lies in reducing
the need for manual annotations  as the non linguistic signals can provide a powerful  albeit
noisy  source of supervision for learning  in a traditional grounding setup it is assumed that
the non linguistic signals are parallel in content to the input text  motivating a machine
translation view of the grounding task  an alternative approach models grounding in the
control framework where the learner actively acquires feedback from the non linguistic environment and uses it to drive language interpretation  below we summarize both approaches 
emphasizing the similarity and differences with our work 
      learning grounding from parallel data
in many applications  linguistic content is tightly linked to perceptual observations  providing a rich source of information for learning language grounding  examples of such parallel
data include images with captions  barnard   forsyth         robocup game events paired
with a text commentary  chen   mooney         and sequences of robot motor actions de   

fibranavan  silver    barzilay

scribed in natural language  tellex et al          the large diversity in the properties of such
parallel data has resulted in the development of algorithms tailored for specific grounding
contexts  instead of an application independent grounding approach  nevertheless  existing
grounding approaches can be characterized along several dimensions that illuminate the
connection between these algorithms 
 representation of non linguistic input the first step in grounding words in
perceptual data is to discretize the non linguistic signal  e g   an image  into a representation that facilitates alignment  for instance  barnard and forsyth        segment images into regions that are subsequently mapped to words  other approaches
intertwine alignment and segmentation into a single step  roy   pentland         as
the two tasks are clearly interrelated  in our application  segmentation is not required
as the state action representation is by nature discrete 
many approaches move beyond discretization  aiming to induce rich hierarchical structures over the non linguistic input  fleischman   roy        chen   mooney       
       for instance  fleischman and roy        parse action sequences using a
context free grammar which is subsequently mapped into semantic frames  chen
and mooney        represent action sequences using first order logic  in contrast 
our algorithm capitalizes on the structure readily available in our data  state action
transitions  while inducing a richer structure on the state action space may benefit mapping  it is a difficult problem in its own right from the field of hierarchical
planning  barto   mahadevan        
 representation of linguistic input early grounding approaches used the bagof words approach to represent input documents  yu   ballard        barnard  
forsyth        fleischman   roy         more recent methods have relied on a richer
representation of linguistic data  such as syntactic trees  chen   mooney        and
semantic templates  tellex et al          our method incorporates linguistic information at multiple levels  using a feature based representation that encodes both words
as well as syntactic information extracted from dependency trees  as shown by our
results  richer linguistic representations can significantly improve model performance 
 alignment another common feature of existing grounding models is that the training
procedure crucially depends on how well words are aligned to non linguistic structures 
for this reason  some models assume that alignment is provided as part of the training
data  fleischman   roy        tellex et al          in other grounding algorithms  the
alignment is induced as part of the training procedure  examples of such approaches
are the methods of barnard and forsyth         and liang et al          both of these
models jointly generate the text and attributes of the grounding context  treating
alignment as an unobserved variable 
in contrast  we do not explicitly model alignment in our model due to the lack of
parallel data  instead  we aim to extract relevant information from text and infuse it
into a control application 
   

filearning to win by reading manuals in a monte carlo framework

      learning grounding from control feedback
more recent work has moved away from the reliance on parallel corpora  using control feedback as the primary source of supervision  the assumption behind this setup is that when
textual information is used to drive a control application  the applications performance will
correlate with the quality of language analysis  it is also assumed that the performance measurement can be obtained automatically  this setup is conducive to reinforcement learning
approaches which can estimate model parameters from the feedback signal  even it is noisy
and delayed 
one line of prior work has focused on the task of mapping textual instructions into a
policy for the control application  assuming that text fully specifies all the actions to be executed in the environment  for example  in our previous work  branavan et al               
this approach was applied to the task of translating instructions from a computer manual
to executable gui actions  vogel and jurafsky        demonstrate that this grounding
framework can effectively map navigational directions to the corresponding path in a map 
a second line of prior work has focused on full semantic parsing  converting a given text
into a formal meaning representation such as first order logic  clarke et al          these
methods have been applied to domains where the correctness of the output can be accurately evaluated based on control feedback  for example  where the output is a database
query which when executed provides a clean  oracle feedback signal for learning  this line
of work also assumes that the text fully specifies the required output 
while our method is also driven by control feedback  our language interpretation task
itself is fundamentally different  we assume that the given text document provides highlevel advice without directly describing the correct actions for every potential game state 
furthermore  the textual advice does not necessarily translate to a single strategy  in fact 
the text may describe several strategies  each contingent on specific game states  for this
reason  the strategy text cannot simply be interpreted directly into a policy  therefore  our
goal is to bias a learned policy using information extracted from text  to this end  we do
not aim to achieve a complete semantic interpretation  but rather use a partial text analysis
to compute features relevant for the control application 
    language analysis and games
even though games can provide a rich domain for situated text analysis  there have only
been a few prior attempts at leveraging this opportunity  gorniak   roy        eisenstein 
clarke  goldwasser    roth        
eisenstein et al         aim to automatically extract information from a collection of
documents to help identify the rules of a game  this information  represented as predicate logic formulae  is estimated in an unsupervised fashion via a generative model  the
extracted formulae  along with observed traces of game play are subsequently fed to an inductive logic program  which attempts to reconstruct the rules of the game  while at the
high level  our goal is similar  i e   to extract information from text useful for an external
task  there are several key differences  firstly  while eisenstein et al         analyze the
text and the game as two disjoint steps  we model both tasks in an integrated fashion  this
allows our model to learn a text analysis pertinent to game play  while at the same time
using text to guide game play  secondly  our method learns both text analysis and game
   

fibranavan  silver    barzilay

play from a feedback signal inherent to the game  avoiding the need for pre compiled game
traces  this enables our method to operate effectively in complex games where collecting a
sufficiently representative set of game traces can be impractical 
gorniak and roy        develop a machine controlled game character which responds
to spoken natural language commands  given traces of game actions manually annotated
with transcribed speech  their method learns a structured representation of the text and
aligned action sequences  this learned model is then used to interpret spoken instructions
by grounding them in the actions of a human player and the current game state  while the
method itself does not learn to play the game  it enables human control of an additional game
character via speech  in contrast to gorniak and roy         we aim to develop algorithms
to fully and autonomously control all actions of one player in the game  furthermore  our
method operates on the games user manual rather than on human provided  contextually
relevant instructions  this requires our model to identify if the text contains information
useful in the current game state  in addition to mapping the text to productive actions 
finally  our method learns from game feedback collected via active interaction without
relying on manual annotations  this allows us to effectively operate on complex games
where collecting traditional labeled traces would be prohibitively expensive 
    monte carlo search for game ai
monte carlo search  mcs  is a state of the art framework that has been very successfully
applied  in prior work  to playing complex games such as go  poker  scrabble  and real time
strategy games  gelly  wang  munos    teytaud        tesauro   galperin        billings 
castillo  schaeffer    szafron        sheppard        schafer        sturtevant        balla
  fern         this framework operates by playing simulated games to estimate the goodness or value of different candidate actions  when the games state and action spaces are
complex  the number of simulations needed for effective play become prohibitively large 
previous application of mcs have addressed this issue using two orthogonal techniques     
they leverage domain knowledge to either guide or prune action selection      they estimate
the value of untried actions based on the observed outcomes of simulated games  this estimate is then used to bias action selection  our mcs based algorithm for games relies on
both of the above techniques  below we describe the differences between our application of
these techniques and prior work 
      leveraging domain knowledge
domain knowledge has been shown to be critically important to achieving good performance from mcs in complex games  in prior work this has been achieved by manually
encoding relevant domain knowledge into the game playing algorithm  for example  via
manually specified heuristics for action selection  billings et al         gelly et al         
hand crafted features  tesauro   galperin         and value functions encoding expert
knowledge  sturtevant         in contrast to such approaches  our goal is to automatically extract and use domain knowledge from relevant natural language documents  thus
bypassing the need for manual specification  our method learns both text interpretation
and game action selection based on the outcomes of simulated games in mcs  this allows it
to identify and leverage textual domain knowledge relevant to the observed game context 
   

filearning to win by reading manuals in a monte carlo framework

action selection
according to
policy function

stochastic state
transition according
to distribution

figure    markov decision process  actions are selected according to policy function  s  a 
given the current state s  the execution of the selected action ai  e g   a     causes
the mdp to transition to a new state s  according to the stochastic state transition
distribution t  s    s  a  

      estimating the value of untried actions
previous approaches to estimating the value of untried actions have relied on two techniques 
the first  upper confidence bounds for tree  uct  is a heuristic used in concert with the
monte carlo tree search variant of mcs  it augments an actions value with an exploration
bonus for rarely visited state action pairs  resulting in better action selection and better
overall game performance  gelly et al         sturtevant        balla   fern         the
second technique is to learn a linear function approximation of action values for the current
state s  based on game feedback  tesauro   galperin        silver  sutton    muller        
even though our method follows the latter approach  we model action value q s  a  via a
non linear function approximation  given the complexity of our application domain  this
non linear approximation generalizes better than a linear one  and as shown by our results
significantly improves performance  more importantly  the non linear model enables our
method to represent text analysis as latent variables  allowing it to use textual information
to estimate the value of untried actions 

   monte carlo search
our task is to leverage textual information to help us win a turn based strategy game against
a given opponent  in this section  we first describe the monte carlo search framework
within which our method operates  the details of our linguistically informed monte carlo
search algorithm are given in section   
    game representation
formally  we represent the given turn based stochastic game as a markov decision process
 mdp   this mdp is defined by the   tuple hs  a  t  ri  where
 state space  s  is the set of all possible states  each state s  s represents a complete
configuration of the game in between player turns 
 action space  a  is the set of all possible actions  in a turn based strategy game  a
player controls multiple game units at each turn  thus  each action a  a represents
the joint assignment of all unit actions executed by the current player during the turn 
   

fibranavan  silver    barzilay

 transition distribution  t  s    s  a   is the probability that executing action a in state
s will result in state s  at the next game turn  this distribution encodes the way the
game state changes due to both the game rules  and the opposing players actions 
for this reason  t  s    s  a  is stochastic  as shown in figure    executing the same
action a at a given state s can result in different outcomes s   
 reward function  r s   r  is the immediate reward received when transitioning to
state s  the value of the reward correlates with the goodness of actions executed up
to now  with higher reward indicating better actions 
all the above aspects of the mdp representation of the game  i e   s  a  t    and r  
 are defined implicitly by the game rules  at each step of the game  the game playing
agent can observe the current game state s  and has to select the best possible action a 
when the agent executes action a  the game state changes according to the state transition
distribution  while t  s    s  a  is not known a priori  state transitions can be sampled from
this distribution by invoking the game code as a black box simulator  i e   by playing the
game  after each action  the agent receives a reward according to the reward function r s  
in a game playing setup  the value of this reward is an indication of the chances of winning
the game from state s  crucially  the reward signal may be delayed  i e   r s  may have a
non zero value only for game ending states such as a win  a loss  or a tie 
the game playing agent selects actions according to a stochastic policy  s  a   which
specifies the probability of selecting action a in state s  the expected total reward after
executing action a in state s  and then following policy  is termed the action value function
q  s  a   our goal is to find the optimal policy    s  a  which maximizes the expected
total reward  i e   maximizes the chances of winning the game  if the optimal action value

function q  s  a  is known  the optimal game playing behavior would be to select the action

a with the highest q  s  a   while it may be computationally hard to find an optimal policy

   s  a  or q  s  a   many well studied algorithms are available for estimating an effective
approximation  sutton   barto        
    monte carlo framework for computer games
the monte carlo search algorithm  shown in figure    is a simulation based search paradigm
for dynamically estimating the action values q  s  a  for a given state st  see algorithm  
for pseudo code   this estimate is based on the rewards observed during multiple roll outs 
each of which is a simulated game starting from state st    specifically  in each roll out 
the algorithm starts at state st   and repeatedly selects and executes actions according to a
simulation policy  s  a   sampling state transitions from t  s    s  a   on game completion
at time    the final reward r s   is measured  and the action value function is updated
accordingly   as in monte carlo control  sutton   barto         the updated action value
   monte carlo search assumes that it is possible to play simulated games  these simulations may be
played against a heuristic ai player  in our experiments  the built in ai of the game is used as the
opponent 
   in general  roll outs are run until game completion  if simulations are expensive  as is the case in our
domain  roll outs can be truncated after a fixed number of steps  this however depends on the availability
of an approximate reward signal at the truncation point  in our experiments  we use the built in score
of the game as the reward  this reward is noisy  but available at every stage of the game 

   

filearning to win by reading manuals in a monte carlo framework

game

copy game
state to
simulator

apply action with
best simulation
outcome to game
single
simulation
rollout

update rollout
policy from
game feedback
after each rollout

simulation for

simulation for

figure    overview of monte carlo search algorithm  for each game state st   an independent set of simulated games or roll outs are done to find the best possible game
action at   each roll out starts at state st   with actions selected according to a
simulation policy  s  a   this policy is learned from the roll outs themselves 
with the roll outs improving the policy  which in turn improves roll out action selection  the process is repeated for every actual game state  with the simulation
policy being relearned from scratch each time 

function q  s  a  is used to define an improved simulation policy  thereby directing subsequent roll outs towards higher scoring regions of the game state space  after a fixed number
of roll outs have been performed  the action with the highest average final reward in the
simulations is selected and played in the actual game state st   this process is repeated
for each state encountered during the actual game  with the action value function being
relearned from scratch for each new game state   the simulation policy usually selects
actions to maximize the action value function  however  sometimes other valid actions are
also randomly explored in case they are more valuable than predicted by the current es   while it is conceivable that sharing the action value function across the roll outs of different game states
would be beneficial  this was empirically not the case in our experiments  one possible reason is that
in our domain  the game dynamics change radically at many points during the game  e g   when a
new technology becomes available  when such a change occurs  it may actually be detrimental to play
according to the action value function from the previous game step  note however  that the action value
function is indeed shared across the roll outs for a single game state st   with parameters updated by
successive roll outs  this is how the learned model helps improve roll out action selection  and thereby
improves game play  the setup of relearning from scratch for each game state has been shown to be
beneficial even in stationary environments  sutton  koop    silver        

   

fibranavan  silver    barzilay

timate of q  s  a   as the accuracy of q  s  a  improves  the quality of action selection
improves and vice versa  in a cycle of continual improvement  sutton   barto        
the success of monte carlo search depends on its ability to make a fast  local estimate
of the action value function from roll outs collected via simulated play  however in games
with large branching factors  it may not be feasible to collect sufficient roll outs  especially
when game simulation is computationally expensive  thus it is crucial that the learned
action value function generalizes well from a small number of roll outs  i e   observed
states  actions and rewards  one way to achieve this is to model the action value function
as a linear combination of state and action attributes 
q  s  a    w
   f  s  a  
here f  s  a   rn is a real valued feature function  and w
  is a weight vector  prior work has
shown such linear value function approximations to be effective in the monte carlo search
framework  silver et al         
note that learning the action value function q s  a  in monte carlo search is related
to reinforcement learning  rl   sutton   barto         in fact  in our approach  we use
standard gradient descent updates from rl to estimate the parameters of q s  a   there is 
however  one crucial difference between these two techniques  in general  the goal in rl is
to find a q s  a  applicable to any state the agent may observe during its existence  in the
monte carlo search framework  the aim is to learn a q s  a  specialized to the current state
s  in essence  q s  a  is relearned for every observed state in the actual game  using the
states  actions and feedback from simulations  while such relearning may seem suboptimal 
it has two distinct advantages  first  since q s  a  only needs to model the current state  it
can be representationally much simpler than a global action value function  second  due to
this simpler representation  it can be learned from fewer observations than a global actionvalue function  sutton et al          both of these properties are important when the state
space is extremely large  as is the case in our domain 

   adding linguistic knowledge to the monte carlo framework
the goal of our work is to improve the performance of the monte carlo search framework
described above  using information automatically extracted from text  in this section  we
describe how we achieve this in terms of model structure and parameter estimation 
    model structure
to achieve our aim of leveraging textual information to improve game play  our method
needs to perform three tasks      identify sentences relevant to the current game state     
label sentences with a predicate structure  and     predict good game actions by combining
game features with text features extracted via the language analysis steps  we first describe
how each of these tasks can be modeled separately before showing how we integrate them
into a single coherent model 
   

filearning to win by reading manuals in a monte carlo framework

procedure playgame   
initialize game state to fixed starting state
s   s 
for t           t do
run n simulated games
for i           n do
 ai   ri    simulategame  st  
end
compute average observed utility for each action
  x
at  arg max
ri
na
a
i ai  a

execute selected action in game
st    t  s    st   at  
end

procedure simulategame  st  
for u   t        do
compute q function approximation
q  su   a    w
   f  su   a 
sample action from action value function in  greedy fashion 

 uniform  a  a 
with probability 
au   su   a   

 arg max q  su   a  otherwise
a

execute selected action in game 
su    t  s    su   au  
if game is won or lost
break
end
update parameters w
  of q  st   a 
return action and observed utility 
return at   r s  
algorithm    the general monte carlo algorithm 
   

fibranavan  silver    barzilay

      modeling sentence relevance
as discussed in section    only a small fraction of a strategy document is likely to provide
guidance relevant to the current game context  therefore  to effectively use information
from a given document d  we first need to identify the sentence yi that is most relevant to
the current game state s and action a   we model this decision as a log linear distribution 
defining the probability of yi being the relevant sentence as 
 

p y   yi  s  a  d   e u yi  s a d   

   

  i   s  a  d   rn is a feature function  and  u are the parameters we need to estimate 
here  y
 
the function   
encodes features that combine the attributes of sentence yi with the
attributes of the game state and action  these features allow the model to learn correlations
between game attributes and the attributes of relevant sentences 
      modeling predicate structure
when using text to guide action selection  in addition to using word level correspondences 
we would also like to leverage information encoded in the structure of the sentence  for
example  verbs in a sentence might be more likely to describe suggested game actions 
we aim to access this information by inducing a task centric predicate structure on the
sentences  that is  we label the words of a sentence as either action description  statedescription or background  given sentence y and its precomputed dependency parse q  we
model the word by word labeling decision in a log linear fashion  i e   the distribution over
the predicate labeling z of sentence y is given by 
p z  y  q    p  e  y  q 
y
 
p ej  j  y  q  

   

j
 

p ej  j  y  q   e v ej  j y q   
  j   j  y  q   rn  
where ej is the predicate label of the j th word  the feature function  e
in addition to encoding word type and part of speech tag  also includes dependency parse
information for each word  these features allow the predicate labeling decision to condition
on the syntactic structure of the sentence 
      modeling the action value function
once the relevant sentence has been identified and labeled with a predicate structure  our
algorithm needs to use this information along with the attributes of the current game state s
to select the best possible game action a  to this end  we redefine the action value function
q s  a  as a weighted linear combination of features of the game and the text information 
q s    a      w
   f  s  a  yi   zi   

   

   we use the approximation of selecting the single most relevant sentence as an alternative to combining
the features of all sentences in the text  weighted by their relevance probability p y   yi  s  a  d   this
setup is computationally more expensive than the one used here 

   

filearning to win by reading manuals in a monte carlo framework

input layer 

deterministic feature
layer 

output layer

hidden layer encoding
sentence relevance
hidden layer encoding
predicate labeling

figure    the structure of our neural network model  each rectangle represents a collection
of units in a layer  and the shaded trapezoids show the connections between layers 
a fixed  real valued feature function  x s  a  d  transforms the game state s  action
a  and strategy document d into the input vector  x  the second layer contains
two disjoint sets of hidden units  y and  z  where  y encodes the sentence relevance
decisions  and  z the predicate labeling  these are softmax layers  where only one
unit is active at any time  the units of the third layer f  s  a  yi   zi   are a set of
fixed real valued feature functions on s  a  d and the active units yi and zi of  y
and  z respectively 

here s    hs  di  a    ha  yi   zi i  w
  is the weight vector  and f  s  a  yi   zi    rn is a feature
function over the state s  action a  relevant sentence yi   and its predicate labeling zi   this
structure of the action value function allows it to explicitly learn the correlations between
textual information  and game states and actions  the action a that maximizes q s  a  is
then selected as the best action for state s   
a   arg max q s  a  
a

      complete joint model
the two text analysis models  and the action value function described above form the three
primary components of our text aware game playing algorithm  we construct a single
principled model from these components by representing each of them via different layers
of the multi layer neural network shown in figure    essentially  the text analysis decisions
are modeled as latent variables by the second  hidden layer of the network  while the final
output layer models the action value function 
   note that we select action a based on q s  a   which depends on the relevant sentence yi   this sentence
itself is selected conditioned on action a  this may look like a cyclic dependency between actions
and sentence relevance  however  that is not the case since q s  a   and therefore sentence relevance
p y s  a  d   is computed for every candidate action a  a  the actual game action a is then selected
from this estimate of q s  a  

   

fibranavan  silver    barzilay

the input layer  x of our neural network encodes the inputs to the model  i e   the
current state s  candidate action a  and document d  the second layer consists of two
disjoint sets of hidden units  y and  z  where each set operates as a stochastic   of n softmax
selection layer  bridle         the activation function for units in this layer is the standard
softmax function 
 x
p yi      x    e ui  x
e uk  x  
k

ith

where yi is the
hidden unit of  y    ui is the weight vector corresponding to yi   and k is
the number of units in the layer  given that this activation function is mathematically
equivalent to a log linear distribution  the layers  y and  z operate like log linear models 
node activation in such a softmax layer simulates sampling from the log linear distribution 
we use layer  y to replicate the log linear model for sentence relevance from equation     
with each node yi representing a single sentence  similarly  each unit zi in layer  z represents
a complete predicate labeling of a sentence  as in equation      
the third feature layer f  of the neural network is deterministically computed given the
active units yi and zi of the softmax layers  and the values of the input layer  each unit in
this layer corresponds to a fixed feature function fk  s  a  yi   zi    r  finally the output layer
encodes the action value function q s  a  as a weighted linear combination of the units of
the feature layer  thereby replicating equation     and completing the joint model 
as an example of the kind of correlations learned by our model  consider figure   
here  a relevant sentence has already been selected for the given game state  the predicate
labeling of this sentence has identified the words irrigate and settler as describing the
action to take  when game roll outs return higher rewards for the irrigate action of the
settler unit  our model can learn an association between this action and the words that
describe it  similarly  it can learn the association between state description words and the
feature values of the current game state  e g   the word city and the binary feature nearcity  this allows our method to leverage the automatically extracted textual information
to improve game play 
    parameter estimation
learning in our method is performed in an online fashion  at each game state st   the
algorithm performs a simulated game roll out  observes the outcome of the simulation  and
updates the parameters  u   v and w
  of the action value function q st   at    as shown in
figure    these three steps are repeated a fixed number of times at each actual game state 
the information from these roll outs is then used to select the actual game action  the
algorithm relearns all the parameters of the action value function for every new game state
st   this specializes the action value function to the subgame starting from st   learning
a specialized q st   at   for each game state is common and useful in games with complex
state spaces and dynamics  where learning a single global function approximation can be
particularly difficult  sutton et al          a consequence of this function specialization
is the need for online learning  since we cannot predict which games states will be seen
   our intention is to incorporate  into action value function  information from only the most relevant
sentence  therefore  in practice  we only perform a predicate labeling of the sentence selected by the
relevance component of the model 

   

filearning to win by reading manuals in a monte carlo framework

settlers unit  candidate action   
plains

features 
action   irrigate and action word    irrigate 
action   irrigate and state word    land 
action   irrigate and terrain   plains
action   irrigate and unit type   settler
state word    city  and near city   true

city

settlers unit  candidate action   

settler unit

relevant text   use settlers to irrigate land near your city 
predicted action words 

 irrigate    settler 

predicted state words 

 land    near    city 

irrigate

features 
action   build city
action   build city
action   build city
action   build city
state word    city 

and
and
and
and
and

build city

action word    irrigate 
state word    land 
terrain   plains
unit type   settler
near city   true

figure    an example of text and game attributes  and resulting candidate action features 
on the left is a portion of a game state with arrows indicating game attributes 
also on the left is a sentence relevant to the game state along with action and
state words identified by predicate labeling  on the right are two candidate
actions for the settler unit along with the corresponding features  as mentioned
in the relevant sentence  irrigate is the better of the two actions  executing it
will lead to future higher game scores  this feedback and the features shown
above allow our model to learn effective mappings  such as between the actionword irrigate and the action irrigate  and between state word city and game
attribute near city 

during testing  function specialization for those states cannot be done a priori  ruling out
the traditional training test separation 
since our model is a non linear approximation of the underlying action value function of
the game  we learn model parameters by applying non linear regression to the observed final
utilities from the simulated roll outs  specifically  we adjust the parameters by stochastic
gradient descent  to minimize the mean squared error between the action value q s  a  and
the final utility r s   for each observed game state s and action a  the resulting update
to model parameters  is of the form 

      r s    q s  a   
 
    r s    q s  a    q s  a    
where  is a learning rate parameter  this minimization is performed via standard error
backpropagation  bryson   ho        rumelhart  hinton    williams         resulting in
the following online parameter updates 
w
   w
    w  q  r s    f  s  a  yi   zj   
 ui   ui   u  q  r s    q  x     p yi     
 vi   vi   v  q  r s    q  x     p zi     
   

fibranavan  silver    barzilay

here w is the learning rate  q   q s  a   and w 
   ui and  vi are the parameters of the
final layer  the sentence relevance layer and the predicate labeling layer respectively  the
derivations of these update equations are given in appendix a

   applying the model
the game we test our model on  civilization ii  is a multi player strategy game set either on
earth or on a randomly generated world  each player acts as the ruler of one civilization 
and starts with a few game units  i e   two settlers  two workers and one explorer  the
goal is to expand your civilization by developing new technologies  building cities and new
units  and to win the game by either controlling the entire world  or successfully sending a
spaceship to another world  the map of the game world is divided into a grid of typically
     squares  where each grid location represents a tile of either land or sea  figure   shows
a portion of this world map from a particular instance of the game  along with the game
units of one player  in our experiments  we consider a two player game of civilization ii
on a map of      squares  the smallest map allowed on freeciv  this map size is used by
both novice human players looking for an easier game  as well as advanced players wanting
a game of shorter duration  we test our algorithms against the built in ai player of the
game  with the difficulty level at the default normal setting  
    game states and actions
we define the game state for monte carlo search  to be the map of the game world  along
with the attributes of each map tile  and the location and attributes of each players cities
and units  some examples of these attributes are shown in figure    the space of possible
actions for each city and unit is defined by the game rules given the current game state 
for example  cities can construct buildings such as harbors and banks  or create new units
of various types  while individual units can move around on the grid  and perform unit
specific actions such as irrigation for settlers  and military defense for archers  since a
player controls multiple cities and units  the players action space at each turn is defined
by the combination of all possible actions for those cities and units  in our experiments  on
average  a player controls approximately    units with each unit having    possible actions 
the resulting action space for a player is very large  i e          to effectively deal with this
large action space  we assume that given the state  the actions of each individual city and
unit are independent of the actions of all other cities and units of the same player    at
the same time  we maximize parameter sharing by using a single action value function for
all the cities and units of the player 

   freeciv has five difficulty settings  novice  easy  normal  hard and cheating  as evidenced by discussions on the games online forum  http   freeciv wikia com index php title forum playing freeciv  
some human players new to the game find even the novice setting too hard 
    since each player executes game actions in turn  i e  opposing units are fixed during an individual players
turn  the opponents moves do not enlarge the players action space 

   

filearning to win by reading manuals in a monte carlo framework

figure    a portion of the game map from one instance of a civilization ii game  three
cities  and several units of a single player are visible on the map  also visible are
the different terrain attributes of map tiles  such as grassland  hills  mountains
and deserts 

nation attributes 
 

city attributes 
 

amount of gold in treasury
  of world controlled
number of cities
population
known technologies

map tile attributes 
 

city population
surrounding terrain and resources
amount of food   resources produced
number of units supported by city
number   type of units present

unit attributes 

terrain type  e g  grassland  mountain  etc 
tile resources  e g  wheat  coal  wildlife  etc 
tile has river
construction on tile  city  road  rail  etc 
types of units  own or enemy  present

 

unit type  e g   worker  explorer  archer  etc 
unit health   hit points
unit experience
is unit in a city 
is unit fortied 

figure    example attributes of game state 

   

fibranavan  silver    barzilay

    utility function
critically important to the monte carlo search algorithm  is the availability of a utility
function that can evaluate the outcomes of simulated game roll outs  in the typical application of the algorithm  the final game outcome in terms of victory or loss is used as the
utility function  tesauro   galperin         unfortunately  the complexity of civilization
ii  and the length of a typical game  precludes the possibility of running simulation roll outs
until game completion  the game  however  provides each player with a real valued game
score  which is a noisy indicator of the strength of their civilization  since we are playing
a two player game  our players score relative to the opponents can be used as the utility
function  specifically  we use the ratio of the game score of the two players   
    features
all the components of our method operate on features computed over a basic set of text and
game attributes  the text attributes include the words of each sentence along with their
parts of speech and dependency parse information such as dependency types and parent
words  the basic game attributes encode game information available to human players
via the games graphical user interface  some examples of these attributes are shown in
figure   
to identify the sentence most relevant to the current game state and candidate action 
the sentence relevance component computes features over the combined basic attributes of
  are of two types  the first
the game and of each sentence from the text  these features  
computes the cartesian product between the attributes of the game and the attributes of
the candidate sentence  the second type consists of binary features that test for overlap
between words from the candidate sentence  and the text labels of the current game state
and candidate action  given that only      of word tokens from the manual overlap with
labels from the game  these similarity features are highly sparse  however  they serve as
signposts to guide the learner  as shown by our results  our method is able to operate
effectively even in the absence of these features  but performs better when they are present 
predicate labeling  unlike sentence relevance  is purely a language task and as such
  compute
operates only over the basic text attributes  the features for this component   
the cartesian product of the candidate predicate label with the words type  part of speech
tag  and dependency parse information  the final component of our model  the action value
approximation  operates over the attributes of the game state  the candidate action  the
sentence selected as relevant  and the predicate labeling of that sentence  the features of
this layer  f   compute a three way cartesian product between the attributes of the candidate
action  the attributes of the game state  and the predicate labeled words of the relevant
  
  and f  compute approximately                 and         features
sentence  overall   
respectively  resulting in a total of         features for our full model  figure   shows
some examples of these features 

    the difference between players scores can also be used as the utility function  however  in practice the
score ratio produced better empirical performance across all algorithms and baselines 

   

filearning to win by reading manuals in a monte carlo framework

sentence relevance features 
  if action   build city
  tile has river   true
  word    build 

  if action   irrigate
  tile is next to city   true
  word    irrigate 

  otherwise

  otherwise

predicate labeling features 
  if label   action
  word    city 
  parent word    build 

  if label   state
  word    city 
  parent label    near 

  otherwise

  otherwise

action value features 
  if action   build city
  tile has river   true
  action word    build 
  state word    river 

  if action   irrigate
  tile terrain   plains
  action word    irrigate 
  state word    city 

  otherwise

  otherwise

figure    some examples of features used in our model  in each feature  conditions that
test game attributes are highlighted in blue  and those that test words in the
game manual are highlighted in red 

   experimental setup
in this section  we describe the datasets  evaluation metrics  and experimental framework
used to test the performance of our method and the various baselines 
    datasets
we use the official game manual of civilization ii as our strategy guide document    the
text of this manual uses a vocabulary of      word types  and is composed of      sentences 
each on average      words long  this manual contains information about the rules of the
game  about the game user interface  and basic strategy advice about different aspects of
the game  we use the stanford parser  de marneffe  maccartney    manning         under
default settings  to generate the dependency parse information for sentences in the game
manual 
    experimental framework
to apply our method to the civilization ii game  we use the games open source reimplementation freeciv    we instrumented freeciv to allow our method to programmatically
    www civfanatics com content civ  reference civ manual zip
    http   freeciv wikia com  game version    

   

fibranavan  silver    barzilay

primary game
monte carlo
player

game
server

modied game
gui client

in memory
file system

game simulation  

game
strategy guide

game
server

modied game
gui client

game state

game simulation  
game
server

modied game
gui client

game simulation  
game
server

modied game
gui client

figure    a diagram of the experimental framework  showing the monte carlo player  the
server for the primary game which the playing aims to win  and multiple game
servers for simulated play  communications between the multiple processes comprising the framework is via unix sockets and an in memory file system 

control the game  i e   to measure the current game state  to execute game actions  to
save load the current game state  and to start and end games   
across all experiments  we start the game at the same initial state and run it for    
steps  at each step  we perform     monte carlo roll outs  each roll out is run for   
simulated game steps before halting the simulation and evaluating the outcome  note that
at each simulated game step  our algorithm needs to select an action for each game unit 
given an average number of units per player of     this results in         decisions during
the     roll outs  the pairing of each of these decisions with the corresponding roll out
outcome is used as a datapoint to update model parameters  we use a fixed learning rate
of        for all experiments  for our method  and for each of the baselines  we run    
independent games in the above manner  with evaluations averaged across the     runs 
we use the same experimental settings across all methods  and all model parameters are
initialized to zero 
our experimental setup consists of our monte carlo player  a primary game which we
aim to play and win  and a set of simulation games  both the primary game and the simula    in addition to instrumentation  the code of freeciv  both the server and client  was changed to increase
simulation speed by several orders of magnitude  and to remove bugs which caused the game to crash 
to the best of our knowledge  the game rules and functionality are identical to the unmodified freeciv
version    

   

filearning to win by reading manuals in a monte carlo framework

tions are simply separate instances of the freeciv game  each instance of the freeciv game
is made up of one server process  which runs the actual game  and one client process  which
is controlled by the monte carlo player  at the start of each roll out  the simulations are
initialized with the current state of the primary game via the game save reload functionality
of freeciv  figure   shows a diagram of this experimental framework 
the experiments were run on typical desktop pcs with single intel core i  cpus   
hyper threaded cores per cpu   the algorithms were implemented to execute   simulation
roll outs in parallel by connecting to   independent simulation games  in this computational
setup  approximately   simulation roll outs are executed per second for our full model  and
a single game of     steps runs in   hours  since we treat the freeciv game code as a
black box  special care was taken to ensure consistency across experiments  all code was
compiled on one specific machine  under a single fixed build environment  gcc         and
all experiments were run under identical settings on a fixed set of machines running a fixed
os configuration  linux kernel            libc         

    evaluation metrics

we wish to evaluate two aspects of our method  how well it improves game play by leveraging textual information  and how accurately it analyzes text by learning from game feedback 
we evaluate the first aspect by comparing our method against various baselines in terms of
the percentage of games won against the built in ai of freeciv  this ai is a fixed heuristic
algorithm designed using extensive knowledge of the game  with the intention of challenging human players    as such  it provides a good open reference baseline  we evaluate our
method by measuring the percentage of games won  averaged over     independent runs 
however  full games can sometimes last for multiple days  making it difficult to do an extensive analysis of model performance and contributing factors  for this reason  our primary
evaluation measures the percentage of games won within the first     game steps  averaged
over     independent runs  this evaluation is an underestimate of model performance 
any game where the player has not won by gaining control of the entire game map within
    steps is considered a loss  since games can remain tied after     steps  two equally
matched average players  playing against each other  will most likely have a win rate close
to zero under this evaluation 

   results

to adequately characterize the performance of our method  we evaluate it with respect to
several different aspects  in this section  we first describe its game playing performance
and analyze the impact of textual information  then  we investigate the quality of the text
analysis produced by our model in terms of both sentence relevance and predicate labeling 
   

fibranavan  silver    barzilay

method
random
built in ai
game only
latent variable
full model
randomized text

  win
 
 
    
    
    
    

  loss
   
 
   
   
   
   

std  err 


    
    
    
    

table    win rate of our method and several baselines within the first     game steps  while
playing against the built in game ai  games that are neither won nor lost are still
ongoing  our models win rate is statistically significant against all baselines  all
results are averaged across     independent game runs  the standard errors shown
are for percentage wins 

method
game only
latent variable
full model

  wins
    
    
    

standard error
    
    
    

table    win rate of our method and two text unaware baselines against the built in ai 
all results are averaged across     independent game runs 

    game performance
table   shows the performance of our method and several baselines on the primary     step
evaluation  in this scenario  our language aware monte carlo algorithm wins on average
      of games  substantially outperforming all baselines  while the best non languageaware method has a win rate of only        the dismal performance of the random baseline
and the games own built in ai  playing against itself  are indications of the difficulty of
winning games within the first     steps  as shown in table    when evaluated on full length
games  our method has a win rate of       compared to       for the best text unaware
baseline   

    while this ai is constrained to follow the rules of the game  it has access to information typically not
available to human players  such as information about the technology  cities and units of its opponents 
our methods on the other hand are restricted to the actions and information available to human players 
    note that the performance of all methods on the full games is different from those listed in our previous
publications  branavan  silver    barzilay      a      b   these previous numbers were biased by a
code flaw in freeciv which caused the game to sporadically crash in the middle game play  while we
originally believed the crash to be random  it was subsequently discovered to happen more often in losing
games  and thereby biasing the win rates of all methods upwards  the numbers presented here are with
this game bug fixed  with no crashes observed in any of the experiments 

   

fiobserved game score

learning to win by reading manuals in a monte carlo framework

monte carlo rollouts

figure     observed game score as a function of monte carlo roll outs for our text aware
full model  and the text unaware latent variable model  model parameters are
updated after each roll out  thus performance improves with roll outs  as can be
seen  our full models performance improves dramatically over a small number
of roll outs  demonstrating the benefit it derives from textual information 

      textual advice and game performance
to verify and characterize the impact of textual advice on our models performance  we
compare it against several baselines that do not have access to textual information  the
simplest of these methods  game only  models the action value function q s  a  as a linear
approximation of the games state and action attributes  this non text aware method wins
only       of games  see table     to confirm that our methods improved performance
is not simply due to its inherently richer non linear approximation  we also evaluate two
ablative non linear baselines  the first of these  latent variable extends the linear actionvalue function of game only with a set of latent variables  it is in essence a four layer
neural network  similar to our full model  where the second layers units are activated only
based on game information  this baseline wins       of games  table     significantly
improving over the linear game only baseline  but still trailing our text aware method by
more than      the second ablative baseline  randomized text  is identical to our model 
except that it is given a randomly generated document as input  we generate this document
by randomly permuting the locations of words in the game manual  thereby maintaining
the documents statistical properties in terms of type frequencies  this ensures that the
number of latent variables in this baseline is equal to that of our full model  thus  this
baseline has a model capacity equal to our text aware method while not having access to
any textual information  the performance of this baseline  which wins only       of games 
confirms that information extracted from text is indeed instrumental to the performance of
our method 
   

fibranavan  silver    barzilay

figure    provides insight into how textual information helps improve game performance
 it shows the observed game score during the monte carlo roll outs for our full model and
the latent variable baseline  as can be seen from this figure  the textual information guides
our model to a high score region of the search space far quicker than the non text aware
method  thus resulting in better overall performance  to evaluate how the performance
of our method varies with the amount of available textual information  we conduct an
experiment where only random portions of the text are given to the algorithm  as shown
in figure     our methods performance varies linearly as a function of the amount of text 
with the randomized text experiment corresponding to the point where no information is
available from text 
      impact of seed vocabulary on performance
the sentence relevance component of our model uses features that compute the similarity
between words in a sentence  and the text labels of the game state and action  this assumes
the availability of a seed vocabulary that names game attributes  in our domain  of the    
unique text labels present in the game      occur in the vocabulary of the game manual 
this results in a sparse seed vocabulary of     words  covering only      of word types
and      of word tokens in the manual  despite this sparsity  the seed vocabulary can have
a potentially large impact on model performance since it provides an initial set of word
groundings  to evaluate the importance of this initial grounding  we test our method with
an empty seed vocabulary  in this setup  our full model wins       of games  showing
that while the seed words are important  our method can also operate effectively in their
absence 
      linguistic representation and game performance
to characterize the contribution of language to game performance  we conduct a series of
evaluations which vary the type and complexity of the linguistic analysis performed by our
method  the results of this evaluation are shown in table    the first of these  sentence
relevance  highlights the contributions of the two language components of our model  this
algorithm  which is identical to our full model but lacks the predicate labeling component 
wins       of games  showing that while it is essential to identify the textual advice relevant
to the current game state  a deeper syntactic analysis of the extracted text substantially
improves performance 
to evaluate the importance of dependency parse information in our language analysis 
we vary the type of features available to the predicate labeling component of our model 
the first of these ablative experiments  no dependency information  removes all dependency
features  leaving predicate labeling to operate only on word type features  the performance
of this baseline  a win rate of        clearly shows that the dependency features are crucial
for model performance  the remaining three methods  no dependency label  no dependency
parent pos tag and no dependency parent word  each drop the dependency feature they
are named after  the contribution of these features to model performance can be seen in
table   
   

fiwin rate

learning to win by reading manuals in a monte carlo framework

random
text

percentage of document text given to our model

figure     the performance of our text aware model as a function of the amount of text
available to it  we construct partial documents by randomly sub sampling sentences from the full game manual  the x axis shows the amount of sentences
given to the method as a ratio of the full text  at the leftmost extreme is
the performance of the randomized text baseline  showing how it fits into the
performance trend at the point of having no useful textual information 
method
full model
sentence relevance
no dependency information
no dependency label
no depend  parent pos tag
no depend  parent word

  win
    
    
    
    
    
    

  loss
   
   
   
   
   
   

std  err 
    
    
    
    
    
    

table    win rates of several ablated versions of our model  showing the contribution of
different aspects of textual information to game performance  sentence relevance
is identical to the full model  except that it lacks the predicate labeling component 
the four methods at the bottom of the table ablate specific dependency features
 as indicated by the methods name  from the predicate labeling component of the
full model 

      model complexity vs computation time trade off
one inherent disadvantage of non linear models  when compared to simpler linear models 
is the increase in computation time required for parameter estimation  in our monte carlo
search setup  model parameters are re estimated after each simulated roll out  therefore 
given a fixed amount of time  more roll outs can be done for a simpler and faster model  by
its very nature  the performance of monte carlo search improves with the number of rollouts  this trade off between model complexity and roll outs is important since a simpler
   

fibranavan  silver    barzilay

   
full model
latent variable
game only
ro
llo

ut

s

   

 

s

ts

  

 

  

ut

 r
oll
 ou

   

  

win rate

   

o
llro

   

   

  

 

  

  

  

  

   

   

   

computation time per game step  seconds 

figure     win rate as a function of computation time per game step  for each montecarlo search method  win rate and computation time were measured for     
    and     roll outs per game step  respectively 

model could compensate by using more roll outs  and thereby outperform more complex
ones  this scenario is particularly relevant in games where players have a limited amount
of time for each turn 
to explore this trade off  we vary the number of simulation roll outs allowed for each
method at each game step  recording the win rate and the average computation time per
game  figure    shows the results of this evaluation for          and     roll outs  while
the more complex methods have higher computational demands  these results clearly show
that even when given a fixed amount of computation time per game step  our text aware
model still produces the best performance by a wide margin 
      learned game strategy
qualitatively  all of the methods described here learn a basic rush strategy  essentially 
they attempt to develop basic technologies  build an army  and take over opposing cities as
quickly as possible  the performance difference between the different models is essentially
due to how well they learn this strategy 
there are two basic reasons why our algorithms learn the rush strategy  first  since we
are attempting to maximize game score  the methods are implicitly biased towards finding
the fastest way to win  which happens to be the rush strategy when playing against
the built in ai of civilization    second  more complex strategies typically require the
coordination of multiple game units  since our models assume game units to be independent 
   

filearning to win by reading manuals in a monte carlo framework

phalanxes are twice as eective at defending cities as warriors 
build the city on plains or grassland with a river running through it 




you can rename the city if you like  but we ll refer to it as washington 
there are many dierent strategies dictating the order in which
advances are researched

after the road is built  use the settlers to start improving the terrain 
s

s

s

a

a

a

a

a

when the settlers becomes active  chose build road 
s

s

a

s

a

a

use settlers or engineers to improve a terrain square within the city radius
a

s



a

a

s

a



s

s

s

s

figure     examples of our methods sentence relevance and predicate labeling decisions 
the box above shows two sentences  identified by green check marks  which
were predicted as relevant  and two which were not  the box below shows
the predicted predicate structure of three sentences  with s indicating state
description a action description and background words unmarked  mistakes
are identified with crosses 

they cannot explicitly learn such coordination  putting many complex strategies beyond
the capabilities of our algorithms 
    accuracy of linguistic analysis
as described in section    text analysis in our method is tightly coupled with game playing
 both in terms of modeling  and in terms of learning from game feedback  we have seen
from the results thus far  that this text analysis does indeed help game play  in this section
we focus on the game driven text analysis itself  and investigate how well it conforms to
more common notions of linguistic correctness  we do this by comparing model predictions
of sentence relevance and predicate labeling against manual annotations 
      sentence relevance
figure    shows examples of the sentence relevance decisions produced by our method 
to evaluate the accuracy of these decisions  we would ideally like to use a ground truth
relevance annotation of the games user manual  this however  is impractical since the
relevance decision is dependent on the game context  and is hence specific to each time step
of each game instance  therefore  we evaluate sentence relevance accuracy using a synthetic
document  we create this document by combining the original game manual with an equal
   

fisentence relevance accuracy

branavan  silver    barzilay

   
   
   
   
sentence relevance
moving average

   
 

  

  

  

  

   

game step

figure     accuracy of our methods sentence relevance predictions  averaged over     independent runs 

number of sentences which are known to be irrelevant to the game  these sentences are
collected by randomly sampling from the wall street journal corpus  marcus  santorini 
  marcinkiewicz           we evaluate sentence relevance on this synthetic document by
measuring the accuracy with which game manual sentences are picked as relevant 
in this evaluation  our method achieves an average accuracy of        given that our
model only has to differentiate between the game manual text and the wall street journal 
this number may seem disappointing  furthermore  as can be seen from figure     the
sentence relevance accuracy varies widely as the game progresses  with a high average of
      during the initial    game steps  in reality  this pattern of high initial accuracy followed by a lower average is not entirely surprising  the official game manual for civilization
ii is written for first time players  as such  it focuses on the initial portion of the game 
providing little strategy advice relevant to subsequent game play    if this is the reason for
the observed sentence relevance trend  we would also expect the final layer of the neural
network to emphasize game features over text features after the first    steps of the game 
this is indeed the case  as can be seen in figure    
to further test this hypothesis  we perform an experiment where the first n steps of the
game are played using our full model  and the subsequent      n steps are played without
using any textual information  the results of this evaluation for several values of n are
given in figure     showing that the initial phase of the game is indeed where information
from the game manual is most useful  in fact  this hybrid method performs just as well
as our full model when n       achieving a       win rate  this shows that our method
    note that sentences from the wsj corpus contain words such as city which can potentially confuse our
algorithm  causing it to select such sentences are relevant to game play 
    this is reminiscent of opening books for games like chess or go  which aim to guide the player to a
playable middle game  without providing much information about subsequent game play 

   

filearning to win by reading manuals in a monte carlo framework

   

game features dominate

   

text features dominate

text feature importance

   

 
  

  

  

  

   

game step

figure     difference between the norms of the text features and game features of the
output layer of the neural network  beyond the initial    steps of the game  our
method relies increasingly on game features 

win rate

   

   

   

  
  

  

  

  

   

  of initial game steps where text information is used

figure     graph showing how the availability of textual information during the initial steps
of the game affects the performance of our full model  textual information is
given to the model for the first n steps  the x axis   beyond which point the
algorithm has no access to text  and becomes equivalent to the latent variable
model  i e   the best non text model 

is able to accurately identify relevant sentences when the information they contain is most
pertinent to game play  and most likely to produce better game performance 
   

fibranavan  silver    barzilay

method
random labeling
model  first     steps
model  first    steps

s a b
     
     
     

s a
     
     
     

table    predicate labeling accuracy of our method and a random baseline  column
s a b shows performance on the three way labeling of words as state  action
or background  while column s a shows accuracy on the task of differentiating
between state and action words 
game attribute

word

state  grassland

 city 

state  grassland

 build 

state  hills

 build 

action  settlers build city

 city 

action  set research

 discovery 

action  settlers build city

 settler 

action  settlers goto location

 build 

action  city build barracks

 construct 

action  research alphabet

 develop 

action  set research

 discovery 

figure     examples of word to game attribute associations that are learned via the feature
weights of our model 
      predicate labeling
figure    shows examples of the predicate structure output of our model  we evaluate the
accuracy of this labeling by comparing it against a gold standard annotation of the game
manual    table   shows the performance of our method in terms of how accurately it labels
words as state  action or background  and also how accurately it differentiates between state
and action words  in addition to showing a performance improvement over the random
baseline  these results display a clear trend  under both evaluations  labeling accuracy is
higher during the initial stages of the game  this is to be expected since the model relies
heavily on textual features during the beginning of the game  see figure     
to verify the usefulness of our methods predicate labeling  we perform a final set of
experiments where predicate labels are selected uniformly at random within our full model 
this random labeling results in a win rate of      a performance similar to the sentence
relevance model which uses no predicate information  this confirms that our method is
able to identify a predicate structure which  while noisy  provides information relevant to
game play  figure    shows examples of how this textual information is grounded in the
game  by way of the associations learned between words and game attributes in the final
layer of the full model  for example  our model learns a strong association between the
    note that a ground truth labeling of words as either action description  state description  or background
is based purely on the semantics of the sentence  and is independent of game state  for this reason 
manual annotation is feasible  unlike in the case of sentence relevance 

   

filearning to win by reading manuals in a monte carlo framework

game state attribute grassland and the words city and build  indicating that textual
information about building cities maybe very useful when a players unit is near grassland 

   conclusions
in this paper we presented a novel approach for improving the performance of control
applications by leveraging information automatically extracted from text documents  while
at the same time learning language analysis based on control feedback  the model biases the
learned strategy by enriching the policy function with text features  thereby modeling the
mapping between words in a manual and state specific action selection  to effectively learn
this grounding  the model identifies text relevant to the current game state  and induces
a predicate structure on that text  these linguistic decisions are modeled jointly using a
non linear policy function trained in the monte carlo search framework 
empirical results show that our model is able to significantly improve game win rate
by leveraging textual information when compared to strong language agnostic baselines 
we also demonstrate that despite the increased complexity of our model  the knowledge
it acquires enables it to sustain good performance even when the number of simulations is
reduced  moreover  deeper linguistic analysis  in the form of a predicate labeling of text 
further improves game play  we show that information about the syntactic structure of
text is crucial for such an analysis  and ignoring this information has a large impact on
model performance  finally  our experiments demonstrate that by tightly coupling control
and linguistic features  the model is able to deliver robust performance in the presence of
the noise inherent in automatic language analysis 

bibliographical note
portions of this work were previously presented in two conference publications  branavan
et al       a      b   this article significantly extends our previous work  most notably by
providing an analysis of model properties such as the impact of linguistic representation on
model performance  dependence of the model on bootstrapping conditions  and the tradeoff between the models representational power and its empirical complexity  section    
the paper also significantly increases the volume of experiments on which we base our
conclusions  in addition  we provide a comprehensive description of the model  providing
full mathematical derivations supporting the algorithm  section     and appendix a  

acknowledgments
the authors acknowledge the support of the nsf  career grant iis          grant iis          the darpa bolt program  hr                 the darpa machine reading
program  fa        c       po              batelle  po         and the microsoft
research new faculty fellowship  thanks to the anonymous reviewers  michael collins 
tommi jaakkola  leslie kaelbling  nate kushman  sasha rush  luke zettlemoyer  and the
mit nlp group for their suggestions and comments  any opinions  findings  conclusions 
or recommendations expressed in this paper are those of the authors  and do not necessarily
reflect the views of the funding organizations 
   

fibranavan  silver    barzilay

appendix a  parameter estimation
the parameter of our model are estimated via standard error backpropagation  bryson  
ho        rumelhart et al          to derive the parameter updates  consider the slightly
simplified neural network shown below  this network is identical to our model  but for the
sake of clarity  it has only a single second layer  y instead of the two parallel second layers
 y and  z  the parameter updates for these parallel layers  y and  z are similar  therefore we
will show the derivation only for  y in addition to the updates for the final layer 

as in our model  the nodes yi in the network above are activated via a softmax function 
the third layer  f   is computed deterministically from the active nodes of the second layer
via the function  g  yi    x   and the output q is a linear combination of f  weighted by w 
 
p yi        x   ui    

e ui  x
x
 
e uk  x
k

f   

x

 g   x  yi   p yi    x   ui   

i

q   w
   f  
our goal is to minimize the mean squared error e by gradient descent  we achieve this by
updating model parameters along the gradient of e with respect to each parameter  using
i as a general term to indicate our models parameters  this update takes the form 
 
 q  r    
 
e
 
i
q
   q  r 
 
i

e  
i

from equation      the updates for final layer parameters are given by 
q
wi

   q  r 
w
   f 
wi
   q  r  fi  

wi    q  r 

   

   

filearning to win by reading manuals in a monte carlo framework

since our model samples the one most relevant sentence yi   and the best predicate labeling
zi   the resulting online updates for the output layer parameters w
  are 
w
   w
    w  q  r s    f  s  a  yi   zj   
where w is the learning rate  and q   q s  a   the updates for the second layers parameters are similar  but somewhat more involved  again  from equation     
ui j

q
ui j

   q  r 
w
   f 
ui j
x

w
 
 g   x  yk   p yi    x   uk  
   q  r 
ui j
   q  r 

k

   q  r  w
    g   x  yi  


p yi    x   ui   
ui j

   

considering the final term in the above equation separately 

p yi    x   ui    
ui j

 e ui  x
 
ui j z


 

where z  

x

e uk  x

k

  eu  i  x
e ui  x ui j z
 u   x 
e i
z
z

 
 
 
 
 
 

e ui  x

  ui  x 

e
log
z
ui j
z
  ui  x  

e

xj 
log z
z
ui j
  ui  x  

e
  z
xj 
z
z ui j
 
 
  ui  x 
e
   x  uk  x
xj 
e
z
z ui j
k
  ui  x  

e
 
 
uk  
x
xj  xj e
z
z
  ui  x  

e
e ui  x
xj   
 
z
z




   

fibranavan  silver    barzilay

therefore  from equation     
ui j


p yi    x   ui  
ui j
  ui  x 


e
e ui  x
   q  r  w
    g   x  yi  
xj   
z
z
   q  r  xj w
    g   x  yi   p yi    x   ui       p yi    x   ui   
   q  r  w
    g   x  yi  

   q  r  xj q     p yi    x   ui     
where q   w
    g   x  yi   p yi    x   ui   

the resulting online updates for the sentence relevance and predicate labeling parameters
 u and  v are 
 ui   ui   u  q  r s    q  x     p yi     
 vi   vi   v  q  r s    q  x     p zi     

   

filearning to win by reading manuals in a monte carlo framework

appendix b  example of sentence relevance predictions
shown below is a portion of the strategy guide for civilization ii  sentences that were
identified as relevant by our text aware model are highlighted in green 

choosing your location 
when building a new city  carefully plan where you place it  citizens can
work the terrain surrounding the city square in an x shaped pattern  see
city radius for a diagram showing the exact dimensions   this area is called
the city radius  the terrain square on which the settlers were standing
becomes the city square   the natural resources available where a
population settles affect its ability to produce food and goods  cities built on
or near water sources can irrigate to increase their crop yields  and cities
near mineral outcroppings can mine for raw materials  on the other hand 
cities surrounded by desert are always handicapped by the aridness of their
terrain  and cities encircled by mountains find arable cropland at a
premium  in addition to the economic potential within the city s radius  you
need to consider the proximity of other cities and the strategic value of a
location  ideally  you want to locate cities in areas that offer a combination
of benefits   food for population growth  raw materials for production  and
river or coastal areas for trade  where possible  take advantage of the
presence of special resources on terrain squares  see terrain   movement
for details on their benefits  
strategic value 
the strategic value of a city site is a final consideration  a city square s
underlying terrain can increase any defender s strength when that city
comes under attack  in some circumstances  the defensive value of a
particular city s terrain might be more important than the economic value 
consider the case where a continent narrows to a bottleneck and a rival
holds the other side  good defensive terrain  hills  mountains  and jungle  is
generally poor for food production and inhibits the early growth of a city  if
you need to compromise between growth and defense  build the city on a
plains or grassland square with a river running through it if possible  this
yields decent trade production and gains a    percent defense bonus 
regardless of where a city is built  the city square is easier to defend than
the same unimproved terrain  in a city you can build the city walls
improvement  which triples the defense factors of military units stationed
there  also  units defending a city square are destroyed one at a time if they
lose  outside of cities  all units stacked together are destroyed when any
military unit in the stack is defeated  units in fortresses are the only
exception  see fortresses   placing some cities on the seacoast gives you
access to the ocean  you can launch ship units to explore the world and to
transport your units overseas  with few coastal cities  your sea power is
inhibited 

   

fibranavan  silver    barzilay

appendix c  examples of predicate labeling predictions
listed below are the predicate labellings computed by our text aware method on example
sentences from the game manual  the predicted labels are indicated below the words with
the letters a  s  and b for action description  state description and background respectively 
incorrect labels are indicated by a red check mark  along with the correct label in brackets 
after the road is built  use the settlers to start improving the terrain 
s

s

s

a

a

a

a

a

when the settlers becomes active  chose build road 
s

s

s

a

a

a

use settlers or engineers to improve a terrain square within the city radius

 s  a 

a

a

a

 a  s 

s

s

s

s

s

bronze working allows you to build phalanx units
s

 b  s 

s

a

a

a

in order to expand your civilization   you need to build other cities

 a  s 

s

s

b

 b  a 

a

in order to protect the city   the phalanx must remain inside

 b s 

 b s 

s

 s a 

a

a

 b a 

as soon as you ve found a decent site   you want your settlers to build a

 b s 

 b s 

s

 b  a 

 a  b 

s

a

permanent settlement   a city

 s  a 

a

in a city you can build the city walls improvement

 a  s 

 b  a 

a

a

a

once the city is undefended   you can move a friendly army into the city and capture it
s

 b  s 

s

a

a

a

a

b

you can build a city on any terrain square except for ocean 
a

s  a 

 b  s 

s

 a  s 

s

you can launch ship units to explore the world and to transport your units overseas
a

s  a 

a

s

 b  s 

 b  s 

s

b

when a city is in disorder  disband distant military units  return them to their home cities 

 a  s 

a

s

a

s  a 

a

s  a 

a

or change their home cities
a

a

a

you can build a wonder only if you have discovered the advance that makes it possible
a

s  a 

s

   

s

s

s

filearning to win by reading manuals in a monte carlo framework

appendix d  examples of learned text to game attribute mappings
shown below are examples of some of the word to game attribute associations learned by
our model  the top ten game attributes with the strongest association by feature weight are
listed for three of the example words  attack  build and grassland  for the fourth
word  settler  only seven attributes had non zero weights in experiments used to collect
these statistics 

attack

build

phalanx  unit 

worker goto  action 

warriors  unit 

settler autosettle  action 

colossus  wonder 

worker autosettle  action 

city walls  city improvement 

pheasant  terrain attribute 

archers  unit 

settler irrigate  action 

catapult  unit 

worker mine  action 

palace  city improvement 

build city walls  action 

coinage  city production 

build catapult  action 

city build warriors  action 

swamp  terrain attribute 

city build phalanx  action 

grassland  terrain attribute 

grassland

settler

settler build city  action 

settlers  state attribute 

worker continue action  action 

settler build city  action 

pheasant  terrain attribute 

city  state attribute 

city build improvement  action 

grassland  terrain attribute 

city max production  action 

plains  terrain attribute 

settlers  state attribute 

road  terrain attribute 

city max food  action 

workers  state attribute 

settler goto  action 
worker build road  action 
pyramids  city attribute 

   

fibranavan  silver    barzilay

appendix e  features used by the model
features used predict sentence relevance
the following templates are used to compute the features for sentence relevance 
 word w is present in sentence 
 number of words that match the text label of the current unit  an attribute in the
immediate neighborhood of the unit  or the action under consideration 
 the units type is u   e g   worker  and word w is present in sentence 
 the action type is a   e g   irrigate  and word w is present in sentence 
features used predict predicate structure
the following templates are used to compute the features for the predicate labeling of words 
the label being considered for the word  i e   action  state or background  is denoted by
l 
 label is l and the word type is w 
 label is l and the part of speech tag of the word is t 
 label is l and the parent word in the dependency tree is w 
 label is l and the dependency type to the dependency parent word is d 
 label is l and the part of speech of the dependency parent word is t 
 label is l and the word is a leaf node in the dependency tree 
 label is l and the word is not a leaf node in the dependency tree 
 label is l and the word matches a state attribute name 
 label is l and the word matches a unit type name 
 label is l and the word matches a action name 
features used to model action value function
the following templates are used to compute the features of the action value approximation 
unless otherwise mentioned  the features look at the attributes of the player controlled by
our model 
 percentage of world controlled 
 percentage of world explored 
 players game score 
 opponents game score 
 number of cities 
 average size of cities 
 total size of cities 
   

filearning to win by reading manuals in a monte carlo framework

 number of units 
 number of veteran units 
 wealth in gold 
 excess food produced 
 excess shield produced 
 excess trade produced 
 excess science produced 
 excess gold produced 
 excess luxury produced 
 name of technology currently being researched 
 percentage completion of current research 
 percentage remaining of current research 
 number of game turns before current research is completed 
the following feature templates are applied to each city controlled by the player 
 current size of city 
 number of turns before city grows in size 
 amount of food stored in city 
 amount of shield stored in city  shields are used to construct new buildings and
units in the city  
 turns remaining before current construction is completed 
 surplus food production in city 
 surplus shield production in city 
 surplus trade production in city 
 surplus science production in city 
 surplus gold production in city 
 surplus luxury production in city 
 distance to closest friendly city 
 average distance to friendly cities 
 city governance type 
 type of building or unit currently under construction 
 types of buildings already constructed in city 
 type of terrain surrounding the city 
 type of resources available in the citys neighborhood 
   

fibranavan  silver    barzilay

 is there another city in the neighborhood 
 is there an enemy unit in the neighborhood 
 is there an enemy city in the neighborhood 
the following feature templates are applied to each unit controlled by the player 
 type of unit 
 moves left for unit in current game turn 
 current health of unit 
 hit points of unit 
 is unit a veteran 
 distance to closest friendly city 
 average distance to friendly cities 
 type of terrain surrounding the unit 
 type of resources available in the units neighborhood 
 is there an enemy unit in the neighborhood 
 is there an enemy city in the neighborhood 
the following feature templates are applied to each predicate labeled word in the sentence
selected as relevant  combined with the current state and action attributes 
 word w is present in sentence  and the action being considered is a 
 word w with predicate label p is present in sentence  and the action being considered
is a 
 word w is present in sentence  the current units type is u  and the action being
considered is a 
 word w with predicate label p is present in sentence  the current units type is u 
and the action being considered is a 
 word w is present in sentence  and the current units type is u 
 word w with predicate label p is present in sentence  and the current units type is
u 
 word w is present in sentence  and an attribute with text label a is present in the
current units neighborhood 
 word w with predicate label p is present in sentence  and an attribute with text label
a is present in the current units neighborhood 

   

filearning to win by reading manuals in a monte carlo framework

references
balla  r     fern  a          uct for tactical assault planning in real time strategy games 
in proceedings of ijcai  pp       
barnard  k     forsyth  d  a          learning the semantics of words and pictures  in
proceedings of iccv  pp         
barto  a  g     mahadevan  s          recent advances in hierarchical reinforcement
learning  discrete event dynamic systems             
billings  d   castillo  l  p   schaeffer  j     szafron  d          using probabilistic knowledge
and simulation to play poker  in proceedings of aaai iaai  pp         
branavan  s   chen  h   zettlemoyer  l     barzilay  r          reinforcement learning for
mapping instructions to actions  in proceedings of acl  pp       
branavan  s   silver  d     barzilay  r       a   learning to win by reading manuals in a
monte carlo framework  in proceedings of acl  pp         
branavan  s   silver  d     barzilay  r       b   non linear monte carlo search in civilization
ii  in proceedings of ijcai  pp           
branavan  s   zettlemoyer  l     barzilay  r          reading between the lines  learning
to map high level instructions to commands  in proceedings of acl  pp           
bridle  j  s          training stochastic model recognition algorithms as networks can lead
to maximum mutual information estimation of parameters  in advances in nips  pp 
       
bryson  a  e     ho  y  c          applied optimal control  optimization  estimation  and
control  blaisdell publishing company 
chen  d  l     mooney  r  j          learning to sportscast  a test of grounded language
acquisition  in proceedings of icml  pp         
chen  d  l     mooney  r  j          learning to interpret natural language navigation
instructions from observations  in proceedings of aaai  pp         
clarke  j   goldwasser  d   chang  m  w     roth  d          driving semantic parsing
from the worlds response  in proceedings of connl  pp       
de marneffe  m  c   maccartney  b     manning  c  d          generating typed dependency parses from phrase structure parses  in proceedings of lrec  pp         
eisenstein  j   clarke  j   goldwasser  d     roth  d          reading to learn  constructing
features from semantic abstracts  in proceedings of emnlp  pp         
fleischman  m     roy  d          intentional context in situated natural language learning 
in proceedings of conll  pp         
gelly  s   wang  y   munos  r     teytaud  o          modification of uct with patterns
in monte carlo go  tech  rep        inria 
goldwasser  d   reichart  r   clarke  j     roth  d          confidence driven unsupervised
semantic parsing  in proceedings of acl  pp           

   

fibranavan  silver    barzilay

gorniak  p     roy  d          speaking with your sidekick  understanding situated speech
in computer role playing games  in proceedings of aiide  pp       
liang  p   jordan  m  i     klein  d          learning semantic correspondences with less
supervision  in proceedings of acl  pp       
liang  p   jordan  m  i     klein  d          learning dependency based compositional
semantics  in proceedings of acl  pp         
marcus  m  p   santorini  b     marcinkiewicz  m  a          building a large annotated
corpus of english  the penn treebank  computational linguistics                 
oates  j  t          grounding knowledge in sensors  unsupervised learning for language
and planning  ph d  thesis  university of massachusetts amherst 
roy  d  k     pentland  a  p          learning words from sights and sounds  a computational model  cognitive science            
rumelhart  d  e   hinton  g  e     williams  r  j          learning representations by
back propagating errors  nature              
schafer  j          the uct algorithm applied to games with imperfect information 
diploma thesis  otto von guericke universitat magdeburg 
sheppard  b          world championship caliber scrabble  artificial intelligence            
       
silver  d   sutton  r     muller  m          sample based learning and search with permanent and transient memories  in proceedings of icml  pp         
siskind  j  m          grounding the lexical semantics of verbs in visual perception using
force dynamics and event logic  journal of artificial intelligence research           
sturtevant  n          an analysis of uct in multi player games  in proceedings of iccg 
pp       
sutton  r  s     barto  a  g          reinforcement learning  an introduction  the mit
press 
sutton  r  s   koop  a     silver  d          on the role of tracking in stationary environments  in proceedings of icml  pp         
tellex  s   kollar  t   dickerson  s   walter  m  r   banerjee  a  g   teller  s     roy  n 
        understanding natural language commands for robotic navigation and mobile
manipulation  in proceedings of aaai  pp           
tesauro  g     galperin  g          on line policy improvement using monte carlo search 
in advances in nips  pp           
vogel  a     jurafsky  d          learning to follow navigational directions  in proceedings
of the acl  pp         
yu  c     ballard  d  h          on the integration of grounding language and learning
objects  in proceedings of aaai  pp         
zettlemoyer  l     collins  m          learning context dependent mappings from sentences
to logical form  in proceedings of acl  pp         

   

fi