journal artificial intelligence research                  

submitted        published      

learning win reading manuals
monte carlo framework
s r k  branavan

branavan csail mit edu

computer science artificial intelligence laboratory
massachusetts institute technology

david silver

d silver cs ucl ac uk

department computer science
university college london

regina barzilay

regina csail mit edu

computer science artificial intelligence laboratory
massachusetts institute technology

abstract
domain knowledge crucial effective performance autonomous control systems 
typically  human effort required encode knowledge control algorithm 
paper  present approach language grounding automatically interprets
text context complex control application  game  uses domain
knowledge extracted text improve control performance  text analysis
control strategies learned jointly using feedback signal inherent application 
effectively leverage textual information  method automatically extracts text
segment relevant current game state  labels task centric predicate
structure  labeled text used bias action selection policy game 
guiding towards promising regions action space  encode model text
analysis game playing multi layer neural network  representing linguistic decisions
via latent variables hidden layers  game action quality via output layer 
operating within monte carlo search framework  estimate model parameters using
feedback simulated games  apply approach complex strategy game
civilization ii using official game manual text guide  results show
linguistically informed game playing agent significantly outperforms language unaware
counterpart  yielding     absolute improvement winning     games
playing built in ai civilization 

   introduction
paper  study task grounding document content control applications
computer games  applications  agent attempts optimize utility
function  e g   game score  learning select situation appropriate actions  complex
domains  finding winning strategy challenging even humans  therefore  human
players typically rely manuals guides describe promising tactics provide
general advice underlying task  surprisingly  textual information never
utilized control algorithms despite potential greatly improve performance 
goal  therefore  develop methods achieve automatic fashion 
c
    
ai access foundation  rights reserved 

fibranavan  silver    barzilay

natural resources available population settles aects ability produce food
goods  cities built near water sources irrigate increase crop yields 
cities near mineral resources mine raw materials  build city plains grassland
square river running possible 

figure    excerpt user manual game civilization ii 
explore question context strategy games  challenging class large scale
adversarial planning problems 
consider instance text shown figure    excerpt user
manual game civilization ii   text describes game locations action
build city effectively applied  stochastic player access
text would gain knowledge hard way  would repeatedly attempt
action myriad states  thereby learning characterization promising state action
pairs based observed game outcomes  games large state spaces  long planning
horizons  high branching factors  approach prohibitively slow ineffective 
algorithm access text  however  could learn correlations words
text game attributes e g   word river places rivers game
thus leveraging strategies described text select better actions 
improve performance control applications using domain knowledge automatically extracted text  need address following challenges 
grounding text state action space control application text
guides provide wealth information effective control strategies  including
situation specific advice well general background knowledge  benefit
information  algorithm learn mapping text
guide  states actions control application  mapping allows
algorithm find state specific advice matching state attributes verbal
descriptions  furthermore  relevant sentence found  mapping biases
algorithm select action proposed guide document  mapping
modeled word level  ideally would use information encoded
structure sentence predicate argument structure  instance 
algorithm explicitly identify predicates state attribute descriptions 
map directly structures inherent control application 
annotation free parameter estimation text analysis tasks relate
well known methods information extraction  prior work primarily focused
supervised methods  setup  text analysis state dependent  therefore annotations need representative entire state space  given enormous state
space continually changes game progresses  collecting annotations
impractical  instead  propose learn text analysis based feedback signal
inherent control application  e g   game score  feedback computed
automatically step game  thereby allowing algorithm continuously
adapt local  observed game context 
   http   en wikipedia org wiki civilization ii

   

filearning win reading manuals monte carlo framework

effective integration extracted text information control application text guides provide complete  step by step advice situations player may encounter  even advice available  learned
mapping may noisy  resulting suboptimal choices  therefore  need design method achieve effective control absence textual advice 
robustly integrating automatically extracted information available 
address challenge incorporating language analysis monte carlo search 
state of the art framework playing complex games  traditionally framework
operates state action features  extending monte carlo search
include textual features  integrate two sources information principled
fashion 
    summary approach
address challenges unified framework based markov decision processes
 mdp   formulation commonly used game playing algorithms  setup consists
game stochastic environment  goal player maximize given
utility function r s  state s  players behavior determined action value
function q s  a  assesses goodness action state based attributes
a 
incorporate linguistic information mdp formulation  expand action
value function include linguistic features  state action features known
point computation  relevant words semantic roles observed 
therefore  model text relevance hidden variable  similarly  use hidden variables
discriminate words describe actions describe state attributes
rest sentence  incorporate hidden variables action value function 
model q s  a  non linear function approximation using multi layer neural network 
despite added complexity  parameters non linear model effectively learned monte carlo search framework  monte carlo search  actionvalue function estimated playing multiple simulated games starting current
game state  use observed reward simulations update parameters
neural network via backpropagation  focuses learning current game state 
allowing method learn language analysis game play appropriate observed
game context 
    evaluation
test method strategy game civilization ii  notoriously challenging game
immense action space   source knowledge guiding model  use
official game manual  baseline  employ similar monte carlo search based player
access textual information  demonstrate linguisticallyinformed player significantly outperforms baseline terms number games
won  moreover  show modeling deeper linguistic structure sentences improves performance  full length games  algorithm yields     improve   civilization ii    igns      list top video games time 
 http   top    ign com      ign top game   html 

   

fibranavan  silver    barzilay

ment language unaware baseline wins     games built in 
hand crafted ai civilization ii  video method playing game available
http   groups csail mit edu rbg code civ video  code data work  along
complete experimental setup preconfigured environment virtual machine
available http   groups csail mit edu rbg code civ 
    roadmap
section    provide intuition benefits integrating textual information
learning algorithms control  section   describes prior work language grounding  emphasizing unique challenges opportunities setup  section positions
work large body research monte carlo based players  section   presents
background monte carlo search applied game playing  section   present
multi layer neural network formulation action value function combines information text control application  next  present monte carlo method
estimating parameters non linear function  sections     focus application algorithm game civilization ii  section   compare method
range competitive game playing baselines  empirically analyze properties algorithm  finally  section   discuss implications research 
conclude 

   learning game play text
section  provide intuitive explanation textual information help improve action selection complex game  clarity  first discuss benefits textual
information supervised scenario  thereby decoupling questions concerning modeling
representation related parameter estimation  assume every state
represented set n features  s    s            sn    given state s  goal select
best possible action aj fixed set a  model task multiclass classification  choice aj represented feature vector   s    aj     s    aj             sn   aj    
here   si   aj        n  represents feature created taking cartesian product  s    s            sn   aj   learn classifier effectively  need training set
sufficiently covers possible combinations state features actions  however  domains complex state spaces large number possible actions  many instances
state action feature values unobserved training 
show generalization power classifier improved using textual information  assume training example  addition state action pair 
contains sentence may describe action taken given state attributes  intuitively  want enrich basic classifier features capture correspondence
states actions  words describe them  given sentence w composed
word types w    w            wm   features form  si   wk    aj   wk  
every     n   k     m  aj a  assuming action described using similar words throughout guide  expect text enriched classifier would able
learn correspondence via features  aj   wk    similar intuition holds learning
correspondence state attributes descriptions represented features
 si   wk    features  classifier connect state action aj based
   

filearning win reading manuals monte carlo framework

evidence provided guiding sentence occurrences contexts
throughout training data  text free classifier may support association
action appear similar state context training set 
benefits textual information extend models trained using control
feedback rather supervised data  training scenario  algorithm assesses
goodness given state action combination simulating limited number game turns
action taken observing control feedback provided underlying
application  algorithm built in mechanism  see section    employs
observed feedback learn feature weights  intelligently samples space search
promising state action pairs  algorithm access collection sentences 
similar feedback based mechanism used find sentences match given stateaction pair  section       state  action description features  si   wk  
 aj   wk    algorithm jointly learns identify relevant sentences map actions
states descriptions  note used classification basis
discussion section  reality methods learn regression function 

   related work
section  first discuss prior work field grounded language acquisition 
subsequently look two areas specific application domain i e   natural language
analysis context games  monte carlo search applied game playing 
    grounded language acquisition
work fits broad area research grounded language acquisition
goal learn linguistic analysis non linguistic situated context  oates       
barnard   forsyth        siskind        roy   pentland        yu   ballard        chen
  mooney        zettlemoyer   collins        liang  jordan    klein        branavan 
chen  zettlemoyer    barzilay        branavan  zettlemoyer    barzilay        vogel   jurafsky        clarke  goldwasser  chang    roth        tellex  kollar  dickerson  walter 
banerjee  teller    roy        chen   mooney        liang  jordan    klein        goldwasser  reichart  clarke    roth         appeal formulation lies reducing
need manual annotations  non linguistic signals provide powerful  albeit
noisy  source supervision learning  traditional grounding setup assumed
non linguistic signals parallel content input text  motivating machine
translation view grounding task  alternative approach models grounding
control framework learner actively acquires feedback non linguistic environment uses drive language interpretation  summarize approaches 
emphasizing similarity differences work 
      learning grounding parallel data
many applications  linguistic content tightly linked perceptual observations  providing rich source information learning language grounding  examples parallel
data include images captions  barnard   forsyth         robocup game events paired
text commentary  chen   mooney         sequences robot motor actions de   

fibranavan  silver    barzilay

scribed natural language  tellex et al          large diversity properties
parallel data resulted development algorithms tailored specific grounding
contexts  instead application independent grounding approach  nevertheless  existing
grounding approaches characterized along several dimensions illuminate
connection algorithms 
representation non linguistic input first step grounding words
perceptual data discretize non linguistic signal  e g   image  representation facilitates alignment  instance  barnard forsyth        segment images regions subsequently mapped words  approaches
intertwine alignment segmentation single step  roy   pentland        
two tasks clearly interrelated  application  segmentation required
state action representation nature discrete 
many approaches move beyond discretization  aiming induce rich hierarchical structures non linguistic input  fleischman   roy        chen   mooney       
       instance  fleischman roy        parse action sequences using
context free grammar subsequently mapped semantic frames  chen
mooney        represent action sequences using first order logic  contrast 
algorithm capitalizes structure readily available data state action
transitions  inducing richer structure state action space may benefit mapping  difficult problem right field hierarchical
planning  barto   mahadevan        
representation linguistic input early grounding approaches used bagof words approach represent input documents  yu   ballard        barnard  
forsyth        fleischman   roy         recent methods relied richer
representation linguistic data  syntactic trees  chen   mooney       
semantic templates  tellex et al          method incorporates linguistic information multiple levels  using feature based representation encodes words
well syntactic information extracted dependency trees  shown
results  richer linguistic representations significantly improve model performance 
alignment another common feature existing grounding models training
procedure crucially depends well words aligned non linguistic structures 
reason  models assume alignment provided part training
data  fleischman   roy        tellex et al          grounding algorithms 
alignment induced part training procedure  examples approaches
methods barnard forsyth         liang et al         
models jointly generate text attributes grounding context  treating
alignment unobserved variable 
contrast  explicitly model alignment model due lack
parallel data  instead  aim extract relevant information text infuse
control application 
   

filearning win reading manuals monte carlo framework

      learning grounding control feedback
recent work moved away reliance parallel corpora  using control feedback primary source supervision  assumption behind setup
textual information used drive control application  applications performance
correlate quality language analysis  assumed performance measurement obtained automatically  setup conducive reinforcement learning
approaches estimate model parameters feedback signal  even noisy
delayed 
one line prior work focused task mapping textual instructions
policy control application  assuming text fully specifies actions executed environment  example  previous work  branavan et al               
approach applied task translating instructions computer manual
executable gui actions  vogel jurafsky        demonstrate grounding
framework effectively map navigational directions corresponding path map 
second line prior work focused full semantic parsing converting given text
formal meaning representation first order logic  clarke et al         
methods applied domains correctness output accurately evaluated based control feedback example  output database
query executed provides clean  oracle feedback signal learning  line
work assumes text fully specifies required output 
method driven control feedback  language interpretation task
fundamentally different  assume given text document provides highlevel advice without directly describing correct actions every potential game state 
furthermore  textual advice necessarily translate single strategy fact 
text may describe several strategies  contingent specific game states 
reason  strategy text cannot simply interpreted directly policy  therefore 
goal bias learned policy using information extracted text  end 
aim achieve complete semantic interpretation  rather use partial text analysis
compute features relevant control application 
    language analysis games
even though games provide rich domain situated text analysis 
prior attempts leveraging opportunity  gorniak   roy        eisenstein 
clarke  goldwasser    roth        
eisenstein et al         aim automatically extract information collection
documents help identify rules game  information  represented predicate logic formulae  estimated unsupervised fashion via generative model 
extracted formulae  along observed traces game play subsequently fed inductive logic program  attempts reconstruct rules game 
high level  goal similar  i e   extract information text useful external
task  several key differences  firstly  eisenstein et al         analyze
text game two disjoint steps  model tasks integrated fashion 
allows model learn text analysis pertinent game play  time
using text guide game play  secondly  method learns text analysis game
   

fibranavan  silver    barzilay

play feedback signal inherent game  avoiding need pre compiled game
traces  enables method operate effectively complex games collecting
sufficiently representative set game traces impractical 
gorniak roy        develop machine controlled game character responds
spoken natural language commands  given traces game actions manually annotated
transcribed speech  method learns structured representation text
aligned action sequences  learned model used interpret spoken instructions
grounding actions human player current game state 
method learn play game  enables human control additional game
character via speech  contrast gorniak roy         aim develop algorithms
fully autonomously control actions one player game  furthermore 
method operates games user manual rather human provided  contextually
relevant instructions  requires model identify text contains information
useful current game state  addition mapping text productive actions 
finally  method learns game feedback collected via active interaction without
relying manual annotations  allows us effectively operate complex games
collecting traditional labeled traces would prohibitively expensive 
    monte carlo search game ai
monte carlo search  mcs  state of the art framework successfully
applied  prior work  playing complex games go  poker  scrabble  real time
strategy games  gelly  wang  munos    teytaud        tesauro   galperin        billings 
castillo  schaeffer    szafron        sheppard        schafer        sturtevant        balla
  fern         framework operates playing simulated games estimate goodness value different candidate actions  games state action spaces
complex  number simulations needed effective play become prohibitively large 
previous application mcs addressed issue using two orthogonal techniques     
leverage domain knowledge either guide prune action selection      estimate
value untried actions based observed outcomes simulated games  estimate used bias action selection  mcs based algorithm games relies
techniques  describe differences application
techniques prior work 
      leveraging domain knowledge
domain knowledge shown critically important achieving good performance mcs complex games  prior work achieved manually
encoding relevant domain knowledge game playing algorithm example  via
manually specified heuristics action selection  billings et al         gelly et al         
hand crafted features  tesauro   galperin         value functions encoding expert
knowledge  sturtevant         contrast approaches  goal automatically extract use domain knowledge relevant natural language documents  thus
bypassing need manual specification  method learns text interpretation
game action selection based outcomes simulated games mcs  allows
identify leverage textual domain knowledge relevant observed game context 
   

filearning win reading manuals monte carlo framework

action selection
according
policy function

stochastic state
transition according
distribution

figure    markov decision process  actions selected according policy function  s  a 
given current state s  execution selected action ai  e g   a     causes
mdp transition new state s  according stochastic state transition
distribution  s    s  a  

      estimating value untried actions
previous approaches estimating value untried actions relied two techniques 
first  upper confidence bounds tree  uct  heuristic used concert
monte carlo tree search variant mcs  augments actions value exploration
bonus rarely visited state action pairs  resulting better action selection better
overall game performance  gelly et al         sturtevant        balla   fern        
second technique learn linear function approximation action values current
state s  based game feedback  tesauro   galperin        silver  sutton    muller        
even though method follows latter approach  model action value q s  a  via
non linear function approximation  given complexity application domain 
non linear approximation generalizes better linear one  shown results
significantly improves performance  importantly  non linear model enables
method represent text analysis latent variables  allowing use textual information
estimate value untried actions 

   monte carlo search
task leverage textual information help us win turn based strategy game
given opponent  section  first describe monte carlo search framework
within method operates  details linguistically informed monte carlo
search algorithm given section   
    game representation
formally  represent given turn based stochastic game markov decision process
 mdp   mdp defined   tuple hs  a  t  ri 
state space  s  set possible states  state represents complete
configuration game in between player turns 
action space  a  set possible actions  turn based strategy game 
player controls multiple game units turn  thus  action represents
joint assignment unit actions executed current player turn 
   

fibranavan  silver    barzilay

transition distribution   s    s  a   probability executing action state
result state s  next game turn  distribution encodes way
game state changes due game rules  opposing players actions 
reason   s    s  a  stochastic shown figure    executing
action given state result different outcomes s   
reward function  r s  r  immediate reward received transitioning
state s  value reward correlates goodness actions executed
now  higher reward indicating better actions 
aspects mdp representation game i e   s  a     r  
defined implicitly game rules  step game  game playing
agent observe current game state s  select best possible action a 
agent executes action a  game state changes according state transition
distribution   s    s  a  known priori  state transitions sampled
distribution invoking game code black box simulator i e   playing
game  action  agent receives reward according reward function r s  
game playing setup  value reward indication chances winning
game state s  crucially  reward signal may delayed i e   r s  may
non zero value game ending states win  loss  tie 
game playing agent selects actions according stochastic policy  s  a  
specifies probability selecting action state s  expected total reward
executing action state s  following policy termed action value function
q  s  a   goal find optimal policy  s  a  maximizes expected
total reward i e   maximizes chances winning game  optimal action value

function q  s  a  known  optimal game playing behavior would select action

highest q  s  a   may computationally hard find optimal policy

 s  a  q  s  a   many well studied algorithms available estimating effective
approximation  sutton   barto        
    monte carlo framework computer games
monte carlo search algorithm  shown figure    simulation based search paradigm
dynamically estimating action values q  s  a  given state st  see algorithm  
pseudo code   estimate based rewards observed multiple roll outs 
simulated game starting state st    specifically  roll out 
algorithm starts state st   repeatedly selects executes actions according
simulation policy  s  a   sampling state transitions  s    s  a   game completion
time   final reward r s   measured  action value function updated
accordingly   monte carlo control  sutton   barto         updated action value
   monte carlo search assumes possible play simulated games  simulations may
played heuristic ai player  experiments  built in ai game used
opponent 
   general  roll outs run game completion  simulations expensive  case
domain  roll outs truncated fixed number steps  however depends availability
approximate reward signal truncation point  experiments  use built in score
game reward  reward noisy  available every stage game 

   

filearning win reading manuals monte carlo framework

game

copy game
state
simulator

apply action
best simulation
outcome game
single
simulation
rollout

update rollout
policy
game feedback
rollout

simulation

simulation

figure    overview monte carlo search algorithm  game state st   independent set simulated games roll outs done find best possible game
action   roll out starts state st   actions selected according
simulation policy  s  a   policy learned roll outs
roll outs improving policy  turn improves roll out action selection  process repeated every actual game state  simulation
policy relearned scratch time 

function q  s  a  used define improved simulation policy  thereby directing subsequent roll outs towards higher scoring regions game state space  fixed number
roll outs performed  action highest average final reward
simulations selected played actual game state st   process repeated
state encountered actual game  action value function
relearned scratch new game state   simulation policy usually selects
actions maximize action value function  however  sometimes valid actions
randomly explored case valuable predicted current es   conceivable sharing action value function across roll outs different game states
would beneficial  empirically case experiments  one possible reason
domain  game dynamics change radically many points game e g  
new technology becomes available  change occurs  may actually detrimental play
according action value function previous game step  note however  action value
function indeed shared across roll outs single game state st   parameters updated
successive roll outs  learned model helps improve roll out action selection  thereby
improves game play  setup relearning scratch game state shown
beneficial even stationary environments  sutton  koop    silver        

   

fibranavan  silver    barzilay

timate q  s  a   accuracy q  s  a  improves  quality action selection
improves vice versa  cycle continual improvement  sutton   barto        
success monte carlo search depends ability make fast  local estimate
action value function roll outs collected via simulated play  however games
large branching factors  may feasible collect sufficient roll outs  especially
game simulation computationally expensive  thus crucial learned
action value function generalizes well small number roll outs i e   observed
states  actions rewards  one way achieve model action value function
linear combination state action attributes 
q  s  a    w
  f  s  a  
f  s  a  rn real valued feature function  w
  weight vector  prior work
shown linear value function approximations effective monte carlo search
framework  silver et al         
note learning action value function q s  a  monte carlo search related
reinforcement learning  rl   sutton   barto         fact  approach  use
standard gradient descent updates rl estimate parameters q s  a   is 
however  one crucial difference two techniques  general  goal rl
find q s  a  applicable state agent may observe existence 
monte carlo search framework  aim learn q s  a  specialized current state
s  essence  q s  a  relearned every observed state actual game  using
states  actions feedback simulations  relearning may seem suboptimal 
two distinct advantages  first  since q s  a  needs model current state 
representationally much simpler global action value function  second  due
simpler representation  learned fewer observations global actionvalue function  sutton et al          properties important state
space extremely large  case domain 

   adding linguistic knowledge monte carlo framework
goal work improve performance monte carlo search framework
described above  using information automatically extracted text  section 
describe achieve terms model structure parameter estimation 
    model structure
achieve aim leveraging textual information improve game play  method
needs perform three tasks      identify sentences relevant current game state     
label sentences predicate structure      predict good game actions combining
game features text features extracted via language analysis steps  first describe
tasks modeled separately showing integrate
single coherent model 
   

filearning win reading manuals monte carlo framework

procedure playgame   
initialize game state fixed starting state
s  s 
         
run n simulated games
          n
 ai   ri   simulategame  st  
end
compute average observed utility action
  x
arg max
ri
na

i ai  a

execute selected action game
st    s    st    
end

procedure simulategame  st  
u        
compute q function approximation
q  su   a    w
  f  su   a 
sample action action value function  greedy fashion 

uniform  a a 
probability
au  su   a   

arg max q  su   a  otherwise


execute selected action game 
su    s    su   au  
game lost
break
end
update parameters w
  q  st   a 
return action observed utility 
return   r s  
algorithm    general monte carlo algorithm 
   

fibranavan  silver    barzilay

      modeling sentence relevance
discussed section    small fraction strategy document likely provide
guidance relevant current game context  therefore  effectively use information
given document d  first need identify sentence yi relevant
current game state action a   model decision log linear distribution 
defining probability yi relevant sentence as 
 

p y   yi  s  a  d  e u yi  s a d   

   

    s  a  d  rn feature function   u parameters need estimate 
 y
 
function   
encodes features combine attributes sentence yi
attributes game state action  features allow model learn correlations
game attributes attributes relevant sentences 
      modeling predicate structure
using text guide action selection  addition using word level correspondences 
would leverage information encoded structure sentence 
example  verbs sentence might likely describe suggested game actions 
aim access information inducing task centric predicate structure
sentences  is  label words sentence either action description  statedescription background  given sentence precomputed dependency parse q 
model word by word labeling decision log linear fashion i e   distribution
predicate labeling z sentence given by 
p z  y  q    p  e  y  q 

 
p ej  j  y  q  

   

j
 

p ej  j  y  q  e v ej  j y q   
  j   j  y  q  rn  
ej predicate label j th word  feature function  e
addition encoding word type part of speech tag  includes dependency parse
information word  features allow predicate labeling decision condition
syntactic structure sentence 
      modeling action value function
relevant sentence identified labeled predicate structure 
algorithm needs use information along attributes current game state
select best possible game action a  end  redefine action value function
q s  a  weighted linear combination features game text information 
q s    a      w
  f  s  a  yi   zi   

   

   use approximation selecting single relevant sentence alternative combining
features sentences text  weighted relevance probability p y   yi  s  a  d  
setup computationally expensive one used here 

   

filearning win reading manuals monte carlo framework

input layer 

deterministic feature
layer 

output layer

hidden layer encoding
sentence relevance
hidden layer encoding
predicate labeling

figure    structure neural network model  rectangle represents collection
units layer  shaded trapezoids show connections layers 
fixed  real valued feature function  x s  a  d  transforms game state s  action
a  strategy document input vector  x  second layer contains
two disjoint sets hidden units  y  z   y encodes sentence relevance
decisions   z predicate labeling  softmax layers  one
unit active time  units third layer f  s  a  yi   zi   set
fixed real valued feature functions s  a  active units yi zi  y
 z respectively 

s    hs  di  a    ha  yi   zi i  w
  weight vector  f  s  a  yi   zi   rn feature
function state s  action a  relevant sentence yi   predicate labeling zi  
structure action value function allows explicitly learn correlations
textual information  game states actions  action maximizes q s  a 
selected best action state s   
  arg max q s  a  


      complete joint model
two text analysis models  action value function described form three
primary components text aware game playing algorithm  construct single
principled model components representing via different layers
multi layer neural network shown figure    essentially  text analysis decisions
modeled latent variables second  hidden layer network  final
output layer models action value function 
   note select action based q s  a   depends relevant sentence yi   sentence
selected conditioned action a  may look cyclic dependency actions
sentence relevance  however  case since q s  a   therefore sentence relevance
p y s  a  d   computed every candidate action a  actual game action selected
estimate q s  a  

   

fibranavan  silver    barzilay

input layer  x neural network encodes inputs model i e  
current state s  candidate action a  document d  second layer consists two
disjoint sets hidden units  y  z  set operates stochastic   of n softmax
selection layer  bridle         activation function units layer standard
softmax function 
 x
p yi      x    e ui  x
e uk  x  
k

ith

yi
hidden unit  y    ui weight vector corresponding yi   k
number units layer  given activation function mathematically
equivalent log linear distribution  layers  y  z operate log linear models 
node activation softmax layer simulates sampling log linear distribution 
use layer  y replicate log linear model sentence relevance equation     
node yi representing single sentence  similarly  unit zi layer  z represents
complete predicate labeling sentence  equation      
third feature layer f  neural network deterministically computed given
active units yi zi softmax layers  values input layer  unit
layer corresponds fixed feature function fk  s  a  yi   zi   r  finally output layer
encodes action value function q s  a  weighted linear combination units
feature layer  thereby replicating equation     completing joint model 
example kind correlations learned model  consider figure   
here  relevant sentence already selected given game state  predicate
labeling sentence identified words irrigate settler describing
action take  game roll outs return higher rewards irrigate action
settler unit  model learn association action words
describe it  similarly  learn association state description words
feature values current game state e g   word city binary feature nearcity  allows method leverage automatically extracted textual information
improve game play 
    parameter estimation
learning method performed online fashion  game state st  
algorithm performs simulated game roll out  observes outcome simulation 
updates parameters  u   v w
  action value function q st      shown
figure    three steps repeated fixed number times actual game state 
information roll outs used select actual game action 
algorithm relearns parameters action value function every new game state
st   specializes action value function subgame starting st   learning
specialized q st     game state common useful games complex
state spaces dynamics  learning single global function approximation
particularly difficult  sutton et al          consequence function specialization
need online learning since cannot predict games states seen
   intention incorporate  action value function  information relevant
sentence  therefore  practice  perform predicate labeling sentence selected
relevance component model 

   

filearning win reading manuals monte carlo framework

settlers unit  candidate action   
plains

features 
action   irrigate action word    irrigate 
action   irrigate state word    land 
action   irrigate terrain   plains
action   irrigate unit type   settler
state word    city  near city   true

city

settlers unit  candidate action   

settler unit

relevant text   use settlers irrigate land near city 
predicted action words 

 irrigate    settler 

predicted state words 

 land    near    city 

irrigate

features 
action   build city
action   build city
action   build city
action   build city
state word    city 







build city

action word    irrigate 
state word    land 
terrain   plains
unit type   settler
near city   true

figure    example text game attributes  resulting candidate action features 
left portion game state arrows indicating game attributes 
left sentence relevant game state along action
state words identified predicate labeling  right two candidate
actions settler unit along corresponding features  mentioned
relevant sentence  irrigate better two actions executing
lead future higher game scores  feedback features shown
allow model learn effective mappings actionword irrigate action irrigate  state word city game
attribute near city 

testing  function specialization states cannot done priori  ruling
traditional training test separation 
since model non linear approximation underlying action value function
game  learn model parameters applying non linear regression observed final
utilities simulated roll outs  specifically  adjust parameters stochastic
gradient descent  minimize mean squared error action value q s  a 
final utility r s   observed game state action a  resulting update
model parameters form 

   r s   q s  a   
 
   r s   q s  a   q s  a    
learning rate parameter  minimization performed via standard error
backpropagation  bryson   ho        rumelhart  hinton    williams         resulting
following online parameter updates 
w
  w
    w  q r s    f  s  a  yi   zj   
 ui  ui   u  q r s    q  x    p yi     
 vi  vi   v  q r s    q  x    p zi     
   

fibranavan  silver    barzilay

w learning rate  q   q s  a   w 
   ui  vi parameters
final layer  sentence relevance layer predicate labeling layer respectively 
derivations update equations given appendix

   applying model
game test model on  civilization ii  multi player strategy game set either
earth randomly generated world  player acts ruler one civilization 
starts game units i e   two settlers  two workers one explorer 
goal expand civilization developing new technologies  building cities new
units  win game either controlling entire world  successfully sending
spaceship another world  map game world divided grid typically
     squares  grid location represents tile either land sea  figure   shows
portion world map particular instance game  along game
units one player  experiments  consider two player game civilization ii
map      squares smallest map allowed freeciv  map size used
novice human players looking easier game  well advanced players wanting
game shorter duration  test algorithms built in ai player
game  difficulty level default normal setting  
    game states actions
define game state monte carlo search  map game world  along
attributes map tile  location attributes players cities
units  examples attributes shown figure    space possible
actions city unit defined game rules given current game state 
example  cities construct buildings harbors banks  create new units
various types  individual units move around grid  perform unit
specific actions irrigation settlers  military defense archers  since
player controls multiple cities units  players action space turn defined
combination possible actions cities units  experiments 
average  player controls approximately    units unit    possible actions 
resulting action space player large i e          effectively deal
large action space  assume given state  actions individual city
unit independent actions cities units player   
time  maximize parameter sharing using single action value function
cities units player 

   freeciv five difficulty settings  novice  easy  normal  hard cheating  evidenced discussions games online forum  http   freeciv wikia com index php title forum playing freeciv  
human players new game find even novice setting hard 
    since player executes game actions turn  i e  opposing units fixed individual players
turn  opponents moves enlarge players action space 

   

filearning win reading manuals monte carlo framework

figure    portion game map one instance civilization ii game  three
cities  several units single player visible map  visible
different terrain attributes map tiles  grassland  hills  mountains
deserts 

nation attributes 
 

city attributes 
 

amount gold treasury
  world controlled
number cities
population
known technologies

map tile attributes 
 

city population
surrounding terrain resources
amount food   resources produced
number units supported city
number   type units present

unit attributes 

terrain type  e g  grassland  mountain  etc 
tile resources  e g  wheat  coal  wildlife  etc 
tile river
construction tile  city  road  rail  etc 
types units  own enemy  present

 

unit type  e g   worker  explorer  archer  etc 
unit health   hit points
unit experience
unit city 
unit fortied 

figure    example attributes game state 

   

fibranavan  silver    barzilay

    utility function
critically important monte carlo search algorithm  availability utility
function evaluate outcomes simulated game roll outs  typical application algorithm  final game outcome terms victory loss used
utility function  tesauro   galperin         unfortunately  complexity civilization
ii  length typical game  precludes possibility running simulation roll outs
game completion  game  however  provides player real valued game
score  noisy indicator strength civilization  since playing
two player game  players score relative opponents used utility
function  specifically  use ratio game score two players   
    features
components method operate features computed basic set text
game attributes  text attributes include words sentence along
parts of speech dependency parse information dependency types parent
words  basic game attributes encode game information available human players
via games graphical user interface  examples attributes shown
figure   
identify sentence relevant current game state candidate action 
sentence relevance component computes features combined basic attributes
  two types first
game sentence text  features  
computes cartesian product attributes game attributes
candidate sentence  second type consists binary features test overlap
words candidate sentence  text labels current game state
candidate action  given      word tokens manual overlap
labels game  similarity features highly sparse  however  serve
signposts guide learner shown results  method able operate
effectively even absence features  performs better present 
predicate labeling  unlike sentence relevance  purely language task
  compute
operates basic text attributes  features component   
cartesian product candidate predicate label words type  part of speech
tag  dependency parse information  final component model  action value
approximation  operates attributes game state  candidate action 
sentence selected relevant  predicate labeling sentence  features
layer  f   compute three way cartesian product attributes candidate
action  attributes game state  predicate labeled words relevant
 
  f  compute approximately                         features
sentence  overall   
respectively resulting total         features full model  figure   shows
examples features 

    difference players scores used utility function  however  practice
score ratio produced better empirical performance across algorithms baselines 

   

filearning win reading manuals monte carlo framework

sentence relevance features 
  action   build city
  tile has river   true
  word    build 

  action   irrigate
  tile is next to city   true
  word    irrigate 

  otherwise

  otherwise

predicate labeling features 
  label   action
  word    city 
  parent word    build 

  label   state
  word    city 
  parent label    near 

  otherwise

  otherwise

action value features 
  action   build city
  tile has river   true
  action word    build 
  state word    river 

  action   irrigate
  tile terrain   plains
  action word    irrigate 
  state word    city 

  otherwise

  otherwise

figure    examples features used model  feature  conditions
test game attributes highlighted blue  test words
game manual highlighted red 

   experimental setup
section  describe datasets  evaluation metrics  experimental framework
used test performance method various baselines 
    datasets
use official game manual civilization ii strategy guide document   
text manual uses vocabulary      word types  composed      sentences 
average      words long  manual contains information rules
game  game user interface  basic strategy advice different aspects
game  use stanford parser  de marneffe  maccartney    manning        
default settings  generate dependency parse information sentences game
manual 
    experimental framework
apply method civilization ii game  use games open source reimplementation freeciv    instrumented freeciv allow method programmatically
    www civfanatics com content civ  reference civ manual zip
    http   freeciv wikia com  game version    

   

fibranavan  silver    barzilay

primary game
monte carlo
player

game
server

modied game
gui client

in memory
file system

game simulation  

game
strategy guide

game
server

modied game
gui client

game state

game simulation  
game
server

modied game
gui client

game simulation  
game
server

modied game
gui client

figure    diagram experimental framework  showing monte carlo player 
server primary game playing aims win  multiple game
servers simulated play  communications multiple processes comprising framework via unix sockets in memory file system 

control game i e   measure current game state  execute game actions 
save load current game state  start end games   
across experiments  start game initial state run    
steps  step  perform     monte carlo roll outs  roll out run   
simulated game steps halting simulation evaluating outcome  note
simulated game step  algorithm needs select action game unit 
given average number units per player     results         decisions
    roll outs  pairing decisions corresponding roll out
outcome used datapoint update model parameters  use fixed learning rate
       experiments  method  baselines  run    
independent games manner  evaluations averaged across     runs 
use experimental settings across methods  model parameters
initialized zero 
experimental setup consists monte carlo player  primary game
aim play win  set simulation games  primary game simula    addition instrumentation  code freeciv  both server client  changed increase
simulation speed several orders magnitude  remove bugs caused game crash 
best knowledge  game rules functionality identical unmodified freeciv
version    

   

filearning win reading manuals monte carlo framework

tions simply separate instances freeciv game  instance freeciv game
made one server process  runs actual game  one client process 
controlled monte carlo player  start roll out  simulations
initialized current state primary game via game save reload functionality
freeciv  figure   shows diagram experimental framework 
experiments run typical desktop pcs single intel core i  cpus   
hyper threaded cores per cpu   algorithms implemented execute   simulation
roll outs parallel connecting   independent simulation games  computational
setup  approximately   simulation roll outs executed per second full model 
single game     steps runs   hours  since treat freeciv game code
black box  special care taken ensure consistency across experiments  code
compiled one specific machine  single fixed build environment  gcc        
experiments run identical settings fixed set machines running fixed
os configuration  linux kernel            libc         

    evaluation metrics

wish evaluate two aspects method  well improves game play leveraging textual information  accurately analyzes text learning game feedback 
evaluate first aspect comparing method various baselines terms
percentage games built in ai freeciv  ai fixed heuristic
algorithm designed using extensive knowledge game  intention challenging human players    such  provides good open reference baseline  evaluate
method measuring percentage games won  averaged     independent runs 
however  full games sometimes last multiple days  making difficult extensive analysis model performance contributing factors  reason  primary
evaluation measures percentage games within first     game steps  averaged
    independent runs  evaluation underestimate model performance
game player gaining control entire game map within
    steps considered loss  since games remain tied     steps  two equally
matched average players  playing other  likely win rate close
zero evaluation 

   results

adequately characterize performance method  evaluate respect
several different aspects  section  first describe game playing performance
analyze impact textual information  then  investigate quality text
analysis produced model terms sentence relevance predicate labeling 
   

fibranavan  silver    barzilay

method
random
built in ai
game
latent variable
full model
randomized text

  win
 
 
    
    
    
    

  loss
   
 
   
   
   
   

std  err 


   
   
   
   

table    win rate method several baselines within first     game steps 
playing built in game ai  games neither lost still
ongoing  models win rate statistically significant baselines 
results averaged across     independent game runs  standard errors shown
percentage wins 

method
game
latent variable
full model

  wins
    
    
    

standard error
   
   
   

table    win rate method two text unaware baselines built in ai 
results averaged across     independent game runs 

    game performance
table   shows performance method several baselines primary     step
evaluation  scenario  language aware monte carlo algorithm wins average
      games  substantially outperforming baselines  best non languageaware method win rate        dismal performance random baseline
games built in ai  playing itself  indications difficulty
winning games within first     steps  shown table    evaluated full length
games  method win rate       compared       best text unaware
baseline   

    ai constrained follow rules game  access information typically
available human players  information technology  cities units opponents 
methods hand restricted actions information available human players 
    note performance methods full games different listed previous
publications  branavan  silver    barzilay      a      b   previous numbers biased
code flaw freeciv caused game sporadically crash middle game play 
originally believed crash random  subsequently discovered happen often losing
games  thereby biasing win rates methods upwards  numbers presented
game bug fixed  crashes observed experiments 

   

fiobserved game score

learning win reading manuals monte carlo framework

monte carlo rollouts

figure     observed game score function monte carlo roll outs text aware
full model  text unaware latent variable model  model parameters
updated roll out  thus performance improves roll outs 
seen  full models performance improves dramatically small number
roll outs  demonstrating benefit derives textual information 

      textual advice game performance
verify characterize impact textual advice models performance 
compare several baselines access textual information 
simplest methods  game only  models action value function q s  a  linear
approximation games state action attributes  non text aware method wins
      games  see table     confirm methods improved performance
simply due inherently richer non linear approximation  evaluate two
ablative non linear baselines  first these  latent variable extends linear actionvalue function game set latent variables  essence four layer
neural network  similar full model  second layers units activated
based game information  baseline wins       games  table     significantly
improving linear game baseline  still trailing text aware method
     second ablative baseline  randomized text  identical model 
except given randomly generated document input  generate document
randomly permuting locations words game manual  thereby maintaining
documents statistical properties terms type frequencies  ensures
number latent variables baseline equal full model  thus 
baseline model capacity equal text aware method access
textual information  performance baseline  wins       games 
confirms information extracted text indeed instrumental performance
method 
   

fibranavan  silver    barzilay

figure    provides insight textual information helps improve game performance
shows observed game score monte carlo roll outs full model
latent variable baseline  seen figure  textual information guides
model high score region search space far quicker non text aware
method  thus resulting better overall performance  evaluate performance
method varies amount available textual information  conduct
experiment random portions text given algorithm  shown
figure     methods performance varies linearly function amount text 
randomized text experiment corresponding point information
available text 
      impact seed vocabulary performance
sentence relevance component model uses features compute similarity
words sentence  text labels game state action  assumes
availability seed vocabulary names game attributes  domain     
unique text labels present game      occur vocabulary game manual 
results sparse seed vocabulary     words  covering      word types
     word tokens manual  despite sparsity  seed vocabulary
potentially large impact model performance since provides initial set word
groundings  evaluate importance initial grounding  test method
empty seed vocabulary  setup  full model wins       games  showing
seed words important  method operate effectively
absence 
      linguistic representation game performance
characterize contribution language game performance  conduct series
evaluations vary type complexity linguistic analysis performed
method  results evaluation shown table    first these  sentence
relevance  highlights contributions two language components model 
algorithm  identical full model lacks predicate labeling component 
wins       games  showing essential identify textual advice relevant
current game state  deeper syntactic analysis extracted text substantially
improves performance 
evaluate importance dependency parse information language analysis 
vary type features available predicate labeling component model 
first ablative experiments  dependency information  removes dependency
features leaving predicate labeling operate word type features  performance
baseline  win rate        clearly shows dependency features crucial
model performance  remaining three methods dependency label  dependency
parent pos tag dependency parent word drop dependency feature
named after  contribution features model performance seen
table   
   

fiwin rate

learning win reading manuals monte carlo framework

random
text

percentage document text given model

figure     performance text aware model function amount text
available it  construct partial documents randomly sub sampling sentences full game manual  x axis shows amount sentences
given method ratio full text  leftmost extreme
performance randomized text baseline  showing fits
performance trend point useful textual information 
method
full model
sentence relevance
dependency information
dependency label
depend  parent pos tag
depend  parent word

  win
    
    
    
    
    
    

  loss
   
   
   
   
   
   

std  err 
   
   
   
   
   
   

table    win rates several ablated versions model  showing contribution
different aspects textual information game performance  sentence relevance
identical full model  except lacks predicate labeling component 
four methods bottom table ablate specific dependency features
 as indicated methods name  predicate labeling component
full model 

      model complexity vs computation time trade off
one inherent disadvantage non linear models  compared simpler linear models 
increase computation time required parameter estimation  monte carlo
search setup  model parameters re estimated simulated roll out  therefore 
given fixed amount time  roll outs done simpler faster model 
nature  performance monte carlo search improves number rollouts  trade off model complexity roll outs important since simpler
   

fibranavan  silver    barzilay

   
full model
latent variable
game
ro
llo

ut



   

 



ts

  

 

  

ut

 r
oll
 ou

   

  

win rate

   


llro

   

   

  

 

  

  

  

  

   

   

   

computation time per game step  seconds 

figure     win rate function computation time per game step  montecarlo search method  win rate computation time measured     
        roll outs per game step  respectively 

model could compensate using roll outs  thereby outperform complex
ones  scenario particularly relevant games players limited amount
time turn 
explore trade off  vary number simulation roll outs allowed
method game step  recording win rate average computation time per
game  figure    shows results evaluation              roll outs 
complex methods higher computational demands  results clearly show
even given fixed amount computation time per game step  text aware
model still produces best performance wide margin 
      learned game strategy
qualitatively  methods described learn basic rush strategy  essentially 
attempt develop basic technologies  build army  take opposing cities
quickly possible  performance difference different models essentially
due well learn strategy 
two basic reasons algorithms learn rush strategy  first  since
attempting maximize game score  methods implicitly biased towards finding
fastest way win happens rush strategy playing
built in ai civilization    second  complex strategies typically require
coordination multiple game units  since models assume game units independent 
   

filearning win reading manuals monte carlo framework

phalanxes twice eective defending cities warriors 
build city plains grassland river running it 




rename city like  we ll refer washington 
many dierent strategies dictating order
advances researched

road built  use settlers start improving terrain 
















settlers becomes active  chose build road 












use settlers engineers improve terrain square within city radius
























figure     examples methods sentence relevance predicate labeling decisions 
box shows two sentences  identified green check marks 
predicted relevant  two not  box shows
predicted predicate structure three sentences  indicating state
description a action description background words unmarked  mistakes
identified crosses 

cannot explicitly learn coordination putting many complex strategies beyond
capabilities algorithms 
    accuracy linguistic analysis
described section    text analysis method tightly coupled game playing
terms modeling  terms learning game feedback  seen
results thus far  text analysis indeed help game play  section
focus game driven text analysis itself  investigate well conforms
common notions linguistic correctness  comparing model predictions
sentence relevance predicate labeling manual annotations 
      sentence relevance
figure    shows examples sentence relevance decisions produced method 
evaluate accuracy decisions  would ideally use ground truth
relevance annotation games user manual  however  impractical since
relevance decision dependent game context  hence specific time step
game instance  therefore  evaluate sentence relevance accuracy using synthetic
document  create document combining original game manual equal
   

fisentence relevance accuracy

branavan  silver    barzilay

   
   
   
   
sentence relevance
moving average

   
 

  

  

  

  

   

game step

figure     accuracy methods sentence relevance predictions  averaged     independent runs 

number sentences known irrelevant game  sentences
collected randomly sampling wall street journal corpus  marcus  santorini 
  marcinkiewicz           evaluate sentence relevance synthetic document
measuring accuracy game manual sentences picked relevant 
evaluation  method achieves average accuracy        given
model differentiate game manual text wall street journal 
number may seem disappointing  furthermore  seen figure    
sentence relevance accuracy varies widely game progresses  high average
      initial    game steps  reality  pattern high initial accuracy followed lower average entirely surprising  official game manual civilization
ii written first time players  such  focuses initial portion game 
providing little strategy advice relevant subsequent game play    reason
observed sentence relevance trend  would expect final layer neural
network emphasize game features text features first    steps game 
indeed case  seen figure    
test hypothesis  perform experiment first n steps
game played using full model  subsequent     n steps played without
using textual information  results evaluation several values n
given figure     showing initial phase game indeed information
game manual useful  fact  hybrid method performs well
full model n       achieving       win rate  shows method
    note sentences wsj corpus contain words city potentially confuse
algorithm  causing select sentences relevant game play 
    reminiscent opening books games chess go  aim guide player
playable middle game  without providing much information subsequent game play 

   

filearning win reading manuals monte carlo framework

   

game features dominate

   

text features dominate

text feature importance

   

 
  

  

  

  

   

game step

figure     difference norms text features game features
output layer neural network  beyond initial    steps game 
method relies increasingly game features 

win rate

   

   

   

  
  

  

  

  

   

  initial game steps text information used

figure     graph showing availability textual information initial steps
game affects performance full model  textual information
given model first n steps  the x axis   beyond point
algorithm access text  becomes equivalent latent variable
model i e   best non text model 

able accurately identify relevant sentences information contain
pertinent game play  likely produce better game performance 
   

fibranavan  silver    barzilay

method
random labeling
model  first     steps
model  first    steps

s a b
     
     
     

s a
     
     
     

table    predicate labeling accuracy method random baseline  column
s a b shows performance three way labeling words state  action
background  column s a shows accuracy task differentiating
state action words 
game attribute

word

state  grassland

 city 

state  grassland

 build 

state  hills

 build 

action  settlers build city

 city 

action  set research

 discovery 

action  settlers build city

 settler 

action  settlers goto location

 build 

action  city build barracks

 construct 

action  research alphabet

 develop 

action  set research

 discovery 

figure     examples word game attribute associations learned via feature
weights model 
      predicate labeling
figure    shows examples predicate structure output model  evaluate
accuracy labeling comparing gold standard annotation game
manual    table   shows performance method terms accurately labels
words state  action background  accurately differentiates state
action words  addition showing performance improvement random
baseline  results display clear trend  evaluations  labeling accuracy
higher initial stages game  expected since model relies
heavily textual features beginning game  see figure     
verify usefulness methods predicate labeling  perform final set
experiments predicate labels selected uniformly random within full model 
random labeling results win rate     performance similar sentence
relevance model uses predicate information  confirms method
able identify predicate structure which  noisy  provides information relevant
game play  figure    shows examples textual information grounded
game  way associations learned words game attributes final
layer full model  example  model learns strong association
    note ground truth labeling words either action description  state description  background
based purely semantics sentence  independent game state  reason 
manual annotation feasible  unlike case sentence relevance 

   

filearning win reading manuals monte carlo framework

game state attribute grassland words city build  indicating textual
information building cities maybe useful players unit near grassland 

   conclusions
paper presented novel approach improving performance control
applications leveraging information automatically extracted text documents 
time learning language analysis based control feedback  model biases
learned strategy enriching policy function text features  thereby modeling
mapping words manual state specific action selection  effectively learn
grounding  model identifies text relevant current game state  induces
predicate structure text  linguistic decisions modeled jointly using
non linear policy function trained monte carlo search framework 
empirical results show model able significantly improve game win rate
leveraging textual information compared strong language agnostic baselines 
demonstrate despite increased complexity model  knowledge
acquires enables sustain good performance even number simulations
reduced  moreover  deeper linguistic analysis  form predicate labeling text 
improves game play  show information syntactic structure
text crucial analysis  ignoring information large impact
model performance  finally  experiments demonstrate tightly coupling control
linguistic features  model able deliver robust performance presence
noise inherent automatic language analysis 

bibliographical note
portions work previously presented two conference publications  branavan
et al       a      b   article significantly extends previous work  notably
providing analysis model properties impact linguistic representation
model performance  dependence model bootstrapping conditions  tradeoff models representational power empirical complexity  section    
paper significantly increases volume experiments base
conclusions  addition  provide comprehensive description model  providing
full mathematical derivations supporting algorithm  section     appendix a  

acknowledgments
authors acknowledge support nsf  career grant iis          grant iis          darpa bolt program  hr                 darpa machine reading
program  fa        c       po              batelle  po         microsoft
research new faculty fellowship  thanks anonymous reviewers  michael collins 
tommi jaakkola  leslie kaelbling  nate kushman  sasha rush  luke zettlemoyer 
mit nlp group suggestions comments  opinions  findings  conclusions 
recommendations expressed paper authors  necessarily
reflect views funding organizations 
   

fibranavan  silver    barzilay

appendix a  parameter estimation
parameter model estimated via standard error backpropagation  bryson  
ho        rumelhart et al          derive parameter updates  consider slightly
simplified neural network shown below  network identical model 
sake clarity  single second layer  y instead two parallel second layers
 y  z  parameter updates parallel layers  y  z similar  therefore
show derivation  y addition updates final layer 

model  nodes yi network activated via softmax function 
third layer  f   computed deterministically active nodes second layer
via function  g  yi    x   output q linear combination f  weighted w 
 
p yi        x   ui    

e ui  x
x
 
e uk  x
k

f   

x

 g   x  yi   p yi    x   ui   



q   w
  f  
goal minimize mean squared error e gradient descent  achieve
updating model parameters along gradient e respect parameter  using
general term indicate models parameters  update takes form 
 
 q r    
 
e
 

q
   q r 
 


e  


equation      updates final layer parameters given by 
q
wi

   q r 
w
  f 
wi
   q r   

wi    q r 

   

   

filearning win reading manuals monte carlo framework

since model samples one relevant sentence yi   best predicate labeling
zi   resulting online updates output layer parameters w
  are 
w
  w
    w  q r s    f  s  a  yi   zj   
w learning rate  q   q s  a   updates second layers parameters similar  somewhat involved  again  equation     
ui j

q
ui j

   q r 
w
  f 
ui j
x

w
 
 g   x  yk   p yi    x   uk  
   q r 
ui j
   q r 

k

   q r  w
   g   x  yi  


p yi    x   ui   
ui j

   

considering final term equation separately 

p yi    x   ui    
ui j

e ui  x
 
ui j z


 

z  

x

e uk  x

k

eu   x
e ui  x ui j z
u   x
e
z
z

 
 
 
 
 
 

e ui  x

 ui  x

e
log
z
ui j
z
 ui  x

e

xj
log z
z
ui j
 ui  x

e
  z
xj
z
z ui j
 
 
 ui  x
e
  x  uk  x
xj
e
z
z ui j
k
 ui  x

e
 
 
uk  
x
xj xj e
z
z
 ui  x

e
e ui  x
xj  
 
z
z




   

fibranavan  silver    barzilay

therefore  equation     
ui j


p yi    x   ui  
ui j
 ui  x


e
e ui  x
   q r  w
   g   x  yi  
xj  
z
z
   q r  xj w
   g   x  yi   p yi    x   ui      p yi    x   ui   
   q r  w
   g   x  yi  

   q r  xj q    p yi    x   ui     
q   w
   g   x  yi   p yi    x   ui   

resulting online updates sentence relevance predicate labeling parameters
 u  v are 
 ui  ui   u  q r s    q  x    p yi     
 vi  vi   v  q r s    q  x    p zi     

   

filearning win reading manuals monte carlo framework

appendix b  example sentence relevance predictions
shown portion strategy guide civilization ii  sentences
identified relevant text aware model highlighted green 

choosing location 
building new city  carefully plan place it  citizens
work terrain surrounding city square x shaped pattern  see
city radius diagram showing exact dimensions   area called
city radius  the terrain square settlers standing
becomes city square   natural resources available
population settles affect ability produce food goods  cities built
near water sources irrigate increase crop yields  cities
near mineral outcroppings mine raw materials  hand 
cities surrounded desert always handicapped aridness
terrain  cities encircled mountains find arable cropland
premium  addition economic potential within city s radius 
need consider proximity cities strategic value
location  ideally  want locate cities areas offer combination
benefits   food population growth  raw materials production 
river coastal areas trade  possible  take advantage
presence special resources terrain squares  see terrain   movement
details benefits  
strategic value 
strategic value city site final consideration  city square s
underlying terrain increase defender s strength city
comes attack  circumstances  defensive value
particular city s terrain might important economic value 
consider case continent narrows bottleneck rival
holds side  good defensive terrain  hills  mountains  jungle 
generally poor food production inhibits early growth city 
need compromise growth defense  build city
plains grassland square river running possible 
yields decent trade production gains    percent defense bonus 
regardless city built  city square easier defend
unimproved terrain  city build city walls
improvement  triples defense factors military units stationed
there  also  units defending city square destroyed one time
lose  outside cities  units stacked together destroyed
military unit stack defeated  units fortresses
exception  see fortresses   placing cities seacoast gives
access ocean  launch ship units explore world
transport units overseas  coastal cities  sea power
inhibited 

   

fibranavan  silver    barzilay

appendix c  examples predicate labeling predictions
listed predicate labellings computed text aware method example
sentences game manual  predicted labels indicated words
letters a  s  b action description  state description background respectively 
incorrect labels indicated red check mark  along correct label brackets 
road built  use settlers start improving terrain 
















settlers becomes active  chose build road 












use settlers engineers improve terrain square within city radius

 a 







 s 











bronze working allows build phalanx units


b  s 









order expand civilization   need build cities

 s 





b

b  a 



order protect city   phalanx must remain inside

b s 

b s 



s a 





b a 

soon you ve found decent site   want settlers build

b s 

b s 



b  a 

 b 





permanent settlement   city

 a 



city build city walls improvement

 s 

b  a 







city undefended   move friendly army city capture


b  s 











b

build city terrain square except ocean 


 a 

b  s 



 s 



launch ship units explore world transport units overseas


 a 





b  s 

b  s 



b

city disorder  disband distant military units  return home cities 

 s 







 a 



 a 



change home cities






build wonder discovered advance makes possible


 a 



   







filearning win reading manuals monte carlo framework

appendix d  examples learned text game attribute mappings
shown examples word game attribute associations learned
model  top ten game attributes strongest association feature weight
listed three example words attack  build grassland  fourth
word  settler  seven attributes non zero weights experiments used collect
statistics 

attack

build

phalanx  unit 

worker goto  action 

warriors  unit 

settler autosettle  action 

colossus  wonder 

worker autosettle  action 

city walls  city improvement 

pheasant  terrain attribute 

archers  unit 

settler irrigate  action 

catapult  unit 

worker mine  action 

palace  city improvement 

build city walls  action 

coinage  city production 

build catapult  action 

city build warriors  action 

swamp  terrain attribute 

city build phalanx  action 

grassland  terrain attribute 

grassland

settler

settler build city  action 

settlers  state attribute 

worker continue action  action 

settler build city  action 

pheasant  terrain attribute 

city  state attribute 

city build improvement  action 

grassland  terrain attribute 

city max production  action 

plains  terrain attribute 

settlers  state attribute 

road  terrain attribute 

city max food  action 

workers  state attribute 

settler goto  action 
worker build road  action 
pyramids  city attribute 

   

fibranavan  silver    barzilay

appendix e  features used model
features used predict sentence relevance
following templates used compute features sentence relevance 
word w present sentence 
number words match text label current unit  attribute
immediate neighborhood unit  action consideration 
units type u   e g   worker  word w present sentence 
action type a   e g   irrigate  word w present sentence 
features used predict predicate structure
following templates used compute features predicate labeling words 
label considered word  i e   action  state background  denoted
l 
label l word type w 
label l part of speech tag word t 
label l parent word dependency tree w 
label l dependency type dependency parent word d 
label l part of speech dependency parent word t 
label l word leaf node dependency tree 
label l word leaf node dependency tree 
label l word matches state attribute name 
label l word matches unit type name 
label l word matches action name 
features used model action value function
following templates used compute features action value approximation 
unless otherwise mentioned  features look attributes player controlled
model 
percentage world controlled 
percentage world explored 
players game score 
opponents game score 
number cities 
average size cities 
total size cities 
   

filearning win reading manuals monte carlo framework

number units 
number veteran units 
wealth gold 
excess food produced 
excess shield produced 
excess trade produced 
excess science produced 
excess gold produced 
excess luxury produced 
name technology currently researched 
percentage completion current research 
percentage remaining current research 
number game turns current research completed 
following feature templates applied city controlled player 
current size city 
number turns city grows size 
amount food stored city 
amount shield stored city  shields used construct new buildings
units city  
turns remaining current construction completed 
surplus food production city 
surplus shield production city 
surplus trade production city 
surplus science production city 
surplus gold production city 
surplus luxury production city 
distance closest friendly city 
average distance friendly cities 
city governance type 
type building unit currently construction 
types buildings already constructed city 
type terrain surrounding city 
type resources available citys neighborhood 
   

fibranavan  silver    barzilay

another city neighborhood 
enemy unit neighborhood 
enemy city neighborhood 
following feature templates applied unit controlled player 
type unit 
moves left unit current game turn 
current health unit 
hit points unit 
unit veteran 
distance closest friendly city 
average distance friendly cities 
type terrain surrounding unit 
type resources available units neighborhood 
enemy unit neighborhood 
enemy city neighborhood 
following feature templates applied predicate labeled word sentence
selected relevant  combined current state action attributes 
word w present sentence  action considered a 
word w predicate label p present sentence  action considered
a 
word w present sentence  current units type u  action
considered a 
word w predicate label p present sentence  current units type u 
action considered a 
word w present sentence  current units type u 
word w predicate label p present sentence  current units type
u 
word w present sentence  attribute text label present
current units neighborhood 
word w predicate label p present sentence  attribute text label
present current units neighborhood 

   

filearning win reading manuals monte carlo framework

references
balla  r     fern  a          uct tactical assault planning real time strategy games 
proceedings ijcai  pp       
barnard  k     forsyth  d  a          learning semantics words pictures 
proceedings iccv  pp         
barto  a  g     mahadevan  s          recent advances hierarchical reinforcement
learning  discrete event dynamic systems             
billings  d   castillo  l  p   schaeffer  j     szafron  d          using probabilistic knowledge
simulation play poker  proceedings aaai iaai  pp         
branavan  s   chen  h   zettlemoyer  l     barzilay  r          reinforcement learning
mapping instructions actions  proceedings acl  pp       
branavan  s   silver  d     barzilay  r       a   learning win reading manuals
monte carlo framework  proceedings acl  pp         
branavan  s   silver  d     barzilay  r       b   non linear monte carlo search civilization
ii  proceedings ijcai  pp           
branavan  s   zettlemoyer  l     barzilay  r          reading lines  learning
map high level instructions commands  proceedings acl  pp           
bridle  j  s          training stochastic model recognition algorithms networks lead
maximum mutual information estimation parameters  advances nips  pp 
       
bryson  a  e     ho  y  c          applied optimal control  optimization  estimation 
control  blaisdell publishing company 
chen  d  l     mooney  r  j          learning sportscast  test grounded language
acquisition  proceedings icml  pp         
chen  d  l     mooney  r  j          learning interpret natural language navigation
instructions observations  proceedings aaai  pp         
clarke  j   goldwasser  d   chang  m  w     roth  d          driving semantic parsing
worlds response  proceedings connl  pp       
de marneffe  m  c   maccartney  b     manning  c  d          generating typed dependency parses phrase structure parses  proceedings lrec  pp         
eisenstein  j   clarke  j   goldwasser  d     roth  d          reading learn  constructing
features semantic abstracts  proceedings emnlp  pp         
fleischman  m     roy  d          intentional context situated natural language learning 
proceedings conll  pp         
gelly  s   wang  y   munos  r     teytaud  o          modification uct patterns
monte carlo go  tech  rep        inria 
goldwasser  d   reichart  r   clarke  j     roth  d          confidence driven unsupervised
semantic parsing  proceedings acl  pp           

   

fibranavan  silver    barzilay

gorniak  p     roy  d          speaking sidekick  understanding situated speech
computer role playing games  proceedings aiide  pp       
liang  p   jordan  m  i     klein  d          learning semantic correspondences less
supervision  proceedings acl  pp       
liang  p   jordan  m  i     klein  d          learning dependency based compositional
semantics  proceedings acl  pp         
marcus  m  p   santorini  b     marcinkiewicz  m  a          building large annotated
corpus english  penn treebank  computational linguistics                 
oates  j  t          grounding knowledge sensors  unsupervised learning language
planning  ph d  thesis  university massachusetts amherst 
roy  d  k     pentland  a  p          learning words sights sounds  computational model  cognitive science            
rumelhart  d  e   hinton  g  e     williams  r  j          learning representations
back propagating errors  nature              
schafer  j          uct algorithm applied games imperfect information 
diploma thesis  otto von guericke universitat magdeburg 
sheppard  b          world championship caliber scrabble  artificial intelligence            
       
silver  d   sutton  r     muller  m          sample based learning search permanent transient memories  proceedings icml  pp         
siskind  j  m          grounding lexical semantics verbs visual perception using
force dynamics event logic  journal artificial intelligence research           
sturtevant  n          analysis uct multi player games  proceedings iccg 
pp       
sutton  r  s     barto  a  g          reinforcement learning  introduction  mit
press 
sutton  r  s   koop  a     silver  d          role tracking stationary environments  proceedings icml  pp         
tellex  s   kollar  t   dickerson  s   walter  m  r   banerjee  a  g   teller  s     roy  n 
        understanding natural language commands robotic navigation mobile
manipulation  proceedings aaai  pp           
tesauro  g     galperin  g          on line policy improvement using monte carlo search 
advances nips  pp           
vogel  a     jurafsky  d          learning follow navigational directions  proceedings
acl  pp         
yu  c     ballard  d  h          integration grounding language learning
objects  proceedings aaai  pp         
zettlemoyer  l     collins  m          learning context dependent mappings sentences
logical form  proceedings acl  pp         

   


