journal of artificial intelligence research                 

submitted        published      

proximity based non uniform abstractions
for approximate planning
jir baum
ann e  nicholson
trevor i  dix

jiri baum com au
ann nicholson monash edu
trevor dix monash edu

faculty of information technology
monash university  clayton  victoria  australia

abstract
in a deterministic world  a planning agent can be certain of the consequences of its
planned sequence of actions  not so  however  in dynamic  stochastic domains where
markov decision processes are commonly used  unfortunately these suffer from the curse of
dimensionality  if the state space is a cartesian product of many small sets  dimensions  
planning is exponential in the number of those dimensions 
our new technique exploits the intuitive strategy of selectively ignoring various dimensions in different parts of the state space  the resulting non uniformity has strong
implications  since the approximation is no longer markovian  requiring the use of a modified planner  we also use a spatial and temporal proximity measure  which responds to
continued planning as well as movement of the agent through the state space  to dynamically adapt the abstraction as planning progresses 
we present qualitative and quantitative results across a range of experimental domains
showing that an agent exploiting this novel approximation method successfully finds solutions to the planning problem using much less than the full state space  we assess and
analyse the features of domains which our method can exploit 

   introduction
in a deterministic world where a planning agent can be certain of the consequences of its
actions  it can plan a sequence of actions  knowing that their execution will necessarily
achieve its goals  this assumption is not appropriate for flexible  multi purpose robots and
other intelligent software agents which need to be able to plan in the dynamic  stochastic
domains in which they will operate  where the outcome of taking an action is uncertain 
for small to medium sized stochastic domains  the theory of markov decision processes
provides algorithms for generating the optimal plan  bellman        howard        puterman   shin         this plan takes into account uncertainty about the outcome of taking
an action  which is specified as a distribution over the possible outcomes  for flexibility 
there is a reward function rather than a simple goal  so that the relative desirability or
otherwise of each situation can be specified 
however  as the domain becomes larger  these algorithms become intractable and approximate solutions become necessary  for instance drummond   bresina        dean 
kaelbling  kirman    nicholson        kim        steinkraus         in particular where
the state space is expressed in terms of dimensions  or as a cartesian product of sets  its
size and the resulting computational cost is exponential in the number of dimensions  on
c
    
ai access foundation  all rights reserved 

fibaum  nicholson   dix

the other hand  fortunately  this results in a fairly structured state space where effective
approximations often should be possible 
our solution is based on selectively ignoring some of the dimensions  in some parts of
the state space  some of the time  in other words  we obtain approximate solutions by
dynamically varying the level of abstraction in different parts of the state space  there
are two aspects to this approach  firstly  the varying level of abstraction introduces some
artefacts  and the planning algorithm must be somewhat modified so as to eliminate these 
secondly  more interestingly  an appropriate abstraction must be selected and later modified
as planning and action progress 
our work is an extension and synthesis of two existing approaches to approximate planning  the locality based approximation of envelope methods  dean et al         and the
structure based approximation of uniform abstraction  nicholson   kaelbling        dearden   boutilier         our work extends both of these by exploiting both structure and
locality  broadening the scope of problems that can be contemplated  baum and nicholson
       introduced the main concepts while full details of our algorithms and experimental
results are presented in baums        thesis  there have been some studies of arbitrary
abstraction  for instance by bertsekas and tsitsiklis         however  these are generally
theoretical and in any case they tended to treat the approximation as markovian  which
would have resulted in unacceptable performance in practice  we improve on this by extending the planning algorithm to deal with the non markovian aspects of the approximation 
finally  we use a measure of locality  introduced by baum and nicholson         that is
similar to but more flexible than the influence measure of munos and moore        
we assume that the agent continues to improve its plan while it is acting and that
planning failures are generally not fatal  we also deal with control error exclusively  sensor
error is not considered and it is assumed that the agent can accurately discern the current
world state  fully observable   and that it accurately knows the state space  the goal or
reward function  and a distribution over the effect of its actions  no learning  
the remainder of this paper is organised as follows  section   reviews the background 
introduces our abstraction and provides our framework  section   discusses planning under
a static non uniform abstraction  and section   presents our method for initially selecting
the non uniform abstraction based on the problem description  section   presents a method
of changing the abstraction based on the policy planned  while sections   and   introduce
a proximity measure and a method of varying the abstraction based on that measure 
respectively  section   presents results based both on direct evaluation of the calculated
policy and on simulation  finally  section   discusses the results and section    gives our
conclusions and outlines possible directions for future work 

   planning under non uniform abstractions
in a non deterministic world where a planning agent cannot be certain of the consequences
of its actions except as probabilities  it cannot plan a simple sequence of actions to achieve
its goals  to be able to plan in such dynamic  stochastic domains  it must use a more
sophisticated approach  markov decision processes are an appropriate and commonly used
representation for this sort of planning problem 
   

fiproximity based non uniform abstractions for planning

    illustrative problems
to aid in exposition  we present two example problems here  the full set of experimental
domains is presented in section     
the two illustrative problems are both from a grid navigation domain  shown in figure   
they both have integer x and y coordinates from   to    three doors which can be either
open or closed and a damage indication which can be either yes or no  the agent can
move in one of the four cardinal directions  open a door if it is next to it  or do nothing 
the doors are fairly difficult to open  with probability of success     per time step  while
moving has an     chance of success  with no effect in the case of failure  running into
a wall or into a closed door causes damage  which cannot be repaired  the transitions are
shown in table    the agent starts in the location marked s  in figure   with the doors
closed and no damage  and the goal is to reach the location marked  with no damage 
x  
y  

 

 

 

 

 

 

 



s 

 

 

k 

 

x  
y  

 

 

 

 

 

 

s 

 

 

k 

 



 

d 

d 

k 

 

 

 

 

 

 



 





d 
 

 



 

 

 

 



d 



k 





 
 

k 

d 

 

 a 

k 

d 
 b 

figure    the layout of the grid navigation domain  the blue arrows show the optimal
path  a  and a suboptimal path  b  through the  keys problem  the  doors
problem has the same grid layout  walls and doors  but with no keys 

the  keys problem also contains keys  which are required to open the doors  the agent
may have any one or more of them at any time  an additional action allows the agent to
pick up each key in the location as shown in the figure  and the open action requires the
corresponding key to be effective  there is no separate unlock action   the  doors problem
contains no keys  the doors are unlocked but closed  and therefore no corresponding
keys or pickup action 
the optimal policy obtained by exact planning for the  doors problem simply takes
the shortest path through door    for the  keys problem  the optimal plan is to collect
keys   and    pass south through door   and east through door    shown in figure   a   a
suboptimal plan is shown in figure   b  
   

fibaum  nicholson   dix

x
stay


y

pre state
d 
d 
d 

post state
d 
d 

dmg



x

y













 
 


y  



















south
 
 




 
 
 
 
y

open






open






















   
   

north
 
 




 
 
 
 
y

open






open






















   
   

east
 
 
 
 
 
 
x

 
 
 
 























open




















   
   
   
   

west
 
 
 
 
 
 
x

 
 
 
 























open




















open
 
 
 
 
 
 


 
 
 
 
 
 










































   

   

   
   
   
   
   

   
   
   
   
   
   
   

d 

dmg



























yes
yes


 
 


y 





















yes
yes


 
 
 
 


x  





































yes
yes


 
 
 
 


x 





































yes
yes


















open
open








open
open








open
open








yes

table    transitions in the  doors problem  showing important and changed dimensions
only  first matching transition is used  where a percentage is shown  the given
post state will occur with that probability  otherwise the state is unchanged  transitions without percentages are deterministic 

   

fiproximity based non uniform abstractions for planning

    exact planning
one approach to exact planning in stochastic
domains
involves using markov decision


ff
 
processes  mdps   a mdp is a tuple s  a  t  r  s   where s is the state space  a is the
set of available actions  t is the transition function  r is the reward function and s   s
is the initial state  the agent begins in state s    at each time step  the agent selects an
action a  a  which  together with the current state  applies t to obtain a distribution over
s  the current state at the next time step is random according to this distribution and we
write prt  s  a  s   for the probability that action a taken in state s will result in the state
s at the next time step  the agent is also given a reward at each time step  calculated by
r from the current state  and possibly also the action selected   the aim of the agent is
to maximise some cumulative function of these rewards  typically the expected discounted 
sum under a discounting factor   in a fully observable mdp  the agent has full knowledge 
in particular  the agent is aware of t   r and the current state when selecting an action 
it is well known that in a fully observable mdp  the optimal solution can be expressed as
a policy    s  a mapping from the current state to the optimum action  the planning
problem  then  is the calculation of this   as a side effect of the calculation  the standard
algorithms also calculate a value function v   s  r  the expected discounted sum of
rewards starting from each state  table   summarises the notation used in this paper 
there are well known iterative algorithms  bellmans        value iteration  howards
       policy iteration  and the modified policy iteration of puterman and shin       
for computing the optimal policy     however  as s becomes larger  the calculation of  
becomes more computationally expensive  this is particularly so if state space is structured
as a cartesian product of dimensions  s   s  s    sd   because then  s  is exponential
in d  since the algorithms explicitly store v and usually   which are functions of s  their
space complexity is therefore also exponential in d  since they iterate over these arrays 
the time complexity is also at least exponential in d  even before any consideration of how
fast these iterative algorithms themselves converge  typically  as d grows  planning in s
quickly becomes intractable  since in practice the amount of computation allowed to the
agent is limited  this necessitates some approximations in the process 
in the  doors problem  there are six dimensions  two for the x and y coordinates  three
for the doors and one for damage   so that s                              open  closed  
 open  closed    open  closed    damage  no damage  and  s            the action space
a is a set of five actions  a    north  south  east  west  open   the transition function
specifies the outcomes of taking each action in each state  the reward function r is   in
the agent is in the h    i location  marked  in the diagram  with no damage    if it is
in any other location with no damage  and   if there is damage  finally  s  is the state
where the agent is in the h    i location  the doors are closed  and there is no damage 
exact planning is listed in our results with  s  and v   s    for comparison  if there is
no approximation  the planner must consider the whole state space s   s  is therefore a
measure of the cost of this planning  directly in terms of space and indirectly in terms
of time  on the other hand  since the planning is exact  the optimal value function v  will
   while our illustrative problems have simple goals of achievement  we use time discounting in order to
remain general and for its mathematical convenience 

   

fibaum  nicholson   dix

symbol
original
s
sd

meaning

abstract
w  p s 


state space  specific state space   worldview  resp  
dimension d of the state space
d  n
number of dimensions
ss
ww
a state
 
s s

initial state
scur  s

current state  in on line planning 
sd  sd
wd  sd
dimension d of the state s or w  resp 
a
set of actions  action space 
a   a
default action
t
t
transition function  formal 
prt  sas       prt  waw       transition function  in use 
r s r
r wr
reward function  one step reward 
v  sr
v  w r
value function  expected discounted sum of rewards 
        
discount factor for the reward
 sa
 wa
policy
   s  a

optimal policy
v  s r

optimal value function  exact value function of   
  i   s  a
  i   w  a
approximate policy  ith approximate policy

exact value function of   note   may be abstract 
v   s  r
v   s  r
v   w  r
approximate value function of   approx  to v  
table    summary of notation  the first column is the notation for the original mdp  the
second is the notation once non uniform abstraction has been applied 

be obtained  along with the optimal policy   ensuring that the agent will expect to obtain
that value  it is against these figures that all approximations must measure 
    uniform abstraction
one method of approximation is to take advantage of these dimensions by ignoring some
of them  those that are irrelevant or only marginally relevant  in order to obtain an
approximate solution  it is uniform in the sense that the same dimensions are ignored
throughout the state space  since this approach attacks the curse of dimensionality where
it originates  at the dimensions  it should be very effective at counteracting it 
dearden and boutilier use this to obtain an exact solution  boutilier        or an approximate one  boutilier   dearden        dearden   boutilier         however  their abstractions are fixed throughout execution  and dimensions are also deleted from the problem
in a pre determined sequence  this makes their approach somewhat inflexible  similarly 
nicholson and kaelbling        propose this technique for approximate planning  they
delete dimensions from the problem based on sensitivity analysis  and refine their abstraction as execution time permits  but it is still uniform  dietterich        uses this kind of
abstraction in combination with hierarchical planning to good effect  a subtask  such as
navigate to location t  can ignore irrelevant dimensions  such as location of items to be
   

fiproximity based non uniform abstractions for planning

picked up or even the ultimate destination of the agent  generally  any time the problem
description is derived from a more general source rather than specified for the particular
problem  uniform abstraction will help  gardiol and kaelbling        use it where some dimensions are relevant  but only marginally  so that ignoring them results in an approximate
solution that can be improved as planning progresses 
unfortunately  however  at least with human specified problems  one would generally
expect all or most mentioned dimensions to be in some way relevant  irrelevant dimensions
will be eliminated by the human designer in the natural course of specifying the problem 
depending on the domain and the situation some marginally relevant dimensions might be
included  but often  these will not be nearly enough for effective approximation 
we do not list comparisons against uniform abstraction in our results for this reason 
in most of our sample domains  it makes little sense  all or almost all of the dimensions
are important to solving the problem  where this is not the case and methods exist for
effective uniform abstraction  they can be integrated with our approach easily 
    non uniform abstraction
our approximation  non uniform abstraction  replaces the state space s with w  a particular type of partition of s  as originally introduced by baum and nicholson         we call
w the worldview  so members of w are worldview states while members of s are specific
states   non uniform abstraction is based on the intuitive idea of ignoring some dimensions
in some parts of the state space  for example  a door is only of interest when the agent is
about to walk through it  and can be ignored in those parts of the state space which are
distant from the door  in a particular member of the worldview wi  w  each dimension is
either taken into account
 concrete  refined in   or ignored altogether  abstract 
q completely
i and each w i is a singleton subset of the corresponding s
w
coarsened out   wi   d
d
d   d
d
for concrete dimensions and equal to sd for abstract dimensions   it is up to the worldview
selection and modification methods to ensure that w remains a partition at all times 
to give an example  in the  doors problem one possible worldview has the location and
damage dimensions concrete in every state  while the door dimensions are each concrete
only in states within two steps of the respective door 
note that the domain is still fully observable  this is not a question of lack of knowledge
about the dimensions in question  but wilful conditional ignorance of them during planning
as a matter of computational expediency  the approximation also subsumes both exact
planning and uniform abstraction  for exact planning  all dimensions can be set uniformly
concrete  so that  w     s  and each worldview state corresponds to one specific state  for
uniform abstraction  the combination of abstract and concrete dimensions can be fixed for
the entire worldview  they can be treated as special cases of our more general approach  
   previously  we used the word envelope for the same concept  baum   nicholson         however 
worldview better describes the approximation used than envelope 
   we do not allow a dimension to be partially considered  we only abstract to the level of dimensions  not
within them  a dimension such as the x coordinate will either have a particular value  or it will be fully
abstract  but it will never be     for instance 
   our modified  calculation reduces to the standard algorithm for uniform or fully concrete worldviews 
so our planner obtains the standard results in these cases 

   

fibaum  nicholson   dix

on the other hand  the approximation is no longer markovian  a dimension that is
abstracted away is indeterminate  in the notation of markov decision processes  this can
only be represented by some distribution over the concrete states  but the dimension is not
stochastic  it has some specific  but ignored  value  the distinction is important because
for a truly stochastic outcome  it can be quite valid to plan to retry some action until it
succeeds  for instance  opening a door in the  doors problem   for a dimension which is
merely ignored  the agent will obtain the same outcome  door is closed  each time it moves
into the region where the dimension is not ignored  so that within the worldview  previous
states can appear to matter  we discuss this further in section   
    comparison to other approaches
non uniform abstractions began to appear in the literature at first usually as a side effect
of a structured method  where the state space is represented as a decision tree based on
the individual dimensions  such as boutilier  dearden  and goldszmidt               note 
however  that the decision tree structure imposes a restriction on the kinds of non uniform
abstraction that can be represented  the dimension at the root of the tree is considered
throughout the state space  and so on  this is a significant restriction and results in a
representation much more limited than our representation  a similar restriction affects
de alfaro and roys        magnifying lens abstraction  with the refinement that multivalued dimensions are taken bit by bit and the bits interleaved  so that each level of the
decision tree halves the space along a different dimension in a pre determined order  as they
note  this would work well where these dimensions correspond to a more or less connected
space  as in a gridworld  but it would do less well with features like the doors of our
grid navigation domain  magnifying lens abstraction calculates upper and lower bounds to
the value function  rather than a single approximation  which is an advantage for guiding
abstraction selection and allows for a definite termination condition  which we lack   on
the other hand  it always considers fully concrete states in part of the algorithm  limiting
its space savings to the square root of the state space  whereas our algorithm can work
with a mixture of variously abstract states not necessarily including any fully concrete
ones  another related approach is variable grids used for discretisation  which can be
indirectly used for some discrete domains  as boutilier  goldszmidt  and sabata        do 
if dimensions can be reasonably approximated as continuous  for instance money   unlike
our approach  variable grids are completely inapplicable to predicates and other binary or
enumerated dimensions  some  such as reyes  sucar  and morales         use techniques
in some ways quite similar to ours for continuous mdps  though they are quite different in
other ways  they consider refinement only  not coarsening  they use sampling  rather than
directly dealing with the domain model  and they use a different refinement method  where
each refinement is evaluated after the fact and then either committed or rolled back 
perhaps the most similar to our approach is one of the modules of steinkraus        
the ignore state variables module  however  the module appears to be completely manual 
requiring input of which variables  dimensions  should be ignored in what parts of the state
space  it also uses the values of the dimensions from the current state scur   rather than a
distribution  which obviously restricts the situations in which it may be used  for instance 
in the  doors problem  the doors could not be ignored in the starting state   finally  since
   

fiproximity based non uniform abstractions for planning

steinkraus        does not analyse or report the relative contributions of the modules to
the solution  nor on the meta planning problem of selecting and arranging the modules  it
is difficult to know to what extent this particular module is useful 
other approaches take advantage of different features of different domains  for instance 
the factored mdp approach  used  for instance  by boutilier et al         or guestrin 
koller  parr    venkataraman        is suitable for domains where parts of the state and
action spaces can be grouped together so that within each group those actions or action
dimensions affect the corresponding states or state dimensions but interaction between the
groups is weak  st aubin  hoey  and boutilier        iterate a symbolic representation in
the form of algebraic decision diagrams to produce approximate solutions  while sanner
and boutilier        iterate a symbolic representation of a whole class of problems in a
domain  using symbolic dynamic programming  first order algebraic decision diagrams and
linear value approximation  to pre compute a generic solution which can then be used to
quickly solve specific problems of that class  while we focus only on the state space  others approximate the action space  typically grouping actions  possibly hierarchically  into
macro actions  after korf         for instance hauskrecht  meuleau  kaelbling  dean 
and boutilier        or botea  enzenberger  muller  and schaeffer        take this approach 
while parr        uses finite state automata for the macro actions and srivastava  immerman  and zilberstein        take it further by using algorithm like plans with branches and
loops  goldman  musliner  boddy  durfee  and wu        reduce the state space while generating the  limited horizon  undiscounted  mdp from a different  non mdp representation
by only including reachable states  pruning those which can be detected as being clearly and
immediately poor  or inferior or equivalent to already generated states  naturally  many of
these approaches can be combined  for instance  gardiol and kaelbling              combine state space abstraction with the envelope work of dean et al          while steinkraus
       uses a modular planner with a view of combining as many approaches as may be
appropriate for a given problem  for more details and further approaches and variants we
refer the reader to a recent survey of the field by daoui  abbad  and tkiouat        
    dynamic approximate planning
the top level algorithm is shown as algorithm    after some initialisation  consisting of
selecting the initial abstraction and setting the policy  value and proximity to a      and
proportionally to the size of each worldview state  respectively   the planner enters an
infinite loop in which it stochastically alternates among five possible calculations  each of
which is described in the following sections  here and elsewhere in the algorithm  we use
stochastic choice as a default in the absence of a more directed method 
the agent is assumed to have processing power available while it is acting  so that it
can continually improve its policy  modify the approximation and updates the focus of its
planning based on the current state  this means that the agent does not need to plan so
well for unlikely possibilities  and can therefore expend more of its planning effort on the
most likely paths and on the closer future  expecting that when and if it reaches other parts
of the state space  it can improve the approximation as appropriate 
   initialising the approximate policy to action a  constitutes a domain specific heuristic  namely  that
there is a known default action a  which is reasonably safe in all states  such as a do nothing action 

   

fibaum  nicholson   dix

algorithm   high level algorithm for approximate planning with dynamic non uniform
abstractions
do select initial abstraction    algorithm     
for all worldview states w do
 w   a    v  w      p w    w 
 s 
do policy and value calculation    algorithm     
loop
choose stochastically do
do policy and value calculation    algorithm     
or
do policy based refinement    algorithm     
or
do proximity calculation    algorithm     
or
do proximity based refinement    algorithm     
or
do proximity based coarsening    algorithm     
input latest current state  output the policy

actual execution of the policy is assumed to be in a separate thread  executive   so
that the planner does not have to concern itself with the timeliness requirements of the
domain  whenever an action needs to be taken  the executive simply uses the policy that it
most recently received from the planner 
dean et al         call this recurrent deliberation  and use it with their locality based
approximation  a similar architecture is used by the circa system  musliner  durfee   
shin        goldman  musliner  krebsbach    boddy        to guarantee hard deadlines 
in circa terminology  the planner is the ais  ai subsystem   and the executive is the
rts  real time subsystem  
an alternative to recurrent deliberation is pre cursor deliberation  where the agent first
plans  and only when it has finished planning does it begin to act  making no further
adjustments to its plan or policy  effectively  for the planner  the current state is constant
and equal to the initial state throughout planning  in this work the pre cursor mode is used
for some of the measurements  as it involves fewer potentially confounding variables 
conceptually  our approach can be divided into two broad parts  the open ended problem of selecting a good abstraction and the relatively closed problem of planning within
that abstraction  since the latter part is more closed  we deal with it first  in the next
section  covering algorithm    we then explore the more open ended part in sections    
covering algorithms    

   solving non uniformly abstracted mdps
given a non uniform abstraction  the simplest way to use it for planning is to take one
of the standard mdp algorithms  such as the modified policy iteration of puterman and
shin         and adapt it to the non uniform abstraction minimally  the formulae translate
   

fiproximity based non uniform abstractions for planning

directly in the obvious fashion   becomes a function of worldview states instead of concrete
states  and so on  as shown in algorithm    using the simple variant for the update policy
for w procedure   probabilities of transition from one worldview state to another are
approximated using a uniform distribution over the concrete states  or possibly some other
distribution  if more information is available  
algorithm   policy and value calculation
repeat n times
for all worldview states w do
do update value for w
for all worldview states w do
do update policy for w
do update value for w
procedure update value for w
if prt  w   w   w      then
   optimisation  v  w  can be calculated directly in this case   
v  w   r w 
 
else
p
v  w   r w     w prt  w   w   w  v  w  

procedure update policy
p for w variant simple
 w   min arg maxa w prt  w  a  w  v  w  

procedure update policy for w variant with locally uniform abstraction
   see section     for discussion of locally uniform abstraction   
absdims   d   
w a   prt  w  a  w        w is abstract in d 
w is abstract in d
d  absdims
lua  w   w  
dimension d of w   dimension d of w d 
  absdims
p
 w  w   


v  w   w w  w   v  w  
p
 w   min arg maxa w prt  w  a  w  v  lua w   

note that in algorithm    a is considered an ordered set with a  as its smallest element
and the minimum is used when the arg max gives more than one possibility  this has
two aspects   a  as a domain specific heuristic  for instance  breaking ties in favour of
the default action when possible  and  b  to avoid policy basedp
refinement  see section   
based on actions that have equal value  secondly  for efficiency  w can be calculated only
over states w with prt  w  a  w        since other states will make no contribution to the
sum  finally  the number n is a tuning parameter which is not particularly critical  we use
n       
of course  replacing the state space s by a worldview w in this way does not  in general 
preserve the markov property  since the actual dynamics may depend on aspects of the state
space that are abstracted in the worldview  in the simple variant we ignore this and assume
the markov property anyway  on the grounds that this is  after all  an approximation 
unfortunately  the resulting performance can have unacceptably large error  including the
outright non attainment of goals 
   

fibaum  nicholson   dix

for instance  in the  doors problem  such a situation will occur at each of the three
doors whenever they are all abstract at s  and concrete near the door in question  the
doors are relatively difficult to open  with only a     probability of success per try  on
the other hand  when moving from an area where they are abstract to an area where they
are concrete  the assumed probability that the door is already open is      when the
calculations are performed  it turns out to be preferable to plan a loop  repeatedly trying
for the illusory     chance of success rather than attempting to open the door at only
    chance of success  the agent will never reach the goal  worse still  in some ways  it
will estimate that the quality of the solution is quite good  v  s           which is in
fact better even than the optimal solutions v   s           while the true quality of the
solution is very poor  v  s               corresponding to never reaching the goal  but not
incurring damage  either  figures are for discounting factor              
regions that take into account a particularly bad piece of information may seem unattractive  as described above  and vice versa  we call this problem the ostrich effect  as
the agent is refusing to accept an unpleasant fact  like the mythical ostrich that buries its
head in the sand  its solution  locally uniform abstraction  is described in the next section 
if the abstracted approximation is simply treated as a mdp in which the agent does not
know which state it will reach  near closed door or near open door   it will not correspond
to the underlying process  which might reach a particular state deterministically  as it does
here   the problem is especially obvious in this example  when the planner plans a loop 
this is reminiscent of a problem noted by cassandra  kaelbling  and kurien         where
a plan derived from a pomdp failed  the actual robot got into a loop in a particular
situation when a sensor was completely reliable contrary to the model 
    locally uniform abstraction
the ostrich effect occurs when states of different abstraction are considered  for instance
one where a door is abstract and one where the same door is concrete and closed  the
solution is to make the abstraction locally uniform  and therefore locally markovian for
the duration of the policy generation iterative step  by making the abstraction locally 
temporarily uniform  the iterative step of the policy generation algorithm never has to work
across the edge of an abstract region  and  since the same information is available in all the
states being considered at each point  there is no impetus for any of them to be favoured or
avoided on that basis  for instance  avoiding a state in which a door is concrete and closed
in favour of one where the door is abstract   the action chosen will be chosen based on the
information only and not on its presence or absence 
this is a modification to the update policy for w procedure of algorithm    as
the states are considered one by one  the region around each state is accessed through a
function that returns a locally uniform version  states that are more concrete than the state
being considered will be averaged so as to ignore the distinctions  as different states are
considered  sometimes the states will be taken for themselves  sometimes their estimated
values v will be averaged with adjacent states  this means that some of the dimensions
will only partially be considered at those states  in most cases  this will mean that
the more concrete region must extend one step beyond the region in which the dimension
   

fiproximity based non uniform abstractions for planning

is immediately relevant  for a dimension to be fully considered at a state  the possible
outcomes of all actions at that state must also be concrete in that dimension 
the modified procedure proceeds as follows  first the dimensions that are abstract in
any possible outcome of the state being updated w are collected in the variable absdims 
then the function lua is constructed which takes worldview states w and returns potential
worldview states w which are like w but abstract in all the dimensions in absdims  as this
is the core of the modification  it is named lua for locally uniform abstraction  since the
potential states returned by lua are not  in general  members of w  and therefore do not
necessarily have a value stored in v   a further function v is constructed which calculates
weighted
averages of the value function v over potential states  as with the other sum 
p
can
be calculated only over states w with w  w     for efficiency  finally  the

w
update step is carried out using the two functions lua and v  
unfortunately  once the modification is applied  the algorithm may or may not converge depending on the worldview  failure to converge occurs when the concrete region is
too small  in some cases  the algorithm will cycle between two policies  or conceivably
more  instead of converging  one must be careful  therefore  with the worldview  to avoid
these situations  or else to detect them and modify the worldview accordingly  the policybased worldview refinement algorithm described in section   below ensures convergence in
practice 

   initial abstraction
at the beginning of planning  the planner must select an initial abstraction  since the
worldview is never completely discarded by the planner  an infelicity at this stage may
impair the entire planning process  as the worldview improvement algorithms can only
make up for some amount the weakness here 
there are different ways to select the initial abstraction  we propose one heuristic
method for selecting the initial worldview based on the problem description  with some
variants  consider for example that each door in the  doors problem is associated with
two locations  that is  those immediately on either side  it makes sense  then  to consider
the status of the door in those two locations  this association can be read off the problem
specification  intuitively  the structure of the solution is likely to resemble the structure
of the problem  this incorporates the structure of the transition function into the initial
worldview  the reward function is also incorporated  reflecting the assumption that the
dimensions on which the reward is based will be important 
we use a two step method to derive the initial worldview  as shown in algorithm   
firstly  the reward function is specified based on particular dimensions  we make those
dimensions concrete throughout the worldview  and leave all other dimensions abstract  in
the  doors problem  these are the x and y and dmg dimensions  so after this step there are
                states in the worldview 
secondly  the transition function is specified by decision trees  one per action  we use
these to find the nexuses between the dimensions  that is  linking points  those points at
which the dimensions interact  each nexus corresponds to one path from the root of the
tree to a leaf  for example  in the  doors problem  the decision tree for the open action
contains a leaf whose ancestors are x  y  d  and a stochastic node  with the choices leading
   

fibaum  nicholson   dix

algorithm   select initial abstraction
   set the worldview completely abstract   
w   s 
   reward step   
if reward step enabled then
for all dimensions d mentioned in the reward tree do
refine the whole worldview in dimension d
   nexus step   
if nexus step enabled then
for all leaf nodes in all action trees do
for all worldview states w matching the pre state do
refine w in the dimensions mentioned in the pre state

to that leaf being labelled respectively       closed and      this corresponds to a nexus at
sx      sy     and sd    closed  the stochastic node is ignored in determining the nexus  
in total  there are four nexuses on each side of each door  in the two locations immediately
adjacent  as shown in figure   a   connecting the relevant door dimension to the x and
y coordinates  the initial worldview is shown in figure   b   with x  y and dmg concrete
everywhere and the doors abstract except that each is concrete in the one location directly
on each side of the door  corresponding to the location of the nexuses on figure   a   after
both steps   w         compared to  s          specific states 

x  

 

 

 

 

 

 

 

 

 

x  

y  

y  

 

 

 

 

 

 

 

 

 

 





 

d 

d 

 





 

d 

d 

 

 

 

 

 

 

 

 

 

 

 

 

 

 a 

 

 

d  d 
 b 

figure    nexus step of the initial abstraction  showing  a  the location of the nexuses in
the  doors problem  there are four nexuses at each   and  b  the locations in
which the door dimensions will be concrete in the initial worldview 

   

fiproximity based non uniform abstractions for planning

for the  keys problem  the location of the nexuses is the same as in figure   a   except
there are more nexuses in each location and some of them also involve the corresponding
key dimensions  thus  in the initial worldview  the locations shown in figure   b  will be
concrete not only in corresponding door dimension  but also  when they are closed  in the
corresponding key dimension  in the states in which the doors are open  the key dimension
remains abstract  the initial worldview size for  keys is  w        
due to the locally uniform abstraction  these concrete door dimensions will be taken
into account only to a very minimal degree  if the worldview were to be used without
further refinement  it is to be expected that the resulting policies would be very poor  the
results  bear out this expectation  the worldview initialization methods therefore are not
intended to be used on their own  but rather as the basis for further refinement  thus 
the real test of the methods is how well they will work when coupled with the worldview
modification methods  described below 

   policy based refinement
this section presents the first of the worldview modification methods  policy based refinement  this method modifies the worldview based directly on the current approximate
policy   in particular  it refines states based on differences between the actions planned at
adjacent  differently abstract states  where such differences indicate that a dimension may
be important  any adjacent states that are abstract in that dimension are refined  i e  that
dimension is made concrete at those states  
the method was previously introduced by baum and nicholson         who showed 
using a small navigation domain example  the  doors problem of this paper   that this
refinement method resulted in a good policy  though not optimal  here we present quantitative results and consider more complex domains 
    motivation
the motivation for this method is twofold  firstly  as already indicated  the method detects
areas in which a particular dimension is important  because it affects the action planned 
and ensures that it is concrete at adjacent states  thus regions where a dimension is taken
into account will expand for as long as the dimension matters  and then stop  secondly  the
method fulfils the requirements for choosing a worldview so as to avoid non convergence in
the policy calculation  as mentioned in section     above 
dimensions are important where they affect the policy  since the policy is the planners
output  they are less important in parts of the state space where they do not affect
the policy  thus  which dimensions need to be concrete and which can remain abstract
can be gleaned for each part of the state space by comparing the optimal actions for the
various states  where the optimal actions are equal  states can be abstract  and where they
differ  states should be concrete  however  we do not have the optimal policy     with an
approximate policy  on a worldview  it is more difficult  however  the planner can compare
policies in areas where a dimension is concrete  and if it is found to be important there 
expand the area in which it is concrete  as policy based refinement and policy calculation
   omitted here as they uninteresting  but presented by baum        

   

fibaum  nicholson   dix

alternate  refinement will continue until the area where the dimension is concrete covers the
whole region in which it is important 
section     above noted that the planning algorithm requires a worldview chosen with
care  the algorithm described in this section detects situations that are potentially problematic under locally uniform abstraction and modifies the worldview to preclude them 
intuitively  the incorrect behaviour occurs where an edge of a concrete region intersects
with a place where there are two fairly similarly valued courses of action  corresponding to
two different paths to the goal 
    method
the method uses the transition function as the definition of adjacent states  so that worldview states w and w are considered adjacent if a   prt  w  a  w        this definition is not
symmetrical in general  since the transition function is not  but that is not a problem for
this method  as can be seen below  the algorithm is shown as algorithm   
algorithm   policy based refinement
candidates  
for all worldview states w do
for all actions a do
for all w   p r w  a  w       do

for all dimensions
 d   w is abstract in d and w is concrete in d do
w
is
abstract
in
d
construct w  
dimension d of w   dimension d of w d    d
a
b
a
if w   w    w       wb   and wa  w     and wb  w     then
   policy is not the same throughout w   
candidates  candidates    w  d  
for all  w  d   candidates do
if w  w then
   replace w with
group of states concrete in d   
 anew
w
is concrete in d
do
for all wnew  
dimension d of wnew   dimension d of w d    d
w  w   wnew  
new
 wnew     w   v  wnew    v  w   p wnew     w w    p w 
w  w    w     discarding also the stored  w   v  w  and p w    

example in the  doors problem  for instance  applying this method during planning
increases the number of worldview states from the initial     to         depending on
the stochastic choices  recall that  s          for comparison   it produces concrete regions
which are nice and tight around the doors  as shown in figure    while allowing the algorithm
to converge to a reasonable solution  the solution is in fact optimal for the given initial
state s    though that is simply a coincidence  since the s  is not taken into account by the
algorithm and some other states have somewhat suboptimal actions  the agent would reach
the goal from these states  but not by the shortest route  
   

fiproximity based non uniform abstractions for planning

x  

 

 

 

 

 

 

 

 

 

x  

y  

 

 

 

 

 

 

 

 

 

y  
d

 

d 

d 

 

k 

 

d  d  d 

d  d  d 

 

 

d  d  d  d 

d 

 

d  k  d  d 

 

d  d 

 

d  d  d  d  d 

 

 

d  d  d 

 

 

d 

 

 

d

d

k 
d

d

d

k 

d

 

d 

 

k 

 

d  d  d 

 

k  k  k 

 a 

d

k  k  k 

d

d

d

 b 

figure    example of non uniform abstraction for the  a   doors and  b   keys problems
with policy based refinement  the x  y and dmg dimensions are concrete everyd
where  d   d  and d  indicate where the corresponding door is concrete  while k  
d
d
k  and k  indicate that the corresponding door is concrete and the corresponding
key is also concrete if the door is closed 

the worldview obtained by this method is often quite compact  for instance  rather
than refining a simple      rectangular region on each side of a door in  doors  as a human
might  this algorithm makes only   locations concrete on the approach side of each door 
which is enough to obtain a good solution  this can be seen on the north sides of doors
d  and d   as well as the west side of door d     concrete locations  due to the edge   on
the departure side of doors d  and d   it is even better  it makes no refinement at all 
south of door d  and east of door d   the action is to move toward the goal  regardless of
the status of the door  the actions are equal  so no refinement takes place 
the south side of door d  seems rather less compact  the concrete area is in fact not
very big    locations for  doors  but it seems excessive compared with the compact
concrete areas elsewhere  this can occur when there is a nexus close to a region where
the best action to take genuinely depends on the status of the dimension in the nexus 
but the difference is small  if somehow the agent found itself at h    i  and policy based
refinement is independent of scur  the optimal path genuinely would depend on whether
door d  is open  the other path being slightly suboptimal in each case  while in theory
such a region could have arbitrarily large extent  it seems to be a relatively minor effect in
practice  here  for instance  it adds a couple of states  which is about    of  w   and it
was not found to be a real problem in any of the domains  or in the domains used in baum 
      
   

fibaum  nicholson   dix

    limitations
policy based refinement can only deal with cases where a single dimension makes a difference  when two dimensions are needed in combination  it will often miss them  for
instance  in the  keys problem each key is quite distant from the corresponding door and
policy based refinement will therefore never find the relationship between the two  at the
key  there appears to be no reason to pick it up  while at the door there appears to be no
means of unlocking it 
obviously  this can be fixed ad hoc by rewarding picking up keys for its own sake 
indeed  some domain formulations in the literature do exactly that  rewarding the agent
for partial achievement of the goal  however  that is not a clean solution  in effect  such
domain specifications cheat by providing such hints 
another problem is that policy based refinement does not provide for coarsening the
worldview  or for modifying it in other ways  for instance as execution progresses and the
planner needs to update the plan  indeed  policy based refinement ignores the initial state
s  altogether  or the current state scur in recurrent planning  thus it produces the same
solution regardless of which part of the problem the agent has actually been asked to solve 
this is a waste of computation in solving those parts which the agent is unlikely to actually
visit  and  perhaps more importantly  carries the penalty of the corresponding loss of
quality in the relevant parts 
the following sections describe proximity based worldview modification  which is needed
to solve domains where combinations of dimensions are important and which also makes
use of s  or scur   as appropriate 

   a proximity measure
in general  the worldview should be mostly concrete near the agent and its planned path
to the goal  to allow detailed planning  but mostly abstract elsewhere  to conserve computational resources  in this section we describe a measure  originally in baum   nicholson 
      which realises this concept  proximity p  which decreases both as a state is further
in the future and as it is less probable   this section extends the brief description of baum
and nicholson         in the following section we then present new worldview modification
methods based directly on the measure 
    motivation
the proximity p is a realisation of the intuitive concept of states being near the agent and
likely to be visited  as opposed to those distant from the agent and unlikely  it naturally
takes into account the current state scur in recurrent planning  or the initial state s  in
pre cursor planning  unlike policy based refinement which ignores them altogether  thus a
planner selecting worldviews based on this proximity measure will produce solutions tailored
to the particular scur or s  and will ignore parts of the mdp that are irrelevant or nearirrelevant to performance from that state  thus it saves computation that would otherwise
   baum and nicholson        used the word likelihood for this measure  we now prefer proximity to
avoid confusion with the other meanings of the word likelihood  munos and moore        use the word
influence for a somewhat similar measure in continuous domains 

   

fiproximity based non uniform abstractions for planning

be wasted in solving those parts which the agent is unlikely to actually visit  and  perhaps
more importantly  carries the advantage of the corresponding gain of quality in the
relevant parts  this allows the agent to deal with problems such as  keys which are beyond
the reach of policy based refinement 
implicitly  the agent plans that when and if it reaches those mostly abstract parts of the
state space  it will improve the approximation as appropriate  the planner thus continually
improves the policy  modifies the approximation and updates the focus of its planning based
on the current state scur   this means refining the regions in which the agent finds itself or
which it is likely to visit  and coarsening away details from regions that it is no longer likely
to visit or those which it has already traversed 
there are three aspects to proximity  temporal  spatial and probabilistic  firstly  the
temporal aspect indicates states that may be encountered in the near future  on an exponentially decaying scale  the second aspect is spatial  the nearness of states  in terms of
the state space  to the agent and its planned path  the spatial aspect is somewhat indirect 
because any spatial structure of the domain is represented only implicitly in the transition
matrix  but the proximity measure will reflect it  these two aspects are combined in the
proximity to give a single real number between   and   for each state  denoted p  p
for proximity   for the spatial aspect and  for the temporal aspect  this number can be
interpreted as a probability  namely the probability of encountering a state  and p
can be interpreted as the probability distribution over states  giving the final  probabilistic
aspect of proximity 
    calculation
the formula for the proximity p is similar to the formula for the value function  there
are three differences  firstly  instead of beginning from the reward function it is based on
an is current state function  cur  secondly  the transition probabilities are time reversed
 that is  the matrix is transposed   this is because the value calculation is based on the
reward function  which occurs in the future  after taking actions   while the is current state
function is based on the present  before taking actions  since the order of taking actions and
the function upon which the formula is based is reversed in time  a similar reversal must
b is used
be applied to the transition probabilities  thirdly  an estimated future policy 
b
b
instead of   in this estimate   is a stochastic policy defined by making  s  a distribution
over actions which assigns some constant probability to the current  s  and distributes the
remaining probability mass among the other actions equally  this distributed probability
mass corresponds to the probability that the policy will change sometime in the future 
or  alternately  the probability that the currently selected action is not yet correct  the
formula is therefore 
x

b     s p s  
pr s    s
   
p s   cur s    p
t

s

where
and

p is the proximity discounting factor     p     

   p if scur   s
cur s   
 
otherwise
   

   

fibaum  nicholson   dix

p
the constant  p was chosen for the current state function so that s p s  converges
to    in other words so that p is a probability distribution  if checked in the near future 
the agent has a probability of p s  of being in state s  assuming it will follow the policy 
and near future is defined so that the probability of checking at time t is proportional to
pt  that is  with p interpreted as a stopping probability   as with the value calculation 
one can instead solve the set of linear equations
x

b     s p s  
pr s    s
   
p s    cur s    p
s

t

or  in matrix notation 
 i  p tbt  p   cur

   

b and i is the identity
where tb is the transition matrix induced by the stochastic policy 
matrix  the implementation uses this matrix form  as shown in algorithm    the proximity
measure needs little adjustment to work with the non uniformly abstract worldview  s is
simply replaced by w in     and      with scur   s becoming scur  w 
algorithm   proximity calculation
solve this matrix equation for p as a linear system 
 i  p tbt  p   cur
the measure has two tuning parameters  the replanning probability and the discounting
factor p   the replanning probability controls the spatial aspect  it trades off focus on the
most likely path and planning for less likely eventualities nearby  similarly  p controls the
temporal aspect  the smaller p is  the more short sighted and greedy the planning will
be  conversely  if p is close to    the planner will spend time planning for the future that
might have been better spent planning for the here and now  this should be set depending
on the reward discounting factor   and on the mode of the planner  here we use p        
replanning probability     
example proximities for the  doors problem are shown in figure   for the initial situation  agent at h    i  all doors closed  and a possible situation later in the execution
 agent at h    i  all doors closed   larger symbols correspond to higher proximity  one can
immediately see the agents planned path to the goal  as the large symbols correspond to
states the agent expects to visit  conversely small proximities show locations that are not
on the agents planned path to the goal  for example  the agent does not expect to visit
any of the states in the south western room  especially once it has already passed by door
   similarly  the proximities around the initial state are much lower when the agent is at
h    i  as it does not expect to need to return 
    discussion
one interesting feature of the resulting numbers is that they emphasise absorbing and nearabsorbing states somewhat more than might be intuitively expected  however  considering
   

fiproximity based non uniform abstractions for planning

x  

 

 

 

 

 

 

 

 

 

x  

y  

y  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

hx      y    i

 

 

 

 

 

 

 

 

 

hx      y    i

figure    proximities in the  doors problem for s  and a possible later scur   symbol size
logarithmic  proximities range from     to        p         replanning probability     

that absorbing states are in general important  this is a good feature  especially since
normally the planner will try to minimise the probability of entering an absorbing state
 unless it is the goal   this feature should help ensure that absorbing states are kept in
mind as long as there is any chance of falling into them  dean et al          for instance 
note that in their algorithm such undesirable absorbing states along the path to the goal
tend to come up as candidates for removal from consideration  due to the low probability of
reaching them with the current policy   and have to make special accommodation for them
so they are not removed from consideration  with the proximity measure emphasising these
states  such special handling is not necessary 
in contrast with this approach  kirman        uses the probabilities after es steps 
where es is  an estimate of  the number of steps the agent will take before switching from
the previous policy to the policy currently being calculated  this assumes that es can be
estimated well  that the current policy is the policy in the executive  and that the oneplanning cycle probability is an appropriate measure  in fact one would prefer at least a
two planning cycle look ahead  so that the agent not only begins within the area of focus of
the new policy  but also remains there throughout the validity of that policy  and probably
longer  since the planners foresight should extend beyond its next thinking cycle  more
philosophically  this reliance on the planning cycle length is not very desirable  as it is an
artefact of the planner rather than intrinsic in the domain 
a somewhat related approach is prioritised sweeping  see for instance barto  bradtke 
  singh         like the present approach  it defines a measure of states which are in
some way interesting  unlike this approach  it then applies that measure to determine
   

fibaum  nicholson   dix

the order in which the formulae of the  and v calculation are applied  so that they are
applied preferentially to the interesting states and less frequently to the uninteresting or
unimportant states  it is well known that the order of calculation in the mdp planning
algorithms can be varied greatly without forfeiting convergence to the optimal policy  and
prioritised sweeping takes advantage of this  often it is done on a measure such as change
in v in previous calculations  but some approaches use look ahead from the current state 
which is in some ways a very simple version of proximity  in fact  it corresponds to a
threshold on p with replanning probability set to     the proximity measure p might well
be a good candidate for this approach  apply  and v calculation to states chosen directly
according to p as a distribution  
munos and moore        use an influence measure on their deterministic continuous
domains  which is very similar to p  in fact  the main difference is that their measure does
not have the two parameters  it re uses the same  and has no replanning probability
 effectively it is zero   this means that it cannot take into account replanning  neither in
the difference in the horizon that it entails  nor in the possibility that the policy may change
before it is acted upon  absorbing states  for instance  would not be emphasised as they
are with proximities 

   proximity based dynamic abstraction
the proximity measure described in the previous section is used to focus the planners attention to areas likely to be useful in the near future  firstly  that means that the worldview
should be made to match the proximities  by refining and coarsening as appropriate  secondly  since the proximity measure takes into account the current state  this method will
automatically update the worldview as the agents circumstances change in the recurrent
mode  that is  when planning and execution are concurrent 
    refinement
high proximity indicates states which the agent is likely to visit in the near future  the
planner should therefore plan for such states carefully  if they are abstract  this is reason
to refine them so as to allow more detailed planning  such states with high proximity are
therefore considered candidates for refinement 
high proximity is defined by a simple threshold  as shown in algorithm   
when refinement occurs  an anomaly sometimes appears  like the anomaly which led to
the policy based refinement method  it arises from different levels of abstraction  but here 
it is not an adjacent more abstract state that causes the problem  but rather a recentlyrefined one  when a state is refined  the values v of the new states are initially estimated
from the states previous value v   however  typically  this means that some of them will
be overestimated and others underestimated  when the policy is being re calculated  the
state with the overestimated value will be attractive 
since the problem directly follows from the moment of refinement  it is self correcting 
after a few iterations  the planner converges to the correct policy and values  however 
   if retaining the theoretical guarantee of convergence to   is desired  care would have to be taken since

p is zero for states which are not reachable from the current state  in practice  of course  optimality or
otherwise as to unreachable states is immaterial 

   

fiproximity based non uniform abstractions for planning

algorithm   proximity based refinement
stochastically choose a dimension d
for all worldview states w do
if p w    threshold and w is abstract in d then
   replace w with
group of states concrete in d   
 anew
w
is concrete in d
new
for all w
 
do
dimension d of wnew   dimension d of w d    d
w  w   wnew  
new
 wnew     w   v  wnew    v  w   p wnew     w w    p w 
w  w    w     discarding also the stored  w   v  w  and p w    

while it is doing so  transient anomalies appear in the policy  and in the worst case  the
planner may replan for some other path  then refine more states and re trigger the same
anomaly  rather large parts of the state space can be spuriously refined in this way 
this occurs because of the combined  and v calculation phase  which may update
 before v has had a chance to converge  the solution is to create a variant phase  v
calculation only  which replaces the  and v calculation phase until the values stabilise 
we do this for two iterations  which appears to be sufficient  an alternative solution would
have been to copy the difference between values at adjacent  more concrete states where
possible  thus obtaining better estimated values at the newly refined states  however  since
the simpler solution of v  only calculation works satisfactorily  this more complex possibility
has not been further explored 
    coarsening
low proximity indicates states which the agent is unlikely to visit in the near future  the
planner therefore need not plan for such states carefully  usually  they will already be
abstract  never having been refined in the first place  however  if they are concrete  if
they have been previously refined  this is reason to coarsen them so as to free up memory
and cpu time for more detailed planning elsewhere  such states with low proximity are
therefore considered candidates for coarsening 
proximity based coarsening is useful primarily in an on line planning scenario with recurrent planning  as the agent itself moves through the state space and the current state
scur changes  so do the states that are likely to be visited in the near future  this is especially useful if the agent finds itself in some unexpected part of the state space  for instance
due to low probability outcomes  or if the agent has planned a path leading only part way
to the goal  perhaps up to a partial reward   in any case  however  the parts of the state
space already traversed can be coarsened in favour of refinement in front of the agent  
one might also imagine that as planning progresses  the planner may wish to concentrate
on different parts of the state space and that coarsening might be useful to cull abandoned
explorations and switch focus  however  we have not observed this with any of our domains
   states already traversed cannot be discarded  even if the agent will never visit them again  since the
worldview is a partition and since the agent does not necessarily know whether it will need to revisit  or
end up revisiting  those states 

   

fibaum  nicholson   dix

and found that in pre cursor mode  coarsening generally worsens the quality of the policies
with no positive contribution 
coarsening proceeds in three steps  as shown in algorithm    the first step is very
similar to proximity based refinement  each time the proximity based coarsening phase is
invoked  the worldview is scanned for states with low proximity  below a threshold   which
are put in a list of candidates  the second step is more tricky  coarsening needs to join
several states into one  however  the representation does not allow arbitrary partitions
as worldviews and therefore does not allow the coarsening together of an arbitrary set of
worldview states  the planner must therefore find a group of states among these lowproximity candidates which can be coarsened into a valid worldview state  such groups can
be detected by the fact that they differ in one dimension only and have the same size as
the dimension  therefore covering it completely  finally  the groups that have been found
are each replaced with a single more abstract state 
algorithm   proximity based coarsening
   collect candidates for coarsening   
candidates   w   p   threshold 
   find groups of candidates that can be coarsened together   
   partition candidates according to the pattern of abstract and concrete dimensions   
patterns  candidates     wa   wb     d   wa is concrete in d  wb is concrete in d 
groups  
for all p  patterns do
for all dimensions d do
if states in p are concrete in d then
   partition p by all dimensions except d  giving potential groups   
potgroups  p     wa   wb     d    d   dimension d of wa   dimension d of wb  
   add all potential groups that are the same size as dimension d to groups   
groups  groups   g  potgroups    g     sd   
   replace each group of states with a single  more abstract state   
for all g  groups do
if g  w then
stochastically choose
wa from g
 new
w
is abstract in d
construct wnew  
dimension d of wnew   dimension d of wa d    d
w  w   wnew  
p
  p
new   


 wnew     wa    v  wnew     g 
wg v  w   p w
wg p w 

w  w   g    discarding also the stored  w   v  w  and p w  for all w  g   

in some cases  it may be impossible to coarsen a section of the worldview despite low
proximity  due to a situation somewhat akin to grid lock  probably the simplest example
is the one in figure    which shows a worldview of five states in a three dimensional binary
specific state space  with three of the states ignoring a different dimension each  while the
remaining two take into account all three  in this situation  no group of states can be
   

fiproximity based non uniform abstractions for planning

s 

 

 

 

 

 

 

 

 

s 

 

 

 

 

 

 

 

 

s 

 

 

 

 

 

 

 

 

w 

w 

w 

w 

w 

figure    a non uniform worldview that cannot be immediately coarsened  the state space
has three binary dimensions  eight states   the worldview has two concrete states 
w  and w    and three abstract states  w    w  and w    each abstract in a different
dimension 

coarsened in any single dimension  before any coarsening is possible  one of the states must
first be refined  but if they all have low p they are not candidates for proximity based
refinement  because of this  integration of a uniform abstraction method into coarsening
would also not be as straightforward as for selecting the initial worldview or for refinement 
unless the worldview is kept uniform  however  even with a non uniform worldview it would
not be difficult  for instance  the dimension could simply be removed only where possible
rather than everywhere 

   results
we run the algorithm over a range of different domains to demonstrate our approach  the
domains divide into two broad groups  the first group consists of the grid navigation
domain only  this was the domain on which intuition was gathered and preliminary runs
were done  however  so while problems in this domain show how well the approach performs 
they cannot show its generality  the second group consists of domains from the literature 
demonstrating how well our approach generalises 
    experimental domains
we introduce our domains in this section  the first five problems are in the grid navigation
domain  two already described in section      shown in figure    with three additional
problems  the remaining domains are based on domains from the literature  in particular
as used by kim        and barry        
as described in section      problems in the grid navigation domain shown in figure  
all have x and y dimensions of size     three door dimensions  binary  open closed  and
a damage dimension  also binary   so far this is the  doors problem  while in the  keys
problem  there are keys which the agent must pick up keys to open the corresponding doors 
the three remaining problems are  key  shuttlebot and       the  key problem is
similar to  keys  except that the agent is only capable of holding one key at a time  so
that instead of three binary dimensions for the keys  there is one four valued dimension
indicating which key the agent holds  or none   the shuttlebot problem introduces a
cyclic goal  with an extra loaded dimension and with the damage dimension tri valued 
   

fibaum  nicholson   dix

 a  grid navigation domain
keys in world
problem
 doors
 
 key
 
 
 keys
 
shuttlebot
    
 

keys held at same time

 
     or  



note

cyclic
tiled

dimensions
 
 
 
 
 

 s 
     
     
      
     
       

 b  robot   k domain
problem
robot     
robot     
robot     
robot     

dimensions
  
  
  
  

 s 
      
       
          
           

 c  factory domain
problem
s factory
s factory 
s factory 
 d  tireworld domain
locations
problem
tire small
 
 
tire medium
tire large
  
  
tire large n 

initial
n 
n 
n  
n 

dimensions
  
  
  

goal
n 
n 
n 
n 

 s 
       
         
          

route length
 
 
 
 

dimensions
  
  
  
  

 s 
     
       
                 
                 

table    experimental domains and problems  each with the dimensionality of the state
space and its size 

while the      variant increases the size of the problem by tiling the grid    in each
direction  by having two extra dimensions  xx and yy  of size      table   a  summarises
the problems in this domain 
the next two domains are based on those of kim         firstly  the robot   k domain 
which are based on kims        robot k domain but reducing the number of actions to
four  the robot   k domain problems consist of a cycle of k rooms as shown in figure   
where each room has a light  analogous to the doors of the  doors problem in that they
enable the agent to move  the four actions in our variant are to go forward  turn the light
in the current room on or off  or do nothing  the original formulation allowed the agent
any combination of toggling lights and going forward  for a total of  k   actions  but we
have reduced this as our approach is not intended to approximate in the action space  the
goal is to move from the first room to the last  there are k     dimensions for a state space
of k k states  as listed in table   b  
   

fiproximity based non uniform abstractions for planning

 
k  

 

 

 
 

figure    the robot   k domain 
drill b
part b 

shape b

drilled

polish b

polish b

shaped

dip b
polished

spray b
handpaint b

painted
glue

connected

bolt
drill a
part a 

shape a

shaped

drilled
polish a

polish a

dip a
polished

spray a

painted

handpaint a

figure    the factory domain 
kims        factory domain   is a series of variants on a simple manufacturing problem  represented purely in predicates  that is  dimensions of size     the agent is to make a
product from two parts which must be drilled  painted and so on and finally joined together 
figure   shows a very simplified diagram  omitting most of the interactions and options 
for instance  to achieve the painted predicate  the agent may spray  dip or handpaint the
object  to connect the two objects  it may use glue or a bolt  and only the latter requires
that they be drilled   and so on  unlike the other domains  partial rewards are available to
the agent for achieving certain subgoals  the problems used here are listed in table   c  
the final domain is the tireworld domain from the      icaps ipc competition
 littman  weissman    bonet        as used by barry         in this domain  a robotic
car is trying to drive from point a to point b  the car has room to carry one spare tire
and some locations have additional spare tires at them  at these locations  if the car is
not carrying a spare  it can pick one up  if there are n locations for the car  there will be
 n     binary dimensions in the problem  as follows  n dimensions are used to represent
the location of the car  the only valid states are states where only one location dimension
is true  but this is not explicitly stated anywhere in the domain    another n dimensions
are used to represent which locations have a spare tire and which do not  the final two
dimensions represent whether the car is carrying a spare and whether it has a flat tire 
    this domain was previously used by hoey  st  aubin  hu  and boutilier        and is based on the
builder domain of dearden and boutilier        which was adapted from standard job shop scheduling
problems used to test partial order planners 
    we touch on this aspect in our discussion in section    but in any case include the domain without
change in order to facilitate comparison with the literature 

   

fibaum  nicholson   dix

n 

n  
n  
n 

n 

n 
n  

n  

n 

n 

n 

n 
n  
n 

n 

n  

n 

n  

n  
n 

n 

n 
n 

n  

n 

n  
n 
n 

n 

tire small

n 

n  

tire medium

n 

tire large

 and goal   
 locations 
figure    the tireworld domain problems  indicating the initial   

barry        uses two of the tireworld problems  labelled small and large  the
small tireworld problem has   locations for    variables and    actions  while the large
one has    locations for    variables and     actions  curiously  in the large problem  there
is a direct road between the initial and goal locations  so that it only takes a single action
to solve the problem  this makes it difficult to assess whether barrys method has  in fact 
scaled up  in addition to these two  we created a medium sized tireworld  with   locations
and    variables  by removing locations from the large tireworld and moving the initial
location to n   further from the goal  these variants are listed in table   d  and shown in
figure    the final variant  tire large n   not shown  is identical to the large tireworld
except the initial location is again moved to n  
    direct evaluation of policies with pre cursor deliberation
with the smaller problems such as  doors   key  and  keys  we can directly evaluate the
approximate policies produced by the planner running with pre cursor deliberation  these
problems are small enough that we can use an exact algorithm to calculate the actual value
function corresponding to these approximate policies  as noted in section      this can be
useful as it involves fewer potentially confounding variables  but it does not exploit the full
potential of our approach   
table   a  shows such results for policy based refinement only  that is  with the
proximity based methods  algorithms      and    disabled  for each problem  the table
lists the size of the problem  s  and the value of the optimal solution at the initial state
    proximity based coarsening  algorithm    is primarily aimed at regions of state space that the agent
has already traversed  but with pre cursor deliberation there is no traversal  coarsening would therefore
be expected to bring limited benefit with pre cursor deliberation and direct evaluation would not be
meaningful to evaluate its performance  it is therefore only evaluated with recurrent deliberation 

   

fiproximity based non uniform abstractions for planning

olu
tio
nv
alu
wo
e
rld
vie
w
siz
e
rel
ati
ve
wo
rld
vie
w
siz
pla
e
nn
er
s
of esti
sol ma
uti te
on
val
ue
act
ua
ls
olu
tio
nv
alu
e

ize

op
tim
al
s

sta
te
sp a
ce
s

dis

cou
nti
ng

fac
tor

v   s     representing the costs and results for exact planning  these are followed by the
size of the worldview  w  as an absolute number and as a percentage of  s   the planners
estimate of the value of its solution at the initial state v  s     and the actual value of its
solution at the initial state v  s     in the first half of each part of the table the discounting
factor is              while in each second half it is          averages over    runs
are shown and the planner is run to             was chosen as an approximation to 
 it is assumed that       phases is sufficient for the planner to converge  in practice 
convergence generally took place much earlier  it is not detected  however  because of the
overall assumption that the planner continues to plan forever  responding to any changing
inputs  which makes convergence somewhat irrelevant 


problem
 s 
v   s   
 w 
 a  policy based refinement only
     
          doors
           
     
 key
           
 keys
            
     
     
    
 doors
           
 key
           
     
     
 keys
            
 b  proximity based refinement only
          doors
                   
 key
                   
 keys
                    
    
 doors
                   
 key
                   
 keys
                    
 c  both policy  and proximity based refinement
          doors
                   
 key
                   
 keys
                    
    
 doors
                   
 key
                   
 keys
                    

 w 
 s 

v  s   

v  s   

   
    
    
   
    
    

     
         
         
     
     
     

     
          
          
     
     
     

   
   
   
   
   
   

     
         
         
     
     
     

     
         
         
     
     
     

   
   
   
   
   
   

     
         
         
     
     
     

     
         
         
     
     
     

table    results for direct evaluation of policies with pre cursor deliberation with three
different refinement methods  evaluated after       phases 

   

fibaum  nicholson   dix

the results in part  a  of the table divide neatly into two types  without keys   doors
problem   the planner succeeds in all ten runs  getting perfect policies for the given starting
state  in the other two problems   key and  keys  planning invariably fails  in these two
problems  the agent must pick up a key while far from the door it opens  and this version
of the planner simply cannot think ahead to that extent  for all three of these  the planner
is somewhat optimistic  estimating a better value than it obtains and in some cases even
better than optimum  for instance  in the  doors problem with              the planners
estimate of the value v  s    is        which is better than both the true value and the
optimum  v  s      v   s             the fractional  w  in the table are due to its being
averaged over ten runs  the final size of the worldview sometimes depends to some extent
on the order in which the dimensions or the states are refined  and this order is randomised
between the runs  for instance  in the  doors problem with              w has various
sizes ranging from     to     states at the end of each of the ten runs  with an average of
      
the results in part  a  are similar for the two values of   the main difference is that the
smaller  leads to smaller numbers  for instance  the value indicating failure is        
 
 
when             but only    when          the values tend to be in multiples of  
 
and with the smaller value of  here    is    rather than          in some cases  this
smaller range can make differences less obvious  for instance  in the the estimated value
 
column  v  s      it is not very clear whether the numbers are approximations to     
 
 and a failure to reach the goal  or     
minus a small number  representing success  
 
represent rewards or costs
while units represent once off rewards or costs  multiples of  
to be obtained in perpetuity  however  this is the expected and desired behaviour  a smaller
 represents a disinterest in the distant future  so that a reward or cost in perpetuity is not
much more important than a once off reward or cost 
table   b  shows the results of ten runs in pre cursor mode to      with proximitybased refinement only  no policy based refinement and no coarsening  for each problem
and   as can be seen  the  doors problem is solved optimally in all cases  this is not
surprising  as it is not a complex problem 
the  key and  keys problems are more interesting  the figures in table   b  arise as the
average of about    successful runs  which have values close to or equal to the optimal values
v   s     and about    unsuccessful runs which have values of          for             
the planner found a successful policy in   of the    runs in the  key problem and   times
out of    in the  keys problem  similarly for the         case    times and   times 
 
respectively   but since the optimal path is quite long compared with  
  so that success
means a reward of        or        while failure is punished by     the effect is more
difficult to discern 
table   c  shows the results for proximity based refinement and policy based refinement combined  no coarsening   naturally  the  doors problem for which either refinement
method alone already obtained the optimal policy shows no improvement  only the worldview size  w  differs slightly from table   b   as policy based refinement is sometimes more
directed  so the  w  tend to be slightly smaller than with the more exploratory proximitybased refinement alone  but larger than with policy based refinement alone 
   

fiproximity based non uniform abstractions for planning

the other two problems   key and  keys  show improvement compared with either of
the refinement methods alone  they are now solved in    of the runs for             
the values           and           represent averages between   and   unsuccessful runs
and   and   successful ones  compared with   and   successful runs for proximity based
refinement only and no successful runs for policy based refinement only  for          the
 key problem is again solved on   of the runs  but due to the discounting and the length
of the path  the goal is very near the horizon  again  with success meaning a reward of
      while a failure receives     the distinction is not very great  the  keys problem
with         did not find a solution at all with these parameters  so it receives a uniform
   for each of its runs  for a suboptimality of about one unit 
the behaviour during these runs is generally quite straightforward  typically  after
initially calculating that the agent cannot reach the goal with the initial worldview  the
worldview size gradually increases  then plateaus  there is no coarsening here  nor movement of the agent  so no other behaviour is really possible  in the successful runs  the
planner plans a route to the goal at some point during the increase  when the worldview becomes sufficient  and v  s    quickly reaches its final value  rarely  the v  s    may oscillate
once or twice first  we omit the graphs here  but they are presented by baum        
    evaluation by simulation with recurrent deliberation
in larger problems  performance can be evaluated by simulation  running the agent in
a simulated world and observing the reward that it collects  in such problems  direct
evaluation is not possible because calculating the actual value function using an exact
algorithm is no longer tractable  simulation with recurrent delibertion is also the context
in which coarsening can be evaluated  for comparison  this section presents results for the
 keys problem evaluated by simulation  both without and with coarsening 
figure   shows a representative sample of the results for simulation on the  keys problem
with both refinement methods and no coarsening  which is the same combination of options
as shown in table   c  in the previous section  evaluated by simulation rather than directly 
each small graph shows a different individual run  as can be seen  the agent behaves
reasonably when working in the recurrent planning mode against the simulation 
the left vertical axes on the graphs represent reward r  plotted with thick red lines 
in run   of figure    for instance  the agent starts off receiving a reward of   for each
step  meaning not at goal  no damage in this domain  from about     onwards  it receives
a reward of   per step  meaning at goal  no damage  the right vertical axes are the
worldview size  w   with thin blue lines  they are shown as details throughout this section 
that is  scaled to the actual worldview sizes rather than the full   s  ranges  taking run  
in figure   again  we see that  w  grows relatively quickly until about     then continues
to grow slowly and eventually levels out a little below        the full state space  for
comparison  is         in run    the agent received reward similarly  but the state space
grew longer  eventually levelling out somewhat above       in runs   and    the agent
failed to reach the goal and continued receiving the reward of   throughout  in run    the
worldview size levelled out a little over       while in run   it steadily grew to about      
the horizontal axes are simulated world time  corresponding to the discrete time steps
of the mdp  there are two other time scales in the simulation  wall clock time  indicating
   

fibaum  nicholson   dix

  

 w 

r

  

    
    

 

    
    

 

    

    

    

    

    

    

    

  

 w 

r

    

  

    
 

  

   

   

   

    

 
   

 

 w 

r

  

   

time

  

   

   

time

r

  

    
    

 

 w 

    
    

 

    

    

    

    

    

    

    

  

    

  

    
 

  

   

 
   

   

   

    

 
   

 

time

  

   

   

   

 
   

time

figure    simulation results   keys problem  policy based and proximity based refinement 
no coarsening  four runs   reward  left axes  thick red lines  and worldview size
 w   right axes  thin blue lines  detail  against world time  horizontal axes  

the passage of real time  and the number of phases the planner has performed  the simulation is configured to take   time step per   s of wall clock time  the number of phases
r
is not controlled and is simply given by the speed of the     ghz intel
  cpu and an implementation coded for flexibility rather than efficiency  ideally  the agent should gradually
move in the general direction of the goal during planning  as this simplifies the problem 
but not so fast that the agent runs too far ahead of the abstraction in the planner 
the planner algorithm does not terminate  since the planner is assumed to keep planning
 and the agent to keep acting  indefinitely  in goal oriented domains  such as most of the
examples in this paper  one might consider achieving the goal to be such a termination
condition  but  a  the example domains assume that the agent will continue with the same
goal as a goal of maintenance  albeit trivial   b  it does not apply at all to non goal oriented
domains and  c  even in goal oriented domains it is not clear how to apply the condition in
the case where the agent fails to reach the goal  in the simulation  therefore  runs were either
terminated manually  when they succeeded or when it appeared that no further progress
   

fiproximity based non uniform abstractions for planning

  

 w 

r

 

  

 

  

    
    
    
    
    
    
    
    
    
 
                              

 

  

 

time

 w 

r

    
    
    
    
    
    
    
    
    
 
                              
time

figure     simulation results illustrating the effect of coarsening on worldview size   keys
problem  policy based refinement  proximity based refinement and coarsening
 two runs  

was likely to be made  or run for a fixed number of world time steps  selected based on the
manually terminated runs with some allowance for variation 
because there is no coarsening in figure    the worldview sizes are monotonic increasing 
different runs refined differently  both the domain and the algorithm are stochastic  at
the beginning of planning  the agent is receiving a reward of   per step  because it is not
yet at the goal  as the worldview size increases  the planner eventually finds a policy which
leads to the goal in runs   and    as can be seen by the better reward   obtained in those
runs  there is a simple relationship between worldview size and performance  runs which
worked with a worldview of about       or larger generally succeeded  those with smaller
worldviews generally did not  on the vast majority of the runs          the agent reached
the goal 
when coarsening  algorithm    is activated  compared to the situation when it is turned
off  the reward gathered by the agent declines slightly  but it still reaches the goal on the
vast majority of runs          figure    shows two of the runs  one successful and one
unsuccessful  for the  keys problem with proximity based coarsening as well as the two
refinement methods  in contrast with figure   where only the two refinement methods are
used  note the effect of the interleaving of the refinement and coarsening  the worldview
size  w   thin blue line  is no longer monotonic  instead being alternately increased and
decreased  which shows up as a jagged line on the graph  slightly fewer of the runs reach
the goal  some decline in solution quality is expected  however  since the goal of coarsening
is to reduce the size of the worldview 
for completeness  we have also tested the agent with both proximity based methods
 algorithms   and    active but with the policy based refinement  algorithm    deactivated 
in this configuration  the agent collects no reward  it generally takes some steps toward
the goal  but without the more directed policy based refinement  the largely exploratory
proximity based methods do not discover the keys and consequently cannot reach the goal 
   

fibaum  nicholson   dix

    the effect of the discounting factor
the shuttlebot problem is similar to the  doors problem but requires the agent to move
back and forth between two locations repeatedly  it is interesting because in preliminary
runs in pre cursor mode it was not solved at all for             while being solved optimally
for          it was considered whether the             case might behave better under
simulation  if the agent took advantage of the possibility of planning only for the nearest
reward and then replanning once that reward is obtained  after all  the agent could function
well even if none of the policies were a good solution by itself  however  as illustrated in
figure     the agents behaviour was very similar to the pre cursor case  for             
 a  refinement only  run    and  b  with coarsening  run    it would pick up the reward
immediately adjacent to s    but no more than that  again  setting the planners discounting
factor  to        c  with coarsening  runs   and    provided much better performance   
note again the effect of the balance of refinement and coarsening in  b  and  c   the
worldview size  w  is nice and steady throughout the runs  though admittedly at a fair
fraction of  s           
    initial worldview
for some of the problems  the standard initial worldviews are too large for the planner  even
modified  smaller initial worldviews obtained by only enabling the nexus step of algorithm  
and disabling the reward step are too large  disabling both the reward and the nexus steps
results in a singleton initial worldview  w    s   which treats the entire state space s
as a single  very  abstract worldview state  unfortunately  this means that the planner
starts with very little in the way of hints as to the direction in which to refine and  at least
initially  no other information on which to base this crucial decision  the upshot is that it
collects no reward  in some cases it remains at the initial state s    in others moves around
the state space  sometimes for some distance  other times in a small loop  but it does
not reach the goal or any of the subgoals   
this is the situation with the factory domain problems of kim         and in fact the
agent collected no reward during any of the simulated runs  even though in quite a few runs
substantial actions were taken  a similar result occurs for the   x   problem  in our grid
navigation domain   no reward was obtained by the agent in this problem  because the
standard initial worldview is somewhat too large for the planner and  again  the singleton
initial worldview does badly  at best  on some runs  the agent took a few limited steps in
the general direction of the goal 
an interesting case is the tireworld domain  again  tire large is too large with the
standard initial worldview and fails to obtain a solution with the reward step of the initial
worldview only  however  with a manually chosen initial worldview that refines the locations
along the path from the start state to the goal before planning begins  the planner solves
not only tire large  but also tire large n  in     of the runs  in one of them after less
than one minute  although that is atypical  
    the rewards appear as two horizontal lines in runs   and    one solid and one broken  because the task
is cyclic  and the agent collects a reward of   twice in each cycle and a reward of   on all the other steps 
    further details of these unsuccessful runs  including the  w  behaviour  are given by baum        

   

fiproximity based non uniform abstractions for planning

 a               no coarsening  one run 
r
 w 
  
    

 b               with coarsening  one run 
r
 w 
  
    

    

    

    

 

    

 

    

    

    

    

    

    

    
 

    
 

    

    

   
 

    

    

    

   

 
    

 

   

time

   

   

   

 c           with coarsening  two runs 
r
 w 
  
    

  

 w 

r

    

    

 

    

    

    

    

    

    

    
 

    
 

    

    

   
    

    

    

    
    

    

 

 

 
    

time

   

 
    

 

time

    

    

    

    

 
    

time

figure     simulation results illustrating the effect of the discounting factor  shuttlebot
problem  policy based and proximity based refinement 

    worldview size and quality
finally  we consider the effect of worldview size and quality on the robot  domain  where
the agent moves through a series of rooms with lights  this domain is an excellent example
where the simulated agent works well  in all runs of the robot      and robot      problems
the agent thought for a small amount of time  then quickly moved to the goal and stayed
there  with only very small worldviews  as can be seen in figure    for the robot     
problem  four representative runs are shown  two with the two refinement methods only
 runs   and    and two with all three methods  runs   and     in all four runs  the worldview
sizes  w  are reasonable  consider that the full state space contains almost half a million
states  so that a       state worldview represents just a fifth of a percent  despite this small
worldview size  however  the planner is effective  after only a few dozen phases  the agent
has reached the goal  the planner works well for robot      and robot      
for comparison  kims        largest robot k problem is robot     though since
robot    has                 actions while our robot   k domain problems have    a
   

fibaum  nicholson   dix

 a  no coarsening  two runs 
r
  

 w 

  

    

 w 

r

    

    
    

 

 
   

   

   

   

   

   

 

 
   

   

 
                                

 
                                

time

time

 b  with proximity based coarsening  two runs 
r
 w 
  
  
    

 w 

r

    

    
    

 

 
   

   

   

   

   

   

 

 
   

   

 
                                

 
                                

time

time

figure     simulation results  robot      problem               policy based and proximity based refinement  with and without proximity based coarsening 

direct comparison would not be valid  on the other hand  our values of k         and
so on  are not necessarily powers of    since  unlike that of kim  our domain specification
always considers room numbers atomic rather than binary numbers  so there is no particular
advantage to powers of   
the results for the robot      problem are beginning to be more interesting than those
for robot      and robot       in figure    a   showing two of the runs with no coarsening
 runs   and     the agent succeeds reasonably promptly and with reasonable worldview
sizes  however  as illustrated in figure    b   when coarsening is active the planner fails to
reach the goal on some of the runs  about      for example  run    and succeeds on others
 about      for example  run     the state space contains almost    million states  so the
successful worldviews in figure    are of the order of       of the full state space size 
figure    shows four representative runs of the robot      problem  again  a  two without coarsening  runs   and    and  b  two with all three methods  runs   and     this problem has a state space of              million states  and the effect noted for robot      is
   

fiproximity based non uniform abstractions for planning

 a  no coarsening  two runs 
r
  

 w 

  

    

 w 

r

    
 

    

    

 

 

    

    

    

    

    

    

    

    

 

    

    
 

  

   

   

   

    

 
   

 

  

   

time

   

   

 w 

r

    
 

 

 

    

    

    

    

    

    

    

    

 

    

    
   

   

   

    
    

    

  

 
   

time

 b  with proximity based coarsening  two runs 
r
 w 
  
  
    

 

    

    

 
   

 

time

  

   

   

   

 
   

time

figure     simulation results  robot      problem               policy based and proximity based refinement  with and without proximity based coarsening 

much more pronounced here  without coarsening  the planner tends to much larger worldviews   these large worldviews then cause the planner to run slowly  as noted in section    
above  the horizontal axes are world time  not planning time  the relation between the two
varies quite significantly between the runs  from more than two policies per time step
with the smaller worldviews to less than one in ten time steps when the worldviews grew
large 
as far as reaching the goal is concerned  the two cases are similar  again  the successful
runs are those which maintain a reasonably sized worldview  such as runs   and    runs
where the worldview size grows big invariably fail  runs   and     the difference is that this
time  the smallest successful worldview  run   in figure     used around       well chosen
worldview states  which is just           of the full state space  if the worldview grows
beyond a miniscule fraction of the state space  and even the        state worldview of
    note that run   is plotted with a different scale on the worldview size  w  axis compared to runs     
and    this makes the details of their behaviour easier to see  but makes the size of run   less obvious 

   

fibaum  nicholson   dix

 a  no coarsening  two runs 
r
  

 

 

 w 

  

     
     
     
     
     
     
    
    
    
    
 
                              

 w 

r

    
    

 

    
    
    
    

 

    
    
 

time

 
                              
time

 b  with proximity based coarsening  two runs 
r
 w 
  
  

 w 

r

    

    

    
 

    
 

    

 

    

    

    

    

    

    

    
 

    

    

    
 

   

   

   

    

 
    

 

time

 
                           
time

figure     simulation results  robot      problem               policy based and proximity based refinement  with and without proximity based coarsening  note the
different scales on the worldview size  w  axis for run  a   

run   in figure    is only           the planner will stall and no further progress will be
possible  even in this challenging environment  the agent reaches the goal in almost half
the runs 

   discussion
section   presented results across a range of experimental domains showing that our method
successfully finds solutions to the planning problem using much less than the full state space 
as well as some of its limitations  in this section we discuss these results and analyse the
features of domains which our method can exploit and those which give it difficulty 
on smaller problems  we could directly evaluate the policies produced by our method
in pre cursor mode  allowing us to better isolate the behaviour of the planner  without the
proximity based methods  the worldviews were quite small and the planner could only solve
   

fiproximity based non uniform abstractions for planning

the  doors problem  however  even this is better than a uniform abstraction  which could
do very little here  even an oracle could at best remove one door and its key in  keys or
two doors in  doors  giving a     relative worldview size  however  if the planner then
used a uniform distribution over the removed dimension  the agent would fail anyway  since
opening the doors that were left in the worldview would be harder than hoping for the best
on the assumed     open door which is actually closed  to succeed  it would also have to
deduce that the abstracted doors should be considered closed  a considerable feat  this is a
function of the sample domains  in other circumstances  uniform abstraction could be very
effective  either as a pre processing step to our approach or integrated with our w selection
methods 
we expected that turning on proximity based refinement  algorithms   and    would
lead to larger worldviews  in general  one would expect larger worldviews to yield better
solutions and smaller worldviews to yield worse solutions  but for lower computational cost 
this means that proximity based refinement should  in general  improve the solution quality 
the results corresponded to this expectation  the worldviews were indeed larger and the
solution quality was higher 
in larger problems  performance could only be evaluated by simulation  running the
agent in a simulated world and observing the reward that it collects  in such problems 
direct evaluation was not possible because calculating the actual value function using an
exact algorithm was no longer tractable  section     therefore presented results for the
 keys problem for comparison with those obtained by direct evaluation of policies with
pre cursor deliberation discussed above  as can be seen  the results correspond  crossconfirming the evaluation methods 
in addition  simulation with recurrent delibertion is the context in which coarsening
could be evaluated  when proximity based coarsening was activated  compared to the
situation when it is turned off  the reward gathered by the agent declines slightly  this
impression of worse performance is somewhat misleading  it is due to the fact that this first
comparison takes place on one of the small problems  which the planner was able to solve
without any coarsening  and in fact without any abstraction at all  thus  the disadvantages
 lower reward collected  are much more apparent than the advantages  lower computational
cost  
on larger problems  where working without abstractions was not an option  the balance
was reversed  in fact  and somewhat counterintuitively  in some of the larger problems
with coarsening active  the successful runs had smaller worldviews than the unsuccessful
runs  clearly  it is not the size of the worldview that determines success  but its quality  a
good worldview enabled efficient calculation of policies that progress toward the goal while
remaining small  a poor worldview simply grew larger  in the smaller problems  a growing
worldview may have eventually covered most of the state space in detail  thus masking the
effect  the planner would find a good policy effectively without any real approximation 
in larger problems  where finding a good policy without approximation was not feasible 
a similarly growing worldview simply slowed the planner down until no further progress
was made  in this situation  the worldview reducing action of proximity based coarsening
became crucial  ensuring that at the least the worldview remained tractably small and
thereby enabled the planner to deal with the problem 
   

fibaum  nicholson   dix

as can be seen from the results  coarsening was successful in this task some of the
time  when the agent paused to replan part way to the goal  it reduces the size of the
worldview to keep it more relevant to the changing circumstances  in other runs  however 
the worldview size grew beyond the capabilities of the planner  in some cases  such as in
the   x   problem  it settled at a higher balance  in others  it appears to have simply
continued growing  in the latter case  it would not appear to be a simple question of
tuning the parameters  when it did find balance  it was at an appropriate worldview size 
there appears to be some other factor  some other quality of the worldview determining
the success or failure of those runs  whether they find balance and reach the goal or grow
too big and fail 
a number of problems were solved poorly or not at all due to our initial abstraction
selection algorithm  algorithm     on these problems  the algorithm produced either a large
worldview that exceeded available memory  either immediately or very shortly afterwards  
so that planning was not possible  or a small worldview in which planning was ineffective 
it could not be set to produce a medium sized worldview  none of the four combinations of
options produced one  in some problems  only the singleton worldview was possible  that
is  both steps of initial abstraction selection disabled  resulting in a worldview aggregating
all states into a single  maximally abstract worldview state  leading typically to no reward
being collected by the agent  at best  it would take a few actions in the general direction
of the goal s   this is considerably worse than previous work  for instance  kim       
obtains approximate solutions for the problems and for their larger variants  as do others
who use this domain or a variant  including hoey et al         and their originators  dearden
and boutilier        
it was the necessity of using a singleton initial worldview which understandably greatly
hurt the performance  an infelicity at the worldview initialisation stage could impair the
entire planning process  since the worldview was never completely discarded by the planner 
the worldview improvement algorithms could have made up for some amount of weakness
in the initial worldview  but a singleton worldview was a poor starting point indeed  this is
similar to an observation made by dean et al         in their work using a reduced envelope
of states  that is  a subset of the state space   their high level algorithms  like those here 
work regardless of the initial envelope  worldview   in practice  however  it is better for
the initial envelope to be chosen with some intelligence  for instance to contain at least a
possible path to the goal  for goal oriented domains   they find this path using a simple
depth first search  and while this itself is not directly applicable to worldviews with their
gradations of abstraction  the overall concept remains  a reasonable initial worldview is
crucial 
the tireworld results confirm this  here the initial worldview with the reward step
enabled was small  since the domain only rewards a single dimension  and planning was
ineffective  once the planner was given a better initial worldview  one which it could
have plausibly calculated  it became quite effective  even in the modified tire large n 
where the initial state was deliberately moved further from the goal  it seems  then  that
while our basic approach is general  our worldview selection and modification methods are
less so  this is good  worldview selection is the less fundamental apect of our approach
and can be easily supplemented by additional methods or even just tuning  in domains like
   

fiproximity based non uniform abstractions for planning

tireworld  it seems that a modified predicate solver that can generate plausible trajectories
from the current state to a goal would do well as part of worldview selection 
it is interesting to compare this with the results of sanner and boutilier        for
tireworld  which they only describe in passing as extremely poorly approximated before
going on to manually tweak the domain for their planner  adding the information that the
locations are mutually exclusive  this makes planning much easier and largely invalidates
any comparison between approaches    it is a fair question  however  to what extent this is a
weakness of their planner and to what extent it is an artefact of the domain  its combination
of representation and narrative seems rather unfortunate  as the narrative with its obvious
  of n intuition will tend to obscure the real features  and the real applicability  of the
propositional representation and hinder rather than help intuition  this would not occur
with a tighter fit between representation and narrative 
others  of course  solve the tireworld domain well  some  such as barry  kaelbling  and
lozano prez         generate the full policy  while others take advantage of the initial state 
as our planner would do  it is difficult to know to what extent their planners are adapted
to the domain and to what extent they are flexible  it seems that only in recent years has it
become common for planners to be tested on domains to which the researchers do not have
access during development  as with some of the icaps ipc domains  rather than being
to a greater or lesser degree hand tuned  usually unconsciously  to the particulars of one
or another domain  as ours was undoubtedly unconsciously tuned to the grid navigation
domain 
an interesting side point is provided by the shuttlebot problem  which was not solved
at all for              other than collecting the trivial reward immediately adjacent to
s    while being solved optimally for          since the simulator itself does not have any
intrinsic discounting factor  it reports each reward as it is collected  one can see that
even though the planner was working with a discounting factor          it provided a
better solution to the             case than when it worked with             in the first
place 
in some ways this better behaviour with the smaller planner discounting factor is reasonable  because while the agents horizon is represented by the world discounting factor  the
horizon of any particular policy  and therefore the planner  is effectively much shorter 
as the policy will be supplanted by a new one relatively soon  thus  it may be useful on
occasion to set the planners discounting factor  lower than the true world discounting factor in order to facilitate planning  however  this may lead to suboptimal  short sighted
policies 

    conclusions
the theory of markov decision processes provides algorithms for optimal planning  however 
in larger domains these algorithms are intractable and approximate solutions are necessary 
where the state space is expressed in terms of dimensions  its size and the resulting computational cost is exponential in the number of dimensions  fortunately  this also results in
a structured state space where effective approximations are possible 
    similarly  kolobov  mausam  and weld        report results on a variant of tireworld rather than on
tireworld itself  without providing an explanation 

   

fibaum  nicholson   dix

our approach is based on selectively ignoring some of the dimensions in some parts
of the state space in order to obtain an approximate solutions at a lower computational
cost  this non uniform abstraction is dynamically adjusted as planning and  in on line
situations  execution progress  and different dimensions may be ignored in different parts of
the state space  this has strong implications  since the resulting approximation is no longer
markovian  however  the approach is both intuitive and practical  it is the synthesis of
two existing approaches  the structure based approximation of uniform abstraction and the
dynamic locality based approximation of envelope methods  like the envelope methods  it
can be limited by its reliance on the initial worldview  or envelope   if that is poor  it will
tend to perform poorly overall  our approach subsumes uniform abstraction completely 
it can be treated as a special case of our more general method 
this paper extends the preliminary work of baum and nicholson        by modifying
the worldview based on the proximity measure  both enlarging and reducing its size  and by
evaluating the behaviour against a simulation  this allows us to test the approach on larger
problems  but more importantly demonstrates both the full strength of the approach and its
limits in terms of the domain features that it can exploit and those that it can exploit only
with adjustment or not at all  the abstraction becomes truly dynamic  reacting to changes
in the agents current state and enabling planning to be tailored to the agents situation
as it changes  as shown by the qualitative and quantitative results presented both here
and by baum         the approach can be effective and efficient in calculating approximate
policies to guide the agent in the simulated worlds 
     future work
one possible direction for future research would be to find worldview initialisation and
modification methods that result in smaller yet still useful worldviews  probably domainspecific  to extend the method to further domains  either larger or with different features 
for example  the factory and tireworld domains are goal oriented and based on predicates 
and a worldview selection or modification method based on a predicate oriented solver could
find possible paths to the goal and ensure that relevant preconditions are concrete along
that path 
interestingly  in the   x   problem  while the proximity based methods did not keep
the worldview size small  they did seem to find a balance at a larger but stil moderate
size  thus another possibility might be to tune the proximity based methods or develop
self tuning variants 
at a number of points  for instance phase selection  our algorithm uses stochastic choice
as a default  this could be replaced by heuristics  learning  or other more directed methods 
one could adapt our method to work with other types of mdps  such as undiscounted
or finite horizon ones  or combine it with other approaches that approximate different aspects of the domains and the planning problem  as described in section      for example 
as mentioned in that section  gardiol and kaelbling              combine hierarchical state
space abstraction somewhat similar to ours with the envelope work of dean et al         
many other combinations would likely be fruitful for planning in domains which have features relevant to multiple methods  similarly  additional refinement or coarsening methods
   

fiproximity based non uniform abstractions for planning

could be added  for instance as one based on the after the fact refinement criterion with
roll back of reyes et al         
on a more theoretical side  one could look for situations in which optimality can be
guaranteed  as hansen and zilberstein        do with their lao  algorithm for the work of
dean et al          observing that if an admissible heuristic is used to evaluate fringe states 
rather than a pragmatically chosen v  out   the algorithm can be related to heuristic search
and acquires a stopping criterion with guaranteed optimality  or  optimality   perhaps a
similar condition could be developed for our approach  with a rather different heuristic 
there are two basic directions in which this work can be further extended in a more
fundamental way  relaxing one of the mdp assumptions  perfect observability or knowledge
of the transition probabilities  a partially observable markov decision process  pomdp 
gives the agent an observation instead of the current state  with the observation partly
random and partly determined by the preceding action and the current state  while the
optimal solution is known in principle  it is quite computationally expensive  since it transforms the pomdp into a larger  continuous  many dimensional mdp on the agents beliefs 
as such  the non uniform abstraction approach could be applied in two different ways  either to the original pomdp  a fairly direct translation  or to the transformed mdp  the
other extension would be to apply the technique when the agent has to learn the transition
probabilities  in particular  the application of our technique to exploration   would be very
interesting  the agent would have to somehow learn about distinctions within single abstract states  so as to distinguish which of them should be refined and which should remain
abstract 

references
de alfaro  l     roy  p          magnifying lens abstraction for markov decision processes 
in proceedings of the   th international conference on computer aided verification 
cav    pp         
barry  j   kaelbling  l  p     lozano prez  t          hierarchical solution of large markov
decision processes  in proceedings of the icaps workshop on planning and scheduling
in uncertain domains 
barry  j  l          fast approximate hierarchical solution of mdps  masters thesis 
massachusetts institute of technology 
barto  a  g   bradtke  s  j     singh  s  p          learning to act using real time dynamic programming  artificial intelligence  special volume  computational research
on interaction and agency                 
baum  j          dynamic non uniform abstractions for approximate planning in large
structured stochastic domains  ph d  thesis  clayton school of information technology  monash university  available at www baum com au jiri baum phd ps gz
    while the learning problem could also be transformed into an mdp on the agents beliefs or experiences 
it would be computationally prohibitive  the standard approaches instead explicitly distinguish exploration  where the agent learns about its domain  but ignores goals  and exploitation  where it achieves
goals  but ignores opportunities to learn  

   

fibaum  nicholson   dix

baum  j     nicholson  a  e          dynamic non uniform abstractions for approximate
planning in large structured stochastic domains  in lee  h  y     motoda  h   eds   
topics in artificial intelligence  proceedings of the  th pacific rim international conference on artificial intelligence  pricai      pp         
bellman  r  e          dynamic programming  princeton university press 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
botea  a   enzenberger  m   muller  m     schaeffer  j          macro ff  improving ai
planning with automatically learned macro operators  journal of articial intelligence
research             
boutilier  c          correlated action effects in decision theoretic regression  in geiger  d  
  shenoy  p   eds    proceedings of the   th conference on uncertainty in artificial
intelligence  uai      pp       
boutilier  c     dearden  r          approximating value trees in structured dynamic programming  in proceedings of the   th international conference on machine learning 
pp       
boutilier  c   dearden  r     goldszmidt  m          exploiting structure in policy construction  in mellish  c  s   ed    proceedings of the   th international joint conference
on artificial intelligence  ijcai      vol     pp           
boutilier  c   dearden  r     goldszmidt  m          stochastic dynamic programming
with factored representations  artificial intelligence                   
boutilier  c   goldszmidt  m     sabata  b          continuous value function approximation for sequential bidding policies  in laskey  k     prade  h   eds    proceedings of
the   th conference on uncertainty in artificial intelligence  uai      pp       
cassandra  a  r   kaelbling  l  p     kurien  j  a          acting under uncertainty  discrete bayesian models for mobile robot navigation  tech  rep  tr cs        computer
science  brown university 
daoui  c   abbad  m     tkiouat  m          exact decomposition approaches for markov
decision processes  a survey  advances in operations research            
dean  t   kaelbling  l  p   kirman  j     nicholson  a  e          planning under time
constraints in stochastic domains  artificial intelligence                 
dearden  r     boutilier  c          abstraction and approximate decision theoretic planning  artificial intelligence                 
dietterich  t  g          hierarchical reinforcement learning with the maxq value function
decomposition  journal of artificial intelligence research             
drummond  m     bresina  j          anytime synthetic projection  maximizing the probability of goal satisfaction  in dietterich  t     swartout  w   eds    proceedings of
the  th national conference on artificial intelligence  aaai      pp         
gardiol  n  h     kaelbling  l  p          envelope based planning in relational mdps  in
advances in neural information processing systems     nips    
   

fiproximity based non uniform abstractions for planning

gardiol  n  h     kaelbling  l  p          adaptive envelope mdps for relational equivalence based planning  tech  rep  mit csail tr           computer science and
artificial intelligence laboratory  massachusetts institute of technology 
goldman  r  p   musliner  d  j   boddy  m  s   durfee  e  h     wu  j          unrolling
complex task models into mdps  in proceedings of the      aaai spring symposium
on game theoretic and decision theoretic agents 
goldman  r  p   musliner  d  j   krebsbach  k  d     boddy  m  s          dynamic
abstraction planning  in kuipers  b     webber  b   eds    proceedings of the   th
national conference on artificial intelligence and  th innovative applications of artificial intelligence conference  aaai iaai      pp         
guestrin  c   koller  d   parr  r     venkataraman  s          efficient solution algorithms
for factored mdps  journal of artificial intelligence research             
hansen  e  a     zilberstein  s          lao   a heuristic search algorithm that finds
solutions with loops  artificial intelligence                 
hauskrecht  m   meuleau  n   kaelbling  l  p   dean  t     boutilier  c          hierarchical
solution of markov decision processes using macro actions  in cooper  g     moral 
s   eds    proceedings of the   th annual conference on uncertainty in artificial
intelligence  uai      pp         
hoey  j   st  aubin  r   hu  a     boutilier  c          spudd  stochastic planning using
decision diagrams  in proceedings of the   th annual conference on uncertainty in
artificial intelligence  uai      pp         
howard  r  a          dynamic programming and markov processes  mit press 
kim  k  e          representations and algorithms for large stochastic planning problems 
ph d  thesis  deptartment of computer science  brown university 
kirman  j          predicting real time planner performance by domain characterization 
ph d  thesis  department of computer science  brown university 
kolobov  a   mausam    weld  d  s          regressing deterministic plans for mdp
function approximation  in workshop on a reality check for planning and scheduling
under uncertainty at icaps 
korf  r          macro operators  a weak method for learning  artificial intelligence         
     
littman  m   weissman  d     bonet  b          tireworld domain  the fifth international
planning competition  ipc    hosted at the international conference on automated
planning and scheduling  icaps       
munos  r     moore  a          variable resolution discretization for high accuracy solutions of optimal control problems  in dean  t   ed    proceedings of the   th international joint conference on artificial intelligence  ijcai      pp           
musliner  d  j   durfee  e  h     shin  k  g          world modeling for the dynamic
construction of real time plans  artificial intelligence            
   

fibaum  nicholson   dix

nicholson  a  e     kaelbling  l  p          toward approximate planning in very large
stochastic domains  in proceedings of the aaai spring symposium on decision theoretic planning  pp         
parr  r          a unifying framework for temporal abstraction in stochastic processes 
in proceedings of the symposium on abstraction reformulation and approximation
 sara      pp        
puterman  m  l     shin  m  c          modified policy iteration algorithms for discounted
markov decision processes  management science               
reyes  a   sucar  l  e     morales  e  f          asisto  a qualitative mdp based recommender system for power plant operation  computacion y sistemas              
sanner  s     boutilier  c          practical solution techniques for first order mdps 
artificial intelligence                    advances in automated plan generation 
srivastava  s   immerman  n     zilberstein  s          abstract planning with unknown
object quantities and properties  in proceedings of the eighth symposium on abstraction  reformulation and approximation  sara      pp         
st aubin  r   hoey  j     boutilier  c          apricodd  approximate policy construction using decision diagrams  in proceedings of conference on neural information
processing systems  pp           
steinkraus  k  a          solving large stochastic planning problems using multiple dynamic abstractions  ph d  thesis  department of electrical engineering and computer
science  massachusetts institute of technology 

   

fi