journal artificial intelligence research                 

submitted        published      

proximity based non uniform abstractions
approximate planning
jir baum
ann e  nicholson
trevor i  dix

jiri baum com au
ann nicholson monash edu
trevor dix monash edu

faculty information technology
monash university  clayton  victoria  australia

abstract
deterministic world  planning agent certain consequences
planned sequence actions  so  however  dynamic  stochastic domains
markov decision processes commonly used  unfortunately suffer curse
dimensionality  state space cartesian product many small sets  dimensions  
planning exponential number dimensions 
new technique exploits intuitive strategy selectively ignoring various dimensions different parts state space  resulting non uniformity strong
implications  since approximation longer markovian  requiring use modified planner  use spatial temporal proximity measure  responds
continued planning well movement agent state space  dynamically adapt abstraction planning progresses 
present qualitative quantitative results across range experimental domains
showing agent exploiting novel approximation method successfully finds solutions planning problem using much less full state space  assess
analyse features domains method exploit 

   introduction
deterministic world planning agent certain consequences
actions  plan sequence actions  knowing execution necessarily
achieve goals  assumption appropriate flexible  multi purpose robots
intelligent software agents need able plan dynamic  stochastic
domains operate  outcome taking action uncertain 
small medium sized stochastic domains  theory markov decision processes
provides algorithms generating optimal plan  bellman        howard        puterman   shin         plan takes account uncertainty outcome taking
action  specified distribution possible outcomes  flexibility 
reward function rather simple goal  relative desirability
otherwise situation specified 
however  domain becomes larger  algorithms become intractable approximate solutions become necessary  for instance drummond   bresina        dean 
kaelbling  kirman    nicholson        kim        steinkraus         particular
state space expressed terms dimensions  cartesian product sets 
size resulting computational cost exponential number dimensions 
c
    
ai access foundation  rights reserved 

fibaum  nicholson   dix

hand  fortunately  results fairly structured state space effective
approximations often possible 
solution based selectively ignoring dimensions  parts
state space  time  words  obtain approximate solutions
dynamically varying level abstraction different parts state space 
two aspects approach  firstly  varying level abstraction introduces
artefacts  planning algorithm must somewhat modified eliminate these 
secondly  interestingly  appropriate abstraction must selected later modified
planning action progress 
work extension synthesis two existing approaches approximate planning  locality based approximation envelope methods  dean et al        
structure based approximation uniform abstraction  nicholson   kaelbling        dearden   boutilier         work extends exploiting structure
locality  broadening scope problems contemplated  baum nicholson
       introduced main concepts full details algorithms experimental
results presented baums        thesis  studies arbitrary
abstraction  instance bertsekas tsitsiklis         however  generally
theoretical case tended treat approximation markovian 
would resulted unacceptable performance practice  improve extending planning algorithm deal non markovian aspects approximation 
finally  use measure locality  introduced baum nicholson        
similar flexible influence measure munos moore        
assume agent continues improve plan acting
planning failures generally fatal  deal control error exclusively  sensor
error considered assumed agent accurately discern current
world state  fully observable   accurately knows state space  goal
reward function  distribution effect actions  no learning  
remainder paper organised follows  section   reviews background 
introduces abstraction provides framework  section   discusses planning
static non uniform abstraction  section   presents method initially selecting
non uniform abstraction based problem description  section   presents method
changing abstraction based policy planned  sections     introduce
proximity measure method varying abstraction based measure 
respectively  section   presents results based direct evaluation calculated
policy simulation  finally  section   discusses results section    gives
conclusions outlines possible directions future work 

   planning non uniform abstractions
non deterministic world planning agent cannot certain consequences
actions except probabilities  cannot plan simple sequence actions achieve
goals  able plan dynamic  stochastic domains  must use
sophisticated approach  markov decision processes appropriate commonly used
representation sort planning problem 
   

fiproximity based non uniform abstractions planning

    illustrative problems
aid exposition  present two example problems here  full set experimental
domains presented section     
two illustrative problems grid navigation domain  shown figure   
integer x coordinates      three doors either
open closed damage indication either yes no  agent
move one four cardinal directions  open door next it  nothing 
doors fairly difficult open  probability success     per time step 
moving     chance success  effect case failure  running
wall closed door causes damage  cannot repaired  transitions
shown table    agent starts location marked s  figure   doors
closed damage  goal reach location marked damage 
x  
y  

 

 

 

 

 

 

 



s 

 

 

k 

 

x  
y  

 

 

 

 

 

 

s 

 

 

k 

 



 

d 

d 

k 

 

 

 

 

 

 



 





d 
 

 



 

 

 

 



d 



k 





 
 

k 

d 

 

 a 

k 

d 
 b 

figure    layout grid navigation domain  blue arrows show optimal
path  a  suboptimal path  b   keys problem   doors
problem grid layout  walls doors  keys 

 keys problem contains keys  required open doors  agent
may one time  additional action allows agent
pick key location shown figure  open action requires
corresponding key effective  there separate unlock action    doors problem
contains keys doors unlocked closed therefore corresponding
keys pickup action 
optimal policy obtained exact planning  doors problem simply takes
shortest path door     keys problem  optimal plan collect
keys      pass south door   east door    shown figure   a  
suboptimal plan shown figure   b  
   

fibaum  nicholson   dix

x
stay




pre state
d 
d 
d 

post state
d 
d 

dmg



x















 
 


y  



















south
 
 




 
 
 
 


open






open






















   
   

north
 
 




 
 
 
 


open






open






















   
   

east
 
 
 
 
 
 
x

 
 
 
 























open




















   
   
   
   

west
 
 
 
 
 
 
x

 
 
 
 























open




















open
 
 
 
 
 
 


 
 
 
 
 
 










































   

   

   
   
   
   
   

   
   
   
   
   
   
   

d 

dmg



























yes
yes


 
 


y 





















yes
yes


 
 
 
 


x  





































yes
yes


 
 
 
 


x 





































yes
yes


















open
open








open
open








open
open








yes

table    transitions  doors problem  showing important changed dimensions
only  first matching transition used  percentage shown  given
post state occur probability  otherwise state unchanged  transitions without percentages deterministic 

   

fiproximity based non uniform abstractions planning

    exact planning
one approach exact planning stochastic
domains
involves using markov decision



 
processes  mdps   mdp tuple s  a  t  r    state space 
set available actions  transition function  r reward function s 
initial state  agent begins state s    time step  agent selects
action a  which  together current state  applies obtain distribution
s  current state next time step random according distribution
write prt  s  a    probability action taken state result state
next time step  agent given reward time step  calculated
r current state  and possibly action selected   aim agent
maximise cumulative function rewards  typically expected discounted 
sum discounting factor   fully observable mdp  agent full knowledge 
particular  agent aware   r current state selecting action 
well known fully observable mdp  optimal solution expressed
policy   mapping current state optimum action  planning
problem  then  calculation   side effect calculation  standard
algorithms calculate value function v   r  expected discounted sum
rewards starting state  table   summarises notation used paper 
well known iterative algorithms  bellmans        value iteration  howards
       policy iteration  modified policy iteration puterman shin       
computing optimal policy   however  becomes larger  calculation
becomes computationally expensive  particularly state space structured
cartesian product dimensions    s  s  sd    s  exponential
d  since algorithms explicitly store v usually   functions s 
space complexity therefore exponential d  since iterate arrays 
time complexity least exponential d  even consideration
fast iterative algorithms converge  typically  grows  planning
quickly becomes intractable  since practice amount computation allowed
agent limited  necessitates approximations process 
 doors problem  six dimensions  two x coordinates  three
doors one damage                              open  closed 
 open  closed   open  closed   damage  damage   s            action space
set five actions     north  south  east  west  open   transition function
specifies outcomes taking action state  reward function r  
agent h    i location  marked diagram  damage   
location damage    damage  finally  s  state
agent h    i location  doors closed  damage 
exact planning listed results  s  v  s    comparison 
approximation  planner must consider whole state space s   s  therefore
measure cost planning directly terms space indirectly terms
time  hand  since planning exact  optimal value function v
   illustrative problems simple goals achievement  use time discounting order
remain general mathematical convenience 

   

fibaum  nicholson   dix

symbol
original

sd

meaning

abstract
w p s 


state space  specific state space   worldview  resp  
dimension state space
n
number dimensions
ss
ww
state
 


initial state
scur

current state  in on line planning 
sd sd
wd sd
dimension state w  resp 

set actions  action space 
a 
default action


transition function  formal 
prt  sas       prt  waw       transition function  in use 
r s r
r wr
reward function  one step reward 
v  sr
v  w r
value function  expected discounted sum rewards 
      
discount factor reward
 sa
 wa
policy
 

optimal policy
v  s r

optimal value function  exact value function  
   
    w
approximate policy  ith approximate policy

exact value function  note  may abstract 
v   r
v   r
v   w r
approximate value function  approx  v  
table    summary notation  first column notation original mdp 
second notation non uniform abstraction applied 

obtained  along optimal policy ensuring agent expect obtain
value  figures approximations must measure 
    uniform abstraction
one method approximation take advantage dimensions ignoring
irrelevant marginally relevant order obtain
approximate solution  uniform sense dimensions ignored
throughout state space  since approach attacks curse dimensionality
originates  dimensions  effective counteracting it 
dearden boutilier use obtain exact solution  boutilier        approximate one  boutilier   dearden        dearden   boutilier         however  abstractions fixed throughout execution  dimensions deleted problem
pre determined sequence  makes approach somewhat inflexible  similarly 
nicholson kaelbling        propose technique approximate planning 
delete dimensions problem based sensitivity analysis  refine abstraction execution time permits  still uniform  dietterich        uses kind
abstraction combination hierarchical planning good effect  subtask 
navigate location t  ignore irrelevant dimensions  location items
   

fiproximity based non uniform abstractions planning

picked even ultimate destination agent  generally  time problem
description derived general source rather specified particular
problem  uniform abstraction help  gardiol kaelbling        use dimensions relevant  marginally  ignoring results approximate
solution improved planning progresses 
unfortunately  however  least human specified problems  one would generally
expect mentioned dimensions way relevant  irrelevant dimensions
eliminated human designer natural course specifying problem 
depending domain situation marginally relevant dimensions might
included  often  nearly enough effective approximation 
list comparisons uniform abstraction results reason
sample domains  makes little sense  almost dimensions
important solving problem  case methods exist
effective uniform abstraction  integrated approach easily 
    non uniform abstraction
approximation  non uniform abstraction  replaces state space w  particular type partition s  originally introduced baum nicholson         call
w worldview  members w worldview states members specific
states   non uniform abstraction based intuitive idea ignoring dimensions
parts state space  example  door interest agent
walk it  ignored parts state space
distant door  particular member worldview wi w  dimension
either taken account
 concrete  refined in   ignored altogether  abstract 
q completely
w singleton subset corresponding
w
coarsened out   wi  

d  

concrete dimensions equal sd abstract dimensions   worldview
selection modification methods ensure w remains partition times 
give example   doors problem one possible worldview location
damage dimensions concrete every state  door dimensions concrete
states within two steps respective door 
note domain still fully observable  question lack knowledge
dimensions question  wilful conditional ignorance planning
matter computational expediency  approximation subsumes exact
planning uniform abstraction  exact planning  dimensions set uniformly
concrete   w     s  worldview state corresponds one specific state 
uniform abstraction  combination abstract concrete dimensions fixed
entire worldview  treated special cases general approach  
   previously  used word envelope concept  baum   nicholson         however 
worldview better describes approximation used envelope 
   allow dimension partially considered  abstract level dimensions 
within them  dimension x coordinate either particular value  fully
abstract  never     instance 
   modified calculation reduces standard algorithm uniform fully concrete worldviews 
planner obtains standard results cases 

   

fibaum  nicholson   dix

hand  approximation longer markovian  dimension
abstracted away indeterminate  notation markov decision processes 
represented distribution concrete states  dimension
stochastic specific  but ignored  value  distinction important
truly stochastic outcome  quite valid plan retry action
succeeds  for instance  opening door  doors problem   dimension
merely ignored  agent obtain outcome  door closed  time moves
region dimension ignored  within worldview  previous
states appear matter  discuss section   
    comparison approaches
non uniform abstractions began appear literature first usually side effect
structured method  state space represented decision tree based
individual dimensions  boutilier  dearden  goldszmidt               note 
however  decision tree structure imposes restriction kinds non uniform
abstraction represented  dimension root tree considered
throughout state space  on  significant restriction results
representation much limited representation  similar restriction affects
de alfaro roys        magnifying lens abstraction  refinement multivalued dimensions taken bit by bit bits interleaved  level
decision tree halves space along different dimension pre determined order 
note  would work well dimensions correspond more or less connected
space  gridworld  would less well features doors
grid navigation domain  magnifying lens abstraction calculates upper lower bounds
value function  rather single approximation  advantage guiding
abstraction selection allows definite termination condition  which lack  
hand  always considers fully concrete states part algorithm  limiting
space savings square root state space  whereas algorithm work
mixture variously abstract states necessarily including fully concrete
ones  another related approach variable grids used discretisation 
indirectly used discrete domains  boutilier  goldszmidt  sabata        do 
dimensions reasonably approximated continuous  for instance money   unlike
approach  variable grids completely inapplicable predicates binary
enumerated dimensions  some  reyes  sucar  morales         use techniques
ways quite similar continuous mdps  though quite different
ways  consider refinement only  coarsening  use sampling  rather
directly dealing domain model  use different refinement method 
refinement evaluated fact either committed rolled back 
perhaps similar approach one modules steinkraus        
ignore state variables module  however  module appears completely manual 
requiring input variables  dimensions  ignored parts state
space  uses values dimensions current state scur   rather
distribution  obviously restricts situations may used  for instance 
 doors problem  doors could ignored starting state   finally  since
   

fiproximity based non uniform abstractions planning

steinkraus        analyse report relative contributions modules
solution  meta planning problem selecting arranging modules 
difficult know extent particular module useful 
approaches take advantage different features different domains  instance 
factored mdp approach  used  instance  boutilier et al         guestrin 
koller  parr    venkataraman        suitable domains parts state
action spaces grouped together within group actions action
dimensions affect corresponding states state dimensions interaction
groups weak  st aubin  hoey  boutilier        iterate symbolic representation
form algebraic decision diagrams produce approximate solutions  sanner
boutilier        iterate symbolic representation whole class problems
domain  using symbolic dynamic programming  first order algebraic decision diagrams
linear value approximation  pre compute generic solution used
quickly solve specific problems class  focus state space  others approximate action space  typically grouping actions  possibly hierarchically 
macro actions  korf         instance hauskrecht  meuleau  kaelbling  dean 
boutilier        botea  enzenberger  muller  schaeffer        take approach 
parr        uses finite state automata macro actions srivastava  immerman  zilberstein        take using algorithm like plans branches
loops  goldman  musliner  boddy  durfee  wu        reduce state space generating  limited horizon  undiscounted  mdp different  non mdp representation
including reachable states  pruning detected clearly
immediately poor  inferior equivalent already generated states  naturally  many
approaches combined  instance  gardiol kaelbling              combine state space abstraction envelope work dean et al          steinkraus
       uses modular planner view combining many approaches may
appropriate given problem  details approaches variants
refer reader recent survey field daoui  abbad  tkiouat        
    dynamic approximate planning
top level algorithm shown algorithm    initialisation  consisting
selecting initial abstraction setting policy  value proximity a     
proportionally size worldview state  respectively   planner enters
infinite loop stochastically alternates among five possible calculations 
described following sections  elsewhere algorithm  use
stochastic choice default absence directed method 
agent assumed processing power available acting 
continually improve policy  modify approximation updates focus
planning based current state  means agent need plan
well unlikely possibilities  therefore expend planning effort
likely paths closer future  expecting reaches parts
state space  improve approximation appropriate 
   initialising approximate policy action a  constitutes domain specific heuristic namely 
known default action a  reasonably safe states  nothing action 

   

fibaum  nicholson   dix

algorithm   high level algorithm approximate planning dynamic non uniform
abstractions
select initial abstraction    algorithm     
worldview states w
 w  a    v  w     p w   w 
 s 
policy value calculation    algorithm     
loop
choose stochastically
policy value calculation    algorithm     

policy based refinement    algorithm     

proximity calculation    algorithm     

proximity based refinement    algorithm     

proximity based coarsening    algorithm     
input latest current state  output policy

actual execution policy assumed separate thread  executive  
planner concern timeliness requirements
domain  whenever action needs taken  executive simply uses policy
recently received planner 
dean et al         call recurrent deliberation  use locality based
approximation  similar architecture used circa system  musliner  durfee   
shin        goldman  musliner  krebsbach    boddy        guarantee hard deadlines 
circa terminology  planner ais  ai subsystem   executive
rts  real time subsystem  
alternative recurrent deliberation pre cursor deliberation  agent first
plans  finished planning begin act  making
adjustments plan policy  effectively  planner  current state constant
equal initial state throughout planning  work pre cursor mode used
measurements  involves fewer potentially confounding variables 
conceptually  approach divided two broad parts  open ended problem selecting good abstraction relatively closed problem planning within
abstraction  since latter part closed  deal first  next
section  covering algorithm    explore open ended part sections    
covering algorithms    

   solving non uniformly abstracted mdps
given non uniform abstraction  simplest way use planning take one
standard mdp algorithms  modified policy iteration puterman
shin         adapt non uniform abstraction minimally  formulae translate
   

fiproximity based non uniform abstractions planning

directly obvious fashion  becomes function worldview states instead concrete
states  on  shown algorithm    using simple variant update policy
w procedure   probabilities transition one worldview state another
approximated using uniform distribution concrete states  or possibly
distribution  information available  
algorithm   policy value calculation
repeat n times
worldview states w
update value w
worldview states w
update policy w
update value w
procedure update value w
prt  w   w   w     
   optimisation v  w  calculated directly case   
v  w  r w 
 
else
p
v  w  r w    w prt  w   w   w  v  w  

procedure update policy
p w variant simple
 w  min arg maxa w prt  w  a  w  v  w  

procedure update policy w variant locally uniform abstraction
   see section     discussion locally uniform abstraction   
absdims  d  
w   prt  w  a  w       w abstract d 
w abstract
absdims
lua w   w  
dimension w   dimension w
  absdims
p
 w w  


v w   w w  w   v  w  
p
 w  min arg maxa w prt  w  a  w  v  lua w   

note algorithm    considered ordered set a  smallest element
minimum used arg max gives one possibility 
two aspects   a  domain specific heuristic  instance  breaking ties favour
default action possible   b  avoid policy basedp
refinement  see section   
based actions equal value  secondly  efficiency  w calculated
states w prt  w  a  w        since states make contribution
sum  finally  number n tuning parameter particularly critical  we use
n       
course  replacing state space worldview w way not  general 
preserve markov property  since actual dynamics may depend aspects state
space abstracted worldview  simple variant ignore assume
markov property anyway  grounds is  all  approximation 
unfortunately  resulting performance unacceptably large error  including
outright non attainment goals 
   

fibaum  nicholson   dix

instance   doors problem  situation occur three
doors whenever abstract s  concrete near door question 
doors relatively difficult open      probability success per try 
hand  moving area abstract area
concrete  assumed probability door already open     
calculations performed  turns preferable plan loop  repeatedly trying
illusory     chance success rather attempting open door
    chance success  agent never reach goal  worse still  ways 
estimate quality solution quite good  v  s         
fact better even optimal solutions v  s          true quality
solution poor  v  s               corresponding never reaching goal  but
incurring damage  either  figures discounting factor             
regions take account particularly bad piece information may seem unattractive  described above  vice versa  call problem ostrich effect 
agent refusing accept unpleasant fact  mythical ostrich buries
head sand  solution  locally uniform abstraction  described next section 
abstracted approximation simply treated mdp agent
know state reach  near closed door near open door   correspond
underlying process  might reach particular state deterministically  as
here   problem especially obvious example  planner plans loop 
reminiscent problem noted cassandra  kaelbling  kurien        
plan derived pomdp failed actual robot got loop particular
situation sensor completely reliable contrary model 
    locally uniform abstraction
ostrich effect occurs states different abstraction considered  instance
one door abstract one door concrete closed 
solution make abstraction locally uniform  therefore locally markovian
duration policy generation iterative step  making abstraction locally 
temporarily uniform  iterative step policy generation algorithm never work
across edge abstract region  and  since information available
states considered point  impetus favoured
avoided basis  for instance  avoiding state door concrete closed
favour one door abstract   action chosen chosen based
information presence absence 
modification update policy w procedure algorithm   
states considered one one  region around state accessed
function returns locally uniform version  states concrete state
considered averaged ignore distinctions  different states
considered  sometimes states taken themselves  sometimes estimated
values v averaged adjacent states  means dimensions
partially considered states cases  mean
concrete region must extend one step beyond region dimension
   

fiproximity based non uniform abstractions planning

immediately relevant  dimension fully considered state  possible
outcomes actions state must concrete dimension 
modified procedure proceeds follows  first dimensions abstract
possible outcome state updated w collected variable absdims 
function lua constructed takes worldview states w returns potential
worldview states w w abstract dimensions absdims 
core modification  named lua locally uniform abstraction  since
potential states returned lua not  general  members w  therefore
necessarily value stored v   function v constructed calculates
weighted
averages value function v potential states  sum 
p

calculated states w w w    efficiency  finally 

w
update step carried using two functions lua v  
unfortunately  modification applied  algorithm may may converge depending worldview  failure converge occurs concrete region
small cases  algorithm cycle two policies  or conceivably
more  instead converging  one must careful  therefore  worldview  avoid
situations  else detect modify worldview accordingly  policybased worldview refinement algorithm described section   ensures convergence
practice 

   initial abstraction
beginning planning  planner must select initial abstraction  since
worldview never completely discarded planner  infelicity stage may
impair entire planning process  worldview improvement algorithms
make amount weakness here 
different ways select initial abstraction  propose one heuristic
method selecting initial worldview based problem description 
variants  consider example door  doors problem associated
two locations  is  immediately either side  makes sense  then  consider
status door two locations  association read problem
specification  intuitively  structure solution likely resemble structure
problem  incorporates structure transition function initial
worldview  reward function incorporated  reflecting assumption
dimensions reward based important 
use two step method derive initial worldview  shown algorithm   
firstly  reward function specified based particular dimensions  make
dimensions concrete throughout worldview  leave dimensions abstract 
 doors problem  x dmg dimensions  step
              states worldview 
secondly  transition function specified decision trees  one per action  use
find nexuses dimensions  is  linking points  points
dimensions interact  nexus corresponds one path root
tree leaf  example   doors problem  decision tree open action
contains leaf whose ancestors x  y  d  stochastic node  choices leading
   

fibaum  nicholson   dix

algorithm   select initial abstraction
   set worldview completely abstract   
w  s 
   reward step   
reward step enabled
dimensions mentioned reward tree
refine whole worldview dimension
   nexus step   
nexus step enabled
leaf nodes action trees
worldview states w matching pre state
refine w dimensions mentioned pre state

leaf labelled respectively       closed      corresponds nexus
sx      sy     sd    closed  the stochastic node ignored determining nexus  
total  four nexuses side door  two locations immediately
adjacent  shown figure   a   connecting relevant door dimension x
coordinates  initial worldview shown figure   b   x  dmg concrete
everywhere doors abstract except concrete one location directly
side door  corresponding location nexuses figure   a  
steps   w         compared  s          specific states 

x  

 

 

 

 

 

 

 

 

 

x  

y  

y  

 

 

 

 

 

 

 

 

 

 





 

d 

d 

 





 

d 

d 

 

 

 

 

 

 

 

 

 

 

 



 

 a 

 

 

d  d 
 b 

figure    nexus step initial abstraction  showing  a  location nexuses
 doors problem  there four nexuses    b  locations
door dimensions concrete initial worldview 

   

fiproximity based non uniform abstractions planning

 keys problem  location nexuses figure   a   except
nexuses location involve corresponding
key dimensions  thus  initial worldview  locations shown figure   b 
concrete corresponding door dimension  also  closed 
corresponding key dimension  states doors open  key dimension
remains abstract  initial worldview size  keys  w        
due locally uniform abstraction  concrete door dimensions taken
account minimal degree  worldview used without
refinement  expected resulting policies would poor 
results  bear expectation  worldview initialization methods therefore
intended used own  rather basis refinement  thus 
real test methods well work coupled worldview
modification methods  described below 

   policy based refinement
section presents first worldview modification methods  policy based refinement  method modifies worldview based directly current approximate
policy   particular  refines states based differences actions planned
adjacent  differently abstract states  differences indicate dimension may
important  adjacent states abstract dimension refined  i e 
dimension made concrete states  
method previously introduced baum nicholson         showed 
using small navigation domain example  the  doors problem paper  
refinement method resulted good policy  though optimal  present quantitative results consider complex domains 
    motivation
motivation method twofold  firstly  already indicated  method detects
areas particular dimension important  affects action planned 
ensures concrete adjacent states  thus regions dimension taken
account expand long dimension matters  stop  secondly 
method fulfils requirements choosing worldview avoid non convergence
policy calculation  mentioned section     above 
dimensions important affect policy  since policy planners
output  less important parts state space affect
policy  thus  dimensions need concrete remain abstract
gleaned part state space comparing optimal actions
various states  optimal actions equal  states abstract 
differ  states concrete  however  optimal policy  
approximate policy worldview  difficult  however  planner compare
policies areas dimension concrete  found important there 
expand area concrete  policy based refinement policy calculation
   omitted uninteresting  presented baum        

   

fibaum  nicholson   dix

alternate  refinement continue area dimension concrete covers
whole region important 
section     noted planning algorithm requires worldview chosen
care  algorithm described section detects situations potentially problematic locally uniform abstraction modifies worldview preclude them 
intuitively  incorrect behaviour occurs edge concrete region intersects
place two fairly similarly valued courses action  corresponding
two different paths goal 
    method
method uses transition function definition adjacent states  worldview states w w considered adjacent   prt  w  a  w        definition
symmetrical general  since transition function not  problem
method  seen below  algorithm shown algorithm   
algorithm   policy based refinement
candidates
worldview states w
actions
w   p r w  a  w      

dimensions
  w abstract w concrete
w

abstract


construct w  
dimension w   dimension w   

b

w   w    w       wb   wa w    wb w   
   policy throughout w   
candidates candidates   w  d  
 w  d  candidates
w w
   replace w
group states concrete   
anew
w
concrete

wnew  
dimension wnew   dimension w   
w w  wnew  
new
 wnew    w   v  wnew   v  w   p wnew    w w    p w 
w w    w     discarding stored  w   v  w  p w    

example  doors problem  instance  applying method planning
increases number worldview states initial             depending
stochastic choices  recall  s          comparison   produces concrete regions
nice tight around doors  shown figure    allowing algorithm
converge reasonable solution  solution fact optimal given initial
state s    though simply coincidence  since s  taken account
algorithm states somewhat suboptimal actions  the agent would reach
goal states  shortest route  
   

fiproximity based non uniform abstractions planning

x  

 

 

 

 

 

 

 

 

 

x  

y  

 

 

 

 

 

 

 

 

 

y  


 

d 

d 

 

k 

 

d  d  d 

d  d  d 

 

 

d  d  d  d 

d 

 

d  k  d  d 

 

d  d 

 

d  d  d  d  d 

 

 

d  d  d 

 

 

d 

 

 





k 






k 



 

d 

 

k 

 

d  d  d 

 

k  k  k 

 a 



k  k  k 







 b 

figure    example non uniform abstraction  a   doors  b   keys problems
policy based refinement  x  dmg dimensions concrete everyd
where  d   d  d  indicate corresponding door concrete  k  


k  k  indicate corresponding door concrete corresponding
key concrete door closed 

worldview obtained method often quite compact  instance  rather
refining simple     rectangular region side door  doors  human
might  algorithm makes   locations concrete approach side door 
enough obtain good solution  seen north sides doors
d  d   well west side door d     concrete locations  due edge  
departure side doors d  d   even better makes refinement all 
south door d  east door d   action move toward goal  regardless
status door actions equal  refinement takes place 
south side door d  seems rather less compact  concrete area fact
big   locations  doors seems excessive compared compact
concrete areas elsewhere  occur nexus close region
best action take genuinely depends status dimension nexus 
difference small  somehow agent found h    i policy based
refinement independent scur optimal path genuinely would depend whether
door d  open  path slightly suboptimal case  theory
region could arbitrarily large extent  seems relatively minor effect
practice  here  instance  adds couple states      w  
found real problem domains  or domains used baum 
      
   

fibaum  nicholson   dix

    limitations
policy based refinement deal cases single dimension makes difference  two dimensions needed combination  often miss them 
instance   keys problem key quite distant corresponding door
policy based refinement therefore never find relationship two 
key  appears reason pick up  door appears
means unlocking it 
obviously  fixed ad hoc rewarding picking keys sake 
indeed  domain formulations literature exactly that  rewarding agent
partial achievement goal  however  clean solution  effect 
domain specifications cheat providing hints 
another problem policy based refinement provide coarsening
worldview  modifying ways  instance execution progresses
planner needs update plan  indeed  policy based refinement ignores initial state
s  altogether  current state scur recurrent planning  thus produces
solution regardless part problem agent actually asked solve 
waste computation solving parts agent unlikely actually
visit  perhaps importantly carries penalty corresponding loss
quality relevant parts 
following sections describe proximity based worldview modification  needed
solve domains combinations dimensions important makes
use s  scur   appropriate 

   proximity measure
general  worldview mostly concrete near agent planned path
goal  allow detailed planning  mostly abstract elsewhere  conserve computational resources  section describe measure  originally baum   nicholson 
      realises concept  proximity p  decreases state
future less probable   section extends brief description baum
nicholson         following section present new worldview modification
methods based directly measure 
    motivation
proximity p realisation intuitive concept states near agent
likely visited  opposed distant agent unlikely  naturally
takes account current state scur recurrent planning  initial state s 
pre cursor planning  unlike policy based refinement ignores altogether  thus
planner selecting worldviews based proximity measure produce solutions tailored
particular scur s  ignore parts mdp irrelevant nearirrelevant performance state  thus saves computation would otherwise
   baum nicholson        used word likelihood measure  prefer proximity
avoid confusion meanings word likelihood  munos moore        use word
influence somewhat similar measure continuous domains 

   

fiproximity based non uniform abstractions planning

wasted solving parts agent unlikely actually visit  perhaps
importantly carries advantage corresponding gain quality
relevant parts  allows agent deal problems  keys beyond
reach policy based refinement 
implicitly  agent plans reaches mostly abstract parts
state space  improve approximation appropriate  planner thus continually
improves policy  modifies approximation updates focus planning based
current state scur   means refining regions agent finds
likely visit  coarsening away details regions longer likely
visit already traversed 
three aspects proximity  temporal  spatial probabilistic  firstly 
temporal aspect indicates states may encountered near future  exponentially decaying scale  second aspect spatial nearness states  in terms
state space  agent planned path  spatial aspect somewhat indirect 
spatial structure domain represented implicitly transition
matrix  proximity measure reflect it  two aspects combined
proximity give single real number     state  denoted p p
proximity  spatial aspect temporal aspect  number
interpreted probability namely probability encountering state p
interpreted probability distribution states  giving final  probabilistic
aspect proximity 
    calculation
formula proximity p similar formula value function 
three differences  firstly  instead beginning reward function based
current state function  cur  secondly  transition probabilities time reversed
 that is  matrix transposed   value calculation based
reward function  occurs future  after taking actions   current state
function based present  taking actions  since order taking actions
function upon formula based reversed time  similar reversal must
b used
applied transition probabilities  thirdly  estimated future policy
b
b
instead   estimate  stochastic policy defined making  s  distribution
actions assigns constant probability current  s  distributes
remaining probability mass among actions equally  distributed probability
mass corresponds probability policy change sometime future 
or  alternately  probability currently selected action yet correct 
formula therefore 
x

b    s p s  
pr s    s
   
p s  cur s    p







p proximity discounting factor    p     

  p scur  
cur s   
 
otherwise
   

   

fibaum  nicholson   dix

p
constant  p chosen current state function p s  converges
   words p probability distribution  checked near future 
agent probability p s  state s  assuming follow policy
near future defined probability checking time proportional
pt  that is  p interpreted stopping probability   value calculation 
one instead solve set linear equations
x

b    s p s  
pr s    s
   
p s    cur s    p




or  matrix notation 
 i p tbt  p   cur

   

b identity
tb transition matrix induced stochastic policy
matrix  implementation uses matrix form  shown algorithm    proximity
measure needs little adjustment work non uniformly abstract worldview 
simply replaced w          scur   becoming scur w 
algorithm   proximity calculation
solve matrix equation p linear system 
 i p tbt  p   cur
measure two tuning parameters  replanning probability discounting
factor p   replanning probability controls spatial aspect  trades focus
likely path planning less likely eventualities nearby  similarly  p controls
temporal aspect  smaller p is  short sighted greedy planning
be  conversely  p close    planner spend time planning future
might better spent planning here and now  set depending
reward discounting factor   mode planner  use p        
replanning probability     
example proximities  doors problem shown figure   initial situation  agent h    i  doors closed  possible situation later execution
 agent h    i  doors closed   larger symbols correspond higher proximity  one
immediately see agents planned path goal  large symbols correspond
states agent expects visit  conversely small proximities show locations
agents planned path goal  example  agent expect visit
states south western room  especially already passed door
   similarly  proximities around initial state much lower agent
h    i  expect need return 
    discussion
one interesting feature resulting numbers emphasise absorbing nearabsorbing states somewhat might intuitively expected  however  considering
   

fiproximity based non uniform abstractions planning

x  

 

 

 

 

 

 

 

 

 

x  

y  

y  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

hx         i

 

 

 

 

 

 

 

 

 

hx         i

figure    proximities  doors problem s  possible later scur   symbol size
logarithmic  proximities range            p         replanning probability     

absorbing states general important  good feature  especially since
normally planner try minimise probability entering absorbing state
 unless goal   feature help ensure absorbing states kept
mind long chance falling them  dean et al          instance 
note algorithm undesirable absorbing states along path goal
tend come candidates removal consideration  due low probability
reaching current policy   make special accommodation
removed consideration  proximity measure emphasising
states  special handling necessary 
contrast approach  kirman        uses probabilities es steps 
es  an estimate of  number steps agent take switching
previous policy policy currently calculated  assumes es
estimated well  current policy policy executive  oneplanning cycle probability appropriate measure  fact one would prefer least
two planning cycle look ahead  agent begins within area focus
new policy  remains throughout validity policy  probably
longer  since planners foresight extend beyond next thinking cycle 
philosophically  reliance planning cycle length desirable 
artefact planner rather intrinsic domain 
somewhat related approach prioritised sweeping  see instance barto  bradtke 
  singh         present approach  defines measure states
way interesting  unlike approach  applies measure determine
   

fibaum  nicholson   dix

order formulae v calculation applied 
applied preferentially interesting states less frequently uninteresting
unimportant states  well known order calculation mdp planning
algorithms varied greatly without forfeiting convergence optimal policy 
prioritised sweeping takes advantage this  often done measure change
v previous calculations  approaches use look ahead current state 
ways simple version proximity  in fact  corresponds
threshold p replanning probability set     proximity measure p might well
good candidate approach  apply v calculation states chosen directly
according p distribution  
munos moore        use influence measure deterministic continuous
domains  similar p  fact  main difference measure
two parameters re uses replanning probability
 effectively zero   means cannot take account replanning  neither
difference horizon entails  possibility policy may change
acted upon  absorbing states  instance  would emphasised
proximities 

   proximity based dynamic abstraction
proximity measure described previous section used focus planners attention areas likely useful near future  firstly  means worldview
made match proximities  refining coarsening appropriate  secondly  since proximity measure takes account current state  method
automatically update worldview agents circumstances change recurrent
mode  is  planning execution concurrent 
    refinement
high proximity indicates states agent likely visit near future 
planner therefore plan states carefully  abstract  reason
refine allow detailed planning  states high proximity
therefore considered candidates refinement 
high proximity defined simple threshold  shown algorithm   
refinement occurs  anomaly sometimes appears  anomaly led
policy based refinement method  arises different levels abstraction  here 
adjacent abstract state causes problem  rather recentlyrefined one  state refined  values v new states initially estimated
states previous value v   however  typically  means
overestimated others underestimated  policy re calculated 
state overestimated value attractive 
since problem directly follows moment refinement  self correcting 
iterations  planner converges correct policy values  however 
   retaining theoretical guarantee convergence desired  care would taken since

p zero states reachable current state  practice  course  optimality
otherwise unreachable states immaterial 

   

fiproximity based non uniform abstractions planning

algorithm   proximity based refinement
stochastically choose dimension
worldview states w
p w    threshold w abstract
   replace w
group states concrete   
anew
w
concrete
new
w
 

dimension wnew   dimension w   
w w  wnew  
new
 wnew    w   v  wnew   v  w   p wnew    w w    p w 
w w    w     discarding stored  w   v  w  p w    

so  transient anomalies appear policy  worst case 
planner may replan path  refine states re trigger
anomaly  rather large parts state space spuriously refined way 
occurs combined v calculation phase  may update
v chance converge  solution create variant phase  v
calculation only  replaces v calculation phase values stabilise 
two iterations  appears sufficient  alternative solution would
copy difference values adjacent  concrete states
possible  thus obtaining better estimated values newly refined states  however  since
simpler solution v  only calculation works satisfactorily  complex possibility
explored 
    coarsening
low proximity indicates states agent unlikely visit near future 
planner therefore need plan states carefully  usually  already
abstract  never refined first place  however  concrete
previously refined reason coarsen free memory
cpu time detailed planning elsewhere  states low proximity
therefore considered candidates coarsening 
proximity based coarsening useful primarily on line planning scenario recurrent planning  agent moves state space current state
scur changes  states likely visited near future  especially useful agent finds unexpected part state space  instance
due low probability outcomes  agent planned path leading part way
goal  perhaps partial reward   case  however  parts state
space already traversed coarsened favour refinement front agent  
one might imagine planning progresses  planner may wish concentrate
different parts state space coarsening might useful cull abandoned
explorations switch focus  however  observed domains
   states already traversed cannot discarded  even agent never visit again  since
worldview partition since agent necessarily know whether need revisit  or
end revisiting  states 

   

fibaum  nicholson   dix

found pre cursor mode  coarsening generally worsens quality policies
positive contribution 
coarsening proceeds three steps  shown algorithm    first step
similar proximity based refinement  time proximity based coarsening phase
invoked  worldview scanned states low proximity  below threshold  
put list candidates  second step tricky  coarsening needs join
several states one  however  representation allow arbitrary partitions
worldviews therefore allow coarsening together arbitrary set
worldview states  planner must therefore find group states among lowproximity candidates coarsened valid worldview state  groups
detected fact differ one dimension size
dimension  therefore covering completely  finally  groups found
replaced single abstract state 
algorithm   proximity based coarsening
   collect candidates coarsening   
candidates  w   p   threshold 
   find groups candidates coarsened together   
   partition candidates according pattern abstract concrete dimensions   
patterns candidates     wa   wb       wa concrete wb concrete d 
groups
p patterns
dimensions
states p concrete
   partition p dimensions except d  giving potential groups   
potgroups p     wa   wb          dimension wa   dimension wb  
   add potential groups size dimension groups   
groups groups  g potgroups    g     sd   
   replace group states single  abstract state   
g groups
g w
stochastically choose
wa g
new
w
abstract
construct wnew  
dimension wnew   dimension wa   
w w  wnew  
p
  p
new  


 wnew    wa    v  wnew    g 
wg v  w   p w
wg p w 

w w   g    discarding stored  w   v  w  p w  w g   

cases  may impossible coarsen section worldview despite low
proximity  due situation somewhat akin grid lock  probably simplest example
one figure    shows worldview five states three dimensional binary
specific state space  three states ignoring different dimension each 
remaining two take account three  situation  group states
   

fiproximity based non uniform abstractions planning

s 

 

 

 

 

 

 

 

 

s 

 

 

 

 

 

 

 

 

s 

 

 

 

 

 

 

 

 

w 

w 

w 

w 

w 

figure    non uniform worldview cannot immediately coarsened  state space
three binary dimensions  eight states   worldview two concrete states 
w  w    three abstract states  w    w  w    abstract different
dimension 

coarsened single dimension  coarsening possible  one states must
first refined  low p candidates proximity based
refinement  this  integration uniform abstraction method coarsening
would straightforward selecting initial worldview refinement 
unless worldview kept uniform  however  even non uniform worldview would
difficult  instance  dimension could simply removed possible
rather everywhere 

   results
run algorithm range different domains demonstrate approach 
domains divide two broad groups  first group consists grid navigation
domain only  domain intuition gathered preliminary runs
done  however  problems domain show well approach performs 
cannot show generality  second group consists domains literature 
demonstrating well approach generalises 
    experimental domains
introduce domains section  first five problems grid navigation
domain  two already described section      shown figure    three additional
problems  remaining domains based domains literature  particular
used kim        barry        
described section      problems grid navigation domain shown figure  
x dimensions size     three door dimensions  binary  open closed 
damage dimension  also binary   far  doors problem   keys
problem  keys agent must pick keys open corresponding doors 
three remaining problems  key  shuttlebot        key problem
similar  keys  except agent capable holding one key time 
instead three binary dimensions keys  one four valued dimension
indicating key agent holds  or none   shuttlebot problem introduces
cyclic goal  with extra loaded dimension damage dimension tri valued 
   

fibaum  nicholson   dix

 a  grid navigation domain
keys world
problem
 doors
 
 key
 
 
 keys
 
shuttlebot
    
 

keys held time

 
      



note

cyclic
tiled

dimensions
 
 
 
 
 

 s 
     
     
      
     
       

 b  robot   k domain
problem
robot     
robot     
robot     
robot     

dimensions
  
  
  
  

 s 
      
       
          
           

 c  factory domain
problem
s factory
s factory 
s factory 
 d  tireworld domain
locations
problem
tire small
 
 
tire medium
tire large
  
  
tire large n 

initial
n 
n 
n  
n 

dimensions
  
  
  

goal
n 
n 
n 
n 

 s 
       
         
          

route length
 
 
 
 

dimensions
  
  
  
  

 s 
     
       
                 
                 

table    experimental domains problems  dimensionality state
space size 

     variant increases size problem tiling grid   
direction  by two extra dimensions  xx yy  size      table   a  summarises
problems domain 
next two domains based kim         firstly  robot   k domain 
based kims        robot k domain reducing number actions
four  robot   k domain problems consist cycle k rooms shown figure   
room light  analogous doors  doors problem
enable agent move  four actions variant go forward  turn light
current room off  nothing  original formulation allowed agent
combination toggling lights going forward  total  k   actions 
reduced approach intended approximate action space 
goal move first room last  k     dimensions state space
k k states  listed table   b  
   

fiproximity based non uniform abstractions planning

 
k  

 

 

 
 

figure    robot   k domain 
drill b
part b 

shape b

drilled

polish b

polish b

shaped

dip b
polished

spray b
handpaint b

painted
glue

connected

bolt
drill
part a 

shape

shaped

drilled
polish

polish

dip
polished

spray

painted

handpaint

figure    factory domain 
kims        factory domain   series variants simple manufacturing problem  represented purely predicates  that is  dimensions size     agent make
product two parts must drilled  painted finally joined together 
figure   shows simplified diagram  omitting interactions options
instance  achieve painted predicate  agent may spray  dip handpaint
object  connect two objects  may use glue bolt  and latter requires
drilled   on  unlike domains  partial rewards available
agent achieving certain subgoals  problems used listed table   c  
final domain tireworld domain      icaps ipc competition
 littman  weissman    bonet        used barry         domain  robotic
car trying drive point point b  car room carry one spare tire
locations additional spare tires them  locations  car
carrying spare  pick one up  n locations car 
 n     binary dimensions problem  follows  n dimensions used represent
location car  valid states states one location dimension
true  explicitly stated anywhere domain    another n dimensions
used represent locations spare tire not  final two
dimensions represent whether car carrying spare whether flat tire 
    domain previously used hoey  st  aubin  hu  boutilier        based
builder domain dearden boutilier        adapted standard job shop scheduling
problems used test partial order planners 
    touch aspect discussion section    case include domain without
change order facilitate comparison literature 

   

fibaum  nicholson   dix

n 

n  
n  
n 

n 

n 
n  

n  

n 

n 

n 

n 
n  
n 

n 

n  

n 

n  

n  
n 

n 

n 
n 

n  

n 

n  
n 
n 

n 

tire small

n 

n  

tire medium

n 

tire large

goal    
locations 
figure    tireworld domain problems  indicating initial    

barry        uses two tireworld problems  labelled small large 
small tireworld problem   locations    variables    actions  large
one    locations    variables     actions  curiously  large problem 
direct road initial goal locations  takes single action
solve problem  makes difficult assess whether barrys method has  fact 
scaled up  addition two  created medium sized tireworld    locations
   variables  removing locations large tireworld moving initial
location n   goal  variants listed table   d  shown
figure    final variant  tire large n   shown  identical large tireworld
except initial location moved n  
    direct evaluation policies pre cursor deliberation
smaller problems  doors   key   keys  directly evaluate
approximate policies produced planner running pre cursor deliberation 
problems small enough use exact algorithm calculate actual value
function corresponding approximate policies  noted section     
useful involves fewer potentially confounding variables  exploit full
potential approach   
table   a  shows results policy based refinement is 
proximity based methods  algorithms         disabled  problem  table
lists size problem  s  value optimal solution initial state
    proximity based coarsening  algorithm    primarily aimed regions state space agent
already traversed  pre cursor deliberation traversal  coarsening would therefore
expected bring limited benefit pre cursor deliberation direct evaluation would
meaningful evaluate performance  therefore evaluated recurrent deliberation 

   

fiproximity based non uniform abstractions planning

olu
tio
nv
alu
wo
e
rld
vie
w
siz
e
rel
ati

wo
rld
vie
w
siz
pla
e
nn
er

esti
sol
uti te

val
ue
act
ua
ls
olu
tio
nv
alu
e

ize

op
tim
al


sta
te
sp
ce


dis

cou
nti
ng

fac
tor

v  s     representing costs results exact planning  followed
size worldview  w  absolute number percentage  s   planners
estimate value solution initial state v  s     actual value
solution initial state v  s     first half part table discounting
factor             second half         averages    runs
shown planner run             chosen approximation
assumed       phases sufficient planner converge  practice 
convergence generally took place much earlier  detected  however 
overall assumption planner continues plan forever  responding changing
inputs  makes convergence somewhat irrelevant 


problem
 s 
v  s   
 w 
 a  policy based refinement
     
          doors
           
     
 key
           
 keys
            
     
     
    
 doors
           
 key
           
     
     
 keys
            
 b  proximity based refinement
          doors
                   
 key
                   
 keys
                    
    
 doors
                   
 key
                   
 keys
                    
 c  policy  proximity based refinement
          doors
                   
 key
                   
 keys
                    
    
 doors
                   
 key
                   
 keys
                    

 w 
 s 

v  s   

v  s   

   
    
    
   
    
    

     
         
         
     
     
     

     
          
          
     
     
     

   
   
   
   
   
   

     
         
         
     
     
     

     
         
         
     
     
     

   
   
   
   
   
   

     
         
         
     
     
     

     
         
         
     
     
     

table    results direct evaluation policies pre cursor deliberation three
different refinement methods  evaluated       phases 

   

fibaum  nicholson   dix

results part  a  table divide neatly two types  without keys   doors
problem   planner succeeds ten runs  getting perfect policies given starting
state  two problems   key  keys  planning invariably fails  two
problems  agent must pick key far door opens  version
planner simply cannot think ahead extent  three these  planner
somewhat optimistic  estimating better value obtains cases even
better optimum  instance   doors problem             planners
estimate value v  s           better true value
optimum  v  s      v  s             fractional  w  table due
averaged ten runs  final size worldview sometimes depends extent
order dimensions states refined  order randomised
runs  instance   doors problem             w various
sizes ranging         states end ten runs  average
      
results part  a  similar two values   main difference
smaller leads smaller numbers  instance  value indicating failure        
 
 
                      values tend multiples  
 
smaller value here       rather          cases 
smaller range make differences less obvious  instance  estimated value
 
column  v  s      clear whether numbers approximations    
 
 and failure reach goal     
minus small number  representing success  
 
represent rewards costs
units represent once off rewards costs  multiples  
obtained perpetuity  however  expected desired behaviour  smaller
represents disinterest distant future  reward cost perpetuity
much important once off reward cost 
table   b  shows results ten runs pre cursor mode      proximitybased refinement  no policy based refinement coarsening  problem
  seen   doors problem solved optimally cases 
surprising  complex problem 
 key  keys problems interesting  figures table   b  arise
average    successful runs  values close equal optimal values
v  s        unsuccessful runs values                     
planner found successful policy      runs  key problem   times
    keys problem  similarly        case    times   times 
 
respectively   since optimal path quite long compared  
  success
means reward        or        failure punished     effect
difficult discern 
table   c  shows results proximity based refinement policy based refinement combined  no coarsening   naturally   doors problem either refinement
method alone already obtained optimal policy shows improvement  worldview size  w  differs slightly table   b   policy based refinement sometimes
directed   w  tend slightly smaller exploratory proximitybased refinement alone  larger policy based refinement alone 
   

fiproximity based non uniform abstractions planning

two problems   key  keys  show improvement compared either
refinement methods alone  solved    runs           
values                     represent averages     unsuccessful runs
    successful ones  compared     successful runs proximity based
refinement successful runs policy based refinement only         
 key problem solved   runs  due discounting length
path  goal near horizon  again  success meaning reward
      failure receives     distinction great   keys problem
       find solution parameters  receives uniform
   runs  suboptimality one unit 
behaviour runs generally quite straightforward  typically 
initially calculating agent cannot reach goal initial worldview 
worldview size gradually increases  plateaus coarsening here  movement agent  behaviour really possible  successful runs 
planner plans route goal point increase  worldview becomes sufficient  v  s    quickly reaches final value  rarely  v  s    may oscillate
twice first  omit graphs here  presented baum        
    evaluation simulation recurrent deliberation
larger problems  performance evaluated simulation  running agent
simulated world observing reward collects  problems  direct
evaluation possible calculating actual value function using exact
algorithm longer tractable  simulation recurrent delibertion context
coarsening evaluated  comparison  section presents results
 keys problem evaluated simulation  without coarsening 
figure   shows representative sample results simulation  keys problem
refinement methods coarsening  combination options
shown table   c  previous section  evaluated simulation rather directly 
small graph shows different individual run  seen  agent behaves
reasonably working recurrent planning mode simulation 
left vertical axes graphs represent reward r  plotted thick red lines 
run   figure    instance  agent starts receiving reward  
step  meaning goal  damage domain      onwards  receives
reward   per step  meaning goal  damage  right vertical axes
worldview size  w   thin blue lines  shown details throughout section 
is  scaled actual worldview sizes rather full   s  ranges  taking run  
figure   again  see  w  grows relatively quickly     continues
grow slowly eventually levels little        full state space 
comparison          run    agent received reward similarly  state space
grew longer  eventually levelling somewhat       runs      agent
failed reach goal continued receiving reward   throughout  run   
worldview size levelled little       run   steadily grew      
horizontal axes simulated world time  corresponding discrete time steps
mdp  two time scales simulation  wall clock time  indicating
   

fibaum  nicholson   dix

  

 w 

r

  

    
    

 

    
    

 

    

    

    

    

    

    

    

  

 w 

r

    

  

    
 

  

   

   

   

    

 
   

 

 w 

r

  

   

time

  

   

   

time

r

  

    
    

 

 w 

    
    

 

    

    

    

    

    

    

    

  

    

  

    
 

  

   

 
   

   

   

    

 
   

 

time

  

   

   

   

 
   

time

figure    simulation results   keys problem  policy based proximity based refinement 
coarsening  four runs   reward  left axes  thick red lines  worldview size
 w   right axes  thin blue lines  detail  world time  horizontal axes  

passage real time  number phases planner performed  simulation configured take   time step per   s wall clock time  number phases
r
controlled simply given speed     ghz intel
  cpu implementation coded flexibility rather efficiency  ideally  agent gradually
move general direction goal planning  simplifies problem 
fast agent runs far ahead abstraction planner 
planner algorithm terminate  since planner assumed keep planning
 and agent keep acting  indefinitely  goal oriented domains 
examples paper  one might consider achieving goal termination
condition   a  example domains assume agent continue
goal goal maintenance  albeit trivial   b  apply non goal oriented
domains  c  even goal oriented domains clear apply condition
case agent fails reach goal  simulation  therefore  runs either
terminated manually  succeeded appeared progress
   

fiproximity based non uniform abstractions planning

  

 w 

r

 

  

 

  

    
    
    
    
    
    
    
    
    
 
                              

 

  

 

time

 w 

r

    
    
    
    
    
    
    
    
    
 
                              
time

figure     simulation results illustrating effect coarsening worldview size   keys
problem  policy based refinement  proximity based refinement coarsening
 two runs  

likely made  run fixed number world time steps  selected based
manually terminated runs allowance variation 
coarsening figure    worldview sizes monotonic increasing 
different runs refined differently domain algorithm stochastic 
beginning planning  agent receiving reward   per step 
yet goal  worldview size increases  planner eventually finds policy
leads goal runs      seen better reward   obtained
runs  simple relationship worldview size performance  runs
worked worldview       larger generally succeeded  smaller
worldviews generally not  vast majority runs          agent reached
goal 
coarsening  algorithm    activated  compared situation turned
off  reward gathered agent declines slightly  still reaches goal
vast majority runs          figure    shows two runs  one successful one
unsuccessful   keys problem proximity based coarsening well two
refinement methods  contrast figure   two refinement methods
used  note effect interleaving refinement coarsening  worldview
size  w   thin blue line  longer monotonic  instead alternately increased
decreased  shows jagged line graph  slightly fewer runs reach
goal  decline solution quality expected  however  since goal coarsening
reduce size worldview 
completeness  tested agent proximity based methods
 algorithms      active policy based refinement  algorithm    deactivated 
configuration  agent collects reward generally takes steps toward
goal  without directed policy based refinement  largely exploratory
proximity based methods discover keys consequently cannot reach goal 
   

fibaum  nicholson   dix

    effect discounting factor
shuttlebot problem similar  doors problem requires agent move
back forth two locations repeatedly  interesting preliminary
runs pre cursor mode solved            solved optimally
        considered whether            case might behave better
simulation  agent took advantage possibility planning nearest
reward replanning reward obtained  all  agent could function
well even none policies good solution itself  however  illustrated
figure     agents behaviour similar pre cursor case             
 a  refinement only  run     b  coarsening  run    would pick reward
immediately adjacent s    that  again  setting planners discounting
factor        c  coarsening  runs      provided much better performance   
note effect balance refinement coarsening  b   c  
worldview size  w  nice steady throughout runs  though admittedly fair
fraction  s           
    initial worldview
problems  standard initial worldviews large planner  even
modified  smaller initial worldviews obtained enabling nexus step algorithm  
disabling reward step large  disabling reward nexus steps
results singleton initial worldview  w    s   treats entire state space
single  very  abstract worldview state  unfortunately  means planner
starts little way hints direction refine and  least
initially  information base crucial decision  upshot
collects reward  cases remains initial state s    others moves around
state space sometimes distance  times small loop
reach goal subgoals   
situation factory domain problems kim         fact
agent collected reward simulated runs  even though quite runs
substantial actions taken  similar result occurs   x   problem  in grid
navigation domain   reward obtained agent problem 
standard initial worldview somewhat large planner and  again  singleton
initial worldview badly  best  runs  agent took limited steps
general direction goal 
interesting case tireworld domain  again  tire large large
standard initial worldview fails obtain solution reward step initial
worldview only  however  manually chosen initial worldview refines locations
along path start state goal planning begins  planner solves
tire large  tire large n      runs  in one less
one minute  although atypical  
    rewards appear two horizontal lines runs      one solid one broken  task
cyclic  agent collects reward   twice cycle reward   steps 
    details unsuccessful runs  including  w  behaviour  given baum        

   

fiproximity based non uniform abstractions planning

 a              coarsening  one run 
r
 w 
  
    

 b              coarsening  one run 
r
 w 
  
    

    

    

    

 

    

 

    

    

    

    

    

    

    
 

    
 

    

    

   
 

    

    

    

   

 
    

 

   

time

   

   

   

 c          coarsening  two runs 
r
 w 
  
    

  

 w 

r

    

    

 

    

    

    

    

    

    

    
 

    
 

    

    

   
    

    

    

    
    

    

 

 

 
    

time

   

 
    

 

time

    

    

    

    

 
    

time

figure     simulation results illustrating effect discounting factor  shuttlebot
problem  policy based proximity based refinement 

    worldview size quality
finally  consider effect worldview size quality robot  domain 
agent moves series rooms lights  domain excellent example
simulated agent works well  runs robot      robot      problems
agent thought small amount time  quickly moved goal stayed
there  small worldviews  seen figure    robot     
problem  four representative runs shown  two two refinement methods
 runs      two three methods  runs       four runs  worldview
sizes  w  reasonable consider full state space contains almost half million
states        state worldview represents fifth percent  despite small
worldview size  however  planner effective  dozen phases  agent
reached goal  planner works well robot      robot      
comparison  kims        largest robot k problem robot     though since
robot                    actions robot   k domain problems   
   

fibaum  nicholson   dix

 a  coarsening  two runs 
r
  

 w 

  

    

 w 

r

    

    
    

 

 
   

   

   

   

   

   

 

 
   

   

 
                                

 
                                

time

time

 b  proximity based coarsening  two runs 
r
 w 
  
  
    

 w 

r

    

    
    

 

 
   

   

   

   

   

   

 

 
   

   

 
                                

 
                                

time

time

figure     simulation results  robot      problem              policy based proximity based refinement  without proximity based coarsening 

direct comparison would valid  hand  values k        
on  necessarily powers    since  unlike kim  domain specification
always considers room numbers atomic rather binary numbers  particular
advantage powers   
results robot      problem beginning interesting
robot      robot       figure    a   showing two runs coarsening
 runs       agent succeeds reasonably promptly reasonable worldview
sizes  however  illustrated figure    b   coarsening active planner fails
reach goal runs  about      example  run    succeeds others
 about      example  run     state space contains almost    million states 
successful worldviews figure    order       full state space size 
figure    shows four representative runs robot      problem   a  two without coarsening  runs       b  two three methods  runs       problem state space            million states  effect noted robot     
   

fiproximity based non uniform abstractions planning

 a  coarsening  two runs 
r
  

 w 

  

    

 w 

r

    
 

    

    

 

 

    

    

    

    

    

    

    

    

 

    

    
 

  

   

   

   

    

 
   

 

  

   

time

   

   

 w 

r

    
 

 

 

    

    

    

    

    

    

    

    

 

    

    
   

   

   

    
    

    

  

 
   

time

 b  proximity based coarsening  two runs 
r
 w 
  
  
    

 

    

    

 
   

 

time

  

   

   

   

 
   

time

figure     simulation results  robot      problem              policy based proximity based refinement  without proximity based coarsening 

much pronounced here  without coarsening  planner tends much larger worldviews   large worldviews cause planner run slowly  noted section    
above  horizontal axes world time  planning time  relation two
varies quite significantly runs two policies per time step
smaller worldviews less one ten time steps worldviews grew
large 
far reaching goal concerned  two cases similar  again  successful
runs maintain reasonably sized worldview  runs      runs
worldview size grows big invariably fail  runs       difference
time  smallest successful worldview  run   figure     used around       well chosen
worldview states            full state space  worldview grows
beyond miniscule fraction state space even        state worldview
    note run   plotted different scale worldview size  w  axis compared runs     
   makes details behaviour easier see  makes size run   less obvious 

   

fibaum  nicholson   dix

 a  coarsening  two runs 
r
  

 

 

 w 

  

     
     
     
     
     
     
    
    
    
    
 
                              

 w 

r

    
    

 

    
    
    
    

 

    
    
 

time

 
                              
time

 b  proximity based coarsening  two runs 
r
 w 
  
  

 w 

r

    

    

    
 

    
 

    

 

    

    

    

    

    

    

    
 

    

    

    
 

   

   

   

    

 
    

 

time

 
                           
time

figure     simulation results  robot      problem              policy based proximity based refinement  without proximity based coarsening  note
different scales worldview size  w  axis run  a   

run   figure             planner stall progress
possible  even challenging environment  agent reaches goal almost half
runs 

   discussion
section   presented results across range experimental domains showing method
successfully finds solutions planning problem using much less full state space 
well limitations  section discuss results analyse
features domains method exploit give difficulty 
smaller problems  could directly evaluate policies produced method
pre cursor mode  allowing us better isolate behaviour planner  without
proximity based methods  worldviews quite small planner could solve
   

fiproximity based non uniform abstractions planning

 doors problem  however  even better uniform abstraction  could
little here  even oracle could best remove one door key  keys
two doors  doors  giving     relative worldview size however  planner
used uniform distribution removed dimension  agent would fail anyway  since
opening doors left worldview would harder hoping best
assumed     open door actually closed  succeed  would
deduce abstracted doors considered closed  considerable feat 
function sample domains  circumstances  uniform abstraction could
effective  either pre processing step approach integrated w selection
methods 
expected turning proximity based refinement  algorithms      would
lead larger worldviews  general  one would expect larger worldviews yield better
solutions smaller worldviews yield worse solutions  lower computational cost 
means proximity based refinement should  general  improve solution quality 
results corresponded expectation  worldviews indeed larger
solution quality higher 
larger problems  performance could evaluated simulation  running
agent simulated world observing reward collects  problems 
direct evaluation possible calculating actual value function using
exact algorithm longer tractable  section     therefore presented results
 keys problem comparison obtained direct evaluation policies
pre cursor deliberation discussed above  seen  results correspond  crossconfirming evaluation methods 
addition  simulation recurrent delibertion context coarsening
could evaluated  proximity based coarsening activated  compared
situation turned off  reward gathered agent declines slightly 
impression worse performance somewhat misleading  due fact first
comparison takes place one small problems  planner able solve
without coarsening  fact without abstraction all  thus  disadvantages
 lower reward collected  much apparent advantages  lower computational
cost  
larger problems  working without abstractions option  balance
reversed  fact  somewhat counterintuitively  larger problems
coarsening active  successful runs smaller worldviews unsuccessful
runs  clearly  size worldview determines success  quality 
good worldview enabled efficient calculation policies progress toward goal
remaining small  poor worldview simply grew larger  smaller problems  growing
worldview may eventually covered state space detail  thus masking
effect  planner would find good policy effectively without real approximation 
larger problems  finding good policy without approximation feasible 
similarly growing worldview simply slowed planner progress
made  situation  worldview reducing action proximity based coarsening
became crucial  ensuring least worldview remained tractably small
thereby enabled planner deal problem 
   

fibaum  nicholson   dix

seen results  coarsening successful task
time  agent paused replan part way goal  reduces size
worldview keep relevant changing circumstances  runs  however 
worldview size grew beyond capabilities planner  cases 
  x   problem  settled higher balance  others  appears simply
continued growing  latter case  would appear simple question
tuning parameters  find balance  appropriate worldview size 
appears factor  quality worldview determining
success failure runs whether find balance reach goal grow
big fail 
number problems solved poorly due initial abstraction
selection algorithm  algorithm     problems  algorithm produced either large
worldview exceeded available memory  either immediately shortly afterwards  
planning possible  small worldview planning ineffective 
could set produce medium sized worldview  none four combinations
options produced one  problems  singleton worldview possible
is  steps initial abstraction selection disabled  resulting worldview aggregating
states single  maximally abstract worldview state leading typically reward
collected agent  best  would take actions general direction
goal s   considerably worse previous work  instance  kim       
obtains approximate solutions problems larger variants  others
use domain variant  including hoey et al         originators  dearden
boutilier        
necessity using singleton initial worldview understandably greatly
hurt performance  infelicity worldview initialisation stage could impair
entire planning process  since worldview never completely discarded planner 
worldview improvement algorithms could made amount weakness
initial worldview  singleton worldview poor starting point indeed 
similar observation made dean et al         work using reduced envelope
states  that is  subset state space   high level algorithms  here 
work regardless initial envelope  worldview   practice  however  better
initial envelope chosen intelligence  instance contain least
possible path goal  for goal oriented domains   find path using simple
depth first search  directly applicable worldviews
gradations abstraction  overall concept remains  reasonable initial worldview
crucial 
tireworld results confirm this  initial worldview reward step
enabled small  since domain rewards single dimension  planning
ineffective  planner given better initial worldview one could
plausibly calculated became quite effective  even modified tire large n 
initial state deliberately moved goal  seems  then 
basic approach general  worldview selection modification methods
less so  good worldview selection less fundamental apect approach
easily supplemented additional methods even tuning  domains
   

fiproximity based non uniform abstractions planning

tireworld  seems modified predicate solver generate plausible trajectories
current state goal would well part worldview selection 
interesting compare results sanner boutilier       
tireworld  describe passing extremely poorly approximated
going manually tweak domain planner  adding information
locations mutually exclusive  makes planning much easier largely invalidates
comparison approaches    fair question  however  extent
weakness planner extent artefact domain  combination
representation narrative seems rather unfortunate  narrative obvious
  of n intuition tend obscure real features  and real applicability 
propositional representation hinder rather help intuition  would occur
tighter fit representation narrative 
others  course  solve tireworld domain well  some  barry  kaelbling 
lozano prez         generate full policy  others take advantage initial state 
planner would do  difficult know extent planners adapted
domain extent flexible  seems recent years
become common planners tested domains researchers
access development  icaps ipc domains  rather
greater lesser degree hand tuned  usually unconsciously  particulars one
another domain  undoubtedly unconsciously tuned grid navigation
domain 
interesting side point provided shuttlebot problem  solved
            other collecting trivial reward immediately adjacent
s    solved optimally         since simulator
intrinsic discounting factor reports reward collected one see
even though planner working discounting factor         provided
better solution            case worked            first
place 
ways better behaviour smaller planner discounting factor reasonable  agents horizon represented world discounting factor 
horizon particular policy therefore planner effectively much shorter 
policy supplanted new one relatively soon  thus  may useful
occasion set planners discounting factor lower true world discounting factor order facilitate planning  however  may lead suboptimal  short sighted
policies 

    conclusions
theory markov decision processes provides algorithms optimal planning  however 
larger domains algorithms intractable approximate solutions necessary 
state space expressed terms dimensions  size resulting computational cost exponential number dimensions  fortunately  results
structured state space effective approximations possible 
    similarly  kolobov  mausam  weld        report results variant tireworld rather
tireworld itself  without providing explanation 

   

fibaum  nicholson   dix

approach based selectively ignoring dimensions parts
state space order obtain approximate solutions lower computational
cost  non uniform abstraction dynamically adjusted planning  in on line
situations  execution progress  different dimensions may ignored different parts
state space  strong implications  since resulting approximation longer
markovian  however  approach intuitive practical  synthesis
two existing approaches  structure based approximation uniform abstraction
dynamic locality based approximation envelope methods  envelope methods 
limited reliance initial worldview  or envelope   poor 
tend perform poorly overall  approach subsumes uniform abstraction completely
treated special case general method 
paper extends preliminary work baum nicholson        modifying
worldview based proximity measure  enlarging reducing size 
evaluating behaviour simulation  allows us test approach larger
problems  importantly demonstrates full strength approach
limits terms domain features exploit exploit
adjustment all  abstraction becomes truly dynamic  reacting changes
agents current state enabling planning tailored agents situation
changes  shown qualitative quantitative results presented
baum         approach effective efficient calculating approximate
policies guide agent simulated worlds 
     future work
one possible direction future research would find worldview initialisation
modification methods result smaller yet still useful worldviews  probably domainspecific  extend method domains  either larger different features 
example  factory tireworld domains goal oriented based predicates 
worldview selection modification method based predicate oriented solver could
find possible paths goal ensure relevant preconditions concrete along
path 
interestingly    x   problem  proximity based methods keep
worldview size small  seem find balance larger stil moderate
size  thus another possibility might tune proximity based methods develop
self tuning variants 
number points  instance phase selection  algorithm uses stochastic choice
default  could replaced heuristics  learning  directed methods 
one could adapt method work types mdps  undiscounted
finite horizon ones  combine approaches approximate different aspects domains planning problem  described section      example 
mentioned section  gardiol kaelbling              combine hierarchical state
space abstraction somewhat similar envelope work dean et al         
many combinations would likely fruitful planning domains features relevant multiple methods  similarly  additional refinement coarsening methods
   

fiproximity based non uniform abstractions planning

could added  instance one based after the fact refinement criterion
roll back reyes et al         
theoretical side  one could look situations optimality
guaranteed  hansen zilberstein        lao  algorithm work
dean et al          observing admissible heuristic used evaluate fringe states 
rather pragmatically chosen v  out   algorithm related heuristic search
acquires stopping criterion guaranteed optimality  or  optimality   perhaps
similar condition could developed approach  rather different heuristic 
two basic directions work extended
fundamental way  relaxing one mdp assumptions  perfect observability knowledge
transition probabilities  partially observable markov decision process  pomdp 
gives agent observation instead current state  observation partly
random partly determined preceding action current state 
optimal solution known principle  quite computationally expensive  since transforms pomdp larger  continuous  many dimensional mdp agents beliefs 
such  non uniform abstraction approach could applied two different ways  either original pomdp  fairly direct translation  transformed mdp 
extension would apply technique agent learn transition
probabilities  particular  application technique exploration   would
interesting agent would somehow learn distinctions within single abstract states  distinguish refined remain
abstract 

references
de alfaro  l     roy  p          magnifying lens abstraction markov decision processes 
proceedings   th international conference computer aided verification 
cav    pp         
barry  j   kaelbling  l  p     lozano prez  t          hierarchical solution large markov
decision processes  proceedings icaps workshop planning scheduling
uncertain domains 
barry  j  l          fast approximate hierarchical solution mdps  masters thesis 
massachusetts institute technology 
barto  a  g   bradtke  s  j     singh  s  p          learning act using real time dynamic programming  artificial intelligence  special volume  computational research
interaction agency                 
baum  j          dynamic non uniform abstractions approximate planning large
structured stochastic domains  ph d  thesis  clayton school information technology  monash university  available www baum com au jiri baum phd ps gz
    learning problem could transformed mdp agents beliefs experiences 
would computationally prohibitive  standard approaches instead explicitly distinguish exploration  agent learns domain  but ignores goals  exploitation  achieves
goals  but ignores opportunities learn  

   

fibaum  nicholson   dix

baum  j     nicholson  a  e          dynamic non uniform abstractions approximate
planning large structured stochastic domains  lee  h  y     motoda  h   eds   
topics artificial intelligence  proceedings  th pacific rim international conference artificial intelligence  pricai      pp         
bellman  r  e          dynamic programming  princeton university press 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
botea  a   enzenberger  m   muller  m     schaeffer  j          macro ff  improving ai
planning automatically learned macro operators  journal articial intelligence
research             
boutilier  c          correlated action effects decision theoretic regression  geiger  d  
  shenoy  p   eds    proceedings   th conference uncertainty artificial
intelligence  uai      pp       
boutilier  c     dearden  r          approximating value trees structured dynamic programming  proceedings   th international conference machine learning 
pp       
boutilier  c   dearden  r     goldszmidt  m          exploiting structure policy construction  mellish  c  s   ed    proceedings   th international joint conference
artificial intelligence  ijcai      vol     pp           
boutilier  c   dearden  r     goldszmidt  m          stochastic dynamic programming
factored representations  artificial intelligence                   
boutilier  c   goldszmidt  m     sabata  b          continuous value function approximation sequential bidding policies  laskey  k     prade  h   eds    proceedings
  th conference uncertainty artificial intelligence  uai      pp       
cassandra  a  r   kaelbling  l  p     kurien  j  a          acting uncertainty  discrete bayesian models mobile robot navigation  tech  rep  tr cs        computer
science  brown university 
daoui  c   abbad  m     tkiouat  m          exact decomposition approaches markov
decision processes  survey  advances operations research            
dean  t   kaelbling  l  p   kirman  j     nicholson  a  e          planning time
constraints stochastic domains  artificial intelligence                 
dearden  r     boutilier  c          abstraction approximate decision theoretic planning  artificial intelligence                 
dietterich  t  g          hierarchical reinforcement learning maxq value function
decomposition  journal artificial intelligence research             
drummond  m     bresina  j          anytime synthetic projection  maximizing probability goal satisfaction  dietterich  t     swartout  w   eds    proceedings
 th national conference artificial intelligence  aaai      pp         
gardiol  n  h     kaelbling  l  p          envelope based planning relational mdps 
advances neural information processing systems    nips    
   

fiproximity based non uniform abstractions planning

gardiol  n  h     kaelbling  l  p          adaptive envelope mdps relational equivalence based planning  tech  rep  mit csail tr           computer science
artificial intelligence laboratory  massachusetts institute technology 
goldman  r  p   musliner  d  j   boddy  m  s   durfee  e  h     wu  j          unrolling
complex task models mdps  proceedings      aaai spring symposium
game theoretic decision theoretic agents 
goldman  r  p   musliner  d  j   krebsbach  k  d     boddy  m  s          dynamic
abstraction planning  kuipers  b     webber  b   eds    proceedings   th
national conference artificial intelligence  th innovative applications artificial intelligence conference  aaai iaai      pp         
guestrin  c   koller  d   parr  r     venkataraman  s          efficient solution algorithms
factored mdps  journal artificial intelligence research             
hansen  e  a     zilberstein  s          lao   heuristic search algorithm finds
solutions loops  artificial intelligence                 
hauskrecht  m   meuleau  n   kaelbling  l  p   dean  t     boutilier  c          hierarchical
solution markov decision processes using macro actions  cooper  g     moral 
s   eds    proceedings   th annual conference uncertainty artificial
intelligence  uai      pp         
hoey  j   st  aubin  r   hu  a     boutilier  c          spudd  stochastic planning using
decision diagrams  proceedings   th annual conference uncertainty
artificial intelligence  uai      pp         
howard  r  a          dynamic programming markov processes  mit press 
kim  k  e          representations algorithms large stochastic planning problems 
ph d  thesis  deptartment computer science  brown university 
kirman  j          predicting real time planner performance domain characterization 
ph d  thesis  department computer science  brown university 
kolobov  a   mausam    weld  d  s          regressing deterministic plans mdp
function approximation  workshop reality check planning scheduling
uncertainty icaps 
korf  r          macro operators  weak method learning  artificial intelligence         
     
littman  m   weissman  d     bonet  b          tireworld domain  fifth international
planning competition  ipc    hosted international conference automated
planning scheduling  icaps       
munos  r     moore  a          variable resolution discretization high accuracy solutions optimal control problems  dean  t   ed    proceedings   th international joint conference artificial intelligence  ijcai      pp           
musliner  d  j   durfee  e  h     shin  k  g          world modeling dynamic
construction real time plans  artificial intelligence            
   

fibaum  nicholson   dix

nicholson  a  e     kaelbling  l  p          toward approximate planning large
stochastic domains  proceedings aaai spring symposium decision theoretic planning  pp         
parr  r          unifying framework temporal abstraction stochastic processes 
proceedings symposium abstraction reformulation approximation
 sara      pp        
puterman  m  l     shin  m  c          modified policy iteration algorithms discounted
markov decision processes  management science               
reyes  a   sucar  l  e     morales  e  f          asisto  qualitative mdp based recommender system power plant operation  computacion sistemas              
sanner  s     boutilier  c          practical solution techniques first order mdps 
artificial intelligence                    advances automated plan generation 
srivastava  s   immerman  n     zilberstein  s          abstract planning unknown
object quantities properties  proceedings eighth symposium abstraction  reformulation approximation  sara      pp         
st aubin  r   hoey  j     boutilier  c          apricodd  approximate policy construction using decision diagrams  proceedings conference neural information
processing systems  pp           
steinkraus  k  a          solving large stochastic planning problems using multiple dynamic abstractions  ph d  thesis  department electrical engineering computer
science  massachusetts institute technology 

   


