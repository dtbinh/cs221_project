journal of artificial intelligence research                  

submitted        published      

online planning algorithms for pomdps
stephane ross
joelle pineau

stephane ross mail mcgill ca
jpineau cs mcgill ca

school of computer science
mcgill university  montreal  canada  h a  a 

sebastien paquet
brahim chaib draa

spaquet damas ift ulaval ca
chaib damas ift ulaval ca

department of computer science and software engineering
laval university  quebec  canada  g k  p 

abstract
partially observable markov decision processes  pomdps  provide a rich framework
for sequential decision making under uncertainty in stochastic domains  however  solving
a pomdp is often intractable except for small problems due to their complexity  here 
we focus on online approaches that alleviate the computational complexity by computing
good local policies at each decision step during the execution  online algorithms generally consist of a lookahead search to find the best action to execute at each time step in
an environment  our objectives here are to survey the various existing online pomdp
methods  analyze their properties and discuss their advantages and disadvantages  and to
thoroughly evaluate these online approaches in different environments under various metrics  return  error bound reduction  lower bound improvement   our experimental results
indicate that state of the art online heuristic search methods can handle large pomdp
domains efficiently 

   introduction
the partially observable markov decision process  pomdp  is a general model for sequential decision problems in partially observable environments  many planning and control problems can be modeled as pomdps  but very few can be solved exactly because of
their computational complexity  finite horizon pomdps are pspace complete  papadimitriou   tsitsiklis        and infinite horizon pomdps are undecidable  madani  hanks 
  condon        
in the last few years  pomdps have generated significant interest in the ai community and many approximation algorithms have been developed  hauskrecht        pineau 
gordon    thrun        braziunas   boutilier        poupart        smith   simmons 
      spaan   vlassis         all these methods are offline algorithms  meaning that they
specify  prior to the execution  the best action to execute for all possible situations  while
these approximate algorithms can achieve very good performance  they often take significant time  e g  more than an hour  to solve large problems  where there are too many
possible situations to enumerate  let alone plan for   furthermore  small changes in the
environments dynamics require recomputing the full policy  which may take hours or days 
c
    
ai access foundation  all rights reserved 

fiross  pineau  paquet    chaib draa

on the other hand  online approaches  satia   lave        washington        barto 
bradtke    singhe        paquet  tobin    chaib draa        mcallester   singh       
bertsekas   castanon        shani  brafman    shimony        try to circumvent the complexity of computing a policy by planning online only for the current information state  online algorithms are sometimes also called agent centered search algorithms  koenig        
whereas an offline search would compute an exponentially large contingency plan considering all possible happenings  an online search only considers the current situation and a small
horizon of contingency plans  moreover  some of these approaches can handle environment
changes without requiring more computation  which allows online approaches to be applicable in many contexts where offline approaches are not applicable  for instance  when the
task to accomplish  as defined by the reward function  changes regularly in the environment 
one drawback of online planning is that it generally needs to meet real time constraints 
thus greatly reducing the available planning time  compared to offline approaches 
recent developments in online pomdp search algorithms  paquet  chaib draa    ross 
      ross   chaib draa        ross  pineau    chaib draa        suggest that combining
approximate offline and online solving approaches may be the most efficient way to tackle
large pomdps  in fact  we can generally compute a very rough policy offline using existing
offline value iteration algorithms  and then use this approximation as a heuristic function
to guide the online search algorithm  this combination enables online search algorithms
to plan on shorter horizons  thereby respecting online real time constraints and retaining a
good precision  by doing an exact online search over a fixed horizon  we can guarantee a
reduction in the error of the approximate offline value function  the overall time  offline
and online  required to obtain a good policy can be dramatically reduced by combining
both approaches 
the main purpose of this paper is to draw the attention of the ai community to online
methods as a viable alternative for solving large pomdp problems  in support of this  we
first survey the various existing online approaches that have been applied to pomdps  and
discuss their strengths and drawbacks  we present various combinations of online algorithms
with various existing offline algorithms  such as qmdp  littman  cassandra    kaelbling 
       fib  hauskrecht         blind  hauskrecht        smith   simmons        and
pbvi  pineau et al          we then compare empirically different online approaches in
two large pomdp domains according to different metrics  average discounted return  error
bound reduction  lower bound improvement   we also evaluate how the available online
planning time and offline planning time affect the performance of different algorithms  the
results of our experiments show that many state of the art online heuristic search methods
are tractable in large state and observation spaces  and achieve the solution quality of stateof the art offline approaches at a fraction of the computational cost  the best methods
achieve this by focusing the search on the most relevant future outcomes for the current
decision  e g  those that are more likely and that have high uncertainty  error  on their longterm values  such as to minimize as quickly as possible an error bound on the performance of
the best action found  the tradeoff between solution quality and computing time offered by
the combinations of online and offline approaches is very attractive for tackling increasingly
large domains 
   

fionline planning algorithms for pomdps

   pomdp model
partially observable markov decision processes  pomdps  provide a general framework
for acting in partially observable environments  astrom        smallwood   sondik       
monahan        kaelbling  littman    cassandra         a pomdp is a generalization
of the mdp model for planning under uncertainty  which gives the agent the ability to
effectively estimate the outcome of its actions even when it cannot exactly observe the state
of its environment 
formally  a pomdp is represented as a tuple  s  a  t  r  z  o  where 
 s is the set of all the environment states  a state is a description of the environment
at a specific moment and it should capture all information relevant to the agents
decision making process 
 a is the set of all possible actions 
 t   s  a  s         is the transition function  where t  s  a  s      pr s   s  a 
represents the probability of ending in state s  if the agent performs action a in state
s 
 r   s  a  r is the reward function  where r s  a  is the reward obtained by
executing action a in state s 
 z is the set of all possible observations 
 o   s  a  z         is the observation function  where o s    a  z    pr z a  s    gives
the probability of observing z if action a is performed and the resulting state is s   
we assume in this paper that s  a and z are all finite and that r is bounded 
a key aspect of the pomdp model is the assumption that the states are not directly
observable  instead  at any given time  the agent only has access to some observation
z  z that gives incomplete information about the current state  since the states are not
observable  the agent cannot choose its actions based on the states  it has to consider
a complete history of its past actions and observations to choose its current action  the
history at time t is defined as 
ht    a    z            zt    at    zt   

   

this explicit representation of the past is typically memory expensive  instead  it is
possible to summarize all relevant information from previous actions and observations in a
probability distribution over the state space s  which is called a belief state  astrom        
a belief state at time t is defined as the posterior probability distribution of being in each
state  given the complete history 
bt  s    pr st   s ht   b    

   

the belief state bt is a sufficient statistic for the history ht  smallwood   sondik        
therefore the agent can choose its actions based on the current belief state bt instead of
all past actions and observations  initially  the agent starts with an initial belief state b   
   

fiross  pineau  paquet    chaib draa

representing its knowledge about the starting state of the environment  then  at any time
t  the belief state bt can be computed from the previous belief state bt    using the previous
action at  and the current observation zt   this is done with the belief state update function
  b  a  z   where bt     bt    at    zt   is defined by the following equation 
bt  s        bt    at    zt   s     

 
pr zt  bt    at   

o s    at    zt  

x

t  s  at    s   bt   s      

ss

where pr z b  a   the probability of observing z after doing action a in belief b  acts as a
normalizing constant such that bt remains a probability distribution 
pr z b  a   

x

o s    a  z 

s  s

x

t  s  a  s   b s  

   

ss

now that the agent has a way of computing its belief  the next interesting question is
how to choose an action based on this belief state 
this action is determined by the agents policy   specifying the probability that the
agent will execute any action in any given belief state  i e   defines the agents strategy
for all possible situations it could encounter  this strategy should maximize the amount of
reward earned over a finite or infinite time horizon  in this article  we restrict our attention
to infinite horizon pomdps where the optimality criterion is to maximize the expected
sum of discounted rewards  also called the return or discounted return   more formally  the
optimal policy   can be defined by the following equation 
 
 
x
x x
   
bt  s 
r s  a  bt   a   b   
    argmax e
t


t  

ss

aa

where          is the discount factor and  bt   a  is the probability that action a will be
performed in belief bt   as prescribed by the policy  
the return obtained by following a specific policy   from a certain belief state b  is
defined by the value function equation v   
 
 
x
x


v  b   
 b  a  rb  b  a    
pr z b  a v    b  a  z    
   
aa

zz

here the function rb  b  a  specifies the immediate expected reward of executing action a
in belief b according to the reward function r 
rb  b  a   

x

b s r s  a  

   

ss

the sum over z in equation   is interpreted as the expected future return over the infinite
horizon of executing action a  assuming the policy  is followed afterwards 
note that with the definitions of rb  b  a   pr z b  a  and   b  a  z   one can view a
pomdp as an mdp over belief states  called the belief mdp   where pr z b  a  specifies
the probability of moving from b to   b  a  z  by doing action a  and rb  b  a  is the immediate
reward obtained by doing action a in b 
   

fionline planning algorithms for pomdps

the optimal policy   defined in equation   represents the action selection strategy
that will maximize equation v   b     since there always exists a deterministic policy that
maximizes v  for any belief states  sondik         we will generally only consider deterministic policies  i e  those that assign a probability of   to a specific action in every belief
state  
the value function v  of the optimal policy   is the fixed point of bellmans equation
 bellman        
 
 
x
v   b    max rb  b  a    
pr z b  a v     b  a  z    
   
aa

zz

another useful quantity is the value of executing a given action a in a belief state b 
which is denoted by the q value 
q  b  a    rb  b  a    

x

pr z b  a v     b  a  z   

   

zz

here the only difference with the definition of v  is that the max operator is omitted  notice
that q  b  a  determines the value of a by assuming that the optimal policy is followed at
every step after action a 
we now review different offline methods for solving pomdps  these are used to guide
some of the online heuristic search methods discussed later  and in some cases they form
the basis of other online solutions 
    optimal value function algorithm
one can solve optimally a pomdp for a specified finite horizon h by using the value
iteration algorithm  sondik         this algorithm uses dynamic programming to compute
increasingly more accurate values for each belief state b  the value iteration algorithm
begins by evaluating the value of a belief state over the immediate horizon t      formally 
let v be a value function that takes a belief state as parameter and returns a numerical
value in r of this belief state  the initial value function is 
v   b    max rb  b  a  
aa

    

the value function at horizon t is constructed from the value function at horizon t    by
using the following recursive equation 
 
 
x
vt  b    max rb  b  a    
pr z b  a vt     b  a  z    
    
aa

zz

the value function in equation    defines the discounted sum of expected rewards that the
agent can receive in the next t time steps  for any belief state b  therefore  the optimal
policy for a finite horizon t is simply to choose the action maximizing vt  b  
 
 
x
pr z b  a vt     b  a  z    
    
t  b    argmax rb  b  a    
aa

zz

   

fiross  pineau  paquet    chaib draa

this last equation associates an action to a specific belief state  and therefore must be
computed for all possible belief states in order to define a full policy 
a key result by smallwood and sondik        shows that the optimal value function for
a finite horizon pomdp can be represented by hyperplanes  and is therefore convex and
piecewise linear  it means that the value function vt at any horizon t can be represented by
a set of  s  dimensional hyperplanes  t                    m    these hyperplanes are often
called  vectors  each defines a linear value function over the belief state space associated
with some action a  a  the value of a belief state is the maximum value returned by one
of the  vectors for this belief state  the best action is the one associated with the  vector
that returned the best value 
x
 s b s  
    
vt  b    max
t

ss

a number of exact value function algorithms leveraging the piecewise linear and convex
aspects of the value function have been proposed in the pomdp literature  sondik       
monahan        littman        cassandra  littman    zhang        zhang   zhang 
       the problem with most of these exact approaches is that the number of  vectors
needed to represent the value function grows exponentially in the number of observations
at each iteration  i e  the size of the set t is in o  a  t    z     since each new  vector
requires computation time in o  z  s      the resulting complexity of iteration t for exact
approaches is in o  a  z  s    t    z     most of the work on exact approaches has focused
on finding efficient ways to prune the set t   such as to effectively reduce computation 
    offline approximate algorithms
due to the high complexity of exact solving approaches  many researchers have worked
on improving the applicability of pomdp approaches by developing approximate offline
approaches that can be applied to larger problems 
in the online methods we review below  approximate offline algorithms are often used
to compute lower and upper bounds on the optimal value function  these bounds are
leveraged to orient the search in promising directions  to apply branch and bound pruning
techniques  and to estimate the long term reward of belief states  as we will show in section
   however  we will generally want to use approximate methods which require very low
computational cost  we will be particularly interested in approximations that use the
underlying mdp  to compute lower bounds  blind policy  and upper bounds  mdp  qmdp 
fib  on the exact value function  we also investigate the usefulness of using more precise
lower bounds provided by point based methods  we now briefly review the offline methods
which will be featured in our empirical investigation  some recent publications provide a
more comprehensive overview of offline approximate algorithms  hauskrecht        pineau 
gordon    thrun        
      blind policy
a blind policy  hauskrecht        smith   simmons        is a policy where the same
action is always executed  regardless of the belief state  the value function of any blind
   the mdp defined by the  s  a  t  r  components of the pomdp model 

   

fionline planning algorithms for pomdps

policy is obviously a lower bound on v  since it corresponds to the value of one specific
policy that the agent could execute in the environment  the resulting value function is
specified by a set of  a   vectors  where each  vector specifies the long term expected
reward of following its corresponding blind policy  these  vectors can be computed using
a simple update rule 
at    s    r s  a    

x

t  s  a  s   at  s  

    

s  s

where a    minss r s  a       once these  vectors are computed  we use equation   
to obtain the lower bound on the value of a belief state  the complexity of each iteration
is in o  a  s      which is far less than exact methods  while this lower bound can be
computed very quickly  it is usually not very tight and thus not very informative 
      point based algorithms
to obtain tighter lower bounds  one can use point based methods  lovejoy        hauskrecht 
      pineau et al          this popular approach approximates the value function by updating it only for some selected belief states  these point based methods sample belief
states by simulating some random interactions of the agent with the pomdp environment 
and then update the value function and its gradient over those sampled beliefs  these approaches circumvent the complexity of exact approaches by sampling a small set of beliefs
and maintaining at most one  vector per sampled belief state  let b represent the set of
sampled beliefs  then the set t of  vectors at time t is obtained as follows 
a  s 
a z
t
bt
t

 
 
 
 

r s  a  
p
a z
    a  z    s        
 a z
 s  s t  s  a  s   o s
t    
i
i
i  i  s    p
p
a
a
a
a z
 b  b      zz argmaxt
ss  s b s   a  a  
p
 b  b   argmaxb ss b s  s   b  b  

    

t

to ensure that this gives a lower bound    is initialized with a single  vector    s   

mins  s aa r s   a 
 
 

since  t      b   each iteration has a complexity in o  a  z  s  b   s  
 b     which is polynomial time  compared to exponential time for exact approaches 
different algorithms have been developed using the point based approach  pbvi  pineau
et al          perseus  spaan   vlassis         hsvi  smith   simmons             
are some of the most recent methods  these methods differ slightly in how they choose
belief states and how they update the value function at these chosen belief states  the
nice property of these approaches is that one can tradeoff between the complexity of the
algorithm and the precision of the lower bound by increasing  or decreasing  the number of
sampled belief points 
      mdp
the mdp approximation consists in approximating the value function v  of the pomdp
by the value function of its underlying mdp  littman et al          this value function is
an upper bound on the value function of the pomdp and can be computed using bellmans
equation 
   

fiross  pineau  paquet    chaib draa

 

m dp
vt  
 s    max r s  a    
aa

x

 

t  s  a  s   vtm dp  s     

s  s

    

p
the value v  b  of a belief state b is then computed as v  b    ss v m dp  s b s   this
can be computed very quickly  as each iteration of equation    can be done in o  a  s     
      qmdp
the qmdp approximation is a slight variation of the mdp approximation  littman et al  
       the main idea behind qmdp is to consider that all partial observability disappear
after a single step  it assumes the mdp solution is computed to generate vtm dp  equation
     given this  we define 
dp
qm
t    s  a    r s  a    

x

t  s  a  s   vtm dp  s    

    

s  s

this approximation defines an  vector for each action  and gives an upper bound on v 
that is tighter than v m dp   i e  vtqm dp  b   vtm dp  b  for all belief b   again  to
obtain the value of a belief state  we use equation     where t will contain one  vector
dp  s  a  for each a  a 
a  s    qm
t
      fib
the two upper bounds presented so far  qmdp and mdp  do not take into account the
partial observability of the environment  in particular  information gathering actions that
may help identify the current state are always suboptimal according to these bounds  to
address this problem  hauskrecht        proposed a new method to compute upper bounds 
called the fast informed bound  fib   which is able to take into account  to some degree 
the partial observability of the environment  the  vector update process is described as
follows 
at    s    r s  a    

x

zz

a 

max

t t

x

o s    a  z t  s  a  s   t  s    

    

s  s

can be initialized to the  vectors found by qmdp at convergence  i e 
the  vectors
a   s    qm dp  s  a   fib defines a single  vector for each action and the value of a belief
state is computed according to equation     fib provides a tighter upper bound than
qmdp   i e  vtf ib  b   vtqm dp  b  for all b    the complexity of the algorithm remains
acceptable  as each iteration requires o  a    s    z   operations 

   online algorithms for pomdps
with offline approaches  the algorithm returns a policy defining which action to execute in
every possible belief state  such approaches tend to be applicable only when dealing with
small to mid size domains  since the policy construction step takes significant time  in large
pomdps  using a very rough value function approximation  such as the ones presented
in section      tends to substantially hinder the performance of the resulting approximate
   

fionline planning algorithms for pomdps

offline approaches
policy construction

policy execution

online approaches

small policy construction step between policy execution steps

figure    comparison between offline and online approaches 
policy  even more recent point based methods produce solutions of limited quality in very
large domains  paquet et al         
hence in large pomdps  a potentially better alternative is to use an online approach 
which only tries to find a good local policy for the current belief state of the agent  the
advantage of such an approach is that it only needs to consider belief states that are reachable from the current belief state  this focuses computation on a small set of beliefs  in
addition  since online planning is done at every step  and thus generalization between beliefs
is not required   it is sufficient to calculate only the maximal value for the current belief
state  not the full optimal  vector  in this setting  the policy construction steps and the
execution steps are interleaved with one another as shown in figure    in some cases  online
approaches may require a few extra execution steps  and online planning   since the policy is
locally constructed and therefore not always optimal  however the policy construction time
is often substantially shorter  consequently  the overall time for the policy construction
and execution is normally less for online approaches  koenig         in practice  a potential
limitation of online planning is when we need to meet short real time constraints  in such
case  the time available to construct the plan is very small compared to offline algorithms 
    general framework for online planning
this subsection presents a general framework for online planning algorithms in pomdps 
subsequently  we discuss specific approaches from the literature and describe how they vary
in tackling various aspects of this general framework 
an online algorithm is divided into a planning phase  and an execution phase  which
are applied alternately at each time step 
in the planning phase  the algorithm is given the current belief state of the agent and
computes the best action to execute in that belief  this is usually achieved in two steps 
first a tree of reachable belief states from the current belief state is built by looking at
several possible sequences of actions and observations that can be taken from the current
belief  in this tree  the current belief is the root node and subsequent reachable beliefs  as
calculated by the   b  a  z  function of equation    are added to the tree as child nodes of
their immediate previous belief  belief nodes are represented using or nodes  at which we
must choose an action  and actions are included in between each layer of belief nodes using
and nodes  at which we must consider all possible observations that lead to subsequent
   

fiross  pineau  paquet    chaib draa

b 

            

 

 

            

          

a 

a 

   

z 
            

b 

        

b 

z 
        b
 

b 

   

z 
       

b 

        

 

a 
   

z 

z 

  

           

   

   

a 
   

   

z 
b 

       

z 

            
   

        

z 
b 

b 

        

figure    an and or tree constructed by the search process for a pomdp with   actions and   observations  the belief states or nodes are represented by triangular nodes and the action and nodes
by circular nodes  the rewards rb  b  a  are represented by values on the outgoing arcs from
or nodes and probabilities pr z b  a  are shown on the outgoing arcs from and nodes  the
values inside brackets represent the lower and upper bounds that were computed according to
equations          assuming a discount factor          also notice in this example that the
action a  in belief state b  could be pruned since its upper bound          is lower than the
lower bound          of action a  in b   

beliefs   then the value of the current belief is estimated by propagating value estimates
up from the fringe nodes  to their ancestors  all the way to the root  according to bellmans
equation  equation     the long term value of belief nodes at the fringe is usually estimated
using an approximate value function computed offline  some methods also maintain both
a lower bound and an upper bound on the value of each node  an example on how such a
tree is contructed and evaluated is presented in figure   
once the planning phase terminates  the execution phase proceeds by executing the best
action found for the current belief in the environment  and updating the current belief and
tree according to the observation obtained 
notice that in general  the belief mdp could have a graph structure with cycles  most
online algorithms handle such a structure by unrolling the graph into a tree  hence  if they
reach a belief that is already elsewhere in the tree  it is duplicated  these algorithms could
always be modified to handle generic graph structures by using a technique proposed in the
lao  algorithm  hansen   zilberstein        to handle cycles  however there are some
advantages and disadvantages to doing this  a more in depth discussion of this issue is
presented in section     
a generic online algorithm implementing the planning phase  lines      and the execution
phase  lines        is presented in algorithm      the algorithm first initializes the tree to
contain only the initial belief state  line     then given the current tree  the planning phase
of the algorithm proceeds by first selecting the next fringe node  line    under which it should
pursue the search  construction of the tree   the expand function  line    constructs the
   

fionline planning algorithms for pomdps

   function onlinepomdpsolver  
static  bc   the current belief state of the agent 
t   an and or tree representing the current search tree 
d  expansion depth 
l  a lower bound on v   
u   an upper bound on v   

  
  
  
  
  
  
  
  
   
   
   
   
   

bc  b 
initialize t to contain only bc at the root
while not executionterminated   do
while not planningterminated   do
b  choosenextnodetoexpand  
expand b   d 
updateancestors b  
end while
execute best action a for bc
perceive a new observation z
bc    bc   a  z 
update tree t so that bc is the new root
end while

algorithm      generic online algorithm 
next reachable beliefs  using equation    under the selected leaf for some pre determined
expansion depth d and evaluates the approximate value function for all newly created
nodes  the new approximate value of the expanded node is propagated to its ancestors
via the updateancestors function  line     this planning phase is conducted until some
terminating condition is met  e g  no more planning time is available or an  optimal action
is found  
the execution phase of the algorithm executes the best action a found during planning
 line     and gets a new observation from the environment  line      next  the algorithm
updates the current belief state and the search tree t according to the most recent action a
and observation z  lines         some online approaches can reuse previous computations
by keeping the subtree under the new belief and resuming the search from this subtree at
the next time step  in such cases  the algorithm keeps all the nodes in the tree t under the
new belief bc and deletes all other nodes from the tree  then the algorithm loops back to
the planning phase for the next time step  and so on until the task is terminated 
as a side note  an online planning algorithm can also be useful to improve the precision
of an approximate value function computed offline  this is captured in theorem     
theorem       puterman        hauskrecht        let v be an approximate value function and    supb  v   b   v  b    then the approximate value v d  b  returned by a dstep lookahead from belief b  using v to estimate fringe node values  has error bounded by
 v   b   v d  b     d  
we notice that for           the error converges to   as the depth d of the search tends to
  this indicates that an online algorithm can effectively improve the performance obtained
by an approximate value function computed offline  and can find an action arbitrarily close
to the optimal for the current belief  however  evaluating the tree of all reachable beliefs
within depth d has a complexity in o   a  z  d  s      which is exponential in d  this
becomes quickly intractable for large d  furthermore  the planning time available during
the execution may be very short and exploring all beliefs up to depth d may be infeasible 
   

fiross  pineau  paquet    chaib draa

hence this motivates the need for more efficient online algorithms that can guarantee similar
or better error bounds 
to be more efficient  most of the online algorithms focus on limiting the number of reachable beliefs explored in the tree  or choose only the most relevant ones   these approaches
generally differ only in how the subroutines choosenextnodetoexpand and expand
are implemented  we classify these approaches into three categories   branch and bound
pruning  monte carlo sampling and heuristic search  we now present a survey of these
approaches and discuss their strengths and drawbacks  a few other online algorithms do
not proceed via tree search  these approaches are discussed in section     
    branch and bound pruning
branch and bound pruning is a general search technique used to prune nodes that are
known to be suboptimal in the search tree  thus preventing the expansion of unnecessary
lower nodes  to achieve this in the and or tree  a lower bound and an upper bound
are maintained on the value q  b  a  of each action a  for every belief b in the tree  these
bounds are computed by first evaluating the lower and upper bound for the fringe nodes
of the tree  these bounds are then propagated to parent nodes according to the following
equations 

l b  
if b  f t  
lt  b   
    
maxaa lt  b  a   otherwise
x
lt  b  a    rb  b  a    
pr z b  a lt    b  a  z   
    
zz



u  b  
if b  f t  
maxaa ut  b  a   otherwise
x
ut  b  a    rb  b  a    
pr z b  a ut    b  a  z   
ut  b   

    
    

zz

where f t   denotes the set of fringe nodes in tree t   ut  b  and lt  b  represent the upper
and lower bounds on v   b  associated to belief state b in the tree t   ut  b  a  and lt  b  a 
represent corresponding bounds on q  b  a   and l b  and u  b  are the bounds used at fringe
nodes  typically computed offline  these equations are equivalent to bellmans equation
 equation     however they use the lower and upper bounds of the children  instead of v   
several techniques presented in section     can be used to quickly compute lower bounds
 blind policy  and upper bounds  mdp  qmdp  fib  offline 
given these bounds  the idea behind branch and bound pruning is relatively simple  if a
given action a in a belief b has an upper bound ut  b  a  that is lower than another action as
lower bound lt  b  a   then we know that a is guaranteed to have a value q  b  a   q  b  a  
thus a is suboptimal in belief b  hence that branch can be pruned and no belief reached
by taking action a in b will be considered 
      rtbss
the real time belief space search  rtbss  algorithm uses a branch and bound approach
to compute the best action to take in the current belief  paquet et al                starting
   

fionline planning algorithms for pomdps

   function expand b  d 
inputs  b 
d 
static  t  
l 
u 

  
  
  
  
  
  
  
  
   
   
   
   
   

the belief node we want to expand 
the depth of expansion under b 
an and or tree representing the current search tree 
a lower bound on v   
an upper bound on v   

if d     then
lt  b   l b 
else
sort actions  a    a            a a    such that u  b  ai    u  b  aj   if i  j
i 
lt  b   
while i   a  and u  b  ai    
plt  b  do
lt  b  ai    rb  b  ai      zz pr z b  ai  expand   b  ai   z   d    
lt  b   max lt  b   lt  b  ai   
ii  
end while
end if
return lt  b 

algorithm      expand subroutine of rtbss 
from the current belief  it expands the and or tree in a depth first search fashion  up to
some pre determined search depth d  the leaves of the tree are evaluated by using a lower
bound computed offline  which is propagated upwards such that a lower bound is maintained
for each node in the tree 
to limit the number of nodes explored  branch and bound pruning is used along the way
to prune actions that are known to be suboptimal  thus excluding unnecessary nodes under
these actions  to maximize pruning  rtbss expands the actions in descending order of their
upper bound  first action expanded is the one with highest upper bound   by expanding
the actions in this order  one never expands an action that could have been pruned if actions
had been expanded in a different order  intuitively  if an action has a higher upper bound
than the other actions  then it cannot be pruned by the other actions since their lower
bound will never exceed their upper bound  another advantage of expanding actions in
descending order of their upper bound is that as soon as we find an action that can be
pruned  then we also know that all remaining actions can be pruned  since their upper
bounds are necessarily lower  the fact that rtbss proceeds via a depth first search also
increases the number of actions that can be pruned since the bounds on expanded actions
become more precise due to the search depth 
in terms of the framework in algorithm      rtbss requires the choosenextnodetoexpand subroutine to simply return the current belief bc   the updateancestors function does not need to perform any operation since bc has no ancestor  root of the tree
t    the expand subroutine proceeds via depth first search up to a fixed depth d  using
branch and bound pruning  as mentioned above  this subroutine is detailed in algorithm
     after this expansion is performed  planningterminated evaluates to true and the
best action found is executed  at the end of each time step  the tree t is simply reinitialized
to contain the new current belief at the root node 
the efficiency of rtbss depends largely on the precision of the lower and upper bounds
computed offline  when the bounds are tight  more pruning will be possible  and the search
will be more efficient  if the algorithm is unable to prune many actions  searching will
   

fiross  pineau  paquet    chaib draa

be limited to short horizons in order to meet real time constraints  another drawback of
rtbss is that it explores all observations equally  this is inefficient since the algorithm
could explore parts of the tree that have a small probability of occurring and thus have a
small effect on the value function  as a result  when the number of observations is large 
the algorithm is limited to exploring over a short horizon 
as a final note  since rtbss explores all reacheable beliefs within depth d  except some
reached by suboptimal actions   then it can guarantee the same error bound as a d step
lookahead  see theorem       therefore  the online search directly improves the precision of
the original  offline  value bounds by a factor  d   this aspect was confirmed empirically in
different domains where the rtbss authors combined their online search with bounds given
by various offline algorithms  in some cases  their results showed a tremendous improvement
of the policy given by the offline algorithm  paquet et al         
    monte carlo sampling
as mentioned above  expanding the search tree fully over a large set of observations is
infeasible except for shallow depths  in such cases  a better alternative may be to sample a
subset of observations at each expansion and only consider beliefs reached by these sampled
observations  this reduces the branching factor of the search and allows for deeper search
within a set planning time  this is the strategy employed by monte carlo algorithms 
      mcallester and singh
the approach presented by mcallester and singh        is an adaptation of the online mdp
algorithm presented by kearns  mansour  and ng         it consists of a depth limited
search of the and or tree up to a certain fixed horizon d where instead of exploring all
observations at each action choice  c observations are sampled from a generative model  the
probabilities pr z b  a  are then approximated using the observed frequencies in the sample 
the advantage of such an approach is that sampling an observation from the distribution
pr z b  a  can be achieved very efficiently in o log  s    log  z    while computing the exact
probabilities pr z b  a  is in o  s     for each observation z  thus sampling can be useful to
alleviate the complexity of computing pr z b  a   at the expense of a less precise estimate 
nevertheless  a few samples is often sufficient to obtain a good estimate as the observations
that have the most effect on q  b  a   i e  those which occur with high probability  are
more likely to be sampled  the authors also apply belief state factorization as in boyen
and koller        to simplify the belief state calculations 
for the implementation of this algorithm  the expand subroutine expands the tree up
to fixed depth d  using monte carlo sampling of observations  as mentioned above  see
algorithm       at the end of each time step  the tree t is reinitialized to contain only the
new current belief at the root 
kearns et al         derive bounds on the depth d and the number of samples c needed
to obtain an  optimal policy with high probability and show that the number of samples
required grows exponentially in the desired accuracy  in practice  the number of samples
required is infeasible given realistic online time constraints  however  performance in terms
of returns is usually good even with many fewer samples 
   

fionline planning algorithms for pomdps

   function expand b  d 
inputs  b 
d 
static  t  
c 

  
  
  
  
  
  
  
  
   
   
   

the belief node we want to expand 
the depth of expansion under b 
an and or tree representing the current search tree 
the number of observations to sample 

if d     then
lt  b   maxaa rb  b  a 
else
lt  b   
for all a  a do
sample z    z    z          zc   from distribution pr z b  a 
p
n  z 
lt  b  a   rb  b  a     zz nz  z    zc expand   b  a  z   d    
lt  b   max lt  b   lt  b  a  
end for
end if
return lt  b 

algorithm      expand subroutine of mcallester and singhs algorithm 

one inconvenience of this method is that no action pruning can be done since monte
carlo estimation is not guaranteed to correctly propagate the lower  and upper  bound
property up the tree  in their article  the authors simply approximate the value of the
fringe belief states by the immediate reward rb  b  a   this could be improved by using any
good estimate of v  computed offline  note also that this approach may be difficult to
apply in domains where the number of actions  a  is large  of course this may further
impact performance 
      rollout
another similar online monte carlo approach is the rollout algorithm  bertsekas   castanon         the algorithm requires an initial policy  possibly computed offline   at each
time step  it estimates the future expected value of each action  assuming the initial policy is followed at future time steps  and executes the action with highest estimated value 
these estimates are obtained by computing the average discounted return obtained over a
set of m sampled trajectories of depth d  these trajectories are generated by first taking
the action to be evaluated  and then following the initial policy in subsequent belief states 
assuming the observations are sampled from a generative model  since this approach only
needs to consider different actions at the root belief node  the number of actions  a  only
influences the branching factor at the first level of the tree  consequently  it is generally
more scalable than mcallester and singhs approach  bertsekas and castanon        also
show that with enough sampling  the resulting policy is guaranteed to perform at least as
well as the initial policy with high probability  however  it generally requires many sampled
trajectories to provide substantial improvement over the initial policy  furthermore  the
initial policy has a significant impact on the performance of this approach  in particular  in
some cases it might be impossible to improve the return of the initial policy by just changing
the immediate action  e g  if several steps need to be changed to reach a specific subgoal to
which higher rewards are associated   in those cases  the rollout policy will never improve
over the initial policy 
   

fiross  pineau  paquet    chaib draa

   function expand b  d 
inputs  b  the belief node we want to expand 
d  the depth of expansion under b 
static  t   an and or tree representing the current search tree 
  a set of initial policies 
m   the number of trajectories of depth d to sample 

   lt  b   
   for all a  a do
   for all    do
  
q  b  a    
  
for i     to m do
  
b  b
  
a  a
  
for j     to d do
  j
   
q  b  a   q  b  a    m
 rb  b  a 
   
z  sampleobservation b  a 
   
b    b  a  z 
   
a   b 
   
end for
   
end for
    end for
    lt  b  a    max q  b  a 
    end for

algorithm      expand subroutine of the parallel rollout algorithm 
to address this issue relative to the initial policy  chang  givan  and chong       
introduced a modified version of the algorithm  called parallel rollout  in this case  the
algorithm starts with a set of initial policies  then the algorithm proceeds as rollout for
each of the initial policies in the set  the value considered for the immediate action is the
maximum over that set of initial policies  and the action with highest value is executed 
in this algorithm  the policy obtained is guaranteed to perform at least as well as the best
initial policy with high probability  given enough samples  parallel rollout can handle
domains with a large number of actions and observations  and will perform well when the
set of initial policies contain policies that are good in different regions of the belief space 
the expand subroutine of the parallel rollout algorithm is presented in algorithm     
the original rollout algorithm by bertsekas and castanon        is the same algorithm
in the special case where the set of initial policies  contains only one policy  the other
subroutines proceed as in mcallester and singhs algorithm 
    heuristic search
instead of using branch and bound pruning or monte carlo sampling to reduce the branching factor of the search  heuristic search algorithms try to focus the search on the most relevant reachable beliefs by using heuristics to select the best fringe beliefs node to expand 
the most relevant reachable beliefs are the ones that would allow the search algorithm to
make good decisions as quickly as possible  i e  by expanding as few nodes as possible 
there are three different online heuristic search algorithms for pomdps that have been
proposed in the past  satia and lave         bi pomdp  washington        and aems
 ross   chaib draa         these algorithms all maintain both lower and upper bounds
on the value of each node in the tree  using equations          and only differ in the
specific heuristic used to choose the next fringe node to expand in the and or tree  we
   

fionline planning algorithms for pomdps

first present the common subroutines for these algorithms  and then discuss their different
heuristics 
recalling the general framework of algorithm      three steps are interleaved several
times in heuristic search algorithms  first  the best fringe node to expand  according to
the heuristic  in the current search tree t is found  then the tree is expanded under
this node  usually for only one level   finally  ancestor nodes values are updated  their
values must be updated before we choose the next node to expand  since the heuristic
value usually depends on them  in general  heuristic search algorithms are slightly more
computationally expensive than standard depth  or breadth first search algorithms  due to
the extra computations needed to select the best fringe node to expand  and the need to
update ancestors at each iteration  this was not required by the previous methods using
branch and bound pruning and or monte carlo sampling  if the complexity of these extra
steps is too high  then the benefit of expanding only the most relevant nodes might be
outweighed by the lower number of nodes expanded  assuming a fixed planning time  
in heuristic search algorithms  a particular heuristic value is associated with every fringe
node in the tree  this value should indicate how important it is to expand this node in
order to improve the current solution  at each iteration of the algorithm  the goal is to find
the fringe node that maximizes this heuristic value among all fringe nodes  this can be
achieved efficiently by storing in each node of the tree a reference to the best fringe node
to expand within its subtree  as well as its associated heuristic value  in particular  the
root node always contains a reference to the best fringe node for the whole tree  when a
node is expanded  its ancestors are the only nodes in the tree where the best fringe node
reference  and corresponding heuristic value  need to be updated  these can be updated
efficiently by using the references and heuristic values stored in the lower nodes via a
dynamic programming algorithm  described formally in equations    and     here ht  b 
denotes the highest heuristic value among the fringe nodes in the subtree of b  bt  b  is a
reference to this fringe node  ht  b  is the basic heuristic value associated to fringe node b 
and ht  b  a  and ht  b  a  z  are factors that weigh this basic heuristic value at each level
of the tree t   for example  ht  b  a  z  could be pr z b  a  in order to give higher weight to
 and hence favor  fringe nodes that are reached by the most likely observations 

ht  b 
if b  f t  

ht  b   

maxaa ht  b  a ht  b  a  otherwise
    
ht  b  a    maxzz ht  b  a  z ht    b  a  z  

b
if b  f t  

bt  b   
bt  b  atb   otherwise
t   
bt  b  a    bt    b  a  zb a
    
t
ab   argmaxaa ht  b  a ht  b  a 
t
zb a
  argmaxzz ht  b  a  z ht    b  a  z  
this procedure finds the fringe node b  f t   that maximizes the overall heuristic value
qdt  b 
ht  bi   ai  ht  bi   ai   zi    where bi   ai and zi represent the ith belief 
ht  bc   b    ht  b  i  
action and observation on the path from bc to b in t   and dt  b  is the depth of fringe
node b  note that ht and bt are only updated in the ancestor nodes of the last expanded
node  by reusing previously computed values for the other nodes  we have a procedure
   

fiross  pineau  paquet    chaib draa

   function expand b 
inputs  b 
static  bc  
t 
l 
u 

  
  
  
  
  
  
  
  
   
   
   
   
   
   
   
   
   
   
   

an or node we want to expand 
the current belief state of the agent 
an and or tree representing the current search tree 
a lower bound on v   
an upper bound on v   

for each a  a do
for each z  z do
b     b  a  z 
ut  b     u  b   
lt  b     l b   
ht  b     ht  b   
bt  b     b 
end for
p
lt  b  a   rb  b  a     p zz pr z b  a lt    b  a  z  
ut  b  a   rb  b  a     zz pr z b  a ut    b  a  z  

t  argmax
zb a
zz ht  b  a  z ht    b  a  z  

t   

t
ht  b  a    ht  b  a  zb a  ht    b  a  zb a
t


bt  b  a   bt    b  a  zb a   
end for
lt  b   max maxaa lt  b  a   lt  b  
ut  b   min maxaa ut  b  a   ut  b  

at
b  argmaxaa ht  b  a ht  b  a 
  b  at  
 h
ht  b   ht  b  at
b
t
b
bt  b   bt  b  at
b  

algorithm      expand   expand subroutine for heuristic search algorithms 
that can find the best fringe node to expand in the tree in time linear in the depth of the
tree  versus exponential in the depth of the tree for the exhaustive search through all fringe
nodes   these updates are performed in both the expand and the updateancestors
subroutines  each of which is described in more detail below  after each iteration  the
choosenextnodetoexpand subroutine simply returns the reference to this best fringe
node stored in the root of the tree  i e  bt  bc   
the expand subroutine used by heuristic search methods is presented in algorithm     
it performs a one step lookahead under the fringe node b  the main difference with respect
to previous methods in sections     and     is that the heuristic value and best fringe node
to expand for these new nodes are computed at lines     and        the best leaf node
in bs subtree and its heuristic value are then computed according to equations    and   
 lines        
the updateancestors function is presented in algorithm      the goal of this function is to update the bounds of the ancestor nodes  and find the best fringe node to expand
next  starting from a given or node b    the function simply updates recursively the ancestor nodes of b  in a bottom up fashion  using equations       to update the bounds and
equations       to update the reference to the best fringe to expand and its heuristic value 
notice that the updateancestors function can reuse information already stored in the
node objects  such that it does not need to recompute   b  a  z   pr z b  a  and rb  b  a  
however it may need to recompute ht  b  a  z  and ht  b  a  according to the new bounds 
depending on how the heuristic is defined 
due to the anytime nature of these heuristic search algorithms  the search usually keeps
going until an  optimal action is found for the current belief bc   or the available planning
   

fionline planning algorithms for pomdps

   function updateancestors b   

  
  
  
  
  
  
  
  
   
   
   
   
   
   

inputs  b    an or node for which we want to update all its ancestors 
static  bc   the current belief state of the agent 
t   an and or tree representing the current search tree 
l  a lower bound on v   
u   an upper bound on v   
while b     bc do
set  b  a  so that action apin belief b is parent node of belief node b 
lt  b  a   rb  b  a     p zz pr z b  a lt    b  a  z  
ut  b  a   rb  b  a     zz pr z b  a ut    b  a  z  

t  argmax
zb a
zz ht  b  a  z ht    b  a  z  
t  h     b  a  z t   
ht  b  a   ht  b  a  zb a
t
b a
t   
bt  b  a   bt    b  a  zb a
lt  b   maxa  a lt  b  a   
ut  b   maxa  a ut  b  a   
 
 

at
b  argmaxa  a ht  b  a  ht  b  a  
  b  at  
 h
ht  b   ht  b  at
t
b
b
bt  b   bt  b  at
b  
b   b
end while

algorithm      updateancestors   updates the bounds of the ancestors of all ancestors
of an or node
time is elapsed  an  optimal action is found whenever ut  bc    lt  bc     or lt  bc   
ut  bc   a     a     argmaxaa lt  bc   a   i e  all other actions are pruned  in which case the
optimal action has been found  
now that we have covered the basic subroutines  we present the different heuristics
proposed by satia and lave         washington        and ross and chaib draa        
we begin by introducing some useful notation 
given any graph structure g  let us denote f g  the set of all fringe nodes in g and
hg  b  b    the set of all sequences of actions and observations that lead to belief node b  from
belief node b in the search graph g  if we have a tree t   then ht  b  b    will contain at most
 
 
a single sequence which we will denote hb b
t   now given a sequence h  hg  b  b    we define
pr hz  b  ha   the probability that we observe the whole sequence of observations hz in h  given
that we start in belief node b and perform the whole sequence of actions ha in h  finally  we
define pr h b    to be the probability that we follow the entire action observation sequence
h if we start in belief b and behave according to policy   formally  these probabilities are
computed as follows 
d h 

pr hz  b  ha    

y

pr hiz  bhi    hia   

    

i  

d h 

pr h b     

y

pr hiz  bhi    hia   bhi    hia   

    

i  

where d h  represents the depth of h  number of actions in the sequence h   hia denotes the
ith action in sequence h  hiz the ith observation in the sequence h  and bhi the belief state
obtained by taking the first i actions and observations of the sequence h from b  note that
bh    b 
   

fiross  pineau  paquet    chaib draa

      satia and lave
the approach of satia and lave        follows the heuristic search framework presented
above  the main feature of this approach is to explore  at each iteration  the fringe node b
in the current search tree t that maximizes the following term 
bc  b

ht  bc   b     d ht

 

c  b
c  b
pr hbt z
 bc   hbt a
  ut  b   lt  b   

    

where b  f t   and bc is the root node of t   the intuition behind this heuristic is simple 
recalling the definition of v    we note that the weight of the value v   b  of a fringe node b
bc  b
c  b
c  b
c  b
is a sequence of optimal
   provided hbt a
 bc   hbt a
on v   bc   would be exactly  d ht   pr hbt z
actions  the fringe nodes where this weight is high have the most effect on the estimate
of v   bc    hence one should try to minimize the error at these nodes first  the term
ut  b   lt  b  is included since it is an upper bound on the  unknown  error v   b   lt  b  
thus this heuristic focuses the search in areas of the tree that most affect the value v   bc  
and where the error is possibly large  this approach also uses branch and bound pruning 
such that a fringe node reached by an action that is dominated in some parent belief b is
never going to be expanded  using the same notation as in algorithms     and      this
heuristic can be implemented by defining ht  b   ht  b  a  and ht  b  a  z   as follows 
ht  b    
ut  b   lt  b  
  if ut  b  a    lt  b  
ht  b  a   
  otherwise 
ht  b  a  z     pr z b  a  

    

the condition ut  b  a    lt  b  ensures that the global heuristic value ht  bc   b        if
bc  b 
is dominated  pruned   this guarantees that such fringe
some action in the sequence ht a
nodes will never be expanded 
satia and laves heuristic focuses the search towards beliefs that are most likely to be
reached in the future  and where the error is large  this heuristic is likely to be efficient in
domains with a large number of observations  but only if the probability distribution over
observations is concentrated over only a few observations  the term ut  b   lt  b  in the
heuristic also prevents the search from doing unnecessary computations in areas of the tree
where it already has a good estimate of the value function  this term is most efficient when
the bounds computed offline  u and l  are sufficiently informative  similarly  node pruning
is only going to be efficient if u and l are sufficiently tight  otherwise few actions will be
pruned 
      bi pomdp
washington        proposed a slightly different approach inspired by the ao algorithm
 nilsson         where the search is only conducted in the best solution graph  in the case
of online pomdps  this corresponds to the subtree of all belief nodes that are reached by
sequences of actions maximizing the upper bound in their parent beliefs 
b
the set of fringe nodes in the best solution graph of g  which we denote f g  
can be
b
defined formally as f g     b  f g  h  hg  bc   b   pr h b  g         where g  b  a     
if a   argmaxa  a ug  b  a    and g  b  a      otherwise  the ao algorithm simply specifies
   

fionline planning algorithms for pomdps

expanding any of these fringe nodes  washington        recommends exploring the fringe
node in fb g   where g is the current acyclic search graph  that maximizes ug  b   lg  b  
washingtons heuristic can be implemented by defining ht  b   ht  b  a  and ht  b  a  z   as
follows 
ht  b    
ut  b   lt  b  
  if a   argmaxa  a ut  b  a    
ht  b  a   
  otherwise 
ht  b  a  z      

    

this heuristic tries to guide the search towards nodes that are reachable by promising
actions  especially when they have loose bounds on their values  possibly large error   one
nice property of this approach is that expanding fringe nodes in the best solution graph is
the only way to reduce the upper bound at the root node bc   this was not the case for
satia and laves heuristic  however  washingtons heuristic does not take into account the
probability pr hz  b  ha    nor the discount factor  d h    such that it may end up exploring
nodes that have a very small probability of being reached in the future  and thus have
little effect on the value of v   bc    hence  it may not explore the most relevant nodes
for optimizing the decision at bc   this heuristic is appropriate when the upper bound u
computed offline is sufficiently informative  such that actions with highest upper bound
would also usually tend to have highest q value  in such cases  the algorithm will focus
its search on these actions and thus should find the optimal action more quickly then if it
explored all actions equally  on the other hand  because it does not consider the observation
probabilities  this approach may not scale well to large observation sets  as it will not be
able to focus its search towards the most relevant observations 
      aems
ross and chaib draa        introduced a heuristic that combines the advantages of bipomdp  and satia and laves heuristic  it is based on a theoretical error analysis of tree
search in pomdps  presented by ross et al         
the core idea is to expand the tree such as to reduce the error on v   bc   as quickly as
possible  this is achieved by expanding the fringe node b that contributes the most to the
error on v   bc    the exact error contribution et  bc   b  of fringe node b on bc in tree t is
defined by the following equation 
bc  b

et  bc   b     d ht

 

pr hbtc  b  bc       v   b   lt  b   

    

this expression requires   and v  to be computed exactly  in practice  ross and chaibdraa        suggest approximating the exact error  v   b   lt  b   by  ut  b   lt  b   
as was done by satia and lave  and washington  they also suggest approximating   by
some policy t   where t  b  a  represents the probability that action a is optimal in its
parent belief b  given its lower and upper bounds in tree t   in particular  ross et al        
considered two possible approximations for     the first one is based on a uniformity
assumption on the distribution of q values between the lower and upper bounds  which
yields 
   

fiross  pineau  paquet    chaib draa

t  b  a   

 

 

t  b a lt  b  
  u
ut  b a lt  b a 
 

if ut  b  a    lt  b  
otherwise 

    

where  is a normalization constant such that the sum of the probabilities t  b  a  over all
actions equals   
the second is inspired by ao and bi pomdp  and assumes that the action maximizing
the upper bound is in fact the optimal action 

  if a   argmaxa  a ut  b  a    
    
t  b  a   
  otherwise 
given the approximation t of     the aems heuristic will explore the fringe node b
that maximizes 
bc  b
 

ht  bc   b     d ht

pr hbtc  b  bc   t   ut  b   lt  b   

    

this can be implemented by defining ht  b   ht  b  a  and ht  b  a  z  as follows 
ht  b    ut  b   lt  b  
ht  b  a    t  b  a  
ht  b  a  z     pr z b  a  

    

we refer to this heuristic as aems  when t is defined as in equation     and aems 
when it is defined as in equation     
let us now examine how aems combines the advantages of both the satia and lave 
and bi pomdp heuristics  first  aems encourages exploration of nodes with loose bounds
and possibly large error by considering the term ut  b   lt  b  as in previous heuristics 
moreover  as in satia and lave  it focuses the exploration towards belief states that are likely
to be encountered in the future  this is good for two reasons  as mentioned before  if a belief
state has a low probability of occurrence in the future  it has a limited effect on the value
v   bc   and thus it is not necessary to know its value precisely  second  exploring the highly
probable belief states increases the chance that we will be able to reuse these computations
in the future  hence  aems should be able to deal efficiently with large observation sets 
assuming the distribution over observations is concentrated over a few observations  finally 
as in bi pomdp  aems favors the exploration of fringe nodes reachable by actions that
seem more likely to be optimal  according to t    this is useful to handle large action
sets  as it focuses the search on actions that look promising  if these promising actions
are not optimal  then this will quickly become apparent  this will work well if the best
actions have the highest probabilities in t   furthermore  it is possible to define t such
that it automatically prunes dominated actions by ensuring that t  b  a      whenever
ut  b  a    lt  b   in such cases  the heuristic will never choose to expand a fringe node
reached by a dominated action 
as a final note  ross et al         determined sufficient conditions under which the
search algorithm using this heuristic is guaranteed to find an  optimal action within finite
time  this is stated in theorem     
   the aems  heuristic was also used for a policy search algorithm by hansen        

   

fionline planning algorithms for pomdps

theorem       ross et al         let      and bc the current belief  if for any tree t and
parent belief b in t where ut  b   lt  b      t  b  a      for a   argmaxa  a ut  b  a    
then the aems algorithm is guaranteed to find an  optimal action for bc within finite time 
we observe from this theorem that it is possible to define many different policies t
under which the aems heuristic is guaranteed to converge  aems  and aems  both
satisfy this condition 
      hsvi
a heuristic similar to aems  was also used by smith and simmons        for their offline
value iteration algorithm hsvi as a way to pick the next belief point at which to perform
 vector backups  the main difference is that hsvi proceeds via a greedy search that
descends the tree from the root node b    going down towards the action that maximizes the
upper bound and then the observation that maximizes pr z b  a  u    b  a  z  l   b  a  z   
at each level  until it reaches a belief b at depth d where  d  u  b   l b       this
heuristic could be used in an online heuristic search algorithm by instead stopping the
greedy search process when it reaches a fringe node of the tree and then selecting this node
as the one to be expanded next  in such a setting  hsvis heuristic would return a greedy
approximation of the aems  heuristic  as it may not find the fringe node which actually
bc  b
maximizes  d ht   pr hbtc  b  bc   t   ut  b   lt  b    we consider this online version of the
hsvi heuristic in our empirical study  section     we refer to this extension as hsvi bfs 
note that the complexity of this greedy search is the same as finding the best fringe node
via the dynamic programming process that updates ht and bt in the updateancestors
subroutine 
    alternatives to tree search
we now present two alternative online approaches that do not proceed via a lookahead
search in the belief mdp  in all online approaches presented so far  one problem is that no
learning is achieved over time  i e  everytime the agent encounters the same belief  it has to
recompute its policy starting from the same initial upper and lower bounds computed offline 
the two online approaches presented next address this problem by presenting alternative
ways of updating the initial value functions computed offline so that the performance of
the agent improves over time as it stores updated values computed at each time step 
however  as is argued below and in the discussion  section       these techniques lead to
other disadvantages in terms of memory consumption and or time complexity 
      rtdp bel
an alternative approach to searching in and or graphs is the rtdp algorithm  barto
et al         which has been adapted to solve pomdps by geffner and bonet         their
algorithm  called rtdp bel  learns approximate values for the belief states visited by
successive trials in the environment  at each belief state visited  the agent evaluates all
possible actions by estimating the expected reward of taking action a in the current belief
   

fiross  pineau  paquet    chaib draa

   function onlinepomdpsolver  
static  bc   the current belief state of the agent 
v    initial approximate value function  computed offline  
v   a hashtable of beliefs and their approximate value 
k  discretization resolution 

   initialize bc to the initial belief state and v to an empty hashtable 
   while not executionterminated   do
p
   for all a  a  evaluate q bc   a    rb  b  a     zz pr z b  a v  discretize   b  a  z   k  
   a  argmaxaa q bc   a 
   execute best action a for bc
   v  discretize bc   k    q bc   a 
   perceive a new observation z
   bc    bc   a  z 
    end while

algorithm      rtdp bel algorithm 
state b with an approximate q value equation 
x
q b  a    rb  b  a    
pr z b  a v    b  a  z   

    

zz

where v  b  is the value learned for the belief b 
if the belief state b has no value in the table  then it is initialized to some heuristic value 
the authors suggest using the mdp approximation for the initial value of each belief state 
the agent then executes the action that returned the greatest q b  a  value  afterwards 
the value v  b  in the table is updated with the q b  a  value of the best action  finally 
the agent executes the chosen action and it makes the new observation  ending up in a new
belief state  this process is then repeated again in this new belief 
the rtdp bel algorithm learns a heuristic value for each belief state visited  to
maintain an estimated value for each belief state in memory  it needs to discretize the
belief state space to have a finite number of belief states  this also allows generalization of
the value function to unseen belief states  however  it might be difficult to find the best
discretization for a given problem  in practice  this algorithm needs substantial amounts
of memory  greater than  gb in some cases  to store all the learned belief state values 
especially in pomdps with large state spaces  the implementation of the rtdp bel
algorithm is presented in algorithm     
the function discretize b  k  returns a discretized belief b  where b   s    round kb s   k
for all states s  s  and v  b  looks up the value of belief b in a hashtable  if b is not present in
the hashtable  the value v   b  is returned by v   supported by experimental data  geffner
and bonet        suggest choosing k             as it usually produces the best results 
notice that for a discretization resolution of k there are o  k      s    possible discretized
beliefs  this implies that the memory storage required to maintain v is exponential in  s  
which becomes quickly intractable  even for mid size problems  furthermore  learning good
estimates for this exponentially large number of beliefs usually requires a very large number
of trials  which might be infeasible in practice  this technique can sometimes be applied
in large domains when a factorized representation is available  in such cases  the belief can
be maintained as a set of distributions  one for subset of conditionaly independent state
variables  and the discretization applied seperately to each distribution  this can greatly
reduce the possible number of discretized beliefs 
   

fionline planning algorithms for pomdps

algorithm
rtbss
mcallester
rollout
satia and lave
washington
aems
hsvi bfs
rtdp bel
sovi

 optimal
yes
high probability
no
yes
acyclic graph
yes
yes
no
yes

anytime
no
no
no
yes
yes
yes
yes
no
yes

branch  
bound
yes
no
no
yes
implicit
implicit
implicit
no
no

monte
carlo
no
yes
yes
no
no
no
no
no
no

heuristic
no
no
no
yes
yes
yes
yes
no
no

learning
no
no
no
no
no
no
no
yes
yes

table    properties of various online methods 

      sovi
a more recent online approach  called sovi  shani et al          extends hsvi  smith  
simmons              into an online value iteration algorithm  this approach maintains a
priority queue of the belief states encountered during the execution and proceeds by doing
 vector updates for the current belief state and the k belief states with highest priority at
each time step  the priority of a belief state is computed according to how much the value
function changed at successor belief states  since the last time it was updated  its authors
also propose other improvements to the hsvi algorithm to improve scalability  such as a
more efficient  vector pruning technique  and avoiding to use linear programs to update and
evaluate the upper bound  the main drawback of this approach is that it is hardly applicable
in large environments with short real time constraints  since it needs to perform a value
iteration update with  vectors online  and this can have very high complexity as the number
of  vectors representing the value function increases  i e  o k s  a  z   s     t      to
compute t   
    summary of online pomdp algorithms
in summary  we see that most online pomdp approaches are based on lookahead search 
to improve scalability  different techniques are used  branch and bound pruning  search
heuristics  and monte carlo sampling  these techniques reduce the complexity from different angles  branch and bound pruning lowers the complexity related to the action space
size  monte carlo sampling has been used to lower the complexity related to the observation space size  and could also potentially be used to reduce the complexity related to the
action space size  by sampling a subset of actions   search heuristics lower the complexity
related to actions and observations by orienting the search towards the most relevant actions and observations  when appropriate  factored pomdp representations can be used
to reduce the complexity related to state  a summary of the different properties of each
online algorithm is presented in table   
   

fiross  pineau  paquet    chaib draa

   empirical study
in this section  we compare several online approaches in two domains found in the pomdp
literature  tag  pineau et al         and rocksample  smith   simmons         we consider a modified version of rocksample  called fieldvisionrocksample  ross   chaib draa 
       that has a higher observation space than the original rocksample  this environment
is introduced as a means to test and compare the different algorithms in environments with
large observation spaces 
    methodology
for each environment  we first compare the real time performance of the different heuristics
presented in section     by limiting their planning time to   second per action  all heuristics
were given the same lower and upper bounds such that their results would be comparable 
the objective here is to evaluate which search heuristic is most efficient in different types of
environments  to this end  we have implemented the different search heuristics  satia and
lave  bi pomdp  hsvi bfs and aems  into the same best first search algorithm  such
that we can directly measure the efficiency of the heuristic itself  results were also obtained
for different lower bounds  blind and pbvi  to verify how this choice affects the heuristics
efficiency  finally  we compare how online and offline times affect the performance of each
approach  except where stated otherwise  all experiments were run on an intel xeon    
ghz with  gb of ram  processes were limited to  gb of ram 
      metrics to compare online approaches
we compare performance first and foremost in terms of average discounted return at execution time  however  what we really seek with online approaches is to guarantee better
solution quality than that provided by the original bounds  in other words  we seek to
reduce the error of the original bounds as much as possible  this suggests that a good
metric for the efficiency of online algorithms is to compare the improvement in terms of the
error bounds at the current belief before and after the online search  hence  we define the
error bound reduction percentage to be 
ut  b   lt  b 
 
    
u  b   l b 
where ut  b   lt  b   u  b  and l b  are defined as in section      the best online algorithm
should provide the highest error bound reduction percentage  given the same initial bounds
and real time constraint 
because the ebr metric does not necessarily reflect true error reduction  we also compare the return guarantees provided by each algorithm  i e  the lower bounds on the expected
return provided by the computed policies for the current belief  because improvement of
the lower bound compared to the initial lower bound computed offline is a direct indicator
of true error reduction  the best online algorithm should provide the greatest lower bound
improvement at the current belief  given the same initial bounds and real time constraint 
formally  we define the lower bound improvement to be 
ebr b      

lbi b    lt  b   l b  
   

    

fionline planning algorithms for pomdps

in our experiments  both the ebr and lbi metrics are evaluated at each time step for the
current belief  we are interested in seeing which approach provides the highest ebr and
lbi on average 
we also consider other metrics pertaining to complexity and efficiency  in particular 
we report the average number of belief nodes maintained in the search tree  methods that
have lower complexity will generally be able to maintain bigger trees  but the results will
show that this does not always relate to higher error bound reduction and returns  we will
also measure the efficiency of reusing part of the search tree by recording the percentage of
belief nodes that were reused from one time step to the next 
    tag
tag was initially introduced by pineau et al          this environment has also been
used more recently in the work of several authors  poupart   boutilier        vlassis  
spaan        pineau        spaan   vlassis        smith   simmons        braziunas  
boutilier        spaan   vlassis        smith   simmons         for this environment  an
approximate pomdp algorithm is necessary because of its large size      states    actions
and    observations   the tag environment consists of an agent that has to catch  tag 
another agent while moving in a    cell grid domain  the reader is referred to the work of
pineau et al         for a full description of the domain  note that for all results presented
below  the belief state is represented in factored form  the domain is such that an exact
factorization is possible 
to obtain results in tag  we run each algorithm in each starting configuration   times 
  i e    runs for each of the     different starting joint positions  excluding the    terminal
states    the initial belief state is the same for all runs and consists of a uniform distribution
over the possible joint agent positions 
table   compares the different heuristics by presenting     confidence intervals on the
average discounted return per run  return   average error bound reduction percentage per
time step  ebr   average lower bound improvement per time step  lbi   average belief
nodes in the search tree per time step  belief nodes   the average percentage of belief nodes
reused per time step  nodes reused   the average online planning time used per time step
 online time   in all cases  we use the fib upper bound and the blind lower bound  note
that the average online time is slightly lower than   second per step because algorithms
sometimes find  optimal solutions in less than a second 
we observe that the efficiency of hsvi bfs  bi pomdp and aems  differs slightly
in this environment and that they outperform the three other heuristics  rtbss  satia
and lave  and aems   the difference can be explained by the fact that the latter three
methods do not restrict the search to the best solution graph  as a consequence  they
explore many irrelevant nodes  as shown by the lower error bound reduction percentage 
lower bound improvement  and nodes reused  this poor reuse percentage explains why
satia and lave  and aems  were limited to a lower number of belief nodes in their search
tree  compared to the other methods which reached averages around   k  the results of the
three other heuristics do not differ much here because the three heuristics only differ in the
way they choose the observations to explore in the search  since only two observations are
possible after the first action and observation  and one of these observations leads directly
   

fiross  pineau  paquet    chaib draa

heuristic
rtbss   
satia and lave
aems 
hsvi bfs
bi pomdp
aems 

return
            
           
           
           
           
           

ebr    
         
         
         
         
         
         

lbi
          
          
          
          
          
          

belief
nodes
          
          
          
          
           
           

nodes
reused    
 
         
         
         
         
         

online
time  ms 
      
      
     
      
      
      

table    comparison of different search heuristics on the tag environment using the blind
policy as a lower bound 

exit

a

figure    rocksample      
to a terminal belief state  the possibility that the heuristics differed significantly was very
limited  due to this limitation of the tag domain  we now compare these online algorithms
in a larger and more complex domain  rocksample 
    rocksample
the rocksample problem was originally presented by smith and simmons         in this
domain  an agent has to explore the environment and sample some rocks  see figure    
similarly to what a real robot would do on the planet mars  the agent receives rewards by
sampling rocks and by leaving the environment  at the extreme right of the environment  
a rock can have a scientific value or not  and the agent has to sample only good rocks 
we define rocksample n  k  as an instance of the rocksample problem with an n  n
grid and k rocks  a state is characterized by k     variables  xp   which defines the position
of the robot and can take values                           n  n   and k variables  x r through xkr  
representing each rock  which can take values  good  bad  
the agent can perform k     actions   n orth  south  east  w est  sample  check           
checkk    the four motion actions are deterministic  the sample action samples the
rock at the agents current location  each checki action returns a noisy observation from
 good  bad  for rock i 
the belief state is represented in factored form by the known position and a set of k
probabilities  namely the probability of each rock being good  since the observation of a rock
   

fionline planning algorithms for pomdps

heuristic
satia and lave
aems 
rtbss   
bi pomdp
hsvi bfs
aems 
aems 
satia and lave
rtbss   
bi pomdp
aems 
hsvi bfs

belief
nodes
ebr    
lbi
nodes
reused    
blind  return               time  s
       
       
  
      
       
           
          
          
      
          
           
          
          
      
  
           
         
          
        
         
           
         
          
        
         
           
         
          
         
         
pbvi  return        b                 time     s
           
         
          
        
         
           
         
          
        
         
           
         
          
      
  
           
         
          
        
         
           
         
          
        
         
           
         
          
        
         
return

online
time  ms 
   
   
   
   
   
   








 
 
 
 
 
 

   
   
   
   
   
   








 
 
 
 
 
 

table    comparison of different search heuristics in rocksample      environment  using
the blind policy and pbvi as a lower bound 

state is independent of the other rock states  it only depends on the known robot position  
the complexity of computing pr z b  a  and   b  a  z  is greatly reduced  effectively  the
computation of pr z b  checki   reduces to  pr z b  checki     pr accurate xp   checki   
pr xir   z        pr accurate xp   checki         pr xir   z    the probability that
   xp  i 
  where  xp   i   
the sensor is accurate on rock i  pr accurate xp   checki    
 
d x
p  i  d 
 
  d xp   i  is the euclidean distance between position xp and the position of rock i 
and d  is a constant specifying the half efficiency distance  pr xir   z  is obtained directly
from the probability  stored in b  that rock i is good  similarly    b  a  z  can be computed
quite easily as the move actions deterministically affect variable xp   and a checki action
only changes the probability associated to xir according to the sensors accuracy 
to obtain our results in rocksample  we run each algorithm in each starting rock configuration    times  i e     runs for each of the  k different joint rock states   the initial
belief state is the same for all these runs and consists of     that each rock is good  plus the
known initial robot position 
      real time performance of online search
in table    we present     confidence intervals on the mean of our metrics of interest  for
rocksample             states     actions    observations   with real time contraints of  
second per action  we compare performance using two different lower bounds  the blind
policy and pbvi  and use qmdp for the upper bound in both cases  the performance of
the policy defined by each lower bound is shown in the comparison header  for rtbss  the
notation rtbss k  indicates a k step lookahead  we use the depth k that yields an average
online time closest to   second per action 
return in terms of the return  we first observe that the aems  and hsvi bfs heuristics
obtain very similar results  each of these obtains the highest return by a slight margin with
one of the lower bounds  bi pomdp obtains a similar return when combined with the
   

fiross  pineau  paquet    chaib draa

pbvi lower bound  but performs much worse with the blind lower bound  the two other
heuristics  satia and lave  and aems   perform considerably worse in terms of return with
either lower bound 
ebr and lbi in terms of error bound reduction and lower bound improvement  aems 
obtains the best results with both lower bounds  hsvi bfs is a close second  this indicates that aems  can more effectively reduce the true error than the other heuristics 
and therefore  guarantees better performance  while bi pomdp tends to be less efficient
than aems  and hsvi bfs  it does significantly better than rtbss  satia and lave  and
aems   which only slightly improve the bounds in both case  satia and lave is unable
to increase the blind lower bound  which explains why it obtains the same return as the
blind policy  we also observe that the higher the error bound reduction and lower bound
improvement  the higher the average discounted return usually is  this confirms our intuition that guiding the search such as to minimize the error at the current belief bc is a good
strategy to obtain better return 
nodes reused in terms of the percentage of nodes reused  aems  and hvsi bfs
generally obtain the best scores  this allows these algorithms to maintain a higher number
of nodes in their trees  which could also partly explain why they outperform the other
heuristics in terms of return  error bound reduction and lower bound improvement  note
that rtbss does not reuse any node in the tree because the algorithm does not store the
tree in memory  as a consequence  the reuse percentage is always   
online time finally  we also observe that aems  requires less average online time per
action than the other algorithms to attain its performance  in general  a lower average
online time means the heuristic is efficient at finding  optimal actions in a small amount
of time  the running time for rtbss is determined by the chosen depth  as it cannot stop
before completing the full lookahead search 
summary overall  we see that aems  and hsvi bfs obtain similar results  however
aems  seems slightly better than hsvi bfs  as it provides better performance guarantees
 lower error  within a shorter period of time  but the difference is not very significant  this
may be due to the small number of observations in this environment  in which case the two
heuristics expand the tree in very similar ways  in the next section  we explore a domain
with many more observations to evaluate the impact of this factor 
the lower performances of the three other heuristics can be explained by various reasons 
in the case of bi pomdp  this is due to the fact that it does not take into account the
observation probabilities pr z b  a  and discount factor  in the heuristic value  hence
it tend to expand fringe nodes that did not affect significantly the value of the current
belief  as for satia and lave  its poor performance in the case of the blind policy can be
explained by the fact that the fringe nodes that maximize this heuristic are always leaves
reached by a sequence of move actions  due to the deterministic nature of the move actions
 pr z b  a      for these actions  whereas check actions have pr z b  a        initially   the
heuristic value for fringe nodes reached by move actions is much higher until the error is
reduced significantly  as a result  the algorithm never explores any nodes under the check
actions  and the robot always follows the blind policy  moving east  never checking or
sampling any rocks   this demonstrates the importance of restricting the choice of which
   

fionline planning algorithms for pomdps

  

  

v b  

  

  

aems 
aems 
bipomdp
hsvibfs
satia

  

   
  

 

  

 

 

  

  

 

  

 

  

time  s 

figure    evolution of the upper and lower bounds on rocksample      

leaves to explore to those reached by a sequence of actions maximizing the upper bound  as
done in aems   hsvi bfs and bi pomdp  in the case of aems   it probably behaves
less efficiently because the term it uses to estimate the probability that a certain action
is optimal is not a good approximation in this environment  moreover  because aems 
does not restrict the exploration to the best solution graph  it probably also suffers  in part 
from the same problems as the satia and lave heuristic  rtbss also did not perform very
well with the blind lower bound  this is due to the short depth allowed to search the
tree  required to have a running time    second action  this confirms that we can do
significantly better than an exhaustive search by having good heuristics to guide the search 
      long term error reduction of online heuristic search
to compare the long term performance of the different heuristics  we let the algorithms run
in offline mode from the initial belief state of the environment  and log changes in the lower
and upper bound values of this initial belief state over      seconds  here  the initial lower
and upper bounds are provided by the blind policy and qmdp respectively  we see from
figure   that satia and lave  aems  and bi pomdp are not as efficient as hsvi bfs
and aems  at reducing the error on the bounds  one interesting thing to note is that
the upper bound tends to decrease slowly but continuously  whereas the lower bound often
increases in a stepwise manner  we believe this is due to the fact that the upper bound is
much tighter than the lower bound  we also observe that most of the error bound reduction
happens in the first few seconds of the search  this confirms that the nodes expanded earlier
in the tree have much more impact on the error of bc than those expanded very far in the
tree  e g  after hundreds of seconds   this is an important result in support of using online
 as opposed to offline  methods 
   

fi  

  

  

  

average discounted return

average discounted return

ross  pineau  paquet    chaib draa

  
  
aems 
hsvibfs
bipomdp

  
  
  
 
   
  

 

  

  
  

  
  
 
   
  

 

  

aems    blind
aems    pbvi   
aems    pbvi    

  

online time  s 

 

  

 

  

online time  s 

figure    comparison of the return as a figure    comparison of the return as a
function of the online time in
function of the online time in
rocksample        for different
rocksample        for different
online methods 
offline lower bounds 

      influence of offline and online time
we now compare how the performance of online approaches is influenced by the available
online and offline times  this allows us to verify if a particular method is better when the
available online time is shorter  or longer   or whether increasing the offline time could be
beneficial 
we consider the three approaches that have shown best overall performance so far  bipomdp  hsvi bfs and aems   and compare their average discounted return as a function of the online time constraint per action  experiments were run in rocksample       
         states     actions    observations  for each of the following online time constraints 
   s     s     s   s   s   s and   s  to vary the offline time  we used   different lower
bounds  blind policy  pbvi with   belief points  and pbvi with    belief points  taking
respectively   s    s  and    s  the upper bound used is qmdp in all cases  these results
were obtained on an intel xeon     ghz processor 
in figure    we observe that aems  fares significantly better than hsvi bfs and
bi pomdp for short time constraints  as the time constraint increases  aems  and
hsvi bfs performs similarly  no significant statistical difference   we also notice that
the performance of bi pomdp stops improving after   second of planning time  this can
be explained by the fact that it does not take into account the observation probabilities
pr z b  a   nor the discount factor  as the search tree grows bigger  more and more fringe
nodes have small probability of being reached in the future  such that it becomes more and
more important to take these probabilities into account in order to improve performance 
otherwise  as we observe in the case of bi pomdp  most expanded nodes do not affect the
quality of the solution found 
from figure    we observe that increasing the offline time has a beneficial effect mostly
when we have very short real time constraints  when more online planning time is available 
   

fionline planning algorithms for pomdps

the difference between the performances of aems  with the blind lower bound  and aems 
with pbvi becomes insignificant  however  for online time constraints smaller than one
second  the difference in performance is very large  intuitively  with very short real time
constraints the algorithm does not have enough time to expand a lot of nodes  such that the
policy found relies much more on the bounds computed offline  on the other hand  with
longer time constraints  the algorithm has enough time to significantly improve the bounds
computed offline  and thus the policy found does not rely as much on the offline bounds 
    fieldvisionrocksample
it seems from the results presented thus far that hsvi bfs and aems  have comparable
performance on standard domains  we note however that these environments have very
small observation sets  assuming observations with zero probability are removed   we
believe aems  is especially well suited for domains with large observation spaces  however 
there are few such standard problems in the literature  we therefore consider a modified
version of the rocksample environment  called fieldvisionrocksample  ross   chaib draa 
       which has an observation space size exponential in the number of rocks 
the fieldvisionrocksample  fvrs  problem differs from the rocksample problem only
in the way the robot is able to perceive the rocks in the environment  recall that in
rocksample  the agent has to do a check action on a specific rock to observe its state
through a noisy sensor  in fvrs  the robot observes the state of all rocks  through the
same noisy sensor  after any action is conducted in the environment  consequently  this
eliminates the use of check actions  and the remaining actions for the robot include only
the four move actions  north  east  south  west  and the sample action  the robot can
perceive each rock as being either good or bad  thus the observation space size is  k for
an instance of the problem with k rocks  as in rocksample  the efficiency of the sensor is
defined through the parameter     d d    where d is the distance of the rock and d  is the
half efficiency distance  we assume the sensors observations are independent for each rock 
in fvrs  the partial observability of the environment is directly proportional to the
parameter d    as d  increases  the sensor becomes more accurate and the uncertainty on
the state of the environment decreases  the value d  defined for the different instances of
rocksample in the work of smith and simmons        is too high for the fvrs problem
 especially in the bigger instances of rocksample   making it almost completely observable 
consequently  we re define the value d  for the different instances of the fieldvisionrocksample according to the size of the grid  n   by considering the fact that
p in an n  n grid 
the largest possible distance between a rock and the robot is  n          it seems reasonable that at this distance  the probability of observing the real state of the rock should
be close to    
p for the problem to remain partially observable  consequently  we define
d     n           
to obtain results for the fvrs domain  we run each algorithm in each starting rock
configurations    times  i e     runs for each of the  k different joint rock states   the
initial belief state is the same for all runs and corresponds to a probability of     that each
rock is good  as well as the known initial position of the robot 
   

fiross  pineau  paquet    chaib draa

heuristic
rtbss   
aems 
satia and lave
hsvi bfs
aems 
bi pomdp
rtbss   
bi pomdp
satia and lave
aems 
aems 
hsvi bfs

belief
nodes
return
ebr    
lbi
nodes
reused    
fvrs       blind  return               time    ms 
           
         
          
          
  
           
         
          
         
          
           
         
          
         
          
           
         
          
         
          
           
         
          
          
          
           
         
          
          
          
fvrs       blind  return               time    ms 
           
          
          
      
  
           
         
          
        
          
           
         
          
        
          
           
         
          
        
          
           
         
          
        
          
           
         
          
        
          

online
time  ms 
        
      
      
       
       
       
   
   
   
   
   
   








 
 
 
 
 
 

table    comparison of different search heuristics on different instances of the fieldvisionrocksample environment 

      real time performance of online search
in table    we present     confidence intervals on the mean for our metrics of interest  we
consider two instances of this environment  fvrs           states    actions     observations  and fvrs            states    actions      observations   in both cases  we use the
qmdp upper bound and blind lower bound  under real time constraints of   second per
action 
return in terms of return  we do not observe any clear winner  bi pomdp performs surpringly well in fvrs      but significantly worse than aems  and hsvi bfs in fvrs      
on the other hand  aems  does significantly better than hsvi bfs in fvrs      but both
get very similar performances in fvrs       satia and lave performs better in this environment than in rocksample  this is likely due to the fact that the transitions in belief
space are no longer deterministic  as was the case with the move actions in rocksample  
in fvrs       we also observe that even when rtbss is given   seconds per action to
perform a two step lookahead  its performance is worse than any of the heuristic search
methods  this clearly shows that expanding all observations equally in the search is not
a good strategy  as many of these observations can have negligible impact for the current
decision 
ebr and lbi in terms of error bound reduction and lower bound improvement  we observe that aems  performs much better than hsvi bfs in fvrs       but not significantly
better in fvrs       on the other hand  bi pomdp obtains similar results to aems  in
fvrs      but does significantly worse in terms of ebr and lbi than in fvrs       this
suggests that aems  is consistently effective at reducing the error  even in environments
with large branching factors 
nodes reused the percentage of belief nodes reused is much lower in fvrs due to the
much higher branching factor  we observe that hsvi bfs has the best reuse percentage
   

fionline planning algorithms for pomdps

  

  

  
  

  
  

  
  

  
aems 
aems 
bipomdp
hsvibfs
satia

v b  

v b  

  

  

aems 
aems 
bipomdp
hsvibfs
satia

  

  
  

  

 
   
  

 

  

 

 

  

  

 

  

   
  

 

  

time  s 

 

  

 

  
time  s 

 

  

 

  

figure    evolution of the upper and lower figure    evolution of the upper and lower
bounds on fieldvisionrocksambounds on fieldvisionrocksample      
ple      

in all environments  however not significantly higher than aems   both of these methods
reuse significantly larger portion of the tree than the other methods  this confirms that
these two methods are able to guide the search towards the most likely beliefs 
      long term error reduction of online heuristic search
overall  while table   confirms the consistent performance of hsvi bfs and aems   the
difference with other heuristics is modest  considering the complexity of this environment 
this may be due to the fact that the algorithms do not have enough time to expand a
significant number of nodes within   second  the long term analysis of the bounds evolution
in figures   and   confirms this  we observe in these figures that the lower bound converges
slightly more rapidly with aems  than with other heuristics  the aems  heuristic also
performs well in the long run on this problem  and seems to be the second best heuristic 
while satia and lave is not far behind  on the other hand  the hsvi bfs heuristic is far
worse in this problem than in rocksample  this seems to be in part due to the fact that
this heuristic takes more time to find the next node to expand than the others  and thus
explores fewer belief states 

   discussion
the previous sections presented and evaluated several online pomdp algorithms  we now
discuss important issues that arise when applying online methods in practice  and summarize
some of their advantages and disadvantages  this should help researchers decide whether
online algorithms are a good approach for solving a given problem 
   

fiross  pineau  paquet    chaib draa

    lower and upper bound selection
online algorithms can be combined with many valid lower and upper bounds  however 
there are some properties that these bounds should satisfy for the online search to perform efficiently in practice  one of the desired properties is that the lower and upper
bound functions
should
property states that b   l b  


p be monotone  the monotone
maxaa rb  b  a     pzz pr z b  a l   b  a  z    for the lower bound and b   u  b  
maxaa rb  b  a     zz pr z b  a u    b  a  z   for the upper bound  this property
guarantees that when a certain fringe node is expanded  its lower bound is non decreasing
and its upper bound is non increasing  this is sufficient to guarantee that the error bound
ut  b   lt  b  on b is non increasing after the expansion of b  such that the error bound
given by the algorithm on the value of the root belief state bc   cannot be worse than the
error bound defined by the initial bounds given  note however that monotonicity is not
necessary for aems to converge to an  optimal solution  as shown in previous work  ross
et al          boundedness is sufficient 
    improving the bounds over time
as we mentioned in our survey of online algorithms  one drawback of many online approaches is that they do not store improvements made to the offline bounds during the
online search  such that  if the same belief state is encountered again  the same computations need to be performed again  restarting from the same offline bounds  a trivial way
to improve this is to maintain a large hashtable  or database  of belief states for which we
have improved the lower and upper bounds in previous search  with their associated new
bounds  there are however many drawbacks in doing this  first every time we want to
evaluate the lower and upper bound of a fringe belief  a search through this hashtable needs
to be performed to check if we have better bounds available  this may require significant
time if the hashtable is large  e g  millions of beliefs   furthermore  experiments conducted
with rtdp bel in large domains  such as rocksample       have shown that such a process
usually runs out of memory  i e  requires more than   gb  before good performance is
achieved and requires several thousands episodes before performing well  paquet        
the authors of rtbss have also tried combining their search algorithm with rtdpbel such as to preserve the improvements made by the search  paquet         while this
combination usually performed better and learned faster than rtdp bel alone  it was found
that in most domains  a few thousand episodes are still required before any improvement
can be seen  in terms of return   hence  such point updates of the offline bounds tend to
be useful in large domains only if the task to accomplish is repeated a very large number
of times 
a better strategy to improve the lower bound might be to save some time to perform
 vector updates for some of the beliefs expanded during the search  such that the offline
lower bound improves over time  such updates have the advantage of improving the lower
bound over the whole belief space  instead of at a single belief state  however this can be
very time consuming  especially in large domains  hence  if we need to act within very short
time constraints  such an approach is infeasible  however if several seconds of planning time
are available per action  then it might be advantageous to use some of this time to perform
 vector updates  rather than use all the available time to search through the tree  a good
   

fionline planning algorithms for pomdps

idea here would be to perform  vector updates for a subset of the beliefs in the search tree 
where the lower bound most improves 
    factored pomdp representations
the efficiency of online algorithms relies heavily on the ability to quickly compute   b  a  z 
and pr z b  a   as these must be computed for evey belief state in the search tree  using
factored pomdp representations is an effective way to reduce the time complexity of computing these quantities  since most environments with large state spaces are structured and
can be described by sets of features  obtaining factored representation of complex systems
should not be an issue in most cases  however  in domains with significant dependencies
between state features  it may be useful to use algorithms proposed by boyen and koller
       and poupart        to find approximate factored representations where most features
are independent  with minimal degradation in the solution quality  while the upper and
lower bounds might not hold anymore if they are computed over the approximate factored
representation  usually it may still yield good results in practice 
    handling graph structure
as we have mentioned before  the general tree search algorithm used by online algorithms
will duplicate belief states whenever there are multiple paths leading to the same posterior
belief from the current belief bc   this greatly simplifies the complexity related to updating
the values of ancestor nodes  and it also reduces the complexity related to finding the
best fringe node to expand  using the technique in section     which is only valid for
trees   the disadvantage of using a tree structure is that inevitably  some computations
will be redundant  as the algorithm will potentially expand the same subtree under every
duplicate belief  to avoid this  we could use the lao algorithm proposed by hansen and
zilberstein        as an extension of ao that can handle generic graph structure  including
cyclic graphs  after each expansion  it runs a value  or policy  iteration algorithm until
convergence among all ancestor nodes in order to update their values 
the heuristics we surveyed in section     can be generalized to guide best first search
algorithms that handle graph structure  like lao   the first thing to notice is that  in
any graph  if a fringe node is reached by multiple paths  then its error contributes multiple
times to the error on the value of bc   under this error contribution perspective  the heuristic
value of such a fringe node should be the sum of its heuristic values over all paths reaching
it  for instance  in the case of the aems heuristic  using the notation we have defined in
section      the global heuristic value of a given fringe node b  on the current belief state
bc in any graph g  can be computed as follows 
hg  bc   b     u  b   l b  

x

 d h  pr h bc   g   

    

hhg  bc  b 

notice that for cyclic graphs  there can be infinitely many paths in hg  bc   b   in such
case  we could use dynamic programming to estimate the heuristic value 
because solving hg  bc   b  for all fringe nodes b in the graph g will require a lot of time
in practice  especially if there are many fringe nodes  we have not experimented with this
method in section    however  it would be practical to use this heuristic if we could find an
   

fiross  pineau  paquet    chaib draa

alternative way to determine the best fringe node without computing hg  bc   b  separately
for each fringe node b and performing an exhaustive search over all fringe nodes 
    online vs  offline time
one important aspect determining the efficiency and applicability of online algorithms is
the amount of time available during the execution for planning  this of course is often taskdependent  for real time problems like robot navigation  this amount of time may be very
short  e g  between     to   second per action  on the other hand for tasks like portfolio
management  where acting every second is not necessary  several minutes could easily be
taken to plan any stock buying selling action  as we have seen from our experiments  the
shorter the available online planning time  the greater the importance of having a good
offline value function to start with  in such case  it is often necessary to reserve sufficient
time to compute a good offline policy  as more and more planning time is available online 
the influence of the offline value function becomes negligible  such that a very rough offline
value function is sufficient to obtain good performance  the best trade off between online
and offline time often depends on how large the problem is  when the branching factor
  a  z   is large and or computing successor belief states takes a long time  then more online
time will be required to achieve a significant improvement over the offline value function 
however  for small problems  an online time of     second per action may be sufficient to
perform near optimally even with a very rough offline value function 
    advantages and disadvantages of online algorithms
we now discuss the advantages and disadvantages of online planning algorithms in general 
      advantages
 most online algorithms can be combined with any offline solving algorithm  assuming
it provides a lower bound or an upper bound on v    such as to improve the quality
of the policy found offline 
 online algorithms require very little offline computation before being executable in an
environment  as they can perform well even using very loose bounds  which are quick
to compute 
 online methods can exploit the knowledge of the current belief to focus computation
on the most relevant future beliefs for the current decision  such that they scale well
to large action and observation spaces 
 anytime online methods are applicable in real time environments  as they can be
stopped whenever planning time runs out  and still provide the best solution found
so far 
      disadvantages
 the branching factor depends on the number of actions and observations  thus if
there are many observations and or actions  it might be impossible to search deep
   

fionline planning algorithms for pomdps

enough  to provide significant improvement of the offline policy  in such cases  sampling methods designed to reduce the branching factor could be useful  while we
cannot guarantee that the lower and upper bounds are still valid when sampling is
used  we can guarantee that they are valid with high probability  given that enough
samples are drawn 
 most online algorithms do not store improvements made to the offline policy by the
online search  and so the algorithm has to plan again with the same bounds each time
the environment is restarted  if time is available  it could be advantageous to add
 vector updates for some belief states explored in the tree  so that the offline bounds
improve with time 

   conclusion
pomdps provide a rich and elegant framework for planning in stochastic partially observable domains  however their time complexity has been a major issue preventing their
application to complex real world systems  this paper thoroughly surveys the various existing online algorithms and the key techniques and approximations used to solve pomdps
more efficiently  we empirically compare these online approaches in several pomdp domains under different metrics  average discounted return  average error bound reduction
and average lower bound improvement  and using different lower and upper bounds  pbvi 
blind  fib and qmdp 
from the empirical results  we observe that some of the heuristic search methods  namely
aems  and hsvi bfs  obtain very good performances  even in domains with large branching factors and large state spaces  these two methods are very similar and perform well
because they orient the search towards nodes that can improve the current approximate
value function as quickly as possible  i e  the belief nodes that have largest error and are
most likely to be reached in the future by promising actions  however  in environments
with large branching factors  we may only have time to expand a few nodes at each turn 
hence  it would be interesting to develop further approximations to reduce the branching
factor in such cases 
in conclusion  we believe that online approaches have an important role to play in
improving the scalability of pomdp solution methods  a good example is the succesful
applications of the rtbss algorithm to the robocuprescue simulation by paquet et al 
        this environment is very challenging as the state space is orders of magnitude
beyond the scope of current algorithms  offline algorithms remain very important to obtain
tight lower and upper bounds on the value function  the interesting question is not whether
online or offline approaches are better  but how we can improve both kinds of approaches 
such that their synergy can be exploited to solve complex real world problems 

acknowledgments
this research was supported by the natural sciences and engineering council of canada
and the fonds quebecois de la recherche sur la nature et les technologies  we would also
like to thank the anonymous reviewers for their helpful comments and suggestions 
   

fiross  pineau  paquet    chaib draa

references
astrom  k  j          optimal control of markov decision processes with incomplete state
estimation  journal of mathematical analysis and applications             
barto  a  g   bradtke  s  j     singhe  s  p          learning to act using real time dynamic
programming  artificial intelligence                
bellman  r          dynamic programming  princeton university press  princeton  nj 
usa 
bertsekas  d  p     castanon  d  a          rollout algorithms for stochastic scheduling
problems  journal of heuristics               
boyen  x     koller  d          tractable inference for complex stochastic processes  in
in proceedings of the fourteenth conference on uncertainty in artificial intelligence
 uai      pp       
braziunas  d     boutilier  c          stochastic local search for pomdp controllers  in the
nineteenth national conference on artificial intelligence  aaai      pp         
cassandra  a   littman  m  l     zhang  n  l          incremental pruning  a simple  fast 
exact method for partially observable markov decision processes  in proceedings of the
thirteenth conference on uncertainty in artificial intelligence  uai      pp       
chang  h  s   givan  r     chong  e  k  p          parallel rollout for online solution
of partially observable markov decision processes  discrete event dynamic systems 
               
geffner  h     bonet  b          solving large pomdps using real time dynamic programming  in proceedings of the fall aaai symposium on pomdps  pp       
hansen  e  a          solving pomdps by searching in policy space  in fourteenth conference on uncertainty in artificial intelligence  uai      pp         
hansen  e  a     zilberstein  s          lao     a heuristic search algorithm that finds
solutions with loops  artificial intelligence                  
hauskrecht  m          value function approximations for partially observable markov
decision processes  journal of artificial intelligence research           
kaelbling  l  p   littman  m  l     cassandra  a  r          planning and acting in
partially observable stochastic domains  artificial intelligence             
kearns  m  j   mansour  y     ng  a  y          a sparse sampling algorithm for nearoptimal planning in large markov decision processes  in proceedings of the sixteenth
international joint conference on artificial intelligence  ijcai      pp           
koenig  s          agent centered search  ai magazine                 
littman  m  l          algorithms for sequential decision making  ph d  thesis  brown
university 
littman  m  l   cassandra  a  r     kaelbling  l  p          learning policies for partially observable environments  scaling up  in proceedings of the   th international
conference on machine learning  icml      pp         
   

fionline planning algorithms for pomdps

lovejoy  w  s          computationally feasible bounds for pomdps  operations research 
               
madani  o   hanks  s     condon  a          on the undecidability of probabilistic planning
and infinite horizon partially observable markov decision problems  in proceedings of
the sixteenth national conference on artificial intelligence   aaai      pp         
mcallester  d     singh  s          approximate planning for factored pomdps using belief state simplification  in proceedings of the   th annual conference on uncertainty
in artificial intelligence  uai      pp         
monahan  g  e          a survey of partially observable markov decision processes  theory 
models and algorithms  management science              
nilsson  n          principles of artificial intelligence  tioga publishing 
papadimitriou  c     tsitsiklis  j  n          the complexity of markov decision processes 
mathematics of operations research                 
paquet  s          distributed decision making and task coordination in dynamic  uncertain and real time multiagent environments  ph d  thesis  laval university 
paquet  s   chaib draa  b     ross  s          hybrid pomdp algorithms  in proceedings
of the workshop on multi agent sequential decision making in uncertain domains
 msdm      pp         
paquet  s   tobin  l     chaib draa  b          an online pomdp algorithm for complex
multiagent environments  in proceedings of the fourth international joint conference
on autonomous agents and multi agent systems  aamas      pp         
pineau  j   gordon  g     thrun  s          point based value iteration  an anytime algorithm for pomdps  in proceedings of the international joint conference on artificial
intelligence  ijcai      pp           
pineau  j   gordon  g     thrun  s          anytime point based approximations for large
pomdps  journal of artificial intelligence research             
pineau  j          tractable planning under uncertainty  exploiting structure  ph d  thesis 
carnegie mellon university 
poupart  p          exploiting structure to efficiently solve large scale partially observable
markov decision processes  ph d  thesis  university of toronto 
poupart  p     boutilier  c          bounded finite state controllers  in advances in neural
information processing systems     nips  
puterman  m  l          markov decision processes  discrete stochastic dynamic programming  john wiley   sons  inc 
ross  s     chaib draa  b          aems  an anytime online search algorithm for approximate policy refinement in large pomdps  in proceedings of the   th international
joint conference on artificial intelligence  ijcai      pp           
ross  s   pineau  j     chaib draa  b          theoretical analysis of heuristic search
methods for online pomdps  in advances in neural information processing systems
    nips  
   

fiross  pineau  paquet    chaib draa

satia  j  k     lave  r  e          markovian decision processes with probabilistic observation of states  management science              
shani  g   brafman  r     shimony  s          adaptation for changing stochastic environments through online pomdp policy learning  in proceedings of the workshop on
reinforcement learning in non stationary environments  ecml       pp       
smallwood  r  d     sondik  e  j          the optimal control of partially observable
markov processes over a finite horizon  operations research                   
smith  t     simmons  r          heuristic search value iteration for pomdps  in proceedings of the   th conference on uncertainty in artificial intelligence  uai      pp 
       
smith  t     simmons  r          point based pomdp algorithms  improved analysis and
implementation  in proceedings of the   th conference on uncertainty in artificial
intelligence  uai      pp         
sondik  e  j          the optimal control of partially observable markov processes  ph d 
thesis  stanford university 
sondik  e  j          the optimal control of partially observable markov processes over the
infinite horizon  discounted costs  operations research                 
spaan  m  t  j     vlassis  n          a point based pomdp algorithm for robot planning 
in in proceedings of the ieee international conference on robotics and automation
 icra      pp           
spaan  m  t  j     vlassis  n          perseus  randomized point based value iteration for
pomdps  journal of artificial intelligence research             
vlassis  n     spaan  m  t  j          a fast point based algorithm for pomdps  in
benelearn       proceedings of the annual machine learning conference of belgium
and the netherlands  pp         
washington  r          bi pomdp  bounded  incremental partially observable markov
model planning  in proceedings of the  th european conference on planning  pp 
       
zhang  n  l     zhang  w          speeding up the convergence of value iteration in partially observable markov decision processes  journal of artificial intelligence research 
         

   

fi