journal artificial intelligence research                  

submitted        published      

online planning algorithms pomdps
stephane ross
joelle pineau

stephane ross mail mcgill ca
jpineau cs mcgill ca

school computer science
mcgill university  montreal  canada  h a  a 

sebastien paquet
brahim chaib draa

spaquet damas ift ulaval ca
chaib damas ift ulaval ca

department computer science software engineering
laval university  quebec  canada  g k  p 

abstract
partially observable markov decision processes  pomdps  provide rich framework
sequential decision making uncertainty stochastic domains  however  solving
pomdp often intractable except small problems due complexity  here 
focus online approaches alleviate computational complexity computing
good local policies decision step execution  online algorithms generally consist lookahead search find best action execute time step
environment  objectives survey various existing online pomdp
methods  analyze properties discuss advantages disadvantages 
thoroughly evaluate online approaches different environments various metrics  return  error bound reduction  lower bound improvement   experimental results
indicate state of the art online heuristic search methods handle large pomdp
domains efficiently 

   introduction
partially observable markov decision process  pomdp  general model sequential decision problems partially observable environments  many planning control problems modeled pomdps  solved exactly
computational complexity  finite horizon pomdps pspace complete  papadimitriou   tsitsiklis        infinite horizon pomdps undecidable  madani  hanks 
  condon        
last years  pomdps generated significant interest ai community many approximation algorithms developed  hauskrecht        pineau 
gordon    thrun        braziunas   boutilier        poupart        smith   simmons 
      spaan   vlassis         methods offline algorithms  meaning
specify  prior execution  best action execute possible situations 
approximate algorithms achieve good performance  often take significant time  e g  hour  solve large problems  many
possible situations enumerate  let alone plan for   furthermore  small changes
environments dynamics require recomputing full policy  may take hours days 
c
    
ai access foundation  rights reserved 

fiross  pineau  paquet    chaib draa

hand  online approaches  satia   lave        washington        barto 
bradtke    singhe        paquet  tobin    chaib draa        mcallester   singh       
bertsekas   castanon        shani  brafman    shimony        try circumvent complexity computing policy planning online current information state  online algorithms sometimes called agent centered search algorithms  koenig        
whereas offline search would compute exponentially large contingency plan considering possible happenings  online search considers current situation small
horizon contingency plans  moreover  approaches handle environment
changes without requiring computation  allows online approaches applicable many contexts offline approaches applicable  instance 
task accomplish  defined reward function  changes regularly environment 
one drawback online planning generally needs meet real time constraints 
thus greatly reducing available planning time  compared offline approaches 
recent developments online pomdp search algorithms  paquet  chaib draa    ross 
      ross   chaib draa        ross  pineau    chaib draa        suggest combining
approximate offline online solving approaches may efficient way tackle
large pomdps  fact  generally compute rough policy offline using existing
offline value iteration algorithms  use approximation heuristic function
guide online search algorithm  combination enables online search algorithms
plan shorter horizons  thereby respecting online real time constraints retaining
good precision  exact online search fixed horizon  guarantee
reduction error approximate offline value function  overall time  offline
online  required obtain good policy dramatically reduced combining
approaches 
main purpose paper draw attention ai community online
methods viable alternative solving large pomdp problems  support this 
first survey various existing online approaches applied pomdps 
discuss strengths drawbacks  present various combinations online algorithms
various existing offline algorithms  qmdp  littman  cassandra    kaelbling 
       fib  hauskrecht         blind  hauskrecht        smith   simmons       
pbvi  pineau et al          compare empirically different online approaches
two large pomdp domains according different metrics  average discounted return  error
bound reduction  lower bound improvement   evaluate available online
planning time offline planning time affect performance different algorithms 
results experiments show many state of the art online heuristic search methods
tractable large state observation spaces  achieve solution quality stateof the art offline approaches fraction computational cost  best methods
achieve focusing search relevant future outcomes current
decision  e g  likely high uncertainty  error  longterm values  minimize quickly possible error bound performance
best action found  tradeoff solution quality computing time offered
combinations online offline approaches attractive tackling increasingly
large domains 
   

fionline planning algorithms pomdps

   pomdp model
partially observable markov decision processes  pomdps  provide general framework
acting partially observable environments  astrom        smallwood   sondik       
monahan        kaelbling  littman    cassandra         pomdp generalization
mdp model planning uncertainty  gives agent ability
effectively estimate outcome actions even cannot exactly observe state
environment 
formally  pomdp represented tuple  s  a  t  r  z  o  where 
set environment states  state description environment
specific moment capture information relevant agents
decision making process 
set possible actions 
         transition function   s  a  s      pr s   s  a 
represents probability ending state s  agent performs action state
s 
r   r reward function  r s  a  reward obtained
executing action state s 
z set possible observations 
  z        observation function  o s    a  z    pr z a  s    gives
probability observing z action performed resulting state s   
assume paper s  z finite r bounded 
key aspect pomdp model assumption states directly
observable  instead  given time  agent access observation
z z gives incomplete information current state  since states
observable  agent cannot choose actions based states  consider
complete history past actions observations choose current action 
history time defined as 
ht    a    z            zt    at    zt   

   

explicit representation past typically memory expensive  instead 
possible summarize relevant information previous actions observations
probability distribution state space s  called belief state  astrom        
belief state time defined posterior probability distribution
state  given complete history 
bt  s    pr st   s ht   b    

   

belief state bt sufficient statistic history ht  smallwood   sondik        
therefore agent choose actions based current belief state bt instead
past actions observations  initially  agent starts initial belief state b   
   

fiross  pineau  paquet    chaib draa

representing knowledge starting state environment  then  time
t  belief state bt computed previous belief state bt    using previous
action at  current observation zt   done belief state update function
 b  a  z   bt    bt    at    zt   defined following equation 
bt  s       bt    at    zt   s     

 
pr zt  bt    at   

o s    at    zt  

x

 s  at    s   bt   s      

ss

pr z b  a   probability observing z action belief b  acts
normalizing constant bt remains probability distribution 
pr z b  a   

x

o s    a  z 

s 

x

 s  a  s   b s  

   

ss

agent way computing belief  next interesting question
choose action based belief state 
action determined agents policy   specifying probability
agent execute action given belief state  i e  defines agents strategy
possible situations could encounter  strategy maximize amount
reward earned finite infinite time horizon  article  restrict attention
infinite horizon pomdps optimality criterion maximize expected
sum discounted rewards  also called return discounted return   formally 
optimal policy defined following equation 
 
 
x
x x
   
bt  s 
r s  a  bt   a   b   
  argmax e



t  

ss

aa

       discount factor  bt   a  probability action
performed belief bt   prescribed policy  
return obtained following specific policy   certain belief state b 
defined value function equation v  
 
 
x
x


v  b   
 b  a  rb  b  a   
pr z b  a v    b  a  z    
   
aa

zz

function rb  b  a  specifies immediate expected reward executing action
belief b according reward function r 
rb  b  a   

x

b s r s  a  

   

ss

sum z equation   interpreted expected future return infinite
horizon executing action a  assuming policy followed afterwards 
note definitions rb  b  a   pr z b  a   b  a  z   one view
pomdp mdp belief states  called belief mdp   pr z b  a  specifies
probability moving b  b  a  z  action a  rb  b  a  immediate
reward obtained action b 
   

fionline planning algorithms pomdps

optimal policy defined equation   represents action selection strategy
maximize equation v  b     since always exists deterministic policy
maximizes v belief states  sondik         generally consider deterministic policies  i e  assign probability   specific action every belief
state  
value function v optimal policy fixed point bellmans equation
 bellman        
 
 
x
v  b    max rb  b  a   
pr z b  a v    b  a  z    
   
aa

zz

another useful quantity value executing given action belief state b 
denoted q value 
q  b  a    rb  b  a   

x

pr z b  a v    b  a  z   

   

zz

difference definition v max operator omitted  notice
q  b  a  determines value assuming optimal policy followed
every step action a 
review different offline methods solving pomdps  used guide
online heuristic search methods discussed later  cases form
basis online solutions 
    optimal value function algorithm
one solve optimally pomdp specified finite horizon h using value
iteration algorithm  sondik         algorithm uses dynamic programming compute
increasingly accurate values belief state b  value iteration algorithm
begins evaluating value belief state immediate horizon      formally 
let v value function takes belief state parameter returns numerical
value r belief state  initial value function is 
v   b    max rb  b  a  
aa

    

value function horizon constructed value function horizon  
using following recursive equation 
 
 
x
vt  b    max rb  b  a   
pr z b  a vt     b  a  z    
    
aa

zz

value function equation    defines discounted sum expected rewards
agent receive next time steps  belief state b  therefore  optimal
policy finite horizon simply choose action maximizing vt  b  
 
 
x
pr z b  a vt     b  a  z    
    
 b    argmax rb  b  a   
aa

zz

   

fiross  pineau  paquet    chaib draa

last equation associates action specific belief state  therefore must
computed possible belief states order define full policy 
key result smallwood sondik        shows optimal value function
finite horizon pomdp represented hyperplanes  therefore convex
piecewise linear  means value function vt horizon represented
set  s  dimensional hyperplanes                        hyperplanes often
called  vectors  defines linear value function belief state space associated
action a  value belief state maximum value returned one
 vectors belief state  best action one associated  vector
returned best value 
x
 s b s  
    
vt  b    max


ss

number exact value function algorithms leveraging piecewise linear convex
aspects value function proposed pomdp literature  sondik       
monahan        littman        cassandra  littman    zhang        zhang   zhang 
       problem exact approaches number  vectors
needed represent value function grows exponentially number observations
iteration  i e  size set o  a  t    z     since new  vector
requires computation time o  z  s      resulting complexity iteration exact
approaches o  a  z  s    t    z     work exact approaches focused
finding efficient ways prune set   effectively reduce computation 
    offline approximate algorithms
due high complexity exact solving approaches  many researchers worked
improving applicability pomdp approaches developing approximate offline
approaches applied larger problems 
online methods review below  approximate offline algorithms often used
compute lower upper bounds optimal value function  bounds
leveraged orient search promising directions  apply branch and bound pruning
techniques  estimate long term reward belief states  show section
   however  generally want use approximate methods require low
computational cost  particularly interested approximations use
underlying mdp  compute lower bounds  blind policy  upper bounds  mdp  qmdp 
fib  exact value function  investigate usefulness using precise
lower bounds provided point based methods  briefly review offline methods
featured empirical investigation  recent publications provide
comprehensive overview offline approximate algorithms  hauskrecht        pineau 
gordon    thrun        
      blind policy
blind policy  hauskrecht        smith   simmons        policy
action always executed  regardless belief state  value function blind
   mdp defined  s  a  t  r  components pomdp model 

   

fionline planning algorithms pomdps

policy obviously lower bound v since corresponds value one specific
policy agent could execute environment  resulting value function
specified set  a   vectors   vector specifies long term expected
reward following corresponding blind policy   vectors computed using
simple update rule 
at    s    r s  a   

x

 s  a  s   at  s  

    

s 

a    minss r s  a        vectors computed  use equation   
obtain lower bound value belief state  complexity iteration
o  a  s      far less exact methods  lower bound
computed quickly  usually tight thus informative 
      point based algorithms
obtain tighter lower bounds  one use point based methods  lovejoy        hauskrecht 
      pineau et al          popular approach approximates value function updating selected belief states  point based methods sample belief
states simulating random interactions agent pomdp environment 
update value function gradient sampled beliefs  approaches circumvent complexity exact approaches sampling small set beliefs
maintaining one  vector per sampled belief state  let b represent set
sampled beliefs  set  vectors time obtained follows 
 s 
a z

bt


 
 
 
 

r s  a  
p
a z
    a  z    s      
 a z
s   s  a  s   o s
t    


 i  s    p
p



a z
 b  b     zz argmaxt
ss  s b s   a  
p
 b  b   argmaxb ss b s  s   b b  

    



ensure gives lower bound    initialized single  vector    s   

mins  s aa r s   a 
 
 

since  t     b   iteration complexity o  a  z  s  b   s  
 b     polynomial time  compared exponential time exact approaches 
different algorithms developed using point based approach  pbvi  pineau
et al          perseus  spaan   vlassis         hsvi  smith   simmons             
recent methods  methods differ slightly choose
belief states update value function chosen belief states 
nice property approaches one tradeoff complexity
algorithm precision lower bound increasing  or decreasing  number
sampled belief points 
      mdp
mdp approximation consists approximating value function v pomdp
value function underlying mdp  littman et al          value function
upper bound value function pomdp computed using bellmans
equation 
   

fiross  pineau  paquet    chaib draa

 

dp
vt  
 s    max r s  a   
aa

x

 

 s  a  s   vtm dp  s     

s 

    

p
value v  b  belief state b computed v  b    ss v dp  s b s  
computed quickly  iteration equation    done o  a  s     
      qmdp
qmdp approximation slight variation mdp approximation  littman et al  
       main idea behind qmdp consider partial observability disappear
single step  assumes mdp solution computed generate vtm dp  equation
     given this  define 
dp
qm
t    s  a    r s  a   

x

 s  a  s   vtm dp  s    

    

s 

approximation defines  vector action  gives upper bound v
tighter v dp   i e  vtqm dp  b  vtm dp  b  belief b   again 
obtain value belief state  use equation     contain one  vector
dp  s  a  a 
 s    qm

      fib
two upper bounds presented far  qmdp mdp  take account
partial observability environment  particular  information gathering actions
may help identify current state always suboptimal according bounds 
address problem  hauskrecht        proposed new method compute upper bounds 
called fast informed bound  fib   able take account  to degree 
partial observability environment   vector update process described
follows 
at    s    r s  a   

x

zz

a 

max



x

o s    a  z t  s  a  s   t  s    

    

s 

initialized  vectors found qmdp convergence  i e 
 vectors
a   s    qm dp  s  a   fib defines single  vector action value belief
state computed according equation     fib provides tighter upper bound
qmdp   i e  vtf ib  b  vtqm dp  b  b    complexity algorithm remains
acceptable  iteration requires o  a    s    z   operations 

   online algorithms pomdps
offline approaches  algorithm returns policy defining action execute
every possible belief state  approaches tend applicable dealing
small mid size domains  since policy construction step takes significant time  large
pomdps  using rough value function approximation  such ones presented
section      tends substantially hinder performance resulting approximate
   

fionline planning algorithms pomdps

offline approaches
policy construction

policy execution

online approaches

small policy construction step policy execution steps

figure    comparison offline online approaches 
policy  even recent point based methods produce solutions limited quality
large domains  paquet et al         
hence large pomdps  potentially better alternative use online approach 
tries find good local policy current belief state agent 
advantage approach needs consider belief states reachable current belief state  focuses computation small set beliefs 
addition  since online planning done every step  and thus generalization beliefs
required   sufficient calculate maximal value current belief
state  full optimal  vector  setting  policy construction steps
execution steps interleaved one another shown figure    cases  online
approaches may require extra execution steps  and online planning   since policy
locally constructed therefore always optimal  however policy construction time
often substantially shorter  consequently  overall time policy construction
execution normally less online approaches  koenig         practice  potential
limitation online planning need meet short real time constraints 
case  time available construct plan small compared offline algorithms 
    general framework online planning
subsection presents general framework online planning algorithms pomdps 
subsequently  discuss specific approaches literature describe vary
tackling various aspects general framework 
online algorithm divided planning phase  execution phase 
applied alternately time step 
planning phase  algorithm given current belief state agent
computes best action execute belief  usually achieved two steps 
first tree reachable belief states current belief state built looking
several possible sequences actions observations taken current
belief  tree  current belief root node subsequent reachable beliefs  as
calculated  b  a  z  function equation    added tree child nodes
immediate previous belief  belief nodes represented using or nodes  at
must choose action  actions included layer belief nodes using
and nodes  at must consider possible observations lead subsequent
   

fiross  pineau  paquet    chaib draa

b 

            

 

 

            

          

a 

a 

   

z 
            

b 

        

b 

z 
        b
 

b 

   

z 
       

b 

        

 

a 
   

z 

z 

  

           

   

   

a 
   

   

z 
b 

       

z 

            
   

        

z 
b 

b 

        

figure    and or tree constructed search process pomdp   actions   observations  belief states or nodes represented triangular nodes action and nodes
circular nodes  rewards rb  b  a  represented values outgoing arcs
or nodes probabilities pr z b  a  shown outgoing arcs and nodes 
values inside brackets represent lower upper bounds computed according
equations          assuming discount factor         notice example
action a  belief state b  could pruned since upper bound          lower
lower bound          action a  b   

beliefs   value current belief estimated propagating value estimates
fringe nodes  ancestors  way root  according bellmans
equation  equation     long term value belief nodes fringe usually estimated
using approximate value function computed offline  methods maintain
lower bound upper bound value node  example
tree contructed evaluated presented figure   
planning phase terminates  execution phase proceeds executing best
action found current belief environment  updating current belief
tree according observation obtained 
notice general  belief mdp could graph structure cycles 
online algorithms handle structure unrolling graph tree  hence 
reach belief already elsewhere tree  duplicated  algorithms could
always modified handle generic graph structures using technique proposed
lao  algorithm  hansen   zilberstein        handle cycles  however
advantages disadvantages this  in depth discussion issue
presented section     
generic online algorithm implementing planning phase  lines      execution
phase  lines        presented algorithm      algorithm first initializes tree
contain initial belief state  line     given current tree  planning phase
algorithm proceeds first selecting next fringe node  line   
pursue search  construction tree   expand function  line    constructs
   

fionline planning algorithms pomdps

   function onlinepomdpsolver  
static  bc   current belief state agent 
  and or tree representing current search tree 
d  expansion depth 
l  lower bound v  
u   upper bound v  

  
  
  
  
  
  
  
  
   
   
   
   
   

bc b 
initialize contain bc root
executionterminated  
planningterminated  
b choosenextnodetoexpand  
expand b   d 
updateancestors b  
end
execute best action bc
perceive new observation z
bc  bc   a  z 
update tree bc new root
end

algorithm      generic online algorithm 
next reachable beliefs  using equation    selected leaf pre determined
expansion depth evaluates approximate value function newly created
nodes  new approximate value expanded node propagated ancestors
via updateancestors function  line     planning phase conducted
terminating condition met  e g  planning time available  optimal action
found  
execution phase algorithm executes best action found planning
 line     gets new observation environment  line      next  algorithm
updates current belief state search tree according recent action
observation z  lines         online approaches reuse previous computations
keeping subtree new belief resuming search subtree
next time step  cases  algorithm keeps nodes tree
new belief bc deletes nodes tree  algorithm loops back
planning phase next time step  task terminated 
side note  online planning algorithm useful improve precision
approximate value function computed offline  captured theorem     
theorem       puterman        hauskrecht        let v approximate value function   supb  v  b  v  b    approximate value v  b  returned dstep lookahead belief b  using v estimate fringe node values  error bounded
 v  b  v  b    
notice         error converges   depth search tends
  indicates online algorithm effectively improve performance obtained
approximate value function computed offline  find action arbitrarily close
optimal current belief  however  evaluating tree reachable beliefs
within depth complexity o   a  z  d  s      exponential d 
becomes quickly intractable large d  furthermore  planning time available
execution may short exploring beliefs depth may infeasible 
   

fiross  pineau  paquet    chaib draa

hence motivates need efficient online algorithms guarantee similar
better error bounds 
efficient  online algorithms focus limiting number reachable beliefs explored tree  or choose relevant ones   approaches
generally differ subroutines choosenextnodetoexpand expand
implemented  classify approaches three categories   branch and bound
pruning  monte carlo sampling heuristic search  present survey
approaches discuss strengths drawbacks  online algorithms
proceed via tree search  approaches discussed section     
    branch and bound pruning
branch and bound pruning general search technique used prune nodes
known suboptimal search tree  thus preventing expansion unnecessary
lower nodes  achieve and or tree  lower bound upper bound
maintained value q  b  a  action a  every belief b tree 
bounds computed first evaluating lower upper bound fringe nodes
tree  bounds propagated parent nodes according following
equations 

l b  
b f t  
lt  b   
    
maxaa lt  b  a   otherwise
x
lt  b  a    rb  b  a   
pr z b  a lt    b  a  z   
    
zz



u  b  
b f t  
maxaa ut  b  a   otherwise
x
ut  b  a    rb  b  a   
pr z b  a ut    b  a  z   
ut  b   

    
    

zz

f t   denotes set fringe nodes tree   ut  b  lt  b  represent upper
lower bounds v  b  associated belief state b tree   ut  b  a  lt  b  a 
represent corresponding bounds q  b  a   l b  u  b  bounds used fringe
nodes  typically computed offline  equations equivalent bellmans equation
 equation     however use lower upper bounds children  instead v  
several techniques presented section     used quickly compute lower bounds
 blind policy  upper bounds  mdp  qmdp  fib  offline 
given bounds  idea behind branch and bound pruning relatively simple 
given action belief b upper bound ut  b  a  lower another action
lower bound lt  b  a   know guaranteed value q  b  a  q  b  a  
thus suboptimal belief b  hence branch pruned belief reached
taking action b considered 
      rtbss
real time belief space search  rtbss  algorithm uses branch and bound approach
compute best action take current belief  paquet et al                starting
   

fionline planning algorithms pomdps

   function expand b  d 
inputs  b 
d 
static   
l 
u 

  
  
  
  
  
  
  
  
   
   
   
   
   

belief node want expand 
depth expansion b 
and or tree representing current search tree 
lower bound v  
upper bound v  

   
lt  b  l b 
else
sort actions  a    a            a a    u  b  ai   u  b  aj   j
i 
lt  b 
 a  u  b  ai    
plt  b 
lt  b  ai   rb  b  ai     zz pr z b  ai  expand   b  ai   z     
lt  b  max lt  b   lt  b  ai   
ii  
end
end
return lt  b 

algorithm      expand subroutine rtbss 
current belief  expands and or tree depth first search fashion 
pre determined search depth d  leaves tree evaluated using lower
bound computed offline  propagated upwards lower bound maintained
node tree 
limit number nodes explored  branch and bound pruning used along way
prune actions known suboptimal  thus excluding unnecessary nodes
actions  maximize pruning  rtbss expands actions descending order
upper bound  first action expanded one highest upper bound   expanding
actions order  one never expands action could pruned actions
expanded different order  intuitively  action higher upper bound
actions  cannot pruned actions since lower
bound never exceed upper bound  another advantage expanding actions
descending order upper bound soon find action
pruned  know remaining actions pruned  since upper
bounds necessarily lower  fact rtbss proceeds via depth first search
increases number actions pruned since bounds expanded actions
become precise due search depth 
terms framework algorithm      rtbss requires choosenextnodetoexpand subroutine simply return current belief bc   updateancestors function need perform operation since bc ancestor  root tree
   expand subroutine proceeds via depth first search fixed depth d  using
branch and bound pruning  mentioned above  subroutine detailed algorithm
     expansion performed  planningterminated evaluates true
best action found executed  end time step  tree simply reinitialized
contain new current belief root node 
efficiency rtbss depends largely precision lower upper bounds
computed offline  bounds tight  pruning possible  search
efficient  algorithm unable prune many actions  searching
   

fiross  pineau  paquet    chaib draa

limited short horizons order meet real time constraints  another drawback
rtbss explores observations equally  inefficient since algorithm
could explore parts tree small probability occurring thus
small effect value function  result  number observations large 
algorithm limited exploring short horizon 
final note  since rtbss explores reacheable beliefs within depth  except
reached suboptimal actions   guarantee error bound d step
lookahead  see theorem       therefore  online search directly improves precision
original  offline  value bounds factor   aspect confirmed empirically
different domains rtbss authors combined online search bounds given
various offline algorithms  cases  results showed tremendous improvement
policy given offline algorithm  paquet et al         
    monte carlo sampling
mentioned above  expanding search tree fully large set observations
infeasible except shallow depths  cases  better alternative may sample
subset observations expansion consider beliefs reached sampled
observations  reduces branching factor search allows deeper search
within set planning time  strategy employed monte carlo algorithms 
      mcallester singh
approach presented mcallester singh        adaptation online mdp
algorithm presented kearns  mansour  ng         consists depth limited
search and or tree certain fixed horizon instead exploring
observations action choice  c observations sampled generative model 
probabilities pr z b  a  approximated using observed frequencies sample 
advantage approach sampling observation distribution
pr z b  a  achieved efficiently o log  s    log  z    computing exact
probabilities pr z b  a  o  s     observation z  thus sampling useful
alleviate complexity computing pr z b  a   expense less precise estimate 
nevertheless  samples often sufficient obtain good estimate observations
effect q  b  a   i e  occur high probability 
likely sampled  authors apply belief state factorization boyen
koller        simplify belief state calculations 
implementation algorithm  expand subroutine expands tree
fixed depth d  using monte carlo sampling observations  mentioned  see
algorithm       end time step  tree reinitialized contain
new current belief root 
kearns et al         derive bounds depth number samples c needed
obtain  optimal policy high probability show number samples
required grows exponentially desired accuracy  practice  number samples
required infeasible given realistic online time constraints  however  performance terms
returns usually good even many fewer samples 
   

fionline planning algorithms pomdps

   function expand b  d 
inputs  b 
d 
static   
c 

  
  
  
  
  
  
  
  
   
   
   

belief node want expand 
depth expansion b 
and or tree representing current search tree 
number observations sample 

   
lt  b  maxaa rb  b  a 
else
lt  b 

sample z    z    z          zc   distribution pr z b  a 
p
n  z 
lt  b  a  rb  b  a    zz nz  z    zc expand   b  a  z     
lt  b  max lt  b   lt  b  a  
end
end
return lt  b 

algorithm      expand subroutine mcallester singhs algorithm 

one inconvenience method action pruning done since monte
carlo estimation guaranteed correctly propagate lower  and upper  bound
property tree  article  authors simply approximate value
fringe belief states immediate reward rb  b  a   could improved using
good estimate v computed offline  note approach may difficult
apply domains number actions  a  large  course may
impact performance 
      rollout
another similar online monte carlo approach rollout algorithm  bertsekas   castanon         algorithm requires initial policy  possibly computed offline  
time step  estimates future expected value action  assuming initial policy followed future time steps  executes action highest estimated value 
estimates obtained computing average discounted return obtained
set sampled trajectories depth d  trajectories generated first taking
action evaluated  following initial policy subsequent belief states 
assuming observations sampled generative model  since approach
needs consider different actions root belief node  number actions  a 
influences branching factor first level tree  consequently  generally
scalable mcallester singhs approach  bertsekas castanon       
show enough sampling  resulting policy guaranteed perform least
well initial policy high probability  however  generally requires many sampled
trajectories provide substantial improvement initial policy  furthermore 
initial policy significant impact performance approach  particular 
cases might impossible improve return initial policy changing
immediate action  e g  several steps need changed reach specific subgoal
higher rewards associated   cases  rollout policy never improve
initial policy 
   

fiross  pineau  paquet    chaib draa

   function expand b  d 
inputs  b  belief node want expand 
d  depth expansion b 
static    and or tree representing current search tree 
  set initial policies 
  number trajectories depth sample 

   lt  b 
  
  
  
q  b  a   
  
   
  
b b
  

  
j    
  j
   
q  b  a  q  b  a   
rb  b  a 
   
z sampleobservation b  a 
   
b  b  a  z 
   
 b 
   
end
   
end
    end
    lt  b  a    max q  b  a 
    end

algorithm      expand subroutine parallel rollout algorithm 
address issue relative initial policy  chang  givan  chong       
introduced modified version algorithm  called parallel rollout  case 
algorithm starts set initial policies  algorithm proceeds rollout
initial policies set  value considered immediate action
maximum set initial policies  action highest value executed 
algorithm  policy obtained guaranteed perform least well best
initial policy high probability  given enough samples  parallel rollout handle
domains large number actions observations  perform well
set initial policies contain policies good different regions belief space 
expand subroutine parallel rollout algorithm presented algorithm     
original rollout algorithm bertsekas castanon        algorithm
special case set initial policies contains one policy 
subroutines proceed mcallester singhs algorithm 
    heuristic search
instead using branch and bound pruning monte carlo sampling reduce branching factor search  heuristic search algorithms try focus search relevant reachable beliefs using heuristics select best fringe beliefs node expand 
relevant reachable beliefs ones would allow search algorithm
make good decisions quickly possible  i e  expanding nodes possible 
three different online heuristic search algorithms pomdps
proposed past  satia lave         bi pomdp  washington        aems
 ross   chaib draa         algorithms maintain lower upper bounds
value node tree  using equations          differ
specific heuristic used choose next fringe node expand and or tree 
   

fionline planning algorithms pomdps

first present common subroutines algorithms  discuss different
heuristics 
recalling general framework algorithm      three steps interleaved several
times heuristic search algorithms  first  best fringe node expand  according
heuristic  current search tree found  tree expanded
node  usually one level   finally  ancestor nodes values updated 
values must updated choose next node expand  since heuristic
value usually depends them  general  heuristic search algorithms slightly
computationally expensive standard depth  breadth first search algorithms  due
extra computations needed select best fringe node expand  need
update ancestors iteration  required previous methods using
branch and bound pruning and or monte carlo sampling  complexity extra
steps high  benefit expanding relevant nodes might
outweighed lower number nodes expanded  assuming fixed planning time  
heuristic search algorithms  particular heuristic value associated every fringe
node tree  value indicate important expand node
order improve current solution  iteration algorithm  goal find
fringe node maximizes heuristic value among fringe nodes 
achieved efficiently storing node tree reference best fringe node
expand within subtree  well associated heuristic value  particular 
root node always contains reference best fringe node whole tree 
node expanded  ancestors nodes tree best fringe node
reference  corresponding heuristic value  need updated  updated
efficiently using references heuristic values stored lower nodes via
dynamic programming algorithm  described formally equations        ht  b 
denotes highest heuristic value among fringe nodes subtree b  bt  b 
reference fringe node  ht  b  basic heuristic value associated fringe node b 
ht  b  a  ht  b  a  z  factors weigh basic heuristic value level
tree   example  ht  b  a  z  could pr z b  a  order give higher weight
 and hence favor  fringe nodes reached likely observations 

ht  b 
b f t  

ht  b   

maxaa ht  b  a ht  b  a  otherwise
    
ht  b  a    maxzz ht  b  a  z ht    b  a  z  

b
b f t  

bt  b   
bt  b  atb   otherwise
  
bt  b  a    bt    b  a  zb a
    

ab   argmaxaa ht  b  a ht  b  a 

zb a
  argmaxzz ht  b  a  z ht    b  a  z  
procedure finds fringe node b f t   maximizes overall heuristic value
qdt  b 
ht  bi   ai  ht  bi   ai   zi    bi   ai zi represent ith belief 
ht  bc   b    ht  b  i  
action observation path bc b   dt  b  depth fringe
node b  note ht bt updated ancestor nodes last expanded
node  reusing previously computed values nodes  procedure
   

fiross  pineau  paquet    chaib draa

   function expand b 
inputs  b 
static  bc  
t 
l 
u 

  
  
  
  
  
  
  
  
   
   
   
   
   
   
   
   
   
   
   

or node want expand 
current belief state agent 
and or tree representing current search tree 
lower bound v  
upper bound v  


z z
b   b  a  z 
ut  b    u  b   
lt  b    l b   
ht  b    ht  b   
bt  b    b 
end
p
lt  b  a  rb  b  a    p zz pr z b  a lt    b  a  z  
ut  b  a  rb  b  a    zz pr z b  a ut    b  a  z  

argmax
zb a
zz ht  b  a  z ht    b  a  z  

  


ht  b  a    ht  b  a  zb a  ht    b  a  zb a



bt  b  a  bt    b  a  zb a   
end
lt  b  max maxaa lt  b  a   lt  b  
ut  b  min maxaa ut  b  a   ut  b  


b argmaxaa ht  b  a ht  b  a 
 b   
 h
ht  b  ht  b 
b

b
bt  b  bt  b 
b  

algorithm      expand   expand subroutine heuristic search algorithms 
find best fringe node expand tree time linear depth
tree  versus exponential depth tree exhaustive search fringe
nodes   updates performed expand updateancestors
subroutines  described detail below  iteration 
choosenextnodetoexpand subroutine simply returns reference best fringe
node stored root tree  i e  bt  bc   
expand subroutine used heuristic search methods presented algorithm     
performs one step lookahead fringe node b  main difference respect
previous methods sections         heuristic value best fringe node
expand new nodes computed lines            best leaf node
bs subtree heuristic value computed according equations      
 lines        
updateancestors function presented algorithm      goal function update bounds ancestor nodes  find best fringe node expand
next  starting given or node b    function simply updates recursively ancestor nodes b  bottom up fashion  using equations       update bounds
equations       update reference best fringe expand heuristic value 
notice updateancestors function reuse information already stored
node objects  need recompute  b  a  z   pr z b  a  rb  b  a  
however may need recompute ht  b  a  z  ht  b  a  according new bounds 
depending heuristic defined 
due anytime nature heuristic search algorithms  search usually keeps
going  optimal action found current belief bc   available planning
   

fionline planning algorithms pomdps

   function updateancestors b   

  
  
  
  
  
  
  
  
   
   
   
   
   
   

inputs  b    or node want update ancestors 
static  bc   current belief state agent 
  and or tree representing current search tree 
l  lower bound v  
u   upper bound v  
b     bc
set  b  a  action apin belief b parent node belief node b 
lt  b  a  rb  b  a    p zz pr z b  a lt    b  a  z  
ut  b  a  rb  b  a    zz pr z b  a ut    b  a  z  

argmax
zb a
zz ht  b  a  z ht    b  a  z  
 h    b  a  z   
ht  b  a  ht  b  a  zb a

b a
  
bt  b  a  bt    b  a  zb a
lt  b  maxa  lt  b  a   
ut  b  maxa  ut  b  a   
 
 


b argmaxa  ht  b   ht  b   
 b   
 h
ht  b  ht  b 

b
b
bt  b  bt  b 
b  
b  b
end

algorithm      updateancestors   updates bounds ancestors ancestors
or node
time elapsed   optimal action found whenever ut  bc   lt  bc   lt  bc  
ut  bc   a     a     argmaxaa lt  bc   a   i e  actions pruned  case
optimal action found  
covered basic subroutines  present different heuristics
proposed satia lave         washington        ross chaib draa        
begin introducing useful notation 
given graph structure g  let us denote f g  set fringe nodes g
hg  b  b    set sequences actions observations lead belief node b 
belief node b search graph g  tree   ht  b  b    contain
 
 
single sequence denote hb b
  given sequence h hg  b  b    define
pr hz  b  ha   probability observe whole sequence observations hz h  given
start belief node b perform whole sequence actions ha h  finally 
define pr h b    probability follow entire action observation sequence
h start belief b behave according policy   formally  probabilities
computed follows 
d h 

pr hz  b  ha    



pr hiz  bhi    hia   

    

i  

d h 

pr h b     



pr hiz  bhi    hia   bhi    hia   

    

i  

d h  represents depth h  number actions sequence h   hia denotes
ith action sequence h  hiz ith observation sequence h  bhi belief state
obtained taking first actions observations sequence h b  note
bh    b 
   

fiross  pineau  paquet    chaib draa

      satia lave
approach satia lave        follows heuristic search framework presented
above  main feature approach explore  iteration  fringe node b
current search tree maximizes following term 
bc  b

ht  bc   b    d ht

 

c  b
c  b
pr hbt z
 bc   hbt a
  ut  b  lt  b   

    

b f t   bc root node   intuition behind heuristic simple 
recalling definition v   note weight value v  b  fringe node b
bc  b
c  b
c  b
c  b
sequence optimal
   provided hbt a
 bc   hbt a
v  bc   would exactly d ht   pr hbt z
actions  fringe nodes weight high effect estimate
v  bc    hence one try minimize error nodes first  term
ut  b  lt  b  included since upper bound  unknown  error v  b  lt  b  
thus heuristic focuses search areas tree affect value v  bc  
error possibly large  approach uses branch and bound pruning 
fringe node reached action dominated parent belief b
never going expanded  using notation algorithms         
heuristic implemented defining ht  b   ht  b  a  ht  b  a  z   follows 
ht  b   
ut  b  lt  b  
  ut  b  a    lt  b  
ht  b  a   
  otherwise 
ht  b  a  z    pr z b  a  

    

condition ut  b  a    lt  b  ensures global heuristic value ht  bc   b       
bc  b 
dominated  pruned   guarantees fringe
action sequence ht a
nodes never expanded 
satia laves heuristic focuses search towards beliefs likely
reached future  error large  heuristic likely efficient
domains large number observations  probability distribution
observations concentrated observations  term ut  b  lt  b 
heuristic prevents search unnecessary computations areas tree
already good estimate value function  term efficient
bounds computed offline  u l  sufficiently informative  similarly  node pruning
going efficient u l sufficiently tight  otherwise actions
pruned 
      bi pomdp
washington        proposed slightly different approach inspired ao algorithm
 nilsson         search conducted best solution graph  case
online pomdps  corresponds subtree belief nodes reached
sequences actions maximizing upper bound parent beliefs 
b
set fringe nodes best solution graph g  denote f g  

b
defined formally f g     b f g  h hg  bc   b   pr h b  g         g  b  a     
  argmaxa  ug  b  a    g  b  a      otherwise  ao algorithm simply specifies
   

fionline planning algorithms pomdps

expanding fringe nodes  washington        recommends exploring fringe
node fb g   where g current acyclic search graph  maximizes ug  b  lg  b  
washingtons heuristic implemented defining ht  b   ht  b  a  ht  b  a  z  
follows 
ht  b   
ut  b  lt  b  
    argmaxa  ut  b  a    
ht  b  a   
  otherwise 
ht  b  a  z      

    

heuristic tries guide search towards nodes reachable promising
actions  especially loose bounds values  possibly large error   one
nice property approach expanding fringe nodes best solution graph
way reduce upper bound root node bc   case
satia laves heuristic  however  washingtons heuristic take account
probability pr hz  b  ha    discount factor d h    may end exploring
nodes small probability reached future  thus
little effect value v  bc    hence  may explore relevant nodes
optimizing decision bc   heuristic appropriate upper bound u
computed offline sufficiently informative  actions highest upper bound
would usually tend highest q value  cases  algorithm focus
search actions thus find optimal action quickly
explored actions equally  hand  consider observation
probabilities  approach may scale well large observation sets 
able focus search towards relevant observations 
      aems
ross chaib draa        introduced heuristic combines advantages bipomdp  satia laves heuristic  based theoretical error analysis tree
search pomdps  presented ross et al         
core idea expand tree reduce error v  bc   quickly
possible  achieved expanding fringe node b contributes
error v  bc    exact error contribution et  bc   b  fringe node b bc tree
defined following equation 
bc  b

et  bc   b    d ht

 

pr hbtc  b  bc     v  b  lt  b   

    

expression requires v computed exactly  practice  ross chaibdraa        suggest approximating exact error  v  b  lt  b    ut  b  lt  b   
done satia lave  washington  suggest approximating
policy    b  a  represents probability action optimal
parent belief b  given lower upper bounds tree   particular  ross et al        
considered two possible approximations   first one based uniformity
assumption distribution q values lower upper bounds 
yields 
   

fiross  pineau  paquet    chaib draa

 b  a   

 

 

 b a lt  b  
 u
ut  b a lt  b a 
 

ut  b  a    lt  b  
otherwise 

    

normalization constant sum probabilities  b  a 
actions equals   
second inspired ao bi pomdp  assumes action maximizing
upper bound fact optimal action 

    argmaxa  ut  b  a    
    
 b  a   
  otherwise 
given approximation   aems heuristic explore fringe node b
maximizes 
bc  b
 

ht  bc   b    d ht

pr hbtc  b  bc     ut  b  lt  b   

    

implemented defining ht  b   ht  b  a  ht  b  a  z  follows 
ht  b    ut  b  lt  b  
ht  b  a     b  a  
ht  b  a  z    pr z b  a  

    

refer heuristic aems  defined equation     aems 
defined equation     
let us examine aems combines advantages satia lave 
bi pomdp heuristics  first  aems encourages exploration nodes loose bounds
possibly large error considering term ut  b  lt  b  previous heuristics 
moreover  satia lave  focuses exploration towards belief states likely
encountered future  good two reasons  mentioned before  belief
state low probability occurrence future  limited effect value
v  bc   thus necessary know value precisely  second  exploring highly
probable belief states increases chance able reuse computations
future  hence  aems able deal efficiently large observation sets 
assuming distribution observations concentrated observations  finally 
bi pomdp  aems favors exploration fringe nodes reachable actions
seem likely optimal  according    useful handle large action
sets  focuses search actions look promising  promising actions
optimal  quickly become apparent  work well best
actions highest probabilities   furthermore  possible define
automatically prunes dominated actions ensuring  b  a      whenever
ut  b  a    lt  b   cases  heuristic never choose expand fringe node
reached dominated action 
final note  ross et al         determined sufficient conditions
search algorithm using heuristic guaranteed find  optimal action within finite
time  stated theorem     
   aems  heuristic used policy search algorithm hansen        

   

fionline planning algorithms pomdps

theorem       ross et al         let     bc current belief  tree
parent belief b ut  b  lt  b       b  a        argmaxa  ut  b  a    
aems algorithm guaranteed find  optimal action bc within finite time 
observe theorem possible define many different policies
aems heuristic guaranteed converge  aems  aems 
satisfy condition 
      hsvi
heuristic similar aems  used smith simmons        offline
value iteration algorithm hsvi way pick next belief point perform
 vector backups  main difference hsvi proceeds via greedy search
descends tree root node b    going towards action maximizes
upper bound observation maximizes pr z b  a  u    b  a  z  l   b  a  z   
level  reaches belief b depth  u  b  l b      
heuristic could used online heuristic search algorithm instead stopping
greedy search process reaches fringe node tree selecting node
one expanded next  setting  hsvis heuristic would return greedy
approximation aems  heuristic  may find fringe node actually
bc  b
maximizes d ht   pr hbtc  b  bc     ut  b  lt  b    consider online version
hsvi heuristic empirical study  section     refer extension hsvi bfs 
note complexity greedy search finding best fringe node
via dynamic programming process updates ht bt updateancestors
subroutine 
    alternatives tree search
present two alternative online approaches proceed via lookahead
search belief mdp  online approaches presented far  one problem
learning achieved time  i e  everytime agent encounters belief 
recompute policy starting initial upper lower bounds computed offline 
two online approaches presented next address problem presenting alternative
ways updating initial value functions computed offline performance
agent improves time stores updated values computed time step 
however  argued discussion  section       techniques lead
disadvantages terms memory consumption and or time complexity 
      rtdp bel
alternative approach searching and or graphs rtdp algorithm  barto
et al         adapted solve pomdps geffner bonet        
algorithm  called rtdp bel  learns approximate values belief states visited
successive trials environment  belief state visited  agent evaluates
possible actions estimating expected reward taking action current belief
   

fiross  pineau  paquet    chaib draa

   function onlinepomdpsolver  
static  bc   current belief state agent 
v    initial approximate value function  computed offline  
v   hashtable beliefs approximate value 
k  discretization resolution 

   initialize bc initial belief state v empty hashtable 
   executionterminated  
p
   a  evaluate q bc   a    rb  b  a    zz pr z b  a v  discretize   b  a  z   k  
   argmaxaa q bc   a 
   execute best action bc
   v  discretize bc   k   q bc   a 
   perceive new observation z
   bc  bc   a  z 
    end

algorithm      rtdp bel algorithm 
state b approximate q value equation 
x
q b  a    rb  b  a   
pr z b  a v    b  a  z   

    

zz

v  b  value learned belief b 
belief state b value table  initialized heuristic value 
authors suggest using mdp approximation initial value belief state 
agent executes action returned greatest q b  a  value  afterwards 
value v  b  table updated q b  a  value best action  finally 
agent executes chosen action makes new observation  ending new
belief state  process repeated new belief 
rtdp bel algorithm learns heuristic value belief state visited 
maintain estimated value belief state memory  needs discretize
belief state space finite number belief states  allows generalization
value function unseen belief states  however  might difficult find best
discretization given problem  practice  algorithm needs substantial amounts
memory  greater  gb cases  store learned belief state values 
especially pomdps large state spaces  implementation rtdp bel
algorithm presented algorithm     
function discretize b  k  returns discretized belief b  b   s    round kb s   k
states s  v  b  looks value belief b hashtable  b present
hashtable  value v   b  returned v   supported experimental data  geffner
bonet        suggest choosing k            usually produces best results 
notice discretization resolution k o  k      s    possible discretized
beliefs  implies memory storage required maintain v exponential  s  
becomes quickly intractable  even mid size problems  furthermore  learning good
estimates exponentially large number beliefs usually requires large number
trials  might infeasible practice  technique sometimes applied
large domains factorized representation available  cases  belief
maintained set distributions  one subset conditionaly independent state
variables  discretization applied seperately distribution  greatly
reduce possible number discretized beliefs 
   

fionline planning algorithms pomdps

algorithm
rtbss
mcallester
rollout
satia lave
washington
aems
hsvi bfs
rtdp bel
sovi

 optimal
yes
high probability

yes
acyclic graph
yes
yes

yes

anytime



yes
yes
yes
yes

yes

branch  
bound
yes


yes
implicit
implicit
implicit



monte
carlo

yes
yes







heuristic



yes
yes
yes
yes



learning







yes
yes

table    properties various online methods 

      sovi
recent online approach  called sovi  shani et al          extends hsvi  smith  
simmons              online value iteration algorithm  approach maintains
priority queue belief states encountered execution proceeds
 vector updates current belief state k belief states highest priority
time step  priority belief state computed according much value
function changed successor belief states  since last time updated  authors
propose improvements hsvi algorithm improve scalability 
efficient  vector pruning technique  avoiding use linear programs update
evaluate upper bound  main drawback approach hardly applicable
large environments short real time constraints  since needs perform value
iteration update  vectors online  high complexity number
 vectors representing value function increases  i e  o k s  a  z   s     t     
compute   
    summary online pomdp algorithms
summary  see online pomdp approaches based lookahead search 
improve scalability  different techniques used  branch and bound pruning  search
heuristics  monte carlo sampling  techniques reduce complexity different angles  branch and bound pruning lowers complexity related action space
size  monte carlo sampling used lower complexity related observation space size  could potentially used reduce complexity related
action space size  by sampling subset actions   search heuristics lower complexity
related actions observations orienting search towards relevant actions observations  appropriate  factored pomdp representations used
reduce complexity related state  summary different properties
online algorithm presented table   
   

fiross  pineau  paquet    chaib draa

   empirical study
section  compare several online approaches two domains found pomdp
literature  tag  pineau et al         rocksample  smith   simmons         consider modified version rocksample  called fieldvisionrocksample  ross   chaib draa 
       higher observation space original rocksample  environment
introduced means test compare different algorithms environments
large observation spaces 
    methodology
environment  first compare real time performance different heuristics
presented section     limiting planning time   second per action  heuristics
given lower upper bounds results would comparable 
objective evaluate search heuristic efficient different types
environments  end  implemented different search heuristics  satia
lave  bi pomdp  hsvi bfs aems  best first search algorithm 
directly measure efficiency heuristic itself  results obtained
different lower bounds  blind pbvi  verify choice affects heuristics
efficiency  finally  compare online offline times affect performance
approach  except stated otherwise  experiments run intel xeon    
ghz  gb ram  processes limited  gb ram 
      metrics compare online approaches
compare performance first foremost terms average discounted return execution time  however  really seek online approaches guarantee better
solution quality provided original bounds  words  seek
reduce error original bounds much possible  suggests good
metric efficiency online algorithms compare improvement terms
error bounds current belief online search  hence  define
error bound reduction percentage be 
ut  b  lt  b 
 
    
u  b  l b 
ut  b   lt  b   u  b  l b  defined section      best online algorithm
provide highest error bound reduction percentage  given initial bounds
real time constraint 
ebr metric necessarily reflect true error reduction  compare return guarantees provided algorithm  i e  lower bounds expected
return provided computed policies current belief  improvement
lower bound compared initial lower bound computed offline direct indicator
true error reduction  best online algorithm provide greatest lower bound
improvement current belief  given initial bounds real time constraint 
formally  define lower bound improvement be 
ebr b     

lbi b    lt  b  l b  
   

    

fionline planning algorithms pomdps

experiments  ebr lbi metrics evaluated time step
current belief  interested seeing approach provides highest ebr
lbi average 
consider metrics pertaining complexity efficiency  particular 
report average number belief nodes maintained search tree  methods
lower complexity generally able maintain bigger trees  results
show always relate higher error bound reduction returns 
measure efficiency reusing part search tree recording percentage
belief nodes reused one time step next 
    tag
tag initially introduced pineau et al          environment
used recently work several authors  poupart   boutilier        vlassis  
spaan        pineau        spaan   vlassis        smith   simmons        braziunas  
boutilier        spaan   vlassis        smith   simmons         environment 
approximate pomdp algorithm necessary large size      states    actions
   observations   tag environment consists agent catch  tag 
another agent moving    cell grid domain  reader referred work
pineau et al         full description domain  note results presented
below  belief state represented factored form  domain exact
factorization possible 
obtain results tag  run algorithm starting configuration   times 
  i e    runs     different starting joint positions  excluding    terminal
states    initial belief state runs consists uniform distribution
possible joint agent positions 
table   compares different heuristics presenting     confidence intervals
average discounted return per run  return   average error bound reduction percentage per
time step  ebr   average lower bound improvement per time step  lbi   average belief
nodes search tree per time step  belief nodes   average percentage belief nodes
reused per time step  nodes reused   average online planning time used per time step
 online time   cases  use fib upper bound blind lower bound  note
average online time slightly lower   second per step algorithms
sometimes find  optimal solutions less second 
observe efficiency hsvi bfs  bi pomdp aems  differs slightly
environment outperform three heuristics  rtbss  satia
lave  aems   difference explained fact latter three
methods restrict search best solution graph  consequence 
explore many irrelevant nodes  shown lower error bound reduction percentage 
lower bound improvement  nodes reused  poor reuse percentage explains
satia lave  aems  limited lower number belief nodes search
tree  compared methods reached averages around   k  results
three heuristics differ much three heuristics differ
way choose observations explore search  since two observations
possible first action observation  one observations leads directly
   

fiross  pineau  paquet    chaib draa

heuristic
rtbss   
satia lave
aems 
hsvi bfs
bi pomdp
aems 

return
           
          
          
          
          
          

ebr    
        
        
        
        
        
        

lbi
         
         
         
         
         
         

belief
nodes
         
         
         
         
          
          

nodes
reused    
 
        
        
        
        
        

online
time  ms 
     
     
     
     
     
     

table    comparison different search heuristics tag environment using blind
policy lower bound 

exit



figure    rocksample      
terminal belief state  possibility heuristics differed significantly
limited  due limitation tag domain  compare online algorithms
larger complex domain  rocksample 
    rocksample
rocksample problem originally presented smith simmons        
domain  agent explore environment sample rocks  see figure    
similarly real robot would planet mars  agent receives rewards
sampling rocks leaving environment  at extreme right environment  
rock scientific value not  agent sample good rocks 
define rocksample n  k  instance rocksample problem n n
grid k rocks  state characterized k     variables  xp   defines position
robot take values                           n  n   k variables  x r xkr  
representing rock  take values  good  bad  
agent perform k     actions   n orth  south  east  w est  sample  check           
checkk    four motion actions deterministic  sample action samples
rock agents current location  checki action returns noisy observation
 good  bad  rock i 
belief state represented factored form known position set k
probabilities  namely probability rock good  since observation rock
   

fionline planning algorithms pomdps

heuristic
satia lave
aems 
rtbss   
bi pomdp
hsvi bfs
aems 
aems 
satia lave
rtbss   
bi pomdp
aems 
hsvi bfs

belief
nodes
ebr    
lbi
nodes
reused    
blind  return               time  s
      
      
  
     
      
          
         
         
     
         
          
         
         
     
  
          
        
         
       
        
          
        
         
       
        
          
        
         
        
        
pbvi  return        b                 time     s
          
        
         
       
        
          
        
         
       
        
          
        
         
     
  
          
        
         
       
        
          
        
         
       
        
          
        
         
       
        
return

online
time  ms 
   
   
   
   
   
   








 
 
 
 
 
 

   
   
   
   
   
   








 
 
 
 
 
 

table    comparison different search heuristics rocksample      environment  using
blind policy pbvi lower bound 

state independent rock states  it depends known robot position  
complexity computing pr z b  a   b  a  z  greatly reduced  effectively 
computation pr z b  checki   reduces to  pr z b  checki     pr accurate xp   checki  
pr xir   z       pr accurate xp   checki       pr xir   z    probability
   xp  i 
   xp   i   
sensor accurate rock i  pr accurate xp   checki    
 
d x
p  i  d 
 
  d xp   i  euclidean distance position xp position rock i 
d  constant specifying half efficiency distance  pr xir   z  obtained directly
probability  stored b  rock good  similarly   b  a  z  computed
quite easily move actions deterministically affect variable xp   checki action
changes probability associated xir according sensors accuracy 
obtain results rocksample  run algorithm starting rock configuration    times  i e     runs  k different joint rock states   initial
belief state runs consists     rock good  plus
known initial robot position 
      real time performance online search
table    present     confidence intervals mean metrics interest 
rocksample             states     actions    observations   real time contraints  
second per action  compare performance using two different lower bounds  blind
policy pbvi  use qmdp upper bound cases  performance
policy defined lower bound shown comparison header  rtbss 
notation rtbss k  indicates k step lookahead  use depth k yields average
online time closest   second per action 
return terms return  first observe aems  hsvi bfs heuristics
obtain similar results  obtains highest return slight margin
one lower bounds  bi pomdp obtains similar return combined
   

fiross  pineau  paquet    chaib draa

pbvi lower bound  performs much worse blind lower bound  two
heuristics  satia lave  aems   perform considerably worse terms return
either lower bound 
ebr lbi terms error bound reduction lower bound improvement  aems 
obtains best results lower bounds  hsvi bfs close second  indicates aems  effectively reduce true error heuristics 
therefore  guarantees better performance  bi pomdp tends less efficient
aems  hsvi bfs  significantly better rtbss  satia lave 
aems   slightly improve bounds case  satia lave unable
increase blind lower bound  explains obtains return
blind policy  observe higher error bound reduction lower bound
improvement  higher average discounted return usually is  confirms intuition guiding search minimize error current belief bc good
strategy obtain better return 
nodes reused terms percentage nodes reused  aems  hvsi bfs
generally obtain best scores  allows algorithms maintain higher number
nodes trees  could partly explain outperform
heuristics terms return  error bound reduction lower bound improvement  note
rtbss reuse node tree algorithm store
tree memory  consequence  reuse percentage always   
online time finally  observe aems  requires less average online time per
action algorithms attain performance  general  lower average
online time means heuristic efficient finding  optimal actions small amount
time  running time rtbss determined chosen depth  cannot stop
completing full lookahead search 
summary overall  see aems  hsvi bfs obtain similar results  however
aems  seems slightly better hsvi bfs  provides better performance guarantees
 lower error  within shorter period time  difference significant 
may due small number observations environment  case two
heuristics expand tree similar ways  next section  explore domain
many observations evaluate impact factor 
lower performances three heuristics explained various reasons 
case bi pomdp  due fact take account
observation probabilities pr z b  a  discount factor heuristic value  hence
tend expand fringe nodes affect significantly value current
belief  satia lave  poor performance case blind policy
explained fact fringe nodes maximize heuristic always leaves
reached sequence move actions  due deterministic nature move actions
 pr z b  a      actions  whereas check actions pr z b  a        initially  
heuristic value fringe nodes reached move actions much higher error
reduced significantly  result  algorithm never explores nodes check
actions  robot always follows blind policy  moving east  never checking
sampling rocks   demonstrates importance restricting choice
   

fionline planning algorithms pomdps

  

  

v b  

  

  

aems 
aems 
bipomdp
hsvibfs
satia

  

   
  

 

  

 

 

  

  

 

  

 

  

time  s 

figure    evolution upper lower bounds rocksample      

leaves explore reached sequence actions maximizing upper bound 
done aems   hsvi bfs bi pomdp  case aems   probably behaves
less efficiently term uses estimate probability certain action
optimal good approximation environment  moreover  aems 
restrict exploration best solution graph  probably suffers  part 
problems satia lave heuristic  rtbss perform
well blind lower bound  due short depth allowed search
tree  required running time   second action  confirms
significantly better exhaustive search good heuristics guide search 
      long term error reduction online heuristic search
compare long term performance different heuristics  let algorithms run
offline mode initial belief state environment  log changes lower
upper bound values initial belief state      seconds  here  initial lower
upper bounds provided blind policy qmdp respectively  see
figure   satia lave  aems  bi pomdp efficient hsvi bfs
aems  reducing error bounds  one interesting thing note
upper bound tends decrease slowly continuously  whereas lower bound often
increases stepwise manner  believe due fact upper bound
much tighter lower bound  observe error bound reduction
happens first seconds search  confirms nodes expanded earlier
tree much impact error bc expanded far
tree  e g  hundreds seconds   important result support using online
 as opposed offline  methods 
   

fi  

  

  

  

average discounted return

average discounted return

ross  pineau  paquet    chaib draa

  
  
aems 
hsvibfs
bipomdp

  
  
  
 
   
  

 

  

  
  

  
  
 
   
  

 

  

aems    blind
aems    pbvi   
aems    pbvi    

  

online time  s 

 

  

 

  

online time  s 

figure    comparison return figure    comparison return
function online time
function online time
rocksample        different
rocksample        different
online methods 
offline lower bounds 

      influence offline online time
compare performance online approaches influenced available
online offline times  allows us verify particular method better
available online time shorter  or longer   whether increasing offline time could
beneficial 
consider three approaches shown best overall performance far  bipomdp  hsvi bfs aems   compare average discounted return function online time constraint per action  experiments run rocksample       
         states     actions    observations  following online time constraints 
   s     s     s   s   s   s   s  vary offline time  used   different lower
bounds  blind policy  pbvi   belief points  pbvi    belief points  taking
respectively   s    s     s  upper bound used qmdp cases  results
obtained intel xeon     ghz processor 
figure    observe aems  fares significantly better hsvi bfs
bi pomdp short time constraints  time constraint increases  aems 
hsvi bfs performs similarly  no significant statistical difference   notice
performance bi pomdp stops improving   second planning time 
explained fact take account observation probabilities
pr z b  a   discount factor  search tree grows bigger  fringe
nodes small probability reached future  becomes
important take probabilities account order improve performance 
otherwise  observe case bi pomdp  expanded nodes affect
quality solution found 
figure    observe increasing offline time beneficial effect mostly
short real time constraints  online planning time available 
   

fionline planning algorithms pomdps

difference performances aems  blind lower bound  aems 
pbvi becomes insignificant  however  online time constraints smaller one
second  difference performance large  intuitively  short real time
constraints algorithm enough time expand lot nodes 
policy found relies much bounds computed offline  hand 
longer time constraints  algorithm enough time significantly improve bounds
computed offline  thus policy found rely much offline bounds 
    fieldvisionrocksample
seems results presented thus far hsvi bfs aems  comparable
performance standard domains  note however environments
small observation sets  assuming observations zero probability removed  
believe aems  especially well suited domains large observation spaces  however 
standard problems literature  therefore consider modified
version rocksample environment  called fieldvisionrocksample  ross   chaib draa 
       observation space size exponential number rocks 
fieldvisionrocksample  fvrs  problem differs rocksample problem
way robot able perceive rocks environment  recall
rocksample  agent check action specific rock observe state
noisy sensor  fvrs  robot observes state rocks 
noisy sensor  action conducted environment  consequently 
eliminates use check actions  remaining actions robot include
four move actions  north  east  south  west  sample action  robot
perceive rock either good bad  thus observation space size  k
instance problem k rocks  rocksample  efficiency sensor
defined parameter    d d    distance rock d 
half efficiency distance  assume sensors observations independent rock 
fvrs  partial observability environment directly proportional
parameter d    d  increases  sensor becomes accurate uncertainty
state environment decreases  value d  defined different instances
rocksample work smith simmons        high fvrs problem
 especially bigger instances rocksample   making almost completely observable 
consequently  re define value d  different instances fieldvisionrocksample according size grid  n   considering fact
p n n grid 
largest possible distance rock robot  n         seems reasonable distance  probability observing real state rock
close    
p problem remain partially observable  consequently  define
d     n          
obtain results fvrs domain  run algorithm starting rock
configurations    times  i e     runs  k different joint rock states  
initial belief state runs corresponds probability    
rock good  well known initial position robot 
   

fiross  pineau  paquet    chaib draa

heuristic
rtbss   
aems 
satia lave
hsvi bfs
aems 
bi pomdp
rtbss   
bi pomdp
satia lave
aems 
aems 
hsvi bfs

belief
nodes
return
ebr    
lbi
nodes
reused    
fvrs       blind  return               time    ms 
          
        
         
         
  
          
        
         
        
         
          
        
         
        
         
          
        
         
        
         
          
        
         
         
         
          
        
         
         
         
fvrs       blind  return               time    ms 
          
         
         
     
  
          
        
         
       
         
          
        
         
       
         
          
        
         
       
         
          
        
         
       
         
          
        
         
       
         

online
time  ms 
       
     
     
      
      
      
   
   
   
   
   
   








 
 
 
 
 
 

table    comparison different search heuristics different instances fieldvisionrocksample environment 

      real time performance online search
table    present     confidence intervals mean metrics interest 
consider two instances environment  fvrs           states    actions     observations  fvrs            states    actions      observations   cases  use
qmdp upper bound blind lower bound  real time constraints   second per
action 
return terms return  observe clear winner  bi pomdp performs surpringly well fvrs      significantly worse aems  hsvi bfs fvrs      
hand  aems  significantly better hsvi bfs fvrs     
get similar performances fvrs       satia lave performs better environment rocksample  likely due fact transitions belief
space longer deterministic  as case move actions rocksample  
fvrs       observe even rtbss given   seconds per action
perform two step lookahead  performance worse heuristic search
methods  clearly shows expanding observations equally search
good strategy  many observations negligible impact current
decision 
ebr lbi terms error bound reduction lower bound improvement  observe aems  performs much better hsvi bfs fvrs       significantly
better fvrs       hand  bi pomdp obtains similar results aems 
fvrs      significantly worse terms ebr lbi fvrs      
suggests aems  consistently effective reducing error  even environments
large branching factors 
nodes reused percentage belief nodes reused much lower fvrs due
much higher branching factor  observe hsvi bfs best reuse percentage
   

fionline planning algorithms pomdps

  

  

  
  

  
  

  
  

  
aems 
aems 
bipomdp
hsvibfs
satia

v b  

v b  

  

  

aems 
aems 
bipomdp
hsvibfs
satia

  

  
  

  

 
   
  

 

  

 

 

  

  

 

  

   
  

 

  

time  s 

 

  

 

  
time  s 

 

  

 

  

figure    evolution upper lower figure    evolution upper lower
bounds fieldvisionrocksambounds fieldvisionrocksample      
ple      

environments  however significantly higher aems   methods
reuse significantly larger portion tree methods  confirms
two methods able guide search towards likely beliefs 
      long term error reduction online heuristic search
overall  table   confirms consistent performance hsvi bfs aems  
difference heuristics modest  considering complexity environment 
may due fact algorithms enough time expand
significant number nodes within   second  long term analysis bounds evolution
figures     confirms this  observe figures lower bound converges
slightly rapidly aems  heuristics  aems  heuristic
performs well long run problem  seems second best heuristic 
satia lave far behind  hand  hsvi bfs heuristic far
worse problem rocksample  seems part due fact
heuristic takes time find next node expand others  thus
explores fewer belief states 

   discussion
previous sections presented evaluated several online pomdp algorithms 
discuss important issues arise applying online methods practice  summarize
advantages disadvantages  help researchers decide whether
online algorithms good approach solving given problem 
   

fiross  pineau  paquet    chaib draa

    lower upper bound selection
online algorithms combined many valid lower upper bounds  however 
properties bounds satisfy online search perform efficiently practice  one desired properties lower upper
bound functions

property states b   l b 


p monotone  monotone
maxaa rb  b  a    pzz pr z b  a l   b  a  z   lower bound b   u  b 
maxaa rb  b  a    zz pr z b  a u    b  a  z   upper bound  property
guarantees certain fringe node expanded  lower bound non decreasing
upper bound non increasing  sufficient guarantee error bound
ut  b  lt  b  b non increasing expansion b  error bound
given algorithm value root belief state bc   cannot worse
error bound defined initial bounds given  note however monotonicity
necessary aems converge  optimal solution  shown previous work  ross
et al          boundedness sufficient 
    improving bounds time
mentioned survey online algorithms  one drawback many online approaches store improvements made offline bounds
online search  that  belief state encountered again  computations need performed again  restarting offline bounds  trivial way
improve maintain large hashtable  or database  belief states
improved lower upper bounds previous search  associated new
bounds  however many drawbacks this  first every time want
evaluate lower upper bound fringe belief  search hashtable needs
performed check better bounds available  may require significant
time hashtable large  e g  millions beliefs   furthermore  experiments conducted
rtdp bel large domains  rocksample       shown process
usually runs memory  i e  requires   gb  good performance
achieved requires several thousands episodes performing well  paquet        
authors rtbss tried combining search algorithm rtdpbel preserve improvements made search  paquet        
combination usually performed better learned faster rtdp bel alone  found
domains  thousand episodes still required improvement
seen  in terms return   hence  point updates offline bounds tend
useful large domains task accomplish repeated large number
times 
better strategy improve lower bound might save time perform
 vector updates beliefs expanded search  offline
lower bound improves time  updates advantage improving lower
bound whole belief space  instead single belief state  however
time consuming  especially large domains  hence  need act within short
time constraints  approach infeasible  however several seconds planning time
available per action  might advantageous use time perform
 vector updates  rather use available time search tree  good
   

fionline planning algorithms pomdps

idea would perform  vector updates subset beliefs search tree 
lower bound improves 
    factored pomdp representations
efficiency online algorithms relies heavily ability quickly compute  b  a  z 
pr z b  a   must computed evey belief state search tree  using
factored pomdp representations effective way reduce time complexity computing quantities  since environments large state spaces structured
described sets features  obtaining factored representation complex systems
issue cases  however  domains significant dependencies
state features  may useful use algorithms proposed boyen koller
       poupart        find approximate factored representations features
independent  minimal degradation solution quality  upper
lower bounds might hold anymore computed approximate factored
representation  usually may still yield good results practice 
    handling graph structure
mentioned before  general tree search algorithm used online algorithms
duplicate belief states whenever multiple paths leading posterior
belief current belief bc   greatly simplifies complexity related updating
values ancestor nodes  reduces complexity related finding
best fringe node expand  using technique section     valid
trees   disadvantage using tree structure inevitably  computations
redundant  algorithm potentially expand subtree every
duplicate belief  avoid this  could use lao algorithm proposed hansen
zilberstein        extension ao handle generic graph structure  including
cyclic graphs  expansion  runs value  or policy  iteration algorithm
convergence among ancestor nodes order update values 
heuristics surveyed section     generalized guide best first search
algorithms handle graph structure  lao   first thing notice that 
graph  fringe node reached multiple paths  error contributes multiple
times error value bc   error contribution perspective  heuristic
value fringe node sum heuristic values paths reaching
it  instance  case aems heuristic  using notation defined
section      global heuristic value given fringe node b  current belief state
bc graph g  computed follows 
hg  bc   b     u  b  l b  

x

d h  pr h bc   g   

    

hhg  bc  b 

notice cyclic graphs  infinitely many paths hg  bc   b  
case  could use dynamic programming estimate heuristic value 
solving hg  bc   b  fringe nodes b graph g require lot time
practice  especially many fringe nodes  experimented
method section    however  would practical use heuristic could find
   

fiross  pineau  paquet    chaib draa

alternative way determine best fringe node without computing hg  bc   b  separately
fringe node b performing exhaustive search fringe nodes 
    online vs  offline time
one important aspect determining efficiency applicability online algorithms
amount time available execution planning  course often taskdependent  real time problems robot navigation  amount time may
short  e g        second per action  hand tasks portfolio
management  acting every second necessary  several minutes could easily
taken plan stock buying selling action  seen experiments 
shorter available online planning time  greater importance good
offline value function start with  case  often necessary reserve sufficient
time compute good offline policy  planning time available online 
influence offline value function becomes negligible  rough offline
value function sufficient obtain good performance  best trade off online
offline time often depends large problem is  branching factor
  a  z   large and or computing successor belief states takes long time  online
time required achieve significant improvement offline value function 
however  small problems  online time     second per action may sufficient
perform near optimally even rough offline value function 
    advantages disadvantages online algorithms
discuss advantages disadvantages online planning algorithms general 
      advantages
online algorithms combined offline solving algorithm  assuming
provides lower bound upper bound v   improve quality
policy found offline 
online algorithms require little offline computation executable
environment  perform well even using loose bounds  quick
compute 
online methods exploit knowledge current belief focus computation
relevant future beliefs current decision  scale well
large action observation spaces 
anytime online methods applicable real time environments 
stopped whenever planning time runs out  still provide best solution found
far 
      disadvantages
branching factor depends number actions observations  thus
many observations and or actions  might impossible search deep
   

fionline planning algorithms pomdps

enough  provide significant improvement offline policy  cases  sampling methods designed reduce branching factor could useful 
cannot guarantee lower upper bounds still valid sampling
used  guarantee valid high probability  given enough
samples drawn 
online algorithms store improvements made offline policy
online search  algorithm plan bounds time
environment restarted  time available  could advantageous add
 vector updates belief states explored tree  offline bounds
improve time 

   conclusion
pomdps provide rich elegant framework planning stochastic partially observable domains  however time complexity major issue preventing
application complex real world systems  paper thoroughly surveys various existing online algorithms key techniques approximations used solve pomdps
efficiently  empirically compare online approaches several pomdp domains different metrics  average discounted return  average error bound reduction
average lower bound improvement  using different lower upper bounds  pbvi 
blind  fib qmdp 
empirical results  observe heuristic search methods  namely
aems  hsvi bfs  obtain good performances  even domains large branching factors large state spaces  two methods similar perform well
orient search towards nodes improve current approximate
value function quickly possible  i e  belief nodes largest error
likely reached future promising actions  however  environments
large branching factors  may time expand nodes turn 
hence  would interesting develop approximations reduce branching
factor cases 
conclusion  believe online approaches important role play
improving scalability pomdp solution methods  good example succesful
applications rtbss algorithm robocuprescue simulation paquet et al 
        environment challenging state space orders magnitude
beyond scope current algorithms  offline algorithms remain important obtain
tight lower upper bounds value function  interesting question whether
online offline approaches better  improve kinds approaches 
synergy exploited solve complex real world problems 

acknowledgments
research supported natural sciences engineering council canada
fonds quebecois de la recherche sur la nature et les technologies  would
thank anonymous reviewers helpful comments suggestions 
   

fiross  pineau  paquet    chaib draa

references
astrom  k  j          optimal control markov decision processes incomplete state
estimation  journal mathematical analysis applications             
barto  a  g   bradtke  s  j     singhe  s  p          learning act using real time dynamic
programming  artificial intelligence                
bellman  r          dynamic programming  princeton university press  princeton  nj 
usa 
bertsekas  d  p     castanon  d  a          rollout algorithms stochastic scheduling
problems  journal heuristics               
boyen  x     koller  d          tractable inference complex stochastic processes 
proceedings fourteenth conference uncertainty artificial intelligence
 uai      pp       
braziunas  d     boutilier  c          stochastic local search pomdp controllers 
nineteenth national conference artificial intelligence  aaai      pp         
cassandra  a   littman  m  l     zhang  n  l          incremental pruning  simple  fast 
exact method partially observable markov decision processes  proceedings
thirteenth conference uncertainty artificial intelligence  uai      pp       
chang  h  s   givan  r     chong  e  k  p          parallel rollout online solution
partially observable markov decision processes  discrete event dynamic systems 
               
geffner  h     bonet  b          solving large pomdps using real time dynamic programming  proceedings fall aaai symposium pomdps  pp       
hansen  e  a          solving pomdps searching policy space  fourteenth conference uncertainty artificial intelligence  uai      pp         
hansen  e  a     zilberstein  s          lao     heuristic search algorithm finds
solutions loops  artificial intelligence                  
hauskrecht  m          value function approximations partially observable markov
decision processes  journal artificial intelligence research           
kaelbling  l  p   littman  m  l     cassandra  a  r          planning acting
partially observable stochastic domains  artificial intelligence             
kearns  m  j   mansour  y     ng  a  y          sparse sampling algorithm nearoptimal planning large markov decision processes  proceedings sixteenth
international joint conference artificial intelligence  ijcai      pp           
koenig  s          agent centered search  ai magazine                 
littman  m  l          algorithms sequential decision making  ph d  thesis  brown
university 
littman  m  l   cassandra  a  r     kaelbling  l  p          learning policies partially observable environments  scaling up  proceedings   th international
conference machine learning  icml      pp         
   

fionline planning algorithms pomdps

lovejoy  w  s          computationally feasible bounds pomdps  operations research 
               
madani  o   hanks  s     condon  a          undecidability probabilistic planning
infinite horizon partially observable markov decision problems  proceedings
sixteenth national conference artificial intelligence   aaai      pp         
mcallester  d     singh  s          approximate planning factored pomdps using belief state simplification  proceedings   th annual conference uncertainty
artificial intelligence  uai      pp         
monahan  g  e          survey partially observable markov decision processes  theory 
models algorithms  management science              
nilsson  n          principles artificial intelligence  tioga publishing 
papadimitriou  c     tsitsiklis  j  n          complexity markov decision processes 
mathematics operations research                 
paquet  s          distributed decision making task coordination dynamic  uncertain real time multiagent environments  ph d  thesis  laval university 
paquet  s   chaib draa  b     ross  s          hybrid pomdp algorithms  proceedings
workshop multi agent sequential decision making uncertain domains
 msdm      pp         
paquet  s   tobin  l     chaib draa  b          online pomdp algorithm complex
multiagent environments  proceedings fourth international joint conference
autonomous agents multi agent systems  aamas      pp         
pineau  j   gordon  g     thrun  s          point based value iteration  anytime algorithm pomdps  proceedings international joint conference artificial
intelligence  ijcai      pp           
pineau  j   gordon  g     thrun  s          anytime point based approximations large
pomdps  journal artificial intelligence research             
pineau  j          tractable planning uncertainty  exploiting structure  ph d  thesis 
carnegie mellon university 
poupart  p          exploiting structure efficiently solve large scale partially observable
markov decision processes  ph d  thesis  university toronto 
poupart  p     boutilier  c          bounded finite state controllers  advances neural
information processing systems     nips  
puterman  m  l          markov decision processes  discrete stochastic dynamic programming  john wiley   sons  inc 
ross  s     chaib draa  b          aems  anytime online search algorithm approximate policy refinement large pomdps  proceedings   th international
joint conference artificial intelligence  ijcai      pp           
ross  s   pineau  j     chaib draa  b          theoretical analysis heuristic search
methods online pomdps  advances neural information processing systems
    nips  
   

fiross  pineau  paquet    chaib draa

satia  j  k     lave  r  e          markovian decision processes probabilistic observation states  management science              
shani  g   brafman  r     shimony  s          adaptation changing stochastic environments online pomdp policy learning  proceedings workshop
reinforcement learning non stationary environments  ecml       pp       
smallwood  r  d     sondik  e  j          optimal control partially observable
markov processes finite horizon  operations research                   
smith  t     simmons  r          heuristic search value iteration pomdps  proceedings   th conference uncertainty artificial intelligence  uai      pp 
       
smith  t     simmons  r          point based pomdp algorithms  improved analysis
implementation  proceedings   th conference uncertainty artificial
intelligence  uai      pp         
sondik  e  j          optimal control partially observable markov processes  ph d 
thesis  stanford university 
sondik  e  j          optimal control partially observable markov processes
infinite horizon  discounted costs  operations research                 
spaan  m  t  j     vlassis  n          point based pomdp algorithm robot planning 
proceedings ieee international conference robotics automation
 icra      pp           
spaan  m  t  j     vlassis  n          perseus  randomized point based value iteration
pomdps  journal artificial intelligence research             
vlassis  n     spaan  m  t  j          fast point based algorithm pomdps 
benelearn       proceedings annual machine learning conference belgium
netherlands  pp         
washington  r          bi pomdp  bounded  incremental partially observable markov
model planning  proceedings  th european conference planning  pp 
       
zhang  n  l     zhang  w          speeding convergence value iteration partially observable markov decision processes  journal artificial intelligence research 
         

   


