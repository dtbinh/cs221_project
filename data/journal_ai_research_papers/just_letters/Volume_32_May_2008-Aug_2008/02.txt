journal of artificial intelligence research                 

submitted        published      

graphical model inference in optimal control of stochastic
multi agent systems
bart van den broek
wim wiegerinck
bert kappen

b vandenbroek science ru nl
w wiegerinck science ru nl
b kappen science ru nl

snn  radboud university nijmegen  geert grooteplein    
nijmegen  the netherlands

abstract
in this article we consider the issue of optimal control in collaborative multi agent
systems with stochastic dynamics  the agents have a joint task in which they have to
reach a number of target states  the dynamics of the agents contains additive control and
additive noise  and the autonomous part factorizes over the agents  full observation of the
global state is assumed  the goal is to minimize the accumulated joint cost  which consists
of integrated instantaneous costs and a joint end cost  the joint end cost expresses the joint
task of the agents  the instantaneous costs are quadratic in the control and factorize over
the agents  the optimal control is given as a weighted linear combination of single agent
to single target controls  the single agent to single target controls are expressed in terms
of diffusion processes  these controls  when not closed form expressions  are formulated
in terms of path integrals  which are calculated approximately by metropolis hastings
sampling  the weights in the control are interpreted as marginals of a joint distribution
over agent to target assignments  the structure of the latter is represented by a graphical
model  and the marginals are obtained by graphical model inference  exact inference of the
graphical model will break down in large systems  and so approximate inference methods are
needed  we use naive mean field approximation and belief propagation to approximate the
optimal control in systems with linear dynamics  we compare the approximate inference
methods with the exact solution  and we show that they can accurately compute the optimal
control  finally  we demonstrate the control method in multi agent systems with nonlinear
dynamics consisting of up to    agents that have to reach an equal number of target states 

   introduction
the topic of control in multi agent systems is characterized by many issues  originating from
various sources  including a wide variety of possible execution plans  uncertainties in the
interaction with the environment  limited operation time and supporting resources  and a
demand for robustness of the joint performance of the agents  such issues are encountered
in  for example  air traffic management  tomlin  pappas    sastry        van leeuwen 
hesseling    rohling         formation flight  ribichini   frazzoli        hu  prandini 
  tomlin         radar avoidance for unmanned air vehicles or fighter aircraft  pachter  
pachter        kamal  gu    postlethwaite        larson  pachter    mears        shi 
wang  liu  wang    zu         and persistent area denial  subramanian   cruz       
liu  cruz    schumacher        castanon  pachter    chandler        
in many control approaches in multi agent systems  stochastic influences in the dynamics
of the agents are not taken into account or assumed negligible  and the dynamics are
c
    
ai access foundation  all rights reserved 

fivan den broek  wiegerinck   kappen

modeled deterministically  if the system is truly deterministic  then the agents can be
optimally controlled by open loop controls  however  when the stochastic influences in
the dynamics are too large to be ignored  open loop controls become far from optimal 
and the multi agent system should no longer be modeled deterministically 
the usual
approach to control in multi agent systems with stochastic dynamics is to model the system
by a markov decision processes  mdp   boutilier        sadati   elhamifar         in
principle  these are solved in discrete space and time by backward dynamic programming 
however  the discretization will make the joint state space of the multi agent system increase
exponentially in the number of agents  and a basic dynamic programming approach will
generally be infeasible  boutilier         an attempt to overcome this is to exploit structures
in the problem and describe the system by a factored mdp  in general these structures will
not be conserved in the value functions  and exact computations remain exponential in the
system size  guestrin  koller  and parr      a  and guestrin  venkataraman  and koller
     b  assumed a predefined approximate structure of the value functions  and thereby
provided an efficient approximate mdp model for multi agent systems  a similar approach
was taken by becker  zilberstein  lesser  and goldman               assuming independent
collaboration of the agents with a global reward function  resulting in transition independent
decentralized mdps 
in this paper we concentrate on multi agent systems where the agents have a joint task
in which they have to reach a number of target states  we model the multi agent system
in continuous space and time  following the approach of wiegerinck  van den broek  and
kappen         we make the following assumptions  the agents are assumed to have
complete and accurate knowledge of the global state of the system  assumption     the
dynamics of each agent is additive in the control and disturbed by additive wiener noise
 assumption     the performance of the agents is valued by a global cost function  which is
an integral of instantaneous costs plus an end cost  the joint task of the agents is modeled
by the end cost  the instantaneous costs are assumed to be quadratic in the control
 assumption     the noise level in the dynamics of the agents is inversely proportional to
the control cost  assumption     finally  we assume that both the autonomous dynamics
and the instantaneous costs factorize over the agents  assumption    
under the assumptions   and    the optimal control problem is partially solved by finding
the optimal expected cost to go  which satisfies the so called stochastic hamilton jacobibellman  shjb  equation  once the optimal expected cost to go is given  the optimal
control is provided as the gradient of the optimal expected cost to go by adopting assumption    the shjb equation is a nonlinear partial differential equation  pde   and this
nonlinearity makes it difficult to solve  a common approach to solving the shjb equation
is to assume  in addition to assumption    that the instantaneous costs and the end cost
in the cost function are quadratic in the state  and that the dynamics are linear in the
state as wellthis is known as linear quadratic control  the optimal expected cost to go
then is quadratic in the state with time varying coefficients  and the problem reduces to
solving the riccati equations that these coefficients satisfy  stengel        ksendal        
otherwise  approximation methods are needed  an approximate approach is given by the
iterative linear quadratic gaussian method  todorov   li         this yields a locally optimal feedback control  and is valid in case there is little noise  we instead follow the approach
of fleming        and adopt assumption    under this assumption the shjb equation can
  

figraphical model inference in mas optimal control

be transformed into a linear pde by performing a logarithmic transformation  its solution
equals the expectation value of a stochastic integral of a diffusion process  in general  this is
not a closed form expression  in this paper we will estimate this expression by formulating
it as a path integral  kappen      a      b   and we estimate the latter using metropolishastings sampling  there are several other ways to estimate the path integral  such as
hamilton monte carlo sampling and the laplace approximation  but these are not covered
in this paper 
the structure of the optimal expected cost to go will generally be very complex due to
the dynamic couplings between the agents  by adopting assumption    the agents will only
be coupled through the joint end cost  which then solely determines the structure of the
optimal expected cost to go  this will result in state transition probabilities that factorize
over the agents  it follows that the optimal control becomes a weighted combination of
single agent to single target controls  the weights are given by a joint distribution over
agent to target assignments  the joint distribution has the same structure as the joint end
cost  the structure of the joint distribution is representable by a factor graph  and the
optimal control problem becomes a graphical model inference problem  wiegerinck et al  
       the complexity of the graphical model inference is exponential in the tree width
of the factor graph  exact inference will be possible by using the junction tree algorithm 
given that the graph is sufficiently sparse and the number of agents is not too large  in
more complex situations approximate inference methods are necessary  and we show that
the optimal control can accurately be approximated in polynomial time  using naive mean
field  mf  approximation or belief propagation  bp   this makes distributed coordination
possible in multi agent systems that are much larger than those that could be treated with
exact inference 
the paper is organized as follows  in sections   and    we provide a review of both
the single and the multi agent stochastic optimal control framework  developed by kappen      a      b  and wiegerinck et al          as an example  we will rederive linear
quadratic control  the general solution is given in terms of a path integral  and we explain
how it can be approximated with metropolis hastings sampling 
in section    we give a factor graph representation of the end cost function  we discuss two graphical model approximate inference methods  naive mean field approximation
and belief propagation  we show that the approximation of the optimal control in both
methods is obtained by replacing the exact weights in the controls with their respective
approximations 
in section    we present numerical results  we make a comparison of the approximate
optimal controls  infered by the naive mean field approximation  belief propagation and
a greedy method  with the exact optimal control  this we do in a multi agent system of
   agents with linear dynamics in a two dimensional state space  and with two target
states  furthermore  we present results from control in multi agent systems with nonlinear
dynamics and a four dimensional state space  in which agents control their forward velocity
and driving direction  the controls are approximated by a combination of metropolishastings sampling  to infer the path integrals  and naive mean field approximation  to infer
the agent to target assignments  this allowed us to control systems of up to    agents
with    target states  these results regarding nonlinear dynamics have only an illustrative
purpose 
  

fivan den broek  wiegerinck   kappen

   stochastic optimal control of a single agent
we consider an agent in a k dimensional continuous state space rk   its state x t  evolving
over time according to the controlled stochastic differential equation
dx t    b x t   t dt   u x t   t dt   dw t  

   

in accordance with assumptions   and   in the introduction  the control of the agent is the
rk  valued function u of x t  and t  the noise in the dynamics is modeled by the wiener
process w t   i e   a normally distributed k dimensional stochastic process in continuous
time with mean   and variance t  and the k  k matrix  which represents the variance
of the noise  any autonomous dynamics are modeled by b  which is a rk  valued function
of x t  and t  the state change dx t  is the sum of the noisy control and the autonomous
dynamics 
the behavior of the agent is valued by a cost function  given the agents state x t    x
at the present time t  and a control u  there is an expected future cost for the agent 



z t
 
d
c u  x  t    eux t  x t     
kru x     k    v  x       
   
 
t
the expectation eux t is taken with respect to the probability measure under which x t 
is the solution to     given the control law u and the condition x t    x  the cost is a
combination of the end cost  x t     which is a function of the end state x t    and an
integral of instantaneous costs  the instantaneous cost is a sum of a state and a control
dependent term  the state dependent term v  x      is the cost of being in state x  
at time   the function v is arbitrary  and represents the environment of the agent  the
control dependent term    kru x     k  is the cost of the control in state x   at time  
where kzk    z  z is the euclidean norm  and r is a full rank k  k matrix  it is quadratic
in the control  in accordance with assumption   in the introduction  and by assumption   
r is related to the variance of the noise in the control via the relation
     r r    

   

where  is a scalar 
the expected cost to go at time t minimized over all controls u defines the optimal
expected cost to go
j x  t    min c u  x  t  
   
u

in appendix a  it is explained that due to the linear quadratic form of the optimization
problemthe dynamics     is linear in the action u  the cost function     is quadratic in
the actionthe minimization can be performed explicitly  yielding a nonlinear partial differential equation in j  the so called stochastic hamilton jacobi bellman  shjb  equation 
the minimum is attained in
u x  t     r r   x j x  t  

   

this is the optimal control  note that it explicitely depends on the state x of the agent at
time t  making it a feedback control 
  

figraphical model inference in mas optimal control

the optimal expected cost to go can be re expressed in terms of a diffusion process  for
a derivation  we again refer to appendix a  
j x  t     log z x  t 

   

where z x  t  is the expectation value



z
 
  t
z x  t    ex t exp   y t    
d v  y     

 t

   

and y   is a diffusion process with y t    x and satisfying uncontrolled dynamics 
dy     b y     d   dw   

   

substituting relations     and     in      we find the optimal control in terms of z x  t  
u x  t      x log z x  t  

   

example    consider an agent in one dimension with a state x t  described by the dynamical equation     without autonomous dynamics  b       the instantaneous cost v is zero 
and the end cost  is a quadratic function around a target state  
 y   


 y      
 

the diffusion process y   that satisfies the uncontrolled dynamics     is normally distributed
around the agents state x   y t  at time t and with a variance       t   hence the state
transition probability for the agent to go from  x  t  to  y  t   in space time is given by the
gaussian density


 y  x  
 y  t  x  t    p
 
exp   
   t  t 
     t  t 
 

the expectation value     is given by the integral

z x  t   

z

 

 y 

dy y  t  x  t e

 

s



r   
 x    
exp   
 
t  t   r   
   t  t   r    

where relation     is used  the optimal control follows from     and     and reads
u x  t   

x
 
t  t   r   

this result is well known  stengel        
  

    

fivan den broek  wiegerinck   kappen

    a path integral formulation
example   shows that for a simple system with no autonomous dynamics  b      or costs
due to the environment  v       we can write down the control explicitly  this is because
the uncontrolled dynamics is normally distributed  and consequently the expectation value
    with quadratic end cost has a closed form expression  in the general situation where b
and v are arbitrary  there no longer exists an explicit expression for the expectation value 
and the optimal control can only be obtained by approximation  we will now discuss how
this is done by taking a path integral approach  kleinert         a detailed derivation of
the expressions presented here is given in appendix b 
in the path integral approach  we write the expectation value     as a path integral 
z x  t    lim z  x t     t   

    

 

where x t      x  t    t and
 

z  x t     t      p
det      n

z

dx t         

z

 

dx tn   e  s  x t        x tn   t     

it is an integral over paths  x t             x tn    in discrete time  the start x t    kept fixed
and n   t  t  taken in a continuous time limit of sending the length of the time steps
   ti    ti to zero  note that in this limit n goes to infinity and the paths become infinite
dimensional objects  the function in the exponent is the cost of the path 
s  x t             x tn    t     
 x t     

n
 
x

 v  x ti    ti    

n
 
x
i  

i  

 
 

 
x ti      x ti  

 r
 b x ti    ti   
  
 


the optimal control becomes a weighted average over controls that are derived from a single
path 
u x t     t      lim
 

z

dx t         

z

dx tn   p x t             x tn    t    u x t             x tn    t         

the weights are given by
 

p x t             x tn    t      p

e  s  x t        x tn   t   
det      n z  x t     t   

 

the control derived from a path  x t             x tn    reads
u x t             x tn    t     

x t     x t   
 b x t     t    


note that it only depends on the first two entries x t    and x t    in the path 
   

    

figraphical model inference in mas optimal control

    path integration by metropolis hastings sampling
the path integral formulation      of the optimal control can generally not be computed 
because it is an integral over uncountably many paths  but there exist several ways to
approximate it  a natural approach goes by stochastic sampling of paths  several methods
of stochastic sampling exist  the one we will use here is known as metropolis hastings
sampling  hastings         in its implementation time will be discretized  we do not take
the limit in      of  decreasing to zero  but instead keep  at a fixed value  a sample path
will be a sequence  xs  t             xs  tn    of vectors in the state space rk   with x t      x the
current state of the agent at the current time t    t  according to equation       we only
need xs  t    and xs  t    to derive the control from a sample path  x t             x tn     the
metropolis hastings sampling ensures that different paths are properly weighted  hence the
optimal control is approximated as follows 
u x t     t    

hx t   i  x t   
 b x t     t    
t   t 

    

where hx t   i is the mean value of xs  t    taken over the sample paths  pseudo code for the
algorithm is given in algorithm   
algorithm    metropolis hastings sampling
input  initial path  x t             x tn   
   s    
   repeat m times 
   define gaussian proposal distribution centered around  x t             x tn   
with variance equal to the noise
   draw sample path  x  t             x  tn    from proposal distribution

   a   exp   s  x t     x t             x tn    t       s  x t     x  t             x  tn    t   
   if a   
  
set  x t             x tn       x  t             x  tn   
   else
  
set  x t             x tn       x  t             x  tn    with probability a
    end if
     xs  t             xs  tn       x t             x tn   
    s   s    
    end repeat
    compute approximate control with equation     

   stochastic optimal control of a multi agent system
we now turn to the issue of optimally controlling a multi agent system of n agents  in
principle  the theory developed for a single agent straightforwardly generalizes to the multiagent situation  each agent a has a k dimensional state xa that satisfies a dynamics similar
to     
dxa  t    ba  xa  t   t dt   ua  x t   t dt   a dwa  t  
    
in accordance with assumptions      and   in the introduction  note that the control of
each agent not only depends on its own state xa   but on the joint state x    x            xn  
   

fivan den broek  wiegerinck   kappen

of the system  the system has a joint cost function similar to      depending on the joint
state x and joint control u    u            un   of the system 



n z t
x
 
u
u
 
d
c  x  t    ex t  x t     
kra ua  x     k   v  xa        
 
t
a  

the expectation eux t is taken with respect to the probability measure under which x t  is
the solution to      given the control law u and the condition that x t    x  the cost is a
combination of the joint end cost  x t     which is a function of the joint end state x t   
and an integral of instantaneous costs  the instantaneous cost factorizes over the agents 
in accordance with assumption   in the introduction  for each agent  it is a sum of a state
dependent term v  xa       and a control dependent term    kra ua  xa      k    similar to
the single agent case  in accordance with assumption   in the introduction  the control cost
of each agent is related to the noise in the agents dynamics via the relation
a a    ra ra     
where  is the same for each agent  the joint cost function is minimized over the joint
control  yielding the optimal expected cost to go j  the optimal expected cost to go is
expressed in terms of a diffusion process via the relation
j x  t     log z x  t  
where z x  t  is the joint expectation value
  
 
n z
 x t
 
d v  ya      
z x  t    ex t exp   y t    


t

    

a  

and the y   t           yn  t  are diffusion processes  with y    y            yn   and y t    x  satisfying
uncontrolled dynamics
dya      ba  ya      d   a dwa    

a              n 

    

the multi agent equivalent of the optimal control     reads
ua  x  t    a a  xa log z x  t  

    

we will now show that the optimal control of an agent can be understood as an expected
control  that is  an integral over target states ya of a transition probability to the target
times the optimal control to that target  to this end  we write the expectation      as an
integral over the end state 
z
n
y
 
z x  t    dye   y 
za  ya   t   xa   t  
    
a  

where the za  ya   t   xa   t  are implicitly defined by



z
z
  t
d v  ya      
dya za  ya   t   xa   t f  ya     exa  t f  ya  t    exp 
 t
   

figraphical model inference in mas optimal control

for arbitrary functions f   substituting      into      yields
z
ua  x  t    dya pa  ya  x  t  ua  ya   xa   t 

    

where
ua  ya   xa   t    a a  xa log za  ya   t   xa   t 

    

is the optimal control for agent a to go from state xa at the current time t to state ya at
the end time t   and pa  ya  x  t  is a marginal of
n

y
 
 
p y x  t   
za  ya   t   xa   t  
e   y 
z x  t 
a  

    discrete end states
the agents have to fulfill a task of arriving at a number of target states at the end time
according to an initially specified way  for example  they should all arrive at the same
target  or they should all arrive at different targets  the targets are considered regions
g            gm in the state space  and the end cost  is modeled as follows 
 

e   y   

x
s

w s 

n
y

wa  ya   sa   

 

wa  ya   sa     e  a  ya  sa    

    

a  

where the sum runs over assignments s    s            sn   of agents a to regions gsa   a  ya   sa  
is a cost function associated to region gsa   returning a low cost if the end state ya of agent
a lies in the region gsa and a high cost otherwise  w s  is a weight  grading the assignments
s and thereby specifying the joint task of the agents  assignments that result in a better
fulfillment of the task have a higher weight  in a situation where all agents have to go to
the same target  for example  a vector s that assigns each agent to a different target will
have a low weight w s  
with this choice of end cost  equation      factorizes as
z x  t   

x
s

where
za  sa   xa   t   

z

w s 

n
y

za  sa   xa   t 

a  

dya za  ya   t   xa   t wa  ya   sa   

    

the interpretation of za  sa   xa   t  is that  log za  sa   xa   t  is the expected cost for agent
a to move from xa to target sa   the optimal control      of a single agent a becomes
ua  x  t   

m
x

p sa  x  t ua  sa   xa   t  

    

sa   

where
ua  sa   xa   t    a a  xa log za  sa   xa   t 
   

    

fivan den broek  wiegerinck   kappen

is the control for agent a to go to target sa   and the weights p sa  x  t  are the single agent
marginals
x
p s x  t 
    
p sa  x  t   
s sa

of the joint distribution
n

y
 
p s x  t   
za  sa   xa   t  
    
w s 
z x  t 
a  


 
 
the weight p s x 
pnt  equals the ratio exp   j s  x  t    exp   j x  t    where j s  x  t   
 log w s   a    log za  sa   x  t  is the optimal expected cost to go in case the agents
have predetermined targets that are specified by the assignment s  an assignment of agents
to targets that has a low expected cost j s  x  t  will yield a high weight p s x  t   and
the associated single agent to single target controls ua  sa   xa   t  will be predominant in the
optimal controls ua  x  t  
    metropolis hastings sampling in multi agent systems
in general  both the controls ua  sa   xa   t  and the marginals p sa  x  t  in the optimal control      do not have a closed form solution  but have to be inferred approximately  the
controls ua  sa   xa   t  can be approximated by the metropolis hastings sampling discussed
in section      inference of the marginals involves the inference of the path integral formulations of the za  sa   xa   t  
z
z
 
 
za  sa   xa   t    lim p
dxa  t          dxa  tn  e  s  xa  t        xa  tn   t   sa  
 
n
 
det    
with xa  t      xa   t    t and

s xa  t             xa  tn    t    sa     a  xa  t    sa  
 

n
 
x

 v  xa  ti    ti    

i  

n
 
x
i  

 
 

 
xa  ti      xa  ti  

 ra
 ba  xa  ti    ti   
  
 


the value of za  sa   xa   t  is generally hard to determine  mackay         possible approximations include the maximum a posteriori  map  estimate and the inclusion of the variance
in the sample paths  a third approximation is to take the average of the path costs as an
estimate of log za  sa   xa   t   this means that the entropy of the distribution in the path
integral is neglected 

   graphical model inference
the additional computational effort in multi agent control compared to single agent control
lies in the computation of the marginals p sa  x  t  of the joint distribution p s x  t   which
involves a sum over all mn assignments s  for small systems this is feasible  but for large
systems this is only so if the summation can be performed efficiently  an efficient approach
is provided by graphical model inference  which relies on a factor graph representation of
the joint distribution 
   

figraphical model inference in mas optimal control

   

   

 

   

 

   

 

   

 

figure    example of a factor graph for a multi agent system of four agents  the couplings
are represented by the factors a  with a                                          

    a factor graph representation of the joint distribution
the complexity of the joint distribution is in part determined by the weights w s  in the end
cost function       these weights determine how the agents consider the states of the other
agents  in the most complex case  the way one agent takes the state of another agent into
account will depend on the states of all the other agents  the situation is less complicated
when an agent considers the states of some agents independently of the states of the others 
this means that the joint end cost has a factorized form 
y
w s   
wa  sa   
    
a

the a being subsets of agents  this structure is represented graphically by a so called factor
graph  kschischang  frey    loeliger         see figure   for an example  the agents a and
the factors a are nodes in the factor graph  represented by circles and squares respectively 
and there is an edge between an agent a and a factor a when a is a member of subset a 
that is  when wa in the factorization of w depends on sa   from      it is immediate that
the joint distribution p s x  t  factorizes according to the same factor graph 
    the junction tree algorithm
efficient inference of the distribution p s x  t  by means of its factor graph representation is
accomplished by using the junction tree algorithm  lauritzen   spiegelhalter         the
complexity of this algorithm is exponential in the induced tree width of the graph  a small
tree width can be expected in systems where the factor graph is sparse  which is the case
when the agents take the states into account of a limited number of other agents  this
implies that multi agent systems with sparse graphs and a limited number of targets are
tractable  wiegerinck et al          the factor graph in figure   is an example of a sparse
graph  on the other hand  should each agent take the state of each other agent into account 
then the junction tree algorithm does not really help  the underlying factor graph is fully
connected and the tree width of the graph equals the number of agents in the system 
exact computation of the optimal control will be intractable in large and complex multiagent systems  since the junction tree algorithm requires memory exponential in the tree
width of the factor graph  instead we can use graphical model approximate inference
methods to approximately infer the marginals       we will proceed with a discussion of
two such methods  naive mean field  mf  approximation  jordan  ghahramani  jaakkola 
  saul        and belief propagation  bp   kschischang et al         yedidia  freeman   
weiss        
   

fivan den broek  wiegerinck   kappen

    naive mean field approximation
our starting point is to note that the optimal expected cost to go is a log partition sum 
also known as a free energy  consider the variational free energy
x
f  q    h log wiq 
hlog za iqa  h q  
a

where h iq and h iqa denote expectation values with respect to distribution q and marginals
qa respectively  and h q  is the entropy of q 
x
h q    
q s  log q s  
s

the optimal expected cost to go equals the variational free energy minimized over all distributions q  in the naive mean field approximation
one considers the variational free energy
q
restricted to factorized distributions q s    a qa  sa    the minimum
f  q 
jmf   min
q
q 

a qa

is an upper bound for the optimal expected cost to go j  it equals j in case the agents are
uncoupled  f has zero gradient in its local minima  that is 
  

f  q   s       qn  sn   
qa  sa  

a              n 

    

with additional constraints for normalization of the distributions qa   solutions to this set
of equations are implicitly given by the mean field equations
za  sa  hw sa iq
qa  sa     pn


sa    za  sa  hw sa iq

    

where hw sa iq is the conditional expectation of w under q given sa  

x y
qa  sa   w s            sn   
hw sa iq  
s       sn  sa

a   a

the mean field equations are solved by means of iteration  this procedure results in a
convergence to a local minimum of the free energy 
the mean field approximation of the optimal control is found by taking the gradient
with respect to x of the minimum jmf of the free energy  this is similar to the exact case
where the optimal control is the gradient of the optimal expected cost to go  equation      
using       we find
x
 
ua  x  t     a a  xa jmf  x  t   
qa  sa  ua  xa   t  sa   

s
a

similar to the exact case  it is an average of the single agent to single target optimal controls
ua  xa   t  sa   given by equation       where the average is taken with respect to the mean
field approximate marginal qa  sa   of agent a 
   

figraphical model inference in mas optimal control

    belief propagation
in belief propagation  we approximate the free energy by the bethe free energy  and we
minimize the latter  the bethe free energy is defined by
fbethe   qa   qa      

x

h log wa iqa 

a

x

h log za iqa  

a

x

h qa     

a

x

 na    h qa   

a

    
it is a function of beliefs qa  sa   and qa  sa    which are non negative normalized functions
that satisfy consistency relations 
a a  a  

x

qa  sa     qa  sa   

sa a

the h qa   and h qa   are the entropies of the beliefs qa and qa   na denotes the number of
neighbors of node a in the factor graph 
belief propagation is an algorithm that computes the beliefs  kschischang et al         
in case the joint distribution p has a factor graph representation that is a tree  belief propagation will converge to beliefs that are the exact marginals of p  and the bethe free energy
of these beliefs equals the optimal expected cost to go j  if the factor graph representation
of p contains cycles  we may still apply belief propagation  yedidia et al         showed
that the fixed points of the algorithm correspond to local extrema of the bethe free energy 
in particular  more advanced variations on the algorithm  heskes  albers    kappen       
teh   welling        yuille        are guaranteed to converge to local minima of the bethe
free energy  heskes        
we find the bp approximation of the optimal control by taking the gradient of the
minimum jbethe of the bethe free energy 
x
 
ua  x  t     a a  xa jbethe  x  t   
qa  sa  ua  xa   t  sa   

s
a

with the ua  xa   t  sa   given by equation       similar to the exact case and the mean field
approximation  the bp approximation of the optimal control is an average of single agent
single target optimal controls  where the average is taken with respect to the belief qa  sa   

   numerical results
in this section  we present numerical results of simulations of optimal control in multiagent systems  the problem of computing the optimal controls      consists of two parts 
the inference of the single agent to single target controls       and the inference of the
marginals      of the global distribution over agent to target assignments  when the dynamics are linear  and the instantaneous costs v are zero  the single agent to single target
controls can be given in closed form  such multi agent systems therefore only know the issue
of infering marginal distributions  in section     we will consider multi agent systems of
this kind  section     deals with the general problem of infering the optimal controls when
the dynamics are nonlinear and the instantaneous costs v are nonzero  in both sections
   

fiexpected target

van den broek  wiegerinck   kappen

position

 
 
 
 

   

 
time

   

 

 
 
 
 

 a  positions

   

 
time

   

 

 b  expected targets

figure    two agents  with noise and control in their positions  need to reach target locations at    and   at end time t      each agent at a different target location  the
positions  a  and expected targets  b  over time 

the joint end cost is given by equation       with
w s   

n
y
a b

wa b  sa   sb   

 c

wa b  sa   sb     exp  sa  sb  
n

    




 
    
a  ya   sa      ya  sa     
wa  ya   sa     exp  a  ya   sa    

 
where c determines the coupling strength between the agents  and the sa are the target
states 
    linear dynamics
we begin with an illustration of optimal control by showing a simulation of an exactly
solvable stochastic multi agent system  in this system of two agents in one dimension  the
agents satisfy dynamics      with ba equal to zero  there are two target states  x        
and x          the task of the agents is for each one to go to a different target  the
instantaneous costs v in the cost function are zero  and the end cost function is given by
equations            and      with       and c      the negative sign of the coupling
strength c implies a repulsion between the agents  the control cost parameter r equals   
the noise level    lies at      the agents start at x     at time t      the end time lies at
t      to prevent overshooting the targets  udt should be small compared to the distance
to the target states  this is done by choosing dt        t  t         
p
figure   shows the agents positions and expected targets
sa      p sa  x  t sa over
time  we see that up to time t      the agents have not decided to which target each of
them will go  and they remain between the two targets  then  after t      a final decision
seems to have been made  this delayed choice is due to a symmetry breaking in the cost togo as time increases  before the symmetry breaking  it is better to keep options open  and
see what the effect of the noise is  after the symmetry breaking  time is too short to wait
longer and a choice has to be made  this phenomenon is typical for multi modal problems 
we proceed with a quantitative comparison of the different control methods that arise
from the exact or approximate inferences of the marginals of the joint distribution      
   

figraphical model inference in mas optimal control

 

  

 

 

cpu time

cost difference

 
 
 
 
 
 
 

  

 

  

 

  

 

   

       
noise

   

  

 

 a  costs

 

   

       
noise

   

 

 b  cpu time

figure    the deviation from the optimal cost  a  and the required cpu time in seconds
 b  as functions of the noise  the lines represent exact       greedy        mf
   and bp    control 

the example we consider is a multi agent system of n      agents in a two dimensional
state space with zero instantaneous costs  v      and no autonomous dynamics  ba      
the end cost function is given by equations            and       the two targets are located
at            and                   and c        the control cost matrix r equals
the identity matrix  the agents start in        at time t      the end time lies at t     
and time steps are of size dt        t  t         
the approximations are naive mean field approximation and belief propagation  as described in section    and greedy control  by greedy control we mean that at each time step
each agent chooses to go to its nearest target  we include this approximation because it is
simple and requires little computation time  and for those reasons it is an obvious choice
for a naive approximation  because a greedy control policy neglects the choices of the other
agents  we expect that it will give an inferior performance 
for each approximation  figure   a  shows the cost under the approximate  optimal 
control minus the cost under exact  optimal  control  averaged over     simulations  and for
different noise levels  the same noise samples were used for the approximate and the exact
control  we see that both naive mean field approximation and belief propagation yield
costs that on average coincide with the cost under exact control  the average cost difference
under both methods does not significantly differ from zero  greedy control  on the other
hand  yields costs that are significantly higher than the costs under exact control  only in
the deterministic limit does it converge to the cost under exact control  when both controls
coincide  figure   b  shows the cpu time required for the calculation of the controls under
the different control methods  this is the average cpu time of an entire simulation  each
simulation consists of    time steps  and at each time step the control is calculated for each
agent  we observe that greedy control is at least    times faster than the other methods 
and exact control is nearly     times more time consuming than the other methods  belief
propagation gives a performance that for all considered noise levels is a bit quicker than the
naive mean field approximation  but this may be the result of implementation details  we
have also done simulations with attractive coupling c        this returned results similar to
the ones with repulsive coupling c       that we presented here 
   

ficumulative control cost

van den broek  wiegerinck   kappen

  
  
  
 
 
 

   

 
time

   

 

figure    the cumulative control cost over time  in case of a strong repulsive coupling
c     and a low noise level           the curves represent exact       mf
    and bp control    

although figure   suggests that belief propagation and naive mean field approximation
perform equally well  this is not always the case  since for certain combinations of the noise
level and the coupling strength the bp control is more costly than mf control and exact
control  the origin of this difference lies in the symmetry breaking  which tends to occur
later under bp and earlier under mf when compared to exact control  we observe this
in figure    which shows the cumulative cost over time for the control methods in the
multi agent system  now with a coupling strength c     and a fixed noise level          
the cumulative costs are averages over     simulations  the cost under mf control lies a
bit higher than the cost under exact control  whereas the cost under bp control initially
is lower than the cost under the other control methods  but at t       it starts to increase
much faster and eventually ends up higher  including the end costs  we found total costs
            under exact control              under mf control  and           under bp
control  this suggests that it is better to have an early symmetry breaking than a late
symmetry breaking 
the time required for computing the control under the various methods depends on the
number of agents in the multi agent system  figure   shows the required cpu time as a
function of the number of agents n in the two dimensional multi agent system considered
above  we see that the exact method requires a cpu time that increases exponentially
with the number of agents  this is what may be expected from the theory  because the
exact method uses the junction tree algorithm which has a complexity that is exponential
in the tree width of the underlying graph  i e   exponential in n  for the greedy method 
the cpu time increases linearly with the number of agents  which is in agreement with the
fact that under greedy control there is no coupling between the agents  the required cpu
time increases polynomially for both the mean field approximation and belief propagation 
    nonlinear dynamics
we now turn to multi agent systems with nonlinear dynamics  to control these systems  we
must approximate both the graphical model inference as well as the single agent to singletarget control problem       we consider a multi agent system in which the agents move in
   

figraphical model inference in mas optimal control

 

  
cpu time

 

  

 

  

 

  

 

  

  

  

  
agents

  

  

figure    the required cpu time in seconds for the calculation of the controls at a different
number of agents  exact       greedy       mf     and bp control    

two dimensions and have a four dimensional state that is specified by the agents location
 xa   ya    its forward velocity va   and its driving direction a   the dynamics of each agent
is given by the equations
dxa   va cos a dt
dya   va sin a dt
dva   ua dt   a dwa
da   a dt   a da  
the first two equations model the kinematics of the agents position for a given forward
velocity and driving direction  the last two equations describe the control of the speed and
the driving direction by application of a forward acceleration ua and an angular velocity
a   the noise in the control is modeled by the standard normal wiener processes wa and
a and the noise level parameters a and a   note that the noise does not act in dimensions
other than those of the control  although the control space counts less dimensions than
the state space  the example does fit in the general framework  we refer to appendix c for
details 
we look at two different tasks  the first task is that of obstacle avoidance in a multiagent system of three agents  the agents each have to reach one of three target locations and
avoid any obstacles in the environment  each target location should be reached by precisely
one agent  we model this with an end cost function  given by equations            and      
with     and c        the targets are located at                    and           and the
agents should arrive with zero velocity  the control cost matrix r is the identity matrix 
        the instantaneous cost v equaled      at the locations of the obstacles  and zero
otherwise  the agents start at time t      the end time lies at t       and time steps dt
are of size      the starting locations of the agents are                    and           and
the agents start with zero velocity  the sample paths are discrete time paths in the twodimensional space of the forward velocity v and the driving direction   they are specified
by their values at times ti   t   i   i              n     with    nt t
  and n      the value
at time t  equals the current state of one of the agents  and the value at time tn equals
one of the target end states  the control for each agent to one of the targets is computed
   

fivan den broek  wiegerinck   kappen

  

  

  

  

  

  

  

  

  

  

 

 

  

  

  

  

  

 

 a  trajectories

 

  

  

  

  

  

 b  sample paths

figure    three agents  with noise and control in their forward velocities and driving directions  have to reach three targets  marked by x  in an environment containing
also a number of walls  each agent starts at a different location  marked by o 
and with zero forward velocity  and each agent should arrive at a different target
with zero velocity without hitting the walls   a  the trajectories that the agents
followed to reach the targets   b  sample paths 

with a metropolis hastings sampling of paths  according to subsection      the proposal
distribution is a  n  dimensional gaussian  centered around the agents current planned
path  and with a variance equal to the noise level in the agents dynamics  the expectation
values za  sa   xa   t  are estimated by the average costs of the sample paths  we have also
tried map estimation of za  sa   xa   t  and an inclusion of the variance in the sample paths 
but the former did not show a significant difference  and the latter returned estimates that
fluctuated heavily  figure   a  shows the environment and the trajectories of the agents
from their starting locations to the targets  each agent manages to avoid the obstacles and
arrive at one of the targets with zero velocity  such that each target is reached by a different
agent 
the second task is that of coordination in the multi agent system as shown in figure   a   in this system there are no instantaneous costs  v       the agents have to move
from their initial positions to a number of target locations  they should arrive at these
locations with zero velocity and horizontal driving direction  there is an equal number
of agents and target locations  and each agent has to reach a different target  the initial
locations are aligned vertically  and so are the target locations  but there is a vertical displacement between the two  thus the agents have to coordinate their movements in order
to reach the targets in a satisfactory way 
the agents start at time    the end time lies at      and they make time steps of size
t t
dt     n
     with n      until dt         at each time step the controls are computed by
a metropolis hastings sampling of paths and a naive mean field approximation to infer the
marginals pa  sa  x  t  that weigh the single agent to single target controls  equations     
and       the sample paths were discretized into seven equidistant time points from the
present time to the end time  the proposal distribution was taken a gaussian  which was
   

figraphical model inference in mas optimal control

centered around the agents current planned path and with a variance equal to the noise
level in the agents dynamics  figure   a  shows an example of the trajectories of a system
of    agents  it was obtained with    sample paths per agent target combination  we
observe that the agents reach the targets  and that each target is reached by precisely one
agent  as required  due to the noise in the second order dynamics of the agents  it takes
the agents less effort to approach a target than to remain there  since the former allows
exploitation of the noise while the latter requires a constant correction of the state changes
caused by the noise  the result is that the trajectories of the agents are more curved and
elongated than what would be expected in the situation without noise  the simulation was
carried out as well for a larger number of agents  figure   b  shows the required cpu time
as a function of the number of agents  both under exact and mf inference of the marginals
of the agents  note that the complexity of the graphical model inference problem scales as
nn   with n the number of agents  exact inference using the junction tree algorithm was
only feasible for n      

   discussion
we studied the use of graphical model inference methods in optimal control of stochastic
multi agent systems in continuous space and time where the agents have a joint task to
reach a number of target states  rather than discretizing  as is commonly done and typically
makes large systems intractable due to the curse of dimensionality  we followed the approach
developed by wiegerinck et al          modeling the system in continuous space and time 
under certain assumptions on the dynamics and the cost function  the solution can be given
in terms of a path integral 
the path integral can be computed in closed form in a few special cases  such as the
linear quadratic case  but in general it has to be approximated  this can be done by a
variety of methods  the method we considered in this paper is mcmc sampling  the
dimension of the sample paths was kept low  n      to limit the curvature of the sample
paths  the gain of limiting the curvature is that the variance in the samples is reduced
and less samples are needed  by limiting the curvature  however  we introduce a bias  in
addition  in the presence of obstacles insufficient curvature would make the sampler return
sample paths that run through the obstacles  we believe that more advanced mcmc
methods such as hybrid mc sampling  duane  kennedy  pendleton    roweth        and
overrelaxation  neal        can improve the inference of the path integrals 
apart from mcmc sampling  there are other approximation methods that one could
consider  such as the laplace approximation or a variational approximation  the laplace
approximation becomes exact in the noiseless limit and could be useful in low noise regimes
as well  the variational approximation approximates the path integral      by a gaussian
process  archambeau  opper  shen  cornford    shawe taylor         and could be particularly useful in the high noise regime  a drawback of the variational approach  however  is
that it cannot be straightforwardly applied to situations with infinite instantaneous costs 
like hard obstacles in the environment that we considered here 
wiegerinck et al         showed that for systems that are sufficiently sparse and in which
the single agent to single target controls can be determined in closed form  e g  linearquadratic control with time independent coefficients  exact inference can be achieved using
   

fivan den broek  wiegerinck   kappen

  

  

  

  

  

 

  

  

  

  

  
  

  

  

 

 

 

  

  

 a  trajectories
 

  

 

cpu time

  

 

  

 

  

 

  

 

  

  
  
number of agents

  

 b  cpu time

figure     a  the trajectories of    agents from starting locations o to    targets x   b 
the required cpu time in seconds as a function of the number of agents  with
the number of targets equal to the number of agents  the lines represent exact
     and mf    inference of the marginals 

   

figraphical model inference in mas optimal control

the junction tree algorithm  van den broek  wiegerinck  and kappen        considered
a multi agent system with second order dynamics  linear autonomous dynamics and zero
instantaneous costs  and showed that graphical model inference by naive mean field approximation significantly outperformed a greedy inference  here we showed that a close to
optimal result can be achieved as well in dense systems  using graphical model approximate
inference methods  the approximation methods that we considered were naive mean field
approximation and belief propagation  we demonstrated their performances in an example
system where exact inference is significantly more time consuming  mean field approximation showed to work very well  returning costs for control equal to the optimal ones  belief
propagation performed similarly  below a certain value for the ratio of coupling strength
to the noise level  the symmetry breaking in the control process takes place earlier under
mean field approximation when compared to exact inference  and later under belief propagation  an early symmetry breaking does not increase the costs for coordination much 
however  a late symmetry breaking does  making the performance under belief propagation
suboptimal 
some variations on the considered case are also possible within the general framework 
wiegerinck  van den broek  and kappen        discuss situations where agents sequentially
visit a number of targets  and where the end time is not fixed  it focusses on prefered
trajectories in state space over time  instead of prefered states at the end time  this is
achieved by modeling the path cost in a way similar to how we have modeled the end cost 
the problem where agents have to intercept a moving target with noisy dynamics is also
covered there 
the control formalism developed by kappen      a      b  and applied to multi agent
coordination by wiegerinck et al         and in this article  demands that the noise and
the control act in the same dimensions  one way to satisfy this constraint is to assume
that the agents are identical  in addition  the single agent dynamics should be such that
the noise and the control act in the same dimensions  we saw that for the two dimensional
second order system in section     this condition was satisfied in a natural way  however 
in general one can think of examples of control problems where equation     is violated  an
interesting future direction of research is to investigate to what extend the path integral
approach can be used as an approximation in such cases 
the paper assumes that the joint state space of the agents is observable to all agents 
for large multi agent systems  however  it will be more realistic that an agent only observes
its own state and the states of agents that are physically nearby  our approach does not
directly apply to such situations  depending on the joint task of the agents  it may be
a valid approximation to do optimal control in the sub system consisting of those agents
that one agent does observe  if the task of the agents is to avoid collisions  then it will be
sufficient to consider only the states of agents that are nearby  but if the task is to all go
to the same target then it will be crucial to have information about the states of all other
agents  a natural alternative to deal with partial observability is to describe the multi agent
system by a decentralized pomdp  seuken   zilberstein         it is not clear however 
how such an approach would combine with the path integral formalism 
the topic of learning has not been addressed in this paper  but clearly is of great
interest  however  one could argue that a sampling procedure to compute the path integral
   

fivan den broek  wiegerinck   kappen

corresponds to a learning of the environment  a discussion on this line of thought can be
found in  kappen        
there are many more possible model extensions worthwhile exploring in future research 
obvious examples are bounded controls  or a limited observation of the global state of the
system  these issues are already of interest to study in the single agent situation  others
apply typically to the multi agent situation  in the context of physical agents  introducing penalties for collisions between agents would become relevant  typically  these types
of model extensions will not have a solution in closed form  and will require additional
approximate numerical methods  some suggestions are given by kappen      a      b  
acknowledgments
we thank the reviewers for their useful comments  we thank joris mooij for making
available useful software  www mbfys ru nl  jorism libdai    this research is part of
the interactive collaborative information systems  icis  project  supported by the dutch
ministry of economic affairs  grant bsik      

appendix a  stochastic optimal control
in this appendix we give a derivation of          and      starting from               and
     detailed discussions can be found in many works on stochastic optimal control  for
example that of kushner         fleming and rishel         fleming         ksendal
        stengel         and kappen      a      b  
the optimal expected cost to go j in a state x at time t is defined as
j x  t    min c u  x  t  
u

where
u

c  x  t   

eux t


z
 x t     

t

t

d




 
 
kru x     k   v  x     
 

    

    

is the expected cost given the control law u  these are the equations     and     in the
main text  we first show that j satisfies the stochastic hamilton jacobi bellman  shjb 
equation


 
      
 

t j   min
kruk    b   u  x j   tr  x j   v  
    
u
 
 

with boundary condition j x  t      x   this equation is derived in the following way  for
any moment in time  between t and t it holds that


z  
 
 
u
ds
kru x s   s k   v  x s   s 
j x  t   
c  x       
 
t


z  
 
 
u
kru x s   s k   v  x s   s   
ds
  min ex t j x       
u
 
t
min eux t
u

the first line follows from dividing the integral from t to t into two integrals  one from t
to  and one from  to t   and using the definition of the cost function c  the second line
   

figraphical model inference in mas optimal control

follows from the definition of j  a rewriting yields


z  
j x       j x  t 
 
 
u
 
    min ex t
ds
 
kru x s   s k   v  x s   s   
u
t
t t
 
taking the limit   t we obtain


dj x t   t   
 
u
  kru x t   t k   v  x t   t   
    min ex t
u
dt
 

    

subsequently  we apply to dj x t   t  the well known chain rule for diffusion processes 
dj x t   t   

x j x t   t 
i

xi

dxi  t   

j x t   t 
  x    j x t   t 
dt  
dxi  t dxj  t       
t
 
xi xj
i j

it differs from the chain rule for deterministic processes in that it also contains a term
quadratic in dx  this extra term does not vanish  because the wiener process appearing in
the dynamics     has quadratic variation that increases linear in time 
eux t  dwi  t dwj  t     ij dt 

    

it follows that in expectation dxi  t dxj  t  is equal to     ij dt  by substituting the dynamics     in       taking expectation values  and using       we obtain


 
j x  t 
 j x  t 
  j x  t 
u
dt    b x  t    u x  t  
dt   tr 
dt 
ex t  dj x t   t    
t
x
xx
substitution into equation      then yields equation      
the minimum of the right hand side of equation      is given by
u    r r   x j 
this is the optimal control 
the minimization in      is removed by inserting the optimal control  this yields a
nonlinear equation for j  we can remove the nonlinearity by using a logarithmic transformation  if we introduce a constant   and define z x  t  through j x  t     log z x  t  
then
   
 
u r ru   u x j      z    x z   r r   x z 
 
 


   
 
      
tr  x j
 
z  x z    x z  z   tr   x  z  
 
 
 

the terms quadratic in x z vanish when   and r are related via equation     
     r r    
when this relation is satisfied  the shjb equation becomes



v
 

  
t z  
 b x  tr  x z

 
  hz 
   

    

fivan den broek  wiegerinck   kappen

where h a linear operator acting on the function z 
equation      must be solved backwards in time with boundary condition z x  t    
 

e  x    we present a solution in terms of a forward diffusion process  it is a common approach in the theory of stochastic processes to give solutions to partial differential equations
in terms of diffusion processes  the solution to equation      is the expectation value



z
 
  t
z x  t    ex t exp   y t    
d v  y       

 t

    

where y   is a process that satisfies the uncontrolled dynamics
dy     b y     d   dw   
and y t    x  the expectation ex t is taken with respect to the probability measure under
which y   satisfies the uncontrolled dynamics with condition y t    x  it is clear that     
matches the boundary condition  to verify that it satisfies equation       we let


z
  t
d v  y       
i t    exp 
 t
we see that

 
v  y t   t i t dt 


let f be the function f  y    exp     y    we again use the chain rule for stochastic
processes and apply it to f  y t     to find
di t   

k
k
x
f  y t    
  x    f  y t    
dyi  t     
dyi  t   dyj  t   
df  y t      
yi
 
yi yj
i  
i j  


f  y t     
 b y t     t   d   dw t    
 
y


 
 
  f  y t    
 tr 
d 
 
yy

we then choose      and d   dt and combine this identity with the previous one to
obtain
df  y t   i t    f  y t   di t    i t df  y t   
  hf  y t   i t dt   y f  y t   i t dw t   
taking the expectation value on both sides makes the term y f  y t   i t dw t   disappear 
and the remaining part 
de  f  y t   i t     he  f  y t   i t   dt 
is just equation      
   

figraphical model inference in mas optimal control

appendix b  the path integral formulation
we are going to write the expectation value     as a path integral  partitioning the time
interval from t to t into n intervals of equal length   t   t    t            tn   t   the
expectation value can be written as follows 
z x  t   

z

dx       

z

 

dxn e   xn  

n
 
y

z xi     ti     xi   ti  

    

i  

where x    x and the z xi     ti     xi   ti   are implicitly defined by



fi
z
z
fi
  ti  
fi
dxi   z xi     ti     xi   ti  f  xi       e f  xi     exp 
d v  y      fiy ti     xi
 ti

for arbitrary functions f   in the limit of infinitesimal   the z xi     ti     xi   ti   satisfy


 
z xi     ti     xi   ti      xi     ti    xi   ti   exp  v  xi   ti    
    


where  xi     ti    xi   ti   is the transition probability of the uncontrolled dynamics     to go
from  xi   ti   to  xi     ti     in space time  the transition probability is given by


 
k    xi    xi  b xi   ti   k 
 
 xi     ti    xi   ti     p
exp 
 
det      
this follows from the dynamics

xi    xi   b xi   ti     w
over the infinitesimal time interval and the observation that the wiener process w is normally distributed around zero with variance   using equation      we may rewrite the
transition probability as
 
   

  
 
x

x
i  
i
exp  
 xi     ti    xi   ti     p
r
 b xi   ti   
    

   
 
 

det    

we obtain the path integral representation of z x  t  by combining equations           
and      in the limit of  going to zero 
z x  t    lim z  x    t   

    

 

with x    x  t    t 
 

and

z  x    t      p
det      n

s  x            xn   t       xn    

n
 
x

z

dx       

 v  xi   ti    

z

n
 
x
i  

i  

   

 

dxn e  s  x       xn  t   

 
 

xi    xi
 

 b xi   ti   
 r
  
 


fivan den broek  wiegerinck   kappen

the optimal control is given by equation     and is proportional to the gradient of
log z x  t   substituting the path integral representation      of z x  t   we find that
u x    t      lim
 

  lim
 

z

z

dx       
dx       

z

z

 

e  s  x       xn  t   

dxn p
 x 
det      n z  x  t   




 
 s  x            xn   t   


dxn p x            xn   t   u x            xn   t   

where
u x            xn   t     
and



x   x 
 b x    t   

 

e  s  x       xn  t   
p x            xn   t      p
 
det      n z  x    t   

note that the control u x            xn   t    that results from a path  x            xn   only depends
on the first two entries x  and x  of the path 

appendix c  dimension reduction
the derivation of the path integral in appendix b was given for the case that both the
state and the control are k dimensional  the particular case that only some dimensions of
the state are controlled can be deduced by taking the limit of infinite control cost along the
dimensions without control  the control along the latter dimensions then becomes zero  as
can be seen from equation      the noise in these dimensions is equal to zero in accordance
with relation      in the path integral formalism the transition probabilities      then
reduce to delta functions along the dimensions without control  the implications for the
mcmc sampling are that the dimension of the space in which to sample is also reduced 
since sampling has only to be performed in the dimensions where there is noise 

references
archambeau  c   opper  m   shen  y   cornford  d     shawe taylor  j          variational inference
for diffusion processes  in advances in neural information processing systems 
becker  r   zilberstein  s   lesser  v     goldman  c  v          transition independent decentralized markov decision processes  in proceedings of the second international joint conference
on autonomous agents and multiagent systems  pp       
becker  r   zilberstein  s   lesser  v     goldman  c  v          solving transition independent
decentralized markov decision processes  journal of artificial intelligence research         
    
boutilier  c          planning  learning and coordination in multiagent decision processes  in
proceedings of the sixth conference on theoretical aspects of rationality and knowledge  pp 
       
castanon  d  a   pachter  m     chandler  p  r          a game of deception  in proceedings of
the   rd ieee conference on decision and control  pp           
duane  s   kennedy  a   pendleton  b     roweth  d          hybrid monte carlo  physics letters
b                  
   

figraphical model inference in mas optimal control

fleming  w  h          exit probabilities and optimal stochastic control  applied mathematics and
optimization            
fleming  w  h     rishel  r  w          deterministic and stochastic optimal control  springerverlag  new york 
guestrin  c   koller  d     parr  r       a   multiagent planning with factored mdps  in advances
in neural information processing systems  vol      pp           
guestrin  c   venkataraman  s     koller  d       b   context specific multiagent coordination and
planning with factored mdps  in eighteenth national conference on artificial intelligence 
pp         
hastings  w          monte carlo sampling methods using markov chains and their applications 
biometrika                
heskes  t          stable fixed points of loopy belief propagation are minima of the bethe free
energy  in advances in neural information processing systems  vol      pp         
heskes  t   albers  k     kappen  b          approximate inference and constrained optimization 
in proceedings of the   th conference on uncertainty in artificial intelligence  pp         
hu  j   prandini  m     tomlin  c          conjugate points in formation constrained optimal
multi agent coordination  a case study  siam journal on control and optimization         
         
jordan  m   ghahramani  z   jaakkola  t     saul  l          an introduction to variational methods
for graphical models  in learning in graphical models  mit press  cambridge 
kamal  w  a   gu  d  w     postlethwaite  i          real time trajectory planning for uavs using
milp  in proceedings of the  th ieee conference on decision and control  and the european
control conference       pp           
kappen  h  j       a   path integrals and symmetry breaking for optimal control theory  journal
of statistical mechanics  theory and experiment  p      
kappen  h  j       b   linear theory for control of nonlinear stochastic systems  physical review
letters                  
kappen  h  j          an introduction to stochastic control theory  path integrals and reinforcement
learning  in aip conference proceedings  vol       pp         
kleinert  h          path integrals in quantum mechanics  statistics  polymer physics  and financial markets  world scientific  singapore 
kschischang  f  r   frey  b  j     loeliger  h  a          factor graphs and the sum product
algorithm  ieee transactions on information theory                 
kushner  h  j          stochastic stability and control  academic press inc   new york 
larson  r  a   pachter  m     mears  m          path planning by unmanned air vehicles for
engaging an integrated radar network  in proceedings of the aiaa guidance  navigation  and
control conference and exhibit 
lauritzen  s     spiegelhalter  d          local computations with probabilities on graphical structures and their application to expert systems  with discussion   j  royal statistical society
series b             
liu  y   cruz  j  b     schumacher  c  j          pop up threat models for persistent area denial 
ieee transactions on aerospace and electronic systems                 
mackay  d  j          information theory  inference  and learning algorithms  cambridge university press 
neal  r  m          learning in graphical models  pp          kluwer academic publishers 
   

fivan den broek  wiegerinck   kappen

ksendal  b          stochastic differential equations  an introduction with applications  springerverlag 
pachter  l     pachter  m          optimal paths for avoiding a radiating source  in proceedings of
the   th ieee conference on decision and control  pp           
ribichini  g     frazzoli  e          efficient coordination of multiple aircraft systems  in proceedings
of the   nd ieee conference on decision and control  vol     pp           
sadati  n     elhamifar  e          semi decentralized control of multi agent systems based on
redundant manipulator optimization methods  in proceedings of the  th ieee international
workshop on advanced motion control  pp         
seuken  s     zilberstein  s          formal models and algorithms for decentralized decision making
under uncertainty  journal of autonomous agents and multi agent systems 
shi  x   wang  x   liu  y   wang  c     zu  c          optimization of fighter aircraft evasive trajectories for radar threats avoidance  in proceedings of the      ieee international conference
on control and automation  pp         
stengel  r          optimal control and estimation  dover publications  new york 
subramanian  s  k     cruz  j  b          adaptive models of pop up threats for multi agent
persistent area denial  in proceedings of the   nd ieee conference on decision and control 
pp         
teh  y     welling  m          the unified propagation and scaling algorithm  in advances in
neural information processing systems  vol      pp         
todorov  e     li  w          a generalized iterative lqg method for locally optimal feedback
control of constrained nonlinear stochastic systems  in proceedings of the american control
conference  pp         
tomlin  c   pappas  g  j     sastry  s          conflict resolution for air traffic management  a study
in multiagent hybrid systems  ieee transactions on automatic control                 
van den broek  b   wiegerinck  w     kappen  b          optimal control in large stochastic multiagent systems  in proceedings of the seventh symposium on adaptive learning agents and
multi agent systems  pp      
van leeuwen  p   hesseling  h     rohling  j          scheduling aircraft using constraint satisfaction 
electronic notes in theoretical computer science             
wiegerinck  w   van den broek  b     kappen  b          stochastic optimal control in continuous
space time multi agent systems  in proceedings of the   nd conference on uncertainty in
artificial intelligence  pp         
wiegerinck  w   van den broek  b     kappen  b          optimal on line scheduling in stochastic
multi agent systems in continuous space time  in proceedings of the sixth international joint
conference on autonomous agents and multiagent systems  pp         
yedidia  j   freeman  w     weiss  y          generalized belief propagation  in advances in neural
information processing systems  vol      pp         
yuille  a          cccp algorithms to minimize the bethe and kikuchi free energies  convergent
alternatives to belief propagation  neural computation                   

   

fi