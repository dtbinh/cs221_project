journal of artificial intelligence research               

submitted       published     

a hierarchy of tractable subsets
for computing stable models
rachel ben eliyahu

rachel cs bgu ac il

mathematics and computer science department
ben gurion university of the negev
p o b       beer sheva        israel

abstract

finding the stable models of a knowledge base is a significant computational problem
in artificial intelligence  this task is at the computational heart of truth maintenance
systems  autoepistemic logic  and default logic  unfortunately  it is np hard  in this
paper we present a hierarchy of classes of knowledge bases  
  
    with the following
properties  first  
  is the class of all stratified knowledge bases  second  if a knowledge
base  is in 
   then  has at most stable models  and all of them may be found in time
     where is the length of the knowledge base and the number of atoms in   third 
for an arbitrary knowledge base   we can find the minimum such that  belongs to 

in time polynomialsin  the size of   and  last  where k is the class of all knowledge bases 
it is the case that    
   k  that is  every knowledge base belongs to some class in the
hierarchy 
 

k

k

o lnk

     

l

n

k

i

k

i

   introduction
the task of computing the stable models of a knowledge base lies at the heart of three of
the fundamental systems in artificial intelligence  ai   truth maintenance systems  tmss  
default logic  and autoepistemic logic  yet  this task is intractable  elkan        kautz  
selman        marek   truszczynski         in this paper  we introduce a hierarchy of
classes of knowledge bases which achieves this task in polynomial time  membership in a
certain class in the hierarchy is testable in polynomial time  hence  given a knowledge base 
the cost of computing its stable models can be bounded prior to the actual computation  if
the algorithms on which this hierarchy is based are used  
first  let us elaborate the relevance of computing stable models to ai tasks  we define
a knowledge base to be a set of rules of the form

c  a         am  not b        not bn

   

where c   all as  and all b s are atoms in some propositional language  substantial efforts to
give a meaning  or semantics  to a knowledge base have been made in the logic programming
community  przymusinska   przymusinski         one of the most successful semantics for
knowledge bases is stable model semantics  bidoit   froidevaux        gelfond   lifschitz 
      fine         which associates any knowledge base with a  possibly empty  set of
models called stable models  intuitively  each stable model represents a set of coherent
c     


ai access foundation and morgan kaufmann publishers  all rights reserved 

fiben eliyahu
conclusions one might deduce from the knowledge base  it turns out that stable models
play a central role in some major deductive systems in ai   

    stable models and tmss

tmss  doyle        are inference systems for nonmonotonic reasoning with default assumptions  the tms manages a set of nodes and a set of justifications  where each node
represents a piece of information and the justifications are rules that state the dependencies
between the nodes  the tms computes a grounded set of nodes and assigns this set to be
the information believed to be true at a given point in time  intuitively  a set of believed
nodes is grounded if it satisfies all the rules  but no node is believed true solely on the basis
of a circular chain of justifications  elkan        pointed out that the nodes of a tms
can be viewed as propositional atoms  and the set of its justifications as a knowledge base 
he showed that the task of computing grounded interpretations for a set of tms justifications corresponds exactly to the task of computing the stable models of the knowledge base
represented by the set of tms justifications 

    stable models and autoepistemic logic

autoepistemic logic was invented by moore        in order to formalize the process of an
agent reasoning about its own beliefs  the language of autoepistemic logic is a propositional
language augmented by a modal operator l  given a theory  a set of formulas  t in
autoepistemic logic  a theory e is called a stable expansion of t iff
e    t sflf jf   e gsf lf jf    e g 
where t  denotes the logical closure of t   we will now restrict ourselves to a subset of
autoepistemic logic in which each formula is of the form
a          am    lb           lbn   c
   
where c   each of the as  and each of the b s are propositional atoms  we call this subset
the class of autoepistemic programs  every autoepistemic program t can be translated into
a knowledge base t by representing the formula     as the knowledge base rule      elkan
       has shown that m is a stable model of t iff there is an expansion e of t such
that m is the set of all positive atoms in e   thus  algorithms for computing stable models
may be used in computing expansions of autoepistemic programs  the relationship between
stable model semantics and autoepistemic logic has also been explored by gelfond       
and gelfond and lifschitz              

    stable models and default logic

default logic is a formalism developed by reiter        for reasoning with default assumptions  a default theory can be viewed as a set of defaults  and a default is defined as an
expression of the form
ff   fi        fin
   



   in logic programming terminology  the knowledge bases discussed in this paper are called normal logic
programs 

  

fia hierarchy of tractable subsets
where ff     and fi         fin are formulas in some first order language  according to reiter  e
is an extension for a default theory  iff e coincides with one of the minimal deductively
closed sets of sentences e   satisfying the condition  that for any grounded instance of a
default     from   if ff   e   and  fi          fin    e   then    e   
now consider the subset of default theories that we call default programs  a default
program is a set of defaults of the form

a          am    b         bn
   
c
in which c   each of the as  and each of the b s are atoms in a propositional language 

each default program  can be associated with a knowledge base  by replacing each
default of the form     with the rule     
gelfond and lifschitz        have shown that the logical closure of a set of atoms e is
an extension of  iff e is a stable model of    algorithms for computing stable models
can thus be used in computing extensions of reiter s default theories 


the paper is organized as follows  in the next section  we define our terminology 
section   presents two algorithms for computing all stable models of a knowledge base 
the complexity of the first of these algorithms depends on the number of atoms appearing
negatively in the knowledge base  while the complexity of the other algorithm depends
on the number of rules having negative atoms in their bodies  in section    we present
the main algorithm of the paper  called algorithm aas  algorithm aas works from the
bottom up on the superstructure of the dependency graph of the knowledge base and uses
the two algorithms presented in section   as subroutines  section   explains how the aas
algorithm can be generalized to handle knowledge bases over a first order language  finally 
in sections   and    we discuss related work and make concluding remarks 

   preliminary definitions
recall that here a knowledge base is defined as a set of rules of the form

c  a         am  not b        not bn

   

where c   each of the as  and each of the b s are propositional atoms  the expression to the
left of   is called the head of the rule  while the expression to the right of   is called
the body of the rule  each of the as is said to appear positive in the rule  and  accordingly 
each of the b s is said to appear negative in the rule  rule     is said to be about c   a rule
with an empty body is called a unit rule  sometimes we will treat a truth assignment  in
other words  interpretation  in propositional logic as a set of atoms   the set of all atoms
assigned true by the interpretation  given an interpretation i and a set of atoms a  ia
denotes the projection of i over a  given two interpretations  i and j   over sets of atoms
   note the appearance of e in the condition 

  

fiben eliyahu
a and b   respectively  the interpretation i   j is defined as follows 
  
if p   a n b
   ij  pp  
if p   btn a
i   j  p       i  p  
if p   a b and i  p     j  p  

  
undefined otherwise
t
if i  p     j  p   for every p   a b   we say that i and j are consistent 

a partial interpretation is a truth assignment over a subset of the atoms  hence  a partial
interpretation can be represented as a consistent set of literals  positive literals represent
the atoms that are true  negative literals the atoms that are false  and the rest are unknown 
a knowledge base will be called horn if all its rules are horn  a model for a theory  set
of clauses  in propositional logic is a truth assignment that satisfies all the clauses  if one
looks at a knowledge base as a theory in propositional logic  a horn knowledge base has a
unique minimal model  recall that a model m is minimal among a set of models m iff there
is no model m    m such that m   m  
given a knowledge base  and a set of atoms m  gelfond and lifschitz defined what is
now called the gelfond lifschitz  gl  transform of  w r t  m  which is a knowledge base
m obtained from  by deleting each rule that has a negative literal not p in its body with
p   m and deleting all negative literals in the bodies of the remaining rules  note that m
is a horn knowledge base  a model m is a stable model of a knowledge base  iff it is the
unique minimal model of m  gelfond   lifschitz        
example     consider the following knowledge base    which will be used as one of the
canonical examples throughout this paper 
   
warm blooded   mammal
live on land   mammal  not ab 
   
female   mammal  not male
   
male   mammal  not female
   
mammal   dolphin
    
ab    dolphin
    
mammal   lion
    
lion  
    
m   flion  mammal  warm blooded  live on land  femaleg is a stable model of     indeed 
 m  the gl transform of   w r t  m  is

 
 
 
 
 
 
 

warm blooded
live on land
female
mammal
ab 
mammal
lion

  

mammal
mammal
mammal
dolphin
dolphin
lion

fia hierarchy of tractable subsets
and m is a minimal model of  m  
a set of atoms s satisfies the body of a rule  iff each atom that appears positive in the
body of  is in s and each atom that appears negative in the body of  is not in s   a set
of atoms s satisfies a rule iff either it does not satisfy its body  or it satisfies its body and
the atom that appears in its head belongs to s  
a proof of an atom is a sequence of rules from which the atom can be derived  formally 
we can recursively define when an atom p has a proof w r t  a set of atoms s and a
knowledge base  
 if the unit rule p   is in   then p has a proof w r t   and s  
 if the rule p  a        am  not b        not bn is in   and for every i           n bi is
not in s   and for every i           m ai already has a proof w r t   and s   then p has
a proof w r t   and s  
theorem      elkan        ben eliyahu   dechter        a set of atoms s is a stable
model of a knowledge base  iff
   s satisfies each rule in   and
   for each atom p in s   there is a proof of p w r t  and s  
it is a simple matter to show that the following lemma is true 
lemma     let  be a knowledge base  and let s be a set of atoms  define 
   s       and
s
   si     si fp jp  a         am  not b         not bn is in  
all of the a s belong to si and none of the b  s belong to s g 
s s 
then s is a stable model of  iff s    
  i
observe that although every stable model is a minimal model of the knowledge base
viewed as a propositional theory  not every minimal model is a stable model 
example     consider the knowledge base
b   not a
both fag and fbg are minimal models of the knowledge base above  but only fbg is a stable
model of this knowledge base 
note that a knowledge base may have one or more stable models  or no stable model at all 
if a knowledge base has at least one stable model  we say that it is consistent 
the dependency graph of a knowledge base  is a directed graph where each atom is a
node and where there is a positive edge directed from p to q iff there is a rule about q
in  in which p appears positive in the body  accordingly  there is a negative edge from
p to q iff there is a rule about q in which p appears negative in the body  recall that a
source of a directed graph is a node with no incoming edges  while a sink is a node with no
outgoing edges  given a directed graph g and a node s in g  the subgraph rooted by s is
the subgraph of g having only nodes t such that there is a path directed from t to s in g 
the children of s in g are all nodes t such that there is an arc directed from t to s in g 
  

fiben eliyahu

example     the dependency graph of   is shown in figure    negative edges are
marked  not   the children of mammal are lion and dolphin  the subgraph rooted by
on land is the subgraph that include the nodes lion  mammal  dolphin  ab   and on land 
not
male

warm blood
female

not

on land
mammal

lion

not

ab 
dolphin

figure    the dependency graph of  
a knowledge base is stratified iff we can assign each atom c a positive integer ic such
that for every rule in the form of     above  for each of the as  ia  ic   and for each of
the b s  ib   ic   it can be readily demonstrated that a knowledge base is stratified iff in
its dependency graph there are no directed cycles going through negative edges  it is well
known in the logic programming community that a stratified knowledge base has a unique
stable model that can be found in linear time  gelfond   lifschitz        apt  blair   
walker        

example       is not a stratified knowledge base  the following knowledge base     is

stratified  we can assign ab  and penguin the number    and each of the other atoms the
number    

live on land
fly
bird
ab 

 
 
 
 
  

bird
bird  not ab 
penguin
penguin

fia hierarchy of tractable subsets
the strongly connected components of a directed graph g make up a partition of its
set of nodes such that  for each subset s in the partition and for each x  y   s   there are
directed paths from x to y and from y to x in g  the strongly connected components are
identifiable in linear time  tarjan        

male

not

not
female
warm blood

on land
mammal

lion

not

ab 
dolphin

figure    the super dependency graph of  
the super dependency graph of a knowledge base   denoted g   is the superstructure of
the dependency graph of   that is  g is a directed graph built by making each strongly
connected component in the dependency graph of  into a node in g   an arc exists from
a node s to a node v iff there is an arc from one of the atoms in s to one of the atoms in v
in the dependency graph of   note that g is an acyclic graph 

example     the super dependency graph of   is shown in figure    the nodes in the

square are grouped into a single node 

   two algorithms for computing stable models
the main contribution of this paper is the presentation of an algorithm whose eciency
depends on the  distance  of the knowledge base from a stratified knowledge base  this
distance will be measured precisely in section    we will first describe two other algorithms
for computing stable models  these two algorithms do not take into account the level of
 stratifiability  of the knowledge base  that is  they will still work in exponential time for
stratified knowledge bases  our main algorithm will use these two algorithms as procedures 
  

fiben eliyahu
given a truth assignment for a knowledge base  we can verify in polynomial time whether
it is a stable model by using lemma      therefore  a straightforward algorithm for computing all stable models can simply check all possible truth assignments and determine whether
each of them is a stable model  the time complexity of this straightforward procedure will
be exponential in the number of atoms used in the knowledge base  below  we present two
algorithms that can often function more eciently than the straightforward procedure 

    an algorithm that depends on the number of negative atoms in the
knowledge base
algorithm all stable   figure    enables us to find all the stable models in time expo 

nential in the number of the atoms that appear negative in the knowledge base 
the algorithm follows from work on abductive extensions of logic programming in which
stable models are characterized in terms of sets of hypotheses that can be drawn as additional information  eshghi   kowalski        dung        kakas   mancarella        
this is done by making negative atoms abductible and by imposing appropriate denials
and disjunctions as integrity constraints  the work of eshghi and kowalski         dung
        and kakas and mancarella        implies the following 
theorem     let  be a knowledge base  and let h be the set of atoms that appear negated
in   m is a stable model of  iff there is an interpretation i over h such that
   for every atom p   h   if p   i   then p   m    
   m   and i are consistent  and
   m   i  m    
where m   is the unique stable model of i  
proof  the proof follows directly from the definition of stable models  suppose m is a
stable model of a knowledge base   and let h be the set of atoms that appear negative in
  then  by definition  m is a stable model of m   but note that m   mh   hence  the
conditions of theorem     hold for m   taking m     m and i   mh   now  suppose  is
a knowledge base and m   m     i   where m   and i are as in theorem      observe that
m   i and  hence  since m   is a stable model of i   m   is a stable model of m   we
will show that m is a stable model of m   first  note that by condition    m  m     thus 
m satisfies all the rules in m and  if an atom p has a proof w  r  t  m   and m   it has
also a proof w  r  t  m and m   so  by theorem      m is a stable model of m and  by
definition  m is a stable model of  
theorem     implies algorithm all stable   figure     which computes all stable
models of a knowledge base   hence  we have the following complexity analysis 
proposition     a knowledge base in which at most k atoms appear negated has at most
 k stable models and all of them can be found in time o nl k    where l is the size of the
knowledge base and n the number of atoms used in the knowledge base 
proof  follows from the fact that computing i and computing the unique stable model
of a positive knowledge base is o nl  
  

fia hierarchy of tractable subsets

all stable   

input  a knowledge base  
output  the set of all stable models of  
   m      
   for each possible interpretation i for the set of all atoms that appear negative in  
do 
 a  compute m     the unique stable model of i  
s
 b  if m   and i are consistent  let m    m fm     i g 
   return m 
figure    algorithm all stable 

    an algorithm that depends on the number of non horn rules
algorithm all stable   figure    depends on the number of rules in which there are

negated atoms  it gets as input a knowledge base   and  it outputs the set of all stable
models of   this algorithm is based upon the observation that a stable model can be
built by attempting all possible means of satisfying the negated atoms in bodies of nonhorn rules  two procedures are called by all stable   unitinst  shown in figure    and
negunitinst  shown in figure    procedure unitinst gets as input a knowledge base  and
a partial interpretation m  unitinst looks recursively for unit rules in   for each unit rule
p     if p is assigned false in m  it follows that m cannot be a part of a model for   and
the procedure returns false  if p is not false in m  the procedure instantiates p to true in
the interpretation m and deletes the positive appearances of p from the body of each rule 
it also deletes from  all the rules about p and all the rules in which p appears negative 
procedure negunitinst receives as input a knowledge base   a partial interpretation
m  and a set of atoms neg  it first instantiates each atom in neg to false and then updates
the knowledge base to reect this instantiation  all the instantiations are recorded in m 
in case of a conict  namely  where the procedure tries to instantiate to true an atom that
is already set to false  the procedure returns false  otherwise  it returns true 

proposition     algorithm all stable  is correct  that is  m is a stable model of a
knowledge base  iff it is generated by all stable    
proof  suppose m is a stable model of a knowledge base   then  by theorem      every
atom set to true in m has a proof w  r  t  m and   let s be the set of all non horn
rules whose bodies are satisfied by m  clearly  at some point this s is checked at step   of
algorithm all stable   when this happens  all atoms that have a proof w  r  t  m and
 will be set to true by the procedure negunitinst  as can be proved by induction on the
length of the proof   hence  m will be generated 
suppose m is generated by all stable     obviously  every rule in  is satisfied by
m  step   c ii   and every atom set to true by negunitinst has a proof w  r  t  m and 
  

fiben eliyahu

all stable   
input  a knowledge base  
output  the set of all stable models of  
   m      
   let  be the set of all non horn rules in  
   for each subset s of   do 
 a  neg   fp jnot p is in the body of some rule in s g 
 b         m      
 c  if negunitinst     neg  m   then
i  for each p such that m p     null  let m p      false 
s
ii  if m satisfies all the rules in   then m    m fmg 
   endfor 
   return m  
figure    algorithm all stable 
unitinst   m 
input  a knowledge base  and a partial interpretation m 
output  updates m using the unit rules of   returns false if there is a conict between
a unit rule and the value assigned to some atom in m  otherwise  returns true  
   while  has unit rules  do 
 a  let p   be a unit rule in  
 b  if m p     false  return false 
 c  m p      true 
 d  erase p from the body of each rule in  
 e  erase from  all rules about p  
 f  erase from  all rules in which p appears negative 
   endwhile 
   return true 
figure    procedure unitinst
  

fia hierarchy of tractable subsets
negunitinst   neg  m 
input  a knowledge base   a set of atoms neg   and a partial interpretation m 
output  updates m assuming the atoms in neg are false  returns false if inconsistency is
detected  otherwise  returns true 
   for each atom p in neg
 a  m p      false 
 b  delete from the body of each rule in  each occurrence of not p  
 c  delete from  each rule in which p appears positive in the body 
   endfor 
   return unitinst   m  
figure    procedure negunitinst
s
s

 
 

lion dolphin ab  mammal warm b on land male female
t

f

f

t

t

t

f

t

t

f

f

t

t

t

t

f

table    models generated by algorithm all stable 
 as is readily observable from the way negunitinst works   hence  by theorem      m is a
stable model of  

proposition     a knowledge base having c non horn rules has at most  c stable models

and all of them can be found in time o nl c   where l is the size of the knowledge base and
n the number of atoms used in the knowledge base 

proof  straightforward  by induction on c 
example     suppose we call all stable  with   as the input knowledge base  at
step     is the set of rules           and      when subsets of  which include both rules
    and     are considered at step    negunitinst will return false because unitinst will

detect inconsistency  when the subset containing both rules     and     is considered  the
stable model s   of table   will be generated  when the subset containing both rules    
and     is considered  the stable model s   of table   will be generated  when all the other
subsets that do not contain both rules     and     are tested at step    the m generated will
not satisfy all the rules in  and  hence  will not appear in the output 
algorithms all stable  and all stable  do not take into account the structure
of the knowledge base  for example  they are not polynomial for the class of stratified
knowledge bases  we present next an algorithm that exploits the structure of the knowledge
base 
  

fiben eliyahu

   a hierarchy of tractable subsets based on the level of stratifiability
of the knowledge base

algorithm acyclic all stable  aas  in figure   exploits the structure of the knowledge
base as it is reected in the super dependency graph of the knowledge base  it computes all
stable models while traversing the super dependency graph from the bottom up  using the
algorithms for computing stable models presented in the previous section as subroutines 
let  be a knowledge base  with each node s in g  the super dependency graph of
   we associate s   as   and ms   s is the subset of  containing all the rules about the
atoms in s  as is the set of all atoms in the subgraph of g rooted by s  and ms is the set of
stable models associated with the subset of the knowledge base  which contains only rules
about atoms in as   initially  ms is empty for every s  the algorithm traverses g from
the bottom up  when at a node s  it first combines all the submodels of the children of s
into a single set of models mc s    if s is a source  then mc s  is set to f g   next  for each
model m in mc s    aas converts s to a knowledge base sm using the gl transform and
other transformations that depend on the atoms in m  then  it finds all the stable models
of sm and combines them with m  the set ms is obtained by repeating this operation for
each m in mc s    aas uses the procedure cartesprod  figure     which receives as input
several sets of models and returns the consistent portion of their cartesian product  if one
of the sets of models which cartesprod gets as input is the empty set  cartesprod will
output an empty set of models  the procedure convert gets as input a knowledge base  
a model m  and a set of atoms s  and performs the following  for each atom p in m  each
positive occurrence of p is deleted from the body of each rule in   for each rule in   if
not p is in the body of the rule and p   m  then the rule is deleted from   if not p is
in the body of a rule and p    m  then  if p    s  not p is deleted from that body  the
procedure all stable called by aas may be one of the procedures previously presented
 all stable  or all stable   or it may be any other procedure that generates all stable
models 

example     suppose aas is called to compute the stable models of    suppose further

that the algorithm traverses the super dependency graph in figure   in the order flion 
dolphin  mammal  ab   on land  warm blooded  female maleg  recall that all the nodes inside the square make up one node that we are calling female male or  for short  fm  
after visiting all the nodes except the last  we have mlion   ffliongg  mdolphin   f g 
mmammal   fflion  mammalgg  mon land   fflion  mammal  onlandgg  mwarm blooded  
fflion  mammal  warm bloodedgg  when visiting the node fm  we have after step   c that
mc fm     mmammal   so step   d loops only once  for m   flion  mammalg  recall that
fm is the knowledge base

female
male

  mammal  not male
  mammal  not female

   note the difference between f g  which is a set of one model   the model that assigns
atoms  and    which is a set that contains no models 

  

false

to all the

fia hierarchy of tractable subsets

acyclic all stable  

input  a knowledge base  
output  the set of all stable models of  
   traverse g from the bottom up  for each node s  do 
 a  ms      
 b  let s         sj be the children of s 
 c  if j      then mc s     f g 
else mc s     cartesprod fms         msj g  
 d  for each m   mc s    do 
i  sm    convert s   m  s  
ii  m    all stable sm   
s
iii  if m       then ms    ms cartesprod ffmg  m g  
   output cartesprod fms         msk g   where s         sk are the sinks of g  
figure    algorithm acyclic all stable  aas 
cartesprod m 
input  a set of sets of models m 
output  a set of models which is the consistent portion of the cartesian product of the
sets in m 
   if m has a single element fe g  then return e  
   m      
   let m     m 
   d    cartesprod m n fm  g  
   for each d in d  do 
 a  for each m in m     do 
s
if m and d are consistent  then m    m fm   dg 
 b  endfor 
   endfor 
   return m  
figure    procedure cartesprod
  

fiben eliyahu
after executing step   d i  we have fm m set to

female
male

  not male
  not female

the above knowledge base has two stable models  ffemaleg and fmaleg  the cartesian
product of the above set with flion  mammalg yields mfm   fflion  mammal  femaleg 
flion  mammal  malegg  at step    the cartesian product of mwarm blooded   mon land  and
mfm is taken  thus  the algorithm outputs fflion  mammal  on land  warm blooded  femaleg 
flion  mammal  on land  warm blooded  malegg  and these are indeed the two stable models
of     note that algorithm aas is more ecient than either all stable  or all stable 
on the knowledge base    

theorem     algorithm aas is correct  that is  m is a stable model of a knowledge base
 iff m is generated by aas when applied to  
proof  let s   s        sn be the ordering of the nodes of the super dependency graph by

which the algorithm is executed  we can show by induction on i that aas  when at node
si  generates all and only the stable models of the portion of the knowledge base composed
of rules that only use atoms from asi  
case i      in this case  at step   d ii of aas  sm   s  thus  the claim follows from the
correctness of the algorithm all stable called in step   d ii 
case i      showing that every model generated is stable is straightforward  by the induction hypothesis and theorem      the other direction is  suppose m is a stable model
of s   show that m is generated  clearly  for each child s of si   the projection of m
onto as is a stable model of the part of the knowledge base that uses only atoms from
as  by induction  mc   which is the projection of m onto the union of as for every
child s of si   must belong to mc si   computed at step   c  therefore  to show that m
is generated  we need only show that m    m   mc is a stable model of simc   this
is easily done using theorem     
we will now analyze the complexity of aas  first  given a knowledge base  and a
set of atoms s  we define   s to be the knowledge base obtained from  by deleting each
negative occurrence of an atom that does not belong to s from the body of every rule 
for example  if    fa  not b  c  not d  ag and s   fbg  then   s   fa  not b  c  ag 
while visiting a node s during the execution of aas  we have to compute at step   d ii all
stable models of some knowledge base sm   using either all stable  or all stable  
the estimated time required to find all stable models of sm is shorter than or equal to the
time required to find all stable models of   s   this occurs because the number of negative
atoms and the number of rules with negative atoms in their bodies in   s is higher than
or equal to the number of negative atoms and the number of rules with negative atoms in
their bodies in sm   regardless of what m is  thus  if   s is a horn knowledge base  we can
find the stable model of   s   and hence of sm   in polynomial time  no matter what m is 
  

fia hierarchy of tractable subsets
if   s is not positive  then we can find all stable models of   s   and hence of sm   in time
min ln   k   ln   c    where l is the length of   s   n the number of atoms used in   s   c the
number of rules in   s that contain negative atoms  and k the number of atoms that appear
negatively in   s  
then  with each knowledge base   we associate a number t as follows  associate a
number vs with every node in g   if   s is a horn knowledge base  then vs is    else  vs is
min  k    c   where c is the number of rules in   s that contain negative atoms from s  and
k is the number of atoms from s that appear negatively in   s   now associate a number ts
with every node s  if s is a leaf node  then ts   vs   if s has children s         sj in g   then
ts   vs  ts        tsj   define t to be ts        tsk   where s         sk are all the sink nodes in
g  
definition     a knowledge base  belongs to 
j if t   j  
theorem     if a knowledge base belongs to 
j for some j   then it has at most j stable
models that can be computed in time o lnj   
proof  by induction on j   the dependency graph and the super dependency graph are
both built in time linear in the size of the knowledge base  so we may only consider the
time it takes to compute all stable models with the super dependency graph given 
case j         
  means that for every node s in g    s is a horn knowledge base  in
other words   is stratified  and therefore it has exactly one stable model  there are
at most n nodes in the graph  at each node  the loop in step   d is executed at most
once  because at most one model is generated at every node  procedure convert runs
in time o ls   where ls is the length of s  we assume that m is stored in an array
where the access to each atom is in constant time   since  for every node s    s is a
horn knowledge base  sm is computed in time o lsn   thus  the overall complexity
is o ln  
case j      by induction on n  the number of nodes in the super dependency graph of  
case n      let s be the single node in g   thus  j   vs  using the algorithms from
section    all stable models of    s can be found in time o lnvs    and  has
at most vs models 
case n      assume without loss of generality that g has a single sink s  to get a
single sink  we can add to the program the rule p   s        sk  where s         sk are
all the sinks and p is a new atom   let c         ck be the children of s  for each
child ci    ci    the part of the knowledge base which corresponds to the subgraph
rooted by ci  must belong to 
ti for some ti  j   by induction on n  for each
child node ci  all stable models of  ci   can be computed in time o lnti    and
 ci  has at most ti stable models  now let us observe what happens when aas
is visiting node s  first  the cartesian product of all the models computed at the
child nodes is taken  this is executed in time o n  t        tk   and yields at most
t        tk models in mc s    for every m   mc s    we call convert  o ln   and
compute all the stable models of sm  o lnvs    we then combine them with m
using cartesprod  o nvs     thus  the overall complexity of computing ms   that
is  of computing all the stable models of   is o lnt        tk  vs     o lnj   
  

fiben eliyahu

note that all stratified knowledge bases belong to 
    and the more that any knowledge
base looks stratified  the more ecient algorithm aas will be 
given a knowledge base   it is easy to find the minimum j such that  belongs to 
j  
this follows because building g and finding c and k for every node in g are polynomialtime tasks  hence 
theorem     given a knowledge base   we can find the minimum j such that  belongs
to 
j in polynomial time 
example     for all the nodes s in g  except fm  vs     vfm      thus      
    
is a stratified knowledge base and therefore belongs to 
  

not

male

not
female
warm blood

on land
mammal

lion

fly

not

not
ab 
dolphin

bird

ab 

penguin

s

figure    the super dependency graph of    
the next example shows that step   of procedure cartesprod is necessary 
example     consider knowledge base   
a   not b
b   not a

c
d
e
f

 
 
 
 

  

a
b
c  d
c

fia hierarchy of tractable subsets

f

e

c

d

not

a

b
not

figure     super dependency graph of  

not

not

c

c

not

not

a

a
b

b

not

not

   

   

figure     dependency graph     and super dependency graph     of  
  

fiben eliyahu
the super dependency graph of   is shown in figure     during the run of algorithm aas 
mab  the set of models computed at the node fa  bg  is set to ffa   bg  f a  bgg  when aas
visits nodes c and d  we get mc   ffa   b  cg  f a  bgg  md   ff a  b  dg  fa   bgg  when
aas visits node e  cartesprod is called on the input fmc   mdg  yielding the output me  
ffa   b  cg  f a  b  dgg  note that cartesprod does not output any model in which both c
and d are true  because the models fa   b  cg and f a  b  dg are inconsistent and cartesprod
checks for consistency in step    when visiting node f   we get mf   ffa   b  c  f g  f a  bgg 
aas then returns cartesprod fme  mf g   which is ffa   b  c  f g  f a  b  dgg 
the next example demonstrates that some models generated at some nodes of the super dependency graph during the run of aas may later be deleted  since they cannot be
completed to a stable model of the whole knowledge base 

example     consider knowledge base   
a
b
c

  not b
  not a
  a  not c

the dependency graph and the super dependency graph of   are shown in figure    
during the run of algorithm aas  mab  the set of models computed at the node fa  bg  is
set to ffag  fbgg  however  only fbg is a stable model of    
despite the deficiency illustrated in example      algorithm aas does have desirable
features  first  aas enables us to compute stable models in a modular fashion  we can use
g as a structure in which to store the stable models  once the knowledge base is changed 
we need to resume computation only at the nodes affected by the change  for example 
suppose that after computing the stable models of the knowledge base     we add tos  
the knowledge base   of example      which gives us a new knowledge base          
the super dependency graph of the new knowledge base   is shown in figure    now we
need only to compute the stable models at the nodes penguin  bird  ab   y  and on land
and then to combine the models generated at the sinks  we do not have to re compute the
stable models at all the other nodes as well 
second  in using the aas algorithm  we do not always have to compute all stable models
up to the root node  if we are queried about an atom that is somewhere in the middle of
the graph  it is often enough to compute only the models of the subgraph rooted by the
node that represents this atom  for example  suppose we are given the knowledge base
  and asked if mammal is true in every stable model of     we can run aas for the
nodes dolphin  lion  and mammal   and then stop  if mammal is true in all the stable
models computed at the node mammal  i e   in all the models in mmammal    we answer
 yes   otherwise  we must continue the computation 
third  the aas algorithm is useful in computing the labeling of a tms subject to
nogoods  a set of nodes of a tms can be declared nogood  which means that all acceptable
labeling should assign false to at least one node in the nogood set   in stable models
terminology  this means that when handling nogoods  we look for stable models in which
   in logic programming terminology nogoods are simply integrity constraints 

  

fia hierarchy of tractable subsets
at least one atom from a nogood is false  a straightforward approach would be to first
compute all the stable models and then choose only the ones that comply with the nogood
constraints  but since the aas algorithm is modular and works from the bottom up 
in many cases it can prevent the generation of unwanted stable models at an early stage 
during the computation  we can exclude the submodels that do not comply with the nogood
constraints and erase these submodels from ms once we are at a node s in the super
dependency graph such that as includes all the members of a certain nogood 

   computing stable models of first order knowledge bases
in this section  we show how we can generalize algorithm aas so that it can find all stable
models of a knowledge base over a first order language with no function symbols  the new
algorithm will be called first acyclic all stable  faas  
we will now refer to a knowledge base as a set of rules of the form

c  a   a        am  not b        not bn

    

where all as  b s  and c are atoms in a first order language with no function symbols  the
definitions of head  body  and positive and negative appearances of an atom are the same
as in the propositional case  in the expression p x        xn   p is called a predicate name 
as in the propositional case  every knowledge base  is associated with a directed graph
called the dependency graph of   in which  a  each predicate name in  is a node   b 
there is a positive arc directed from a node p to a node q iff there is a rule in  in which
p is a predicate name in one of the ai s and q is a predicate name in the head  and  c 
there is a negative arc directed from a node p to a node q iff there is a rule in  in which
p is a predicate name in one of the bi s and q is a predicate name in the head  the super
dependency graph  g   is defined in an analogous manner  we define a stratified knowledge
base to be a knowledge base in which there are no cycles through the negative edges in the
dependency graph of the knowledge base 
a knowledge base will be called safe iff each of its rules is safe  a rule is safe iff all the
variables appearing in the head of the rule or in predicates appearing negative in the rule
also appear in positive predicates in the body of the rule  in this section  we assume that
knowledge bases are safe  the herbrand base of a knowledge base is the set of all atoms
constructed using predicate names and constants from the knowledge base  the set of
ground instances of a rule is the set of rules obtained by consistently substituting variables
from the rule with constants that appear in the knowledge base in all possible ways  the
ground instance of a knowledge base is the union of all ground instances of its rules  note
that the ground instance of a first order knowledge base can be viewed as a propositional
knowledge base 
a model for a knowledge base is a subset m of the knowledge base s herbrand base 
this subset has the property that for every rule in the grounded knowledge base  if all the
atoms that appear positive in the body of the rule belong to m and all the atoms that
appear negative in the body of the rule do not belong to m   then the atom in the head of
the rule belongs to m   a stable model for a first order knowledge base  is a herbrand
model of   which is also a stable model of the grounded version of  
  

fiben eliyahu

first acyclic all stable  

input  a first order knowledge base  
output  all the stable models of  
   traverse g from the bottom up  for each node s  do 
 a  ms      
 b  let s         sj be the children of s 
 c  mc s     cartesprod fms         msj g  
 d  for each m   mc s  do
ms    mssall stable s sfp  jp   mg 
   output cartesprod fms         msk g   where s         sk are the sinks of g  
figure     algorithm first acyclic all stable  faas 
we now present faas  an algorithm that computes all stable models of a first order
knowledge base  let  be a first order knowledge base  as in the propositional case  with
each node s in g  the super dependency graph of    we associate s   as   and ms   s is
the subset of  containing all the rules about predicates whose names are in s  as is the
set of all predicate names p that appear in the subgraph of g rooted by s  ms are the
stable models associated with the sub knowledge base of  that contains only rules about
predicates whose names are in as   initially  ms is empty for every s  algorithm faas
traverses g from the bottom up  when at a node s  the algorithm first combines all the
submodels of the children of s into a single set of models  mc s    then  for each model
m in mc s   it calls a procedure that finds all the stable models of s union the set of all
unit clauses p   where p   m  the procedure all stable called by faas can be any
procedure that computes all the stable models of a first order knowledge base  because
procedure all stable computes stable models for only parts of the knowledge base  it
may take advantage of some fractions of the knowledge base being stratified or having any
other property that simplifies computation of the stable models of a fraction 

theorem     algorithm faas is correct  that is  m is a stable model of a knowledge base
 iff m is one of the models in the output when applying faas to  

proof  as the proof of theorem     

note that the more that a knowledge base appears stratified  the more ecient algorithm
faas becomes 

example     consider knowledge base   
warm blooded x  
live on land x  
female x  

  mammal x  
  mammal x    not ab  x  
  mammal x    not male x  
  

fia hierarchy of tractable subsets
male x  
mammal x  
ab  x  
mammal x  
dolphin flipper 

 
 
 
 
 

mammal x    not female x  
dolphin x  
dolphin x  
lion x  

  bird x  
  bird x    not ab  x  
  penguin x  
  penguin x  
 
the super dependency graph of     g    is the same as the super dependency graph of
live on land x  
fly  x  
bird x  
ab  x  
bird bigbird 

the knowledge base    see figure     observe that when at node mammal  for example 
s
in step   d the algorithm looks for all stable models of the knowledge base     mammal
f  dolphin flipper g  where mammal  fmammal x    dolphin x    mammal x    lion x  g 
  is a stratified knowledge base that has a unique stable model that can be found eciently 
hence  algorithm faas saves us from having to ground all the rules of the knowledge base
before starting to calculate the models  and it can take advantage of parts of the knowledge
base being stratified 

   related work

in recent years  quite a few algorithms have been developed for reasoning with stable models 
nonetheless  as far as we know  the work presented here is original in the sense that it
provides a partition of the set of all the knowledge bases into a hierarchy of tractable
classes  the partition is based on the structure of the dependency graph  intuitively  the
task of computing all the stable models of a knowledge base using algorithm aas becomes
increasingly complex as the  distance  of the knowledge base from being stratified becomes
larger  next  we summarize the work that seems to us most relevant 
algorithm aas is based on an idea that appears in the work of lifschitz and turner
        where they show that in many cases a logic program can be divided into two parts 
such that one part  the  bottom  part  does not refer to the predicates defined in the  top 
part  they then explain how the task of computing the stable models of a program can be
simplified when the program is split into parts  algorithm aas  using the superstructure
of the dependency graph  exploits a specific method for splitting the program 
bell et al         and subrahmanian et al         implement linear and integer programming techniques in order to compute stable models  among other nonmonotonic logics   however  it is dicult to assess the merits of their approaches in terms of complexity 
ben eliyahu and dechter        illustrate how a knowledge base  can be translated into
a propositional theory t such that each model of the latter corresponds to a stable model
of the former  it follows from this that the problem of finding all the stable models of
a knowledge base corresponds to the problem of finding all the models of a propositional
theory  satoh and iwayama        provide a nondeterministic procedure for computing
  

fiben eliyahu
the stable models of logic programs with integrity constraints  junker and konolige       
present an algorithm for computing tms  labels  antoniou and langetepe        introduce
a method for representing some classes of default theories as normal logic programs in such
a way that sldnf resolution can be used to compute extensions  pimentel and cuadrado
       develop a label propagation algorithm that uses data structures called compressible
semantic trees in order to implement a tms  their algorithm is based on stable model semantics  the algorithms developed by marek and truszczynski        for autoepistemic
logic can also be adopted for computing stable models  the procedures by marek and
truszczynski         antoniou and langetepe         pimentel and cuadrado         beneliyahu and dechter         satoh and iwayama         bell et al          subrahmanian
et al          and junker and konolige        do not take advantage of the structure of
the knowledge base as reected in its dependency graph  and therefore are not ecient for
stratified knowledge bases 
sacca and zaniolo        present a backtracking fixpoint algorithm for constructing one
stable model of a first order knowledge base  this algorithm is similar to algorithm allstable  presented here in section   but its complexity is worse than the complexity of
all stable   they show how the backtracking fixpoint algorithm can be modified to
handle stratified knowledge bases in an ecient manner  but the algorithm needs further
adjustments before it can deal eciently with knowledge bases that are very close to being
stratified  leone et al         present an improved backtracking fixpoint algorithm for
computing one stable model of a datalog  program and discuss how the improved algorithm
can be implemented  one of the procedures called by the improved algorithm is based on
the backtracking fixpoint algorithm of sacca and zaniolo         like the backtracking
fixpoint algorithm  the improved algorithm as is does not take advantage of the structure
of the program  i e   it is not ecient for programs that are close to being stratified 
several tractable subclasses for computing extensions of default theories  and  hence 
computing stable models  are known  kautz   selman        papadimitriou   sideri 
      palopoli   zaniolo        dimopoulos   magirou        ben eliyahu   dechter 
       some of these tractable subclasses are characterized using a graph that reects
dependencies in the program between atoms and rules  the algorithms presented in these
papers are complete only for a subclass of all knowledge bases  however  algorithms for
computing extensions of stratified default theories or extensions of default theories that
have no odd cycles  in some precise sense  are given by papadimitriou and sideri       
and cholewinski      a      b  
algorithms for handling a tms with nogoods have been developed in the ai community by doyle        and charniak et al          but  as elkan        points out  these
algorithms are not always faithful to the semantics of the tms and their complexities have
not been analyzed  dechter and dechter        provide algorithms for manipulating a tms
when it is represented as a constraint network  the eciency of their algorithms depends
on the structure of the constraint network representing the tms  and the structure they
employ differs from the dependency graph of the knowledge base 
  

fia hierarchy of tractable subsets

   conclusion
the task of computing stable models is at the heart of several systems central to ai 
including tmss  autoepistemic logic  and default logic  this task has been shown to be
np hard  in this paper  we present a partition of the set of all knowledge bases to classes

    
        such that if a knowledge base  is in 
k   then  has at most k stable models 
and they may all be found in time o lnk   where l is the length of the knowledge base and
n the number of atoms in   moreover  for an arbitrary knowledge base   we can find the
minimum k such that  belongs to 
k in time linear in the size of   intuitively  the more
the knowledge base is stratified  the more ecient our algorithm becomes  we believe that
beyond stratified knowledge bases  the more expressive the knowledge base is  i e  the more
rules with nonstratified negation in the knowledge base   the less likely it will be needed 
hence  our analysis should be quite useful  in addition  we show that algorithm aas has
several advantages in a dynamically changing knowledge base  and we provide applications
for answering queries and implementing a tms s nogood strategies  we also illustrate a
generalization of algorithm aas for the class of first order knowledge bases 
algorithm aas can easily be adjusted to find only one stable model of a knowledge
base  while traversing the super dependency graph  we generate only one model at each
node  if we arrive at a node where we cannot generate a model based on what we have
computed so far  we backtrack to the most recent node where several models were available
to choose from and take the next model that was not yet chosen  the worst case time
complexity of this algorithm is equal to the worst case time complexity of the algorithm for
finding all stable models because we may have to exhaust all possible ways of generating a
stable model before finding out that a certain knowledge base does not have a stable model
at all  nevertheless  we believe that in the average case  finding just one model will be
easier than finding them all  a similar modification of the aas algorithm is required if we
are interested in finding one model in which one particular atom gets the value true 
this work is another attempt to bridge the gap between the declarative systems  e g  
default logic  autoepistemic logic  and the procedural systems  e g   atms  prolog  of the
nonmonotonic reasoning community  it is argued that while the declarative methods are
sound  they are impractical since they are computationally expensive  and while the procedural methods are more ecient  it is dicult to completely understand their performance
or to evaluate their correctness  the work presented here illustrates that the declarative
and the procedural approaches can be combined to yield an ecient yet formally supported
nonmonotonic system 

acknowledgments
thanks to luigi palopoli for useful comments on earlier draft of this paper and to michelle
bonnice and gadi dechter for editing on parts of the manuscript  many thanks to the
anonymous referees for very useful comments 
some of this work was done while the author was visiting the cognitive systems laboratory  computer science department  university of california  los angeles  california 
usa  this work was partially supported by nsf grant iri         and by air force oce
of scientific research grant  f                
  

fiben eliyahu

references

antoniou  g     langetepe  e          soundness and completeness of a logic programming
approach to default logic  in aaai     proceedings of the   th national conference
on artificial intelligence  pp           aaai press  menlo park  calif 
apt  k   blair  h     walker  a          towards a theory of declarative knowledge  in
minker  j   ed    foundations of deductive databases and logic programs  pp         
morgan kaufmann 
bell  c   nerode  a   ng  r     subrahmanian  v          mixed integer programming
methods for computing non monotonic deductive databases  journal of the acm 
                  
ben eliyahu  r     dechter  r          propositional semantics for disjunctive logic programs  annals of mathematics and artificial intelligence             a short version
appears in jicslp     proceedings of the      joint international conference and
symposium on logic programming 
ben eliyahu  r     dechter  r          default reasoning using classical logic  artificial
intelligence                    
bidoit  n     froidevaux  c          minimalism subsumes default logic and circumscription
in stratified logic programming  in lics     proceedings of the ieee symposium on
logic in computer science  pp         ieee computer science press  los alamitos 
calif 
charniak  e   riesbeck  c  k     mcdermott  d  v          artificial intelligence programming  chap      lawrence erlbaum  hillsdale  nj 
cholewinski  p       a   reasoning with stratified default theories  in marek  w  v  
nerode  a     truszczynski  m   eds    logic programming and nonmonotonic reasoning  proceedings of the  rd international conference  pp           lecture notes in
computer science       springer verlag  berlin 
cholewinski  p       b   stratified default theories  in pacholski  l     tiuryn  a   eds   
computer science logic   th workshop  csl     selected papers  pp           lecture
notes in computer science       springer verlag  berlin 
dechter  r     dechter  a          structure driven algorithms for truth maintenance 
artificial intelligence                 
dimopoulos  y     magirou  v          a graph theoretic approach to default logic  journal
of information and computation               
doyle  j          a truth maintenance system  artificial intelligence              
dung  p  m          negation as hypothesis  an abductive foundation for logic programming  in furukawa  k   ed    iclp     proceedings of the  th international conference
on logic programming  pp        mit press 
  

fia hierarchy of tractable subsets
elkan  c          a rational reconstruction of nonmonotonic truth maintenance systems 
artificial intelligence              
eshghi  k     kowalski  r  a          abduction compared with negation by failure  in levi 
g     martelli  m   eds    iclp     proceedings of the  th international conference
on logic programming  pp           mit press 
fine  k          the justification of negation as failure  logic  methodology and philosophy
of science             
gelfond  m          on stratified autoepistemic theories  in aaai     proceedings of the
 th national conference on artificial intelligence  pp           morgan kaufmann 
gelfond  m     lifschitz  v          the stable model semantics for logic programming  in
kowalski  r  a     bowen  k  a   eds    logic programming  proceedings of the  th
international conference  pp             mit press 
gelfond  m     lifschitz  v          classical negation in logic programs and disjunctive
databases  new generation computing             
junker  u     konolige  k          computing the extensions of autoepistemic and default logics with a tms  in aaai     proceedings of the  th national conference on
artificial intelligence  pp           aaai press 
kakas  a  c     mancarella  p          stable theories for logic programs  in saraswat 
v     udea  k   eds    islp     proceedings of the      international symposium on
logic programming  pp          mit press 
kautz  h  a     selman  b          hard problems for simple default logics  artificial
intelligence              
leone  n   romeo  n   rullo  m     sacca  d          effective implementation of negation
in database logic query languages  in atzeni  p   ed    logidata   deductive
database with complex objects  pp           lecture notes in computer science      
springer verlag  berlin 
lifschitz  v     turner  h          splitting a logic program  in van hentenryck  p   ed   
iclp     proceedings of the   th international conference on logic programming  pp 
       mit press 
marek  v  w     truszczynski  m          nonmonotonic logic  context dependent reasoning  springer verlag  berlin 
marek  w     truszczynski  m          autoepistemic logic  journal of the acm     
        
moore  r  c          semantical consideration on nonmonotonic logic  artificial intelligence            
palopoli  l     zaniolo  c          polynomial time computable stable models   annals of
mathematics and artificial intelligence  in press 
  

fiben eliyahu
papadimitriou  c  h     sideri  m          default theories that always have extensions 
artificial intelligence              
pimentel  s  g     cuadrado  j  l          a truth maintenance system based on stable
models  in lusk  e  l     overbeek  r  a   eds    iclp     proceedings of the     
north american conference on logic programming  pp           mit press 
przymusinska  h     przymusinski  t          semantic issues in deductive databases and
logic programs  in banerji  r  b   ed    formal techniques in artificial intelligence 
a sourcebook  pp           north holland  new york 
reiter  r          a logic for default reasoning  artificial intelligence             
sacca  d     zaniolo  c          stable models and non determinism in logic programs with
negation  in pods     proceedings of the  th acm sigact sigmod sigart
symposium on principles of database systems  pp           acm press 
satoh  k     iwayama  n          computing abduction by using the tms  in furukawa  k 
 ed    iclp     proceedings of the  th international conference on logic programming 
pp           mit press 
subrahmanian  v   nau  d     vago  c          wfs   branch and bound   stable models 
ieee transactions on knowledge and data engineering                 
tarjan  r          depth first search and linear graph algorithms  siam journal on
computing             

  

fi