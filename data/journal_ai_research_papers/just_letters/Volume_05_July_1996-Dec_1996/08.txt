journal of artificial intelligence research                 

submitted       published      

exploiting causal independence in bayesian network inference
nevin lianwen zhang

lzhang   cs   ust  hk

department of computer science 
university of science   technology  hong kong

david poole

poole   cs   ubc   ca

department of computer science  university of british columbia 
     main mall  vancouver  b c   canada v t  z 

abstract
a new method is proposed for exploiting causal independencies in exact bayesian network inference  a bayesian network can be viewed as representing a factorization of a joint probability
into the multiplication of a set of conditional probabilities  we present a notion of causal independence that enables one to further factorize the conditional probabilities into a combination of even
smaller factors and consequently obtain a finer grain factorization of the joint probability  the new
formulation of causal independence lets us specify the conditional probability of a variable given its
parents in terms of an associative and commutative operator  such as or  sum or max  on the
contribution of each parent  we start with a simple algorithm ve for bayesian network inference
that  given evidence and a query variable  uses the factorization to find the posterior distribution of
the query  we show how this algorithm can be extended to exploit causal independence  empirical
studies  based on the cpcs networks for medical diagnosis  show that this method is more efficient
than previous methods and allows for inference in larger networks than previous algorithms 

   introduction
reasoning with uncertain knowledge and beliefs has long been recognized as an important research
issue in ai  shortliffe   buchanan        duda et al          several methodologies have been
proposed  including certainty factors  fuzzy sets  dempster shafer theory  and probability theory 
the probabilistic approach is now by far the most popular among all those alternatives  mainly due
to a knowledge representation framework called bayesian networks or belief networks  pearl       
howard   matheson        
bayesian networks are a graphical representation of  in dependencies amongst random variables 
a bayesian network  bn  is a dag with nodes representing random variables  and arcs representing
direct influence  the independence that is encoded in a bayesian network is that each variable is
independent of its non descendents given its parents 
bayesian networks aid in knowledge acquisition by specifying which probabilities are needed 
where the network structure is sparse  the number of probabilities required can be much less than the
number required if there were no independencies  the structure can be exploited computationally to
make inference faster  pearl        lauritzen   spiegelhalter        jensen et al         shafer  
shenoy        
the definition of a bayesian network does not constrain how a variable depends on its parents 
often  however  there is much structure in these probability functions that can be exploited for knowledge acquisition and inference  one such case is where some dependencies depend on particular
values of other variables  such dependencies can be stated as rules  poole         trees  boutilier

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fiz hang   p oole

et al         or as multinets  geiger   heckerman         another is where the the function can be
described using a binary operator that can be applied to values from each of the parent variables  it
is the latter  known as causal independencies  that we seek to exploit in this paper 
causal independence refers to the situation where multiple causes contribute independently to
a common effect  a well known example is the noisy or gate model  good         knowledge
engineers have been using specific causal independence models in simplifying knowledge acquisition  henrion        olesen et al         olesen   andreassen         heckerman        was the
first to formalize the general concept of causal independence  the formalization was later refined by
heckerman and breese        
kim and pearl        showed how the use of noisy or gate can speed up inference in a special
kind of bns known as polytrees  dambrosio              showed the same for two level bns with
binary variables  for general bns  olesen et al         and heckerman        proposed two ways
of using causal independencies to transform the network structures  inference in the transformed
networks is more efficient than in the original networks  see section    
this paper proposes a new method for exploiting a special type of causal independence  see section    that still covers common causal independence models such as noisy or gates  noisy maxgates  noisy and gates  and noisy adders as special cases  the method is based on the following
observation  a bn can be viewed as representing a factorization of a joint probability into the multiplication of a list of conditional probabilities  shachter et al         zhang   poole        li  
dambrosio         the type of causal independence studied in this paper leads to further factorization of the conditional probabilities  section     a finer grain factorization of the joint probability
is obtained as a result  we propose to extend exact inference algorithms that only exploit conditional
independencies to also make use of the finer grain factorization provided by causal independence 
the state of art exact inference algorithm is called clique tree propagation  ctp   lauritzen  
spiegelhalter        jensen et al         shafer   shenoy         this paper proposes another algorithm called variable elimination  ve    section     that is related to spi  shachter et al         li
  dambrosio         and extends it to make use of the finer grain factorization  see sections      
and     rather than compiling to a secondary structure and finding the posterior probability for each
variable  ve is query oriented  it needs only that part of the network relevant to the query given the
observations  and only does the work necessary to answer that query  we chose ve instead of ctp
because of its simplicity and because it can carry out inference in large networks that ctp cannot
deal with 
experiments  section     have been performed with two cpcs networks provided by pradhan 
the networks consist of     and     nodes respectively and they both contain abundant causal independencies  before this paper  the best one could do in terms of exact inference would be to first
transform the networks by using jensen et al s or heckermans technique and then apply ctp  in
our experiments  the computer ran out of memory when constructing clique trees for the transformed
networks  when this occurs one cannot answer any query at all  however  the extended ve algorithm has been able to answer almost all randomly generated queries with twenty or less observations
 findings  in both networks 
one might propose to first perform jensen et al s or heckermans transformation and then apply
ve   our experiments show that this is significantly less efficient than the extended ve algorithm 
we begin with a brief review of the concept of a bayesian network and the issue of inference 
   

fie xploiting c ausal i ndependence

in

b ayesian n etwork i nference

   bayesian networks
we assume that a problem domain is characterized by a set of random variables  beliefs are represented by a bayesian network  bn   an annotated directed acyclic graph  where nodes represent
the random variables  and arcs represent probabilistic dependencies amongst the variables  we use
the terms node and variable interchangeably  associated with each node is a conditional probability of the variable given its parents 
in addition to the explicitly represented conditional probabilities  a bn also implicitly represents
conditional independence assertions  let x    x         xn be an enumeration of all the nodes in a bn
such that each node appears after its children  and let xi be the set of parents of a node xi   the
bayesian network represents the following independence assertion 
each variable xi is conditionallyindependent of the variables in fx    x          xi   g given
values for its parents 
the conditional independence assertions and the conditional probabilities together entail a joint probability over all the variables  by the chain rule  we have 

p  x   x          xn 

 

 

n
y
i  
n
y
i  

p  xi jx   x          xi   
p  xi jx   
i

   

where the second equation is true because of the conditional independence assertions  the conditional probabilities p  xi jxi   are given in the specification of the bn  consequently  one can  in
theory  do arbitrary probabilistic reasoning in a bn 
    inference
inference refers to the process of computing the posterior probability p  x jy  y    of a set x of
query variables after obtaining some observations y  y    here y is a list of observed variables and
y  is the corresponding list of observed values  often  x consists of only one query variable 
in theory  p  x jy  y    can be obtained from the marginal probability p  x  y    which in turn
can be computed from the joint probability p  x    x          xn  by summing out variables outside
x  y one by one  in practice  this is not viable because summing out a variable from a joint probability requires an exponential number of additions 
the key to more efficient inference lies in the concept of factorization  a factorization of a joint
probability is a list of factors  functions  from which one can construct the joint probability 
a factor is a function from a set of variables into a number  we say that the factor contains a variable if the factor is a function of that variable  or say it is a factor of the variables on which it depends 
suppose f  and f  are factors  where f  is a factor that contains variables x           xi  y          yj 
we write this as f   x           xi  y          yj    and f  is a factor with variables y           yj   z          zk  
where y           yj are the variables in common to f  and f    the product of f  and f  is a factor that
is a function of the union of the variables  namely x           xi  y          yj   z          zk   defined by 

f  f   x          xi  y          yj   z          zk    f  x          xi  y          yj  f  y          yj   z          zk  

 

   

fiz hang   p oole

c

b

a

e

e

 

 

e 

figure    a bayesian network 
let f  x           xi   be a function of variable x          xi   setting  say x  in f  x           xi  to a particular
value ff yields f  x   ff  x          xi   which is a function of variables x           xi 
if f  x          xi  is a factor  we can sum out a variable  say x    resulting in a factor of variables
x           xi  defined

x

 

x 

f   x          xi    f  x   ff    x          xi         f  x  ffm  x          xi 

where ff           ffm are the possible values of variable x  
because of equation      a bn can be viewed as representing a factorization of a joint probability 
for example  the bayesian network in figure   factorizes the joint probability p  a  b  c  e   e   e  
into the following list of factors 

p  a   p  b   p  c   p  e ja  b  c   p  e ja  b  c   p  e je   e    
multiplying those factors yields the joint probability 
suppose a joint probability p  z    z          zm   is factorized into the multiplication of a list of factors f    f         fm   while obtaining p  z           zm   by summing out z  from p  z    z          zm   requires an exponential number of additions  obtaining a factorization of p  z           zm   can often be
done with much less computation  consider the following procedure 
procedure sum out f   z   




inputs  f  a list of factors  z  a variable 
output  a list of factors 

   remove from the f all the factors  say f         fk   that contain z  
   add the new factor

p qk f to f and return f  
z i   i

theorem   suppose a joint probability p  z    z          zm  is factorized into the multiplication of a
list f of factors  then sum out f   z    returns a list of factors whose multiplicationis p  z           zm   
   

fie xploiting c ausal i ndependence

in

b ayesian n etwork i nference

proof  suppose f consists of factors f    f         fm and suppose z  appears in and only in factors
f   f        fk   then

p  z           zm 

 

 

x
z 

p  z    z          zm  

m
xy
z  i  

fi    

k
xy
z  i  

m
y

fi   

i k  

fi   

the theorem follows   
only variables that appear in the factors f    f         fk participated in the computation of sum out f   z    
and those are often only a small portion of all the variables  this is why inference in a bn can be
tractable in many cases  even if the general problem is np hard  cooper        

   the variable elimination algorithm
based on the discussions of the previous section  we present a simple algorithm for computing p  x jy  y    
the algorithm is based on the intuitions underlying dambrosios symbolic probabilistic inference
 spi   shachter et al         li   dambrosio         and first appeared in zhang and poole        
it is essentially dechter       s bucket elimination algorithm for belief assessment 
the algorithm is called variable elimination  ve   because it sums out variables from a list of
factors one by one  an ordering  by which variables outside x  y to be summed out is required as
an input  it is called an elimination ordering 
procedure ve  f   x  y  y    





inputs  f  the list of conditional probabilities in a bn 
x  a list of query variables 
y  a list of observed variables 
y   the corresponding list of observed values 
  an elimination ordering for variables outside x  y  
output  p  x jy  y    

   set the observed variables in all factors to their corresponding observed values 
   while  is not empty 

 a  remove the first variable z from  
 b  call sum out f   z    endwhile

   set h   the multiplication of all the factors on f  
   h is a function of variables in x     

   return h x   

p h x       renormalization   
x

theorem   the output of ve f   x  y  y     is indeed p  x jy  y    
proof  consider the following modifications to the procedure  first remove step    then the factor
h produced at step   will be a function of variables in x and y   add a new step after step   that sets
the observed variables in h to their observed values 
   

fiz hang   p oole

let f  y  a  be a function of variable y and of variables in a  we use f  y  a jy ff to denote
f  y ff  a   let f  y      g  y      and h y  z     be three functions of y and other variables  it is
evident that

f  y    g  y    jy ff   f  y    jy ff g y    jy ff 
x
x
 
h y  z     jy ff    h y  z    jy ff   
z

z

consequently  the modifications do not change the output of the procedure 
according to theorem    after the modifications the factor produced at step   is simply the marginal
probability p  x  y    consequently  the output is exactly p  x jy  y      
the complexity of ve can be measured by the number of numerical multiplications and numerical summations it performs  an optimal elimination ordering is one that results in the least complexity  the problem of finding an optimal elimination ordering is np complete  arnborg et al         
commonly used heuristics include minimum deficiency search  bertele   brioschi        and maximum cardinality search  tarjan   yannakakis         kjrulff        has empirically shown that
minimum deficiency search is the best existing heuristic  we use minimum deficiency search in our
experiments because we also found it to be better than the maximum cardinality search 
   

ve

versus clique tree propagation

clique tree propagation  lauritzen   spiegelhalter        jensen et al         shafer   shenoy 
      has a compilation step that transforms a bn into a secondary structure called clique tree or
junction tree  the secondary structure allows ctp to compute the answers to all queries with one
query variable and a fixed set of observations in twice the time needed to answer one such query in
the clique tree  for many applications this is a desirable property since a user might want to compare
the posterior probabilities of different variables 
ctp takes work to build the secondary structure before any observations have been received 
when the bayesian network is reused  the cost of building the secondary structure can be amortized
over many cases  each observation entails a propagation though the network 
given all of the observations  ve processes one query at a time  if a user wants the posterior
probabilities of several variables  or for a sequence of observations  she needs to run ve for each of
the variables and observation sets 
the cost  in terms of the number of summations and multiplications  of answering a single query
with no observations using ve is of the same order of magnitude as using ctp  a particular clique
tree and propagation sequence encodes an elimination ordering  using ve on that elimination ordering results in approximately the same summations and multiplications of factors as in the ctp  there
is some discrepancy  as ve does not actually form the marginals on the cliques  but works with conditional probabilities directly   observations make ve simpler  the observed variables are eliminated
at the start of the algorithm   but each observation in ctp requires propagation of evidence  because
ve is query oriented  we can prune nodes that are irrelevant to specific queries  geiger et al        
lauritzen et al         baker   boult         in ctp  on the other hand  the clique tree structure is
kept static at run time  and hence does not allow pruning of irrelevant nodes 
ctp encodes a particular space time tradeoff  and ve another  ctp is particularly suited to the
case where observations arrive incrementally  where we want the posterior probability of each node 
   

fie xploiting c ausal i ndependence

in

b ayesian n etwork i nference

and where the cost of building the clique tree can be amortized over many cases  ve is suited for
one off queries  where there is a single query variable and all of the observations are given at once 
unfortunately  there are large real world networks that ctp cannot deal with due to time and
space complexities  see section    for two examples   in such networks  ve can still answer some
of the possible queries because it permits pruning of irrelevant variables 

   causal independence
bayesian networks place no restriction on how a node depends on its parents  unfortunately this
means that in the most general case we need to specify an exponential  in the number of parents 
number of conditional probabilities for each node  there are many cases where there is structure in
the probability tables that can be exploited for both acquisition and for inference  one such case that
we investigate in this paper is known as causal independence 
in one interpretation  arcs in a bn represent causal relationships  the parents c   c          cm of a
variable e are viewed as causes that jointly bear on the effect e  causal independence refers to the
situation where the causes c    c          cm contribute independently to the effect e 
more precisely  c   c          cm are said to be causally independent w r t  effect e if there exist
random variables              m that have the same frame  i e   the same set of possible values  as e
such that
   for each i  i probabilistically depends on ci and is conditionally independent of all other cj s
and all other j s given ci   and
   there exists a commutative and associative binary operator
e             m  
using the independence notion of pearl         let
given z   the first condition is 

 over the frame of e such that

i  x  y jz   mean that x is independent of y

i     fc          cm            mgjc  
and similarly for the other variables  this entails i     cj jc   and i     j jc   for each cj and j
where j      
we refer to i as the contribution of ci to e  in less technical terms  causes are causally independent w r t  their common effect if individual contributions from different causes are independent and
the total influence on the effect is a combination of the individual contributions 
we call the variable e a convergent variable as it is where independent contributions from different sources are collected and combined  and for the lack of a better name   non convergent variables
will simply be called regular variables  we call  the base combination operator of e 
the definition of causal independence given here is slightly different than that given by heckerman and breese        and srinivas         however  it still covers common causal independence
models such as noisy or gates  good        pearl         noisy max gates  dez         noisy
and gates  and noisy adders  dagum   galper        as special cases  one can see this in the following examples 
example    lottery  buying lotteries affects your wealth  the amounts of money you spend on
buying different kinds of lotteries affect your wealth independently  in other words  they are causally
   

fiz hang   p oole

independent w r t  the change in your wealth  let c          ck denote the amounts of money you spend
on buying k types of lottery tickets  let           k be the changes in your wealth due to buying the
different types of lottery tickets respectively  then  each i depends probabilistically on ci and is
conditionally independent of the other cj and j given ci   let e be the total change in your wealth
due to lottery buying  then e         k   hence c           ck are causally independent w r t  e  the
base combination operator of e is numerical addition  this example is an instance of a causal independence model called noisy adders 
if c           ck are the amounts of money you spend on buying lottery tickets in the same lottery 
then c           ck are not causally independent w r t  e  because winning with one ticket reduces the
chance of winning with the other  thus    is not conditionally independent of   given c   however 
if the ci represent the expected change in wealth in buying tickets in the same lottery  then they would
be causally independent  but not probabilistically independent  there would be arcs between the ci s  
example    alarm  consider the following scenario  there are m different motion sensors each
of which are connected to a burglary alarm  if one sensor activates  then the alarm rings  different
sensors could have different reliability  we can treat the activation of sensor i as a random variable 
the reliability of the sensor can be reflected in the i   we assume that the sensors fail independently  
assume that the alarm can only be caused by a sensor activation   then alarm        m   the
base combination operator here is the logical or operator  this example is an instance of a causal
independence model called the noisy or gate 
the following example is not an instance of any causal independence models that we know 
example    contract renewal  faculty members at a university are evaluated in teaching  research 
and service for the purpose of contract renewal  a faculty members contract is not renewed  renewed without pay raise  renewed with a pay raise  or renewed with double pay raise depending on
whether his performance is evaluated unacceptable in at least one of the three areas  acceptable in
all areas  excellent in one area  or excellent in at least two areas 
let c    c   and c  be the fractions of time a faculty member spends on teaching  research  and
service respectively  let i represent the evaluation he gets in the ith area  it can take values      
and   depending on whether the evaluation is unacceptable  acceptable  or excellent  the variable
i depends probabilistically on ci  it is reasonable to assume that i is conditionally independent of
other cj s and other j s given ci  
let e represent the contract renewal result  the variable can take values          and   depending
on whether the contract is not renewed  renewed with no pay raise  renewed with a pay raise  or
renewed with double pay raise  then e       where the base combination operator  is given
in this following table 

 
 
 
 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

   this is called the exception independence assumption by pearl        
   this is called the accountability assumption by pearl         the assumption can always be satisfied by introducing
a node that represent all other causes  henrion        

   

fie xploiting c ausal i ndependence

in

b ayesian n etwork i nference

so  the fractions of time a faculty member spends in the three areas are causally independent
w r t  the contract renewal result 
in the traditional formulation of a bayesian network we need to specify an exponential  in the
number of parents  number of conditional probabilities for a variable  with causal independence 
the number of conditional probabilities p  i jci   is linear in m  this is why causal independence
can reduce complexity of knowledge acquisition  henrion        pearl        olesen et al        
olesen   andreassen         in the following sections we show how causal independence can also
be exploited for computational gain 
    conditional probabilities of convergent variables
ve allows us to exploit structure in a bayesian network by providing a factorization of the joint probability distribution  in this section we show how causal independence can be used to factorize the
joint distributioneven further  the initial factors in the ve algorithm are of the form p  ejc          cm  
we want to break this down into simpler factors so that we do not need a table exponential in m  the
following proposition shows how causal independence can be used to do this 

proposition   let e be a node in a bn and let c    c          cm be the parents of e  if c   c          cm
are causally independent w r t  e  then the conditional probability p  ejc          cm  can be obtained
from the conditional probabilities p  i jci  through

p  e ffjc          cm   

x

ff     ffk  ff

p     ff  jc       p  m  ffmjcm   

   

for each value ff of e  here  is the base combination operator of e 
proof   the definition of causal independence entails the independence assertions

i     fc          cmgjc   and i      jc   
by the axiom of weak union  pearl        p       we have i      jfc          cmg   thus all of the i
mutually independent given fc          cmg 
also we have  by the definition of causal independence i     fc          cm gjc    so
p   jfc   c          cmg    p   jc  
thus we have 

p  e ffjc          cm 
  p      m  ffjc          cm 
x
 
p    ff           m ffm jc          cm 
ff     ff  ff
x
 
p    ff  jc          cm  p    ff  jc          cm     p  m ffm jc          cm 
ff     ff  ff
x
 
p    ff  jc  p    ff jc      p  m  ffm jcm 
ff     ff  ff
 
k

k

k

the next four sections develop an algorithm for exploiting causal independence in inference 
   thanks to an anonymous reviewer for helping us to simplify this proof 

   

fiz hang   p oole

   causal independence and heterogeneous factorizations
in this section  we shall first introduce an operator for combining factors that contain convergent
variables  the operator is a basic ingredient of the algorithm to be developed in the next three sections  using the operator  we shall rewrite equation     in a form that is more convenient to use in
inference and introduce the concept of heterogeneous factorization 
consider two factors f and g   let e         ek be the convergent variables that appear in both f and
g   let a be the list of regular variables that appear in both f and g   let b be the list of variables that
appear only in f   and let c be the list of variables that appear only in g   both b and c can contain
convergent variables  as well as regular variables  suppose i is the base combination operator of
ei  then  the combination f 
g of f and g is a function of variables e        ek and of the variables
in a  b   and c   it is defined by  

f 
g  e  x
ff           ek  ffkx
  a  b  c  
 
   
f  e  ff           ek  ffk    a  b 
ff     ff    ff 

ffk  k ffk   ffk

g  e  ff           ek ffk   a  c   

   

for each value ffi of ei   we shall sometimes write f 
g as f  e           ek   a  b  
g  e          ek   a  c  
to make explicit the arguments of f and g  
note that base combination operators of different convergent variables can be different 
the following proposition exhibits some of the basic properties of the combination operator 
 
proposition      if f and g do not share any convergent variables  then f 
g is simply the multiplication of f and g      the operator 
 is commutative and associative 
proof  the first item is obvious  the commutativity of 
 follows readily from the commutativity of
multiplication and the base combination operators  we shall prove the associativity of 
 in a special
case  the general case can be proved by following the same line of reasoning 
suppose f   g   and h are three factors that contain only one variable e and the variable is convergent  we need to show that  f 
g  
h f 
 g 
h   let  be the base combination operator of e  by
the associativity of   we have  for any value ff of e  that

f 
g  
h e ff 

 

 
 
 
 

x

f 
g  e ff  h e ff  
x
x
 
f  e ff   g e ff   h e ff   
ff  ff   ff ff  ff   ff 
x
f  e ff  g  e ff   h e ff   
ff  ff  ff   ff
x
x
f  e ff   
g  e ff   h e ff   
ff  ff   ff

ff  ff   ff

ff ff   ff 

   note that the base combination operators under the summations are indexed  with each convergent variable is an associated operator  and we always use the binary operator that is associated with the corresponding convergent variable 
in the examples  for ease of exposition  we will use one base combination operator  where there is more than one type
of base combination operator  e g   we may use or  sum and max for different variables in the same network   we
have to keep track of which operators are associated with which convergent variables  this will  however  complicate
the description 

   

fie xploiting c ausal i ndependence

x

 

ff  ff   ff

in

b ayesian n etwork i nference

f  e ff  g 
h e ff  

f 
 g 
h  e ff  

 

the proposition is hence proved  
the following propositions give some properties for 
 that correspond to the operations that we
exploited for the algorithm ve   the proofs are straight forward and are omitted 
proposition   suppose f and g are factors and variable z appears in f and not in g   then

x

xz
z

x

fg 

 

 

f 
g  

 

 

 

 

z
x
z

f  g  and
f  
g 

proposition   suppose f   g and h are factors such that g and h do not share any convergent variables  then

g  f 
h     gf  
h 

   

    rewriting equation  
noticing that the contribution variable i has the same possible values as e  we define functions

fi e  ci  by

fi e ff  ci     p  i  ffjci  
for any value ff of e  we shall refer to fi as the contributing factor of ci to e 
by using the operator 
  we can now rewrite equation     as follows
p  ejc          cm    
mi   fi  e  ci  

   

it is interesting to notice the similarity between equation     and equation      in equation    
conditional independence allows one to factorize a joint probability into factors that involve less
variables  while in equation     causal independence allows one to factorize a conditional probability
into factors that involve less variables  however  the ways by which the factors are combined are
different in the two equations 
    heterogeneous factorizations
consider the bayesian network in figure    it factorizes the joint probability p  a  b  c  e   e   e  
into the following list of factors 

p  a   p  b   p  c   p  e ja  b  c   p  e ja  b  c   p  e je   e    
we say that this factorization is homogeneous because all the factors are combined in the same way 
i e   by multiplication 
now suppose the ei s are convergent variables  then their conditional probabilities can be further factorized as follows 

p  e ja  b  c 
p  e ja  b  c 
p  e  je    e  

 
 
 

f    e   a 
f    e    b 
f    e   c  
f    e   a 
f    e    b 
f    e   c  
f    e   e  
f    e   e    
   

fiz hang   p oole

where the factor f   e    a   for instance  is the contributing factor of a to e   
we say that the following list of factors

f   e    a   f   e   b   f   e   c   f   e    a   f   e    b   f   e    c   f   e    e    f   e    e   
p  a   p  b   and p  c 
   
constitute a heterogeneous factorization of p  a  b  c  e   e   e   because the joint probability can be
obtained by combining those factors in a proper order using either multiplication or the operator 
 
the word heterogeneous is to signify the fact that different factor pairs might be combined in different ways  we call each fij a heterogeneous factor because it needs to be combined with the other
fik s by the operator 
 before it can be combined with other factors by multiplication  in contrast 
we call the factors p  a   p  b   and p  c  homogeneous factors 
we shall refer to that heterogeneous factorization as the heterogeneous factorization represented
by the bn in figure    it is obvious that this heterogeneous factorization is of finer grain than the
homogeneous factorization represented by the bn 

   flexible heterogeneous factorizations and deputation
this paper extends ve to exploit this finer grain factorization  we will compute the answer to a query
by summing out variables one by one from the factorization just as we did in ve  
the correctness of ve is guaranteed by the fact that factors in a homogeneous factorization can
be combined  by multiplication  in any order and by the distributivity of multiplication over summations  see the proof of theorem    
according to proposition    the operator 
 is distributive over summations  however  factors in
a heterogeneous factorization cannot be combined in arbitrary order  for example  consider the heterogeneous factorization      while it is correct to combine f   e    a  and f    e   b  using 
  and to
combine f    e   e    and f    e   e    using 
  it is not correct to combine f    e   a  and f    e   e  
with 
  we want to combine these latter two by multiplication  but only after each has been combined with its sibling heterogeneous factors 
to overcome this difficulty  a transformation called deputation will be performed on our bn 
the transformation does not change the answers to queries  and the heterogeneous factorization
represented by the transformed bn is flexible in the following sense 
a heterogeneous factorization of a joint probability is flexible if 
the joint probability
 

multiplication of all homogeneous factors

 combination  by 
  of all heterogeneous factors 

   

this property allows us to carry out multiplication of homogeneous factors in arbitrary order 
and since 
 is associative and commutative  combination of heterogeneous factors in arbitrary order  if the conditions of proposition   are satisfied  we can also exchange a multiplication with a
combination by 
  to guarantee the conditions of proposition    the elimination ordering needs to
be constrained  sections   and    
the heterogeneous factorization of p  a  b  c  e   e   e   given at the end of the previous section is
not flexible  consider combining all the heterogeneous factors  since the operator 
 is commutative
   

fie xploiting c ausal i ndependence

in

b ayesian n etwork i nference

c

b

a
e 

e 

e 

e 
e 
e 

figure    the bn in figure   after the deputation of convergent variables 
and associative  one can first combine  for each i  all the fik s  obtaining the conditional probability
of ei   and then combine the resulting conditional probabilities  the combination

p  e  ja  b  c 
p  e  ja  b  c 
p  e je    e  
is not the same as the multiplication

p  e  ja  b  c p  e ja  b  c p  e je   e  
because the convergent variables e  and e  appear in more than one factor  consequently  equation
    does not hold and the factorization is not flexible  this problem arises when a convergent variable is shared between two factors that are not siblings  for example  we do not want to combine
f    e   a  and f    e    e   using 
  in order to tackle this problem we introduce a new deputation
variable so that each heterogeneous factor contains a single convergent variable 
deputation is a transformation that one can apply to a bn to make the heterogeneous factorization represented by the bn flexible  let e be a convergent variable  to depute e is to make a copy
e  of e  make the parents of e be parents of e    replace e with e  in the contributing factors of e  make
e  the only parent of e  and set the conditional probability p  eje    as follows 

p  eje     

 

 
 

if e   e 
otherwise

   

we shall call e  the deputy of e  the deputy variable e  is a convergent variable by definition  the
variable e  which is convergent before deputation  becomes a regular variable after deputation  we
shall refer to it as a new regular variable  in contrast  we shall refer to the variables that are regular
before deputation as old regular variables  the conditional probability p  e  je  is a homogeneous
factor by definition  it will sometimes be called the deputing function and written as i  e   e  since it
ensures that e  and e always take the same value 
a deputation bn is obtained from a bn by deputing all the convergent variables  in a deputation
bn  deputy variables are convergent variables and only deputy variables are convergent variables 
   

fiz hang   p oole

figure   shows the deputation of the bn in figure    it factorizes the joint probability

p  a  b  c  e   e    e   e    e   e   
into homogeneous factors

p  a   p  b   p  c   i  e    e    i  e    e    i  e    e   
and heterogeneous factors

f   e     a   f   e    b   f   e     c   f   e     a   f   e    b   f   e    c   f   e    e    f   e    e   
this factorization has three important properties 
   each heterogeneous factor contains one and only one convergent variable   recall that the ei s
are no longer convergent variables and their deputies are  
   each convergent variable e  appears in one and only one homogeneous factor  namely the
deputing function i  e   e  
   except for the deputing functions  none of the homogeneous factors contain any convergent
variables 
those properties are shared by the factorization represented by any deputation bn 
proposition   the heterogeneous factorization represented by a deputation bn is flexible 
proof  consider the combination  by 
  of all the heterogeneous factors in the deputation bn  since
the combination operator 
 is commutative and associative  we can carry out the combination in following two steps  first for each convergent  deputy  variable e    combine all the heterogeneous factors that contain e    yielding the conditional probability p  e  je   of e    then combine those resulting
conditional probabilities  it follows from the first property mentioned above that for different convergent variables e   and e    p  e   je    and p  e   je    do not share convergent variables  hence the
combination of the p  e  je  s is just the multiplication of them  consequently  the combination 
by 
  of all heterogeneous factors in a deputation bn is just the multiplication of the conditional
probabilities of all convergent variables  therefore  we have
 

 

 

 

the joint probability of variables in a deputation bn
  multiplication of conditional probabilities of all variables
 

multiplication of conditional probabilities of all regular variables

 

multiplication of all homogeneous factors

multiplication of conditional probabilities of all convergent variables

combination  by 
  of all heterogeneous factors 
the proposition is hence proved   
deputation does not change the answer to a query  more precisely  we have
proposition   the posterior probability p  x jy  y    is the same in a bn as in its deputation 
   

fie xploiting c ausal i ndependence

in

b ayesian n etwork i nference

proof  let r  e   and e   be the lists of old regular  new regular  and deputy variables in the deputation bn respectively  it suffices to show that p  r  e   is the same in the original bn as in the
deputation bn  for any new regular variable e  let e  be its deputy  it is easy to see that the quantity
p i  e   e p  e j   in the deputation bn is the same as p  ej   in the original bn  hence 
e
e
e
 

 

p  r  ex  in the deputation bn
 
p  r  e  e   
e y
x
y
 
p  rjr    p  eje p  e  je   
e r r
 e
y
y ex
 
p  rjr     i  e   e p  e je   
r r
e e e
y
y
 
p  rjr   p  eje  
 

 

 

 

 

r r

 

the proposition is proved 

 

e  e

p  r  e   in the original bn 

   tidy heterogeneous factorizations
so far  we have only encountered heterogeneous factorizations that correspond to bayesian networks 
in the following algorithm  the intermediate heterogeneous factorizations do not necessarily correspond by bns  they do have the property that they combine to form the appropriate marginal probabilities  the general intuition is that the heterogeneous factors must combine with their sibling heterogeneous factors before being multiplied by factors containing the original convergent variable 
in the previous section  we mentioned three properties of the heterogeneous factorization represented by a deputation bn  and we used the first property to show that the factorization is flexible 
the other two properties qualify the factorization as a tidy heterogeneous factorization  which is defined below 
let z    z         zk be a list of variables in a deputation bn such that if a convergent  deputy 
variable e  is in fz   z           zk g  so is the corresponding new regular variable e  a flexible heterogeneous factorization of p  z    z          zk   is said to be tidy if

   for each convergent  deputy  variable e  fz   z          zk g  the factorization contains the deputing function i  e   e  and it is the only homogeneous factor that involves e   

   except for the deputing functions  none of the homogeneous factors contain any convergent
variables 
as stated earlier  the heterogeneous factorization represented by a deputation bn is tidy 
under certain conditions  to be given in theorem    one can obtain a tidy factorization of p  z           zk  
by summing out z  from a tidy factorization of p  z    z          zk   using the the following procedure 
procedure sum out  f    f   z  



inputs  f   a list of homogeneous factors 
f   a list of heterogeneous factors 
z  a variable 
   

fiz hang   p oole



output  a list of heterogeneous factors and a list of homogeneous factors 

   remove from f  all the factors that contain z   multiply them resulting in  say  f  
if there are no such factors  set f  nil 

   remove from f  all the factors that contain z   combine them by using 
 resulting
in  say  g   if there are no such factors  set g  nil 

p f to f  
 
z
p
else add the new  heterogeneous  factor z fg to f  
return  f   f   

   if g  nil  add the new  homogeneous  factor
  
  

theorem   suppose a list of homogeneous factors f  and a list of heterogeneous factors f  constitute a tidy factorization of p  z    z          zk    if z  is either a convergent variable  or an old regular
variable  or a new regular variable whose deputy is not in the list fz          zk g  then the procedure
sum out  f    f   z   returns a tidy heterogeneous factorization of p  z           zk   
the proof of this theorem is quite long and hence is given in the appendix 

   causal independence and inference
our task is to compute p  x jy  y    in a bn  according to proposition    we can do this in the
deputation of the bn 
an elimination ordering consisting of the variables outside x  y is legitimate if each deputy
variable e  appears before the corresponding new regular variable e  such an ordering can be found
using  with minor adaptations  minimum deficiency search or maximum cardinality search 
the following algorithm computes p  x jy  y    in the deputation bn  it is called ve   because
it is an extension of ve  
procedure ve    f   f   x  y  y    





inputs  f   the list of homogeneous factors
in the deputation bn 
f   the list of heterogeneous factors
in the deputation bn 
x  a list of query variables 
y  a list of observed variables 
y   the corresponding list of observed values 
  a legitimate elimination ordering 
output  p  x jy  y    

   set the observed variables in all factors to their observed values 
   while  is not empty 

 remove the first variable z from  
  f   f     sum out  f   f   z   endwhile
   

fie xploiting c ausal i ndependence

in

b ayesian n etwork i nference

   set h multiplication of all factors in f 
 combination  by 
  of all factors in f  
   h is a function of variables in x     
   return h x   

p h x       renormalization   
x

theorem   the output of ve    f   f   x  y  y     is indeed p  x jy  y    
proof  consider the following modifications to the algorithm  first remove step    then the factor
h produced at step   is a function of variables in x and y   add a new step after step   that sets
the observed variables in h to their observed values  we shall first show that the modifications do
not change the output of the algorithm and then show that the output of the modified algorithm is
p  x jy  y    
let f  y      g  y      and h y  z     be three functions of y and other variables  it is evident that

f  y    g  y    jy ff   f  y    jy ff g y    jy ff 
x
x
 
h y  z     jy ff    h y  z    jy ff   
z

z

if y is a regular variable  we also have

f  y    
g  y    jy ff   f  y    jy ff 
g  y    jy ff  
consequently  the modifications do not change the output of the procedure 
since the elimination ordering  is legitimate  it is always the case that if a deputy variable e  has
not been summed out  neither has the corresponding new regular variable e  let z         zk be the remaining variables in  at any time during the execution of the algorithm  then  e   fz           zk g implies e fz           zk g  this and the fact that the factorization represented by a deputation bn is tidy
enable us to repeatedly apply theorem   and conclude that  after the modifications  the factor created
at step   is simply the marginal probability p  x  y    consequently  the output is p  x jy  y      
    an example
this subsection illustrates ve   by walking through an example  consider computing the p  e  je    
in the deputation bayesian network shown in figure    suppose the elimination ordering  is  a  b 
c  e    e    e   and e     after the first step of ve   

f    fp  a   p  b   p  c   i  e    e    i  e    e    i  e    e    g 
f    ff   e    a   f   e    b   f   e    c   f   e    a   f   e    b   f   e    c   f   e    e    f   e    e  g 
now the procedure enters the while loop and it sums out the variables in  one by one 
after summing out a 
f    fp  b   p  c   i  e    e    i  e    e    i  e    e    g 
f    ff   e    b   f  p e    c   f   e    b   f   e    c   f   e    e    f   e    e      e    e   g 
where    e    e      a p  a f   e     a f   e     a  
after summing out b 
f    fp  c   i  e    e    i  e    e    i  e    e    g 
f    ff   e    c   f  p e    c   f   e    e    f   e    e      e    e       e    e   g 
where    e    e      b p  b f   e     b f   e    b  
   

fiz hang   p oole

after summing out c 
f    fi  e    e    i  e    e    i  e    e    g 
f    ff   e    e    fp
   e     e      e     e       e     e       e     e   g 
 
 
where    e   e     c p  c f    e    c f   e     c  
after summing out e    
f    fi  e    e    i  e    e    g 
f    ff   e    e    fp
   e     e      e    e   g 
where    e   e      e i  e     e      e    e   
    e    e   
    e    e     
 
after summing out e    
f    fi  e    e    g 
f    ff   e    e    fp
   e     e      e    e  g 
where    e   e     e i  e     e     e    e    
 
after summing out e   
f    fi  e    e    g 
f    ff   e    e    p  e    e  g 
where    e    e     e  f   e     e     e    e   
finally  after summing out e    
f       
f    f   e  g  p
where    e     e i  e     e     f   e    e   
   e    e      now the procedure enters step    where
 
p
there is nothing to do in this example  finally  the procedure returns   e     e    e     which is
p  e je       the required probability 
 

 

 

    comparing ve and ve  
in comparing ve and ve     we notice that when summing out a variable  they both combine only
those factors that contain the variable  however  the factorization that the latter works with is of
finer grain than the factorization used by the former  in our running example  the latter works with a
factorization which initially consists of factors that contain only two variables  while the factorization the former uses initially include factors that contain five variables  on the other hand  the latter
uses the operator 
 which is more expensive than multiplication  consider  for instance  calculating
f  e  a 
g e  b   suppose e is a convergent variable and all variables are binary  then the operation requires    numerical multiplications and         numerical summations  on the other hand 
multiplying f  e  a  and g  e  b  only requires    numerical multiplications 

despite the expensiveness of the operator 
  ve   is more efficient than ve   we shall provide
empirical evidence in support of this claim in section     to see a simple example where this is true 
consider the bn in figure       where e is a convergent variable  suppose all variables are binary 
then  computing p  e  by ve using the elimination ordering c    c   c   and c  requires          
           numerical multiplications and                                                  
numerical additions  on the other hand  computing p  e  in the deputation bn shown in figure     
by ve   using the elimination ordering c   c   c    c   and e  requires only                    
              numerical multiplications and                             numerical additions 
note that summing out e  requires          numerical multiplications because after summing out
ci s  there are four heterogeneous factors  each containing the only argument e    combining them
   

fie xploiting c ausal i ndependence

c 

c 

c 

in

b ayesian n etwork i nference

c 

c 

c 

c 

c 

e

e

e
   

c 

   

c 

c 
e 

c 
e 

c 

c 

c 

c 

e 

e 

e

e

 

e
   

   

figure    a bn  its deputation and transformations 
pairwise requires     multiplications  the resultant factor needs to be multiplied with the deputing
factor i  e   e   which requires    numerical multiplications 

   previous methods
two methods have been proposed previously for exploiting causal independence to speed up inference in general bns  olesen et al         heckerman         they both use causal independence to
transform the topology of a bn  after the transformation  conventional algorithms such as ctp or
ve are used for inference 
we shall illustrate those methods by using the bn in figure       let  be the base combination
operator of e  let i denote the contribution of ci to e  and let fi  e  ci  be the contributing factor of ci
to e 
the parent divorcing method  olesen et al         transforms the bn into the one in figure      
after the transformation  all variables are regular and the new variables e  and e  have the same
possible values as e  the conditional probabilities of e  and e  are given by

p  e  jc   c   f  e  c  
f   e  c   
p  e  jc   c   f  e  c  
f   e  c   

the conditional probability of e is given by

p  e ffje  ff   e  ff        if ff ff  ff  
for any value ff of e  ff  of e    and ff  of e    we shall use pd to refer to the algorithm that first
performs the parent divorcing transformation and then uses ve for inference 
   

fiz hang   p oole

the temporal transformation by heckerman        converts the bn into the one in figure      
again all variables are regular after the transformation and the newly introduced variables have the
same possible values as e  the conditional probability of e  is given by

p  e  ffjc     f    ff  c   
for each value ff of e    for i          the conditional probability of ei  e  stands for e  is given by

p  ei  ffjei    ff    ci   

x

ff  ff   ff

fi e ff   ci  

for each possible value ff of ei and ff  of ei     we shall use tt to refer to the algorithm that first
performs the temporal transformation and then uses ve for inference 
the factorization represented by the original bn includes a factor that contain five variables 
while factors in the transformed bns contain no more than three variables  in general  the transformations lead to finer grain factorizations of joint probabilities  this is why pd and tt can be more
efficient than ve  
however  pd and tt are not as efficient as ve     we shall provide empirical evidence in support
of this claim in the next section  here we illustrate it by considering calculating p  e   doing this in
figure      by ve using the elimination ordering c   c   c    c   e    and e  would require          
                      numerical multiplications and    numerical additions   doing the same in
figure      using the elimination ordering c    e    c   e    c   e    c  would require               
                      numerical multiplications and    numerical additions  in both cases  more
numerical multiplications and additions are performed than ve     the differences are more drastic
in complex networks  as will be shown in the next section 
the saving for this example may seem marginal  it may be reasonable to conjecture that  as
olesons method produces families with three elements  this marginal saving is all that we can hope
for  producing factors of two elements rather than cliques of three elements  however  interacting
causal variables can make the difference more extreme  for example  if we were to use olesons
method for the bn of figure    we produce  the network of figure    any triangulation for this
network has at least one clique with four or more elements  yet ve   does not produce a factor with
more than two elements 
note that as far as computing p  e  in the networks shown in figure   is concerned  ve   is more
efficient than pd  pd is more efficient than tt  and tt is more efficient than ve   our experiments
show that this is true in general 
   this is exactly the same number of operations required to determine p  e  using clique tree propagation on the same
network  the clique tree for figure      has three cliques  one containing fc    c    e  g  one containing fc    c    e  g  and
once containing fe    e    eg  the first clique contains   elements  to construct it requires              multiplications 
the message that needs to be sent to the third clique is the marginal on e  which is obtained by summing out c  and
c    similarly for the second clique  the third clique again has   elements and requires    multiplications to construct 
in order to extract p  e  from this clique  we need to sum out e  and e    this shown one reason why ve   can be more
efficient that ctp or ve  ve   never constructs a factor with three variables for this example  note however  that an
advantage of ctp is that the cost of building the cliques can be amortized over many queries 
   note that we need to produce two variables both of which represent noisy a  b  we need two variables as the noise
applied in each case is independent  note that if there was no noise in the network  if e    a  b  c  we only
need to create one variable  but also e  and e  would be the same variable  or at least be perfectly correlated   in this
case we would need a more complicated example to show our point 

   

fie xploiting c ausal i ndependence

a

in

b ayesian n etwork i nference

b

e

c

  

e  

e

e

 

 

e 

figure    the result of applying olesons method to the bn of figure   

    experiments
the cpcs networks are multi level  multi valued bns for medicine  they were created by pradhan
et al         based on the computer based patient case simulation system  cpcs pm  developed
by parker and miller         two cpcs networks  were used in our experiments  one of them
consists of     nodes and     arcs  and the other contains     nodes  they are among the largest
bns in use at the present time 
the cpcs networks contain abundant causal independencies  as a matter of fact  each non root
variable is a convergent variable with base combination operator max  they are good test cases for
inference algorithms that exploit causal independencies 
     ctp based approaches versus ve  based approaches
as we have seen in the previous section  one kind of approach for exploiting causal independencies
is to use them to transform bns  thereafter  any inference algorithms  including ctp or ve   can be
used for inference 
we found the coupling of the network transformation techniques and ctp was not able to carry
out inference in the two cpcs networks used in our experiments  the computer ran out memory
when constructing clique trees for the transformed networks  as will be reported in the next subsection  however  the combination of the network transformation techniques and ve was able to answer
many queries 
this paper has proposed a new method of exploiting causal independencies  we have observed
that causal independencies lead to a factorization of a joint probability that is of finer grain than
the factorization entailed by conditional independencies alone  one can extend any inference algorithms  including ctp and ve   to exploit this finer grain factorization  this paper has extended
ve and obtained an algorithm called ve     ve   was able to answer almost all queries in the two
cpcs networks  we conjecture  however  that an extension of ctp would not be able to carry out
inference with the two cpcs networks at all  because the resources that ve   takes to answer any
query in a bn can be no more than those an extension of ctp would take to construct a clique tree
   obtained from ftp   camis stanford edu pub pradhan 
v    txt and cpcs networks std       

   

the file names are cpcs lm sm k  

fi  
  
  
  
  
  
  
  
  
 
 

number of queries

number of queries

z hang   p oole

  ve  
  pd 
  tt 
  ve 

number of queries

                   
cpu time in seconds

  
  
  
  
  
  
  
  
  
 
 

   ve  
   pd 
   tt 
   ve 

                   
cpu time in seconds

  
  
  
  
  
  
  
  
  
 
 

   ve  
   pd 
   tt 

                      
cpu time in seconds

figure    comparisons in the     node bn 

for the bn and there are  as will be seen in the next subsection  queries in the two cpcs networks
that ve   was not able to answer 
in summary  ctp based approaches are not or would not be able to deal with the two cpcs
networks  while ve  based approaches can  to different extents  
     comparisons of ve  based approaches
this subsection provides experimental data to compare the ve  based approaches namely pd  tt 
and ve     we also compare those approaches with ve itself to determine how much can be gained
by exploiting causal independencies 
in the     node network  three types of queries with one query variable and five  ten  or fifteen
observations respectively were considered  fifty queries were randomly generated for each query
type  a query is passed to the algorithms after nodes that are irrelevant to it have been pruned  in general  more observations mean less irrelevant nodes and hence greater difficulty to answer the query 
the cpu times the algorithms spent in answering those queries were recorded 
in order to get statistics for all algorithms  cpu time consumption was limited to ten seconds
and memory consumption was limited to ten megabytes 
the statistics are shown in figure    in the charts  the curve  ve   for instance  displays the
time statistics for ve   on queries with five observations  points on the x axis represent cpu times
   

fi  
  
  
  
  
  
  
  
  
 
 

in

b ayesian n etwork i nference

number of queries

number of queries

e xploiting c ausal i ndependence

  ve  
  pd 
  tt 
  ve 

                   
cpu time in seconds

  
  
  
  
  
  
  
 
 

   ve  
   pd 
   tt 

                      
cpu time in seconds

figure    comparisons in the     node bn 
in seconds  for any time point  the corresponding point on the y axis represents the number of fiveobservation queries that were each answered within the time by ve    
we see that while ve   was able to answer all the queries  pd and tt were not able to answer
some of the ten observation and fifteen observation queries  ve was not able to answer a majority
of the queries 
to get a feeling about the average performances of the algorithms  regard the curves as representing functions of y   instead of x  the integration  along the y  axis  of the curve   pd  for instance 
is roughly the total amount of time pd took to answer all the ten observation queries that pd was
able to answer  dividing this by the total number of queries answered  one gets the average time pd
took to answer a ten observation query 
it is clear that on average  ve   performed significantly better than pd and tt  which in turn
performed much better than ve   the average performance of pd on five  or ten observation queries
are roughly the same as that of tt  and slightly better on fifteen observation queries 
in the     node network  two types of queries with five or ten observations were considered and
fifty queries were generated for each type  the same space and time limits were imposed as in the
    node networks  moreover  approximations had to be made  real numbers smaller than        
were regarded as zero  since the approximations are the same for all algorithms  the comparisons
are fair 
the statistics are shown in figure    the curves  ve  and   ve  are hardly visible because
they are very close to the y  axis 
again we see that on average  ve   performed significantly better than pd  pd performed significantly better than tt  and tt performed much better than ve  
one might notice that tt was able to answer thirty nine ten observation queries  more than that
ve   and pd were able to  this is due to the limit on memory consumption  as we will see in the
next subsection  with the memory consumption limit increased to twenty megabytes  ve   was able
to answer forty five ten observation queries exactly under ten seconds 
     effectiveness of ve  
we have now established that ve   is the most efficient ve  based algorithm for exploiting causal
independencies  in this section we investigate how effective ve   is 
   

fiz hang   p oole

the     node bn
number of queries

number of queries

the     node bn
  
  
  
  
  
  
  
  
  
 
 

  ve  
   ve  
   ve  
   ve  

                      
cpu time in seconds

  
  
  
  
  
  
  
  
  
 
 

  ve  
   ve  
   ve  

 

                      
cpu time in seconds

figure    time statistics for ve    
experiments have been carried out in both of the two cpcs networks to answer this question  in
the     node network  four types of queries with one query variable and five  ten  fifteen  or twenty
observations respectively were considered  fifty queries were randomly generated for each query
type  the statistics of the times ve   took to answer those queries are given by the left chart in figure
   when collecting the statistics  a ten mb memory limit and a ten second cpu time limit were
imposed to guide against excessive resource demands  we see that all fifty five observation queries
in the network were each answered in less than half a second  forty eight ten observation queries 
forty five fifteen observation queries  and forty twenty observation queries were answered in one
second  there is  however  one twenty observation query that ve   was not able to answer within
the time and memory limits 
in the     node network  three types of queries with one query variable and five  ten  or fifteen 
observations respectively were considered  fifty queries were randomly generated for each query
type  unlike in the previous section  no approximations were made  a twenty mb memory limit
and a forty second cpu time limit were imposed  the time statistics is shown in the right hand side
chart  we see that ve   was able to answer all most all queries and a majority of the queries were
answered in little time  there are  however  three fifteen observation queries that ve   was not able
to answer 

    conclusions
this paper has been concerned with how to exploit causal independence in exact bn inference  previous approaches  olesen et al         heckerman        use causal independencies to transform
bns  efficiency is gained because inference is easier in the transformed bns than in the original
bns 
a new method has been proposed in this paper  here is the basic idea  a bayesian network
can be viewed as representing a factorization of a joint probability into the multiplication of a list of
conditional probabilities  we have studied a notion of causal independence that enables one to further
factorize the conditional probabilities into a combination of even smaller factors and consequently
obtain a finer grain factorization of the joint probability 
we propose to extend inference algorithms to make use of this finer grain factorization  this
paper has extended an algorithm called ve   experiments have shown that the extended ve algo   

fie xploiting c ausal i ndependence

in

b ayesian n etwork i nference

rithm  ve     is significantly more efficient than if one first performs olesen et al s or heckermans
transformation and then apply ve  
the choice of ve instead of the more widely known ctp algorithm is due to its ability to work in
networks that ctp cannot deal with  as a matter of fact  ctp was not able to deal with the networks
used in our experiments  even after olesen et al s or heckermans transformation  on the other hand 
ve   was able to answer almost all randomly generated queries with and a majority of the queries
were answered in little time  it would be interesting to extend ctp to make use of the finer grain
factorization mentioned above 
as we have seen in the previous section  there are queries  especially in the     node network 
that took ve   a long time to answer  there are also queries that ve   was not able to answer  for
those queries  approximation is a must  we employed an approximation technique when comparing
algorithms in the     node network  the technique captures  to some extent  the heuristic of ignoring
minor distinctions  in future work  we are developing a way to bound the error of the technique and
an anytime algorithm based on the technique 

acknowledgements
we are grateful to malcolm pradhan and gregory provan for sharing with us the cpcs networks 
we also thank jack breese  bruce dambrosio  mike horsch  runping qi  and glenn shafer for
valuable discussions  and ronen brafman  chris geib  mike horsch and the anonymous reviewers for very helpful comments  mr  tak yin chan has been a great help in the experimentations 
research was supported by nserc grant ogpoo       the institute for robotics and intelligent
systems  hong kong research council grant hkust      e and sino software research center
grant ssrc      eg   

appendix a  proof of theorem  
theorem   suppose a list of homogeneous factors f  and a list of heterogeneous factors f  constitute a tidy factorization of p  z    z          zk    if z  is either a convergent variable  or an old regular
variable  or a new regular variable whose deputy is not in the list fz           zk g  then the procedure
sum out  f    f   z   returns a tidy heterogeneous factorization of p  z           zk   
proof  suppose f         fr are all the heterogeneous factors and g        gs are all the homogeneous
factors  also suppose f         fl   g        gm are all the factors that contain z    then

p  z           zk  

 

 

 

 

x

p  z    z          zk 

z 

x
z 


rj  fj

x

s
y
i  

gi


lj  fj   
  
rj l   fj   

m
y

s
y

gi
i   i m  
m
s
x l
y
y
  
j    fj
gi  
  
rj l   fj   
gi
z 
i  
i m  
z 

  

   

gi

   

fiz hang   p oole

 

x

m
s
y
y
l
r
  

j   fj gi  
  
j l  fj   
gi  
z 
i  
i m  

    

where equation      is due to proposition    equation     is follows from proposition    as a matter
q g due to the
of fact  if z  is a convergent variable  then it is the only convergent variable in m
i   i
first condition of tidiness  the condition of proposition   is satisfied because z  does not appear
in fl          fr   on the other hand  if z  is an old regular variable or a new regular variable whose
q g contains no convergent variables due to the
deputy does not appear in the list z         zk   then m
i   i
second condition of tidiness  again the condition of proposition   is satisfied  we have thus proved
that sum out  f    f   z   yields a flexible heterogeneous factorization of p  z           zk   
let e  be a convergent variable in the list z         zk   then z  cannot be the corresponding new regular variable e  hence the factor i  e   e  is not touched by sum out  f    f   z    consequently  if
we can show that the new factor created by sum out  f    f   z   is either a heterogeneous factor
or a homogeneous factor that contain no convergent variable  then the factorization returned is tidy 
suppose sum out  f    f   z   does not create a new homogeneous factor  then no heterogeneous factors in f  contain z    if z  is a convergent variable  say e   then i  e   e  is the only homop
geneous factor that contain e    the new factor is e i  e   e   which does contain any convergent
variables  if z  is an old regular variable or a new regular variable whose deputy is not in the list z   
     zk   all the factors that contain z  do not contain any convergent variables  hence the new factor
again does not contain any convergent variables  the theorem is thus proved   
 

references
arnborg  s   corneil  d  g     proskurowski  a          complexity of finding embedding in a
k tree  siam j  alg  disc  meth                
baker  m     boult  t          pruning bayesian networks for efficient computation  in proc  sixth
conf  on uncertainty in artificial intelligence  pp         cambridge  mass 
bertele  u     brioschi  f          nonserial dynamic programming  vol     of mathematics in
science and engineering  academic press 
boutilier  c   friedman  n   goldszmidt  m     koller  d          context specific independence in
bayesian networks  in e  horvitz and f  jensen  ed    proc  twelthth conf  on uncertainty in
artificial intelligence  pp         portland  oregon 
cooper  g  f          the computational complexity of probabilistic inference using bayesian belief
networks  artificial intelligence                  
dagum  p     galper  a          additive belief network models  in d  heckerman and a  mamdani
 ed    proc  ninth conf  on uncertainty in artificial intelligence  pp       washington d c 
dambrosio         local expression languages for probabilistic dependence  international journal of approximate reasoning              
dambrosio  b          symbolic probabilistic inference in large bn o networks  in r  lopez de
mantaras and d  poole  ed    proc  tenth conf  on uncertainty in artificial intelligence  pp 
       seattle 
   

fie xploiting c ausal i ndependence

in

b ayesian n etwork i nference

dechter  r          bucket elimination  a unifying framework for probabilistic inference  in e 
horvits and f  jensen  ed    proc  twelthth conf  on uncertainty in artificial intelligence  pp 
       portland  oregon 
dez  f  j          parameter adjustment in bayes networks  the generalized noisy or gate  in d 
heckerman and a  mamdani  ed    proc  ninth conf  on uncertainty in artificial intelligence 
pp        washington d c 
duda  r  o   hart  p  e     nilsson  n  j          subjective bayesian methods for rule based inference systems  in proc  afips nat  comp  conf   pp           
geiger  d     heckerman  d          knowledge representation and inference in similarity networks
and bayesian multinets  artificial intelligence           
geiger  d   verma  t     pearl  j          d separation  from theorems to algorithms  in m  henrion
et  al   ed    uncertainty in artificial intelligence    pp          north holland  new york 
good  i          a causal calculus  i   british journal of philosophy of science             
heckerman  d          causal independence for knowledge acquisition and inference  in proc  of
the ninth conference on uncertainty in artificial intelligence  pp         
heckerman  d     breese  j          a new look at causal independence  in proc  of the tenth
conference on uncertainty in artificial ingelligence  pp         
henrion  m          some practical issues in constructing belief networks  in l  kanal and t  levitt
and j  lemmer  ed    uncertainty in artificial intelligence  pp          north holland 
howard  r  a     matheson  j  e          influence diagrams  in howard  r  a     matheson 
j   eds    the principles and applications of decision analysis  pp          strategic decisions group  ca 
jensen  f  v   lauritzen  s  l     olesen  k  g          bayesian updating in causal probabilistic
networks by local computations  computational statistics quaterly            
kim  j     pearl  j          a computational model for causal and diagnostic reasoning in inference
engines  in proc  of the eighth international joint conference on artificial intelligence  pp 
       karlsruhe  germany 
kjrulff  u          triangulation of graphs   algorithms giving small total state space  tech  rep 
r        department of mathematics and computer science  strandvejen  dk      aalborg 
denmark 
lauritzen  s  l   dawid  a  p   larsen  b  n     leimer  h  g          independence properties of
directed markov fields  networks             
lauritzen  s  l     spiegelhalter  d  j          local computations with probabilities on graphical
structures and their application to expert systems  journal of the royal statistical society 
series b                
   

fiz hang   p oole

li  z     dambrosio  b          efficient inference in bayes networks as a combinatorial optimization problem  international journal of approximate reasoning              
olesen  k  g     andreassen  s          specification of models in large expert systems based on
causal probabilistic networks  artificial intelligence in medicine            
olesen  k  g   kjrulff  u   jensen  f   falck  b   andreassen  s     andersen  s  k          a
munin network for the median nerve   a case study on loops  applied artificial intelligence 
          
parker  r     miller  r          using causal knowledge to creat simulated patient cases  the cpsc
project as an extension of internist    in proc    th symp  comp  appl  in medical care  pp 
       los alamitos  ca  ieee comp soc press 
pearl  j          probabilistic reasoning in intelligent systems  networks of plausible inference 
morgan kaufmann  san mateo  ca 
poole  d          probabilistic horn abduction and bayesian networks  artificial intelligence        
      
pradhan  m   provan  g   middleton  b     henrion  m          knowledge engineering for large
belief networks  in r  lopez de mantaras and d  poole  ed    proc  tenth conf  on uncertainty
in artificial intelligence  pp         seattle 
shachter  r  d   dambrosio  b  d     del favero  b  d          symbolic probabilistic inference
in belief networks  in proc   th national conference on artificial intelligence  pp        
boston  mit press 
shafer  g     shenoy  p          probability propagation  annals of mathematics and artificial
intelligence            
shortliffe  e     buchanan  g  b          a model of inexact reasoning in medicine  math  biosci  
           
srinivas  s          a generalization of noisy or model  in proc  of the ninth conference on uncertainty in artificial intelligence  pp         
tarjan  r  e     yannakakis  m          simple linear time algorithm to test chordality of graphs 
test acyclicity of hypergraphs  and selectively reduce acyclic hypergraphs  siam j  comput  
           
zhang  n  l     poole  d          a simple approach to bayesian network computations  in proc 
of the tenth canadian conference on artificial intelligence  pp         

   

fi