journal artificial intelligence research                 

submitted       published      

exploiting causal independence bayesian network inference
nevin lianwen zhang

lzhang   cs   ust  hk

department computer science 
university science   technology  hong kong

david poole

poole   cs   ubc   ca

department computer science  university british columbia 
     main mall  vancouver  b c   canada v t  z 

abstract
new method proposed exploiting causal independencies exact bayesian network inference  bayesian network viewed representing factorization joint probability
multiplication set conditional probabilities  present notion causal independence enables one factorize conditional probabilities combination even
smaller factors consequently obtain finer grain factorization joint probability  new
formulation causal independence lets us specify conditional probability variable given
parents terms associative commutative operator  or  sum max 
contribution parent  start simple algorithm bayesian network inference
that  given evidence query variable  uses factorization find posterior distribution
query  show algorithm extended exploit causal independence  empirical
studies  based cpcs networks medical diagnosis  show method efficient
previous methods allows inference larger networks previous algorithms 

   introduction
reasoning uncertain knowledge beliefs long recognized important research
issue ai  shortliffe   buchanan        duda et al          several methodologies
proposed  including certainty factors  fuzzy sets  dempster shafer theory  probability theory 
probabilistic approach far popular among alternatives  mainly due
knowledge representation framework called bayesian networks belief networks  pearl       
howard   matheson        
bayesian networks graphical representation  in dependencies amongst random variables 
bayesian network  bn  dag nodes representing random variables  arcs representing
direct influence  independence encoded bayesian network variable
independent non descendents given parents 
bayesian networks aid knowledge acquisition specifying probabilities needed 
network structure sparse  number probabilities required much less
number required independencies  structure exploited computationally
make inference faster  pearl        lauritzen   spiegelhalter        jensen et al         shafer  
shenoy        
definition bayesian network constrain variable depends parents 
often  however  much structure probability functions exploited knowledge acquisition inference  one case dependencies depend particular
values variables  dependencies stated rules  poole         trees  boutilier

c      ai access foundation morgan kaufmann publishers  rights reserved 

fiz hang   p oole

et al         multinets  geiger   heckerman         another function
described using binary operator applied values parent variables 
latter  known causal independencies  seek exploit paper 
causal independence refers situation multiple causes contribute independently
common effect  well known example noisy or gate model  good         knowledge
engineers using specific causal independence models simplifying knowledge acquisition  henrion        olesen et al         olesen   andreassen         heckerman       
first formalize general concept causal independence  formalization later refined
heckerman breese        
kim pearl        showed use noisy or gate speed inference special
kind bns known polytrees  dambrosio              showed two level bns
binary variables  general bns  olesen et al         heckerman        proposed two ways
using causal independencies transform network structures  inference transformed
networks efficient original networks  see section    
paper proposes new method exploiting special type causal independence  see section    still covers common causal independence models noisy or gates  noisy maxgates  noisy and gates  noisy adders special cases  method based following
observation  bn viewed representing factorization joint probability multiplication list conditional probabilities  shachter et al         zhang   poole        li  
dambrosio         type causal independence studied paper leads factorization conditional probabilities  section     finer grain factorization joint probability
obtained result  propose extend exact inference algorithms exploit conditional
independencies make use finer grain factorization provided causal independence 
state of art exact inference algorithm called clique tree propagation  ctp   lauritzen  
spiegelhalter        jensen et al         shafer   shenoy         paper proposes another algorithm called variable elimination  ve    section     related spi  shachter et al         li
  dambrosio         extends make use finer grain factorization  see sections      
    rather compiling secondary structure finding posterior probability
variable  query oriented  needs part network relevant query given
observations  work necessary answer query  chose instead ctp
simplicity carry inference large networks ctp cannot
deal with 
experiments  section     performed two cpcs networks provided pradhan 
networks consist         nodes respectively contain abundant causal independencies  paper  best one could terms exact inference would first
transform networks using jensen et al s heckermans technique apply ctp 
experiments  computer ran memory constructing clique trees transformed
networks  occurs one cannot answer query all  however  extended algorithm able answer almost randomly generated queries twenty less observations
 findings  networks 
one might propose first perform jensen et al s heckermans transformation apply
  experiments show significantly less efficient extended algorithm 
begin brief review concept bayesian network issue inference 
   

fie xploiting c ausal ndependence



b ayesian n etwork nference

   bayesian networks
assume problem domain characterized set random variables  beliefs represented bayesian network  bn  annotated directed acyclic graph  nodes represent
random variables  arcs represent probabilistic dependencies amongst variables  use
terms node variable interchangeably  associated node conditional probability variable given parents 
addition explicitly represented conditional probabilities  bn implicitly represents
conditional independence assertions  let x    x         xn enumeration nodes bn
node appears children  let xi set parents node xi  
bayesian network represents following independence assertion 
variable xi conditionallyindependent variables fx    x          xi   g given
values parents 
conditional independence assertions conditional probabilities together entail joint probability variables  chain rule  have 

p  x   x          xn 

 

 

n

i  
n

i  

p  xi jx   x          xi   
p  xi jx   


   

second equation true conditional independence assertions  conditional probabilities p  xi jxi   given specification bn  consequently  one can 
theory  arbitrary probabilistic reasoning bn 
    inference
inference refers process computing posterior probability p  x jy  y    set x
query variables obtaining observations  y    list observed variables
y  corresponding list observed values  often  x consists one query variable 
theory  p  x jy  y    obtained marginal probability p  x     turn
computed joint probability p  x    x          xn  summing variables outside
x  y one one  practice  viable summing variable joint probability requires exponential number additions 
key efficient inference lies concept factorization  factorization joint
probability list factors  functions  one construct joint probability 
factor function set variables number  say factor contains variable factor function variable  say factor variables depends 
suppose f  f  factors  f  factor contains variables x           xi  y          yj
write f   x           xi  y          yj   f  factor variables y           yj   z          zk  
y           yj variables common f  f    product f  f  factor
function union variables  namely x           xi  y          yj   z          zk   defined by 

f  f   x          xi  y          yj   z          zk    f  x          xi  y          yj  f  y          yj   z          zk  

 

   

fiz hang   p oole

c

b



e

e

 

 

e 

figure    bayesian network 
let f  x           xi   function variable x          xi   setting  say x  f  x           xi  particular
value yields f  x   ff  x          xi   function variables x           xi 
f  x          xi  factor  sum variable  say x    resulting factor variables
x           xi  defined

x

 

x 

f   x          xi    f  x   ff    x          xi      f  x  ffm  x          xi 

ff           ffm possible values variable x  
equation      bn viewed representing factorization joint probability 
example  bayesian network figure   factorizes joint probability p  a  b  c  e   e   e  
following list factors 

p  a   p  b   p  c   p  e ja  b  c   p  e ja  b  c   p  e je   e    
multiplying factors yields joint probability 
suppose joint probability p  z    z          zm   factorized multiplication list factors f    f         fm   obtaining p  z           zm   summing z  p  z    z          zm   requires exponential number additions  obtaining factorization p  z           zm   often
done much less computation  consider following procedure 
procedure sum out f   z   




inputs  f list factors  z variable 
output  list factors 

   remove f factors  say f         fk   contain z  
   add new factor

p qk f f return f  
z i  

theorem   suppose joint probability p  z    z          zm  factorized multiplication
list f factors  sum out f   z    returns list factors whose multiplicationis p  z           zm   
   

fie xploiting c ausal ndependence



b ayesian n etwork nference

proof  suppose f consists factors f    f         fm suppose z  appears factors
f   f        fk  

p  z           zm 

 

 

x
z 

p  z    z          zm  


xy
z  i  

   

k
xy
z  i  




  

i k  

  

theorem follows   
variables appear factors f    f         fk participated computation sum out f   z    
often small portion variables  inference bn
tractable many cases  even general problem np hard  cooper        

   variable elimination algorithm
based discussions previous section  present simple algorithm computing p  x jy  y    
algorithm based intuitions underlying dambrosios symbolic probabilistic inference
 spi   shachter et al         li   dambrosio         first appeared zhang poole        
essentially dechter       s bucket elimination algorithm belief assessment 
algorithm called variable elimination  ve   sums variables list
factors one one  ordering variables outside x  y summed required
input  called elimination ordering 
procedure  f   x  y  y    





inputs  f list conditional probabilities bn 
x list query variables 
list observed variables 
y  corresponding list observed values 
elimination ordering variables outside x  y  
output  p  x jy  y    

   set observed variables factors corresponding observed values 
   empty 

 a  remove first variable z  
 b  call sum out f   z    endwhile

   set h   multiplication factors f  
   h function variables x     

   return h x   

p h x       renormalization   
x

theorem   output ve f   x  y  y     indeed p  x jy  y    
proof  consider following modifications procedure  first remove step    factor
h produced step   function variables x   add new step step   sets
observed variables h observed values 
   

fiz hang   p oole

let f  y  a  function variable variables a  use f  y  a jy ff denote
f  y ff  a   let f  y      g  y      h y  z     three functions variables 
evident

f  y    g  y    jy ff   f  y    jy ff g y    jy ff 
x
x
 
h y  z     jy ff    h y  z    jy ff   
z

z

consequently  modifications change output procedure 
according theorem    modifications factor produced step   simply marginal
probability p  x     consequently  output exactly p  x jy  y      
complexity measured number numerical multiplications numerical summations performs  optimal elimination ordering one results least complexity  problem finding optimal elimination ordering np complete  arnborg et al         
commonly used heuristics include minimum deficiency search  bertele   brioschi        maximum cardinality search  tarjan   yannakakis         kjrulff        empirically shown
minimum deficiency search best existing heuristic  use minimum deficiency search
experiments found better maximum cardinality search 
   



versus clique tree propagation

clique tree propagation  lauritzen   spiegelhalter        jensen et al         shafer   shenoy 
      compilation step transforms bn secondary structure called clique tree
junction tree  secondary structure allows ctp compute answers queries one
query variable fixed set observations twice time needed answer one query
clique tree  many applications desirable property since user might want compare
posterior probabilities different variables 
ctp takes work build secondary structure observations received 
bayesian network reused  cost building secondary structure amortized
many cases  observation entails propagation though network 
given observations  processes one query time  user wants posterior
probabilities several variables  sequence observations  needs run
variables observation sets 
cost  terms number summations multiplications  answering single query
observations using order magnitude using ctp  particular clique
tree propagation sequence encodes elimination ordering  using elimination ordering results approximately summations multiplications factors ctp  there
discrepancy  actually form marginals cliques  works conditional probabilities directly   observations make simpler  the observed variables eliminated
start algorithm   observation ctp requires propagation evidence 
query oriented  prune nodes irrelevant specific queries  geiger et al        
lauritzen et al         baker   boult         ctp  hand  clique tree structure
kept static run time  hence allow pruning irrelevant nodes 
ctp encodes particular space time tradeoff  another  ctp particularly suited
case observations arrive incrementally  want posterior probability node 
   

fie xploiting c ausal ndependence



b ayesian n etwork nference

cost building clique tree amortized many cases  suited
one off queries  single query variable observations given once 
unfortunately  large real world networks ctp cannot deal due time
space complexities  see section    two examples   networks  still answer
possible queries permits pruning irrelevant variables 

   causal independence
bayesian networks place restriction node depends parents  unfortunately
means general case need specify exponential  in number parents 
number conditional probabilities node  many cases structure
probability tables exploited acquisition inference  one case
investigate paper known causal independence 
one interpretation  arcs bn represent causal relationships  parents c   c          cm
variable e viewed causes jointly bear effect e  causal independence refers
situation causes c    c          cm contribute independently effect e 
precisely  c   c          cm said causally independent w r t  effect e exist
random variables              frame  i e   set possible values  e

   i  probabilistically depends ci conditionally independent cj
j given ci  
   exists commutative associative binary operator
e              
using independence notion pearl         let
given z   first condition is 

frame e

 x  jz   mean x independent

    fc          cm            mgjc  
similarly variables  entails     cj jc       j jc   cj j
j      
refer contribution ci e  less technical terms  causes causally independent w r t  common effect individual contributions different causes independent
total influence effect combination individual contributions 
call variable e convergent variable independent contributions different sources collected combined  and lack better name   non convergent variables
simply called regular variables  call base combination operator e 
definition causal independence given slightly different given heckerman breese        srinivas         however  still covers common causal independence
models noisy or gates  good        pearl         noisy max gates  dez         noisy
and gates  noisy adders  dagum   galper        special cases  one see following examples 
example    lottery  buying lotteries affects wealth  amounts money spend
buying different kinds lotteries affect wealth independently  words  causally
   

fiz hang   p oole

independent w r t  change wealth  let c          ck denote amounts money spend
buying k types lottery tickets  let           k changes wealth due buying
different types lottery tickets respectively  then  depends probabilistically ci
conditionally independent cj j given ci   let e total change wealth
due lottery buying  e      k   hence c           ck causally independent w r t  e 
base combination operator e numerical addition  example instance causal independence model called noisy adders 
c           ck amounts money spend buying lottery tickets lottery 
c           ck causally independent w r t  e  winning one ticket reduces
chance winning other  thus    conditionally independent   given c   however 
ci represent expected change wealth buying tickets lottery  would
causally independent  probabilistically independent  there would arcs ci s  
example    alarm  consider following scenario  different motion sensors
connected burglary alarm  one sensor activates  alarm rings  different
sensors could different reliability  treat activation sensor random variable 
reliability sensor reflected   assume sensors fail independently  
assume alarm caused sensor activation   alarm      m  
base combination operator logical operator  example instance causal
independence model called noisy or gate 
following example instance causal independence models know 
example    contract renewal  faculty members university evaluated teaching  research 
service purpose contract renewal  faculty members contract renewed  renewed without pay raise  renewed pay raise  renewed double pay raise depending
whether performance evaluated unacceptable least one three areas  acceptable
areas  excellent one area  excellent least two areas 
let c    c   c  fractions time faculty member spends teaching  research 
service respectively  let represent evaluation gets ith area  take values      
  depending whether evaluation unacceptable  acceptable  excellent  variable
depends probabilistically ci  reasonable assume conditionally independent
cj j given ci  
let e represent contract renewal result  variable take values            depending
whether contract renewed  renewed pay raise  renewed pay raise 
renewed double pay raise  e       base combination operator given
following table 

 
 
 
 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

   called exception independence assumption pearl        
   called accountability assumption pearl         assumption always satisfied introducing
node represent causes  henrion        

   

fie xploiting c ausal ndependence



b ayesian n etwork nference

so  fractions time faculty member spends three areas causally independent
w r t  contract renewal result 
traditional formulation bayesian network need specify exponential 
number parents  number conditional probabilities variable  causal independence 
number conditional probabilities p  i jci   linear m  causal independence
reduce complexity knowledge acquisition  henrion        pearl        olesen et al        
olesen   andreassen         following sections show causal independence
exploited computational gain 
    conditional probabilities convergent variables
allows us exploit structure bayesian network providing factorization joint probability distribution  section show causal independence used factorize
joint distributioneven further  initial factors algorithm form p  ejc          cm  
want break simpler factors need table exponential m 
following proposition shows causal independence used this 

proposition   let e node bn let c    c          cm parents e  c   c          cm
causally independent w r t  e  conditional probability p  ejc          cm  obtained
conditional probabilities p  i jci 

p  e ffjc          cm   

x

ff     ffk  ff

p     ff  jc       p  m  ffmjcm   

   

value e  base combination operator e 
proof   definition causal independence entails independence assertions

    fc          cmgjc        jc   
axiom weak union  pearl        p            jfc          cmg   thus
mutually independent given fc          cmg 
have  definition causal independence     fc          cm gjc   
p   jfc   c          cmg    p   jc  
thus have 

p  e ffjc          cm 
  p     ffjc          cm 
x
 
p    ff           m ffm jc          cm 
ff     ff  ff
x
 
p    ff  jc          cm  p    ff  jc          cm  p  m ffm jc          cm 
ff     ff  ff
x
 
p    ff  jc  p    ff jc   p  m  ffm jcm 
ff     ff  ff
 
k

k

k

next four sections develop algorithm exploiting causal independence inference 
   thanks anonymous reviewer helping us simplify proof 

   

fiz hang   p oole

   causal independence heterogeneous factorizations
section  shall first introduce operator combining factors contain convergent
variables  operator basic ingredient algorithm developed next three sections  using operator  shall rewrite equation     form convenient use
inference introduce concept heterogeneous factorization 
consider two factors f g   let e         ek convergent variables appear f
g   let list regular variables appear f g   let b list variables
appear f   let c list variables appear g   b c contain
convergent variables  well regular variables  suppose base combination operator
ei  then  combination f
g f g function variables e        ek variables
a  b   c   defined by  

f
g  e  x
ff           ek  ffkx
  a  b  c  
 
   
f  e  ff           ek  ffk    a  b 
ff     ff    ff 

ffk  k ffk   ffk

g  e  ff           ek ffk   a  c   

   

value ffi ei   shall sometimes write f
g f  e           ek   a  b  
g  e          ek   a  c  
make explicit arguments f g  
note base combination operators different convergent variables different 
following proposition exhibits basic properties combination operator
 
proposition      f g share convergent variables  f
g simply multiplication f g      operator
commutative associative 
proof  first item obvious  commutativity
follows readily commutativity
multiplication base combination operators  shall prove associativity
special
case  general case proved following line reasoning 
suppose f   g   h three factors contain one variable e variable convergent  need show  f
g  
h f
 g
h   let base combination operator e 
associativity   have  value e 

f
g  
h e ff 

 

 
 
 
 

x

f
g  e ff  h e ff  
x
x
 
f  e ff   g e ff   h e ff   
ff  ff   ff ff  ff   ff 
x
f  e ff  g  e ff   h e ff   
ff  ff  ff   ff
x
x
f  e ff   
g  e ff   h e ff   
ff  ff   ff

ff  ff   ff

ff ff   ff 

   note base combination operators summations indexed  convergent variable associated operator  always use binary operator associated corresponding convergent variable 
examples  ease exposition  use one base combination operator  one type
base combination operator  e g   may use or  sum max different variables network  
keep track operators associated convergent variables  will  however  complicate
description 

   

fie xploiting c ausal ndependence

x

 

ff  ff   ff



b ayesian n etwork nference

f  e ff  g
h e ff  

f
 g
h  e ff  

 

proposition hence proved  
following propositions give properties
correspond operations
exploited algorithm   proofs straight forward omitted 
proposition   suppose f g factors variable z appears f g  

x

xz
z

x

fg 

 

 

f
g  

 

 

 

 

z
x
z

f  g 
f  
g 

proposition   suppose f   g h factors g h share convergent variables 

g  f
h     gf  
h 

   

    rewriting equation  
noticing contribution variable possible values e  define functions

fi e  ci 

fi e ff  ci     p  i  ffjci  
value e  shall refer contributing factor ci e 
using operator
  rewrite equation     follows
p  ejc          cm   
mi    e  ci  

   

interesting notice similarity equation     equation      equation    
conditional independence allows one factorize joint probability factors involve less
variables  equation     causal independence allows one factorize conditional probability
factors involve less variables  however  ways factors combined
different two equations 
    heterogeneous factorizations
consider bayesian network figure    factorizes joint probability p  a  b  c  e   e   e  
following list factors 

p  a   p  b   p  c   p  e ja  b  c   p  e ja  b  c   p  e je   e    
say factorization homogeneous factors combined way 
i e   multiplication 
suppose ei convergent variables  conditional probabilities factorized follows 

p  e ja  b  c 
p  e ja  b  c 
p  e  je    e  

 
 
 

f    e   a 
f    e    b 
f    e   c  
f    e   a 
f    e    b 
f    e   c  
f    e   e  
f    e   e    
   

fiz hang   p oole

factor f   e    a   instance  contributing factor e   
say following list factors

f   e    a   f   e   b   f   e   c   f   e    a   f   e    b   f   e    c   f   e    e    f   e    e   
p  a   p  b   p  c 
   
constitute heterogeneous factorization p  a  b  c  e   e   e   joint probability
obtained combining factors proper order using either multiplication operator
 
word heterogeneous signify fact different factor pairs might combined different ways  call fij heterogeneous factor needs combined
fik operator
combined factors multiplication  contrast 
call factors p  a   p  b   p  c  homogeneous factors 
shall refer heterogeneous factorization heterogeneous factorization represented
bn figure    obvious heterogeneous factorization finer grain
homogeneous factorization represented bn 

   flexible heterogeneous factorizations deputation
paper extends exploit finer grain factorization  compute answer query
summing variables one one factorization  
correctness guaranteed fact factors homogeneous factorization
combined  by multiplication  order distributivity multiplication summations  see proof theorem    
according proposition    operator
distributive summations  however  factors
heterogeneous factorization cannot combined arbitrary order  example  consider heterogeneous factorization      correct combine f   e    a  f    e   b  using
 
combine f    e   e    f    e   e    using
  correct combine f    e   a  f    e   e  

  want combine latter two multiplication  combined sibling heterogeneous factors 
overcome difficulty  transformation called deputation performed bn 
transformation change answers queries  heterogeneous factorization
represented transformed bn flexible following sense 
heterogeneous factorization joint probability flexible if 
joint probability
 

multiplication homogeneous factors

combination  by
  heterogeneous factors 

   

property allows us carry multiplication homogeneous factors arbitrary order 
since
associative commutative  combination heterogeneous factors arbitrary order  conditions proposition   satisfied  exchange multiplication
combination
  guarantee conditions proposition    elimination ordering needs
constrained  sections      
heterogeneous factorization p  a  b  c  e   e   e   given end previous section
flexible  consider combining heterogeneous factors  since operator
commutative
   

fie xploiting c ausal ndependence



b ayesian n etwork nference

c

b


e 

e 

e 

e 
e 
e 

figure    bn figure   deputation convergent variables 
associative  one first combine  i  fik s  obtaining conditional probability
ei   combine resulting conditional probabilities  combination

p  e  ja  b  c 
p  e  ja  b  c 
p  e je    e  
multiplication

p  e  ja  b  c p  e ja  b  c p  e je   e  
convergent variables e  e  appear one factor  consequently  equation
    hold factorization flexible  problem arises convergent variable shared two factors siblings  example  want combine
f    e   a  f    e    e   using
  order tackle problem introduce new deputation
variable heterogeneous factor contains single convergent variable 
deputation transformation one apply bn make heterogeneous factorization represented bn flexible  let e convergent variable  depute e make copy
e  e  make parents e parents e    replace e e  contributing factors e  make
e  parent e  set conditional probability p  eje    follows 

p  eje     

 

 
 

e   e 
otherwise

   

shall call e  deputy e  deputy variable e  convergent variable definition 
variable e  convergent deputation  becomes regular variable deputation 
shall refer new regular variable  contrast  shall refer variables regular
deputation old regular variables  conditional probability p  e  je  homogeneous
factor definition  sometimes called deputing function written  e   e  since
ensures e  e always take value 
deputation bn obtained bn deputing convergent variables  deputation
bn  deputy variables convergent variables deputy variables convergent variables 
   

fiz hang   p oole

figure   shows deputation bn figure    factorizes joint probability

p  a  b  c  e   e    e   e    e   e   
homogeneous factors

p  a   p  b   p  c   i  e    e    i  e    e    i  e    e   
heterogeneous factors

f   e     a   f   e    b   f   e     c   f   e     a   f   e    b   f   e    c   f   e    e    f   e    e   
factorization three important properties 
   heterogeneous factor contains one one convergent variable   recall ei
longer convergent variables deputies are  
   convergent variable e  appears one one homogeneous factor  namely
deputing function  e   e  
   except deputing functions  none homogeneous factors contain convergent
variables 
properties shared factorization represented deputation bn 
proposition   heterogeneous factorization represented deputation bn flexible 
proof  consider combination 
  heterogeneous factors deputation bn  since
combination operator
commutative associative  carry combination following two steps  first convergent  deputy  variable e    combine heterogeneous factors contain e    yielding conditional probability p  e  je   e    combine resulting
conditional probabilities  follows first property mentioned different convergent variables e   e    p  e   je    p  e   je    share convergent variables  hence
combination p  e  je  s multiplication them  consequently  combination 

  heterogeneous factors deputation bn multiplication conditional
probabilities convergent variables  therefore 
 

 

 

 

joint probability variables deputation bn
  multiplication conditional probabilities variables
 

multiplication conditional probabilities regular variables

 

multiplication homogeneous factors

multiplication conditional probabilities convergent variables

combination  by
  heterogeneous factors 
proposition hence proved   
deputation change answer query  precisely 
proposition   posterior probability p  x jy  y    bn deputation 
   

fie xploiting c ausal ndependence



b ayesian n etwork nference

proof  let r  e   e   lists old regular  new regular  deputy variables deputation bn respectively  suffices show p  r  e   original bn
deputation bn  new regular variable e  let e  deputy  easy see quantity
p  e   e p  e j   deputation bn p  ej   original bn  hence 
e
e
e
 

 

p  r  ex  deputation bn
 
p  r  e  e   
e
x

 
p  rjr    p  eje p  e  je   
e r r
 e

ex
 
p  rjr      e   e p  e je   
r r
e e e


 
p  rjr   p  eje  
 

 

 

 

 

r r

 

proposition proved 

 

e  e

p  r  e   original bn 

   tidy heterogeneous factorizations
far  encountered heterogeneous factorizations correspond bayesian networks 
following algorithm  intermediate heterogeneous factorizations necessarily correspond bns  property combine form appropriate marginal probabilities  general intuition heterogeneous factors must combine sibling heterogeneous factors multiplied factors containing original convergent variable 
previous section  mentioned three properties heterogeneous factorization represented deputation bn  used first property show factorization flexible 
two properties qualify factorization tidy heterogeneous factorization  defined below 
let z    z         zk list variables deputation bn convergent  deputy 
variable e  fz   z           zk g  corresponding new regular variable e  flexible heterogeneous factorization p  z    z          zk   said tidy

   convergent  deputy  variable e  fz   z          zk g  factorization contains deputing function  e   e  homogeneous factor involves e   

   except deputing functions  none homogeneous factors contain convergent
variables 
stated earlier  heterogeneous factorization represented deputation bn tidy 
certain conditions  given theorem    one obtain tidy factorization p  z           zk  
summing z  tidy factorization p  z    z          zk   using following procedure 
procedure sum out  f    f   z  



inputs  f  list homogeneous factors 
f  list heterogeneous factors 
z variable 
   

fiz hang   p oole



output  list heterogeneous factors list homogeneous factors 

   remove f  factors contain z   multiply resulting in  say  f  
factors  set f  nil 

   remove f  factors contain z   combine using
resulting
in  say  g   factors  set g  nil 

p f f  
 
z
p
else add new  heterogeneous  factor z fg f  
return  f   f   

   g  nil  add new  homogeneous  factor
  
  

theorem   suppose list homogeneous factors f  list heterogeneous factors f  constitute tidy factorization p  z    z          zk    z  either convergent variable  old regular
variable  new regular variable whose deputy list fz          zk g  procedure
sum out  f    f   z   returns tidy heterogeneous factorization p  z           zk   
proof theorem quite long hence given appendix 

   causal independence inference
task compute p  x jy  y    bn  according proposition   
deputation bn 
elimination ordering consisting variables outside x  y legitimate deputy
variable e  appears corresponding new regular variable e  ordering found
using  minor adaptations  minimum deficiency search maximum cardinality search 
following algorithm computes p  x jy  y    deputation bn  called  
extension  
procedure    f   f   x  y  y    





inputs  f  list homogeneous factors
deputation bn 
f  list heterogeneous factors
deputation bn 
x list query variables 
list observed variables 
y  corresponding list observed values 
legitimate elimination ordering 
output  p  x jy  y    

   set observed variables factors observed values 
   empty 

remove first variable z  
 f   f     sum out  f   f   z   endwhile
   

fie xploiting c ausal ndependence



b ayesian n etwork nference

   set h multiplication factors f 
combination  by
  factors f  
   h function variables x     
   return h x   

p h x       renormalization   
x

theorem   output    f   f   x  y  y     indeed p  x jy  y    
proof  consider following modifications algorithm  first remove step    factor
h produced step   function variables x   add new step step   sets
observed variables h observed values  shall first show modifications
change output algorithm show output modified algorithm
p  x jy  y    
let f  y      g  y      h y  z     three functions variables  evident

f  y    g  y    jy ff   f  y    jy ff g y    jy ff 
x
x
 
h y  z     jy ff    h y  z    jy ff   
z

z

regular variable 

f  y    
g  y    jy ff   f  y    jy ff
g  y    jy ff  
consequently  modifications change output procedure 
since elimination ordering legitimate  always case deputy variable e 
summed out  neither corresponding new regular variable e  let z         zk remaining variables time execution algorithm  then  e   fz           zk g implies e fz           zk g  fact factorization represented deputation bn tidy
enable us repeatedly apply theorem   conclude that  modifications  factor created
step   simply marginal probability p  x     consequently  output p  x jy  y      
    example
subsection illustrates   walking example  consider computing p  e  je    
deputation bayesian network shown figure    suppose elimination ordering is  a  b 
c  e    e    e   e     first step ve   

f    fp  a   p  b   p  c   i  e    e    i  e    e    i  e    e    g 
f    ff   e    a   f   e    b   f   e    c   f   e    a   f   e    b   f   e    c   f   e    e    f   e    e  g 
procedure enters while loop sums variables one one 
summing a 
f    fp  b   p  c   i  e    e    i  e    e    i  e    e    g 
f    ff   e    b   f  p e    c   f   e    b   f   e    c   f   e    e    f   e    e      e    e   g 
   e    e      p  a f   e     a f   e     a  
summing b 
f    fp  c   i  e    e    i  e    e    i  e    e    g 
f    ff   e    c   f  p e    c   f   e    e    f   e    e      e    e       e    e   g 
   e    e      b p  b f   e     b f   e    b  
   

fiz hang   p oole

summing c 
f    fi  e    e    i  e    e    i  e    e    g 
f    ff   e    e    fp
   e     e      e     e       e     e       e     e   g 
 
 
   e   e     c p  c f    e    c f   e     c  
summing e    
f    fi  e    e    i  e    e    g 
f    ff   e    e    fp
   e     e      e    e   g 
   e   e      e i  e     e      e    e   
   e    e   
   e    e     
 
summing e    
f    fi  e    e    g 
f    ff   e    e    fp
   e     e      e    e  g 
   e   e     e i  e     e     e    e    
 
summing e   
f    fi  e    e    g 
f    ff   e    e    p  e    e  g 
   e    e     e  f   e     e     e    e   
finally  summing e    
f       
f    f   e  g  p
   e     e i  e     e     f   e    e   
  e    e      procedure enters step   
 
p
nothing example  finally  procedure returns   e     e    e    
p  e je       required probability 
 

 

 

    comparing  
comparing     notice summing variable  combine
factors contain variable  however  factorization latter works
finer grain factorization used former  running example  latter works
factorization initially consists factors contain two variables  factorization former uses initially include factors contain five variables  hand  latter
uses operator
expensive multiplication  consider  instance  calculating
f  e  a 
g e  b   suppose e convergent variable variables binary  operation requires    numerical multiplications         numerical summations  hand 
multiplying f  e  a  g  e  b  requires    numerical multiplications 

despite expensiveness operator
    efficient   shall provide
empirical evidence support claim section     see simple example true 
consider bn figure       e convergent variable  suppose variables binary 
then  computing p  e  using elimination ordering c    c   c   c  requires          
           numerical multiplications                                                  
numerical additions  hand  computing p  e  deputation bn shown figure     
  using elimination ordering c   c   c    c   e  requires                    
              numerical multiplications                             numerical additions 
note summing e  requires          numerical multiplications summing
ci s  four heterogeneous factors  containing argument e    combining
   

fie xploiting c ausal ndependence

c 

c 

c 



b ayesian n etwork nference

c 

c 

c 

c 

c 

e

e

e
   

c 

   

c 

c 
e 

c 
e 

c 

c 

c 

c 

e 

e 

e

e

 

e
   

   

figure    bn  deputation transformations 
pairwise requires     multiplications  resultant factor needs multiplied deputing
factor  e   e   requires    numerical multiplications 

   previous methods
two methods proposed previously exploiting causal independence speed inference general bns  olesen et al         heckerman         use causal independence
transform topology bn  transformation  conventional algorithms ctp
used inference 
shall illustrate methods using bn figure       let base combination
operator e  let denote contribution ci e  let  e  ci  contributing factor ci
e 
parent divorcing method  olesen et al         transforms bn one figure      
transformation  variables regular new variables e  e 
possible values e  conditional probabilities e  e  given

p  e  jc   c   f  e  c  
f   e  c   
p  e  jc   c   f  e  c  
f   e  c   

conditional probability e given

p  e ffje  ff   e  ff        ff ff  ff  
value e  ff  e    ff  e    shall use pd refer algorithm first
performs parent divorcing transformation uses inference 
   

fiz hang   p oole

temporal transformation heckerman        converts bn one figure      
variables regular transformation newly introduced variables
possible values e  conditional probability e  given

p  e  ffjc     f    ff  c   
value e    i          conditional probability ei  e  stands e  given

p  ei  ffjei    ff    ci   

x

ff  ff   ff

fi e ff   ci  

possible value ei ff  ei     shall use tt refer algorithm first
performs temporal transformation uses inference 
factorization represented original bn includes factor contain five variables 
factors transformed bns contain three variables  general  transformations lead finer grain factorizations joint probabilities  pd tt
efficient  
however  pd tt efficient     shall provide empirical evidence support
claim next section  illustrate considering calculating p  e  
figure      using elimination ordering c   c   c    c   e    e  would require          
                      numerical multiplications    numerical additions  
figure      using elimination ordering c    e    c   e    c   e    c  would require               
                      numerical multiplications    numerical additions  cases 
numerical multiplications additions performed     differences drastic
complex networks  shown next section 
saving example may seem marginal  may reasonable conjecture that 
olesons method produces families three elements  marginal saving hope
for  producing factors two elements rather cliques three elements  however  interacting
causal variables make difference extreme  example  use olesons
method bn figure    produce  network figure    triangulation
network least one clique four elements  yet   produce factor
two elements 
note far computing p  e  networks shown figure   concerned   
efficient pd  pd efficient tt  tt efficient   experiments
show true general 
   exactly number operations required determine p  e  using clique tree propagation
network  clique tree figure      three cliques  one containing fc    c    e  g  one containing fc    c    e  g 
containing fe    e    eg  first clique contains   elements  construct requires              multiplications 
message needs sent third clique marginal e  obtained summing c 
c    similarly second clique  third clique   elements requires    multiplications construct 
order extract p  e  clique  need sum e  e    shown one reason  
efficient ctp ve    never constructs factor three variables example  note however 
advantage ctp cost building cliques amortized many queries 
   note need produce two variables represent noisy b  need two variables noise
applied case independent  note noise network e    b c
need create one variable  e  e  would variable  or least perfectly correlated  
case would need complicated example show point 

   

fie xploiting c ausal ndependence





b ayesian n etwork nference

b

e

c

  

e  

e

e

 

 

e 

figure    result applying olesons method bn figure   

    experiments
cpcs networks multi level  multi valued bns medicine  created pradhan
et al         based computer based patient case simulation system  cpcs pm  developed
parker miller         two cpcs networks  used experiments  one
consists     nodes     arcs  contains     nodes  among largest
bns use present time 
cpcs networks contain abundant causal independencies  matter fact  non root
variable convergent variable base combination operator max  good test cases
inference algorithms exploit causal independencies 
     ctp based approaches versus  based approaches
seen previous section  one kind approach exploiting causal independencies
use transform bns  thereafter  inference algorithms  including ctp  
used inference 
found coupling network transformation techniques ctp able carry
inference two cpcs networks used experiments  computer ran memory
constructing clique trees transformed networks  reported next subsection  however  combination network transformation techniques able answer
many queries 
paper proposed new method exploiting causal independencies  observed
causal independencies lead factorization joint probability finer grain
factorization entailed conditional independencies alone  one extend inference algorithms  including ctp   exploit finer grain factorization  paper extended
obtained algorithm called       able answer almost queries two
cpcs networks  conjecture  however  extension ctp would able carry
inference two cpcs networks all  resources   takes answer
query bn extension ctp would take construct clique tree
   obtained ftp   camis stanford edu pub pradhan 
v    txt cpcs networks std       

   

file names cpcs lm sm k  

fi  
  
  
  
  
  
  
  
  
 
 

number queries

number queries

z hang   p oole

  ve  
  pd 
  tt 
  ve 

number queries

                   
cpu time seconds

  
  
  
  
  
  
  
  
  
 
 

   ve  
   pd 
   tt 
   ve 

                   
cpu time seconds

  
  
  
  
  
  
  
  
  
 
 

   ve  
   pd 
   tt 

                      
cpu time seconds

figure    comparisons     node bn 

bn are  seen next subsection  queries two cpcs networks
  able answer 
summary  ctp based approaches would able deal two cpcs
networks   based approaches  to different extents  
     comparisons  based approaches
subsection provides experimental data compare  based approaches namely pd  tt 
    compare approaches determine much gained
exploiting causal independencies 
    node network  three types queries one query variable five  ten  fifteen
observations respectively considered  fifty queries randomly generated query
type  query passed algorithms nodes irrelevant pruned  general  observations mean less irrelevant nodes hence greater difficulty answer query 
cpu times algorithms spent answering queries recorded 
order get statistics algorithms  cpu time consumption limited ten seconds
memory consumption limited ten megabytes 
statistics shown figure    charts  curve  ve   instance  displays
time statistics   queries five observations  points x axis represent cpu times
   

fi  
  
  
  
  
  
  
  
  
 
 



b ayesian n etwork nference

number queries

number queries

e xploiting c ausal ndependence

  ve  
  pd 
  tt 
  ve 

                   
cpu time seconds

  
  
  
  
  
  
  
 
 

   ve  
   pd 
   tt 

                      
cpu time seconds

figure    comparisons     node bn 
seconds  time point  corresponding point y axis represents number fiveobservation queries answered within time    
see   able answer queries  pd tt able answer
ten observation fifteen observation queries  able answer majority
queries 
get feeling average performances algorithms  regard curves representing functions   instead x  integration  along  axis  curve   pd  instance 
roughly total amount time pd took answer ten observation queries pd
able answer  dividing total number queries answered  one gets average time pd
took answer ten observation query 
clear average    performed significantly better pd tt  turn
performed much better   average performance pd five  ten observation queries
roughly tt  slightly better fifteen observation queries 
    node network  two types queries five ten observations considered
fifty queries generated type  space time limits imposed
    node networks  moreover  approximations made  real numbers smaller        
regarded zero  since approximations algorithms  comparisons
fair 
statistics shown figure    curves  ve    ve  hardly visible
close  axis 
see average    performed significantly better pd  pd performed significantly better tt  tt performed much better  
one might notice tt able answer thirty nine ten observation queries 
  pd able to  due limit memory consumption  see
next subsection  memory consumption limit increased twenty megabytes    able
answer forty five ten observation queries exactly ten seconds 
     effectiveness  
established   efficient  based algorithm exploiting causal
independencies  section investigate effective   is 
   

fiz hang   p oole

    node bn
number queries

number queries

    node bn
  
  
  
  
  
  
  
  
  
 
 

  ve  
   ve  
   ve  
   ve  

                      
cpu time seconds

  
  
  
  
  
  
  
  
  
 
 

  ve  
   ve  
   ve  

 

                      
cpu time seconds

figure    time statistics    
experiments carried two cpcs networks answer question 
    node network  four types queries one query variable five  ten  fifteen  twenty
observations respectively considered  fifty queries randomly generated query
type  statistics times   took answer queries given left chart figure
   collecting statistics  ten mb memory limit ten second cpu time limit
imposed guide excessive resource demands  see fifty five observation queries
network answered less half second  forty eight ten observation queries 
forty five fifteen observation queries  forty twenty observation queries answered one
second  is  however  one twenty observation query   able answer within
time memory limits 
    node network  three types queries one query variable five  ten  fifteen 
observations respectively considered  fifty queries randomly generated query
type  unlike previous section  approximations made  twenty mb memory limit
forty second cpu time limit imposed  time statistics shown right hand side
chart  see   able answer queries majority queries
answered little time  are  however  three fifteen observation queries   able
answer 

    conclusions
paper concerned exploit causal independence exact bn inference  previous approaches  olesen et al         heckerman        use causal independencies transform
bns  efficiency gained inference easier transformed bns original
bns 
new method proposed paper  basic idea  bayesian network
viewed representing factorization joint probability multiplication list
conditional probabilities  studied notion causal independence enables one
factorize conditional probabilities combination even smaller factors consequently
obtain finer grain factorization joint probability 
propose extend inference algorithms make use finer grain factorization 
paper extended algorithm called   experiments shown extended algo   

fie xploiting c ausal ndependence



b ayesian n etwork nference

rithm      significantly efficient one first performs olesen et al s heckermans
transformation apply  
choice instead widely known ctp algorithm due ability work
networks ctp cannot deal with  matter fact  ctp able deal networks
used experiments  even olesen et al s heckermans transformation  hand 
  able answer almost randomly generated queries majority queries
answered little time  would interesting extend ctp make use finer grain
factorization mentioned above 
seen previous section  queries  especially     node network 
took   long time answer  queries   able answer 
queries  approximation must  employed approximation technique comparing
algorithms     node network  technique captures  extent  heuristic ignoring
minor distinctions  future work  developing way bound error technique
anytime algorithm based technique 

acknowledgements
grateful malcolm pradhan gregory provan sharing us cpcs networks 
thank jack breese  bruce dambrosio  mike horsch  runping qi  glenn shafer
valuable discussions  ronen brafman  chris geib  mike horsch anonymous reviewers helpful comments  mr  tak yin chan great help experimentations 
research supported nserc grant ogpoo       institute robotics intelligent
systems  hong kong research council grant hkust      e sino software research center
grant ssrc      eg   

appendix a  proof theorem  
theorem   suppose list homogeneous factors f  list heterogeneous factors f  constitute tidy factorization p  z    z          zk    z  either convergent variable  old regular
variable  new regular variable whose deputy list fz           zk g  procedure
sum out  f    f   z   returns tidy heterogeneous factorization p  z           zk   
proof  suppose f         fr heterogeneous factors g        gs homogeneous
factors  suppose f         fl   g        gm factors contain z   

p  z           zk  

 

 

 

 

x

p  z    z          zk 

z 

x
z 


rj  fj

x



i  

gi


lj  fj  
 
rj l   fj   







gi
i   i m  


x l


  
j    fj
gi 
 
rj l   fj   
gi
z 
i  
i m  
z 

  

   

gi

   

fiz hang   p oole

 

x





l
r
  

j   fj gi 
 
j l  fj   
gi  
z 
i  
i m  

    

equation      due proposition    equation     follows proposition    matter
q g due
fact  z  convergent variable  convergent variable
i  
first condition tidiness  condition proposition   satisfied z  appear
fl          fr   hand  z  old regular variable new regular variable whose
q g contains convergent variables due
deputy appear list z         zk  
i  
second condition tidiness  condition proposition   satisfied  thus proved
sum out  f    f   z   yields flexible heterogeneous factorization p  z           zk   
let e  convergent variable list z         zk   z  cannot corresponding new regular variable e  hence factor  e   e  touched sum out  f    f   z    consequently 
show new factor created sum out  f    f   z   either heterogeneous factor
homogeneous factor contain convergent variable  factorization returned tidy 
suppose sum out  f    f   z   create new homogeneous factor  heterogeneous factors f  contain z    z  convergent variable  say e    e   e  homop
geneous factor contain e    new factor e  e   e   contain convergent
variables  z  old regular variable new regular variable whose deputy list z   
     zk   factors contain z  contain convergent variables  hence new factor
contain convergent variables  theorem thus proved   
 

references
arnborg  s   corneil  d  g     proskurowski  a          complexity finding embedding
k tree  siam j  alg  disc  meth                
baker  m     boult  t          pruning bayesian networks efficient computation  proc  sixth
conf  uncertainty artificial intelligence  pp         cambridge  mass 
bertele  u     brioschi  f          nonserial dynamic programming  vol     mathematics
science engineering  academic press 
boutilier  c   friedman  n   goldszmidt  m     koller  d          context specific independence
bayesian networks  e  horvitz f  jensen  ed    proc  twelthth conf  uncertainty
artificial intelligence  pp         portland  oregon 
cooper  g  f          computational complexity probabilistic inference using bayesian belief
networks  artificial intelligence                  
dagum  p     galper  a          additive belief network models  d  heckerman a  mamdani
 ed    proc  ninth conf  uncertainty artificial intelligence  pp       washington d c 
dambrosio         local expression languages probabilistic dependence  international journal approximate reasoning              
dambrosio  b          symbolic probabilistic inference large bn o networks  r  lopez de
mantaras d  poole  ed    proc  tenth conf  uncertainty artificial intelligence  pp 
       seattle 
   

fie xploiting c ausal ndependence



b ayesian n etwork nference

dechter  r          bucket elimination  unifying framework probabilistic inference  e 
horvits f  jensen  ed    proc  twelthth conf  uncertainty artificial intelligence  pp 
       portland  oregon 
dez  f  j          parameter adjustment bayes networks  generalized noisy or gate  d 
heckerman a  mamdani  ed    proc  ninth conf  uncertainty artificial intelligence 
pp        washington d c 
duda  r  o   hart  p  e     nilsson  n  j          subjective bayesian methods rule based inference systems  proc  afips nat  comp  conf   pp           
geiger  d     heckerman  d          knowledge representation inference similarity networks
bayesian multinets  artificial intelligence           
geiger  d   verma  t     pearl  j          d separation  theorems algorithms  m  henrion
et  al   ed    uncertainty artificial intelligence    pp          north holland  new york 
good  i          causal calculus  i   british journal philosophy science             
heckerman  d          causal independence knowledge acquisition inference  proc 
ninth conference uncertainty artificial intelligence  pp         
heckerman  d     breese  j          new look causal independence  proc  tenth
conference uncertainty artificial ingelligence  pp         
henrion  m          practical issues constructing belief networks  l  kanal t  levitt
j  lemmer  ed    uncertainty artificial intelligence  pp          north holland 
howard  r  a     matheson  j  e          influence diagrams  howard  r  a     matheson 
j   eds    principles applications decision analysis  pp          strategic decisions group  ca 
jensen  f  v   lauritzen  s  l     olesen  k  g          bayesian updating causal probabilistic
networks local computations  computational statistics quaterly            
kim  j     pearl  j          computational model causal diagnostic reasoning inference
engines  proc  eighth international joint conference artificial intelligence  pp 
       karlsruhe  germany 
kjrulff  u          triangulation graphs   algorithms giving small total state space  tech  rep 
r        department mathematics computer science  strandvejen  dk      aalborg 
denmark 
lauritzen  s  l   dawid  a  p   larsen  b  n     leimer  h  g          independence properties
directed markov fields  networks             
lauritzen  s  l     spiegelhalter  d  j          local computations probabilities graphical
structures application expert systems  journal royal statistical society 
series b                
   

fiz hang   p oole

li  z     dambrosio  b          efficient inference bayes networks combinatorial optimization problem  international journal approximate reasoning              
olesen  k  g     andreassen  s          specification models large expert systems based
causal probabilistic networks  artificial intelligence medicine            
olesen  k  g   kjrulff  u   jensen  f   falck  b   andreassen  s     andersen  s  k         
munin network median nerve   case study loops  applied artificial intelligence 
          
parker  r     miller  r          using causal knowledge creat simulated patient cases  cpsc
project extension internist    proc    th symp  comp  appl  medical care  pp 
       los alamitos  ca  ieee comp soc press 
pearl  j          probabilistic reasoning intelligent systems  networks plausible inference 
morgan kaufmann  san mateo  ca 
poole  d          probabilistic horn abduction bayesian networks  artificial intelligence        
      
pradhan  m   provan  g   middleton  b     henrion  m          knowledge engineering large
belief networks  r  lopez de mantaras d  poole  ed    proc  tenth conf  uncertainty
artificial intelligence  pp         seattle 
shachter  r  d   dambrosio  b  d     del favero  b  d          symbolic probabilistic inference
belief networks  proc   th national conference artificial intelligence  pp        
boston  mit press 
shafer  g     shenoy  p          probability propagation  annals mathematics artificial
intelligence            
shortliffe  e     buchanan  g  b          model inexact reasoning medicine  math  biosci  
           
srinivas  s          generalization noisy or model  proc  ninth conference uncertainty artificial intelligence  pp         
tarjan  r  e     yannakakis  m          simple linear time algorithm test chordality graphs 
test acyclicity hypergraphs  selectively reduce acyclic hypergraphs  siam j  comput  
           
zhang  n  l     poole  d          simple approach bayesian network computations  proc 
tenth canadian conference artificial intelligence  pp         

   


