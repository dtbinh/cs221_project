journal artificial intelligence research               

submitted       published     

acquiring word meaning mappings
natural language interfaces
cynthia a  thompson

cindi cs utah edu

school computing  university utah
salt lake city  ut           

raymond j  mooney

mooney cs utexas edu

department computer sciences  university texas
austin  tx           

abstract
paper focuses system  wolfie  word learning interpreted examples   acquires semantic lexicon corpus sentences paired semantic
representations  lexicon learned consists phrases paired meaning representations  wolfie part integrated system learns transform sentences
representations logical database queries 
experimental results presented demonstrating wolfies ability learn useful
lexicons database interface four different natural languages  usefulness
lexicons learned wolfie compared acquired similar system 
results favorable wolfie  second set experiments demonstrates wolfies ability
scale larger difficult  albeit artificially generated  corpora 
natural language acquisition  difficult gather annotated data needed
supervised learning  however  unannotated data fairly plentiful  active learning
methods attempt select annotation training informative examples 
therefore potentially useful natural language applications  however 
results date active learning considered standard classification tasks 
reduce annotation effort maintaining accuracy  apply active learning semantic
lexicons  show active learning significantly reduce number annotated
examples required achieve given level performance 

   introduction overview
long standing goal field artificial intelligence enable computer understanding human languages  much progress made reaching goal  much
remains done  artificial intelligence systems meet goal  first need
ability parse sentences  transform representation easily
manipulated computers  several knowledge sources required parsing 
grammar  lexicon  parsing mechanism 
natural language processing  nlp  researchers traditionally attempted build
knowledge sources hand  often resulting brittle  inefficient systems take
significant effort build  goal overcome knowledge acquisition
bottleneck applying methods machine learning  develop apply methods
empirical corpus based nlp learn semantic lexicons  active learning
reduce annotation effort required learn them 

c
    
ai access foundation morgan kaufmann publishers  rights reserved 

fithompson   mooney

semantic lexicon one nlp component typically challenging time consuming construct update hand  notion semantic lexicon  formally defined
section    list phrase meaning pairs  meaning representation
determined language understanding task hand  taking compositional view sentence meaning  partee  meulen    wall         paper describes
system  wolfie  word learning interpreted examples   acquires semantic
lexicon phrase meaning pairs corpus sentences paired semantic representations  goal automate lexicon construction integrated nlp system
acquires semantic lexicons parsers natural language interfaces single
training set annotated sentences 
although many others  sebillot  bouillon    fabre        riloff   jones        siskind 
      hastings        grefenstette        brent        presented systems learning
information lexical semantics  present system learning lexicons phrasemeaning pairs  further  work unique combination several features  though
prior work included aspects  first  output used system 
chill  zelle   mooney        zelle         learns parse sentences semantic
representations  second  uses fairly straightforward batch  greedy  heuristic learning
algorithm requires small number examples generalize well  third 
easily extendible new representation formalisms  fourth  requires prior knowledge
although exploit initial lexicon provided  finally  simplifies learning
problem making several assumptions training data  described
section     
test wolfies ability acquire semantic lexicon natural language interface
geographical database using corpus queries collected human subjects
annotated logical form  test  wolfie integrated chill 
learns parsers requires semantic lexicon  previously built manually   results
demonstrate final acquired parser performs nearly accurately answering novel
questions using learned lexicon using hand built lexicon  wolfie
compared alternative lexicon acquisition system developed siskind        
demonstrating superior performance task  finally  corpus translated
spanish  japanese  turkish  experiments conducted demonstrating ability
learn successful lexicons parsers variety languages 
second set experiments demonstrates wolfies ability scale larger
difficult  albeit artificially generated  corpora  overall  results demonstrate robust
ability acquire accurate lexicons directly usable semantic parsing 
integrated system  task building semantic parser new domain simplified 
single representative corpus sentence representation pairs allows acquisition
semantic lexicon parser generalizes well novel sentences 
building annotated corpus arguably less work building entire nlp
system  still simple task  redundancies errors may occur training data 
goal minimize annotation effort  yet still achieve reasonable level
generalization performance  case natural language  frequently large
amount unannotated text available  would automatically  intelligently 
choose available sentences annotate 

 

fiacquiring word meaning mappings

using technique called active learning  active learning research
area machine learning features systems automatically select informative examples annotation training  cohn  atlas    ladner         primary goal
active learning reduce number examples system trained on  thereby
reducing example annotation cost  maintaining accuracy acquired information  demonstrate usefulness active learning techniques  compared
accuracy parsers lexicons learned using examples chosen active learning
lexicon acquisition  learned using randomly chosen examples  finding active
learning saved significant annotation cost training randomly chosen examples 
savings demonstrated geography query domain 
summary  paper provides new statement lexicon acquisition problem
demonstrates machine learning technique solving problem  next  combining previous research  show entire natural language interface
acquired one training corpus  further  demonstrate application active
learning techniques minimize number sentences annotate training input
integrated learning system 
remainder paper organized follows  section   gives background
information chill introduces siskinds lexicon acquisition system 
compare wolfie section    sections     formally define learning problem
describe wolfie algorithm detail  section   present discuss experiments
evaluating wolfies performance learning lexicons database query domain
artificial corpus  next  section   describes evaluates use active learning
techniques wolfie  sections     discuss related research future directions 
respectively  finally  section   summarizes research results 

   background
section give overview chill  system research adds to 
describe jeff siskinds lexicon acquisition system 
    chill
output produced wolfie used assist larger language acquisition system 
particular  currently used part input parser acquisition system called
chill  constructive heuristics induction language learning   chill uses inductive
logic programming  muggleton        lavrac   dzeroski        learn deterministic
shift reduce parser  tomita        written prolog  input chill corpus
sentences paired semantic representations  input required wolfie 
parser learned capable mapping sentences correct representations  well
generalizing well novel sentences  paper  limit discussion chills
ability acquire parsers map natural language questions directly prolog queries
executed produce answer  zelle   mooney         following two
sample queries database u s  geography  paired corresponding prolog
query 

 

fithompson   mooney

 sentence  representation 
training
examples

wolfie

chill

lexicon
 phrase  meaning 

final
parser
prolog

figure    integrated system
capital state biggest population 
answer c   capital s c   largest p   state s   population s p      
state texarkana located in 
answer s   state s   eq c cityid texarkana      loc c s    
chill treats parser induction problem learning rules control actions
shift reduce parser  parsing  current context maintained stack
buffer containing remaining input  parsing complete  stack contains
representation input sentence  three types operators parser uses
construct logical queries  one introduction onto stack predicate needed
sentence representation due phrases appearance front input buffer 
operators require semantic lexicon background knowledge  details
two parsing operators  see zelle mooney         using wolfie 
lexicon provided automatically  figure   illustrates complete system 
    jeff siskinds lexicon learning research
closely related previous research automated lexicon acquisition
siskind         inspired work rayner  hugosson  hagert        
comparing system section    describe main features
research section  goal one cognitive modeling childrens acquisition
lexicon  lexicon used comprehension generation  goal
machine learning engineering one  focuses lexicon comprehension
use parsing  using learning process claim cognitive plausibility 
goal learning lexicon generalizes well small number training
examples 
system takes incremental approach acquiring lexicon  learning proceeds
two stages  first stage learns symbols representation used
 

fiacquiring word meaning mappings

 capital  capital       
 biggest  largest       
 highest point  high point       
 through  traverse       
 has  loc      

 state  state     
 in  loc       
 long  len       
 capital  capital     

figure    sample semantic lexicon
final conceptual expression represents meaning word  using versionspace approach  second stage learns symbols put together form
final representation  example  learning meaning word raise 
algorithm may learn set  cause  go  up  first stage put together
form expression cause x  go y  up   second stage 
siskind        shows effectiveness approach series artificial corpora 
system handles noise  lexical ambiguity  referential uncertainty  large corpora  usefulness lexicons learned compared correct  artificial
lexicon  goal experiments presented evaluate correctness
completeness learned lexicons  earlier work  siskind        evaluated versions
technique quite small corpus real english japanese sentences  extend
evaluation demonstration systems usefulness performing real world natural
language processing tasks  using larger corpus real sentences 

   lexicon acquisition problem
although end goal acquire entire natural language interface  currently
divide task two parts  lexicon acquisition component parser acquisition
component  section  discuss problem acquiring semantic lexicons
assist parsing acquisition parsers  training input consists natural language
sentences paired meaning representations  pairs extract lexicon
consisting phrases paired meaning representations  training pairs
given previous section  sample lexicon shown figure   
    formal definition
present learning problem formally  definitions needed 
following use terms string substring  extend straight forwardly
natural language sentences phrases  respectively  refer labeled trees  making
assumption semantic meanings interest represented such 
common representations recast labeled trees forests  formalism extends
easily latter 
definition  let v   e finite alphabets vertex labels edge labels  respectively 
let v finite nonempty set vertices  l total function l   v v   e set unordered
pairs distinct vertices called edges  total function   e e   g    v  l  e  a 
labeled graph 

 

fithompson   mooney

string    girl ate pasta cheese 
  vertex edge labels 

tree  
 
 
 

ingest
patient
agent
person food
age type accomp
sex

 
 

 

 

female

child pasta

food

type
cheese

 

interpretation f     t   
f    girl     
f    ate      
f    pasta      
f    the cheese      

figure    labeled trees interpretations
definition  labeled tree connected  acyclic labeled graph 
figure   shows labeled tree t   with vertices      left  associated vertex
edge labels right  function l is  
 

    ingest       person       food       female       child       pasta  
    food       cheese    

tree t  semantic representation sentence s    girl ate pasta
cheese  using conceptual dependency  schank        representation prolog list form 
meaning is 
 ingest 

agent  person  sex female  age child  
patient  food  type pasta  accomp  food  type cheese    

definition  u v path graph g finite alternating sequence vertices edges
g  vertex repeated  begins vertex u ends vertex v 
edge sequence connects vertex precedes sequence
vertex follows sequence 
definition  directed  labeled tree    v  l  e  a  labeled tree whose edges consist
ordered pairs vertices  distinguished vertex r  called root  property
every v v   directed r v path   underlying
undirected unlabeled graph induced  v  e  connected  acyclic graph 
definition  interpretation f finite string directed  labeled tree
one to one function mapping subset s  substrings s  two strings
s  overlap  vertices root range f  
   omit enumeration function e could given similar manner  example        
agent  element e 

 

fiacquiring word meaning mappings

girl  

person
sex
age
female

pasta   food

type

child

pasta

cheese   food
type
cheese

ate   ingest

figure    meanings
interpretation provides information parts meaning sentence
originate phrases  figure    show interpretation  f    s  t   
note domain f    since s  subset substrings s  thus
allowing words meaning  disallow overlapping substrings
domain  cheese cheese could map vertices t   
definition  given interpretation f string tree t  element p domain
f   meaning p relative s  t  f connected subgraph whose vertices
include f  p  descendents except vertices range f
descendents 
meanings sense concern lowest level phrasal meanings  occurring
terminal nodes semantic grammar  namely entries semantic lexicon 
grammar used construct meanings longer phrases entire sentences 
motivation previously stated constraint root must included
range f   want vertices sentence representation included
meaning phrase  note meaning p directed tree f  p 
root  figure   shows meanings phrase domain interpretation function
f  shown figure    show labels vertices edges readability 
definition  given finite set st f triples   s    t    f               sn   tn   fn   
si finite string  ti directed  labeled tree  interpretation function
si ti   let language lst f    p            pk   st f union substrings 
occur domain   pj lst f   meaning set pj   denoted
mst f  pj     set meanings pj relative si   ti     si   ti     st f  
consider two meanings isomorphic trees taking labels
account 
example  given sentence s    man ate cheese  labeled tree t  pictured
figure    f  defined as  f   ate       f   man       f   the cheese      
   consider two substrings string contain characters order 
irrespective positions within larger string occur 
   omit subscript set st f obvious context 

 

fithompson   mooney

string s    man ate cheese  
tree t   

t  vertex edge labels 
ingest
patient
agent

 
 

 

 

person food
type
age
sex
 

 

male

adult

cheese

figure    second tree
meaning set cheese respect st f      s    t    f       s    t    f       food 
type cheese    one meaning though f  f  map cheese different vertices
two trees  subgraphs denoting meaning cheese two
functions isomorphic 
definition  given finite set st f triples   s    t    f               sn   tn   fn   
si finite string  ti directed  labeled tree  interpretation
function si ti   covering lexicon expressed st f
  p  m    p lst f    p   
covering lexicon l expressed st f      s    t    f       s    t    f     is 
 

 girl   person  sex female  age child   
 man   person  sex male  age adult   
 ate   ingest   
 pasta   food  type pasta   
 the cheese   food  type cheese     

idea covering lexicon provides  string  sentence  si   meaning
phrases sentence  further  meanings trees whose labeled
vertices together include labeled vertices tree ti representing meaning
si   vertices duplicated  containing vertices ti   edge labels may
may included  since idea due syntax 
parser provide  edges capturing lexical semantics lexicon  note
include covering lexicon phrases  substrings  domains
s  words empty tree meaning included covering lexicon 
note general use phrase mean substrings sentences  whether
consist one word  one  finally strings covering lexicon may
contain overlapping words even though domain individual interpretation
function must not  since overlapping words could occurred different sentences 
finally  ready define learning problem hand 

 

fiacquiring word meaning mappings

lexicon acquisition problem 
given  multiset strings    s            sn   multiset labeled trees    t            tn   
find  multiset interpretation functions  f    f            fn    cardinality
covering lexicon expressed st f      s    t    f               sn   tn   fn    minimized 
set found  say found minimal set interpretations  or minimal
covering lexicon    
less formally  learner presented multiset sentences  s  paired
meanings  t    goal learning find smallest lexicon consistent data 
lexicon paired listing phrases occurring domain f
 where f multiset interpretation functions found  elements
meaning sets  motivation finding lexicon minimal size usual bias
towards simplicity representation generalization beyond training data 
definition allows phrases length  usually want limit length
phrases considered inclusion domain interpretation functions 
efficiency purposes 
determine set interpretation functions set strings trees 
one unique covering lexicon expressed st f   however  might
set interpretation functions possible  may result lexicon smallest
cardinality  example  covering lexicon given previous example
minimal covering lexicon  two sentences given  could find minimal  though
rather degenerate  lexicons as 
 

 girl 
 man 

 ingest  agent  person  sex female  age child  
patient  food  type pasta  accomp  food  type cheese     
 ingest  agent  person  sex male  age adult  
patient  food  type cheese     

type lexicon becomes less likely size corpus grows 
    implications definition
definition lexicon acquisition problem differs given authors 
including riloff jones         siskind         manning         brent        others 
discussed section    definition problem makes assumptions
training input  first  making f function instead relation  definition
assumes meaning phrase sentence appears representation
sentence  single use assumption  second  making f one to one  assumes
exclusivity  vertex sentences representation due one phrase
sentence  third  assumes phrases meaning connected subgraph sentences
representation  distributed representation  connectedness assumption 
first assumption may hold representation languages  present
problem domains considered  second third assumptions perhaps
less problematic respect general language use 
definition assumes compositionality  meaning sentence derived
meanings phrases contains  addition  perhaps connecting
information specific representation hand  derived external sources
 

fithompson   mooney

noise  words  vertices sentences representation included
within meaning word phrase sentence  assumption similar
linking rules jackendoff         used previous work grammar
language acquisition  e g   haas jayaraman        siskind         
debate linguistics community ability compositional techniques
handle phenomena  fillmore        goldberg         making assumption simplifies
learning process works reasonably domains interest here  also  since
allow multi word phrases lexicon  e g    kick bucket  die       one objection
compositionality addressed 
definition allows training input which 
   words phrases multiple meanings  is  homonymy might occur
lexicon 
   several phrases map meaning  is  synonymy might occur
lexicon 
   words sentence map meanings  leaving unused
assignment words meanings  
   phrases contiguous words map parts sentences meaning representation 
particular note lexical ambiguity    above   note could derived
ambiguous lexicon as 
 

 girl   person  sex female  age child   
 ate   ingest   
 ate   ingest  agent  person  sex male  age adult    
 pasta   food  type pasta   
 the cheese   food  type cheese     

sample corpus  lexicon  ate ambiguous word  earlier example
minimizes ambiguity resulting alternative  intuitively pleasing lexicon 
problem definition first minimizes number entries lexicon  learning
algorithm exploit preference minimizing ambiguity 
note definition allows training input sentences
ambiguous  paired one meaning   since given sentence  a multiset 
might appear multiple times appear one meaning  fact  training data
consider section   ambiguous sentences 
definition lexicon acquisition problem fit cleanly traditional
definition learning classification  training example contains sentence
semantic parse  trying extract semantic information phrases
sentence  example potentially contains information multiple target
concepts  phrases   trying pick relevant features  vertices
   fact  assumptions except single use made siskind         see section  
details 
   words may  however  serve cues parser assemble sentence meanings word
meanings 

  

fiacquiring word meaning mappings

representation  corresponding correct meaning phrase  course  assumptions single use  exclusivity  connectedness  compositionality impose additional
constraints  addition multiple examples one learning scenario 
access negative examples  derive implicit negatives 
possibility ambiguous synonymous phrases 
ways problem related clustering  capable learning
multiple  potentially non disjoint categories  however  clear clustering
system could made learn phrase meaning mappings needed parsing  finally 
current systems learn multiple concepts commonly use examples concepts
negative examples concept currently learned  implicit assumption made
concepts disjoint  unwarranted assumption presence
synonymy 

   wolfie algorithm example
section  first discuss issues considered design algorithm 
describe fully section     
    solving lexicon acquisition problem
first attempt solve lexicon acquisition problem might examine interpretation functions across corpus  choose one s  minimal lexicon size 
number possible interpretation functions given input pair dependent
size sentence representation  sentence w words   w   
possible phrases  particular challenge 
however  number possible interpretation functions grows extremely quickly
size input  sentence p phrases associated tree n vertices 
number possible interpretation functions is 
c  n    

c
x
i  

 
 
 i     n i   c i  

   

c min p  n   derivation formula follows  must choose
phrases use domain f   choose one phrase  two 
number min p  n   if n   p assign n phrases since f one to one  
p


 

 

p 
i  p i  

number phrases chosen  permute phrases 
order assigned vertices different  i  permutations  must choose vertices include range interpretation
function  choose root time  choosing vertices 
n   choose   vertices left choosing root 
n 
i 

 

 

 n    
 
 i     n i  
  

fithompson   mooney

full number possible interpretation functions then 
min p n 

x
i  

p 
 n    
i 
 
i  p i  
 i     n i  

simplifies equation    n   p  largest term equation c   
p   grows least exponentially p  general number interpretation
functions large allow enumeration  therefore  finding lexicon examining
interpretations across corpus  choosing lexicon s  minimum size  clearly
tractable 
instead finding interpretations  one could find set candidate meanings
phrase  final meaning s  phrase could chosen way
minimizes lexicon size  one way find candidate meanings fracture meanings
sentences phrase appears  siskind        defined fracturing  he calls
unlink  operation  terms result includes subterms
expression plus   representation formalism  corresponds finding possible
connected subgraphs meaning  adding empty graph  interpretation
function technique discussed  fracturing would lead exponential blowup
number candidate meanings phrase  lower bound number connected
subgraphs full binary tree n vertices obtained noting subset
 n        leaves may deleted still maintain connectivity remaining tree 
thus  counting ways leaves deleted gives us lower bound   n     
fractures   completely rule fracturing part technique lexicon
learning since trees tend get large  indeed siskind uses many
systems  constraints help control search  however  wish avoid
chance exponential blowup preserve generality approach tasks 
another option force chill essentially induce lexicon own 
model  would provide chill ambiguous lexicon phrase paired
every fracture every sentence appears  chill would decide
set fractures leads correct parse training sentence  would
include final learned parser lexicon combination  thus search would
become exponential  furthermore  even small representations  would likely lead
system poor generalization ability  siskinds work  e g   siskind 
      took syntactic constraints account encounter difficulties 
versions handle lexical ambiguity 
could efficiently find good candidates  standard induction algorithm could
attempt use source training examples phrase  however 
attempt use list candidate meanings one phrase negative examples
another phrase would flawed  learner could know advance phrases
possibly synonymous  thus phrase lists use negative examples
phrase meanings  also  many representation components would present lists
one phrase  source conflicting evidence learner  even without
presence synonymy  since positive examples available  one might think
using specific conjunctive learning  finding intersection representations
   thanks net citizen dan hirshberg help analysis 

  

fiacquiring word meaning mappings

phrase  p  of two words  
     collect training examples p appears
     calculate lics  sampled  pairs examples representations
     l lics  add  p  l  set candidate lexicon entries
input representations covered  candidate lexicon entries remain do 
     add best  phrase  meaning  pair candidate entries lexicon
     update candidate meanings phrases sentences phrase learned
return lexicon learned  phrase  meaning  pairs 

figure    wolfie algorithm overview
phrase  proposed anderson         however  meanings ambiguous
phrase disjunctive  intersection would empty  similar difficulty would
expected positive only compression muggleton        
    solution  wolfie
analysis leads us believe lexicon acquisition problem computationally intractable  therefore  perform efficient search best lexicon 
use standard induction algorithm  therefore  implemented wolfie   
outlined figure    finds approximate solution lexicon acquisition problem  approach generate set candidate lexicon entries  final
learned lexicon derived greedily choosing best lexicon item point 
hopes finding final  minimal  covering lexicon  actually learn interpretation
functions  guarantee find covering lexicon   even
search interpretation functions  using greedy search would guarantee covering
input  course guarantee minimal lexicon found  however 
later present experimental results demonstrating greedy approach performs
well 
wolfie first derives initial set candidate meanings phrase  algorithm
generating candidates  lics  attempts find maximally common meaning
phrase  biases toward finding small lexicon covering many vertices tree
once  finding lexicon actually cover input  second  wolfie chooses
final lexicon entries candidate set  one time  updating candidate set
goes  taking account assumptions single use  connectedness  exclusivity 
basic scheme choosing entries candidate set maximize prediction
meanings given phrases  find general meanings  adds tension
lics  cover many vertices  generality  biases towards fewer vertices  however  generality  lics  helps lead small lexicon since general meaning
likely apply widely across corpus 
   code available upon request first author 
   though  course  interpretation functions way guarantee covering lexicon see
siskind        alternative 

  

fithompson   mooney

answer  

 

 

 



 

state  

eq  

 

 


 

c

cityid  

loc  
 
 
c



 
texarkana
figure    tree variables
let us explain algorithm detail way example  using spanish
instead english illustrate difficulty somewhat clearly  consider following
corpus 
   cual es el capital del estado con la poblacion mas grande 
answer c   capital s c   largest p   state s   population s p      
   cual es la punta mas alta del estado con la area mas grande 
answer p   high point s p   largest a   state s   area s a      
   en que estado se encuentra texarkana 
answer s   state s   eq c cityid texarkana      loc c s    
   que capital es la mas grande 
answer a  largest a  capital a    
   que es la area de los estados unitos 
answer a   area c a   eq c countryid usa     
   cual es la poblacion de un estado que bordean utah 
answer p   population s p   state s   next to s m   eq m stateid utah     
   que es la punta mas alta del estado con la capital madison 
answer c   high point b c   loc c b   state b  
capital b a   eq a cityid madison       

sentence representations slightly different tree representations given
problem definition  main difference addition existentially quantified
variables shared leaves representation tree  mentioned section     
representations prolog queries database  given query  create
tree conforms formalism  addition quantified variables 
example shown figure   representation third sentence  vertex
predicate name arity  prolog style  e g   state    quantified variables
leaves  outgoing edge  n  m  vertex n  edge labeled
argument position filled subtree rooted m  edge labeled
given argument position  argument free variable  vertex labeled
  

fiacquiring word meaning mappings

variable  which occur leaves  existentially quantified variable whose scope
entire tree  or query   learned lexicon  however  need maintain
identity variables across distinct lexical entries 
another representation difference strip answer predicate
input learner   thus allowing forest directed trees input rather single
tree  definition problem easily extends root tree
forest must domain interpretation function 
evaluation system using representation given section      evaluation
using representation without variables forests presented section      previously
 thompson        presented results demonstrating learning representations different
form  case role representation  fillmore        augmented conceptual dependency  schank        information  last representation conforms directly
problem definition 
now  continuing example solving lexicon acquisition problem
corpus  let us assume simplification  although required  sentences
stripped phrases know empty meanings  e g   que  es  con  la  
similarly assume known phrases refer directly given database
constants  e g   location names   remove phrases meaning
training input 
      candidate generation phase
initial candidate meanings phrase produced computing maximally common
substructure s  sampled pairs representations sentences contain it 
derive common substructure computing largest isomorphic connected subgraphs
 lics  two labeled trees  taking labels account isomorphism  analogous
largest common subgraph problem  garey   johnson        solvable polynomial
time if  assume  inputs trees k  number edges include 
given  thus  start k set equal largest number edges two trees
compared  test common subgraph s   iterate k      stopping one
subgraphs found given k 
prolog query representation  algorithm complicated bit variables 
therefore  use lics addition similar computing least general generalization first order clauses  plotkin         lgg two sets literals least
general set literals subsumes sets literals  add allowing
term argument literal conjunction  algorithm tries orderings
matching terms conjunction  overall  algorithm finding lics
two trees prolog representation first finds common labeled edges
vertices usual lics  treats variables equivalent  then  computes
least general generalization  conjunction taken account  resulting trees
converted back literals  example  given two trees 

   predicate omitted chill initializes parse stack answer predicate  thus
word mapped it 

  

fithompson   mooney

phrase
capital 

grande 
estado 

punta mas 
encuentra 

lics
largest     
capital     
state   
largest   state    
largest     
largest   state    
state   
 population s     state s  
capital     
high point     
 state s   loc   s  
high point     
state   
 state s   loc   s  

sentences
   
   
   
   
        
   
                                 
   
   
   
   
   
   
 

table    sample candidate lexical entries derivation
answer c   largest p   state s   population s p     capital s c    
answer p   high point s p   largest a   state s   area s a       
common meaning answer   largest   state      note lics two trees
may unique  may multiple common subtrees contain
number edges  case lics returns multiple answers 
sets initial candidate meanings phrases sample corpus
shown table    example show lics pairs phrase
appears in  actual algorithm randomly sample subset efficiency reasons 
golem  muggleton   feng         phrases appearing one sentence
 e g   encuentra   entire sentence representation  excluding database constant
given background knowledge  used initial candidate meaning  candidates
typically generalized step     algorithm correct portion
representation added lexicon  see example below 
      adding final lexicon
deriving initial candidates  greedy search begins  heuristic used evaluate
candidates attempts help assure small covering lexicon learned  heuristic
first looks weighted sum two components  p phrase candidate
meaning 
   p  m   p  p  p   m  p  m    p  p  p  m   p  
   generality
then  ties value broken preferring less ambiguous  those fewer current
meanings  shorter phrases  first component analogous cluster evaluation
  

fiacquiring word meaning mappings

heuristic used cobweb  fisher         measures utility clusters based
attribute value pairs categories  instead meanings phrases  probabilities
estimated training data updated learning progresses account
phrases meanings already covered  see updating works
continue example algorithm  goal part heuristic
maximize probability predicting correct meaning randomly sampled
phrase  equality holds bayes theorem  looking right side  p  m   p  
expected probability meaning correctly guessed given phrase  p 
assumes strategy probability matching  meaning chosen p
probability p  m   p  correct probability  term  p  p   biases
component common phrase is  interpreting left side equation 
first term biases towards lexicons low ambiguity  second towards low synonymy 
third towards frequent meanings 
second component heuristic  generality  computed negation
number vertices meanings tree structure  helps prefer smaller 
general meanings  example  candidate set above  else equal 
generality portion heuristic would prefer state     generality value    
largest   state      state s  loc   s    generality value    
meaning estado  learning meaning fewer terms helps evenly distribute
vertices sentences representation among meanings phrases sentence 
thus leads lexicon likely correct  see this  note
pairs words tend frequently co occur  grande estado example  
joint representation  meaning  likely set candidate meanings
words  preferring general meaning  easily ignore incorrect joint
meanings 
example experiments  use weight    first component
heuristic  weight   second  first component smaller absolute
values therefore given higher weight  modulo consideration  results
overly sensitive weights automatically setting using cross validation
training set  kohavi   john        little effect overall performance  table  
illustrate calculation heuristic measure fourteen pairs 
value all  calculation shows sum multiplying    first component
heuristic multiplying   second component  first component simplified
follows 
  p     p   
  p   
p  p  p  m   p    


 

  p   
 p 
  p   number times phrase p appears corpus  initial number
candidate phrases    p   number times meaning paired
phrase p  ignore since number phrases corpus
pair  effect ranking  highest scoring pair  estado  state     
added lexicon 
next candidate generalization step        described algorithmically figure   
one key ideas algorithm phrase meaning choice constrain
candidate meanings phrases yet learned  given assumption portion
representation due one phrase sentence  exclusivity   part
  

fithompson   mooney

candidate lexicon entry
 capital  largest       
 capital  capital       
 capital  state       
 grande  largest   state      
 grande  largest       
 estado  largest   state      
 estado  state     
 estado   population s     state s   
 estado  capital       
 estado  high point       
 estado   state s   loc   s    
 punta mas  high point       
 punta mas  state     
 encuentra   state s   loc   s    

heuristic value
                        
     
     
                       
  
                    
                     
 
 
 
 
  
                     
                    

table    heuristic value sample candidate lexical entries

given  learned phrase meaning pair  l  g 
sentence representation pairs containing l g  mark covered 
candidate phrase meaning pair  p  m  
p occurs training pairs  l  g 
vertices intersect vertices g
occurrences covered
remove  p  m  set candidate pairs 
else
adjust heuristic value  p  m  needed account
newly covered nodes training representations 
generalize remove covered nodes  obtaining m   
calculate heuristic value new candidate pair  p  m    
candidate meanings remain uncovered phrase
derive new lics uncovered representations
calculate heuristic values 

figure    candidate generalization phase

  

fiacquiring word meaning mappings

representation covered  phrase sentence paired meaning
 at least sentence   therefore  step     candidate meanings words
sentences word learned generalized exclude representation
learned  use operation analogous set difference finding remaining
uncovered vertices representation generalizing meanings eliminate covered
vertices candidate pairs  example  meaning largest      learned
phrase sentence    meaning left behind would forest consisting
trees high point s     state s   area s      also  generalization results
empty tree  new lics calculated  example  since state    covered
sentences                candidates several words sentences
generalized  example  meaning  state s   loc   s   encuentra 
generalized loc       new heuristic value                       also 
single use assumption allows us remove candidate pairs containing estado
set candidate meanings  since learned pair covers occurrences estado
set 
note pairwise matchings generate candidate items  together updating candidate set  enable multiple meanings learned ambiguous phrases 
makes algorithm less sensitive initial rate sampling lics  example 
note capital ambiguous data set  though ambiguity artifact
way query language designed  one ordinarily think
ambiguous word  however  meanings learned  second pair added
final lexicon  grande  largest        causes generalization empty
meaning first candidate entry table    since new lics sentence  
generated  entire remaining meaning added candidate meaning set
capital mas 
subsequently  greedy search continues resulting lexicon covers training
corpus  candidate phrase meanings remain  rare cases  learning errors occur
leave portions representations uncovered  example  following lexicon
learned 
 estado  state     
 grande  largest     
 area  area     
 punta  high point       
 poblacion  population       
 capital  capital       
 encuentra  loc       
 alta  loc       
 bordean  next to     
 capital  capital     
next section  discuss ability wolfie learn lexicons useful
parsers parser acquisition 

  

fithompson   mooney

   evaluation wolfie
following two sections discuss experiments testing wolfies success learning lexicons
real artificial corpora  comparing several cases previously developed
lexicon learning system 
    database query application
section describes experimental results database query application  first
corpus discussed contains     questions u s  geography  paired prolog
query extract answer question database  domain originally
chosen due availability hand built natural language interface  geobase 
database containing     facts  geobase supplied turbo prolog    
 borland international         designed specifically domain  questions
corpus collected asking undergraduate students generate english questions
database  though given cursory knowledge database without
given chance use it  broaden test      sentences translated
spanish  turkish  japanese  japanese translations word segmented roman
orthography  translated questions paired appropriate logical queries
english corpus 
evaluate learned lexicons  measured utility background knowledge
chill  performed choosing random set    test examples
learning lexicons parsers increasingly larger subsets remaining     examples
 increasing    examples time   training  test examples parsed using
learned parser  submit resulting queries database  compare
answers generated submitting correct representation database 
record percentage correct  matching  answers  using difficult gold standard
retrieving correct answer  avoid measures partial accuracy believe
adequately measure final utility  repeated process ten different random training
test sets evaluated performance differences using two tailed  paired t test
significance level p      
compared system incremental  on line  lexicon learner developed siskind
        make equitable comparison batch algorithm  ran simulated batch mode  repeatedly presenting corpus     times  analogous running
    epochs train neural network  actually add new kinds data
learn  allows algorithm perform inter sentential inference directions corpus instead one  point compare accuracy
size training corpus  metric optimized siskind  worried
difference execution time here    lexicons learned running siskinds
system incremental mode  presenting corpus single time  resulted substantially
lower performance preliminary experiments data  removed wolfies
ability learn phrases one word  since current version siskinds system
    cpu times two system directly comparable since one written prolog
lisp  however  learning time two systems approximately siskinds
system run incremental mode  seconds     training examples 

  

fiacquiring word meaning mappings

  

  

  

accuracy

  

  

  

  

chill handbuilt
chill testlex
chill wolfie
chill siskind
geobase

  

  

 
 

  

   
   
training examples

   

   

figure    accuracy english geography corpus
ability  finally  made comparisons parsers learned chill
using hand coded lexicon background knowledge 
similar applications  many terms  state city names 
whose meanings automatically extracted database  therefore  tests
run names given learner initial lexicon  helpful
required  section     gives results different task initial lexicon 
however  unless otherwise noted  tests within section       strip
sentences phrases known empty meanings  unlike example section   
      comparisons using english
first experiment comparison original english corpus  figure   shows
learning curves chill using lexicons learned wolfie  chill wolfie 
siskinds system  chill siskind   uppermost curve  chill handbuilt  shows
chills performance given hand built lexicon  chill testlex shows performance words never appear training data  e g   test sentences 
deleted hand built lexicon  since learning algorithm chance learning
these   finally  horizontal line shows performance geobase benchmark 
results show lexicon learned wolfie led parsers almost
accurate generated using hand built lexicon  best accuracy achieved
parsers using hand built lexicon  followed hand built lexicon words
test set removed  followed wolfie  followed siskinds system  systems
well better geobase time reach     training examples 
differences wolfie siskinds system statistically significant training
  

fithompson   mooney

lexicon
hand built
wolfie
siskind

coverage
    
    
     

ambiguity
   
   
   

entries
  
    
     

table    lexicon comparison
example sizes  results show wolfie learn lexicons support learning
successful parsers  better perspective learned
competing system  also  comparing chill testlex curve  see
drop accuracy hand built lexicon due words test set system
seen training  fact  none differences chill wolfie
chill testlex statistically significant 
one implicit hypotheses problem definition coverage training
data implies good lexicon  results show coverage          training examples wolfie versus       siskind  addition  lexicons learned siskinds
system ambiguous larger learned wolfie  wolfies lexicons average     meanings per word  average size      entries  after
    training examples  versus     meanings per word       entries siskinds lexicons  comparison  hand built lexicon     meanings per word    entries 
differences  summarized table    undoubtedly contribute final performance
differences 
      performance natural languages
next  examined performance two systems spanish version corpus 
figure    shows results  differences using wolfie siskinds learned
lexicons chill statistically significant training set sizes 
show performance hand built lexicons  without phrases present
testing set  performance compared hand built lexicon test set phrases
removed still competitive  difference significant     examples 
figure    shows accuracy learned parsers wolfies learned lexicons
four languages  performance differences among four languages quite small 
demonstrating methods language dependent 
      larger corpus
next  present results larger  diverse corpus geography domain 
additional sentences collected computer science undergraduates
introductory ai course  set questions smaller corpus collected
students german class  special instructions complexity queries desired 
ai students tended ask complex diverse queries  task give five
interesting questions associated logical form homework assignment  though
direct access database  requested give least
one sentence whose representation included predicate containing embedded predicates 

  

fiacquiring word meaning mappings

   
  
  
  

accuracy

  
  
  
  

span chill handbuilt
span chill testlex
span chill wolfie
span chill siskind

  
  
 
 

  

   
   
training examples

   

   

   

   

figure     accuracy spanish

   
  
  
  

accuracy

  
  
  
  

english
spanish
japanese
turkish

  
  
 
 

  

   
   
training examples

figure     accuracy four languages

  

fithompson   mooney

   
  
  
  

accuracy

  
  
  
  

chill
wolfie
geobase

  
  
 
 

  

   

   

   
   
training examples

   

   

   

   

figure     accuracy larger geography corpus
example largest s  state s    asked variety sentences 
    new sentences  total      including original     sentences  
experiments  split data     training sentences    test sentences     random splits  trained wolfie chill before  goal
see whether wolfie still effective difficult corpus  since
approximately    novel words new sentences  therefore  tested performance chill extended hand built lexicon  test  stripped sentences
phrases known empty meanings  example section      again 
use phrases one word  since seem make significant
difference domain  results  compare wolfies lexicons chill using
hand built lexicons without phrases appear test set 
figure    shows resulting learning curves  differences chill using
hand built learned lexicons statistically significant                   
examples  four nine data points   mixed results indicate
difficulty domain variable vocabulary  however  improvement
machine learning methods geobase hand built interface much dramatic
corpus 
      lics versus fracturing
one component algorithm yet evaluated explicitly candidate generation
method  mentioned section      could use fractures representations sentences
phrase appears generate candidate meanings phrase  instead
lics  used approach compared previously described method
using largest isomorphic connected subgraphs sampled pairs representations

  

fiacquiring word meaning mappings

   
  
  
  

accuracy

  
  
  
  

fractwolfie
wolfie

  
  
 
 

  

   
   
training examples

   

   

figure     fracturing vs  lics  accuracy
candidate meanings  attempt fair comparison  sampled representations
fracturing  using number source representations number pairs
sampled lics 
accuracy chill using resulting learned lexicons background knowledge shown figure     using fracturing  fractwolfie  shows little advantage 
none differences two systems statistically significant 
addition  number initial candidate lexicon entries choose
much larger fracturing lics method  shown figure     true even
though sampled number representations pairs lics 
larger number fractures arbitrary representation number lics
arbitrary pair  finally  wolfies learning time using fracturing greater
using lics  shown figure     cpu time shown seconds 
summary  differences show utility lics method generating
candidates  thorough method result better performance  results
longer learning times  one could claim handicapping fracturing since
sampling representations fracturing  may indeed help accuracy 
learning time number candidates would likely suffer even further  domain
larger representations  differences learning time would even dramatic 
    artificial data
previous section showed wolfie successfully learns lexicons natural corpus
realistic task  however  demonstrates success relatively small corpus
one representation formalism  show algorithm scales well
lexicon items learn  ambiguity  synonymy  factors

  

fithompson   mooney

   

number candidates

   

   

   

fractwolfie
wolfie

   

   

 
 

  

   
   
training examples

   

   

figure     fracturing vs  lics  number candidates

 

   

learning time  sec 

 

   

 

   

 

   

fractwolfie
wolfie

 
 

  

   
   
training examples

   

figure     fracturing vs  lics  learning time

  

   

fiacquiring word meaning mappings

difficult control using real data input  also  large corpora available
annotated semantic parses  therefore present experimental results
artificial corpus  corpus  sentences representations completely
artificial  sentence representation variable free representation  suggested
work jackendoff        others 
corpus discussed below  random lexicon mapping words simulated meanings
first constructed    original lexicon used generate corpus random
utterances paired meaning representation  using corpus input
wolfie     learned lexicon compared original lexicon  weighted precision
weighted recall learned lexicon measured  precision measures percentage
lexicon entries  i e   word meaning pairs  system learns correct 
recall measures percentage lexicon entries hand built lexicon
correctly learned system 
precision  
recall  

  correct pairs
  pairs learned

  correct pairs
 
  pairs hand built lexicon

get weighted precision recall measures  weight results pair
words frequency entire corpus  not training corpus   models
likely learned correct meaning arbitrarily chosen word
corpus 
generated several lexicons associated corpora  varying ambiguity rate  number meanings per word  synonymy rate  number words per meaning   siskind
        meaning representations generated using set conceptual symbols
combined form meaning word  number conceptual symbols used
lexicon noted describe corpus below  lexicon       
senses variable free simulate noun like meanings        contained
one three variables denote open argument positions simulate verb like meanings 
remainder words  the remaining     empty meaning simulate function words  addition  functors meaning could depth two
arity two  example noun like meaning f   f  f      verbmeaning f   a f   b    conceptual symbols example f    f   f    f   
f    using multi level meaning representations demonstrate learning
complex representations geography database domain  none
hand built meanings phrases lexicon functors embedded arguments 
used grammar generate utterances meanings original lexicon 
terminal categories selected using distribution based zipfs law  zipf        
zipfs law  occurrence frequency word inversely proportional ranking
occurrence 
started baseline corpus generated lexicon     words using    conceptual symbols ambiguity synonymy       sentence meaning pairs generated 
    thanks jeff siskind initial corpus generation software  enhanced tests 
    tests  allowed wolfie learn phrases length two 

  

fithompson   mooney

   
  
  
  

accuracy

  
  
  
  

precision
recall

  
  
 
 

   

   

   

   
    
training examples

    

    

    

    

figure     baseline artificial corpus
split five training sets      sentences each  figure    shows weighted
precision recall curves initial test  demonstrates good scalability
slightly larger corpus lexicon u s  geography query domain 
second corpus generated second lexicon      words using    conceptual symbols  increasing ambiguity      meanings per word  time       pairs
generated corpus split five sets      training examples each  weighted
precision      examples drops       previous level        weighted
recall              full learning curve shown figure     quick comparison siskinds performance corpus confirmed system achieved comparable
performance  showing current methods  close best performance
able obtain difficult corpus  one possible explanation smaller
performance difference two systems corpus versus geography domain domain  correct meaning word necessarily
general  terms number vertices  candidate meanings  therefore 
generality portion heuristic may negatively influence performance wolfie
domain 
finally  show change performance increasing ambiguity increasing
synonymy  holding number words conceptual symbols constant  figure    shows
weighted precision recall      training examples increasing levels ambiguity  holding synonymy level constant  figure    shows results increasing
levels synonymy  holding ambiguity constant  increasing level synonymy
effect results much increasing level ambiguity  expected 
holding corpus size constant increasing number competing meanings
word increases number candidate meanings created wolfie decreasing
amount evidence available meaning  e g   first component heuristic
  

fiacquiring word meaning mappings

  

  

accuracy

  

  

  

precision
recall

  

  

 
 

   

   

   

   
    
training examples

    

    

    

    

figure     ambiguous artificial corpus
   

  

  

recall
precision

accuracy

  

  

  

  

  

  
 

    

   
number meanings per word

    

 

figure     increasing level ambiguity
measure  makes learning task difficult  hand  increasing
level synonymy potential mislead learner 
number training examples required reach certain level accuracy
informative  table    show point standard precision     first

  

fithompson   mooney

   

accuracy

  

  

recall
precision

  

  
 

    

   
number words per meaning

    

 

figure     increasing level synonymy
ambiguity level
   
    
   

number examples
   
   
    

table    number examples reach     precision
reached level ambiguity  note  however  measured accuracy
set     training examples  numbers table approximate 
performed second test scalability two corpora generated lexicons
order magnitude larger tests  tests  use lexicon
containing      words using     conceptual symbols  generated corpus
ambiguity  one lexicon ambiguity synonymy similar found
wordnet database  beckwith  fellbaum  gross    miller         ambiguity
approximately      meanings per word synonymy     words per meaning 
corpora contained       no ambiguity       examples  respectively  split
data five sets      training examples each  easier large corpus  maximum
average weighted precision recall             training examples 
harder corpus  maximum average            training examples 

   active learning
indicated previous sections  built integrated system language
acquisition flexible useful  however  major difficulty remains  construction
training corpora  though annotating sentences still arguably less work building
  

fiacquiring word meaning mappings

apply learner n bootstrap examples  creating classifier 
examples remain annotator unwilling label examples  do 
use recently learned classifier annotate unlabeled instance 
find k instances lowest annotation certainty 
annotate instances 
train learner bootstrap examples examples annotated far 

figure     selective sampling algorithm
entire system hand  annotation task time consuming error prone 
further  training pairs often contain redundant information  would minimize
amount annotation required still maintaining good generalization accuracy 
this  turned methods active learning  active learning research area
machine learning features systems automatically select informative
examples annotation training  angluin        seung  opper    sompolinsky        
rather relying benevolent teacher random sampling  primary goal
active learning reduce number examples system trained on 
maintaining accuracy acquired information  active learning systems may construct examples  request certain types examples  determine set
unsupervised examples usefully labeled  last approach  selective sampling
 cohn et al          particularly attractive natural language learning  since
abundance text  would annotate informative sentences 
many language learning tasks  annotation particularly time consuming since requires
specifying complex output rather category label  reducing number
training examples required greatly increase utility learning 
section  explore use active learning  specifically selective sampling 
lexicon acquisition  demonstrate active learning  fewer examples required
achieve accuracy obtained training randomly chosen examples 
basic algorithm selective sampling relatively simple  learning begins
small pool annotated examples large pool unannotated examples  learner
attempts choose informative additional examples annotation  existing work
area emphasized two approaches  certainty based methods  lewis   catlett 
       committee based methods  mccallum   nigam        freund  seung  shamir 
  tishby        liere   tadepalli        dagan   engelson        cohn et al         
focus former 
certainty based paradigm  system trained small number annotated
examples learn initial classifier  next  system examines unannotated examples 
attaches certainties predicted annotation examples  k examples
lowest certainties presented user annotation retraining  many
methods attaching certainties used  typically attempt estimate
probability classifier consistent prior training data classify new
example correctly 

  

fithompson   mooney

learn lexicon examples annotated far
   phrase unannotated sentence 
entries learned lexicon
certainty average heuristic values entries
else  one word phrase
certainty zero
   rank sentences use 
total certainty phrases step  
  phrases counted step  

figure     active learning wolfie
figure    presents abstract pseudocode certainty based selective sampling 
ideal situation  batch size  k  would set one make intelligent decisions
future choices  efficiency reasons retraining batch learning algorithms 
frequently set higher  results number classification tasks demonstrated
general approach effective reducing need labeled examples  see citations
above  
applying certainty based sample selection wolfie requires determining certainty
complete annotation potential new training example  despite fact individual
learned lexical entries parsing operators perform part overall annotation task 
therefore  general approach compute certainties pieces example 
case  phrases  combine obtain overall certainty example  since lexicon
entries contain explicit uncertainty parameters  used wolfies heuristic measure
estimate uncertainty 
choose sentences annotated round  first bootstrapped initial
lexicon small corpus  keeping track heuristic values learned items 
then  unannotated sentence  took average heuristic values
lexicon entries learned phrases sentence  giving value zero unknown
words eliminating consideration words assume known advance 
database constants  thus  longer sentences known phrases would
lower certainty shorter sentences number known phrases 
desirable since longer sentences informative lexicon learning point
view  sentences lowest values chosen annotation  added
bootstrap corpus  new lexicon learned  technique summarized figure    
evaluate technique  compared active learning learning randomly selected examples  measuring effectiveness learned lexicons background knowledge chill  used  smaller  u s  geography corpus  original
wolfie tests  using lexicons background knowledge parser acquisition  and
using examples parser acquisition  
trial following experiments  first randomly divide data
training test set  then  n      bootstrap examples randomly selected

  

fiacquiring word meaning mappings

   

  

accuracy

  

  
wolf active
wolfie
geobase
  

 
 

  

   
   
training examples

   

   

figure     using lexicon certainty active learning
training examples step active learning  least certain k      examples
remaining training examples selected added training set  result
learning set evaluated step  accuracy resulting learned
parsers compared accuracy learned using randomly chosen examples
learn lexicons parsers  section    words  think k examples
round chosen randomly 
figure    shows accuracy unseen data parsers learned using lexicons learned
wolfie examples chosen randomly actively  annotation
savings around    examples using active learning  maximum accuracy reached
    examples  versus     random examples  advantage using active
learning clear beginning  though differences two curves
statistically significant     training examples  since learning lexicons
parsers  choosing examples based wolfies certainty measures  boost could
improved even chill say examples chosen  see thompson  califf 
mooney        description active learning chill 

   related work
section  divide previous research related topics areas lexicon
acquisition active learning 
    lexicon acquisition
work automated lexicon language acquisition dates back siklossy        
demonstrated system learned transformation patterns logic back natural

  

fithompson   mooney

language  already noted  closely related work jeff siskind 
described briefly section   whose system ran comparisons section   
definition learning problem compared mapping problem  siskind 
       formulation differs several respects  first  sentence representations terms instead trees  however  shown figure    terms
represented trees conform formalism minor additions  next 
notion interpretation involve type tree  carries entire representation
sentence root  also  clear would handle quantified variables
representation sentences  skolemization possible  generalization across
sentences would require special handling  make single use assumption
not  another difference bias towards minimal number lexicon entries 
attempts find monosemous lexicon  later work  siskind        relaxes allow
ambiguity noise  still biases towards minimizing ambiguity  however  formal
definition explicitly allow lexical ambiguity  handles heuristic manner 
this  though  may lead robustness method face noise  finally 
definition allows phrasal lexicon entries 
siskinds work topic explored many different variations along continuum
using many constraints requiring time incorporate new example  siskind 
       versus constraints requiring training data  siskind         thus  perhaps earlier systems would able learn lexicons section  
quickly  crucially systems allow lexical ambiguity  thus may
learned accurate lexicon  detailed comparisons versions
system outside scope paper  goal wolfie learn possibly
ambiguous lexicon examples possible  thus made comparisons along
dimension alone 
siskinds approach  ours  takes account constraints word meanings
justified exclusivity compositionality assumptions  approach
somewhat general handles noise referential uncertainty  uncertainty
meaning sentence thus multiple possible candidates  
specialized applications meaning  or meanings  known  experimental
results section   demonstrate advantage method application 
demonstrated system capable learning reasonably accurate lexicons large 
ambiguous  noisy artificial corpora  accuracy assured learning
algorithm converges  occur smaller corpus experiments ran 
also  already noted  system operates incremental on line fashion  discarding
sentence processes it  batch  addition  search word meanings
proceeds two stages  discussed section      using common substructures 
combine two stages wolfie  systems greedy aspects 
choice next best lexical entry  choice discard utterances noise create
homonymous lexical entry  finally  system compute statistical correlations
words possible meanings  does 
besides siskinds work  others approach problem cognitive
perspective  example  de marcken        uses child language learning motivation  approaches segmentation problem instead learning semantics 
training input  uses flat list tokens semantic representations 
  

fiacquiring word meaning mappings

segment sentences words  uses variant expectation maximization  dempster 
laird    rubin         together form parsing dictionary matching techniques 
segment sentences associate segments likely meaning 
childes corpus  algorithm achieves high precision  recall provided 
others taking cognitive approach demonstrate language understanding ability
carry task parsing  example  nenov dyer        describe
neural network model map visual verbal motor commands  colunga
gasser        use neural network modeling techniques learning spatial concepts 
feldman colleagues berkeley  feldman  lakoff    shastri        actively
pursuing cognitive models acquisition semantic concepts  another berkeley effort 
system regier        given examples pictures paired natural language
descriptions apply picture  learns judge whether new sentence true
given picture 
similar work suppes  liang  bottner        uses robots demonstrate lexicon learning  robot trained cognitive perceptual concepts associated
actions  learns execute simple commands  along similar lines  tishby gorin
       system learns associations words actions  use
statistical framework learn associations  handle structured representations  similarly  oates  eyler walker  cohen        discuss acquisition lexical
hierarchies associated meaning defined sensory environment robot 
problem automatic construction translation lexicons  smadja  mckeown   
hatzivassiloglou        melamed        wu   xia        kumano   hirakawa        catizone  russell    warwick        gale   church        brown   et al         definition
similar own  methods compute association scores
pairs  in case  word word pairs  use greedy algorithm choose best translation s  word  take advantage constraints pairs  one
exception melamed         however  approach allow phrases lexicon synonymy within one text segment  does  also  yamazaki  pazzani 
merz        learn translation rules semantic hierarchies parsed parallel
sentences japanese english  course  main difference body
work paper map words semantic structures  words 
mentioned introduction  large body work learning lexical
semantics using different problem formulations own  example  collins
singer         riloff jones         roark charniak         schneider       
define semantic lexicons grouping words semantic categories  latter
case  add relational information  result typically applied semantic lexicon
information extraction entity tagging  pedersen chen        describe method
acquiring syntactic semantic features unknown word  assuming access
initial concept hierarchy  give experimental results  many systems  fukumoto  
tsujii        haruno        johnston  boguraev    pustejovsky        webster   marcus 
      focus acquisition verbs nouns  rather types words  also 
authors named either experimentally evaluate systems  show
usefulness learned lexicons specific application 
several authors  rooth  riezler  prescher  carroll    beil        collins        ribas 
      manning        resnik        brent        discuss acquisition subcategoriza  

fithompson   mooney

tion information verbs  others describe work learning selectional restrictions
 manning        brent         different information required
mapping semantic representation  could useful source information
constrain search  li        expands subcategorization work
inducing clustering information  finally  several systems  knight        hastings       
russell        learn new words context  assuming large initial lexicon
parsing system already available 
another related body work grammar acquisition  especially areas tightly
integrate grammar lexicon  categorial grammars  retore   bonato 
      dudau sofronie  tellier    tommasi        watkinson   manandhar        
theory categorial grammar ties lexical semantics  semantics
often used inference support high level tasks database
retrieval  learning syntax semantics together arguably difficult task 
aforementioned work evaluated large corpora  presumably primarily
due difficulty annotation 
    active learning
respect additional active learning techniques  cohn et al         among
first discuss certainty based active learning methods detail  focus neural
network approach active learning version space concepts 
researchers applying machine learning natural language processing utilized active
learning  hwa        schohn   cohn        tong   koller        thompson et al        
argamon engelson   dagan        liere   tadepalli        lewis   catlett        
majority addressed classification tasks part speech tagging
text categorization  example  liere tadepalli        apply active learning
committees problem text categorization  show improvements
active learning similar obtain  use committee winnow based
learners traditional classification task  argamon engelson dagan       
apply committee based learning part of speech tagging  work  committee
hidden markov models used select examples annotation  lewis catlett       
use heterogeneous certainty based methods  simple classifier used select
examples annotated presented powerful classifier 
however  many language learning tasks require annotating natural language text
complex output  parse tree  semantic representation  filled template 
application active learning tasks requiring complex outputs well
studied  exceptions hwa         soderland         thompson et al         
latter two include work active learning applied information extraction  thompson
et al         includes work active learning semantic parsing  hwa        describes
interesting method evaluating statistical parsers uncertainty  applied
syntactic parsing 

   future work
although wolfies current greedy search method performed quite well  better search
heuristic alternative search strategy could result improvements 
  

fiacquiring word meaning mappings

thoroughly evaluate wolfies ability learn long phrases  restricted ability
evaluations here  another issue robustness face noise  current algorithm
guaranteed learn correct lexicon even noise free corpus  addition noise
complicates analysis circumstances mistakes likely happen 
theoretical empirical analysis issues warranted 
referential uncertainty could handled  increase complexity  forming
lics pairs representations phrase appears 
alternative representations sentence  then  pair added lexicon 
sentence containing word  representations eliminated
contain learned meaning  provided another representation contain  thus allowing
lexical ambiguity   plan flesh evaluate results 
different avenue exploration apply wolfie corpus sentences paired
common query language  sql  corpora easily constructible
recording queries submitted existing sql applications along english forms 
translating existing lists sql queries english  presumably easier direction
translate   fact training data used learn semantic
lexicon parser helps limit overall burden constructing complete natural
language interface 
respect active learning  experiments additional corpora needed test
ability approach reduce annotation costs variety domains  would
interesting explore active learning natural language processing problems
syntactic parsing  word sense disambiguation  machine translation 
current results involved certainty based approach  however  proponents
committee based approaches convincing arguments theoretical advantages 
initial attempts adapting committee based approaches systems
successful  however  additional research topic indicated  one critical problem
obtaining diverse committees properly sample version space  cohn et al         

   conclusions
acquiring semantic lexicon corpus sentences labeled representations
meaning important problem widely studied  present
formalism learning problem greedy algorithm find approximate solution
it  wolfie demonstrates fairly simple  greedy  symbolic learning algorithm performs
well task obtains performance superior previous lexicon acquisition system
corpus geography queries  results demonstrate methods extend
variety natural languages besides english  scale fairly well larger 
difficult corpora 
active learning new area machine learning almost exclusively
applied classification tasks  demonstrated successful application
complex natural language mappings phrases semantic meanings  supporting
acquisition lexicons parsers  wealth unannotated natural language data 
along difficulty annotating data  make selective sampling potentially
invaluable technique natural language learning  results realistic corpora indicate
example annotations savings high     achieved employing active

  

fithompson   mooney

sample selection using simple certainty measures predictions unannotated data 
improved sample selection methods applications important language problems
hold promise continued progress using machine learning construct effective
natural language processing systems 
experiments corpus based natural language presented results
subtask natural language  results whether learned subsystems
successfully integrated build complete nlp system  experiments presented
paper demonstrated two learning systems  wolfie chill  successfully
integrated learn complete nlp system parsing database queries executable
logical form given single corpus annotated queries  demonstrated
potential active learning reduce annotation effort learning nlp 

acknowledgments
would thank jeff siskind providing us software  help
adapting use corpus  thanks agapito sustaita  esra erdem 
marshall mayberry translation efforts  three anonymous reviewers
comments helped improve paper  research supported
national science foundation grants iri         iri         

references
anderson  j  r          induction augmented transition networks  cognitive science    
       
angluin  d          queries concept learning  machine learning            
argamon engelson  s     dagan  i          committee based sample selection probabilistic classifiers  journal artificial intelligence research             
beckwith  r   fellbaum  c   gross  d     miller  g          wordnet  lexical database
organized psycholinguistic principles  zernik  u   ed    lexical acquisition 
exploiting on line resources build lexicon  pp          lawrence erlbaum 
hillsdale  nj 
borland international         turbo prolog     reference guide  borland international 
scotts valley  ca 
brent  m          automatic acquisition subcategorization frames untagged text 
proceedings   th annual meeting association computational linguistics  acl      pp         
brown  p     et al          statistical approach machine translation  computational
linguistics               
catizone  r   russell  g     warwick  s          deriving translation data bilingual
texts  proceedings first international lexical acquisition workshop 
  

fiacquiring word meaning mappings

cohn  d   atlas  l     ladner  r          improving generalization active learning 
machine learning                 
collins  m     singer  y          unsupervised models named entity classification 
proceedings conference empirical methods natural language processing
large corpora  emnlp vlc     university maryland 
collins  m  j          three generative  lexicalised models statistical parsing  proceedings   th annual meeting association computational linguistics
 acl      pp       
colunga  e     gasser  m          linguistic relativity word acquisition  computational approach  proceedings twenty first annual conference
cognitive science society  pp         
dagan  i     engelson  s  p          committee based sampling training probabilistic classifiers  proceedings twelfth international conference machine
learning  icml      pp         san francisco  ca  morgan kaufman 
de marcken  c          acquisition lexicon paired phoneme sequences
semantic representations  lecture notes computer science  vol       pp       
springer verlag 
dempster  a   laird  n     rubin  d          maximum likelihood incomplete data
via em algorithm  journal royal statistical society b          
dudau sofronie  tellier    tommasi         learning categorial grammars semantic
types  proceedings   th amsterdam colloquium  pp       
feldman  j   lakoff  g     shastri  l          neural theory language project
http   www icsi berkeley edu ntl  international computer science institute  university
california  berkeley  ca 
fillmore  c          case case  bach  e     harms  r  t   eds    universals
linguistic theory  holt  reinhart winston  new york 
fillmore  c          mechanisms construction grammar  axmaker  s   jaisser 
a     singmeister  h   eds    proceedings fourteenth annual meeting
berkeley linguistics society  pp       berkeley  ca 
fisher  d  h          knowledge acquisition via incremental conceptual clustering  machine
learning            
freund  y   seung  h  s   shamir  e     tishby  n          selective sampling using
query committee algorithm  machine learning             
fukumoto  f     tsujii  j          representation acquisition verbal polysemy 
papers      aaai symposium representation acquisition
lexical knowledge  polysemy  ambiguity  generativity  pp       stanford  ca 

  

fithompson   mooney

gale  w     church  k          identifying word correspondences parallel texts 
proceedings fourth darpa speech natural language workshop 
garey  m     johnson  d          computers intractability  guide theory
np completeness  freeman  new york  ny 
goldberg  a          constructions  construction grammar approach argument
structure  university chicago press 
grefenstette  g          sextant  extracting semantics raw text  implementation
details  integrated computer aided engineering        
haas  j     jayaraman  b          context free definite clause grammars  typetheoretic approach  journal logic programming              
haruno  m          case frame learning method japanese polysemous verbs 
papers      aaai symposium representation acquisition
lexical knowledge  polysemy  ambiguity  generativity  pp       stanford  ca 
hastings  p          implications automatic lexical acquisition mechanism 
wermter  s   riloff  e     scheler  c   eds    connectionist  statistical  symbolic approaches learning natural language processing  springer verlag  berlin 
hwa  r          minimizing training corpus parser acquisition  proceedings
fifth computational natural language learning workshop 
jackendoff  r          semantic structures  mit press  cambridge  ma 
johnston  m   boguraev  b     pustejovsky  j          acquisition interpretation
complex nominals  papers      aaai symposium representation
acquisition lexical knowledge  polysemy  ambiguity  generativity  pp 
     stanford  ca 
knight  k          learning word meanings instruction  proceedings thirteenth
national conference artificial intelligence  aaai      pp         portland  or 
kohavi  r     john  g          automatic parameter selection minimizing estimated
error  proceedings twelfth international conference machine learning
 icml      pp         tahoe city  ca 
kumano  a     hirakawa  h          building mt dictionary parallel texts based
linguistic statistical information  proceedings fifteenth international
conference computational linguistics  pp       
lavrac  n     dzeroski  s          inductive logic programming  techniques applications  ellis horwood 
lewis  d  d     catlett  j          heterogeneous uncertainty sampling supervised
learning  proceedings eleventh international conference machine learning  icml      pp         san francisco  ca  morgan kaufman 
  

fiacquiring word meaning mappings

li  h          probabilistic approach lexical semantic knowledge acquisition structural disambiguation  ph d  thesis  university tokyo 
liere  r     tadepalli  p          active learning committees text categorization 
proceedings fourteenth national conference artificial intelligence  aaai     pp         providence  ri 
manning  c  d          automatic acquisition large subcategorization dictionary
corpora  proceedings   st annual meeting association computational linguistics  acl      pp         columbus  oh 
mccallum  a  k     nigam  k          employing em pool based active learning
text classification  proceedings fifteenth international conference
machine learning  icml      pp         madison  wi  morgan kaufman 
melamed  i  d          automatic evaluation uniform filter cascades inducing n best
translation lexicons  proceedings third workshop large corpora 
melamed  i  d          models translational equivalence among words  computational
linguistics                 
muggleton  s   ed            inductive logic programming  academic press  new york 
ny 
muggleton  s          inverse entailment progol  new generation computing journal 
           
muggleton  s     feng  c          efficient induction logic programs  proceedings
first conference algorithmic learning theory tokyo  japan  ohmsha 
nenov  v  i     dyer  m  g          perceptually grounded language learning  part  
dete  neural procedural model  connection science             
oates  t   eyler walker  z     cohen  p          using syntax learn semantics 
experiment language acquisition mobile robot  tech  rep         university
massachusetts  computer science department 
partee  b   meulen  a     wall  r          mathematical methods linguistics  kluwer
academic publishers  dordrecht  netherlands 
pedersen  t     chen  w          lexical acquisition via constraint solving  papers
     aaai symposium representation acquisition lexical
knowledge  polysemy  ambiguity  generativity  pp         stanford  ca 
plotkin  g  d          note inductive generalization  meltzer  b     michie  d 
 eds    machine intelligence  vol      elsevier north holland  new york 
rayner  m   hugosson  a     hagert  g          using logic grammar learn lexicon 
tech  rep  r       swedish institute computer science 

  

fithompson   mooney

regier  t          human semantic potential  spatial language constrained connectionism  mit press 
resnik  p          selection information  class based approach lexical relationships 
ph d  thesis  university pennsylvania  cis department 
retore  c     bonato  r          learning rigid lambek grammars minimalist grammars
structured sentences  proceedings third learning language logic
workshop strasbourg  france 
ribas  f          experiment learning appropriate selectional restrictions
parsed corpus  proceedings fifteenth international conference computational linguistics  pp         
riloff  e     jones  r          learning dictionaries information extraction multilevel bootstrapping  proceedings sixteenth national conference artificial
intelligence  aaai      pp           orlando  fl 
roark  b     charniak  e          noun phrase co occurrence statistics semi automatic
semantic lexicon construction  proceedings   th annual meeting
association computational linguistics coling     acl coling      pp 
         
rooth  m   riezler  s   prescher  d   carroll  g     beil  f          inducing semantically
annotated lexicon via em based clustering  proceedings   th annual meeting
association computational linguistics  pp         
russell  d          language acquisition unification based grammar processing system using real world knowledge base  ph d  thesis  university illinois  urbana 
il 
schank  r  c          conceptual information processing  north holland  oxford 
schneider  r          lexically intensive algorithm domain specific knowledge acquisition  proceedings joint conference new methods language processing
computational natural language learning  pp       
schohn  g     cohn  d          less more  active learning support vector machines  proceedings seventeenth international conference machine learning  icml        pp         stanford  ca 
sebillot  p   bouillon  p     fabre  c          inductive logic programming corpus based
acquisition semantic lexicons  proceedings  nd learning language logic
 lll  workshop lisbon  portugal 
seung  h  s   opper  m     sompolinsky  h          query committee  proceedings
acm workshop computational learning theory pittsburgh  pa 
siklossy  l          natural language learning computer  simon  h  a     siklossy 
l   eds    representation meaning  experiments information processsing
systems  prentice hall  englewood cliffs  nj 
  

fiacquiring word meaning mappings

siskind  j  m          learning word to meaning mappings  broeder  p     murre  j 
 eds    models language acquisition  inductive deductive approaches  oxford
university press 
siskind  j  m          naive physics  event perception  lexical semantics language
acquisition  ph d  thesis  department electrical engineering computer science  massachusetts institute technology  cambridge  ma 
siskind  j  m          computational study cross situational techniques learning
word to meaning mappings  cognition               
siskind  j  m          lexical acquisition constraint satisfaction  tech  rep  ircs       
university pennsylvania 
smadja  f   mckeown  k  r     hatzivassiloglou  v          translating collocations
bilingual lexicons  statistical approach  computational linguistics              
soderland  s          learning information extraction rules semi structured free
text  machine learning             
suppes  p   liang  l     bottner  m          complexity issues robotic machine learning
natural language  lam  l     naroditsky  v   eds    modeling complex phenomena  proceedings  rd woodward conference  pp          springer verlag 
thompson  c  a   califf  m  e     mooney  r  j          active learning natural language
parsing information extraction  proceedings sixteenth international
conference machine learning  icml      pp         bled  slovenia 
thompson  c  a          acquisition lexicon semantic representations sentences 
proceedings   rd annual meeting association computational linguistics  acl      pp         cambridge  ma 
tishby  n     gorin  a          algebraic learning statistical associations language
acquisition  computer speech language          
tomita  m          efficient parsing natural language  kluwer academic publishers 
boston 
tong  s     koller  d          support vector machine active learning applications
text classification  proceedings seventeenth international conference
machine learning  icml        pp          stanford  ca 
watkinson  s     manandhar  s          unsupervised lexical learning categorial
grammars using lll corpus  learning language logic  lll  workshop bled 
slovenia 
webster  m     marcus  m          automatic acquisition lexical semantics verbs
sentence frames  proceedings   th annual meeting association
computational linguistics  acl      pp         

  

fithompson   mooney

wu  d     xia  x          large scale automatic extraction english chinese translation lexicon  machine translation                  
yamazaki  t   pazzani  m     merz  c          learning hierarchies ambiguous natural
language data  proceedings twelfth international conference machine
learning  icml      pp         san francisco  ca  morgan kaufmann 
zelle  j  m          using inductive logic programming automate construction
natural language parsers  ph d  thesis  department computer sciences  university texas  austin  tx  appears artificial intelligence laboratory technical
report ai        
zelle  j  m     mooney  r  j          learning parse database queries using inductive
logic programming  proceedings thirteenth national conference artificial
intelligence  aaai      pp           portland  or 
zipf  g          human behavior principle least effort  addison wesley  new
york  ny 

  


