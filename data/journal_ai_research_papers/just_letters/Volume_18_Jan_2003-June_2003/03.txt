journal of artificial intelligence research                  

submitted        published     

acquiring correct knowledge
for natural language generation
ehud reiter
somayajulu g  sripada

ereiter csd abdn ac uk
ssripada csd abdn ac uk

department of computing science 
university of aberdeen  aberdeen ab    ue  uk

roma robertson

roma robertson ed ac uk

division of community health sciences   general practice section
university of edinburgh
edinburgh eh   dx  uk

abstract
natural language generation  nlg  systems are computer software systems that produce texts in english and other human languages  often from non linguistic input data 
nlg systems  like most ai systems  need substantial amounts of knowledge  however  our
experience in two nlg projects suggests that it is difficult to acquire correct knowledge
for nlg systems  indeed  every knowledge acquisition  ka  technique we tried had significant problems  in general terms  these problems were due to the complexity  novelty 
and poorly understood nature of the tasks our systems attempted  and were worsened by
the fact that people write so differently  this meant in particular that corpus based ka
approaches suffered because it was impossible to assemble a sizable corpus of high quality
consistent manually written texts in our domains  and structured expert oriented ka techniques suffered because experts disagreed and because we could not get enough information
about special and unusual cases to build robust systems  we believe that such problems
are likely to affect many other nlg systems as well  in the long term  we hope that new
ka techniques may emerge to help nlg system builders  in the shorter term  we believe
that understanding how individual ka techniques can fail  and using a mixture of different
ka techniques with different strengths and weaknesses  can help developers acquire nlg
knowledge that is mostly correct 

   introduction
natural language generation  nlg  systems use artificial intelligence  ai  and natural language processing techniques to automatically generate texts in english and other human
languages  typically from some non linguistic input data  reiter   dale         as with
most ai systems  an essential part of building an nlg system is knowledge acquisition  ka  
that is acquiring relevant knowledge about the domain  the users  the language used in the
texts  and so forth 
ka for nlg can be based on structured expert oriented techniques  such as think aloud
protocols and sorting  or on machine learning and corpus analysis  which are currently very
popular in other areas of natural language processing  we have used both types of techniques in two nlg projects that included significant ka efforts  stop  reiter  robertson   
osman         which generated tailored smoking cessation letters  and sumtime mousam
c
    
ai access foundation and morgan kaufmann publishers  all rights reserved 

fireiter  sripada    robertson

 sripada  reiter  hunter  yu    davy         which generated weather forecasts  in both
projects  and for all techniques tried  the main problem turned out to be knowledge quality 
evaluation and validation exercises identified flaws in the knowledge acquired using every
technique  the flaws were due to a variety of factors  but perhaps the basic underlying
reason for them was the nature of the writing tasks we were attempting to automate  they
were 
 complex  as are many tasks that involve interacting with humans   hence a lot of
knowledge was needed to cover the numerous special cases and unusual circumstances 
 sometimes novel  not done by humans   hence sometimes there were no experts at
the task as a whole  and no existing corpora of texts to analyse 
 poorly understood  hence we did not have good theoretical models to structure the
knowledge being acquired  and fill in gaps in the knowledge acquired from experts or
corpora  and
 ambiguous  allowed multiple solutions   hence different experts and corpus authors
produced very different texts  solutions  from the same input data 
these problems of course occur to some degree in ka for other expert system and natural
language processing tasks  but we believe they may be especially severe for nlg 
we do not have a good solution for these problems  and indeed believe that ka is one
of the biggest problems in applied nlg  after all  there is no point in using ai techniques
to build a text generation system if we cannot acquire the knowledge needed by the ai
techniques 
in the longer term  more basic research into ka for nlg is badly needed  in the shorter
term  however  we believe that developers are more likely to acquire correct knowledge
when building an nlg system if they understand likely types of errors in the knowledge
acquired from different ka techniques  also  to some degree the different ka techniques we
have tried have complementary strengths and weaknesses  this suggests using a variety of
different techniques  so that the weaknesses of one technique are compensated for by the
strengths of other techniques 
in the remainder of this paper we give background information on nlg  ka  and our
systems  describe the various ka techniques we used to build our systems and the problems
we encountered  and then discuss more generally why ka for nlg is difficult and how
different ka techniques can be combined 

   background
in this section we give some background information on natural language generation and
knowledge acquistion and validation  we also introduce and briefly describe the stop and
sumtime mousam systems 

   

fiacquiring correct knowledge for nlg

    natural language generation
natural language generation is the subfield of artificial intelligence that is concerned with
automatically generating written texts in human languages  often from non linguistic input
data  nlg systems often have three stages  reiter   dale        
 document planning decides on the content and structure of the generated text  for
example that a smoking cessation letter should start with a section that discusses the
pros and cons of smoking 
 microplanning decides on how information and structure should be expressed linguistically  for example  that the phrase by mid afternoon should be used in a weather
report to refer to the time      
 surface realisation generates an actual text according to the decisions made in previous stages  ensuring that the text conforms to the grammar of the target language
 english in our systems  
nlg systems require many types of knowledge in order to carry out these tasks  in particular  kittredge  korelsky  and rambow        point out that nlg systems need domain
knowledge  similar to that needed by expert systems   communication knowledge  similar
to that needed by other natural language processing systems   and also domain communication knowledge  dck   dck is knowledge about how information in a domain is usually
communicated  including standard document structures  sublanguage grammars  and specialised lexicons  dck plays a role in all aspects of language technology  for example  a
speech recogniser will work better in a given domain if it is trained on a corpus of texts
from that domain   but it may be especially important in nlg 
    knowledge acquisition and validation
knowledge acquisition is the subfield of artificial intelligence that is concerned with acquiring the knowledge needed to build ai systems  broadly speaking the two most common
types of ka techniques are 
 techniques based on working with experts in a structured fashion  such as structured interviews  think aloud protocols  sorting  and laddered grids  scott  clayton 
  gibson        buchanan   wilkins         and
 techniques based on learning from data sets of correct solutions  such as text corpora  
these are currently very popular in natural language processing and used for many
different types of knowledge  ranging from grammar rules to discourse models  for an
overview  see jurafsky   martin        
there are of course other possible ka techniques as well  including directly asking experts
for knowledge  and conducting scientific experiments  some research has been done on
evaluating and comparing ka techniques  but such research can be difficult to interpret
because of methodological problems  shadbolt  ohara    crow        
research has also been done on verifying and validating knowledge to check that it
is correct  adelman   riedel         verification techniques focus on detecting logical
   

fireiter  sripada    robertson

anomalies and inconsistencies that often reflect mistakes in the elicitation or coding process 
we will not further discuss these  as such errors are not our primary concern in this paper 
validation techniques focus on detecting whether the knowledge acquired is indeed correct
and will enable the construction of a good system  these are very relevant to efforts to
detect problems in knowledge acquired for nlg  adelman and riedel        describe two
general types of validation techniques      having experts check the acquired knowledge and
built systems  and     using a library of test cases with known inputs and outputs  in other
words  just as knowledge can be acquired from experts or from data sets of correct solutions 
knowledge can also be validated by experts or by data sets of correct solutions  knowledge
can also be validated experimentally  by determining if the system as a whole works and has
the intended effect on its users  of course care must be taken that the validation process
uses different resources than the acquisition process  for example  knowledge acquired from
an expert should not be validated by that expert  and knowledge learned from a data set
should not be validated by that data set 
there has not been a great deal of previous research on knowledge acquisition for nlg 
reiter  robertson  and osman        summarise previous efforts in this area  generally
corpus analysis  analysis of collections of manually written texts  has been the most popular
ka technique for nlg  as in other areas of natural language processing  although sometimes
it is supplemented by expert oriented techniques  goldberg  driedger    kittredge       
mckeown  kukich    shaw         walker  rambow  and rogati        have attempted
to learn nlg rules from user ratings of generated texts  which can perhaps be considered a
type of experiment based ka 
    stop
stop  reiter  robertson    osman        is an nlg system that generates tailored smokingcessation letters  tailoring is based on a   page multiple choice questionnaire about the
smokers habits  health  concerns  and so forth  an extract from a questionnaire is shown
in figure    and an extract from the stop letter generated from this questionnaire is shown
in figure    we have changed the name of the smoker to preserve confidentiality   from a
ka perspective  the most important knowledge needed in stop is what content and phrasing
is appropriate for an individual smoker  for example 
 what information should be given in a letter  the example letter in figure    for
instance  emphasises things the smoker dislikes about smoking  confidence building 
and dealing with stress and weight gain  but it does not recommend specific techniques
for stopping smoking 
 should a letter adopt a positive youll feel better if you stop tone  as done in the
letter in figure     or should it adopt a negative smoking is killing you tone 
stop was never operationally deployed  but it was tested with real smokers in a clinical
trial  during which     smokers received stop letters  lennox  osman  reiter  robertson 
friend  mccann  skatun    donnan         this evaluation  incidentally  showed that stop
letters were no more effective than control non tailored letters 

   

fiacquiring correct knowledge for nlg

smoking questionnaire

please answer by marking the most appropriate box for each question like this   

q  have you smoked a cigarette in the last week  even a puff 
yes  
please complete the following questions

please read the questions carefully 
q 

home situation 
live
 
alone



no

please return the questionnaire unanswered in the
envelope provided  thank you 
if you are not sure how to answer  just give the best answer you can 

live with

husband wife partner



live with
other adults

  boys

   girls

q 

number of children under    living at home

q 

does anyone else in your household smoke   if so  please mark all boxes which apply 
husband wife partner 
other family member 
others 

q 



live with
children

how long have you smoked for     years
tick here if you have smoked for less than a year


q 

how many cigarettes do you smoke in a day   please mark the amount below 

less than   
q 

      

        

       

        

   or more

how soon after you wake up do you smoke your first cigarette   please mark the time below 

within   minutes 

       minutes  

        minutes 

q 

do you find it difficult not to smoke in places where it is
forbidden eg in church  at the library  in the cinema 

q 

which cigarette would you hate most to give up 

q  

do you smoke more frequently during the first hours after
waking than during the rest of the day 

q   do you smoke if you are so ill that you are in bed most of the
day 
q  
are you intending to stop
smoking in the next  
months 

yes



no

 

after    minutes 
yes

 no 

the first one in the morning  
any of the others 
yes

no

 

yes

no

 

q   if yes  are you intending to stop smoking
within the next month 
yes no 
q   if no  would you like to stop smoking if it was
easy 
yes not sure  
no 




 

figure    first page of example smoker questionnaire

   

fismoking information for heather stewart
we know that all of these make it more likely that you will be able to stop 
most people who stop smoking for good have more than one attempt 

people stop smoking when they really want to stop  it is encouraging that
you have many good reasons for stopping  the scales show the good
and bad things about smoking for you  they are tipped in your favour 

overcoming your barriers to stopping   

things you like

it s relaxing
it stops stress
you enjoy it
it relieves boredom
it stops weight gain
it stops you craving

things you dislike
it makes you less fit
it s a bad example for kids
you re addicted
it s unpleasant for others
other people disapprove
it s a smelly habit
it s bad for you
it s expensive
it s bad for others  health

you could do it   
most people who really want to stop eventually succeed  in fact    
million people in britain have stopped smoking   and stayed stopped   in
the last    years  many of them found it much easier than they expected 
although you don t feel confident that you would be able to stop if you
were to try  you have several things in your favour 




you have stopped before for more than a month 
you have good reasons for stopping smoking 
you expect support from your family  your friends  and your
workmates 

you said in your questionnaire that you might find it difficult to stop
because smoking helps you cope with stress  many people think that
cigarettes help them cope with stress  however  taking a cigarette only
makes you feel better for a short while  most ex smokers feel calmer and
more in control than they did when they were smoking  there are some
ideas about coping with stress on the back page of this leaflet 
you also said that you might find it difficult to stop because you would put
on weight  a few people do put on some weight  if you did stop smoking 
your appetite would improve and you would taste your food much better 
because of this it would be wise to plan in advance so that you re not
reaching for the biscuit tin all the time  remember that putting on weight
is an overeating problem  not a no smoking one  you can tackle it later
with diet and exercise 

and finally   
we hope this letter will help you feel more confident about giving up
cigarettes  if you have a go  you have a real chance of succeeding 
with best wishes 
the health centre 

reiter  sripada    robertson

   

figure    extract from letter generated from figure   questionnaire

you have good reasons to stop   

fiacquiring correct knowledge for nlg

day

hour

        
        
        
        
        
        
        

 
 
  
  
  
  
 

wind
direction
wsw
wsw
wsw
wsw
sw
ssw
ssw

wind speed
   m altitude 
  
 
 
 
 
 
  

wind speed
   m alt 
  
  
 
 
 
  
  

figure    wind data extract from    jun      numerical weather prediction
knowledge acquisition in stop was primarily based on structured expert oriented ka
techniques  including in particular sorting and think aloud protocols  knowledge was acquired from five health professionals  three doctors  a nurse  and a health psychologist 
these experts were knowledgeable about smoking and about patient information  but they
were not experts on writing tailored smoking cessation letters  in fact there are no experts
at this task  since no one manually writes tailored smoking cessation letters 
it is not unusual for an nlg system to attempt a task which is not currently performed by
human experts  other examples include descriptions of software models  lavoie  rambow 
  reiter         customised descriptions of museum items  oberlander  odonnell  knott 
  mellish         and written feedback for adult literacy students  williams  reiter   
osman         knowledge validation in stop was mostly based on feedback from users
 smokers   and on the results of the clinical trial 
    sumtime mousam
sumtime mousam  sripada  reiter  hunter    yu        is an nlg system that generates
marine weather forecasts for offshore oil rigs  from numerical weather simulation data  an
extract from sumtime mousams input data is shown in figure    and an extract from the
forecast generated from this data is shown in figure    from a ka perspective  the main
knowledge needed by sumtime mousam was again what content and expression was best
for users  for example 
 what changes in a meteorological parameter are significant enough to be reported in
the text  the forecast in figure    for example  mentions changes in wind direction
but not changes in wind speed 
 what words and phrases should be used to communicate time  for example  should
     be described as early evening  as in figure    or as late afternoon 
sumtime mousam is currently being used operationally by a meteorological company  to
generate draft forecasts which are post edited by human forecasters 
knowledge acquisition in sumtime mousam was based on both corpus analysis of
manually written forecasts and structured ka with expert meteorologists  unlike the experts we worked with in stop  the meteorologists we worked with in sumtime mousam

   

fireiter  sripada    robertson

forecast        gmt  wed    jun     
wind kts 
  m  wsw      gradually backing sw by early evening and ssw by
midnight 
  m  wsw       gradually backing sw by early evening and ssw by
midnight 
waves m 
sig ht          mainly sw swell 
max ht          mainly sw swell 
per sec 
wave period  wind wave     mainly   second sw swell 
windwave period      
swell period      
weather 
partly cloudy becoming overcast with light rain around midnight 
vis nm  
greater than    reduced to     in precipitation 
air temp c        rising      around midnight 
cloud oktas ft       cu sc           lowering     st sc         around
midnight 
figure    extract from forecast generated for    jun     
were experienced at writing the target texts  weather forecasts   the forecast corpus included the numerical weather simulation data that the forecasters used when writing the
forecasts  as well as the actual forecast texts  sripada  reiter  hunter    yu        
knowledge validation in sumtime mousam has mostly been conducted by checking
knowledge acquired from the corpus with the experts  and checking knowledge acquired
from the experts against the corpus  in other words  we have tried to make the validation
technique as different as possible from the acquisition technique  we are currently evaluating
sumtime mousam as a system by measuring the number of edits that forecasters make
to the computer generated draft forecasts 

   knowledge acquisition techniques tried
in this section we summarise the main ka techniques we used in stop and sumtimemousam  for each technique we give an example of the knowledge acquired  and discuss
what we learned when we tried to validate the knowledge  table   gives a very high level
overview of the major advantages and disadvantages of the different techniques we tried 
when the different techniques were perhaps most useful  and what types of knowledge they
were best suited to acquiring  using the classification of section       as this table shows 
no one technique is clearly best  they all have different strengths and weaknesses  probably
the best overall ka strategy is to use a mix of different techniques  we will further discuss
this in section   

   

fiacquiring correct knowledge for nlg

techniques

advantages

disadvantages

directly ask
experts
structured ka
with experts
corpus
analysis

get big picture

many gaps  may
not match practice
limited coverage 
experts variable
hard to create 
texts inconsistent 
poor models for nlg
local optimisation 
not major changes

expert
revision

get details 
get rationale
get lots of
knowledge
quickly
fix problems
in knowledge

when
useful
initial
prototype
flesh out
prototype
robustness 
unusual cases

types of
knowledge
domain 
dck
depends
on expert
dck 
communication

improve
system

all

table    summary evaluation of ka techniques for nlg
    directly asking experts for knowledge
the simplest and perhaps most obvious ka technique for nlg is to simply ask experts how
to write the texts in question  in both stop and sumtime mousam  experts initially
gave us spreadsheets or flowcharts describing how they thought texts should be generated 
in both projects  it also turned out that the experts description of how texts should be
generated did not in fact match how people actually wrote the texts in question  this is a
common finding in ka  and it is partially due to the fact that it is difficult for experts to
introspectively examine the knowledge they use in practice  anderson         this is why
proponents of expert oriented ka prefer structured ka techniques 
for example  at the beginning of sumtime mousam  one of the meteorologists gave
us a spreadsheet which he had designed  which essentially encoded how he thought some
parts of weather forecasts should be generated  the spreadsheet did not generate a complete
weather forecast   we analysed the logic used in the spreadsheet  and largely based the first
version of sumtime mousam on this logic 
one goal of our analysis was to create an algorithm that could decide when a change
in a parameter value was significant enough so that it should be mentioned in the weather
report  the spreadsheet used context dependent change thresholds to make this decision 
for example  a change in the wind speed would be mentioned if
 the change was    knots or more  and the final wind speed was    knots or less 
 the change was   knots or more  and the final wind speed was between    and   
knots  or
 the change was    knots or more  and the final wind speed was over    knots 
the context dependent thresholds reflect the usage of the weather reports by the users  in
this case  oil company staff making decisions related to north sea offshore oil rigs   for
example  if a user is deciding how to unload a supply boat  moderate changes in wind
speed dont matter at low speeds  because light winds have minimal impact on supply
boat operations  and at high speeds  because the boat wont even attempt to unload in
very heavy winds   but may affect decisions at in between speeds  the context dependent
   

fireiter  sripada    robertson

thresholds would be expected to vary according to the specific forecast recipient  and should
be set in consultation with the recipient 
from our perspective  there were two main pieces of knowledge encoded in this algorithm 
   the absolute size of a change determines whether it should be mentioned or not  and
   the threshold for significance depends on the context and ultimately on how the user
will use the information 
      validation of direct expert knowledge
we checked these rules by comparing them to what we observed in our corpus analysis of
manually written forecasts  section       this suggested that while     above is probably
correct      may be incorrect  in particular  a linear segmentation model  sripada et al  
       which basically looks at changes in slope rather than changes in the absolute value of a
parameter  better matches the corpus texts  the expert who designed the spreadsheet model
agreed that segmentation was probably a better approach  he also essentially commented
that one reason for his use of the absolute size model was that this was something that was
easily comprehensible to someone who was neither a programmer nor an expert at numerical
data analysis techniques 
in other words  in addition to problems in introspecting knowledge  it also perhaps is
not reasonable to expect a domain expert to be able to write a sophisticated data analysis
algorithm based on his expertise  this is not an issue if the knowledge needed is purely
declarative  as it is in many ai applications  but if we need procedural or algorithmic
knowledge  we must bear in mind that domain experts may not have sufficient computational
expertise to express their knowledge as a computer algorithm 
      role of directly asking experts for knowledge
although the experts spreadsheet in sumtime mousam was far from ideal  it was extremely useful as a starting point  it specified an initial system which we could build fairly
easily  and which produced at least vaguely plausible output  much the same in fact happened in stop  when one of the doctors gave us a flowchart which certainly had many
weaknesses  but which was useful as an initial specification of a relatively easy to build and
somewhat plausible system  in both stop and sumtime mousam  as indeed in other nlg
projects we have been involved in  having an initial prototype system working as soon as
possible was very useful for developing our ideas and for explaining to domain experts and
other interested parties what we were trying to do 
in terms of the types of knowledge mentioned in section      both the stop flowchart
and the sumtime mousam spreadsheet specified domain knowledge  for example  how
smokers should be categorised  and domain communication knowledge  for example  the use
of ranges instead of single numbers to communicate wind speed   the stop flowchart did not
specify any generic communication knowledge such as english grammar and morphology 
the author probably believed we knew more about such things than he did  the sumtimemousam spreadsheet did in effect include a few english grammar rules  but these were just
to get the spreadsheet to work  the author did not have much confidence in them 
   

fiacquiring correct knowledge for nlg

in summary  we think directly asking experts for knowledge is an excellent way to
quickly build an initial system  especially if the nlg developers can supply communication
knowledge that the domain expert may not possess  but once the initial system is in place 
it is probably best to use other ka techniques  at least in poorly understood areas such as
nlg  however  in applications where there is a solid theoretical basis  and the expert can
simply say build your system according to theory x  an experts direct knowledge may
perhaps be all that is needed 
    structured expert oriented ka  think aloud protocols
there are numerous types of structured expert oriented ka techniques  including thinkaloud protocols  sorting  and structured interviews  scott et al          we will focus here
on think aloud protocols  which is the technique we have used the most  we have tried
other structured ka techniques as well  such as sorting  reiter et al          we will not
describe these here  but our broad conclusions about other structured ka techniques were
similar to our conclusions about think aloud protocols 
in a think aloud protocol  an expert carries out the task in question  in our case  writing
a text  while thinking aloud into an audio  or video  recorder  we used think aloud
protocols in both stop and sumtime mousam  they were especially important in stop 
where they provided the basis for most content and phrasing rules 
a simple example of the think aloud process is as follows  one of the doctors wrote a
letter for a smoker who had tried to stop before  and managed to stop for several weeks before
starting again  the doctor made the following comments in the think aloud transcript 
has he tried to stop smoking before  yes  and the longest he has managed
to stop  he has ticked the one week right up to three months and thats
encouraging in that he has managed to stop at least once before  because it is
always said that the people who have had one or two goes are more likely to
succeed in the future 
he also included the following paragraph in the letter that he wrote for this smoker 
i see that you managed to stop smoking on one or two occasions before but have
gone back to smoking  but you will be glad to know that this is very common
and most people who finally stop smoking have had one or two attempts in the
past before they finally succeed  what it does show is that you are capable of
stopping even for a short period  and that means you are much more likely to be
able to stop permanently than somebody who has never ever stopped smoking
at all 
after analysing this session  we proposed two rules 
 if  previous attempt to stop  then  message  more likely to succeed 
 if  previous attempt to stop  then  message  most people who stop have a few
unsuccessful attempts first 

   

fireiter  sripada    robertson

the final system incorporated a rule  based on several ka sessions  not just the above
one  that stated that if the smoker had tried to stop before  and if the letter included a
section on confidence building  then the confidence building section should include a short
message about previous attempts to stop  if the smoker had managed to quit for more than
one week  this should be mentioned in the message  otherwise the message should mention
the recency of the smokers previous cessation attempt if this was within the past   months 
the actual text generated from this rule in the example letter of figure   is
although you dont feel confident that you would be able to stop if you were to
try  you have several things in your favour 
 you have stopped before for more than a month 
note that the text produced by the actual stop code is considerably simpler than the
text originally written by the expert  this is fairly common  as are simplifications in the
logic used to decide whether to include a message in a letter or not  in many cases this is
due to the expert having much more knowledge and expertise than the computer system
 reiter   dale        pp        in general  the process of deriving implementable rules
for nlg systems from think aloud protocols is perhaps more of an art than a science  not
least because different experts often write texts in very different ways 
      validation of structured ka knowledge
we attempted to verify some of the rules acquired from stop think aloud sessions by performing a series of small experiments where we asked smokers to comment on a letter  or to
compare two versions of a letter  many of the rules were supported by these experiments for example  people in general liked the recap of smoking likes and dislikes  see you have
good reasons to stop      section of figure     however  one general negative finding
of these experiments was that the tailoring rules were insufficiently sensitive to unusual or
atypical aspects of individual smokers  and most smokers were probably unusual or atypical
in some way  for example  stop letters did not go into the medical details of smoking  as
none of the think aloud expert written letters contained such information   and while this
seemed like the right choice for many smokers  a few smokers did say that they would have
liked to see more medical information about smoking  another example is that  again based
on the think aloud sessions  we adopted a positive tone and did not try to scare smokers 
and again this seemed right for most smokers  but some smokers said that a more brutal
approach would be more effective for them 
the fact that our experts did not tailor letters in such ways may possibly reflect the
fact that such tailoring would not have been appropriate for the relatively small number of
specific cases they considered in our think aloud sessions  we had    think aloud sessions
with experts  who looked at    different smoker questionnaires    questionnaires were considered by two experts   this may sound like a lot  but it is a drop in the ocean when we
consider how tremendously variable people are 
comments made by smokers during the stop clinical trial  reiter  robertson    osman        also revealed some problems with think aloud derived rules  for example  we
decided not to include practical how to stop information in letters for people not currently intending to stop smoking  smoker comments suggest that this was a mistake  in
   

fiacquiring correct knowledge for nlg

fact  some experts did include such information in think aloud letters for such people  and
some did not  our decision not to include this information was influenced by the stages
of change theoretical model  prochaska   diclemente        of behaviour change  which
states that how to stop advice is inappropriate for people not currently intending to stop 
in retrospect  this decision was probably a mistake 
we repeated two of our think aloud exercises    months after we originally performed
them  that is  we went back to one of our experts and gave him two questionnaires he had
analysed    months earlier  and asked him to once again think aloud while writing letters
based on the questionnaires  the letters that the expert wrote in the second session were
somewhat different from the ones he had originally written  and were preferred by smokers
over the letters he had originally written  reiter et al          this suggests that our experts
were not static knowledge sources  but were themselves learning about the task of writing
tailored smoking cessation letters during the course of the project  perhaps this should not
be a surprise given that none of the experts had ever attempted to write such letters before
getting involved with our project 
      role of structured expert oriented ka
structured expert oriented ka was certainly a useful way to expand  refine  and generally
improve initial prototypes constructed on the basis of experts direct knowledge  by focusing
on actual cases and by structuring the ka process  we learned many things which the
experts did not mention directly  we obtained all the types of knowledge mentioned in
section      by working with experts with the relevant expertise  for example in stop we
acquired domain knowledge  such as the medical effects of smoking  from doctors  domain
communication knowledge  such as which words to use  from a psychologist with expertise
in writing patient information leaflets  and communication knowledge about graphic design
and layout from a graphic designer 
however  structured expert oriented ka did have some problems  including in particular coverage and variability  as mentioned above     sessions that examined    smoker
questionnaires could not possibly give good coverage of the population of smokers  given
how complex and variable people are  as for variation  the fact that different experts wrote
texts in very different ways made it difficult to extract rules from the think aloud protocols 
we undoubtably made some mistakes in this regard  such as not giving how to stop information to people not currently intending to stop smoking  perhaps we should have focused
on a single expert in order to reduce variation  however  our experiences suggested that
different experts were better at different types of information  and also that experts changed
over time  so we might see substantial variation even in texts from a single author   these
observations raise doubts about the wisdom and usefulness of a single expert strategy 
in short  the complexity of nlg tasks means that a very large number of structured ka
sessions may be needed to get good coverage  and the fact that there are numerous ways to
write texts to fulfill a communicative goal means that different experts tend to write very
differently  which makes analysis of structured ka sessions difficult 

   

fireiter  sripada    robertson

    corpus analysis
in recent years there has been great interest in natural language processing and other areas
of ai in using machine learning techniques to acquire knowledge from relevant data sets  for
example  instead of building a medical diagnosis system by trying to understand how expert
doctors diagnose diseases  we can instead analyse data sets of observed symptoms and actual
diseases  and use statistical and machine learning techniques to determine which symptoms
predict which disease  similarly  instead of building an english grammar by working with
expert linguists  we can instead analyse large collections of grammatical english texts in
order to learn the allowable structures  grammar  of such texts  such collections of texts
are called corpora in natural language processing 
there has been growing interest in applying such techniques to learn the knowledge
needed for nlg  for example  barzilay and mckeown        used corpus based machine
learning to learn paraphrase possibilities  duboue and mckeown        used corpus based
machine learning to learn how np constituents should be ordered  and hardt and rambow
       used corpus based machine learning to learn rules for vp ellipsis 
some nlg researchers  such as mckeown et al          have used the term corpus
analysis to refer to the manual analysis  without using machine learning techniques  of a
small set of texts which are written explicitly for the nlg project by domain experts  and
hence are not naturally occurring   this is certainly a valid and valuable ka technique 
but we regard it as a form of structured expert oriented ka  in some ways similar to thinkaloud protocols  in this paper  corpus analysis refers to the use of machine learning and
statistical techniques to analyse collections of naturally occurring texts 
corpus analysis in our sense of the word was not possible in stop because we did not have
a collection of naturally occurring texts  since doctors do not currently write personalised
smoking cessation letters   we briefly considered analysing the example letters produced
in the think aloud sessions with machine learning techniques  but we only had    such
texts  and we believed this would be too few for successful learning  especially given the
high variability between experts  in other words  perhaps the primary strength of corpus
analysis is its ability to extract information from large data sets  but if there are no large
data sets to extract information from  then corpus analysis loses much of its value 
in sumtime mousam  we were able to acquire and analyse a substantial corpus of     
human written weather forecasts  along with the data files that the forecasters looked at
when writing the forecasts  sripada et al          details of our corpus analysis procedures
and results have been presented elsewhere  reiter   sripada      a  sripada et al         
and will not be repeated here 
      validation of corpus analysis knowledge
while many of the rules we acquired from corpus analysis were valid  some rules were
problematical  primarily due to two factors  individual variations between the writers  and
writers making choices that were appropriate for humans but not for nlg systems 
a simple example of individual variation and the problems it causes is as follows  one of
the first things we attempted to learn from the corpus was how to express numbers in wind
statements  we initially did this by searching for the most common textual realisation of
each number  this resulted in rules that said that   should be expressed as    but   should
   

fiacquiring correct knowledge for nlg

form
 
  
 
  

f 
 
 
 
 

f 
 
 
  
 

f 
 
 
 
   

f 
 
  
 
   

f 
   
 
  
 

unknown
 
 
 
  

total
   
  
   
   

table    usage of              in wind statements  by forecaster
be expressed as     now it is probably acceptable for a forecast to always include leading
zeros for single digits  that is  use    and      and to never include leading zeros  that is 
use   and     however  it is probably not acceptable to mix the two  that is  use   and   
in the same forecast   which is what our rules would have led to 
the usage of           and    by each individual forecaster is shown in table    as
this table suggests  each individual forecaster is consistent  forecasters f  and f  always
include leading zeros  while forecasters f  and f  never include leading zeros  f  in fact
is also consistent and always omits leading zeros  for example he uses   instead of     the
reason that the overall statistics favour   over    but    over   is that individuals also differ
in which descriptions of wind speed they prefer to use  for example  f  never explicitly
mentions low wind speeds such as   or   knots  and instead always uses generic phrases such
as    or less  f   f   and f  use a mix of generic phrases and explicit numbers for low
wind speeds  and f  always uses explicit numbers and never uses generic phrases  some of
the forecasters  especially f   also have a strong preference for even numbers  this means
that the statistics for   vs     are dominated by f   the only forecaster who both explicitly
mentions low wind speeds and does not prefer even numbers   while the statistics for   vs 
   are dominated by f   who uses this number a lot because he avoids both generic phrases
and odd numbers   hence the somewhat odd result that the corpus overall favours   over
   but    over   
this example is by no means unique  reiter and sripada      b  explain how a more
complex analysis using this corpus  whose goal was to determine the most common time
phrase for each time  similarly led to unacceptable rules  again largely because of individual
differences between the forecasters 
there are obvious methods to deal with the problems caused by individual variation  for
example  we could restrict the corpus to texts from one author  although this does have the
major drawback of significantly reducing the size of the corpus  we could also use a more
sophisticated model  such as learning one rule for how all single digit numbers are expressed 
not separate rules for each number  or we could analyse the behaviour of individuals and
identify choices  such as presence of a leading zero  that vary between individuals but are
consistently made by any given individual  and then make such choices parameters which
the user of the nlg system can specify  this last option is probably the best for nlg
systems  reiter  sripada    williams         and is the one used in sumtime mousam
for the leading zero choice 
our main point is simply that we would have been in trouble if we had just accepted our
initial corpus derived rules  use   and     without question  as most corpus researchers
are of course aware  the result of corpus analysis depends on what is being learned  for
example  a rule on how to realise    or a rule on how to realise all single digit numbers 
   

fireiter  sripada    robertson

and on what features are used in the learning  for example  just the number  or the number
and the author   in more complex analyses  such as our analysis of time phrase choice rules
 reiter   sripada      b   the result also depends on the algorithms used for learning and
alignment  the dependence of corpus analysis on these choices means that the results of
a particular analysis are not guaranteed to be correct and need to be validated  checked 
just like the results of other ka techniques  also  what is often the best approach from an
nlg perspective  namely identifying individual variations and letting the user choose which
variation he or she prefers  requires analysing differences between individual writers  to
the best of our knowledge most published nl corpus analyses have not done this  perhaps
in part because many popular corpora do not include author information 
the other recurring problem with corpus derived rules was cases where the writers
produced sub optimal texts that in particular were shorter than they should have been 
probably because such texts were quicker to write  for instance  we noticed that when
a parameter changed in a more or less steady fashion throughout a forecast period  the
forecasters often omitted a time phrase  for example  if a s wind rose steadily in speed
from    to    over the course of a forecast period covering a calendar day  the forecasters
might write s      rising to        instead of s      rising to       by midnight 
a statistical corpus analysis showed that the null time phrase was the most common one
in such contexts  used in     of cases  the next most common time phrase  later  was only
used in     of cases  accordingly  we programmed our system to omit the time phrase
in such circumstances  however  when we asked experts to comment on and revise our
generated forecasts  section       they told us that this behaviour was incorrect  and that
forecasts were more useful to end users if they included explicit time phrases and did not rely
on the readers remembering when forecast periods ended  in other words  in this example
the forecasters were doing the wrong thing  which of course meant that the rule produced
by corpus analysis was incorrect 
we dont know why the forecasters did this  but discussions with the forecast managers
about this and other mistakes  such as forecast authors describing wind speed and direction
as changing at the same time  even when they actually were predicted to change at different
times  suggested that one possible cause is the desire to write forecasts quickly  in particular 
numerical weather predictions are constantly being updated  and customers want their
forecasts to be based on the most up to date prediction  this can limit the amount of time
available to write forecasts 
in fact it can be perfectly rational for human writers to cut corners because of time
limitations  if the forecasters believe  for example  that quickly writing a forecast at the
last minute will let them use more up to date prediction data  and that the benefits of more
up to date data outweighs the costs of abbreviated texts  then they are making the right
decision when they write shorter than optimal texts  an nlg system  however  faces a very
different set of tradeoffs  for example  omitting a time phrase is unlikely to speed up an
nlg system   which means that it should not blindly imitate the choices made by human
writers 
this problem is perhaps a more fundamental one than the individual variation problem 
because it can not be solved by appropriate choices as to what is being learned  what
features are considered  and so forth  corpus analysis  however it is performed  learns the
choice rules used by human authors  if these rules are inappropriate for an nlg system 
   

fiacquiring correct knowledge for nlg

then the rules learned by corpus analysis will be inappropriate ones as well  regardless of
how the corpus analysis is carried out 
in very general terms  corpus analysis certainly has many strengths  such as looking
at what people do in practice  and collecting large data sets which can be statistically
analysed  but pure corpus analysis does perhaps suffer from the drawback that it gives no
information on why experts made the choices they made  which means that blindly imitating
a corpus can lead to inappropriate behaviour when the human writers face a different set
of constraints and tradeoffs than the nlg system 
      role of corpus analysis
corpus analysis and machine learning are wonderful ways to acquire knowledge if
   there is a large data set  corpus  that covers unusual and boundary cases as well as
normal cases 
   the members of the data set  corpus  are correct in that they are what we would like
the software system to produce  and
   the members of the data set  corpus  are consistent  modulo some noise   for example
any given input generally leads to the same output 
these conditions are probably satisfied when learning rules for medical diagnosis or speech
recognition  however  they were not satisfied in our projects  none of the above conditions
were satisfied in stop  and only the first was satisfied in sumtime mousam 
of course  there may be ways to alleviate some of these problems  for example  we could
try to acquire general communication knowledge which is not domain dependent  such as
english grammar  from general corpora such as the british national corpus  we could
argue that certain aspects of manually written texts  such as lexical usage  are unlikely to
be adversely affected by time pressure and hence are probably correct  and we could analyse
the behaviour of individual authors in order to enhance consistency  in other words  treat
author as an input feature on a par with the actual numerical or semantic input data   there
is scope for valuable research here  which we hope will be considered by people interested
in corpus based techniques in nlg 
we primarily used corpus analysis in sumtime mousam to acquire domain communication knowledge  such as how to linguistically express numbers and times in weather
forecasts  when to elide information  and sublanguage constraints on the grammar of our
weather forecasts  corpus analysis of course can also be used to acquire generic communication knowledge such as english grammar  but as mentioned above this is probably best
done on a large general corpus such as the british national corpus  we did not use corpus
analysis to acquire domain knowledge about meteorology  meteorological researchers in fact
do use machine learning techniques to learn about meteorology  but they analyse numeric
data sets of actual and predicted weather  they do not analyse textual corpora 
in summary  machine learning and corpus based techniques are extremely valuable if
the above conditions are satisfied  and in particular offer a cost effective solution to the
problem of acquiring the large amount of knowledge needed in complex nlg applications
 section         acquiring large amounts of knowledge using expert oriented ka techniques
   

fireiter  sripada    robertson

is expensive and time consuming because it requires many sessions with experts  in contrast 
if a large corpus of consistent and correct texts can be created  then large amounts of
knowledge can be extracted from it at low marginal cost  but like all learning techniques 
corpus analysis is very vulnerable to the garbage in  garbage out principle  if the corpus
is small  incorrect  and or inconsistent  then the results of corpus analysis may not be
correct 
    expert revision
in both stop and sumtime mousam  we made heavy use of expert revision  that is  we
showed generated texts to experts and asked them to suggest changes that would improve
them  in a sense  expert revision could be considered to be a type of structured expertoriented ka  but it seems to have somewhat different strengths and weaknesses than the
techniques mentioned in section      so we treat it separately 
as an example of expert revision  an early version of the stop system used the phrase
there are lots of good reasons for stopping  one of the experts commented during a revision
session that the phrasing should be changed to emphasise that the reasons listed  in this
particular section of the stop letter  were ones the smoker himself had selected in the
questionnaire he filled out  this eventually led to the revised wording it is encouraging that
you have many good reasons for stopping  which is in the first paragraph of the example
letter in figure    an example of expert revision in sumtime mousam was mentioned in
section      when we showed experts generated texts that omitted some end of period time
phrases  they told us this was incorrect  and we should include such time phrases 
in stop  we also tried revision sessions with recipients  smokers   this was less successful
than we had hoped  part of the problem was the smokers knew very little about stop  unlike
our experts  who were all familiar with the project   and often made comments which were
not useful for improving the system  such as i did stop for    days til my daughter threw a
wobbly and then i wanted a cigarette and bought some  also  most of our comments came
from well educated and articulate smokers  such as university students  it was harder to get
feedback from less well educated smokers  such as single mothers living in council  public
housing  estates  hence we were unsure if the revision comments we obtained were generally
applicable or not 
      validation of expert revision knowledge
we did not validate expert revision knowledge as we did with the other techniques  indeed 
we initially regarded expert revision as a validation technique  not a ka technique  although
in retrospect it probably makes more sense to think of it as a ka technique 
on a qualitative level  though  expert revision has certainly resulted in a lot of useful
knowledge and ideas for changing texts  and in particular proved a very useful way of
improving the handling of unusual and boundary cases  for example  we changed the way
we described uneventful days in sumtime mousam  when the weather changed very little
during a day  based on revision sessions 
the comment was made during stop that revision was best at suggesting specific localised changes to generated text  and less useful in suggesting larger changes to the system 
one of the stop experts suggested  after the system was built  that he might have been
   

fiacquiring correct knowledge for nlg

able to suggest larger changes if we had explained the systems reasoning to him  instead of
just giving him a letter to revise  in other words  just as we asked experts to think aloud
as they wrote letters  in order to understand their reasoning  it could be useful in revision
sessions if experts understood what the computer system was thinking as well as what
it actually produced  davis and lenat        page      have similarly pointed out that
explanations can help experts debug and improve knowledge based systems 
      role of expert revision
we have certainly found expert revision to be an extremely useful technique for improving
nlg systems  and furthermore it is useful for improving all types of knowledge  domain 
domain communication  and communication   but at the same time revision does seem to
largely be a local optimisation technique  if an nlg system is already generating reasonable
texts  then revision is a good way of adjusting the systems knowledge and rules to improve
the quality of generated text  but like all local optimisation techniques  expert revision
may tend to push systems towards a local optimum  and may be less well suited to finding
radically different solutions that give a better result 

   discussion  problems revisited
in section   we explained that writing tasks can be difficult to automate because these are
complex  often novel  poorly understood  and allow multiple solutions  in this section we
discuss each of these problems in more detail  based on our experiences with stop and
sumtime mousam 
    complexity
because nlg systems communicate with humans  they need knowledge about people  language  and how people communicate  since all of these are very complex  that means that in
general nlg systems need a lot of complex knowledge  this is one of the reasons why knowledge acquisition for nlg is so difficult  if we recall the distinction in section     between
domain knowledge  domain communication knowledge  and communication knowledge  it
may be that communication knowledge  such as grammar  is generic and hence can be acquired once  perhaps by corpus based techniques  and then used in many applications  and
domain knowledge is similar to what is needed by other ai systems  so problems acquiring
it are not unique to nlg  but domain communication knowledge  such as the optimal tone
of a smoking letter and how this tone can be achieved  or when information in a weather
forecast can be elided  is application dependent  and hence cannot be acquired generically 
and is also knowledge about language and communication  and hence is complex   hence
ka for nlg may always require acquiring complex knowledge 
in our experience  the best way to acquire complex knowledge robustly is to get information on how a large number of individual cases are handled  this can be done by corpus
analysis if a suitable corpus can be created  it can also sometimes be done by expert revision  if experts have the time to look at a large number of generated texts  in this regard
it may be useful to tell them to only comment on major problems and to ignore minor
difficulties  but however the knowledge is acquired  it will require a substantial effort 

   

fireiter  sripada    robertson

    novelty
of course  many ai systems need complex knowledge  so the above comments are hardly
unique to nlg  but one aspect of nlg which perhaps is more unusual is that many of the
tasks nlg systems are expected to perform are novel tasks that are not currently done by
humans  most ai expert systems attempt to replicate the performance of human experts
in areas such as medical diagnosis and credit approval  similarly  most language technology
systems attempt to replicate the performance of human language users in tasks such as
speech recognition and information retrieval  but many nlg applications are like stop 
and attempt a task that no human performs  even in sumtime mousam  an argument
could be made that the task humans actually perform is writing weather forecasts under
time constraints  which is in fact different from the task performed by sumtime mousam 
novelty is a fundamental problem  because it means that knowledge acquired from
expert oriented ka may not be reliable  since the experts are not in fact experts at the
actual nlg task   and that a corpus of manually written texts probably does not exist 
this means that none of the ka techniques described above are likely to work  indeed 
acquiring novel knowledge is almost the definition of scientific research  so perhaps the only
way to acquire such knowledge is to conduct scientific research in the domain  of course 
only some knowledge will need to be acquired in this way  even in a novel application it is
likely that much of the knowledge needed  such as grammar and morphology  is not novel 
on the other hand  novelty perhaps is also an opportunity for nlg  one of the drawbacks of conventional expert systems is that their performance is often limited to that of
human experts  in which case users may prefer to consult actual experts instead of computer
systems  but if there are no experts at a task  an nlg system may be used even if its output
is far from ideal 
    poorly understood tasks
a perhaps related problem is that there are no good theoretical models for many of the
choices that nlg systems need to make  for example  the ultimate goal of stop is to
change peoples behaviour  and a number of colleagues have suggested that we base stop
on argumentation theory  as grasso  cawsey  and jones        did for their dietary advice
system  however  argumentation theory focuses on persuading people to change their beliefs
and desires  whereas the goal of stop was more to encourage people to act on beliefs and
desires they already had  in other words  stops main goal was to encourage people who
already wanted to stop smoking to make a serious cessation attempt  not to convince people
who had no desire to quit that they should change their mind about the desirability of
smoking  the most applicable theory we could find was stages of change  prochaska  
diclemente         and indeed we partially based stop on this theory  however  the results
of our evaluation suggested that some of the choices and rules that we based on stages of
change were incorrect  as mentioned in section       
similarly  one of the problems in sumtime mousam is generating texts that will be
interpreted correctly despite the fact that different readers have different idiolects and in
particular probably interpret words in different ways  reiter   sripada      a  roy        
theoretical guidance on how to do this would have been very useful  but we were not able
to find any such guidance 
   

fiacquiring correct knowledge for nlg

the lack of good theoretical models means that nlg developers cannot use such models
to fill in the cracks between knowledge acquired from experts or from data sets  as can be
done by ai systems in better understood areas such as scheduling or configuring machinery 
this in turn means that a lot of knowledge must be acquired  in applications where there is
a good theoretical basis  the goal of ka is perhaps to acquire a limited amount of high level
information about search strategies  taxonomies  the best way to represent knowledge  etc 
once these have been determined  the details can be filled in by theoretical models  but in
applications where details cannot be filled in from theory and need to be acquired  much
more knowledge is needed  acquiring such knowledge with structured expert oriented ka
could be extremely expensive and time consuming  corpus based techniques are cheaper if
a large corpus is available  however  the lack of a good theoretical understanding perhaps
contributes to the problem that we do not know which behaviour we observe in the corpus
is intended to help the reader  and hence should be copied by an nlg system  and which
behaviour is intended to help the writer  and hence perhaps should not be copied  
    expert variation
perhaps in part because of the lack of good theories  in both stop and sumtime mousam
we observed considerable variation between experts  in other words  different experts wrote
quite different texts from the same input data  in stop we also discovered that experts
changed how they wrote over time  section        
variability caused problems for both structured expert oriented ka  because different
experts told us different things  and for corpus analysis  because variation among corpus
authors made it harder to extract a consistent set of rules with good coverage   however 
variation seems to have been less of a problem with revision  we suspect this is because
experts vary less when they are very confident about a particular decision  and in revision
experts tended to focus on things they were confident about  which was not the case with
the other ka techniques 
in a sense variability may be especially dangerous in corpus analysis  because there is
no information in a corpus about the degree of confidence authors have in individual decisions  and also because developers may not even realise that there is variability between
authors  especially if the corpus does not include author information  in contrast  structured expert oriented techniques such as think aloud do sometimes give information about
experts confidence  and also variations between experts are usually obvious 
we experimented with various techniques for resolving differences between experts authors 
such as group discussions and focusing on the decisions made by one particular expert  none
of these were really satisfactory  given our experiences with revision  perhaps the best way
to reduce variation is to develop ka techniques that very clearly distinguish between decisions experts are confident in and decisions they have less confidence in 

   development methodology  using multiple ka techniques
from a methodological perspective  the fact that different ka techniques have different
strengths and weaknesses suggests that it makes sense to use a mixture of several different
ka techniques  for example  if both structured expert oriented ka and corpus analysis
are used  then the explanatory information from the expert oriented ka can be used to
   

fireiter  sripada    robertson

help identify which decisions are intended to help the reader and which are intended to
help the writer  thus helping overcome a problem with corpus analysis  and the broader
coverage of corpus analysis can show how unusual and boundary cases should be handled 
thus overcoming a problem with expert oriented ka 
it also may make sense to use different techniques at different points in the development
process  for example  directly asking experts for knowledge could be stressed at the initial
stages of a project  and used to build a very simple initial prototype  structured ka with
experts and corpus analysis could be stressed during the middle phases of a project  when
the prototype is fleshed out and converted into something resembling a real system  and
revision could be used in the later stages of a project  when the system is being refined and
improved 
this strategy  which is graphically shown in figure    is basically the one we followed
in both stop and sumtime mousam  note that it suggests that knowledge acquisition is
something that happens throughout the development process  in other words  we do not first
acquire knowledge and then build a system  knowledge acquisition is an ongoing process
which is closely coupled with the general software development effort  of course  this is
hardly a novel observation  and there are many development methodologies for knowledgebased systems that stress iterative development and continual ka  adelman   riedel 
      
in the short term  we believe that using a development methodology that combines
different ka techniques in this manner  and also validating knowledge as much as possible 
are the best strategies for acquiring nlg knowledge  we also believe that whenever possible
knowledge that is acquired in one way should be validated in another way  in other words 
we do not recommend validating corpus acquired knowledge using corpus techniques  even
if the validation is done with a held out test set   or validating expert acquired knowledge
using expert based validation  even if the validation is done using a different expert   it
is preferable  although not always possible  to validate corpus acquired knowledge with
experts  and to validate expert acquired knowledge with a corpus 
another issue related to development methodology is the relationship between knowledge acquisition and system evaluation  although these are usually considered to be separate activities  in fact they can be closely related  for example  we are currently running
an evaluation of sumtime mousam which is based on the number of edits that forecasters
manually make to computer generated forecasts before publishing them  this is similar to
edit cost evaluations of machine translation systems  jurafsky   martin        page      
however  these edits are also an excellent source of data for improving the system via expert
revision  to take one recent example  a forecaster edited the computer generated text sse
      gradually backing se       by dropping the last speed range  giving sse
      gradually backing se  this can be considered as evaluation data    token
edits needed to make text acceptable   or as ka data  we need to adjust our rules for eliding
similar but not identical speed ranges  
in other words  real world feedback on the effectiveness and quality of generated texts
can often be used to either improve or evaluate an nlg system  how such data should
be used depends on the goals of the project  in scientific projects whose goal is to test
hypotheses  it may be appropriate at some point to stop improving a system and use all new
effectiveness data purely for evaluation and hypothesis testing  in a sense this is analogous
   

fiacquiring correct knowledge for nlg

directly ask experts
for knowledge
initial prototype

structured ka
with experts

corpus analysis

initial version of full system

expert revision

final system
figure    our methodology
to holding back part of a corpus for testing purposes  in applied projects whose goal is
to build a maximally useful system  however  it may be more appropriate to use all of the
effectiveness data to improve the quality of the generated texts 

   conclusion
acquiring correct knowledge for nlg is very difficult  because the knowledge needed is
largely knowledge about people  language  and communication  and such knowledge is complex and poorly understood  furthermore  perhaps because writing is more of an art than a
science  different people write very differently  which further complicates the knowledge acquisition process  and many nlg systems attempt novel tasks not currently done manually 
which makes it very hard to find knowledgeable experts or good quality corpora  perhaps
because of these problems  every single ka technique we tried in stop and sumtimemousam had major problems and limitations 
there is no easy solution to these problems  in the short term  we believe it is useful
to use a mixture of different ka techniques  since techniques have different strengths and
weaknesses   and to validate knowledge whenever possible  preferably using a different tech 

   

fireiter  sripada    robertson

nique than the one used to acquire the knowledge  it also helps if developers understand
the weaknesses of different techniques  such as the fact that structured expert oriented ka
may not give good coverage of the complexities of people and language  and the fact that
corpus based ka does not distinguish between behaviour intended to help the reader and
behaviour intended to help the writer 
in the longer term  we need more research on better ka techniques for nlg  if we cannot
reliably acquire the knowledge needed by ai approaches to text generation  then there is
no point in using such approaches  regardless of how clever our algorithms or models are 
the first step towards developing better ka techniques is to acknowledge that current ka
techniques are not working well  and understand why this is the case  we hope that this
paper constitutes a useful step in this direction 

acknowledgements
numerous people have given us valuable comments over the past five years as we struggled with ka for nlg  too many to acknowledge here  but we would like to thank sandra
williams for reading several drafts of this paper and considering it in the light of her own experiences  and to thank the anonymous reviewers for their very helpful comments  we would
also like to thank the experts we worked with in stop and sumtime mousam  without
whom this work would not be possible  this work was supported by the uk engineering and
physical sciences research council  epsrc   under grants gr l      and gr m      
and by the scottish office department of health under grant k opr     d    

references
adelman  l     riedel  s          handbook for evaluating knowledge based systems 
kluwer 
anderson  j          cognitive psychology and its implications  fourth edition   freeman 
barzilay  r     mckeown  k          extracting paraphrases from a parallel corporus 
in proceedings of the   th meeting of the association for computation linguistics
 acl      pp       
buchanan  b     wilkins  d   eds            readings in knowledge acquisition and learning  morgan kaufmann 
davis  r     lenat  d          knowledge based systems in artificial intelligence  mcgraw
hill 
duboue  p     mckeown  k          empirically estimating order constraints for content
planning in generation  in proceedings of the   th meeting of the association for
computation linguistics  acl      pp         
goldberg  e   driedger  n     kittredge  r          using natural language processing to
produce weather forecasts  ieee expert              
grasso  f   cawsey  a     jones  r          dialectical argumentation to solve conflicts
in advice giving  a case study in the promotion of healthy nutrition  international
journal of human computer studies               
   

fiacquiring correct knowledge for nlg

hardt  d     rambow  o          generation of vp ellipsis  a corpus based approach 
in proceedings of the   th meeting of the association for computation linguistics
 acl      pp         
jurafsky  d     martin  j          speech and language processing  prentice hall 
kittredge  r   korelsky  t     rambow  o          on the need for domain communication
language  computational intelligence                
lavoie  b   rambow  o     reiter  e          customizable descriptions of object oriented
models  in proceedings of the fifth conference on applied natural language processing  anlp        pp         
lennox  s   osman  l   reiter  e   robertson  r   friend  j   mccann  i   skatun  d     donnan  p          the cost effectiveness of computer tailored and non tailored smoking
cessation letters in general practice  a randomised controlled study  british medical
journal                
mckeown  k   kukich  k     shaw  j          practical issues in automatic document
generation  in proceedings of the fourth conference on applied natural language
processing  anlp        pp      
oberlander  j   odonnell  m   knott  a     mellish  c          conversation in the museum  experiments in dynamic hypermedia with the intelligent labelling explorer  new
review of hypermedia and multimedia          
prochaska  j     diclemente  c          stages of change in the modification of problem
behaviors  sage 
reiter  e     dale  r          building natural language generation systems  cambridge
university press 
reiter  e   robertson  r     osman  l          knowledge acquisition for natural language
generation  in proceedings of the first international conference on natural language
generation  pp         
reiter  e   robertson  r     osman  l          lessons from a failure  generating tailored
smoking cessation letters  artificial intelligence            
reiter  e     sripada  s       a   human variation and lexical choice  computational
linguistics             
reiter  e     sripada  s       b   should corpora texts be gold standards for nlg   in
proceedings of the second international conference on natural language generation 
pp        
reiter  e   sripada  s     williams  s          acquiring and using limited user models in
nlg  in proceedings of the      european workshop on natural language generation  pp       
roy  d          learning visually grounded words and syntax for a scene description task 
computer speech and language             
scott  a  c   clayton  j     gibson  e          a practical guide to knowledge acquisition 
addison wesley 
   

fireiter  sripada    robertson

shadbolt  n   ohara  k     crow  l          the experimental evaluation of knowledge acquisition techniques and methods  history  problems and new directions  international
journal of human computer studies             
sripada  s   reiter  e   hunter  j     yu  j          segmenting time series for weather
forecasting  in applications and innovations in intelligent systems x  pp         
springer verlag 
sripada  s   reiter  e   hunter  j     yu  j          summarising neonatal time series data 
in proceedings of the research note sessions of the eacl       pp         
sripada  s   reiter  e   hunter  j   yu  j     davy  i          modelling the task of summarising time series data using ka techniques  in applications and innovations in
intelligent systems ix  pp          springer verlag 
walker  m   rambow  o     rogati  m          training a sentence planner for spoken
dialogue using boosting  computer speech and language             
williams  s   reiter  e     osman  l          experiments with discourse level choices
and readability  in proceedings of the      european workshop on natural language
generation  pp         

   

fi