journal of artificial intelligence research               

submitted       published     

acquiring word meaning mappings
for natural language interfaces
cynthia a  thompson

cindi cs utah edu

school of computing  university of utah
salt lake city  ut           

raymond j  mooney

mooney cs utexas edu

department of computer sciences  university of texas
austin  tx           

abstract
this paper focuses on a system  wolfie  word learning from interpreted examples   that acquires a semantic lexicon from a corpus of sentences paired with semantic
representations  the lexicon learned consists of phrases paired with meaning representations  wolfie is part of an integrated system that learns to transform sentences into
representations such as logical database queries 
experimental results are presented demonstrating wolfies ability to learn useful
lexicons for a database interface in four different natural languages  the usefulness of
the lexicons learned by wolfie are compared to those acquired by a similar system  with
results favorable to wolfie  a second set of experiments demonstrates wolfies ability
to scale to larger and more difficult  albeit artificially generated  corpora 
in natural language acquisition  it is difficult to gather the annotated data needed
for supervised learning  however  unannotated data is fairly plentiful  active learning
methods attempt to select for annotation and training only the most informative examples 
and therefore are potentially very useful in natural language applications  however  most
results to date for active learning have only considered standard classification tasks  to
reduce annotation effort while maintaining accuracy  we apply active learning to semantic
lexicons  we show that active learning can significantly reduce the number of annotated
examples required to achieve a given level of performance 

   introduction and overview
a long standing goal for the field of artificial intelligence is to enable computer understanding of human languages  much progress has been made in reaching this goal  but much also
remains to be done  before artificial intelligence systems can meet this goal  they first need
the ability to parse sentences  or transform them into a representation that is more easily
manipulated by computers  several knowledge sources are required for parsing  such as a
grammar  lexicon  and parsing mechanism 
natural language processing  nlp  researchers have traditionally attempted to build
these knowledge sources by hand  often resulting in brittle  inefficient systems that take
a significant effort to build  our goal here is to overcome this knowledge acquisition
bottleneck by applying methods from machine learning  we develop and apply methods
from empirical or corpus based nlp to learn semantic lexicons  and from active learning to
reduce the annotation effort required to learn them 

c
    
ai access foundation and morgan kaufmann publishers  all rights reserved 

fithompson   mooney

the semantic lexicon is one nlp component that is typically challenging and time consuming to construct and update by hand  our notion of semantic lexicon  formally defined
in section    is that of a list of phrase meaning pairs  where the meaning representation is
determined by the language understanding task at hand  and where we are taking a compositional view of sentence meaning  partee  meulen    wall         this paper describes
a system  wolfie  word learning from interpreted examples   that acquires a semantic
lexicon of phrase meaning pairs from a corpus of sentences paired with semantic representations  the goal is to automate lexicon construction for an integrated nlp system that
acquires both semantic lexicons and parsers for natural language interfaces from a single
training set of annotated sentences 
although many others  sebillot  bouillon    fabre        riloff   jones        siskind 
      hastings        grefenstette        brent        have presented systems for learning
information about lexical semantics  we present here a system for learning lexicons of phrasemeaning pairs  further  our work is unique in its combination of several features  though
prior work has included some of these aspects  first  its output can be used by a system 
chill  zelle   mooney        zelle         that learns to parse sentences into semantic
representations  second  it uses a fairly straightforward batch  greedy  heuristic learning
algorithm that requires only a small number of examples to generalize well  third  it is
easily extendible to new representation formalisms  fourth  it requires no prior knowledge
although it can exploit an initial lexicon if provided  finally  it simplifies the learning
problem by making several assumptions about the training data  as described further in
section     
we test wolfies ability to acquire a semantic lexicon for a natural language interface
to a geographical database using a corpus of queries collected from human subjects and
annotated with their logical form  in this test  wolfie is integrated with chill  which
learns parsers but requires a semantic lexicon  previously built manually   the results
demonstrate that the final acquired parser performs nearly as accurately at answering novel
questions when using a learned lexicon as when using a hand built lexicon  wolfie is
also compared to an alternative lexicon acquisition system developed by siskind        
demonstrating superior performance on this task  finally  the corpus is translated into
spanish  japanese  and turkish  and experiments are conducted demonstrating an ability
to learn successful lexicons and parsers for a variety of languages 
a second set of experiments demonstrates wolfies ability to scale to larger and more
difficult  albeit artificially generated  corpora  overall  the results demonstrate a robust
ability to acquire accurate lexicons directly usable for semantic parsing  with such an
integrated system  the task of building a semantic parser for a new domain is simplified  a
single representative corpus of sentence representation pairs allows the acquisition of both
a semantic lexicon and parser that generalizes well to novel sentences 
while building an annotated corpus is arguably less work than building an entire nlp
system  it is still not a simple task  redundancies and errors may occur in the training data 
a goal should be to also minimize the annotation effort  yet still achieve a reasonable level
of generalization performance  in the case of natural language  there is frequently a large
amount of unannotated text available  we would like to automatically  but intelligently 
choose which of the available sentences to annotate 

 

fiacquiring word meaning mappings

we do this here using a technique called active learning  active learning is a research
area in machine learning that features systems that automatically select the most informative examples for annotation and training  cohn  atlas    ladner         the primary goal
of active learning is to reduce the number of examples that the system is trained on  thereby
reducing the example annotation cost  while maintaining the accuracy of the acquired information  to demonstrate the usefulness of our active learning techniques  we compared
the accuracy of parsers and lexicons learned using examples chosen by active learning for
lexicon acquisition  to those learned using randomly chosen examples  finding that active
learning saved significant annotation cost over training on randomly chosen examples  this
savings is demonstrated in the geography query domain 
in summary  this paper provides a new statement of the lexicon acquisition problem
and demonstrates a machine learning technique for solving this problem  next  by combining this with previous research  we show that an entire natural language interface can
be acquired from one training corpus  further  we demonstrate the application of active
learning techniques to minimize the number of sentences to annotate as training input for
the integrated learning system 
the remainder of the paper is organized as follows  section   gives more background
information on chill and introduces siskinds lexicon acquisition system  which we will
compare to wolfie in section    sections   and   formally define the learning problem and
describe the wolfie algorithm in detail  in section   we present and discuss experiments
evaluating wolfies performance in learning lexicons in a database query domain and for
an artificial corpus  next  section   describes and evaluates our use of active learning
techniques for wolfie  sections   and   discuss related research and future directions 
respectively  finally  section   summarizes our research and results 

   background
in this section we give an overview of chill  the system that our research adds to  we also
describe jeff siskinds lexicon acquisition system 
    chill
the output produced by wolfie can be used to assist a larger language acquisition system 
in particular  it is currently used as part of the input to a parser acquisition system called
chill  constructive heuristics induction for language learning   chill uses inductive
logic programming  muggleton        lavrac   dzeroski        to learn a deterministic
shift reduce parser  tomita        written in prolog  the input to chill is a corpus of
sentences paired with semantic representations  the same input required by wolfie  the
parser learned is capable of mapping the sentences into their correct representations  as well
as generalizing well to novel sentences  in this paper  we limit our discussion to chills
ability to acquire parsers that map natural language questions directly into prolog queries
that can be executed to produce an answer  zelle   mooney         following are two
sample queries for a database on u s  geography  paired with their corresponding prolog
query 

 

fithompson   mooney

 sentence  representation 
training
examples

wolfie

chill

lexicon
 phrase  meaning 

final
parser
prolog

figure    the integrated system
what is the capital of the state with the biggest population 
answer c   capital s c   largest p   state s   population s p      
what state is texarkana located in 
answer s   state s   eq c cityid texarkana      loc c s    
chill treats parser induction as the problem of learning rules to control the actions of
a shift reduce parser  during parsing  the current context is maintained in a stack and a
buffer containing the remaining input  when parsing is complete  the stack contains the
representation of the input sentence  there are three types of operators that the parser uses
to construct logical queries  one is the introduction onto the stack of a predicate needed in
the sentence representation due to a phrases appearance at the front of the input buffer 
these operators require a semantic lexicon as background knowledge  for details on this
and the other two parsing operators  see zelle and mooney         by using wolfie  the
lexicon is provided automatically  figure   illustrates the complete system 
    jeff siskinds lexicon learning research
the most closely related previous research into automated lexicon acquisition is that of
siskind         itself inspired by work by rayner  hugosson  and hagert         as we
will be comparing our system to his in section    we describe the main features of his
research in this section  his goal is one of cognitive modeling of childrens acquisition of the
lexicon  where that lexicon can be used for both comprehension and generation  our goal
is a machine learning and engineering one  and focuses on a lexicon for comprehension and
use in parsing  using a learning process that does not claim any cognitive plausibility  and
with the goal of learning a lexicon that generalizes well from a small number of training
examples 
his system takes an incremental approach to acquiring a lexicon  learning proceeds in
two stages  the first stage learns which symbols in the representation are to be used in the
 

fiacquiring word meaning mappings

 capital  capital       
 biggest  largest       
 highest point  high point       
 through  traverse       
 has  loc      

 state  state     
 in  loc       
 long  len       
 capital  capital     

figure    sample semantic lexicon
final conceptual expression that represents the meaning of a word  by using a versionspace approach  the second stage learns how these symbols are put together to form the
final representation  for example  when learning the meaning of the word raise  the
algorithm may learn the set  cause  go  up  during the first stage and put them together
to form the expression cause x  go y  up   during the second stage 
siskind        shows the effectiveness of his approach on a series of artificial corpora 
the system handles noise  lexical ambiguity  referential uncertainty  and very large corpora  but the usefulness of lexicons learned is only compared to the correct  artificial
lexicon  the goal of the experiments presented there was to evaluate the correctness and
completeness of learned lexicons  earlier work  siskind        also evaluated versions of his
technique on a quite small corpus of real english and japanese sentences  we extend that
evaluation to a demonstration of the systems usefulness in performing real world natural
language processing tasks  using a larger corpus of real sentences 

   the lexicon acquisition problem
although in the end our goal is to acquire an entire natural language interface  we currently
divide the task into two parts  the lexicon acquisition component and the parser acquisition
component  in this section  we discuss the problem of acquiring semantic lexicons that
assist parsing and the acquisition of parsers  the training input consists of natural language
sentences paired with their meaning representations  from these pairs we extract a lexicon
consisting of phrases paired with their meaning representations  some training pairs were
given in the previous section  and a sample lexicon is shown in figure   
    formal definition
to present the learning problem more formally  some definitions are needed  while in the
following we use the terms string and substring  these extend straight forwardly to
natural language sentences and phrases  respectively  we also refer to labeled trees  making
the assumption that the semantic meanings of interest can be represented as such  most
common representations can be recast as labeled trees or forests  and our formalism extends
easily to the latter 
definition  let v   e be finite alphabets of vertex labels and edge labels  respectively 
let v be a finite nonempty set of vertices  l a total function l   v  v   e a set of unordered
pairs of distinct vertices called edges  and a a total function a   e  e   g    v  l  e  a  is
a labeled graph 

 

fithompson   mooney

string s    the girl ate the pasta with the cheese 
t   with its vertex and edge labels 

tree t  
 
 
 

ingest
patient
agent
person food
age type accomp
sex

 
 

 

 

female

child pasta

food

type
cheese

 

interpretation f   from s   to t   
f    girl     
f    ate      
f    pasta      
f    the cheese      

figure    labeled trees and interpretations
definition  a labeled tree is a connected  acyclic labeled graph 
figure   shows the labeled tree t   with vertices      on the left  with associated vertex
and edge labels on the right  the function l is  
 

    ingest       person       food       female       child       pasta  
    food       cheese    

the tree t  is a semantic representation of the sentence s    the girl ate the pasta with the
cheese  using a conceptual dependency  schank        representation in prolog list form 
the meaning is 
 ingest 

agent  person  sex female  age child  
patient  food  type pasta  accomp  food  type cheese    

definition  a u v path in a graph g is a finite alternating sequence of vertices and edges
of g  in which no vertex is repeated  that begins with vertex u and ends with vertex v  and
in which each edge in the sequence connects the vertex that precedes it in the sequence to
the vertex that follows it in the sequence 
definition  a directed  labeled tree t    v  l  e  a  is a labeled tree whose edges consist of
ordered pairs of vertices  with a distinguished vertex r  called the root  with the property
that for every v  v   there is a directed r v path in t   and such that the underlying
undirected unlabeled graph induced by  v  e  is a connected  acyclic graph 
definition  an interpretation f from a finite string s to a directed  labeled tree t is a
one to one function mapping a subset s  of the substrings of s  such that no two strings in
s  overlap  into the vertices of t such that the root of t is in the range of f  
   we omit enumeration of the function e but it could be given in a similar manner  for example        
agent  is an element of e 

 

fiacquiring word meaning mappings

girl  

person
sex
age
female

pasta   food

type

child

pasta

the cheese   food
type
cheese

ate   ingest

figure    meanings
the interpretation provides information about what parts of the meaning of a sentence
originate from which of its phrases  in figure    we show an interpretation  f    of s  to t   
note that with is not in the domain of f    since s  is a subset of the substrings of s  thus
allowing some words in s to have no meaning  because we disallow overlapping substrings
in the domain  both cheese and the cheese could not map to vertices in t   
definition  given an interpretation f of string s to tree t  and an element p of the domain
of f   the meaning of p relative to s  t  f is the connected subgraph of t whose vertices
include f  p  and all its descendents except any other vertices in the range of f and their
descendents 
meanings in this sense concern the lowest level of phrasal meanings  occurring at the
terminal nodes of a semantic grammar  namely the entries in the semantic lexicon  the
grammar can then be used to construct the meanings of longer phrases and entire sentences 
this is our motivation for the previously stated constraint that the root must be included
in the range of f   we want all vertices in the sentence representation to be included in the
meaning of some phrase  note that the meaning of p is also a directed tree with f  p  as its
root  figure   shows the meanings of each phrase in the domain of interpretation function
f  shown in figure    we show only the labels on the vertices and edges for readability 
definition  given a finite set st f of triples   s    t    f               sn   tn   fn    where each
si is a finite string  each ti is a directed  labeled tree  and each fi is an interpretation function
from si to ti   let the language lst f    p            pk   of st f be the union of all substrings 
that occur in the domain of some fi   for each pj  lst f   the meaning set of pj   denoted
mst f  pj     is the set of all meanings of pj relative to si   ti   fi for some   si   ti   fi   st f  
we consider two meanings to be the same if they are isomorphic trees taking labels into
account 
for example  given sentence s    the man ate the cheese  the labeled tree t  pictured
in figure    and f  defined as  f   ate       f   man       f   the cheese       the
   we consider two substrings to be the same string if they contain the same characters in the same order 
irrespective of their positions within the larger string in which they occur 
   we omit the subscript on m when the set st f is obvious from context 

 

fithompson   mooney

string s    the man ate the cheese  
tree t   

t  with its vertex and edge labels 
ingest
patient
agent

 
 

 

 

person food
type
age
sex
 

 

male

adult

cheese

figure    a second tree
meaning set of the cheese with respect to st f      s    t    f       s    t    f     is   food 
type cheese    just one meaning though f  and f  map the cheese to different vertices
in the two trees  because the subgraphs denoting the meaning of the cheese for the two
functions are isomorphic 
definition  given a finite set st f of triples   s    t    f               sn   tn   fn    where each
si is a finite string  each ti is a directed  labeled tree  and each fi is an interpretation
function from si to ti   the covering lexicon expressed by st f is
  p  m    p  lst f   m  m  p   
the covering lexicon l expressed by st f      s    t    f       s    t    f     is 
 

 girl   person  sex female  age child   
 man   person  sex male  age adult   
 ate   ingest   
 pasta   food  type pasta   
 the cheese   food  type cheese     

the idea of a covering lexicon is that it provides  for each string  sentence  si   a meaning
for some of the phrases in that sentence  further  these meanings are trees whose labeled
vertices together include each of the labeled vertices in the tree ti representing the meaning
of si   with no vertices duplicated  and containing no vertices not in ti   edge labels may
or may not be included  since the idea is that some of them are due to syntax  which the
parser will provide  those edges capturing lexical semantics are in the lexicon  note that
because we only include in the covering lexicon phrases  substrings  that are in the domains
of the fi s  words with the empty tree as meaning are not included in the covering lexicon 
note also that we will in general use phrase to mean substrings of sentences  whether
they consist of one word  or more than one  finally the strings in the covering lexicon may
contain overlapping words even though those in the domain of an individual interpretation
function must not  since those overlapping words could have occurred in different sentences 
finally  we are ready to define the learning problem at hand 

 

fiacquiring word meaning mappings

the lexicon acquisition problem 
given  a multiset of strings s    s            sn   and a multiset of labeled trees t    t            tn   
find  a multiset of interpretation functions  f    f            fn    such that the cardinality of
the covering lexicon expressed by st f      s    t    f               sn   tn   fn    is minimized 
if such a set is found  we say we have found a minimal set of interpretations  or a minimal
covering lexicon    
less formally  a learner is presented with a multiset of sentences  s  paired with their
meanings  t    the goal of learning is to find the smallest lexicon consistent with this data 
this lexicon is the paired listing of all phrases occurring in the domain of some fi  f
 where f is the multiset of interpretation functions found  with each of the elements in
their meaning sets  the motivation for finding a lexicon of minimal size is the usual bias
towards simplicity of representation and generalization beyond the training data  while
this definition allows for phrases of any length  we will usually want to limit the length
of phrases to be considered for inclusion in the domain of the interpretation functions  for
efficiency purposes 
once we determine a set of interpretation functions for a set of strings and trees  there
is only one unique covering lexicon expressed by st f   however  this might not be the
only set of interpretation functions possible  and may not result in the lexicon with smallest
cardinality  for example  the covering lexicon given with the previous example is not a
minimal covering lexicon  for the two sentences given  we could find minimal  though
rather degenerate  lexicons such as 
 

 girl 
 man 

 ingest  agent  person  sex female  age child  
patient  food  type pasta  accomp  food  type cheese     
 ingest  agent  person  sex male  age adult  
patient  food  type cheese     

this type of lexicon becomes less likely as the size of the corpus grows 
    implications of the definition
this definition of the lexicon acquisition problem differs from that given by other authors 
including riloff and jones         siskind         manning         brent        and others 
as further discussed in section    our definition of the problem makes some assumptions
about the training input  first  by making f a function instead of a relation  the definition
assumes that the meaning for each phrase in a sentence appears once in the representation
of that sentence  the single use assumption  second  by making f one to one  it assumes
exclusivity  that each vertex in a sentences representation is due to only one phrase in the
sentence  third  it assumes that a phrases meaning is a connected subgraph of a sentences
representation  not a more distributed representation  the connectedness assumption  while
the first assumption may not hold for some representation languages  it does not present a
problem in the domains we have considered  the second and third assumptions are perhaps
less problematic with respect to general language use 
our definition also assumes compositionality  that the meaning of a sentence is derived
from the meanings of the phrases it contains  in addition  perhaps to some connecting
information specific to the representation at hand  but is not derived from external sources
 

fithompson   mooney

such as noise  in other words  all the vertices of a sentences representation are included
within the meaning of some word or phrase in that sentence  this assumption is similar
to the linking rules of jackendoff         and has been used in previous work on grammar
and language acquisition  e g   haas and jayaraman        siskind          while there is
some debate in the linguistics community about the ability of compositional techniques to
handle all phenomena  fillmore        goldberg         making this assumption simplifies
the learning process and works reasonably for the domains of interest here  also  since we
allow multi word phrases in the lexicon  e g    kick the bucket  die       one objection
to compositionality can be addressed 
this definition also allows training input in which 
   words and phrases have multiple meanings  that is  homonymy might occur in the
lexicon 
   several phrases map to the same meaning  that is  synonymy might occur in the
lexicon 
   some words in a sentence do not map to any meanings  leaving them unused in the
assignment of words to meanings  
   phrases of contiguous words map to parts of a sentences meaning representation 
of particular note is lexical ambiguity    above   note that we could have also derived an
ambiguous lexicon such as 
 

 girl   person  sex female  age child   
 ate   ingest   
 ate   ingest  agent  person  sex male  age adult    
 pasta   food  type pasta   
 the cheese   food  type cheese     

from our sample corpus  in this lexicon  ate is an ambiguous word  the earlier example
minimizes ambiguity resulting in an alternative  more intuitively pleasing lexicon  while
our problem definition first minimizes the number of entries in the lexicon  our learning
algorithm will also exploit a preference for minimizing ambiguity 
also note that our definition allows training input in which sentences themselves are
ambiguous  paired with more than one meaning   since a given sentence in s  a multiset 
might appear multiple times appear with more than one meaning  in fact  the training data
that we consider in section   does have some ambiguous sentences 
our definition of the lexicon acquisition problem does not fit cleanly into the traditional
definition of learning for classification  each training example contains a sentence and its
semantic parse  and we are trying to extract semantic information about some of the phrases
in that sentence  so each example potentially contains information about multiple target
concepts  phrases   and we are trying to pick out the relevant features  or vertices of the
   in fact  all of these assumptions except for single use were made by siskind         see section   for
details 
   these words may  however  serve as cues to a parser on how to assemble sentence meanings from word
meanings 

  

fiacquiring word meaning mappings

representation  corresponding to the correct meaning of each phrase  of course  our assumptions of single use  exclusivity  connectedness  and compositionality impose additional
constraints  in addition to this multiple examples in one learning scenario  we do not
have access to negative examples  nor can we derive any implicit negatives  because of the
possibility of ambiguous and synonymous phrases 
in some ways the problem is related to clustering  which is also capable of learning
multiple  potentially non disjoint categories  however  it is not clear how a clustering
system could be made to learn the phrase meaning mappings needed for parsing  finally 
current systems that learn multiple concepts commonly use examples for other concepts as
negative examples of the concept currently being learned  the implicit assumption made
by doing this is that concepts are disjoint  an unwarranted assumption in the presence of
synonymy 

   the wolfie algorithm and an example
in this section  we first discuss some issues we considered in the design of our algorithm 
then describe it fully in section     
    solving the lexicon acquisition problem
a first attempt to solve the lexicon acquisition problem might be to examine all interpretation functions across the corpus  then choose the one s  with minimal lexicon size  the
number of possible interpretation functions for a given input pair is dependent on both the
size of the sentence and its representation  in a sentence with w words  there are  w   
possible phrases  not a particular challenge 
however  the number of possible interpretation functions grows extremely quickly with
the size of the input  for a sentence with p phrases and an associated tree with n vertices 
the number of possible interpretation functions is 
c  n     

c
x
i  

 
 
 i      n  i   c  i  

   

where c is min p  n   the derivation of the above formula is as follows  we must choose
which phrases to use in the domain of f   and we can choose one phrase  or two  or any
number up to min p  n   if n   p we can only assign n phrases since f is one to one   or
p
i

 

 

p 
i  p  i  

where i is the number of phrases chosen  but we can also permute these phrases  so that
the order in which they are assigned to the vertices is different  there are i  such permutations  we must also choose which vertices to include in the range of the interpretation
function  we have to choose the root each time  so if we are choosing i vertices  we have
n    choose i    vertices left after choosing the root  or
n 
i 

 

 

 n     
 
 i      n  i  
  

fithompson   mooney

the full number of possible interpretation functions is then 
min p n 

x
i  

p 
 n     
 i  
 
i  p  i  
 i      n  i  

which simplifies to equation    when n   p  the largest term of this equation is c   
p   which grows at least exponentially with p  so in general the number of interpretation
functions is too large to allow enumeration  therefore  finding a lexicon by examining all
interpretations across the corpus  then choosing the lexicon s  of minimum size  is clearly
not tractable 
instead of finding all interpretations  one could find a set of candidate meanings for
each phrase  from which the final meaning s  for that phrase could be chosen in a way that
minimizes lexicon size  one way to find candidate meanings is to fracture the meanings
of sentences in which a phrase appears  siskind        defined fracturing  he also calls
it the unlink  operation  over terms such that the result includes all subterms of an
expression plus   in our representation formalism  this corresponds to finding all possible
connected subgraphs of a meaning  and adding the empty graph  like the interpretation
function technique just discussed  fracturing would also lead to an exponential blowup in
the number of candidate meanings for a phrase  a lower bound on the number of connected
subgraphs for a full binary tree with n vertices is obtained by noting that any subset of
the  n        leaves may be deleted and still maintain connectivity of the remaining tree 
thus  counting all of the ways that leaves can be deleted gives us a lower bound of   n     
fractures   this does not completely rule out fracturing as part of a technique for lexicon
learning since trees do not tend to get very large  and indeed siskind uses it in many of his
systems  with other constraints to help control the search  however  we wish to avoid any
chance of exponential blowup to preserve the generality of our approach for other tasks 
another option is to force chill to essentially induce a lexicon on its own  in this
model  we would provide to chill an ambiguous lexicon in which each phrase is paired
with every fracture of every sentence in which it appears  chill would then have to decide
which set of fractures leads to the correct parse for each training sentence  and would only
include those in a final learned parser lexicon combination  thus the search would again
become exponential  furthermore  even with small representations  it would likely lead
to a system with poor generalization ability  while some of siskinds work  e g   siskind 
      took syntactic constraints into account and did not encounter such difficulties  those
versions did not handle lexical ambiguity 
if we could efficiently find some good candidates  a standard induction algorithm could
then attempt to use them as a source of training examples for each phrase  however 
any attempt to use the list of candidate meanings of one phrase as negative examples for
another phrase would be flawed  the learner could not know in advance which phrases
are possibly synonymous  and thus which phrase lists to use as negative examples of other
phrase meanings  also  many representation components would be present in the lists of
more than one phrase  this is a source of conflicting evidence for a learner  even without
the presence of synonymy  since only positive examples are available  one might think of
using most specific conjunctive learning  or finding the intersection of all the representations
   thanks to net citizen dan hirshberg for help with this analysis 

  

fiacquiring word meaning mappings

for each phrase  p  of at most two words  
     collect the training examples in which p appears
     calculate lics from  sampled  pairs of these examples representations
     for each l in the lics  add  p  l  to the set of candidate lexicon entries
until the input representations are covered  or no candidate lexicon entries remain do 
     add the best  phrase  meaning  pair from the candidate entries to the lexicon
     update candidate meanings of phrases in the same sentences as the phrase just learned
return the lexicon of learned  phrase  meaning  pairs 

figure    wolfie algorithm overview
for each phrase  as proposed by anderson         however  the meanings of an ambiguous
phrase are disjunctive  and this intersection would be empty  a similar difficulty would be
expected with the positive only compression of muggleton        
    our solution  wolfie
the above analysis leads us to believe that the lexicon acquisition problem is computationally intractable  therefore  we can not perform an efficient search for the best lexicon 
nor can we use a standard induction algorithm  therefore  we have implemented wolfie   
outlined in figure    which finds an approximate solution to the lexicon acquisition problem  our approach is to generate a set of candidate lexicon entries  from which the final
learned lexicon is derived by greedily choosing the best lexicon item at each point  in the
hopes of finding a final  minimal  covering lexicon  we do not actually learn interpretation
functions  so do not guarantee that we will find a covering lexicon   even if we were to
search for interpretation functions  using a greedy search would also not guarantee covering
the input  and of course it also does not guarantee that a minimal lexicon is found  however 
we will later present experimental results demonstrating that our greedy approach performs
well 
wolfie first derives an initial set of candidate meanings for each phrase  the algorithm
for generating candidates  lics  attempts to find a maximally common meaning for each
phrase  which biases toward both finding a small lexicon by covering many vertices of a tree
at once  and finding a lexicon that actually does cover the input  second  wolfie chooses
final lexicon entries from this candidate set  one at a time  updating the candidate set as
it goes  taking into account our assumptions of single use  connectedness  and exclusivity 
the basic scheme for choosing entries from the candidate set is to maximize the prediction
of meanings given phrases  but also to find general meanings  this adds a tension between
lics  which cover many vertices  and generality  which biases towards fewer vertices  however  generality  like lics  helps lead to a small lexicon since a general meaning will more
likely apply widely across a corpus 
   the code is available upon request from the first author 
   though  of course  interpretation functions are not the only way to guarantee a covering lexicon  see
siskind        for an alternative 

  

fithompson   mooney

answer  

 

 

 

s

 

state  

eq  

 

 
s

 

c

cityid  

loc  
 
 
c

s

 
texarkana
figure    tree with variables
let us explain the algorithm in further detail by way of an example  using spanish
instead of english to illustrate the difficulty somewhat more clearly  consider the following
corpus 
    cual es el capital del estado con la poblacion mas grande 
answer c   capital s c   largest p   state s   population s p      
    cual es la punta mas alta del estado con la area mas grande 
answer p   high point s p   largest a   state s   area s a      
    en que estado se encuentra texarkana 
answer s   state s   eq c cityid texarkana      loc c s    
    que capital es la mas grande 
answer a  largest a  capital a    
    que es la area de los estados unitos 
answer a   area c a   eq c countryid usa     
    cual es la poblacion de un estado que bordean a utah 
answer p   population s p   state s   next to s m   eq m stateid utah     
    que es la punta mas alta del estado con la capital madison 
answer c   high point b c   loc c b   state b  
capital b a   eq a cityid madison       

the sentence representations here are slightly different than the tree representations given in
the problem definition  with the main difference being the addition of existentially quantified
variables shared between some leaves of a representation tree  as mentioned in section     
the representations are prolog queries to a database  given such a query  we can create
a tree that conforms to our formalism  but with this addition of quantified variables  an
example is shown in figure   for the representation of the third sentence  each vertex is
a predicate name and its arity  in the prolog style  e g   state    with quantified variables
at some of the leaves  for each outgoing edge  n  m  of a vertex n  the edge is labeled with
the argument position filled by the subtree rooted by m  if there is not an edge labeled
with a given argument position  the argument is a free variable  each vertex labeled with a
  

fiacquiring word meaning mappings

variable  which can occur only at leaves  is an existentially quantified variable whose scope
is the entire tree  or query   the learned lexicon  however  does not need to maintain the
identity between variables across distinct lexical entries 
another representation difference is that we will strip the answer predicate from the
input to our learner   thus allowing a forest of directed trees as input rather than a single
tree  the definition of the problem easily extends such that the root of each tree in the
forest must be in the domain of some interpretation function 
evaluation of our system using this representation is given in section      evaluation
using a representation without variables or forests is presented in section      we previously
 thompson        presented results demonstrating learning representations of a different
form  that of a case role representation  fillmore        augmented with conceptual dependency  schank        information  this last representation conforms directly to our
problem definition 
now  continuing with the example of solving the lexicon acquisition problem for this
corpus  let us also assume for simplification  although not required  that sentences are
stripped of phrases that we know have empty meanings  e g   que  es  con  and la  
we will similarly assume that it is known that some phrases refer directly to given database
constants  e g   location names   and remove those phrases and their meaning from the
training input 
      candidate generation phase
initial candidate meanings for a phrase are produced by computing the maximally common
substructure s  between sampled pairs of representations of sentences that contain it  we
derive common substructure by computing the largest isomorphic connected subgraphs
 lics  of two labeled trees  taking labels into account in the isomorphism  the analogous
largest common subgraph problem  garey   johnson        is solvable in polynomial
time if  as we assume  both inputs are trees and if k  the number of edges to include  is
given  thus  we start with k set equal to the largest number of edges in the two trees being
compared  test for common subgraph s   and iterate down to k      stopping when one or
more subgraphs are found for a given k 
for the prolog query representation  the algorithm is complicated a bit by variables 
therefore  we use lics with an addition similar to computing the least general generalization of first order clauses  plotkin         the lgg of two sets of literals is the least
general set of literals that subsumes both sets of literals  we add to this by allowing that
when a term in the argument of a literal is a conjunction  the algorithm tries all orderings
in its matching of the terms in the conjunction  overall  our algorithm for finding the lics
between two trees in the prolog representation first finds the common labeled edges and
vertices as usual in lics  but treats all variables as equivalent  then  it computes the
least general generalization  with conjunction taken into account  of the resulting trees as
converted back into literals  for example  given the two trees 

   the predicate is omitted because chill initializes the parse stack with the answer predicate  and thus
no word has to be mapped to it 

  

fithompson   mooney

phrase
capital 

grande 
estado 

punta mas 
encuentra 

lics
largest     
capital     
state   
largest   state    
largest     
largest   state    
state   
 population s     state s  
capital     
high point     
 state s   loc   s  
high point     
state   
 state s   loc   s  

from sentences
   
   
   
   
        
   
                                 
   
   
   
   
   
   
 

table    sample candidate lexical entries and their derivation
answer c   largest p   state s   population s p     capital s c    
answer p   high point s p   largest a   state s   area s a       
the common meaning is answer   largest   state      note that the lics of two trees
may not be unique  there may be multiple common subtrees that both contain the same
number of edges  in this case lics returns multiple answers 
the sets of initial candidate meanings for some of the phrases in the sample corpus are
shown in table    while in this example we show the lics for all pairs that a phrase
appears in  in the actual algorithm we randomly sample a subset for efficiency reasons 
as in golem  muggleton   feng         for phrases appearing in only one sentence
 e g   encuentra   the entire sentence representation  excluding the database constant
given as background knowledge  is used as an initial candidate meaning  such candidates
are typically generalized in step     of the algorithm to only the correct portion of the
representation before they are added to the lexicon  we will see an example of this below 
      adding to the final lexicon
after deriving initial candidates  the greedy search begins  the heuristic used to evaluate
candidates attempts to help assure that a small but covering lexicon is learned  the heuristic
first looks at the weighted sum of two components  where p is the phrase and m its candidate
meaning 
   p  m   p   p  p   m   p  m    p  p   p  m   p  
   the generality of m
then  ties in this value are broken by preferring less ambiguous  those with fewer current
meanings  and shorter phrases  the first component is analogous the cluster evaluation
  

fiacquiring word meaning mappings

heuristic used by cobweb  fisher         which measures the utility of clusters based on
attribute value pairs and categories  instead of meanings and phrases  the probabilities
are estimated from the training data and then updated as learning progresses to account
for phrases and meanings already covered  we will see how this updating works as we
continue through our example of the algorithm  the goal of this part of the heuristic
is to maximize the probability of predicting the correct meaning for a randomly sampled
phrase  the equality holds by bayes theorem  looking at the right side  p  m   p   is
the expected probability that meaning m is correctly guessed for a given phrase  p  this
assumes a strategy of probability matching  in which a meaning m is chosen for p with
probability p  m   p  and correct with the same probability  the other term  p  p   biases
the component by how common the phrase is  interpreting the left side of the equation  the
first term biases towards lexicons with low ambiguity  the second towards low synonymy 
and the third towards frequent meanings 
the second component of the heuristic  generality  is computed as the negation of
the number of vertices in the meanings tree structure  and helps prefer smaller  more
general meanings  for example  in the candidate set above  if all else were equal  the
generality portion of the heuristic would prefer state     with generality value     over
largest   state     and  state s  loc   s    each with generality value     as the
meaning of estado  learning a meaning with fewer terms helps evenly distribute the
vertices in a sentences representation among the meanings of the phrases in that sentence 
and thus leads to a lexicon that is more likely to be correct  to see this  we note that some
pairs of words tend to frequently co occur  grande and estado in our example   and
so their joint representation  meaning  is likely to be in the set of candidate meanings for
both words  by preferring a more general meaning  we easily ignore these incorrect joint
meanings 
in this example and all experiments  we use a weight of    for the first component of
the heuristic  and a weight of   for the second  the first component has smaller absolute
values and is therefore given a higher weight  modulo this consideration  results are not
overly sensitive to the weights and automatically setting them using cross validation on the
training set  kohavi   john        had little effect on overall performance  in table   we
illustrate the calculation of the heuristic measure for some of the above fourteen pairs  and
its value for all  the calculation shows the sum of multiplying    by the first component of
the heuristic and multiplying   by the second component  the first component is simplified
as follows 
  p     m  p   
  m  p   
p  p   p  m   p    


 
t
  p   
 p 
where   p   is the number of times phrase p appears in the corpus  t is the initial number
of candidate phrases  and   m  p   is the number of times that meaning m is paired with
phrase p  we can ignore t since the number of phrases in the corpus is the same for each
pair  and has no effect on the ranking  the highest scoring pair is  estado  state      so
it is added to the lexicon 
next is the candidate generalization step        described algorithmically in figure   
one of the key ideas of the algorithm is that each phrase meaning choice can constrain the
candidate meanings of phrases yet to be learned  given the assumption that each portion of
the representation is due to at most one phrase in the sentence  exclusivity   once part of a
  

fithompson   mooney

candidate lexicon entry
 capital  largest       
 capital  capital       
 capital  state       
 grande  largest   state      
 grande  largest       
 estado  largest   state      
 estado  state     
 estado   population s     state s   
 estado  capital       
 estado  high point       
 estado   state s   loc   s    
 punta mas  high point       
 punta mas  state     
 encuentra   state s   loc   s    

heuristic value
                        
     
     
                       
  
                    
                     
 
 
 
 
  
                     
                    

table    heuristic value of sample candidate lexical entries

given  a learned phrase meaning pair  l  g 
for all sentence representation pairs containing l and g  mark them as covered 
for each candidate phrase meaning pair  p  m  
if p occurs in some training pairs with  l  g  then
if the vertices of m intersect the vertices of g then
if all occurrences of m are now covered then
remove  p  m  from the set of candidate pairs 
else
adjust the heuristic value of  p  m  as needed to account
for newly covered nodes of the training representations 
generalize m to remove covered nodes  obtaining m    and
calculate the heuristic value of the new candidate pair  p  m    
if no candidate meanings remain for an uncovered phrase then
derive new lics from uncovered representations and
calculate their heuristic values 

figure    the candidate generalization phase

  

fiacquiring word meaning mappings

representation is covered  no other phrase in the sentence can be paired with that meaning
 at least for that sentence   therefore  in step     the candidate meanings for words in
the same sentences as the word just learned are generalized to exclude the representation
just learned  we use an operation analogous to set difference when finding the remaining
uncovered vertices of the representation when generalizing meanings to eliminate covered
vertices from candidate pairs  for example  if the meaning largest      were learned
for a phrase in sentence    the meaning left behind would be a forest consisting of the
trees high point s    and  state s   area s      also  if the generalization results
in an empty tree  new lics are calculated  in our example  since state    is covered
in sentences             and    the candidates for several other words in those sentences
are generalized  for example  the meaning  state s   loc   s   for encuentra  is
generalized to loc       with a new heuristic value of                       also  our
single use assumption allows us to remove all candidate pairs containing estado from the
set of candidate meanings  since the learned pair covers all occurrences of estado in that
set 
note that the pairwise matchings to generate candidate items  together with this updating of the candidate set  enable multiple meanings to be learned for ambiguous phrases 
and makes the algorithm less sensitive to the initial rate of sampling for lics  for example 
note that capital is ambiguous in this data set  though its ambiguity is an artifact of
the way that the query language was designed  and one does not ordinarily think of it as
an ambiguous word  however  both meanings will be learned  the second pair added to
the final lexicon is  grande  largest        which causes a generalization to the empty
meaning for the first candidate entry in table    and since no new lics from sentence  
can be generated  its entire remaining meaning is added to the candidate meaning set for
both capital and mas 
subsequently  the greedy search continues until the resulting lexicon covers the training
corpus  or until no candidate phrase meanings remain  in rare cases  learning errors occur
that leave some portions of representations uncovered  in our example  the following lexicon
is learned 
 estado  state     
 grande  largest     
 area  area     
 punta  high point       
 poblacion  population       
 capital  capital       
 encuentra  loc       
 alta  loc       
 bordean  next to     
 capital  capital     
in the next section  we discuss the ability of wolfie to learn lexicons that are useful for
parsers and parser acquisition 

  

fithompson   mooney

   evaluation of wolfie
the following two sections discuss experiments testing wolfies success in learning lexicons
for both real and artificial corpora  comparing it in several cases to a previously developed
lexicon learning system 
    a database query application
this section describes our experimental results on a database query application  the first
corpus discussed contains     questions about u s  geography  paired with their prolog
query to extract the answer to the question from a database  this domain was originally
chosen due to the availability of a hand built natural language interface  geobase  to
a database containing about     facts  geobase was supplied with turbo prolog    
 borland international         and designed specifically for this domain  the questions in
the corpus were collected by asking undergraduate students to generate english questions for
this database  though they were given only cursory knowledge of the database without being
given a chance to use it  to broaden the test  we had the same     sentences translated into
spanish  turkish  and japanese  the japanese translations are in word segmented roman
orthography  translated questions were paired with the appropriate logical queries from
the english corpus 
to evaluate the learned lexicons  we measured their utility as background knowledge
for chill  this is performed by choosing a random set of    test examples and then
learning lexicons and parsers from increasingly larger subsets of the remaining     examples
 increasing by    examples each time   after training  the test examples are parsed using
the learned parser  we then submit the resulting queries to the database  compare the
answers to those generated by submitting the correct representation to the database  and
record the percentage of correct  matching  answers  by using the difficult gold standard
of retrieving a correct answer  we avoid measures of partial accuracy that we believe do not
adequately measure final utility  we repeated this process for ten different random training
and test sets and evaluated performance differences using a two tailed  paired t test with a
significance level of p       
we compared our system to an incremental  on line  lexicon learner developed by siskind
        to make a more equitable comparison to our batch algorithm  we ran his in a simulated batch mode  by repeatedly presenting the corpus     times  analogous to running
    epochs to train a neural network  while this does not actually add new kinds of data
over which to learn  it allows his algorithm to perform inter sentential inference in both directions over the corpus instead of just one  our point here is to compare accuracy over the
same size training corpus  a metric not optimized for by siskind  we are not worried about
the difference in execution time here    and the lexicons learned when running siskinds
system in incremental mode  presenting the corpus a single time  resulted in substantially
lower performance in preliminary experiments with this data  we also removed wolfies
ability to learn phrases of more than one word  since the current version of siskinds system
    the cpu times of the two system are not directly comparable since one is written in prolog and the
other in lisp  however  the learning time of the two systems is approximately the same if siskinds
system is run in incremental mode  just a few seconds with     training examples 

  

fiacquiring word meaning mappings

  

  

  

accuracy

  

  

  

  

chill handbuilt
chill testlex
chill wolfie
chill siskind
geobase

  

  

 
 

  

   
   
training examples

   

   

figure    accuracy on english geography corpus
does not have this ability  finally  we made comparisons to the parsers learned by chill
when using a hand coded lexicon as background knowledge 
in this and similar applications  there are many terms  such as state and city names 
whose meanings can be automatically extracted from the database  therefore  all tests
below were run with such names given to the learner as an initial lexicon  this is helpful
but not required  section     gives results for a different task with no such initial lexicon 
however  unless otherwise noted  for all tests within this section       we did not strip
sentences of phrases known to have empty meanings  unlike in the example of section   
      comparisons using english
the first experiment was a comparison on the original english corpus  figure   shows
learning curves for chill when using the lexicons learned by wolfie  chill wolfie  and
by siskinds system  chill siskind   the uppermost curve  chill handbuilt  shows
chills performance when given the hand built lexicon  chill testlex shows the performance when words that never appear in the training data  e g   are only in the test sentences 
are deleted from the hand built lexicon  since a learning algorithm has no chance of learning
these   finally  the horizontal line shows the performance of the geobase benchmark 
the results show that a lexicon learned by wolfie led to parsers that were almost as
accurate as those generated using a hand built lexicon  the best accuracy is achieved by
parsers using the hand built lexicon  followed by the hand built lexicon with words only in
the test set removed  followed by wolfie  followed by siskinds system  all the systems
do as well or better than geobase by the time they reach     training examples  the
differences between wolfie and siskinds system are statistically significant at all training
  

fithompson   mooney

lexicon
hand built
wolfie
siskind

coverage
    
    
     

ambiguity
   
   
   

entries
  
    
     

table    lexicon comparison
example sizes  these results show that wolfie can learn lexicons that support the learning
of successful parsers  and that are better from this perspective than those learned by a
competing system  also  comparing to the chill testlex curve  we see that most of the
drop in accuracy from a hand built lexicon is due to words in the test set that the system
has not seen during training  in fact  none of the differences between chill wolfie and
chill testlex are statistically significant 
one of the implicit hypotheses of our problem definition is that coverage of the training
data implies a good lexicon  the results show a coverage of      of the     training examples for wolfie versus       for siskind  in addition  the lexicons learned by siskinds
system were more ambiguous and larger than those learned by wolfie  wolfies lexicons had an average of     meanings per word  and an average size of      entries  after
    training examples  versus     meanings per word and       entries in siskinds lexicons  for comparison  the hand built lexicon had     meanings per word and    entries 
these differences  summarized in table    undoubtedly contribute to the final performance
differences 
      performance for other natural languages
next  we examined the performance of the two systems on the spanish version of the corpus 
figure    shows the results  the differences between using wolfie and siskinds learned
lexicons for chill are again statistically significant at all training set sizes  we also again
show the performance with hand built lexicons  both with and without phrases present only
in the testing set  the performance compared to the hand built lexicon with test set phrases
removed is still competitive  with the difference being significant only at     examples 
figure    shows the accuracy of learned parsers with wolfies learned lexicons for
all four languages  the performance differences among the four languages are quite small 
demonstrating that our methods are not language dependent 
      a larger corpus
next  we present results on a larger  more diverse corpus from the geography domain 
where the additional sentences were collected from computer science undergraduates in
an introductory ai course  the set of questions in the smaller corpus was collected from
students in a german class  with no special instructions on the complexity of queries desired 
the ai students tended to ask more complex and diverse queries  their task was to give five
interesting questions and the associated logical form for a homework assignment  though
again they did not have direct access to the database  they were requested to give at least
one sentence whose representation included a predicate containing embedded predicates  for

  

fiacquiring word meaning mappings

   
  
  
  

accuracy

  
  
  
  

span chill handbuilt
span chill testlex
span chill wolfie
span chill siskind

  
  
 
 

  

   
   
training examples

   

   

   

   

figure     accuracy on spanish

   
  
  
  

accuracy

  
  
  
  

english
spanish
japanese
turkish

  
  
 
 

  

   
   
training examples

figure     accuracy on all four languages

  

fithompson   mooney

   
  
  
  

accuracy

  
  
  
  

chill
wolfie
geobase

  
  
 
 

  

   

   

   
   
training examples

   

   

   

   

figure     accuracy on the larger geography corpus
example largest s  state s    and we asked for variety in their sentences  there were
    new sentences  for a total of      including the original     sentences  
for these experiments  we split the data into     training sentences and    test sentences  for    random splits  then trained wolfie and then chill as before  our goal was
to see whether wolfie was still effective for this more difficult corpus  since there were
approximately    novel words in the new sentences  therefore  we tested against the performance of chill with an extended hand built lexicon  for this test  we stripped sentences
of phrases known to have empty meanings  as in the example of section      again  we
did not use phrases of more than one word  since these do not seem to make a significant
difference in this domain  for these results  we compare wolfies lexicons for chill using
hand built lexicons without phrases that only appear in the test set 
figure    shows the resulting learning curves  the differences between chill using
the hand built and learned lexicons are statistically significant at                and    
examples  four out of the nine data points   the more mixed results here indicate both the
difficulty of the domain and the more variable vocabulary  however  the improvement of
machine learning methods over the geobase hand built interface is much more dramatic
for this corpus 
      lics versus fracturing
one component of the algorithm not yet evaluated explicitly is the candidate generation
method  as mentioned in section      we could use fractures of representations of sentences
in which a phrase appears to generate the candidate meanings for that phrase  instead
of lics  we used this approach and compared it to the previously described method of
using the largest isomorphic connected subgraphs of sampled pairs of representations as

  

fiacquiring word meaning mappings

   
  
  
  

accuracy

  
  
  
  

fractwolfie
wolfie

  
  
 
 

  

   
   
training examples

   

   

figure     fracturing vs  lics  accuracy
candidate meanings  to attempt a more fair comparison  we also sampled representations
for fracturing  using the same number of source representations as the number of pairs
sampled for lics 
the accuracy of chill when using the resulting learned lexicons as background knowledge are shown in figure     using fracturing  fractwolfie  shows little or no advantage 
none of the differences between the two systems are statistically significant 
in addition  the number of initial candidate lexicon entries from which to choose is
much larger for fracturing than our lics method  as shown in figure     this is true even
though we sampled the same number of representations as pairs for lics  because there
are a larger number of fractures for an arbitrary representation than the number of lics
for an arbitrary pair  finally  wolfies learning time when using fracturing is greater than
that when using lics  as shown in figure     where the cpu time is shown in seconds 
in summary  these differences show the utility of lics as a method for generating
candidates  a more thorough method does not result in better performance  and also results
in longer learning times  one could claim that we are handicapping fracturing since we are
only sampling representations for fracturing  this may indeed help the accuracy  but the
learning time and the number of candidates would likely suffer even further  in a domain
with larger representations  the differences in learning time would be even more dramatic 
    artificial data
the previous section showed that wolfie successfully learns lexicons for a natural corpus
and a realistic task  however  this demonstrates success on only a relatively small corpus
and with one representation formalism  we now show that our algorithm scales up well
with more lexicon items to learn  more ambiguity  and more synonymy  these factors are

  

fithompson   mooney

   

number of candidates

   

   

   

fractwolfie
wolfie

   

   

 
 

  

   
   
training examples

   

   

figure     fracturing vs  lics  number of candidates

 

   

learning time  sec 

 

   

 

   

 

   

fractwolfie
wolfie

 
 

  

   
   
training examples

   

figure     fracturing vs  lics  learning time

  

   

fiacquiring word meaning mappings

difficult to control when using real data as input  also  there are no large corpora available
that are annotated with semantic parses  we therefore present experimental results on an
artificial corpus  in this corpus  both the sentences and their representations are completely
artificial  and the sentence representation is a variable free representation  as suggested by
the work of jackendoff        and others 
for each corpus discussed below  a random lexicon mapping words to simulated meanings
was first constructed    this original lexicon was then used to generate a corpus of random
utterances each paired with a meaning representation  after using this corpus as input to
wolfie     the learned lexicon was compared to the original lexicon  and weighted precision
and weighted recall of the learned lexicon were measured  precision measures the percentage
of the lexicon entries  i e   word meaning pairs  that the system learns that are correct 
recall measures the percentage of the lexicon entries in the hand built lexicon that are
correctly learned by the system 
precision  
recall  

  correct pairs
  pairs learned

  correct pairs
 
  pairs in hand built lexicon

to get weighted precision and recall measures  we then weight the results for each pair
by the words frequency in the entire corpus  not just the training corpus   this models
how likely we are to have learned the correct meaning for an arbitrarily chosen word in the
corpus 
we generated several lexicons and associated corpora  varying the ambiguity rate  number of meanings per word  and synonymy rate  number of words per meaning   as in siskind
        meaning representations were generated using a set of conceptual symbols that
combined to form the meaning for each word  the number of conceptual symbols used in
each lexicon will be noted when we describe each corpus below  in each lexicon       
of the senses were variable free to simulate noun like meanings  and       contained from
one to three variables to denote open argument positions to simulate verb like meanings 
the remainder of the words  the remaining     had the empty meaning to simulate function words  in addition  the functors in each meaning could have a depth of up to two
and an arity of up to two  an example noun like meaning is f   f  f      and a verbmeaning f   a f   b    the conceptual symbols in this example are f    f   f    f   
and f    by using these multi level meaning representations we demonstrate the learning
of more complex representations than those in the geography database domain  none of the
hand built meanings for phrases in that lexicon had functors embedded in arguments  we
used a grammar to generate utterances and their meanings from each original lexicon  with
terminal categories selected using a distribution based on zipfs law  zipf         under
zipfs law  the occurrence frequency of a word is inversely proportional to its ranking by
occurrence 
we started with a baseline corpus generated from a lexicon of     words using    conceptual symbols and no ambiguity or synonymy       sentence meaning pairs were generated 
    thanks to jeff siskind for the initial corpus generation software  which we enhanced for these tests 
    in these tests  we allowed wolfie to learn phrases of up to length two 

  

fithompson   mooney

   
  
  
  

accuracy

  
  
  
  

precision
recall

  
  
 
 

   

   

   

   
    
training examples

    

    

    

    

figure     baseline artificial corpus
we split this into five training sets of      sentences each  figure    shows the weighted
precision and recall curves for this initial test  this demonstrates good scalability to a
slightly larger corpus and lexicon than that of the u s  geography query domain 
a second corpus was generated from a second lexicon  also of     words using    conceptual symbols  but increasing the ambiguity to      meanings per word  this time       pairs
were generated and the corpus split into five sets of      training examples each  weighted
precision at      examples drops to       from the previous level of        and weighted
recall to       from        the full learning curve is shown in figure     a quick comparison to siskinds performance on this corpus confirmed that his system achieved comparable
performance  showing that with current methods  this is close to the best performance that
we are able to obtain on this more difficult corpus  one possible explanation for the smaller
performance difference between the two systems on this corpus versus the geography domain is that in this domain  the correct meaning for a word is not necessarily the most
general  in terms of number of vertices  of all its candidate meanings  therefore  the
generality portion of the heuristic may negatively influence the performance of wolfie in
this domain 
finally  we show the change in performance with increasing ambiguity and increasing
synonymy  holding the number of words and conceptual symbols constant  figure    shows
the weighted precision and recall with      training examples for increasing levels of ambiguity  holding the synonymy level constant  figure    shows the results at increasing
levels of synonymy  holding ambiguity constant  increasing the level of synonymy does not
effect the results as much as increasing the level of ambiguity  which is as we expected 
holding the corpus size constant but increasing the number of competing meanings for a
word increases the number of candidate meanings created by wolfie while decreasing the
amount of evidence available for each meaning  e g   the first component of the heuristic
  

fiacquiring word meaning mappings

  

  

accuracy

  

  

  

precision
recall

  

  

 
 

   

   

   

   
    
training examples

    

    

    

    

figure     a more ambiguous artificial corpus
   

  

  

recall
precision

accuracy

  

  

  

  

  

  
 

    

   
number of meanings per word

    

 

figure     increasing the level of ambiguity
measure  and makes the learning task more difficult  on the other hand  increasing the
level of synonymy does not have the potential to mislead the learner 
the number of training examples required to reach a certain level of accuracy is also
informative  in table    we show the point at which a standard precision of     was first

  

fithompson   mooney

   

accuracy

  

  

recall
precision

  

  
 

    

   
number of words per meaning

    

 

figure     increasing the level of synonymy
ambiguity level
   
    
   

number of examples
   
   
    

table    number of examples to reach     precision
reached for each level of ambiguity  note  however  that we only measured accuracy after
each set of     training examples  so the numbers in the table are approximate 
we performed a second test of scalability on two corpora generated from lexicons an
order of magnitude larger than those in the above tests  in these tests  we use a lexicon
containing      words and using     conceptual symbols  we generated both a corpus with
no ambiguity  and one from a lexicon with ambiguity and synonymy similar to that found
in the wordnet database  beckwith  fellbaum  gross    miller         the ambiguity there
is approximately      meanings per word and the synonymy     words per meaning  these
corpora contained       no ambiguity  and      examples  respectively  and we split the
data into five sets of      training examples each  for the easier large corpus  the maximum
average of weighted precision and recall was        at      training examples  while for the
harder corpus  the maximum average was       at      training examples 

   active learning
as indicated in the previous sections  we have built an integrated system for language
acquisition that is flexible and useful  however  a major difficulty remains  the construction
of training corpora  though annotating sentences is still arguably less work than building
  

fiacquiring word meaning mappings

apply the learner to n bootstrap examples  creating a classifier 
until no examples remain or the annotator is unwilling to label more examples  do 
use most recently learned classifier to annotate each unlabeled instance 
find the k instances with the lowest annotation certainty 
annotate these instances 
train the learner on the bootstrap examples and all examples annotated so far 

figure     selective sampling algorithm
an entire system by hand  the annotation task is also time consuming and error prone 
further  the training pairs often contain redundant information  we would like to minimize
the amount of annotation required while still maintaining good generalization accuracy 
to do this  we turned to methods in active learning  active learning is a research area
in machine learning that features systems that automatically select the most informative
examples for annotation and training  angluin        seung  opper    sompolinsky        
rather than relying on a benevolent teacher or random sampling  the primary goal of
active learning is to reduce the number of examples that the system is trained on  while
maintaining the accuracy of the acquired information  active learning systems may construct their own examples  request certain types of examples  or determine which of a set
of unsupervised examples are most usefully labeled  the last approach  selective sampling
 cohn et al          is particularly attractive in natural language learning  since there is an
abundance of text  and we would like to annotate only the most informative sentences  for
many language learning tasks  annotation is particularly time consuming since it requires
specifying a complex output rather than just a category label  so reducing the number of
training examples required can greatly increase the utility of learning 
in this section  we explore the use of active learning  specifically selective sampling  for
lexicon acquisition  and demonstrate that with active learning  fewer examples are required
to achieve the same accuracy obtained by training on randomly chosen examples 
the basic algorithm for selective sampling is relatively simple  learning begins with a
small pool of annotated examples and a large pool of unannotated examples  and the learner
attempts to choose the most informative additional examples for annotation  existing work
in the area has emphasized two approaches  certainty based methods  lewis   catlett 
       and committee based methods  mccallum   nigam        freund  seung  shamir 
  tishby        liere   tadepalli        dagan   engelson        cohn et al          we
focus here on the former 
in the certainty based paradigm  a system is trained on a small number of annotated
examples to learn an initial classifier  next  the system examines unannotated examples 
and attaches certainties to the predicted annotation of those examples  the k examples with
the lowest certainties are then presented to the user for annotation and retraining  many
methods for attaching certainties have been used  but they typically attempt to estimate
the probability that a classifier consistent with the prior training data will classify a new
example correctly 

  

fithompson   mooney

learn a lexicon with the examples annotated so far
   for each phrase in an unannotated sentence 
if it has entries in the learned lexicon then
its certainty is the average of the heuristic values of those entries
else  if it is a one word phrase then
its certainty is zero
   to rank sentences use 
total certainty of phrases from step  
  of phrases counted in step  

figure     active learning for wolfie
figure    presents abstract pseudocode for certainty based selective sampling  in an
ideal situation  the batch size  k  would be set to one to make the most intelligent decisions
in future choices  but for efficiency reasons in retraining batch learning algorithms  it is
frequently set higher  results on a number of classification tasks have demonstrated that
this general approach is effective in reducing the need for labeled examples  see citations
above  
applying certainty based sample selection to wolfie requires determining the certainty
of a complete annotation of a potential new training example  despite the fact that individual
learned lexical entries and parsing operators perform only part of the overall annotation task 
therefore  our general approach is to compute certainties for pieces of an example  in our
case  phrases  and combine these to obtain an overall certainty for an example  since lexicon
entries contain no explicit uncertainty parameters  we used wolfies heuristic measure to
estimate uncertainty 
to choose the sentences to be annotated in each round  we first bootstrapped an initial
lexicon from a small corpus  keeping track of the heuristic values of the learned items 
then  for each unannotated sentence  we took an average of the heuristic values of the
lexicon entries learned for phrases in that sentence  giving a value of zero to unknown
words but eliminating from consideration any words that we assume are known in advance 
such as database constants  thus  longer sentences with only a few known phrases would
have a lower certainty than shorter sentences with the same number of known phrases  this
is desirable since longer sentences will be more informative from a lexicon learning point
of view  the sentences with the lowest values were chosen for annotation  added to the
bootstrap corpus  and a new lexicon learned  our technique is summarized in figure    
to evaluate our technique  we compared active learning to learning from randomly selected examples  again measuring the effectiveness of learned lexicons as background knowledge for chill  we again used the  smaller  u s  geography corpus  as in the original
wolfie tests  using the lexicons as background knowledge during parser acquisition  and
using the same examples for parser acquisition  
for each trial in the following experiments  we first randomly divide the data into a
training and test set  then  n      bootstrap examples are randomly selected from the

  

fiacquiring word meaning mappings

   

  

accuracy

  

  
wolf active
wolfie
geobase
  

 
 

  

   
   
training examples

   

   

figure     using lexicon certainty for active learning
training examples and in each step of active learning  the least certain k      examples
of the remaining training examples are selected and added to the training set  the result
of learning on this set is evaluated after each step  the accuracy of the resulting learned
parsers was compared to the accuracy of those learned using randomly chosen examples to
learn lexicons and parsers  as in section    in other words  we can think of the k examples
in each round as being chosen randomly 
figure    shows the accuracy on unseen data of parsers learned using the lexicons learned
by wolfie when examples are chosen randomly and actively  there is an annotation
savings of around    examples by using active learning  the maximum accuracy is reached
after     examples  versus     with random examples  the advantage of using active
learning is clear from the beginning  though the differences between the two curves are only
statistically significant at     training examples  since we are learning both lexicons and
parsers  but only choosing examples based on wolfies certainty measures  the boost could
be improved even further if chill had a say in the examples chosen  see thompson  califf 
and mooney        for a description of active learning for chill 

   related work
in this section  we divide the previous research on related topics into the areas of lexicon
acquisition and active learning 
    lexicon acquisition
work on automated lexicon and language acquisition dates back to siklossy         who
demonstrated a system that learned transformation patterns from logic back to natural

  

fithompson   mooney

language  as already noted  the most closely related work is that of jeff siskind  which we
described briefly in section   and whose system we ran comparisons to in section    our
definition of the learning problem can be compared to his mapping problem  siskind 
       that formulation differs from ours in several respects  first  his sentence representations are terms instead of trees  however  as shown in figure    terms can also be
represented as trees that conform to our formalism with some minor additions  next  his
notion of interpretation does involve a type of tree  but carries the entire representation of
a sentence up to the root  also  it is not clear how he would handle quantified variables
in the representation of sentences  skolemization is possible  but then generalization across
sentences would require special handling  we make the single use assumption and he does
not  another difference is our bias towards a minimal number of lexicon entries  while he
attempts to find a monosemous lexicon  his later work  siskind        relaxes this to allow
ambiguity and noise  but still biases towards minimizing ambiguity  however  his formal
definition does not explicitly allow lexical ambiguity  but handles it in a heuristic manner 
this  though  may lead to more robustness than our method in the face of noise  finally 
our definition allows phrasal lexicon entries 
siskinds work on this topic has explored many different variations along a continuum of
using many constraints but requiring more time to incorporate each new example  siskind 
       versus few constraints but requiring more training data  siskind         thus  perhaps his earlier systems would have been able to learn the lexicons of section   more
quickly  but crucially those systems did not allow lexical ambiguity  and thus also may
not have learned as accurate a lexicon  more detailed comparisons to such versions of the
system are outside the scope of this paper  our goal with wolfie is to learn a possibly
ambiguous lexicon from as few examples as possible  and we thus made comparisons along
this dimension alone 
siskinds approach  like ours  takes into account constraints between word meanings
that are justified by the exclusivity and compositionality assumptions  his approach is
somewhat more general in that it handles noise and referential uncertainty  uncertainty
about the meaning of a sentence and thus multiple possible candidates   while ours is
specialized for applications where the meaning  or meanings  is known  the experimental
results in section   demonstrate the advantage of our method for such an application  he has
demonstrated his system to be capable of learning reasonably accurate lexicons from large 
ambiguous  and noisy artificial corpora  but this accuracy is only assured if the learning
algorithm converges  which did not occur for our smaller corpus in the experiments we ran 
also  as already noted  his system operates in an incremental or on line fashion  discarding
each sentence as it processes it  while ours is batch  in addition  his search for word meanings
proceeds in two stages  as discussed in section      by using common substructures  we
combine these two stages in wolfie  both systems do have greedy aspects  ours in the
choice of the next best lexical entry  his in the choice to discard utterances as noise or create
a homonymous lexical entry  finally  his system does not compute statistical correlations
between words and their possible meanings  while ours does 
besides siskinds work  there are others who approach the problem from a cognitive
perspective  for example  de marcken        also uses child language learning as a motivation  but approaches the segmentation problem instead of the learning of semantics 
for training input  he uses a flat list of tokens for semantic representations  but does not
  

fiacquiring word meaning mappings

segment sentences into words  he uses a variant of expectation maximization  dempster 
laird    rubin         together with a form of parsing and dictionary matching techniques 
to segment the sentences and associate the segments with their most likely meaning  on
the childes corpus  the algorithm achieves very high precision  but recall is not provided 
others taking the cognitive approach demonstrate language understanding by the ability
to carry out some task such as parsing  for example  nenov and dyer        describe a
neural network model to map between visual and verbal motor commands  and colunga
and gasser        use neural network modeling techniques for learning spatial concepts 
feldman and his colleagues at berkeley  feldman  lakoff    shastri        are actively
pursuing cognitive models of the acquisition of semantic concepts  another berkeley effort 
the system by regier        is given examples of pictures paired with natural language
descriptions that apply to the picture  and learns to judge whether a new sentence is true
of a given picture 
similar work by suppes  liang  and bottner        uses robots to demonstrate lexicon learning  a robot is trained on cognitive and perceptual concepts and their associated
actions  and learns to execute simple commands  along similar lines  tishby and gorin
       have a system that learns associations between words and actions  but they use a
statistical framework to learn these associations  and do not handle structured representations  similarly  oates  eyler walker  and cohen        discuss the acquisition of lexical
hierarchies and their associated meaning as defined by the sensory environment of a robot 
the problem of automatic construction of translation lexicons  smadja  mckeown   
hatzivassiloglou        melamed        wu   xia        kumano   hirakawa        catizone  russell    warwick        gale   church        brown   et al         has a definition
similar to our own  while most of these methods also compute association scores between
pairs  in their case  word word pairs  and use a greedy algorithm to choose the best translation s  for each word  they do not take advantage of the constraints between pairs  one
exception is melamed         however  his approach does not allow for phrases in the lexicon or for synonymy within one text segment  while ours does  also  yamazaki  pazzani 
and merz        learn both translation rules and semantic hierarchies from parsed parallel
sentences in japanese and english  of course  the main difference between this body of
work and this paper is that we map words to semantic structures  not to other words 
as mentioned in the introduction  there is also a large body of work on learning lexical
semantics but using different problem formulations than our own  for example  collins and
singer         riloff and jones         roark and charniak         and schneider       
define semantic lexicons as a grouping of words into semantic categories  and in the latter
case  add relational information  the result is typically applied as a semantic lexicon for
information extraction or entity tagging  pedersen and chen        describe a method
for acquiring syntactic and semantic features of an unknown word  assuming access to an
initial concept hierarchy  but they give no experimental results  many systems  fukumoto  
tsujii        haruno        johnston  boguraev    pustejovsky        webster   marcus 
      focus only on acquisition of verbs or nouns  rather than all types of words  also  the
authors just named either do not experimentally evaluate their systems  or do not show the
usefulness of the learned lexicons for a specific application 
several authors  rooth  riezler  prescher  carroll    beil        collins        ribas 
      manning        resnik        brent        discuss the acquisition of subcategoriza  

fithompson   mooney

tion information for verbs  and others describe work on learning selectional restrictions
 manning        brent         both of these are different from the information required
for mapping to semantic representation  but could be useful as a source of information to
further constrain the search  li        further expands on the subcategorization work by
inducing clustering information  finally  several systems  knight        hastings       
russell        learn new words from context  assuming that a large initial lexicon and
parsing system are already available 
another related body of work is grammar acquisition  especially those areas that tightly
integrate the grammar with a lexicon  such as with categorial grammars  retore   bonato 
      dudau sofronie  tellier    tommasi        watkinson   manandhar         the
theory of categorial grammar also has ties with lexical semantics  but these semantics
have not often been used for inference in support of high level tasks such as database
retrieval  while learning syntax and semantics together is arguably a more difficult task 
the aforementioned work has not been evaluated on large corpora  presumably primarily
due to the difficulty of annotation 
    active learning
with respect to additional active learning techniques  cohn et al         were among the
first to discuss certainty based active learning methods in detail  they focus on a neural
network approach to active learning in a version space of concepts  only a few of the
researchers applying machine learning to natural language processing have utilized active
learning  hwa        schohn   cohn        tong   koller        thompson et al        
argamon engelson   dagan        liere   tadepalli        lewis   catlett         and
the majority of these have addressed classification tasks such as part of speech tagging
and text categorization  for example  liere and tadepalli        apply active learning
with committees to the problem of text categorization  they show improvements with
active learning similar to those that we obtain  but use a committee of winnow based
learners on a traditional classification task  argamon engelson and dagan        also
apply committee based learning to part of speech tagging  in their work  a committee of
hidden markov models is used to select examples for annotation  lewis and catlett       
use heterogeneous certainty based methods  in which a simple classifier is used to select
examples that are then annotated and presented to a more powerful classifier 
however  many language learning tasks require annotating natural language text with
a complex output  such as a parse tree  semantic representation  or filled template  the
application of active learning to tasks requiring such complex outputs has not been well
studied  the exceptions being hwa         soderland         thompson et al          the
latter two include work on active learning applied to information extraction  and thompson
et al         includes work on active learning for semantic parsing  hwa        describes
an interesting method for evaluating a statistical parsers uncertainty  when applied for
syntactic parsing 

   future work
although wolfies current greedy search method has performed quite well  a better search
heuristic or alternative search strategy could result in improvements  we should also more
  

fiacquiring word meaning mappings

thoroughly evaluate wolfies ability to learn long phrases  as we restricted this ability in
the evaluations here  another issue is robustness in the face of noise  the current algorithm
is not guaranteed to learn a correct lexicon in even a noise free corpus  the addition of noise
complicates an analysis of circumstances in which mistakes are likely to happen  further
theoretical and empirical analysis of these issues is warranted 
referential uncertainty could be handled  with an increase in complexity  by forming
lics from more pairs of representations with which a phrase appears  but not between
alternative representations of the same sentence  then  once a pair is added to the lexicon 
for each sentence containing that word  representations can be eliminated if they do not
contain the learned meaning  provided another representation does contain it  thus allowing
for lexical ambiguity   we plan to flesh this out and evaluate the results 
a different avenue of exploration is to apply wolfie to a corpus of sentences paired
with the more common query language  sql  such corpora should be easily constructible
by recording queries submitted to existing sql applications along with their english forms 
or translating existing lists of sql queries into english  presumably an easier direction to
translate   the fact that the same training data can be used to learn both a semantic
lexicon and a parser also helps limit the overall burden of constructing a complete natural
language interface 
with respect to active learning  experiments on additional corpora are needed to test
the ability of our approach to reduce annotation costs in a variety of domains  it would
also be interesting to explore active learning for other natural language processing problems
such as syntactic parsing  word sense disambiguation  and machine translation 
our current results have involved a certainty based approach  however  proponents of
committee based approaches have convincing arguments for their theoretical advantages 
our initial attempts at adapting committee based approaches to our systems were not very
successful  however  additional research on this topic is indicated  one critical problem is
obtaining diverse committees that properly sample the version space  cohn et al         

   conclusions
acquiring a semantic lexicon from a corpus of sentences labeled with representations of
their meaning is an important problem that has not been widely studied  we present both a
formalism of the learning problem and a greedy algorithm to find an approximate solution to
it  wolfie demonstrates that a fairly simple  greedy  symbolic learning algorithm performs
well on this task and obtains performance superior to a previous lexicon acquisition system
on a corpus of geography queries  our results also demonstrate that our methods extend
to a variety of natural languages besides english  and that they scale fairly well to larger 
more difficult corpora 
active learning is a new area of machine learning that has been almost exclusively
applied to classification tasks  we have demonstrated its successful application to more
complex natural language mappings from phrases to semantic meanings  supporting the
acquisition of lexicons and parsers  the wealth of unannotated natural language data 
along with the difficulty of annotating such data  make selective sampling a potentially
invaluable technique for natural language learning  our results on realistic corpora indicate
that example annotations savings as high as     can be achieved by employing active

  

fithompson   mooney

sample selection using only simple certainty measures for predictions on unannotated data 
improved sample selection methods and applications to other important language problems
hold the promise of continued progress in using machine learning to construct effective
natural language processing systems 
most experiments in corpus based natural language have presented results on some
subtask of natural language  and there are few results on whether the learned subsystems
can be successfully integrated to build a complete nlp system  the experiments presented
in this paper demonstrated how two learning systems  wolfie and chill  were successfully
integrated to learn a complete nlp system for parsing database queries into executable
logical form given only a single corpus of annotated queries  and further demonstrated the
potential of active learning to reduce the annotation effort for learning for nlp 

acknowledgments
we would like to thank jeff siskind for providing us with his software  and for all his help
in adapting it for use with our corpus  thanks also to agapito sustaita  esra erdem 
and marshall mayberry for their translation efforts  and to the three anonymous reviewers
for their comments which helped improve the paper  this research was supported by the
national science foundation under grants iri         and iri         

references
anderson  j  r          induction of augmented transition networks  cognitive science    
       
angluin  d          queries and concept learning  machine learning            
argamon engelson  s     dagan  i          committee based sample selection for probabilistic classifiers  journal of artificial intelligence research             
beckwith  r   fellbaum  c   gross  d     miller  g          wordnet  a lexical database
organized on psycholinguistic principles  in zernik  u   ed    lexical acquisition 
exploiting on line resources to build a lexicon  pp          lawrence erlbaum 
hillsdale  nj 
borland international         turbo prolog     reference guide  borland international 
scotts valley  ca 
brent  m          automatic acquisition of subcategorization frames from untagged text 
in proceedings of the   th annual meeting of the association for computational linguistics  acl      pp         
brown  p     et al          a statistical approach to machine translation  computational
linguistics               
catizone  r   russell  g     warwick  s          deriving translation data from bilingual
texts  in proceedings of the first international lexical acquisition workshop 
  

fiacquiring word meaning mappings

cohn  d   atlas  l     ladner  r          improving generalization with active learning 
machine learning                 
collins  m     singer  y          unsupervised models for named entity classification  in
proceedings of the conference on empirical methods in natural language processing
and very large corpora  emnlp vlc     university of maryland 
collins  m  j          three generative  lexicalised models for statistical parsing  in proceedings of the   th annual meeting of the association for computational linguistics
 acl      pp       
colunga  e     gasser  m          linguistic relativity and word acquisition  a computational approach  in proceedings of the twenty first annual conference of the
cognitive science society  pp         
dagan  i     engelson  s  p          committee based sampling for training probabilistic classifiers  in proceedings of the twelfth international conference on machine
learning  icml      pp         san francisco  ca  morgan kaufman 
de marcken  c          the acquisition of a lexicon from paired phoneme sequences and
semantic representations  in lecture notes in computer science  vol       pp       
springer verlag 
dempster  a   laird  n     rubin  d          maximum likelihood from incomplete data
via the em algorithm  journal of the royal statistical society b          
dudau sofronie  tellier    tommasi         learning categorial grammars from semantic
types  in proceedings of the   th amsterdam colloquium  pp       
feldman  j   lakoff  g     shastri  l          the neural theory of language project
http   www icsi berkeley edu ntl  international computer science institute  university
of california  berkeley  ca 
fillmore  c          the case for case  in bach  e     harms  r  t   eds    universals in
linguistic theory  holt  reinhart and winston  new york 
fillmore  c          the mechanisms of construction grammar  in axmaker  s   jaisser 
a     singmeister  h   eds    proceedings of the fourteenth annual meeting of the
berkeley linguistics society  pp       berkeley  ca 
fisher  d  h          knowledge acquisition via incremental conceptual clustering  machine
learning            
freund  y   seung  h  s   shamir  e     tishby  n          selective sampling using the
query by committee algorithm  machine learning             
fukumoto  f     tsujii  j          representation and acquisition of verbal polysemy  in
papers from the      aaai symposium on the representation and acquisition of
lexical knowledge  polysemy  ambiguity  and generativity  pp       stanford  ca 

  

fithompson   mooney

gale  w     church  k          identifying word correspondences in parallel texts  in
proceedings of the fourth darpa speech and natural language workshop 
garey  m     johnson  d          computers and intractability  a guide to the theory of
np completeness  freeman  new york  ny 
goldberg  a          constructions  a construction grammar approach to argument
structure  the university of chicago press 
grefenstette  g          sextant  extracting semantics from raw text  implementation
details  integrated computer aided engineering        
haas  j     jayaraman  b          from context free to definite clause grammars  a typetheoretic approach  journal of logic programming              
haruno  m          a case frame learning method for japanese polysemous verbs  in
papers from the      aaai symposium on the representation and acquisition of
lexical knowledge  polysemy  ambiguity  and generativity  pp       stanford  ca 
hastings  p          implications of an automatic lexical acquisition mechanism  in
wermter  s   riloff  e     scheler  c   eds    connectionist  statistical  and symbolic approaches to learning for natural language processing  springer verlag  berlin 
hwa  r          on minimizing training corpus for parser acquisition  in proceedings of
the fifth computational natural language learning workshop 
jackendoff  r          semantic structures  the mit press  cambridge  ma 
johnston  m   boguraev  b     pustejovsky  j          the acquisition and interpretation of
complex nominals  in papers from the      aaai symposium on the representation
and acquisition of lexical knowledge  polysemy  ambiguity  and generativity  pp 
     stanford  ca 
knight  k          learning word meanings by instruction  in proceedings of the thirteenth
national conference on artificial intelligence  aaai      pp         portland  or 
kohavi  r     john  g          automatic parameter selection by minimizing estimated
error  in proceedings of the twelfth international conference on machine learning
 icml      pp         tahoe city  ca 
kumano  a     hirakawa  h          building an mt dictionary from parallel texts based
on linguistic and statistical information  in proceedings of the fifteenth international
conference on computational linguistics  pp       
lavrac  n     dzeroski  s          inductive logic programming  techniques and applications  ellis horwood 
lewis  d  d     catlett  j          heterogeneous uncertainty sampling for supervised
learning  in proceedings of the eleventh international conference on machine learning  icml      pp         san francisco  ca  morgan kaufman 
  

fiacquiring word meaning mappings

li  h          a probabilistic approach to lexical semantic knowledge acquisition and structural disambiguation  ph d  thesis  university of tokyo 
liere  r     tadepalli  p          active learning with committees for text categorization  in
proceedings of the fourteenth national conference on artificial intelligence  aaai     pp         providence  ri 
manning  c  d          automatic acquisition of a large subcategorization dictionary from
corpora  in proceedings of the   st annual meeting of the association for computational linguistics  acl      pp         columbus  oh 
mccallum  a  k     nigam  k          employing em and pool based active learning
for text classification  in proceedings of the fifteenth international conference on
machine learning  icml      pp         madison  wi  morgan kaufman 
melamed  i  d          automatic evaluation and uniform filter cascades for inducing n best
translation lexicons  in proceedings of the third workshop on very large corpora 
melamed  i  d          models of translational equivalence among words  computational
linguistics                 
muggleton  s   ed            inductive logic programming  academic press  new york 
ny 
muggleton  s          inverse entailment and progol  new generation computing journal 
           
muggleton  s     feng  c          efficient induction of logic programs  in proceedings of
the first conference on algorithmic learning theory tokyo  japan  ohmsha 
nenov  v  i     dyer  m  g          perceptually grounded language learning  part  
dete  a neural procedural model  connection science             
oates  t   eyler walker  z     cohen  p          using syntax to learn semantics  an
experiment in language acquisition with a mobile robot  tech  rep         university
of massachusetts  computer science department 
partee  b   meulen  a     wall  r          mathematical methods in linguistics  kluwer
academic publishers  dordrecht  the netherlands 
pedersen  t     chen  w          lexical acquisition via constraint solving  in papers
from the      aaai symposium on the representation and acquisition of lexical
knowledge  polysemy  ambiguity  and generativity  pp         stanford  ca 
plotkin  g  d          a note on inductive generalization  in meltzer  b     michie  d 
 eds    machine intelligence  vol      elsevier north holland  new york 
rayner  m   hugosson  a     hagert  g          using a logic grammar to learn a lexicon 
tech  rep  r       swedish institute of computer science 

  

fithompson   mooney

regier  t          the human semantic potential  spatial language and constrained connectionism  mit press 
resnik  p          selection and information  a class based approach to lexical relationships 
ph d  thesis  university of pennsylvania  cis department 
retore  c     bonato  r          learning rigid lambek grammars and minimalist grammars
from structured sentences  in proceedings of the third learning language in logic
workshop strasbourg  france 
ribas  f          an experiment on learning appropriate selectional restrictions from a
parsed corpus  in proceedings of the fifteenth international conference on computational linguistics  pp         
riloff  e     jones  r          learning dictionaries for information extraction by multilevel bootstrapping  in proceedings of the sixteenth national conference on artificial
intelligence  aaai      pp           orlando  fl 
roark  b     charniak  e          noun phrase co occurrence statistics for semi automatic
semantic lexicon construction  in proceedings of the   th annual meeting of the
association for computational linguistics and coling     acl coling      pp 
         
rooth  m   riezler  s   prescher  d   carroll  g     beil  f          inducing a semantically
annotated lexicon via em based clustering  in proceedings of the   th annual meeting
of the association for computational linguistics  pp         
russell  d          language acquisition in a unification based grammar processing system using a real world knowledge base  ph d  thesis  university of illinois  urbana 
il 
schank  r  c          conceptual information processing  north holland  oxford 
schneider  r          a lexically intensive algorithm for domain specific knowledge acquisition  in proceedings of the joint conference on new methods in language processing
and computational natural language learning  pp       
schohn  g     cohn  d          less is more  active learning with support vector machines  in proceedings of the seventeenth international conference on machine learning  icml        pp         stanford  ca 
sebillot  p   bouillon  p     fabre  c          inductive logic programming for corpus based
acquisition of semantic lexicons  in proceedings of  nd learning language in logic
 lll  workshop lisbon  portugal 
seung  h  s   opper  m     sompolinsky  h          query by committee  in proceedings
of the acm workshop on computational learning theory pittsburgh  pa 
siklossy  l          natural language learning by computer  in simon  h  a     siklossy 
l   eds    representation and meaning  experiments with information processsing
systems  prentice hall  englewood cliffs  nj 
  

fiacquiring word meaning mappings

siskind  j  m          learning word to meaning mappings  in broeder  p     murre  j 
 eds    models of language acquisition  inductive and deductive approaches  oxford
university press 
siskind  j  m          naive physics  event perception  lexical semantics and language
acquisition  ph d  thesis  department of electrical engineering and computer science  massachusetts institute of technology  cambridge  ma 
siskind  j  m          a computational study of cross situational techniques for learning
word to meaning mappings  cognition               
siskind  j  m          lexical acquisition as constraint satisfaction  tech  rep  ircs       
university of pennsylvania 
smadja  f   mckeown  k  r     hatzivassiloglou  v          translating collocations for
bilingual lexicons  a statistical approach  computational linguistics              
soderland  s          learning information extraction rules for semi structured and free
text  machine learning             
suppes  p   liang  l     bottner  m          complexity issues in robotic machine learning
of natural language  in lam  l     naroditsky  v   eds    modeling complex phenomena  proceedings of the  rd woodward conference  pp          springer verlag 
thompson  c  a   califf  m  e     mooney  r  j          active learning for natural language
parsing and information extraction  in proceedings of the sixteenth international
conference on machine learning  icml      pp         bled  slovenia 
thompson  c  a          acquisition of a lexicon from semantic representations of sentences 
in proceedings of the   rd annual meeting of the association for computational linguistics  acl      pp         cambridge  ma 
tishby  n     gorin  a          algebraic learning of statistical associations for language
acquisition  computer speech and language          
tomita  m          efficient parsing for natural language  kluwer academic publishers 
boston 
tong  s     koller  d          support vector machine active learning with applications
to text classification  in proceedings of the seventeenth international conference on
machine learning  icml        pp          stanford  ca 
watkinson  s     manandhar  s          unsupervised lexical learning with categorial
grammars using the lll corpus  in learning language in logic  lll  workshop bled 
slovenia 
webster  m     marcus  m          automatic acquisition of the lexical semantics of verbs
from sentence frames  in proceedings of the   th annual meeting of the association
for computational linguistics  acl      pp         

  

fithompson   mooney

wu  d     xia  x          large scale automatic extraction of an english chinese translation lexicon  machine translation                  
yamazaki  t   pazzani  m     merz  c          learning hierarchies from ambiguous natural
language data  in proceedings of the twelfth international conference on machine
learning  icml      pp         san francisco  ca  morgan kaufmann 
zelle  j  m          using inductive logic programming to automate the construction of
natural language parsers  ph d  thesis  department of computer sciences  university of texas  austin  tx  also appears as artificial intelligence laboratory technical
report ai        
zelle  j  m     mooney  r  j          learning to parse database queries using inductive
logic programming  in proceedings of the thirteenth national conference on artificial
intelligence  aaai      pp           portland  or 
zipf  g          human behavior and the principle of least effort  addison wesley  new
york  ny 

  

fi