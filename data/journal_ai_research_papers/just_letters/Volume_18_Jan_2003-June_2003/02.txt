journal of artificial intelligence research                 

submitted        published     

learning to order bdd variables in verification
orna grumberg
shlomi livne
shaul markovitch

orna cs technion ac il
slivne cs technion ac il
shaulm cs technion ac il

computer science department
technion   israel institute of technology
haifa        israel

abstract
the size and complexity of software and hardware systems have significantly increased
in the past years  as a result  it is harder to guarantee their correct behavior  one
of the most successful methods for automated verification of finite state systems is model
checking  most of the current model checking systems use binary decision diagrams  bdds 
for the representation of the tested model and in the verification process of its properties 
generally  bdds allow a canonical compact representation of a boolean function  given an
order of its variables   the more compact the bdd is  the better performance one gets
from the verifier  however  finding an optimal order for a bdd is an np complete problem 
therefore  several heuristic methods based on expert knowledge have been developed for
variable ordering 
we propose an alternative approach in which the variable ordering algorithm gains
ordering experience from training models and uses the learned knowledge for finding
good orders  our methodology is based on offline learning of pair precedence classifiers
from training models  that is  learning which variable pair permutation is more likely to
lead to a good order  for each training model  a number of training sequences are evaluated 
every training model variable pair permutation is then tagged based on its performance on
the evaluated orders  the tagged permutations are then passed through a feature extractor
and are given as examples to a classifier creation algorithm  given a model for which an
order is requested  the ordering algorithm consults each precedence classifier and constructs
a pair precedence table which is used to create the order 
our algorithm was integrated with smv  which is one of the most widely used verification systems  preliminary empirical evaluation of our methodology  using real benchmark
models  shows performance that is better than random ordering and is competitive with
existing algorithms that use expert knowledge  we believe that in sub domains of models
 alu  caches  etc   our system will prove even more valuable  this is because it features the
ability to learn sub domain knowledge  something that no other ordering algorithm does 

   introduction
the size and complexity of software and hardware systems have significantly increased in
the past years  as a result  it is harder to guarantee their correct behavior  thus  formal
methods  preferably computerized  are needed for this task 
one of the most successful methods for automated verification of finite state systems is
temporal logic model checking  clarke  emerson    sistla        queille   sifakis        
temporal logics are suitable formalisms for describing the behavior of a program over time 
a model checking procedure receives a finite state model of the system and a specification
c
    
ai access foundation and morgan kaufmann publishers  all rights reserved 

figrumberg  livne    markovitch

written as a temporal logic formula  it returns yes if the model satisfies the formula
 meaning that the system behaves according to the specification   otherwise  it returns
no  along with a counter example that demonstrates a bad behavior 
model checking has been very successful in finding subtle errors in various systems 
it is currently recognized by the hardware industry as an important component of the
development phase of new designs  however  model checking procedures often suffer from
high space requirements  needed for holding the transition relation and the intermediate
results 
one of the most promising solutions to this problem is the use of binary decision diagrams  bdds   akers        bryant        as the basic data structure in model checking 
bdds are canonical representations of boolean functions and are often very concise in size 
their conciseness also yields efficiency in computation time  since it is straightforward to
represent the transition relation and the intermediate results as boolean functions  bdds
are particularly suitable for model checking  today  existing industrial bdd based verifiers  such as ibms rulebase  beer  ben david  eisner    landver        and motorolas
verdict  kaufmann   pixley        are used by many companies in their development
infrastructure 
the size of a bdd for a given function is sensitive to the ordering of the variables in
the bdd  however  finding an optimal ordering  which yields a smallest bdd for a given
function  is an np complete problem  bollig   wegener         therefore  several heuristic
algorithms based on expert knowledge have been developed for variable ordering in the
hope of reducing the bdd size  unfortunately  and in spite of the resources invested  these
algorithms do not produce good enough variable orders  the reason for this may be that
only general rules are used and no domain specific knowledge is exploited 
the goal of this research is to develop learning techniques for acquiring and using
domain specific knowledge for variable ordering  we assume the availability of one or more
training models  the training models are used for off line acquisition of ordering experience
which can be used for ordering variables of a previously unseen model 
we first present a method for converting the ordering learning task into a concept
learning problem  the concept is the set of all ordered variable pairs that are in the right
order  the examples are ordered pairs of variables of a given training model  we show
a statistical method for tagging examples based on evaluated training orders and present
a set of variable pair features  the result is a standard concept learning problem  we
apply decision tree learning to generate a decision tree for each training model  when used
for an unseen model  we combine the trees and generate a partial order which is used for
generating the required order  we also present an extension of the algorithm which learns
context based precedence relations 
our algorithm was integrated with smv  mcmillan         which is the backbone of
many verification systems  empirical evaluation of our methodology  using real benchmark
models of hardware designs  shows performance that is much better than random ordering
and is competitive with existing algorithms that use expert knowledge 
section   contains background on model checking  section   presents our main algorithm
with empirical evaluation  section   shows the context based algorithm  our conclusions
are presented in section   
  

filearning to order bdd variables in verification

   background
model checking was introduced by clarke and emerson        and by queille and sifakis
       in the early     s  they presented algorithms that automatically reason about
temporal properties of finite state systems by exploring the state space  the use of binary
decision diagrams  bdds  to represent finite state systems and to perform symbolic state
traversal is called symbolic model checking  the use of bdds has greatly extended the
capacity of model checkers  models with       and more states are routinely being verified 
bdds were introduced by akers        as compact representations for boolean functions  bryant        proposed ordered binary decision diagrams  obdds  as canonical
representations of boolean functions  he also showed algorithms for computing boolean
operations efficiently on obdds 
the following subsection gives an overview of how finite state systems are represented
in symbolic model checking  bdds are then described and the variable ordering problem is
defined  existing algorithms for static variable ordering algorithms are reviewed  finally 
a brief description of machine learning algorithms used for ordering is given 
    finite state machines in symbolic model checking
finite state systems  fsm  can be described by defining the set of possible states in a
system and the transition relation between these states  a state typically describes values
of components  e g   latches in digital circuits   where each component is represented by
a state variable  let v    v    v       vn    be the set of variables in a system  let k vi be
the set of possible values for variable v i   then a state in the system can be described by
assigning values to all the variables in v   the set of all possible states s a is
sa   kv   kv        kvn   
a state can be written using a function that is true only in this state 
vn 
i  

 vi    cj   

where cj  kvi is the value of vi in the state  a set of states can be described by a function
as the disjunction of the functions that represent the states 
figure   shows a   bit counter  a state in the   bit counter can be described by a
tuple which gives an assignment to the   variables v     v    v    for example  the tuple h       i
represents the state with v       v       v       the corresponding boolean expression for
the state is  v          v          v        
in order to describe a system  we also need to specify its transition relation  the
transition relation describes all the possible transitions of each system state  it can thus be
described by pairs of states  hpresent state  next statei  where next state is a system state
after a transition from the present state  the variables in v will represent the present state
variables  and for each variable vi  v we will define a corresponding next state variable
vi   v     v   will denote the set of next state variables 
an example of a valid transition for the   bit counter is from h       i to h       i  the
boolean function which represents this transition is  v           v          v    
     v           v           v          the transition relation can be represented by a
  

figrumberg  livne    markovitch

v 

v 

v 

figure      bit counter
present state
v  v 
v 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

next state
v   v   v  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

table      bit counter transition relation table
boolean function which is the disjunction of the boolean functions of each of the transitions 
table   shows the transition relation for the   bit counter 
an alternative method for describing the transition relation is for each state variable
to define its valid next states  this form is known as the partitioned transition relation 
the transition relation is then described by a set of functions  instead of one   one for each
variable  for variable vi   a boolean function ti  v  vi    defines the next value of vi   vi    given
that the current state of the system is v  
for synchronous systems  in which there is a simultaneous transition of all the system
components  the transition relation is

vn 
i  

ti  v  vi    

in model checking it is common to use the partitioned transition relation form of representation  since it is usually more compact in memory requirements and thus allows the handling
of larger systems  for the   bit counter  the next state boolean functions are given below 
where  stands for the boolean operator xor 

t   v  v        v      v   
t   v  v        v       v   v    
t   v  v        v       v    v   v      
  

filearning to order bdd variables in verification

    binary decision diagrams
a binary decision diagram  bdd  is a dag  directed acyclic graph  representation of a
boolean function  a bdd is composed of two sink nodes and several non sink nodes  the
two sink nodes  labeled   and    represent the corresponding boolean values  each non sink
node is labeled with a boolean variable v and has two outgoing edges labeled    or then 
and    or else   each non sink node represents the boolean function corresponding to its  
edge if v      or the boolean function corresponding to its   edge if v     
an ordered binary decision diagram  obdd  is a bdd with the constraint that the
variables are ordered  and every root to sink path in the obdd visits the variables in
ascending order 
a reduced ordered binary decision diagram  robdd  is an obdd where each node
represents a distinct logic function  this representation is a canonical bdd representation
and the most compact representation possible for a given boolean function and a variable
ordering 
v 

v 

v 

v 

v 

v 

v 
v 
v 

 

 

 
 
 

 

 

 

 
 

 

 

 

 

 

 

 
 

 

 
 

 

 

 

 

 

 

 

 

v 
v 
v 

 
 

 

 

 

 
 

 

 

 

 a 

 b 

v 

v 

t 

v 

t 

v 

v 
v 
v 
v 

t 

v 

 

v 
v 
v 

t 
 
 
 

 

 

 

 

 

 

 

 

 
 

 

 
 

 

 
 

 

 

 

 

t 
t 
 
 
 

 c 

 

 

 

 

 

 

 

 d 

figure      bit counter transition relation  a   b  and partitioned transition relation  c   d 

figure    a   b  shows the obdd and robdd  respectively  representations of the
transition relation function for the   bit counter  the dashed lines are the   edges and the
solid lines are the   edges  robdds have only two leaf nodes  one with   and one with
   we drew them several times to enhance readability  robdds can also use complement
edges  which produces an even more compact representation  we did not use complement
edges  also for reasons of readability  figure    c   d  shows the obdd and robdd
representations of the partitioned transition relation of the   bit counter  the variable
order v    v     v    v     v    v   was used in all the representations  variable ordering algorithms
in model checking place the next state variable v i  adjacent to the present state variable v i  
  

figrumberg  livne    markovitch

for the rest of this document we will refer to robdds as bdds  unless we explicitly state
otherwise  
bollig and wegener        proved that finding an optimal variable ordering is an npcomplete problem  an order is optimal if it yields a bdd with the smallest number of
nodes  bryant        pointed out that variable ordering greatly influences the size of the
bdd  he showed that for a boolean function  one variable ordering may yield a bdd that
is exponential in the number of variables  while a different ordering may yield a bdd of
polynomial size 
v 

v 

v 

v 

v 

v 

v 

v 

 

 

 

 a 

 
 b 

figure    robdds for the function f  v   v   v   v      v    v     v    v  
figure   gives an example of the effect of variable ordering on the bdd size for the function f  v   v   v   v      v    v     v    v    in  a  the variable ordering is v   v   v   v 
and in  b  the variable ordering is v   v   v   v  
various algorithms have been developed for variable ordering  exact algorithms  ishiura 
sawada    yajima        drechsler  drechsler    slobodova        friedman   supowit 
      are algorithms that find the optimal order  these algorithms use a method similar
to dynamic programming with pruning to find the optimal order  due to the complexity
of the problem  exact algorithms are only practical for small cases  and one usually has to
turn to other heuristic methods  these heuristic methods can be roughly divided into two
groups 
   static ordering  aziz  tasiran    brayton        butler  ross    rohit kapur       
chung  hajj    patel        fujii  ootomo    hori        jain  adams    fujita       
fujita  fujisawa    kawato        malik  wang  brayton    sangiovanni vincentelli 
      touati  savoj  lin  brayton    sangiovanni vincetelli        which try to find
a good ordering before constructing the bdd  most of these algorithms are based on
the topological structure of the verified system 
   dynamic ordering  rudell        meinel   slobodova        bollig  lobbing    wegener        meinel   slobodova        meinel  somenzi    theobald        ishiura
et al         bern  meinel    slobodova        fujita  kukimoto    brayton       
mercer  kapur    ross        zhuang  benten    cheung        drechsler  becker 
  

filearning to order bdd variables in verification

  gockel        panda   somenzi        panda  somenzi    plessier         which
given a bdd with some variable order  reorder the variables in the hope of finding a
smaller bdd 
in model checking procedures  variable ordering is a central component  at the initial
phase of model checking  when the system is translated into a bdd representation  static
ordering is used  the order built at this stage greatly influences the memory usage during
the whole computation  however  since model checking keeps producing and eliminating
bdds  the variable order should be changed dynamically in order to effect the size of the
current bdds  dynamic ordering is used in order to achieve this goal  it is applied by the
model checking procedure whenever the size of the bdds reaches a certain threshold 
since our work introduces a static ordering algorithm based on machine learning  the
next subsection presents a review of the existing static algorithms  most of these algorithms
were developed for combinational circuits  i e   models whose outputs depend only on their
current inputs and not on inputs of previous cycles  and were described with hardware
terminology  in order to simplify the description  we will describe them with the terminology
we have used so far 
    static ordering
static ordering algorithms try to find an initial good order for the bdd  to do so  they
extract topological data from the model and use this data to determine an order  all the
algorithms convert the model  described by a set of next state functions  into a directed
graph known as the model connectivity graph  vertices in the graph are variables and
boolean operations  gates   a variable vertex represents a variable  while a gate vertex
represents a function  the edges ni  nj in the graph are between ni   which is either a
variable or gate vertex  and nj   which is a gate vertex  an edge ni  nj is placed if the
function represented by ni is an operand  i e   an immediate subfunction  of the function
represented by nj   we can divide the static algorithms into four groups that differ in the
way they use the graph information 
      graph search algorithms
the method suggested by malik et al         assigns to each vertex a level metric and orders
the variables in decreasing level value  the level of vertices with no out edges is set to be
zero and the level of every other vertex  v i   is set to be level vi     maxvj  vi vj  level vj      
this method resembles a bfs  breadth first search  which originates in nodes that have no
out edges  and progresses backwards in the model  fujita et al         proposed executing
a dfs  depth first search  from the vertices with no out edges  and progressing backwards 
variables in this algorithm are added in post order form 
the algorithms of malik et al  and fujita et al  were designed for cases where only one
function should be represented in a bdd  this is hardly ever the case in model checking 
butler et al         adapted the algorithm of fujita et al  to models with multiple starting
points  that is  multiple vertices with no out edges   their heuristic guides the algorithm
to select the first vertex as the vertex that represents the function which depends on the
maximum number of variables  this heuristic also guides the search to advance  backwards 
  

figrumberg  livne    markovitch

from an inner vertex to the vertex that leads to the maximum number of different variables 
a tie breaking heuristic  fujita  fujisawa    matsunaga        for the enhanced algorithm
advises selecting  in case of a tie  the vertex with the maximum number of out edges 
the dfs based methods append the variables to the variable order  another dfs based
algorithm relies on interleaving the variables in the order  fujii et al          the algorithm
adds a variable after the variable which precedes it in the dfs order 
      graph evaluation algorithms
graph evaluation algorithms use the model graph to evaluate the model variables and to
perform guided search based on their evaluation values  minato et al         propagate
values backward through the graph  starting from vertices with no out edges  whose value
is set to    in vertices of boolean operations  the values on the out edges are summed and
the value obtained is divided equally between the in edges  this is done recursively until a
vertex of a variable is reached  at variable vertices the propagated values are accumulated
as the variable evaluation value  the order is constructed by iteratively adding the variable
with the highest value  removing it from the graph  and updating the values 
chung et al        proposed two algorithm frameworks  the first framework is composed of two sweeps  in the first sweep each vertex is assigned a value  the values are set
by a propagating algorithm that starts from variable vertices with no in edges and advances
forward  by their out edges  to all the vertices in the graph  in the second sweep a guided
dfs initiated from vertices with no out edges is executed  this search is executed backward
in the graph and is guided by the maximal value  this means that the order of traversal
among vertex ancestors is according to their assigned value  a number of heuristics to
compute the values of the vertices were proposed 
   level based sets the value of variables with no input edges to be zero  the value of
the other vertices is set to be the maximal vertex value over its inputs plus one 
   fanout based propagates two values through the graph  one for each boolean value  
at a boolean operation vertex the values are not summed and passed along  rather 
they are computed according to the boolean operation at the vertex  the initial values
are of variables with no input edges  their value is set to be the number of out edges
the variable has 
in the second framework proposed by chung et al   the shortest distance between each
pair of variables is computed  the total distance of a variable is computed as the sum of
its distances to all the variables  the variable with the lowest total distance is selected as
the first variable  the next variable is selected as the closest variable to the last ordered
variable  ties are broken according to the distance to previous ordered variables 
all the graph evaluation algorithms try to order the variables so that the variable that
most influences the models next state functions will be first  the algorithms differ by the
methodology they use to order the other variables  some algorithms order them so that
variables which substantially influence the models next state functions are placed higher
in the variable order  toward the beginning of the order   other algorithms place the other
variables according to their proximity to previously ordered variables 
  

filearning to order bdd variables in verification

      decomposition algorithms
decomposition algorithms break down the model into parts  the algorithms then solve
two different problems  the first is finding a good order for each part  and the second is
finding the order of the parts  the order is constructed by combining the solutions of the
two problems 
the algorithm of malik et al  was extended and adapted for finite state machines  fsm 
by toutai et al          in their algorithm  a model is decomposed to its next state functions 
each of which is considered separately  variables of each next state function are ordered
according to malik et al  the next state functions are then ordered by a cost function 
they are ordered so that functions with many overlapping variables will be adjacent  the
variable order is obtained by adding the variables of the next state functions according to
the order of the parts  while removing variables that already exist 
the algorithm of aziz et al         decomposes the model in a different way  a model
is a hierarchical composition that is constructed by joining a number of internal parts that
pass information among themselves  usually  there is less communication among the parts
than within them  variables of an internal part tend to depend highly on one another 
the algorithm uses a process communication graph  pcg   which models the hierarchical
structure of the model and the communication between the parts  in a pcg each vertex is
an internal part  and an edge i  j connects vertex i and vertex j if part j depends on a
bit of part i  the pcg has parallel edges i  j  one for each bit value in i that j depends
upon  alternatively  the edges could be weighted 
given an order of the parts  an upper bound on the bdd size of the model can be computed  the computation is based on the size of the parts and the amount of communication
between them  heuristics guided by the upper bound are applied in order to determine the
order of the parts  the order of the variables in each part is decided by one of the previous
ordering algorithms 
      sample based algorithms
sample based static algorithms  jain et al         are not real static algorithms in the
sense that they do not create the order based on information extracted from the model
description  sample algorithms perform tests on parts of the model  building transition
relations and reachable states   for each part  a number of orders are evaluated  the good
orders are then merged to create a complete order for the model  sampling algorithms use
traditional algorithms in order to find the candidate orders for the parts  these candidate
orders are then checked by the sampling algorithm 
      summary
a majority of the graph search algorithms and graph evaluation algorithms were developed
for other problems and adapted for symbolic model checking  some of the algorithms
were developed in the context of combinational circuits  while others were developed for the
simple case of one function  in symbolic model checking the models are rarely combinational
 their outputs almost always depend also on inputs of previous cycles   and there is more
than one function to display  adapting the existing algorithms to conform to the needs of
  

figrumberg  livne    markovitch

symbolic model checking has had various degrees of success  most of the adapted algorithms
are heuristic and apply a simple rule with some logical reasoning behind it 
the decomposition algorithms are either heuristic or provide a theoretical upper bound 
however  the bounds they use are rarely realistic  for most models we require much smaller
bdds  the algorithms are also based on decomposing the model into parts and solving the
ordering of each part using graph search algorithms  thus  they also inherit the drawbacks
of these algorithms 
despite the efforts that have been invested and the many algorithms that have been
developed for static ordering  the results are not yet satisfactory  the produced bdds
are too large to manipulate  and dynamic ordering must be applied  one problem with
the above approaches is their generality  they do not utilize domain specific knowledge 
domain specific knowledge is essential for solving the majority of complex problems  it is
also difficult to retrieve  in the next subsection we discuss machine learning methods for
acquiring domain specific knowledge for ordering tasks 
    learning to order elements
learning to order elements can be done by first trying to induce a partial order  which can
then be used for generating a total order  in this context  a partial order is usually called
a preference predicate  preference predicate induction is based on a set of tagged pairs of
elements where the binary tag identifies the preferred element  broos and branting       
present a method for inducing a preference predicate using nearest neighbor classification 
the distance between an untagged pair and each tagged pair is computed as the sum of
distances between the corresponding elements  the closest tagged pair is selected  the
preferred element of the untagged pair is the one matching the preferred element in the
tagged pair 
utgoff and saxena        represent a pair a  b by the concatenated feature vector
ha          an   b          bn i  the preference predicate is a decision tree induced from these examples 
utgoff and clouse        represent a preference predicate by a polynomial  let a  
ha          an i   and b   hb          bn i be a pair of elements represented by feature vectors  let
w            wn be a set of weights  the preference predicate p is defined as follows 
p  a  b   

 

n
 
i   wi  ai  bi     
  otherwise

p

each example represents a linear constraint and the weights are found by solving the set of
constraints 
cohen  schapire and singer        extended the above mechanism by allowing any
preference function fi instead of  ai  bi   in the above expressions  they also present
two methods for generating a total order based on the induced preference predicate  both
methods use the preference predicate to construct a graph where the nodes are the elements
to be ordered and a directed edge is placed between two elements that have a precedence
relation  two algorithms for inferring the order from the graph are given  the first defines
for each node a degree which equals the sum of the outgoing edges minus the sum of the
incoming edges  the order is then constructed by selecting the node with the greatest
  

filearning to order bdd variables in verification

degree and removing its edges from the graph  the second algorithm constructs the order
in two stages  in the first stage  all the strongly connected components of the graph are
found  and they are ordered according to the dependencies between them  in the second
stage the elements of each component are ordered using the first algorithm 

   a learning algorithm for static variable ordering
producing a good variable order requires extensive understanding of bdds and their relation
to the model they represent  such knowledge can be manually inserted by a human expert 
however  this task is too complex for large models  therefore  it is rarely done  existing
static ordering algorithms use relatively simple heuristic rules that are based on expert
knowledge  these rules look at the model structure to compose the ordering  since the
rules are to be applied to all variables in all the models  they are general and thus limited
in the ability to produce good orders  alternatively  we can try to build a program that
automatically acquires more specific knowledge based on ordering experience  in this section
we present such an algorithm 
the first step in building such a learning algorithm is deciding what knowledge we wish
to acquire from the ordering experience  the existing ordering algorithms demonstrate that
the precedence relation between variables is a key consideration for the order creation  the
graph search algorithms and the search based graph evaluation algorithms try to place a
variable after the variables
that influence its next state value  generally  a variable order
n
of n variables yields   precedence pairs  a precedence pair v i  vj denotes that variable
vi should precede vj in the variable order  for example  the variable order a  b  c  d yields
the precedence pairs a  b  a  c  a  d  b  c  b  d  c  d 
the above task of learning precedence pairs can be transformed into a concept learning
task  a concept learning task is defined by 
 a universe x over which the concept is learned 
 a concept c  a subset of items in x that we want to learn  usually marked by its
associated boolean characteristic function f c   
 a set of examples  pairs of the form hx  f c  x i  where x  x 
 a set of features  functions above x that allow generalization 
for many learning tasks it is difficult to transform the problem to the format listed
above  it is already clear from the discussion above that the general concept we wish to
learn is the set of variable pairs in which the first should precede the second in the variable
ordering   
more precisely  we define the universe over which the concept is learned as the set of all
pairs h vi   vj    m i  where  vi  vj   is an ordered variable pair comprised of v i and vj   which
are variables in the model m   since we expect that some pairs will have no preferred order 
we define a ternary instead of a binary concept  the ternary concept has the following
classes 
   in practice  we will need only a small subset of the precedence pairs for constructing a total order 

  

figrumberg  livne    markovitch

   c    the class of all h vi   vj    m i for which it is preferable to place v i prior to vj in
order to get a good initial order 
   c   the class of all h vi   vj    m i for which it is preferable to place v i after vj in order
to get a good initial order 
   c    the class of all h vi   vj    m i for which placing vi before vj is just as likely to lead
to a good variable order as placing v i after vj  
in the following subsections we describe the algorithms for learning and using this concept 
    algorithm framework
we start with the description of the general framework of the learning algorithm  our goal
is to find variable orders that yield bdds with small number of nodes  given a training
model  the algorithm first generates a set of orders of its variables  we define a utility
function u over variable orders as following  each of the orders is used as the initial order
for building the bdd representation of the model     this bdd  denoted m bdd  includes
the models partitioned transition relation and its set of initial states  the utility u of a
generated order is then defined to be reversely proportional to the the number of nodes in
the m bdd constructed with this order 
a subset that consists of all the variable pairs that appear together in some next state
function is selected by the example extractor from all the possible variable pairs  we call
such pairs interacting variable pairs  for example  if next x    y  z then  y  z  is an
interacting variable pair  the example tagger tags each of the selected ordered pairs with
one of the classes c    c   or c    based on the evaluated orders  the tagged pairs are
forwarded to the feature extractor which  based on the model  computes for each pair its
feature vector  the learner  which is an id   quinlan        decision tree generator  uses
the tagged feature vectors to create a pair precedence classifier 
several training models are used in this manner to construct different pair precedence
classifiers  when solving a new unseen problem  these pair precedence classifiers are used
by the ordering algorithm to create a variable order 
the learning framework for creating a pair precedence classifier of a training model is
given in figure    the complete data flow is displayed in figure    the following subsections
describe in greater detail the components of the framework 
    the training sequence generator
the goal of the training sequence generator is to produce orders with high variance in quality
which is exploited by the tagger  see subsection       the simplest strategy for generating
such sequences is by producing random orders  this is indeed the strategy we have used in
the experiments described in this paper  one potential problem with this approach is with
domains where good orders  or bad orders  are rare  in such a case  a random generator
will not necessarily produce sequences with the desired diversity in quality 
   we use the smv  mcmillan        system for this purpose 

  

filearning to order bdd variables in verification

input   training model
output   precedence classifier
   create sample orders 
   use smv to evaluate the utility of each sample order by the m bdd size 
   find the interacting variable pairs of the training model 
   based on the evaluated sampled orders  tag each ordered pair that is based on an
interacting variable pair 
   transform each tagged pair to a tagged feature vector 
   create a classifier based on the tagged feature vectors 
figure    training model precedence classifier construction

training
order
smv

training
model

evaluated
orders

tagged
pairs

example

feature
extractor

tagger

m bdd

example

interacting
variable
pairs

learner

extractor

real
model

ordering
algorithm

classifier

tagged
feature
vectors

order

figure    data flow

an alternative approach is to actively try producing orders that are very good and orders
that are very bad  therefore creating a large diversity in quality  one way of producing a
good order is by taking the orders that are the result of the dynamic ordering process 
another option is by using an existing static ordering algorithm  one interesting idea
is to try and bootstrap the process by using the results of the adaptive ordering algorithm
as training examples thus resulting in progressively more diverse input 
  

figrumberg  livne    markovitch

    the example extractor
given a set of n variables  we can extract n   n     example ordered pairs for training 
but should we actually use all these ordered pairs as examples 
there are two main reasons for being selective about what examples to use 
   each example carries computational costs associated with tagging  feature extraction 
and the added computation by the induction procedure 
   noisy examples are known to have harmful effect on the induction process 
the process of selecting a subset of examples  to be tagged  out of a set of untagged
examples is called selective sampling  there are two common ways of performing selective
sampling  one is by automatic methods that use various general metrics for selecting
informative examples  lindenbaum  markovitch    rusakov         the other way is by
using domain specific heuristics about the potential of an example to be informative 
in this work we use the second approach  consider a function f over m variables 
represented within a bdd of n variables  where m  n   the number of nodes used to
represent f depends only on the relative order of the m variables  this means that changing
the order of the other n  m variables would not influence the bdd representation of the
function f  
the bdd representation of a model to be checked consists of the initial states of the
model and the next state functions of the variables  since the bdd representation for the
initial states is typically small  we do not take it into account  therefore  when looking for
examples  we consider only the next state functions  usually  each such function is defined
only over a subset of all the model variables  thus  the order of a pair of variables  v i   vj   
that do not appear together in any next state function is less likely to affect the quality of
the generated order  we therefore filter out such pairs 
    the example tagger
an ordered variable pair  vi   vj   should be tagged as belonging to c  if it is preferable to
place vi before vj   let v    v            vn   be the set of variables of a given model  let o be
the set of all possible orderings over v   let o vi vj be the set of all o  o where vi precedes
vj   the ordered variable pair  vi   vj   is defined to be preferable to  vj   vi   if and only if
average u o  o  ovj vi    average u o  o  ovi vj   
since it is not feasible to evaluate all the possible orders  we sample the space of possible
orders  evaluate them and partition the samples to two sets as above  as the averages now
only estimate the real averages  we replace the term smaller in the above definition
with significantly smaller  this is determined by the unpaired t test  which tests the
significance  with a given confidence  of the difference between the averages of two samples
of two populations 
more precisely  for each variable pair v i   vj   the set of sampled orders s  o is partitioned into two subsets svi vj  ovi vj and svj vi  ovj vi   an unpaired t test with
a predetermined confidence level is used to check if the averages of the set utilities differ
significantly  if they do  the ordered pair corresponding to the set with the smaller average
  

filearning to order bdd variables in verification

is tagged with   and the other ordered pair is tagged with    meaning that they belong
to c  and c   respectively   otherwise  the average difference is not significant  and both
ordered pairs are tagged with    meaning that they belong to c     
a more elaborative approach could use the t value as a weight on how important a particular order is  these weights could solve conflicts in the ordering process  such a scheme
would require  however  a method to incorporate weights into the induction algorithm  one
method is by trying to induce a continues function instead of a ternary function 
    the feature extractor
if we want to generalize from training models to future unseen models  we cannot represent
the pairs by the variable names  rather  we should use a representation that can be used
across models  most induction algorithms require that the examples be represented by
feature vectors 
the process of constructing an appropriate feature set is a crucial part of applying a
learning algorithm to a problem  it is a common knowledge engineering process where a
domain expert comes up with a set of features that might be relevant  it is the role of
the induction algorithm  then  to find out what combination of features are relevant to the
specific problem 
we have come up with a set of features over variable pairs  these features are extracted
from the model connectivity graph  some of these attributes are inspired by traditional
static ordering algorithms  the attributes can be categorized into three groups 
 variable attributes are defined on a single variable and try to capture its characteristics
in the model  one example is the variable dependence attribute  which equals the
number of variables on which a variable depends  this attribute was inspired by the
value used by butler et al         to guide the dfs search  a higher value indicates
that a larger portion of the models variables are needed to determine the variables
next state value  thus  a higher value may indicate that the variable location should
be lower in the order  another example is the variable dependency  which takes the
complementary view of variable dependence  the attribute equals the number of
variables that depend on a given variable  a higher value may indicate that the
variable should be placed higher in the variable order 
 symmetric pair attributes are defined on a variable pair v i   vj   these attributes try to
capture the strength of the bond between the two variables  as well as that between
this pair and the other variables in the model  for example  pair minimal distance
measures the shortest path between the variables in the model connectivity graph  a
shorter path can indicate a stronger bond between the variables  the distance based
ordering framework  chung et al         uses a similar feature to order variables 
another example is pair mutual dependency  which counts the number of variables
whose next state function depends on both v i and vj  
 non symmetric pair attributes try to capture the relationship between the two variables  for example  the pair dependency ratio is the ratio between the variabledependency values of the two variables  if the ratio is relatively high or low  it may
indicate the relative order of the two  pair ns distance evaluates the influence of one
  

figrumberg  livne    markovitch

variable on the next state value of the other  it does so by measuring the distance
between the variables in the subgraph that represents the next state function 
the complete list of attributes can be found in appendix a 
    the induction algorithm
after the feature extraction phase  our data is represented as a set of tagged feature vectors 
this type of representation can be used to produce classifiers by many induction algorithms 
including decision trees  hunt  marin    stone        friedman        quinlan       
breiman  frieman  olshen    stone         neural networks  widrow   hoff        parker 
      rumelhart   mcclelland        and nearest neighbor  cover   hart        duda  
hart         we have decided to use decision tree classifiers because of their relatively fast
learning and fast classification  fast classification is especially important since we wish to
be competitive with other ordering algorithms and the number of variable pairs we need to
classify is large 
decision trees have been researched thoroughly in the last decade  producing many
valuable extensions  one such extension enables the decision tree to give not only the
classification of items but also to associate with each such classification a confidence estimation  we have used this variant to allow conflict resolution  this will be described in
section       
    the ordering algorithm
the outcome of the learning process described in the last four subsections is a set of decision
trees  one for each training model 
we could also generate one tree based on the union of generated samples  one advantage
of the multiple tree approach is that we expect the examples from the same model to be more
consistent  allowing generating compact trees  in contrast  a set of examples coming from
different models is likely to be more noisy  yielding a large tree  in addition  the multiple tree
version allows us using a voting scheme during the ordering process  as described below 
given a model m  the algorithm first extracts the interacting variable pairs  each
of the classifiers is then applied to the feature vector representations of these pairs  for
each classifier  the classifications of all the pairs are gathered to form a precedence table 
these tables are then merged into one table  the order creation algorithm uses the merged
precedence table to construct the models variable order  the following subsections describe
the components in greater detail  figure   shows the data flow in the ordering algorithm 
      building the precedence table
to build a precedence table based on a given classifier  the algorithm asks two questions for
each interacting variable pair vi   vj  
   should vi  vj  
   should vj  vi  
  

filearning to order bdd variables in verification

pair
precedence
classifier
pair
precedence

real

pair

feature

problem

extractor

extractor

classifier

pair

pair

tree

table

pair

pair

tree

table

merger

merged
pair
precedence
classifier

pair

pair

tree

table

table

order
creation
algorithm

variable
order

figure    ordering algorithm data flow

 
 
 
 
 
 
 
 
 

vi  v j  
no
no
no
yes
yes
yes
unknown
unknown
unknown

vj  vi  
no
yes
unknown
no
yes
unknown
no
yes
unknown

vi   vj order
unknown
v j  vi
unknown
v i  vj
unknown
unknown
unknown
unknown
unknown

table    pair order table
if the two agree  the pair order is set to the agreed order  if they disagree  the order is
set to unknown  table   summarizes all the possible answers for the two questions and the
resulting pair order 
      the merging algorithm
after constructing the pair precedence tables from the training models classifiers  we merge
the tables using a voting scheme  for each variable pair v i   vj   we count the number of tables
that vote vi  vj and the number of tables that vote vj  vi   we then decide their pair
order according to the majority  ignoring the unknown votes  
assuming that the majority vote chooses the order v i  vj   the confidence for this vote
conf  v v  conf  v v  
is computed by vote vii vjj   vote vjjvii    where vote vi  vj   is the number of tables that vote
  

figrumberg  livne    markovitch

vi  vj and conf  vi  vj   is the sum of the confidence values of these votes  vote v j  vi  
and conf  vj  vi   are defined similarly  if this value turns out to be lower than      we set
it to a minimal value of     
      cycle resolution
in order to build a total  strict order out of the merged table  the table must not contain
any cycle  however  the above algorithm does not guarantee this  we therefore have to
apply a cycle resolution algorithm that makes the table cycle free 
the precedence table can be seen as a directed graph in which the nodes are variables 
and there is a weighted edge vi  vj if and only if vi  vj   there are many possible ways to
eliminate cycles in a directed graph  one reasonable bias is removing the least number of
edges  this problem is known as the minimum feedback arc set and is proven to be np hard
 karp         approximation algorithms for this problem exist  even  naor  schieber   
sudan         but they are too costly for our purposes 
we use instead a simple greedy algorithm to solve the problem  all the constraints
 edges  are gathered into a list and sorted in a decreasing order according to their weights
 i e   their confidence   a graph is initialized to hold only the variable vertices  the list of
edges is then traversed and each edge is added if it does not close a cycle 
      pair precedence ordering
at this stage of the algorithm  we hold an acyclic merged precedence table  the last step of
the ordering process is to convert the partial order represented by this table to a total order 
this is done by topological ordering  at each stage  the algorithm finds all the minimal
variables  i e   variables that are not constrained to follow other unordered variable  from
this set  we select a variable vadd with maximal fan out and add it after the last ordered
variable  we then add all the variables which are larger than v add but do not appear in
any constraint with an unordered variable  we do this because it is desirable to place
interacting variables near each other  the pair precedence ordering  ppo  algorithm is
listed in figure    figure   lists the selection of v add in ppo  
one possible change to the ordering process is to delay the cycle resolution to the last
stage  we call this version cycle resolution on demand  the modified algorithm does not
perform any cycle resolution on the merged table  instead  the algorithm works with the
merged table that may contain cycles  if the table contains a cycle  the algorithm must
reach a stage where not all the variables are ordered and there are no minimal variables 
in this case the algorithm performs cycle resolution as before and continues the ordering
process 
    experiments
we performed an empirical evaluation of the ppo algorithm using models from the
iscas    brglez  bryan    kozminski        benchmark  the iscas   benchmark circuits
have been used to empirically evaluate many algorithms that deal with various aspects of
circuit design  chamberlain        wahba   borrione        nakamura  takagi  kimura 
  watanabe        long  iyer    abramovici        iyer   abramovici        konuk
  larrabee         we discovered that some of the circuits are insensitive to the initial
   

filearning to order bdd variables in verification

input   the merged pair precedence table 
output   a variable order 
let v be the set of all variables 
let before v  v        v    v    v  v     
let after v  v        v    v    v    v  

   vc    vi  bef ore vi   v       or af ter vi   v       
vn c   v  v c
   while vc    
 a  vmin    vi  vc  af ter vi   vc      
 b  vadd   argmaxvi vmin  bef ore vi   vc   
 c  order   order    vadd

a

b

 d  vc   vc   vadd  
 e  for each vi  vc
if af ter vi   vc      and bef ore vi   vc      then
order   order    vi   vc   vc   vi  
   add vn c to end of variable list 
a  if more than one exists  select one 
b  add variable to order 

figure    pair precedence ordering
ordering  this means that the entire sample of initial orders yielded model bdds of similar
sizes  we eliminated these circuits from the set  out of the remaining circuits we selected
those with a number of variables that smv can handle  we ended up with the following
five circuits  s           s           s          s            s            the numbers in
parentheses stand for the number of variables in each model 
we began with an offline learning session where the three smaller models  s      s     
s      are used as training models  for each of these models we generated     random
orders and extracted examples as described in the previous section  the algorithm then
induced three precedence classifiers in the form of decision trees 
the number     was selected since it proved to be sufficiently large  in real application
the algorithm can be used as an anytime algorithm where training sequences are generated
as long as the user is willing to wait for the offline learner  an alternative approach would
keep aside a validation set that would be used for testing the systems performance  the
training could have then be stopped when the learning curve flattens 
the algorithm was tested on the two larger models  s      s       for each of the
models  the three learned decision trees were used to generate the merged precedence table 
   

figrumberg  livne    markovitch

pair
merged
table

minimal
elements
unordered
variables

find
minimal

filter
maximal

maximal
minimal
elements

selected
element
select
one

variable
order

figure    pair precedence ordering v add selection

our ppo algorithm  with cycle resolution on demand  was compared to the random
algorithm  in addition  we compared our results to two advanced graph search algorithms
for static ordering  the dfs append algorithm of fujita et al         and the interleave
algorithm of fujii et al          in both algorithms we used the adaptation for multiple
starting points  butler et al         and its expanded version  which includes the tie breaking
rule  fujita et al          the random results were taken based on     variable orders  the
two other algorithms were each run    times on every model  the performance of the
ordering algorithms is measured by the number of nodes in the model bdds  partitioned
transition relation and initial states  
table   and figure   show the results obtained  the table shows that on model s     
ppo outperformed the random order by more than       on model s      ppo outperformed the random order by    
ppo vs  random

 

   

x   

random
ppo

 

bdd nodes

   

 

   

 

s    

model

s    

figure    comparative histogram of ppo vs  random

   

filearning to order bdd variables in verification

model
s    
s    

random
average
std
      
      
               

ppo
average
std
             
      
     

table    comparative table of ppo vs  random
the comparison of our algorithm to the two static algorithms is given in figure    
the results show that our learning algorithm  after training  becomes competitive with the
existing ordering algorithms written by experts 
 

  

x   

append
interleave
ppo

 
 
 

bdd nodes

 
 
 
 
 
 
 

s    

model

s    

figure     comparative histogram of ppo vs  static
to evaluate the utility of the learned knowledge we would like to compare the performance of the ordering process with and without the learned knowledge  ordering without
any learned knowledge is equivalent to random ordering  the comparison of our results to
the random ordering algorithm reveals that the learner indeed induced meaningful knowledge during the learning process  our method is also much more stable than random
ordering on s     as indicated by comparing the standard deviation  this large variance
in the results of the random ordering is indeed exploited by our tagging procedure as explained in section      the small variance in the results obtained by random ordering on
s     can explain why the improvement obtained by the ppo algorithm is much smaller
on this circuit  a more sophisticated training sequence generator  such as those described
in section      might have been more successful with that circuit 
the comparison to the hand crafted algorithms may look disappointing at first look
since the results of the learning system are not better than the existing algorithms  recall 
however  that we are comparing an automated learning process to human expertise  most
of the works in empirical machine learning make comparisons between the performance of
various learning algorithms  it is not common to compare the performance of a learning
   

figrumberg  livne    markovitch

algorithm with a human expert or an expert system since in most cases it is clear that handcrafted algorithms would outperform automated learning processes  since there are hardly
any other learning systems that were built to solve the bdd variable ordering problem we
could not make the more common comparison between learning systems 

   learning context based precedence for static ordering
the precedence relation is one of the key considerations used by traditional static ordering
algorithms  another key consideration is the clustering of variables and their subsequent
ordering  the algorithms try to place highly interacting variables near each other 
the effect of the variable clustering in a bdd can be seen in the simple example given in
figure    in this function  switching the two variables v   and v  increases the bdd size by
  nodes  for this function  all the orders in which the variables of each of the two clusters 
v    v  and v     v    are kept together yield the minimal bdd representation  other variable
orders yield a less compact bdd  thus  in this function  the only key consideration is the
compliance with clustering  precedence is not taken into account  
    variable distance
the above discussion leads to the hypothesis that the distance between variables is an important factor when considering alternative orders  one way to obtain distance information
is by learning the distance function between pairs of variables  there are  however  two
problems with this approach 
   the target distance function is not well defined across models  for example  if we
train on small models  the absolute distance function is not likely to be applicable for
large models 
   information on absolute distances between variables is not sufficient to construct a
good ordering  this is because the absolute distance does not uniquely define the
order between the variables  in fact  it defines two possible orders  where one is the
reverse of the other 
the example in figure    demonstrates that an order and its reverse can yield bdds
that are significantly different in size  each of the bdds in figure    represents two
functions  f   a  b  c  d  e     a   b   c    c   d  and f   a  b  c  d  e     a   b  
c    c   e   the absolute distance between the variables in the orders is clearly the
same  however  the upper bdd is approximately double the size of the lower one 
we wanted to check whether in realistic examples reverse orders can yield bdds that
are significantly different in size  we tested models from the iscas   benchmarks
and created       variable orders for each model  for each order  we compared its
quality with the quality of the reversed order  we found that in many cases one order
was exceptionally good while the reversed one was exceptionally bad  thus  learning
the absolute distance is not sufficient  and more information is needed 
we conclude that there are problems inherent both in learning and in utilizing absolute
distances  still  clustering is a key consideration and should be pursued  we suggest 
   

filearning to order bdd variables in verification

f 

a

f 

b
c
d
e

 

 
 

 

 

 

 

 
 

 

 

 

 a 
f 

e
f 

d
c
b
a

 

 
 

 
 

 

 

 

 b 

figure     robdds for the functions f    a  b  c  d  e     a   b   c    c   d  and
f   a  b  c  d  e     a   b   c    c   e 

alternatively  learning the relative distance that determines  for variables v i   vj   and vk  
which of vj   vk should be closer to vi   given that vi precedes the other two 
the remainder of this section describes a method for learning and utilizing context based
precedence to infer the relative distance between variables 
    context based precedence
a context precedence relation is a triplet v i  vj  vk   given that vi precedes vj and vk   the
variable vj should come before the variable vk   thus  the context precedence relation adds
context to pair ordering decisions 
as in pair precedence learning  we define the universe to be the set of pairs h v i   vj   vk    m i
where vi   vj   vk are variables in the model m   the universe is divided into three classes 
c    c   c    as before  examples for these classes are drawn in the same way  the pair
precedence framework can be applied with minor changes to work with context precedence
relations  these minor changes are described below 
    the example tagger
a variable triplet  vi   vj   vk   should be tagged as c  if  given that vi precedes vj and vk   it is
preferable to place vj before vk  i e   vi  vj  vk    as in pair precedence learning  we use a
set of evaluated variable orders for the tagging  any set of such orders can be partitioned to
three subsets  depending on which of the three variables is first  given a partition defined
by vi  for example   we can test the order of v j and vk using t test  as described in section
   

figrumberg  livne    markovitch

     to reduce the number of noisy examples  we use only the partition that yields the most
significant t test results 
    the feature extractor
the attributes of a triplet  vi   vj   vk   are computed based on the attributes of the two pairs
vi  vj and vi  vk   each attribute value is the division subtraction of two corresponding
attribute values from the two pair attributes 
more precisely  assume that the pair v i  vj has attributes f   vi   vj            fn  vi   vj   and
the pair vi  vk has attributes f   vi   vk            fn  vi   vk    then the triple  vi   vj   vk   has
attributes f   vi   vj   f   vi   vk            fn  vi   vj   fn  vi   vk    if some of the fl  vi   vk   can be  
then the corresponding attributes are subtracted instead of divided 
as an example consider an attribute f l which is pair minimal distance  see section      
if fl  vi   vj   fl  vi   vk   is greater than   than the shortest path between v i and vj is larger
than the shortest path between vi and vk   this attribute can indicate that v k should appear
closer to vi  
similarly  if fl is pair mutual dependency then fl  vi   vj   fl  vi   vk       indicates that the
number of variables whose next state function depends on both v i and vj is greater than
those depending on both vi and vk   this may indicate that it is preferable to keep v i and
vj close together 
    the ordering algorithm
the outcome of the learning phase is a set of decision trees  one for each model  this is the
same as in the case of context free pairs  in this subsection we describe ways to use these
trees for ordering 
      building the context precedence table
while in the case of pair precedence we had a table of size n    where n is the number of
variables   we now produce one such table for each context variable  for each table we
perform inconsistency elimination similar to that described in section        here  however 
when we ask the classifier the two questions v j   vk and vk   vj   we add the context variable
vi to the query 
      pair precedence ordering with context precedence filtering
the ordering algorithm uses the pair precedence table in the same way as the ppo algorithm  however  it was often found to be the case that the ppo algorithm had several
minimal variables  even after employing the maximal fanout filter  we use the contextbased precedence table to further reduce the size of the set of minimal elements  we use
the variables in the already ordered sequence as context variables and look at their associated tables  if the set of minimal elements contains a pair of variables constrained as
vj  vk in one of the tables  we eliminate vk from the set  figure    lists the code which
when added to the ppo algorithm  accepts a variable set v add  from which we previously
selected randomly   and returns one variable  we call the new algorithm ppo cpf  
figure    lists the selection of vadd in ppocpf  
   

filearning to order bdd variables in verification

input   the set of candidate variables to be added  v add   and the merged context precedence
table 
output   a variable to be added 
let after v  vi   vj      hvi   vj i vi  vi   vj  vj  vi  vj  v  
 
b   vadd
   vi  vadd  af ter vi   vinorder   vadd      
 
b   if vadd
    then
 
select randomly one variable from v add
else select randomly one variable from v add

figure     pair precedence ordering with context precedence filtering
context
merged
table

pair
merged
table

minimal
elements
unordered
variables

find
minimal

filter
maximal

maximal
minimal
elements

filter
context
constrained

unconstrained
minimal
elements

selected
element
select
one

variable
order

figure     pair precedence ordering with context precedence filtering v add selection

    experiments
we have evaluated the performance of ppo cpf   performing off line learning on the
training models followed by ordering of the test models  the results are shown in figure    
for comparison we also show the performance of the ppo algorithm and the two expert
algorithms 
the p p o cp f algorithm outperforms all the other algorithms on the two tested models 
the results show that the context based precedence relations add valuable information 
we have tested the effect of the resources invested in the learning phase on the performance of the algorithms  since the learning examples are tagged based on evaluated training
orders  and since the evaluation of the training orders is the most resource consuming operation  we used the number of these orders as the resource estimator  figure    shows the
learning curves of our algorithms  that is  it shows how the system performance changes
according to the offline resources consumed  the number of training orders evaluated  
without testing any random order  our system has no knowledge on which to build
the precedence classifiers  and thus its performance is equivalent to random ordering  the
   

figrumberg  livne    markovitch

 

  

x   

append
interleave
ppo
cpf
ppo

 
 
 

bdd nodes

 
 
 
 
 
 
 

s    

model

s    

figure     comparative histogram of ordering algorithms
 

   

x   

s    
s    

 

   

bdd nodes

   

   

   

 

   

   
 

  

  

  

  
   
   
number of examples

   

   

   

   

figure     learning curves of the ppo cpf algorithm for the two testing models

tagging based on    orders is too noisy  while it improves the performance of s      it
degrades the performance of s      forty orders are sufficient to generate stable tagging 
which yields improved classifiers and therefore improved ordering quality 

   discussion
the work described in this paper presents a general framework for using machine learning
methods to solve the static variable ordering problem  our method assumes the availability
   

filearning to order bdd variables in verification

of training models  for each training model  the learning algorithm generates a set of
random orders and evaluates them by building their associated bdds  each ordered pair
of interacting variables is then tagged as a good example if it appears more frequently
in highly valued orders  the ordered pairs are converted to feature based representations
and are then given  with their associated tags  to an induction algorithm  when ordering
variables of a new unseen model  the resulting classifiers  one for each model  are used to
determine the ordering of variable pairs  we also present an extension of this method that
learns context based ordering 
our algorithm was empirically tested on real models  its performance was significantly
better than random ordering  meaning that the algorithm was able to acquire useful ordering
knowledge  our results were slightly better than existing static ordering algorithms handcrafted by experts  this result is significant if we compare it to applications of learning
systems to other domains  we would surely appreciate an induction algorithm that produces
a classifier with performance comparable to that of an expert system built by a medical
expert  a chess learning program that is able to learn an evaluation function that is
equivalent in power to a function produced by an expert will be similarly appreciated 
we therefore claim that the ability of or learning algorithm to achieve results that are as
good as manually designed algorithms indicates strong learning capabilities 
in most learning algorithms  we expect to get better performance when the testing
problems are similar to the training problems  in the verification domain  we expect to get
good results when the testing and training models come from a family of similar models 
there are several occasions in which models are similar enough to be considered a family 
models of different versions of a design under development  models which are reduced
versions of a design  each with respect to a different property  models of designs with
a similar functionality like alus  arbiters  pipelines and bus controllers  unfortunately 
due to the difficulty in obtaining suitable real models for our experiments  we ended up
experimenting with training and testing models that are not related  we expect to achieve
much better results for related models 
compared with previous work in machine learning  our precedence relations most resemble these of utgoff and saxena         our ordering approach  in which we construct a
total order of elements by finding the precedence relation between them  is in essence the
same as that of cohen  schapire and singer         specifically  the second ordering algorithm of cohen  schapire and singer also uses the topological ordering approach to create
an order  their algorithm initially finds in the precedence graph the connected components
and  after ordering them  using topological ordering   finds the order in each connected
component  however  since the quality of the final order is determined by the sum of constraints adhered to  all topological orders have theoretically the same quality  we found
that in the bdd variable ordering problem not all topological orders have the same quality 
thus  we developed a topological ordering that takes into consideration those features that
we recognized as true for variable orders in bdds 
our work also differs from previous research in that it introduces the notion of contextbased precedence  using this concept we were able to create an ordering algorithm that
produces the best results 
there are several directions for extending the work described here  one problem with
our current empirical evaluation is the small number of models  in spite of our extensive
   

figrumberg  livne    markovitch

search efforts we were not able to find a large set of suitable examples  the majority of the
known examples are very simple  compared with real industry problems   producing small
model bdd representations with very little variance  we are currently in the process of
approaching companies that use model checking  in this way we hope to obtain additional
real models  preferably from families of the designs described above 
the attributes of the variable pairs were partially based on substantive research in the
field of static algorithms  we could not find such information on which to base contextbased variable attributes  thus  we also based these attributes on those of the variable
pairs  nevertheless  we believe that human experts in this field may have information that
can lead to the development of better attributes  the development of such attributes should
help to capture in a better way the context based precedence concept 
given our current results  an immediate question is whether the concept of precedence
pairs  context and non context  can be extended to triplets  quadruples  etc  such precedence relations take into account a larger part of the model and thus may possess valuable
information  such an extension  however  could carry high cost during learning and  even
worse  during ordering 
our framework for solving the static variable ordering problem was shown to be valuable
in model checking  model checking is only one field of verification in which bdds are used 
bdds are also used in verification for simulation and equivalence checking  our algorithm
can be applied for these problems as well  we are unaware of special static variable ordering
algorithms for these fields  but if such do exist  variable attributes based on these algorithms
should be added 
the most interesting future direction is the generalization of our framework for other
ordering problems  ordering a set of objects is a very common sub task in problem solving 
the most common approach for tackling such a problem is to evaluate each object using
a utility function and order the objects according to their utilities  such an approach is
taken  for example  by most heuristic search algorithms  in many problems  however  it is
much easier to determine the relative order of two objects than to give each object a global
utility value  few works have applied learning to ordering techniques that are not utility
based  cohen et al          the algorithms described in section   and section   can be
applied to any ordering problem if a method for evaluating training orders is available  and
a set of meaningful pair features can be defined 
we believe that the research presented in this paper contributes both to the field of
machine learning and to the field of formal verification  for machine learning  it presents
a new methodology for learning to order elements  this methodology can be applied to
various kinds of ordering problems  for formal verification  it presents new learning based
techniques for variable ordering  finding good variable ordering techniques is one of the
key problems in this field 

appendix a  variable pair attributes
the following definitions and symbols will be used in the attribute description 
 n s vi   for the next state function of variable v i
 vi   vj to indicate that variable vi depends on variable vj s value  vj  ns vi   
   

filearning to order bdd variables in verification

 vi    vj to indicate that variable vi interacts with variable vj  vi   vj and or vj   vi  
   variables for the number of variables in the model
a   variable attributes
the attributes computed for vi are
   variable dependence  the number of variables upon which v i depends    vj  vi   vj    
   variable dependency  the number of variables that depend on v i    vj  vj   vi    
   variable dependency size  the sum of function sizes that depend on v i  

p

vj  vi

  vk  n s vj     

   variable dependency average size 
the average function size dependent on v i
 
p
vj  vi

  vk n s vj    

  vj  vj  vi   

   variable dependence dependency ratio  the proportion between the number of vari 
ables on which vi depends and the number of variables that depend on it

  vj  vi  vj   
  vj  vj  vi   

   variable interaction  the number of variables interacting with v i    vj  vi    vj    
   variable dependence percentage 
the percentage of model variables on which v i de 
pends

  vj  vi  vj   
 variables

   variable dependency percentage 
the percentage of model variables that depend on v i
 
  vj  vj  vi   
 variables

   variable interaction percentage 
the percentage of model variables interacting with v i
 
  vj  vi   vj   
 variables

a   variable pair attributes
the attributes computed for hvi   vj i are
 symmetric attributes
   pair minimal distance  the minimal distance between v i  vj in the model graph
   pair minimal distance eval  the minimal distance between v i  vj in the model
graph divided by the number of times it appears
   pair minimal dependency  the number of variables that depend on the pair with
the minimal distance
   pair minimal dependency eval  the minimal distance between v i  vj in the model
graph divided by number of variables that depend on the minimal distance
   

figrumberg  livne    markovitch

   pair minimal connection class  the minimal distance between the v i  vj connection class  the operators that can be applied on two variables were divided into
classes and the operator that connected the two variables in the minimal distance
class was extracted 
   pair minimal maximal  the maximal sized ns v k   connecting the pair in minimal distance
   pair minimal maximal eval  the minimal distance between v i  vj in the model
graph divided by maximal sized ns v k   connecting the pair in minimal distance
   pair sum distance  the sum of distances between v i  vj in the model graph
   pair dependency ns size  the sum of ns v k   sizes that are dependent on vi and
p
vj   vk  vi   vk  vj  vl  n s vk    

    pair sum distance dependency ratio  the sum of distances between v i  vj in the
model graph divided by sum of ns vk   sizes that are dependent on vi and vj
    pair mutual dependence  the number of variables on which both v i  vj depend
   vk  vi   vk   vj   vk    
    pair mutual dependency  the number of variables that depend on v i and vj
   vk  vk   vi   vk   vj    
    pair mutual interaction  the number of variables that interact with v i and vj
   vk  vi    vk   vi    vk    

    pair mutual ns dependency  v i depends on vj and vj depends on vi    vi   vj   vj   vi  
 non symmetric attributes   those computed for the pair hv i   vj i with relevance to vi  
   pair ns distance  the distance between v i  vj in ns vi  
   pair dependence ratio  the ratio between the number of variables  
that v i depends
on and the number of variables that v j depends on

  vl  vi  vl   
  vm  vj  vm   

   pair dependency ratio  the ratio between the number of variables
  that depend
on vi and the number of variable that depend on v j

  vl  vl  vi   
  vm  vm  vj   

   pair interaction ratio  the ratio between the number of variables that  interact
  vl  vi   vl   
  vm  vj   vm   

with vi and the number of variables that interact with v j

   pair dependence flag  the number of variables that v i depends on
  compared to
the number of variables that vj depends on

  vl  vi  vl   
  vm  vj  vm   

      

   pair interaction flag  the number of variables that interact with v i compared
to
 
the number of variables that vj interacts with

   

  vl  vi   vl   
  vm  vj   vm   

      

filearning to order bdd variables in verification

references
akers  s          binary decision diagrams  ieee transactions on computers  c        
       
aziz  a   tasiran  s     brayton  r          bdd variable ordering for interacting finite
state machines  in proceedings of the   st design automation conference  dac   pp 
        san diego  california 
beer  i   ben david  s   eisner  c     landver  a          rulebase  an industry oriented
formal verification tool  in proceedings of the   rd design automation conference
 dac   pp          las vegas  nevada  ieee computer society press 
bern  j   meinel  c     slobodova  a          efficient obdd based boolean manipulation in cad beyond current limits  in proceedings of the   nd design automation
conference  dac   pp          san francisco  california 
bollig  b   lobbing  m     wegener  i          simulated annealing to improve variable orderings for obdds  in proceedings of the international workshop on logic synthesis 
pp   b          granlibakken  california 
bollig  b     wegener  i          improving the variable ordering of obdds is np complete 
ieee transactions on computers                  
breiman  l   frieman  j  h   olshen  r  a     stone  c  j          classification and
regression trees  wadsworth publishing company  belmont  california  u s a 
brglez  f   bryan  d     kozminski  k          combinational profiles of sequential benchmark circuits  in proceedings of the international symposium on circuits and systems 
pp            portland  oregon 
broos  p     branting  k          compositional instance based learning  in proceedings
of the   th national conference on artificial intelligence  pp          menlo park 
california  aaai press 
bryant  r          graph based algorithms for boolean function manipulation  ieee transactions on computers  c                
butler  k  m   ross  d  e     rohit kapur  a  m  r  m          heuristics to compute
variable orderings for efficient manipulation of ordered binary decision diagrams  in
proceedings of the   th design automation conference  dac   pp          san francisco  california 
chamberlain  r          parallel logic simulation of vlsi systems  in proceedings of the
  nd design automation conference  dac   pp          san francisco  california 
chung  p   hajj  i     patel  j          efficient variable ordering heuristics for shared
robdd  in proceedings of the international symposium on circuits and systems 
pp            chicago  illinois 
clarke  e  m   emerson  f  a     sistla  a  p          automatic verification of finite
state concurrent systems using temporal logic specifications  acm transactions on
programming languages and systems                
   

figrumberg  livne    markovitch

cohen  w  w   schapire  r  e     singer  y          learning to order things  journal of
artificial intelligence research             
cover  t  m     hart  p  e          nearest neighbor pattern classification  ieee transactions on information theory           
drechsler  r   becker  b     gockel  n          genetic algorithm for variable ordering of
obdds  ieee proceedings on computers and digital techniques                  
drechsler  r   drechsler  n     slobodova  a          fast exact minimization of bdds 
in proceedings of the   th design automation conference  dac   pp          san
francisco  california 
duda  r  o     hart  p  e          pattern classification and scene analysis  john wiley
and sons  new york 
even  g   naor  j   schieber  b     sudan  m          approximating minimum feedback
sets and multi cuts in directed graphs  algorithmica             
friedman  j          a recursive partitioning decision rule for nonparametric classification 
ieee transactions on computers  c                
friedman  s  j     supowit  k  j          finding the optimal variable ordering for binary
decision diagrams  in proceedings of the   th design automation conference  dac  
pp          miami beach  florida 
fujii  h   ootomo  g     hori  c          interleaving based variable ordering methods
for ordered binary decision diagrams  in proceedings of the ieee acm international
conference on computer aided design  pp        santa clara  california 
fujita  m   fujisawa  h     kawato  n          evaluation and improvements of boolean
comparison method based on binary decision diagrams  in proceedings of the international conference on computer aided design  pp      santa clara  california 
fujita  m   fujisawa  h     matsunaga  y          variable ordering algorithms for ordered
binary decision diagrams and their evaluation  ieee transactions on computer aided
design of integrated circuits and systems              
fujita  m   kukimoto  y     brayton  r          bdd minimization by truth table permutations  in proceedings of the international workshop on logic synthesis  pp         
lake tahoe  california 
hunt  e   marin  j     stone  p          experiments in induction  academic press  new
york 
ishiura  n   sawada  h     yajima  s          minimization of binary decision diagrams
based on exchanges of variables  in proceedings of the international conference on
computer aided design  pp          santa clara  california 
iyer  m     abramovici  m          fire  a fault independent combinational redundancy
identification algorithm  ieee transactions on vlsi systems            
jain  j   adams  w     fujita  m          sampling schemes for computing variable orderings  in proceedings of the international conference on computer aided design  pp 
        san jose  california 
   

filearning to order bdd variables in verification

karp  r  m          reducibility among combinatorial problems  in miller  r     thatcher 
j   eds    complexity of computer computations  pp         new york  plenum
press 
kaufmann  m     pixley  c          intertwined development and formal verification of a
  x bus model  in proceedings of the international conference on computer design 
vlsi in computers and processors  iccd      pp        austin  texas 
konuk  h     larrabee  r          explorations of sequential atpg using boolean satisfiability  in proceedings of the   th ieee vlsi test symposium  pp       
lindenbaum  m   markovitch  s     rusakov  d          selective sampling for nearest
neighbor classifiers  in proceedings of the sixteenth national confernce on artificial
intelligence  pp          orlando  florida 
long  d   iyer  m     abramovici  m          identifying sequentially untestable faults
using illegal states  in proceedings of the   th ieee vlsi test symposium  pp      
los alamitos  california 
malik  s   wang  a   brayton  r     sangiovanni vincentelli  a          logic verification
using binary decision diagrams in a logic synthesis environment  in proceedings of the
international conference on computer aided design  pp      santa clara  california 
mcmillan  k          symbolic model checking  an approach to the state explosion problem  kluwer academic publisher 
meinel  c     slobodova  a          speeding up variable ordering of obdds  in proceedings of the international conference on computer aided design  pp          austin 
texas 
meinel  c     slobodova  a          sample method for minimization of obdds  in proceedings of the conference on current trends in theory and practice of informatics 
vol       of lecture notes in computer science  pp          springer verlag  new
york 
meinel  c   somenzi  f     theobald  t          linear sifting of decison diagrams  in
proceedings of the   th design automation conference  dac   pp          anaheim 
california 
mercer  m  r   kapur  r     ross  d  e          functional approaches to generating
orderings for efficient symbolic representations  in proceedings of the   th design
automation conference  dac  
minato  s   ishiura  n     yajima  s          shared binary decision diagrams with attributed edges for efficient boolean function manipulation  in proceedings of the   th
design automation conference  dac   pp        orlando  florida 
nakamura  k   takagi  k   kimura  s     watanabe  k          waiting false path analysis
of sequential logic circuits for performance optimization  in proceedings of the international conference on computer aided design  pp          san jose  california 
panda  s     somenzi  f          who are the variables in your neighbourhood  in proceedings of the international conference on computer aided design  pp        san
jose  california 
   

figrumberg  livne    markovitch

panda  s   somenzi  f     plessier  b  f          symmetry detection and dynamic variable
ordering of decision diagrams  in proceedings of the international conference on
computer aided design  pp          san jose  california 
parker  d  b          learning logic  tech  rep  tr     center for computational research
in economics and management science  mit  cambridge  ma 
queille  j     sifakis  j          specification and verification of concurrent systems in
cesar  in dezani ciancaglini  m     montanari  u   eds    proceedings of the  th
international symposium on programming  vol      of lecture notes in computer
science  pp          springer verlag  new york 
quinlan  j  r          discovering rules by induction from large collections of examples 
in expert systems in the micro electronic age  pp          edinburgh university
press 
quinlan  j  r          induction of decision trees  machine learning               
rudell  r          dynamic variable ordering for ordered binary decision diagrams  in
proceedings of the international conference on computer aided design  pp       
santa clara  california 
rumelhart  d  e     mcclelland  j  l          parallel distibuted processing  exploration
in the microstructure of cognition   vol       mit press 
touati  h   savoj  h   lin  b   brayton  r     sangiovanni vincetelli  a          implicit
state enumeration of finite state machines using bdds  in proceedings of the international conference on computer aided design  pp          santa clara  california 
utgoff  p     clouse  j          two kinds of training information for evaluation function
learning  in proceedings of the ninth national conference on artificial intelligence 
pp          anaheim  california 
utgoff  p  e     saxena  s          learning a preference predicate  in proceedings of the
fourth international workshop on machine learning  pp          irvine  california 
wahba  a     borrione  d          design error diagnosis in sequential circuits  lecture
notes in computer science              
widrow  b     hoff  m  e          adaptive switching circuits  in      ire wescon
convention record  pp         new york 
zhuang  n   benten  m     cheung  p          improved variable ordering of bdds with
novel genetic algorithm  in proceedings of the international symposium on circuits
and systems   vol     pp          atlanta  georgia 

   

fi