journal of articial intelligence research                  

submitted        published      

wrapper maintenance  a machine learning approach
kristina lerman

lerman isi edu

usc information sciences institute
     admiralty way
marina del rey  ca       usa

steven n  minton

minton fetch com

fetch technologies
     admiralty way
marina del rey  ca       usa

craig a  knoblock

knoblock isi edu

usc information sciences institute and fetch technologies
     admiralty way
marina del rey  ca       usa

abstract
the proliferation of online information sources has led to an increased use of wrappers
for extracting data from web sources  while most of the previous research has focused
on quick and ecient generation of wrappers  the development of tools for wrapper maintenance has received less attention  this is an important research problem because web
sources often change in ways that prevent the wrappers from extracting data correctly  we
present an ecient algorithm that learns structural information about data from positive
examples alone  we describe how this information can be used for two wrapper maintenance applications  wrapper verication and reinduction  the wrapper verication system
detects when a wrapper is not extracting correct data  usually because the web source has
changed its format  the reinduction algorithm automatically recovers from changes in the
web source by identifying data on web pages so that a new wrapper may be generated for
this source  to validate our approach  we monitored    wrappers over a period of a year 
the verication algorithm correctly discovered    of the    wrapper changes  and made
   mistakes  resulting in precision of      and recall of       we validated the reinduction algorithm on ten web sources  we were able to successfully reinduce the wrappers 
obtaining precision and recall values of      and      on the data extraction task 

   introduction
there is a tremendous amount of information available online  but much of this information
is formatted to be easily read by human users  not computer applications  extracting
information from semi structured web pages is an increasingly important capability for
web based software applications that perform information management functions  such as
shopping agents  doorenbos  etzioni    weld        and virtual travel assistants  knoblock 
minton  ambite  muslea  oh    frank      b  ambite  barish  knoblock  muslea  oh   
minton         among others  these applications  often referred to as agents  rely on
web wrappers that extract information from semi structured sources and convert it to
a structured format  semi structured sources are those that have no explicitly specied
grammar or schema  but have an implicit grammar that can be used to identify relevant
c
    
ai access foundation and morgan kaufmann publishers  all rights reserved 

filerman  minton   knoblock

information on the page  even text sources such as email messages have some structure in
the heading that can be exploited to extract the date  sender  addressee  title  and body
of the messages  other sources  such as online catalogs  have a very regular structure that
can be exploited to extract all the data automatically 
wrappers rely on extraction rules to identify the data eld to be extracted  semiautomatic creation of extraction rules  or wrapper induction  has been an active area of
research in recent years  knoblock  lerman  minton    muslea      a  kushmerick  weld 
  doorenbos         the most advanced of these wrapper generation systems use machine
learning techniques to learn the extraction rules by example  for instance  the wrapper
induction tool developed at usc  knoblock et al       a  muslea  minton    knoblock 
      and commercialized by fetch technologies  allows the user to mark up data to be
extracted on several example pages from an online source using a graphical user interface 
the system then generates landmark based extraction rules for these data that rely on
the page layout  the usc wrapper tool is able to eciently create extraction rules from
a small number of examples  moreover  it can extract data from pages that contain lists 
nested structures  and other complicated formatting layouts 
in comparison to wrapper induction  wrapper maintenance has received less attention 
this is an important problem  because even slight changes in the web page layout can break
a wrapper that uses landmark based rules and prevent it from extracting data correctly  in
this paper we discuss our approach to the wrapper maintenance problem  which consists of
two parts  wrapper verication and reinduction  a wrapper verication system monitors
the validity of data returned by the wrapper  if the site changes  the wrapper may extract
nothing at all or some data that is not correct  the verication system will detect data
inconsistency and notify the operator or automatically launch a wrapper repair process 
a wrapper reinduction system repairs the extraction rules so that the wrapper works on
changed pages 

pages to
be labeled

web
pages

reinduction system

gui

labeled
web pages

wrapper
induction
system

wrapper
extracted
data
change
detected

automatic
re labeling

wrapper
verification

figure    life cycle of a wrapper
figure   graphically illustrates the entire life cycle of a wrapper  as shown in the gure 
the wrapper induction system takes a set of web pages labeled with examples of the data to
be extracted  the output of the wrapper induction system is a wrapper  consisting of a set
   

fiwrapper maintenance

of extraction rules that describe how to locate the desired information on a web page  the
wrapper verication system uses the functioning wrapper to collect extracted data  it then
learns patterns describing the structure of data  these patterns are used to verify that the
wrapper is correctly extracting data at a later date  if a change is detected  the system can
automatically repair a wrapper by using this structural information to locate examples of
data on the new pages and re running the wrapper induction system with these examples 
at the core of these wrapper maintenance applications is a machine learning algorithm that
learns structural information about common data elds  in this paper we introduce the
algorithm  dataprog  and describe its application to the wrapper maintenance tasks in
detail  though we focus on web applications  the learning technique is not web specic 
and can be used for data validation in general 
note that we distinguish two types of extraction rules  landmark based rules that extract data by exploiting the structure of the web page  and content based rules  which we
refer to as content patterns or simply patterns  that exploit the structure of the eld itself 
our previous work focused on learning landmark rules for information extraction  muslea 
minton    knoblock         the current work shows that augmenting these rules with
content based patterns provides a foundation for sophisticated wrapper maintenance applications 

   learning content patterns
the goal of our research is to extract information from semi structured information sources 
this typically involves identifying small chunks of highly informative data on formatted
pages  as opposed to parsing natural language text   either by convention or design  these
elds are usually structured  phone numbers  prices  dates  street addresses  names  schedules  etc  several examples of street addresses are given in fig     clearly  these strings
are not arbitrary  but share some similarities  the objective of our work is to learn the
structure of such elds 
     admiralty way
      pico boulevard
    oak street
     main street
     adams boulevard

figure    examples of a street address eld

    data representation
in previous work  researchers described the elds extracted from web pages by a characterlevel grammar  goan  benson    etzioni        or a collection of global features  such as the
number of words and the density of numeric characters  kushmerick         we employ an
intermediate word level representation that balances the descriptive power and specicity
of the character level representation with the compactness and computational eciency of
the global representation  words  or more accurately tokens  are strings generated from
   

filerman  minton   knoblock

an alphabet containing dierent types of characters  alphabetic  numeric  punctuation 
etc  we use the tokens character types to assign it to one or more syntactic categories 
alphabetic  numeric  etc  these categories form a hierarchy depicted in fig     where the
arrows point from more general to less general categories  a unique specic token type is
created for every string that appears in at least k examples  as determined in a preprocessing
step  the hierarchical representation allows for multi level generalization  thus  the token
boulevard belongs to the general token types alphanum  alphanumeric strings   alpha
 alphabetic strings   upper  capitalized words   as well as to the specic type representing
the string boulevard  this representation is exible and may be expanded to include
domain specic information  for example  the numeric type is divided into categories
that include range information about the number  large  larger than        medium
 medium numbers  between    and       and small  smaller than     and number of
digits        and  digit  likewise  we may explicitly include knowledge about the type
of information being parsed  e g   some   digit numbers could be represented as zipcode 

token

punct

alphanum

alpha

upper

number

lower

small medium large

allcaps

ca

html

 digit

boulevard

 digit

 digit

   

figure    portion of the token type syntactic hierarchy
we have found that a sequence of specic and general token types is more useful for
describing the content of information than the character level nite state representations
used in previous work  carrasco   oncina        goan et al          the character level
description is far too ne grained to compactly describe data and  therefore  leads to poor
generality  the coarse grained token level representation is more appropriate for most web
data types  in addition  the data representation schemes used in previous work attempt
to describe the entire data eld  while we use only the starting and ending sequences 
or patterns  of tokens to capture the structure of the data elds  the reason for this is
similar to the one above  using the starting and ending patterns allows us to generalize
the structural information for many complex elds which have a lot of variability  such
elds  e g   addresses  usually have some regularity in how they start and end that we
can exploit  we call the starting and ending patterns collectively a data prototype  as
an example  consider a set of street addresses in fig     all of the examples start with a
   

fiwrapper maintenance

pattern  number upper  and end with a specic type  boulevard  or more generally
 upper   note that the pattern language does not allow loops or recursion  we believe
that recursive expressions are not useful representations of the types of data we are trying
to learn  because they are harder to learn and lead to over generalization 
    learning from positive examples
the problem of learning the data prototype from a set of examples that are labeled as
belonging  or not  to a class may be stated in one of two related ways  as a classication
or as a conservation task  in the classication task  both positive and negative instances of
the class are used to learn a rule that will correctly classify new examples  classication
algorithms  like foil  quinlan         use negative examples to guide the specialization
of the rule  they construct discriminating descriptions  those that are satised by the
positive examples and not the negative examples  the conservation task  on the other hand 
attempts to nd a characteristic description  dietterich   michalski        or conserved
patterns  brazma  jonassen  eidhammer    gilbert         in a set of positive examples of a
class  unlike the discriminating description  the characteristic description will often include
redundant features  for example  when learning a description of street addresses  with city
names serving as negative examples  a classication algorithm will learn that  number  is
a good description  because all the street addresses start with it and none of the city names
do  the capitalized word that follows the number in addresses is a redundant feature 
because it does not add to the discriminating power of the learned description  however  if
an application using this description encounters a zipcode in the future  it will incorrectly
classify it as a street address  this problem could have been avoided if  number upper 
was learned as a description of street addresses  therefore  when negative examples are
not available to the learning algorithm  the description has to capture all the regularity
of data  including the redundant features  in order to correctly identify new instances of
the class and dierentiate them from other classes  ideally  the characteristic description
learned from positive examples alone is the same as the discriminating description learned by
the classication algorithm from positive and negative examples  where negative examples
are drawn from innitely many classes  while most of the widely used machine learning
algorithms  e g   decision trees  quinlan         inductive logic programming  muggleton 
       solve the classication task  there are fewer algorithms that learn characteristic
descriptions 
in our applications  an appropriate source of negative examples is problematic  therefore 
we chose to frame the learning problem as a conservation task  we introduce an algorithm
that learns data prototypes from positive examples of the data eld alone  the algorithm
nds statistically signicant sequences of tokens  a sequence of token types is signicant
if it occurs more frequently than would be expected if the tokens were generated randomly
and independently of one another  in other words  each such sequence constitutes a pattern
that describes many of the positive examples of data and is highly unlikely to have been
generated by chance 
the algorithm estimates the baseline probability of a token types occurrence from the
proportion of all types in the examples of the data eld that are of that type  suppose we
are learning a description of the set of street addresses in fig     and have already found
   

filerman  minton   knoblock

a signicant token sequence  e g   the pattern consisting of the single token  number 
 and want to determine whether the more specic pattern   number upper   is also
a signicant pattern  knowing the probability of occurrence of the type upper  we can
compute how many times upper can be expected to follow number completely by chance 
if we observe a considerably greater number of these sequences  we conclude that the longer
pattern is also signicant 
we use hypothesis testing  papoulis        to decide whether a pattern is signicant 
the null hypothesis is that observed instances of this pattern were generated by chance 
via the random  independent generation of the individual token types  hypothesis testing
decides  at a given condence level  whether the data supports rejecting the null hypothesis 
suppose n identical sequences have been generated by a random source  the probability
that a token type t  whose overall probability of occurrence is p  will be the next type in
k of these sequences has a binomial distribution  for a large n  the binomial distribution
approaches a normal distribution p  x      with    np and      np  p   the cumulative
probability is the probability of observing at least n  events 
p  k  n     

 
n 

p  x     dx

   

we use polynomial approximation formulas  abramowitz   stegun        to compute the
value of the integral 
the signicance level of the test    is the probability that the null hypothesis is rejected
even though it is true  and it is given by the cumulative probability above  suppose we set
         this means that we expect to observe at least n  events    of the time under the
null hypothesis  if the number of observed events is greater  we reject the null hypothesis
 at the given signicance level   i e   decide that the observation is signicant  note that
the hypothesis we test is derived from observation  data   this constraint reduces the
number of degrees of freedom of the test  therefore  we must subtract one from the number
of observed events  this also prevents the anomalous case when a single occurrence of a
rare event is judged to be signicant 
    dataprog algorithm
we now describe dataprog  the algorithm that nds statistically signicant patterns in a
set of token sequences  during the preprocessing step the text is tokenized  and the tokens
are assigned one or more syntactic types  see figure     the patterns are encoded in a
type of prex tree  where each node corresponds to a token type  dataprog relies on
signicance judgements to grow the tree and prune the nodes  every path through the
resulting tree starting at the root node corresponds to a signicant pattern found by the
algorithm  in this section  we focus the discussion on the version of the algorithm that
learns starting patterns  the algorithm is easily adapted to learn ending patterns 
we present the pseudocode of the dataprog algorithm in table    dataprog grows
the pattern tree incrementally by     nding all signicant specializations  i e   longer patterns  of a pattern and     pruning the less signicant of the generalizations  or specializations  among patterns of the same length  as the last step  dataprog extracts all
signicant patterns from the pattern tree  including those generalizations  i e   shorter patterns  found to be signicant given the more specic  i e   longer  patterns 
   

fiwrapper maintenance

dataprog main loop
create root node of tree 
for next node q of tree
create children of q 
prune nodes 
extract patterns from tree 

create children of q
for each token type t at next position in examples
let c   newnode 
let c token   t 
let c examples   q examples that are followed by t 
let c count    c examples  
let c pattern   concat q pattern t   
if signicant c count  q count  t probability 
addchildtotree c  q  
end if
end t loop

prune nodes
for each child c of q
for each sibling s of c s t  s pattern  c pattern
let n   c count  s count
if not signicant n  q count  c token probability  
delete c 
break 
else
delete s 
end if
end s loop
end c loop

extract patterns from tree
create empty list 
for every node q of tree
for every child c of q 
let n   c count  i  si  count si  children c  
if signicant  n  q count  c token probability 
add c pattern to the list 
return  list of patterns  

table    pseudocode of the dataprog algorithm

   

filerman  minton   knoblock

the tree is empty initially  and children are added to the root node  the children
represent all tokens that occur in the rst position in the training examples more often
than expected by chance  for example  when learning addresses from the examples in
fig     the root will have two child nodes  alphanum and number  the tree is extended
incrementally at each node q  a new child is added to q for every signicant specialization
of the pattern ending at q  as explained previously  a child node is judged to be signicant
with respect to its parent node if the number of occurrences of the pattern ending at the
child node is suciently large  given the number of occurrences of the pattern ending at the
parent node and the baseline probability of the token type used to extend the pattern  to
illustrate on our addresses example  suppose we have already found that a pattern  number
upper  is signicant  there are ve ways to extend the tree  see fig     given the data 
 number upper alphanum    number upper alpha    number upper upper  
 number upper street    number upper boulevard   and  number upper way  
all but the last of these patterns are judged to be signicant at          for example 
 number upper upper  is signicant  because upper follows the pattern  number
upper  ve out of ve times   and the probability of observing at least that many longer
sequences purely by chance is          since this probability is less than   we judge this
sequence to be signicant 

root
number
upper

alphanum

alpha

upper

boulevard

street

figure    pattern tree that describes the structure of addresses  dashed lines link to nodes
that are deleted during the pruning step 

the next step is to prune the tree  the algorithm examines each pair of sibling nodes 
one of which is more general than the other  and eliminates the less signicant of the pair 
more precisely  the algorithm iterates through the newly created children of q  from the
most to least general  and for every pair of children ci and cj   such that ci  pattern 
cj  pattern  i e   cj  pattern is strictly more general than ci  pattern   the algorithm keeps
only cj if it explains signicantly more data  otherwise  it keeps only ci    
   such small numbers are used for illustrative purposes only  the typical data sets from which the
patterns are learned are much larger 
   the calculation of this cumulative probability depends on the occurrence probability of upper  we count
the occurrence of each token type independently of the others  in our example  occurrence probability
 relative fraction  of type upper is      
   dataprog is based on an earlier version of the algorithm  datapro  described in the conference paper  lerman   minton         note that in the original version of the algorithm  the specic patterns
were always kept  regardless of whether the more general patterns were found to be signicant or not 

   

fiwrapper maintenance

let us illustrate the pruning step with the example pattern tree in fig     we can eliminate the node alphanum  because all the examples that match the pattern  number upper alphanum  also match the pattern  number upper alpha   thus  alphanum is
not signicant given its specialization alpha  we can eliminate node alpha for a similar
reason  next  we check whether  number upper upper  is signicant given the patterns
 number upper boulevard  and  number upper street   there are   instances of the
address eld that match the pattern  number upper boulevard   and   addresses that
match  number upper street   if  number upper upper  matches signicantly more
than   addresses  it will be retained and the more specic patterns will be pruned from the
tree  otherwise  it will be deleted and the more specic ones kept  because every example
is described by at most one pattern of a given length  the pruning step ensures that the size
of the tree remains polynomial in the number of tokens  thereby  guaranteeing a reasonable
performance of the algorithm 
once the entire tree has been expanded  the nal step is to extract all signicant patterns
from the tree  here  the algorithm judges whether the shorter  more general  pattern  e g  
 number upper   is signicant given the longer specializations of it  e g    number
upper boulevard  and  number upper street   this amounts to testing whether the
excess number of examples that are explained by the shorter pattern  and not by the longer
patterns  is signicant  any pattern that ends at a terminal node of the tree is signicant 
note that the set of signicant patterns may not cover all the examples in the data set  just
a fraction of them that occur more frequently than expected by chance  at some signicance
level   tables    show examples of several data elds from a yellow pages source  bigbook 
and a stock quote source  y ahoo quote   as well as the starting patterns learned for each
eld 

   applications of pattern learning
as we explained in the introduction  wrapper induction systems use information from the
layout of web pages to create data extraction rules and are therefore vulnerable to changes
in the layout  which occur frequently when the site is redesigned  in some cases the wrapper
continues to extract  but the data is no longer correct  the output of the wrapper may also
change because the format of the source data itself has changed  e g   when   is dropped
from the price eld       instead of         or book availability changes from ships
immediately to in stock  ships immediately  because other applications  such as web
agents  ambite et al         chalupsky et al          rely on data extracted by wrappers 
wrapper maintenance is an important research problem  we divide the wrapper maintenance problem into two parts  each described separately in the paper  wrapper verication
automatically detects when a wrapper is not extracting data correctly from a web source 
while wrapper reinduction automatically xes broken wrappers  both applications learn a
description of data  of which patterns learned by dataprog are a signicant part 

this introduced a strong bias for specic patterns into the results  which led to a high proportion of
false positives during the wrapper verication experiments  eliminating the specicity bias  improved
the performance of the algorithm on the verication task 

   

filerman  minton   knoblock

business name
chado tea house
saladang
information sciences institute
chaya venice
acorda therapeutics
cajun kitchen
advanced medical billing services
vega   electrical corporation
  st century foundation
tis the season gift shop
hide sushi japanese restaurant
aoat sushi
prebica coee   cafe
l  orangerie
emils hardware
natalee thai restaurant
casablanca
antica pizzeria
nobu photographic studio
lotus eaters
essex on coney
national restaurant
siam corner cafe
grand casino french bakery
alejo  s presto trattoria
titos tacos mexican restaurant inc
killer shrimp
manhattan wonton co
starting patterns
 alpha upper 
 alpha upper upper restaurant 
 alpha  

address
     west  st street
    south fair oaks avenue
     admiralty way
    navy street
    west   th street
    south fairview avenue
     river road
     east  th street
    east   th street
   lincoln road
     sawtelle boulevard
   east colorado boulevard
     glencoe avenue
    north la cienega boulevard
     south robertson boulevard
    south robertson boulevard
    lincoln boulevard
      maxella avenue
    west   th street
     th avenue
     coney island avenue
    brighton beach avenue
      national boulevard
     main street
     lincoln boulevard
      washington place
    washington boulevard
     melrose place
 number upper upper 
 number upper upper avenue 
 number upper upper boulevard 

table    examples of the business name and address elds from the bigbook source  and
the patterns learned from them

   

fiwrapper maintenance

city
los angeles
pasadena
marina del rey
venice
new york
goleta
marcy
brooklyn
new york
bualo
los angeles
pasadena
marina del rey
west hollywood
los angeles
los angeles
venice
marina del rey
new york
new york
brooklyn
brooklyn
los angeles
culver city
marina del rey
culver city
marina del rey
west hollywood
starting patterns
 upper upper 
 upper upper rey 

state
ca
ca
ca
ca
ny
ca
ny
ny
ny
ny
ca
ca
ca
ca
ca
ca
ca
ca
ny
ny
ny
ny
ca
ca
ca
ca
ca
ca

phone
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           

 allcaps 

    digit    digit   large 

 

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

table    examples of the city  state and phone number elds from the bigbook source  and
the patterns learned from them

   

filerman  minton   knoblock

price change
        
        
        
        
        
         
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
starting patterns
 punct  digit    digit 

ticker
intc
ibm
aol
t
lu
athm
coms
csco
gte
aapl
mot
hwp
dell
gm
cien
egrp
hlit
rimm
c
gps
cflo
dclk
nt
bfre
qcom

volume
              
             
              
             
             
             
              
              
             
             
             
             
              
             
             
             
         
         
             
             
         
             
             
         
             

 allcaps 

 number    digit    digit 

price
         
          
          
         
  
        
          
         
          
         
         
          
         
          
   
        
           
         
          
        
         
   
         
         
          
 medium  digit   number 
 medium          

table    data examples from the y ahoo quote source  and the patterns learned from them

   

fiwrapper maintenance

    wrapper verification
if the data extracted by the wrapper changes signicantly  this is an indication that the
web source may have changed its format  our wrapper verication system uses examples
of data extracted by the wrapper in the past that are known to be correct in order to
acquire a description of the data  the learned description contains features of two types 
patterns learned by dataprog and global numeric features  such as the density of tokens
of a particular type  the application then checks that this description still applies to the
new data extracted by the wrapper  thus  wrapper verication is a specic instance of the
data validation task 
the verication algorithm works in the following way  a set of queries is used to retrieve
html pages from which the wrapper extracts  correct  training examples  the algorithm
then computes the values of a vector of features   k  that describes each eld of the training
examples  these features include the patterns that describe the common beginnings  or
endings  of the eld  during the verication phase  the wrapper generates a set of  new 
test examples from pages retrieved using the same set of queries  and computes the feature
vector  r associated with each eld of the test examples  if the two distributions   k and  r
 see fig      are statistically the same  at some signicance level   the wrapper is judged to
be extracting correctly  otherwise  it is judged to have failed 

  
training set
test set

feature value

  
  
  
 
 
 
 
 
 

 

 

 

 

 

 

feature

figure    a hypothetical distribution of features over the training and test examples

each eld is described by a vector  whose ith component is the value of the ith feature 
such as the number of examples that match pattern j  in addition to patterns  we use the
following numeric features to describe the sets of training and test examples  the average
number of tuples per page  mean number of tokens in the examples  mean token length  and
the density of alphabetic  numeric  html tag and punctuation types  we use goodness of
t method  papoulis       to decide whether the two distributions are the same  to use
the goodness of t method  we must rst compute pearsons test statistic for the data  the
pearsons test statistic is dened as 
   

filerman  minton   knoblock

q 

m

 ti  ei   
i  

ei

   

where ti is the observed value of the ith feature in the test data  and ei is the expected
value for that feature  and m is the number of features  for the patterns ei   nri  n  
where ri is the number of training examples explained by the ith patter  n is the number
of examples in the training set and n is the number of examples in the test set  for numeric
features ei is simply the value of that feature for the training set  the test statistic q has
a chi squared distribution with m    independent degrees of freedom  if q      m       
we conclude that at signicance level  the two distributions are the same  otherwise  we
conclude that they are dierent  values of   for dierent values of  and m can be looked
up in a statistics table or calculated using an approximation formula 
in order to use the test statistic reliably  it helps to use as many independent features as
possible  in the series of verication experiments reported in  lerman   minton         we
used the starting and ending patterns and the average number of tuples per page feature
when computing the value of q  we found that this method tended to overestimate the
test statistic  because the features  starting and ending patterns  were not independent  in
the experiments reported in this paper  we use only the starting patterns  but in order to
increase the number of features  we added numeric features to the description of data 
      results
we monitored    wrappers  representing    distinct web sources  over a period of ten
months  from may      to march       the sources are listed in table    for each wrapper 
the results of      queries were stored periodically  every     days  we used the same
query set for each source  except for the hotel source  because it accepted dated queries 
and we had to change the dates periodically to get valid results  each set of new results
 test examples  was compared with the last correct wrapper output  training examples  
the verication algorithm used dataprog to learn the starting patterns and numeric
features for each eld of the training examples and made a decision at a high signicance
level  corresponding to           about whether the test set was statistically similar to the
training set  if none of the starting patterns matched the test examples or if the data was
found to have changed signicantly for any data eld  we concluded that the wrapper failed
to extract correctly from the source  otherwise  if all the data elds returned statistically
similar data  we concluded that the wrapper was working correctly 
a manual check of the     comparisons revealed    wrapper changes attributable to
changes in the source layout and data format   the verication algorithm correctly discovered    of these changes and made    mistakes  of these mistakes     were false positives 
which means that the verication program decided that the wrapper failed when in reality
it was working correctly  only two of the errors were the more important false negatives 
meaning that the algorithm did not detect a change in the data source  the numbers above
   seven of these were  in fact  internal to the wrapper itself  as when the wrapper was modied to extract
       instead of       for the price eld  because these actions were mostly outside of our control 
we chose to classify them as wrapper changes 

   

fiwrapper maintenance

source
airport
altavista
amazon
arrowlist

type
tuple list
list
tuple
list

bigbook
barnes n oble
borders
cuisinenet

tuple
tuple
list
list

geocoder
hotel
mapquest
northernlight
parking
quote
smartpages
showtimes
theatre
w ashington p ost
whitepages
yahoo people
y ahoo quote
yahoo weather
cia f actbook

tuple
list
tuple
list
list
tuple
tuple
list
list
tuple
list
list
tuple
tuple
tuple

data fields
airport code  name
url  title
book author  title  price  availability  isbn
part number  manufacturer  price  status 
description  url
business name  address  city  state  phone
book author  title  price  availability  isbn
book author  title  price  availability
restaurant name  cuisine  address  city  state 
phone  link
latitude  longitude  street  city  state
name  price  distance  url
hours  minutes  distance  url
url  title
lotname  dailyrate
stock ticker  price  pricechange  volume
name  address  city  state  phone
movie  showtimes
theater name  url  address
taxi price
business name  address  city  state  phone
name  address  city  state  phone
stock ticker  price  pricechange  volume
temperature  forecast
country area  borders  population  etc 

table    list of sources used in the experiments and data elds extracted from them  source
type refers to how much data a source returns in response to a query  a single
tuple or a list of tuples  for airport source  the type changed from a single tuple
to a list over time 

   

filerman  minton   knoblock

result in the following precision  recall and accuracy values 
p

 

r  
a  

true positives
        
true positives   f alse positives
true positives
        
true positives   f alse negatives
true positives   true negatives
        
positives   negatives

these results are an improvement over those reported in  lerman   minton        
which produced p         r         a         the poor precision value reported in that
work was due to    false positives obtained on the same data set  we attribute the improvements both to eliminating the specicity bias in the patterns learned by dataprog
and to changing the feature set to include only the starting patterns and additional numeric
features  note that this improvement does not result simply from adding numeric features 
to check this  we ran the verication experiments on a subset of data  the last     comparisons  using only the global numeric features and obtained p        and r         whereas
using both patterns and numeric features results in values of p        and r        for
the same data set 
      discussion of results
though we have succeeded in signicantly reducing the number of false positives  we have
not managed to eliminate them altogether  there are a number of reasons for their presence 
some of which point to limitations in our approach 
we can split the types of errors into roughly three not entirely independent classes 
improper tokenization  incomplete data coverage  and data format changes  the url eld
 table    accounted for a signicant fraction of the false positives  in large part due to
the design of our tokenizer  which splits text strings on punctuation marks  if the url
contains embedded punctuation  as part of the alphanumeric key associated with the user or
session id   it will be split into a varying number of tokens  so that it is hard to capture the
regularity of the eld  the solution is to rewrite the tokenizer to recognize urls for which
well dened specications exist  we will address this problem in our ongoing work  our
algorithm also failed sometimes  e g   arrowlist  showtimes  when it learned very long and
specic descriptions  it is worth pointing out  however  that it performed correctly in over
two dozen comparisons for these sources  these types of errors are caused by incomplete
data coverage  a larger  more varied training data set would produce more general patterns 
which would perform better on the verication task  a striking example of the data coverage
problem occurred for the stock quotes source  the day the training data was collected  there
were many more down movements in the stock price than up  and the opposite was true on
the day the test data was collected  as a result  the price change elds for those two days
were dissimilar  finally  because dataprog learns the format of data  false positives will
inevitably result from changes in the data format and do not indicate a problem with the
algorithm  this is the case for the factbook source  where the units of area changed from
km  to sq km 
   

fiwrapper maintenance

hotel  mapquest    cases   url eld contains alphanumeric keys  with embedded punctuation symbols  the tokenizer splits the eld into many tokens  the key or its
format changes from 
http         stamp q aaiegsp   itn hot  da      itn agencies newitn      to
http         stamp  beggeqrco itn hot  da      itn agencies newitn     
on one occasion  the server name inside the url changed  from
http   enterprise mapquest com mqmapgend mqmapgenrequest       to
http   sitemap mapquest com mqmapgend mqmapgenrequest      
showtimes  arrowlist    cases    instance of the showtimes eld and part number and
description elds  arrowlist  are very long  many long  overly specic patterns are
learned for these elds  e g  
   number    digit allcaps       small    digit       small    digit            digit
         digit        digit        digit         digit  

altavista    case   database of the search engine appears to have been updated  a dierent
set of results is returned for each query 
quote    case   data changed  there were many more positive than negative price movements in the test examples
factbook    case   data format changed 
f rom  number km   
to  number sq km  
table    list of sources of false positive results on the verication task

   

filerman  minton   knoblock

    wrapper reinduction
if the wrapper stops extracting correctly  the next challenge is to rebuild it automatically  cohen         the extraction rules for our wrappers  muslea et al          as well as
many others  cf   kushmerick et al         hsu   dung          are generated by a machine
learning algorithm  which takes as input several pages from a source and labeled examples
of data to extract from each page  it is assumed that the user labeled all examples correctly  if we label at least a few pages for which the wrapper fails by correctly identifying
examples of data on them  we can use these examples as input to the induction algorithm 
such as stalker   to generate new extraction rules   note that we do not need to identify the data on every page  depending on how regular the data layout is  stalker can
learn extraction rules using a small number of correctly labeled pages  our solution is to
bootstrap the wrapper induction process  which learns landmark based rules  by learning
content based rules  we want to re learn the landmark based rules  because for the types
of sites we use  these rules tend to be much more accurate and ecient than content based
rules 
we employ a method that takes a set of training examples  extracted from the source
when the wrapper was known to be working correctly  and a set of pages from the same
source  and uses a mixture of supervised and unsupervised learning techniques to identify
examples of the data eld on new pages  we assume that the format of data did not
change  patterns learned by dataprog play a signicant role in the reinduction task 
in addition to patterns  other features  such as the length of the training examples and
structural information about pages are used  in fact  because page structure is used during
a critical step of the algorithm  we discuss our approach to learning it in detail in the next
paragraph 
      page template algorithm
many web sources use templates  or page skeletons  to automatically generate pages and
ll them with results of a database query  this is evident in the example in fig     the
template consists of the heading results  followed by the number of results that match
the query  the phrase click links associated with businesses for more information  then
the heading all listings  followed by the anchors map  driving directions  add
to my directory and the bolded phrase appears in the category  obviously  data is not
part of the template  rather  it appears in the slots between template elements 
given two or more example pages from the same source  we can induce the template
used to generate them  table     the template nding algorithm looks for all sequences
of tokens  both html tags and text  that appear exactly once on each page  the
algorithm works in the following way  we pick the smallest page in the set as the template
seed  starting with the rst token on this page  we grow a sequence by appending tokens
   it does not matter  in fact  matter which wrapper induction system is used  we can easily replace
stalker with hlrt  kushmerick et al         to generate extraction rules 
   in this paper we will only discuss wrapper reinduction for information sources that return a single tuple
of results per page  or a detail page  in order to create data extraction rules for sources that return lists
of tuples  the stalker wrapper induction algorithm requires user to specify the rst and last elements
of the list  as well as at least two consecutive elements  therefore  we need to be able to identify these
data elements with a high degree of certainty 

   

fiwrapper maintenance

 a 

 b 
figure    fragments of two web pages from the same source displaying restaurant information 

   

filerman  minton   knoblock

to it  subject to the condition that the sequence appears on every page  if we managed to
build a sequence thats at least three tokens long    and this sequence appears exactly once
on each page  it becomes part of the page template  templates play an important role in
helping identify correct data examples on pages 
input 
p   set of n web pages
output 
t   page template
begin
p   shortest p  
t   null
s   null
for t   rsttoken p  to lasttoken p 
s   concat s  t 
if   s appears on every page in p  
s   s
continue
else

n  n
page   count s  page 
if   n   n and length s      
add to template t  s 
end if
s   null
end if
end for
end
table    pseudocode of the template nding algorithm

      automatic labeling algorithm
figure   is a schematic outline of the reinduction algorithm  which consists of automatic
data labeling and wrapper induction  because the latter aspect is described in detail in
other work  muslea et al          we focus the discussion below on the automatic data
labeling algorithm 
first  dataprog learns the starting and ending patterns that describe the set of training examples  these training examples have been collected during wrappers normal operation  while it was correctly extracting data from the web source  the patterns are used
to identify possible examples of the data eld on the new pages  in addition to patterns 
we also calculate the mean  and its variance  of the number of tokens in the training examples  each new page is then scanned to identify all text segments that begin with one
of the starting patterns and end with one of the ending patterns  text segments that con   the best value for the minimum length for the page template element was determined empirically to be
three 

   

fiwrapper maintenance

extract

extracted
data

wrapper

learn
labeled
web pages

wrapper
induction
system

patterns
apply

web
pages

extracts
score

group

figure    schematic outline of the reinduction algorithm
tain signicantly more or fewer tokens than expected based on the old number of tokens
distribution  are eliminated from the set of candidate extracts  the learned patterns are
often too general and will match many  possibly hundreds  text segments on each page 
among these spurious text segments is the correct example of the data eld  the rest of
the discussion is concerned with identifying the correct examples of data on pages 
we exploit some simple a priori assumptions about the structure of web pages to help
us separate interesting extracts from noise  we expect examples of the same data eld to
appear roughly in the same position and in the same context on each page  for example 
fig    shows fragments of two web pages from the same source displaying restaurant information  on both pages the relevant information about the restaurant appears after the
heading all listings and before the phrase appears in the category   thus  we
expect the same eld  e g   address  to appear in the same place  or slot  within the page
template  moreover  the information we are trying to extract will not usually be part of
the page template  therefore  candidate extracts that are part of the page template can
be eliminated from consideration  restaurant address always follows restaurant name  in
bold  and precedes the city and zip code  i e   it appears in the same context on every page 
a given eld is either visible to the user on every page  or it is invisible  part of an html
tag  on every page  in order to use this information to separate extracts  we describe each
candidate extract by a feature vector  which includes positional information  dened by the
 page template  slot number and context  the context is captured by the adjacent tokens 
one token immediately preceding the candidate extract and one token immediately following it  we also use a binary feature which has the value one if the token is visible to the
user  and zero if it is part of an html tag  once the candidate extracts have been assigned
feature vectors  we split them into groups  so that within each group  the candidate extracts
are described by the same feature vector 
the next step is to score groups based on their similarity to the training examples  we
expect the highest scoring group to contain correct examples of the data eld  one scoring
method involves assigning a rank to the groups based on how many extracts they have
in common with the training examples  this technique generally works well  because at
least some of the data usually remains the same when the web page layout changes  of
   

filerman  minton   knoblock

course  this assumption does not apply to data that changes frequently  such as weather
information  ight arrival times  stock quotes  etc  however  we have found that even in
these sources  there is enough overlap in the data that our approach works  if the scoring
algorithm assigns zero to all groups  i e   there exist no extracts in common with the training
examples  a second scoring algorithm is invoked  this scoring method follows the wrapper
verication procedure and nds the group that is most similar to the training examples
based on the patterns learned from the training examples 
the nal step of the wrapper reinduction process is to provide the extracts in the top
ranking group to the stalker wrapper induction algorithm  muslea et al         along
with the new pages  stalker learns data extraction rules for the changed pages  note
that examples provided to stalker are required to be the correct examples of the eld  if
the set of automatically labeled examples includes false positives  stalker will not learn
correct extraction rules for that eld  false negatives are not a problem  however  if the
reinduction algorithm could not nd the correct example of data on a page  that page is
simply not used in the wrapper induction stage 
      results
to evaluate the reinduction algorithm we used only the ten sources  listed in table    that
returned a single tuple of results per page  a detail page   the method of data collection
was described in sec         over the period between october      and march      there
were eight format changes in these sources  since this set is much too small for evaluation
purposes  we created an articial test set by considering all ten data sets collected for each
source during this period  we evaluated the algorithm by using it to extract data from
web pages for which correct output is known  specically  we took ten tuples from a set
collected on one date  and used this information to extract data from ten pages  randomly
chosen  collected at a later date  regardless of whether the source had actually changed
or not  we reserved the remaining pages collected at a later date for testing the learned
stalker rules 
the output of the reinduction algorithm is a list of tuples extracted from ten pages  as
well as extraction rules generated by stalker for these pages  though in most cases we
were not able to extract every eld on every pages  we can still learn good extraction rules
with stalker as long as few examples of each eld are correctly labeled  we evaluated the
reinduction algorithm in two stages  rst  we checked how many data elds for each source
were identied successfully  second  we checked the quality of the learned stalker rules
by using them to extract data from test pages 
extracting with content based rules we judged a data eld to be successfully extracted if the automatic labeling algorithm was able to identify it correctly on at least two
of the ten pages  this is the minimum number of examples stalker needs to create extraction rules  in practice  such a low success rate only occurred for one eld each in two
   we did not use the geocoder and cia f actbook wrappers in the experiments  the geocoder wrapper
accessed the source through another application  therefore  the pages were not available to us for analysis 
the reason for excluding the f actbook is that it is a plain text source  while our methods apply to web
pages  note also that in the verication experiments  we had two wrappers for the mapquest source 
each extracting dierent data  in the experiments described below  we used the one that contained more
data for this time period 

   

fiwrapper maintenance

of the sources  quote and y ahoo quote  for all other sources  if a eld was successfully
extracted  it was correctly identied in at least three  and in most cases almost all  of the
pages in the set  a false positive occurred when the reinduction algorithm incorrectly identied some text on a page as a correct example of a data eld  in many cases  false positives
consisted of partial elds  e g   cloudy rather than mostly cloudy  yahoo weather  
a false negative occurred when the algorithm did not identify any examples of a data eld 
we ran the reinduction experiment attempting to extract the elds listed in table    the
second column of the table lists the fractions of data sets for which the eld was successfully
extracted  we were able to correctly identify elds     times across all data sets making
   mistakes  of which    were attributed to false positives and    to the false negatives 
there are several reasons the reinduction algorithm failed to operate perfectly  in many
cases the reason was the small training set   we can achieve better learning for the yellowpages type sources bigbook and smartpages by using more training examples  see fig     
in two cases  the errors were attributable to changes in the format of data  which resulted
in the failure of patterns to capture the structure of data correctly  e g   the airport source
changed airport names from capitalized words to allcaps  and in the quote source in which
the patterns were not able to identify negative price changes because they were learned
for a data set in which most of the stocks had a positive price change  for two sources
the reinduction algorithm could not distinguish between correct examples of the eld and
other examples of the same data type  for the quote source  in some cases it extracted
opening price or high price for the stock price eld  while for the yahoo weather source  it
extracted high or low temperature  rather than the current temperature  this problem was
also evident in the smartpages source  where the city name appeared in several places on
the page  in these cases  user intervention or meta analysis of the elds may be necessary
to improve results of data extraction 
extracting with landmark based rules the nal validation experiment consisted of
using the automatically generated wrappers to extract data from test pages  the last three
columns in table   list precision  recall and accuracy for extracting data from test pages 
the performance is very good for most elds  with the notable exception of the state eld
of bigbook source  for that eld  the pattern  allcaps  was overly general  and a wrong
group received the highest score during the scoring step of the reinduction algorithm  the
average precision and recall values were p        and r        
within the data set we studied  ve sources  listed in table    experienced a total of seven
changes  in addition to these sources  the airport source changed the format of the data it
returned  but since it simultaneously changed the presentation of data from a detail page to
a list  we could not use this data to learn stalker rules  table   shows the performance of
the automatically reinduced wrappers for the changed sources  for most elds precision p  
the more important of the performance measures  is close to its maximum value  indicating
that there were few false positives  however  small values of recall indicate that not all
examples of these elds were extracted  this result can be traced to a limitation of our
approach  if the same eld appears in a dierent context  more than one rule is necessary
   limitations in the data collection procedure prevented us from accumulating large data sets for all
sources  therefore  in order to keep the methodology uniform across all sources  we decided to use
smaller training sets 

   

filerman  minton   knoblock

source field
airport code
airport name
amazon author
amazon title
amazon price
amazon isbn
amazon availability
barnes n oble author
barnes n oble title
barnes n oble price
barnes n oble isbn
barnes n oble availability
bigbook name
bigbook street
bigbook city
bigbook state
bigbook phone
mapquest time
mapquest distance
quote pricechange
quote ticker
quote volume
quote shareprice
smartpages name
smartpages street
smartpages city
smartpages state
smartpages phone
y ahoo quote pricechange
y ahoo quote ticker
y ahoo quote volume
y ahoo quote shareprice
w ashington p ost price
w eather temp
w eather outlook
average

ex  
   
  
   
  
   
   
  
   
  
  
   
  
  
  
  
   
  
   
   
  
  
   
  
  
  
 
   
   
   
   
   
  
   
  
  
  

p
   
   
    
    
   
   
   
    
    
   
   
   
   
   
    
    
   
   
   
    
    
   
    
   
   
    
   
    
   
   
   
   
   
    
    
    

r
   
   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
   
    
    
    
    
   
    
   
    

table    reinduction results on ten web sources  the rst column lists the fraction of the
elds for each source that were correctly extracted by the pattern based algorithm 
we judged the eld to be extracted if the algorithm correctly identied at least
two examples of it  the last two columns list precision and recall on the data
extraction task using the reinduced wrappers 

   

fiwrapper maintenance

   

extraction accuracy    

  

  

  
phone
state
city
name
street

  

 
 

 

  

  

  

  

number of training examples

figure    performance of the reinduction algorithm for the elds in the smartpages source
as the size of the training set is increased

source field
amazon author
amazon title
amazon price
amazon isbn
amazon availability
barnes n oble author
barnes n oble title
barnes n oble price
barnes n oble isbn
barnes n oble availability
quote pricechange
quote ticker

p
   
   
   
   
   
   
   
   
   
   
   
   

r
   
   
   
   
   
   
   
   
   
   
   
   

a
   
   
   
   
   
   
   
   
   
   
   
   

source field
smartpages name
smartpages street
smartpages city
smartpages state
smartpages phone
y ahoo quote pricechange
y ahoo quote ticker
y ahoo quote volume
y ahoo quote shareprice
quote volume
quote shareprice

p
   
n a
   
   
n a
   
   
   
   

r
   
   
   
   
   
   
   
   
   

a
   
   
   
   
   
   
   
   
   

   
   

   
n a

   
   

table    precision  recall  and accuracy of the learned stalker rules for the changed
sources

to extract it from a source  in such cases  we extract only a subset of the examples that
share the same context  but ignore the rest of the examples 
as mentioned earlier  we believe we can achieve better performance for the yellow pagestype sources bigbook and smartpages by using more training examples  figure   shows the
eect increasing the size of the training example set on the performance of the automatically
generated wrappers for the smartpages source  as the number of training examples goes
up  the accuracy of most extracted elds goes up 
   

filerman  minton   knoblock

      lists
we have also applied the reinduction algorithm to extract data from pages containing lists
of tuples  and  in many cases  have successfully extracted at least several examples of each
eld from several pages  however  in order to learn the correct extraction rules for sources
returning lists of data  stalker requires that the rst  last and at least two consecutive
list elements be correctly specied  the methods presented here cannot guarantee that
the required list elements are extracted  unless all the list elements are extracted  we are
currently working on new approaches to data extraction from lists  lerman  knoblock   
minton        that will enable us to use stalker to learn the correct data extraction rules 

   previous work
there has been a signicant amount of research activity in the area of pattern learning  in
the section below we discuss two approaches  grammar induction and relational learning 
and compare their performance to dataprog on tasks in the web wrapper application
domain  in section     we review previous work on topics related to wrapper maintenance 
and in section     we discuss related work in information extraction and wrapper induction 
    pattern learning
      grammar induction
several researchers have addressed the problem of learning the structure  or patterns  describing text data  in particular  grammar induction algorithms have been used in the past
to learn the common structure of a set of strings  carrasco and oncina proposed alergia  carrasco   oncina         a stochastic grammar induction algorithm that learns a
regular language from positive examples of the language  alergia starts with a nite
state automaton  fsa  that is initialized to be a prex tree that represents all the strings
of the language  alergia uses a state merging approach  angluin        stolcke   omohundro        in which the fsa is generalized by merging pairs of statistically similar  at
some signicance level  subtrees  similarity is based purely on the relative frequencies of
substrings encoded in the subtrees  the end result is a minimum fsa that is consistent
with the grammar 
goan et al   goan et al         found that when applied to data domains commonly
found on the web  such as addresses  phone numbers  etc   alergia tended to merge
too many states  resulting in an over general grammar  they proposed modications to
alergia  resulting in algorithm wil  aimed at reducing the number of faulty merges  the
modications were motivated by the observation that each symbol in a string belong to one
of the following syntactic categories  number  lower  upper and delim  when
viewed on the syntactic level  data strings contain additional structural information that can
be eectively exploited to reduce the number of faulty merges  wil merges two subtrees if
they are similar  in the alergia sense  and also if  at every level  they contain nodes that
are of the same syntactic type  wil also adds a wildcard generalization step in which the
transitions corresponding to symbols of the same category that are approximately evenly
distributed over the range of that syntactic type  e g      for numerals  are replaced with
a single transition corresponding to the type  e g   number   goan et al  demonstrated
   

fiwrapper maintenance

that the grammars learned by wil were more eective in recognizing new strings in several
relevant web domains 
we compared the performance of wil to dataprog on the wrapper verication task 
we used wil to learn the grammar on the token level using data examples extracted by
the wrappers  not on the character level as was done by goan et al another dierence from
goan et al  was that  whereas they needed on the order of     strings to arrive at a high
accuracy rate  we have on the order of      examples to work with  note that we can
no longer apply the wildcard generalization step to the fsa because we would need many
more examples to decide whether the token is approximately evenly distributed over that
syntactic type  instead  we compare dataprog against two versions of wil  one without
wildcard generalization  wil    and one in which every token in the initial fsa is replaced
by its syntactic type  wil    in addition to the syntactic types used by goan et al   we
also had to introduce another type alnum to be consistent with the patterns learned by
dataprog  neither version of wil allows for multi level generalization 
the algorithms were tested on data extracted by wrappers from    web sources on ten
dierent occasions over a period of several months  see sec        results of      queries
were stored every time  for each wrapper  one data set was used as the training examples 
and the data set extracted on the very next date was used as test examples  we used wil 
and wil  to learn the grammar of each eld of the training examples and then used the
grammar to recognize the test examples  if the grammar recognized more than     of the
test examples of a data eld  we concluded that it recognized the entire data eld  otherwise 
we concluded that the grammar did not recognize the eld  possibly because the data itself
has changed  this is the same procedure we used in the wrapper verication experiments 
and it is described in greater detail in section        over the period of time covered by
the data  there were    occasions on which a web site changed  thereby causing the data
extracted by the wrapper to change as well  the precision and recall values for wil 
 grammar induction on specic tokens  were p         and r         for wil   grammar
induction on wildcards representing tokens syntactic categories  the values were p       
and r         wil  learned an overly specic grammar  which resulted in a high rate
of false positives on the verication task  while wil  learned an overly general grammar 
resulting in slightly more false negatives  the recall and precision value of dataprog for
the same data were p        and r       
recently thollard et al   thollard  dupont    de la higuera        introduced mdi  an
extension to alergia  mdi has been shown to generate better grammars in at least one
domain by reducing the number of faulty merges between states   mdi replaces alergias
state merging criterion with a more global measure that attempts to minimize the kullbackleibler divergence between the learned automaton and the training sample while at the same
time keeping the size of the automaton as small as possible  it is not clear whether mdi
 or a combination of mdi wil  will lead to better grammars for common web data types 
we suspect not  because regular grammars capture just a few of the multitude of data types
found on the web  for example  business names  such as restaurant names shown in table  
may not have a well dened structure  yet many of them start with two capitalized words
and end with the word restaurant  which constitute patterns learned by dataprog 
   

filerman  minton   knoblock

      relational learning
as a sequence of n tokens  a pattern can also be viewed as a non recursive n ary predicate 
therefore  we can use a relation learning algorithm like foil  quinlan        to learn them 
given a set of positive and negative examples of a class  foil learns rst order predicate
logic clauses dening the class  specically  it nds a discriminating description that covers
many positive and none of the negative examples 
we used foil   with the no negative literals option to learn patterns describing several
dierent data elds  in all cases the closed world assumption was used to construct negative
examples from the known objects  thus  for the bigbook source  names and addresses were
the negative examples for the phone number class  we used the following encoding to
translate the training examples to allow foil   to learn logical relations  for each data eld 
foil learned clauses of the form
data f ield a     p  a  f ollowed by a  b  p  b   

   

as a denition of the eld  where a and b are tokens  and the terms on the right hand side
are predicates  the predicate f ollowed by a  b  expresses the sequential relation between
the tokens  the predicate p  a  allows us to specify the token a as a specic token  e g  
john a   or a general type  e g   upper a   alpha a    thus  allowing foil the same
multi level generalization capability as dataprog 
we ran foil   on the examples associated with the bigbook  see tables      the
relational denitions learned by foil   from these examples are shown in table    
in many cases  there were similarities between the denitions learned by foil and
the patterns learned by dataprog  though clauses learned by foil tended to be overly
general  another problem was when given examples of a class with little structure  such
as names and book titles  foil tended to create clauses that covered single examples  or it
failed to nd any clauses  in general  the description learned by foil depended critically
on what we supplied as negative examples of that eld  for example  if we were trying to
learn a denition for book titles in the presence of prices  foil would learn that something
that starts with a capitalized word is a title  if author names were supplied as negative
examples as well  the learned denition would have been dierent  therefore  using foil
in situations where the complete set of negative examples is not known or available  is
problematic 
    wrapper maintenance
kushmerick  kushmerick        addressed the problem of wrapper verication by proposing
an algorithm rapture to verify that a wrapper correctly extracts data from a web page 
in that work  each data eld was described by a collection of global features  such as
word count  average word length  and density of types  i e   proportion of characters in the
training examples that are of an html  alphabetic  or numeric type  rapture calculated
the mean and variance of each features distribution over the training examples  given a
set of queries for which the wrapper output is known  rapture generates a new result for
each query and calculates the probability of generating the observed value for every feature 
individual feature probabilities are then combined to produce an overall probability that
the wrapper has extracted data correctly  if this probability exceeds a certain threshold 
   

fiwrapper maintenance

    warning 
name a 
name a 
name a 

the following definition does not cover    tuples in the relation
   allcaps a   followed by a b 
   upper a   followed by a b   number b 
   followed by a b   venice b 

street a     large a   followed by a b 
street a     medium a   followed by a b   alphanum b 

   warning 
city a 
city a 
city a 
city a 
city a 

the following definition does not cover   tuples in the relation
   los a 
   marina a 
   new a 
   brooklyn a 
   west a   followed by a b   alpha b 

state a     ca a 
state a     ny a 
phone a       a 

table     denitions learned by foil   for the bigbook source
rapture decides that the wrapper is correct  otherwise  that it has failed  kushmerick
found that the html density alone can correctly identify almost all of the changes in
the sources he monitored  in fact  adding other features in the probability calculation
signicantly reduced algorithms performance  we compared raptures performance on
the verication task to our approach  and found that rapture missed    wrapper changes
 false negatives  if it relied solely on the html density feature    
there has been relatively little prior work on the wrapper reinduction problem  cohen  cohen        adapted whirl  a soft logic that incorporates a notion of statistical
text similarity  to recognize page structure of a narrow class of pages  those containing
simple lists and simple hotlists  dened as anchor url pairs   previously extracted data 
combined with page structure recognition heuristics  was used to reconstruct the wrapper
once the page structure changed  cohen conducted wrapper maintenance experiments using original data and corrupted data as examples for whirl  however  his procedure for
corrupting data was neither realistic nor representative of how data on the web changes 
although we cannot at present guarantee good performance of our algorithm on the wrapper reinduction for sources containing lists  we handle the realistic data changes in web
sources returning detail pages 
    although we use a dierent statistical test and cannot compare the performance of our algorithm to
rapture directly  we doubt that it would outperform our algorithm on our data set if it used all global
numeric features  because  as we noted in section        using patterns as well as global numeric features
in the verication task outperforms using numeric features only 

   

filerman  minton   knoblock

    information extraction
our system  as used in the reinduction task  is related in spirit to the many information
extraction  ie  systems developed both by our group and others in that it uses a learned
representation of data to extract information from specic texts  like wrapper induction
systems  see  muslea et al         kushmerick et al         freitag   kushmerick         
it is domain independent and works best with semi structured data  e g   web pages  it
does not handle free text as well as other systems  such as autoslog  rilo        and
whisk  soderland         because free text has fewer non trivial regularities the algorithm
can exploit  unlike wrapper induction  it does not extract data based on the features that
appear near it in text  but rather based on the content of data itself  however  unlike whisk 
which also learns content rules  our reinduction system represents each eld independently
of the other elds  which can be an advantage  for instance  when a web source changes the
order in which data elds appear  another dierence is that our system is designed to run
automatically  without requiring any user interaction to label informative examples  in the
main part because it is purely automatic  the reinduction system fails to achieve the accuracy
of other ie systems which rely on labeled examples to train the system  however  we do
not see it as a major limitation  since it was designed to complement existing extraction
tools  rather than supersede them  in other words  we consider the reinduction task to be
successful if it can accurately extract a sucient number of examples to use in a wrapper
induction system  the system can then use the resulting wrapper to accurately extract the
rest of the data from the source 
there are many similarities between our approach and that used by the roadrunner system  developed concurrently with our system and reported recently in  crescenzi 
mecca    merialdo      b      a   the goal of that system is to automatically extract
data from web sources by exploiting similarities in page structure across multiple pages 
roadrunner works by inducing the grammar of web pages by comparing several pages
containing long lists of data  the grammar is expressed at the html tag level  so it is
similar to the extraction rules generated by stalker  the roadrunner system has been
shown to successfully extract data from several web sites  the two signicant dierences
between that work and ours are  i  they do not have a way of detecting changes to know
when the wrapper has to be rebuilt and  ii  our reinduction algorithm works on detail pages
only  while roadrunner works only on lists  we believe that our data centric approach is
more exible and will allow us to extract data from more diverse information sources than
the roadrunner approach that only looks at page structure 

   conclusion
in this paper we have described the dataprog algorithm  which learns structural information about a data eld from a set of examples of the eld  we use these patterns in
two web wrapper maintenance applications   i  verification  detecting when a wrapper
stops extracting data correctly from a web source  and  ii  reinduction  identifying new
examples of the data eld in order to rebuild the wrapper if it stops working  the verication algorithm performed with an accuracy of      much better than results reported
in our earlier work  lerman   minton         in the reinduction task  the patterns were
used to identify a large number of data elds on web pages  which were in turn used to
   

fiwrapper maintenance

automatically learn stalker rules for these web sources  the new extraction rules were
validated by using them to successfully extract data from sets of test pages 
there remains work to be done on wrapper maintenance  our current algorithms are not
sucient to automatically re generate stalker rules for sources that return lists of tuples 
however  preliminary results indicate  lerman et al         that it is feasible to combine
information about the structure of data with a priori expectations about the structure of
web pages containing lists to automatically extract data from lists and assign it to rows and
columns  we believe that these techniques will eventually eliminate the need for the user to
mark up web pages and enable us to automatically generate wrappers for web sources  another exciting direction for future work is using the dataprog algorithm to automatically
create wrappers for new sources in some domain given existing wrappers for other sources
in the same domain  for example  we can learn the author  title and price elds for the
amazonbooks source  and use them to extract the same elds on the barnes n oblebooks
source  preliminary results show that this is indeed feasible  automatic wrapper generation
is an important cornerstone of information based applications  including web agents 

   acknowledgments
we would like to thank priyanka pushkarna for carrying out the wrapper verication experiments 
the research reported here was supported in part by the defense advanced research
projects agency  darpa  and air force research laboratory under contract agreement
numbers f         c       f                 f                 in part by the air force
oce of scientic research under grant number f                 in part by the integrated
media systems center  a national science foundation  nsf  engineering research center 
cooperative agreement number eec         and in part by the nsf under award number
dmi          the u s  government is authorized to reproduce and distribute reports for
governmental purposes notwithstanding any copy right annotation thereon  the views
and conclusions contained herein are those of the authors and should not be interpreted as
necessarily representing the ocial policies or endorsements  either expressed or implied  of
any of the above organizations or any person connected with them 

references
abramowitz  m     stegun  i  a          handbook of mathematical functions with formulas  graphs and mathematical tables  applied math  series     national bureau of
standards  washington  d c 
ambite  j  l   barish  g   knoblock  c  a   muslea  m   oh  j     minton  s         
getting from here to there  interactive planning and agent execution for optimizing
travel  in the fourteenth innovative applications of articial intelligence conference
 iaai        edmonton  alberta  canada       
angluin  d          inference of reversible languages  journal of the acm                 
   

filerman  minton   knoblock

brazma  a   jonassen  i   eidhammer  i     gilbert  d          approaches to the automatic discovery of patterns in biosequences  tech  rep   department of informatics 
university of bergen 
carrasco  r  c     oncina  j          learning stochastic regular grammars by means of a
state merging method  lecture notes in computer science           
chalupsky  h   et al          electric elves  applying agent technology to support human
organizations  in proceedings of the thirteenth annual conference on innovative
applications of articial intelligence  iaai        seattle  wa 
cohen  w  w          recognizing structure in web pages using similarity queries  in proc 
of the   th national conference on articial intelligence  aaai        pp       
crescenzi  v   mecca  g     merialdo  p       a   automatic web information extraction
in the roadrunner system  in proceedings of the international workshop on data
semantics in web information systems  daswis       
crescenzi  v   mecca  g     merialdo  p       b   roadrunner  towards automatic data
extraction from large web sites  in proceedings of the   th conference on very large
databases  vldb  rome  italy 
dietterich  t     michalski  r          inductive learning of structural descriptions   articial intelligence             
doorenbos  r  b   etzioni  o     weld  d  s          a scalable comparison shopping
agent for the world wide webs  in proceeding of the first international confence on
autonomous agents  marina del rey 
freitag  d     kushmerick  n          boosted wrapper induction  in proceedings of the  th
conference on articial intelligence  aaai        pp          aaai press  menlo
park  ca 
goan  t   benson  n     etzioni  o          a grammar inference algorithm for the world
wide web   in proceedings of aaai spring symposium on machine learning in information access  stanford university  ca 
hsu  c  n     dung  m  t          generating nite state transducers for semi structured
data extraction from the web  journal of information systems             
knoblock  c  a   lerman  k   minton  s     muslea  i       a   accurately and reliably
extracting data from the web  a machine learning approach  ieee data engineering
bulletin               
knoblock  c  a   minton  s   ambite  j  l   muslea  m   oh  j       frank  m       b  
mixed initiative  multi source information assistants  in the tenth international
world wide web conference  www     hong kong 
kushmerick  n          regression testing for wrapper maintenance   in proceedings of the
  th national conference on articial intelligence  aaai       
   

fiwrapper maintenance

kushmerick  n   weld  d  s     doorenbos  r  b          wrapper induction for information extraction  in proceedings of the intl  joint conference on articial intelligence
 ijcai   pp         
lerman  k   knoblock  c  a     minton  s          automatic data extraction from lists and
tables in web sources  in proceedings of the workshop on advances in text extraction
and mining  ijcai       menlo park  aaai press 
lerman  k     minton  s          learning the common structure of data  in proceedings
of the   th national conference on articial intelligence  aaai       menlo park 
aaai press 
muggleton  s          inductive logic programming  new generation computing        
    
muslea  i   minton  s     knoblock  c          wrapper induction for semistructured webbased information sources   in proceedings of the conference on automated learning
and discovery  conald  
muslea  i   minton  s     knoblock  c  a          hierarchical wrapper induction for
semistructured information sources  autonomous agents and multi agent systems    
      
papoulis  a          probability and statistics  prentice hall  englewood clis  nj 
quinlan  j  r          learning logical denitions from relations   machine learning        
       
quinlan  j  r          c     programs for machine learning  morgan kaufmann  san
mateo  ca 
rilo  e          automatically constructing a dictionary for information extraction tasks 
in proceedings of the   th national conference on articial intelligence  pp        
menlo park  ca  usa  aaai press 
soderland  s          learning information extraction rules for semi structured and free
text  machine learning                   
stolcke  a     omohundro  s          inference of nite state probabilistic grammars  in
proceedings of the  nd int  colloquium on grammar induction   icgi      pp     
    
thollard  f   dupont  p     de la higuera  c          probabilistic dfa inference using
kullback leibler divergence and minimality  in proceedings of the   th international
conf  on machine learning  pp          morgan kaufmann  san francisco  ca 

   

fi