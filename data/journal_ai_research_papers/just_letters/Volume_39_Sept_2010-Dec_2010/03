journal artificial intelligence research                 

submitted        published      

clustering want 
inducing ideal clustering minimal feedback
sajib dasgupta
vincent ng

sajib hlt utdallas edu
vince hlt utdallas edu

human language technology research institute
university texas dallas
    west campbell road  mail station ec  
richardson  tx            u s a 

abstract
traditional research text clustering largely focused grouping documents
topic  conceivable user may want cluster documents along dimensions 
authors mood  gender  age  sentiment  without knowing users intention 
clustering algorithm group documents along prominent dimension 
may one user desires  address problem clustering documents
along user desired dimension  previous work focused learning similarity metric
data manually annotated users intention human construct
feature space interactive manner clustering process  goal
reducing reliance human knowledge fine tuning similarity function selecting
relevant features required approaches  propose novel active clustering
algorithm  allows user easily select dimension along wants
cluster documents inspecting small number words  demonstrate
viability algorithm variety commonly used sentiment datasets 

   introduction
text clustering one major application domains demonstrating viability
clustering algorithm  traditional research text clustering largely focused
grouping documents topic  conceivable user may want cluster documents along dimensions  authors mood  gender  age  sentiment  since
virtually existing text clustering algorithms produce one clustering given
set documents  natural question is  clustering necessarily one user desires  words  text clustering algorithm always produce clustering along
user desired dimension 
answer question depends large extent whether user successfully communicate intention clustering algorithm  traditionally 
achieved designing good similarity function capture similarity
pair documents  ideal clustering produced  typically involves
identify set features useful inducing desired clusters  liu  li 
lee    yu         however  manually identifying right set features timeconsuming knowledge intensive  may even require lot domain expertise 
fact resulting similarity function typically easily portable domains
particularly unappealing machine learning perspective  overcome weakness 
c
    
ai access foundation  rights reserved 

fidasgupta   ng

researchers attempted learn similarity metric side information  xing  ng 
jordan    russell         constraints pairs documents must must
appear cluster  wagstaff  cardie  rogers    schrodl        
contrast  recent work focused active clustering  clustering algorithm
incorporate user feedback clustering process help ensure documents grouped according user desired dimension  one way
user incrementally construct set relevant features interactive fashion  bekkerman  raghavan  allan    eguchi        raghavan   allan        roth   small        
another way user correct mistakes made clustering algorithm
clustering iteration specifying whether two existing clusters merged
split  balcan   blum         major drawback associated active clustering
algorithms involve considerable amount human feedback  needs
provided iteration clustering process  furthermore  identifying clusters
merging splitting balcan blums algorithm may easy appears 
merge split decision user makes  sample large number
documents cluster s   read documents  base decision
extent documents  dis similar other 
article  attack problem clustering documents according user interest
different angle  aim knowledge lean approach problem
approach produce clustering documents along user desired dimension
without relying human knowledge fine tuning similarity function selecting
relevant features  unlike existing approaches  end  propose novel active
clustering algorithm  assumes input simple feature representation  composed
unigrams only  simple similarity function  i e   dot product   operates
    inducing important clustering dimensions  given set documents 
clustering dimension represented  small  number automatically selected words
representative dimension      user choose dimension along
wants cluster documents examining automatically selected words 
comparison aforementioned feedback mechanisms  arguably much simpler 
require user cursory look small number features
dimension all  opposed user generate feature space
interactive manner identify clusters need merged split clustering
iteration 
evaluate active clustering algorithm task sentiment based clustering 
goal cluster set documents  e g   reviews  according polarity
 e g   thumbs thumbs down  expressed author without using labeled
data  decision focus sentiment based clustering motivated several reasons 
one reason relatively little work sentiment based clustering 
mentioned before  existing work text clustering focused topic based clustering 
high accuracies achieved even datasets large number classes
 e g      newsgroups   despite large amount recent work sentiment analysis
   use term clustering dimension refer dimension along set documents
clustered  example  set movie reviews clustered according genre  e g   action  romantic 
documentary  sentiment  e g   positive  negative  neutral  

   

fiinducing ideal clustering minimal feedback

review  
sound system seem little better
 the cds skipping much   bottom line
didnt fix problem cds still skipping noticeably 
although bad before     
review  
john lynch wrote classic spanish american revolutions           
describes events led independence latin america spain 
book starts rio de la plata ends mexico central america 
curiously one note common pattern highly stratified societies lead spanish    
reluctance spanish monarchy  and later even liberals  led independence    
interested better understanding latin   this great book must 
lynch cleverly combines historical economic facts hispanic american societies    

table    snippets two reviews illustrate two challenges polarity classification 
one reviews sentimentally ambiguous  review    
objective materials review significantly outnumber subjective counterparts
 review    

opinion mining  much focused supervised methods  see pang   lee       
comprehensive survey field  
another equally important reason focus sentiment based clustering concerned challenges task presents natural language processing  nlp 
researchers  broadly speaking  complexity sentiment based clustering arises
two sources  first  reviews sentimentally ambiguous  containing positive negative sentiment bearing words phrases  review   table   shows snippet
review dvd domain illustrates sentimental ambiguity problem 
phrases little better  skipping  bad convey positive sentiment 
phrases didnt fix skipping noticeably negative sentiment bearing  hence 
unless sentiment analyzer performs deeper linguistic analysis  difficult
analyzer determine polarity review  second  objective materials review tend significantly outnumber subjective counterparts  reviewer typically
devotes large portion review describing features product assigning rating it  consequently  sentiment analyzer uses word  phrase based
feature representation composed mostly features irrelevant respect
polarity determination  shown review   table   snippet book review
illustrates problem  see  three words phrases  classic  great
book  cleverly  review correspond objective materials 
aforementioned complications present significant challenges even supervised polarity classification systems  let alone sentiment based clustering algorithms 
access labeled data  illustrate difficulty two complications impose sentiment based clustering  consider task clustering set movie
reviews  since review may contain description plot authors sentiment 
clustering algorithm may cluster reviews along either plot dimension sentiment
dimension  without knowing users intention  clustered along
   

fidasgupta   ng

prominent dimension  assuming usual bag of words representation  prominent
dimension likely plot  uncommon review devoted almost
exclusively plot  author briefly expressing sentiment end
review  even reviews contain mostly subjective materials  prominent
dimension may still sentiment owing aforementioned sentimental ambiguity
problem  presence positive negative sentiment bearing words reviews renders sentiment dimension hidden  i e   less prominent  far clustering
concerned 
sum  contributions article five fold 
propose novel active clustering algorithm cluster set documents
along user desired dimension without labeled data side information
manually specified automatically acquired must link cannot link constraints 
comparison existing active clustering approaches  algorithm appeal
requiring much simpler human feedback 
demonstrate viability algorithm evaluating performance
sentiment datasets  via set human experiments  typically
absent papers involve algorithms incorporating user feedback 
results led deeper understanding spectral clustering  specifically 
propose novel application top eigenvectors produced spectral clustering
algorithm  use unveil important clustering dimensions text
collection 
results implications domain adaptation  topic recently
received lot attention nlp community  specifically  show
sentiment dimension manually identified one domain used automatically
identify sentiment dimension new  similar  domain 
preliminary results datasets possess one clustering dimension  e g  
collection book dvd reviews  clustered sentiment
type product concerned  indicate algorithm capable producing
multiple clusterings dataset  one along dimension  hence  algorithm
potentially reveal information dataset possible traditional
clustering algorithms  produce single clustering data 
ability produce multiple clusterings particularly useful feature user
idea wants documents clustered  due
lack knowledge data  instance   even user knowledge
data knows wants documents clustered  algorithm help
unveil hidden dimensions previously aware may
interest her 
rest article organized follows  section   presents basics spectral
clustering  facilitate discussion active clustering algorithm section
   describe human experiments evaluation results several sentiment datasets
section   significance work section    finally  discuss related work
section   conclude section   
   

fiinducing ideal clustering minimal feedback

   spectral clustering
given clustering task  important question ask is  clustering algorithm
use  popular choice k means  nevertheless  well known k means
major drawback able separate data points linearly separable
given feature space  e g   see dhillon  guan    kulis        cai  he    han        
moreover  since k means clusters documents directly given feature space 
text applications typically comprises hundreds thousands features  performance
could adversely affected curse dimensionality  spectral clustering algorithms
developed response problems k means  section  first present
one commonly used algorithms spectral clustering  section       then 
provide intuition behind spectral clustering  section       finally  describe two ways
use resulting eigenvectors produce clustering  section      
    algorithm
let x  x            xn   set n data points clustered    x x similarity
function defined x  similarity matrix captures pairwise similarities
 i e   si j   s xi   xj     many clustering algorithms  spectral clustering algorithm
takes input outputs k way partition c    c    c        ck    i e   ki   ci   x
i  j      j   ci cj      equivalently  one think spectral clustering learning
partitioning function f   which  rest article  represented vector
f  i             k  indicates cluster xi assigned  note
cluster labels interchangeable even renamed without loss
generality 
among well known spectral clustering algorithms  e g   weiss        shi   malik 
      kannan  vempala    vetta         adopt one proposed ng  jordan 
weiss         arguably widely used  main steps ng et
al s spectral clustering algorithm 
   create diagonal matrix whose  i i  th entry sum i th row s 
construct laplacian matrix  l       sd      
   find eigenvalues eigenvectors l 
   create new matrix eigenvectors correspond largest eigenvalues 
   data point rank reduced point m dimensional space  normalize
point unit length  while retaining sign value  
   apply k means cluster data points using resulting eigenvectors 
words  spectral clustering clusters data points low dimensional space 
dimension corresponds top eigenvector laplacian matrix 
   follow ng et al         employ normalized dual form usual laplacian s 

   

fidasgupta   ng

    intuition behind spectral clustering
may immediately clear spectral clustering produces meaningful partitioning set points  theoretical justifications behind spectral clustering 
since mathematics quite involved  provide intuitive justification
clustering technique way sufficient reader understand active
clustering algorithm section    refer interested reader shi maliks       
seminal paper spectral clustering details  since apply spectral clustering
produce   way clustering given set data points rest article 
center discussion   way clustering subsection 
spectral clustering employs graph theoretic notion grouping  specifically  set
data points arbitrary feature space represented undirected weighted graph 
node corresponds data point  edge weight two nodes xi
xj similarity  si j  
given graph formulation  reasonable way produce   way partitioning
data points minimize similarity resulting two clusters  c  c   
hence  reasonable objective function minimize cut value 
x
cut c    c     
si j  f  i  f  j     
i j

without loss generality  define f follows 

    c 
f  i   
    c 
mentioned before  use     cluster labels here  interchangeable
fact renamed whatever way want 
one problem minimizing cut value  noticed wu leahy        
objective favors producing unbalanced clusters one contains
small number nodes  words  bias towards isolating small set
nodes  mentioned shi malik         surprising  since
number edges involved cut  and hence cut value  tends increase sizes
two clusters become relatively balanced 
closer examination minimum cut criterion reveals problem  minimizes inter cluster similarity  makes attempt maximize intra cluster similarity 
address weakness  shi malik        propose minimize instead normalized
cut value  n cut  takes account inter cluster dissimilarity intra cluster
similarity  specifically 
cut c    c   
cut c    c   
 
 
assoc c    c  c    assoc c    c  c   
p
assoc a  b   computed xi a xj b si j   total connection nodes
nodes b  given definition  cut resulting unbalanced clusters
longer small n cut value  see reason  consider case c  consists
one node  case  assoc c    c  c      cut c    c     making n cut c    c    large 
n cut c    c     

   

fiinducing ideal clustering minimal feedback

algebra  express n cut follows 
n cut c    c     

f  d s f
f df

subject constraints  df  t      
rp
d i 


pic 

ic  d i 
rp
f  i   
d i 


pic  d i 
ic 

  c 
  c 

d i    d i  i   defined section       first constraint  specifies
df orthogonal    intuitively understood follows  since    constant
vector entries    cannot used induce partition  constraint
avoids trivial solution points assigned cluster 
unfortunately  papadimitriou proves minimizing normalized cut np complete
problem  even special case graphs regular grids  see shi   malik       
proof   hence  following shi malik  relax minimization problem dropping
second constraint allowing entry f take real value rather one
two discrete values  seeking real valued solution following problem 
minn

f

f  d s f
f df

   

subject
df   
assuming g       f   rewrite problem    
minn

g

gt      d s d     g
gt g

   

subject
g       
following standard rayleigh ritz theorem  one prove solution
problem      g  eigenvector corresponds second smallest eigenvalue
     d s d       equivalently  eigenvector corresponds second largest
eigenvector d    sd       laplacian matrix l defined section     
simplicity  henceforth refer eigenvector corresponds n th largest
eigenvalue l simply n th eigenvector denote en   
   besides normalized cut  ratio cut  chan  schlag    zien         average association  shi   malik        
min max cut  ding  he  zha  gu    simon        used objective functions
spectral clustering algorithms 
   given problem     involves minimizing rayleigh quotient  may seem somewhat unintuitive
solution second eigenvector l rather first eigenvector  reason attributed
constraint associated problem  specifies solution g perpendicular
d       first eigenvector l 

   

fidasgupta   ng

idea behind spectral clustering  second eigenvector l approximate solution problem minimizing normalized cut   course  since second
eigenvector real valued solution  convert partitioning function
used cluster data points  section     explains two simple ways
converting eigenvector partitioning function 
turns eigenvectors l convey useful information
data  specifically  impose additional constraint problem     forcing solution orthogonal second eigenvector l  solution becomes third
eigenvector  hence  third eigenvector thought suboptimal solution
problem      meaning used impose reasonably good partition
data points  perhaps importantly  since eigenvectors l orthogonal
 because l symmetric   clustering produced using third eigenvector
likely correspond different dimension data produced second
eigenvector 
generally  limit solution space real valued vectors
orthogonal first eigenvectors l  solution constrained optimization
problem  m      th eigenvector l  words  top eigenvectors
l intuitively thought revealing important dimension data  although
subsequent eigenvectors progressively less ideal far clustering concerned 
    clustering eigenvectors
ng et al         point out  different authors still disagree eigenvectors
use  derive clusters them  subsection  describe two common
methods determining eigenvectors use  method  show
derive clusters using selected eigenvector s   methods serve baselines
evaluation 
      method    using second eigenvector
since shi malik        show second eigenvector  e    approximate solution
problem minimizing normalized cut  perhaps surprising
e  commonly chosen eigenvector deriving partition  however  since e 
real valued solution constrained optimization problem  need specify
derive clusters it 
clustering using e  trivial  since linearization points  one simple way
determine threshold partitioning them  however  follow ng et al        
cluster points using   means one dimensional space 
      method    using top eigenvectors
recall section     eigen decomposing laplacian matrix  data point
represented co ordinates  second method  use   means cluster data
points m dimensional space  effectively exploiting top eigenvectors 
   fact  since f   d    g  pre multiply second eigenvector l d    get
solution problem      following ng et al          employ second eigenvector l directly
clustering  ignoring term d     

   

fiinducing ideal clustering minimal feedback

   active clustering algorithm
mentioned before  sentiment based clustering challenging  part due fact
reviews clustered along one dimension  section  describe
active clustering algorithm  makes easy user specify dimension
along wants cluster data points sentiment  recall algorithm first
applies spectral clustering reveal important dimensions data 
lets user select desired dimension  i e   sentiment   motivate importance
user feedback  helps understand two baseline clustering algorithms described
section      based spectral methods rely user feedback  may always yield sentiment based clustering  begin with  consider first
method  second eigenvector used induce partition  recall
second eigenvector reveals prominent dimension data  hence  sentiment
prominent dimension  which happen non sentiment bearing words
outnumber sentiment bearing words bag of words representation review  
resulting clustering reviews may sentiment oriented  similar line
reasoning used explain second baseline clustering algorithm 
clusters based top eigenvectors  may always work well  since eigenvector corresponds different dimension  and  particular  correspond
non sentiment dimensions   using represent review may hamper accurate computation similarity two reviews far clustering along sentiment
dimension concerned  rest section  discuss detail major steps
active clustering algorithm  allows easy incorporation user feedback 
    step    identify important clustering dimensions
rely simple method identifying important clustering dimensions given
text collection  employ top eigenvectors laplacian important clustering dimensions  method motivated fact e    second eigenvector
laplacian  optimal real valued solution objective function spectral
clustering minimizes  i e   normalized cut  shi   malik         therefore optimal
clustering dimension  importantly  exploit rarely utilized observation discussed
section      remaining eigenvectors suboptimal solutions  with ei suboptimal increases   top eigenvectors  i e   small values  
less suboptimal  may still yield reasonably good  though optimal  clusterings
data therefore serve good clustering dimensions  existing applications
spectral clustering mainly clustered data points space defined top
eigenvectors  attempted use ei  with      separately
produce clusterings  unlike ours  note first eigenvector  constant vector 
simply assigns data points cluster therefore typically ignored 
    step    identify relevant features partition
given eigen decomposition step    first obtain second m th
eigenvectors  correspond important dimensions data  next
question is  determine dimension captures user interest  one way
   

fidasgupta   ng

user inspect m  partitions reviews decide
corresponds closely sentiment based clustering  main drawback associated
kind user feedback user may read large number reviews
order make decision  hence  reduce human effort  employ alternative
procedure      identify informative features characterizing partition 
    user inspect features rather reviews  make easy
human identify clustering dimension  features chosen
useful distinguishing reviews two clusters 
identify rank informative features  employ method call maximum
margin feature ranking  mmfr    recall maximum margin classifier  e g   support
vector machine  separates data points two classes maximizing margin
separation  specifically  maximum margin hyperplane defined w x b     
x feature vector representing arbitrary data point  w  a weight vector  b  a
scalar  parameters learned solving following constrained optimization
problem 
x
 

min kwk    c
 

subject
ci  w xi b     

  n 

ci         class i th training point xi   degree misclassification xi   c regularization parameter balances training error model
complexity 
use w identify informative features partition  note
informative features large absolute weight values  feature large
positive  negative  weight strongly indicative positive  negative  class   exploit
observation identify informative features partition     training
binary svm classifier  partition  data points cluster assumed
class value      sorting features according svm learned feature
weights      generating two ranked lists informative features using top bottom
f features  respectively 
given ranked lists generated   partitions  user select one
partitions dimensions relevant sentiment inspecting many features
ranked lists needed  picking relevant dimension  user
label one two feature lists associated dimension positive
negative  since feature list represents one clusters  cluster associated
positive list labeled positive cluster associated negative list
labeled negative 
   note commonly used feature selection techniques log likelihood ratio information
gain applied identify informative features  see yang   pedersen       
overview  
   notion using svm feature weights measures feature informativeness explored
work  see  instance  work fung         gilad bachrach  navot  tishby        
kugler  aoki  kuroyanagi  iwata  nugroho        details 
   svm classifiers article trained using svmlight package  joachims      a  
learning parameters set default values 

   

fiinducing ideal clustering minimal feedback

comparison existing user feedback mechanisms assisting clustering algorithm 
requires comparatively little human intervention  require user select
dimension examining small number features  opposed user construct
feature space identify clusters need merged split required
methods 
    step    identify unambiguous reviews
caveat  however  mentioned introduction  many reviews contain
positive negative sentiment bearing words  ambiguous reviews likely
clustered incorrectly unambiguous counterparts  since ranked lists
features derived partition  presence ambiguous reviews
adversely affect identification informative features using mmfr  result 
remove ambiguous reviews deriving informative features partition 
employ simple method identifying unambiguous reviews  computation
eigenvalues  data point factors orthogonal projections
data points affinity  ambiguous data points receive orthogonal
projections positive negative data points  hence near zero
values pivot eigenvectors  words  points near zero values
eigenvectors ambiguous large absolute values  therefore sort
data points according corresponding values eigenvector  keep
top n   bottom n   data points  induce informative features
resulting     data points  present user select
desired partition  
    step    cluster along selected eigenvector
finally  employ   means cluster reviews along eigenvector selected
user  regardless whether review ambiguous not 

   evaluation
section  describe experiments aim evaluate effectiveness active
clustering algorithm provide insights it 
    experimental setup
begin discussing details datasets  document preprocessing method 
implementation spectral clustering  evaluation metrics 
   note     somewhat arbitrary choice  underlying choice merely assumption
fraction reviews unambiguous  see evaluation section  reviews
classified according polarity high accuracy  consequently  features induced
resulting clusters high quality  additional experiments revealed list top ranking
features change significantly induced smaller number unambiguous reviews 

   

fidasgupta   ng

      datasets
use five sentiment datasets  including widely used movie review dataset  mov 
 pang  lee    vaithyanathan        well four datasets containing reviews four
different types products amazon  books  boo   dvds  dvd   electronics  ele  
kitchen appliances  kit    blitzer  dredze    pereira         dataset     
labeled reviews       positives      negatives   illustrate difference
topic based clustering sentiment based clustering  show topic based clustering results pol  dataset created taking documents two sections
   newsgroups discuss issues cryptography politics  namely  sci crypt
talks politics 
      document preprocessing
preprocess document  first tokenize downcase it  represent
vector unstemmed unigrams  assumes value     indicates
presence absence document  addition  remove vector punctuation 
numbers  words length one  words occur single review 
following common practice information retrieval community  exclude
words high document frequency  many stopwords domain specific
general purpose words  e g   movies movie domain   preliminary examination
evaluation datasets reveals words typically comprise     vocabulary 
decision exactly many terms remove dataset subjective  large
corpus typically requires removals small corpus  consistent  simply
sort vocabulary document frequency remove top       henceforth
refer document representation bag of words  bow  representation 
      spectral learning setup
following common practice spectral learning text domains  e g   kamvar  klein 
  manning        cai et al          compute similarity two reviews
taking dot product feature vectors  ng et al s        spectral clustering
algorithm  set diagonal entries similarity matrix    addition  set
   words  consider second fifth eigenvectors  assuming
sufficient capturing desired clusterings   
      evaluation metrics
employ two evaluation metrics  first  report results dataset terms
accuracy  percentage documents label assigned system
gold standard label  second  following kamvar et al          evaluate
clusters produced approach gold standard clusters using adjusted
rand index  ari   corrected for chance version rand index 
specifically  given set n data points two clusterings points  u v  
    note setting   somewhat arbitrary choice  number eigenvectors
used active clustering algorithm 

   

fiinducing ideal clustering minimal feedback

u    u    u            um   clusters v    v    v            vn   n clusters  ari
computed follows 
nij
 

p bj
p
  a i
j     
ari u  v       p p b
p
ai p
j

j       
j
      
p

ij

n
 
bj
n
      



formula  nij number common objects ui vj   whereas ai bj
number objects ui vj   respectively  ari ranges      better clusterings
higher ari values 
    baseline systems
subsection  describe baseline results  first two baseline systems
ones described section      last two arguably sophisticated clustering
algorithms employed attempt strengthen baseline results 
      clustering using second eigenvector
first baseline  adopt shi maliks        approach cluster reviews
using second eigenvector  e    described section      results pol
sentiment datasets  expressed terms accuracy ari  shown row   tables  a
 b  respectively  owing randomness choice seeds   means 
experimental results involving   means averaged ten independent runs   
see  baseline achieves accuracy       pol  much lower
accuracies  of        sentiment datasets  performance trend
observed ari  results provide suggestive evidence producing sentimentbased clustering requires different features producing topic based clustering 
many cases  salient features tend topic based  difference
sentiment based clustering topic based clustering illuminated
experiments section     
addition  worth noting baseline achieves much lower accuracies
ari values boo  dvd  ele remaining two sentiment datasets  since
e  captures prominent dimension  results suggest sentiment dimension
prominent dimension three datasets  fact  intuitively
plausible  instance  book domain  positive book reviews typically contain
short description content  reviewer briefly expressing sentiment
somewhere review  similarly electronics domain  electronic product reviews
typically aspect oriented  reviewer talking pros cons
aspect product  e g   battery  durability   since reviews likely contain
positive negative sentiment bearing words  sentiment based clustering unlikely
captured e   
    note clustering one dimensional space  as baseline  yields stable results regardless
choice seeds  results ten runs exhibit nearly zero variance 

   

fidasgupta   ng

system variation
 nd eigenvector
top five eigenvectors
interested reader model
nmf
system

pol
    
    
    
    
    

mov
    
    
    
    
    

accuracy
kit boo
         
         
         
         
         

dvd
    
    
    
    
    

ele
    
    
    
    
    

 ari 
dvd
    
    
    
    
    

ele
    
    
    
    
    

 a 

system variation
 nd eigenvector
top five eigenvectors
interested reader model
nmf
system

pol
    
    
    
    
    

adjusted rand index
mov kit boo
              
    
         
    
         
              
              

 b 

table    results terms  a  accuracy  b  adjusted rand index six datasets
obtained using bag of words document representation  strongest result s 
dataset boldfaced 

      clustering using top five eigenvectors
second baseline  represent data point using top five eigenvectors  i e   e 
e     cluster using   means five dimensional space  described
section      hence  thought ensemble approach  clustering
decision collectively made five eigenvectors   
results shown row   tables  a  b    comparison first baseline 
see improvements accuracy ari pol three sentiment datasets
first baseline performs poorly  i e   boo  dvd  ele   drastic
improvement observed ele  however  performance remaining two sentiment
datasets deteriorates  results attributed fact boo  dvd 
ele  e  capture sentiment dimension  since eigenvector
ensemble does  see improvements  hand  e  already captured
sentiment dimension mov kit  result  employing additional dimensions 
may sentiment related  may introduce noise computation
similarities reviews 
    first eigenvector produce trivial clustering data points reside
cluster  commonly used combination top eigenvectors create low dimensional
space data points clustered  see work ng et al         details 
    clustering five dimensional space  observe results highly sensitive
choice seeds  instance  variances accuracy observed ten runs pol  mov 
kit  boo  dvd  ele                                     respectively 

   

fiinducing ideal clustering minimal feedback

      clustering using interested reader model
third baseline kamvar et al s        unsupervised clustering algorithm  which  according authors  ideally suited text clustering  recently proved
special case ratio cut optimization  kulis  basu  dhillon    mooney         specifically  introduce new laplacian inspired interested reader model 
laplacian computed  s   dmax d  dmax   defined section
     except si j    one js k nearest neighbors j one k
nearest neighbors  dmax maximum rowsum s  identity matrix  since
performance highly sensitive k  tested values                    k report row   tables  a  b best results  somewhat disappointingly  despite
algorithmic sophistication fact reporting best results  baseline
offer consistent improvements previous two  comparison first
baseline  achieves better performance pol worse performance sentiment
datasets  first baseline  results boo  dvd ele particularly poor 
      clustering using non negative matrix factorization
non negative matrix factorization  nmf  recently shown xu  liu  gong
       effective document clustering  re implementing algorithm 
evaluate six datasets    shown row   tables  a  b best results obtained running algorithm five times  comparison first baseline 
nmf achieves better performance ele  comparable performance mov  worse
performance remaining datasets 
    active clustering algorithm
subsection  describe human automatic experiments evaluating active
clustering algorithm 
      human experiments
unlike four baselines  active clustering algorithm requires users specify
four dimensions  defined second fifth eigenvectors  closely
related sentiment inspecting set features derived unambiguous reviews
dimension using mmfr  better understand easy human select
desired dimension given features  performed experiment independently
five humans  all computer science graduate students affiliated
research  computed agreement rate 
specifically  dataset  showed human judge top     features
cluster according mmfr  see tables    subset     features induced
six datasets  lightly shared columns correspond sentiment
dimension selected majority human judges     addition  informed
    matrix factorization use code downloaded http   www csie ntu edu tw cjlin nmf index html 
    human judges reported inspecting top     features sufficient identifying
sentiment dimension  note user clustering algorithm may request inspect many
features wants 

   

fidasgupta   ng

e 
c 
serder
armenian
turkey
armenians
muslims
sdpa
argic
davidian
dbd ura
troops
c 
sternlight

pgp
crypto
algorithm

likely
access
idea
cryptograph

pol
e 
e 
c 
c 
beyer
serbs
arabs
palestinians
andi
muslims
research
wrong
israelis
department
tim
bosnia
uci
live
ab
matter
z virginia
freedom
holocaust
politics
c 
escrow
sternlight
algorithm
access
net
des
privacy
uk
systems
pgp

c 
standard
sternlight
des
escrow
employer
net
york
jake
code
algorithm

e 
c 
escrow
serial
algorithm
chips
ensure
care
strong
police
omissions
excepted
c 
internet
uucp
uk
net
quote
ac
co

ai
mit

table    top ten features induced dimension pol domain  shaded
columns correspond dimensions selected human judges  e           e  top eigenvectors  c  c  clusters 

   

fiinducing ideal clustering minimal feedback

e 
c 
relationship
son
tale
husband
perfect
drama
focus
strong
beautiful
nature
c 
worst
stupid
waste
bunch

video
worse
boring
guess
anyway

mov
e 
e 
c 
c 
production
jokes
earth
kids
sequences
live
aliens
animation
war
disney
crew
animated
alien
laughs
planet
production
horror
voice
evil
hilarious
c 
sex
romantic
school
relationship
friends
jokes
laughs
sexual
cute
mother

c 
thriller
killer
murder
crime
police
car
dead
killed
starts
violence

e 
c 
starts
person
saw
feeling
lives
told
happen

felt
happened
c 
comic
sequences
michael
supporting
career
production
peter
style
latest
entertaining

table    top ten features induced dimension mov domain  shaded
columns correspond dimensions selected human judges  e           e  top eigenvectors  c  c  clusters 

   

fidasgupta   ng

boo
e 
c 
history
must
modern
important
text
reference
excellent
provides
business


e 
c 
series
man
history
character
death

war
seems
political
american

e 
c 
loved
highly
easy
enjoyed
children

although
excellent
understand
three

e 
c 
must
wonderful
old
feel
away
children
year
someone
man
made

c 
plot

thought
boring
got
character


ending
fan

c 
buy
bought
information
easy
money
recipes
pictures
look
waste
copy

c 
money
bad
nothing
waste
buy
anything

already
instead
seems

c 
boring
series
history
pages
information

highly
page
excellent


table    top ten features induced dimension boo domain  shaded
columns correspond dimensions selected human judges  e           e  top eigenvectors  c  c  clusters 

   

fiinducing ideal clustering minimal feedback

ele
e 
c 
mouse
cable
cables
case
red
monster
picture
kit
overall
paid

e 
c 
music
really
ipod

little
headphones
hard
excellent
need
fit

e 
c 
easy
used
card
fine
using
problems
fine
drive
computer
install

e 
c 
amazon
cable
card
recommend
dvd
camera
fast
far
printer
picture

c 
working
never

phone
days
headset
money
months
return
second

c 
worked
problem
never
item
amazon
working
support
months
returned
another

c 
money
worth
amazon

return
years
much
headphones
sony
received

c 
phone

worked
power
battery
unit
set
phones
range
little

table    top ten features induced dimension ele domain  shaded
columns correspond dimensions selected human judges  e           e  top eigenvectors  c  c  clusters 

   

fidasgupta   ng

e 
c 
love
clean
nice
size
set
kitchen
easily
sturdy
recommend
price
c 
months
still
back
never
worked
money

amazon
return
machine

kit
e 
e 
c 
c 
works
really
water
nice
clean
works
work

ice
quality
makes
small
thing
sturdy
need
little
keep
think
best
item
c 
price
item
set
ordered
amazon
gift
got
quality
received
knives

c 

years
love
never
clean
months

pan

pans

e 
c 
pan
oven
cooking
made
pans
better
heat
cook
using
clean
c 
love
coffee

recommend
makes

size
little
maker
cup

table    top ten features induced dimension kit domain  shaded
columns correspond dimensions selected human judges  e           e  top eigenvectors  c  c  clusters 

   

fiinducing ideal clustering minimal feedback

e 
c 
worth
bought
series
money
season
fan
collection
music
tv
thought
c 
young

actors
men
cast
seems
job
beautiful
around
director

dvd
e 
e 
c 
c 
music
video
collection
music
excellent
found
wonderful
feel
must
bought
loved
workout
perfect
daughter
highly
recommend
makes

special
disappointed
c 
worst
money
thought
boring
nothing
minutes
waste
saw
pretty
reviews

c 
series
cast
fan
stars
original
comedy
actors
worth
classic
action

e 
c 
money
quality
video
worth
found
version
picture
waste
special
sound
c 
saw
watched
loved
enjoy
whole
got
family
series
season
liked

table    top ten features induced dimension dvd domain  shaded
columns correspond dimensions selected human judges  e           e  top eigenvectors  c  c  clusters 

   

fidasgupta   ng

judge
 
 
 
 
 
agreement

pol
     
   
 
   
 
   

mov
 
 
   
 
 
    

kit
 
 
 
 
 
   

boo
 
 
 
 
 
    

dvd
 
 
 
 
 
    

ele
 
 
 
   
 
    

table    human agreement rate  shown eigenvectors selected five judges 
intended dimension  example  pol  judge told intended
clustering politics vs  science  also  determined one dimension
relevant intended clustering  instructed rank dimensions
terms relevance  relevant one would appear first list 
dimensions  expressed terms ids eigenvectors  selected
five judges dataset shown table    agreement rate  shown
last row table  computed based highest ranked dimension selected
judge  see  perfect agreement achieved four five sentiment
datasets  remaining two datasets  near perfect agreement achieved 
results  together fact took five six minutes identify relevant
dimension  indicate asking human determine intended dimension based
solely informative features viable task 
      clustering results
next  cluster      documents dataset using dimension selected
majority human judges  clustering results shown row   tables  a
 b  comparison best baseline dataset  see algorithm
performs substantially better boo  dvd ele  almost level mov
kit  slightly worse pol  note improvements observed boo  dvd
ele attributed failure e  capture sentiment dimension  perhaps
importantly  exploiting human feedback  algorithm achieved stable
performance across datasets four baselines   
      identification unambiguous documents
recall features largest mmfr computed unambiguous
documents only  get idea accurate algorithm identifying unambiguous
documents is  show table    accuracy obtained unambiguous documents
dataset clustered using eigenvector selected majority judges 
see  accuracy dataset higher corresponding accuracy
shown row   table  a  fact  accuracy     achieved
    first baseline  since clustering one dimensional space here  results
sensitive choice seeds  yielding zero variance ten independent runs 

   

fiinducing ideal clustering minimal feedback

accuracy

pol
    

mov
    

kit
    

boo
    

dvd
    

ele
    

table     accuracies unambiguous documents 

  labels

pol
   

mov
   

kit
   

boo
   

dvd
   

ele
   

table     transductive svm results 
one dataset  suggests method identifying unambiguous documents
reasonably accurate 
note crucial able achieve high accuracy unambiguous documents  clustering accuracy low  features induced clusters may
accurate representation corresponding dimension  human judge may
difficult time identifying intended dimension  fact  human judges reported difficulty identifying correct dimension ele dataset  attributed
part low accuracy achieved unambiguous documents 
      user feedback versus labeled data
recall four baselines unsupervised  whereas algorithm characterized
semi supervised  relies user feedback select intended dimension  hence 
surprising see average clustering performance algorithm
better baselines 
fairer comparison  conduct another experiment compare
algorithm semi supervised sentiment classification system  uses transductive svm underlying semi supervised learner  specifically  goal
experiment determine many labeled documents needed order transductive learner achieve level performance algorithm  answer
question  first give transductive learner access      documents
dataset unlabeled data  next  randomly sample    unlabeled documents assign
true label  re train classifier compute accuracy     
documents  keep adding labeled data     iteration  reaches
accuracy achieved algorithm  results experiment shown table    
owing randomness involved selection unlabeled documents  results
averaged ten independent runs  see  user feedback equivalent
effort hand annotating     documents per dataset average 
      multiple relevant eigenvectors
seen table    human judges selected one eigenvector
datasets  e g           pol        mov        ele   however  never took
account extra eigenvectors previous experiments  better understand
   

fidasgupta   ng

system

pol
acc ari
         

mov
acc ari
         

ele
acc ari
         

table     results obtained using multiple relevant eigenvectors pol  mov
ele datasets 

accuracy

pol
    

mov
    

kit
    

boo
    

dvd
    

ele
    

table     supervised classification accuracies 
whether extra eigenvectors help improve accuracy ari  conduct another
experiment apply   means cluster documents space defined
selected eigenvectors  table    shows accuracy ari results averaged
ten independent runs  see  results pol considerably better
obtained highest ranked eigenvector used  suggesting
extra eigenvectors contain useful information  however  results mov ele drop
slightly addition extra eigenvectors  indicating extra sentiment
dimensions useful 
      supervised classification results
next  present results supervised classification five sentiment datasets 
one expect largely unsupervised approach offer comparable performance
fully supervised approach  believe fully supervised results enable
reader get sense work stands among existing work identifying
sentiment datasets  specifically  report table    averaged    fold crossvalidation accuracies  svm classifier trained nine folds tested
remaining fold fold experiment  see  results lag behind supervised
results          datasets 
    alternative document representations
experiments  represented document bag words
frequent      words removed  is  course  way represent
document  subsection  examine two alternative document representations
attempt better understand effect document representation classification results 
first document representation  represent document using unigrams
appear remove frequent words document vector 
bag of all words  boaw  representation motivated fact frequencies
function words shown many studies useful features various kinds non topic based classification  e g   finn   kushmerick        stein  argamon 
  frieder        abbasi  chen    salem        koppel  schler    argamon        
   

fiinducing ideal clustering minimal feedback

system variation
 nd eigenvector
top five eigenvectors
interested reader model
nmf
system

pol
    
    
    
    
    

mov
    
    
    
    
    

accuracy
kit boo
         
         
         
         
         

dvd
    
    
    
    
    

ele
    
    
    
    
    

 a 

system variation
 nd eigenvector
top five eigenvectors
interested reader model
nmf
system

pol
    
    
    
    
    

adjusted rand index  ari 
mov kit
boo dvd
    
    
    
    
    
    
    
    
    
    
    
    
                   
    
    
         

ele
    
    
    
    
    

 b 

table     results terms  a  accuracy  b  adjusted rand index six datasets
obtained using bag of all words document representation  strongest result s 
dataset boldfaced 

accuracy ari results obtained re running four baselines well system
using document representation shown tables   a   b  respectively  comparing tables  a   a  see words used features  best
accuracy achieved dataset drops      high frequency words
removed spectral clustering applied  similar trends observed ari
results shown tables  b   b  overall  results substantiate hypothesis
retaining high frequency words document representation adverse effect
performance clustering algorithms 
next  experiment another representation  specifically one document represented using sentiment bearing words contains  understand
motivation behind bag of sentiment words  bosw  representation  recall introduction one way encourage clustering algorithm produce user desired
clustering design feature space contains features
useful producing user desired clustering  since desire sentiment based clustering  design feature space composed solely sentiment bearing words  since
hand crafted subjectivity lexicon  i e   lexicon word manually labeled
prior polarity     english readily available  automatically construct feature
space consists words  positive negative  polarity according
subjectivity lexicon  represent document using resulting feature space 
    prior polarity word polarity computed without regard context word
appears 

   

fidasgupta   ng

system variation
 nd eigenvector
top five eigenvectors
interested reader model
nmf
system

mov
    
    
    
    
    

kit
    
    
    
    
    

accuracy
boo dvd
         
    
    
    
    
         
         

ele
    
    
    
    
    

 a 

system variation
 nd eigenvector
top five eigenvectors
interested reader model
nmf
system

adjusted rand index  ari 
mov kit boo dvd ele
                        
    
                   
    
         
         
    
                   
                        
 b 

table     results terms  a  accuracy  b  adjusted rand index five
sentiment datasets obtained using bag of sentiment words document representation 
strongest result s  dataset boldfaced 

goal  then  determine whether bosw document representation improve
sentiment based clustering results obtained using bow representation 
identify sentiment bearing words experiment  employ subjectivity lexicon introduced work wilson  wiebe  hoffmann           lexicon contains
     words  hand labeled prior polarity positive  negative 
neutral  create new subjectivity lexicon l retain words
wilson et al s lexicon either positive negative polarity  bosw
representation document composed words appear
l document 
accuracy ari results baselines system obtained employing
bosw representation shown tables   a   b  respectively  consider first
second eigenvector baseline  nmf  interested reader model  comparison
corresponding results tables  a  b  bow representation
used  see performance improves boo  dvd  ele datasets
cases  drops mov kit datasets  top five eigenvectors baseline 
performance increases dvd slightly mov  drops remaining datasets 
finally  using bosw representation causes performance system drop
datasets 
overall  results seem suggest whether bosw representation document yields better clustering results bow representation rather dependent
underlying domain clustering algorithm  nevertheless  see best
    see http   www cs pitt edu mpqa  

   

fiinducing ideal clustering minimal feedback

clustering accuracy ari achieved sentiment dataset using bosw representation significantly lower obtained using bow representation  speculate
two reasons poorer results  first  general purpose subjectivity lexicon
cover sentiment bearing words  particular  words sentiment oriented
context particular domain neutral polarity otherwise may omitted bosw document representation  second  non sentiment bearing words
might useful identifying sentiment 
    domain adaptation
mentioned introduction  majority existing approaches sentiment classification supervised  one weakness supervised approaches given
new domain  one needs go expensive process collecting large amount
annotated data order train accurate polarity classifier    one may argue
active clustering algorithm suffers weakness  user needs identify
sentiment dimension domain  one way address weakness domain
adaptation  specifically  investigate whether sentiment dimension manually identified one domain  henceforth source domain  used automatically identify
sentiment dimension new domain  henceforth target domain   hypothesize
domain adaptation feasible  especially two domains sentimentally similar  i e   significant overlap features characterize sentiment
dimensions two domains  
result  propose following method automatically identifying sentiment
dimension target domain  y  using sentiment dimension manually identified
source domain  x  assume sentiment dimension domain x defined
x
x
eigenvector ex   moreover  assume c e c e two vectors top ranked
features  obtained using mmfr  characterize two clusters induced ex  with    
features cluster   now  given target domain y  first compute similarity
ex ys top eigenvectors  ey           ey    similarity two
eigenvectors ex ey defined
x



x



x



x



max  c e   c e      c e   c e     c e   c e      c e   c e   
here  similarity function computes similarity two feature vectors 
experiments  simply set dot product  allows us capture
degree overlap two feature vectors  then  posit eigenvector
 ey            ey    highest overlap one defines sentiment dimension   
determine effectiveness method  compare automatically selected
eigenvector human selected eigenvector domain  results shown
table     row column j indicates sentiment dimension
target domain j successfully identified using sentiment dimension manually
    collecting annotated data trivial dealing review data  necessarily
true kinds data  instance  people express opinions sentiment political blogs
floor debates  associated postings transcripts may explicitly annotated
sentiment labels 
    note two arguments max function correspond two different ways creating
mapping feature vectors two domains 

   

fidasgupta   ng

domain
mov
dvd
boo
ele
kit

mov


n
n
n

dvd



n


boo
n


n
n

ele
n
n
n



kit
n





table     domain adaptation results 

identified source domain i  n indicates failure  instance  know
sentiment dimension dvd domain  through human feedback   domain
adaptation method used correctly identify sentiment domain mov
vice versa  however  domain adaptation using method always successful 
instance  knowing sentiment dimension mov allow us correctly predict
sentiment dimension ele  interestingly  ignore boo kit pair  domain
adaptation exhibits symmetry  symmetry  mean domain x used
identify correct sentiment dimension domain y  domain used
identify correct sentiment dimension domain x  intuitively makes sense 
x successfully used identify sentiment dimension y  likely
two domains share lot sentiment words  consequently  using adapt x
likely successful  boo kit pair represents case domain adaptation
successful one direction  domain adaptation successful boo kit 
similarity sentiment dimensions two domains high  see
discussion next paragraph details   contributes failure adaptation
direction 
mentioned beginning subsection  hypothesize domain adaptation likely successful two domains consideration similar
other  test hypothesis  show table   a similarity manually
identified eigenvector corresponding automatically identified eigenvector
pair domains  three points deserve mention  first  long similarity value
least     domain adaptation successful  also  long similarity value   
domain adaptation unsuccessful  hence  results substantiate hypothesis
domain adaptation likely successful two domains consideration
similar other  would interesting see two thresholds
used predict whether domain adaptation successful given new pair domains 
second  domain adaptation directions likely successful similarity
value sufficiently high  mentioned before  similarity value high 
two domains share many sentiment words common  may turn contribute
successful domain adaptation directions  five domains considering 
long similarity value least     domain adaptation directions
successful  third  worth reiterating even similarity value falls
threshold  imply domain adaptation fail  mentioned before 
sentiment dimension domain  correctly  identified long similarity
   

fiinducing ideal clustering minimal feedback

domain
mov
dvd
boo
ele
kit

mov

  
   
   
   

dvd
  

  
   
  

boo
   
  

   
    

ele
   
    
    

  

kit
   
  
 
  


boo
   
  

   
   

ele
   
   
   

  

kit
   
 
 
  


boo
   
 

   
   

ele
   
   
   

 

kit
   
 
 
 


 a 

domain
mov
dvd
boo
ele
kit

mov

 
   
   
   

dvd
  

  
   
 
 b 

domain
mov
dvd
boo
ele
kit

mov

 
   
   
   

dvd
 

 
   
 
 c 

table     similarity results domain adaptation   a  shows similarity
sentiment eigenvector source domain eigenvector similar target
domain   b  shows similarity sentiment eigenvector source domain
second similar eigenvector target domain   c  shows similarity gap 
difference corresponding entries  a   b  

sentiment dimension domain x highest among four eigenvectors y 
case boo kit domain pair 
far attempted correlate success domain adaptation similarity manually selected eigenvector source domain eigenvector
similar target domain  may worth consider similarity
manually selected eigenvector second similar eigenvector
target domain  gap similarity may give indication success domain adaptation  determine whether better correlation success
domain adaptation similarity gap  compute     similarity
eigenvector manually selected source domain second similar eigenvector
target domain  see table   b  well     similarity gap  see table   c  
simply difference corresponding entries tables   a   b 
see table   c  appears correlation success
domain adaptation gap values  particular  gap value least    domain
   

fidasgupta   ng

adaptation successful  however  gap value    domain adaptation unsuccessful  nevertheless  gap values help predict domain pairs
success domain adaptation cannot predicted using similarity values table   a
 e g   domain pairs low similarity yet domain adaptable   moreover 
fail predict success domain adaptation many domain pairs  specifically
gap value     
    subjectivity lexicon versus human feedback
one might argue access subjectivity lexicon  could use automatically identify right sentiment dimension  thus obviating need human feedback
altogether  subsection  investigate whether indeed feasible use handbuilt general purpose sentiment lexicon identify eigenvector corresponds
sentiment dimension new domain 
experiment  use subjectivity lexicon l described section     
mentioned before  l contains words wilson et al s        subjectivity
lexicon marked prior polarity positive negative  procedure
automatically identifying sentiment dimension using l similar one described
domain adaptation section  second fifth eigenvectors  first
compute similarity eigenvector l  choose eigenvector
highest similarity l  domain adaptation  compute similarity
l eigenvector ex
x

x

x

x

max  c l   c e      c l   c e     c l   c e      c l   c e   
c l c l represent words l labeled positive negative rex
x
spectively  c e c e top ranked features  obtained using mmfr 
characterize two clusters induced ex  with     features cluster   similarity function computes similarity two feature vectors  domain
adaptation  simply set dot product 
results indicate successfully identified right eigenvector using l
five domains  note l general purpose  i e   domain independent 
lexicon containing generic sentiment bearing words  good enough identify
correct sentiment dimension five different domains  worth noting sentiment
dimension mov domain highest similarity l  i e       five
domains  suggesting highest ranked sentiment features mov domain  according mmfr  largely generic  dvd second largest similarity l      
followed boo       kit      ele       comparatively low similarity values
kit ele indicative fact highest ranked sentiment features
largely domain specific 
finally  although subjectivity lexicon obviates need human feedback 
emphasize undermine contribution feedback oriented clustering
technique  following reasons  first  thinking text mining perspective  would
good approach knowledge free possible  employing handcrafted subjectivity lexicon makes system resource dependent  fact  subjectivity
lexicon may readily available vast majority natural languages  second 
   

fiinducing ideal clustering minimal feedback

want method potentially applicable non sentiment domains  e g   spam vs 
spam   faced problem hand built lexicon may
available 
    single data  multiple clusterings
mentioned previously  set documents clustered along different dimensions 
example  movie reviews clustered sentiment  positive vs  negative  genre
 e g   action  romantic documentary   natural question is  produce different
clusterings given set documents  corresponds different dimension 
vast majority existing text clustering algorithms  answer no 
cluster along exactly one dimension  typically prominent dimension 
hand  since algorithm induces important clustering dimensions
dataset  principle used produce  distinct  clustering 
hypothesize generate multiple clusterings given dataset along important
dimensions 
test claim algorithm produce multiple clusterings  evaluate
four datasets possess multiple clustering dimensions  namely mov dvd  boodvd  dvd ele  mov kit    example  boo dvd dataset consists
reviews taken boo dvd domains  hence  augmented dataset
composed      reviews       two contributing domains  
clustered according either topic  e g   book vs  dvd  sentiment    note
four pairs domains used create augmented datasets chosen carefully 
specifically  two augmented datasets  mov dvd boo dvd  created
constituent domains mutually domain adaptable according table    
remaining two  dvd ele mov kit  created constituent domains
domain adaptable  goal see whether active clustering algorithm able
produce topic  sentiment based clusterings datasets different levels
sentimental similarity 
clustering procedure almost identical one described section    essence 
    compute top five eigenvectors laplacian matrix      learn top ranked
features corresponding e  e  according mmfr      ask human judges
identify eigenvectors corresponding topic dimension sentiment
dimension      use   means produce two clusterings reviews  one according
selected topic dimension selected sentiment dimension 
section      conducted human automatic experiments determine viability
algorithm 
    reason employing augmented datasets obviate need
additional human annotations  guarantee least two dimensions along
clusters formed  thus allowing us directly test ability produce multiple clusterings 
possible evaluate algorithms ability generate multiple clusterings using mov
dataset  by clustering along genre sentiment   decided leave future investigation  since
documents mov annotated genre information 
    confused topic sentiment mixture models  mei  ling  wondra  su    zhai        
goal first use topic models mine major aspects product online review
assign ratings extracted aspect  hand  goal design clustering
algorithm capable generating multiple clusterings dataset 

   

fidasgupta   ng

judge
 
 
 
 
 
agreement

mov dvd
 
 
 
 
 
    

boo dvd
 
 
 
 
 
    

dvd ele
 
 
 
 
 
    

mov kit
 
 
 
 
 
    

dvd ele
 
   
   
 
 
   

mov kit
 
   
 
 
 
   

 a 

judge
 
 
 
 
 
agreement

mov dvd
 
   
   
 
 
    

boo dvd
   
   
   
   
   
    
 b 

table     human agreement rate selecting  a  topic dimension  b  sentiment
dimension augmented datasets  shown eigenvectors selected
human judges 

      human experiments
employed five human judges involved human experiments section    
independently determine topic dimension sentiment dimension
four augmented datasets using top features according mmfr  before 
human judge identifies one relevant eigenvector particular dimension 
ask rank eigenvectors according relevance  finally  take topic sentiment
dimension ranked first largest number judges human selected
topic sentiment dimension 
tables   a   b show respectively topic sentiment dimensions  expressed
terms ids eigenvectors  selected five judges augmented
dataset  shown tables human agreement rate  computed
based highest ranked dimension selected judge  several points
human experiments deserve mention 
first  dataset  human judges managed find one eigenvector  out
top five  corresponds topic least one eigenvector corresponds
sentiment  perhaps importantly  human agreement rate least    
achieved four datasets respect selecting eigenvector s  correspond
topic sentiment dimensions  results together provide suggestive evidence
    eigen decomposition procedure active clustering algorithm effective
enough unearth topic sentiment dimensions present
   

fiinducing ideal clustering minimal feedback

dataset      proposal incorporating user feedback via inspecting small
number features viable 
second  topic sentiment prominent dimensions datasets 
fact second eigenvector captures topic dimension four datasets suggests
topic prominent dimension sentiment  fact  human judges
reported topic dimension identified quite easily  achieving perfect agreement
identifying topic dimension  provides empirical evidence speculation
topic typically  though always  prominent dimension sentiment
dimensions exist dataset 
third  reasonably high human agreement rate identifying sentiment dimension achieved  perfect agreement two datasets     agreement rate
remaining two  see table   b details   human judges reported difficult identify sentiment dimension s   especially two datasets composed
sentimentally dissimilar domains 
attempt gain insight judges found difficult identify
sentiment dimension s   show tables      top ranked features induced
dimension using mmfr four augmented datasets  lightly shaded columns
correspond eigenvectors chosen topic dimension darkly shaded columns
correspond eigenvectors chosen sentiment dimension  examining
results  believe points deserve mention 
first  top features generated sentiment eigenvector s  mov dvd
boo dvd  two datasets composed sentimentally similar constituent domains 
clearly sentiment oriented  making relatively easy human judges determine
sentiment eigenvector s   case dvd ele mov kit  two datasets
composed dissimilar domains  top features noisier  i e   many
necessarily sentiment oriented   thus making tougher judges locate
sentiment eigenvector s   fact  one see top features generated
sentiment eigenvector s  tables      mov dvd boo dvd
clearly sentiment oriented dvd ele mov kit 
surprising sentimentally dissimilar constituent domains are  noisier top features generated sentiment eigenvector s  are 
however  constituent domains sentimentally similar  tend many
sentiment bearing words common  implies sentiment bearing words
appear frequently augmented datasets constituent datasets 
hence  combining two domains helps boost influence sentiment bearing
words  increasing chance appearing higher list features ranked
mmfr  reinforcement effect intuitively explains sentiment eigenvector
clearly dominated sentiment words datasets composed sentimentally similar domains  hand  constituent domains sentimentally dissimilar  tend
many sentiment bearing words common  result  influence
sentiment bearing words present one two constituent domains
diluted larger number non sentiment bearing words result combining
two domains  words  features clearly sentiment oriented
one rather domains may longer appear sufficiently high ranked list
features  fact  saw tables        sentiment eigenvector contaminated
   

fidasgupta   ng

e 
c 
roles
drama
murder
meets
crime
supporting
involving
convincing
tale
lead
c 
bought
season
buy
disappointed
fan
amazon
buying
copy
dvds
watched

mov dvd
e 
e 
c 
c 
wonderful recommend
excellent
fan
beautiful
liked
personal
book
collection
read
view
excellent
art
amazing
highly
definitely
fantastic
highly
deal
absolutely
c 
stupid
boring
dull
mean
terrible
save
lame
run
guys
except

c 
buy
house
rent
waste
wait
kill
murder
obvious
season
dvds

e 
c 
kids
children
loved
child
son
daughter
boy
school
wonderful
heart
c 
quality
dark
war
horror
release
fan
earth
production
suspense
sound

table     top ten features induced dimension mov dvd domain 
lightly darkly shaded columns correspond topic sentiment dimensions respectively
selected human judges  e           e  top eigenvectors  c  c  clusters 

   

fiinducing ideal clustering minimal feedback

e 
c 
reader
important
subject
understanding
modern
information
examples
political
business
nature
c 
saw
watched
actors
liked
music
season
humor
comedy
favorite
ending

boo dvd
e 
e 
c 
c 
bought
excellent
disappointed wonderful
easy
highly
information collection
price
music
waste
special
workout
classic
helpful
video
expected
perfect
reviews
amazing

e 
c 
loved
enjoyed
children
year
wonderful
child
fun
son
friends
highly

c 
young
men
cast
role
actors
script
scene
war
performance
action

c 
version
quality
waste
worst
review
original
edition
collection
amazon
format

c 
boring
ending
waste
reviews

novel
maybe
pages
stupid
finish

table     top ten features induced dimension boo dvd domain 
lightly darkly shaded columns correspond topic sentiment dimensions respectively
selected human judges  e           e  top eigenvectors  c  c  clusters 

   

fidasgupta   ng

e 
c 
funny
acting
family
actors
action
plot
enjoy
young
wonderful
comedy
c 
unit
battery
purchased
device
problems
tried
working
plug
charge
computer

dvd ele
e 
e 
c 
c 
easy
fine
small
problems
perfect
worked
excellent
months
highly
easy
nice
working
low
computer
comfortable
day
ipod
card
headphones
drive
c 
amazon
item
review
company
return
took
check
saw
card
worked

c 
amazon
tv
purchase
disappointed
item
purchased
reviews
wanted
received
ipod

e 
c 
video
card
camera
fast
easy
cable
picture
pictures
paper
digital
c 
phone
waste
unit
battery
getting
low
power
hear
worst
batteries

table     top ten features induced dimension dvd ele domain 
lightly darkly shaded columns correspond topic sentiment dimensions respectively
selected human judges  e           e  top eigenvectors  c  c  clusters 

   

fiinducing ideal clustering minimal feedback

e 
c 
james
directed
sex
hour
drama
relationship
death
direction
tv
michael
c 
food
recommend
pot
purchased
mine
kitchen
mixer
handle
size
store

mov kit
e 
e 
c 
c 
pan
coffee
cooking
clean
clean
machine
pans
ice
cook
maker
heat
plastic
oven
cup
heavy
fill
food
months
stick
working
c 
months
purchased
worked
broke
amazon
coffee
replacement
month
tried
service

c 
item
price
sheets
ordered
amazon
received
beautiful
dishes
arrived
sets

e 
c 
price
clean
kitchen
knife
knives
size
sharp
dishwasher
cutting
attractive
c 
pan
toaster
oven
pans
heat
return
bottom
worked
read
toast

table     top ten features induced dimension mov kit domain 
lightly darkly shaded columns correspond topic sentiment dimensions respectively
selected human judges  e           e  top eigenvectors  c  c  clusters 

   

fidasgupta   ng

 nd eigenvector
top five eigenvectors
interested reader model
nmf
system

mov dvd
acc ari
         
         
         
         
         

boo dvd
acc ari
         
         
         
         
         

dvd ele
acc ari
         
         
         
         
         

mov kit
acc ari
         
         
         
         
         

dvd ele
acc ari
         
         
         
         
         

mov kit
acc ari
         
         
         
         
         

 a 

 nd eigenvector
top five eigenvectors
interested reader model
nmf
system

mov dvd
acc ari
         
         
         
         
         

boo dvd
acc ari
         
         
         
         
         
 b 

table     results  a  topic based clustering  b  sentiment based clustering
four augmented datasets  strongest results dataset boldfaced 

number features necessarily sentiment bearing  make difficult
human judges identify sentiment dimension 
another interesting point note datasets  seems
one eigenvector correspond sentiment  instance  boo dvd dataset 
five human judges agreed e  e  correspond sentiment dimension 
closer examination two eigenvectors  shown table     reveals interesting
pattern  e    positive features  in c    came dvd domain negative
features  in c    came boo domain  whereas e    positive features  in
c    came boo domain negative features  in c    came dvd 
words  e  partitions reviews according positive dvd negative
boo  whereas e  reverse  suggests eigen decomposition procedure
smart enough merge positive negative sentiment bearing words
two domains together  perhaps even importantly  e  e 
partitioning reviews along sentiment dimension topic dimension 
      clustering results
rows    tables   a   b show topic  sentiment based clustering results
four baseline text clustering algorithms described section      note
baselines produce one clustering documents per dataset 
hence  baseline  topic based clustering results produced comparing
clustering gold standard topic based clustering  sentiment based
   

fiinducing ideal clustering minimal feedback

clustering results produced comparing clustering gold standard
sentiment based clustering 
see topic based results table   a  baseline cluster
using second eigenvector achieves best average clustering results four
augmented datasets  potentially attributed fact e  corresponds
topic dimension four datasets according human judges  described
human experiments  however  clustering using e  produce best clustering
results four datasets  fact  interested reader model achieves best results
mov dvd  dvd ele  mov kit  nevertheless  results boo dvd
worst among baselines  true top five eigenvectors baseline
nmf  yielded poor results mov dvd  addition  nmfs results
boo dvd mov kit promising either 
far sentiment based baseline clustering results concerned  see rows   
table   b   best average performance achieved nmf  except three cases
 nmf mov dvd mov kit  well top five eigenvectors mov dvd  
baseline results particularly promising  accuracy results low fifties
ari results close zero 
topic  sentiment based clustering results produced algorithm shown
row   tables   a   b  specifically  results obtained grouping
reviews according eigenvectors manually selected topic sentiment dimensions  respectively  hence  unlike baselines  topic based clustering
sentiment based clustering produced algorithm different other 
before  cases human judges selected one eigenvector dimension  use eigenvector ranked first frequently  see 
accuracies topic based clustering reasonably high  ranging             
results suggest possible achieve high performance topic based  or
precisely  domain based  clustering dataset even another prominent clustering dimension  i e   sentiment  present  hand  despite existence eigenvectors
clearly capture sentiment dimension datasets  e g   e  mov dvd
dataset   sentiment based clustering accuracies ari values lower
topic based clustering  potentially attributed reason mentioned
introduction  fact reviews sentimentally ambiguous makes non trivial
classify  comparison four baselines  algorithm achieves best
average performance four datasets comparatively stable performance
across datasets 
worth noting sentiment based clustering results produced algorithm
mov dvd boo dvd higher dvd ele mov kit 
perhaps surprising  discussed before  human judges found difficult
identify sentiment eigenvector dvd ele mov kit mov dvd
boo dvd  owing part fact many top ranked features sentiment
eigenvector dvd ele mov kit sentiment oriented  turn
attributed fact datasets correspond domain pairs
sentimentally dissimilar  mentioned above  two sentimentally dissimilar constituent
domains tend many sentiment bearing words common  consequently 
influence sentiment bearing words present one two constituent
   

fidasgupta   ng

domains diluted larger number non sentiment bearing words result
combining two domains  making difficult produce good sentiment based
clustering  hand  combining two domains helps boost influence
sentiment bearing words  increasing chance appearing higher
list features ranked mmfr producing good sentiment based clustering 
interestingly  algorithm achieves better topic based clustering results two
datasets dvd ele mov kit achieves poorer sentiment based clustering results  fact  topic based clustering accuracies dvd ele mov kit
near perfect              dvd ele mov kit respectively 
means coincidence  constituent domains augmented dataset highly
dissimilar  i e   word usage tends differ considerably other   topic clusters well separated hence high topic based clustering results
achieved  similar line reasoning explain algorithm finds comparatively
difficult produce good topic based clustering mov dvd boo dvd 
constituent domains similar 
results seem suggest higher topic based accuracy ari implies lower
sentiment based accuracy ari vice versa  speculate constituent
domains similar  sentiment bearing features tend similar result 
sentiment based results tend good topic based results tend poor  additional
experiments needed determine reason 
overall  results provide supporting evidence feedback oriented algorithm
produce multiple clusterings dataset  particular  even though sentimentbased clustering accuracies high topic based clustering accuracies
augmented datasets  current level performance algorithm arguably reasonable  especially considering fact sentiment based clustering challenging task
traditional clustering algorithms fail even produce one clustering 
      multiple relevant eigenvectors
recall table   b four augmented datasets  least one
judge indicated one eigenvector relevant sentiment dimension 
however  producing sentiment based clustering results using system table   b  used eigenvector ranked frequently human judges 
better understand whether using relevant eigenvectors help improve results sentiment based clustering  repeat experiment apply   means
cluster documents space defined eigenvectors determined
relevant least one judge  specifically  cluster following set
eigenvectors        mov dvd        boo dvd        dvd ele       
mov kit 
accuracy ari results experiment shown table     comparison
results last row table   b  see using additional relevant eigenvectors
yields better results boo dvd dataset  may easy
determine reason  believe poorer results observed boo dvd
attributed impurity e    captures sentiment topic 
discussed before  hand  additional sentiment eigenvectors chosen
   

fiinducing ideal clustering minimal feedback

system

mov dvd
acc ari
         

boo dvd
acc ari
         

dvd ele
acc ari
         

mov kit
acc ari
         

table     results sentiment based clustering obtained using multiple relevant eigenvectors four augmented datasets 
three augmented datasets seem impurity problem 
capture sentiment dimension one constituent domains 

   significance work
believe approach significant following aspects 
   producing clustering according user interest  proposed novel framework enabled spectral clustering algorithm take account human
feedback produce clustering along dimension interest user 
particularly appealing aspect approach concerned relatively minimal human feedback demands  user needs take cursory look
small number features representative induced dimension 
worth noting human inspect select automatically induced
clustering dimension new form interaction human clustering
algorithm  enables human easily engage various clustering tasks help improve performance easy  low effort manner  believe approach 
belongs emerging family interactive algorithms allows user
make small  guiding tweaks thereby get results much better would otherwise
possible  future information retrieval 
   inducing human interpretable clustering dimensions  dimensions produced spectral clustering dimensionality reduction algorithms  e g   latent
semantic indexing  lsi   deerwester  dumais  furnas  landauer    harshman       
generally considered non interpretable  sebastiani         unlike dimension
original feature space  typically corresponds word type therefore interpreted human easily  results preliminary study challenge
common wisdom  show context text clustering dimension
low dimensional space induced spectral clustering interpreted
human  believe ability produce human interpretable dimensions enables us
employ spectral clustering  and perhaps dimensionality reduction based clustering algorithms  text processing intelligent manner  especially
case respect selecting dimensions pertinent task
hand  example  existing applications spectral clustering topic based
clustering task  e g   xu et al         he  cai  liu    ma        hu  deng  guo    xu 
       dimensions low dimensional space typically used  since
showed dimensions produced spectral clustering dataset
necessarily topic related  potentially improve topic based clustering results
   

fidasgupta   ng

employing non topic related dimensions clustering process  addition 
since induced dimensions correspond non topic dimensions 
use produce non topic based clusterings  particular  given recent surge
interest nlp community text classification along non topic dimensions
sentiment gender  e g   garera   yarowsky        jurafsky  ranganath   
mcfarland         approach offers solution tasks rely
labeled data  unlike majority existing approaches non topic based text classification  supervised nature  overall  believe nlp researchers
fully exploited power spectral clustering  hence rewards
understanding spectral clustering light results may significant 
   producing multiple clusterings  majority existing text clustering
algorithms produce single clustering dataset  approach potentially
used produce multiple clusterings  one along important clustering
dimensions induced via novel application spectral clustering 
finally  worth mentioning task inducing clustering dimensions reminiscent influential topic modeling task  blei  ng    jordon         whose goal
discover major topics set documents unsupervised manner  note
two tasks fundamentally different  topic model attempts discover major
topics set documents  dimension model aims discover major clustering
dimensions  nevertheless  two models bear resemblance many ways 
first  employ clustering discover information text collection unsupervised manner  second  display learned information human using
representative words  topic model represents induced topic using words
representative topic  dimension model represents induced clustering
dimension using words representative two document clusters involved dimension  finally  induced topics clustering dimensions human recognizable 
are  human needed assign labels them  believe induction
clustering dimensions potential substantially enhance capability existing
text analysis algorithms discover knowledge text collection unsupervised
manner complementing information induced topic model 

   related work
introduction  discussed related work producing user desired clustering 
section  focus discussing related work topic based clustering classification  sentiment classification  active learning  producing multiple clusterings computational
stylistics 
topic based text clustering  traditional research text clustering focused primarily topic based clustering  owing large part darpas topic detection
tracking initiative     s  many different clustering algorithms used  including non hierarhical algorithms k means expectation maximization  em 
hierarchical algorithms single link  complete link  group average  singlepass  hatzivassiloglou  gravano    maganti         algorithms cluster given set
   

fiinducing ideal clustering minimal feedback

documents feature space typically spanned unigrams  however 
clustering high dimensional space allow distance two documents reliably computed due curse dimensionality  consequently 
recent work focused development algorithms cluster documents lowdimensional space constructed via dimensionality reduction  representative members
family dimensionality reduction based clustering algorithms include traditional algorithms based lsi  deerwester et al          well recently proposed
 and arguably better performing  algorithms spectral clustering  shi   malik       
ng et al          non negative matrix factorization  xu et al          locality preserving indexing  he et al          locality discriminating indexing  hu et al          despite
development new clustering algorithms  primarily evaluated
respect ability produce topic based clusterings 
topic based text classification  yang liu        put it  text classification
inherently supervised learning task  fact  arguably one popular
tasks supervised learning techniques applied information retrieval
community     s  see sebastiani        comprehensive overview related work
machine learning text classification   nevertheless  annotated documents
needed training high performance supervised text classifier expensive
obtain  result  researchers investigated possibility performing text
classification little even labeled data  attempts led development
general purpose semi supervised text classification algorithms combine labeled
unlabeled data using transduction  joachims      b  em  nigam  mccallum  thrun   
mitchell         latter used combination active learning  mccallum   nigam         recently  sandler        proposed unsupervised text
classification algorithm based mixture modeling lsi based dimensionality
reduction 
sentiment classification  mentioned introduction  despite large amount
recent work sentiment analysis opinion mining  much focused supervised
methods  see pang   lee        comprehensive survey field   one weakness existing supervised polarity classification systems typically
domain  language specific  hence  given new domain language  one needs
go expensive process collecting large amount annotated data order
train high performance polarity classifier  recent attempts made
leverage existing sentiment corpora lexicons automatically create annotated resources
new domains languages  however  methods require existence either
parallel corpus machine translation engine projecting translating annotations lexicons
resource rich language target language  banea  mihalcea  wiebe    hassan 
      wan         domain similar enough target domain  blitzer et al  
       target domain language fails meet requirement  sentiment based
clustering unsupervised polarity classification become appealing alternatives  unfortunately  exceptions  e g   semi supervised sentiment analysis  riloff   wiebe 
      sindhwani   melville        dasgupta   ng      a  li  zhang    sindhwani        
tasks largely under investigated nlp community  turneys        work
perhaps one notable examples unsupervised polarity classification  however 
   

fidasgupta   ng

system learns semantic orientation phrases review unsupervised manner  information used predict polarity review heuristically 
domain adaptation  domain adaptation  known transfer learning  one
focal research areas machine learning nlp recent years  goal
leverage labeled data available one domain  the source domain  build
classifier another domain  the target domain   techniques domain adaptation
applied various nlp tasks  including part of speech tagging  noun phrase chunking 
syntactic parsing  named entity recognition  word sense disambiguation  e g   daume iii
  marcu        chan   ng        duame iii        jiang   zhai      a      b  
particular relevance work domain adaptation techniques specifically developed
text sentiment classification  e g   blitzer  mcdonald    pereira        finn  
kushmerick        blitzer et al         gao  fan  jiang    han        ling  dai  xue  yang 
  yu        tan  cheng  wang    xu         worth noting domain adaptation
setting different traditional setting  traditionally  sophisticated classifiers and or
automatically constructed mapping features two domains used
adaptation process  setting  however  simply utilize sentiment dimension
manually selected source domain automatically identify sentiment
dimension target domain 
active clustering  active learning heavily investigated machine learning paradigm
aims achieve better generalization bounds lower annotation costs  cohn  atlas 
  ladner         traditional active learning setting  human requested
annotate data points classifier uncertain  e g   cohn et al         
recent research active learning involved asking human identify label
features useful classification task hand  e g   bekkerman et al        
raghavan   allan        druck  settles    mccallum        roth   small        
mentioned introduction  active learning applied clustering setting 
goal encouraging algorithm produce user intended clustering
data clustered along multiple dimensions  different variants active clustering
proposed  request human label pair data points must link
cannot link indicate whether two points must must reside cluster
 e g   wagstaff et al         bilenko  basu    mooney         others human
determine whether two clusters merged split hierarchical clustering
process  e g   balcan   blum         active clustering algorithm yet another variant 
ask human select clustering desires set automatically produced
clusterings 
generation multiple clusterings  notion text collections may clustered
multiple independent ways discussed literature computational stylistics
 see lim  lee    kim        biber   kurjian        grieve smith        tambouratzis  
vassiliou        gries  wulff    davies        example   machine learning 
attempts design algorithms producing multiple clusterings dataset 
operate semi supervised setting  e g   gondek   hofmann       
davidson   qi         totally unsupervised  e g   caruana  elhawary  nguyen 
  smith        jain  meka    dhillon         instance  caruana et al s        meta
clustering algorithm produces different clusterings dataset running k means
   

fiinducing ideal clustering minimal feedback

times  time random selection seeds random weighting features 
goal present local minimum found k means possible clustering  however 
propose mechanism determining clusterings one
user desires  approach  relies spectral clustering rather k means
producing multiple clusterings  fills gap soliciting user feedback determine
user desired clustering 

   conclusions future work
unsupervised clustering algorithms typically group objects along prominent dimension  part owing objective simultaneously maximizing inter cluster similarity intra cluster dissimilarity  hence  users intended clustering dimension
prominent dimension  unsupervised clustering algorithms fail
miserably  address problem  proposed active clustering algorithm 
allows us mine user intended  possibly hidden  dimension data produce
desired clustering  mechanism differs competing methods requires
limited feedback  select intended dimension  user needs inspect
small number features  demonstrated viability via set human automatic
experiments challenging  yet under investigated task sentiment based clustering  obtaining promising results  additional experiments provided suggestive evidence
    domain adaptation successfully applied identify sentiment dimension
new domain domains consideration sentimentally similar      hand crafted
subjectivity lexicon  available  used replace user feedback needed select
sentiment eigenvector domain      algorithm potentially used
produce multiple clusterings datasets possess multiple clustering dimensions 
equally importantly  empirically demonstrated possible human
interpret dimension produced spectral clustering algorithm  contrary common
wisdom dimensions automatically constructed rank reduced space noninterpretable  believe nlp researchers fully exploited power spectral
clustering  hence rewards understanding spectral clustering light results
may significant  finally  proposal represent induced clustering dimension
sets informative features facilitates exploratory text analysis  potentially enhancing
capability existing text analysis algorithms complementing information provided
unsupervised models  e g   topic model  
future work  plan explore several extensions active clustering algorithm 
first  active clustering algorithm potentially used produce multiple clusterings dataset  one interesting future direction would examine theoretical
guarantees  determining whether able produce distinct clusterings qualitatively strong  see dasgupta   ng      a      b  example   second  plan use
algorithm combination existing feedback oriented methods  e g   bekkerman et al  
      roth   small        improving performance  instance  instead
user construct relevant feature space scratch  simply extend set
informative features identified user selected dimension  third  since none
steps algorithm specifically designed sentiment classification  plan apply
non topic based text classification tasks recently received lot in   

fidasgupta   ng

terest nlp community  gender classification  i e   task determining
gender author document   finally  plan adopt richer representation
document exploits features polarity oriented words obtained hand built
machine learned sentiment lexicons  e g   hu   liu        wiebe  wilson  bruce  bell 
  martin        andreevskaia   bergler        mohammad  dunne    dorr        rao
  ravichandran         derived finer grained  i e   sentential  sub sentential 
phrase based  sentiment analysis methods  e g   wilson et al         kennedy   inkpen 
      polanyi   zaenen        mcdonald  hannan  neylon  wells    reynar        choi
  cardie         richer features may make easier user identify
desired dimension using method 

bibliographic note
portions work previously presented conference publication  dasgupta  
ng      b   current article extends work several ways  notably     
detailed introduction spectral clustering  section           inclusion two
baseline systems  section           investigation effect document representation
clustering performance  section           addition three new sections focusing
issues domain adaptation  section       employing manually constructed subjectivity
lexicon  section       producing multiple clusterings dataset  section       well
    description significance work  section    

acknowledgments
authors acknowledge support national science foundation  nsf  grant iis         thank four anonymous reviewers helpful comments
unanimously recommending article publication jair  opinions  findings 
conclusions recommendations expressed article authors
necessarily reflect views official policies  either expressed implied  nsf 

references
abbasi  a   chen  h     salem  a          sentiment analysis multiple languages  feature
selection opinion classification web forums  acm transactions information
systems         
andreevskaia  a     bergler  s          mining wordnet fuzzy sentiment  sentiment
tag extraction wordnet glosses  proceedings   th conference
european chapter association computational linguistics  eacl   pp     
    
balcan  m  f     blum  a          clustering interactive feedback  proceedings
  th international conference algorithmic learning theory  alt   pp     
    
banea  c   mihalcea  r   wiebe  j     hassan  s          multilingual subjectivity analysis using machine translation  proceedings      conference empirical
methods natural language processing  emnlp   pp         
   

fiinducing ideal clustering minimal feedback

bekkerman  r   raghavan  h   allan  j     eguchi  k          interactive clustering
text collections according user specified criterion  proceedings   th
international joint conference artificial intelligence  ijcai   pp         
biber  d     kurjian  j          towards taxonomy web registers text types 
multidimensional analysis  language computers                 
bilenko  m   basu  s     mooney  r  j          integrating constraints machine learning
semi supervised clustering  proceedings   st international conference
machine learning  icml   pp       
blei  d  m   ng  a  y     jordon  m  i          latent dirichlet allocation  journal
machine learning research             
blitzer  j   dredze  m     pereira  f          biographies  bollywood  boom boxes
blenders  domain adaptation sentiment classification  proceedings   th
annual meeting association computational linguistics  acl   pp         
blitzer  j   mcdonald  r     pereira  f          domain adaptation structural correspondence learning  proceedings      conference empirical methods
natural language processing  emnlp   pp         
cai  d   he  x     han  j          document clustering using locality preserving indexing 
ieee transactions knowledge data engineering                    
caruana  r   elhawary  m  f   nguyen  n     smith  c          meta clustering 
proceedings  th ieee international conference data mining  icdm   pp     
    
chan  p  k   schlag  d  f     zien  j  y          spectral k way ratio cut partitioning
clustering  ieee transactions computer aided design               
chan  y  s     ng  h  t          domain adaptation active learning word sense
disambiguation  proceedings   th annual meeting association
computational linguistics  acl   pp       
choi  y     cardie  c          learning compositional semantics structural inference subsentential sentiment analysis  proceedings      conference
empirical methods natural language processing  emnlp   pp         
cohn  d   atlas  l     ladner  r          improving generalization active learning 
machine learning                 
dasgupta  s     ng  v       a   mine easy  classify hard  semi supervised approach automatic sentiment classification  proceedings joint conference
  th annual meeting acl  th international joint conference
natural language processing afnlp  acl ijcnlp   pp         
dasgupta  s     ng  v       b   topic wise  sentiment wise  otherwise  identifying
hidden dimension unsupervised text classification  proceedings     
conference empirical methods natural language processing  emnlp   pp 
       
dasgupta  s     ng  v       a   mining clustering dimensions  proceedings   th
international conference machine learning  icml   pp         
   

fidasgupta   ng

dasgupta  s     ng  v       b   towards subjectifying text clustering  proceedings
  rd annual international acm sigir conference research development
information retrieval  sigir   pp         
daume iii  h     marcu  d          domain adaptation statistical classifiers  journal
artificial intelligence research             
davidson  i     qi  z          finding alternative clusterings using constraints  proceedings  th ieee international conference data mining  icdm   pp         
deerwester  s   dumais  s  t   furnas  g  w   landauer  t  k     harshman  r         
indexing latent semantic analysis  journal american society information
science                 
dhillon  i   guan  y     kulis  b          kernel k means  spectral clustering normalized cuts  proceedings   th acm sigkdd international conference
knowledge discovery data mining  kdd   pp         
ding  c   he  x   zha  h   gu  m     simon  h  d          min max cut algorithm
graph partitioning data clustering  proceedings      international
conference data mining  icdm   pp         
druck  g   settles  b     mccallum  a          active learning labeling features  proceedings      conference empirical methods natural language processing
 emnlp   pp       
duame iii  h          frustratingly easy domain adaptation  proceedings   th
annual meeting association computational linguistics  acl   pp         
finn  a     kushmerick  n          learning classify documents according genre 
journal american society information science technology          
         
fung  g          disputed federalist papers  svm feature selection via concave minimization  proceedings      conference diversity computing  pp 
     
gao  j   fan  w   jiang  j     han  j          knowledge transfer via multiple model local
structure mapping  proceeding   th acm sigkdd international conference
knowledge discovery data mining  kdd   pp         
garera  n     yarowsky  d          modeling latent biographic attributes conversational
genres  proceedings joint conference   th annual meeting
acl  th international joint conference natural language processing
afnlp  acl ijcnlp   pp         
gilad bachrach  r   navot  a     tishby  n          margin based feature selection theory
algorithms  proceedings   st international conference machine
learning  icml   pp       
gondek  d     hofmann  t          non redundant data clustering  proceedings
 th ieee international conference data mining  icdm   pp       
gries  s   wulff  s     davies  m          corpus linguistic applications  current studies 
new directions  rodopi 
   

fiinducing ideal clustering minimal feedback

grieve smith  a          envelope variation multidimensional register genre
analyses  language computers               
hatzivassiloglou  v   gravano  l     maganti  a          investigation linguistic
features clustering algorithms topical document clustering  proceedings
  rd annual international acm sigir conference research development
information retrieval  sigir   pp         
he  x   cai  d   liu  h     ma  w  y          locality preserving indexing document
representation  proceedings   th annual international acm sigir conference research development information retrieval  sigir   pp        
hu  j   deng  w   guo  j     xu  w          locality discriminating indexing document
classification  proceedings   th annual international acm sigir conference
research development information retrieval  sigir   poster   pp     
    
hu  m     liu  b          mining opinion features customer reviews  proceedings
  th national conference artificial intelligence  aaai   pp         
jain  p   meka  r     dhillon  i  s          simultaneous unsupervised learning disparate
clusterings  proceedings siam international conference data mining  sdm  
pp         
jiang  j     zhai  c       a   instance weighting domain adaptation nlp  proceedings   th annual meeting association computational linguistics
 acl   pp         
jiang  j     zhai  c       b   two stage approach domain adaptation statistical
classifiers  proceedings   th conference information knowledge
management  cikm   pp         
joachims  t       a   making large scale svm learning practical  scholkopf  b    
smola  a   eds    advances kernel methods   support vector learning  pp       
mit press 
joachims  t       b   transductive inference text classification using support vector
machines  proceedings   th international conference machine learning
 icml   pp         
jurafsky  d   ranganath  r     mcfarland  d          extracting social meaning  identifying interactional style spoken conversation  proceedings human language
technologies       annual conference north american chapter
association computational linguistics  naacl hlt   pp         
kamvar  s   klein  d     manning  c          spectral learning  proceedings   th
international joint conference artificial intelligence  ijcai   pp         
kannan  r   vempala  s     vetta  a          clusterings  good  bad spectral 
journal acm                 
kennedy  a     inkpen  d          sentiment classifiation movie reviews using contextual
valence shifters  computational intelligence                 
   

fidasgupta   ng

koppel  m   schler  j     argamon  s          computational methods authorship attribution  journal american society information science technology 
            
kugler  m   aoki  k   kuroyanagi  s   iwata  a     nugroho  a          feature subset
selection support vector machines using confident margin  proceedings
     ieee international joint conference neural networks  ijcnn   pp         
kulis  b   basu  s   dhillon  i     mooney  r          semi supervised graph based clustering  kernel approach  machine learning              
li  t   zhang  y     sindhwani  v          non negative matrix tri factorization approach
sentiment classification lexical prior knowledge  proceedings joint
conference   th annual meeting acl  th international joint
conference natural language processing afnlp  acl ijcnlp   pp     
    
lim  c   lee  k     kim  g          multiple sets features automatic genre classification web documents  information processing management                   
ling  x   dai  w   xue  g   yang  q     yu  y          spectral domain transfer learning 
proceeding   th acm sigkdd international conference knowledge
discovery data mining  kdd   pp         
liu  b   li  x   lee  w  s     yu  p  s          text classification labeling words 
proceedings   th national conference artificial intelligence  aaai   pp 
       
mccallum  a  k     nigam  k          employing em pool based active learning
text classification  proceedings   th international conference machine
learning  icml   pp          madison  wi  morgan kaufmann 
mcdonald  r   hannan  k   neylon  t   wells  m     reynar  j          structured models
fine to coarse sentiment analysis  proceedings   th annual meeting
association computational linguistics  acl   pp         
mei  q   ling  x   wondra  m   su  h     zhai  c          sentiment mixture  modeling
facets opinions weblogs  proceedings   th world wide web conference
 www   pp         
mohammad  s   dunne  c     dorr  b          generating high coverage semantic orientation lexicons overtly marked words thesaurus  proceedings
     conference empirical methods natural language processing  emnlp  
pp         
ng  a   jordan  m     weiss  y          spectral clustering  analysis algorithm 
advances neural information processing systems     nips  
nigam  k   mccallum  a   thrun  s     mitchell  t          text classification labeled
unlabeled documents using em  machine learning                   
pang  b     lee  l          opinion mining sentiment analysis  foundations trends
information retrieval               
   

fiinducing ideal clustering minimal feedback

pang  b   lee  l     vaithyanathan  s          thumbs up  sentiment classification using
machine learning techniques  proceedings      conference empirical
methods natural language processing  emnlp   pp        association computational linguistics 
polanyi  l     zaenen  a          contextual valence shifters  computing attitude
affect text  theory applications  springer verlag 
raghavan  h     allan  j          interactive algorithm asking incorporating
feature feedback support vector machines  proceedings   th annual
international acm sigir conference research development information
retrieval  sigir   pp       
rao  d     ravichandran  d          semi supervised polarity lexicon induction  proceedings   th conference european chapter association computational linguistics  eacl   pp         
riloff  e     wiebe  j          learning extraction patterns subjective expressions 
proceedings      conference empirical methods natural language
processing  emnlp   pp         
roth  d     small  k          interactive feature space construction using semantic information  proceedings   th conference computational natural language
learning  conll   pp       
sandler  m          use linear programming unsupervised text classification 
proceedings   th acm sigkdd international conference knowledge
discovery data mining  kdd   pp         
sebastiani  f          machine learning automated text categorization  acm computing
surveys              
shi  j     malik  j          normalized cuts image segmentation  ieee transactions
pattern analysis machine intelligence                 
sindhwani  v     melville  p          document word co regularization semi supervised
sentiment analysis  proceedings  th ieee international conference data
mining  icdm   pp           
stein  s   argamon  s     frieder  o          effect ocr errors stylistic text
classification  proceedings   th annual international acm sigir conference
research development information retrieval  sigir   poster   pp     
    
tambouratzis  g     vassiliou  m          employing thematic variables enhancing classification accuracy within author discrimination experiments  literary linguistic
computing                 
tan  s   cheng  x   wang  y     xu  h          adapting naive bayes domain adaptation
sentiment analysis  proceedings   st european conference information
retrieval  ecir   pp         
turney  p          thumbs thumbs down  semantic orientation applied unsupervised classification reviews  proceedings   th annual meeting
association computational linguistics  acl   pp         
   

fidasgupta   ng

wagstaff  k   cardie  c   rogers  s     schrodl  s          constrained k means clustering
background knowledge  proceedings   th international conference
machine learning  icml   pp         
wan  x          using bilingual knowledge ensemble techniques unsupervised chinese sentiment analysis  proceedings      conference empirical methods
natural language processing  emnlp   pp         
weiss  y          segmentation using eigenvectors  unifying view  proceedings
international conference computer vision  iccv   pp         
wiebe  j  m   wilson  t   bruce  r   bell  m     martin  m          learning subjective
language  computational linguistics                 
wilson  t   wiebe  j  m     hoffmann  p          recognizing contextual polarity
phrase level sentiment analysis  proceedings joint human language technology conference      conference empirical methods natural language
processing  hlt emnlp   pp         
wu  z     leahy  r  m          optimal graph theoretic appproach data clustering
application image segmentation  ieee transactions pattern analysis
machine intelligence                    
xing  e  p   ng  a  y   jordan  m  i     russell  s  j          distance metric learning
application clustering side information  advances neural information
processing systems     nips   pp         
xu  w   liu  x     gong  y          document clustering based non negative matrix
factorization  proceedings   th annual international acm sigir conference
research development information retrieval  sigir   pp         
yang  y     liu  x          re examination text categorization methods  proceedings   nd annual international acm sigir conference research
development information retrieval  sigir   pp       
yang  y     pedersen  j  o          comparative study feature selection text categorization  proceedings   th international conference machine learning
 icml   pp         

   


