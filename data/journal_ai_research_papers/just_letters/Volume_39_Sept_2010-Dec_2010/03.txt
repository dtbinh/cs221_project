journal of artificial intelligence research                 

submitted        published      

which clustering do you want 
inducing your ideal clustering with minimal feedback
sajib dasgupta
vincent ng

sajib hlt utdallas edu
vince hlt utdallas edu

human language technology research institute
university of texas at dallas
    west campbell road  mail station ec  
richardson  tx            u s a 

abstract
while traditional research on text clustering has largely focused on grouping documents
by topic  it is conceivable that a user may want to cluster documents along other dimensions 
such as the authors mood  gender  age  or sentiment  without knowing the users intention 
a clustering algorithm will only group documents along the most prominent dimension 
which may not be the one the user desires  to address the problem of clustering documents
along the user desired dimension  previous work has focused on learning a similarity metric
from data manually annotated with the users intention or having a human construct a
feature space in an interactive manner during the clustering process  with the goal of
reducing reliance on human knowledge for fine tuning the similarity function or selecting
the relevant features required by these approaches  we propose a novel active clustering
algorithm  which allows a user to easily select the dimension along which she wants to
cluster the documents by inspecting only a small number of words  we demonstrate the
viability of our algorithm on a variety of commonly used sentiment datasets 

   introduction
text clustering is one of the major application domains for demonstrating the viability of
a clustering algorithm  while traditional research on text clustering has largely focused
on grouping documents by topic  it is conceivable that a user may want to cluster documents along other dimensions  such as the authors mood  gender  age  or sentiment  since
virtually all existing text clustering algorithms can produce just one clustering of a given
set of documents  a natural question is  is this clustering necessarily the one the user desires  in other words  can a text clustering algorithm always produce a clustering along the
user desired dimension 
the answer to this question depends to a large extent on whether the user can successfully communicate her intention to the clustering algorithm  traditionally  this can be
achieved by designing a good similarity function that can capture the similarity between
a pair of documents  so that her ideal clustering can be produced  this typically involves
having her identify a set of features that is useful for inducing the desired clusters  liu  li 
lee    yu         however  manually identifying the right set of features is both timeconsuming and knowledge intensive  and may even require a lot of domain expertise  the
fact that the resulting similarity function is typically not easily portable to other domains is
particularly unappealing from a machine learning perspective  to overcome this weakness 
c
    
ai access foundation  all rights reserved 

fidasgupta   ng

researchers have attempted to learn a similarity metric from side information  xing  ng 
jordan    russell         such as constraints on which pairs of documents must or must
not appear in the same cluster  wagstaff  cardie  rogers    schrodl        
by contrast  recent work has focused on active clustering  where a clustering algorithm
can incorporate user feedback during the clustering process to help ensure that the documents are grouped according to the user desired dimension  one way to do this is to have
the user incrementally construct a set of relevant features in an interactive fashion  bekkerman  raghavan  allan    eguchi        raghavan   allan        roth   small        
another way is to have the user correct the mistakes made by the clustering algorithm in
each clustering iteration by specifying whether two existing clusters should be merged or
split  balcan   blum         a major drawback associated with these active clustering
algorithms is that they involve a considerable amount of human feedback  which needs to
be provided in each iteration of the clustering process  furthermore  identifying clusters
for merging or splitting in balcan and blums algorithm may not be as easy as it appears 
for each merge or split decision the user makes  she has to sample a large number of
documents from the cluster s   read through the documents  and base her decision on the
extent to which the documents are  dis similar to each other 
in this article  we attack the problem of clustering documents according to user interest
from a different angle  we aim to have a knowledge lean approach to this problem  an
approach that can produce a clustering of the documents along the user desired dimension
without relying on human knowledge for fine tuning the similarity function or selecting
the relevant features  unlike existing approaches  to this end  we propose a novel active
clustering algorithm  which assumes as input a simple feature representation  composed of
unigrams only  and a simple similarity function  i e   the dot product   and operates by
    inducing the important clustering dimensions  of a given set of documents  where each
clustering dimension is represented by a  small  number of automatically selected words
that are representative of the dimension  and     have the user choose the dimension along
which she wants to cluster the documents by examining these automatically selected words 
in comparison to the aforementioned feedback mechanisms  ours is arguably much simpler 
we only require that the user have a cursory look at a small number of features for each
dimension once and for all  as opposed to having the user generate the feature space in an
interactive manner or identify clusters that need to be merged or split in each clustering
iteration 
we evaluate our active clustering algorithm on the task of sentiment based clustering 
where the goal is to cluster a set of documents  e g   reviews  according to the polarity
 e g   thumbs up or thumbs down  expressed by the author without using any labeled
data  our decision to focus on sentiment based clustering is motivated by several reasons 
one reason is that there has been relatively little work on sentiment based clustering  as
mentioned before  existing work on text clustering has focused on topic based clustering 
where high accuracies can be achieved even for datasets with a large number of classes
 e g      newsgroups   and despite the large amount of recent work on sentiment analysis
   we use the term clustering dimension to refer to a dimension along which a set of documents can be
clustered  for example  a set of movie reviews can be clustered according to genre  e g   action  romantic 
or documentary  or sentiment  e g   positive  negative  neutral  

   

fiinducing your ideal clustering with minimal feedback

review  
the sound from my system did seem to be a little better
 the cds were not skipping as much   but the bottom line is it
didnt fix the problem as the cds are still skipping noticeably 
although not as bad as before     
review  
john lynch wrote a classic in spanish american revolutions           
he describes all the events that led to the independence of latin america from spain 
the book starts in rio de la plata and ends in mexico and central america 
curiously one can note a common pattern of highly stratified societies lead by spanish    
the reluctance of spanish monarchy  and later even of liberals  led to independence    
for all of those who are interested in a better understanding of latin   this great book is a must 
lynch cleverly combines historical and economic facts about the hispanic american societies    

table    snippets of two reviews that illustrate the two challenges of polarity classification 
one is that reviews are sentimentally ambiguous  review     and the other is that the
objective materials in a review can significantly outnumber their subjective counterparts
 review    

and opinion mining  much of it has focused on supervised methods  see pang   lee       
for a comprehensive survey of the field  
another equally important reason for our focus on sentiment based clustering is concerned with the challenges that this task presents to natural language processing  nlp 
researchers  broadly speaking  the complexity of sentiment based clustering arises from
two sources  first  reviews are sentimentally ambiguous  containing both positive and negative sentiment bearing words and phrases  review   of table   shows a snippet of a
review from the dvd domain that illustrates the sentimental ambiguity problem  while
the phrases a little better  not skipping  and not as bad convey a positive sentiment 
the phrases didnt fix and skipping noticeably are negative sentiment bearing  hence 
unless a sentiment analyzer performs deeper linguistic analysis  it will be difficult for the
analyzer to determine the polarity of the review  second  the objective materials in a review tend to significantly outnumber their subjective counterparts  as a reviewer typically
devotes a large portion of the review to describing the features of a product before assigning a rating to it  consequently  any sentiment analyzer that uses a word  or phrase based
feature representation will be composed of mostly features that are irrelevant with respect
to polarity determination  shown in review   of table   is a snippet of a book review
that illustrates this problem  as we can see  all but three words phrases  classic  great
book  cleverly  in this review correspond to objective materials 
the aforementioned complications present significant challenges even to supervised polarity classification systems  let alone sentiment based clustering algorithms  which do not
have access to any labeled data  to further illustrate the difficulty that these two complications impose on sentiment based clustering  consider the task of clustering a set of movie
reviews  since each review may contain a description of the plot and the authors sentiment 
a clustering algorithm may cluster reviews along either the plot dimension or the sentiment
dimension  and without knowing the users intention  they will be clustered along the most
   

fidasgupta   ng

prominent dimension  assuming the usual bag of words representation  the most prominent
dimension will more likely be plot  as it is not uncommon for a review to be devoted almost
exclusively to the plot  with the author briefly expressing her sentiment only at the end of
the review  even if the reviews contain mostly subjective materials  the most prominent
dimension may still not be sentiment owing to the aforementioned sentimental ambiguity
problem  the presence of both positive and negative sentiment bearing words in these reviews renders the sentiment dimension hidden  i e   less prominent  as far as clustering is
concerned 
in sum  our contributions in this article are five fold 
 we propose a novel active clustering algorithm that can cluster a set of documents
along the user desired dimension without any labeled data or side information such as
manually specified or automatically acquired must link and cannot link constraints 
in comparison to existing active clustering approaches  our algorithm has the appeal
of requiring much simpler human feedback 
 we demonstrate the viability of our algorithm not only by evaluating its performance
on sentiment datasets  but also via a set of human experiments  which is typically
absent in papers that involve algorithms for incorporating user feedback 
 our results have led to a deeper understanding of spectral clustering  specifically  we
propose a novel application of the top eigenvectors produced by a spectral clustering
algorithm  where we use them to unveil the important clustering dimensions of a text
collection 
 our results also have implications for domain adaptation  a topic that has recently
received a lot of attention in the nlp community  specifically  we show that the
sentiment dimension manually identified for one domain can be used to automatically
identify the sentiment dimension for a new  but similar  domain 
 preliminary results on datasets that possess more than one clustering dimension  e g  
a collection of book and dvd reviews  which can be clustered by sentiment or by the
type of the product concerned  indicate that our algorithm is capable of producing
multiple clusterings of a dataset  one along each dimension  hence  our algorithm can
potentially reveal more information from a dataset than is possible with traditional
clustering algorithms  which can only produce a single clustering of the data  the
ability to produce multiple clusterings is a particularly useful feature for a user who
does not have any idea of how she wants the documents to be clustered  due to the
lack of knowledge of the data  for instance   even if a user has some knowledge of the
data and knows how she wants the documents to be clustered  our algorithm can help
unveil other hidden dimensions that she is not previously aware of but may also be
of interest to her 
the rest of this article is organized as follows  section   presents the basics of spectral
clustering  which will facilitate the discussion of our active clustering algorithm in section
   we describe our human experiments and evaluation results on several sentiment datasets
in section   and the significance of our work in section    finally  we discuss related work
in section   and conclude in section   
   

fiinducing your ideal clustering with minimal feedback

   spectral clustering
when given a clustering task  an important question to ask is  which clustering algorithm
should we use  a popular choice is k means  nevertheless  it is well known that k means has
the major drawback of not being able to separate data points that are not linearly separable
in the given feature space  e g   see dhillon  guan    kulis        cai  he    han        
moreover  since k means clusters documents directly in the given feature space  which for
text applications typically comprises hundreds of thousands of features  its performance
could be adversely affected by the curse of dimensionality  spectral clustering algorithms
were developed in response to these problems with k means  in this section  we first present
one of the most commonly used algorithms for spectral clustering  section       then  we
provide the intuition behind spectral clustering  section       finally  we describe two ways
to use the resulting eigenvectors to produce a clustering  section      
    algorithm
let x  x            xn   be a set of n data points to be clustered  s   x  x   be a similarity
function defined over x  and s be a similarity matrix that captures pairwise similarities
 i e   si j   s xi   xj     like many other clustering algorithms  a spectral clustering algorithm
takes s as input and outputs a k way partition c    c    c        ck    i e   ki   ci   x and
i  j   i    j   ci  cj      equivalently  one can think of spectral clustering as learning
a partitioning function f   which  in the rest of this article  will be represented as a vector
such that f  i              k  indicates the cluster to which xi should be assigned  note
that the cluster labels are interchangeable and can even be renamed without any loss of
generality 
among the well known spectral clustering algorithms  e g   weiss        shi   malik 
      kannan  vempala    vetta         we adopt the one proposed by ng  jordan  and
weiss         as it is arguably the most widely used  below are the main steps of ng et
al s spectral clustering algorithm 
   create the diagonal matrix d whose  i i  th entry is the sum of the i th row of s 
and then construct the laplacian matrix  l   d     sd      
   find the eigenvalues and the eigenvectors of l 
   create a new matrix from the m eigenvectors that correspond to the m largest eigenvalues 
   each data point is now rank reduced to a point in the m dimensional space  normalize
each point to unit length  while retaining the sign of each value  
   apply k means to cluster the data points using the resulting m eigenvectors 
in other words  spectral clustering clusters data points in a low dimensional space  where
each dimension corresponds to a top eigenvector of the laplacian matrix 
   we follow ng et al         and employ a normalized dual form of the usual laplacian d  s 

   

fidasgupta   ng

    intuition behind spectral clustering
it may not be immediately clear why spectral clustering produces a meaningful partitioning of a set of points  there are theoretical justifications behind spectral clustering  but
since the mathematics is quite involved  we will only provide an intuitive justification of
this clustering technique in a way that is sufficient for the reader to understand our active
clustering algorithm in section    and refer the interested reader to shi and maliks       
seminal paper on spectral clustering for details  since we will only apply spectral clustering
to produce a   way clustering of a given set of data points in the rest of this article  we will
center our discussion on   way clustering in this subsection 
spectral clustering employs a graph theoretic notion of grouping  specifically  a set of
data points in an arbitrary feature space is represented as an undirected weighted graph 
where each node corresponds to a data point  and the edge weight between two nodes xi
and xj is their similarity  si j  
given this graph formulation  a reasonable way to produce a   way partitioning of the
data points is to minimize the similarity between the resulting two clusters  c  and c   
hence  a reasonable objective function to minimize is the cut value  where
x
cut c    c     
si j  f  i   f  j     
i j

without loss of generality  we can define f as follows 

    i  c 
f  i   
    i  c 
as mentioned before  while we use   and   as cluster labels here  they are interchangeable
and can in fact be renamed in whatever way we want 
one problem with minimizing the cut value  as noticed by wu and leahy         is
that this objective favors producing unbalanced clusters in which one of them contains a
very small number of nodes  in other words  there is a bias towards isolating a small set
of nodes  as mentioned by shi and malik         this should not be surprising  since the
number of edges involved in the cut  and hence the cut value  tends to increase as the sizes
of the two clusters become relatively balanced 
a closer examination of the minimum cut criterion reveals the problem  while it minimizes inter cluster similarity  it makes no attempt to maximize intra cluster similarity  to
address this weakness  shi and malik        propose to minimize instead the normalized
cut value  n cut  which takes into account both inter cluster dissimilarity and intra cluster
similarity  more specifically 
cut c    c   
cut c    c   
 
 
assoc c    c   c    assoc c    c   c   
p
where assoc a  b   computed as xi a xj b si j   is the total connection from the nodes in
a to the nodes in b  given this definition  a cut resulting from unbalanced clusters will no
longer have a small n cut value  to see the reason  consider the case where c  consists of
just one node  in this case  assoc c    c   c      cut c    c     making n cut c    c    large 
n cut c    c     

   

fiinducing your ideal clustering with minimal feedback

after some algebra  we can express n cut as follows 
n cut c    c     

f t  d  s f
f t df

subject to the constraints that  df  t       and
 rp
d i 


pic 

ic  d i 
rp
f  i   
d i 


  pic  d i 
ic 

  i  c 
  i  c 

where d i    d i  i   as defined in section       the first constraint  which specifies that
df is orthogonal to    can be intuitively understood as follows  since    being a constant
vector where all of its entries are    cannot be used to induce a partition  this constraint
avoids the trivial solution in which all points are assigned to the same cluster 
unfortunately  papadimitriou proves that minimizing normalized cut is an np complete
problem  even for the special case of graphs on regular grids  see shi   malik        for the
proof   hence  following shi and malik  we relax this minimization problem by dropping
the second constraint and allowing each entry of f to take on a real value rather than one
of two discrete values  seeking a real valued solution to the following problem 
minn

f 

f t  d  s f
f t df

   

subject to
df    
assuming that g   d     f   we can rewrite problem     as
minn

g

gt d      d  s d     g
gt g

   

subject to
g  d       
following the standard rayleigh ritz theorem  one can prove that the solution to
problem      g  is the eigenvector that corresponds to the second smallest eigenvalue of
d      d  s d       or equivalently  the eigenvector that corresponds to the second largest
eigenvector of d    sd       which is the laplacian matrix l defined in section      for
simplicity  we will henceforth refer to the eigenvector that corresponds to the n th largest
eigenvalue of l simply as its n th eigenvector and denote it as en   
   besides normalized cut  ratio cut  chan  schlag    zien         average association  shi   malik        
and min max cut  ding  he  zha  gu    simon        have also been used as objective functions for
spectral clustering algorithms 
   given that problem     involves minimizing a rayleigh quotient  it may seem somewhat unintuitive that
its solution is the second eigenvector of l rather than its first eigenvector  the reason can be attributed
to the constraint associated with the problem  which specifies that the solution g is perpendicular to
d       the first eigenvector of l 

   

fidasgupta   ng

this is the idea behind spectral clustering  the second eigenvector of l is an approximate solution to the problem of minimizing normalized cut   of course  since the second
eigenvector is a real valued solution  we will have to convert it into a partitioning function
so that it can be used to cluster the data points  section     explains two simple ways of
converting this eigenvector into a partitioning function 
it turns out that the other eigenvectors of l also convey useful information about the
data  specifically  if we impose an additional constraint to problem     forcing the solution to be orthogonal to the second eigenvector of l  then the solution becomes the third
eigenvector  hence  the third eigenvector can be thought of as a suboptimal solution to
problem      meaning that it can also be used to impose a reasonably good partition of
the data points  perhaps more importantly  since the eigenvectors of l are orthogonal to
each other  because l is symmetric   the clustering produced by using the third eigenvector
is likely to correspond to a different dimension of the data than that produced by the second
eigenvector 
more generally  if we limit the solution space to only those real valued vectors that are
orthogonal to the first m eigenvectors of l  then the solution to our constrained optimization
problem is the  m      th eigenvector of l  in other words  each of the top eigenvectors of
l can intuitively be thought of as revealing an important dimension of the data  although
subsequent eigenvectors are progressively less ideal as far as clustering is concerned 
    clustering with eigenvectors
as ng et al         point out  different authors still disagree on which eigenvectors to
use  and how to derive clusters from them  in this subsection  we describe two common
methods for determining which eigenvectors to use  and for each method  we show how to
derive clusters using the selected eigenvector s   these methods will serve as baselines in
our evaluation 
      method    using the second eigenvector only
since shi and malik        show that the second eigenvector  e    is the approximate solution
to the problem of minimizing the normalized cut  it should perhaps not be surprising that
e  is commonly chosen as the only eigenvector for deriving a partition  however  since e 
is a real valued solution to the constrained optimization problem  we need to specify how
we can derive clusters from it 
clustering using e  is trivial  since we have a linearization of the points  one simple way
is to determine the threshold for partitioning them  however  we follow ng et al        
and cluster the points using   means in this one dimensional space 
      method    using the top m eigenvectors
recall from section     that after eigen decomposing the laplacian matrix  each data point
is represented by m co ordinates  in the second method  we use   means to cluster the data
points in this m dimensional space  effectively exploiting all of the top m eigenvectors 
   in fact  since f   d    g  we have to pre multiply the second eigenvector of l by d    to get the
solution to problem      but following ng et al          we employ the second eigenvector of l directly
for clustering  ignoring the term d     

   

fiinducing your ideal clustering with minimal feedback

   our active clustering algorithm
as mentioned before  sentiment based clustering is challenging  in part due to the fact that
reviews can be clustered along more than one dimension  in this section  we describe our
active clustering algorithm  which makes it easy for a user to specify that the dimension
along which she wants to cluster the data points is sentiment  recall that our algorithm first
applies spectral clustering to reveal the most important dimensions of the data  and then
lets the user select the desired dimension  i e   sentiment   to motivate the importance of
user feedback  it helps to understand why the two baseline clustering algorithms described
in section      which are also based on spectral methods but do not rely on user feedback  may not always yield a sentiment based clustering  to begin with  consider the first
method  where only the second eigenvector is used to induce the partition  recall that the
second eigenvector reveals the most prominent dimension of the data  hence  if sentiment is
not the most prominent dimension  which can happen if the non sentiment bearing words
outnumber the sentiment bearing words in the bag of words representation of a review  
then the resulting clustering of the reviews may not be sentiment oriented  a similar line
of reasoning can be used to explain why the second baseline clustering algorithm  which
clusters based on all of the top eigenvectors  may not always work well  since each eigenvector corresponds to a different dimension  and  in particular  some of them correspond to
non sentiment dimensions   using all of them to represent a review may hamper the accurate computation of the similarity of two reviews as far as clustering along the sentiment
dimension is concerned  in the rest of this section  we discuss in detail the major steps of
our active clustering algorithm  which allows easy incorporation of user feedback 
    step    identify the important clustering dimensions
we rely on a simple method for identifying the important clustering dimensions of a given
text collection  we employ the top eigenvectors of the laplacian as the important clustering dimensions  this method is motivated by the fact that e    the second eigenvector of
the laplacian  is the optimal real valued solution to the objective function that spectral
clustering minimizes  i e   normalized cut  shi   malik         and is therefore an optimal
clustering dimension  more importantly  we exploit a rarely utilized observation discussed
in section      while the remaining eigenvectors are all suboptimal solutions  with ei being more suboptimal as i increases   the top eigenvectors  i e   those with small i values  
being less suboptimal  may still yield reasonably good  though not optimal  clusterings of
the data and can therefore serve as good clustering dimensions  existing applications of
spectral clustering have mainly clustered data points in the space defined by all of the top
eigenvectors  and have not attempted to use each of the ei s  with i      separately to
produce clusterings  unlike ours  note that the first eigenvector  being a constant vector 
simply assigns all data points to the same cluster and therefore is typically ignored 
    step    identify the relevant features for each partition
given the eigen decomposition from step    we first obtain the second through the m th
eigenvectors  which correspond to the most important dimensions of the data  the next
question is  how can we determine which dimension captures the user interest  one way to
   

fidasgupta   ng

do this is to have the user inspect each of the m  partitions of the reviews and decide which
corresponds most closely to a sentiment based clustering  the main drawback associated
with this kind of user feedback is that the user may have to read a large number of reviews
in order to make a decision  hence  to reduce human effort  we employ an alternative
procedure  we     identify the most informative features for characterizing each partition 
and     have the user inspect just the features rather than the reviews  to make it easy for
a human to identify a clustering dimension  the features should be chosen so that they are
useful for distinguishing the reviews in the two clusters 
to identify and rank the informative features  we employ a method that we call maximum
margin feature ranking  mmfr    recall that a maximum margin classifier  e g   a support
vector machine  separates data points from two classes while maximizing the margin of
separation  specifically  a maximum margin hyperplane is defined by w  x  b      where
x is a feature vector representing an arbitrary data point  and w  a weight vector  and b  a
scalar  are parameters that are learned by solving the following constrained optimization
problem 
x
 
i
min kwk    c
 
i
subject to
ci  w  xi  b      i  

   i  n 

where ci          is the class of the i th training point xi   i is the degree of misclassification of xi   and c is a regularization parameter that balances training error and model
complexity 
we use w to identify the most informative features for a partition  note that the most
informative features are those with large absolute weight values  a feature with a large
positive  negative  weight is strongly indicative of the positive  negative  class   we exploit
this observation and identify the most informative features for a partition by     training a
binary svm classifier  on the partition  where data points in the same cluster are assumed
to have the same class value      sorting the features according to the svm learned feature
weights  and     generating two ranked lists of informative features using the top and bottom
f features  respectively 
given the ranked lists generated for each of the m    partitions  the user will select one
of the partitions dimensions as most relevant to sentiment by inspecting as many features
in the ranked lists as needed  after picking the most relevant dimension  the user will
label one of the two feature lists associated with this dimension as positive and the other
as negative  since each feature list represents one of the clusters  the cluster associated
with the positive list is labeled positive and the cluster associated with the negative list
is labeled negative 
   note that other commonly used feature selection techniques such as log likelihood ratio and information
gain can also be applied to identify these informative features  see yang   pedersen        for an
overview  
   the notion of using svm feature weights as measures of feature informativeness has also been explored
in other work  see  for instance  the work of fung         gilad bachrach  navot  and tishby        
and kugler  aoki  kuroyanagi  iwata  and nugroho        for details 
   all the svm classifiers in this article are trained using the svmlight package  joachims      a   with
the learning parameters set to their default values 

   

fiinducing your ideal clustering with minimal feedback

in comparison to existing user feedback mechanisms for assisting a clustering algorithm 
ours requires comparatively little human intervention  we only require that the user select a
dimension by examining a small number of features  as opposed to having the user construct
the feature space or identify clusters that need to be merged or split as is required with
other methods 
    step    identify the unambiguous reviews
there is a caveat  however  as mentioned in the introduction  many reviews contain both
positive and negative sentiment bearing words  these ambiguous reviews are more likely
to be clustered incorrectly than their unambiguous counterparts  since the ranked lists
of features are derived from each partition  the presence of these ambiguous reviews can
adversely affect the identification of informative features using mmfr  as a result  we
remove the ambiguous reviews before deriving informative features from a partition 
we employ a simple method for identifying unambiguous reviews  in the computation
of eigenvalues  each data point factors out the orthogonal projections of each of the other
data points with which they have an affinity  ambiguous data points receive the orthogonal
projections from both the positive and negative data points  and hence they have near zero
values in the pivot eigenvectors  in other words  the points with near zero values in the
eigenvectors are more ambiguous than those with large absolute values  we therefore sort
the data points according to their corresponding values in the eigenvector  and keep only
the top n   and the bottom n   data points  we induce the informative features only from
the resulting     of the data points  and present them to the user so that she can select
the desired partition  
    step    cluster along the selected eigenvector
finally  we employ   means to cluster all the reviews along the eigenvector selected by the
user  regardless of whether a review is ambiguous or not 

   evaluation
in this section  we describe experiments that aim to evaluate the effectiveness of our active
clustering algorithm and provide insights into it 
    experimental setup
we begin by discussing the details on the datasets  the document preprocessing method 
the implementation of spectral clustering  and the evaluation metrics 
   note that     is a somewhat arbitrary choice  underlying this choice is merely the assumption that a
fraction of the reviews is unambiguous  as we will see in the evaluation section  these reviews can be
classified according to their polarity with a high accuracy  consequently  the features induced from the
resulting clusters are also of high quality  additional experiments revealed that the list of top ranking
features does not change significantly when induced from a smaller number of unambiguous reviews 

   

fidasgupta   ng

      datasets
we use five sentiment datasets  including the widely used movie review dataset  mov 
 pang  lee    vaithyanathan        as well as four datasets containing reviews of four
different types of products from amazon  books  boo   dvds  dvd   electronics  ele  
and kitchen appliances  kit    blitzer  dredze    pereira         each dataset has     
labeled reviews       positives and      negatives   to illustrate the difference between
topic based clustering and sentiment based clustering  we will also show topic based clustering results on pol  a dataset created by taking all the documents from the two sections
of    newsgroups that discuss issues in cryptography and politics  namely  sci crypt and
talks politics 
      document preprocessing
to preprocess a document  we first tokenize and downcase it  and then represent it as a
vector of unstemmed unigrams  each of which assumes a value of   or   that indicates its
presence or absence in the document  in addition  we remove from the vector punctuation 
numbers  words of length one  and words that occur in only a single review 
following common practice in the information retrieval community  we also exclude
words with a high document frequency  many of which are stopwords or domain specific
general purpose words  e g   movies in the movie domain   a preliminary examination of
our evaluation datasets reveals that these words typically comprise     of a vocabulary 
the decision of exactly how many terms to remove from each dataset is subjective  a large
corpus typically requires more removals than a small corpus  to be consistent  we simply
sort the vocabulary by document frequency and remove the top       we will henceforth
refer to this document representation as the bag of words  bow  representation 
      spectral learning setup
following common practice in spectral learning for text domains  e g   kamvar  klein 
  manning        cai et al          we compute the similarity between two reviews by
taking the dot product of their feature vectors  as in ng et al s        spectral clustering
algorithm  we set the diagonal entries of the similarity matrix to    in addition  we set m
to    in other words  we consider the second through fifth eigenvectors  assuming that they
are sufficient for capturing the desired clusterings   
      evaluation metrics
we employ two evaluation metrics  first  we report results for each dataset in terms of
accuracy  which is the percentage of documents for which the label assigned by our system
is the same as the gold standard label  second  following kamvar et al          we evaluate
the clusters produced by our approach against the gold standard clusters using the adjusted
rand index  ari   which is the corrected for chance version of the rand index  more
specifically  given a set of n data points and two clusterings of these points  u and v  
    note that setting m to   is a somewhat arbitrary choice  and that any number of eigenvectors can be
used in our active clustering algorithm 

   

fiinducing your ideal clustering with minimal feedback

where u    u    u            um   has m clusters and v    v    v            vn   has n clusters  ari
is computed as follows 
nij 
 

 p bj 
p
   i a i
j     
ari u  v       p a  p b 
p
ai  p
j
i
j      i  
j
   i    
p

ij

n
 
bj 
n
      



in this formula  nij is the number of common objects in ui and vj   whereas ai and bj are the
number of objects in ui and vj   respectively  ari ranges from   to    better clusterings
have higher ari values 
    baseline systems
in this subsection  we describe our baseline results  the first two baseline systems are the
ones described in section      and the last two are arguably more sophisticated clustering
algorithms that are employed in an attempt to strengthen our baseline results 
      clustering using the second eigenvector only
as our first baseline  we adopt shi and maliks        approach and cluster the reviews
using only the second eigenvector  e    as described in section      results on pol and the
sentiment datasets  expressed in terms of accuracy and ari  are shown in row   of tables  a
and  b  respectively  owing to the randomness in the choice of seeds for   means  these and
all other experimental results involving   means are averaged over ten independent runs   
as we can see  this baseline achieves an accuracy of       on pol  but much lower
accuracies  of        on the sentiment datasets  the same performance trend can be
observed with ari  these results provide suggestive evidence that producing a sentimentbased clustering requires different features than producing a topic based clustering  and that
in many cases  the more salient features tend to be topic based  the difference between
sentiment based clustering and topic based clustering will be further illuminated by the
experiments in section     
in addition  it is worth noting that this baseline achieves much lower accuracies and
ari values on boo  dvd  and ele than on the remaining two sentiment datasets  since
e  captures the most prominent dimension  these results suggest that sentiment dimension
is not the most prominent dimension in these three datasets  in fact  this is intuitively
plausible  for instance  in the book domain  positive book reviews typically contain a
short description of the content  with the reviewer only briefly expressing her sentiment
somewhere in the review  similarly for the electronics domain  electronic product reviews
are typically aspect oriented  with the reviewer talking about the pros and cons of each
aspect of the product  e g   battery  durability   since the reviews are likely to contain both
positive and negative sentiment bearing words  the sentiment based clustering is unlikely
to be captured by e   
    note that clustering in a one dimensional space  as in this baseline  yields very stable results regardless
of the choice of seeds  our results over the ten runs exhibit nearly zero variance 

   

fidasgupta   ng

system variation
 nd eigenvector only
top five eigenvectors
interested reader model
nmf
our system

pol
    
    
    
    
    

mov
    
    
    
    
    

accuracy
kit boo
         
         
         
         
         

dvd
    
    
    
    
    

ele
    
    
    
    
    

 ari 
dvd
    
    
    
    
    

ele
    
    
    
    
    

 a 

system variation
 nd eigenvector only
top five eigenvectors
interested reader model
nmf
our system

pol
    
    
    
    
    

adjusted rand index
mov kit boo
              
    
         
    
         
              
              

 b 

table    results in terms of  a  accuracy and  b  adjusted rand index for the six datasets
obtained using the bag of words document representation  the strongest result s  for each
dataset are boldfaced 

      clustering using the top five eigenvectors
as our second baseline  we represent each data point using the top five eigenvectors  i e   e 
through e     and cluster them using   means in this five dimensional space  as described in
section      hence  this can be thought of as an ensemble approach  where the clustering
decision is collectively made by the five eigenvectors   
results are shown in row   of tables  a and  b    in comparison to the first baseline 
we see improvements in accuracy and ari for pol and the three sentiment datasets on
which the first baseline performs poorly  i e   boo  dvd  and ele   with the most drastic
improvement observed on ele  however  performance on the remaining two sentiment
datasets deteriorates  these results can be attributed to the fact that for boo  dvd 
and ele  e  does not capture the sentiment dimension  but since some other eigenvector
in the ensemble does  we see improvements  on the other hand  e  has already captured
the sentiment dimension in mov and kit  as a result  employing additional dimensions 
which may not be sentiment related  may only introduce noise into the computation of the
similarities between the reviews 
    while the first eigenvector can only produce a trivial clustering in which all data points reside in the
same cluster  it is commonly used in combination with other top eigenvectors to create a low dimensional
space in which data points are clustered  see the work of ng et al         for more details 
    when clustering in a five dimensional space  we observe that the results can be highly sensitive to the
choice of seeds  for instance  the variances in the accuracy observed over the ten runs for pol  mov 
kit  boo  dvd  and ele are                               and       respectively 

   

fiinducing your ideal clustering with minimal feedback

      clustering using the interested reader model
our third baseline is kamvar et al s        unsupervised clustering algorithm  which  according to the authors  is ideally suited for text clustering  and has recently been proved to
be a special case of ratio cut optimization  kulis  basu  dhillon    mooney         specifically  they introduce a new laplacian inspired by the interested reader model  this
laplacian is computed as  s   dmax i  d  dmax   where d and s are defined as in section
     except that si j    if i is not one of js k nearest neighbors and j is not one of is k
nearest neighbors  dmax is the maximum rowsum of s  and i is the identity matrix  since
its performance is highly sensitive to k  we tested values of                    for k and report in row   of tables  a and  b the best results  somewhat disappointingly  despite its
algorithmic sophistication and the fact that we are reporting the best results  this baseline
does not offer consistent improvements over the previous two  in comparison to the first
baseline  it achieves better performance on pol but worse performance on all the sentiment
datasets  like the first baseline  its results on boo  dvd and ele are particularly poor 
      clustering using non negative matrix factorization
non negative matrix factorization  nmf  has recently been shown by xu  liu  and gong
       to be effective for document clustering  after re implementing this algorithm  we
evaluate it on our six datasets    shown in row   of tables  a and  b are the best results obtained after running the algorithm five times  in comparison to the first baseline 
nmf achieves better performance on ele  comparable performance on mov  and worse
performance on the remaining datasets 
    our active clustering algorithm
in this subsection  we describe human and automatic experiments for evaluating our active
clustering algorithm 
      human experiments
unlike the four baselines  our active clustering algorithm requires users to specify which
of the four dimensions  defined by the second through fifth eigenvectors  are most closely
related to sentiment by inspecting a set of features derived from the unambiguous reviews
for each dimension using mmfr  to better understand how easy it is for a human to select
the desired dimension given the features  we performed the experiment independently with
five humans  all of whom are computer science graduate students not affiliated with this
research  and computed the agreement rate 
specifically  for each dataset  we showed each human judge the top     features for
each cluster according to mmfr  see tables    for a subset of these     features induced
for each of the six datasets  where the lightly shared columns correspond to the sentiment
dimension selected by the majority of the human judges     in addition  we informed her
    for matrix factorization we use the code downloaded from http   www csie ntu edu tw cjlin nmf index html 
    while all human judges reported that inspecting the top     features is sufficient for identifying the
sentiment dimension  we note that a user of our clustering algorithm may request to inspect as many
features as she wants 

   

fidasgupta   ng

e 
c 
serder
armenian
turkey
armenians
muslims
sdpa
argic
davidian
dbd ura
troops
c 
sternlight
wouldn
pgp
crypto
algorithm
isn
likely
access
idea
cryptograph

pol
e 
e 
c 
c 
beyer
serbs
arabs
palestinians
andi
muslims
research
wrong
israelis
department
tim
bosnia
uci
live
ab
matter
z virginia
freedom
holocaust
politics
c 
escrow
sternlight
algorithm
access
net
des
privacy
uk
systems
pgp

c 
standard
sternlight
des
escrow
employer
net
york
jake
code
algorithm

e 
c 
escrow
serial
algorithm
chips
ensure
care
strong
police
omissions
excepted
c 
internet
uucp
uk
net
quote
ac
co
didn
ai
mit

table    top ten features induced for each dimension for the pol domain  the shaded
columns correspond to the dimensions selected by the human judges  e           e  are the top eigenvectors  c  and c  are the clusters 

   

fiinducing your ideal clustering with minimal feedback

e 
c 
relationship
son
tale
husband
perfect
drama
focus
strong
beautiful
nature
c 
worst
stupid
waste
bunch
wasn
video
worse
boring
guess
anyway

mov
e 
e 
c 
c 
production
jokes
earth
kids
sequences
live
aliens
animation
war
disney
crew
animated
alien
laughs
planet
production
horror
voice
evil
hilarious
c 
sex
romantic
school
relationship
friends
jokes
laughs
sexual
cute
mother

c 
thriller
killer
murder
crime
police
car
dead
killed
starts
violence

e 
c 
starts
person
saw
feeling
lives
told
happen
am
felt
happened
c 
comic
sequences
michael
supporting
career
production
peter
style
latest
entertaining

table    top ten features induced for each dimension for the mov domain  the shaded
columns correspond to the dimensions selected by the human judges  e           e  are the top eigenvectors  c  and c  are the clusters 

   

fidasgupta   ng

boo
e 
c 
history
must
modern
important
text
reference
excellent
provides
business
both

e 
c 
series
man
history
character
death
between
war
seems
political
american

e 
c 
loved
highly
easy
enjoyed
children
again
although
excellent
understand
three

e 
c 
must
wonderful
old
feel
away
children
year
someone
man
made

c 
plot
didn
thought
boring
got
character
couldn
ll
ending
fan

c 
buy
bought
information
easy
money
recipes
pictures
look
waste
copy

c 
money
bad
nothing
waste
buy
anything
doesn
already
instead
seems

c 
boring
series
history
pages
information
between
highly
page
excellent
couldn

table    top ten features induced for each dimension for the boo domain  the shaded
columns correspond to the dimensions selected by the human judges  e           e  are the top eigenvectors  c  and c  are the clusters 

   

fiinducing your ideal clustering with minimal feedback

ele
e 
c 
mouse
cable
cables
case
red
monster
picture
kit
overall
paid

e 
c 
music
really
ipod
too
little
headphones
hard
excellent
need
fit

e 
c 
easy
used
card
fine
using
problems
fine
drive
computer
install

e 
c 
amazon
cable
card
recommend
dvd
camera
fast
far
printer
picture

c 
working
never
before
phone
days
headset
money
months
return
second

c 
worked
problem
never
item
amazon
working
support
months
returned
another

c 
money
worth
amazon
over
return
years
much
headphones
sony
received

c 
phone
off
worked
power
battery
unit
set
phones
range
little

table    top ten features induced for each dimension for the ele domain  the shaded
columns correspond to the dimensions selected by the human judges  e           e  are the top eigenvectors  c  and c  are the clusters 

   

fidasgupta   ng

e 
c 
love
clean
nice
size
set
kitchen
easily
sturdy
recommend
price
c 
months
still
back
never
worked
money
did
amazon
return
machine

kit
e 
e 
c 
c 
works
really
water
nice
clean
works
work
too
ice
quality
makes
small
thing
sturdy
need
little
keep
think
best
item
c 
price
item
set
ordered
amazon
gift
got
quality
received
knives

c 
ve
years
love
never
clean
months
over
pan
been
pans

e 
c 
pan
oven
cooking
made
pans
better
heat
cook
using
clean
c 
love
coffee
too
recommend
makes
over
size
little
maker
cup

table    top ten features induced for each dimension for the kit domain  the shaded
columns correspond to the dimensions selected by the human judges  e           e  are the top eigenvectors  c  and c  are the clusters 

   

fiinducing your ideal clustering with minimal feedback

e 
c 
worth
bought
series
money
season
fan
collection
music
tv
thought
c 
young
between
actors
men
cast
seems
job
beautiful
around
director

dvd
e 
e 
c 
c 
music
video
collection
music
excellent
found
wonderful
feel
must
bought
loved
workout
perfect
daughter
highly
recommend
makes
our
special
disappointed
c 
worst
money
thought
boring
nothing
minutes
waste
saw
pretty
reviews

c 
series
cast
fan
stars
original
comedy
actors
worth
classic
action

e 
c 
money
quality
video
worth
found
version
picture
waste
special
sound
c 
saw
watched
loved
enjoy
whole
got
family
series
season
liked

table    top ten features induced for each dimension for the dvd domain  the shaded
columns correspond to the dimensions selected by the human judges  e           e  are the top eigenvectors  c  and c  are the clusters 

   

fidasgupta   ng

judge
 
 
 
 
 
agreement

pol
     
   
 
   
 
   

mov
 
 
   
 
 
    

kit
 
 
 
 
 
   

boo
 
 
 
 
 
    

dvd
 
 
 
 
 
    

ele
 
 
 
   
 
    

table    human agreement rate  also shown are the eigenvectors selected by the five judges 
of the intended dimension  for example  for pol  the judge was told that the intended
clustering was politics vs  science  also  if she determined that more than one dimension
was relevant to the intended clustering  she was instructed to rank these dimensions in
terms of relevance  where the most relevant one would appear first in the list 
the dimensions  expressed in terms of the ids of the eigenvectors  selected by each of
the five judges for each dataset are shown in table    the agreement rate  shown in the
last row of the table  was computed based on only the highest ranked dimension selected
by each judge  as we can see  perfect agreement is achieved for four of the five sentiment
datasets  and for the remaining two datasets  near perfect agreement is achieved  these
results  together with the fact that it took five to six minutes to identify the relevant
dimension  indicate that asking a human to determine the intended dimension based on
solely the informative features is a viable task 
      clustering results
next  we cluster all      documents for each dataset using the dimension selected by the
majority of the human judges  the clustering results are shown in row   of tables  a
and  b  in comparison to the best baseline for each dataset  we see that our algorithm
performs substantially better on boo  dvd and ele  at almost the same level on mov
and kit  but slightly worse on pol  note that the improvements observed for boo  dvd
and ele can be attributed to the failure of e  to capture the sentiment dimension  perhaps
most importantly  by exploiting human feedback  our algorithm has achieved more stable
performance across the datasets than the four baselines   
      identification of unambiguous documents
recall that the features with the largest mmfr were computed from the unambiguous
documents only  to get an idea of how accurate our algorithm for identifying unambiguous
documents is  we show in table    the accuracy obtained when the unambiguous documents
in each dataset were clustered using the eigenvector selected by the majority of the judges 
as we can see  the accuracy on each dataset is higher than the corresponding accuracy
shown in row   of table  a  in fact  an accuracy of more than     was achieved on all
    as in the first baseline  since we are clustering in a one dimensional space here  the results are not
sensitive to the choice of seeds  yielding zero variance over the ten independent runs 

   

fiinducing your ideal clustering with minimal feedback

accuracy

pol
    

mov
    

kit
    

boo
    

dvd
    

ele
    

table     accuracies on unambiguous documents 

  labels

pol
   

mov
   

kit
   

boo
   

dvd
   

ele
   

table     transductive svm results 
but one dataset  this suggests that our method of identifying unambiguous documents is
reasonably accurate 
note that it is crucial to be able to achieve a high accuracy on the unambiguous documents  if clustering accuracy is low  the features induced from the clusters may not be an
accurate representation of the corresponding dimension  and the human judge may have a
difficult time identifying the intended dimension  in fact  some human judges reported difficulty in identifying the correct dimension for the ele dataset  and this can be attributed
in part to the low accuracy achieved on the unambiguous documents 
      user feedback versus labeled data
recall that our four baselines are unsupervised  whereas our algorithm can be characterized
as semi supervised  as it relies on user feedback to select the intended dimension  hence  it
should not be surprising to see that the average clustering performance of our algorithm is
better than that of the baselines 
to do a fairer comparison  we conduct another experiment in which we compare our
algorithm against a semi supervised sentiment classification system  which uses a transductive svm as the underlying semi supervised learner  more specifically  the goal of this
experiment is to determine how many labeled documents are needed in order for the transductive learner to achieve the same level of performance as our algorithm  to answer
this question  we first give the transductive learner access to the      documents for each
dataset as unlabeled data  next  we randomly sample    unlabeled documents and assign
them the true label  we then re train the classifier and compute its accuracy on the     
documents  we keep adding more labeled data     in each iteration  until it reaches the
accuracy achieved by our algorithm  results of this experiment are shown in table    
owing in the randomness involved in the selection of unlabeled documents  these results
are averaged over ten independent runs  as we can see  our user feedback is equivalent to
the effort of hand annotating     documents per dataset on average 
      multiple relevant eigenvectors
as seen from table    some human judges selected more than one eigenvector for some
datasets  e g           for pol        for mov  and       for ele   however  we never took
into account these extra eigenvectors in our previous experiments  to better understand
   

fidasgupta   ng

our system

pol
acc ari
         

mov
acc ari
         

ele
acc ari
         

table     results obtained using multiple relevant eigenvectors for the pol  mov and
ele datasets 

accuracy

pol
    

mov
    

kit
    

boo
    

dvd
    

ele
    

table     supervised classification accuracies 
whether these extra eigenvectors can help improve accuracy and ari  we conduct another
experiment in which we apply   means to cluster the documents in the space defined by all
of the selected eigenvectors  table    shows the accuracy and ari results that are averaged
over ten independent runs  as we can see  the results for pol are considerably better
than those obtained when only the highest ranked eigenvector is used  suggesting that the
extra eigenvectors contain useful information  however  the results on mov and ele drop
slightly with the addition of the extra eigenvectors  indicating that the extra sentiment
dimensions are not useful 
      supervised classification results
next  we present results for supervised classification on our five sentiment datasets  while
one should not expect our largely unsupervised approach to offer comparable performance
to a fully supervised approach  we believe that having fully supervised results will enable
the reader to get a sense of where our work stands among existing work on identifying
sentiment in these datasets  specifically  we report in table    averaged    fold crossvalidation accuracies  where an svm classifier is trained on nine folds and tested on the
remaining fold in each fold experiment  as we can see  our results lag behind the supervised
results by          on these datasets 
    alternative document representations
in the above experiments  we represented each document as a bag of words with the most
frequent      of the words removed  this is  of course  not the only way to represent a
document  in this subsection  we examine two alternative document representations in an
attempt to better understand the effect of document representation on classification results 
in our first document representation  we represent a document using all of the unigrams
that appear in it and do not remove the frequent words from the document vector  this
bag of all words  boaw  representation is motivated by the fact that the frequencies of
function words and the like have been shown in many studies to be useful features for various kinds of non topic based classification  e g   finn   kushmerick        stein  argamon 
  frieder        abbasi  chen    salem        koppel  schler    argamon         the
   

fiinducing your ideal clustering with minimal feedback

system variation
 nd eigenvector only
top five eigenvectors
interested reader model
nmf
our system

pol
    
    
    
    
    

mov
    
    
    
    
    

accuracy
kit boo
         
         
         
         
         

dvd
    
    
    
    
    

ele
    
    
    
    
    

 a 

system variation
 nd eigenvector only
top five eigenvectors
interested reader model
nmf
our system

pol
    
    
    
    
    

adjusted rand index  ari 
mov kit
boo dvd
    
    
    
    
    
    
    
    
    
    
    
    
                   
    
    
         

ele
    
    
    
    
    

 b 

table     results in terms of  a  accuracy and  b  adjusted rand index for the six datasets
obtained using the bag of all words document representation  the strongest result s  for each
dataset are boldfaced 

accuracy and ari results obtained by re running our four baselines as well as our system
using this document representation are shown in tables   a and   b  respectively  comparing tables  a and   a  we can see that when all words are used as features  the best
accuracy achieved for each dataset drops by      than when the high frequency words are
removed before spectral clustering is applied  similar trends can be observed with the ari
results shown in tables  b and   b  overall  these results substantiate our hypothesis that
retaining the high frequency words in the document representation has an adverse effect on
the performance of these clustering algorithms 
next  we experiment with another representation  specifically one in which each document is represented using only the sentiment bearing words it contains  to understand the
motivation behind this bag of sentiment words  bosw  representation  recall from the introduction that one way to encourage the clustering algorithm to produce the user desired
clustering is to design the feature space so that it contains all and only those features that
are useful for producing the user desired clustering  since we desire a sentiment based clustering  we can design a feature space composed of solely sentiment bearing words  since a
hand crafted subjectivity lexicon  i e   a lexicon where each word is manually labeled with
its prior polarity     for english is readily available  we can automatically construct a feature
space that consists of only those words that have a  positive or negative  polarity according
to the subjectivity lexicon  and represent a document using the resulting feature space  the
    the prior polarity of a word is its polarity computed without regard to the context in which the word
appears 

   

fidasgupta   ng

system variation
 nd eigenvector only
top five eigenvectors
interested reader model
nmf
our system

mov
    
    
    
    
    

kit
    
    
    
    
    

accuracy
boo dvd
         
    
    
    
    
         
         

ele
    
    
    
    
    

 a 

system variation
 nd eigenvector only
top five eigenvectors
interested reader model
nmf
our system

adjusted rand index  ari 
mov kit boo dvd ele
                        
    
                   
    
         
         
    
                   
                        
 b 

table     results in terms of  a  accuracy and  b  adjusted rand index for the five
sentiment datasets obtained using the bag of sentiment words document representation 
the strongest result s  for each dataset are boldfaced 

goal  then  is to determine whether the bosw document representation can improve the
sentiment based clustering results obtained using the bow representation 
to identify sentiment bearing words in our experiment  we employ the subjectivity lexicon introduced in the work of wilson  wiebe  and hoffmann           the lexicon contains
     words  each of which is hand labeled with a prior polarity of positive  negative 
or neutral  we create a new subjectivity lexicon l in which we retain only those words
in wilson et al s lexicon that have either a positive or negative polarity  the bosw
representation of a document is then composed of all and only those words that appear in
both l and the document 
the accuracy and ari results of our baselines and our system obtained when employing
the bosw representation are shown in tables   a and   b  respectively  consider first the
second eigenvector only baseline  nmf  and the interested reader model  in comparison
to their corresponding results in tables  a and  b  where the bow representation was
used  we can see that performance improves on the boo  dvd  and ele datasets in most
cases  but drops on the mov and kit datasets  for the top five eigenvectors baseline 
performance increases on dvd and slightly on mov  but drops on the remaining datasets 
finally  using the bosw representation causes the performance of our system to drop on
all datasets 
overall  these results seem to suggest that whether the bosw representation of a document yields better clustering results than its bow representation is rather dependent on
the underlying domain and clustering algorithm  nevertheless  we can see that the best
    see http   www cs pitt edu mpqa  

   

fiinducing your ideal clustering with minimal feedback

clustering accuracy ari achieved for each sentiment dataset using the bosw representation is significantly lower than that obtained using the bow representation  we speculate
two reasons for the poorer results  first  the general purpose subjectivity lexicon does not
cover all of the sentiment bearing words  in particular  words that are sentiment oriented
in the context of a particular domain but have a neutral polarity otherwise may be omitted from the bosw document representation  second  some non sentiment bearing words
might be useful for identifying sentiment 
    domain adaptation
as mentioned in the introduction  the majority of existing approaches to sentiment classification is supervised  one weakness of these supervised approaches is that when given a
new domain  one needs to go through the expensive process of collecting a large amount of
annotated data in order to train an accurate polarity classifier    one may argue that our
active clustering algorithm suffers from the same weakness  the user needs to identify the
sentiment dimension for each domain  one way to address this weakness is through domain
adaptation  specifically  we investigate whether the sentiment dimension manually identified for one domain  henceforth the source domain  can be used to automatically identify
the sentiment dimension for a new domain  henceforth the target domain   we hypothesize
that domain adaptation is feasible  especially if the two domains are sentimentally similar  i e   there is a significant overlap between the features that characterize the sentiment
dimensions of the two domains  
as a result  we propose the following method for automatically identifying the sentiment
dimension for the target domain  y  using the sentiment dimension manually identified for
the source domain  x  assume that the sentiment dimension of domain x is defined by
x
x
eigenvector ex   moreover  assume that c e and c e are the two vectors of the top ranked
features  obtained using mmfr  that characterize the two clusters induced by ex  with    
features in each cluster   now  given the target domain y  we first compute the similarity
between ex and each of ys top eigenvectors  ey           ey    where the similarity between two
eigenvectors ex and ey is defined as
x

y

x

y

x

y

x

y

max  c e   c e      c e   c e     c e   c e      c e   c e   
here   is a similarity function that computes the similarity between two feature vectors 
in our experiments  we simply set it to be the dot product  which allows us to capture the
degree of overlap between the two feature vectors  then  we posit the eigenvector from
 ey            ey    that has the highest overlap as the one that defines the sentiment dimension   
to determine the effectiveness of our method  we compare the automatically selected
eigenvector with the human selected eigenvector for each domain  results are shown in
table     where a y in row i and column j indicates that the sentiment dimension for
target domain j has been successfully identified by using the sentiment dimension manually
    while collecting annotated data is trivial when dealing with review data  the same is not necessarily
true for other kinds of data  for instance  people express their opinions and sentiment in political blogs
and floor debates  but the associated postings and transcripts may not be explicitly annotated with
sentiment labels 
    note that the two arguments to the max function correspond to the two different ways of creating the
mapping between the feature vectors in the two domains 

   

fidasgupta   ng

domain
mov
dvd
boo
ele
kit

mov

y
n
n
n

dvd
y

y
n
y

boo
n
y

n
n

ele
n
n
n

y

kit
n
y
y
y


table     domain adaptation results 

identified for source domain i  and an n indicates a failure  for instance  if we know
the sentiment dimension of the dvd domain  through human feedback   then our domain
adaptation method can be used to correctly identify the sentiment domain of mov and
vice versa  however  domain adaptation using our method is not always successful  for
instance  knowing the sentiment dimension of mov does not allow us to correctly predict
the sentiment dimension of ele  interestingly  if we ignore the boo kit pair  then domain
adaptation exhibits symmetry  by symmetry  we mean that if domain x can be used to
identify the correct sentiment dimension for domain y  then domain y can be used to
identify the correct sentiment dimension for domain x  this intuitively makes sense  if
x can successfully be used to identify the sentiment dimension for y  it is likely that the
two domains share a lot of sentiment words  consequently  using y to adapt to x is also
likely to be successful  the boo kit pair represents a case in which domain adaptation is
successful in only one direction  while domain adaptation is successful from boo to kit 
the similarity between the sentiment dimensions of the two domains is not high  see the
discussion in the next paragraph for details   which contributes to the failure to adaptation
in the other direction 
as mentioned at the beginning of this subsection  we hypothesize that domain adaptation is likely to be successful if the two domains under consideration are similar to each
other  to test this hypothesis  we show in table   a the similarity between the manually
identified eigenvector and the corresponding automatically identified eigenvector for each
pair of domains  three points deserve mention  first  as long as the similarity value is at
least     domain adaptation is successful  also  as long as the similarity value is at most   
domain adaptation is unsuccessful  hence  these results substantiate our hypothesis that
domain adaptation is more likely to be successful if the two domains under consideration
are more similar to each other  it would be interesting to see if these two thresholds can
be used to predict whether domain adaptation is successful given a new pair of domains 
second  domain adaptation in both directions are likely to be successful if the similarity
value is sufficiently high  as mentioned before  if the similarity value is high  then the
two domains share many sentiment words in common  which may in turn contribute to
successful domain adaptation in both directions  for the five domains we are considering 
as long as the similarity value is at least     then domain adaptation in both directions will
be successful  third  it is worth reiterating that even if the similarity value falls below this
threshold  it does not imply that domain adaptation will fail  as mentioned before  the
sentiment dimension for domain y will be  correctly  identified as long as its similarity with
   

fiinducing your ideal clustering with minimal feedback

domain
mov
dvd
boo
ele
kit

mov

  
   
   
   

dvd
  

  
   
  

boo
   
  

   
    

ele
   
    
    

  

kit
   
  
 
  


boo
   
  

   
   

ele
   
   
   

  

kit
   
 
 
  


boo
   
 

   
   

ele
   
   
   

 

kit
   
 
 
 


 a 

domain
mov
dvd
boo
ele
kit

mov

 
   
   
   

dvd
  

  
   
 
 b 

domain
mov
dvd
boo
ele
kit

mov

 
   
   
   

dvd
 

 
   
 
 c 

table     similarity results for domain adaptation   a  shows the similarity between the
sentiment eigenvector in the source domain and the eigenvector most similar to it in the target
domain   b  shows the similarity between the sentiment eigenvector in the source domain and the
second most similar eigenvector in the target domain   c  shows the similarity gap  which is the
difference between the corresponding entries in  a  and  b  

the sentiment dimension for domain x is highest among the four eigenvectors for y  as is
the case with the boo kit domain pair 
so far we have attempted to correlate the success of domain adaptation with the similarity between the manually selected eigenvector in the source domain and the eigenvector
most similar to it in the target domain  it may be worth to also consider the similarity
between the manually selected eigenvector and the second most similar eigenvector in the
target domain  as the gap in similarity may give an indication as to the success of domain adaptation  to determine whether there is a better correlation between the success
of domain adaptation and this similarity gap  we compute     the similarity between the
eigenvector manually selected for the source domain and its second most similar eigenvector
in the target domain  see table   b  as well as     the similarity gap  see table   c   which
is simply the difference between the corresponding entries in tables   a and   b  as we
can see from table   c  there also appears to be some correlation between the success of
domain adaptation and the gap values  in particular  if the gap value is at least    domain
   

fidasgupta   ng

adaptation is successful  however  if the gap value is at most    domain adaptation is unsuccessful  nevertheless  these gap values do not help to predict the domain pairs where the
success of domain adaptation cannot be predicted using the similarity values in table   a
 e g   the domain pairs that have low similarity and yet are domain adaptable   moreover 
they fail to predict the success of domain adaptation for many domain pairs  specifically
those where the gap value is between   and   
    subjectivity lexicon versus human feedback
one might argue that if we had access to a subjectivity lexicon  we could use it to automatically identify the right sentiment dimension  thus obviating the need for human feedback
altogether  in this subsection  we investigate whether it is indeed feasible to use a handbuilt general purpose sentiment lexicon to identify the eigenvector that corresponds to the
sentiment dimension in a new domain 
for our experiment  we use the subjectivity lexicon l described in section      as
mentioned before  l contains all and only those words in wilson et al s        subjectivity
lexicon that are marked with a prior polarity of positive or negative  the procedure for
automatically identifying the sentiment dimension using l is similar to the one described
in the domain adaptation section  for each of the second through fifth eigenvectors  we first
compute the similarity between the eigenvector and l  and then choose the eigenvector
that has the highest similarity with l  as in domain adaptation  we compute the similarity
between l and an eigenvector ex as
x

x

x

x

max  c l   c e      c l   c e     c l   c e      c l   c e   
where c l and c l represent the words in l that are labeled as positive and negative rex
x
spectively  and c e and c e are the top ranked features  obtained using mmfr  that
characterize the two clusters induced by ex  with     features in each cluster    is a similarity function that computes the similarity between two feature vectors  as in domain
adaptation  we simply set it to be the dot product 
our results indicate that we successfully identified the right eigenvector using l for
each of the five domains  note that while l is a general purpose  i e   domain independent 
lexicon containing only generic sentiment bearing words  it is good enough to identify the
correct sentiment dimension for five different domains  it is worth noting that the sentiment
dimension of the mov domain has the highest similarity with l  i e       out of the five
domains  suggesting that the highest ranked sentiment features of the mov domain  according to mmfr  are largely generic  dvd has the second largest similarity with l      
followed by boo       kit      and ele       the comparatively low similarity values
for kit and ele are indicative of the fact that their highest ranked sentiment features are
largely domain specific 
finally  although a subjectivity lexicon obviates the need for human feedback  we should
emphasize that this does not undermine the contribution of our feedback oriented clustering
technique  for the following reasons  first  thinking from a text mining perspective  it would
be good to have an approach that is as knowledge free as possible  employing a handcrafted subjectivity lexicon makes our system resource dependent  in fact  a subjectivity
lexicon may not be readily available for the vast majority of natural languages  second  we
   

fiinducing your ideal clustering with minimal feedback

want our method to be potentially applicable to non sentiment domains  e g   spam vs  not
spam   where we are again faced with the same problem that a hand built lexicon may not
be available 
    single data  multiple clusterings
as mentioned previously  a set of documents can be clustered along different dimensions 
for example  movie reviews can be clustered by sentiment  positive vs  negative  or genre
 e g   action  romantic or documentary   a natural question is  can we produce different
clusterings of a given set of documents  each of which corresponds to a different dimension 
for the vast majority of existing text clustering algorithms  the answer is no  they can
only cluster along exactly one dimension  which is typically the most prominent dimension 
on the other hand  since our algorithm induces the important clustering dimensions of
a dataset  each of which can in principle be used to produce a  distinct  clustering  we
hypothesize that it can generate multiple clusterings of a given dataset along its important
dimensions 
to test our claim that our algorithm can produce multiple clusterings  we evaluate it
on four datasets that possess multiple clustering dimensions  namely mov dvd  boodvd  dvd ele  and mov kit    for example  the boo dvd dataset consists of all
the reviews taken from the boo and dvd domains  hence  each augmented dataset
is composed of      reviews       from each of the two contributing domains   which can
be clustered according to either topic  e g   book vs  dvd  or sentiment    note that
the four pairs of domains used to create the augmented datasets were chosen carefully 
specifically  two augmented datasets  mov dvd and boo dvd  were created such that
their constituent domains are mutually domain adaptable according to table     and the
remaining two  dvd ele and mov kit  were created such that their constituent domains
are not domain adaptable  our goal is to see whether our active clustering algorithm is able
to produce both topic  and sentiment based clusterings for datasets with different levels of
sentimental similarity 
the clustering procedure is almost identical to the one described in section    in essence 
we     compute the top five eigenvectors of the laplacian matrix      learn the top ranked
features corresponding to e  through e  according to mmfr      ask the human judges
to identify the eigenvectors corresponding to both the topic dimension and the sentiment
dimension  and     use   means to produce two clusterings of the reviews  one according
to the selected topic dimension and the other the selected sentiment dimension  as in
section      we conducted human and automatic experiments to determine the viability of
our algorithm 
    the reason for our employing these augmented datasets is that they not only obviate the need for
additional human annotations  but also guarantee that there are at least two dimensions along which
clusters can be formed  thus allowing us to directly test its ability to produce multiple clusterings  while
it is also possible to evaluate our algorithms ability to generate multiple clusterings using the mov
dataset  by clustering along genre and sentiment   we decided to leave this for future investigation  since
the documents in mov are not annotated with genre information 
    this is not to be confused with topic sentiment mixture models  mei  ling  wondra  su    zhai        
where the goal is to first use topic models to mine the major aspects of a product from an online review
and then assign ratings to each extracted aspect  on the other hand  our goal is to design a clustering
algorithm that is capable of generating multiple clusterings of a dataset 

   

fidasgupta   ng

judge
 
 
 
 
 
agreement

mov dvd
 
 
 
 
 
    

boo dvd
 
 
 
 
 
    

dvd ele
 
 
 
 
 
    

mov kit
 
 
 
 
 
    

dvd ele
 
   
   
 
 
   

mov kit
 
   
 
 
 
   

 a 

judge
 
 
 
 
 
agreement

mov dvd
 
   
   
 
 
    

boo dvd
   
   
   
   
   
    
 b 

table     human agreement rate for selecting  a  the topic dimension and  b  the sentiment
dimension for the augmented datasets  also shown are the eigenvectors selected by the
human judges 

      human experiments
we employed the same five human judges involved in the human experiments in section    
to independently determine the topic dimension and the sentiment dimension for each of
the four augmented datasets using only the top features according to mmfr  as before  if
a human judge identifies more than one relevant eigenvector for a particular dimension  we
ask her to rank the eigenvectors according to relevance  finally  we take the topic sentiment
dimension that is the ranked first by the largest number of judges as the human selected
topic sentiment dimension 
tables   a and   b show respectively the topic and sentiment dimensions  expressed in
terms of the ids of the eigenvectors  selected by each of the five judges for each augmented
dataset  also shown in the tables is the the human agreement rate  which was computed
based on only the highest ranked dimension selected by each judge  several points from
these human experiments deserve mention 
first  for each dataset  all human judges managed to find one eigenvector  out of the
top five  that corresponds to topic and at least one other eigenvector that corresponds
to sentiment  perhaps more importantly  a human agreement rate of at least     was
achieved on all four datasets with respect to selecting the eigenvector s  that correspond
to the topic and sentiment dimensions  these results together provide suggestive evidence
that     the eigen decomposition procedure in our active clustering algorithm is effective
enough to unearth both the topic and sentiment dimensions when both of them are present
   

fiinducing your ideal clustering with minimal feedback

in a dataset  and     our proposal for incorporating user feedback via inspecting a small
number of features is viable 
second  while both topic and sentiment are prominent dimensions in these datasets  the
fact that the second eigenvector captures the topic dimension for all four datasets suggests
that topic is a more prominent dimension than sentiment  in fact  all of our human judges
reported that the topic dimension can be identified quite easily  achieving perfect agreement
on identifying the topic dimension  this provides empirical evidence for our speculation
that topic is typically  though not always  a more prominent dimension than sentiment
when both dimensions exist in a dataset 
third  while reasonably high human agreement rate for identifying the sentiment dimension was achieved  perfect agreement on two datasets and     agreement rate on the
remaining two  see table   b for details   the human judges have reported that it was difficult to identify the sentiment dimension s   especially for the two datasets composed of
sentimentally dissimilar domains 
in an attempt to gain insight into why the judges found it difficult to identify the
sentiment dimension s   we show in tables      the top ranked features induced for each
dimension using mmfr for the four augmented datasets  where the lightly shaded columns
correspond to the eigenvectors chosen for the topic dimension and the darkly shaded columns
correspond to the eigenvectors chosen for the sentiment dimension  after examining these
results  we believe that a few points deserve mention 
first  the top features generated from the sentiment eigenvector s  for mov dvd and
boo dvd  the two datasets composed of sentimentally similar constituent domains  are
clearly sentiment oriented  making it relatively easy for the human judges to determine the
sentiment eigenvector s   not so is the case for dvd ele and mov kit  the two datasets
composed of dissimilar domains  where the top features are noisier  i e   many of them
are not necessarily sentiment oriented   thus making it tougher for the judges to locate
the sentiment eigenvector s   in fact  one can see from just the top features generated by
the sentiment eigenvector s  in tables      that those for mov dvd and boo dvd are
clearly more sentiment oriented than those for dvd ele and mov kit 
it should not be surprising that the more sentimentally dissimilar the constituent domains are  the noisier the top features generated from the sentiment eigenvector s  are 
however  if the constituent domains are sentimentally similar  they tend to have many
sentiment bearing words in common  this implies that these sentiment bearing words will
appear more frequently in the augmented datasets than in each of the constituent datasets 
hence  combining the two domains helps to boost the influence of these sentiment bearing
words  increasing the chance of their appearing higher up in the list of features ranked
by mmfr  this reinforcement effect intuitively explains why the sentiment eigenvector is
clearly dominated by sentiment words for datasets composed of sentimentally similar domains  on the other hand  if the constituent domains are sentimentally dissimilar  they tend
not to have many sentiment bearing words in common  as a result  the influence of the
sentiment bearing words that are present in only one of the two constituent domains will be
diluted by the larger number of non sentiment bearing words that result from combining
the two domains  in other words  the features that are clearly sentiment oriented in just
one rather than both domains may no longer appear sufficiently high in the ranked list of
features  in fact  as we saw in tables    and     the sentiment eigenvector is contaminated
   

fidasgupta   ng

e 
c 
roles
drama
murder
meets
crime
supporting
involving
convincing
tale
lead
c 
bought
season
buy
disappointed
fan
amazon
buying
copy
dvds
watched

mov dvd
e 
e 
c 
c 
wonderful recommend
excellent
fan
beautiful
liked
personal
book
collection
read
view
excellent
art
amazing
highly
definitely
fantastic
highly
deal
absolutely
c 
stupid
boring
dull
mean
terrible
save
lame
run
guys
except

c 
buy
house
rent
waste
wait
kill
murder
obvious
season
dvds

e 
c 
kids
children
loved
child
son
daughter
boy
school
wonderful
heart
c 
quality
dark
war
horror
release
fan
earth
production
suspense
sound

table     top ten features induced for each dimension for the mov dvd domain  the
lightly and darkly shaded columns correspond to the topic and sentiment dimensions respectively
as selected by the human judges  e           e  are the top eigenvectors  c  and c  are the clusters 

   

fiinducing your ideal clustering with minimal feedback

e 
c 
reader
important
subject
understanding
modern
information
examples
political
business
nature
c 
saw
watched
actors
liked
music
season
humor
comedy
favorite
ending

boo dvd
e 
e 
c 
c 
bought
excellent
disappointed wonderful
easy
highly
information collection
price
music
waste
special
workout
classic
helpful
video
expected
perfect
reviews
amazing

e 
c 
loved
enjoyed
children
year
wonderful
child
fun
son
friends
highly

c 
young
men
cast
role
actors
script
scene
war
performance
action

c 
version
quality
waste
worst
review
original
edition
collection
amazon
format

c 
boring
ending
waste
reviews
couldn
novel
maybe
pages
stupid
finish

table     top ten features induced for each dimension for the boo dvd domain  the
lightly and darkly shaded columns correspond to the topic and sentiment dimensions respectively
as selected by the human judges  e           e  are the top eigenvectors  c  and c  are the clusters 

   

fidasgupta   ng

e 
c 
funny
acting
family
actors
action
plot
enjoy
young
wonderful
comedy
c 
unit
battery
purchased
device
problems
tried
working
plug
charge
computer

dvd ele
e 
e 
c 
c 
easy
fine
small
problems
perfect
worked
excellent
months
highly
easy
nice
working
low
computer
comfortable
day
ipod
card
headphones
drive
c 
amazon
item
review
company
return
took
check
saw
card
worked

c 
amazon
tv
purchase
disappointed
item
purchased
reviews
wanted
received
ipod

e 
c 
video
card
camera
fast
easy
cable
picture
pictures
paper
digital
c 
phone
waste
unit
battery
getting
low
power
hear
worst
batteries

table     top ten features induced for each dimension for the dvd ele domain  the
lightly and darkly shaded columns correspond to the topic and sentiment dimensions respectively
as selected by the human judges  e           e  are the top eigenvectors  c  and c  are the clusters 

   

fiinducing your ideal clustering with minimal feedback

e 
c 
james
directed
sex
hour
drama
relationship
death
direction
tv
michael
c 
food
recommend
pot
purchased
mine
kitchen
mixer
handle
size
store

mov kit
e 
e 
c 
c 
pan
coffee
cooking
clean
clean
machine
pans
ice
cook
maker
heat
plastic
oven
cup
heavy
fill
food
months
stick
working
c 
months
purchased
worked
broke
amazon
coffee
replacement
month
tried
service

c 
item
price
sheets
ordered
amazon
received
beautiful
dishes
arrived
sets

e 
c 
price
clean
kitchen
knife
knives
size
sharp
dishwasher
cutting
attractive
c 
pan
toaster
oven
pans
heat
return
bottom
worked
read
toast

table     top ten features induced for each dimension for the mov kit domain  the
lightly and darkly shaded columns correspond to the topic and sentiment dimensions respectively
as selected by the human judges  e           e  are the top eigenvectors  c  and c  are the clusters 

   

fidasgupta   ng

 nd eigenvector only
top five eigenvectors
interested reader model
nmf
our system

mov dvd
acc ari
         
         
         
         
         

boo dvd
acc ari
         
         
         
         
         

dvd ele
acc ari
         
         
         
         
         

mov kit
acc ari
         
         
         
         
         

dvd ele
acc ari
         
         
         
         
         

mov kit
acc ari
         
         
         
         
         

 a 

 nd eigenvector only
top five eigenvectors
interested reader model
nmf
our system

mov dvd
acc ari
         
         
         
         
         

boo dvd
acc ari
         
         
         
         
         
 b 

table     results on  a  topic based clustering and  b  sentiment based clustering for the
four augmented datasets  the strongest results for each dataset are boldfaced 

by a number of features that are not necessarily sentiment bearing  which make it difficult
for the human judges to identify the sentiment dimension 
another interesting point to note is that for some datasets  there seems to be more than
one eigenvector that correspond to sentiment  for instance  for the boo dvd dataset  all
five human judges agreed that both e  and e  correspond to the sentiment dimension  a
closer examination of these two eigenvectors  shown in table     reveals a very interesting
pattern  in e    the positive features  in c    came from the dvd domain and the negative
features  in c    came from the boo domain  whereas in e    the positive features  in
c    came from boo domain and the negative features  in c    came from dvd  in other
words  e  partitions the reviews according to the positive of dvd and the negative of
boo  whereas e  does the reverse  this suggests that the eigen decomposition procedure
is smart enough not to merge the positive and negative sentiment bearing words from
the two domains together  perhaps even more importantly  both e  and e  are not only
partitioning the reviews along the sentiment dimension but also the topic dimension 
      clustering results
rows    of tables   a and   b show the topic  and sentiment based clustering results for
the same four baseline text clustering algorithms that were described in section      note
that each of these baselines can only produce one clustering of the documents per dataset 
hence  for each baseline  the topic based clustering results are produced by comparing
this clustering against the gold standard topic based clustering  and the sentiment based
   

fiinducing your ideal clustering with minimal feedback

clustering results are produced by comparing the same clustering against the gold standard
sentiment based clustering 
as we can see from these topic based results in table   a  the baseline in which we cluster
using only the second eigenvector achieves the best average clustering results over the four
augmented datasets  this can potentially be attributed to the fact that e  corresponds to
the topic dimension for all four datasets according to the human judges  as described in the
human experiments  however  clustering using only e  does not produce the best clustering
results on all four datasets  in fact  the interested reader model achieves the best results
on mov dvd  dvd ele  and mov kit  nevertheless  its results on boo dvd are the
worst among the baselines  the same is true for the top five eigenvectors baseline and
nmf  both of them have yielded poor results on mov dvd  in addition  nmfs results on
boo dvd and mov kit are not promising either 
as far as the sentiment based baseline clustering results are concerned  see rows   
of table   b   the best average performance is achieved by nmf  except for three cases
 nmf on mov dvd and mov kit  as well as top five eigenvectors on mov dvd  
these baseline results are not particularly promising  with accuracy results in the low fifties
and ari results close to zero 
the topic  and sentiment based clustering results produced by our algorithm are shown
in row   of tables   a and   b  specifically  these results are obtained by grouping the
reviews according to the eigenvectors manually selected for the topic and sentiment dimensions  respectively  hence  unlike the baselines  the topic based clustering and the
sentiment based clustering produced by our algorithm are different from each other  as
before  in cases where the human judges selected more than one eigenvector for each dimension  we use the eigenvector that is ranked first most frequently  as we can see  the
accuracies for topic based clustering are reasonably high  ranging from       to       
these results suggest that it is possible to achieve high performance topic based  or more
precisely  domain based  clustering for a dataset even when another prominent clustering dimension  i e   sentiment  is present  on the other hand  despite the existence of eigenvectors
that clearly capture the sentiment dimension for these datasets  e g   e  for the mov dvd
dataset   the sentiment based clustering accuracies and ari values are lower than those of
topic based clustering  this can potentially be attributed to the reason mentioned in the
introduction  the fact that reviews are sentimentally ambiguous makes them non trivial
to classify  in comparison to the four baselines  our algorithm achieves not only the best
average performance over the four datasets but also comparatively very stable performance
across these datasets 
it is worth noting that the sentiment based clustering results produced by our algorithm
for mov dvd and boo dvd are higher than those for dvd ele and mov kit  this is
perhaps not surprising  as discussed before  the human judges have found it more difficult
to identify the sentiment eigenvector for dvd ele and mov kit than for mov dvd and
boo dvd  owing in part to the fact that many of the top ranked features in the sentiment
eigenvector for dvd ele and mov kit are not sentiment oriented  which in turn can
be attributed to the fact that both of these datasets correspond to domain pairs that
are sentimentally dissimilar  as mentioned above  two sentimentally dissimilar constituent
domains tend not to have many sentiment bearing words in common  and consequently  the
influence of the sentiment bearing words that are present in only one of the two constituent
   

fidasgupta   ng

domains will be diluted by the larger number of non sentiment bearing words that result
from combining the two domains  making it difficult to produce a good sentiment based
clustering  on the other hand  combining the two domains helps to boost the influence of
these sentiment bearing words  increasing the chance of their appearing higher up in the
list of features ranked by mmfr and producing a good sentiment based clustering 
interestingly  our algorithm achieves better topic based clustering results on the two
datasets  dvd ele and mov kit  where it achieves poorer sentiment based clustering results  in fact  the topic based clustering accuracies on dvd ele and mov kit are
near perfect        and       for dvd ele and mov kit respectively  this is by no
means a coincidence  when the constituent domains of an augmented dataset are highly
dissimilar  i e   their word usage tends to differ considerably from each other   the topic clusters are well separated from each other and hence high topic based clustering results can be
achieved  a similar line of reasoning can explain why our algorithm finds it comparatively
more difficult to produce a good topic based clustering for mov dvd and boo dvd 
where the constituent domains are similar 
these results seem to suggest that a higher topic based accuracy ari implies a lower
sentiment based accuracy ari and vice versa  we speculate that when the constituent
domains are similar  their sentiment bearing features tend to be similar and as a result 
sentiment based results tend to be good and topic based results tend to be poor  additional
experiments are needed to determine the reason 
overall  these results provide supporting evidence that our feedback oriented algorithm
can produce multiple clusterings of a dataset  in particular  even though the sentimentbased clustering accuracies are not as high as the topic based clustering accuracies for the
augmented datasets  the current level of performance of our algorithm is arguably reasonable  especially considering the fact that sentiment based clustering is a challenging task
and that traditional clustering algorithms fail to even produce more than one clustering 
      multiple relevant eigenvectors
recall from table   b that for each of the four augmented datasets  there is at least one
judge who indicated that more than one eigenvector is relevant to the sentiment dimension 
however  when producing the sentiment based clustering results using our system in table   b  we only used the eigenvector that was ranked most frequently by the human judges 
to better understand whether using more relevant eigenvectors can help improve the results for sentiment based clustering  we repeat the experiment in which we apply   means
to cluster the documents in the space defined by all the eigenvectors that were determined
as relevant by at least one judge  more specifically  we cluster with the following set of
eigenvectors        for mov dvd        for boo dvd        for dvd ele  and      
for mov kit 
the accuracy and ari results of this experiment are shown in table     in comparison to
the results in the last row of table   b  we see that using additional relevant eigenvectors
yields better results for all but the boo dvd dataset  while it may not be easy to
determine the reason  we believe that the poorer results observed on boo dvd can be
attributed to the impurity of e    which captures not only sentiment but also topic  as
discussed before  on the other hand  the additional sentiment eigenvectors chosen for the
   

fiinducing your ideal clustering with minimal feedback

our system

mov dvd
acc ari
         

boo dvd
acc ari
         

dvd ele
acc ari
         

mov kit
acc ari
         

table     results on sentiment based clustering obtained using multiple relevant eigenvectors for the four augmented datasets 
other three augmented datasets do not seem to have this impurity problem  as they all
capture the sentiment dimension for only one of the constituent domains 

   significance of our work
we believe that our approach is significant in the following aspects 
   producing a clustering according to user interest  we proposed a novel framework in which we enabled a spectral clustering algorithm to take into account human
feedback and produce a clustering along the dimension of interest to the user  a
particularly appealing aspect of our approach is concerned with the relatively minimal human feedback it demands  where the user just needs to take a cursory look
at a small number of features that are representative of each induced dimension  it
is worth noting that having a human inspect and select an automatically induced
clustering dimension is a new form of interaction between a human and a clustering
algorithm  it enables a human to easily engage in various clustering tasks to help improve their performance in an easy  low effort manner  we believe that our approach 
which belongs to an emerging family of interactive algorithms that allows the user to
make small  guiding tweaks and thereby get results much better than would otherwise
be possible  is the future of information retrieval 
   inducing human interpretable clustering dimensions  the dimensions produced by spectral clustering or other dimensionality reduction algorithms  e g   latent
semantic indexing  lsi   deerwester  dumais  furnas  landauer    harshman       
are generally considered non interpretable  sebastiani         unlike a dimension in
the original feature space  which typically corresponds to a word type and can therefore be interpreted by a human easily  the results of our preliminary study challenge
this common wisdom  we show in the context of text clustering that a dimension
in the low dimensional space induced by spectral clustering can be interpreted by a
human  we believe the ability to produce human interpretable dimensions enables us
to employ spectral clustering  and perhaps other dimensionality reduction based clustering algorithms  for text processing in a more intelligent manner  this is especially
the case with respect to selecting the dimensions that are pertinent to the task at
hand  for example  in existing applications of spectral clustering to the topic based
clustering task  e g   xu et al         he  cai  liu    ma        hu  deng  guo    xu 
       all of the dimensions in the low dimensional space are typically used  since
we showed that not all dimensions produced by spectral clustering for a dataset are
necessarily topic related  we can potentially improve topic based clustering results by
   

fidasgupta   ng

not employing the non topic related dimensions in the clustering process  in addition 
since some of these induced dimensions correspond to non topic dimensions  we can
use them to produce non topic based clusterings  in particular  given the recent surge
of interest in the nlp community in text classification along non topic dimensions
such as sentiment and gender  e g   garera   yarowsky        jurafsky  ranganath   
mcfarland         our approach offers a solution to these tasks that does not rely on
labeled data  unlike the majority of existing approaches to non topic based text classification  which are supervised in nature  overall  we believe that nlp researchers
have not fully exploited the power of spectral clustering  and hence the rewards of
understanding spectral clustering in light of our results may be significant 
   producing multiple clusterings  while the majority of existing text clustering
algorithms can produce a single clustering of a dataset  our approach can potentially
be used to produce multiple clusterings  one along each of the important clustering
dimensions induced via a novel application of spectral clustering 
finally  it is worth mentioning that the task of inducing clustering dimensions is reminiscent of the influential topic modeling task  blei  ng    jordon         whose goal is to
discover the major topics of a set of documents in an unsupervised manner  note that the
two tasks are fundamentally different  while a topic model attempts to discover the major
topics in a set of documents  our dimension model aims to discover the major clustering
dimensions  nevertheless  the two models bear resemblance to each other in many ways 
first  they both employ clustering to discover information from a text collection in an unsupervised manner  second  they both display the learned information to a human using
representative words  a topic model represents each induced topic using words that are
representative of each topic  and our dimension model represents each induced clustering
dimension using words representative of the two document clusters involved in the dimension  finally  not all induced topics and clustering dimensions are human recognizable  but
for those that are  a human is needed to assign labels to them  we believe that the induction
of clustering dimensions has the potential to substantially enhance the capability of existing
text analysis algorithms to discover knowledge from a text collection in an unsupervised
manner by complementing the information induced by a topic model 

   related work
in the introduction  we discussed related work on producing a user desired clustering  in this
section  we focus on discussing related work on topic based clustering and classification  sentiment classification  active learning  and producing multiple clusterings in computational
stylistics 
topic based text clustering  traditional research on text clustering has focused primarily on topic based clustering  owing in large part to darpas topic detection and
tracking initiative in the     s  many different clustering algorithms have been used  including non hierarhical algorithms such as k means and expectation maximization  em 
and hierarchical algorithms such as single link  complete link  group average  and singlepass  hatzivassiloglou  gravano    maganti         these algorithms cluster a given set of
   

fiinducing your ideal clustering with minimal feedback

documents in a feature space that is typically spanned by all of the unigrams  however 
clustering in such a high dimensional space does not allow the distance between two documents to be reliably computed due to the curse of dimensionality  consequently  more
recent work has focused on the development of algorithms that cluster documents in a lowdimensional space constructed via dimensionality reduction  representative members of
this family of dimensionality reduction based clustering algorithms include traditional algorithms that are based on lsi  deerwester et al          as well as more recently proposed
 and arguably better performing  algorithms such as spectral clustering  shi   malik       
ng et al          non negative matrix factorization  xu et al          locality preserving indexing  he et al          and locality discriminating indexing  hu et al          despite the
development of these new clustering algorithms  they have primarily been evaluated with
respect to their ability to produce topic based clusterings 
topic based text classification  as yang and liu        put it  text classification is
inherently a supervised learning task  in fact  it is arguably one of the most popular
tasks to which supervised learning techniques were applied in the information retrieval
community in the     s  see sebastiani        for a comprehensive overview of related work
on machine learning for text classification   nevertheless  the annotated documents that
are needed for training a high performance supervised text classifier can be expensive to
obtain  as a result  some researchers have investigated the possibility of performing text
classification with little or even no labeled data  such attempts have led to the development
of general purpose semi supervised text classification algorithms that combine labeled and
unlabeled data using transduction  joachims      b  or em  nigam  mccallum  thrun   
mitchell         the latter of which has been used in combination with active learning  mccallum   nigam         more recently  sandler        has proposed an unsupervised text
classification algorithm that is based on mixture modeling and lsi based dimensionality
reduction 
sentiment classification  as mentioned in the introduction  despite the large amount of
recent work on sentiment analysis and opinion mining  much of it has focused on supervised
methods  see pang   lee        for a comprehensive survey of the field   one weakness of these existing supervised polarity classification systems is that they are typically
domain  and language specific  hence  when given a new domain or language  one needs to
go through the expensive process of collecting a large amount of annotated data in order
to train a high performance polarity classifier  some recent attempts have been made to
leverage existing sentiment corpora or lexicons to automatically create annotated resources
for new domains or languages  however  such methods require the existence of either a
parallel corpus machine translation engine for projecting translating annotations lexicons
from a resource rich language to the target language  banea  mihalcea  wiebe    hassan 
      wan         or a domain that is similar enough to the target domain  blitzer et al  
       when the target domain or language fails to meet this requirement  sentiment based
clustering and unsupervised polarity classification become appealing alternatives  unfortunately  with a few exceptions  e g   semi supervised sentiment analysis  riloff   wiebe 
      sindhwani   melville        dasgupta   ng      a  li  zhang    sindhwani        
these tasks are largely under investigated in the nlp community  turneys        work is
perhaps one of the most notable examples of unsupervised polarity classification  however 
   

fidasgupta   ng

while his system learns the semantic orientation of the phrases in a review in an unsupervised manner  this information is used to predict the polarity of a review heuristically 
domain adaptation  domain adaptation  also known as transfer learning  has been one
of the focal research areas in machine learning and nlp in recent years  where the goal
is to leverage the labeled data available for one domain  the source domain  to build a
classifier for another domain  the target domain   techniques for domain adaptation has
been applied to various nlp tasks  including part of speech tagging  noun phrase chunking 
syntactic parsing  named entity recognition  and word sense disambiguation  e g   daume iii
  marcu        chan   ng        duame iii        jiang   zhai      a      b   of
particular relevance to our work are domain adaptation techniques specifically developed
for text and sentiment classification  e g   blitzer  mcdonald    pereira        finn  
kushmerick        blitzer et al         gao  fan  jiang    han        ling  dai  xue  yang 
  yu        tan  cheng  wang    xu         it is worth noting that our domain adaptation
setting is different from the traditional setting  traditionally  sophisticated classifiers and or
an automatically constructed mapping of features between the two domains are used in the
adaptation process  in our setting  however  we simply utilize the sentiment dimension
that is manually selected for the source domain to automatically identify the sentiment
dimension for the target domain 
active clustering  active learning is a heavily investigated machine learning paradigm
that aims to achieve better generalization bounds with lower annotation costs  cohn  atlas 
  ladner         while in a traditional active learning setting  a human is requested to
annotate the data points that a classifier is most uncertain about  e g   cohn et al         
recent research in active learning has involved asking a human to identify or label the
features that are useful for the classification task at hand  e g   bekkerman et al        
raghavan   allan        druck  settles    mccallum        roth   small         as
mentioned in the introduction  active learning has been applied in a clustering setting  with
the goal of encouraging an algorithm to produce the user intended clustering when the
data can be clustered along multiple dimensions  different variants of active clustering
have been proposed  some request a human to label a pair of data points as must link or
cannot link to indicate whether the two points must or must not reside in the same cluster
 e g   wagstaff et al         bilenko  basu    mooney         while others have a human
determine whether two clusters should be merged or split during a hierarchical clustering
process  e g   balcan   blum         our active clustering algorithm is yet another variant 
we ask a human to select the clustering she desires from a set of automatically produced
clusterings 
generation of multiple clusterings  the notion that text collections may be clustered
in multiple independent ways has been discussed in the literature on computational stylistics
 see lim  lee    kim        biber   kurjian        grieve smith        tambouratzis  
vassiliou        gries  wulff    davies        for example   in machine learning  there
have been attempts to design algorithms for producing multiple clusterings of a dataset 
while some of them operate in a semi supervised setting  e g   gondek   hofmann       
davidson   qi         some are totally unsupervised  e g   caruana  elhawary  nguyen 
  smith        jain  meka    dhillon         for instance  caruana et al s        meta
clustering algorithm produces m different clusterings of a dataset by running k means m
   

fiinducing your ideal clustering with minimal feedback

times  each time with a random selection of seeds and a random weighting of features  its
goal is to present each local minimum found by k means as a possible clustering  however 
they do not propose any mechanism for determining which of these m clusterings is the one
the user desires  our approach  which relies on spectral clustering rather than k means for
producing multiple clusterings  fills this gap by soliciting user feedback to determine the
user desired clustering 

   conclusions and future work
unsupervised clustering algorithms typically group objects along the most prominent dimension  in part owing to their objective of simultaneously maximizing inter cluster similarity and intra cluster dissimilarity  hence  if the users intended clustering dimension
is not the most prominent dimension  these unsupervised clustering algorithms will fail
miserably  to address this problem  we proposed an active clustering algorithm  which
allows us to mine the user intended  possibly hidden  dimension of the data and produce
the desired clustering  this mechanism differs from competing methods in that it requires
very limited feedback  to select the intended dimension  the user only needs to inspect a
small number of features  we demonstrated its viability via a set of human and automatic
experiments with the challenging  yet under investigated task of sentiment based clustering  obtaining promising results  additional experiments provided suggestive evidence that
    domain adaptation can be successfully applied to identify the sentiment dimension for a
new domain if the domains under consideration are sentimentally similar      a hand crafted
subjectivity lexicon  if available  can be used to replace the user feedback needed to select
the sentiment eigenvector of a domain  and     our algorithm can potentially be used to
produce multiple clusterings for datasets that possess multiple clustering dimensions 
equally importantly  we empirically demonstrated that it is possible for a human to
interpret a dimension produced by a spectral clustering algorithm  contrary to the common
wisdom that the dimensions in an automatically constructed rank reduced space are noninterpretable  we believe that nlp researchers have not fully exploited the power of spectral
clustering  and hence the rewards of understanding spectral clustering in light of our results
may be significant  finally  our proposal to represent an induced clustering dimension as
sets of informative features facilitates exploratory text analysis  potentially enhancing the
capability of existing text analysis algorithms by complementing the information provided
by other unsupervised models  e g   a topic model  
in future work  we plan to explore several extensions to our active clustering algorithm 
first  as our active clustering algorithm can potentially be used to produce multiple clusterings of a dataset  one interesting future direction would be to examine its theoretical
guarantees  determining whether it is able to produce distinct clusterings that are qualitatively strong  see dasgupta   ng      a      b  for example   second  we plan to use our
algorithm in combination with existing feedback oriented methods  e g   bekkerman et al  
      roth   small        for improving its performance  for instance  instead of having
the user construct a relevant feature space from scratch  she can simply extend the set of
informative features identified for the user selected dimension  third  since none of the
steps in our algorithm is specifically designed for sentiment classification  we plan to apply
it to other non topic based text classification tasks that have recently received a lot of in   

fidasgupta   ng

terest in the nlp community  such as gender classification  i e   the task of determining the
gender of the author of a document   finally  we plan to adopt a richer representation of a
document that exploits features such as polarity oriented words obtained from hand built
or machine learned sentiment lexicons  e g   hu   liu        wiebe  wilson  bruce  bell 
  martin        andreevskaia   bergler        mohammad  dunne    dorr        rao
  ravichandran         or those derived from finer grained  i e   sentential  sub sentential 
phrase based  sentiment analysis methods  e g   wilson et al         kennedy   inkpen 
      polanyi   zaenen        mcdonald  hannan  neylon  wells    reynar        choi
  cardie         as richer features may make it further easier for the user to identify the
desired dimension when using our method 

bibliographic note
portions of this work were previously presented in a conference publication  dasgupta  
ng      b   the current article extends this work in several ways  most notably      a
detailed introduction to spectral clustering  section           the inclusion of two more
baseline systems  section           an investigation of the effect of document representation
on clustering performance  section           the addition of three new sections focusing on
issues in domain adaptation  section       employing a manually constructed subjectivity
lexicon  section       and producing multiple clusterings of a dataset  section       as well
as     a description of the significance of our work  section    

acknowledgments
the authors acknowledge the support of national science foundation  nsf  grant iis         we thank the four anonymous reviewers for their helpful comments and for
unanimously recommending this article for publication in jair  any opinions  findings 
conclusions or recommendations expressed in this article are those of the authors and do
not necessarily reflect the views or official policies  either expressed or implied  of nsf 

references
abbasi  a   chen  h     salem  a          sentiment analysis in multiple languages  feature
selection for opinion classification in web forums  acm transactions on information
systems         
andreevskaia  a     bergler  s          mining wordnet for a fuzzy sentiment  sentiment
tag extraction from wordnet glosses  in proceedings of the   th conference of the
european chapter of the association for computational linguistics  eacl   pp     
    
balcan  m  f     blum  a          clustering with interactive feedback  in proceedings of
the   th international conference on algorithmic learning theory  alt   pp     
    
banea  c   mihalcea  r   wiebe  j     hassan  s          multilingual subjectivity analysis using machine translation  in proceedings of the      conference on empirical
methods in natural language processing  emnlp   pp         
   

fiinducing your ideal clustering with minimal feedback

bekkerman  r   raghavan  h   allan  j     eguchi  k          interactive clustering of
text collections according to a user specified criterion  in proceedings of the   th
international joint conference on artificial intelligence  ijcai   pp         
biber  d     kurjian  j          towards a taxonomy of web registers and text types  a
multidimensional analysis  language and computers                 
bilenko  m   basu  s     mooney  r  j          integrating constraints and machine learning
in semi supervised clustering  in proceedings of the   st international conference on
machine learning  icml   pp       
blei  d  m   ng  a  y     jordon  m  i          latent dirichlet allocation  journal of
machine learning research             
blitzer  j   dredze  m     pereira  f          biographies  bollywood  boom boxes and
blenders  domain adaptation for sentiment classification  in proceedings of the   th
annual meeting of the association for computational linguistics  acl   pp         
blitzer  j   mcdonald  r     pereira  f          domain adaptation with structural correspondence learning  in proceedings of the      conference on empirical methods in
natural language processing  emnlp   pp         
cai  d   he  x     han  j          document clustering using locality preserving indexing 
ieee transactions on knowledge and data engineering                    
caruana  r   elhawary  m  f   nguyen  n     smith  c          meta clustering  in
proceedings of  th ieee international conference on data mining  icdm   pp     
    
chan  p  k   schlag  d  f     zien  j  y          spectral k way ratio cut partitioning and
clustering  ieee transactions on computer aided design               
chan  y  s     ng  h  t          domain adaptation with active learning for word sense
disambiguation  in proceedings of the   th annual meeting of the association for
computational linguistics  acl   pp       
choi  y     cardie  c          learning with compositional semantics as structural inference for subsentential sentiment analysis  in proceedings of the      conference on
empirical methods in natural language processing  emnlp   pp         
cohn  d   atlas  l     ladner  r          improving generalization with active learning 
machine learning                 
dasgupta  s     ng  v       a   mine the easy  classify the hard  a semi supervised approach to automatic sentiment classification  in proceedings of the joint conference
of the   th annual meeting of the acl and the  th international joint conference
on natural language processing of the afnlp  acl ijcnlp   pp         
dasgupta  s     ng  v       b   topic wise  sentiment wise  or otherwise  identifying the
hidden dimension for unsupervised text classification  in proceedings of the     
conference on empirical methods in natural language processing  emnlp   pp 
       
dasgupta  s     ng  v       a   mining clustering dimensions  in proceedings of the   th
international conference on machine learning  icml   pp         
   

fidasgupta   ng

dasgupta  s     ng  v       b   towards subjectifying text clustering  in proceedings of
the   rd annual international acm sigir conference on research and development
in information retrieval  sigir   pp         
daume iii  h     marcu  d          domain adaptation for statistical classifiers  journal
of artificial intelligence research             
davidson  i     qi  z          finding alternative clusterings using constraints  in proceedings of the  th ieee international conference on data mining  icdm   pp         
deerwester  s   dumais  s  t   furnas  g  w   landauer  t  k     harshman  r         
indexing by latent semantic analysis  journal of american society of information
science                 
dhillon  i   guan  y     kulis  b          kernel k means  spectral clustering and normalized cuts  in proceedings of the   th acm sigkdd international conference on
knowledge discovery and data mining  kdd   pp         
ding  c   he  x   zha  h   gu  m     simon  h  d          a min max cut algorithm
for graph partitioning and data clustering  in proceedings of the      international
conference on data mining  icdm   pp         
druck  g   settles  b     mccallum  a          active learning by labeling features  in proceedings of the      conference on empirical methods in natural language processing
 emnlp   pp       
duame iii  h          frustratingly easy domain adaptation  in proceedings of the   th
annual meeting of the association for computational linguistics  acl   pp         
finn  a     kushmerick  n          learning to classify documents according to genre 
journal of the american society for information science and technology          
         
fung  g          the disputed federalist papers  svm feature selection via concave minimization  in proceedings of the      conference on diversity in computing  pp 
     
gao  j   fan  w   jiang  j     han  j          knowledge transfer via multiple model local
structure mapping  in proceeding of the   th acm sigkdd international conference
on knowledge discovery and data mining  kdd   pp         
garera  n     yarowsky  d          modeling latent biographic attributes in conversational
genres  in proceedings of the joint conference of the   th annual meeting of the
acl and the  th international joint conference on natural language processing of
the afnlp  acl ijcnlp   pp         
gilad bachrach  r   navot  a     tishby  n          margin based feature selection  theory
and algorithms  in proceedings of the   st international conference on machine
learning  icml   pp       
gondek  d     hofmann  t          non redundant data clustering  in proceedings of the
 th ieee international conference on data mining  icdm   pp       
gries  s   wulff  s     davies  m          corpus linguistic applications  current studies 
new directions  rodopi 
   

fiinducing your ideal clustering with minimal feedback

grieve smith  a          the envelope of variation in multidimensional register and genre
analyses  language and computers               
hatzivassiloglou  v   gravano  l     maganti  a          an investigation of linguistic
features and clustering algorithms for topical document clustering  in proceedings of
the   rd annual international acm sigir conference on research and development
in information retrieval  sigir   pp         
he  x   cai  d   liu  h     ma  w  y          locality preserving indexing for document
representation  in proceedings of the   th annual international acm sigir conference on research and development in information retrieval  sigir   pp        
hu  j   deng  w   guo  j     xu  w          locality discriminating indexing for document
classification  in proceedings of the   th annual international acm sigir conference
on research and development in information retrieval  sigir   poster   pp     
    
hu  m     liu  b          mining opinion features in customer reviews  in proceedings of
the   th national conference on artificial intelligence  aaai   pp         
jain  p   meka  r     dhillon  i  s          simultaneous unsupervised learning of disparate
clusterings  in proceedings of siam international conference on data mining  sdm  
pp         
jiang  j     zhai  c       a   instance weighting for domain adaptation in nlp  in proceedings of the   th annual meeting of the association for computational linguistics
 acl   pp         
jiang  j     zhai  c       b   a two stage approach to domain adaptation for statistical
classifiers  in proceedings of the   th conference on information and knowledge
management  cikm   pp         
joachims  t       a   making large scale svm learning practical  in scholkopf  b    
smola  a   eds    advances in kernel methods   support vector learning  pp       
mit press 
joachims  t       b   transductive inference for text classification using support vector
machines  in proceedings of the   th international conference on machine learning
 icml   pp         
jurafsky  d   ranganath  r     mcfarland  d          extracting social meaning  identifying interactional style in spoken conversation  in proceedings of human language
technologies  the      annual conference of the north american chapter of the
association for computational linguistics  naacl hlt   pp         
kamvar  s   klein  d     manning  c          spectral learning  in proceedings of the   th
international joint conference on artificial intelligence  ijcai   pp         
kannan  r   vempala  s     vetta  a          on clusterings  good  bad and spectral 
journal of the acm                 
kennedy  a     inkpen  d          sentiment classifiation of movie reviews using contextual
valence shifters  computational intelligence                 
   

fidasgupta   ng

koppel  m   schler  j     argamon  s          computational methods in authorship attribution  journal of the american society for information science and technology 
            
kugler  m   aoki  k   kuroyanagi  s   iwata  a     nugroho  a          feature subset
selection for support vector machines using confident margin  in proceedings of the
     ieee international joint conference on neural networks  ijcnn   pp         
kulis  b   basu  s   dhillon  i     mooney  r          semi supervised graph based clustering  a kernel approach  machine learning              
li  t   zhang  y     sindhwani  v          a non negative matrix tri factorization approach
to sentiment classification with lexical prior knowledge  in proceedings of the joint
conference of the   th annual meeting of the acl and the  th international joint
conference on natural language processing of the afnlp  acl ijcnlp   pp     
    
lim  c   lee  k     kim  g          multiple sets of features for automatic genre classification of web documents  information processing and management                   
ling  x   dai  w   xue  g   yang  q     yu  y          spectral domain transfer learning 
in proceeding of the   th acm sigkdd international conference on knowledge
discovery and data mining  kdd   pp         
liu  b   li  x   lee  w  s     yu  p  s          text classification by labeling words  in
proceedings of the   th national conference on artificial intelligence  aaai   pp 
       
mccallum  a  k     nigam  k          employing em and pool based active learning for
text classification  in proceedings of the   th international conference on machine
learning  icml   pp          madison  wi  morgan kaufmann 
mcdonald  r   hannan  k   neylon  t   wells  m     reynar  j          structured models
for fine to coarse sentiment analysis  in proceedings of the   th annual meeting of
the association of computational linguistics  acl   pp         
mei  q   ling  x   wondra  m   su  h     zhai  c          sentiment mixture  modeling
facets and opinions in weblogs  in proceedings of the   th world wide web conference
 www   pp         
mohammad  s   dunne  c     dorr  b          generating high coverage semantic orientation lexicons from overtly marked words and a thesaurus  in proceedings of the
     conference on empirical methods in natural language processing  emnlp  
pp         
ng  a   jordan  m     weiss  y          on spectral clustering  analysis and an algorithm 
in advances in neural information processing systems     nips  
nigam  k   mccallum  a   thrun  s     mitchell  t          text classification from labeled
and unlabeled documents using em  machine learning                   
pang  b     lee  l          opinion mining and sentiment analysis  foundations and trends
in information retrieval               
   

fiinducing your ideal clustering with minimal feedback

pang  b   lee  l     vaithyanathan  s          thumbs up  sentiment classification using
machine learning techniques  in proceedings of the      conference on empirical
methods in natural language processing  emnlp   pp        association for computational linguistics 
polanyi  l     zaenen  a          contextual valence shifters  in computing attitude and
affect in text  theory and applications  springer verlag 
raghavan  h     allan  j          an interactive algorithm for asking and incorporating
feature feedback into support vector machines  in proceedings of the   th annual
international acm sigir conference on research and development in information
retrieval  sigir   pp       
rao  d     ravichandran  d          semi supervised polarity lexicon induction  in proceedings of the   th conference of the european chapter of the association for computational linguistics  eacl   pp         
riloff  e     wiebe  j          learning extraction patterns for subjective expressions 
in proceedings of the      conference on empirical methods in natural language
processing  emnlp   pp         
roth  d     small  k          interactive feature space construction using semantic information  in proceedings of the   th conference on computational natural language
learning  conll   pp       
sandler  m          on the use of linear programming for unsupervised text classification 
in proceedings of the   th acm sigkdd international conference on knowledge
discovery and data mining  kdd   pp         
sebastiani  f          machine learning in automated text categorization  acm computing
surveys              
shi  j     malik  j          normalized cuts and image segmentation  ieee transactions
on pattern analysis and machine intelligence                 
sindhwani  v     melville  p          document word co regularization for semi supervised
sentiment analysis  in proceedings of the  th ieee international conference on data
mining  icdm   pp           
stein  s   argamon  s     frieder  o          the effect of ocr errors on stylistic text
classification  in proceedings of the   th annual international acm sigir conference
on research and development in information retrieval  sigir   poster   pp     
    
tambouratzis  g     vassiliou  m          employing thematic variables for enhancing classification accuracy within author discrimination experiments  literary and linguistic
computing                 
tan  s   cheng  x   wang  y     xu  h          adapting naive bayes to domain adaptation
for sentiment analysis  in proceedings of the   st european conference on information
retrieval  ecir   pp         
turney  p          thumbs up or thumbs down  semantic orientation applied to unsupervised classification of reviews  in proceedings of the   th annual meeting of the
association for computational linguistics  acl   pp         
   

fidasgupta   ng

wagstaff  k   cardie  c   rogers  s     schrodl  s          constrained k means clustering
with background knowledge  in proceedings of the   th international conference on
machine learning  icml   pp         
wan  x          using bilingual knowledge and ensemble techniques for unsupervised chinese sentiment analysis  in proceedings of the      conference on empirical methods
in natural language processing  emnlp   pp         
weiss  y          segmentation using eigenvectors  a unifying view  in proceedings of the
international conference on computer vision  iccv   pp         
wiebe  j  m   wilson  t   bruce  r   bell  m     martin  m          learning subjective
language  computational linguistics                 
wilson  t   wiebe  j  m     hoffmann  p          recognizing contextual polarity in
phrase level sentiment analysis  in proceedings of the joint human language technology conference and the      conference on empirical methods in natural language
processing  hlt emnlp   pp         
wu  z     leahy  r  m          an optimal graph theoretic appproach to data clustering
and its application to image segmentation  ieee transactions on pattern analysis
and machine intelligence                    
xing  e  p   ng  a  y   jordan  m  i     russell  s  j          distance metric learning with
application to clustering with side information  in advances in neural information
processing systems     nips   pp         
xu  w   liu  x     gong  y          document clustering based on non negative matrix
factorization  in proceedings of the   th annual international acm sigir conference
on research and development in information retrieval  sigir   pp         
yang  y     liu  x          a re examination of text categorization methods  in proceedings of the   nd annual international acm sigir conference on research and
development in information retrieval  sigir   pp       
yang  y     pedersen  j  o          a comparative study on feature selection in text categorization  in proceedings of the   th international conference on machine learning
 icml   pp         

   

fi