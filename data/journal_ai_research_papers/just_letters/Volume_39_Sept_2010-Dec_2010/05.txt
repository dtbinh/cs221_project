journal of artificial intelligence research                 

submitted        published      

intrusion detection using continuous time bayesian networks
jing xu
christian r  shelton

jingxu   cs   ucr   edu
cshelton   cs   ucr   edu

department of computer science and engineering
university of california  riverside
riverside  ca        usa

abstract
intrusion detection systems  idss  fall into two high level categories  network based systems
 nids  that monitor network behaviors  and host based systems  hids  that monitor system calls 
in this work  we present a general technique for both systems  we use anomaly detection  which
identifies patterns not conforming to a historic norm  in both types of systems  the rates of change
vary dramatically over time  due to burstiness  and over components  due to service difference  
to efficiently model such systems  we use continuous time bayesian networks  ctbns  and avoid
specifying a fixed update interval common to discrete time models  we build generative models
from the normal training data  and abnormal behaviors are flagged based on their likelihood under
this norm  for nids  we construct a hierarchical ctbn model for the network packet traces
and use rao blackwellized particle filtering to learn the parameters  we illustrate the power of
our method through experiments on detecting real worms and identifying hosts on two publicly
available network traces  the mawi dataset and the lbnl dataset  for hids  we develop a novel
learning method to deal with the finite resolution of system log file time stamps  without losing the
benefits of our continuous time model  we demonstrate the method by detecting intrusions in the
darpa      bsm dataset 

   introduction
misuse or abuse of computer systems is a critical issue for system administrators  our goal is to
detect these attacks that attempt to compromise the performance quality of a particular host machine 
it is time consuming and error prone to acquire labeled data that contains both good and bad
behaviors from which to build a classifier  additionally  the frequency with which attacks are developed can make maintaining a database of all previously seen attacks inefficient or even infeasible 
anomaly detection can identify new attacks even if the attack type was unknown beforehand  unsupervised learning allows the anomaly detector to adapt to changing environments  thereby extending
its domain of usefulness  by modeling normal behavior from historic clean data  we can identify
abnormal activity without a direct prior model of the attack by simply comparing its deviation from
the learned norm 
in a network based intrusion detection system  nids   the network packet traces are monitored 
network traffic traces collect information from a networks data stream and provide an external
view of the network behavior  in a host based intrusion detection system  hids   the internal state
of a computing system is analyzed  system call logs are a convenient way of monitoring executing
programs behavior through their operating system calls 
both systems are composed of activities that happen at dramatically different time granularity 
users alternate between busily using their computer and resting  during the busy period  a burst of
action may cause a peak of network traffic flow or operating system usage  however  during the
c
    
ai access foundation  all rights reserved 

fix u   s helton

resting period  the computer just maintains its regular running pattern  and network or system activities are much less intense  e g  automatically checking email every few minutes  even within each
of these global modes there are variations  therefore  a dynamic model that requires discretizing
the time is not efficient  we develop intrusion detection techniques using continuous time bayesian
networks  ctbns   nodelman  shelton    koller        for both data types  although the two
data are of completely different formats and semantic meaning  we demonstrate the flexibility of a
continuous time generative model  such as a ctbn  to describe either 
our first effort is to detect anomalies from network traffic traces  nids   abnormal traffic must
differ in some way from the normal traffic patterns  while this difference may be very subtle and
difficult to detect  the more subtle the attack  the longer the attack will take and the more it will
stress the patience of the attacker  looking at summarized information like flow statistics is not
helpful  especially for stealthy worms which can mingle well with normal traffic by sacrificing their
spreading speed and scale  we  therefore  feel that looking for abnormalities in the detailed network
traffic flow level is a utile method for finding attacks  a network flow for a given host machine
is a sequence of continuous time asynchronous events  furthermore  these events form a complex
structured system  where statistical dependencies relate network activities like packet emissions and
connections  we employ ctbns to reason about these structured stochastic network processes 
our ctbn model contains a number of observed network events  packet emissions and concurrent port connections changes   to allow our model to be more descriptive  we also add latent
variables that tie the activity variables together  exact inference in this method is no longer feasible 
therefore  we use rao blackwellized particle filtering  rbpf  to estimate the parameters 
our second effort is to detect intrusions using system call logs  hids   a system log file contains an ordered list of calls made to a computers operating system by an executing program  we
focus on analyzing the ordering and the context of the sequence  rather than simply counting the
overall statistics  a ctbn is a natural way of modeling such sequential data  because of the finite
resolution of computer clock  all the system calls issued within a clock tick are assigned a same
time stamp  therefore the data stream consists of long periods of time with no activity  followed by
sequences of calls in which the order is correctly recorded  but the exact timing information is lost 
this poses a new challenge for ctbn reasoning  we present here a learning method for such type
of data without resorting to time discretization 
we validate our nids technique on the mawi dataset and the lbnl dataset  and our hids
technique on the darpa      bsm dataset  both applications give good results when compared
with other method 
in section   we discuss the related work in intrusion detection  in section   we review continuoustime markov processes and continuous time bayesian networks  in section   we describe the ctbn
model and the rbpf inference algorithms for the nids problem  in section   we describe the
ctbn model and the parameter estimation algorithm for hids  including how to deal with imprecise timing measurements  in section   we show our experimental results for both of the applications 

   related work
much of the previous work in intrusion detection focuses on one area only  either detecting the
network traffic or mining the system call logs  the work of eskin  arnold  prerau  portnoy  and
stolfo        is similar to our approach in that they apply their method to both of these kinds of
   

fii ntrusion d etection using ctbn s

data  they map data elements to a feature space and detect anomalies by determining which points
lie in sparse regions using cluster based estimation  k nearest neighbors and one class svm  they
use a data dependent normalization feature map for network traffic data and a spectrum kernel for
system call traces 
    nids
for network traffic data  we build upon our previous work  xu   shelton         there we made
the assumption that network activities are independent across different ports  this allowed us to
factorize the model into port level submodels and standard exact inference techniques could be used
for parameter learning  in this paper  we remove this restriction  there is no application specific
reason that traffic should be independent by ports  by tying the traffic together  our model describes
more complicated structural dependencies among variables  we derive a rao blackwellized particle
filtering algorithm to estimate the parameters for our model  our work also differs in that we are
not only interested in the intrusion detection problem  but host identity recognition as well 
as a signature based detection algorithm  we share many of the assumptions of karagiannis 
papagiannaki  and faloutsos         in particular  we also assume that we do not have access to
the internals of the machines on the networks  which rules out methods like those of malan and
smith         cha         qin and lee         and eskin et al          however  we differ in
that our approach does not rely on preset values  require human intervention and interpretation  nor
assume that we have access to network wide traffic information  network wide data and human
intervention have advantages  but they can also lead to difficulties  data collation in the face of an
attack and increased human effort   so we chose to leave them out of our solution 
many learning  or adaptive  methods have been proposed for network data  some of these 
for example  those of zuev and moore        and soule  salamatian  taft  emilion  and papagiannali         approach the problem as a classification task which requires labeled data  dewaele 
fukuda  and borgnat        profile the statistical characteristics of anomalies by using random projection techniques  sketches  to reduce the data dimensionality and a multi resolution non gaussian
marginal distribution to extract anomalies at different aggregation levels  the goal of such papers
is usually not to detect attacks but rather to classify non attacks by traffic type  if applied to attack
detection  they would risk missing new types of attacks  furthermore  they frequently treat each
network activity separately  instead of considering their temporal context 
lakhina  crovella  and diot        has a nice summary of adaptive  or statistical  methods that
look at anomaly detection  instead of classification   they use an entropy based method for the
entire network traffic  many of the other methods  such as that of ye  emran  chen  and vilbert
        use either statistical tests or subspace methods that assume the features of the connections
or packets are distributed normally  rieck and laskov        model the language features like
n grams and words from connection payloads  xu  zhang  and bhattacharyya        also use
unsupervised methods  but they concentrate on clustering traffic across a whole network  similarly 
soule  salamatian  and taft        build an anomaly detector based on markov models  but it is for
the network traffic patterns as a whole and does not function at the host level 
the work of soule et al         is very similar in statistical flavor to our work  they also
fit a distribution  in their case  a histogram modeled as a dirichlet distribution  to network data 
however  they model flow level statistics  whereas we work at the level of individual connections 
additionally  they are attempting network wide clustering of flows instead of anomaly detection 
   

fix u   s helton

the work of moore and zuev         like our approach  models traffic with graphical models  in
particular  naive bayes networks  but their goal is to categorize network traffic instead of detecting
attacks  kruegel  mutz  robertson  and valeur        present a bayesian approach to the detecting
problem as an event classification task while we only care about whether the host is under attack
during an interval 
the work of lazarevic  ertoz  kumar  ozgur  and srivastava        is also similar to our work 
it is one of the few papers to attempt to find attacks at the host level  they employ nearest neighbor 
a mahalanobis distance approach  and a density based local outliers method  each using    features
of the connections  although their methods make the standard i i d  assumption about the data
 and therefore miss the temporal context of the connection  and use    features  compared to our
few features   we compare our results to theirs in section    as the closest prior work  agosta 
duik wasser  chandrashekar  and livadas        present an adaptive detector whose threshold is
time varying  it is similar to our work in that they also rely on model based algorithms  but they
employ the host internal states like cpu loads which are not available to us 
while there has been a great variety of previous work  our work is novel in that it detects
anomalies at the host level using only the timing features of network activities  we do not consider
each connection  or packet  in isolation  but rather in a complex context  we capture the statistical
dynamic dependencies between packets and connections to find sequences of network traffic that
are anomalous as a group 
    hids
previous work on detecting intrusions in system call logs can be roughly grouped into two categories  sequence based and feature based  sequence based methods focus on the sequential order
of the events while feature based methods treat system calls as independent data elements  our
method belongs to the former category since we use a ctbn to model the dynamics of the sequences 
time delay embedding  tide  and sequence time delay embedding  stide  are two examples of
sequence based methods  forrest  a hofmeyr  somayaji    a longstaff        a hofmeyr  forrest    somayaji         they generalize the data by building a database storing previously seen
system call sub sequences  and test by looking up subsequences in the database  these methods are
straightforward and often achieve good results  we compare with them in our experiments  tandon
and chan        look at a richer set of attributes like return value and arguments associated with a
system call while we only make use of the system call names 
feature based methods like those of hu  liao  and vemuri        use the same dataset we use 
the darpa      bsm dataset  but their training data is noisy and they try to find a classification
hyperplane using robust support vector machines  rsvms  to separate normal system call profiles
from intrusive ones  eskin        also works on noisy data  they make the assumption that their
training data contains a large portion of normal elements and few anomalies  they present a mixture
of distribution over normal and abnormal data and calculate the likelihood change if a data point is
moved from normal part to abnormal part to get the optimum data partition 
yeung and ding        try to use both techniques  they provide both dynamic and static behavioral models for system call data  for the dynamic method  a hidden markov model  hmm 
is used to model the normal system events and a likelihood is calculated for each testing sequence
and compared against a certain threshold  our work for the system call traces problem is very close
   

fii ntrusion d etection using ctbn s

to their framework since we also build a dynamic model for the sequential data and compute the
likelihood of a testing example as a score  but we are different in that our ctbn models the continuous time dynamics rather than time sliced behaviors  for the static method  they represent the
normal behavior by a command occurrence frequency distribution and measure the distance from
the testing example to this norm by cross entropy  the dataset they use is kdd archive dataset 
    other work
simma et al         also use a continuous time model to reason about network traffic  they apply
their method to find dependences in exterprise level services  their model is non markovian  but
also deals with network events as the basic observational unit 
to estimate the parameters of the large network we build for the network traffic data  we use
rao blackwellized particle filters  rbpfs   doucet  de freitas  murphy  and russel        propose
a rbpf algorithm for dynamic bayesian networks that works in discrete time fashion by exploiting
the structure of the dbn  ng  pfeffer  and dearden        extend the rbpf to continuous time dynamic systems and apply the method to the k   experimental mars rover at nasa ames research
center  their model is a hybrid system containing both discrete and continuous variables  they
use particle filters for the discrete variables and unscented filters for the continuous variables  our
work are similar to theirs in that we apply a rbpf to a ctbn  but our model only contains discrete
variables and our evidence is over continuous time  as opposed to only snapshots of the system
state  

   continuous time bayesian networks
we begin by briefly reviewing the definition of markov processes and continuous time bayesian
networks  ctbns  
    homogeneous markov process
a finite state  continuous time  homogeneous markov process xt is described by an initial distribution px  and  given a state space v al x     x         xn    an n  n matrix of transition intensities 



qx   


qx 
q x  x 
  
 

q x  x 
qx 
  
 

q xn x 

q xn x 

      q x  xn
      q x  xn
  
  
 
 
      qxn




 


p
qxi xj is the intensity  or rate  of transition from state xi to state xj and qxi   j  i qxi xj  
the transient behavior of xt can be described as follows  variable x stays in state x for time
exponentially distributed with parameter qx   the probability density function f for xt remaining
at x for duration t is fx  q  t    qx exp qx t  for t     the expected time to the next transition
given the state is currently x is   qx   upon transitioning  x shifts to state x  with probability
xx    qxx   qx   note that given qx   xx  and qxx  are iosmorphic  we will sometime gives formulae
in terms of xx  where it simplifies the expression 
the distribution over the state of the process x at some future time t  px  t   can be computed
directly from qx   if px  is the distribution over x at time    represented as a vector   then  letting
   

fix u   s helton

exp be the matrix exponential 
px  t    px  exp qx  t   
    complete data
complete data for an hmp are represented by a set of trajectories d           n    each trajectory
i is a complete set of state transitions  d     xd   td   x d     meaning that x stayed in state xd for a
duration of td   and then transitioned to state x d   therefore we know the exact state of the variable
x at any time    t  t  
    sufficient statistics and likelihood
given an hmp and its full data d  the likelihood of a single state transition d     xd   td   x d     d
is
lx  q     d     qxd exp qxd td    xd x d    
the likelihood function for d can be decomposed by transition 
y
y
lx  q     d     
lx  q   d   
lx     d  
dd

dd

y
y y m  x x   
    qxm  x  exp qx t  x    
xx 
  
x x    x

x

if we take the log of the above function  we get the log likelihood 
lx  q     d    lx  q   d    lx     d 
x
x
 
 m  x  ln qx    qx t  x   
m  x  x    ln xx      
x    x

x

m  x  x   

here
and t  x  are the sufficient statistics of the hmp
model  m  x  x    is the number
p
of times x transitions from the state x to x    we denote m  x    x  m  x  x     the total number of
times the system leaves state x  t  x  is the total duration that x stays in the state x 
    learning from complete data
to estimate the parameters of the transition intensity matrix q  we maximize the above log likelihood function  this yields the maximum likelihood estimates 
qx  

m  x 
 
t  x 

xx   

m  x  x   
 
m  x 

    incomplete data
incomplete data from an hmp are composed partially observed trajectories d           n    each
trajectory i consists of a set of d     sd   td   dt   observations  where sd is a subsystem  a
nonempty subset of the states of x  of the process  each of the triplets specifies an interval
evidence  it states that the variable x is in the subsystem sd from time td to time td   dt  some of
the observations may be duration free  i e   we only observe x  sd at time t  but do not know how
long it stayed there  this is called a point evidence and can be generalized using the same triplet
notation described above by setting the duration to be    for a partially observed trajectory  we only
observe sequences of subsystems  and do not observe the state transitions within the subsystems 
   

fii ntrusion d etection using ctbn s

    expected sufficient statistics and expected likelihood
we can consider possible completions of a partially observed trajectory that specify the transitions
that are consistent with the partial trajectory  by combining the partial trajectory and its completion 
we get a full trajectory  we define d               n    to be completions of all the partial trajectories
in d  given a model  we have a distriubtion over d    given d 
for data d    the expected sufficient statistics with respect to the probability density over possible completions of the data are t  x   m  x  x    and m  x   the expected log likelihood is
e lx  q     d       e lx  q   d       e lx     d    
x
x
 m  x  ln qx    qx t  x   
 
m  x  x    ln xx      
x    x

x

    learning from incomplete data
the expectation maximization  em  algorithm can be used to find a local maximum of the likelihood
from partial trajectory  the em algorithm iterates over the following e step and m step until the
convergence on the derived likelihood function 
e step  given the current hmp parameters  compute the expected sufficient statistics  t  x  
m  x  x    and m  x  for the data set d  this is the most complex part of the algorithm  we give
further details below 
m step  from the computed expected sufficient statistics  update the new model parameters for
the next em iteration 
m  x  x   
m  x 
  xx   
 
qx  
t  x 
m  x 
now we show how to calculate the expected sufficient statistics using the forward backward
message passing method 
a trajectory   d can be devided into n intervals where each of the interval is separated
by adjacent event changes  assume the trajectory spans the time interval     t    and let   v  w  be
the observed evidence between time v and w  including events on the time stamp v and w  and let
  v  w  be the same set of evidence but excluding v and w  let s be the subsystem the states are
restricted on this interval 
we define
t   p  xt        t    t   p    t  t     xt  
to be vectors  indexed by possible assignments to xt    similarly  we define the corresponding
distribution that excludes certain point evidence as follows 
t   p  xt        t   

t    p    t  t     xt    

denote j to be a vector of all  s except for its j th position being    and denote ij be a matrix
of all  s except that the element on i th row and j th column is   
we are now able to show the derived expected sufficient statistics  for time 
z t
e t  x    
p  xt        t   x dt
 
n
  z ti  
x
 
 
p  xt        t   x dt  
p       t   
ti
i  

   

fix u   s helton

the constant fraction at the beginning of the last line serves to make the total expected time over all
j sum to    the integral on each interval can be further expressed as
z

w

z

w

v exp qs  t  v  xx exp qs  w  t  w dt  

p  xt        t   x dt  
v

v

where qs is the same as qx except all elements that correspond to transitions to or from s are set
to   
the equation for expected transition counts can similarly be defined 
n  

e m  x  x      

x
qx x 
 
 
ti x x  t i
p       t   
i  
n
  z ti  
x
 
ti exp qs  t  ti   x x  exp qs  ti    t  ti   dt   
i  

ti

the integrals appearing in e t   and e m   can be computed via a standard ode solver  like
the runge kutta method  press  teukolsky  vetterling    flannery         such a method uses an
adaptive step size to move quickly through times of few expected changes and more slowly through
times of rapid transitions 
now the only remaining problem is to calculate  and   let qss  be the transitioning intensity
matrix of the hmp from one subsystem s to another s     this matrix is the same as qx   but only
elements corresponding to transitions from s to s   are non zero 
ti   ti  exp qsi   ti  ti      
ti   ti qsi  si  
ti   exp qsi  ti    ti   ti    
ti   qsi  si ti  
during this forward backward calculation  it is also trivial to answer queries such as
p  xt   x        t     

 
 xx t  
p     t

    continuous time bayesian networks
while hmps are good for modeling many dynamic systems  they have their limitations when the
systems have multiple components because the state space grows exponentially in the number of
variables  an hmp does not model the variable independencies and therefore it has to use a unified
state x to represent the joint behavior of all the involving components in the system  in the this
section  we show how a continuous time bayesian network can be used to address this issue 
nodelman et al         extend the theory of hmps and present continuous time bayesian networks  ctbns   which model the joint dynamics of several local variables by allowing the transition
model of each local variable x to be a markov process whose parametrization depends on some
subset of other variables u  
   

fii ntrusion d etection using ctbn s

    definition
we first give an definition of an inhomogeneous markov process called a conditional markov process  it is a critical concept for us to formally introduce the ctbn framework 
definition    nodelman  shelton    koller        a conditional markov process x is an inhomogeneous markov process whose intensity matrix varies as a function of the current values of a set
of discrete conditioning variables u   it is parametrized using a conditional intensity matrix  cim 
qx u  a set of homogeneous intensity matrices qx u   one for each instantiation of values u to u  
we call u the parents of x  when the set of u is empty  the cim is simply a standard intensity
matrix 
cims provide a way to model the temporal behavior of one variable conditioned on some other
variables  by putting these local models together  we have a joint structured model  a continuous
time bayesian network 
definition    nodelman et al         a continuous time bayesian network n over a set of stochastic processes x consists of two components  an initial distribution px    specified as a bayesian
network b over a set of random variables x  and a continuous transition model  specified using a
directed  possibly cyclic  graph g whose nodes are x  x  ux denotes the parents of x in g  each
variable x  x is associated with a conditional intensity matrix  qx ux  
the dynamics of a ctbn are quantitatively defined by a graph  the instantaneous evolution of
a variable depends only on the current value of its parents in the graph  the quantitative description
of a variables dynamics is given by a set of intensity matrices  one for each value of its parents 
that means the transition behavior of the variable is controlled by the current values of its parents 
the standard notion of d separation from bayesian networks carries over to ctbns  because
graphs are cyclic and variables represent processes  not single random variables   the implications
are a little different  a variable  process  is still independent of its non descendants given its parents 
and it is still independent of everything given its markov blanket  any variable that is either a parent 
a child  or a parent of a child   cycles can cause parents to also be children  but provided they are
considered as both  the above definitions still hold  more importantly  the notion of given works
only if the full trajectory for the variable in question is known  therefore  x and its grandchildren
are not independent given xs childrens values at a single instant  rather  they are only independent
given xs childrens full trajectories from time   until the last time of interest 
if we amalgamate all the variables in the ctbn together  we get a single homogeneous markov
process over the joint state space  in the joint state intensity matrix  a rate of   is assigned to any
transition that involves changing more than one variables value at the exact same time  all other
intensities can be found by looking up the value in the corresponding conditional intensity matrix
for the variable that changes  the diagonal elements are the negative row sums 
forward sampling can be done quickly in a ctbn without generating the full joint intensity
matrix  we keep track of the next event time for each variable  sampled from the relevant exponential distribution given the current values of itself and its parent   we then select the earliest
event time and change that variable  sampling from the multinomial distribution implied by the row
of that variables relevant intensity matrix   the next event time for the variable that just changed
and all of its children must be resampled  but no other variables time must be resampled due to the
memoriless property of the exponential distribution  in this way a sequence of events  a trajectory 
can be sampled 
   

fix u   s helton

     learning
in the context of ctbns  the model parameters consist of the ctbn structure g  the initial distribution p  parameterized by a regular bayesian network  and the conditional intensity matrices  cims 
of each variable in the network  in this section  we assume the ctbn structure is known to us  so
we only focus on the parameter learning  we also assume the model is irreducible  so the initial
distribution p  becomes less important in the context of ctbn inference and learning  especially
when the time range becomes significantly large  therefore  parameter learning in our context is
to estimate the conditional intensity matrices qxi  ui for each variable xi   where ui is the set of
parent variables of xi  
       l earning from c omplete data
nodelman et al         presented an efficient way to learn a ctbn model from fully observed
trajectories  with complete data  we know full instantiations to all the variables for the whole
trajectory  so we know which cim is governing the transition dynamics of each variable at any
time  the sufficient statistics are m  x  x   u   the number of times x transitions from the state x
to x  given its parent instantiation u  and t  x u  p
the total duration that x stays in the state x
given its parent instantiation u  we denote m  x u    x  m  x  x   u  
the likelihood function for d can be decomposed as
y
ln  q     d   
lxi  qxi  ui   d lxi  xi  ui   d  
   
xi x

where
lx  qx u   d   

yy
u

m  x u 

qx u

exp qx u t  x u  

   

x

and
lx     d   

yy y
u

m
 
xx
   u  x  x  u   

   

x x    x

if we put the above functions together and take the log  we get the log likelihood component for
a single variable x 
lx  q     d    lx  q   d    lx     d 
xx
 
m  x u  ln qx  u   q x u  t  x u 
u

 

x

xx x
u

m  x  x   u  ln xx   u    

   

x x    x

by maximizing the above log likelihood function  the model parameters can be estimated as
qx u  

m  x u 
 
t  x u 

xx   u  

   

m  x  x   u 
 
m  x u 

   

fii ntrusion d etection using ctbn s

       l earning from i ncomplete data
nodelman  shelton  and koller        present the expectation maximization  em  algorithm to
learn a ctbn model from partially observed trajectories d  the expected sufficient statistics are
m  x  x   u   the expected number of times that x transitions from state x to x  when its parent set
u takes the values u  and t  x u   the expected p
amount of time that x stays in the state x under the
parent instantiation u  we denote m  x u  to be x  m  x  x   u   the expected log likelihood can be
decomposed in the same way as in equation    except that the sufficient statistics m  x  x   u   t  x u 
and m  x u  are now replaced with expected sufficient statistics m  x  x   u   t  x u  and m  x u  
the em algorithm for a ctbn works essentially in the same way as for an hmp  the expectation step is to calculate the expected sufficient statistics using inference method  will be described
in section        the maximization step is to update the model parameters 

qx u  

m  x u 
 
t  x u 

xx   u  

m  x  x   u 
 
m  x u 

     inference
now given a ctbn model and some  partially  observed data  we would like to query the model 
for example  we may wish to calculate the expected sufficient statistics for the above em algorithm 
       e xact i nference
nodelman et al         provide an exact inference algorithm using expectation maximization to
reason and learn the parameters from partially observed data  this exact inference algorithm requires flattening all the variables into a single markov process and performing inference as in an
hmp  it has the problem that it makes the state space grow exponentially large  therefore  the exact
inference method is only feasible for problems with very small state spaces 
       a pproximate i nference
because of the issue addressed below  much work has been done on ctbn approximate inference  nodelman  koller  and shelton        present an expectation propagation algorithm  saria 
nodelman  and koller        give another message passing algorithm that adapts the time granularity  cohn  el hay  friedman  and kupferman        provide a mean field variational approach 
el hay  friedman  and kupferman        show a gibbs sampling method approach using monte
carlo expectation maximization  fan and shelton        give another sampling based approach
that uses importance sampling  el hay  cohn  friedman  and kupferman        describe a different expectation propagation approach 
to estimate the parameters of the models we build for the two applications  nids and hids  
we employ inference algorithms including exact inference and a rao blackwellized particle filtering
 rbpf  algorithm  depending on the model size  ng et al         extended rbpf to ctbns  their
model was a hybrid system containing both discrete and continuous variable  they used particle
filters for the discrete variables and unscented filters for the continuous variable  our work are
similar to this work in the method of applying rbpf to ctbns  but our model contains only discrete
variables and our evidence is over continuous intervals 
   

fix u   s helton

port
  
    
   
   
    
   
     
     

description
world wide web http
http alternate
http protocol over tls ssl
authentication service
talarian tcp
pop  protocol over tls ssl
unknown
unknown

port
  
   
   
   
    
    
    
   

description
world wide wed http
netbios session service
http protocol over tls ssl
microsoft ds
msnp
gadget gate   way
at c license manager
post office protocol   version  

figure    ranking of the most frequent ports on mawi dataset  left  and lbnl dataset  right  

     ctbn applications
although inference and learning algorithms have been well developed for ctbns  there have been
only a few applications to real world problems  nodelman and horvitz        used ctbns to
reason about users presence and availability over time  ng et al         used ctbns to monitor
a mobile robot  nodelman et al         used ctbns to model life event history  fan and shelton
       modeled social networks via ctbns  our previous work  xu   shelton        presented
an nids for host machine using ctbns  but did not include hids 

   anomaly detection using network traffic
in this section  we present an algorithm to detect anomalies in network traffic data using ctbns 
we only focus on a single host on the network  the sequence and timing of events  e g  packet
transimission and connection establishment  are very important in network traffic flow  it matters
not just how many connections were initiated in the past minute  but also their timing  if they were
evenly spaced the trace is probably normal  but if they all came in a quick burst it is more suspicious 
similarly  the sequence is important  if the connections were made to sequentially increasing ports
it is more likely to be a scanning virus  whereas the same set of ports in random order is more likely
to be normal traffic  these are merely simple examples  we would like to detect more complex
patterns 
a typical machine in the network may have diverse activities with various service types  e g 
http  smtp   the destination port number roughly describes the type of service to which a particular network activity belongs  some worms propagate malicious traffic toward certain well known
ports to affect the quality of the associated services  by looking at traffic associated with different
ports we are more sensitive to subtle variations that do not appear if we aggregate trace information
across ports  figure   shows the most popular ports ranked by their frequencies in the network
traffic on the datasets we use  described in more depth later   these services are  to some extent 
independent of each other  we therefore model each ports traffic with its own ctbn submodel 
we denote  as the whole observed traffic sequences on the particular host  and j as the traffic
associated with port j 
   

fii ntrusion d etection using ctbn s

g

n
h

pin

pout

cinc

cdec

figure    ctbn model for network traffic as a plate model  n is the number of port  

    a ctbn model for network traffic
we use the same port level submodel as our previous work  xu   shelton         we have a latent
variable h and four fully observed toggle variables  pin   pout   cinc   cdec  
the nodes packet in  pin   and packet out  pout   represent the transmission of a packet to or from
the host  they have no intrinsic state  the transmission of a packet is an essentially instantaneous
event  therefore they have events  or transitions  without having state  this is modeled using a
toggle variable in which an event is evidence of a change in the state of the variable and the rate of
transition associated with each state is required to be the same 
the nodes connection increase cin and connection decrease cdec together describe the status
of the number of concurrent connections c active on the host  notice that c can only increase or
decrease by one at any given event  the beginning or ending time of a connection   we assume that
the arrival of a new connection and the termination of an existing connection are both independent
of the number of other connections  thus the intensity with which some connection starts  or stops 
is same as any other connections  therefore  these are also modeled as toggle variables 
node h has   states that represent different abstract attributes about the machines internal state 
the toggle variables  pin   pout   cinc and cdec   are each allowed to change only for   of the states
of h and they are required to have the same rate for both of these states    hidden states per toggle
variable was chosen as a balance between expressive power and model efficiency 
in previous work  we assumed that the traffic associated with different ports are independent of
each other  so the port level submodels are isolated  here we remove this restriction by introducing
another latent variable g that ties the port submodels together  the full model is shown in figure   
    parameter learning using rbpf
to calculate the expected sufficient statistics in the e step of em for parameter learning  the exact
inference algorithm of nodelman et al         flattens all the variables into a joint intensity matrix
and reasons about the resulting homogeneous markov process  the time complexity is exponential
in the number of variables  for example  if there are   port models  the network contains    variables
in total  approximate inference techniques like the clique tree algorithm  nodelman et al         
message passing algorithms  nodelman et al         saria et al          importance sampling  fan
   

fix u   s helton

  shelton        and gibbs sampling  el hay et al         overcome this problem by sacrificing
accuracy 
we notice that our model has a nice tree structure which makes rao blackwellized particle
filtering  rbpf  a perfect fit  rbpf uses a particle filter to sample a portion of the variables and
analytically integrates out the rest  it decomposes the model structure efficiently and thus reduces
the sampling space 
if we denote the n port level hidden variables as h         hn   the posterior
distribution of the
qn
whole model can be factorized as p  g  h         hn        p  g      i   p  hi   g      note
that g and hi are processes  so this probability is a density over complete trajectories  we use a
particle filter to estimate gs conditional distribution p  g      as a set of sampled trajectories of
g  it is difficult to sample directly from the posterior distribution  so we use an importance sampler
to sample a particle from a proposal distribution and the particles are weighted by the ratio of its
likelihood under the posterior distribution to the likelihood under the proposal distribution  doucet
et al          since the variable g is latent and has no parents  we can use forward sampling
to sample the particles from p  g  and the weight of each particle is simply the likelihood of 
conditioned on this trajectory for g  fan   shelton         each port level submodel is then dseparated from the rest of the network  given full trajectory of g  see section     for d separation
in ctbns   since each is small  only   hidden states   they can be marginalized out exactly  that
is  we can calculate p  i   g   where i is the portion of the trajectory for submodel i  exactly 
marginalizing out hi with the   recursions from section     
the expected sufficient statistics  ess  for any variable x in a ctbn are tx u  x u   the expected amount of time x stays at state x given its parent instantiation u  and mx u  x  x   u   the
expected number of transitions from state x to x  given xs parent instantiation u  let g i  p  g  
i  
i              m be the particles  we define their likelihood weights to be wi   pp g g 
i   and let
p
w   i wi be the sum of the weights  then general importance sampling allows that an expected
sufficient statistic can be estimated in the following way  where ss is any sufficient statistic 

e g h       hn  p  g h       hn      ss g  h            hn   
  egp  g    eh       hn p  h       hn  g     ss g  h            hn   
  x
wi eh       hn p  h       hn  gi      ss g i   h            hn     

w
i

the expected sufficient statistics of the whole model are in two categories  those that depend
only on g  ess g   and those that depend on a port model k  ess g  hk   k    ess g  is simply
the summation of counts  the amount of time g stays at some state  or the number of times g
transitions from one state to another  from the particles  weighted by the particle weights 

egp  g     ss g   

  x
wi ss g i    
w
i

   

   

fii ntrusion d etection using ctbn s

function wholemodel estep
input  current model t   evidence 
output  expected sufficient statistics ess
ess     ess g   ess s    g           ess sn   g  
initialize ess as empty
for each particle g i   g             g m    g i  p  g 
for each sj   s            sn  
 p  j  g i    ess sj   g i      submodel estep g i   t  sj    j  
for each sj   s            sn  
q
ess sj   g    ess sj   g    k  j p  k  g i    ess sj   g i  
i 
essgi   countgss gq
ess g    ess g    j p  j  g i    essgi
return ess

figure    rao blackwellized particle filtering estep for the whole model
ess g  hk   k   can be calculated for each submodel independently 
eg h       hn p  g h       hn      ss g  hk   k   
z
  x
wi

p  hk  g i   k  ss g i   hk   k   dhk
w
hk
i
q
i z
  x j p  j  g  
p  hk  g i   k  ss g i   hk   k   dhk
 
w
p    
hk
i
z
x
y
 

p  j  g i  
p  hk   k  g i  ss g i   hk   k   dhk  
w
hk
i

   

j  k

the integrals are over all possible trajectories for the hidden process hk   the first line holds by
d separation  we need only average over the submodel k  given an assignment to g   the second
line expands the weight  the last line combines the weight term for submodel k with the terms
in the integral to get the likelihood of hk and the submodel data  the constant of proportionality
p
will cancel in the subsequent maximization  or it can be reconstructed by noting that x tx u  x u 
should be the total time
r of the interval 
this last integral  hk p  hk   k  g i  ss g i   hk   k   dhk   and p  j  g i   can be calculated using the
technique described by nodelman et al          for exact ess calculation  the calculations are
very similar the integrals of section      except they intensity matrices can change from interval to
interval  they are a function of the sampled trajectory gi   
the full e step algorithm is shown in figure    sk represents all of the variables in submodel
k   function submodel estep calculates the expected sufficient statistics and the likelihood for a
subnet model  equation     function countgss counts the empirical time and transition statistics
from the sampled trajectory of g  equation    
in em  we use the ess as if they were the true sufficient statistics to maximize the likelihood
with respect to the parameters  for a regular ctbn variable x  such as our hidden variable g
and h   equation   performs the maximization  for our toggle variables  e g  pi   the likelihood
   

fix u   s helton

component for the toggle variable is
y

mp

qpi  ui exp qpi  u t  u   u  

u

which can be found by setting qx u to be the same value  qpi  u   for all x  tieing the parameters  and
simplifying the product over x in equation    thus the maximum likelihood parameter estimate is
qpi  u  

mpi
t  u   u 

where mpi is the number of events for variable pi and qpi  u is the only parameter  the rate of
switching 
we synchronize the particles at the end of each window  see section      and resample as
normal for a particle filter at those points  that is  we propagate the particles forward  but stop them
all at the end of the window  resample based on the weights  and then continue with the new set of
particles  in general  the particles are not aligned by time  except at these resampling points 
    online testing using likelihood
once the ctbn model has been fitted to historic data  we detect attacks by computing the likelihood
of a window of the data  see section      under the model  if the likelihood falls below a threshold 
we flag the window as anomalous  otherwise  we mark it as normal 
in our experiments  we fix the window to be of a fixed time length  tw   therefore  if the
window of interest starts at time t   we wish to calculate p   t  t   tw          t    where   s  t 
represents the observed connections and packets from time s to time t  again  we use a rbpf to
estimate this probability  the samples at time t represent the prior distribution p  g        t    
propagating them forward across the window of length tw produces a set of trajectories for g 
g i   each submodel k can evalute p  k  t  t   tw     g i   by exact marginalization  the sum of the
vector t  tw   the forward message   the weighted average  over samples g k   of the product of the
submodel probabilities is our estimate of p    t  t   tw          t    

   anomaly detection using system calls
now we turn to the problem of detecting anomalies using system call logs 
    a ctbn model for system calls
system call logs monitor the kernel activities of machines  they record detailed information of the
sequence of system calls to operating system  many malicious attacks on the host can be revealed
directly from the internal logs 
we analyze the audit log format of suns solaris basic security module  bsm  praudit audit
logs  each user level and kernel event record has at least three tokens  header  subject  and return 
an event begins with a header in the format of  header  record length in bytes  audit record version
number  event description  event description modifier  time and date  the subject line consists of 
subject  user audit id  effective user id  effective group id  real user id  real group id  process
id  session id  and terminal id consisting of a device and machine name  a return with a return
value indicating the success of the event closes the record 
   

fii ntrusion d etection using ctbn s

h

s 

s 

sn

figure    ctbn model for system call data

 

s   s        sk

ti   ti   t

ti t

ti

ti   ti   t

time

figure    system call traces with a finite resolution clock  resolution   t  
we construct a ctbn model similar to our port level network model  individual system calls
s         sn   which are the event description fields in the header token  are transiently observed  they
happen instantaneously with no duration  we treat them as toggle variables like packets in the
network model  we also introduce a hidden variable h as a parent of the system calls variables to
allow correlations among them  this hidden variable is designed to model the internal state of the
machine  although such a semantic meaning is not imposed by our method  put together  our system
call model looks like figure   
if the state space of the hidden variable h is of size m  the transition rate matrix of h is



qh   


qh 
q h  h 
  
 

q h  h 
qh 
  
 

q hm h 

q hm h 

      q h  hm
      q h  hm
  
  
 
 
      qhm




 


and the transition intensity rate of the toggle variable s  s given the current value of its parent h
is qs hi   i           m 
to estimate the ctbn model parameters  we again use the expectation maximization  em 
algorithm  the expected sufficient statistics we need to calculate for our model are
 mhi hj   the expected number of times h transitions from state i to j 
 thi   the expected amount of time h stays in state i  and
 ms hi   the expected number of times system call s is evoked when h is in state i 
   

fix u   s helton

the maximum likelihood parameters are
mhi hj
thi
ms hi
 
 
thi

q hi hj  
qs hi

    parameter estimation with finite resolution clocks
because of the finite resolution of computer clocks  multiple instantaneous events  system calls 
occur within a single clock tick  therefore in the audit logs  a batch of system calls may be recorded
as being executed at a same time point  rather than their real time stamp  as a result of this finite
time accuracy  however  the correct order of the events is kept in the logs  that is  we know exactly
that system call s  follows s  if they are recorded in this order in the audit logs  thus all the system
call timings are only partially observed  this type of partial observation has not previously been
considered in ctbn inference  a typical trajectory  over     t   of system call data is shown in
figure    a batch of system calls are evoked at some time after ti but before the next clock tick 
followed by a quiet period of arbitrary length  and yet another bunch of events at some time after
ti   and so on 
let t  t  denote the evidence over interval  t   t    t  t   denote the evidence over  t   t    and
t   t  denote the evidence over  t   t    we define the vectors
ti   p ht     ti  
i

t i   p t   t  ht   
i

i

where ht is the value of h just prior to the transition at ti   and ht  is value just afterward  we
i
i
also define the vectors
ti   p hti     t   
i

ti   p ti  t  hti  
where the evidence at the transition time ti is included  we follow the forward backward algorithm
to compute ti and ti for all ti at which there is an event  to do this  we split any interval  ti   ti    
into a spike period  ti   ti   t    t is one resolution clock   during which there is a batch of
system calls  and a quite period  ti   t   ti     over which no events exist  and do the propagations
separately 
for a spike period  ti   ti   t    if the observed event sequence is s    s         sk   we construct an
artificial markov process x with the following intensity matrix 




qx   



qh q   
   
 
  qh q       
 
  
  
  
  
  
 
 
 
 
 
 
 
      qh qk
 
 
   
  qh
   





 



fii ntrusion d etection using ctbn s

where



qh   


p
qh   ss qs h 
q h  h 
  
 

qh 

q hm h 

q h  h 
p
 ss qs h 
  
 

   
   
  
 
      qhm

q hm h 

q h  hm
q h  hm
  
 
p
 ss qs hm







and



qi   


qsi  h 
 
  
 
 

 
qsi  h 
  
 

   
   
  
 

 
 
  
 

 

 

qsi  hm







x tracks the evidence sequence s   s        sk   qx is a square block matrix of dimension
m   k       each block is an m  m matrix  the subsystem x has k     blocks of states  the
first block represents the state of h before any events  the second block represents h after exactly
one event  s    happens  the third block represents h after s  followed by s  happens  and so
on  the last block represents h after all the events finish executing in order  the subsystem has
zero transition intensities everywhere except along the sequence pass  the diagonal of qh is the
same matrix as that of qh except that the transition intensities of all the system call variables are
subtracted  this is because the full system includes transitions that were not observed  while those
transition rates were set to zero  to force the system to agree with the evidence   such conditioning
does not change the diagonal elements of the rate matrix  nodelman et al          within each of
the k     states of a block  h can freely change its value  therefore  the non diagonal elements
of qh have the same intensities as qh   upon transitioning  x can only transit from some state to
another according to the event sequence  therefore  most of the blocks are   matrices except those
to the immediate right of the diagonal blocks  the transition behavior is described by the matrix
qi   qi has   intensities on non diagonal entries because h and s can not change simultaneously 
the diagonal element qi  h  h  is the intensities of event si happening  given the current value of
the hidden state is h 
we take the forward pass as an example to describe the propagation  the backward pass can be
performed similarly  right before ti   ti has m dimensions  we expand it to m k      dimensions
to form ti which only has non zero probabilities in the first m states  ti now describes the
distribution over the subsystem x  ti eqx t represents the probability distribution at time ti   t  
given that some prefix of the observed sequence occurred  we take only the last m state probabilities
to condition on the entire sequence happening  thus resulting in an m dimensional vector  ti  t  
for a quiet period  ti   t   ti      no evidence is observed  therefore ti  t is propagated to
ti   using qh   the rate matrix conditioned on only h events occuring 
ti     ti  t exp qh  ti    ti  t     
when we are done with the full forward backward pass over the whole trajectory  we can calculate the expected sufficient statistics mhi hj   thi and ms hi   again  we refer to the work of nodelman
et al         for the algorithm 
   

fix u   s helton

    testing using likelihood
once we have learned the model from the normal process in the system call logs  we calculate
the log likelihood of a future process under the model  the log likelihood is then compared to a
predefined threshold  if it is below the threshold  a possible anomaly is indicated  with only a single
hidden variable  these calculations can be done exactly 

   evaluation
to evaluate our methodology  we constructed experiments on two different types of data  network
traffic traces and system call logs  in the following sections  we show the experiment results on both
tasks 
a dynamic bayesian network  dbn  is another popular technique for graphical modeling of
temporal data  because they slice time  events without state changes  instantaneous events  are
difficult to model  any reasonable time resolution will result with multiple events for the same
variable over one time period  there is no standard way of encoding this in a dbn  if we use a toggle
variable  it only records the parity of the number of events over the time interval  furthermore  for
the nids  events are very bursty  during active times  multiple packets are emited per second 
during inactive times  there may be no activity for hours  finding a suitable sampling rate that
maintains the efficency of the model is difficult  for the hids  the problem is more acute  we do
not know of any way of modeling timing ambiguity in a dbn without throwing away all timing
information or adding a mathematical framework that essentially turns the dbn into the ctbn
described here  in general  we could not find a suitable way to apply a dbn to these problems
without essentially turning the dbn into a ctbn by very finely slicing time and then applying
numeric tricks to speed up inference that amount to converting the stochastic matrices into rate
matrices and using numeric integration for the matrix exponential 
we have compared against current adaptive methods for each problem individually  these include nearest neighbor  support vector machines  and sequence time delaying embedding  we give
further details on these methods below 
    experiment results on network traffic
in this section  we present our experiment results on nids 
      datasets
we verify our approach on two publicly available real network traffic trace repositories  the mawi
working group backbone traffic mawi and the lbnl icsi internal enterprise traffic lbnl 
the mawi backbone traffic is part of the wide project which has collected raw daily packet
header traces since       it records the network traffic through the inter pacific tunnel between
japan and the usa  the dataset uses tcpdump and ip anonymizing tools to record    minute
traces every day  and consists mostly of traffic from or to japanese universities  in our experiment 
we use the traces from january  st to  th of       with            connections over a total time of
one hour 
the lbnl traces are recorded from a medium sized site  with emphasis on characterizing internal enterprise traffic  publicly released in an anonymized form  the lbnl data collects more than
   

fii ntrusion d etection using ctbn s

  packets flowing from source to destination
  packets flowing from destination to source
  connections by the same source in the last   seconds
  connections to the same destination in the last   seconds
  different services from the same source in the last   seconds
  different services to the same destination in the last   seconds
  connections by the same source in the last     connections
  connections to the same destination in the last     connections
  connections with the same port and source in the last     connections
  connections with the same port and destination in the last     connections
figure    features for nearest neighbor approach from the work of  lazarevic et al         
    hours network traces from thousands of internal hosts  from what is publicly released  we take
one hour traces from january  th        the latest date available   with           total connections 
      w orm d etection
we start with the problem of worm detection  we split traffic traces for each host  half for training
and half for testing  we learn a ctbn model from the training data for each of the hosts  since the
network data available are clean traffic with no known intrusions  we inject real attack traces into
the testing data  in particular  we inject ip scanner  w   mydoom  and slammer  we then slide
a fixed time window over the testing traces  report a single log likelihood value for each sliding
window  and compare it with a predefined threshold  if it is below the threshold  we predict it as
an abnormal time period  we define the ground truth for a window to be abnormal if any attack
traffic exists in the interval  and normal otherwise  the window size we use is    seconds  we only
consider windows that contain at least one network event 
we compare our method employing rbpf with our previous factored ctbn model  xu  
shelton         connection counting  nearest neighbor  parzen window detector  yeung   chow 
       and one class svm with a spectrum string kernel  leslie  eskin    noble        
the connection counting method is straightforward  we score a window by the number of
initiated connections in the window  as most worms aggregate many connections in a short time 
this method captures this particular anomaly well 
to make nearest neighbor competitive  we try to extract a reasonable set of features  we follow
the feature selection of the work of lazarevic et al          who use a total of    features  not all
of their features are available in our data  those available are shown in figure    notice that these
features are associated with each connection record  to apply the nearest neighbor method to our
window based testing framework  we first calculate the nearest distance of each connection inside
the window to the training set  which is composed of normal traffic only   and assign the maximum
among them as the score for the window  similarly  for the parzen window approach  we apply the
same feature set and assign the maximum density among all the connections inside a window to be
the score of that window 
besides the above feature based algorithms  we would also like to see how sequence based
approaches compare against our methods  these algorithms are widely used in network anomaly
detection  like our approach  they treat the traffic traces as stream data so that sequential contexts
   

fix u   s helton

 

 

   

   

   

   

   

nearest neighbor
connection count
parzen window
svmspectrum
ctbn  factored
ctbn  rbpf   

   

 
 

    

    
    
false positive rate

    

   

   

nearest neighbor
connection count
parzen window
svmspectrum
ctbn  factored
ctbn  rbpf   

   

 
 

   

    

    

   

   

 
 

   

 

   

   

   

   

nearest neighbor
connection count
parzen window
svmspectrum
ctbn  factored
ctbn  rbpf   

 
 

    

    
    
false positive rate

ip scanning

    

   

   

   

nearest neighbor
connection count
parzen window
svmspectrum
ctbn  factored
ctbn  rbpf   

   

 
 

    

    
    
false positive rate

mydoom

    

   

true positive rate

 

   

    

    
    
false positive rate

    

   

slammer

 

   

nearest neighbor
connection count
parzen window
svmspectrum
ctbn  factored
ctbn  rbpf   

   

mydoom

true positive rate

true positive rate

lbnl

ip scanning

    
    
false positive rate

true positive rate

 

true positive rate

true positive rate

mawi

can be explored  one class svm with spectrum string kernel was chosen for comparison  we
implemented a spectrum kernel in the libsvm library  chang   lin         we give the network
activities  such as a connection starting or ending  or a packet emmision or receipt  inside each portlevel submodel a distinct symbol  the sequence of these symbols are fed to the algorithm as inputs 
a decision surface is trained from normal training traffic  in testing  for each sliding window  the
distance from this window string to the decision hyperplane is reported as the window score  we
also tried experiments using the edit distance kernel  but their results are dominated by the spectrum
kernel  so we do not report them here 

   

   

nearest neighbor
connection count
parzen window
svmspectrum
ctbn  factored
ctbn  rbpf   

   

 
 

    

    
    
false positive rate

    

   

slammer

figure    roc curves of testing results on ip scanning attack  mydoom attack and slammer attack 
          top  mawi  bottom  lbnl 

when injecting the attack traffic  we randomly pick a starting point somewhere in the first half
of the test trace and insert worm traffic for a duration equal to  times the length of the full testing
trace  the shorter  is  the harder it is to detect the anomaly  we choose  to be       for all
the experiments in this work to challenge the detection tasks  we also scaled back the rates of the
worms  when running at full speed  a worm is easy to detect for any method  when it slows down
 and thus blends into the background traffic better   it becomes more difficult to detect  we let  be
the scaling rate  e g      indicates a worm running at one tenth of its normal speed  
for our method  we set the state space of variable g to be   and variable h to be    we use
    samples for particle filtering  and resample the particles after every    seconds  for the svm
spectrum kernel method  we choose the sub sequence length to be   and the parameter  to be     
we show the roc curves of all the methods in figure    the curves show the overall performance on the    most active hosts for each dataset  each point on the curves corresponds to a
   

fii ntrusion d etection using ctbn s

 

 

   

   
true positive rate

true positive rate

different threshold of the algorithm  our ctbn method out performs the other algorithms except
in the single case of the mydoom attack against a background of the lbnl traffic  in many cases 
the advantages of the ctbn approach are pronounced 
for all of the mawi data  the factored and non factored ctbn models perform comparably 
we believe this is because the data only captures connections that traverse a trans pacific link 
therefore  not all of the connections in or out of a machine are represented  this makes reasoning
about the global pattern of interaction for a machine difficult  for the lbnl data  one attack  ip
scanning  shows no advantage to a non factored model  one attack  mydoom  shows a distinct
advantage  and one attack  slammer  indicates some advantage  depending on the desired false
positive rate  this demonstrate some advantage to jointly modeling the traffic across all ports 
although it is clear this advantage is not uniform over traffic patterns and attack types 

   

   

   

   

   

   
connection count
ctbn  rbpf   

 
 

    

    
    
false positive rate

    

connection count
ctbn  rbpf   

   

 
 

    

    
    
false positive rate

    

   

figure    roc curves of testing results for slammer attack on mawi dataset demonstrating the
effect of slowing the attack rate  left           right          

we also show how the roc curves shift as we scale back the worm running speed  in figure   
as firewalls are built to be more sensitive to block malicious traffic  worms have to act more stealthy
to sneak through  we demonstrate the robustness of our method compared to the best competitor
 connection counts  to the speed of the worms attack 
      h ost i dentification
identifying individual hosts based on their network traffic patterns is another useful application of
our model  for instance  a household usually installs a network router  each family members
computer is connected to this router  to the outside internet  the network traffic going out of the
router behaves as if it is coming from one peer  but it is actually coming from different people 
dad will possibly read sports news while kids surf on social networks  it is interesting as well as
useful to tell which family member is contributing the current network traffic  host identification
can also be used to combat identity theft  when a network identity is abused by the attacker  host
identification techniques can help the network administrator tell whether the current network traffic
of this host is consistent with its usual pattern or not 
   

fix u   s helton

the first set of experiments we construct is a host model fitting competition  the same   
hosts picked for the worm detection tasks from lbnl dataset compose our testing pool  we learn
the coupled ctbn model for each host  we split the test traces  clean  of a particular host into
segments with lengths of    seconds  for each of the segments  we compute the log likelihood of
the segment under the learned model from all the hosts  including its own   and label the segment
with the host that achieves the highest value  we compute a confusion matrix c whose element cij
equals the fraction of test traces of host i for which model j has highest log likelihood  we expect
to see the highest hit rates fall on the diagonals because ideally a host should be best described by its
own model  table   shows our results on the dataset of lbnl  the vast majority of traffic windows
are assigned to the correct host  with the exception of host    the diagonals are distinctly higher than
other elements in the same row  for comparison  we performed the same experiment using svm
spectrum kernel method  again  we selected the sub sequence length to be   and the parameter
 to be      we tried multiple methods for normalization  of the distance to the hyperplane  and
variations of parameters  all produced very poor results with almost all of the windows assigned to
a single host  we omit the table of results 
host
 
 
 
 
 
 
 
 
 
  

 
    
    
 
 
 
 
    
 
    
 

 
 
 
           
           
 
 
 
 
 
 
           
           
 
         
           
              
              

 
 
 
 
 
  
 
 
 
      
 
 
      
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
                     
    
           
      
 
 
 
      
 
    
 
      
      
 
                
      
 
                
    

figure    confusion matrix for lbnl for host identification using ctbn
our second experiment is a host traffic differentiation task  we mingle the network traffic from
another host with the analyzed host  we expect the detection method to successfully tell apart the
two  to verify this idea  we pick one host among the    we choose above from lbnl dataset and
split its traffic evenly into training and testing  we again learn the model from training data  for
testing data  we randomly choose a period and inject another hosts traffic as if it were a worm 
our goal is to identify the period as abnormal since the hosts traffic is no longer its own behavior 
figure    displays the results from two such combination tests  the parameters for injecting the
traffic as a worm are                    in the left graph  the nearest neighbor and parzen
window curve overlap  and both ctbn curves overlap  in the right graph  the coupled ctbn curve
substantially outperforms all the other curves 
    experiment results on system call logs
in this section  we present our experiment results on hids 
   

fi 

 

   

   

   
   

nearest neighbor
connection count
parzen window
svmspectrum
ctbn  factored
ctbn  rbpf  

   
 
 

    

    
    
false positive rate

    

true positive rate

true positive rate

i ntrusion d etection using ctbn s

nearest neighbor
connection count
parzen window
svmspectrum
ctbn  factored
ctbn  rbpf  

   
   
   
 
 

   

    

    
    
false positive rate

    

   

figure     roc curves of testing results on host identification on the lbnl data  left  host   
nearest neighbor curve and parzen window curve overlap  both ctbn curves overlap 
right  host   

week
 
 
 
 
 
 
 

  normal
processes
   
   
   
   
   
   
   

  attack
processes
 
 
  
   
  
  
 

system call
close
ioctl
mmap
open
fcntl
stat
access

  occurrence
      
     
     
     
    
    
    

system call
execve
chdir
chroot
unlink
chown
mkdir
chmod

  occurrence
    
    
   
  
  
 
 

figure     left  darpa bsm process summary  right  darpa bsm system call summary
      dataset
the dataset we used is the      darpa intrusion detection evaluation data set from mit lincoln
laboratory  seven weeks of training data that contain labeled network based attacks in the midst of
normal background data are publicly available at the darpa website  the solaris basic security
module  bsm  praudit audit data on system call logs are provided for research analysis  we follow
kang  fuller  and honavar        to cross index the bsm logs and produce a labeled list file that
labels individual processes  the resulting statistics are shown on the left table of figure     the
frequency of all the system calls appearing in the dataset is summarized in descending order on the
right of figure    
      a nomaly d etection
our experimental goal is to detect anomalous processes  we train our ctbn model on normal
processes only and test on a mixture of both normal and attack processes  the state space of the
   

fi 

 

   

   
true positive rate

true positive rate

x u   s helton

   
   
ctbn
svmspectrum
stide
nearest neighbor

   
 
 

    

    
    
false positive rate

    

   
   
ctbn
svmspectrum
stide
nearest neighbor

   

    

 
 

    

    
    
false positive rate

    

    

figure     roc curves for bsm data detection  left  training on week   and combined testing
results on week   to    right  training on week   and test on week    stide curve and
ctbn curve overlap

hidden variable h is set to    the log likelihood of a whole process under the learned model
represents the score of this process  we compare to the score with a predefined threshold to classify
the process as a normal one or a system abuse 
we implement sequence time delaying embedding  stide  and stide with frequency threshold
 t stide  for comparison  warrender  forrest    pearlmutter         these two algorithms build a
database of all previously seen normal sequences of system calls and compare the testing sequences
with it  they are straightforward and perform very well empirically on most of the system call log
datasets  we choose the parameter k  the sequence length to be    and h  the locality frame length  to
be     the results for t stide are not shown in the following resulting graphs since they overlapped
with stide in almost all cases 
other approaches we compare against are nearest neighbor and one class svm with spectrum
string kernel and edit distance kernel  we follow hu et al         and transform a process into a
feature vector  consisting of the occurrence numbers of each system call in the process  the nearest
distance between a testing process and the training set of processes is assigned as the score  for
one class svm  processes are composed of strings of system calls  normal processes are used for
learning the bounding surface and the signed distance to it is assigned as the score  we set the subsequence length to be   and the parameter  to be      again  since the edit distance kernel results
are dominated by the spectrum kernel  we do not show them 
figure    displays the results from two experiment settings  in the left graph  we train the
model on the normal processes from week   and test it on all the processes from weeks   to    in
the right graph  we train on normal processes from week   and test it on all the processes from week
   the richest in attack processes volume  because attacks are relatively rare compared to normal
traffic  we are most interested in the region of the roc curves with small false positive rates  so
we only show the curves in the area where the false positive rate falls in the region            our
ctbn method beats nearest neighbor and svm with spectrum kernel in both experiments  stide
performs slightly better than our method in the combined test  but achieves the same accuracy in
   

fii ntrusion d etection using ctbn s

the experiment using only week   and testing on week    the advantage to the ctbn model over
stide is that it can be easily combined with other prior knowledge and other data sources  such as the
network data from nids   we demonstrate that there is no loss of performance from such flexibility 

   conclusions
in the realm of temporal reasoning  we have introduced two additions to the ctbn literature  first 
we demonstrated a rao blackwellized particle filter with continuous evidence  second  we demonstrated that we can learn and reason about data that contains imprecise timings  while still refraining
from discretizing time 
in the realm of intrusion detection  we have demonstrated a framework that performs well on two
related tasks with very different data types  by concentrating purely on event timing  without the
consideration of complex features  we were able to out perform existing methods  the continuoustime nature of our model aided greatly in modeling the bursty event sequences that occur in systems
logs and network traffic  we did not have to resort to time slicing  either producing rapid slices that
are inefficient for quite periods  or lengthy slices that miss the timing of bursty events 
a combination of the two sources of information  system calls and network events  would be
straight forward with the model we have produced  we believe it would result in more accurate
detection  the collection of such data is difficult  however  we leave it as an interesting next step 

acknowledgments
this project was supported by intel research and uc micro  by the air force office of scientific
research  fa                 and by the defense advanced research project agency  hr               

references
agosta  j  m   duik wasser  c   chandrashekar  j     livadas  c          an adaptive anomaly
detector for worm detection  in workshop on tackling computer systems problems with
machine learning techniques 
a hofmeyr  s   forrest  s     somayaji  a          intrusion detection using sequences of system
calls  journal of computer security            
cha  b          host anomaly detection performance analysis based on system call of neuro fuzzy
using soundex algorithm and n gram technique  in systems communications  icw  
chang  c  c     lin  c  j          libsvm  a library for support vector machines  http   
www csie ntu edu tw cjlin libsvm 
cohn  i   el hay  t   friedman  n     kupferman  r          mean field variational approximation
for continous time bayesian networks  in uncertainty in artificial intelligence 
dewaele  g   fukuda  k     borgnat  p          extracting hidden anomalies using sketch and non
gaussian multiresulotion statistical detection procedures  in acm sigcomm 
doucet  a   de freitas  n   murphy  k     russel  s          rao blackwellised particle filtering
for dynamic bayesian networks  in uncertainty in artificial intelligence 
   

fix u   s helton

el hay  t   cohn  i   friedman  n     kupferman  r          continuous time belief propagation 
in proceedings of the twenty seventh international conference on machine learning 
el hay  t   friedman  n     kupferman  r          gibbs sampling in factorized continous time
markov processes  in uncertainty in artificial intelligence 
eskin  e          anomaly detection over noisy data using learned probability distributions  in
international conference on machine learning 
eskin  e   arnold  a   prerau  m   portnoy  l     stolfo  s          a geometric framework for
unsupervised anomaly detection  detecting intrusions in unlabeled data  in barbara  d    
jajodia  s   eds    applications of data mining in computer security  kluwer 
fan  y     shelton  c  r          sampling for approximate inference in continuous time bayesian
networks  in symposium on artificial intelligence and mathematics 
fan  y     shelton  c  r          learning continuous time social network dynamics  in proceedings of the twenty fifth international conference on uncertainty in artificial intelligence 
forrest  s   a hofmeyr  s   somayaji  a     a longstaff  t          a sense of self for unix processes  in ieee symposium on security and privacy  pp         
hu  w   liao  y     vemuri  v          robust support vector machines for anomaly detection in
computer security  in international conference on machine learning and applications 
kang  d  k   fuller  d     honavar  v          learning classifiers for misuse detetction using a
bag of system calls representation  in ieee international conferences on intelligence and
security informatics 
karagiannis  t   papagiannaki  k     faloutsos  m          blinc  multilevel traffic classification
in the dark  in acm sigcomm 
kruegel  c   mutz  d   robertson  w     valeur  f          bayesian event classification for intrusion
detection  in annual computer security applications conference 
lakhina  a   crovella  m     diot  c          mining anomalies using traffic feature distributions 
in acm sigcomm  pp       
lazarevic  a   ertoz  l   kumar  v   ozgur  a     srivastava  j          a compare study of anomaly
detection schemes in network intrusion detection  in siam international conference on data
mining 
lbnl 
lbnl icsi enterprise tracing project  
enterprise tracing overview html  

http   www icir org 

leslie  c   eskin  e     noble  w  s          the spectrum kernel  a string kernel for svm protein
classification  in pacific symposium on biocomputing           
malan  d  j     smith  m  d          host based detection of worms through peer to peer cooperation  in workshop on rapid malcode 
mawi  mawi working group traffic archive   http   mawi nezu wide ad jp mawi  
moore  a  w     zuev  d          internet traffic classification using bayesian analysis techniques 
in acm sigmetrics 
ng  b   pfeffer  a     dearden  r          continuous time particle filtering  in national conference
on artificial intelligence  pp           
   

fii ntrusion d etection using ctbn s

nodelman  u     horvitz  e          continuous time bayesian networks for inferring users presence and activities with extensions for modeling and evaluation  tech  rep  msr tr         
microsoft research 
nodelman  u   koller  d     shelton  c  r          expectation propagation for continuous time
bayesian networks  in uncertainty in artificial intelligence  pp         
nodelman  u   shelton  c  r     koller  d          continuous time bayesian networks  in uncertainty in artificial intelligence  pp         
nodelman  u   shelton  c  r     koller  d          learning continuous time bayesian networks 
in uncertainty in artificial intelligence  pp         
nodelman  u   shelton  c  r     koller  d          expectation maximization and complex duration
distributions for continuous time bayesian networks  in uncertainty in artificial intelligence 
pp         
press  w  h   teukolsky  s  a   vetterling  w  t     flannery  b  p          numerical recipes in c
 second edition   cambridge university press 
qin  x     lee  w          attack plan recognition and prediction using causal networks  in annual
computer security application conference  pp         
rieck  k     laskov  p          language models for detection of unknown attacks in network
traffic  in journal in computer virology 
saria  s   nodelman  u     koller  d          reasoning at the right time granularity  in uncertainty
in artificial intelligence 
simma  a   goldszmidt  m   maccormick  j   barham  p   black  r   isaacs  r     mortier  r 
        ct nor  representing and reasoning about events in continuous time  in uncertainty in artificial intelligence 
soule  a   salamatian  l   taft  n   emilion  r     papagiannali  k          flow classification by
histogram  in acm sigmetrics 
soule  a   salamatian  k     taft  n          combining filtering and statistical methods for
anomaly detection  in internet measurement conference  pp         
tandon  g     chan  p  k          learning useful system call attributes for anomaly detection  in
the florida artificial intelligence research society conference  pp          
warrender  c   forrest  s     pearlmutter  b          detecting intrusions using system calls  alternative data models  in ieee symposium on security and privacy  ieee computer society 
xu  j     shelton  c  r          continuous time bayesian networks for host level network intrusion
detection  in european conference on machine learning 
xu  k   zhang  z  l     bhattacharyya  s          profiling internet backbone traffic  behavior
models and applications  in acm sigcomm 
ye  n   emran  s  m   chen  q     vilbert  s          multivariate statistical analysis of audit trails
for host based intrusion detection  ieee transactions of computers                
yeung  d  y     chow  c          parzen window network intrusion detectors  in international
conference on pattern recognition 
   

fix u   s helton

yeung  d  y     ding  y          user profiling for intrusion detection using dynamic and static
behavioral models  advances in knowledge discovery and data mining               
zuev  d     moore  a          internet traffic classification using bayesian analysis techniques  in
acm sigmetrics 

   

fi