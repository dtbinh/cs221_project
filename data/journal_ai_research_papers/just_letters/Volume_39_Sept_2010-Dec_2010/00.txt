journal of artificial intelligence research               

submitted        published      

planning with noisy probabilistic relational rules
tobias lang
marc toussaint

tobias lang tu berlin de
mtoussai cs tu berlin de

machine learning and robotics group
technische universitat berlin
franklinstrae              berlin  germany

abstract
noisy probabilistic relational rules are a promising world model representation for several reasons  they are compact and generalize over world instantiations  they are usually
interpretable and they can be learned effectively from the action experiences in complex
worlds  we investigate reasoning with such rules in grounded relational domains  our algorithms exploit the compactness of rules for efficient and flexible decision theoretic planning 
as a first approach  we combine these rules with the upper confidence bounds applied to
trees  uct  algorithm based on look ahead trees  our second approach converts these
rules into a structured dynamic bayesian network representation and predicts the effects
of action sequences using approximate inference and beliefs over world states  we evaluate
the effectiveness of our approaches for planning in a simulated complex  d robot manipulation scenario with an articulated manipulator and realistic physics and in domains of
the probabilistic planning competition  empirical results show that our methods can solve
problems where existing methods fail 

   introduction
building systems that act autonomously in complex environments is a central goal of artificial intelligence  nowadays  a i  systems are on par with particularly intelligent humans
in specialized tasks such as playing chess  they are hopelessly inferior to almost all humans  however  in deceivingly simple tasks of everyday life  such as clearing a desktop 
preparing a cup of tea or manipulating chess figures  the current state of the art in reasoning  planning  learning  perception  locomotion  and manipulation is so far removed from
human level abilities  that we cannot yet contemplate working in an actual domain of interest  pasula  zettlemoyer    kaelbling         performing common object manipulations
is indeed a challenging task in the real world  we can choose from a very large number of
distinct actions with uncertain outcomes and the number of possible situations is basically
unseizable 
to act in the real world  we have to accomplish two tasks  first  we need to understand
how the world works  for example  a pile of plates is more stable if we place the big plates
at its bottom  it is a hard job to build a tower from balls  filling tea into a cup may lead to a
dirty table cloth  autonomous agents need to learn such world knowledge from experience
to adapt to new environments and not to rely on human hand crafting  in this paper  we
employ a recent solution for learning  pasula et al          once we know about the possible
effects of our actions  we face a second challenging problem  how can we use our acquired
knowledge in reasonable time to find a sequence of actions suitable to achieve our goals 
c
    
ai access foundation  all rights reserved 

filang   toussaint

this paper investigates novel algorithms to tackle this second task  namely planning  we
pursue a model based approach for planning in complex domains  in contrast to modelfree approaches which compute policies directly from experience with respect to fixed goals
 also called habit based decision making   we follow a purposive decision making approach
 botvinick   an        and use learned models to plan for the goal and current state at
hand  in particular  we simulate the probabilistic effects of action sequences  this approach
has interesting parallels in recent neurobiology and cognitive science results suggesting that
the behavior of intelligent mammals is driven by internal simulation or emulation  it has
been found that motor structures in the cortex are activated during planning  while the
execution of motor commands is suppressed  hesslow        grush        
probabilistic relational world model representations have received significant attention
over the last years  they enable to generalize over object identities to unencountered situations and objects of similar types and to account for indeterministic action effects and noise 
we will review several such approaches together with other related work in section    noisy
indeterministic deictic  nid  rules  pasula et al         capture the world dynamics in an
elegant compact way  they are particularly appealing as they can be learned effectively
from experience  the existing approach for planning with these rules relies on growing
full look ahead trees in the grounded domain  due to the very large action space and the
stochasticity of the world  the computational burden to plan just a single action with this
method in a given situation can be overwhelmingly large  this paper proposes two novel
ways for reasoning efficiently in the grounded domain using learned nid rules  enabling fast
planning in complex environments with varying goals  first  we apply the existing upper
confidence bounds applied to trees  uct  algorithm  kocsis   szepesvari        with nid
rules  in contrast to full grown look ahead trees  uct samples actions selectively  thereby
cutting suboptimal parts of the tree early  second  we introduce the probabilistic relational
action sampling in dbns planning algorithm  prada  which uses probabilistic inference
to cope with uncertain action outcomes  instead of growing look ahead trees with sampled successor states like the previous approaches  prada applies approximate inference
techniques to propagate the effects of actions  in particular  we make three contributions
with prada   i  following the idea of framing planning as a probabilistic inference problem  shachter        toussaint  storkey    harmeling         we convert nid rules into
a dynamic bayesian network  dbn  representation   ii  we derive an approximate inference method to cope with the state complexity of a time slice of the resulting network 
thereby  we can efficiently predict the effects of action sequences   iii  for planning based
on sampling action sequences  we propose a sampling distribution for plans which takes predicted state distributions into account  we evaluate our planning approaches in a simulated
complex  d robot manipulation environment with realistic physics  with an articulated humanoid manipulating objects of different types  see fig      this domain contains billions of
world states and a large number of potential actions  we learn nid rules from experience
in this environment and apply them with our planning approaches in different planning
scenarios of increasing difficulty  furthermore  we provide results of our approaches on
the planning domains of the most recent international probabilistic planning competition 
for this purpose  we discuss the relation between nid rules and the probabilistic planning
domain definition language  ppddl  used for the specification of these domains 
 

fiplanning with noisy probabilistic relational rules

we begin this paper by discussing the related work in section   and reviewing the
background of our work  namely stochastic relational representations  nid rules  the formalization of decision theoretic planning and graphical models in section    in section   
we present two planning algorithms that build look ahead trees to cope with stochastic
actions  in section    we introduce prada which uses approximate inference for planning 
in section    we present our empirical evaluation demonstrating the utility of our planning
approaches  finally  we conclude and outline future directions of research 

   related work
the problem of decision making and planning in stochastic relational domains has been approached in different ways  the field of relational reinforcement learning  rrl   dzeroski 
de raedt    driessens        van otterlo        investigates value functions and q functions
that are defined over all possible ground states and actions of a relational domain  the key
idea is to describe important world features in terms of abstract logical formulas enabling
generalization over objects and situations  model free rrl approaches learn value functions
for states and actions directly from experience  q function estimators include relational
regression trees  dzeroski et al         and instance based regression using distance metrics between relational states such as graph kernels  driessens  ramon    gartner        
model free approaches enable planning for the specific problem type used in the training
examples  e g  on x  y    and thus may be inappropriate in situations where the goals of
the agent change quickly  e g  from on x  y   to inhand x   in contrast  model based rrl
approaches first learn a relational world model from the state transition experiences and
then use this model for planning  for example in the form of relational probability trees
for individual state attributes  croonenborghs  ramon  blockeel    bruynooghe        or
svms using graph kernels  halbritter   geibel         the stochastic relational nid rules
of pasula et al         are a particularly appealing action model representation  as it has
been shown empirically that they can learn the dynamics of complex environments 
once a probabilistic relational world model is available  either learned or handcrafted  
one can pursue decision theoretic planning in different ways  within the machine learning
community  a popular direction of research formalizes the problem as a relational markov
decision process  rmdp  and develops dynamic programming algorithms to compute solutions  i e  policies over complete state and action spaces  many algorithms reason in
the lifted abstract representation without grounding or referring to particular problem instances  boutilier  reiter  and price        introduce symbolic dynamic programming 
the first exact solution technique for rmdps which uses logical regression to construct
minimal logical partitions of the state space required to make all necessary value function
distinctions  this approach has not been implemented as it is difficult to keep the firstorder state formulas consistent and of manageable size  based on these ideas  kersting  van
otterlo  and de raedt        propose an exact value iteration algorithm for rmdps using
logic programming  called rebel  they employ a restricted language to represent rmdps
so that they can reason efficiently over state formulas  holldobler and skvortsova       
present a first order value iteration algorithm  fovia  using a different restricted language 
karabaev and skvortsova        extend fovia by combining first order reasoning about
actions with a heuristic search restricted to those states that are reachable from the initial
 

filang   toussaint

state  wang  joshi  and khardon        derive a value iteration algorithm based on using
first order decision diagrams  fodds  for goal regression  they introduce reduction operators for fodds to keep the representation small  which may require complex reasoning 
an empirical evaluation has not been provided  joshi  kersting  and khardon        apply
model checking to reduce fodds and generalize them to arbitrary quantification 
all these techniques form an interesting research direction as they reason exactly about
abstract rmdps  they employ different methods to ensure exact regression such as theorem proving  logical simplification  or consistency checking  therefore  principled approximations of these techniques that can discover good policies in more difficult domains are
likewise worth investigating  for instance  gretton and thiebaux        employ first order
regression to generate a suitable hypothesis language which they then use for policy induction  thereby  their approach avoids formula rewriting and theorem proving  while still
requiring model checking  sanner and boutilier              present a first order approximate linear programming approach  foalp   prior to producing plans  they approximate
the value function based on linear combinations of abstract first order value functions 
showing impressive results on solving rmdps with millions of states  fern  yoon  and
givan        consider a variant of approximate policy iteration  api  where they replace
the value function learning step with a learning step in policy space  they make use of a
policy space bias as described by a generic relational knowledge representation and simulate trajectories to improve the learned policy  kersting and driessens        describe a
non parametric policy gradient approach which can deal with propositional  continuous and
relational domains in a unified way 
instead of working in the lifted representation  one may reason in the grounded domain 
this makes it straightforward to account for two special characteristics of nid rules  the
noise outcome and the uniqueness requirement of rules  when grounding an rmdp which
specifies rewards only for a set of goal states  one might in principle apply any of the traditional a i  planning methods used for propositional representations  weld        boutilier 
dean    hanks         traditionally  planning is often cast as a search problem through
a state and action space  restricting oneself to the portion of the state space that is considered to contain goal states and to be reachable from the current state within a limited
horizon  much research within the planning community has focused on deterministic domains and thus cant be applied straightforwardly in stochastic worlds  a common approach
for probabilistic planning  however  is to determinize the planning problem and apply deterministic planners  kuter  nau  reisner    goldman         indeed  ff replan  yoon 
fern    givan        and its extension using hindsight optimization  yoon  fern  givan   
kambhampati        have shown impressive performance on many probabilistic planning
competition domains  the common variant of ff replan considers each probabilistic outcome of an action as a separate deterministic action  ignoring the respective probabilities 
it then runs the deterministic fast forward  ff  planner  hoffmann   nebel        on the
determinized problem  ff uses a relaxation of the planning problem  it ignores the delete
effects of actions and applies clever heuristics to prune the search space  ff replan outputs
a sequence of actions and expected states  each time an action execution leads to a state
which is not in the plan  ff replan has to replan  i e   recompute a new plan from scratch
in the current state  the good performance of ff replan in many probabilistic domains
has been explained by the structure of these problems  little   thiebaux         it has
 

fiplanning with noisy probabilistic relational rules

been argued that ff replan should be less appropriate in domains in which the probability
of reaching a dead end is non negligible and where the outcome probabilities of actions need
to be taken into account to construct a good policy 
many participants of the most recent probabilistic planning competition  ippc       
extend ff replan to deal with the probabilities of action outcomes  see the competition
website for brief descriptions of the algorithms   the winner of the competition  rff
 teichteil konigsbuch  kuter    infantes         computes a robust policy offline by generating successive execution paths leading to the goal using ff  the resulting policy has
low probability of failing  lppff uses subgoals generated from a determinization of the
probabilistic planning problem to divide it into smaller manageable problems  hmdpps
strategy is similar to the all outcomes determinization of ff replan  but accounts for the
probability associated with each outcome  seh  wu  kalyanam    givan        extends
a heuristic function of ff replan to cope with local optima in plans by using stochastic
enforced hill climbing 
a common approach to reasoning in a more general reward maximization context which
avoids explicitly dealing with uncertainty is to build look ahead trees by sampling successor
states  two algorithms which follow this idea  namely sst  kearns  mansour    ng       
and uct  kocsis   szepesvari         are investigated in this paper 
another approach by buffet and aberdeen        directly optimizes a parameterized
policy using gradient descent  they factor the global policy into simple approximate policies
for starting each action and sample trajectories to cope with probabilistic effects 
instead of sampling state transitions  we propose the planning algorithm prada in this
paper  based on lang   toussaint      a  which accounts for uncertainty in a principled
way using approximate inference  domshlak and hoffmann        propose an interesting
planning approach which comes closest to our work  they introduce a probabilistic extension of the ff planner  using complex algorithms for building probabilistic relaxed planning
graphs  they construct dynamic bayesian networks  dbns  from hand crafted strips operators and reason about actions and states using weighted model counting  their dbn
representation  however  is inadequate for the type of stochastic relational rules that we use 
for the same reasons why the naive dbn model which we will discuss in sec      is inappropriate  planning by inference approaches  toussaint   storkey        spread information
also backwards through dbns and calculate posteriors over actions  resulting in policies
over complete state spaces   how to use backward propagation or even full planning by
inference in relational domains is an open issue 
all approaches working in the grounded representation have in common that the number
of states and actions will grow exponentially with the number of objects  to apply them in
domains with very many objects  these approaches need to be combined with complementary
methods that reduce the state and action space complexity in relational domains  for
instance  one can focus on envelopes of states which are high utility subsets of the state
space  gardiol   kaelbling         one can ground the representation only with respect to
relevant objects  lang   toussaint      b   or one can exploit the equivalence of actions
 gardiol   kaelbling         which is particularly useful in combination with ignoring
certain predicates and functions of the relational logic language  gardiol   kaelbling        
 

filang   toussaint

   background
in this section  we set up the theoretical background for the planning algorithms we will
present in subsequent sections  first  we describe relational representations to define world
states and actions  then we will present noisy indeterministic deictic  nid  rules in detail
and thereafter define the problem of decision theoretic planning in stochastic relational
domains  finally  we briefly review dynamic bayesian networks 
    state and action representation
a relational domain is represented by a relational logic language l  the set of logical
predicates p and the set of logical functions f contain the relationships and properties that
can hold for domain objects  the set of logical predicates a comprises the possible actions
in the domain  a concrete instantiation of a relational domain is made up of a finite set of
objects o  if the arguments of a predicate or function are all concrete  i e  taken from o  we
call it grounded  a concrete world state s is fully described as a conjunction of all grounded
 potentially negated  predicates and function values  concrete actions a are described by
positive grounded predicates from a  the arguments of predicates and functions can also
be abstract logical variables which can represent any object  if a predicate or function
has only abstract arguments  we call it abstract  abstract predicates and functions enable
generalization over objects and situations  we will speak of grounding a formula  if we
apply a substitution  that maps all of the variables appearing in  to objects in o 
a relational model t of the transition dynamics specifies p  s   a  s   the probability
of a successor state s  if action a is performed in state s  in this paper  this is usually
a non deterministic distribution  t is typically defined compactly in terms of formulas
over abstract predicates and functions  this enables abstraction from object identities and
concrete domain instantiations  for instance  consider a set of n cups  the effects of trying
to grab any of these cups may be described by the same single abstract model instead of
using n individual models  to apply t in a given world state  one needs to ground t with
respect to some of the objects in the domain  nid rules are an elegant way to specify such
a model t and are described in the following 
    noisy indeterministic deictic rules
we want to learn a relational model of a stochastic world and use it for planning  pasula
et al         have recently introduced an appealing action model representation based on
noisy indeterministic deictic  nid  rules which combine several advantages 
 a relational representation enabling generalization over objects and situations 
 indeterministic action outcomes with probabilities to account for stochastic domains 
 deictic references for actions to reduce action space 
 noise outcomes to avoid explicit modeling of rare and overly complex outcomes  and
 the existence of an effective learning algorithm 
 

fiplanning with noisy probabilistic relational rules

table   shows an exemplary nid rule for our complex robot manipulation domain 
fig    depicts a situation where this rule can be used for prediction  formally  a nid rule
r is given as

pr  
  r    x  




  
 
   
ar  x     r  x   

 
r mr  x  
p

r m
r


pr  
  r  
where x is a set of logical variables in the rule  which represent a  sub  set of abstract
objects   in the rules which define our world models all formulas are abstract  i e   their
arguments are logical variables  the rule r consists of preconditions  namely that action
ar is applied on x and that the state p
context r is fulfilled  and mr    different outcomes
with associated probabilities pr i     i   pr i      each outcome r i  x   describes which
predicates and functions change when the rule is applied  the context r  x   and outcomes
r i  x   are conjunctions of  potentially negated  literals constructed from the predicates in
p as well as equality statements comparing functions from f to constant values  besides the
explicitely stated outcomes r i  i       the so called noise outcome r   models implicitly
all other potential outcomes of this rule  in particular  this includes the rare and overly
complex outcomes typical for noisy domains  which we do not want to cover explicitly for
compactness and generalization reasons  for instance  in the context of the rule depicted in
fig    a potential  but highly improbable outcome is to grab the blue cube while pushing all
other objects of the table  the noise outcome allows to account for this without the burden
of explicitly stating it 
the arguments of the action a xa   may be a true subset xa  x of the variables x
of the rule  the remaining variables are called deictic references d   x   xa and denote
objects relative to the agent or action being performed  using deictic references has the
advantage to decrease the arity of action predicates  this in turn reduces the size of the
action space by at least an order of magnitude  which can have significant effects on the
planning problem  for instance  consider a binary action predicate which in a world of
n objects has n  groundings in contrast to a unary action predicate which has only n
groundings 
as above  let  denote a substitution that maps variables to constant objects     x  o 
applying  to an abstract rule r x   yields a ground rule r  x     we say a ground rule r
covers a state s and a ground action a if s    r and a   ar   let  be a set of ground nid
rules  we define  a      r   r    ar   a  to be the set of rules that provide predictions for
action a  if r is the only rule in  a  to cover a and state s  we call it the unique covering rule
for a in s  if a state action pair  s  a  has a unique covering rule r  we calculate p  s    s  a 
by taking all outcomes of r into account weighted by their respective probabilities 
r

 

 

p  s  s  a    p  s  s  r   

m
x

pr i p  s   r i   s    pr   p  s   r     s  

   

i  

where  for i      p  s    r i   s  is a deterministic distribution that is one for the unique
state constructed from s taking the changes of r i into account  the distribution given
 

filang   toussaint

table    example nid rule for a complex robot manipulation scenario  which models to
try to grab a ball x  the cube y is implicitly defined as the one below x  deictic
referencing   x ends up in the robots hand with high probability  but might
also fall on the table  with a small probability something unpredictable happens 
confer fig    for an example application 

grab x    on x  y    ball x   cube y    table z 

       inhand x   on x  y  
      on x  z   on x  y  


      noise

figure    the nid rule defined in table   can be used to predict the effects of action
grab ball  in the situation on the left side  the right side depicts the possible
successor states as predicted by the rule  the noise outcome is indicated by a
question mark and does not define a unique successor state 

the noise outcome  p  s    r     s   is unknown and needs to be estimated  pasula et al  use a
worst case constant bound pmin  p  s   r     s  to lower bound p  s   s  a   alternatively  to
come up with a well defined distribution  one may assign very low probability to very many
successor states  as described in more detail in sec       our planning algorithm prada
exploits the factored state representation of a grounded relational domain to achieve this
by predicting each state attribute to change with a very low probability 
if a state action pair  s  a  does not have a unique covering rule r  e g  two rules cover
 s  a  providing conflicting predictions   one can predict the effects of a by means of a
noisy default rule r which explains all effects with changing state attributes as noise 
p  s   s  r     p  s    r      s   essentially  using r expresses that we do not know what
will happen  this is not meaningful and thus disadvantageous for planning   hence  one
should bias a nid rules learner to learn rules with contexts which are likely to be mutually
exclusive   for this reason  the concept of unique covering rules is crucial in planning with
nid rules  here  we have to pay the price for using deictic references  when using an
abstract nid rule for prediction  we always have to ensure that its deictic references have
unique groundings  this may require examining a large part of the state representation  so
 

fiplanning with noisy probabilistic relational rules

that proper storage of the ground state and efficient indexing techniques for logical formula
evaluation are needed 
the ability to learn models of the environment from experience is a crucial requirement
for autonomous agents  the problem of learning rule sets is in general np hard  but efficiency guarantees on the sample complexity can be given for many learning subtasks with
suitable restrictions  walsh         pasula et al         have proposed a supervised batch
learning algorithm for complete nid rules  this algorithm learns the structure of rules
as well as their parameters from experience triples  s  a  s     stating the observed successor
state s  after action a was applied in state s  it performs a greedy search through the space
of rule sets  it optimizes the tradeoff between maximizing the likelihood of the experience
triples and minimizing the complexity of the current hypothesis rule set  by optimizing
the scoring metric
x
x
s    
log p  s    s  rs a    
p en  r   
   
 s a s   

r

where rs a is either the unique covering rule for  s  a  or the noisy default rule r and 
is a scaling parameter that controls the influence of regularization  p en  r  penalizes the
complexity of a rule and is defined as the total number of literals in r 
the noise outcome of nid rules is crucial for learning  the learning algorithm is initialized with a rule set comprising only the noisy default rule r and then iteratively adds
new rules or modifies existing ones using a set of search operators  the noise outcome
allows avoiding overfitting  as we do not need to model rare and overly complex outcomes
explicitly  its drawback is that its successor state distribution p  s    r     s  is unknown 
to deal with this problem  the learning algorithm uses a lower bound pmin to approximate
this distribution  as described above  this algorithm uses greedy heuristics in its attempt
to learn complete rules  so no guarantees on its behavior can be given  pasula et al   however  report impressive results in complex noisy environments  in sec       we confirm their
results in a simulated noisy robot manipulation scenario  our major motivation for employing nid rules is that we can learn them from observed actions and state transitions 
furthermore  our planning approach prada can exploit their simple structure  which is
similar to probabilistic strips operators  and convert them into a dbn representation 
we provide a detailed comparison of nid rules and ppddl in appendix b  while nid
rules do not support all features of a sophisticated domain description language such as
ppddl  they can compactly capture the dynamics of many interesting planning domains 
    decision theoretic planning
the problem of decision theoretic planning is to find actions a  a in a given state s which
are expected to maximize future rewards for states and actions  boutilier et al         
in classical planning  this reward is usually defined in terms of a clear cut goal which is
either fulfilled or not fulfilled in a state  this can be expressed by means of a logical
formula   typically  this formula is a partial state description so that there exists more
than one state where  holds  for example  the goal might be to put all our romance
books on a specific shelf  no matter where the remaining books are lying  in this case 
planning involves finding a sequence of actions a such that executing a starting in s will
 

filang   toussaint

result in a world state s  with s       in stochastic domains  however  the outcomes of
actions are uncertain  probabilistic planning is inherently harder than its deterministic
counterpart  littman  goldsmith    mundhenk         in particular  achieving a goal
state with certainty is typically unrealistic  instead  one may define a lower bound  on
the probability for achieving a goal state  a second source of uncertainty next to uncertain
action outcomes is the uncertainty about the initial state s  we will ignore the latter in the
following and always assume deterministic initial states  as we will see later  however  it is
straightforward to incorporate uncertainty about the initial state using one of our proposed
planning approaches 
instead of a classical planning task which is finished once we have achieved a state
where the goal is fulfilled  our task may also be ongoing  for instance  our goal might be to
keep the desktop tidy  this can be formalized by means of a reward function over states 
which yields high reward for desirable states  for simplicity  here we assume rewards do
not depend on actions   this is the approach taken in reinforcement learning formalisms
 sutton   barto         classical planning goals can easily be formalized with such a
reward function  we cast the scenario of planning in a stochastic relational domain in a
relational markov decision process  rmdp  framework  boutilier et al          we follow
the notation of van otterlo        and define an rmdp as a   tuple  s  a  t  r   in contrast
to enumerated state spaces  here the state space s has a relational structure defined by
logical predicates p and functions f  which yield the ground atoms with arguments taken
from the set of domain objects o  the action space a is defined by positive predicates a
with arguments from o  t   s  a  s         is a transition distribution and r   s  r
the reward function  both t and r can make use of the factored relational representation
of s and a to abstract from states and actions  as discussed in the following  typically  the
state space s and the action space a of a relational domain are very large  consider for
instance a domain of   objects where we use   binary predicates to represent states  in this
 
case  the number of states is             relational world models encapsulate the transition
probabilities t in a compact way exploiting the relational structure  for example  nid rules
as described in eq      achieve this by generalized partial world state descriptions in the
form of conjunctions of abstract literals  the compactness of these models  however  does
not carry over directly to the planning problem 
a  deterministic  policy    s  a tells us which action to take in a given state  for
a fixed horizon d and a discount
          we are interested in maximizing the
pd factor
t r   the value of a factored state is defined as the
discounted total reward r  

t
t  
expected return from state s following policy  
v   s    e r   s    s     

   

a solution to an rmdp  and thus to the problem of planning  is an optimal policy   which
maximizes the expected return  it can be defined by the bellman equation 
x


v   s    r s     max 
p  s    s  a v   s      
aa

  

s 

   

fiplanning with noisy probabilistic relational rules

similarly  one can define the value q  s  a  of an action a in state s as the expected return
after action a is taken in state s  using policy  to select all subsequent actions 
q  s  a    e r   s    s  a    a   
x
  r s    
v   s   p  s    s  a   

   
   

s 

the q values for the optimal policy   let us define the optimal action a and the optimal
value of a state as


a   argmax q  s  a 

and

   

aa




v   s    max q  s  a   
aa

   

in enumerated unstructured state spaces  state and q values can be computed using dynamic programming methods resulting in optimal policies over the complete state space 
recently  promising approaches exploiting relational structure have been proposed that apply similar ideas to solve or approximate solutions in rdmps on an abstract level  without
referring to concrete objects from o   see related work in sec      alternatively  one may
reason in the grounded relational domain  this makes it straightforward to account for the
noise outcome and the uniqueness requirement of nid rules  usually  one focuses on estimating the optimal action values for the given state  this approach is appealing for agents
with varying goals  where quickly coming up with a plan for the problem at hand is more
appropriate than computing an abstract policy over the complete state space  although
grounding simplifies the problem  decision theoretic planning in the propositionalized representation is a challenging task in complex stochastic domains  in sections   and    we
present different algorithms reasoning in the grounded relational domain for estimating the
optimal q values of actions  and action sequences  for a given state 
    dynamic bayesian networks
dynamic bayesian networks  dbns  model the development of stochastic systems over
time  the prada planning algorithm which we introduce in sec    makes use of this
kind of graphical model to evaluate the stochastic effects of action sequences in factored
grounded relational world states  therefore  we will briefly review bayesian networks and
their dynamic extension here 
a bayesian network  bn   jensen        is a compact representation of the joint probability distribution over a set of random variables x by means of a directed acyclic graph
g  the nodes in g represent the random variables  while the edges define their dependencies and thereby express conditional independence assumptions  the value x of a variable
x  x depends only on the values of its immediate ancestors in g  which are called the
parents p a x  of x  conditional probability functions at each node define p  x   p a x   
in case of discrete variables  they may be defined in form of conditional probability tables 
a bn is a very compact representation of a distribution over x if all nodes have only few
parents or their conditional probability functions have significant local structure  this will
play a crucial role in our development of the graphical models for prada 
  

filang   toussaint

a dbn  murphy        extends the bn formalism to model a dynamic system evolving
over time  usually  the focus is on discrete time stochastic processes  the underlying
system itself  in our case  a world state  is represented by a bn b  and the dbn maintains
a copy of this bn for every time step  a dbn can be defined as a pair of bns  b    b   
where b  is a  deterministic or uncertain  prior which defines the state of the system at the
initial state t      and b is a two slice bn which defines the dependencies between two
successive time steps t and t      this implements a first order markov assumption  the
variables at time t     depend only on other variables at time t     or on variables at t 

   planning with look ahead trees
to plan with nid rules  one can treat the domain described by the
ulary as a relational markov decision process as discussed in sec 
we present two value based reinforcement learning algorithms which
generative model to build look ahead trees starting from the initial
used to estimate the values of actions and states 

relational logic vocab     in the following 
employ nid rules as a
state  these trees are

    sparse sampling trees
the sparse sampling tree  sst  algorithm  kearns et al         for mdp planning samples
randomly sparse  but full grown look ahead trees of states starting with the given state as
root  this suffices to compute near optimal actions for any state of an mdp  given a
planning horizon d and a branching factor b  sst works as follows  see fig      in each tree
node  representing a state    i  sst takes all possible actions into account  and  ii  for each
action it takes b samples from the successor state distribution using a generative model for
the transitions  e g  the transition model t of the mdp  to build tree nodes at the next
level  values of the tree nodes are computed recursively from the leaves to the root using
the bellman equation  in a given node  the q value of each possible action is estimated
by averaging over all values of the b children states for this action  then  the maximizing
q value over all actions is chosen to estimate the value of the given node  sst has the
favorable property that it is independent of the total number of states of the mdp  as it
only examines a restricted subset of the state space  nonetheless  it is exponential in the
time horizon taken into account 
pasula et al         apply sst for planning with nid rules  when sampling the noise
outcome while planning with sst  they assume to stay in the same state  but discount
the estimated value  we refer to this adaptation when we speak of sst planning in the
remainder of the paper  if an action does not have a unique covering rule  we use the noisy
default rule r to predict its effects  it is always better to perform a don othing action
instead where staying in the same state does not get punished  hence  in sst planning one
can discard all actions for a given state which do not have unique covering rules 
while sst is near optimal  in practice it is only feasible for very small branching factor
b and planning horizon d  let the number of actions be a  then the number of nodes at
horizon d is  ba d    this number can be reduced if the same outcome of a rule is sampled
multiple times   as an illustration  assume we have    possible actions per time step and
set parameters d     and b      the choice of pasula et al  in their experiments   to plan a
single action for a given state  one has to visit                        states  while smaller
  

fiplanning with noisy probabilistic relational rules

figure    the sst planning algorithm samples sparse  but full grown look ahead trees to
estimate the values of actions and states 

choices of b lead to faster planning  they result in a significant accuracy loss in realistic
domains  as kearns et al  note  sst is only useful if no special structure that permits
compact representation is available  in sec     we will introduce an alternative planning
approach based on approximate inference that exploits the structure of nid rules 
    sampling trees with upper confidence bounds
the upper confidence bounds applied to trees  uct  algorithm  kocsis   szepesvari 
      also samples a search tree of subsequent states starting with the current state as root 
in contrast to sst which generates b successor states for every action in a state  the idea of
uct is to choose actions selectively in a given state and thus to sample selectively from the
successor state distribution  uct tries to identify large subsets of suboptimal actions early
in the sampling procedure and to focus on promising parts of the look ahead tree instead 
uct builds its look ahead tree by repeatedly sampling simulated episodes from the
initial state using a generative model  e g  the transition model t of the mdp  an episode is a
sequence of states  rewards and actions until a limited horizon d  s    r    a    s    r    a        sd   rd  
after each simulated episode  the values of the tree nodes  representing states  are updated
online and the simulation policy is improved with respect to the new values  as a result  a
distinct value is estimated for each state action pair in the tree by monte carlo simulation 
more precisely  uct follows the following policy in tree node s  if there exist actions
from s which have not been explored yet  then uct samples one of these using a uniform
distribution  otherwise  if all actions have been explored at least once  then uct selects
the action that maximizes an upper confidence bound qo
u ct  s  a  on the estimated action
  

filang   toussaint

value qu ct  s  a  
s
qo
u ct  s  a 

  qu ct  s  a    c

log ns
 
ns a

u ct  s    argmax qo
u ct  s  a   

    
    

a

where ns a counts the number of times that actionpa has been selected from state s  and ns
counts the total number of visits to state s  ns   a ns a   the bias parameter c defines the
influence of the number of previous action selections and thereby controls the extent of the
upper confidence bound 
at the end of an episode  the value of each encountered state action pair  st   at      
t   d  is updated using the total discounted rewards 
nst  at  nst  at      
qu ct  st   at    qu ct  st   at    

    
 
nst  at

d
x

 

 

 t t rt   qu ct  st   at     

    

t   t

the policy of uct implements an exploration exploitation tradeoff  it balances between
exploring currently suboptimal looking actions that have been selected seldom thus far and
exploiting currently best looking actions to get more precise estimates of their values  the
total number of episodes controls the accuracy of ucts estimates and has to be balanced
with its overall running time 
uct has achieved remarkable results in challenging domains such as the game of go
 gelly   silver         to the best of our knowledge  we are the first to apply uct for
planning in stochastic relational domains  using nid rules as a generative model  we adapt
uct to cope with noise outcomes in the same fashion as sst  we assume to stay in the
same state and discount the obtained rewards  thus  uct takes only actions with unique
covering rules into account  for the same reasons as sst does 

   planning with approximate inference
uncertain action outcomes characterize complex environments  but make planning in relational domains substantially more difficult  the sampling based approaches discussed in
the previous section tackle this problem by repeatedly generating samples from the outcome
distribution of an action using the transition probabilities of an mdp  this leads to lookahead trees that easily blow up with the planning horizon  instead of sampling successor
states  one may maintain a distribution over states  a so called belief  in the following 
we introduce an approach for planning in grounded stochastic relation domains which propagates beliefs over states in the sense of state monitoring  first  we show how to create
compact graphical models for nid rules  then we develop an approximate inference method
to efficiently propagate beliefs  with this in hand  we describe our probabilistic relational
action sampling in dbns planning algorithm  prada   which samples action sequences
in an informed way and evaluates these using approximate inference in dbns  then  an
example is presented to illustrate the reasoning of prada  finally  we discuss prada in
comparison to the approaches of the previous section  sst and uct  and present a simple
extension of prada 
  

fiplanning with noisy probabilistic relational rules

 a 

 b 

figure    graphical models for nid rules   a  naive dbn   b  dbn exploiting nid factorization

    graphical models for nid rules
decision theoretic problems where agents need to choose appropriate actions can be represented by means of markov chains and dynamic bayesian networks  dbns  which are
augmented by decision nodes to specify the agents actions  boutilier et al          in the
following  we discuss how to convert nid rules to dbns which the prada algorithm will
use to plan with probabilistic inference  we denote random variables by upper case letters
 e g  s   their values by the corresponding lower case letters  e g   s  dom s    variable
vectors by bold upper case letters  e g  s    s    s    s     and value vectors by bold lower
case letters  e g  s    s    s    s      we also use column notation  e g  s       s    s    s    
a naive way to convert nid rules to dbns is shown in fig    a   states are represented
by a vector s    s            sn   where for each ground predicate in p there is a binary si
and for each ground function in f there is an sj with range according to the represented
function  actions are represented by an integer variable a which indicates the action out
of a vector of ground action predicates in a  the reward gained in a state is represented
by u and may depend only on a subset of the state variables  it is possible to express
arbitrary reward expectations p  u   s  with binary u  cooper         how can we define
the transition dynamics using nid rules in this naive model  assume we are given a set of
fully abstract nid rules  we compute all groundings of these rules w r t  the objects of the
domain and get the set  of k different ground nid rules  the parents of a state variable
si  at the successor time step include the action variable a and the respective variable si
at the predecessor time step  the other parents of si  are determined as follows  for each
rule r   where the literal corresponding to si  appears in the outcomes of r  all variables
sk corresponding to literals in the preconditions of r are parents of si    as typically si  can
be manipulated by several actions which in turn are modeled by several rules  the total
number of parents of si  can be very large  this problem is worsened by the usage of deictic
references in the nid rules  as they increase the total number k of ground rules in   the
resulting local structure of the conditional probability function of si  is very complex  as one
has to account for the uniqueness of covering rules  these complex dependencies between
two time slices make this representation unfeasible for planning 
  

filang   toussaint

therefore  we exploit the structure of nid rules to model a state transition with the
compact graphical model shown in fig    b  representing the joint distribution
p  u    s    o  r     a  s 

 

p  u    s    p  s    o  r  s  p  o   r  p  r   a    p     s   

    

which we will explain in detail in the following  as before  assume we are given a set of
fully abstract nid rules  for which we compute the set  of k different ground nid rules
w r t  the objects in the domain  in addition to s  s    a  u and u   as above  we use a
binary random variable i for each rule to model the event that its context holds  which
is the case if all required literals hold  let i   be the indicator function which is   if the
argument evaluates to true and   otherwise  then  we have


k
k
y
y
 
p     s   
p  i  s i      
sj   sri  j   
    
i
i  

i  

j i  

v

we use i i to express a logical conjunction    n   the function    yields the set of
indices of the state variables in s  on which  depends  sri denotes the configuration of the
state variables corresponding to the literals in the context of ri   we use an integer valued
variable r ranging over k    possible values to identify the rule which predicts the effects
of the action  if it exists  this is the unique covering rule for the current state action pair 
i e   the only rule r   a  modeling action a whose context holds 


 
p  r   r a      i r   a   r     
r         
    
r   a   r 

if no unique covering rule exists  we predict no changes as indicated by the special value
r      assuming not to execute the action  similarly as sst and uct do  


 
 
p  r       a     
i r     
r        
    
r   a   r 

r a 

the integer valued variable o represents the outcome of the action as predicted by the
rule  it ranges over m possible values where m is the maximum number of outcomes all
rules in  have  to ensure a sound semantics  we introduce empty dummy outcomes with
zero probability for those rules whose number of outcomes is less than m   the probability
of an outcome is defined as in the corresponding rule 
p  o   o   r    pr o  
we define the probability of the successor state as
y
p  s    o  s  r   
p  s i   o  si   r   

    

    

i

which is one for the unique state that is constructed from s taking the changes according
to r o into account  if outcome o specifies a value for si    this value will have probability
  

fiplanning with noisy probabilistic relational rules

one  otherwise  the value of this state variable persists from the previous time step  as
rules usually change only a small subset of s  persistence most often applies  the resulting
dependency p  s i   o  r  si   of a variable si  at time step t     is compact  in contrast to the
naive dbn in fig    a   it has only three parents  namely the variables for the outcome 
the rule and its predecessor at the previous time step  this simplifies the specification of
a conditional probability function for s   significantly and enables efficient inference  as we
will see later  the probability of the reward is given by


 
p  u         s      i 
sj    j   
    
j u    

the function  u     yields the set of indices of the state variables in s    on which u   depends 
the configuration of these variables that corresponds to our planning goal is denoted by
   uncertain initial states can be naturally accounted for by specifying priors p  s     we
renounce the specification of a prior here  however  as the initial state s  will always be given
in our experiments later to enable comparison to the look ahead tree based approaches sst
and uct which require deterministic initial states  which might also be sampled from a
prior   our choice for the distribution p  a  used for sampling actions will be described in
sec      
for simplicity we have ignored derived predicates and functions which are defined in
terms of other predicates or functions in the presentation of our graphical model  derived
concepts may increase the compactness of rules  if dependencies among concepts are acyclic 
it is straightforward to include derived concepts in our model by intra state dependencies
for the corresponding variables  indeed  we will use derived predicates in our experiments 
we are interested in inferring posterior state distributions p  st   a  t    given the sequence of previous actions  where we omit conditioning on the initial state for simplicity  
exact inference is intractable in our graphical model  when constructing a junction tree 
we will get cliques that comprise whole markov slices  all variables representing the state at
a certain time step   consider eliminating all state variables st     due to moralization  the
outcome variable o will be connected to all state variables in st   after elimination of o 
all variables in st will form a clique  thus  we have to make use of approximate inference
techniques  general loopy belief propagation  lbp  is unfeasible due to the deterministic
dependencies in small cycles which inhibit convergence  we also conducted some preliminary tests in small networks with a damping factor  but without success  it is an interesting
open question whether there are ways to alternate between propagating deterministic information and running lbp on the remaining parts of the network  e g   whether methods such
as mc sat  poon   domingos        can be successfully applied in decision making contexts as ours  in the next subsection  we propose a different approximate inference scheme
using a factored frontier  ff   the ff algorithm describes a forward inference procedure
that computes exact marginals in the next time step subject to a factored approximation
of the previous time step  here  our advantage is that we can exploit the structure of the
involved dbns to come up with formulas for these marginals  ff is related to passing only
forward messages  in contrast to lbp  information is not propagated backwards  note that
our approach does not condition on rewards  as in full planning by inference  and samples
actions  so that backward reasoning is uninformative 
  

filang   toussaint

    approximate inference
in the following  we present an efficient method for approximate inference in the previously
proposed dbns exploiting the factorization of nid rules  we focus on the mathematical
derivations  an illustrative example will be provided in sec      
we follow the idea of the factored frontier  ff  algorithm  murphy   weiss        and
approximate the belief with a product of marginals 
y
p  st   a  t    
p  sti   a  t     
    
i

we define
 sti      p  sti   a  t    and
 st      p  st   a  t    

n
y

    
 sti  

    

i  

and derive a ff filter for the dbn model in fig    b   we are interested in inferring the
state distribution at time t     given an action sequence a  t and calculate the marginals of
the state attributes as
t  
 st  
  a  t  
i     p  si
x
 
p  st  
  rt   a  t    p  rt   a  t    
i

    
    

rt

in eq        we use all rules for prediction  weighted by their respective posteriors p  rt   a  t   
this reflects the fact that depending on the state we use different rules to model the same
action  the weight p  rt   a  t   is   for all rules not modeling action at   for the remaining
rules which do model at   the weights correspond to the posterior over those parts of the
state space where the according rule is used for prediction 
we compute the first term in      as
x
p  st  
  rt   a  t     
p  st  
  rt   sti   p  sti   rt   a  t   
i
i
sti



x

p  st  
  rt   sti    sti    
i

    

sti

here  we sum over all possible values of the variable si at the previous time step t  intuitively  we take into account all potential pasts to arrive at value st  
at the next
i
t   st   enables us to easily predict the probabilities
time step  the resulting term p  st  
 
r
i
i
at the next time step as discussed below  each such prediction is weighted by the marginal
 sti   of the respective previous value  the approximation in      assumes that sti is conditionally independent of rt   this is not true in general as the choice of a rule for prediction
depends on the current state and thus also on attribute si   to improve on this approximation one can examine whether sti is part of the context of rt   if this is the case  we can infer
the state of sti from knowing rt   however  we found our approximation to be sufficient 
  

fiplanning with noisy probabilistic relational rules

as one would expect  we calculate the successor state distribution p  st  
  rt   sti   by
i
t
taking the different outcomes o of r into account weighted by their respective probabilities
p  o   rt   
x
p  st  
  rt   sti    
p  st  
  o  rt   sti   p  o   rt    
    
i
i
o

this shows us how to update the belief over sit   if we predict with rule rt   p  st  
  o  rt   sti  
i
t  
is a deterministic distribution  if o changes the value of si   si is set accordingly  otherwise  the value sti persists 
lets turn to the computation of the second term in eq        p  rt   a  t    the posterior
over rules  the trick is to use the context variables  and to exploit the assumption that a
rule r models the state transition if and only if it uniquely covers  at   st    which is indicated
by an appropriate assignment of the   this can then be further reduced to an expression
involving only the marginals     we start with
x
p  rt   r   a  t    
p  rt   r   t   a  t   p  t   a  t  
t




 

  i r   at    p tr     

tr        a  t  

r   at    r 




 

  i r   at    p  tr       a  t    p 

tr        tr      a  t    

r   at    r 

    
to simplify the summation over t   we only have to consider the unique assignment of the
context variables when r is used for prediction  provided it models the action  as indicated
by i r   at     this is the case if its context tr holds  while the contexts tr  of all other
competing rules r  for action at do not hold 
we calculate the second term in      by summing over all states s as
x
y
x
p  tr       a  t     
p  tr       st    st   
p  tr       st  
 stj  
    
st

 

y

st

 sjt   sr j  

 

j

    

j tr  

the approximation in      is the ff assumption  in       sr denotes the configuration of
the state variables according to the context of r like in       we sum out all variables not in
the context of r  only the variables in rs context remain  the terms  sjt   sr j   correspond
to the probabilities of the respective literals 
the third term in      is the joint posterior over the contexts of the competing rules r 
given that rs context already holds  we are interested in the situation where none of these
other contexts hold  we calculate this as


 
y
p
tr        tr      a  t   
p  tr        tr      a  t     
    
r   at    r 

r   at    r 

  

filang   toussaint

approximating it by the product of the individual posteriors  the latter are computed as
x
p  tr        tr      a  t     
p  tr        st   p  st   tr      a  t   
    
t

s
if r r   
     q
t
      i t     si   sr   i   otherwise
 

r 
t

    

i  r  

where the if condition expresses a logical contradiction of the contexts of r and r    if their
contexts contradict  then r  s context will surely not hold given that rs context holds 
otherwise  we know that the state attributes apppearing in the contexts of both r and r 
do hold as we condition on r      therefore  we only have to examine the remaining state
attributes of r  s context  again  we approximate this posterior with the ff marginals 
finally  we compute the reward probability straightforwardly as
x
y
p  u t       st  p  st   a  t    s    
 sit   i    
    
p  u t       a  t     
st

i u t  

where  denotes the configuration of state variables corresponding to the planning goal as
in       as above  the summation over states is simplified by the ff assumption resulting
in a product of the marginals of the required state attributes 
the overall computational costs of propagating the effects of an action are quadratic in
the number of rules for this action  for each such rule we have to calculate the probability
that none of the others applies  and linear in the maximum numbers of context literals and
manipulated state attributes of those rules 
our inference framework requires an approximation for the distribution p  s    r     s 
 cf  eq       to cope with the noise outcome of nid rules  from the training data used to
learn rules  we estimate which predicates and functions change value over time as follows  let
sc  s contain the corresponding variables  we estimate for each rule r the average number
n r of changed state attributes when the noise outcome applies  due to our factored frontier
approach  we can consider the noise effects for each variable independently  we approximate
r
the probability that si  sc changes in rs noise outcome by   snc     in case of change  all
changed values of si have equal probability 
    planning
the dbn representation in fig    b  together with the approximate inference method described in the last subsection enable us to derive a novel planning algorithm for stochastic
relational domains  the probabilistic relational action sampling in dbns planning algorithm  prada  plans by sampling action sequences in an informed way based on predicted
beliefs over states and evaluating these action sequences using approximate inference 
more precisely  we sample sequences of actions a  t   of length t   for     t  t   we
infer the posteriors over states p  st   a  t    s    and rewards p  ut   a  t    s     in the sense
of filtering or state monitoring   then  we calculate the value of an action sequence with a
discount factor          as
q s    a  t       

t
x

 t p  u t       a  t    s     

t  

  

    

fiplanning with noisy probabilistic relational rules

we choose the first action of the best sequence a   argmaxa  t   q a  t     s     if its
value exceeds a certain threshold   e g          otherwise  we continue sampling actionsequences until either an action is found or planning is given up  the quality of the found
plan can be controlled by the total number of action sequence samples and has to be traded
off with the time that is available for planning 
we aim for a strategy to sample good action sequences with high probability  we
propose to choose with equal probability among the actions that have a unique covering
rule for the current state  thereby  we avoid the use of the noisy default rule r which
models action effects as noise and is thus of poor use in planning  for the action at time t 
prada samples from the distribution


 
x
t
tr        a  t   
    
p tr     
psample
 a  
r a 

r   a   r 

this is a sum over all rules for action a  for each such rule we add the posterior that it is the
unique covering rule  i e  that its context tr holds  while the contexts tr  of the competing
rules r  do not hold  this sampling distribution takes the current state distribution into
account  thus  the probability to sample an action sequence a predicting the state sequence
s            st depends on the likelihood of the state sequence given a  the more likely the required outcomes are  the more likely the next actions will be sampled  using this policy 
prada does not miss actions which sst and uct explore  as the following proposition
states  proof in appendix a  
proposition    the set of action sequences prada samples with non zero probability
is a super set of the ones of sst and uct 
in our experiments  we replan after each action is executed without reusing the knowledge of previous time steps  this simple strategy helps to get a general impression of
pradas planning performance and complexity  other strategies are easily conceivable 
for instance  one might execute the entire sequence without replanning  trading off faster
computation times with a potential loss in the achieved reward  in noisy environments  it
might seem a better strategy to combine the reuse of previous plans with replanning  for
instance  one could omit the first action of the previous plan  which has just been executed 
and examine the suitability of the remaining actions in the new state  while we consider
only the single best action sequence  in many planning domains it might also be beneficial
to marginalize over all sequences with the same first action  for instance  an action a 
might lead to a number of reasonable sequences  none of which are the best  while another
action a  is the first of one very good sequence  but also many bad ones  in which case one
might favor a   
    illustrative example
let us consider the small planning problem in table   to illustrate the reasoning procedure
of prada  our domain is a noisy cubeworld represented by predicates table x   cube x  
on x  y    inhand x  and clear x   y on y  x  where a robot can perform two types
of actions  it may either lift a cube x by means of action grab x  or put the cube which is
  

filang   toussaint

held in hand on top of another object x using puton x   the start state s  shown in   a 
contains three cubes a  b and c stacked in a pile on table t  the goal shown in   b  is to
get the middle cube b on top of the top cube a  our world model provides three abstract
nid rules to predict action effects  shown in table   c   only the first rule has uncertain
outcomes  it models to grab an object which is below another object  in contrast  grabbing
a clear object  rule    and putting an object somewhere  rule    always leads to the same
successor state 
first  prada constructs a dbn to represent the planning problem  for this purpose 
it computes the grounded rules with respect to the objects o    a  b  c  t  shown in   d  
most potential grounded rules can be ignored  one can deduce from the abstract rules which
predicates are changeable  in combination with the specifications in s    this prunes most
grounded rules  for instance  we know from s  that t is the table  thus  no ground rule
with action argument x   t needs to be constructed as all rules require cube x  
based on the dbn  prada samples action sequences and evaluates their expected
rewards  in the following  we investigate this procedure for the sampling of action sequence
 grab b   puton a    table   e  presents the inferred values of the dbn variables and
other auxiliary quantities  the marginals   eq        of the state variables at t     are
set deterministically according to s    we calculate the posteriors over context variables
p     a  t    according to eq        in our example  at t     there is one rule with
probability     for each of the actions grab a   grab b  and grab c   in contrast  there are
no rules with non zero probability for the various puton   actions  by the help of eq       
we calculate the probability of each rule r to be the unique covering rule for the respective
action  listed under unique rule  note that we do not condition on a fixed action at thus
far   this is the case if context r of r holds  while all contexts r  of the competing rules
r  for the same action do not hold  at t      this is the same as the posterior of r alone 
the resulting probabilities are used to calculate the sampling distribution of eq        first 
we compute the probability for each action to have a unique covering rule which is a simple
sum over probabilities of the previous step  listed under action coverage in the table   then 
we normalize these values to get a sampling distribution psample     at t      this results in
a sampling distribution which is uniform over the three actions with unique rules  assume
we sample a    grab b   grabbing blue cube b   variable r specifies the ground rules to
use for predicting the state marginals at the next time step  we can infer its posterior
according to eq        here  p  r        b act    a          
things get more interesting at t      here  we observe the effects of the factored
frontier  for instance  consider calculating the posterior over context r for ground rule
r       b att   grabbing blue cube b which is below yellow a  using eq       
p     b att    a      on a  b     on b  t     cube a     cube b     table t  
                                 
in contrast  the exact value is p     b att    a           according to the third outcome of
abstract rule   used to predict a    the imprecision is due to ignoring the correlations  ff
regards the marginals for on a  b  and on b  t  as independent  while in fact they are fully
correlated 
at t      the action grab a  has three ground rules with non zero context probabilities
 grabbing a from either b  c or t   this is due to the three different outcomes of abstract
  

fiplanning with noisy probabilistic relational rules

table    example of pradas factored frontier inference
 a  start state
s     on a  b   on b  c   on c  t  
cube a   cube b   cube c   table t  

 b  goal
    on b  a  

 c  abstract nid rules with example situations
rule   
grab x    on y  x   on x  z   cube x   cube y    table t  

       inhand x   on y  z   on y  x   on x  z 
      inhand x   on y  t    on y  x   on x  z 


      on x  t    on x  z 

rule   
grab x    cube x   clear x   on x  y  

      inhand x   on x  y  


 e  inferred posteriors in pradas
ff
inference
for
action sequence
 grab b   puton a  
t  

t  

t  

state marginals 
on a  b 
on a  c 
on a  t 
on b  a 
on b  c 
on b  t 
on c  t 
inhand b 
clear a 
clear b 
clear c 
goal u

   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
    
   
   
   
   

p     a  t   
   b act 
   b att 
   c btt 
   a b 
   a c 
   a t 
   b t 
   c t 
   a b 
   c b 
   t b 

   
   
   
   
   
   
   
   
   
   
   

   
    
   
   
   
   
    
   
   
   
   

unique rule
    b act 
   
   
    b att 
          
    c att 
   
    
    c btt 
   
   
    a b 
   
    
    a c 
   
    
    a t 
   
    
    b t 
         
    c t 
   
    
    a b 
   
   
    c b 
   
   
    t b 
   
   
action coverage
grab a 
   
    
grab b 
         
grab c 
   
   
puton a 
   
   
puton c 
   
   
puton t 
   
   
sample distribution
psample  grab a  
          
psample  grab b  
           
psample  grab c  
          
psample  puton a  
         
psample  puton c  
         
psample  puton t  
         
p  rt   rt   a  t  
rt       b act 
   
   
rt       a b 
   
   
rt    
   
   

rule   
puton x    inhand y    cube y  

      on y  x   inhand x 


 d  grounded nid rules
grounded rule action
substitution
    a bbt 
grab a   x  a  y  b  z  b  t  t 
    a bct 
grab a   x  a  y  b  z  c  t  t 
   
    c bbt 
grab c   x  c  y  b  z  b  t  t 
    a b 
grab a 
 x  a  y  b 
    a c 
grab a 
 x  a  y  c 
    a t 
grab a 
 x  a  y  t 
   
    c t 
grab c 
 x  c  y  t 
    a b 
puton a 
 x  a  y  b 
    a c 
puton a 
 x  a  y  c 
   
    t c 
puton t 
 x  a  y  c 

  

filang   toussaint

rule    as an example  we calculate the probability of rule     a c   grabbing a from c  to
be the unique covering rule for grab a  at t     as
p     a c      a b       a t    a   
 p     a c    a          p     a b    a           p     a t    a    
                                    
after some more calculations  we determine the sampling distribution at t      assume
we sample action puton a   this results in rule    a  b   putting b on a  being used for
prediction with     probability  since this is its probability to be the unique covering rule
for action puton a   the remaining mass     of the posterior is assigned to those parts of
the state space where no unique covering rule is available for puton a   in this case  we use
the default rule r      corresponding to not performing the action  so that with probability
    the values of the state variables persist 
finally  let us infer the marginals at t     using eq        as an example  we calculate
 inhand b t      let i b  be brief for inhand b   we sum over the ground rules rt   taking
the potential values i b t   and i b t   at the previous time step t     into account 
x
 i b t     
p  rt     a        p  i b t     rt     i b t      i b t    
rt  

  p  i b t     rt     i b t      i b t      
                                                                
as discussed above  only the ground rule    a  b  and the default rule play a role in this
prediction  in effect  the belief that b is inhand decreases from     to      after having tried
to put b on a  as expected  similarly  we calculate the posterior of on b  a  as      this is
also the expected probability to reach the goal when performing the actions grab b  and
puton a    here  pradas inferred value coincides with the true posterior  
for comparison  the probability to reach the goal is     when performing the actions
grab a   puton t   grab b  and puton a   i e   when we clear b before we grab it  this plan
is safer  i e   has higher probability  but takes more actions 
    comparison of the planning approaches
the most prominent difference between the presented planning approaches is in their way
to account for the stochasticity of action effects  on the one hand  sst and uct repeatedly take samples from successor state distributions and estimate the value of an action by
building look ahead trees  on the other hand  prada maintains beliefs over states and
propagates indetermistic action effects forward  more precisely  prada and sst follow
opposite approaches  prada samples actions and calculates the state transitions approximately by means of probabilistic inference  while sst considers all actions  and thus is exact
in its action search  and samples state transitions  the price for considering all actions is
ssts overwhelmingly large computational cost  uct remedies this issue and samples action sequences and thus state transitions selectively  it uses previously sampled episodes to
build upper confidence bounds on the estimates for action values in specific states  which
are used to adapt the policy for the next episode  it is not straightforward to translate
  

fiplanning with noisy probabilistic relational rules

this adaptive policy to prada since prada works on beliefs over states instead of states
directly  therefore  we chose the simple policy for prada to sample randomly from all
actions with a unique covering rule in a state  in the form of a sampling distribution to
account for beliefs over states  
prada returns a whole plan that will transform the world state into one where the goal
is fulfilled with a probability exceeding a given threshold   in the spirit of conformant planning or probabilistic planning with no observability  kushmerick  hanks    weld        
due to their outcome sampling  sst and uct cannot return such a plan in a straightforward way  instead  they provide a policy for many successor states based on their estimates
of the action values in their look ahead tree  the estimates of states deeper in the tree are
less reliable as they have been built from less episodes  if an action has been executed and a
new state is observed  these estimates can be reused  thus far  prada does not take any
knowledge gained in previous action sequence samples into account to adapt its policy  an
elegant way to achieve this and to better exploit goal knowledge might use backpropagation
through our dbns to plan completely by inference  toussaint   storkey         this is
beyond the scope of this paper  as it is not clear how to do this in a principled way in the
large state and action spaces of relational domains  alternatively  prada could give high
weight to the second action of the previous best plan  below in sec       we show another
simple way to make use of previous episodes to find better plans 
prada can afford its simple action sampling strategy as it evaluates large numbers
of action sequences efficiently and does not have to grow look ahead trees to account for
indeterministic effects  this points at an important difference  all three algorithms are faced
with search spaces of action sequences which are exponential in the horizon  to calculate
the value of a given action sequence  however  sst and uct still need exponential time due
to their outcome sampling  in contrast  prada propagates the state transitions forward
and thus is linear in the horizon 
like all approximate planning algorithms  neither sst  uct nor prada can be expected to perform ideally in all situations  sst and uct sample action outcomes and hence
face problems if important outcomes only have small probability  for instance  consider an
agent that wants to escape a room with two locked doors  if it hits the first door which is
made of wood it has a chance of      to break it and escape  the second door is made of
iron and has only a chance of       to break  sst and uct may take a very long time to
detect that it is    times better to repeatedly hit the wooden door  in contrast  prada
recognizes this immediately after having reasoned about each of the actions once as it takes
all outcomes into account  on the other hand  in pradas approximate inference procedure the correlations among state variables get lost while sst and uct preserve them as
they sample complete successor states  this can impair pradas planning performance in
situations where correlations are crucial  consider the following simple domain with two
state attributes a and b  the agent can choose from two actions modeled by the rules

action   
action   






      a  b
  and
      a  b



      a  b
      b  a




  

 

filang   toussaint

the goal is to make both attributes either true or false  i e       a  b    a  b   for
both actions  the resulting marginals will be  a          a          b        and
 b         due to its factored frontier  prada cannot distinguish between both actions
although action  will achieve the goal  while action  will not 
pradas estimated probabilities of states and rewards may differ significantly from
their true values  this does not harm its performance in many domains as our experiments
indicate  sec      we suppose the reason for this is that while pradas estimated probabilities can be imprecise  they enable a correct ranking of action sequences  and in planning 
we are interested in choosing the best action instead of calculating correctly its value 
a further difference between the proposed algorithms is in their way to handle the noise
outcome of rules  prada assigns very small probability to all successor states  in the spirit
of the noise outcome  in contrast  for sst and uct it does not make sense to sample from
such a distribution  as any single successor state has extremely low probability and will be
inadequate to estimate state and action values  hence  they use the described workaround
to assume to stay in the same state  while discounting obtained rewards 
it is straightforward for prada to deal with uncertain initial states  uncertainty of
initial states is common in complex environments and may for instance be caused by partial
observability or noisy sensors  this uncertainty has its natural representation in the belief
state prada works on  in contrast  sst and uct cannot account for uncertain initial
states directly  but would have to sample from the prior distribution 
    an extension  adaptive prada
we present a very simple extension of prada to increase its planning accuracy  we
exploit the fact that prada evaluates complete sequences of actions  in contrast to sst
and uct where the actions taken at t     depend on the sampled outcomes  adaptive
prada  a prada  examines the best action sequence found by prada  while prada
chooses the first action of this sequence without further reasoning  a prada inspects each
single action of this sequence and decides by simulation whether it can be deleted  the
resulting shortened sequence may lead to an increased expected reward  this is the case if
actions do not have significant effects on achieving the goal or if they decrease the success
probability  if such actions are omitted  the states with high reward are reached earlier and
their rewards are discounted less  for instance  consider the goal to grab a blue ball  an
action sequence that grabs a red cube  puts it onto the table and only then grabs the blue
ball can be improved by omitting the first two actions which are unrelated to the goal 
more precisely  a prada takes pradas action sequence ap with the highest value
and investigates iteratively for each action whether it can be deleted  an action can be
deleted from the plan if the resulting plan has a higher reward likelihood  this idea is
formalized in algorithm    the crucial calculation of this algorithm is to compute values
q s    a  t     as defined in eq       and restated here for convenience 
 

q s   a

  t  

  

t
x

 t p  u t       a  t    s     

t  

pradas approximate inference procedure is particularly suitable for calculating all required p  u t       a  t    s     it performs this calculation in time linear in the length t of
  

fiplanning with noisy probabilistic relational rules

algorithm   adaptive prada  a prada 
input  pradas plan ap
output  a pradas plan aa
   aa  ap
   for t     to t   t    do
  
while true do
  
let a be a plan of length t  
  
a  t   a  t 
b omit at
a
t   t  
t t  
  
a
 aa
  
at    don othing
  
if q s    a    q s    aa   then
  
aa  a
   
else
   
break
   
end if
   
end while
    end for
    return aa

the action sequence  while sst and uct would require time exponential in t because of
their outcome sampling 

   evaluation
we have implemented all presented planning algorithms and the learning algorithm for
nid rules in c    our code is available at www user tu berlin de lang prada   we
evaluate our approaches in two different scenarios  the first is an intrinsically noisy complex simulated environment where we learn nid rules from experience and use these to
plan  second  we apply our algorithms on the benchmarks of the uncertainty part of the
international planning competition      
    simulated robot manipulation environment
we perform experiments in a simulated complex robot manipulation environment where a
robot manipulates objects scattered on a table  fig      before we report our results in three
series of experiments on different tasks of increasing difficulty  we first describe this domain
in detail  we use a  d rigid body dynamics simulator  ode  that enables a realistic behavior of the objects  this simulator is available at www user tu berlin de lang dwsim  
objects are cubes and balls of different sizes and colors  the robot can grab objects and
put them on top of other objects or on the table  the actions of the robot are affected by
noise  in this domain  towers of objects are not straight lined  it is easier to put an object
on top of a big cube than on top of a small cube while it is difficult to put something on
top of a ball  piles of objects may topple over  objects may fall off the table in which case
they become out of reach for the robot 
we represent this domain with predicates on x  y    inhand x   upright x   out x   if
an object has fallen off the table   function size x  and unary typing predicates cube x  
ball x   table x   these predicates are obtained by querying the state of the simulator and
  

filang   toussaint

figure    a simulated robot plays with cubes and balls of different sizes scattered on a
table  objects that have fallen off the table cannot be manipulated anymore 

translating it according to simple hand made guidelines  thereby sidestepping the difficult
problem of converting the agents observations into an internal representation  for instance 
on a  b  holds if a and b exert friction forces on each other and as z coordinate is greater
than the one of b  while their x  and y coordinates are similar  besides these primitive
concepts  we also use the derived predicate clear x   y on y  x   we found this
predicate to enable more compact and accurate rules  which is reflected in the values of the
objective function of the rule learning algorithm given in eq      
we define three different types of actions  these actions correspond to motor primitives
whose effects we want to learn and exploit  the grab x  action triggers the robot to open
its hand  move its hand next to x  let it grab x and raise the robot arm again  the
execution of this action is not influenced by any further factors  for example  if a different
object y has been held in the hand before  it will fall down on either the table or a third
object just below y   if there are objects on top of x  these are very likely to fall down 
the puton x  action centers the robots hand at a certain distance above x  opens it and
raises the hand again  for instance  if there is an object z on x  the object y that was
potentially inhand may end up on z or z might fall off x  the don othing   action triggers
no movement of the robots arm  the robot might choose this action if it thinks that any
other action could be harmful with respect to its expected reward  we emphasize again
that actions always execute  regardless of the state of the world  also  actions which are
rather unintuitive for humans such as trying to grab the table or to put an object on top of
itself are carried out  the robot has to learn by itself the effects of such motor primitives 
due to its intrinsic noise and its complexity  this simulated robot manipulation scenario
is a challenging domain for both learning compact world models as well as planning  if there
are o objects and f different object sizes  the action space contains  o   actions while the
 
state space is huge with f o  o   o different states  not excluding states one would classify as
impossible given some intuition about real world physics  
we use the rule learning algorithm of pasula et al         with the same parameter
settings to learn three different sets of fully abstract nid rules  each rule set is learned
  

fiplanning with noisy probabilistic relational rules

from independent training sets of     experience triples  s  a  s    that specify how the world
changed from state s to successor state s  when an action a was executed  assuming full
observability  training data to learn rules are generated in a world of six cubes and four
balls of two different sizes by performing random actions with a slight bias to build high
piles  our resulting rule sets contain       and    rules respectively  these rule sets provide
approximate partial models to the true world dynamics  they generalize over the situations
of the experiences  but may not account for situations that are completely different from
what the agent has seen before  to enforce compactness and avoid overfitting  rules are
regularized  hence  the learning algorithm may sometimes favor to model rarely experienced
state transitions as low probability outcomes in more general rules  thereby trading off
accuracy for compactness  this in combination with the general noisiness of the world
causes the need to carefully account for the probabilities of the world when reasoning with
these rules 
we perform three series of experiments with planning tasks of increasing difficulty  in
each series  we test the planners in different worlds with varying numbers of cubes and
balls  thus  we transfer the knowledge gained in the training world to different  but similar
worlds by using abstract nid rules  for each object number  we create five different worlds 
per rule set and world  we perform three independent runs with different random seeds 
to evaluate the different planning approaches  we compute the mean performances and
planning times over the fixed  but randomly generated  set of    trials    learned rule sets 
  worlds    random seeds  
we choose the parameters of the planning algorithms as follows  for sst  we report results for different branching factors b  as far as the resulting runtimes allow  similarly  uct
and  a  prada each have a parameter that balances their planning time and the quality
of their found actions  for uct  this is the number of episodes  while for  a  prada
this is the number of sampled action sequences  depending on the experiment  we set both
heuristically such that the tradeoff between planning time and quality is reasonable  in
particular  for a fair comparison we pay attention that uct  prada and a prada get
about the same planning times  if not reported otherwise  furthermore  for uct we set
the bias parameter c to     which we found heuristically to perform best  for all planners
and experiments  we set the discounting factor for future rewards to          a crucial
parameter is the planning horizon d  which heavily influences planning time  of course  d
cannot be known a priori  therefore  if not reported otherwise  we deliberately set d larger
than required for uct and  a  prada to suggest that our algorithms are also effective
when d can only be estimated  indeed  we found in all our experiments that as long as d is
not too small  its exact choice does not have significant effects on ucts and  a  pradas
planning quality  unlike its effects on planning times  in contrast  we set the horizon d
for sst always as small as possible  in which case its planning times are still very large 
if a planning algorithm does not find a suitable action in a given situation  we restart the
planning procedure  sst builds a new tree  uct runs more episodes and  a  prada
takes new action sequence samples  if in a given situation after    planning runs a suitable
action still is not found  the trial fails 
furthermore  we use ff replan  yoon et al         as a baseline  as we discuss in
more detail with the related work in sec     ff replan determinizes the planning problem 
thereby ignoring outcome probabilities  ff replan has shown impressive results on the
  

filang   toussaint

domains of the probabilistic planning competitions  these domains are carefully designed
by humans  their action dynamics definitions are complete  accurate and consistent and are
used as the true world dynamics in the according experiments  in contrast to the learned
nid rules we use here which estimate approximate partial models of our robot manipulation
domain  to be able to use the derived predicate clear x  in the ff replan implementation
of our experiments  we included the appropriate literals of this predicate by hand in the
outcomes of the rules  while our sst  uct and  a  prada implementations infer these
values automatically from the definition of clear x   we report results of ff replan with
these  almost original  learned rules using the all outcomes determinization scheme  denoted
by ff replan all below   using single outcome schemes always led to worse performance  
some of these rules are very general  putting only few restrictions on the arguments and
deictic references   in this case  more actions appear applicable in a given state than make
sense from an intuitive human perspective which hurts ff replan much more than the other
methods  resulting in large planning times for ff replan  for instance  a rule may model
the toppling over of a small tower including object x when trying to put an object y on top
of the tower  one outcome might specify y to end up below x  while this is only possible
if y is a cube  of course  the learning algorithm may choose to omit a typing predicate
cube x  due to regularization  as it prefers compact rules and none of its experiences might
require this additional predicate  therefore  we created modified rule sets by hand where we
introduced typing predicates where appropriate to make contexts more distinct  below  we
denote our results with these modified rule sets as ff replan all  and ff replan single  
using all outcomes and single most probable outcome determinization schemes 
      high towers
in our first series of experiments  we investigate building high towers which was the planning
task in the work of pasula et al          more precisely  the reward in a state is defined as
the average height of objects  this constitutes an easy planning problem as many different
actions may increase the reward  object identities do not matter  and a small planning
horizon d is sufficient  we set sst to horizon d      pasula et al  s choice  with different
branching factors b and uct and  a  prada to horizon d      in our experiments  initial
states do not contain already stacked objects  so the reward for performing no actions is
   table   and fig    present our results  sst is not competitive  for a branching factor
b      it is slower than uct and  a  prada by at least an order of magnitude  for
b      its performance is poor  in this series of experiments  we designed the worlds of   
objects to contain many big cubes  this explains the relatively good performance of sst in
these worlds  as the number of good plans is large  as mentioned above  we control uct 
prada and a prada to have about the same times available for planning  all three
approaches perform far better than sst in almost all experiments  the difference between
uct  prada and a prada is never significant 
this series of experiments indicates that planning approaches using full grown lookahead trees like sst are inappropriate even for easy planning problems  in contrast  approaches that exploit look ahead trees in a clever way such as uct seem to be the best
choice for easy tasks which require a small planning horizon and can be solved by many
alternative good plans  the performance of the planning approaches using approximate
  

fiplanning with noisy probabilistic relational rules

table    high towers problem  reward denotes the discounted total reward for different
numbers of objects  cubes balls and table   the reward for performing no actions
is    all data points are averages over    trials created from   learned rule sets 
  worlds and   random seeds  standard deviations of the mean estimators are
shown  ff replan all  and ff replan single  use hand made modifications of
the original learned rule sets  fig    visualizes these results 
objects

planner
ff replan all
ff replan all 
ff replan single 

   

sst  b   
sst  b   
sst  b   
uct
prada
a prada

sst  b   
sst  b   
sst  b   
uct
prada
a prada

sst  b   
sst  b   
sst  b   
uct
prada
a prada

          
          
          

           
          
          

    
    
    
    
    
    

          
             
             
          
          
          

          
          
          

            
            
          








    
    
    
    
    
    

           
           
             
           
           
           

          
          
          

             
            
          








             
             
         
           
           
           

    
     
     
     
     
     

ff replan all
ff replan all 
ff replan single 
    

trial time  s 

     
     
     
     
     
     

ff replan all
ff replan all 
ff replan single 
   

reward

     
     
     
     
     
     

  








    
    
    
    
    
    

filang   toussaint

ff replan all
ff replan all 
ff replan single 
sst b  
sst b  
sst b  
uct
prada
a prada

  
  
 

    

trial time  s 

discounted total reward

     

   
  
 

 

 
objects

  

 

 a  reward

 
objects

  

 b  time

figure    high towers problem visualization of the results presented in table    the reward
for performing no actions is    all data points are averages over    trials created
from   learned rule sets    worlds and   random seeds  error bars for the standard
deviations of the mean estimators are shown  please note the log scale in  b  

inference  prada and a prada  however  comes close to the one of uct  showing also
their suitability for such scenarios 
ff replan focuses on exploiting conjunctive goal structures and cannot deal with quantified goals  as the grounded reward structure of this task consists of a disjunction of
different tower combinations  ff replan has to pick an arbitrary tower combination as its
goal  therefore  to apply ff replan we sample tower combinations according to the rewards they achieve  i e   situations with high towers are more probable  and do not exclude
combinations with balls at the bottom of towers as they are not prohibited by the reward
structure  as yoon et al  note  the obvious pitfall of this  goal formula sampling  approach
is that some groundings of the goal are not reachable or are much more expensive to reach
from the initial state  when ff replan cannot find a plan  we do not execute an action 
but sample a new ground goal formula at the next time step  preserving already achieved
tower structures 
ff replan performs significantly worse than the previous planning approaches  the
major reason for this is that ff replan often comes up with plans exploiting low probability
outcomes of rules  in contrast to sst  uct and  a  prada which reason over the
probabilities  to illustrate this  consider the example rule in fig    which models putting
a ball on top of a cube  it has two explicit outcomes  the ball usually ends up on the
cube  sometimes  however  it falls on the table  ff replan can misuse this rule as a tricky
way to put a ball on the table  ignoring that this often will fail  as the results of ffreplan single  show  taking only most probable outcomes into account does not remedy
this problem  there are often two to three outcomes with similar probabilities so such a
choice seems unjustified  sometimes  the intuitively expected outcome is split up into
different outcomes with low probabilities  which however vary only in features irrelevant for
the planning problem  such as upright    
  

fiplanning with noisy probabilistic relational rules

table    desktop clearance problem  reward denotes the discounted total reward for different numbers of objects  cubes balls and table   the reward for performing no
actions is    all data points are averages over    trials created from   learned rulesets    worlds and   random seeds  standard deviations of the mean estimators
are shown  ff replan all  and ff replan single  use hand made modifications
of the original learned rule sets  fig    visualizes these results 
obj 

planner
ff replan all
ff replan all 
ff replan single 

   

sst  b   
uct
prada
a prada

sst  b   
uct
prada
a prada

sst  b   
uct
prada
a prada

          
          
          

         
        
        

    
    
    
    

            
         
         
         

          
          
          

         
        
        






    
    
    
    

         
          
          
          

          
          
          

          
         
        


           
           
           

   h
          
          
          

    
     
     
     

ff replan all
ff replan all 
ff replan single 
    

trial time  s 

    
    
     
     

ff replan all
ff replan all 
ff replan single 
   

reward






      desktop clearance
the task in our second series of experiments is to clear up the desktop  objects are lying
splattered all over the table in the beginning  an object is cleared if it is part of a tower
containing all other objects of the same class  an object class is simply defined in terms of
color which is additionally provided to the state representation of the robot  the reward of
the robot is defined as the number of cleared objects  in our experiments  classes contain
    objects with at most   ball  in order to enable successful piling   our starting situations contain some piles  but only with objects of different classes  thus  the reward for
performing no actions is    desktop clearance is more difficult than building high towers 
as the number of good plans yielding high rewards is significantly reduced 
we set the planning horizon d     optimal for sst which is required to clear up a
class of   objects  namely grabing and putting three objects  as above  by contrast we set
d      for uct and  a  prada to show that they can deal with overestimation of the
usually unknown optimal horizon d  table   and fig    present our results  the horizon
d     overburdens sst as can be seen from its large planning times  even for b      sst
takes almost    minutes on average in worlds of   objects  while over   hours in worlds of
  objects  therefore  we did not try sst for greater b  in contrast  the planning times
  

fi  

     

  

ff replan all
ff replan all 
ff replan single 
sst b  
uct
prada
a prada

  
  
 
 
 
 
 

 
objects

    
trial time  s 

discounted total reward

lang   toussaint

   
  
 
 

  

 a  reward

 
objects

  

 b  time

figure    desktop clearance problem  visualization of the results presented in table    the
reward for performing no actions is    all data points are averages over    trials
created from   learned rule sets    worlds and   random seeds  error bars for the
standard deviations of the mean estimators are shown  note the log scale in  b  

of uct  prada and a prada  again controlled to be about the same and to enable
reasonable performance  are two orders of magnitude smaller  although overestimating the
planning horizon  for a trial they take on average about   s in worlds of   objects      
minutes in worlds of   objects and     minutes in worlds of    objects  nonetheless  uct 
prada and a prada perform significantly better than sst  in all worlds  prada and
a prada in turn outperform uct  in particular in worlds with many objects  a prada
finds the best plans among all planners  all planners gain more reward in worlds of   objects
in comparison to worlds of   objects  as the number of objects that can be cleared increases
as well as the number of classes and thus of good plans  the worlds of    objects contain
the same numbers of object classes like the worlds of   objects  but with more objects 
making planning more difficult 
overall  our findings in the desktop clearance experiments indicate that while sst is
inappropriate  uct achieves good performance in planning scenarios which require medium
planning horizons and where there are several  but not many alternative plans  approaches
using approximate inference like prada and a prada  however  seem to be more appropriate in such scenarios of intermediate difficulty 
furthermore  our results indicate that ff replan is inadequate for the clearance task 
we sample target classes randomly to provide a goal structure to ff replan  the tower
structure within a target class in turn is also randomly chosen  the bad performance of
ff replan is due to the reasons described in the previous experiments  in particular the
plans of ff replan often rely on low probability outcomes 
  

fiplanning with noisy probabilistic relational rules

table    reverse tower problem  the trial times and numbers of executed actions are given
for the successful trials for different numbers of objects  cubes and table   all
data points are averages over    trials created from   learned rule sets    worlds
and   random seeds  standard deviations of the mean estimators are shown  ffreplan all  and ff replan single  use hand made modifications of the original
learned rule sets 
objects

   

   

   

planner

success rate

trial time  s 

executed actions

ff replan all
ff replan all 
ff replan single 

    
    
    

        
         
        

          
         
         

sst  b   
sst  b   
uct
prada
a prada

    
    
    
    
    

    day
             
         
         

         
         
         

ff replan all
ff replan all 
ff replan single 

    
    
    

           
         

         
         

uct
prada
a prada

    
    
    

   h
         
         

         
         

ff replan all
ff replan all 
ff replan single 

    
    
    

            
           

         
         

prada
a prada

    
    

            
            

         
         

      reverse tower
to explore the limits of uct  prada and a prada  we conducted a final series of
experiments where the task is to reverse towers of c cubes which requires at least  c
actions  each cube needs to be grabbed and put somewhere at least once   apart from the
long planning horizon  this is difficult due to the noise in the simulated world  towers can
become unstable and topple over with cubes falling off the table  to decrease this noise
slightly to obtain more reliable results  we forbid the robot to grab objects that are not clear
 i e   below other objects   we set a limit of    executed actions on each trial  if thereafter
the reversed tower still is not built  the trial fails  the trial also fails if one of the required
objects falls off the table 
table   presents our results  we cannot get sst with optimal planning horizon d     
to solve this problem even for five cubes  although the space of possible actions is reduced
due to the mentioned restriction  sst has enormous runtimes  with b      sst does not find
suitable actions  no leaves with the goal state  in several starting situations  the increased
planning horizon leads to a high probability of sampling at least one unfavorable outcome
for a required action  for b     a single tree traversal of sst takes more than a day  we
found uct to also require large planning times in order to achieve a reasonable success
rate  therefore  we set the planning horizons optimal for uct  in worlds of   cubes  uct
with optimal d      has a success rate of about     while taking on average more than   
  

filang   toussaint

minutes in case of success  for   cubes  however  uct with optimal d      never succeeds
even when planning times exceed   hours  in contrast  we can afford an overestimating
horizon d      for prada and a prada  in worlds of   cubes  prada and a prada
achieve success rates of     and     respectively in less than half a minute  a pradas
average number of executed actions in case of success is almost optimal  in worlds of  
cubes  the success rates of prada and a prada are still about      taking a bit more
than a minute on average in case of success  when their trials fail  this is most often due
to cubes falling off the table and not because they cannot find appropriate actions  cubes
falling off the table is also a main reason why the success rates of prada and a prada
drop to     and     respectively in worlds of   cubes when towers become rather unstable 
planning times in successful trials  however  also increase to more than    minutes indicating
the limitations of these planning approaches  nonetheless  the mean number of executed
actions in successful trials is still almost optimal for a prada 
overall  the reverse tower experiments indicate that planning approaches using lookahead trees fail in tasks that require long planning horizons and can only be achieved by
very few plans  given the huge action and state spaces in relational domains  the chances
that uct simulates an episode with exactly the required actions and successor states are
very small  planning approaches using approximate inference like prada and a prada
have the crucial advantage that the stochasticity of actions does not affect their runtime
exponentially in the planning horizon  of course  their search space of action sequences still
is exponential in the planning horizon so that problems requiring long horizons are hard to
solve also for them  our experiments show that by using the very simple  though principled
extension a prada  we can gain significant performance improvements 
our results also show that ff replan fails to provide good plans when using the original
learned rule sets  this is surprising as the characteristics of the reverse tower task seem
to favor ff replan in comparison to the other methods  there is a single conjunctive goal
structure and the number of good plans is very small while these plans require long horizons 
as the results of ff replan all  and ff replan single  indicate  ff replan can achieve
a good performance with the adapted rule sets that have been modified by hand to restrict
the number of possible actions in a state  while this constitutes a proof of concept of
ff replan  it shows the difficulty of applying ff replan with learned rule sets 

      summary
our results demonstrate that successful planning with learned world models  here in the
form of rules  may require to explicitly account for the quantification of predictive uncertainty  more concretely  methods applying look ahead trees  uct  and approximate
inference   a  prada  outperform ff replan on different tasks of varying difficulty  furthermore   a  prada can solve planning tasks with long horizons  where uct fails  only
if one post processes the learned rules by hand to clarify their application contexts and
the planning problem uses a conjunctive goal structure and requires few and long plans 
ff replan performs better than uct and  a  prada 
  

fiplanning with noisy probabilistic relational rules

    ippc      benchmarks
in the second part of our evaluation  we apply our proposed approaches on the benchmarks
of the latest international probabilistic planning competition  the uncertainty part of the
international planning competition in       ippc         the involved domains differ in
many characteristics  such as the number of actions  the required planning horizons and
the reward structures  as the competition results show  no planning algorithm performs
best everywhere  thus  these benchmarks give an idea for what types of problems sst 
uct and  a  prada may be useful  we convert the ppddl domain specifications into
nid rules along the lines described in sec  b    the resulting rule sets are used to run our
implementations of sst  uct and  a  prada on the benchmark problems 
each of the seven benchmark domains consists of    problem instances  an instance
specifies a goal and a starting state  instances vary not only in problem size  but also
in their reward structures  including action costs   so a direct comparison is not always
possible  in the competition  each instance was considered independently  planners were
given a restricted amount of time     minutes for problems     of each domain and   
minutes for the others  to cover as many repetitions of the very same problem instance as
possible up to a maximum of a     trials  trials differed in the random seeds resulting
in potentially different state transitions  the planners were evaluated with respect to the
number of trials ending in a goal state and the collected reward averaged over all trials 
eight planners entered in the competition  including ff replan which was not an official participant  they are discussed with the related work in sec     for their results  which
are too voluminous to be presented here  we refer the reader to the website of the competition  below  we provide a qualitative comparison of our methods to the results of these
planners  we do not attempt a direct quantitative comparison for several reasons  first 
the different hardware prevents timing comparisons  second  competition participants have
frequently not been able to successfully cover trials of a single or all instances of a domain 
it is difficult to tell the reasons for this from the results tables  the planner might have
been overburdened by the problem  might have faced temporary technical problems with
the client server architecture framework of the competition or could not cope with certain
ppddl constructs which could have been rewritten in a simpler format 
third and most importantly  we have not optimized our implementations to reuse previous planning efforts  instead  we fully replan for each single action  within a trial and
across trials   the competition evaluation scheme puts replanners at a disadvantage  in
particular those which replan each single action   instead of replanning  a good strategy for
the competition is to spend most planning time before starting the first trial and then reuse
the resulting insights  such as conditional plans and value functions  for all subsequent trials
with a minimum of additional planning  indeed  this strategy has often been adopted as
many trial time results indicate  we acknowledge that this is a fair procedure to evaluate
planners which compute policies over large parts of the state space before acting  we feel 
however  that this is counter to the idea of our approaches  uct and  a  prada are
meant for flexible planning with varying goals and different situations  thus  what we are
interested in is the average time to compute good actions and successfully solve a problem
instance when there is no prior knowledge available 
  

filang   toussaint

table    benchmarks of the ippc       the first column of a table specifies the problem
instance  suc  is the success rate  the trial time and the number of executed
actions are given for the successful trials  where applicable  the reward for all
trials is shown  all results are achieved with full replanning within a trial and
across trials 
 a  search and rescue
planner

suc  trial time  s 

actions

 c  blocksworld

reward

sst
uct
  
prada
a prada

   
  
   
   

       
      
      
      

sst
uct
prada
a prada

   
  
   
   

        
      
      
      

      
       
       
       

sst
uct
prada
a prada

  
  
  
  

        
       
      
      

             
             
              
              

actions

reward

uct
   prada
a prada

  
   
   

       
      
      

              
              
              

sst
uct
  
prada
a prada

 
 
   
   





                
                



       
       

  

uct
prada
a prada

  
  
  

       
      
      

             
              
              

  

prada
a prada

   
   

                
                

        
        

  

uct
prada
a prada

  
  
  

       
       
       

             
              
              

  

uct
prada
a prada

   
   
  

  

uct
prada
a prada

  
  
  

         
       
      

             
              
              

  

prada
a prada

  
  

                 
                 

      
      

uct
prada
a prada

  
  
  

                  
               
               

  

  

uct
prada
a prada

  
  
 

               
                
                 

      
     
     

uct
   prada
a prada

  
  
  

         
       
       

  

prada
a prada

 
  

              
              

       
       

  

prada
a prada

  
 

              
         


      
      

  

prada
a prada

  
  

        
       

       
       

     
     

  

prada
a prada

  
  

         
         

     
       

     
     

  

prada
a prada

  
  

                         
                       

  

  

             
             
              
              

planner

      
      
      
      

     
     
     

 
   
   
   


      
      
      

  

uct
prada
a prada

   
  
  

               
       
    
               

  

uct
prada
a prada

  
  
  

                
                
                

uct
   prada
a prada

  
 
 

              
               
                 



      
      

  

  

                       

suc  trial time  s 

                          
                         
                          

 e  exploding blocksworld
planner

actions

sst
uct
prada
a prada

  





               
               

planner

             
             
              

suc  trial time  s 

reward

 
 
  
  

prada

suc  trial time  s 

 d  boxworld

 b  triangle tireworld
planner

actions

sst
uct
  
prada
a prada


      
      
      

  

suc  trial time  s 
        
         
      
      

actions

sst
uct
  
prada
a prada

 
 
  
  

  

prada
a prada

  
  

               
               

  

prada
a prada

  
  

               
               

  

prada
a prada

  
  

               
               

  

prada
a prada

   
   

  

prada
a prada

  
  

                
               

  

prada
a prada

  
  

                
                

      
      

      
      
      
      

      
      

fiplanning with noisy probabilistic relational rules

therefore  for each single problem instance we perform     trials with different random
seeds using full replanning  a trial is aborted if a goal state is not reached within some
maximum number of actions varying slightly for each benchmark  about    actions   we
present the success rates and the mean estimators of trial times  executed actions and
rewards with their standard deviations in table   for the problem instances where at least
one trial was successfully covered in reasonable time 
search and rescue  table   a   is the only domain where sst  with branching factor
   is able to find plans within reasonable time  with significantly larger runtimes than
uct and  a  prada  the success rates and the rewards indicate that prada and aprada are superior to uct and scale up to rather big problem instances  to give an idea
w r t  the ippc evaluation scheme  uct solves successfully    trials of the first instance
within    minutes with full replanning  while prada and a prada solve all trials with
full replanning  in fact  despite of replanning each single action  prada and a prada
show the same success rates as the best planners of the benchmark except for the very large
problem instances  within the competition  only the participants fsp rbh and fsp rdh
achieved comparably satisfactory results   we conjecture that the success of our methods is
due to that fact that this domain requires to account carefully for the outcome probabilities 
but does not involve very long planning horizons 
triangle tireworld  table   b   is the only domain where uct outperforms prada
and a prada  although at a higher computational cost  the more depth first like style of
planning of uct seems useful in this domain  to give an idea w r t  the ippc evaluation
scheme  uct performs    successful trials of the first instance within    minutes  while
prada and a prada achieve    and    trials resp  using full replanning  but uct solves
more trials in the more difficult instances  the required planning horizons increase quickly
with the problem instances  our approaches cannot cope with the large problem instances 
which only three competition participants  rff bg  rff pg  hmdpp  could cover 
our methods face problems when the required planning horizons are very large  while
the number of plans with non zero probability is small  this becomes evident in the
blocksworld benchmark  table   c    this domain is different from the robot manipulation environment of our first evaluation in sec       the latter is considerably more
stochastic and provides more actions in a given situation  e g   we may grab objects within
a pile   blocksworld is the only domain where our approaches are inferior to ff replan  to
give an idea w r t  the ippc evaluation scheme  uct does not perform a single successful
trial of the first instance within    minutes  while prada and a prada achieve    and
   trials resp  using full replanning 
in the boxworld domain  table   d    our approaches can exploit the fact that the
delivery of boxes is  almost  independent of the delivery of other boxes  in most problem
instances this is further helped by the intermediate rewards for delivered boxes   in contrast
to uct  prada and a prada scale up to relatively large problem instances  prada
and a prada solve all     trials of the first problem instance  requiring on average    
min and     min resp  with full replanning  only two competition participants solved
trials successfully in this domain  rff bg and rff pg   to give an idea w r t  the ippc
evaluation scheme  uct does not perform a single successful trial within    minutes  while
prada completes   and a prada   trials  this small number can be explained by the
large plan lengths where each single action is computed with full replanning 
  

filang   toussaint

finally  in the exploding blocksworld domain  table   e   prada and a prada
perform better or as good as the competition participants  to give an idea w r t  the ippc
evaluation scheme  uct achieves only a single successful trial within    minutes  while
prada and a prada complete    and    trials resp  
we did not perform any experiments in either the sysadmin or the schedule domain  their ppddl specifications cannot be converted into nid rules due to the involved
universal effects  in contrast  this has been possible for the boxworld domain despite of
the universal effects there  in the boxworld problem instances  the universally quantified
variables always refer to exactly one object which we exploit for conversion to nid rules 
 note that this can be understood as a trick to implement deictic references in ppddl
by means of universal effects  the according action operator  however  has odd semantics 
boxes could end up in two different cities at the same time   furthermore  we ignored the
rectangle tireworld domain  which together with the triangle tireworld domain makes
up the   tireworlds benchmark  as its problem instances have faulty goal descriptions  they
should include not dead   this has not been critical to name a winner in the competition as
personally communicated by olivier buffet  
      summary
the majority of the ppddl descriptions of the ippc benchmarks can be converted into
nid rules  indicating the broad spectrum of planning problems which can be covered by
nid rules  our results demonstrate that our approaches perform comparably to or better
than state of the art planners on many traditional hand crafted planning problems  this
hints at the generality of our methods for probabilistic planning beyond the type of robotic
manipulation domains considered in sec       our methods perform particularly well in
domains where outcome probabilities need to be carefully accounted for  they face problems
when the required planning horizons are very large  while the number of plans with non zero
probability is small  this can be avoided by intermediate rewards 

   discussion
we have presented two approaches for planning with probabilistic relational rules in grounded
domains  our methods are designed to work on learned rules which provide approximate
partial models of noisy worlds  our first approach is an adaptation of the uct algorithm
which samples look ahead trees to cope with action stochasticity  our second approach 
called prada  models the uncertainty over states explicitly in terms of beliefs and employs
approximate inference in graphical models for planning  when we combine our planning
algorithms with an existing rule learning algorithm  an intelligent agent can  i  learn a
compact model of the dynamics of a complex noisy environment and  ii  quickly derive appropriate actions for varying goals  results in a complex simulated robotics domain show
that our methods outperform the state of the art planner ff replan on a number of different planning tasks  in contrast to ff replan  our methods reason over the probabilities
of action outcomes  this is necessary if the world dynamics are noisy and only partial and
approximate world models are available 
however  our planners also perform remarkably well on many traditional probabilistic
planning problems  this is demonstrated by our results on ippc benchmarks  where we
  

fiplanning with noisy probabilistic relational rules

have shown that ppddl descriptions can be converted to a large extent to the kind of rules
our planners use  this hints at the general purpose character of particularly prada and
the potential benefits of its techniques for probabilistic planning  for instance  our methods
can be expected to perform similarly well in large propositional mdps which do not exhibit
a relational structure 
so far  our planning approaches deal in reasonable time with problems containing up
to       objects  implying billions of world states  and requiring planning horizons of up
to       time steps  nonetheless  our approaches are still limited in that they rely on
reasoning in the grounded representation  if very many objects need to be represented or if
the representation language gets very rich  our approaches need to be combined with other
methods that reduce state and action space complexity  lang   toussaint      b  
    outlook
in its current form  the approximate inference procedure of prada relies on the specific
compact dbns compiled from rules  the development of similar factored frontier filters
for arbitrary dbns  e g  derived from more general ppddl descriptions  is promising 
similarly  the adaptation of pradas factored frontier techniques into existing probabilistic
planners is worth of investigation 
using probabilistic relational rules for backward planning appears appealing  it is
straightforward to learn nid rules that regress actions by providing reversed triples  s    a  s 
to the rule learning algorithm  stating the predecessor state s for a state s  if an action a has
been applied before  backward planning  which can be combined with forward planning 
has received a lot of attention in classical planning and may be fruitful for both planning
with look ahead trees as well as planning using approximate inference  by means of propagating backwards through our dbns  one may ultimately derive algorithms that calculate
posteriors over actions  leading to true planning by inference  instead of sampling actions  
an important direction for improving our prada algorithm is to make it adapt its
action sequence sampling strategy to the experience of previous samples  we have introduced a very simple extension  a prada  to achieve this  but more sophisticated methods
are conceivable  learning rule sets online and exploiting them immediately by our planning method is also an important direction of future research in order to enable acting in
the real world  where we want to behave effectively right from the start  improving the
rule framework for more efficient and effective planning is another interesting issue  for
instance  instead of using a noisy default rule  one may use mixture models to deal with
actions with several  non unique  covering rules  or in general use parallel rules that work
on different hierarchical levels or different aspects of the underlying system 

acknowledgments
we thank the anonymous reviewers for their careful and thorough comments which have
greatly improved this paper  we thank sungwook yoon for providing us an implementation
of ff replan  we thank olivier buffet for answering our questions on the probabilistic
planning competition       this work was supported by the german research foundation
 dfg   emmy noether fellowship to         
  

filang   toussaint

appendix a  proof of proposition  
proposition    sec       the set of action sequences prada samples with non zero
probability is a super set of the ones of sst and uct 
proof  let a  t   be an action sequence that was sampled by sst  or uct   thus 
there exists a state sequence s  t and a rule sequence r  t   such that in every state st
 t   t    action at has a unique covering rule rt that predicts the successor state st   with
probability pt      for  if pt      then st   would never be sampled by sst  or uct  
we have to show that t     t   t   p  st   a  t    s         if this is the case then
t
psample  at       as at has the unique covering rule rt in st and at will eventually be sampled 
p  s            is obvious  now assume p  st   a  t    s         if we execute at   we will
get p  st     a  t   s     pt p  st   a  t    s         the posterior p  st     a  t   s    can be greater
 first inequality  due to persistence or to previous states having non zero probability that
also lead to st   given at  
the set of action sequences prada samples is larger than that of sst  or uct  as
sst  or uct  refuses to model the noise outcomes of rules  assume an action a and state
s to be the only state where a has a unique covering rule  if an episode to s can only be
simulated by means of rule predictions with the noise outcome  this action will never be
sampled by sst  or uct   as the required states are never sampled   in contrast  prada
also models the effects of the noise outcome by giving very low probability to all possible
successor states with the heuristic described above  

appendix b  relation between nid rules and ppddl
we use nid rules  sec       as relational model of the transition dynamics of probabilistic actions  besides allowing for negative literals in the preconditions  nid rules extend
probabilistic strips operators  kushmerick et al         blum   langford        by two
special constructs  namely deictic references and noise outcomes  which are crucial for learning compact rule sets  an alternative language to specify probabilistic relational planning
problems used by the international probabilistic planning competitions  ippc        is
the probabilistic planning domain definition language  ppddl   younes   littman        
ppddl is a probabilistic extension of a subset of pddl  derived from the deterministic
action description language  adl   adl  in turn  introduced universal and conditional
effects and negative precondition literals into the  deterministic  strips representation 
thus  ppddl allows for the usage of syntactic constructs which are beyond the expressive
power of nid rules  however  many ppddl descriptions can be converted into nid rules 
before taking a closer look at how to convert ppddl and nid rule representations
into each other  we clarify what is meant by action in each of the formalisms  giving an
intuition of the line of thinking when using either of these  we understand by abstract
action an abstract action predicate  e g  pickup x   intuitively  this defines a certain type
of action  the stochastic state transitions according to an abstract action can be specified by
both abstract nid rules as well as abstract ppddl action operators  also called schemata  
typically  several different abstract nid rules model the same abstract action  specifying
state transitions in different contexts  in contrast  usually only one abstract ppddl action
  

fiplanning with noisy probabilistic relational rules

operator is used to model an abstract action  context dependent effects are modeled by
means of conditional and universal effects 
to make predictions in a specific situation for a concrete action  a grounded action
predicate such as pickup greencube    the strategy within the nid rule framework is to
ground the set of abstract nid rules and examine which ground rules cover this state action
pair  if there is exactly one such ground rule  it is chosen for prediction  if there is no such
rule or if there is more than one  the contexts of nid rules do not have to be mutually
exclusive   one chooses the noisy default rule  essentially saying that one does not know
what will happen  other strategies are conceivable  but not pursued here   in contrast  as
there is usually exactly one operator per abstract action in ppddl domains  there is no
need of the concept of operator uniqueness and to distinguish between ground actions and
operators 
b   converting ppddl to nid rules
in the following  we discuss how to convert ppddl features into a nid rule representation 
while it may be impossible to convert a ppddl action operator into a single nid rule 
one may often translate it into a set of rules with at most a polynomial increase in the size
of representation  table   provides an example of a converted ppddl action operator of
the ippc domain exploding blocksworld  as nid rules support many  but not all of the
features a sophisticated domain description language such as ppddl provides  using rules
will not lead to compact representations in all possible domains  our experiments  however 
show that the dynamics of many interesting planning domains can be specified compactly 
furthermore  additional expressive power in rule contexts can be gained by using derived
predicates which allow to bring in various kinds of logical formulas such as quantification 
conditional effects a conditional effect in a ppddl operator takes the form when c
then e  it can be accounted for by two nid rules  the first rule adds c to its context and
e to its outcomes  while the second adds c to its context and ignores e 
universal effects ppddl allows to define universal effects  these specify effects for all
objects that meet some preconditions  an example is the reboot action of the sysadmin
domain of the ippc      competition  it specifies that every computer other than the one
rebooted can independently go down with probability     if it is connected to a computer
that is already down  this cannot be expressed in a nid rule framework  while we can
refer to objects other than the action arguments via deictic references  we require these
deictic references to be unique  for the reboot action  we would need a unique way to refer
to each other computer which cannot be achieved without significant modifications  for
example  such as enumerating the other computers via separate predicates  
disjunctive preconditions and quantification ppddl operators allow for disjunctive preconditions  including implications  for instance  the search and rescue domain
of the ippc      competition defines an action operator goto x  with the precondition
 x    base   humanalive    a disjunction a  b   a  b  can be accounted for
by either using two nid rules  with the first rule having a in the context and the second
rule having a  b  alternatively  one may introduce a derived predicate c  a  b  in
general  the trick of derived predicates allows to overcome syntactical limitations of nid
  

filang   toussaint

table    example for converting a ppddl action operator into nid rules  the putdownoperator of the ippc benchmark domain exploding blocksworld  a  contains a
conditional effect which can be accounted for by two nid rules which either exclude
 b  or include  c  this condition in their context 
    action putdown

 a 

  parameters   b  block 
  precondition  and  holding  b   nodestroyedt able  
  ef f ect  and  emptyhand   ont able  b   not  holding  b  
 probabilistic      when  nodetonated  b   and  not  nodestroyedt able    not  nodetonated b      
 
 b 
putdown x    block x   holding x   nodestroyedt able    nodetonated x 

      emptyhand x   ont able x   holding x 

 c 
putdown x    block x   holding x   nodestroyedt able    nodetonated x 

      emptyhand x   ont able x   holding x 

      emptyhand x   ont able x   holding x   nodestroyedt able    nodetonated x 

rules and bring in various kinds of logical formulas such as quantifications  as discussed by
pasula et al          derived predicates are an important prerequisite to being able to learn
compact and accurate rules 
types terms may be typed in ppddl  e g  drivet o c  city   typing of objects and
variables in predicates and functions can be achieved in nid rules by the usage of typing
predicates within the context  e g  using an additional predicate city c  
state transition rewards in ppddl  one can encode markovian rewards associated
with state transitions  including action costs as negative rewards  using fluents and update
rules in action effects  one can achieve this in nid rules by associating rewards with the
outcomes of rules 
b   converting nid rules to ppddl
we show in the following that the way nid rules are used in sst  uct and prada at
planning time can be handled via at most a polynomial blowup in representational size 
the basic building blocks of a nid rule  i e  the context as well as the outcomes  transfer
one to one to ppddl action operators  the deictic references  the uniqueness requirement
of covering rules and the noise outcome need special attention 
deictic references deictic references in nid rules allow to refer to objects which are
not action arguments  in ppddl  one can refer to such objects by means of universal
conditional effects  there is an important restriction  however  a deictic reference needs to
pick out a single unique object in order to apply  if it picks out none or many  the rule fails
to apply  there are two ways to ensure this uniqueness requirement within ppddl  first 
  

fiplanning with noisy probabilistic relational rules

if allowing quantified preconditions  an explicit uniqueness precondition for each deictic
reference d can be introduced  using universal quantification  it constrains all objects
satisfying the preconditions d of d to be identical  i e   x  y   d  x     d  y    
x   y   where  are some other variables  alternatively  uniqueness of deictic references
can be achieved by a careful planning problem specification  which however cannot be
guaranteed when learning rules 
uniqueness of covering rules the contexts of nid rules do not have to be mutually
exclusive  when we want to use a rule for prediction  as in planning   we need to ensure that
it uniquely covers the given state action pair  the procedural evaluation process for nid
rules can be encoded declaratively in ppddl using modified conditions which explicitly
negate the contexts of competing rules  for instance  if there are three nid rules with
potentially overlapping contexts a  b  and c  propositional for simplicity   the ppddl
action operator may define four conditions  c     a  b  c   c     a  b  c  
c     a  b  c   c      a  b  c    a  b    a  c    b  c    conditions c   
c  and c  test for uniqueness of the corresponding nid rules and subsume their outcomes 
condition c  tests for non uniqueness  either no covering rule or multiple covering rules 
and models potential changes as noise  analogous to the situations in a nid rule context in
which the noisy default rule would be used 
noise outcome the noise outcome of a nid rule subsumes seldom or utterly complex
outcomes  it relaxes the frame assumption  even not explicitly stated things may change
with a certain probability  this comes at the price of the difficulty to ensure a well defined
successor state distribution p  s    s  a   in contrast  ppddl needs to explicitly specify
everything that might change  this may be an important reason why it is difficult to come
up with an effective learning algorithm for ppddl 
while in principle ppddl does not provide for a noise outcome  the way our approaches
account for it in planning can be encoded in ppddl  we either treat the noise outcome
as having no effects  in sst and uct  basically a noop operator then  which is trivially
translated to ppddl  or we consider the probability of each state attribute to change
independently  in prada  which can be encoded in ppddl with independent universal
probabilistic effects 
the noise outcome allows to always make predictions for an arbitrary action  if there
are no or multiple covering rules  we may use the  albeit not very informative  prediction
of the default rule  such cases can be dealt with in ppddl action operators using explicit
conditions as described in the previous paragraph 

references
blum  a     langford  j          probabilistic planning in the graphplan framework  in
proc  of the fifth european conference on planning  ecp   pp         
botvinick  m  m     an  j          goal directed decision making in prefrontal cortex 
a computational framework  in advances in neural information processing systems
 nips   pp         
  

filang   toussaint

boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions and computational leverage  journal of artificial intelligence research 
        
boutilier  c   reiter  r     price  b          symbolic dynamic programming for first order
mdps  in proc  of the int  conf  on artificial intelligence  ijcai   pp         
buffet  o     aberdeen  d          the factored policy gradient planner  artificial intelligence journal                    
cooper  g          a method for using belief networks as influence diagrams  in proc  of
the fourth workshop on uncertainty in artificial intelligence  pp       
croonenborghs  t   ramon  j   blockeel  h     bruynooghe  m          online learning and
exploiting relational models in reinforcement learning  in proc  of the int  conf  on
artificial intelligence  ijcai   pp         
domshlak  c     hoffmann  j          probabilistic planning via heuristic forward search
and weighted model counting  journal of artificial intelligence research             
driessens  k   ramon  j     gartner  t          graph kernels and gaussian processes for
relational reinforcement learning  machine learning                  
dzeroski  s   de raedt  l     driessens  k          relational reinforcement learning 
machine learning          
fern  a   yoon  s     givan  r          approximate policy iteration with a policy language
bias  solving relational markov decision processes  journal of artificial intelligence
research                
gardiol  n  h     kaelbling  l  p          envelope based planning in relational mdps  in
proc  of the conf  on neural information processing systems  nips  
gardiol  n  h     kaelbling  l  p          action space partitioning for planning  in proc  of
the aaai conf  on artificial intelligence  aaai   pp         
gardiol  n  h     kaelbling  l  p          adaptive envelope mdps for relational
equivalence based planning  tech  rep  mit csail tr           mit cs   ai lab 
cambridge  ma 
gelly  s     silver  d          combining online and offline knowledge in uct  in proc  of
the int  conf  on machine learning  icml   pp         
gretton  c     thiebaux  s          exploiting first order rgeression in inductive policy
selection  in proc  of the conf  on uncertainty in artificial intelligence  uai   pp 
       
grush  r          conscious thought as simulation of behaviour and perception  behaviorial
and brain sciences             
  

fiplanning with noisy probabilistic relational rules

halbritter  f     geibel  p          learning models of relational mdps using graph kernels 
in proc  of the mexican conference on artificial intelligence  micai   pp         
hesslow  g          conscious thought as simulation of behaviour and perception  trends
in cognitive science                
hoffmann  j     nebel  b          the ff planning system  fast plan generation through
heuristic search  journal of artificial intelligence research             
holldobler  s     skvortsova  o          a logic based approach to dynamic programming 
in aaai workshop  learning and planning in mdps  pp       
ippc

       
sixth international planning competition 
http   ippc      loria fr wiki index php main page 

uncertainty

part  

jensen  f          an introduction to bayesian networks  springer verlag  new york 
joshi  s   kersting  k     khardon  r          generalized first order decision diagrams for
first order mdps  in proc  of the int  conf  on artificial intelligence  ijcai   pp 
         
karabaev  e     skvortsova  o          a heuristic search algorithm for solving first order
mdps  in proc  of the conf  on uncertainty in artificial intelligence  uai   pp 
       
kearns  m  j   mansour  y     ng  a  y          a sparse sampling algorithm for nearoptimal planning in large markov decision processes  machine learning           
       
kersting  k     driessens  k          nonparametric policy gradients  a unified treatment of propositional and relational domains  in proc  of the int  conf  on machine
learning  icml   pp         
kersting  k   van otterlo  m     de raedt  l          bellman goes relational  in proc  of
the int  conf  on machine learning  icml   pp         
kocsis  l     szepesvari  c          bandit based monte carlo planning  in proc  of the
european conf  on machine learning  ecml   pp         
kushmerick  n   hanks  s     weld  d          an algorithm for probabilistic planning 
artificial intelligence                   
kuter  u   nau  d  s   reisner  e     goldman  r  p          using classical planners to
solve nondeterministic planning problems  in proc  of the int  conf  on automated
planning and scheduling  icaps   pp         
lang  t     toussaint  m       a   approximate inference for planning in stochastic relational worlds  in proc  of the int  conf  on machine learning  icml   pp         
lang  t     toussaint  m       b   relevance grounding for planning in relational domains 
in proc  of the european conf  on machine learning  ecml   pp         
  

filang   toussaint

little  i     thiebaux  s          probabilistic planning vs replanning  in icaps workshop
international planning competition  past  present and future 
littman  m  l   goldsmith  j     mundhenk  m          the computational complexity of
probabilistic planning  journal of artificial intelligence research         
murphy  k  p          dynamic bayesian networks  representation  inference and learning  ph d  thesis  uc berkeley 
murphy  k  p     weiss  y          the factored frontier algorithm for approximate inference in dbns  in proc  of the conf  on uncertainty in artificial intelligence  uai  
pp         
pasula  h  m   zettlemoyer  l  s     kaelbling  l  p          learning symbolic models of
stochastic domains  journal of artificial intelligence research             
poon  h     domingos  p          sound and efficient inference with probabilistic and
deterministic dependencies  in proc  of the aaai conf  on artificial intelligence
 aaai  
sanner  s     boutilier  c          approximate solution techniques for factored first order
mdps  in proc  of the int  conf  on automated planning and scheduling  icaps  
pp         
sanner  s     boutilier  c          practical solution techniques for first order mdps 
artificial intelligence                    
shachter  r          probabilistic inference and influence diagrams  operations research 
           
sutton  r  s     barto  a  g          reinforcement learning  an introduction  the mit
press 
teichteil konigsbuch  f   kuter  u     infantes  g          aggregation for generating
policies in mdps  in to appear in proc  of int  conf  on autonomous agents and
multiagent systems 
toussaint  m     storkey  a          probabilistic inference for solving discrete and continuous state markov decision processes  in proc  of the int  conf  on machine learning
 icml   pp         
toussaint  m   storkey  a     harmeling  s          expectation maximization methods
for solving  po mdps and optimal control problems  in chiappa  s     barber  d 
 eds    inference and learning in dynamic models  cambridge university press 
van otterlo  m          the logic of adaptive behavior  ios press  amsterdam 
walsh  t  j          efficient learning of relational models for sequential decision making 
ph d  thesis  rutgers  the state university of new jersey  new brunswick  nj 
  

fiplanning with noisy probabilistic relational rules

wang  c   joshi  s     khardon  r          first order decision diagrams for relational
mdps  journal of artificial intelligence research             
weld  d  s          recent advances in ai planning  ai magazine                
wu  j  h   kalyanam  r     givan  r          stochastic enforced hill climbing  in proc  of
the int  conf  on automated planning and scheduling  icaps   pp         
yoon  s  w   fern  a     givan  r          ff replan  a baseline for probabilistic planning 
in proc  of the int  conf  on automated planning and scheduling  icaps   pp     
    
yoon  s  w   fern  a   givan  r     kambhampati  s          probabilistic planning via
determinization in hindsight  in proc  of the aaai conf  on artificial intelligence
 aaai   pp           
younes  h  l     littman  m  l          ppddl     an extension to pddl for expressing
planning domains with probabilistic effects  tech  rep   carnegie mellon university 

  

fi