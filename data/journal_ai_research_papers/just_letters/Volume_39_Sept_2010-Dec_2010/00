journal artificial intelligence research               

submitted        published      

planning noisy probabilistic relational rules
tobias lang
marc toussaint

tobias lang tu berlin de
mtoussai cs tu berlin de

machine learning robotics group
technische universitat berlin
franklinstrae              berlin  germany

abstract
noisy probabilistic relational rules promising world model representation several reasons  compact generalize world instantiations  usually
interpretable learned effectively action experiences complex
worlds  investigate reasoning rules grounded relational domains  algorithms exploit compactness rules efficient flexible decision theoretic planning 
first approach  combine rules upper confidence bounds applied
trees  uct  algorithm based look ahead trees  second approach converts
rules structured dynamic bayesian network representation predicts effects
action sequences using approximate inference beliefs world states  evaluate
effectiveness approaches planning simulated complex  d robot manipulation scenario articulated manipulator realistic physics domains
probabilistic planning competition  empirical results show methods solve
problems existing methods fail 

   introduction
building systems act autonomously complex environments central goal artificial intelligence  nowadays  a i  systems par particularly intelligent humans
specialized tasks playing chess  hopelessly inferior almost humans  however  deceivingly simple tasks everyday life  clearing desktop 
preparing cup tea manipulating chess figures  current state art reasoning  planning  learning  perception  locomotion  manipulation far removed
human level abilities  cannot yet contemplate working actual domain interest  pasula  zettlemoyer    kaelbling         performing common object manipulations
indeed challenging task real world  choose large number
distinct actions uncertain outcomes number possible situations basically
unseizable 
act real world  accomplish two tasks  first  need understand
world works  example  pile plates stable place big plates
bottom  hard job build tower balls  filling tea cup may lead
dirty table cloth  autonomous agents need learn world knowledge experience
adapt new environments rely human hand crafting  paper 
employ recent solution learning  pasula et al          know possible
effects actions  face second challenging problem  use acquired
knowledge reasonable time find sequence actions suitable achieve goals 
c
    
ai access foundation  rights reserved 

filang   toussaint

paper investigates novel algorithms tackle second task  namely planning 
pursue model based approach planning complex domains  contrast modelfree approaches compute policies directly experience respect fixed goals
 also called habit based decision making   follow purposive decision making approach
 botvinick   an        use learned models plan goal current state
hand  particular  simulate probabilistic effects action sequences  approach
interesting parallels recent neurobiology cognitive science results suggesting
behavior intelligent mammals driven internal simulation emulation 
found motor structures cortex activated planning 
execution motor commands suppressed  hesslow        grush        
probabilistic relational world model representations received significant attention
last years  enable generalize object identities unencountered situations objects similar types account indeterministic action effects noise 
review several approaches together related work section    noisy
indeterministic deictic  nid  rules  pasula et al         capture world dynamics
elegant compact way  particularly appealing learned effectively
experience  existing approach planning rules relies growing
full look ahead trees grounded domain  due large action space
stochasticity world  computational burden plan single action
method given situation overwhelmingly large  paper proposes two novel
ways reasoning efficiently grounded domain using learned nid rules  enabling fast
planning complex environments varying goals  first  apply existing upper
confidence bounds applied trees  uct  algorithm  kocsis   szepesvari        nid
rules  contrast full grown look ahead trees  uct samples actions selectively  thereby
cutting suboptimal parts tree early  second  introduce probabilistic relational
action sampling dbns planning algorithm  prada  uses probabilistic inference
cope uncertain action outcomes  instead growing look ahead trees sampled successor states previous approaches  prada applies approximate inference
techniques propagate effects actions  particular  make three contributions
prada   i  following idea framing planning probabilistic inference problem  shachter        toussaint  storkey    harmeling         convert nid rules
dynamic bayesian network  dbn  representation   ii  derive approximate inference method cope state complexity time slice resulting network 
thereby  efficiently predict effects action sequences   iii  planning based
sampling action sequences  propose sampling distribution plans takes predicted state distributions account  evaluate planning approaches simulated
complex  d robot manipulation environment realistic physics  articulated humanoid manipulating objects different types  see fig      domain contains billions
world states large number potential actions  learn nid rules experience
environment apply planning approaches different planning
scenarios increasing difficulty  furthermore  provide results approaches
planning domains recent international probabilistic planning competition 
purpose  discuss relation nid rules probabilistic planning
domain definition language  ppddl  used specification domains 
 

fiplanning noisy probabilistic relational rules

begin paper discussing related work section   reviewing
background work  namely stochastic relational representations  nid rules  formalization decision theoretic planning graphical models section    section   
present two planning algorithms build look ahead trees cope stochastic
actions  section    introduce prada uses approximate inference planning 
section    present empirical evaluation demonstrating utility planning
approaches  finally  conclude outline future directions research 

   related work
problem decision making planning stochastic relational domains approached different ways  field relational reinforcement learning  rrl   dzeroski 
de raedt    driessens        van otterlo        investigates value functions q functions
defined possible ground states actions relational domain  key
idea describe important world features terms abstract logical formulas enabling
generalization objects situations  model free rrl approaches learn value functions
states actions directly experience  q function estimators include relational
regression trees  dzeroski et al         instance based regression using distance metrics relational states graph kernels  driessens  ramon    gartner        
model free approaches enable planning specific problem type used training
examples  e g  on x     thus may inappropriate situations goals
agent change quickly  e g  on x    inhand x   contrast  model based rrl
approaches first learn relational world model state transition experiences
use model planning  example form relational probability trees
individual state attributes  croonenborghs  ramon  blockeel    bruynooghe       
svms using graph kernels  halbritter   geibel         stochastic relational nid rules
pasula et al         particularly appealing action model representation 
shown empirically learn dynamics complex environments 
probabilistic relational world model available  either learned handcrafted  
one pursue decision theoretic planning different ways  within machine learning
community  popular direction research formalizes problem relational markov
decision process  rmdp  develops dynamic programming algorithms compute solutions  i e  policies complete state action spaces  many algorithms reason
lifted abstract representation without grounding referring particular problem instances  boutilier  reiter  price        introduce symbolic dynamic programming 
first exact solution technique rmdps uses logical regression construct
minimal logical partitions state space required make necessary value function
distinctions  approach implemented difficult keep firstorder state formulas consistent manageable size  based ideas  kersting  van
otterlo  de raedt        propose exact value iteration algorithm rmdps using
logic programming  called rebel  employ restricted language represent rmdps
reason efficiently state formulas  holldobler skvortsova       
present first order value iteration algorithm  fovia  using different restricted language 
karabaev skvortsova        extend fovia combining first order reasoning
actions heuristic search restricted states reachable initial
 

filang   toussaint

state  wang  joshi  khardon        derive value iteration algorithm based using
first order decision diagrams  fodds  goal regression  introduce reduction operators fodds keep representation small  may require complex reasoning 
empirical evaluation provided  joshi  kersting  khardon        apply
model checking reduce fodds generalize arbitrary quantification 
techniques form interesting research direction reason exactly
abstract rmdps  employ different methods ensure exact regression theorem proving  logical simplification  consistency checking  therefore  principled approximations techniques discover good policies difficult domains
likewise worth investigating  instance  gretton thiebaux        employ first order
regression generate suitable hypothesis language use policy induction  thereby  approach avoids formula rewriting theorem proving  still
requiring model checking  sanner boutilier              present first order approximate linear programming approach  foalp   prior producing plans  approximate
value function based linear combinations abstract first order value functions 
showing impressive results solving rmdps millions states  fern  yoon 
givan        consider variant approximate policy iteration  api  replace
value function learning step learning step policy space  make use
policy space bias described generic relational knowledge representation simulate trajectories improve learned policy  kersting driessens        describe
non parametric policy gradient approach deal propositional  continuous
relational domains unified way 
instead working lifted representation  one may reason grounded domain 
makes straightforward account two special characteristics nid rules 
noise outcome uniqueness requirement rules  grounding rmdp
specifies rewards set goal states  one might principle apply traditional a i  planning methods used propositional representations  weld        boutilier 
dean    hanks         traditionally  planning often cast search problem
state action space  restricting oneself portion state space considered contain goal states reachable current state within limited
horizon  much research within planning community focused deterministic domains thus cant applied straightforwardly stochastic worlds  common approach
probabilistic planning  however  determinize planning problem apply deterministic planners  kuter  nau  reisner    goldman         indeed  ff replan  yoon 
fern    givan        extension using hindsight optimization  yoon  fern  givan   
kambhampati        shown impressive performance many probabilistic planning
competition domains  common variant ff replan considers probabilistic outcome action separate deterministic action  ignoring respective probabilities 
runs deterministic fast forward  ff  planner  hoffmann   nebel       
determinized problem  uses relaxation planning problem  ignores delete
effects actions applies clever heuristics prune search space  ff replan outputs
sequence actions expected states  time action execution leads state
plan  ff replan replan  i e   recompute new plan scratch
current state  good performance ff replan many probabilistic domains
explained structure problems  little   thiebaux        
 

fiplanning noisy probabilistic relational rules

argued ff replan less appropriate domains probability
reaching dead end non negligible outcome probabilities actions need
taken account construct good policy 
many participants recent probabilistic planning competition  ippc       
extend ff replan deal probabilities action outcomes  see competition
website brief descriptions algorithms   winner competition  rff
 teichteil konigsbuch  kuter    infantes         computes robust policy offline generating successive execution paths leading goal using ff  resulting policy
low probability failing  lppff uses subgoals generated determinization
probabilistic planning problem divide smaller manageable problems  hmdpps
strategy similar all outcomes determinization ff replan  accounts
probability associated outcome  seh  wu  kalyanam    givan        extends
heuristic function ff replan cope local optima plans using stochastic
enforced hill climbing 
common approach reasoning general reward maximization context
avoids explicitly dealing uncertainty build look ahead trees sampling successor
states  two algorithms follow idea  namely sst  kearns  mansour    ng       
uct  kocsis   szepesvari         investigated paper 
another approach buffet aberdeen        directly optimizes parameterized
policy using gradient descent  factor global policy simple approximate policies
starting action sample trajectories cope probabilistic effects 
instead sampling state transitions  propose planning algorithm prada
paper  based lang   toussaint      a  accounts uncertainty principled
way using approximate inference  domshlak hoffmann        propose interesting
planning approach comes closest work  introduce probabilistic extension planner  using complex algorithms building probabilistic relaxed planning
graphs  construct dynamic bayesian networks  dbns  hand crafted strips operators reason actions states using weighted model counting  dbn
representation  however  inadequate type stochastic relational rules use 
reasons naive dbn model discuss sec      inappropriate  planning inference approaches  toussaint   storkey        spread information
backwards dbns calculate posteriors actions  resulting policies
complete state spaces   use backward propagation even full planning
inference relational domains open issue 
approaches working grounded representation common number
states actions grow exponentially number objects  apply
domains many objects  approaches need combined complementary
methods reduce state action space complexity relational domains 
instance  one focus envelopes states high utility subsets state
space  gardiol   kaelbling         one ground representation respect
relevant objects  lang   toussaint      b   one exploit equivalence actions
 gardiol   kaelbling         particularly useful combination ignoring
certain predicates functions relational logic language  gardiol   kaelbling        
 

filang   toussaint

   background
section  set theoretical background planning algorithms
present subsequent sections  first  describe relational representations define world
states actions  present noisy indeterministic deictic  nid  rules detail
thereafter define problem decision theoretic planning stochastic relational
domains  finally  briefly review dynamic bayesian networks 
    state action representation
relational domain represented relational logic language l  set logical
predicates p set logical functions f contain relationships properties
hold domain objects  set logical predicates comprises possible actions
domain  concrete instantiation relational domain made finite set
objects o  arguments predicate function concrete  i e  taken o 
call grounded  concrete world state fully described conjunction grounded
 potentially negated  predicates function values  concrete actions described
positive grounded predicates a  arguments predicates functions
abstract logical variables represent object  predicate function
abstract arguments  call abstract  abstract predicates functions enable
generalization objects situations  speak grounding formula
apply substitution maps variables appearing objects o 
relational model transition dynamics specifies p  s   a  s   probability
successor state s  action performed state s  paper  usually
non deterministic distribution  typically defined compactly terms formulas
abstract predicates functions  enables abstraction object identities
concrete domain instantiations  instance  consider set n cups  effects trying
grab cups may described single abstract model instead
using n individual models  apply given world state  one needs ground
respect objects domain  nid rules elegant way specify
model described following 
    noisy indeterministic deictic rules
want learn relational model stochastic world use planning  pasula
et al         recently introduced appealing action model representation based
noisy indeterministic deictic  nid  rules combine several advantages 
relational representation enabling generalization objects situations 
indeterministic action outcomes probabilities account stochastic domains 
deictic references actions reduce action space 
noise outcomes avoid explicit modeling rare overly complex outcomes 
existence effective learning algorithm 
 

fiplanning noisy probabilistic relational rules

table   shows exemplary nid rule complex robot manipulation domain 
fig    depicts situation rule used prediction  formally  nid rule
r given

pr  
  r    x  




  
 
   
ar  x     r  x  

 
r mr  x  
p

r m
r


pr  
  r  
x set logical variables rule  which represent  sub  set abstract
objects   rules define world models formulas abstract  i e  
arguments logical variables  rule r consists preconditions  namely action
ar applied x state p
context r fulfilled  mr    different outcomes
associated probabilities pr i    i   pr i      outcome r i  x   describes
predicates functions change rule applied  context r  x   outcomes
r i  x   conjunctions  potentially negated  literals constructed predicates
p well equality statements comparing functions f constant values  besides
explicitely stated outcomes r i  i       so called noise outcome r   models implicitly
potential outcomes rule  particular  includes rare overly
complex outcomes typical noisy domains  want cover explicitly
compactness generalization reasons  instance  context rule depicted
fig    potential  highly improbable outcome grab blue cube pushing
objects table  noise outcome allows account without burden
explicitly stating it 
arguments action a xa   may true subset xa x variables x
rule  remaining variables called deictic references   x   xa denote
objects relative agent action performed  using deictic references
advantage decrease arity action predicates  turn reduces size
action space least order magnitude  significant effects
planning problem  instance  consider binary action predicate world
n objects n  groundings contrast unary action predicate n
groundings 
above  let denote substitution maps variables constant objects    x o 
applying abstract rule r x   yields ground rule r  x     say ground rule r
covers state ground action    r   ar   let set ground nid
rules  define  a      r   r   ar   a  set rules provide predictions
action a  r rule  a  cover state s  call unique covering rule
s  state action pair  s  a  unique covering rule r  calculate p  s    s  a 
taking outcomes r account weighted respective probabilities 
r

 

 

p  s  s  a    p  s  s  r   


x

pr i p  s   r i   s    pr   p  s   r     s  

   

i  

where       p  s    r i   s  deterministic distribution one unique
state constructed taking changes r i account  distribution given
 

filang   toussaint

table    example nid rule complex robot manipulation scenario  models
try grab ball x  cube implicitly defined one x  deictic
referencing   x ends robots hand high probability  might
fall table  small probability something unpredictable happens 
confer fig    example application 

grab x    on x     ball x   cube y    table z 

      inhand x   on x   
      on x  z   on x   


      noise

figure    nid rule defined table   used predict effects action
grab ball  situation left side  right side depicts possible
successor states predicted rule  noise outcome indicated
question mark define unique successor state 

noise outcome  p  s    r     s   unknown needs estimated  pasula et al  use
worst case constant bound pmin p  s   r     s  lower bound p  s   s  a   alternatively 
come well defined distribution  one may assign low probability many
successor states  described detail sec       planning algorithm prada
exploits factored state representation grounded relational domain achieve
predicting state attribute change low probability 
state action pair  s  a  unique covering rule r  e g  two rules cover
 s  a  providing conflicting predictions   one predict effects means
noisy default rule r explains effects changing state attributes noise 
p  s   s  r     p  s    r      s   essentially  using r expresses know
happen  meaningful thus disadvantageous planning   hence  one
bias nid rules learner learn rules contexts likely mutually
exclusive   reason  concept unique covering rules crucial planning
nid rules  here  pay price using deictic references  using
abstract nid rule prediction  always ensure deictic references
unique groundings  may require examining large part state representation 
 

fiplanning noisy probabilistic relational rules

proper storage ground state efficient indexing techniques logical formula
evaluation needed 
ability learn models environment experience crucial requirement
autonomous agents  problem learning rule sets general np hard  efficiency guarantees sample complexity given many learning subtasks
suitable restrictions  walsh         pasula et al         proposed supervised batch
learning algorithm complete nid rules  algorithm learns structure rules
well parameters experience triples  s  a  s     stating observed successor
state s  action applied state s  performs greedy search space
rule sets  optimizes tradeoff maximizing likelihood experience
triples minimizing complexity current hypothesis rule set optimizing
scoring metric
x
x
s    
log p  s    s  rs a  
p en  r   
   
 s a s   

r

rs a either unique covering rule  s  a  noisy default rule r
scaling parameter controls influence regularization  p en  r  penalizes
complexity rule defined total number literals r 
noise outcome nid rules crucial learning  learning algorithm initialized rule set comprising noisy default rule r iteratively adds
new rules modifies existing ones using set search operators  noise outcome
allows avoiding overfitting  need model rare overly complex outcomes
explicitly  drawback successor state distribution p  s    r     s  unknown 
deal problem  learning algorithm uses lower bound pmin approximate
distribution  described above  algorithm uses greedy heuristics attempt
learn complete rules  guarantees behavior given  pasula et al   however  report impressive results complex noisy environments  sec       confirm
results simulated noisy robot manipulation scenario  major motivation employing nid rules learn observed actions state transitions 
furthermore  planning approach prada exploit simple structure  which
similar probabilistic strips operators  convert dbn representation 
provide detailed comparison nid rules ppddl appendix b  nid
rules support features sophisticated domain description language
ppddl  compactly capture dynamics many interesting planning domains 
    decision theoretic planning
problem decision theoretic planning find actions given state
expected maximize future rewards states actions  boutilier et al         
classical planning  reward usually defined terms clear cut goal
either fulfilled fulfilled state  expressed means logical
formula   typically  formula partial state description exists
one state holds  example  goal might put romance
books specific shelf  matter remaining books lying  case 
planning involves finding sequence actions executing starting
 

filang   toussaint

result world state s  s       stochastic domains  however  outcomes
actions uncertain  probabilistic planning inherently harder deterministic
counterpart  littman  goldsmith    mundhenk         particular  achieving goal
state certainty typically unrealistic  instead  one may define lower bound
probability achieving goal state  second source uncertainty next uncertain
action outcomes uncertainty initial state s  ignore latter
following always assume deterministic initial states  see later  however 
straightforward incorporate uncertainty initial state using one proposed
planning approaches 
instead classical planning task finished achieved state
goal fulfilled  task may ongoing  instance  goal might
keep desktop tidy  formalized means reward function states 
yields high reward desirable states  for simplicity  assume rewards
depend actions   approach taken reinforcement learning formalisms
 sutton   barto         classical planning goals easily formalized
reward function  cast scenario planning stochastic relational domain
relational markov decision process  rmdp  framework  boutilier et al          follow
notation van otterlo        define rmdp   tuple  s  a  t  r   contrast
enumerated state spaces  state space relational structure defined
logical predicates p functions f  yield ground atoms arguments taken
set domain objects o  action space defined positive predicates
arguments o           transition distribution r   r
reward function  r make use factored relational representation
abstract states actions  discussed following  typically 
state space action space relational domain large  consider
instance domain   objects use   binary predicates represent states 
 
case  number states             relational world models encapsulate transition
probabilities compact way exploiting relational structure  example  nid rules
described eq      achieve generalized partial world state descriptions
form conjunctions abstract literals  compactness models  however 
carry directly planning problem 
 deterministic  policy   tells us action take given state 
fixed horizon discount
         interested maximizing
pd factor
r   value factored state defined
discounted total reward r  


t  
expected return state following policy  
v  s    e r   s    s     

   

solution rmdp  thus problem planning  optimal policy
maximizes expected return  defined bellman equation 
x


v  s    r s    max 
p  s    s  a v  s      
aa

  

s 

   

fiplanning noisy probabilistic relational rules

similarly  one define value q  s  a  action state expected return
action taken state s  using policy select subsequent actions 
q  s  a    e r   s    s  a    a   
x
  r s   
v  s   p  s    s  a   

   
   

s 

q values optimal policy let us define optimal action optimal
value state


  argmax q  s  a 



   

aa




v  s    max q  s  a   
aa

   

enumerated unstructured state spaces  state q values computed using dynamic programming methods resulting optimal policies complete state space 
recently  promising approaches exploiting relational structure proposed apply similar ideas solve approximate solutions rdmps abstract level  without
referring concrete objects o   see related work sec      alternatively  one may
reason grounded relational domain  makes straightforward account
noise outcome uniqueness requirement nid rules  usually  one focuses estimating optimal action values given state  approach appealing agents
varying goals  quickly coming plan problem hand
appropriate computing abstract policy complete state space  although
grounding simplifies problem  decision theoretic planning propositionalized representation challenging task complex stochastic domains  sections     
present different algorithms reasoning grounded relational domain estimating
optimal q values actions  and action sequences  given state 
    dynamic bayesian networks
dynamic bayesian networks  dbns  model development stochastic systems
time  prada planning algorithm introduce sec    makes use
kind graphical model evaluate stochastic effects action sequences factored
grounded relational world states  therefore  briefly review bayesian networks
dynamic extension here 
bayesian network  bn   jensen        compact representation joint probability distribution set random variables x means directed acyclic graph
g  nodes g represent random variables  edges define dependencies thereby express conditional independence assumptions  value x variable
x x depends values immediate ancestors g  called
parents p a x  x  conditional probability functions node define p  x   p a x   
case discrete variables  may defined form conditional probability tables 
bn compact representation distribution x nodes
parents conditional probability functions significant local structure 
play crucial role development graphical models prada 
  

filang   toussaint

dbn  murphy        extends bn formalism model dynamic system evolving
time  usually  focus discrete time stochastic processes  underlying
system  in case  world state  represented bn b  dbn maintains
copy bn every time step  dbn defined pair bns  b    b   
b   deterministic uncertain  prior defines state system
initial state      b two slice bn defines dependencies two
successive time steps      implements first order markov assumption 
variables time     depend variables time     variables t 

   planning look ahead trees
plan nid rules  one treat domain described
ulary relational markov decision process discussed sec 
present two value based reinforcement learning algorithms
generative model build look ahead trees starting initial
used estimate values actions states 

relational logic vocab     following 
employ nid rules
state  trees

    sparse sampling trees
sparse sampling tree  sst  algorithm  kearns et al         mdp planning samples
randomly sparse  full grown look ahead trees states starting given state
root  suffices compute near optimal actions state mdp  given
planning horizon branching factor b  sst works follows  see fig      tree
node  representing state    i  sst takes possible actions account   ii 
action takes b samples successor state distribution using generative model
transitions  e g  transition model mdp  build tree nodes next
level  values tree nodes computed recursively leaves root using
bellman equation  given node  q value possible action estimated
averaging values b children states action  then  maximizing
q value actions chosen estimate value given node  sst
favorable property independent total number states mdp 
examines restricted subset state space  nonetheless  exponential
time horizon taken account 
pasula et al         apply sst planning nid rules  sampling noise
outcome planning sst  assume stay state  discount
estimated value  refer adaptation speak sst planning
remainder paper  action unique covering rule  use noisy
default rule r predict effects  always better perform othing action
instead staying state get punished  hence  sst planning one
discard actions given state unique covering rules 
sst near optimal  practice feasible small branching factor
b planning horizon d  let number actions a  number nodes
horizon  ba d    this number reduced outcome rule sampled
multiple times   illustration  assume    possible actions per time step
set parameters     b      the choice pasula et al  experiments   plan
single action given state  one visit                       states  smaller
  

fiplanning noisy probabilistic relational rules

figure    sst planning algorithm samples sparse  full grown look ahead trees
estimate values actions states 

choices b lead faster planning  result significant accuracy loss realistic
domains  kearns et al  note  sst useful special structure permits
compact representation available  sec     introduce alternative planning
approach based approximate inference exploits structure nid rules 
    sampling trees upper confidence bounds
upper confidence bounds applied trees  uct  algorithm  kocsis   szepesvari 
      samples search tree subsequent states starting current state root 
contrast sst generates b successor states every action state  idea
uct choose actions selectively given state thus sample selectively
successor state distribution  uct tries identify large subsets suboptimal actions early
sampling procedure focus promising parts look ahead tree instead 
uct builds look ahead tree repeatedly sampling simulated episodes
initial state using generative model  e g  transition model mdp  episode
sequence states  rewards actions limited horizon d  s    r    a    s    r    a        sd   rd  
simulated episode  values tree nodes  representing states  updated
online simulation policy improved respect new values  result 
distinct value estimated state action pair tree monte carlo simulation 
precisely  uct follows following policy tree node s  exist actions
explored yet  uct samples one using uniform
distribution  otherwise  actions explored least once  uct selects
action maximizes upper confidence bound qo
u ct  s  a  estimated action
  

filang   toussaint

value qu ct  s  a  

qo
u ct  s  a 

  qu ct  s  a    c

log ns
 
ns a

u ct  s    argmax qo
u ct  s  a   

    
    



ns a counts number times actionpa selected state s  ns
counts total number visits state s  ns   ns a   bias parameter c defines
influence number previous action selections thereby controls extent
upper confidence bound 
end episode  value encountered state action pair  st       
  d  updated using total discounted rewards 
nst  at nst  at      
qu ct  st     qu ct  st      

    
 
nst  at


x

 

 

rt  qu ct  st       

    

t   t

policy uct implements exploration exploitation tradeoff  balances
exploring currently suboptimal looking actions selected seldom thus far
exploiting currently best looking actions get precise estimates values 
total number episodes controls accuracy ucts estimates balanced
overall running time 
uct achieved remarkable results challenging domains game go
 gelly   silver         best knowledge  first apply uct
planning stochastic relational domains  using nid rules generative model  adapt
uct cope noise outcomes fashion sst  assume stay
state discount obtained rewards  thus  uct takes actions unique
covering rules account  reasons sst does 

   planning approximate inference
uncertain action outcomes characterize complex environments  make planning relational domains substantially difficult  sampling based approaches discussed
previous section tackle problem repeatedly generating samples outcome
distribution action using transition probabilities mdp  leads lookahead trees easily blow planning horizon  instead sampling successor
states  one may maintain distribution states  so called belief  following 
introduce approach planning grounded stochastic relation domains propagates beliefs states sense state monitoring  first  show create
compact graphical models nid rules  develop approximate inference method
efficiently propagate beliefs  hand  describe probabilistic relational
action sampling dbns planning algorithm  prada   samples action sequences
informed way evaluates using approximate inference dbns  then 
example presented illustrate reasoning prada  finally  discuss prada
comparison approaches previous section  sst uct  present simple
extension prada 
  

fiplanning noisy probabilistic relational rules

 a 

 b 

figure    graphical models nid rules   a  naive dbn   b  dbn exploiting nid factorization

    graphical models nid rules
decision theoretic problems agents need choose appropriate actions represented means markov chains dynamic bayesian networks  dbns 
augmented decision nodes specify agents actions  boutilier et al         
following  discuss convert nid rules dbns prada algorithm
use plan probabilistic inference  denote random variables upper case letters
 e g  s   values corresponding lower case letters  e g   dom s    variable
vectors bold upper case letters  e g     s    s    s     value vectors bold lower
case letters  e g     s    s    s      use column notation  e g  s       s    s    s    
naive way convert nid rules dbns shown fig    a   states represented
vector    s            sn   ground predicate p binary si
ground function f sj range according represented
function  actions represented integer variable indicates action
vector ground action predicates a  reward gained state represented
u may depend subset state variables  possible express
arbitrary reward expectations p  u   s  binary u  cooper         define
transition dynamics using nid rules naive model  assume given set
fully abstract nid rules  compute groundings rules w r t  objects
domain get set k different ground nid rules  parents state variable
si  successor time step include action variable respective variable si
predecessor time step  parents si  determined follows 
rule r literal corresponding si  appears outcomes r  variables
sk corresponding literals preconditions r parents si    typically si 
manipulated several actions turn modeled several rules  total
number parents si  large  problem worsened usage deictic
references nid rules  increase total number k ground rules  
resulting local structure conditional probability function si  complex  one
account uniqueness covering rules  complex dependencies
two time slices make representation unfeasible planning 
  

filang   toussaint

therefore  exploit structure nid rules model state transition
compact graphical model shown fig    b  representing joint distribution
p  u    s    o  r    a  s 

 

p  u    s    p  s    o  r  s  p  o   r  p  r   a    p     s   

    

explain detail following  before  assume given set
fully abstract nid rules  compute set k different ground nid rules
w r t  objects domain  addition s  s    a  u u   above  use
binary random variable rule model event context holds 
case required literals hold  let i   indicator function  
argument evaluates true   otherwise  then 


k
k


 
p     s   
p  i  s i      
sj   sri  j  
    

i  

i  

j i  

v

use express logical conjunction   n   function    yields set
indices state variables s  depends  sri denotes configuration
state variables corresponding literals context ri   use integer valued
variable r ranging k    possible values identify rule predicts effects
action  exists  unique covering rule current state action pair 
i e   rule r  a  modeling action whose context holds 


 
p  r   r a      r  a  r    
r        
    
r   a   r 

unique covering rule exists  predict changes indicated special value
r      assuming execute action  similarly sst uct do  


 
 
p  r       a     
r    
r        
    
r   a   r 

r a 

integer valued variable represents outcome action predicted
rule  ranges possible values maximum number outcomes
rules have  ensure sound semantics  introduce empty dummy outcomes
zero probability rules whose number outcomes less   probability
outcome defined corresponding rule 
p  o     r    pr o  
define probability successor state

p  s    o  s  r   
p  s i   o  si   r   

    

    



one unique state constructed taking changes according
r o account  outcome specifies value si    value probability
  

fiplanning noisy probabilistic relational rules

one  otherwise  value state variable persists previous time step 
rules usually change small subset s  persistence often applies  resulting
dependency p  s i   o  r  si   variable si  time step     compact  contrast
naive dbn fig    a   three parents  namely variables outcome 
rule predecessor previous time step  simplifies specification
conditional probability function   significantly enables efficient inference 
see later  probability reward given


 
p  u         s     
sj    j  
    
j u    

function  u     yields set indices state variables s    u   depends 
configuration variables corresponds planning goal denoted
  uncertain initial states naturally accounted specifying priors p  s    
renounce specification prior here  however  initial state s  always given
experiments later enable comparison look ahead tree based approaches sst
uct require deterministic initial states  which might sampled
prior   choice distribution p  a  used sampling actions described
sec      
simplicity ignored derived predicates functions defined
terms predicates functions presentation graphical model  derived
concepts may increase compactness rules  dependencies among concepts acyclic 
straightforward include derived concepts model intra state dependencies
corresponding variables  indeed  use derived predicates experiments 
interested inferring posterior state distributions p  st   a  t    given sequence previous actions  where omit conditioning initial state simplicity  
exact inference intractable graphical model  constructing junction tree 
get cliques comprise whole markov slices  all variables representing state
certain time step   consider eliminating state variables st     due moralization 
outcome variable connected state variables st   elimination o 
variables st form clique  thus  make use approximate inference
techniques  general loopy belief propagation  lbp  unfeasible due deterministic
dependencies small cycles inhibit convergence  conducted preliminary tests small networks damping factor  without success  interesting
open question whether ways alternate propagating deterministic information running lbp remaining parts network  e g   whether methods
mc sat  poon   domingos        successfully applied decision making contexts ours  next subsection  propose different approximate inference scheme
using factored frontier  ff   algorithm describes forward inference procedure
computes exact marginals next time step subject factored approximation
previous time step  here  advantage exploit structure
involved dbns come formulas marginals  related passing
forward messages  contrast lbp  information propagated backwards  note
approach condition rewards  as full planning inference  samples
actions  backward reasoning uninformative 
  

filang   toussaint

    approximate inference
following  present efficient method approximate inference previously
proposed dbns exploiting factorization nid rules  focus mathematical
derivations  illustrative example provided sec      
follow idea factored frontier  ff  algorithm  murphy   weiss       
approximate belief product marginals 

p  st   a  t   
p  sti   a  t     
    


define
 sti      p  sti   a  t   
 st      p  st   a  t   

n


    
 sti  

    

i  

derive filter dbn model fig    b   interested inferring
state distribution time     given action sequence a  t calculate marginals
state attributes
t  
 st  
  a  t  
    p  si
x
 
p  st  
  rt   a  t    p  rt   a  t    


    
    

rt

eq        use rules prediction  weighted respective posteriors p  rt   a  t   
reflects fact depending state use different rules model
action  weight p  rt   a  t     rules modeling action   remaining
rules model   weights correspond posterior parts
state space according rule used prediction 
compute first term     
x
p  st  
  rt   a  t     
p  st  
  rt   sti   p  sti   rt   a  t   


sti



x

p  st  
  rt   sti    sti    


    

sti

here  sum possible values variable si previous time step t  intuitively  take account potential pasts arrive value st  
next

  st   enables us easily predict probabilities
time step  resulting term p  st  
 
r


next time step discussed below  prediction weighted marginal
 sti   respective previous value  approximation      assumes sti conditionally independent rt   true general choice rule prediction
depends current state thus attribute si   improve approximation one examine whether sti part context rt   case  infer
state sti knowing rt   however  found approximation sufficient 
  

fiplanning noisy probabilistic relational rules

one would expect  calculate successor state distribution p  st  
  rt   sti  


taking different outcomes r account weighted respective probabilities
p  o   rt   
x
p  st  
  rt   sti    
p  st  
  o  rt   sti   p  o   rt    
    




shows us update belief sit   predict rule rt   p  st  
  o  rt   sti  

t  
deterministic distribution  changes value si   si set accordingly  otherwise  value sti persists 
lets turn computation second term eq        p  rt   a  t    posterior
rules  trick use context variables exploit assumption
rule r models state transition uniquely covers  at   st    indicated
appropriate assignment   reduced expression
involving marginals     start
x
p  rt   r   a  t    
p  rt   r     a  t   p  t   a  t  





 

  i r  at    p tr     

tr        a  t 

r   at    r 




 

  i r  at    p  tr       a  t    p

tr        tr      a  t   

r   at    r 

    
simplify summation   consider unique assignment
context variables r used prediction  provided models action  indicated
i r  at     case context tr holds  contexts tr 
competing rules r  action hold 
calculate second term      summing states
x

x
p  tr       a  t     
p  tr       st    st  
p  tr       st  
 stj  
    
st

 



st

 sjt   sr j  

 

j

    

j tr  

approximation      assumption        sr denotes configuration
state variables according context r       sum variables
context r  variables rs context remain  terms  sjt   sr j   correspond
probabilities respective literals 
third term      joint posterior contexts competing rules r 
given rs context already holds  interested situation none
contexts hold  calculate


 

p
tr        tr      a  t 
p  tr        tr      a  t     
    
r   at    r 

r   at    r 

  

filang   toussaint

approximating product individual posteriors  latter computed
x
p  tr        tr      a  t     
p  tr        st   p  st   tr      a  t   
    



r r 
    q

    i t     si   sr   i   otherwise
 

r 


    

i  r  

if condition expresses logical contradiction contexts r r   
contexts contradict  r  context surely hold given rs context holds 
otherwise  know state attributes apppearing contexts r r 
hold condition r      therefore  examine remaining state
attributes r  context  again  approximate posterior marginals 
finally  compute reward probability straightforwardly
x

p  u       st  p  st   a  t    s   
 sit      
    
p  u       a  t     
st

i u  

denotes configuration state variables corresponding planning goal
      above  summation states simplified assumption resulting
product marginals required state attributes 
overall computational costs propagating effects action quadratic
number rules action  for rule calculate probability
none others applies  linear maximum numbers context literals
manipulated state attributes rules 
inference framework requires approximation distribution p  s    r     s 
 cf  eq       cope noise outcome nid rules  training data used
learn rules  estimate predicates functions change value time follows  let
sc contain corresponding variables  estimate rule r average number
n r changed state attributes noise outcome applies  due factored frontier
approach  consider noise effects variable independently  approximate
r
probability si sc changes rs noise outcome   snc     case change 
changed values si equal probability 
    planning
dbn representation fig    b  together approximate inference method described last subsection enable us derive novel planning algorithm stochastic
relational domains  probabilistic relational action sampling dbns planning algorithm  prada  plans sampling action sequences informed way based predicted
beliefs states evaluating action sequences using approximate inference 
precisely  sample sequences actions a  t   length        
infer posteriors states p  st   a  t    s    rewards p  ut   a  t    s     in sense
filtering state monitoring   then  calculate value action sequence
discount factor        
q s    a  t       


x

p  u       a  t    s     

t  

  

    

fiplanning noisy probabilistic relational rules

choose first action best sequence   argmaxa  t   q a  t     s    
value exceeds certain threshold  e g         otherwise  continue sampling actionsequences either action found planning given up  quality found
plan controlled total number action sequence samples traded
time available planning 
aim strategy sample good action sequences high probability 
propose choose equal probability among actions unique covering
rule current state  thereby  avoid use noisy default rule r
models action effects noise thus poor use planning  action time t 
prada samples distribution


 
x

tr        a  t   
    
p tr     
psample
 a 
r a 

r   a   r 

sum rules action a  rule add posterior
unique covering rule  i e  context tr holds  contexts tr  competing
rules r  hold  sampling distribution takes current state distribution
account  thus  probability sample action sequence predicting state sequence
s            st depends likelihood state sequence given a  likely required outcomes are  likely next actions sampled  using policy 
prada miss actions sst uct explore  following proposition
states  proof appendix a  
proposition    set action sequences prada samples non zero probability
super set ones sst uct 
experiments  replan action executed without reusing knowledge previous time steps  simple strategy helps get general impression
pradas planning performance complexity  strategies easily conceivable 
instance  one might execute entire sequence without replanning  trading faster
computation times potential loss achieved reward  noisy environments 
might seem better strategy combine reuse previous plans replanning 
instance  one could omit first action previous plan  executed 
examine suitability remaining actions new state  consider
single best action sequence  many planning domains might beneficial
marginalize sequences first action  instance  action a 
might lead number reasonable sequences  none best  another
action a  first one good sequence  many bad ones case one
might favor a   
    illustrative example
let us consider small planning problem table   illustrate reasoning procedure
prada  domain noisy cubeworld represented predicates table x   cube x  
on x     inhand x  clear x  y on y  x  robot perform two types
actions  may either lift cube x means action grab x  put cube
  

filang   toussaint

held hand top another object x using puton x   start state s  shown   a 
contains three cubes a  b c stacked pile table t  goal shown   b 
get middle cube b on top top cube a  world model provides three abstract
nid rules predict action effects  shown table   c   first rule uncertain
outcomes  models grab object another object  contrast  grabbing
clear object  rule    putting object somewhere  rule    always leads
successor state 
first  prada constructs dbn represent planning problem  purpose 
computes grounded rules respect objects    a  b  c  t  shown   d  
potential grounded rules ignored  one deduce abstract rules
predicates changeable  combination specifications s    prunes
grounded rules  instance  know s  table  thus  ground rule
action argument x   needs constructed rules require cube x  
based dbn  prada samples action sequences evaluates expected
rewards  following  investigate procedure sampling action sequence
 grab b   puton a    table   e  presents inferred values dbn variables
auxiliary quantities  marginals  eq        state variables    
set deterministically according s    calculate posteriors context variables
p     a  t    according eq        example      one rule
probability     actions grab a   grab b  grab c   contrast 
rules non zero probability various puton   actions  help eq       
calculate probability rule r unique covering rule respective
action  listed unique rule  note condition fixed action thus
far   case context r r holds  contexts r  competing rules
r  action hold       posterior r alone 
resulting probabilities used calculate sampling distribution eq        first 
compute probability action unique covering rule simple
sum probabilities previous step  listed action coverage table   then 
normalize values get sampling distribution psample          results
sampling distribution uniform three actions unique rules  assume
sample a    grab b   grabbing blue cube b   variable r specifies ground rules
use predicting state marginals next time step  infer posterior
according eq        here  p  r        b act    a          
things get interesting      here  observe effects factored
frontier  instance  consider calculating posterior context r ground rule
r       b att   grabbing blue cube b yellow a  using eq       
p     b att    a     on a  b    on b  t    cube a    cube b    table t  
                             
contrast  exact value p     b att    a           according third outcome
abstract rule   used predict a    imprecision due ignoring correlations 
regards marginals on a  b  on b  t  independent  fact fully
correlated 
     action grab a  three ground rules non zero context probabilities
 grabbing either b  c t   due three different outcomes abstract
  

fiplanning noisy probabilistic relational rules

table    example pradas factored frontier inference
 a  start state
s     on a  b   on b  c   on c  t  
cube a   cube b   cube c   table t  

 b  goal
   on b  a  

 c  abstract nid rules example situations
rule   
grab x    on y  x   on x  z   cube x   cube y    table t  

      inhand x   on y  z   on y  x   on x  z 
      inhand x   on y     on y  x   on x  z 


      on x     on x  z 

rule   
grab x    cube x   clear x   on x   

      inhand x   on x   


 e  inferred posteriors pradas

inference

action sequence
 grab b   puton a  
t  

t  

t  

state marginals
on a  b 
on a  c 
on a  t 
on b  a 
on b  c 
on b  t 
on c  t 
inhand b 
clear a 
clear b 
clear c 
goal u

   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
    
   
   
   
   

p     a  t   
   b act 
   b att 
   c btt 
   a b 
   a c 
   a t 
   b t 
   c t 
   a b 
   c b 
   t b 

   
   
   
   
   
   
   
   
   
   
   

   
    
   
   
   
   
    
   
   
   
   

unique rule
    b act 
   
   
    b att 
          
    c att 
   
    
    c btt 
   
   
    a b 
   
    
    a c 
   
    
    a t 
   
    
    b t 
         
    c t 
   
    
    a b 
   
   
    c b 
   
   
    t b 
   
   
action coverage
grab a 
   
    
grab b 
         
grab c 
   
   
puton a 
   
   
puton c 
   
   
puton t 
   
   
sample distribution
psample  grab a  
          
psample  grab b  
           
psample  grab c  
          
psample  puton a  
         
psample  puton c  
         
psample  puton t  
         
p  rt   rt   a  t  
rt       b act 
   
   
rt       a b 
   
   
rt    
   
   

rule   
puton x    inhand y    cube y  

      on y  x   inhand x 


 d  grounded nid rules
grounded rule action
substitution
    a bbt 
grab a   x a  b  z b  t 
    a bct 
grab a   x a  b  z c  t 
   
    c bbt 
grab c   x c  b  z b  t 
    a b 
grab a 
 x a  b 
    a c 
grab a 
 x a  c 
    a t 
grab a 
 x a  t 
   
    c t 
grab c 
 x c  t 
    a b 
puton a 
 x a  b 
    a c 
puton a 
 x a  c 
   
    t c 
puton t 
 x a  c 

  

filang   toussaint

rule    example  calculate probability rule     a c   grabbing c 
unique covering rule grab a     
p     a c      a b       a t    a   
p     a c    a        p     a b    a         p     a t    a    
                                
calculations  determine sampling distribution      assume
sample action puton a   results rule    a  b   putting b a  used
prediction     probability since probability unique covering rule
action puton a   remaining mass     posterior assigned parts
state space unique covering rule available puton a   case  use
default rule r      corresponding performing action  probability
    values state variables persist 
finally  let us infer marginals     using eq        example  calculate
 inhand b t      let i b  brief inhand b   sum ground rules rt   taking
potential values i b t   i b t   previous time step     account 
x
 i b t    
p  rt     a        p  i b t     rt     i b t      i b t    
rt  

  p  i b t     rt     i b t      i b t      
                                                            
discussed above  ground rule    a  b  default rule play role
prediction  effect  belief b inhand decreases          tried
put b a  expected  similarly  calculate posterior on b  a      
expected probability reach goal performing actions grab b 
puton a    here  pradas inferred value coincides true posterior  
comparison  probability reach goal     performing actions
grab a   puton t   grab b  puton a   i e   clear b grab it  plan
safer  i e   higher probability  takes actions 
    comparison planning approaches
prominent difference presented planning approaches way
account stochasticity action effects  one hand  sst uct repeatedly take samples successor state distributions estimate value action
building look ahead trees  hand  prada maintains beliefs states
propagates indetermistic action effects forward  precisely  prada sst follow
opposite approaches  prada samples actions calculates state transitions approximately means probabilistic inference  sst considers actions  and thus exact
action search  samples state transitions  price considering actions
ssts overwhelmingly large computational cost  uct remedies issue samples action sequences thus state transitions selectively  uses previously sampled episodes
build upper confidence bounds estimates action values specific states 
used adapt policy next episode  straightforward translate
  

fiplanning noisy probabilistic relational rules

adaptive policy prada since prada works beliefs states instead states
directly  therefore  chose simple policy prada sample randomly
actions unique covering rule state  in form sampling distribution
account beliefs states  
prada returns whole plan transform world state one goal
fulfilled probability exceeding given threshold   spirit conformant planning probabilistic planning observability  kushmerick  hanks    weld        
due outcome sampling  sst uct cannot return plan straightforward way  instead  provide policy many successor states based estimates
action values look ahead tree  estimates states deeper tree
less reliable built less episodes  action executed
new state observed  estimates reused  thus far  prada take
knowledge gained previous action sequence samples account adapt policy 
elegant way achieve better exploit goal knowledge might use backpropagation
dbns plan completely inference  toussaint   storkey        
beyond scope paper  clear principled way
large state action spaces relational domains  alternatively  prada could give high
weight second action previous best plan  sec       show another
simple way make use previous episodes find better plans 
prada afford simple action sampling strategy evaluates large numbers
action sequences efficiently grow look ahead trees account
indeterministic effects  points important difference  three algorithms faced
search spaces action sequences exponential horizon  calculate
value given action sequence  however  sst uct still need exponential time due
outcome sampling  contrast  prada propagates state transitions forward
thus linear horizon 
approximate planning algorithms  neither sst  uct prada expected perform ideally situations  sst uct sample action outcomes hence
face problems important outcomes small probability  instance  consider
agent wants escape room two locked doors  hits first door
made wood chance      break escape  second door made
iron chance       break  sst uct may take long time
detect    times better repeatedly hit wooden door  contrast  prada
recognizes immediately reasoned actions takes
outcomes account  hand  pradas approximate inference procedure correlations among state variables get lost sst uct preserve
sample complete successor states  impair pradas planning performance
situations correlations crucial  consider following simple domain two
state attributes b  agent choose two actions modeled rules

action   
action   






      a  b
 
      a  b



      a  b
      b 




  

 

filang   toussaint

goal make attributes either true false  i e      a b   a b  
actions  resulting marginals  a          a          b       
 b         due factored frontier  prada cannot distinguish actions
although action  achieve goal  action  not 
pradas estimated probabilities states rewards may differ significantly
true values  harm performance many domains experiments
indicate  sec      suppose reason pradas estimated probabilities imprecise  enable correct ranking action sequences planning 
interested choosing best action instead calculating correctly value 
difference proposed algorithms way handle noise
outcome rules  prada assigns small probability successor states spirit
noise outcome  contrast  sst uct make sense sample
distribution  single successor state extremely low probability
inadequate estimate state action values  hence  use described workaround
assume stay state  discounting obtained rewards 
straightforward prada deal uncertain initial states  uncertainty
initial states common complex environments may instance caused partial
observability noisy sensors  uncertainty natural representation belief
state prada works on  contrast  sst uct cannot account uncertain initial
states directly  would sample prior distribution 
    extension  adaptive prada
present simple extension prada increase planning accuracy 
exploit fact prada evaluates complete sequences actions contrast sst
uct actions taken     depend sampled outcomes  adaptive
prada  a prada  examines best action sequence found prada  prada
chooses first action sequence without reasoning  a prada inspects
single action sequence decides simulation whether deleted 
resulting shortened sequence may lead increased expected reward  case
actions significant effects achieving goal decrease success
probability  actions omitted  states high reward reached earlier
rewards discounted less  instance  consider goal grab blue ball 
action sequence grabs red cube  puts onto table grabs blue
ball improved omitting first two actions unrelated goal 
precisely  a prada takes pradas action sequence ap highest value
investigates iteratively action whether deleted  action
deleted plan resulting plan higher reward likelihood  idea
formalized algorithm    crucial calculation algorithm compute values
q s    a  t     defined eq       restated convenience 
 

q s  

  t  

  


x

p  u       a  t    s     

t  

pradas approximate inference procedure particularly suitable calculating required p  u       a  t    s     performs calculation time linear length
  

fiplanning noisy probabilistic relational rules

algorithm   adaptive prada  a prada 
input  pradas plan ap
output  a pradas plan aa
   aa ap
          
  
true
  
let plan length  
  
a  t  a  t 
b omit

t   t  
t t  
  

aa
  
  othing
  
q s    a    q s    aa  
  
aa
   
else
   
break
   
end
   
end
    end
    return aa

action sequence  sst uct would require time exponential
outcome sampling 

   evaluation
implemented presented planning algorithms learning algorithm
nid rules c    code available www user tu berlin de lang prada  
evaluate approaches two different scenarios  first intrinsically noisy complex simulated environment learn nid rules experience use
plan  second  apply algorithms benchmarks uncertainty part
international planning competition      
    simulated robot manipulation environment
perform experiments simulated complex robot manipulation environment
robot manipulates objects scattered table  fig      report results three
series experiments different tasks increasing difficulty  first describe domain
detail  use  d rigid body dynamics simulator  ode  enables realistic behavior objects  simulator available www user tu berlin de lang dwsim  
objects cubes balls different sizes colors  robot grab objects
put top objects table  actions robot affected
noise  domain  towers objects straight lined  easier put object
top big cube top small cube difficult put something
top ball  piles objects may topple over  objects may fall table case
become reach robot 
represent domain predicates on x     inhand x   upright x   out x   if
object fallen table   function size x  unary typing predicates cube x  
ball x   table x   predicates obtained querying state simulator
  

filang   toussaint

figure    simulated robot plays cubes balls different sizes scattered
table  objects fallen table cannot manipulated anymore 

translating according simple hand made guidelines  thereby sidestepping difficult
problem converting agents observations internal representation  instance 
on a  b  holds b exert friction forces z coordinate greater
one b  x  y coordinates similar  besides primitive
concepts  use derived predicate clear x  y on y  x   found
predicate enable compact accurate rules  reflected values
objective function rule learning algorithm given eq      
define three different types actions  actions correspond motor primitives
whose effects want learn exploit  grab x  action triggers robot open
hand  move hand next x  let grab x raise robot arm again 
execution action influenced factors  example  different
object held hand before  fall either table third
object   objects top x  likely fall down 
puton x  action centers robots hand certain distance x  opens
raises hand again  instance  object z x  object
potentially inhand may end z z might fall x  othing   action triggers
movement robots arm  robot might choose action thinks
action could harmful respect expected reward  emphasize
actions always execute  regardless state world  also  actions
rather unintuitive humans trying grab table put object top
carried out  robot learn effects motor primitives 
due intrinsic noise complexity  simulated robot manipulation scenario
challenging domain learning compact world models well planning 
objects f different object sizes  action space contains  o   actions
 
state space huge f  o   o different states  not excluding states one would classify
impossible given intuition real world physics  
use rule learning algorithm pasula et al         parameter
settings learn three different sets fully abstract nid rules  rule set learned
  

fiplanning noisy probabilistic relational rules

independent training sets     experience triples  s  a  s    specify world
changed state successor state s  action executed  assuming full
observability  training data learn rules generated world six cubes four
balls two different sizes performing random actions slight bias build high
piles  resulting rule sets contain          rules respectively  rule sets provide
approximate partial models true world dynamics  generalize situations
experiences  may account situations completely different
agent seen before  enforce compactness avoid overfitting  rules
regularized  hence  learning algorithm may sometimes favor model rarely experienced
state transitions low probability outcomes general rules  thereby trading
accuracy compactness  combination general noisiness world
causes need carefully account probabilities world reasoning
rules 
perform three series experiments planning tasks increasing difficulty 
series  test planners different worlds varying numbers cubes
balls  thus  transfer knowledge gained training world different  similar
worlds using abstract nid rules  object number  create five different worlds 
per rule set world  perform three independent runs different random seeds 
evaluate different planning approaches  compute mean performances
planning times fixed  but randomly generated  set    trials    learned rule sets 
  worlds    random seeds  
choose parameters planning algorithms follows  sst  report results different branching factors b  far resulting runtimes allow  similarly  uct
 a  prada parameter balances planning time quality
found actions  uct  number episodes   a  prada
number sampled action sequences  depending experiment  set
heuristically tradeoff planning time quality reasonable 
particular  fair comparison pay attention uct  prada a prada get
planning times  reported otherwise  furthermore  uct set
bias parameter c     found heuristically perform best  planners
experiments  set discounting factor future rewards         crucial
parameter planning horizon d  heavily influences planning time  course 
cannot known a priori  therefore  reported otherwise  deliberately set larger
required uct  a  prada suggest algorithms effective
estimated  indeed  found experiments long
small  exact choice significant effects ucts  a  pradas
planning quality unlike effects planning times  contrast  set horizon
sst always small possible  case planning times still large 
planning algorithm find suitable action given situation  restart
planning procedure  sst builds new tree  uct runs episodes  a  prada
takes new action sequence samples  given situation    planning runs suitable
action still found  trial fails 
furthermore  use ff replan  yoon et al         baseline  discuss
detail related work sec     ff replan determinizes planning problem 
thereby ignoring outcome probabilities  ff replan shown impressive results
  

filang   toussaint

domains probabilistic planning competitions  domains carefully designed
humans  action dynamics definitions complete  accurate consistent
used true world dynamics according experiments contrast learned
nid rules use estimate approximate partial models robot manipulation
domain  able use derived predicate clear x  ff replan implementation
experiments  included appropriate literals predicate hand
outcomes rules sst  uct  a  prada implementations infer
values automatically definition clear x   report results ff replan
 almost original  learned rules using all outcomes determinization scheme  denoted
ff replan all below   using single outcome schemes always led worse performance  
rules general  putting restrictions arguments
deictic references   case  actions appear applicable given state make
sense intuitive human perspective hurts ff replan much
methods  resulting large planning times ff replan  instance  rule may model
toppling small tower including object x trying put object top
tower  one outcome might specify end x  possible
cube  course  learning algorithm may choose omit typing predicate
cube x  due regularization  prefers compact rules none experiences might
require additional predicate  therefore  created modified rule sets hand
introduced typing predicates appropriate make contexts distinct  below 
denote results modified rule sets ff replan all  ff replan single  
using all outcomes single most probable outcome determinization schemes 
      high towers
first series experiments  investigate building high towers planning
task work pasula et al          precisely  reward state defined
average height objects  constitutes easy planning problem many different
actions may increase reward  object identities matter  small planning
horizon sufficient  set sst horizon      pasula et al  choice  different
branching factors b uct  a  prada horizon      experiments  initial
states contain already stacked objects  reward performing actions
   table   fig    present results  sst competitive  branching factor
b      slower uct  a  prada least order magnitude 
b      performance poor  series experiments  designed worlds   
objects contain many big cubes  explains relatively good performance sst
worlds  number good plans large  mentioned above  control uct 
prada a prada times available planning  three
approaches perform far better sst almost experiments  difference
uct  prada a prada never significant 
series experiments indicates planning approaches using full grown lookahead trees sst inappropriate even easy planning problems  contrast  approaches exploit look ahead trees clever way uct seem best
choice easy tasks require small planning horizon solved many
alternative good plans  performance planning approaches using approximate
  

fiplanning noisy probabilistic relational rules

table    high towers problem  reward denotes discounted total reward different
numbers objects  cubes balls table   reward performing actions
   data points averages    trials created   learned rule sets 
  worlds   random seeds  standard deviations mean estimators
shown  ff replan all  ff replan single  use hand made modifications
original learned rule sets  fig    visualizes results 
objects

planner
ff replan all
ff replan all 
ff replan single 

   

sst  b   
sst  b   
sst  b   
uct
prada
a prada

sst  b   
sst  b   
sst  b   
uct
prada
a prada

sst  b   
sst  b   
sst  b   
uct
prada
a prada

         
         
         

          
         
         

    
    
    
    
    
    

         
            
            
         
         
         

         
         
         

           
           
         








    
    
    
    
    
    

          
          
            
          
          
          

         
         
         

            
           
         








            
            
        
          
          
          

    
     
     
     
     
     

ff replan all
ff replan all 
ff replan single 
    

trial time  s 

     
     
     
     
     
     

ff replan all
ff replan all 
ff replan single 
   

reward

     
     
     
     
     
     

  








    
    
    
    
    
    

filang   toussaint

ff replan all
ff replan all 
ff replan single 
sst b  
sst b  
sst b  
uct
prada
a prada

  
  
 

    

trial time  s 

discounted total reward

     

   
  
 

 

 
objects

  

 

 a  reward

 
objects

  

 b  time

figure    high towers problem visualization results presented table    reward
performing actions    data points averages    trials created
  learned rule sets    worlds   random seeds  error bars standard
deviations mean estimators shown  please note log scale  b  

inference  prada a prada  however  comes close one uct  showing
suitability scenarios 
ff replan focuses exploiting conjunctive goal structures cannot deal quantified goals  grounded reward structure task consists disjunction
different tower combinations  ff replan pick arbitrary tower combination
goal  therefore  apply ff replan sample tower combinations according rewards achieve  i e   situations high towers probable  exclude
combinations balls bottom towers prohibited reward
structure  yoon et al  note  obvious pitfall  goal formula sampling  approach
groundings goal reachable much expensive reach
initial state  ff replan cannot find plan  execute action 
sample new ground goal formula next time step  preserving already achieved
tower structures 
ff replan performs significantly worse previous planning approaches 
major reason ff replan often comes plans exploiting low probability
outcomes rules contrast sst  uct  a  prada reason
probabilities  illustrate this  consider example rule fig    models putting
ball top cube  two explicit outcomes  ball usually ends
cube  sometimes  however  falls table  ff replan misuse rule tricky
way put ball table ignoring often fail  results ffreplan single  show  taking probable outcomes account remedy
problem  often two three outcomes similar probabilities
choice seems unjustified  sometimes  intuitively expected outcome split
different outcomes low probabilities  however vary features irrelevant
planning problem  such upright    
  

fiplanning noisy probabilistic relational rules

table    desktop clearance problem  reward denotes discounted total reward different numbers objects  cubes balls table   reward performing
actions    data points averages    trials created   learned rulesets    worlds   random seeds  standard deviations mean estimators
shown  ff replan all  ff replan single  use hand made modifications
original learned rule sets  fig    visualizes results 
obj 

planner
ff replan all
ff replan all 
ff replan single 

   

sst  b   
uct
prada
a prada

sst  b   
uct
prada
a prada

sst  b   
uct
prada
a prada

         
         
         

        
       
       

    
    
    
    

           
        
        
        

         
         
         

        
       
       






    
    
    
    

        
         
         
         

         
         
         

         
        
       


          
          
          

   h
         
         
         

    
     
     
     

ff replan all
ff replan all 
ff replan single 
    

trial time  s 

    
    
     
     

ff replan all
ff replan all 
ff replan single 
   

reward






      desktop clearance
task second series experiments clear desktop  objects lying
splattered table beginning  object cleared part tower
containing objects class  object class simply defined terms
color additionally provided state representation robot  reward
robot defined number cleared objects  experiments  classes contain
    objects   ball  in order enable successful piling   starting situations contain piles  objects different classes  thus  reward
performing actions    desktop clearance difficult building high towers 
number good plans yielding high rewards significantly reduced 
set planning horizon     optimal sst required clear
class   objects  namely grabing putting three objects  above  contrast set
     uct  a  prada show deal overestimation
usually unknown optimal horizon d  table   fig    present results  horizon
    overburdens sst seen large planning times  even b      sst
takes almost    minutes average worlds   objects    hours worlds
  objects  therefore  try sst greater b  contrast  planning times
  

fi  

     

  

ff replan all
ff replan all 
ff replan single 
sst b  
uct
prada
a prada

  
  
 
 
 
 
 

 
objects

    
trial time  s 

discounted total reward

lang   toussaint

   
  
 
 

  

 a  reward

 
objects

  

 b  time

figure    desktop clearance problem  visualization results presented table   
reward performing actions    data points averages    trials
created   learned rule sets    worlds   random seeds  error bars
standard deviations mean estimators shown  note log scale  b  

uct  prada a prada  controlled enable
reasonable performance  two orders magnitude smaller  although overestimating
planning horizon  trial take average   s worlds   objects      
minutes worlds   objects     minutes worlds    objects  nonetheless  uct 
prada a prada perform significantly better sst  worlds  prada
a prada turn outperform uct  particular worlds many objects  a prada
finds best plans among planners  planners gain reward worlds   objects
comparison worlds   objects  number objects cleared increases
well number classes thus good plans  worlds    objects contain
numbers object classes worlds   objects  objects 
making planning difficult 
overall  findings desktop clearance experiments indicate sst
inappropriate  uct achieves good performance planning scenarios require medium
planning horizons several  many alternative plans  approaches
using approximate inference prada a prada  however  seem appropriate scenarios intermediate difficulty 
furthermore  results indicate ff replan inadequate clearance task 
sample target classes randomly provide goal structure ff replan  tower
structure within target class turn randomly chosen  bad performance
ff replan due reasons described previous experiments  particular
plans ff replan often rely low probability outcomes 
  

fiplanning noisy probabilistic relational rules

table    reverse tower problem  trial times numbers executed actions given
successful trials different numbers objects  cubes table  
data points averages    trials created   learned rule sets    worlds
  random seeds  standard deviations mean estimators shown  ffreplan all  ff replan single  use hand made modifications original
learned rule sets 
objects

   

   

   

planner

success rate

trial time  s 

executed actions

ff replan all
ff replan all 
ff replan single 

    
    
    

       
        
       

         
        
        

sst  b   
sst  b   
uct
prada
a prada

    
    
    
    
    

    day
            
        
        

        
        
        

ff replan all
ff replan all 
ff replan single 

    
    
    

          
        

        
        

uct
prada
a prada

    
    
    

   h
        
        

        
        

ff replan all
ff replan all 
ff replan single 

    
    
    

           
          

        
        

prada
a prada

    
    

           
           

        
        

      reverse tower
explore limits uct  prada a prada  conducted final series
experiments task reverse towers c cubes requires least  c
actions  each cube needs grabbed put somewhere least once   apart
long planning horizon  difficult due noise simulated world  towers
become unstable topple cubes falling table  decrease noise
slightly obtain reliable results  forbid robot grab objects clear
 i e   objects   set limit    executed actions trial  thereafter
reversed tower still built  trial fails  trial fails one required
objects falls table 
table   presents results  cannot get sst optimal planning horizon     
solve problem even five cubes  although space possible actions reduced
due mentioned restriction  sst enormous runtimes  b      sst find
suitable actions  no leaves goal state  several starting situations increased
planning horizon leads high probability sampling least one unfavorable outcome
required action  b    single tree traversal sst takes day 
found uct require large planning times order achieve reasonable success
rate  therefore  set planning horizons optimal uct  worlds   cubes  uct
optimal      success rate     taking average   
  

filang   toussaint

minutes case success    cubes  however  uct optimal      never succeeds
even planning times exceed   hours  contrast  afford overestimating
horizon      prada a prada  worlds   cubes  prada a prada
achieve success rates         respectively less half minute  a pradas
average number executed actions case success almost optimal  worlds  
cubes  success rates prada a prada still      taking bit
minute average case success  trials fail  often due
cubes falling table cannot find appropriate actions  cubes
falling table main reason success rates prada a prada
drop         respectively worlds   cubes towers become rather unstable 
planning times successful trials  however  increase    minutes indicating
limitations planning approaches  nonetheless  mean number executed
actions successful trials still almost optimal a prada 
overall  reverse tower experiments indicate planning approaches using lookahead trees fail tasks require long planning horizons achieved
plans  given huge action state spaces relational domains  chances
uct simulates episode exactly required actions successor states
small  planning approaches using approximate inference prada a prada
crucial advantage stochasticity actions affect runtime
exponentially planning horizon  course  search space action sequences still
exponential planning horizon problems requiring long horizons hard
solve them  experiments show using simple  though principled
extension a prada  gain significant performance improvements 
results show ff replan fails provide good plans using original
learned rule sets  surprising characteristics reverse tower task seem
favor ff replan comparison methods  single conjunctive goal
structure number good plans small plans require long horizons 
results ff replan all  ff replan single  indicate  ff replan achieve
good performance adapted rule sets modified hand restrict
number possible actions state  constitutes proof concept
ff replan  shows difficulty applying ff replan learned rule sets 

      summary
results demonstrate successful planning learned world models  here
form rules  may require explicitly account quantification predictive uncertainty  concretely  methods applying look ahead trees  uct  approximate
inference   a  prada  outperform ff replan different tasks varying difficulty  furthermore   a  prada solve planning tasks long horizons  uct fails 
one post processes learned rules hand clarify application contexts
planning problem uses conjunctive goal structure requires long plans 
ff replan performs better uct  a  prada 
  

fiplanning noisy probabilistic relational rules

    ippc      benchmarks
second part evaluation  apply proposed approaches benchmarks
latest international probabilistic planning competition  uncertainty part
international planning competition       ippc         involved domains differ
many characteristics  number actions  required planning horizons
reward structures  competition results show  planning algorithm performs
best everywhere  thus  benchmarks give idea types problems sst 
uct  a  prada may useful  convert ppddl domain specifications
nid rules along lines described sec  b    resulting rule sets used run
implementations sst  uct  a  prada benchmark problems 
seven benchmark domains consists    problem instances  instance
specifies goal starting state  instances vary problem size 
reward structures  including action costs   direct comparison always
possible  competition  instance considered independently  planners
given restricted amount time     minutes problems     domain   
minutes others  cover many repetitions problem instance
possible maximum     trials  trials differed random seeds resulting
potentially different state transitions  planners evaluated respect
number trials ending goal state collected reward averaged trials 
eight planners entered competition  including ff replan official participant  discussed related work sec     results 
voluminous presented here  refer reader website competition  below  provide qualitative comparison methods results
planners  attempt direct quantitative comparison several reasons  first 
different hardware prevents timing comparisons  second  competition participants
frequently able successfully cover trials single instances domain 
difficult tell reasons results tables  planner might
overburdened problem  might faced temporary technical problems
client server architecture framework competition could cope certain
ppddl constructs could rewritten simpler format 
third importantly  optimized implementations reuse previous planning efforts  instead  fully replan single action  within trial
across trials   competition evaluation scheme puts replanners disadvantage  in
particular replan single action   instead replanning  good strategy
competition spend planning time starting first trial reuse
resulting insights  such conditional plans value functions  subsequent trials
minimum additional planning  indeed  strategy often adopted
many trial time results indicate  acknowledge fair procedure evaluate
planners compute policies large parts state space acting  feel 
however  counter idea approaches  uct  a  prada
meant flexible planning varying goals different situations  thus 
interested average time compute good actions successfully solve problem
instance prior knowledge available 
  

filang   toussaint

table    benchmarks ippc       first column table specifies problem
instance  suc  success rate  trial time number executed
actions given successful trials  applicable  reward
trials shown  results achieved full replanning within trial
across trials 
 a  search rescue
planner

suc  trial time  s 

actions

 c  blocksworld

reward

sst
uct
  
prada
a prada

   
  
   
   

       
      
      
      

sst
uct
prada
a prada

   
  
   
   

        
      
      
      

      
       
       
       

sst
uct
prada
a prada

  
  
  
  

        
       
      
      

             
             
              
              

actions

reward

uct
   prada
a prada

  
   
   

       
      
      

              
              
              

sst
uct
  
prada
a prada

 
 
   
   





                
                



       
       

  

uct
prada
a prada

  
  
  

       
      
      

             
              
              

  

prada
a prada

   
   

                
                

        
        

  

uct
prada
a prada

  
  
  

       
       
       

             
              
              

  

uct
prada
a prada

   
   
  

  

uct
prada
a prada

  
  
  

         
       
      

             
              
              

  

prada
a prada

  
  

                 
                 

      
      

uct
prada
a prada

  
  
  

                  
               
               

  

  

uct
prada
a prada

  
  
 

               
                
                 

      
     
     

uct
   prada
a prada

  
  
  

         
       
       

  

prada
a prada

 
  

              
              

       
       

  

prada
a prada

  
 

              
        


      
      

  

prada
a prada

  
  

        
       

       
       

     
     

  

prada
a prada

  
  

         
         

     
       

     
     

  

prada
a prada

  
  

                         
                       

  

  

             
             
              
              

planner

      
      
      
      

     
     
     

 
   
   
   


      
      
      

  

uct
prada
a prada

   
  
  

               
       
    
               

  

uct
prada
a prada

  
  
  

                
                
                

uct
   prada
a prada

  
 
 

              
               
                 



      
      

  

  

                       

suc  trial time  s 

                          
                         
                          

 e  exploding blocksworld
planner

actions

sst
uct
prada
a prada

  





               
               

planner

             
             
              

suc  trial time  s 

reward

 
 
  
  

prada

suc  trial time  s 

 d  boxworld

 b  triangle tireworld
planner

actions

sst
uct
  
prada
a prada


      
      
      

  

suc  trial time  s 
        
         
      
      

actions

sst
uct
  
prada
a prada

 
 
  
  

  

prada
a prada

  
  

               
               

  

prada
a prada

  
  

               
               

  

prada
a prada

  
  

               
               

  

prada
a prada

   
   

  

prada
a prada

  
  

                
               

  

prada
a prada

  
  

                
                

      
      

      
      
      
      

      
      

fiplanning noisy probabilistic relational rules

therefore  single problem instance perform     trials different random
seeds using full replanning  trial aborted goal state reached within
maximum number actions varying slightly benchmark  about    actions  
present success rates mean estimators trial times  executed actions
rewards standard deviations table   problem instances least
one trial successfully covered reasonable time 
search rescue  table   a   domain sst  with branching factor
   able find plans within reasonable time significantly larger runtimes
uct  a  prada  success rates rewards indicate prada aprada superior uct scale rather big problem instances  give idea
w r t  ippc evaluation scheme  uct solves successfully    trials first instance
within    minutes full replanning  prada a prada solve trials
full replanning  fact  despite replanning single action  prada a prada
show success rates best planners benchmark except large
problem instances  within competition  participants fsp rbh fsp rdh
achieved comparably satisfactory results   conjecture success methods
due fact domain requires account carefully outcome probabilities 
involve long planning horizons 
triangle tireworld  table   b   domain uct outperforms prada
a prada  although higher computational cost  depth first like style
planning uct seems useful domain  give idea w r t  ippc evaluation
scheme  uct performs    successful trials first instance within    minutes 
prada a prada achieve       trials resp  using full replanning  uct solves
trials difficult instances  required planning horizons increase quickly
problem instances  approaches cannot cope large problem instances 
three competition participants  rff bg  rff pg  hmdpp  could cover 
methods face problems required planning horizons large 
number plans non zero probability small  becomes evident
blocksworld benchmark  table   c    domain different robot manipulation environment first evaluation sec       latter considerably
stochastic provides actions given situation  e g   may grab objects within
pile   blocksworld domain approaches inferior ff replan 
give idea w r t  ippc evaluation scheme  uct perform single successful
trial first instance within    minutes  prada a prada achieve   
   trials resp  using full replanning 
boxworld domain  table   d    approaches exploit fact
delivery boxes  almost  independent delivery boxes  in problem
instances helped intermediate rewards delivered boxes   contrast
uct  prada a prada scale relatively large problem instances  prada
a prada solve     trials first problem instance  requiring average    
min     min resp  full replanning  two competition participants solved
trials successfully domain  rff bg rff pg   give idea w r t  ippc
evaluation scheme  uct perform single successful trial within    minutes 
prada completes   a prada   trials  small number explained
large plan lengths single action computed full replanning 
  

filang   toussaint

finally  exploding blocksworld domain  table   e   prada a prada
perform better good competition participants  give idea w r t  ippc
evaluation scheme  uct achieves single successful trial within    minutes 
prada a prada complete       trials resp  
perform experiments either sysadmin schedule domain  ppddl specifications cannot converted nid rules due involved
universal effects  contrast  possible boxworld domain despite
universal effects there  boxworld problem instances  universally quantified
variables always refer exactly one object exploit conversion nid rules 
 note understood trick implement deictic references ppddl
means universal effects  according action operator  however  odd semantics 
boxes could end two different cities time   furthermore  ignored
rectangle tireworld domain  together triangle tireworld domain makes
  tireworlds benchmark  problem instances faulty goal descriptions 
include not dead   this critical name winner competition
personally communicated olivier buffet  
      summary
majority ppddl descriptions ippc benchmarks converted
nid rules  indicating broad spectrum planning problems covered
nid rules  results demonstrate approaches perform comparably better
state of the art planners many traditional hand crafted planning problems 
hints generality methods probabilistic planning beyond type robotic
manipulation domains considered sec       methods perform particularly well
domains outcome probabilities need carefully accounted for  face problems
required planning horizons large  number plans non zero
probability small  avoided intermediate rewards 

   discussion
presented two approaches planning probabilistic relational rules grounded
domains  methods designed work learned rules provide approximate
partial models noisy worlds  first approach adaptation uct algorithm
samples look ahead trees cope action stochasticity  second approach 
called prada  models uncertainty states explicitly terms beliefs employs
approximate inference graphical models planning  combine planning
algorithms existing rule learning algorithm  intelligent agent  i  learn
compact model dynamics complex noisy environment  ii  quickly derive appropriate actions varying goals  results complex simulated robotics domain show
methods outperform state of the art planner ff replan number different planning tasks  contrast ff replan  methods reason probabilities
action outcomes  necessary world dynamics noisy partial
approximate world models available 
however  planners perform remarkably well many traditional probabilistic
planning problems  demonstrated results ippc benchmarks 
  

fiplanning noisy probabilistic relational rules

shown ppddl descriptions converted large extent kind rules
planners use  hints general purpose character particularly prada
potential benefits techniques probabilistic planning  instance  methods
expected perform similarly well large propositional mdps exhibit
relational structure 
far  planning approaches deal reasonable time problems containing
      objects  implying billions world states  requiring planning horizons
      time steps  nonetheless  approaches still limited rely
reasoning grounded representation  many objects need represented
representation language gets rich  approaches need combined
methods reduce state action space complexity  lang   toussaint      b  
    outlook
current form  approximate inference procedure prada relies specific
compact dbns compiled rules  development similar factored frontier filters
arbitrary dbns  e g  derived general ppddl descriptions  promising 
similarly  adaptation pradas factored frontier techniques existing probabilistic
planners worth investigation 
using probabilistic relational rules backward planning appears appealing 
straightforward learn nid rules regress actions providing reversed triples  s    a  s 
rule learning algorithm  stating predecessor state state s  action
applied before  backward planning  combined forward planning 
received lot attention classical planning may fruitful planning
look ahead trees well planning using approximate inference  means propagating backwards dbns  one may ultimately derive algorithms calculate
posteriors actions  leading true planning inference  instead sampling actions  
important direction improving prada algorithm make adapt
action sequence sampling strategy experience previous samples  introduced simple extension  a prada  achieve this  sophisticated methods
conceivable  learning rule sets online exploiting immediately planning method important direction future research order enable acting
real world  want behave effectively right start  improving
rule framework efficient effective planning another interesting issue 
instance  instead using noisy default rule  one may use mixture models deal
actions several  non unique  covering rules  general use parallel rules work
different hierarchical levels different aspects underlying system 

acknowledgments
thank anonymous reviewers careful thorough comments
greatly improved paper  thank sungwook yoon providing us implementation
ff replan  thank olivier buffet answering questions probabilistic
planning competition       work supported german research foundation
 dfg   emmy noether fellowship         
  

filang   toussaint

appendix a  proof proposition  
proposition    sec       set action sequences prada samples non zero
probability super set ones sst uct 
proof  let a  t   action sequence sampled sst  or uct   thus 
exists state sequence s  t rule sequence r  t   every state st
 t      action unique covering rule rt predicts successor state st  
probability pt      for  pt      st   would never sampled sst  or uct  
show t        p  st   a  t    s         case

psample  at       unique covering rule rt st eventually sampled 
p  s            obvious  assume p  st   a  t    s         execute  
get p  st     a  t   s    pt p  st   a  t    s         posterior p  st     a  t   s    greater
 first inequality  due persistence previous states non zero probability
lead st   given  
set action sequences prada samples larger sst  or uct 
sst  or uct  refuses model noise outcomes rules  assume action state
state unique covering rule  episode
simulated means rule predictions noise outcome  action never
sampled sst  or uct   as required states never sampled   contrast  prada
models effects noise outcome giving low probability possible
successor states heuristic described above 

appendix b  relation nid rules ppddl
use nid rules  sec       relational model transition dynamics probabilistic actions  besides allowing negative literals preconditions  nid rules extend
probabilistic strips operators  kushmerick et al         blum   langford        two
special constructs  namely deictic references noise outcomes  crucial learning compact rule sets  alternative language specify probabilistic relational planning
problems used international probabilistic planning competitions  ippc       
probabilistic planning domain definition language  ppddl   younes   littman        
ppddl probabilistic extension subset pddl  derived deterministic
action description language  adl   adl  turn  introduced universal conditional
effects negative precondition literals  deterministic  strips representation 
thus  ppddl allows usage syntactic constructs beyond expressive
power nid rules  however  many ppddl descriptions converted nid rules 
taking closer look convert ppddl nid rule representations
other  clarify meant action formalisms  giving
intuition line thinking using either these  understand abstract
action abstract action predicate  e g  pickup x   intuitively  defines certain type
action  stochastic state transitions according abstract action specified
abstract nid rules well abstract ppddl action operators  also called schemata  
typically  several different abstract nid rules model abstract action  specifying
state transitions different contexts  contrast  usually one abstract ppddl action
  

fiplanning noisy probabilistic relational rules

operator used model abstract action  context dependent effects modeled
means conditional universal effects 
make predictions specific situation concrete action  a grounded action
predicate pickup greencube    strategy within nid rule framework
ground set abstract nid rules examine ground rules cover state action
pair  exactly one ground rule  chosen prediction 
rule one  the contexts nid rules mutually
exclusive   one chooses noisy default rule  essentially saying one know
happen  other strategies conceivable  pursued here   contrast 
usually exactly one operator per abstract action ppddl domains 
need concept operator uniqueness distinguish ground actions
operators 
b   converting ppddl nid rules
following  discuss convert ppddl features nid rule representation 
may impossible convert ppddl action operator single nid rule 
one may often translate set rules polynomial increase size
representation  table   provides example converted ppddl action operator
ippc domain exploding blocksworld  nid rules support many 
features sophisticated domain description language ppddl provides  using rules
lead compact representations possible domains  experiments  however 
show dynamics many interesting planning domains specified compactly 
furthermore  additional expressive power rule contexts gained using derived
predicates allow bring various kinds logical formulas quantification 
conditional effects conditional effect ppddl operator takes form c
e  accounted two nid rules  first rule adds c context
e outcomes  second adds c context ignores e 
universal effects ppddl allows define universal effects  specify effects
objects meet preconditions  example reboot action sysadmin
domain ippc      competition  specifies every computer one
rebooted independently go probability     connected computer
already down  cannot expressed nid rule framework 
refer objects action arguments via deictic references  require
deictic references unique  reboot action  would need unique way refer
computer cannot achieved without significant modifications  for
example  enumerating computers via separate predicates  
disjunctive preconditions quantification ppddl operators allow disjunctive preconditions  including implications  instance  search and rescue domain
ippc      competition defines action operator goto x  precondition
 x    base  humanalive    disjunction b   b  accounted
either using two nid rules  first rule context second
rule b  alternatively  one may introduce derived predicate c b 
general  trick derived predicates allows overcome syntactical limitations nid
  

filang   toussaint

table    example converting ppddl action operator nid rules  putdownoperator ippc benchmark domain exploding blocksworld  a  contains
conditional effect accounted two nid rules either exclude
 b  include  c  condition context 
    action putdown

 a 

  parameters   b block 
  precondition  and  holding  b   nodestroyedt able  
  ef f ect  and  emptyhand   ont able  b   not  holding  b  
 probabilistic      when  nodetonated  b   and  not  nodestroyedt able    not  nodetonated b      
 
 b 
putdown x    block x   holding x   nodestroyedt able    nodetonated x 

      emptyhand x   ont able x   holding x 

 c 
putdown x    block x   holding x   nodestroyedt able    nodetonated x 

      emptyhand x   ont able x   holding x 

      emptyhand x   ont able x   holding x   nodestroyedt able    nodetonated x 

rules bring various kinds logical formulas quantifications  discussed
pasula et al          derived predicates important prerequisite able learn
compact accurate rules 
types terms may typed ppddl  e g  drivet o c city   typing objects
variables predicates functions achieved nid rules usage typing
predicates within context  e g  using additional predicate city c  
state transition rewards ppddl  one encode markovian rewards associated
state transitions  including action costs negative rewards  using fluents update
rules action effects  one achieve nid rules associating rewards
outcomes rules 
b   converting nid rules ppddl
show following way nid rules used sst  uct prada
planning time handled via polynomial blowup representational size 
basic building blocks nid rule  i e  context well outcomes  transfer
one to one ppddl action operators  deictic references  uniqueness requirement
covering rules noise outcome need special attention 
deictic references deictic references nid rules allow refer objects
action arguments  ppddl  one refer objects means universal
conditional effects  important restriction  however  deictic reference needs
pick single unique object order apply  picks none many  rule fails
apply  two ways ensure uniqueness requirement within ppddl  first 
  

fiplanning noisy probabilistic relational rules

allowing quantified preconditions  explicit uniqueness precondition deictic
reference introduced  using universal quantification  constrains objects
satisfying preconditions identical  i e   x     x     y   
x     variables  alternatively  uniqueness deictic references
achieved careful planning problem specification  however cannot
guaranteed learning rules 
uniqueness covering rules contexts nid rules mutually
exclusive  want use rule prediction  as planning   need ensure
uniquely covers given state action pair  procedural evaluation process nid
rules encoded declaratively ppddl using modified conditions explicitly
negate contexts competing rules  instance  three nid rules
potentially overlapping contexts a  b  c  propositional simplicity   ppddl
action operator may define four conditions  c     a b c   c     a b c  
c     a b c   c      a b c   a b   a c   b c    conditions c   
c  c  test uniqueness corresponding nid rules subsume outcomes 
condition c  tests non uniqueness  either covering rule multiple covering rules 
models potential changes noise  analogous situations nid rule context
noisy default rule would used 
noise outcome noise outcome nid rule subsumes seldom utterly complex
outcomes  relaxes frame assumption  even explicitly stated things may change
certain probability  comes price difficulty ensure well defined
successor state distribution p  s    s  a   contrast  ppddl needs explicitly specify
everything might change  may important reason difficult come
effective learning algorithm ppddl 
principle ppddl provide noise outcome  way approaches
account planning encoded ppddl  either treat noise outcome
effects  in sst uct  basically noop operator then  trivially
translated ppddl  consider probability state attribute change
independently  in prada  encoded ppddl independent universal
probabilistic effects 
noise outcome allows always make predictions arbitrary action 
multiple covering rules  may use  albeit informative  prediction
default rule  cases dealt ppddl action operators using explicit
conditions described previous paragraph 

references
blum  a     langford  j          probabilistic planning graphplan framework 
proc  fifth european conference planning  ecp   pp         
botvinick  m  m     an  j          goal directed decision making prefrontal cortex 
computational framework  advances neural information processing systems
 nips   pp         
  

filang   toussaint

boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions computational leverage  journal artificial intelligence research 
        
boutilier  c   reiter  r     price  b          symbolic dynamic programming first order
mdps  proc  int  conf  artificial intelligence  ijcai   pp         
buffet  o     aberdeen  d          factored policy gradient planner  artificial intelligence journal                    
cooper  g          method using belief networks influence diagrams  proc 
fourth workshop uncertainty artificial intelligence  pp       
croonenborghs  t   ramon  j   blockeel  h     bruynooghe  m          online learning
exploiting relational models reinforcement learning  proc  int  conf 
artificial intelligence  ijcai   pp         
domshlak  c     hoffmann  j          probabilistic planning via heuristic forward search
weighted model counting  journal artificial intelligence research             
driessens  k   ramon  j     gartner  t          graph kernels gaussian processes
relational reinforcement learning  machine learning                  
dzeroski  s   de raedt  l     driessens  k          relational reinforcement learning 
machine learning          
fern  a   yoon  s     givan  r          approximate policy iteration policy language
bias  solving relational markov decision processes  journal artificial intelligence
research                
gardiol  n  h     kaelbling  l  p          envelope based planning relational mdps 
proc  conf  neural information processing systems  nips  
gardiol  n  h     kaelbling  l  p          action space partitioning planning  proc 
aaai conf  artificial intelligence  aaai   pp         
gardiol  n  h     kaelbling  l  p          adaptive envelope mdps relational
equivalence based planning  tech  rep  mit csail tr           mit cs   ai lab 
cambridge  ma 
gelly  s     silver  d          combining online offline knowledge uct  proc 
int  conf  machine learning  icml   pp         
gretton  c     thiebaux  s          exploiting first order rgeression inductive policy
selection  proc  conf  uncertainty artificial intelligence  uai   pp 
       
grush  r          conscious thought simulation behaviour perception  behaviorial
brain sciences             
  

fiplanning noisy probabilistic relational rules

halbritter  f     geibel  p          learning models relational mdps using graph kernels 
proc  mexican conference artificial intelligence  micai   pp         
hesslow  g          conscious thought simulation behaviour perception  trends
cognitive science                
hoffmann  j     nebel  b          planning system  fast plan generation
heuristic search  journal artificial intelligence research             
holldobler  s     skvortsova  o          logic based approach dynamic programming 
aaai workshop  learning planning mdps  pp       
ippc

       
sixth international planning competition 
http   ippc      loria fr wiki index php main page 

uncertainty

part  

jensen  f          introduction bayesian networks  springer verlag  new york 
joshi  s   kersting  k     khardon  r          generalized first order decision diagrams
first order mdps  proc  int  conf  artificial intelligence  ijcai   pp 
         
karabaev  e     skvortsova  o          heuristic search algorithm solving first order
mdps  proc  conf  uncertainty artificial intelligence  uai   pp 
       
kearns  m  j   mansour  y     ng  a  y          sparse sampling algorithm nearoptimal planning large markov decision processes  machine learning           
       
kersting  k     driessens  k          nonparametric policy gradients  unified treatment propositional relational domains  proc  int  conf  machine
learning  icml   pp         
kersting  k   van otterlo  m     de raedt  l          bellman goes relational  proc 
int  conf  machine learning  icml   pp         
kocsis  l     szepesvari  c          bandit based monte carlo planning  proc 
european conf  machine learning  ecml   pp         
kushmerick  n   hanks  s     weld  d          algorithm probabilistic planning 
artificial intelligence                   
kuter  u   nau  d  s   reisner  e     goldman  r  p          using classical planners
solve nondeterministic planning problems  proc  int  conf  automated
planning scheduling  icaps   pp         
lang  t     toussaint  m       a   approximate inference planning stochastic relational worlds  proc  int  conf  machine learning  icml   pp         
lang  t     toussaint  m       b   relevance grounding planning relational domains 
proc  european conf  machine learning  ecml   pp         
  

filang   toussaint

little  i     thiebaux  s          probabilistic planning vs replanning  icaps workshop
international planning competition  past  present future 
littman  m  l   goldsmith  j     mundhenk  m          computational complexity
probabilistic planning  journal artificial intelligence research         
murphy  k  p          dynamic bayesian networks  representation  inference learning  ph d  thesis  uc berkeley 
murphy  k  p     weiss  y          factored frontier algorithm approximate inference dbns  proc  conf  uncertainty artificial intelligence  uai  
pp         
pasula  h  m   zettlemoyer  l  s     kaelbling  l  p          learning symbolic models
stochastic domains  journal artificial intelligence research             
poon  h     domingos  p          sound efficient inference probabilistic
deterministic dependencies  proc  aaai conf  artificial intelligence
 aaai  
sanner  s     boutilier  c          approximate solution techniques factored first order
mdps  proc  int  conf  automated planning scheduling  icaps  
pp         
sanner  s     boutilier  c          practical solution techniques first order mdps 
artificial intelligence                    
shachter  r          probabilistic inference influence diagrams  operations research 
           
sutton  r  s     barto  a  g          reinforcement learning  introduction  mit
press 
teichteil konigsbuch  f   kuter  u     infantes  g          aggregation generating
policies mdps  appear proc  int  conf  autonomous agents
multiagent systems 
toussaint  m     storkey  a          probabilistic inference solving discrete continuous state markov decision processes  proc  int  conf  machine learning
 icml   pp         
toussaint  m   storkey  a     harmeling  s          expectation maximization methods
solving  po mdps optimal control problems  chiappa  s     barber  d 
 eds    inference learning dynamic models  cambridge university press 
van otterlo  m          logic adaptive behavior  ios press  amsterdam 
walsh  t  j          efficient learning relational models sequential decision making 
ph d  thesis  rutgers  state university new jersey  new brunswick  nj 
  

fiplanning noisy probabilistic relational rules

wang  c   joshi  s     khardon  r          first order decision diagrams relational
mdps  journal artificial intelligence research             
weld  d  s          recent advances ai planning  ai magazine                
wu  j  h   kalyanam  r     givan  r          stochastic enforced hill climbing  proc 
int  conf  automated planning scheduling  icaps   pp         
yoon  s  w   fern  a     givan  r          ff replan  baseline probabilistic planning 
proc  int  conf  automated planning scheduling  icaps   pp     
    
yoon  s  w   fern  a   givan  r     kambhampati  s          probabilistic planning via
determinization hindsight  proc  aaai conf  artificial intelligence
 aaai   pp           
younes  h  l     littman  m  l          ppddl     extension pddl expressing
planning domains probabilistic effects  tech  rep   carnegie mellon university 

  


