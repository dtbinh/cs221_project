journal of artificial intelligence research                  

submitted       published     

agnostic pointwise competitive selective classification
yair wiener
ran el yaniv

wyair tx technion ac il
rani cs technion ac il

computer science department
technion  israel institute of technology
haifa        israel

abstract
a pointwise competitive classifier from class f is required to classify identically to the
best classifier in hindsight from f  for noisy  agnostic settings we present a strategy for
learning pointwise competitive classifiers from a finite training sample provided that the
classifier can abstain from prediction at a certain region of its choice  for some interesting hypothesis classes and families of distributions 
the measure of this

 rejected region is
   
shown to be diminishing at rate    o  polylog m   log     m   
  with high probability  where m is the sample size   is the standard confidence parameter  and       are
smoothness parameters of a bernstein type condition of the associated excess loss class
 related to f and the     loss   exact implementation of the proposed learning strategy
is dependent on an erm oracle that is hard to compute in the agnostic case  we thus
consider a heuristic approximation procedure that is based on svms  and show empirically
that this algorithm consistently outperforms a traditional rejection mechanism based on
distance from decision boundary 

   introduction
given a labeled training set and a class of models f  is it possible to select from f  based
on a finite training sample  a model whose predictions are always identical to best model
in hindsight  while classical results from statistical learning theory surely preclude such a
possibility within the standard model  when changing the rules of the game it is possible 
indeed  consider a game where our classifier is allowed to abstain from prediction without
penalty in some region of its choice  a k a classification with a reject option   for this game 
and assuming a noise free realizable setting  it was shown by el yaniv and wiener       
that one can train a perfect classifier that never errs whenever it is willing to predict 
while always abstaining will render such perfect classification vacuous  it was shown that
for a quite broad set of problems  each specified by an underlying distribution family and
a hypothesis class   perfect realizable classification is achievable with a rejection rate that
diminishes quickly to zero with the training sample size 
in general  perfect classification cannot be achieved in a noisy setting  in this paper  our
objective is to achieve pointwise competitiveness  a property ensuring that the prediction at
every non rejected test point is identical to the prediction of the best predictor in hindsight
from the same class  here we consider pointwise competitive selective classification and
generalize the results of el yaniv and wiener        to the agnostic case  in particular 
we show that pointwise competitive classification is achievable with high probability by a
learning strategy called low error selective strategy  less   given a training sample sm

c
    
ai access foundation  all rights reserved 

fiwiener   el yaniv

and a hypothesis class f  less outputs a pointwise competitive selective classifier  f  g  
where f  x  is a standard classifier  and g x  is a selection function that qualifies some of
the predictions as dont knows  see definitions in section     the classifier f is simply
taken to be the empirical risk minimizer  erm  classifier  f  pointwise competitiveness is
achieved through g as follows  using standard concentration inequalities  we show that the
true risk minimizer  f    achieves empirical error that is close that of f  thus  with high
probability f  belongs to the class of low empirical error hypotheses  now all that is left
to do is set g x  such that it allows the prediction of the label of x  as f x   if and only
if all the hypotheses in this low error class unanimously agree on the label of x  in the
simpler  realizable setting  el yaniv   wiener         this low error class simply reduces
to the version space 
the bulk of our analysis  in sections      and    concerns coverage bounds for less 
namely  showing that the measure of the region where the classifier  f  g  refuses to classify 
diminishes quickly  with high probability  as the training sample size grows  see section  
for a formal definition   we provide several general and distribution dependent coverage
bounds  in particular  we show  in corollaries    and     respectively  high probability
bounds for the coverage  f  g  of the classifier  f  g    of the form 


 f  g         o  polylog m   log     m       

for linear models under  unknown  distribution p  x  y    where x are feature space points
and y are labels  whose marginal p  x  is any finite mixture of gaussians  and for axis
aligned rectangles under p  x  y   whose marginal p  x  is a product distribution  where
      are bernstein class smoothness parameters depending on the hypothesis class and the
underlying distribution  and the loss function      in our case  
at the outset  efficient implementation of less seems to be out of reach as we are
required to track the supremum of the empirical error over a possibly infinite hypothesis
subset  which in general might be intractable  to overcome this computational difficulty  we
propose a reduction of this problem to a problem of calculating  two  constrained erms 
for any given test point x  we calculate the erm over the training sample sm with a
constraint on the label of x  one positive label constraint and one negative   we show
that thresholding the difference in empirical error between these two constrained erms is
equivalent to tracking the supremum over the entire  infinite  hypothesis subset  based
on this reduction we introduce in section   a disbelief principle that motivates a heuristic implementation of less  which relies on constrained svms  and mimics the optimal
behavior 
in section   we present some numerical examples over medical classification problems
and examine the empirical performance of the new algorithm and compare its performance
with that of the widely used selective classification method for rejection  based on distance
from decision boundary 

   pointwise competitive selective classification  preliminary
definitions
let x be some feature space  for example  d dimensional vectors in rd   and y be some
output space  in standard classification  the goal is to learn a classifier f   x  y  using
   

fiagnostic pointwise competitive selective classification

a finite training sample of m labeled examples  sm     xi   yi   m
i     assumed to be sampled
i i d  from some unknown underlying distribution p  x  y   over x  y  the classifier is to
be selected from some hypothesis class f  let    y  y         be a bounded loss function 
in selective classification  el yaniv   wiener         the learning algorithm receives sm
and is required to output a selective classifier  defined to be a pair  f  g   where f  f is a
classifier  and g   x         is a selection function  serving as qualifier for f as follows  for
any x  x    f  g  x    f  x  iff g x       otherwise  the classifier outputs i dont know 
the general performance of a selective predictor is characterized in terms of two quantities  coverage and risk  the coverage of  f  g  is  f  g    ep  g x     the true risk of  f  g  
with respect to some loss function   is the average loss of f restricted to its region of activity
as qualified by g  and normalized by its coverage  r f  g    ep   f  x   y   g x     f  g   it
is easy to verify that if g     and therefore  f  g        then r f  g  reduces
to the famil  pm
iar risk functional r f     ep   f  x   y    for a classifier f   let r f     m i    f  xi    yi   
the standard empirical error of f over the sample sm   let f   argminf f r f   be the
empirical risk minimizer  erm   and let f    argminf f r f   be the true risk minimizer
with respect to unknown distribution p  x  y     clearly  the true risk minimizer f  is unknown  a selective classifier  f  g  is called pointwise competitive if for any x  x   for which
g x       f  x    f   x  

   low error selective strategy  less 
for any hypothesis class f  hypothesis f  f  distribution p   sample sm   and real number
r      define the true and empirical low error sets 

 
v f  r    f   f   r f     r f     r
   
and

n
o
v f  r    f   f   r f     r f     r  

   

throughout the paper we denote by  m    d  the slack of a standard uniform deviation
bound  given in terms of the training sample size  m  the confidence parameter    and the
vc dimension  d  of the class f 
s

  ln  
 d ln  me
d
 m    d     
 
   
m
the following theorem is a slight extension of the statement made by bousquet  boucheron 
and lugosi        p       
theorem    bousquet et al          let  be the     loss function and f  a hypothesis
class whose vc dimension is d  for any           with probability of at least     over
the choice of sm from p m   any hypothesis f  f satisfies
r f    r f      m    d  
similarly  r f    r f      m    d  under the same conditions 
 

more formally  f  is a classifier such that r f      inf f f r f   and inf f f p   x  y    f  x     f   x       
the existence of such a  measurable  f  is guaranteed under sufficient considerations  see hanneke       
pp          

   

fiwiener   el yaniv

remark    the use of theorem   and  in particular  vc bounds for classification problems
     loss  is not mandatory for developing the theory presented in this paper  similar
theories can be developed using other types of bounds  e g   rademacher or compression
bounds  for other learning problems 
let g  f  the disagreement set  hanneke      a  el yaniv   wiener        w r t  g
is defined as
dis g     x  x   f    f   g s t  f   x     f   x    
   
let us now motivate the low error selective strategy  less  whose pseudo code appears
in strategy    the strategy is define whenever the empirical risk minimizer  erm  exists 
for example  in the case of the     loss  using a standard uniform deviation bound  such as
the one in theorem    one can show that the training error of the true risk minimizer  f   
cannot be too far from the training error of the empirical risk minimizer  f  therefore 

we can guarantee  with high probability  that the empirical low error class v f  r  applied
with appropriately chosen r  includes the true risk minimizer f    the selection function
g is now constructed to accept a subset of the domain x   on which all hypotheses in the
empirical low error set unanimously agree  strategy   formulates this idea  we call it a
strategy rather then an algorithm because it lacks implementation details  indeed  it is
not clear at the outset if this strategy can be implemented 
strategy   agnostic low error selective strategy  less 
input  sm   m    d
output  a pointwise competitive selective classifier  h  g  w p     

   set f   erm
  f  sm    i e   f is any empirical risk minimizer from f w r t  sm
   set g   v f    m      d   see eq      and     
   construct g such that g x       x   x   dis  g  
   f   f
we now begin the analysis of less  the following lemma establishes its pointwise competitiveness  in section   we develop general coverage bounds in terms of an undetermined
disagreement coefficient  then  in section   we present distribution dependent bounds that
do not rely on the disagreement coefficient 
lemma    pointwise competitiveness   let  be the     loss function and f  a hypothesis
class whose vc dimension is d  let      be given and let  f  g  be the selective classifier
chosen by less  then  with probability of at least         f  g  is a pointwise competitive
selective classifier 
proof  by theorem    with probability of at least       
r f     r f        m      d   
clearly  since f  minimizes the true error  r f     r f   applying again theorem    we
know that with probability of at least       
r f   r f      m      d   
   

fiagnostic pointwise competitive selective classification

using the union bound  it follows that with probability of at least       
r f     r f       m      d   
hence  with probability of at least       


f   v f     m      d    g 

by definition  less constructs the selection function g x  such that it equals one iff x 
x  dis  g    thus  for any x  x   for which g x       all the hypotheses in g agree  and in
particular f  and f agree  therefore  f  g  is pointwise competitive with high probability 

   general coverage bounds for less in terms of the disagreement
coefficient
we require the following definitions to facilitate the coverage analysis  for any f  f and
r      define the set b f  r  of all hypotheses that reside in a ball of radius r around f  


 
 

b f  r    f  f   pr f  x     f  x   r  
xp

for any g  f  and distribution p   we denote by g the volume of the disagreement set
of g  see       g   pr  dis g    let r      the disagreement coefficient  hanneke 
      of the hypothesis class f with respect to the target distribution p is
 r      f   r      sup

r r 

b f    r 
 
r

   

the disagreement coefficient will be utilized later on in our analysis  see also a discussion
on its characteristics after corollary    the associated excess loss class of the class f and
the loss function   massart        mendelson        bartlett  mendelson    philips       
is defined as
xl f    x  y      f  x   y    f   x   y    f  f   
whenever f and  are fixed we abbreviate xl   xl f    x  y   xl is said to be a
         bernstein class with respect to p  where          and        if every h  xl
satisfies
eh      eh    
   
bernstein classes arise in many natural situations  see  e g   koltchinskii        bartlett  
mendelson        bartlett   wegkamp         for example  if the conditional probability
p  y  x  is bounded away from      or it satisfies tsybakovs noise conditions    then the
excess loss function is a bernstein class  bartlett   mendelson        tsybakov         
 

if the data was generated from any unknown deterministic hypothesis with limited noise then p  y  x  is
bounded away from     

 

specifically  for the     loss  assumption a in proposition   in the work of tsybakov         is equivalent

to the bernstein class condition of equation     above with       
  where  is the tsybakov noise
parameter 

   

fiwiener   el yaniv

in the following sequence of lemmas and theorems we assume a binary hypothesis class f
with vc dimension d  an underlying distribution p over x      and that  is the     loss
function  also  xl denotes the associated excess loss class  our results can be extended to
loss functions other than     by similar techniques to those used by beygelzimer  dasgupta 
and langford        
in figure   we schematically depict the hypothesis class f  the gray area   the target
hypothesis  filled black circle outside f   and the best hypothesis in the class f    the
distance of two points in the diagram relates to the distance between two hypothesis under
the marginal distribution p  x   our first observation is that if the excess loss class is
         bernstein class  then the set of low true error  depicted in figure    a   resides
within a larger ball centered around f   see figure    b   

figure    the set of low true error  a  resides within a ball around f   b  
lemma    if xl is a          bernstein class with respect to p   then for any r    


v f    r   b f      r   

proof  if f  v f    r  then  by definition  e  i f  x     y     e  i f   x     y      r  by
linearity of expectation we have 
e  i f  x     y    i f   x     y     r 
since xl is          bernstein 
e  i f  x     f   x      e   i f  x     y    i f   x     y    
n
o
  e   f  x   y     f   x   y       eh      eh  
     e  i f  x     y    i f   x     y       


by      e  i f  x     f   x       r    therefore  by definition  f  b f      r   
   

   

fiagnostic pointwise competitive selective classification

so far we have seen that the set of low true error resides within a ball around f    now
we would like to prove that with high probability the set of low empirical error  depicted
in figure    a   resides within the set of low true error  see figure    b    we emphasize
that the distance between hypotheses in figure    a  is based on the empirical error  while
the distance in figure    b  is based on the true error 

figure    the set of low empirical error  a  resides within the set of low true error  b  
lemma    for any r      and           with probability of at least     
v f  r   v  f       m      d    r   
proof  if f  v f  r   then  by definition  r f    r f    r  since f minimizes the empirical
error  we know that r f   r f     using theorem   twice  and applying the union bound 
we see that with probability of at least     
r f    r f      m      d 



r f     r f       m      d  

therefore 
r f    r f         m      d    r 
and
f  v  f       m      d    r   

we have shown that  with high probability  the set of low empirical error is a subset of
a certain ball around f    therefore  the probability that at least two hypotheses in the set
of low empirical error will disagree with each other is bounded by the probability that at
least two hypotheses in that ball around f  will disagree with each other  fortunately  the
latter is bounded by the disagreement coefficient as established in the following lemma 

   

fiwiener   el yaniv

lemma    for any r     and           with probability of at least     
v f  r          m      d    r     r    
where  r    is the disagreement coefficient of f with respect to p   applied with r   
   m      d     see      
proof  applying lemmas   and   we get that with probability of at least     


v f  r   b f          m      d    r    

therefore 



v f  r   b f          m      d    r    

by the definition of the disagreement coefficient      for any r   r    b f    r     r   r  
recalling that      and thus observing that r         m      d    r         m      d     
r    the proof is complete 
we are now in a position to state our first coverage bound for the selective classifier
constructed by less  this bound is given in terms of the disagreement coefficient 
corollary    let f be a hypothesis class as in theorem    and assume that xl is a
         bernstein class w r t  p   let  f  g  be the selective classifier constructed by less 
then  with probability of at least       f  g  is a pointwise competitive selective classifier
and
 f  g             m      d      r    

where  r    is the disagreement coefficient of f with respect to p   and r       m      d     

proof  
by lemma    with
 probability of at least       f  g  is pointwise competitive  set

g   v f      m      d    by construction  f   f  and the selection function g x  equals
one iff x  x   dis  g   thus  by the definition of coverage   f  g    e g x        g 
therefore  applications of lemma   and the union bound imply that with probability of at
least       f  g  is pointwise competitive and its coverage satisfies 
 f  g    e g x        g            m      d      r    

noting that  r  is monotone non increasing with r  we know that the coverage bound
of corollary   clearly applies with      the quantity     has been discussed in numerous papers and has been shown to be finite in various settings including thresholds in r
under any distribution            hanneke         linear
 separators through the origin
in rd under uniform distribution on the sphere       d   hanneke         and linear
separators in rd under smooth data distribution bounded away from zero       c f   d 
where c f    is an unknown constant that depends on the target hypothesis   friedman 
       for these cases  an application of corollary   is sufficient to guarantee pointwisecompetitiveness with bounded coverage that converges to one  unfortunately for many
hypothesis classes and distributions the disagreement coefficient     is infinite  hanneke 
       fortunately  if the disagreement coefficient  r  grows slowly with respect to   r  as
shown in wang        under sufficient smoothness conditions   corollary   is sufficient to
guarantee bounded coverage 

   

fiagnostic pointwise competitive selective classification

   more distribution dependent coverage bounds for less
in this section we establish distribution dependent coverage bounds for less  the starting
point of these bounds is the following corollary 
corollary    let f be a hypothesis class as in theorem    and assume that f has disagreement coefficient
 r      o  polylog    r    
   
w r t  distribution p   and that xl is a          bernstein class w r t  the same distribution 
let  f  g  be the selective classifier chosen by less  then  with probability of at least     
 f  g  is pointwise competitive and its coverage satisfies 
 


polylog m 
      
 f  g         o
 log
 
m

proof  plugging in     in the coverage bound of corollary   immediately yields the result 

corollary   states fast coverage bounds for less in cases where the disagreement coefficient grows slowly with respect to   r     recent results on disagreement based active
learning and selective prediction  wiener et al         wiener        established tight relations between the disagreement coefficient and an empirical quantity called the version space
compression set size  this quantity has been analyzed by el yaniv and wiener        in
the context of realizable selective classification  and there are known distribution dependent
bounds for it  our plan for the rest of this section is to introduce the version space compression set size  discuss its relation to the disagreement coefficient  and then show how to
apply those results in the agnostic setting 
while we are interested in solving the agnostic case  we will now consider for a moment
the realizable setting and utilize known results that will be used in our analysis  specifically 
we now assume that f   f with p y   f   x  x   x      for all x  x   where
 x  y    p   given a training sample sm   let vsf  s be the induced version space  i e   the
set of all hypotheses consistent with the given sample sm   the version space compression
set size  denoted n sm     n f  sm    is defined to be the size of the smallest subset of
sm inducing the same version space  hanneke      b  el yaniv   wiener         being a
function of sm   clearly n sm   is a random variable  and for any specific realization sm its
value is unique 
for any m and           define the version space compression set size minimal bound as
bn  m      min  b  n   p n sm    b         

   

we rely on the following lemma  wiener et al          for the sake of self containment we
provide its proof in the appendix 
 

when the disagreement coefficient does not grow ploy logarithmically with   r  but is still o   r     it is
still possible to prove a lower bound on the coverage  specifically  if  r      o     r      with       one

can show that  f  g      o     m         

   

fiwiener   el yaniv

lemma    wiener et al          in the realizablecase  if bn  m 
    o  polylog m  log       

 
 
or bn m       o  polylog m    then  r      o polylog r   

obviously  the statement of lemma   only holds  and is well defined  within a realizable
setting  the version space compression set size is only defined for this setting   we now turn
back to the agnostic setting and consider an arbitrary underlying distribution p over x y 
recall that in the agnostic setting  we let f    x  y denote a  measurable  classifier
such that r f      inf f f r f   and inf f f p   x  y    f  x     f   x        which is guaranteed
to exist under sufficient assumptions  see hanneke        section       we call f  an infimal
 best  hypothesis  of f  w r t  p    clearly there can be several different infimal hypotheses 
we note  however  that if xl is a          bernstein class with respect to p  as we assume
in this paper   then lemma   ensures that all infimal hypotheses are identical up to measure
zero 
the definitions of version space and version space compression set size can be naturally
generalized to the agnostic setting with respect to an infimal hypothesis  wiener et al  
      as follows  let f  be an infimal hypothesis of f w r t  p   the agnostic version space
of sm is
vsf  sm  f     f  f    x  y   sm   f  x    f   x   

the agnostic version space compression set size  denoted n sm     n f  sm   f     is defined
to be the size of the smallest subset of sm inducing the agnostic version space vsf  sm  f   
finally  extend also the definition of the version space compression set minimal bound to
the agnostic setting as follows 
bn  m    f      min b  n   p n f  sm   f     b        

the key observation that allows for surprisingly easy utilization of lemma   in the
agnostic setting is that the disagreement coefficient depends only on the hypothesis class
f and the marginal distribution p  x   using an infimal hypothesis f  we can therefore
take any agnostic learning problem and consider its realizable projection  whereby points
are labeled by f  and it has the same marginal distribution p  x   these two problems
will have  essentially  the same disagreement coefficients  this idea was initially observed
by hanneke        and wiener         here we formulate it as a slight variation of the
formulation in the work of wiener  hanneke  and el yaniv        
we define the disagreement in the agnostic setting as in     with respect to an infimal hypothesis f    for any agnostic learning problem  f  p   we define its realizable
projection  f    p    as follows  let f    f   f    where f  is an infimal hypothesis of
the agnostic problem  define p  to be a distribution with marginal p   x    p  x   and
p y   f   x  x   x      for all x  x   it is easy to verify that  f    p    is a realizable
learning problem  i e   f   f  with pp   x y    y   f   x  x   x      for all x  x  
lemma     realizable projection   given any agnostic learning problem   f  p    let
 f    p    be its realizable projection  let  r    and   r    be the associated disagreement coefficients of the agnostic and realizable projection problems  respectively  then   r       r    

proof  first note that  and  depend  respectively  on p and p  only via f  and the
marginal distributions p  x    p   x   since f  f   f      f    we readily get that
 r       r    
   

fiagnostic pointwise competitive selective classification

let us summarize the above derivation  given an agnostic problem  f  p    consider
its realizable projection  f    p     if bn  m      o  polylog m  log        or bn  m         
o  polylog m    for the realizable problem  then by lemma     r      o  polylog    r     
which  by lemma     also holds in the original agnostic problem  therefore  corollary  
applies and we obtain a fast coverage bound for less w r t   f  p   
new agnostic coverage bounds for less are obtained using the following known bounds
for the  realizable  version space compression set size  the first one  by el yaniv and wiener
        applies to the problem of learning linear separators under a mixture of gaussian
distributions  the following theorem is a direct application of lemma    in the work of
el yaniv and wiener        
theorem     el yaniv   wiener         for any d  n  n  let x  rd   f be the space of
linear separators on rd   and p be any distribution with marginal over rd that is a mixture
of n multivariate normal distributions  then  there is a constant cd n      depending on
d  n  but otherwise independent of p   such that m    
bn  m         cd n  log m  d   
applying theorem     together with lemma     lemma   and corollary    immediately
yields the following result 
corollary     assume the conditions of theorem     assume also that xl is a         bernstein class w r t  p  x  y    let  f  g  be the selective classifier constructed by less 
then  with probability of at least       f  g  is a pointwise competitive selective classifier
and


 f  g         o  polylog m   log     m       
the second version space compression set size bound concerns realizable learning of
axis aligned rectangles under product densities over rn   such bounds have been previously
proposed by wiener  hanneke  and el yaniv        and el yaniv and wiener              
we now state  without proof  a recent bound  wiener  hanneke    el yaniv        giving
version space compression set size bound for this learning problem  whose positive class is
bounded away from zero  

theorem     wiener et al          for d  m  n and             let x  rd   for any p
with marginal distribution over rd that is a product of densities over rd with marginals
having continuous cdfs  and for f the space of axis aligned rectangles f on rd with
p   x  y    f  x         
 
 d
 d
bn  m    
ln
 


here again  an application of theorem     together with lemma     lemma   and corollary   yields the following corollary 
corollary     for d  m  n and             let x  rd   let p  x  y   be an underlying
distribution with marginal p  x  that is a product of densities over rd with marginals having
continuous cdfs  let f the space of axis aligned rectangles f on rd with p   x  y    f  x   
      assume that xl is a          bernstein class w r t  p  x  y    let  f  g  be the
   

fiwiener   el yaniv

selective classifier constructed by less  then  with probability of at least       f  g  is a
pointwise competitive selective classifier and


 f  g         o  polylog m   log     m       

   erm oracles and the disbelief principle
at the outset  efficient construction of the selection function g prescribed by less seems
to be out of reach as we are required to verify  for each point x in question  whether all
hypotheses in the low error class agree on its label  moreover  g should be computed for the
entire domain  luckily  it is possible to compute g in a lazy manner and we now show
how to compute g x  by calculating  two  constrained erms  for any given test point x 
we calculate the erm over the training sample sm with a constraint on the label of x  one
positive label constraint and one negative   we show that thresholding the difference in
empirical error between these two constrained erms is equivalent to tracking the supremum
over the entire  infinite  hypothesis subset  the following lemma establishes this reduction 
lemma     let  f  g  be a selective classifier chosen by less after observing the training
sample sm   let f be an empirical risk minimizer over sm   let x be any point in x and
define
o

n
fx   argmin r f     f  x    sign f x   
f f

i e   an empirical risk minimizer forced to label x the opposite from f x   then
g x     



r fx    r f      m      d   

proof  first note that according to the definition of v  see eq      


r fx    r f      m      d   fx  v f     m      d   

    

    

to prove the first direction     of       assume that the rhs of      holds  by       we
get that both f  fx  v  however  by construction  f x    fx  x   so x  dis v  and
g x      
to prove the other direction      assume that r fx    r f       m      d   under
this assumption  we will prove that for any f   v  f   x    f x   and therefore  x 
x   dis v   entailing that g x       indeed  assume by contradiction that there exists
f   v such that f   x    fx  x     f x   by construction  it holds that
r f     r fx     r f       m      d   
so f    v  contradiction 
lemma    tells us that in order to decide if point x should be rejected we need to measure
the empirical error r fx   of a special empirical risk minimizer  fx   which is constrained to
label x the opposite from h x   if this error is sufficiently close to r h   our classifier cannot
be too sure about the label of x and we must reject it  thus  provided we can compute
these erms  we can decide whether to predict or reject any individual test point x  x  
   

fiagnostic pointwise competitive selective classification

without actually constructing g for the entire domain x   figure   illustrates this principle
for a   dimensional example  the hypothesis class is the class of linear classifiers in r 
and the source distribution is two normal distributions  negative samples are represented
by blue circles and positive samples by red squares  as usual  f denotes the empirical

figure    constrained erm 
risk minimizer  let us assume that we want to classify point x    this point is classified
positive by f  therefore  we force this point to be negative and calculate the restricted
erm  depicted by doted line marked fx     the difference between the empirical risk of f
and fx  is not large enough  so point x  will be rejected  however  if we want to classify
point x    the difference between the empirical risk of f and fx  is quite large and the point
will be classified as positive 
equation      motivates the following definition of a disbelief index df  x  sm   for
each individual point in x   specifically  for any x  x   define its disbelief index w r t  sm
and f 
d x    df  x  sm     r fx    r f  
observe that d x  is large whenever our model is sensitive to the label of x in the sense
that when we are forced to bend our best model to fit the opposite label of x  our model
substantially deteriorates  giving rise to a large disbelief index  this large d x  can be
interpreted as our disbelief in the possibility that x can be labeled so differently  in this
case we should definitely predict the label of x using our unforced model  conversely  if
d x  is small  our model is indifferent to the label of x and in this sense  is not committed
to its label  in this case we should abstain from prediction at x  notice that less is a
specific application of thresholded disbelief index 
we note that a similar technique of using an erm oracle that can enforce an arbitrary
number of example based constraints was used by dasgupta  hsu  and monteleoni      a 
and beygelzimer  hsu  langford  and zhang         in the context of active learning  as
in our disbelief index  the difference between the empirical risk  or importance weighted
empirical risk  see beygelzimer et al         of two erm oracles  with different constraints 
is used to estimate prediction confidence 

   

fiwiener   el yaniv

   

    
    

    
test error

test error

    
   
    

    
    

    
    

    

    
 

   

   

   

   

    
   

 

c

   

   

   

   

   

c

figure    rc curve of our technique  depicted in red  compared to rejection based on
distance from decision boundary  depicted in dashed green line   the rc curve
in right figure zooms into the lower coverage regions of the left curve 

in practical applications of selective prediction it is desirable to allow for some control
over the trade off between risk and coverage  in other words  it is desirable to be able to
develop the entire risk coverage  rc  curve for the classifier at hand  see  e g   el yaniv  
wiener        and let the user choose the cutoff point along this curve in accordance with
other practical considerations and constraints  the disbelief index facilitates an exploration
of the risk coverage trade off curve for our classifier as follows  given a pool of test points we
can rank these test points according to their disbelief index  and points with low index should
be rejected first  thus  this ranking provides the means for constructing a risk coverage
trade off curve  ignoring for the moment implementation details  which are discussed in
section     a typical rc curve generated by less is depicted in figure    red curve     the
dashed green rc curve was computed using the traditional distance based techniques for
rejection  see discussion of this common technique in section    the right graph is a zoom
in section of the entire rc curve  depicted on the left graph   the dashed horizontal line is
the test error of f  on the entire domain and the dotted line is the bayes error  while for
high coverage values the two techniques are statistically indistinguishable  for any coverage
less than     we get a significant advantage for less  it is clear that in this case not only
the estimation error was reduced  but also the test error goes significantly below the optimal
test error of f  for low coverage values 
interestingly  the disbelief index generates rejection regions that are fundamentally different than those obtained by the traditional distance based techniques for rejection  see
section     to illustrate this point  and still ignoring implementation details   consider
figure   where we depict the rejection regions for a training sample of     points sampled
from a mixture of two identical normal distributions  centered at different locations   the
height map in this figure  which correspond to disbelief index magnitude  a   and distance
from decision boundary  b   reflect the confidence regions of each technique according to
its own confidence measure 
 

the learning problem is the same synthetic problem used for generating figure   

   

fiagnostic pointwise competitive selective classification

 a 

 b 

figure    linear classifier  confidence height map using  a  disbelief index   b  distance
from decision boundary 

figure    svm with polynomial kernel  confidence height map using  a  disbelief index 
 b  distance from decision boundary 

to intuitively explain the height map of figure   a   recall that the disbelief index is
the difference between the empirical error of the erm and the restricted erm  if a test
point resides in a high density region  we expect that forcing the wrong label for that point
will result in a large increase of the training error  as a result  the denser the area is  the
larger the disbelief index  and therefore  the higher the classification confidence 
the second synthetic  d source distribution we consider is even more striking  here x
is distributed uniformly over                and the labels are sampled according to the
following conditional distribution

      x   sin x    
p  y     x    x    x      
      else 
the thick red line depicts the decision boundary of the bayes classifier  the hight maps
in figure   depict the rejection regions obtained by  our approximation of  less and by

   

fiwiener   el yaniv

the traditional  distance from decision boundary  technique for a training sample of   
points sampled from this distribution  averaged over     iterations   here the hypothesis
class used for training was svm with a polynomial kernel of degree    the qualitative
difference between these two techniques  and in particular  the nice fit of the disbelief
principle technique compared to svm is quite surprising 

figure    rc curves for svm with linear kernel  our method in solid red  and rejection
based on distance from decision boundary in dashed green  horizontal axis  c 
represents coverage 

   heuristic procedure using svm and its empirical performance
the computation of a  constrained  erm oracle can be efficiently achieved in the case of
realizable learning with linear models  see  e g   el yaniv   wiener        and in the case
of linear regression  wiener   el yaniv         however  in a noisy setting the computation
of the linear erm oracle can be reduced to a variant of the max fls and c max fls
problems  with strict and non strict inequalities   amaldi   kann         unfortunately 

   

fiagnostic pointwise competitive selective classification

max fls is apx complete  within a factor     c max fls is max ind set hard  and
cannot be approximated efficiently at all  moreover  there are extensions of these results
to other classes  including axis aligned hyper rectangles  showing that approximating erm
for these classes is np hard  ben david et al         
while at present it is not known if these hardness results  and other related lower
bounds  hold for half spaces under nice distributions such as gaussian  mixtures   we note
that tauman kalai et al         studied the problem of agnostically learning halfspaces
under distributional assumptions  in particular  they showed that if the data distribution is
uniform over the d dimensional unit sphere  or hyper cube  and other related distributions  
 
then it is possible to agnostically learn  accurate halfspaces in time poly d      however 
it is known that these particular distributions do not elicit effective pointwise competitive
learning  on the contrary  the uniform distribution over the unit sphere is among the
worst possible distributions for pointwise competitive classification  and disagreement based
active learning  unless one utilizes homogeneous halfspaces  see discussion in  e g   el yaniv
  wiener        

figure    svm with linear kernel  the maximum coverage for a distance based rejection
technique that allows the same error rate as our method with a specific coverage 

   

fiwiener   el yaniv

having discussed these computational hurdles  we should recall that much of applied
machine learning research and many of its applications are doing quite well with heuristic
approximations  rather than formal ones   when practical performance is the objective 
clever heuristics and tricks can sometimes make the difference  at this point in the paper
we therefore switch from theory to practice  aiming at implementing a rejection method
inspired by the disbelief principle and see how well they work on real world problems 
we approximate the erm as follows  using support vector machines  svms  we use
a high c value      in our experiments  to penalize more on training errors than on small
margin  see definitions of the svm parameters in  e g  chang   lin         in this way the
solution to the optimization problem tend to get closer to the erm  in order to estimate
r fx   we have to restrict the svm optimizer to only consider hypotheses that classify the
point x in a specific way  to accomplish this we use a weighted svm for unbalanced data 
we add the point x as another training point with weight    times larger than the weight
of all training points combined  thus  the penalty for misclassification of x is very large
and the optimizer finds a solution that doesnt violate the constraint 
another problem we face is that the disbelief index is a noisy statistic that highly
depends on the sample sm   to overcome this noise we use robust statistics  first we
    s           s k   using bootstrap sampling
generate an odd number k of different samples  sm
m
m
 we used k        for each sample we calculate the disbelief index for all test points and for
each point take the median of these measurements as the final index  we also note that for
any finite training sample the disbelief index is a discrete variable  it is often the case that
several test points share the same disbelief index  in those cases we can use any confidence
measure as a tie breaker  in our experiments we use distance from decision boundary to
break ties  focusing on svms with a linear kernel we compared the rc  risk coverage 
curves achieved by the proposed method with those achieved by svm with rejection based
on distance from decision boundary  this latter approach is very common in practical
applications of selective classification  for implementation we used libsvm  chang  
lin        
we tested our algorithm on standard medical diagnosis problems from the uci repository  including all datasets used by grandvalet  rakotomamonjy  keshet  and canu        
we transformed nominal features to numerical ones in a standard way using binary indicator attributes  we also normalized each attribute independently so that its dynamic
range is         no other preprocessing was employed  in each iteration we choose uniformly
at random non overlapping training set      samples  and test set      samples  for each
dataset   the svm was trained on the entire training set  and test samples were sorted
according to confidence  either using distance from decision boundary or disbelief index  
figure   depicts the rc curves of our technique  red solid line  and rejection based on
distance from decision boundary  green dashed line  for linear kernel on all   datasets  all
results are averaged over     iterations  error bars show standard error   with the exception
of the hepatitis dataset  in which both methods were statistically indistinguishable  in
all other datasets the proposed method exhibits significant advantage over the traditional
approach  we would like to highlight the performance of the proposed method on the
pima dataset  while the traditional approach cannot achieve error less than    for any
 

due to the size of the hepatitis dataset the test set was limited to    samples 

   

fiagnostic pointwise competitive selective classification

figure    rc curves for svm with rbf kernel  our method in solid red and rejection
based on distance from decision boundary in dashed green 

rejection rate  in our approach the test error decreases monotonically to zero with rejection
rate  furthermore  a clear advantage for our method over a large range of rejection rates is
evident in the haberman dataset    
for the sake of fairness  we note that the running time of our algorithm  as presented
here  is substantially longer than the traditional technique  the performance of our algorithm can be substantially improved when many unlabeled samples are available  in this
case the rejection function can be evaluated on the unlabeled samples to generate a new
labeled sample  then a new rejection classifier can be trained on this sample 
figure   depicts the maximum coverage for a distance based rejection technique that
allows the same error rate as our method with a specific coverage  for example  let us
assume that our method can have an error rate of     with coverage of     and the
 

the haberman dataset contains survival data of patients who had undergone surgery for breast cancer 
with estimated         new cases of breast cancer in the united states during       society        an
improvement of    affects the lives of more than      women 

   

fiwiener   el yaniv

figure     svm with rbf kernel  the maximum coverage for a distance based rejection
technique that allows the same error rate as our method with a specific coverage 

distance based rejection technique achieves the same error with maximum coverage of     
then the point            will be on the red line  thus  if the red line is bellow the diagonal
then our technique has an advantage over distance based rejection and visa versa  as an
example  consider the haberman dataset  and observe that regardless of the rejection rate 
distance based technique cannot achieve the same error as our technique with coverage
lower than     
figures   and    depict the results obtained with rbf kernel  in this case a statistically
significant advantage for our technique was observed for all datasets 

   related work
pointwise competitive classification is a unique and extreme instance of classification with
an abstention option  an idea which emerged from the pattern recognition community  was
first proposed and studied    years ago by chow               and generated lots of interest

   

fiagnostic pointwise competitive selective classification

 fumera et al         tortorella        santos pereira   pires        fumera   roli       
pietraszek        bounsiar et al         landgrebe et al         herbei   wegkamp       
hellman        el yaniv   pidan        bartlett   wegkamp        wegkap        freund
et al          taking a broader perspective  pointwise competitive selective prediction  and
in particular  classification  is a particular instance of the broader concept of confidencerated learning  whereby the learner must formally quantify confidence in its prediction 
achieving effective confidence rated prediction  including abstention  is a longstanding and
challenging goal in a number of disciplines and research communities  let us first discuss
some of the most prominent approaches to confidence rated prediction and note how they
related to the present work 
in the knows what it knows  kwik  framework studied in reinforcement learning  li 
littman    walsh        strehl   littman        li   littman        a similar notion
to pointwise competitiveness is studied  and coverage rates are analyzed  li et al        
li         however  kwik was limited to the realizable model and is concerned with an
adversarial setting where both the target hypothesis and the training data are selected by
an adversary  while all positive results for the kwik adversarial setting apply to the
statistical pointwise competitive prediction setting  where training examples are sampled
i i d    this adversarial setting precludes non trivial coverage for all the interesting hypothesis
classes currently addressed by pointwise competitive prediction  this deficiency comes as no
surprise because the kwik adversarial setting is much more challenging than the statistical
pointwise competitive prediction assumptions 
the conformal prediction framework  vovk  gammerman    shafer        provides
hedged predictions by allowing the possibility of multi labeled predictions and guarantees
a user desired confidence rate in an asymptotic sense  conformal prediction is mainly concerned with an online probabilistic setting  rather than predicting a single label for each
sample point  a conformal predictor can assign multiple labels  any user defined confidence level  for the error rate can be asymptotically guaranteed  when interpreting these
multi labeled predictions as rejection  we can compare it to pointwise competitive prediction  in this sense  conformal prediction can construct online predictors with a reject option
that have asymptotic performance guarantees  a few important differences between conformal predictions and pointwise competitive prediction can be pointed out  while both
approaches provide hedged predictions  they use different notions of hedging  whereas
in pointwise competitive prediction the goal is to guarantee that with high probability over
the training sample our predictor agrees with the best predictor in the same class over all
points in the accepted domain  the goal in conformal predictions is to provide guarantees
for the average error rate  where the average is taken over all possible samples and test
points   in this sense  conformal prediction cannot achieve pointwise competitiveness  in
addition  conformal prediction also utilizes a different notion of error than the one used in
the pointwise competitive model  while pointwise competitive prediction is focused on performance guarantees for the error rate only on the covered  accepted  examples  conformal
prediction provides a guarantee for all examples  including those that have multiple predictions or none at all   by increasing the multi labeled prediction rate  uncertain prediction  
 

as noted by vovk et al   it is impossible to achieve the conditional probability of error equal to  given the
observed examples  but it is the unconditional probability of error that equals   therefore  it implicitly
involves averaging over different data sequences     vovk et al         p       

   

fiwiener   el yaniv

the error rate can be decreased to any arbitrarily small value  this is not the case with the
pointwise competitive prediction error notion on the covered examples  which is bounded
below by the bayes error on the covered region  finally  conformal prediction mentions
a notion of efficiency  which is similar to coverage but  to the best of our knowledge  no
finite sample results have been established  another interesting scheme in the vicinity of
confidence rated learning is the the guaranteed error machine  gem   campi         in
the gem model the reject option is considered as a correct answer  which means that risk
can be reduced arbitrarily  as in conformal prediction  
pointwise competitive classification is a special case of pointwise competitive prediction
 el yaniv   wiener              wiener   el yaniv        el yaniv   wiener       
wiener        wiener et al          pointwise competitive selective classification was first
addressed by el yaniv and wiener        where the realizable case was studied  in that
paper pointwise competitiveness was termed perfect classification   the present article
extends pointwise competitive classification to noisy problems
there are also a number of theoretical studies of  general  selective classification  not
pointwise competitive   freund et al         studied a simple ensemble method for binary
classification  given a hypothesis class f  the method outputs a weighted average of all
the hypotheses in f  where the weight of each hypothesis exponentially depends on its
individual training error  their algorithm abstains from prediction whenever the weighted
average of all individual predictions is close to zero  they were able to bound the probability
of misclassification by  r f       m  and  under some conditions  they proved a bound of
 r f       f  m  on the rejection rate  the less strategy can be viewed as an extreme
variation of the freund et al  method  we include in our ensemble only hypotheses with
sufficiently low empirical error and we abstain if the weighted average of all predictions is
not definitive          our risk and coverage bounds are asymptotically tighter 
excess risk bounds were developed by herbei and wegkamp        for a model where
each rejection incurs a cost in           their bound applies to any empirical risk minimizer
over a hypothesis class of ternary hypotheses  whose output is in     reject    see also
various extensions by wegkap        and bartlett and wegkamp        
a rejection mechanism for svms based on distance from decision boundary is perhaps
the most widely known and used rejection technique  it is routinely used in medical applications  mukherjee et al         guyon et al         mukherjee         few papers proposed
alternative techniques for rejection in the case of svms  those include taking the reject
area into account during optimization  fumera   roli         training two svm classifiers
with asymmetric cost  sousa  mora    cardoso         and using a hinge loss  bartlett  
wegkamp         grandvalet et al         proposed an efficient implementation of svm
with a reject option using a double hinge loss  they empirically compared their results
with two other selective classifiers  the one proposed by bartlett and wegkamp        and
the traditional rejection based on distance from decision boundary  in their experiments
there was no statistically significant advantage to either method compared to the traditional
approach for high rejection rates 
pointwise selective classification is strongly tied to disagreement based active learning 
for the realizable case  el yaniv and wiener        presented a reduction of stream based
active learning with the cal algorithm of cohn et al         to pointwise competitive
classification  this reduction roughly states that if the rejection rate  the reciprocal of

   

fiagnostic pointwise competitive selective classification

coverage  of less is o polylog m   m  then the problem  f  p   is actively learnable
by cal with exponential speedup  a consequence of this reduction resulted in the first
exponential speedup bounds for cal with general linear models under any finite mixture
of gaussians  the other direction  showing that exponential speedup for cal implies the
above rejection rate for less  in the realizable setting  was recently established by wiener
       and by wiener  hanneke  and el yaniv         using two different techniques  
the version space compression set size  which is extensively utilized in the present
work  has been introduced implicitly by hanneke      b  as a special case of the extended
teaching dimension  and in that context  the version space compression set is called the
minimal specifying set  it was introduced explicitly by el yaniv and wiener        in the
context of selective classification  and was proved by el yaniv and wiener        to be a
special case of the extended teaching dimension of hanneke      b   relations between
the disagreement coefficient and the version space compression set size were first discussed
el yaniv and wiener         sharp ties between these two quantities  such as those stated
in lemma    and others were very recently developed by wiener  hanneke  and el yaniv
       

   concluding remarks
we find the existence of pointwise competitive classification quite fascinating  the striking
feature of such a classifier is that  by definition  a pointwise competitive predictor is free
of estimation error and cannot overfit  this means that our hypothesis class can be as
expressive as we like and still we will be protected from overfitting  however  without
effective coverage bounds our pointwise competitive classifier may refuse to predict at all
times 
the current paper  and recent studies on both selective prediction  el yaniv   wiener 
      and active learning  wiener  hanneke    el yaniv         place the version space
compression set size at the center of stage  as a leading quantity that can drive results and
intuition in both domains  at present  this is the only known technique able to prove fast
coverage for pointwise competitive classification and exponential label complexity speedup
for disagreement based active learning for both general linear models under a fixed mixture
of gaussians and axis aligned rectangles under product distributions   is it possible to
extend these results beyond linear classifiers and axis aligned rectangles under interesting
distribution families  for example  it is plausible that existing results for axis aligned
rectangles can be extended to decision trees 
the formal relationship between active learning and pointwise competitive classification
 el yaniv   wiener        wiener        wiener et al         created a powerful synergy
that allows for migrating results between these two models  currently  this formal connection is manifested via two links  the first  within a realizable setting  is the equivalence
of less based classification with fast coverage to cal based active learning with exponential speedup  the second link consists of bounds that relate the underlying complexity
measures  the disagreement coefficient in active learning  and version space compression
set size in pointwise competitive classification  a number of other non established relations that can significantly substantiate the interaction between the two problems could be
considered  for example  is it possible to prove a direct equivalence between less based

   

fiwiener   el yaniv

pointwise competitive agnostic classification with fast coverage rates and less based active
learning with exponential speedup  we expect that a resolution of this question will have
various interesting implications  for example  such a relationship could potentially facilitate
the migration of very interesting algorithms and techniques devised for active learning to
the pointwise competitive framework  an immediate candidate is the algorithm of beygelzimer et al          which builds on ideas of dasgupta et al       b  and beygelzimer et al 
        resembling the implementation proposed for less via calls to  a constrained  erm
oracle  this algorithm works without tracking the version space for both the final choice of
the hypothesis as well as the querying component  instead  for querying  it relies on an
erm oracle that enforces at most one example based constrain  thus  the importanceweighting technique on which it is based resembles the disbelief principle we outline here 
in this regard  it will be very interesting to also consider and migrate ideas from active
learning algorithms emerging from the online learning branch  orabona   cesa bianchi 
      cesa bianchi et al         dekel et al         while using  as required  online to batch
conversion techniques  zhang        kakade   tewari        cesa bianchi   gentile       
dekel        
the less strategy requires a unanimous vote among all hypotheses in a low empirical
error subset of the hypothesis class  when considering  e g   linear models  this subset of
hypotheses is uncountable  and in any case  even if it is finite  its size can be huge  clearly 
less is an extremely radical and defensive strategy  an immediate question that arises
is whether the less unanimity requirement can be relaxed to a majority vote  can we
achieve pointwise competitiveness with only a  strong  majority vote instead of unanimity 
besides the greater flexibility of a general voting scheme  which may lead to different types
of interesting learning algorithms  such a relaxation can potentially ease the computational
complexity of implementing less  which  as discussed above  is a bottleneck in agnostic
classification   for example  with a relaxed voting scheme we might utilize hypothesis
sampling  for which a classical example in a related context is the celebrated query bycommittee  qbc  strategy  seung et al         freund et al         fine et al         giladbachrach        gilad bachrach et al          however  if strict pointwise competitiveness
is advocated  it is easy to see any strong majority vote is not sufficient  indeed  consider an
f  that differs from all other hypotheses in f on a single point in x   unless the probability
of this point is very large  not the typical case   with high probability this point is not part
of the training set sm   and therefore  any majority vote  even very strong  will label it the
opposite of f    hence  in the worst case  even a strong majority is not sufficient for pointwise
competitiveness  as a natural compromise for the pointwise competitiveness objective  one
can revert to standard excess risk bounds  bartlett et al         whereby we compare the
overall average performance of our predictor  r f    to that of the optimal predictor  r f   
 not pointwise   in this regard  the work of freund  mansour  and schapire        discussed
in section    is such a result with its excess risk bound r f  g    r f  
    o    m    

  is a hyper parameter  and coverage bound  f  g       r f     o ln  f   m     
considering excess risk bounds against f    is it possible to beat the above risk and coverage
bounds using a relaxed voting scheme for rejection  what would be the optimal bounds
in a fully agnostic setting  can better bounds be devised for specific distributions like
gaussian mixtures  we note that the freund et al  strategy is also interesting because

   

fiagnostic pointwise competitive selective classification

the final aggregated predictor is in general outside of f and can  in principle  significantly
outperform f   f  the above bound does not elicit such a behavior   this emphasizes the
potential usefulness of ensembles  applied not only in the rejection scheme  but also in the
final predictor  recall that in the less strategy the final predictor always belongs to f 
thus  when considering ensembles and allowing excess risk bounds  there can be even more
ambitious goals  such as strictly beating f  on average 

acknowledgments
we thank the anonymous referees for their good comments  and are grateful to steve hanneke for helpful and insightful discussions  also  we warmly thank the intel collaborative
research institute for computational intelligence  icri ci   and israel science foundation
 isf  for their generous support 

appendix a  some proofs
the proof of lemma   below relies on the following lemma     wiener et al          whose
proof is also provided here for the sake of self containment 

lemma     wiener et al          in the realizable case  for any r          


 
 
 
 
       
 r     max max   bn
r   
r r     


proof  we will prove that  for any r         

 


b f    r 
 
 
 max   bn
 
       
r
r   

    

the result then follows by taking the supremum of both sides over r   r       
fix r          let m     r  and for i              m   define sm i   sm     xi   yi     also
define dm i   dis vsf  sm i  b f    r   and m i   p xi  dm i  sm i     p  dm i  y  
if b f    r m            clearly holds  otherwise  suppose b f    r m        if xi 
dis vsf  sm i    then we must have  xi   yi    csm   so

n sm   

m
x
i  

 

dis vsf  sm i    xi   

   

fiwiener   el yaniv

therefore 
p  n sm          b f    r m 
 
 m
x

p
dis vsf  s
   xi          b f   r m
 

p
 p

m i

i  
 m
x

dm i  xi  

 

i  
 m
x

       b f   r m

dis b f   r    xi  

 

i  

 p

 

m
x



dis b f   r    xi  



 

 

 p

 

i  
m
x

p



dis b f   r    xi  
i  
m
x

i  
 m
x

i  
 m
x
 

 

 



 

 

dm i  xi  

dm i  xi  



m
x

 p

dis b f   r    xi  

i  

m
x

 
b f    r m 
dis b f   r    xi   
  
 

dm i  xi  

m
x

 

 

i  

       b f   r m

 
 

dis b f   r    xi     b f   r m
 



 
b f    r m 
dis b f   r    xi   
  

dis b f   r    xi  

 



m
x
i  

 

 

 
 

dis b f   r    xi    b f   r m
 

       b f    r m

i  

 

 



dis b f   r    xi  

i  



 

dm i  xi  



 

        b f   r m

 

since we are considering the case b f    r m        a chernoff bound implies
 
m
x

 exp  b f    r m        e   
p
dis b f   r    xi          b f   r m
 

i  

furthermore  markovs inequality implies
p

m
x

 

dis b f   r    xi  

i  



 

dm i  xi  

 

        b f    r m


mb f    r   e

hp
m

i  

 

dm i  xi  

       mb f    r 

since the xi values are exchangeable 
 
 m
m
m
fi
h h
ii x
x
x




fi
e e dm i  xi  fism i  
e m i   me m m  
e
dm i  xi    
 

i  

 

i  

i  

   

i

 

fiagnostic pointwise competitive selective classification

it can be shown  hanneke        that this is at least
m    r m  b f    r  
in particular  when b f    r m        we must have r                which implies
    r   r        so that we have
 
 m
x

e
dm i  xi         mb f   r  
 

i  

altogether  we have established that
p  n sm          b f    r m   

mb f    r        mb f    r 
  e 
       mb f    r 
  
 
  e          
  


 
thus  since n sm    bn m    
with probability at least        we must have that


b f    r 
 
 
bn m 
        b f    r m        
  
r

proof of lemma    assuming that bn  m      o polylog m  log   holds  there exists
a constant              for
because bn  m    is non which bn  m        o  polylog m   

 
 
 bn  m       and thus bn m    
  o  polylog m    therefore 
increasing with   bn m    




 

 
 
max bn m 
  o max polylog m    o polylog
 
  
r 
m  r 
m  r 
and using lemma    we have 




 
 r     max
max   bn m 
     
  
m  r  



 

 
 
 
  o polylog
          max bn m 
  
r 
m  r 

references
amaldi  e     kann  v          the complexity and approximability of finding maximum
feasible subsystems of linear relations  theoretical computer science                  
bartlett  p  l   jordan  m  i     mcauliffe  j  d          convexity  classification  and risk
bounds  journal of the american statistical association                    
bartlett  p     mendelson  s          discussion of      ims medallion lecture  local
rademacher complexities and oracle inequalities in risk minimization by v  koltchinskii  annals of statistics               

   

fiwiener   el yaniv

bartlett  p   mendelson  s     philips  p          local complexities for empirical risk
minimization  in colt  proceedings of the workshop on computational learning
theory  morgan kaufmann publishers 
bartlett  p     wegkamp  m          classification with a reject option using a hinge loss 
journal of machine learning research              
ben david  s   eiron  n     long  p          on the difficulty of approximately maximizing
agreements  journal of computer and system sciences                 
beygelzimer  a   dasgupta  s     langford  j          importance weighted active learning 
in proceedings of the   th annual international conference on machine learning  pp 
      acm 
beygelzimer  a   hsu  d   langford  j     zhang  t          agnostic active learning without
constraints  advances in neural information processing systems    
beygelzimer  a   dasgupta  s     langford  j          importance weighted active learning 
in proceedings of the   th annual international conference on machine learning  pp 
      acm 
beygelzimer  a   hsu  d   langford  j     zhang  t          agnostic active learning without
constraints  arxiv preprint arxiv           
bounsiar  a   grall  e     beauseroy  p          a kernel based rejection method for supervised classification  international journal of computational intelligence            
bousquet  o   boucheron  s     lugosi  g          introduction to statistical learning
theory  in advanced lectures on machine learning  vol       of lecture notes in
computer science  pp          springer 
campi  m          classification with guaranteed probability of error  mach  learn          
     
cesa bianchi  n     gentile  c          improved risk tail bounds for on line algorithms 
information theory  ieee transactions on                 
cesa bianchi  n   gentile  c     orabona  f          robust bounds for classification via
selective sampling  in proceedings of the   th annual international conference on
machine learning  pp          acm 
chang  c     lin  c          libsvm  a library for support vector machines  acm
transactions on intelligent systems and technology                software available
at http   www csie ntu edu tw  cjlin libsvm 
chow  c          an optimum character recognition system using decision function  ieee
trans  computer                
chow  c          on optimum recognition error and reject trade off  ieee trans  on
information theory           
cohn  d   atlas  l     ladner  r          improving generalization with active learning 
machine learning                 
dasgupta  s   hsu  d     monteleoni  c       a   a general agnostic active learning algorithm  in nips 

   

fiagnostic pointwise competitive selective classification

dasgupta  s   monteleoni  c     hsu  d  j       b   a general agnostic active learning
algorithm  in advances in neural information processing systems  pp         
dekel  o          from online to batch learning with cutoff averaging   nips 
dekel  o   gentile  c     sridharan  k          robust selective sampling from single and
multiple teachers   in colt  pp         
el yaniv  r     pidan  d          selective prediction of financial trends with hidden
markov models  in nips  pp         
el yaniv  r     wiener  y          on the foundations of noise free selective classification 
journal of machine learning research               
el yaniv  r     wiener  y          agnostic selective classification  in neural information
processing systems  nips  
el yaniv  r     wiener  y          active learning via perfect selective classification 
journal of machine learning research             
el yaniv  r     wiener  y          on the version space compression set size and its
applications  in vovk  v   papadopoulos  h     gammerman  a   eds    measures of
complexity  festschrift for alexey chervonenkis  springer  berlin 
fine  s   gilad bachrach  r     shamir  e          query by committee  linear separation
and random walks  theoretical computer science                
freund  y   mansour  y     schapire  r          generalization bounds for averaged classifiers  annals of statistics                   
freund  y   seung  h   shamir  e     tishby  n          selective sampling using the query
by committee algorithm  machine learning             
friedman  e          active learning for smooth problems  in proceedings of the   nd
annual conference on learning theory 
fumera  g     roli  f          support vector machines with embedded reject option  in
pattern recognition with support vector machines  first international workshop  pp 
       
fumera  g   roli  f     giacinto  g          multiple reject thresholds for improving
classification reliability  lecture notes in computer science       
gilad bachrach  r          to pac and beyond  ph d  thesis  the hebrew university of
jerusalem 
gilad bachrach  r   navot  a     tishby  n          query by committee made real  in
nips 
grandvalet  y   rakotomamonjy  a   keshet  j     canu  s          support vector machines with a reject option  in nips  pp          mit press 
guyon  i   weston  j   barnhill  s     vapnik  v          gene selection for cancer classification using support vector machines   machine learning         
hanneke  s       a   a bound on the label complexity of agnostic active learning  in icml 
pp         

   

fiwiener   el yaniv

hanneke  s       b   teaching dimension and the complexity of active learning  in proceedings of the   th annual conference on learning theory  colt   vol       of lecture
notes in artificial intelligence  pp       
hanneke  s          theoretical foundations of active learning  ph d  thesis  carnegie
mellon university 
hanneke  s          a statistical theory of active learning  unpublished 
hanneke  s          activized learning  transforming passive to active with improved label
complexity  the journal of machine learning research                  
hellman  m          the nearest neighbor classification rule with a reject option  ieee
trans  on systems sc  and cyb             
herbei  r     wegkamp  m          classification with reject option  the canadian journal
of statistics                 
kakade  s     tewari  a          on the generalization ability of online strongly convex
programming algorithms  in advances in neural information processing systems
 nips   pp         
koltchinskii  v               ims medallion lecture  local rademacher complexities and
oracle inequalities in risk minimization  annals of statistics               
landgrebe  t   tax  d   paclk  p     duin  r          the interaction between classification
and reject performance for distance based reject option classifiers  pattern recognition
letters                 
li  l     littman  m  l          reducing reinforcement learning to kwik online regression 
annals of mathematics and artificial intelligence         
li  l   littman  m     walsh  t          knows what it knows  a framework for self aware
learning  in proceedings of the   th international conference on machine learning  pp 
        acm 
li  l          a unifying framework for computational reinforcement learning theory  ph d 
thesis  rutgers  the state university of new jersey 
massart  p          some applications of concentration inequalities to statistics  in annales
de la faculte des sciences de toulouse  vol     pp          universite paul sabatier 
mendelson  s          improving the sample complexity using global data  information
theory  ieee transactions on                   
mukherjee  s          chapter    classifying microarray data using support vector machines 
in of scientists from the university of pennsylvania school of medicine and the school
of engineering and applied science  kluwer academic publishers 
mukherjee  s   tamayo  p   slonim  d   verri  a   golub  t   mesirov  j  p     poggio  t 
        support vector machine classification of microarray data  tech  rep   ai memo
      massachusetts institute of technology 
orabona  f     cesa bianchi  n          better algorithms for selective sampling  in
proceedings of the   th international conference on machine learning  icml     
pp         

   

fiagnostic pointwise competitive selective classification

pietraszek  t          optimizing abstaining classifiers using roc analysis  in proceedings of the twenty second international conference on machine learning icml   pp 
       
santos pereira  c     pires  a          on optimal reject rules and roc curves  pattern
recognition letters                 
seung  h   opper  m     sompolinsky  h          query by committee  in proceedings of
the fifth annual workshop on computational learning theory  colt   pp         
society  a  c          cancer facts   figures       
sousa  r   mora  b     cardoso  j          an ordinal data method for the classification
with reject option  in icmla  pp          ieee computer society 
strehl  a  l     littman  m  l          online linear regression and its application to
model based reinforcement learning  in advances in neural information processing
systems  pp           
tauman kalai  a   klivans  a   mansour  y     servedio  r          agnostically learning
halfspaces  siam j  comput                    
tortorella  f          an optimal reject rule for binary classifiers  lecture notes in computer
science               
tsybakov  a          optimal aggregation of classifiers in statistical learning  annals of
mathematical statistics             
vovk  v   gammerman  a     shafer  g          algorithmic learning in a random world 
springer  new york 
wang  l          smoothness  disagreement coefficient  and the label complexity of agnostic
active learning  jmlr           
wegkap  m          lasso type classifiers with a reject option  electronic journal of
statistics            
wiener  y          theoretical foundations of selective prediction  ph d  thesis  technion
 israel institute of technology 
wiener  y     el yaniv  r          pointwise tracking the optimal regression function  in
advances in neural information processing systems     pp           
wiener  y   hanneke  s     el yaniv  r          a compression technique for analyzing
disagreement based active learning  arxiv preprint arxiv           
zhang  t          data dependent concentration bounds for sequential prediction algorithms  in learning theory  pp          springer 

   

fi