journal of artificial intelligence research               

submitted        published      

coherent predictive inference under exchangeability
with imprecise probabilities
gert de cooman
jasper de bock

gert decooman ugent be
jasper debock ugent be

ghent university  systems research group
technologieparkzwijnaarde    
     zwijnaarde  belgium

mrcio alves diniz

marcio alves diniz gmail com

federal university of so carlos  department of statistics
rod  washington luis  km    
so carlos  brazil

abstract
coherent reasoning under uncertainty can be represented in a very general manner by
coherent sets of desirable gambles  in a context that does not allow for indecision  this leads
to an approach that is mathematically equivalent to working with coherent conditional
probabilities  if we do allow for indecision  this leads to a more general foundation for coherent
 imprecise  probabilistic inference  in this framework  and for a given finite category set 
coherent predictive inference under exchangeability can be represented using bernstein
coherent cones of multivariate polynomials on the simplex generated by this category set 
this is a powerful generalisation of de finettis representation theorem allowing for both
imprecision and indecision 
we define an inference system as a map that associates a bernstein coherent cone of
polynomials with every finite category set  many inference principles encountered in the
literature can then be interpreted  and represented mathematically  as restrictions on such
maps  we discuss  as particular examples  two important inference principles  representation
insensitivitya strengthened version of walleys representation invarianceand specificity 
we show that there is an infinity of inference systems that satisfy these two principles 
amongst which we discuss in particular the skeptically cautious inference system  the inference
systems corresponding to  a modified version of  walley and bernards imprecise dirichlet
multinomial models  idmm   the skeptical idmm inference systems  and the haldane
inference system  we also prove that the latter produces the same posterior inferences as
would be obtained using haldanes improper prior  implying that there is an infinity of
proper priors that produce the same coherent posterior inferences as haldanes improper one 
finally  we impose an additional inference principle that allows us to characterise uniquely
the immediate predictions for the idmm inference systems 

   introduction
this paper deals with predictive inference for categorical variables  we are therefore concerned
with a  possibly infinite  sequence of variables xn that assume values in some finite set of
categories a  after having observed a number n of them  and having found that  say x    x   
x    x            xn   xn   we consider some subjects belief model for the next n variables
xn           xn n   in the probabilistic traditionand we want to build on this tradition in the
     ai access foundation  all rights reserved 

fide cooman  de bock    diniz

context of this paperthis belief can be modelled by a conditional predictive probability
mass function pn   x            xn   on the set an of their possible values  these probability mass
functions can be used for prediction or estimation  for statistical inferences  and in decision
making involving the uncertain values of these variables  in this sense  predictive inference lies
at the heart of statistics  and more generally  of learning under uncertainty  for this reason 
it is also of crucial importance for dealing with uncertainty in artificial intelligence  where
for instance  intelligent systems have to learn about multinomial probabilities  or markov
transition probabilities  rates of occurrence for phenomena  local probabilities in bayesian or
credal networks and so on  we refer to the synthesis by geisser        and the collection of
essays by zabell        for good introductions to predictive inference and the underlying
issues that the present paper will also be concerned with 
what connects these predictive probability mass functions for various values of n  n and
 x            xn   are the requirements of time consistency and coherence  the former requires
that when n   n    then pn    x            xn   can be obtained from pn    x            xn   through the
usual marginalisation procedure  while the latter essentially demands that these conditional
probability mass functions should be connected with time consistent unconditional probability
mass functions through bayess rule 
a common assumption about the variables xn is that they are exchangeable  meaning
roughly that the subject believes that the order in which they are observed  or present
themselves  has no influence on the decisions and inferences he will make regarding these
variables  this assumption  and the analysis of its consequences  goes back to de finetti
        see also cifarelli   regazzini         his famous representation theorem states 
in essence  that the time consistent and coherent conditional and unconditional predictive
probability mass functions associated with a countably infinite exchangeable sequence of
variables in a are completely characterised by  and completely characterisea unique
probability measure on the borel sets of the simplex of all probability mass functions on a 
called their representation  
this leads us to the central problem of predictive inference  since there is an infinity of
such probability measures on the simplex  which one does a subject choose in a particular
context  and how can a given choice be motivated and justified  the subjectivists of de
finettis persuasion might answer that this question needs no answer  a subjects personal
predictive probabilities are entirely his  and time consistency and coherence are the only
requirements he should heed  earlier scholars  like laplace and bayes  whom we would now
also call subjectivists  invoked the principle of indifference to justify using a specific class of
predictive mass functions  proponents of the logicist approach to predictive inference would
try enunciating general inference principles in order to narrow down  and hopefully eliminate
entirely  the possible choices for the representing probability measures on the simplex  the
logicians w  e  johnson        and  in a much more systematic fashion  rudolf carnap       
         unless the observed sequence has probability zero 
   actually  in order to clarify the connection with what we shall do later on  the essence of de finettis
argument is that the representation is a coherent prevision on the set of all multinomial polynomialsor
equivalently  of all continuous real functionson this simplex  de cooman  quaeghebeur    miranda 
    b   as a  finitely additive  coherent prevision  it can be extended uniquely only so far as to the set
of all lower semicontinuous functions  but it does determine a unique  countably additive  probability
measure on the borel sets of that simplex  through the f  riesz representation theorem  de cooman  
miranda      a  troffaes   de cooman        

 

ficoherent predictive inference under exchangeability

tried to develop an axiom system for predictive inference based on such reasonable inference
principles  carnaps first group of axioms is related to what we have called coherence  but
as we suggested  these by themselves are too weak to single out a particular predictive
model  his second group consisted of invariance axioms  including exchangeability  he also
included an axiom of instantial relevance  translating the intuitive principle that predictive
inferences should actually learn from experience  his last axiom  predictive irrelevance  was
also proposed earlier by johnson and called the sufficientness postulate by good        
armed with these axioms  carnap was able to derive a continuum of probabilistic inference
rules  closely related to the dirichlet multinomial model and to the imprecise dirichlet
multinomial model  idmm  proposed by walley        and walley and bernard        
which we discuss in appendices c and d  respectively 
our point of view holds the middle ground between the subjectivist and logicist positions 
it should be possible for a subject to make assessments for certain predictive probabilities 
and to combine these with certain inference principles he finds reasonable  or which suit his
purpose for the problem at hand  indeed  the inference systems we introduce and discuss
in section    and the notion of conservative coherent inferenceor natural extensionwe
associate with them  provide an elegant framework and tools for making conservative coherent
predictive inferences that combine  local  subjective probability assessments with  general 
inference principles  and our work in section    on characterising the immediate predictions
for the idmm constitutes an exercise inor an example forprecisely that 
this idea of conservative probabilistic inference brings us to what we believe is the
main contribution of this paper  it is a central idea in de finettis        approach to
probabilitybut also of course implicit in the markov and chebyshev inequalitiesthat
when a subject makes probability assessments  we can consider them as bounds on so called
precise probability models  calculating such most conservative but tightest bounds is indeed
what de finettis        fundamental theorem of prevision  see also lad        is about 
the theory of imprecise probabilities  brought to a synthesis by williams        and walley
              but going back to boole        and keynes         with crucial contributions
by quite a number of statisticians and philosophers  smith        levi        seidenfeld 
schervish    kadane         looks at conservative probabilistic inference precisely in this
way  how can we calculate as efficiently as possible the consequencesin the sense of most
conservative tightest boundsof making certain probability assessments  these may be local
assessments  such as inequalities imposed on the probabilities or previsions of certain events
or variables  or structural assessments  such as independence  or exchangeability 
one advantage of imprecise probability models is that they allow for imprecision  or in
other words  the use of partial probability assessments using bounding inequalities rather
than equalities  another  related  advantage is that they allow for indecision to be modelled
explicitly  loosely stated  if the imposed bounds on probabilities allow for more than one
probability model as a solution  it may very well be that of two actions  the first has the
higher expected utility for one compatible probability model  and the smaller for another
compatible probability model  meaning that neither action is robustly preferred over the other 
so with this current stated model for his beliefs  a subject is then undecided between these
actions  in section    we give a concise overview of the relevant ideas  models and techniques
in the field of imprecise probabilities  a much more extensive and detailed recent overview of
this area of research was published by augustin  coolen  de cooman  and troffaes        
 

fide cooman  de bock    diniz

the present paper  then  can be described as an application of ideas in imprecise probabilities to predictive inference  its aim is to studyand develop a general framework for
dealing withconservative coherent predictive inference using imprecise probability models 
using such models will also allow us to represent a subjects indecision  which we believe is a
natural state to be in when knowing  or having learned little  about the problem at hand 
it seems important that theories of learning under uncertainty in general  and predictive
inference in particular  at least allow us  i  to start out with conservative  very imprecise and
indecisive models when little has been learned  and  ii  to become more precise and decisive
as more observations come in  we shall see that the abstract notion of an inference system
that we introduce further on  allows forbut does not necessarily forcesuch behaviour 
and we shall give a number of examples of concrete inference systems that display it 
our work here builds on  but manages to reach much further than  an earlier paper by
one of the authors  de cooman  miranda    quaeghebeur      a   one reason why it does
so  is that this earlier work deals only with immediate prediction models  and as we shall
see further on  predictive inference using imprecise probabilities is not completely determined
by immediate prediction  contrary to what we can expect when using precise probabilities 
but the main reason is that we are now in a position to use a very powerful mathematical
language to represent imprecise probabilistic inferences  walleys        coherent sets of
desirable gambles  earlier imprecise probability models  boole              koopman       
centred on lower and upper probability bounds for eventsor propositions  later on  walley 
      section       it became apparent that this language of events and lower and upper
probabilities is lacking in power of expression  a much more expressive theory uses random
variables and their lower previsions or expectations  this successful theory of coherent lower
previsions is by now quite well developed  walley        augustin et al         troffaes  
de cooman         but it faces a number of problems  such as its mathematical as well as
conceptual complexity  especially when dealing with conditioning and independence  and
the fact that  as is the case with many other approaches to probability  and as we shall see
further on in section      it has issues with conditioning on sets of  lower  probability zero 
a very attractive solution to these problems was offered by walley         in the form of
coherent sets of desirable gambles  inspired by earlier ideas  smith        williams      b 
seidenfeld  schervish    kadane         here  the primitive notions are not probabilities of
events  nor expectations of random variables  the focus is rather on whether a gamble  or
a risky transaction  is desirable to a subjectstrictly preferred to the zero transaction  or
status quo  and a basic belief model is now not a probability measure or lower prevision 
but a set of desirable gambles  of course  stating that a gamble is desirable also leads to a
particular lower prevision assessment  it provides a lower bound of zero on the prevision of
the gamble  we explain why we prefer to use sets of desirable gambles as basic uncertainty
models in section   
in summary  then  our aim in this paper is to use sets of desirable gambles to extend the
existing probabilistic theory of predictive inference  let us explain in some detail how we
intend to go about doing this  the basic building blocks are introduced in sections     as
already indicated above  we give an overview of relevant notions and results concerning our
imprecise probability model of choicecoherent sets of desirable gamblesin section    in
particular  we explain how to use them for conservative inference as well as conditioning 
 

ficoherent predictive inference under exchangeability

how to derive more commonly used models  such as lower previsions and lower probabilities 
from them  and how they relate to precise probability models 
in section    we explain how we can describe a subjects beliefs about a sequence of
variables in terms of predictive sets of desirable gambles  and the derived notion of predictive
lower previsions  these imprecise probability models generalise the above mentioned predictive
probability mass functions pn   x            xn    and they constitute the basic tools we shall be
working with  we also explain what are the proper formulations for the above mentioned
time consistency and coherence requirements in this more general context 
in section    we discuss a number of inference principles that we believe could be reasonably
imposed on predictive inferences  and we show how to represent them mathematically in
terms of predictive sets of desirable gambles and lower previsions  pooling invarianceor
what walley        has called the representation invariance principle  rip and renaming
invariance seem reasonable requirements for any type of predictive inference  and category
permutation invariance seems a natural thing to require when starting from a state of
complete ignorance  taken together  they constitute what we call representation insensitivity 
it means that predictive inferences remain essentially unchanged when we transform the
set of categories  or in other words that they are essentially insensitive to the choice of
representationthe category set  another inference principle we look at imposes the so called
specificity property  when predictive inference is specific  then for a certain type of question
involving a restricted number of categories  a more general model can be replaced by a more
specific model that deals only with the categories of interest  and will produce the same
relevant inferences  bernard        
the next important step is taken in section    where we recall from the literature  de
cooman et al       b  de cooman   quaeghebeur        how to deal with exchangeability
when our predictive inference models are imprecise  we recall that de finettis representation
theorem can be significantly generalised  in this case  the time consistent and coherent
predictive sets of desirable gambles are completely characterised by a set of  multivariate 
polynomials on the simplex of all probability mass functions on the category set   this
set of polynomials must satisfy a number of properties  which taken together define the
notion of bernstein coherence  without becoming too technical at this point  the conclusion
of this section is that  in our more general context  the precise probabilistic notion of a
representing probability measure on the simplex of all probability mass functions is replaced
by a bernstein coherent set of polynomials on this simplex  this set of polynomials serves
completely the same purpose as the representing probability measure  it completely determines 
and conveniently and densely summarises  all predictive inferences  this is the reason why
the rest of the developments in the paper are expressed in terms of such bernstein coherent
sets of polynomials 
we introduce coherent inference systems in section   as maps that associate with any
finite set of categories a bernstein coherent set of polynomials on the simplex of probability
mass functions on that set  so a coherent inference system is a way of fixing completely all
coherent predictive inferences for all possible category sets  our reasons for introducing such
coherent inference systems are twofold  first of all  the inference principles in section   impose
connections between predictive inferences for different category sets  so we can represent such
   in contradistinction with de finettis version  our version has no problems with conditioning on observed
sequences of  lower  probability zero 

 

fide cooman  de bock    diniz

inference principles mathematically as restrictions on coherent inference systems  which is the
main topic of section    secondly  it allows us to extend the method of natural extensionor
conservative inferenceintroduced in section      to also take into account principles for
predictive inference  or more generally  predictive inference for multiple category sets at once 
this leads to a method of combining  local  predictive probability assessments with  global 
inference principles to produce the most conservative predictive inferences compatible with
them 
as a first illustration of the power of our methodology  we look at immediate prediction
in section    what implications do representation insensitivity and specificity have for
predictive inference about the single next observation  we show that our approach allows us
to streamline  simplify and significantly extend previous attempts in this direction by de
cooman et al       a  
the material in sections     shows  by producing explicit examples  that there are
quite a few different typeseven uncountable infinitiesof coherent inference systems that
are representation insensitive and or specific  we discuss the vacuous and nearly vacuous
inference systems in sections   and     the skeptically cautious inference system in section    
the family of idmm inference systems in section     the family of skeptical idmm inference
systems in section     and the haldane inference system in section     most of these inference
systems  apart from the idmm  appear here for the first time  also  we believe that we are
the first to publish a detailed and explicitas well as still elegantproof that the idmm
inference systems are indeed representation insensitive and specific  it should already be
mentioned here  however  that our idmm inference systems are based on a modified  and
arguably better behaved  version of the models originally introduced by walley and bernard
 see walley        walley   bernard        bernard         we refer to appendix d for more
explanation  with a proof that the original idmm is not specific and that  contrary to what
is often claimed  it does not satisfy the so called nestedness property 
our results disprove the conjecture  bernard        de cooman et al       a  that the
idmm inference systemsour version or the original oneare the only ones  or even the
most conservative ones  that satisfy both representation insensitivity and specificity  but we
do show in section    that the idmm family of immediate predictionswhich are the same
for our version and for the original oneare in a definite sense the most conservative ones
that are representation insensitive and specific  and satisfy another requirement  which we
have called having concave surprise 
in the conclusion  section     we point to a number of surprising consequences of our
results  and discuss avenues for further research 
in order to make this paper as self contained as possible  we have included a number of
appendices with additional discussion  to help the reader find his way through the many
notions and notations we need in this paper  appendix a provides of list of the most common
ones  with a short hint at their meaning  and where they are introduced  appendix b provides
useful and necessary background on the theory of multivariate polynomials on simplices  and
the important part that bernstein basis polynomials have there  our discussion of idmm
inference systems relies quite heavily on dirichlet densities on simplices  and the expectation
operators associated with them  we discuss their most important and relevant properties in
appendix c  appendix d contains a discussion of the original idm and idmm models  as
proposed by walley and bernard  see walley              walley   bernard        bernard 
 

ficoherent predictive inference under exchangeability

       where we show that some of the claims they make about this model need to be
more carefully formulated  as we stated above  this is our main reason for introducing  in
section     our own modified version of the idmm models  which does not suffer from such
shortcomings  and produces the same immediate prediction models as the original version 
finally  in an effort to make this lengthy paper as readable as possible  we have moved all
proofs  and some additional technical discussion  to appendix e 

   imprecise probability models
in this section  we give a concise overview of imprecise probability models for representing 
and making inferences and decisions under  uncertainty  as suggested in the introduction 
we shall focus on sets of desirable gambles as our uncertainty models of choice 
let us briefly summarise in the next section why  in the present paper  we work with such
sets as our basic uncertainty models for doing conservative probabilistic inference  the reader
who wants to dispense with motivation can proceed to section      where we introduce the
mathematics behind these models  in later sections  we shall of course also briefly mention
derived results in terms of the more familiar language of  lower  previsions and probabilities 
    why sets of desirable gambles 
first of all  a number of examples in the literature  moral        couso   moral        de
cooman   quaeghebeur        de cooman   miranda        have shown that working
with and making inferences using such models is more general and more expressive  it is
also simpler and more elegant from a mathematical point of view  and it has a very intuitive
geometrical interpretation  quaeghebeur         we shall see in sections     and   that
marginalisation and conditioning are especially straightforward  and that there are no issues
with conditioning on sets of  lower  probability zero 
also  it should become apparent from the discussion in section      and has been explained
in some detail by moral and wilson        and de cooman and miranda         that the
similarity between accepting a gamble on the one hand and accepting a proposition to be true
on the other  gives a very logical flavour to conservative probabilistic inference  indeed  there
is a strong analogy between the two  which connects conservative probabilistic inferencealso
called natural extension in the fieldwith logical deduction  where in classical propositional
logic we are looking for the smallest deductively closed set that contains a number of given
propositions  in an imprecise probabilities context we are looking for the smallest coherent set
of desirable gambles that contains a number of given gambles  in the context of this analogy 
precise probability models are closely related to complete  or maximal  deductively closed
setsperfect information states  this is a clear indication that precise probability models by
themselves are not well suited for dealing with conservative inference  and that we need the
broader context of imprecise probability models as a natural language and setting in which
to do this  so in summary  working with sets of desirable gambles encompasses and subsumes
as special cases both classical  or precise  probabilistic inference and inference in classical
propositional logic  see the detailed discussion by de cooman and miranda        
finally  as we briefly explain in section    de cooman and quaeghebeur        have
shown that working with sets of coherent desirable gambles is especially illuminating in the
context of modelling exchangeability assessments  it exposes the simple geometrical meaning
 

fide cooman  de bock    diniz

of the notion of exchangeability  and leads to a simple and particularly elegant proof of a
significant generalisation of de finettis        representation theorem for exchangeable
random variables 
in summary  we work with sets of desirable gambles because they are the most powerful 
expressive and general models at hand  because they are very intuitive to work withthough
unfortunately less familiar to most people not closely involved in the field  and  very
importantly  because they avoid problems with conditioning on sets of  lower  probability
zero  for more details  we refer to the work of walley         moral         couso and moral
        de cooman and quaeghebeur         and quaeghebeur        
    coherent sets of desirable gambles and natural extension
we consider a variable x that assumes values in some finite  possibility space a  we model
a subjects beliefs about the value of x by looking at which gambles on this variable the
subject finds desirable  meaning that he strictly prefers  them to the zero gamblethe status
quo  this is a very general approach  that extends the usual rationalist and subjectivist
approach to probabilistic modelling to allow for indecision and imprecision 
a gamble is a real valued function f on a  it is interpreted as an uncertain reward f  x 
that depends on the value of x  and is expressed in units of some predetermined linear utility 
it represents the reward the subject gets in a transaction where first the actual value x of
x is determined  and then the subject receives the amount of utility f  x which may be
negative  meaning he has to pay it  throughout the paper  we use the device of writing f  x 
when we want to make clear what variable x the gamble f depends on 
events are subsets of the possibility space a  with any event b  a we can associate a
special gamble ib   called its indicator  which assumes the value   on b and   elsewhere 
we denote the set of all gambles on a by l a   it is a linear space under point wise
addition of gambles  and point wise multiplication of gambles with real numbers  for any
subset a of l a   posi a  is the set of all positive linear combinations of gambles in a 
posi a    

 
n

 
k fk   fk  a  k  r     n  n  

   

k  

here  n is the set of natural numbers  without zero   and r   is the set of all positive real
numbers  a convex cone of gambles is a subset a of l a  that is closed under positive linear
combinations  meaning that posi a    a 
for any two gambles f and g on a  we write f  g if  x  a f  x   g x   and f   g
if f  g and f    g  a gamble f     is called positive  a gamble g    is called non positive 
   for the sake of simplicity  we restrict this discussion to finite possibility spaces  because this is all we
really need for the purposes of this paper  in a very limited number of remarks further on  we shall have
occasion to mention related notions for infinite possibility spaces  but we will give ample references there
to guide the interested reader to the relevant literature 
   we want to point out that the notion of strict preferenceor preference without indifferencecommonly
used in preference modelling  should not be confused with walleys        section        notion of strict
desirability  which is only one of the many ways to construct from a lower prevision a set of gambles that
are strictly preferred to the zero gamble  see also the discussion near the end of section      for more
details  we refer to a recent paper by quaeghebeur  de cooman  and hermans        

 

ficoherent predictive inference under exchangeability

l    a  denotes the convex cone of all positive gambles  and l   a  the convex cone of all
non positive gambles 
we collect the gambles that a subject finds desirablestrictly prefers  to the zero gamble
into his set of desirable gambles  and we shall take such sets as our basic uncertainty models 
of course  they have to satisfy certain rationality criteria 
definition    coherence   a set of desirable gambles d  l a  is called coherent if it
satisfies the following requirements 
d     
  d 
d   l    a   d 
d   d   posi d  
d a  denotes the set of all coherent sets of desirable gambles on a 
requirement d  turns d into a convex cone  due to d   it includes l    a   by d d   it
avoids non positivity 
d   if f    then f 
  posi d   or equivalently l   a   posi d     
l    a  is the smallest coherent subset of l a   this so called vacuous model therefore
reflects minimal commitments on the part of the subject  if he knows absolutely nothing
about the likelihood of the different outcomes  he will only strictly prefer to zero those
gambles that never decrease his wealth and have some possibility of increasing it 
when d   d    a subject with a set of desirable gambles d  is more conservative  or less
committal  than a subject with a set of desirable gambles d    simply because the latter strictly
prefers to zero all the gambles that the former does  and possibly more  the inclusion relation
imposes a natural partial ordering on sets of desirable gambles  with a simple interpretation
of is at least as conservative as 
 for any non empty family of coherent sets of desirable gambles di   i  i  its intersection
ii di is still coherent  this simple result underlies the notion of  conservative  coherent
inference  if a subject gives us an assessmenta set a  l a  of gambles on a that he
finds desirablethen it tells us exactly when this assessment can be extended to a coherent
set of desirable gambles  and how to construct the smallestand therefore least committal
or most conservativesuch set 
theorem    natural extension  de cooman   quaeghebeur         let a  l a   and
define its natural extension by  

ea   
 d  d a    a  d   
then the following statements are equivalent 
 i  a avoids non positivity  l   a   posi a     
 ii  a is included in some coherent set of desirable gambles 
   see footnote   

   as usual  in this expression  we let    l a  

 

fide cooman  de bock    diniz

 iii  ea    l a  
 iv  the set of desirable gambles ea is coherent 
 v  ea is the smallest coherent set of desirable gambles that includes a 
when any  and hence all  of these equivalent statements holds  ea   posi l    a   a  
moreover  a is coherent if and only if a    l a  and ea   a 
    maximal coherent sets of desirable gambles
an element d of d a  is called maximal if it is not strictly included in any other element of
d a   or in other words  if adding any gamble f to d makes sure we can no longer extend
the set d   f   to a set that is still coherent 
 d   d a   d  d   d   d    
m a  denotes the set of all maximal elements of d a   a coherent set of desirable gambles
d is maximal if and only if for all non zero gambles f on a  f 
  d  f  d  see couso  
moral       for the case of finite a  and de cooman   quaeghebeur       for the infinite
case   coherence and natural extension can be described completely in terms of maximal
elements 
theorem    couso   moral        de cooman   quaeghebeur         a set a avoids
non positivity
if and only if there is some maximal d  m a  such that a  d  moreover 

ea    d  m a    a  d  
    conditioning with sets of desirable gambles
let us suppose that our subject has a coherent set d of desirable gambles on a  expressing his
beliefs about the value that a variable x assumes in a  we can then ask what his so called
updated set dcb of desirable gambles on b would be  were he to receive the additional
informationand nothing morethat x actually belongs to some subset b of a  the
updating  or conditioning  rule for sets of desirable gambles states that 
g  dcb  gib  d for all gambles g on b 

   

it states that the gamble g is desirable to a subject were he to observe that x  b if
and only if the called off gamble gib is desirable to him  this called off gamble gib is the
gamble on the variable x that gives a zero rewardis called offunless x  b  and in
that case reduces to the gamble g on the new possibility space b  the updated set dcb
is a set of desirable gambles on b that is still coherent  provided that d is  de cooman
  quaeghebeur         see the discussions by moral         couso and moral         de
cooman and quaeghebeur         de cooman and miranda        and quaeghebeur       
for more detailed information on updating sets of desirable gambles 
    coherent lower previsions
we now use coherent sets of desirable gambles to introduce derived concepts  such as coherent
lower previsions  and probabilities 
  

ficoherent predictive inference under exchangeability

given a coherent set of desirable gambles d  the functional p defined on l a  by
p  f      sup    r   f    d  for all f  l a  

   

is a coherent lower prevision  walley        thm          this means that it is a lower
envelope of the expectations associated with some set of probability mass functions   or 
equivalently  that it satisfies the following coherence properties  walley              de
cooman   quaeghebeur        miranda   de cooman        troffaes   de cooman        
p   p  f    min f for all gambles f on a 
p   p  f   g   p  f     p  g  for all gambles f  g on a 
p   p  f     p  f   for all gambles f on a and all real     
here we used the notation min f    min  f  x    x  a   max f is defined similarly  the
conjugate upper prevision p is defined by p  f      inf    r     f  d    p  f    the
following properties are implied by p p  
p   max f  p  f    p  f    min f for all gambles f on a 
p   p  f       p  f      and p  f       p  f      for all gambles f on a and all   r 
for any gamble f   p  f   is called the lower prevision of f   and it follows from equation    
that it can be interpreted as the subjects supremum desirable price for buying the gamble f  
for any event b  p  ib   is also denoted by p  b   and called the lower probability of b  it
can be interpreted as the subjects supremum desirable rate for betting on b  similarly for
upper previsions and upper probabilities 
the lower prevision associated with the vacuous set of desirable gambles l    a  is given
by p  f     min f   it is called the vacuous lower prevision  and it is the point wise smallest 
or most conservative  of all coherent lower previsions 
the coherent conditional model dcb  with b a non empty subset of a  induces a conditional lower prevision p   b  on l b   by invoking equation     
p  g b     sup    r   g    dcb    sup    r    g   ib  d 
for all gambles g on b     
it is not difficult to show  walley        that p and p   b  are related through the following
coherence condition 
p   g  p  g b  ib       for all g  l b  
 gbr 
called the generalised bayes rule  this rule allows us to infer p   b  uniquely from p  
provided that p  b       otherwise  there is usually an infinity of coherent lower previsions
p   b  that are coherent with p in the sense that they satisfy  gbr   or equivalently  that
there is some coherent set of desirable gambles d that leads to both p and p   b   two
   this statement is valid because we are working with finite a  for infinite a  similar results can be shown
to hold  walley        de cooman   quaeghebeur        miranda   de cooman        troffaes  
de cooman         and then the expectations involved are coherent previsionsexpectation operators
associated with finitely additive probability measures  see also the discussion in section     

  

fide cooman  de bock    diniz

particular conditioning rules  namely natural and regular extension  walley        miranda
  de cooman         always produce conditional lower previsions that satisfy gbr  and
are therefore coherent with p   when p  b     but not necessarily when p  b      they
always produce the point wise smallest and largest coherent conditional lower previsions 
respectively  miranda        miranda   de cooman         
many different coherent sets of desirable gambles lead to the same coherent lower prevision
p   and they typically differ only in their boundaries  in this sense  coherent sets of desirable
gambles are more informative than coherent lower previsions  a gamble with positive lower
prevision is always desirable and one with a negative lower prevision never  but a gamble
with zero lower prevision lies on the border of the set of desirable gambles  and the lower
prevision does not generally provide information about the desirability of such gambles  if
such border behaviour is importantand it is when dealing with conditioning on events with
zero  lower  probability  walley        moral        couso   moral        quaeghebeur 
     it is useful to work with sets of desirable gambles rather than lower previsions  because
as equations     and     tell us  they allow us to derive unique conditional models from
unconditional ones  with a coherent set of desirable gambles d there corresponds a unique
conditional set of desirable gambles dcb and a unique conditional lower prevision p   b   for
any non empty event b  the smallest set of desirable gambles that induces a given coherent
lower prevision  is called the associated set of strictly desirable gambles  walley        and is
given by  f  l a    f     or p  f         see the papers by walley        and quaeghebeur
       for additional discussion about why sets of desirable gambles are more informative
than coherent lower previsions 
    linear previsions and credal sets
when the coherent lower and the upper prevision coincide on all gambles  then the real
functional p defined on l a  by p  f      p  f     p  f   for all f  l a  is a coherent prevision 
since we assumed that a is finite    this means that it corresponds
to the expectation

operator associated with a probability mass function p  p  f     xa f  x p x     ep  f   for
all f  l a   where p x     p  i x    for all x  a  this happens in particular if the lower
and upper previsions are induced by a maximal coherent set of desirable gambles  indeed  up
to boundary behaviour  the so called precise probability models p correspond to maximal
coherent sets of desirable gambles  see the discussions by williams      a   miranda and
zaffalon        proposition    and couso and moral        section    for more information 
for coherent previsions p   the generalised bayes rule  gbr  reduces to bayess rule 
p  gib     p  b p  g b  for all g  l b  

 br 

indicating that this central probabilistic updating rule is a special case of equation     
   the conditional lower previsions in section    on the idmm are produced by regular extension  the
models in sections        and    have the same lower previsions amongst them  but in nearly all cases have
very different conditional lower previsions  even though in these cases the natural and regular extensions
coincidethey are vacuous there 
    as already hinted at in footnote    similar things can still be said for infinite a  but this would unduly
complicate the discussion  for more details  see the work by walley         troffaes and de cooman
       and miranda and de cooman        

  

ficoherent predictive inference under exchangeability

because we assumed that a is finite  we can define the so called credal set m p   associated
with a coherent lower prevision p as 
m p       p  a    f  l a  ep  f    p  f     
which is a closed and convex subset of the so called simplex a of all probability mass
functions on a    then p is the lower envelope of m p    p  f     min  ep  f     p  m p   
for all f  l a   walley        miranda   de cooman        troffaes   de cooman 
       in this sense  such convex closed sets of precise probability models can also be seen
as imprecise probability models  and they are mathematically equivalent to coherent lower
previsions  they are therefore also less general and powerful than coherent sets of desirable
gambles  and also suffer from problems with conditioning on events with  lower  probability
zero   

   predictive inference
predictive inference  in the specific sense we are focussing on here  considers a number of
variables x            xn assuming values in the same category set awe define a category set
as any non empty finite set    in what follows  we shall have occasion to use many different
category sets  and we shall use italic capitals such as a  b  c  d        to refer to them 
we start our discussion of predictive inference models in the most general and representationally powerful language  coherent sets of desirable gambles  as introduced in the previous
section  further on  we shall also pay some attention to more specific derived models  such
as predictive lower previsions  and predictive lower probabilities 
predictive inference assumes generally that a number n of observations have been made 
    x            xn   of the first n variables x            xn   based on this
so we know the values 
n c
 for the values
 a subject then has a posterior predictive model da
observation sample  
n
n
 is a coherent set of
that the next n variables xn             xn n assume in a   this da c
desirable gambles f  xn             xn n   on an   here we assume that n  n  on the other
hand  we want to allow that n  n     n       which is the set of all natural numbers with
zero  we also want to be able to deal with the case where no previous observations have been
n a prior predictive model    of course 
made  in that case  we call the corresponding model da
technically speaking  n   n  n 
as we said  the subject may also have a prior  unconditional model  for when no obn of
servations have yet been made  in its most general form  this will be a coherent set da
    see section     for an explicit definition of a  
    using sets of full conditional measures  dubins        cozman         rather than sets of probability
mass functions  leads to an imprecise probability model that is related to sets of desirable gambles  couso
  moral         and has no problems with conditioning on sets of lower probability zero either  but we
feel it is less elegant and mathematically more complicated 
    for formal reasons  we include the trivial case of category sets with a single element  in which case we are
certain about the value that the variables assume 
    so the terms posterior and prior in association with predictive models indicate whether or not previous
observations have been made  but  in order to avoid the well known issues with temporal coherence
 zaffalon   miranda         we are assuming here that the prior and posterior models are based on a
subjects beliefs before any observations have been made  so the posterior models refer to hypothetical
future situations 

  

fide cooman  de bock    diniz

n
desirable gambles f  x            xn   on an   for some n  n  he may also have coherent sets da
n
of desirable gambles f  x            xn   on a   where n can be any natural number such that
n and d n must then be related to each other through the following
n  n  and the sets da
a
marginalisation  or time consistency  requirement   
n
n
f  x            xn    da
 f  x            xn    da
for all gambles f on an  

   

in this expression  and throughout this paper  we identify a gamble f on an with its cylindrical
extension f   on an   defined by f    x            xn           xn      f  x            xn   for all  x            xn    an  
if we introduce the marginalisation operator margn         l an    then the time consistency
n   marg  d n     d n  l an   
condition can also be rewritten simply as da
n
a
a
n and posterior  conditional  ones d n c
prior  unconditional  predictive models da
a  must
also be related through the following updating requirement 
n
n
  f  xn             xn n  i  
f  xn             xn n    da
c
  x            xn    da

for all gambles f on an      
which is a special case of equation      the gamble f  xn             xn n   is desirable after observ if and only if the gamble f  xn             xn n  i  
ing the sample 
  x            xn   is desirable
before any observations are made  this called off gamble f  xn             xn n  i  
  x            xn  

is the gamble that gives zero rewardis called offunless the first n observations are  
and in that case reduces to the gamble f  xn             xn n   on the remaining variables
xn             xn n   the updating requirement is a generalisation of bayess rule for updating 
and in fact reduces to it when the sets of desirable gambles lead to  precise  probability
mass functions  as described in section     and proved in detail by walley        and also
by de cooman and miranda         but contrary to bayess rule for probability mass
functions  the updating rule     for coherent sets of desirable gambles clearly does not suffer
from problems when the conditioning event has  lower  probability zero  it allows us to infer
a unique conditional model from an unconditional one  regardless of the  lower or upper 
probability of the conditioning event  we refer to the work of de cooman and miranda
       for detailed discussions of marginalisation and updating of sets of desirable gambles in
a many variable context 
as explained in section      we can use the relationship     to derive prior  unconditional 
n through 
predictive lower previsions p na    on l an   from the prior set da
n
p na  f      sup    r   f    da
  for all gambles f on an and all    n  n 

 on l an   from the posterior
and posterior  conditional  predictive lower previsions p na    
n
 through 
sets da c
 
 
n
    sup   r   f    da
 for all gambles f on an  
p na  f   
c
    see also the related discussion of this notion by de cooman and miranda      b  and de cooman and
quaeghebeur         it should not be confused with the temporal consistency discussed by goldstein
             and zaffalon and miranda        

  

ficoherent predictive inference under exchangeability

further on  we shall also want to condition predictive lower previsions on the additional
information that  xn             xn n    b n   for some proper subset b of a  using the ideas in
sections     and      this leads for instance to the following lower prevision 
 
 
n
 b n      sup   r    g   ib n  da
 for all gambles g on b n  
p na  g  
c
   
 conditioned on the event b n  
which is the lower prevision p na    

   principles for predictive inference
so far  we have introduced coherence  marginalisation and updating as basic rationality
requirements that prior and posterior predictive inference models must satisfy  but it could
be envisaged that other requirementsother inference principlescan be imposed on our
inference models  because we want to show further on how to deal with such additional
requirements in a theory for conservative predictive inference  we now discuss  by way of
examples  a number of additional conditions  which have been suggested by a number of
authors as reasonable properties ofor requirements forpredictive inference models  we
want to stress here that by considering these requirements as examples  we do not want to
defend using them in all circumstances  or mean to suggest that they are always reasonable or
useful  they are what they are  inference principles that we might want to impose  and whose
implications for conservative predictive inference we might therefore want to investigate 
    pooling invariance
we first consider walleys        notion of representation invariance  which we prefer to call
pooling invariance  consider any set of categories a  and a partition b of a with non empty
partition classes  we can of course consider the partition b as a set of categories as well 
therefore  in order to streamline the discussion and notation  we shall henceforth denote it
by bas stated before  we want to use italic capitals for category sets  each of its elements
some subset c of acorresponds to a single new category  which consists of the original
categories x  c being pooledconsidered as one  denote by  x  the unique element of the
partition b that an original category x  a belongs to  this leads us to consider a surjective
 onto  map  from a to b 
we say that a gamble g on an does not differentiate between pooled categories when 
g     g   for all     an such that  k              n   xk      yk   
which means that there is some gamble f on b n such that 
   an  g     f   x              xn    
the idea underlying this formulaor requirementis that with a sample     x            xn   
an   there corresponds a sample       x              xn     b n of pooled categories  pooling
invariance requires that for gambles g   f   that do not differentiate between pooled
categories  it should make no difference whether we make predictive inferences using the set
of original categories a  or using the set of pooled categories b  more formally  in terms of
predictive lower previsions 

  

fide cooman  de bock    diniz

   p nb  f   

p na  f      p nb  f   and p na  f    
  an  
for all n  n  n considered  all gambles f on b n and all 
or alternatively  and more generally  in terms of predictive sets of desirable gambles 
n
n
n
n
  f  db

f    da
 f  db
and f    da
c
c

  an  
for all n  n  n considered  all gambles f on b n and all 
pooling invariance seems a reasonable principle to uphold in cases where the category
set is not known in full detail  in that case it is useful to start from a limited set of broadly
defined categories  and allow the creation of new ones  by pooling or splitting old categories
as the observations proceed  in this context  recall walleys        example  if we have a
closed bag containing coloured marbles  what is the probability of drawing a red marble
from it  with no further information  our subject has no idea about the colours of the
marbles in the bag  making it difficult to construct a suitable detailed category set for
such an experiment  after a few draws from the bag  if the predictive inference model used
respects pooling invariance  the inferences that are made about red marbles when he uses
the category set  red  yellow  blue  other  should be the same as those using the category
set  red  non red   where all the colours different from red are pooled together into a single
category  it appears that pooling invariance is a typically useful principle  for instance  in
sampling species problems  when one wants to assess the prevalence of a given species in
certain area 
there is a special case of pooling invariance  called embedding invariance    which concentrates on the case without prior observations  in terms of lower previsions 
p na  f      p nb  f   for all n  n considered  and all gambles f on b n  
or alternatively  and more generally  in terms of sets of desirable gambles 
n
n
f    da
 f  db
for all n  n considered  and all gambles f on b n  

    renaming invariance
besides pooling invariance  we may also require renaming invariance  as long as no confusion
can arise  it should not matter for a subjects predictive inferences what names  or labels  he
gives to the different categories 
this may seem too trivial to even mention  and as far as we know  it is always implicitly
taken for granted in predictive inference  but it will be well to devote some attention to it
here  in order to distinguish it from the category permutation invariance to be discussed
shortly  with which it is easily confused if we do not pay proper attention  if we have a
renaming bijection  a one to one and onto map   between a set of original categories a
and a set of renamed categories c  where we clearly distinguish between the elements of
a and those of c  then with a sample     x            xn    an of original categories  there
corresponds a sample of renamed categories       x              xn     and with a gamble
    walley calls the underlying requirement that the  lower  probability of an event a should not depend on
the possibility space into which a is embedded  the embedding principle  walley        section        

  

ficoherent predictive inference under exchangeability

f on the set c n of renamed samples  there corresponds a gamble f   on the set an of
original samples  clearly  we can then require that it should make no difference whether we
make predictive inferences using the set of original categories a  or using the set of renamed
categories c  more formally  in terms of predictive lower previsions 
   p nc  f   

p na  f      p nc  f   and p na  f    
  an  
for all n  n  n considered  all gambles f on c n and all 
or alternatively  and more generally  in terms of predictive sets of desirable gambles 
n
n
n
n
  f  dc

f    da
 f  dc
and f    da
c
c

  an  
for all n  n  n considered  all gambles f on c n and all 
    category permutation invariance
we shall be especially interested in predictive inference where a subject starts from a state of
prior ignorance  in such a state  he has no reason to distinguish between the different elements
of any set of categories a he has chosen  to formalise this idea  consider a permutation
  of the elements of a    with a sample  in an   there corresponds a permuted sample
        x               xn     and with any gamble f on an   there corresponds a permuted
gamble f    on an   if a subject has no reason to distinguish between categories z and their
images   z   it make sense to require the following category permutation invariance   
   p na  f    

p na  f       p na  f   and p na  f     
  an  
for all n  n  n considered  all gambles f on an and all 
or alternatively  and more generally  in terms of predictive sets of desirable gambles 
n
n
n
n
  f  da

f     da
 f  da
and f     da
c
c 

  an  
for all n  n  n considered  all gambles f on an and all 
formally  this requirement closely resembles renaming invariance  but whereas the latter is
a trivial requirement  category permutation invariance is a symmetry requirement between
categories that can only be justified when our subject has no reason to distinguish between
them  which may for instance be justified when he starts out from a state of prior ignorance 
to draw attention to the difference between the two in a somewhat loose manner  category
permutation invariance allows for confusion between new and old categories  something which
renaming invariance carefully avoids 
to see why such a principle could be reasonable  recall walleys        bag of marbles
example  introduced above when discussing pooling invariance  since  before having drawn any
    this permutation   of the elements of a  or in other words of the categories  should be contrasted with
permutations  of the order of the observations  i e  of the time set             n   considered in our discussion
of exchangeability  further on in section   
    this requirement is related to the notion of  weak  permutation invariance that de cooman and miranda
       have studied in much detail in a paper dealing with symmetry in uncertainty modelling  it goes
back to walleys        section        symmetry principle 

  

fide cooman  de bock    diniz

marbles from the bag  our subject has no idea how the marbles are coloured  he is in a state
of complete prior ignorance  therefore  if he starts out with the sample space  red  non red  
and observes the outcomes of a few draws  say twice non red  he can consider the probability
of obtaining a red marble on the next draw  but due to the symmetry originating in complete
ignorance  if he were to permute the categories  calling the red marbles non red and the
non red ones red  the situation he is now looking at is completely the same as before  and
therefore his probability of obtaining a non red marble on the next draw after observing
twice red  must be the same as that for observing a red one  after observing non red twice 
this principle is reminiscent of the axiom a  proposed by carnap        for his system of
inductive logic  of course  this is not a reasonable principle when our subject has some prior
knowledge about the problem that would  for instance  allow him to impose an ordering on
the categories 
    representation insensitivity
we shall call representation insensitivity the combination of pooling  renaming and category
permutation invariance  it means that predictive inferences remain essentially unchanged
when we transform the set of categories  or in other words that they are insensitive to
the choice of representationthe category set  it is not difficult to see that representation
insensitivity can be formally characterised as follows  consider two category sets a and
d such that there is a so called relabelling map    a  d that is onto  i e  such that
d    a       x    x  a   then with a sample  in an   there corresponds a transformed
sample       x              xn    in dn   and with any gamble f on dn there corresponds a
gamble f   on an  
      representation insensitivity
for all category sets a and d such that there is an onto map    a  d  all n  n  n
  an and all gambles f on dn  
considered  all 
   p nd  f    

p na  f      p nd  f   and p na  f    

 ri  

or alternatively  and more generally  in terms of predictive sets of desirable gambles 
n
n
n
n
  f  dd

f    da
 f  dd
and f    da
c
c 

 ri  

there is also the weaker combination of pooling  renaming and category permutation
invariance for models with no prior observations 
      prior representation insensitivity
for all category sets a and d such that there is an onto map    a  d  all n  n considered
and all gambles f on dn  
p na  f      p nd  f   
 ei  
or alternatively  and more generally  in terms of sets of desirable gambles 
n
n
f    da
 f  dd
 

  

 ei  

ficoherent predictive inference under exchangeability

    specificity
we now turn to another  rather peculiar but in our view intuitively appealing  potential property of predictive inferences  assume that in addition to observing a sample of observations
 of n observations in a category set a  our subject comes to know or determine in some

way that the n following observations will belong to a proper subset b of a  and nothing
elsewe might suppose for instance that an observation of  xn             xn n   has been made 
but that it is imperfect  and only allows him to conclude that  xn             xn n    b n  
we can then impose the following requirement  which uses models conditioned on the
event b n   such conditional models have been introduced through equations     and      see
also the discussion leading to equation      near the end of section   
      specificity
  an and all
for all category sets a and b such that b  a  all n  n  n considered  all 
gambles f on b n  
 b n     p nb  f  
 b   
p na  f  b n     p nb  f   and p na  f   

 sp  

or alternatively  and more generally  in terms of predictive sets of desirable gambles 
n
n
n
n
  f  db
 b 
f ib n  da
 f  db
and f ib n  da
c
c

 sp  

 b is the tuple of observations obtained by eliminating from the tuple 
 all observawhere 
 b is the empty tuple  so when no observations in
tions not in b  in these expressions  when 
 are in b  the posterior predictive model is simply taken to reduce to the prior predictive

model 
specificity means that the predictive inferences that a subject makes are the same as the
ones he would get by focussing on the category set b  and at the same time discarding all the
previous observations producing values outside b  in effect only retaining the observations
that were inside b  it is as if knowing that the future observations belong to b allows our
subject to ignore all the previous observations that happened to lie outside b  the term
specificity in this context seems to have been proposed by bernard               based on
work by rouanet and lecoutre         in a so called specific inference approach  for questions 
inferences and decisions involving only a restricted number of categories  a more general
model can be replaced by a more specific model that deals only with the categories of interest 
and if specificity is respected  the general and the specific models will produce the same
inferences  specificity seems to be a relevant principle when analysing categorical data that
can be described by tree structures  as in the case of  for instance  patients that are classified
according to symptoms  bernard        
to give a very simple example involving  once again  walleys bag of marbles  our subject
may have observed  after some drawings  green  red  blue and white marbles  he is asked for
his probability of drawing a red marble next  but some other observer has already seen what
it is  and informs us that it is either green or redperhaps due to bad lighting conditions or
because shes colour blind  if the subject uses a specific inference model  he can disregard
the previous observations involving other colours than green and red 
  

fide cooman  de bock    diniz

    prior near ignorance
we use the notion of near ignorance as defined by walley        p       to give the following
definition of prior near ignorance in our context of predictive inference  see also the related
discussions by walley        section         walley        section    and walley and bernard
       section       we also refer to the paper by piatti  zaffalon  trojani  and hutter       
for an interesting discussion of why prior near ignorance may produce undesirable results in
certain contexts 
      prior near ignorance
the prior model for any single variable xk assuming values in some arbitrary category set a
is vacuous  so for any category set a  any n  n considered  any    k  n and all gambles
f on a 
p na  extnk  f      min f 
or alternatively  and more generally  in terms of sets of desirable gambles 
n
extnk  f    da
 f     

where extnk  f   denotes the cylindrical extension of f to a gamble on an   it is defined by
extnk  f   x            xn      f  xk   for all  x            xn    an   a perhaps more intuitive  if less
formally correct  notation for this gamble is f  xk   
theorem    prior representation insensitivity implies prior near ignorance 
this simple result implies that no model all of whose predictive previsions are precise can
be prior representation insensitive  let alone representation insensitive  as its prior model
for immediate predictions should then be vacuous  we shall see in section    that it is
nevertheless possible for representation insensitive coherent inferences to deploy precise
posterior predictive previsions 

   adding exchangeability to the picture
we are now  for the remainder of this paper  going to add two additional assumptions 
the first assumption is that there is  in principle  no upper bound on the number of
variables that we can take into account  in other words  when we are considering n variables
x            xn   we can always envisage looking at one more variable xn     this effectively
means that we are dealing with a countably infinite sequence of variables x            xn        
that assume values in the same category set a 
n of coherent
for our predictive inference models  this means that there is a sequence da
sets of desirable gambles on an   n  n  this sequence should of course be time consistent in
the sense of requirement      meaning that
n 
n 
n 
 n    n   n  n   n   da
  margn   da
    da
 l an     

the second assumption is that this sequence of variables is exchangeable  which means 
roughly speaking  that the subject believes that the order in which these variables are observed 
  

ficoherent predictive inference under exchangeability

or present themselves  has no influence on the decisions and inferences he will make regarding
them   
in this section  we explain succinctly how to deal with these assumptions technically  and
what their consequences are for the predictive models we are interested in  for a detailed
discussion and derivation of the results presented here  we refer to the papers by de cooman
et al       b  and de cooman and quaeghebeur        
we begin with some useful notation  which will be employed numerous times in what
follows  consider any element   ra   we consider  as an a tuple  with as many  real 
components
 x  r as there are categories x in a  for any subset b  a  we then denote by
b    xb x the sum of its components over b 
    permutations  count vectors and the hypergeometric distribution
consider an arbitrary n  n  we denote by     x            xn   a generic  arbitrary element of an  
p n is the set of all permutations  of the index set             n   with any such permutation  
we can associate a permutation of an   also denoted by   and defined by   k    x k    or
in other words   x            xn       x              x n     similarly  we lift  to a permutation  t of
l an   by letting  t f    f    so   t f        f    
the permutation invariant atoms             p n      an are the smallest permutation invariant subsets of an   we introduce the counting map    an  nan          
where the count vector     is the a tuple with components
tz         k              n    xk   z   for all z  a 
and the set of possible count vectors for n observations in a is given by
 
 
nan      na
    ma   n  

   

   

so tz    is the number of times the category z appears in the sample   if         then
        an            so the atom    is completely determined by the single count
vector  of all its elements  and is therefore also denoted by    
we also consider the linear expectation operator hyna     associated with the uniform
distribution on the invariant atom    
hyna  f      


 
f    for all gambles f on an  
    

    

  

where the number of elements            in the invariant atom    is given by the
multinomial coefficient 
 
     
ma
n
n 
   
    
 
 
    


za mz  
this expectation operator in equation      characterisesor is the one associated with
a  multivariate  hyper geometric distribution  johnson  kotz    balakrishnan        section        associated with random sampling without replacement from an urn with n balls
    exchangeability was also assumed by carnaphis axiom a and johnson         who named it the
permutation postulate 

  

fide cooman  de bock    diniz

of types z  a  whose composition is characterised by the count vector   this is borne out
 
by the fact that  for any   an   with    n   n and         
 
          if    
hyna  i       
 
otherwise
is the probability of randomly selecting  without replacement  a sequence of n  balls of types
 from an urn with n balls whose composition is determined by the count vector   see
also the running example below for a more concrete illustration 
this hyper geometric expectation operator can also be seen as a linear transformation
hyna between the linear space l an   and the generally much lower dimensional linear space
l nan    turning a gamble f on an into a so called count gamble hyna  f      hyna  f    on
count vectors 
running example  in order to make our argumentation  and the notions we introduce and
discuss  more tangible and concrete  we shall use a very simple running example  to which
we shall come back repeatedly in a number of sections  the notations and assumptions made
here will be maintained throughout the series 
consider a  potentially infinite  sequence of coin flips  whose successive outcomes we
denote by the variables x    x          xn         assuming values in the category set  h   t    to
make this somewhat more interesting than the usual run of the mill example  assume that at
each stepfor each coin flipnathalie selects a coin from a bag of three coins  and hands it
to arthur  who then proceeds to flip it  the coin is then put back into the bag for the next
step  the subject whose beliefs we are modelling  may or may not know something about
the nature of the coins  or about how nathalie is choosing the coins for the subsequent flips 
she might choose them completely at random  or she might have a specific deterministic
mechanism for selecting them  or      
    h   t   h   h   of the first n     observed coin flips  the count
consider the sequence 
 that corresponds to this sequence is given by its components
vector    
th   h   t   h   h        and tt   h   t   h   h        
           letting the first component always refer to h   from now
and we will denote it by 
on  the corresponding permutation invariant atom is
  h   t   h   h                   t   h   h   h     h   t   h   h     h   h   t   h     h   h   h   t   
  
    

 
    elements  the set of possible count vectors is given by n h
 t    
      h   t     t   h      h   t    of
                                          consider the event ht
two different outcomes for the first two observations  then

and it has           

 
 
hy  h  t    iht
                             
 
 
is the probability of observing two different outcomes in two random draws without replacement from an urn containing three balls marked h and one ball marked t   and whose
composition is therefore determined by the count vector        

  

ficoherent predictive inference under exchangeability

    the multinomial distribution
next  we consider the simplex a of all probability mass functions  on a 

 
 
a      ra       and a       where  as before  a   
x  

    

xa

with a probability mass function   a on a  there corresponds the following multinomial
expectation operator mnna       
mnna  f      


an

f   



ztz    for all gambles f on an  

    

za

which characterises the multinomial distribution  associated with n independent trials of an
experiment with possible outcomes in a and probability mass function   observe that
 


    
f       
zmz
mnna  f     
  
n
za
na
  


n
mz
 
hya  f     
z   comnna  hyna  f     
n
na

za

where we used the so called count multinomial expectation operator   


comnna  g     
g    
zmz for all gambles g on nan  
n
na

    

za

running example  consider n     independent trials of an experiment with possible outcomes
in the category set  h   t   and probability mass function     h   t    then
 
   
 
 
mn  h  t    iht
   h   t       h t    h t    h t    h t  h   t      h t  

   observe  by the way  that mnn
gives the probability of the event ht
   h   t     
 h  t    iht
 h t for all n    
with the gamble fht
    iht
 on observation sequences  x            x     there corresponds
 
a count gamble ght
    hy h  t    fht
    given by 
ght
            and ght
         

 
 
 
and ght
and ght
and ght
         
         
            
 
 
 

and

 
     
   
 
comn  h  t    g  h   t       h t
   h
t    h
t    h t
 
 
 
leads to the same polynomial as before  as it should 



    to avoid confusion  we make a  perhaps non standard  distinction between the multinomial expectation 
which is associated with sequences of observations  and the count multinomial expectation  associated
with their count vectors 
    see footnote    

  

fide cooman  de bock    diniz

    multivariate polynomials

let us introduce the notation na    mn nam for the set of all possible count vectors
corresponding to samples of at least one observation  in equation      we can also let n     
which turns na  into the singleton
containing only the null count vector    all of whose

components are zero  then mn  nam   na      is the set of all possible count vectors 
for any such count vector   na       we consider the  multivariate  bernstein basis
polynomial ba  of degree ma on a   defined by 
 
 

ma  mz
mz
ba          
z  
z for all   a  
    

za

za

in particular  of course  ba       
any linear combination p of bernstein basis polynomials of degree n    is a  multivariate 
polynomial on a   whose degree deg p  is at most n    we denote the linear space of all these
polynomials of degree up to n by v n  a   of course  polynomials of degree zero are simply real
constants  we have gathered relevant and useful information about multivariate polynomials
in appendix b  it follows from the discussion there that  for any n     we can introduce a
linear isomorphism comnna between the linear spaces l nan   and v n  a  
 with any gamble g
on nan   there corresponds a polynomial comnna  g     comnna  g     n n g  ba  in
a
v n  a   and conversely  for any polynomial p  v n  a  there is a unique gamble bnp on nan
such that p   comnna  bnp      observe that in particular  for any n    and   nan  
comnna         ba     for all   a  
    

we denote by v  a     nn  v n  a  the linear space of all  multivariate  polynomials on a  
of arbitrary degree 
a set ha  v  a  of polynomials on a is called bernstein coherent if it satisfies the
following properties 
b     
  ha  
b   v    a   ha  
b   posi ha     ha  
here  v    a  is the set of bernstein positive polynomials on a   those polynomials p for which
there is some n  deg p  such that bnp      it follows from proposition    in appendix b
that v    a  is a subset of the set v     a  of all polynomials p such that p       for all 
in the interior int a         a    x  a x      of a   as a consequence of b b   we
find for the set v   a     v    a  of bernstein negative polynomials that 
b   v   a   ha    
    the degree may be smaller than n because the sum of all bernstein basis polynomials of fixed degree is
one  strictly speaking  these polynomials p are restrictions to a of multivariate polynomials q on ra  
called representations of p  for any p  there are multiple representations  with possibly different degrees 
the smallest such degree is then called the degree deg p  of p 
    strictly speaking  equation      only defines the count multinomial expectation operator comnn
a for
n      but it is clear that the definition extends trivially to the case n     

  

ficoherent predictive inference under exchangeability

finally  every bernstein coherent set ha of polynomials on a induces a lower prevision
h a on v  a  defined by 
h a  p     sup    r   p    ha   for all p  v  a  

    

this lower prevision is coherent  in the mathematical sense that it satisfies the coherence
requirements p p    
    exchangeability and the representation theorem
we are now ready to deal with exchangeability  we shall give a definition for coherent sets of
desirable gambles that generalises de finettis              definition  and which allows for a
significant generalisation of his representation theorem 
first of all  fix n  n  then the subject considers the variables x            xn to be
exchangeable when he does not distinguish between any gamble f on an and its permuted
version  t f   or in other words  if the gamble f   t f is equivalent to the zero gamble foror
indifferent tohim  this means that he has a so called set of indifferent gambles 
 
 
n
   f   t f   f  l an   and   p n  
ia
n   then this set must be compatible
if the subject also has a coherent set of desirable gambles da
n   in the sense that it must satisfy the rationality
with the set of indifferent gambles ia
n
n
n
requirement da   ia   da   see the detailed explanations and justifications by de cooman
and quaeghebeur        and quaeghebeur et al         of this so called desiring sweetened
n   are
deals requirement  we then say that the sequence x            xn   and the model da
exchangeable 
next  the countably infinite sequence of variables x            xn       is called exchangeable if
n  n  n
all the finite subsequences x            xn are  for n  n  this means that all models da
are exchangeable  they should of course also be time consistent 
we can now formulate a powerful generalisation of de finettis              representation
theorem  which is a straightforward compilation of various results proved by de cooman
and quaeghebeur        

theorem    representation theorem  de cooman   quaeghebeur         the sequence
n of desirable gambles on an   n  n is coherent  time consistent and exchangeable
of sets da
if and only if there is a bernstein coherent set ha of polynomials on a such that for all
  na and all 
     

n  n  all gambles f on an   all 
n
n
  mnna  f  ba 
f  da
 mnna  f    ha and f  da
c
  ha  

in that case this representation ha is unique and given by ha   



    

n
n
nn mna  da   

it follows from condition      that ha completely determines all predictive inferences about
n and all
the sequence of variables x            xn           as it fixes all prior predictive models da
    actually  a suitably adapted version  where the underlying possibility space need no longer be finite
 walley        troffaes   de cooman         and where the domain is restricted to the polynomials on
a  de cooman   quaeghebeur        

  

fide cooman  de bock    diniz

n c 
    this tells us that the representation ha is a set of
posterior predictive models da
polynomials that plays the same role as a probability measure  or density  or distribution
function  on a in the precise probabilistic case 
indeed  the corresponding coherent lower prevision h a on v  a  is given by equation      
and it can be shown to determine a convex closed  compact  set

m h a       ha    p  v  a  ha  p   h a  p  
of coherent previsions ha on v  a   walley        de cooman et al       b  de cooman  
quaeghebeur        troffaes   de cooman         as we pointed out in footnote  and
will come back to further on in footnote   each such coherent prevision ha uniquely
determines a  additive probability measure on the borel sets of a   and therefore the set of
polynomials ha   via m h a    uniquely determines a set of such probability measures  but  as
we have argued before  ha is more informative than h a and m h a    and has no problems
with conditioning on sets of lower probability zero  a bernstein coherent set of polynomials
ha determines a unique lower prevision h a   and therefore through m h a   a unique set of
probability measuresand densities if they are absolutely continuouson the simplex a  
but the converse is not necessarilyand usually notthe case  a set of probability densities
can be used to define a coherent set of polynomialswe provide an example of how to do
this in section   but there will generally be more than one coherent set of polynomials
that leads to this same set of densities  and the updating behaviour for these different sets of
polynomials can be different on conditioning events of lower probability zero 
n c
 only depend on
condition      also tells us that the posterior predictive models da
 through the count vector 
       
 count vectors are sufficient
the observed sequence 
statistics under exchangeability  for this reason  we shall from now on denote these posterior
n c
n c 
 as well as by da
 also  every now and then  we shall use
predictive models by da
n
n
da c  as an alternative notation for da  
an immediate but interesting consequence of theorem   is that updating on observations
preserves exchangeability  after observing the values of the first n variables  with count
 the remaining sequence of variables xn     xn           is still exchangeable  and
vector  
condition      tells us that its representation is given by the bernstein coherent set of
 defined by 
polynomials ha c
     p  v  a    ba 
ha c
 p  ha    

    

if we compare this with expressions     and      this tells us that  essentially  bernstein
basis polynomials serve as likelihood functions for updating sets of polynomials  we use
 to refer to the coherent lower prevision on v  a  derived from ha c
 by means of
h a    
      we find that ha c    ha and h a        h a  
equation       for the special case 
 are related through the following version of the generalised
observe that h a and h a    
bayes rule 

h a   p  h a  p   b
    
       for all p  v  a  
a 
 is completely determined by ha   one can consider ha as a prior model on
clearly  ha c
 plays the role of the posterior that is derived from it 
the parameter space a   and ha c
    this should be contrasted with the usual precise probabilistic version  where the posterior predictive
models are only uniquely determined if the observed sequences has non zero probability  see also footnote   

  

ficoherent predictive inference under exchangeability

we see from condition      and equation      thatsimilarly to what happens in a preciseprobabilistic settingthe multinomial distribution serves as a direct link between on the one
n and  on the other hand 
hand  the prior ha and its prior predictive inference models da
n c 
 and its posterior predictive inference models da
 recalling our
the posterior ha c

  na      
convention for       we can summarise this as follows  for all n  n and all 
 
 
n
   f  l an     mnna  f    ha c

da
c
    
and  as an immediate consequence 
 
 
   sup   r   mnna  f      ha c
 for all f  l an  
p na  f   

    

or  equivalently 
   h a  mnna  f    
 for all f  l an   
p na  f   

    

from a practical point of view  equation      will often be easier to work with than equa will often admit a simpler expression
tion       because as we shall see further on  h a    
 compare equations                  and      with equations                 
than ha c 
 is not always uniquely determined by h a   the relaand       respectively  but  h a    
 uniquely from h a if the prior lower probability
tion      only allows us to determine h a    

h a  ba 
 
of
observing
is
non zero 
therefore 
the sets of polynomials ha are the more


 uniquely  as a quite dramatic
fundamental models  as they allow us to determine the ha c
illustration of this  we shall further on in sections        and    come across a number of
quite different inference systemswith different ha that give rise to the same prior h a

but different posterior h a     
running example  we now assume that our subject assesses the sequence of coin flips to be
exchangeable  and that he finds desirable any gamble of the type   i h    xn    for some fixed
          so his upper probability for observing heads on any coin flip is at most   since
we infer from equation      that for any n  n  mnn
 h  t    i h    xn       h   we infer from
theorem   that this assessment corresponds to the following coherent set of polynomials 
 
 
h      p         h     p   v     h   t            r  and max              
which is the smallest bernstein coherent set of polynomials that contains the polynomial
  h   for more explanation  see also the discussions by de cooman et al       b  and
de cooman and quaeghebeur         it then followsafter some manipulationsfrom
equation      and proposition    that the corresponding lower prevision on v   h   t    is
completely determined by the following optimisation 
h   p    sup

min  p      h    

   h  t  

 is given by
hence  the lower probability of the event ht
h    h t     sup min   x    x     x         
  x     

and its upper probability by
h    h t     h    h t     inf max   x    x    x    
  x     

  

fide cooman  de bock    diniz

 

 
      
 
 

if     
otherwise 


this tells us that exchangeability alone already guarantees that the upper probability of ht
 
is at most     if all three coins in the bag are assumed to be biased towards heads  so        
this upper probability drops below     

to finish this section on representation  we want to stress that the polynomials on a
should not be given a behavioural interpretation as gambles that may or may not be desirable 
they are merely mathematical and representational tools that help us characterise which
gambles on observation sequences are desirable    similarly  the set of polynomials ha and
the lower prevision h a are merely mathematical tools that allow for a more convenient
representation of predictive models on observation sequences 
running example  to illustrate why the polynomial representation is so much more convenient
and efficient  recall that if we want to make inferences about a sequence of coin flips of
length n  we need to work with sets of desirable gambles on  h   t  n   or in other words 
with cones in a  n  dimensional space  if we work with their polynomial representations 
we are led to consider cones of polynomials of degree up to n  which constitute a linear
space that is spanned by the n     bernstein basis polynomials of degree n  and is therefore
only n     dimensional  working with these polynomial representations therefore leads to a
dramaticexponentialreduction in complexity 


   reasoning about inference systems
we have seen in the previous section that  once we fix a category set a  predictive inferences
about exchangeable sequences assuming values in a are completely determined by a bernstein
coherent set ha of polynomials on a   so if we had some way of associating a bernstein
coherent set ha with every possible set of categories a  this would completely fix all predictive
inferences  this leads us to the following definition 
definition    inference systems   we denote by f the collection of all category sets  i e  finite
non empty sets  an inference system is a map  that maps any category set a  f to some
set of polynomials  a    ha on a   an inference system  is called coherent if for all
category sets a  f   a  is a bernstein coherent set of polynomials on a  
so  a coherent inference system is a way to systematically associate coherent predictive
inferences with any category set  since the inference principles in section   impose connections
between predictive inferences for different category sets  we now see that we can interpret
these inference principlesor rather  represent them mathematicallyas properties of  or
restrictions on  coherent inference systems  this is what we shall do in section    and
it provides one important motivation for our introducing such systems  another  equally
    it makes no operational  behavioural sense to consider the notion of accepting a polynomial  or finding it
desirable  this is very much like the classical case  where for de finetti        the probability distributions
on the simplex a are only to be used as as mathematical representations  but have no direct behavioural
meaningalthough some bayesians less careful about foundations than de finetti might not care to make
this distinction 

  

ficoherent predictive inference under exchangeability

important reason for doing so  is that it allows us to extend the method of natural extension
or conservative inferenceintroduced in section      to also take into account inference
principles for predictive inference  or more generally  predictive inference for multiple category
sets at once 
to see how this comes about  let us show how we can do conservative reasoning with
inference systems  for any two inference systems   and     we say that   is less committal 
or more conservativethan     and we write   v   if
 a  f    a      a  
this simply means that the predictive inferences for each category set a are less committal
for the first than for the second inference system  if we denote by s the set of all inference
systems  then clearly this set is partially ordered by v  actually  it is a complete lattice 
where the infimum and supremum of any non empty family i   i  i are given by 
 

 
 
 


inf i  a   
i  a  and sup i  a   
i  a  for all category sets a 
ii

ii

ii

ii

we denote by c the set of all coherent inference systems 
c       s    a  f  a  is bernstein coherent   

    

then it is clear that c is a complete meet semilattice  meaning that it is closed under arbitrary
non empty infima   
 i  i i  c  inf i  c 
    
ii

the bottom of this structurethe most conservative coherent inference systemis called
the vacuous inference system v   and it is the coherent inference system given by 
v  a    v    a  for all category sets a 
we shall come back in some detail to this vacuous inference system in section   
the property      allows us to do conservative reasoning with coherent inference systems 
suppose  for instance  that for some collection of category sets f  f  we have assessments
a in the form of a set of polynomials aa  v  a   a  f  then  if it exists  the most
conservative coherent inference system a that is compatible with these assessments is given
by 
a   inf    c    a  f aa   a    
and  of course  it will exist if and only the set of polynomials aa is included in some
bernstein coherent set of polynomials ha on a  for all a  f  in that case  it is not difficult
to see  given the discussion in section      that a  a    posi v    a   aa   for a  f and
a  a    v    a  for a  f   f 
    it is not necessarily closed under suprema  however  as the union of bernstein coherent sets of polynomials
need not be bernstein coherent 

  

fide cooman  de bock    diniz

   representation insensitivity and specificity under exchangeability
let us now investigate what form the inference principles of representation insensitivity  ri  
and specificity  sp   take for predictive inference under exchangeability  when such inference
can be completely characterised by bernstein coherent sets of polynomials  this will allow us
to reformulate these principles as constraints onor properties ofinference systems 
    representation insensitivity
we recall the notations and assumptions in section      with the surjective  onto  map
   a  d we associate the surjective map r   ra  rd by letting 
r   z   



x

for all   ra and all z  d 

    

xa    x  z

this map allows us to give the following elegant characterisation of representation insensitivity 
theorem    a coherent inference system  is representation insensitive if and only if for
all category sets a and d such that there is an onto map    a  d  for all p  v  d  and
all   na      
 p  r  ba    a   pbd r      d  
 ri  
running example  assume now that the coins in the bag are actually rather thick  implying
that there is a non negligible chance that they do not fall on one of their flat sides  but
remain upright  if we denote this new up state by u   then we have a new category set
a     h   t   u    if we also consider a new flat state f   meaning either heads or tails  then
we can also consider  instead of a  the category set d     f   u   that does not distinguish
between heads and tails  the relabelling map  with  h       t      f and  u      u
identifies the proper relations between the categories in a and d 
suppose now that we want to say something about the lower probability of the event
 of observing u on one flip and h or t on the other  immediately after observing the
uf
sequence  h   u   h   t   with count vector             the last count in three from now on
refers to the number of u s in the observation sequence  in the a domain  the gamble iuf

n
  
can be expressed by the polynomial q   mn h  t  u    iuf
    n    given by 
q       h   t  u for all    h  t  u    
    belong
so we want to find out whether polynomials of the type    h   t  u     h
t u
to   h   t   u     see equation      
on the other hand  as we have seen previously  in the d domain  the gamble iuf
 can
be expressed by the polynomial p given by p      f u for all    f  u     observe that
q   p  r   the count vector              in the a domain corresponds to a count vector
r             in the d domain  where the first component refers to the number of f s and
the second to the number of u s  so here  we need to check whether polynomials of the type
  f u     f u belong to   f   u    

    as before in similar contexts  it is easy to check that this polynomial remains the same for all n    

  

ficoherent predictive inference under exchangeability

the nice thing about representation insensitivity is that it makes checking whether
   
polynomials of the type    h   t  u     h
t u belong to   h   t   u    in the adomain equivalent to checking whether polynomials of the type   f u     f u belong
to   f   u    in the d domain 

very interestingly  representation insensitivity is preserved under taking arbitrary nonempty infima of coherent inference systems  which allows us to look for the most conservative
representation insensitive coherent inference system that is compatible with an assessment a
on f  in a way that is a straightforward extension of the discussion near the end of section   
theorem    consider any non empty family i   i  i of representation insensitive coherent
inference systems  then their infimum inf ii i is a representation insensitive coherent
inference system as well 
    specificity
next  we turn to specificity  and recall the notations and assumptions in section      let us
define the surjective restriction map rb   ra  rb by 
rb   z    z for all   ra and all z  b 

    

so in particular  rb    is the count vector on b obtained by restricting to b the  indices of
the  components of the count vector  on a  we also define the one to one injection map
ia   rb  ra by 
 
x if x  b
ia   x   
for all   rb and all x  a 
    
 
otherwise
this map can be used to define the following one to one maps irb a   v  b   v  a   for any
r  n    as follows 

irb a  p    
bdeg p  r
  ba ia    for all polynomials p in v  b  
    
p
deg p  r

nb

they derive their meaning from the following observation  a polynomial p on b can be
equivalently represented in any bernstein basis on b of degree deg p    r  but when we
interpret these different representations as polynomials on a   they are no longer equivalent 
and lead to different polynomials irb a  p   r  n    the following propositions clarify what
exactly the effect of the operator irb a is 
proposition    for any polynomial p on b and any r  n    irb a  p   ia   p 
we introduce the following notation  for any   a such that b        
b    rb    b  
observe that   


whenever

 
  
b
b
b
proposition     consider any polynomial p on b   any r  n  and any   a   when
deg p    r     then p   c  r  and irb a  p     c  otherwise  when deg p    r     
 
deg p  r
b
p   
if b    
b 
irb a  p    
 
otherwise 
  

fide cooman  de bock    diniz

the maps irb a allow us to give the following elegant characterisation of specificity 
theorem     a coherent inference system  is specific if and only if for all category sets a
and b such that b  a  for all p  v  b   all   na      and all r  n   
irb a  p ba    a   pbb rb      b  

 sp  

running example  suppose  as before  that we have made the observation  h   u   h   t   
with count vector               we are interested in the posterior lower probability of the
   where somebody has told us that neither of the two subsequent coin flipsafter
event ht
the first fourresulted in u   in a specific inference system  we are allowed to consider this
predictive inference problem in the reduced category space b    h   t    rather than in the
category space a    h   t   u    but then  in the b space  we have to use the reduced count
vector rb              obtained by leaving out the number of observed u s  the polynomials
we are lead to consider here  are therefore of the type   h t     h t   for which we want
to know whether they belong to   f   u    
in the a space  the polynomial p      h t    whose degree is deg p       is
transformed into the polynomials
 
 
t
h
r
ib a  p      
   h   t    r     h t   h   t      h   t  r
h   t h    t
for r  n    it follows from the argumentation in the proof of theorem    that the original
problem requires us to check whether polynomials of the type
 
  h t   h   t      h   t  r   h
t u

are in   h   t   u     specificity allows us to look at the problem in the b space  which is
easier 

observe the close formal similarity between the conditions  ri   and  sp    it should
therefore not surprise us that specificity  too  is preserved under taking arbitrary non empty
infima of inference systems 
theorem     consider any non empty family i   i  i of specific coherent inference systems 
then their infimum inf ii i is a specific coherent inference system as well 
let us denote by crs the set of all coherent inference systems that are both representation
insensitive and specific  it follows from theorems   and    that crs   like c  is closed under
arbitrary non empty infima  so we can perform conservative reasoning  in very much the
same way as we discussed near the end of section   

   immediate prediction
if we have an inference system   we can look at the special case of immediate prediction 
where for a given category set a  after observing a sample of n    variables with count vector
  nan   we want to express beliefs about the value that the next observation xn   will

assume in a  so this is the specific case of predictive inference with n      and condition     
  nan  
can now be simplified somewhat  as for all gambles f on a and all 
 
 
  ba 
f  da
 sa  f     a  and f  da
c
 sa  f     a  

  

ficoherent predictive inference under exchangeability

where we let the
so called sampling expectation sa  f   be the linear polynomial on a given
by sa  f       xa f  x x for all   a  
the reason for this is that na     x   x  a  where x is the count vector corresponding
to a single observation of category x  or in other words  exz   xz for all z  a  kronecker
delta   hence  for all x  a and any   a  
    
    
 
 
x
x
f
 z 
 
f
 x 
and
b
  
 
zez   x  
hy a  f  x    
a 
x
x

x
za

z   

leading to 
mn a  f     



hy a  f  x  ba x     

 
x na



f  x x   sa  f    

    

xa

it is a matter of straightforward verification that  due to the bernstein coherence of ha   the
  c
 is a coherent set of desirable gambles on a  for
so called immediate prediction model da
  na       it induces the following predictive lower previsions 
every count vector 
 
 
 
   sup   r   f    da

p  a  f   
c
  sup    r    sa  f     ba 
   a    

    

immediate prediction in the context of exchangeable imprecise probability models has
been studied in some detail by de cooman et al       a   lower previsions  rather than
sets of desirable gambles  were the model of choice in that paper  and because of that  the
authors encountered problems with conditioning on sets of  lower  probability zero  in fact 
it is these problems that provided the motivation for dealing with the much more general
problem of  not necessarily immediate  predictive inference using sets of desirable gambles
in the present paper  in this section  we want to illustrate how many of the results proved
there can be made stronger  and with easier proofs  as is borne out in appendix e    in the
present context 
the requirement  ri   for representation insensitivity reduces to the following simpler
requirement on immediate prediction models  for all category sets a and d such that there
is an onto map    a  d  for all gambles f on d and all   na      
 
 
f    da
c  f  dd
cr    

 ri  

similarly  the requirement  sp   for specificity reduces to the following simpler requirement
on immediate prediction models  for all category sets a and b such that b  a  for all
gambles f on b and all   na      
 
 
f ib  da
c  f  db
crb    

 sp  

let us now show that there is a simple characterisation of the immediate prediction
models that satisfy representation insensitivity  to get there  observe that we can consider
any gamble g on a category set a as a  surjective  pooling map from a to the finite subset
g a  of ralso a category set  the corresponding rg   ra  rg a  is given by 

rg   r  
x for all r  g a  
xa   g x  r

  

fide cooman  de bock    diniz

this simple idea allows for an intriguing reformulation of the representation insensitivity
requirement on immediate prediction models 
proposition     the immediate prediction models associated with a coherent inference
system are representation insensitive if and only if for all category sets a  all gambles g on
a and all count vectors   na      
 
 
g  da
c  idg a   dg a 
crg    

 ri  

here  for any non empty set b  we denote by idb the identity map on b  defined by idb  z     z
for all z  b 
proposition    tells us that whether a gamble is desirable depends only on the values it
assumesand not on where they are assumedand on the number of times each of these
values has been observed in the pastor rather would have been if we had been observing
the g xk   rather than the xk  
let us now focus on what happens for events  consider any event b  a that is nontrivial meaning that b is neither empty nor equal to a  then for any real  the gamble
ib   assumes two values      and   so we see after applying proposition    that for
all   na      
 
 
ib    da
c  id      d    
c mb   ma b   

and therefore
 
 
 
p  a  b     sup   r   ib    da
c
 
 
 
  sup   r   id      d    
c mb   ma b       ma   mb   

    
    

meaning that  by representation insensitivity  the predictive lower probability for a non trivial
event b depends only on the number of times mb that it has been observed in the past
experiments  and the total number of observations ma   the same thing holds for its predictive
upper probability     ma   ma  mb    for precise predictive probabilities  a similar property
is known as johnsons sufficientness postulate  johnson        zabell        
so for any representation insensitive coherent inference system 
we see that we can define a
 
 
so called lower probability function     n  k   n    k  n         through equation      
which completely characterises the one step ahead predictive lower and upper probabilities  
for all non trivial events and all count vectors  we shall now use the representation insensitivity
and specificity requirements to try and say more about this lower probability function  the
following theorem strengthens  simplifies  and extends similar results by de cooman et al 
     a  
theorem     consider a representation insensitive coherent inference system   then the
associated lower probability function  has the following properties 
l    is bounded      n  k     for all n  k  n  such that k  n 
l    is super additive in its second argument   n  k        n  k     n     for all
n  k     n  such that k      n 
          but not necessarily the predictive lower and upper previsions      

  

ficoherent predictive inference under exchangeability

l    n         for all n  n   
l    n  k   k n     and n n        for all n  k  n  such that    k  n 
l    is non decreasing in its second argument   n  k    n     for all n  k     n  such
that k     n 
l    n  k    n      k     n  k   n      k        n      k   for all n  k  n  such that
k  n 
l    is non increasing in its first argument   n      k    n  k  for all n  k  n  such
that k  n 
l   suppose that  n         for all n  n  and let sn   
 
  then sn    and sn    sn  
 n       n s
n

 
 n   

 n  or equivalently 

if  is moreover specific  then  has the following properties 
n
l   consider any real          and suppose that           then  n  n     n
for
 
all n  n    as a consequence  consider any s     and suppose that           s   then
n
 n  n   n s
for all n  n   

we know from theorem   that representation insensitive coherent inference systems are
near ignorant  meaning that they are vacuous and therefore completely indecisive about any
single observation when no prior observations have been made  this is also borne out by
theorem    l   let us define the imprecision function by
 n  k         n  n  k    n  k  for all n  k  n  such that k  n 

    

 

it is clear that p a  b    p  a  b      ma   mb   is the width of the probability interval
for an event b that has been observed before mb out of ma times  for a representation
insensitive coherent inference system whose imprecision function  n  k  satisfies the following
property 
 
 n      k    n  k 
for all    k  n 
    
 n      k        n  k 
the imprecision does not increase as the total number of observations increases  this suggests
that such representation insensitive coherent inference systems will display some of the
desirable behaviour mentioned in the introduction  they are conservative when little has been
learned  and they never become less precise as more observations come in  in the following
sections  we intendamongst other thingsto take a closer look at whether this behaviour
is present in a number of such systems 
immediate prediction is very important for predictive inference with precise probabilities 
as the law of total probability guarantees that it is completely determined by its immediate
predictions  perhaps surprisingly  this is not the case for predictive inference with imprecise
probabilities  appendix d provides a counterexample  this also points to some of the
limitations in scope of the earlier work by de cooman et al       a   for this reason  we now
leave immediate prediction models for what they are  and in the rest of this paper concentrate
on the more general notion of an inference system 
  

fide cooman  de bock    diniz

   the vacuous inference system
in this and the following sections  we provide explicit and interesting examples of representation insensitive  and of specific coherent inference systems  we begin with the simplest
one  the vacuous inference system v   which we introduced in section   as the smallest 
or most conservative  coherent inference system  it associates with any category set a the
smallest bernstein coherent set v  a    hv a    v    a  containing all the bernstein positive
polynomialsthe ones that are guaranteed to be there anyway  by bernstein coherence alone 
we deduce from proposition    in appendix b that 
   hv a   v    a  for all 
  na      
hv a c
and from proposition    in appendix b that 
 
 
   h v a  p    sup   r   p    v    a 
h v a  p  
  min p   min p   for all p  v  a  
a

the predictive models for this inference system are now straightforward to find  as they
  na       we
follow directly from equations      and       for any n  n and any 
deduce that 
 
 
n
n
   f  l an     mnna  f    v    a   
dv a
  dv a
c
    
and
   min mnna  f    for all f  l an   
p nv a  f     p nv a  f   
a

    

in particular 
 
 
   l    a  
dv a
  dv a
c

p  v a  f  

 


p  v a  f   

    

  min f for all f  l a  

    

and
v  n  k      for all n  k  n  such that k  n 

    

these are the most conservative exchangeable predictive models there are  and they arise from
making no other assessments than exchangeability alone  as we gather from equations     
      they are not very interesting  because they involve no non trivial commitments  and
they do not allow for learning from observations  this is also borne out by the corresponding
imprecision function  which is given by 
v  n  k      for all n  k  n  such that k  n 
running example  we have seen before that mnn h  t    iht
       h t for all n     and
therefore
n
p nv  h  t    iht
     p v  h  t    iht
           

min

mnn h  t    iht
     

max

mnn h  t    iht
     

 h  t  

min

 h  t  

 h t    

and
n

n

p v  h  t    iht
     p v  h  t    iht
           

 h  t  

  

 
 h t    
 h  t  
 
max

ficoherent predictive inference under exchangeability

this shows that the vacuous inference model does not produce completely vacuous inferences 
it allows us to find out the consequences of making no other assessments than exchangeability 
but it does not allow us to change our lower and upper probabilities and previsions when
new observations come in 

even though it makes no non trivial inferences  the vacuous inference system satisfies
representation insensitivity  but it is not specific 
theorem     the vacuous inference system v is coherent and representation insensitive 
let us show by means of a counterexample that v is not specific 
running example  let us go back to inferences about the category space a    h   t   u   and
the reduced category space b    h   t    consider the polynomial p      h  h t    t
on  h  t     this polynomial is bernstein positiveso p  v     h   t   because
p       h  h t    t   h   t      h    t
has an expansion in the bernstein basis of degree   that is positive  but let us now consider
the corresponding polynomial on  h  t  u    
 
 
q      i b a  p     h
 h t   t
 

    

this polynomial is not bernstein positive  it is easy to see that for every n  n   
 
 
q      h
 h t   t
  h   t   u  n
n   so q   i   p  
will always have a term h t u
  v     h   t   u     and we infer from
b a
theorem    that v cannot be specific 


in the following sections  we shall prove that there are an infinity of more committal 
specific and representation insensitive coherent inference systems  we begin by introducing
a slightly modified version of the vacuous inference system that is coherent  representation
insensitive and specific 

    the nearly vacuous inference system
let us introduce the nearly vacuous inference system nv the reason for its name will
become clear presentlyby 
nv  a     hnv a    v     a      p  v  a       int a   p       
for all category sets a 
since v     a  consists of all the polynomials that are positive on int a    we deduce from
  na       hnv a c
   hnv a   v     a 
proposition    in appendix b that  for any 
and that 
   h nv a  p   
h nv a  p  

p     min p   for all p  v  a  

inf
int a  

  

a

fide cooman  de bock    diniz

since we know from proposition    in appendix b  and the counterexample following it  that
generally speaking v    a   v     a   we see that this inference system is less conservative
than the vacuous one  as was the case for the vacuous inference system  the predictive models
for this nearly vacuous inference system are straightforward to find  as they follow directly
  na       we deduce that 
from equations      and       for any n  n and any 
 
 
n
n
   f  l an     mnna  f    v     a   
dnv a
  dnv a
c
and
   min mnna  f    for all f  l an   
p nnv a  f     p nnv a  f   
a

in particular 
 
 
   l    a  
dnv a
  dnv a
c

   min f for all f  l a  
p  nv a  f     p  nv a  f   
we see that the immediate prediction models  and the predictive lower previsions  for this
inference system are exactly the same as the ones for the vacuous inference systems    they
too do not allow for learning from observations 
interestingly  and in contrast with the vacuous inference system  the nearly vacuous
inference system is specific  which already tells us that crs     
theorem     the nearly vacuous inference system nv is coherent  representation insensitive and specific  nv  crs  

    the skeptically cautious inference system
we now construct a rather simple inference system that is quite intuitive and slightly more
informative than the vacuous and nearly vacuous ones  suppose that our subject uses the
following system for making inferences based on a sequence of n     observations with count
 in a category set a  he is skeptical in that he believes that in the future  he will
vector  
only observe categories that he has seen previously  so only categories in the set 
     x  a   mx       
a  

    

but he is also cautious  because his beliefs about which of these already observed categories
will be observed in the future  are nearly vacuous  to explain this  assume first that in
particular  for n future observations  he has vacuous beliefs about which count vector he will
observe in the set
 
 
n
n


  na    y  a   a   my       na 

 
 that he holds possible after observing the count vector  

of those future count vectors 
    by lemma    in appendix b 
namely those count vectors with no observation outside a   
    this is a first example that shows that the immediate prediction models do not completely determine the
inference system  we shall come across another example in appendix d 
    the last equality in the equation above is actually a device that allows us to identify the count vectors on
 are zero  with count vectors on a   
 we shall be using it repeatedly 
a whose components outside a  
without explicit further mention  in the rest of this paper 

  

ficoherent predictive inference under exchangeability

this would lead us to associate the following set of polynomials with any count vector   na  
 
 
 
n
v  
 a     p  v  a     n  deg p   bnp  na  
  
 
 
 
  p  v  a    p a    v  a     
but  because we already know that the vacuous models v    a  do not lead to specific systems 
whereas the nearly vacuous models v     a  do  we will modify this slightly  and rather
associate the following set of polynomials with any count vector   na  
 
 
  
v  
 a     p  v  a    p a    v     a     
  
the polynomials in v  
 a  are desirable in representation   after observing a sample with
count vector   so we infer from equation      that the subject considers as desirable in
representation all polynomials in 
 
 
  
  
v  
 a ba    pba    p  v  
 a   

we are thus led to consider the following assessment 

  
asc a   
v  
 a ba   
na

and the set of all its positive linear combinations 
hsc a

   posi  asc a    

 
 

pk ba k      n  nk  n  k 

nank   pk

 



  
v 
 a 
k 

      

k  

the following proposition guarantees that the sets hsc a are the appropriate most conservative
models that summarise the exchangeable inferences for our skeptically cautious subject 
proposition     hsc a is the smallest bernstein coherent set of polynomials on a that
includes asc a  
this also shows that the inference system sc   defined by sc  a     hsc a for all category
sets a  is coherent  we shall call it the skeptically cautious inference system 
we now want to find out how updating works in this system  to this end  we introduce a
slight generalisation of the set defined in equation       consider any   na       and let
hsc a    

 
 

pk ba k      n  nk  n    ma   nk      k 

nank   pk

 



  
v  
 a 
k 

 

k  

    
so we see that  in particular  hsc a   hsc a  for      
the sets hsc a  have the following interesting characterisation 
    as stated before  polynomials have no direct behavioural but only an indirect representational meaning 
as conveniently condensed representations for desirable gambles on observation sequences  hence our
caution here in using the term desirable in representation 

  

fide cooman  de bock    diniz

proposition     for all   na      
 
 
hsc a    p  v  a           k  min sa   p  p k  v     k   

    

where
 
 
sa   p       
  k  a   a    k and p k       

    

by min sa   p  we mean the set of all minimal  or non dominating  elements of sa   p  
so min sa   p      c  sa   p     k  sa   p   k  c  k    c    we formally extend
 
equation      to include the case       so a       and sa    p      
  k  a   p k       
proposition     for all   na       hsc a c   hsc a   
by combining this result with equation       we can deriveadmittedly rather involved
expressions for the predictive sets of desirable gambles for the skeptically cautious inference
  na      
system  for all n  n and 
 
 
n
   f  l an     mnna  f    hsc a 
c
    
dsc a
  
  na  
for immediate prediction  these expressions simplify significantly  for any 
 
 
 
 
   f  l a    f  a  
dsc a
  l    a  and dsc a
c
      l    a  

    

  na  
the lower previsions that are derived from hsc a 
 are more tractable  for any 
  
h sc a  p    min p x   and h sc a  p  
xa

min p   for all p  v  a  

a  


    

where  for any x  a  x is the degenerate probability mass function on a that assigns all
probability mass to x 
the predictive lower previsions for the skeptically cautious inference system are now
  na  
easily obtained by combining equations      and       for any n  n and any 
  
p nsc a  f   

min mnna  f    for all f  l an  

a  


    

and
p nsc a  f     min f  x  x          x  for all f  l an   

    

   min f  x  for all f  l a  
p  sc a  f     min f and p  sc a  f   

    

xa

in particular 

xa  

the lower probability function is given by 
 
  if k   n    
sc  n  k   
  otherwise

for all n  k  n  such that k  n 

and the corresponding imprecision function by 
 
  if n     or     k   n
sc  n  k   
  otherwise
  

for all n  k  n  such that k  n 

ficoherent predictive inference under exchangeability

running example  as before  mnn h  t    iht
       h t for all n     if we also take into
account that  h   t              h   t    we get 
n
p nsc  h  t    iht
     p sc  h  t    iht
           

min

mnn h  t    iht
     

 h  t  

max

mnn h  t    iht
     

 h  t  

 h  t  

min

 h t    

max

 
 h t    
 

and
n

n

p sc  h  t    iht
     p sc  h  t    iht
           

 h  t  

because all categories are observed for the count vector       meaning that  h   t            
 h   t  we find the same inferences as for the vacuous inference system 

interestingly  the coherent inference system sc also satisfies both representation insensitivity and specificity 
theorem     the skeptically cautious inference system sc is coherent  representation
insensitive and specific  sc  crs  

    the idmm inference systems
imprecise dirichlet modelsor idms  for shortare a family of parametric inference models
introduced by walley        as conveniently chosen sets of dirichlet densities dia     with
constant prior weight s 
 
 
 dia         ksa     with ksa      ra
    
     a   s    s     int a     
for any value of the  so called  hyperparameter s  r   and any category set a  the dirichlet
densities dia     are defined on int a    see appendix c for an explicit definition and
extensive discussion 
these idms generalise the imprecise beta models introduced earlier by walley         in
a later paper  walley and bernard        focussed on a closely related family of predictive
inference models  which they called the imprecise dirichlet multinomial modelsor idmms 
for short    we refer to these papers  and to a more recent overview paper by bernard
       for extensive motivating discussion of the idm m s  their inferences and properties 
for precise dirichlet models and their expectations  and the related dirichlet multinomial
models  we have gathered in appendix c the most important facts  properties and results 
necessary for a proper understanding of our present discussion of the idm m s in the context
of inference systems 
one of the reasons walley        had for suggesting the idm as a reasonable model is
precisely that it satisfies the pooling   invariance properties we discussed in section      this
is also discussed with more emphasis by walley and bernard        and bernard         but
we know of no detailed and explicit formulations of these properties in the literature  and
the proofs we have seen are fairly sketchy  bernard              also suggests that the idm
    in the later paper  walley and bernard        clearly distinguish in name between the parametric idms
and the predictive idmms  while in the earlier paper by walley         both types of models are referred
to as idms 
    walley uses the term representation invariance rather than pooling invariance 

  

fide cooman  de bock    diniz

and the underlying precise dirichlet models satisfy a so called specificity property  which
we have tried to translate to the present context of predictive inference in section     
in the present section  we use the ideas behind walley and bernards idm m s to construct
an interesting family of coherent inference systems  and we give a detailed and formal proof
in appendix e of the fact that these inference systems are indeed representation insensitive
and specific  interestingly  we shall need a slightly modified version of walleys idm m 
to make things work  the reason for this is that walleys original version  as described by
expression       has a number of less desirable properties  that seem to have been either
unknown to  or ignored by  walley and bernard  we describe these shortcomings in some
detail in appendix d  for our present purposes  it suffices to mention that  contrary to what
is often claimed  and in contradistinction with our new version  inferences using the original
version of the idm m  do not necessarily become more conservative  or less committal  as
the hyperparameter s increases 
in our version  rather than using the hyperparameter sets ksa   we consider the sets
 
 
sa      ra
     a   s for all s  r    
observe that
 
 
sa   s     s   r     s    s and   int a    



 

ksa  

  s   s

for any s  r     and any category set a  we now consider the following set of polynomials
p  with positive dirichlet expectation dia  p   for all hyperparameters   sa  
s
    p  v  a       sa   dia  p         
hidm a

we shall see further on in theorem    that this set is bernstein coherent  we call the inference
system sidm   defined by 
s
sidm  a     hidm a
for all category sets a 

the idmm inference system with hyperparameter s      the corresponding updated models
  na       given by 
are  for any 
s
    p  v  a       sa   dia  p 
         
hidm a
c

    

   inf s dia  p 
     for all p  v  a  
h sidm a  p  

    

and
a

using these expressions  the predictive models for the idmm inference system are straightforward to find  it suffices to apply equations      and       for any n  n and any
  na      

 
 
s n
   f  l an        sa   dia  mnna  f   
          
didm a
c
    
and
   inf s dia  mnna  f   
     for all f  l an   
p s n
idm a  f   
a

  

    

ficoherent predictive inference under exchangeability

where  using the notations introduced in appendix c 
       dimnna  hyna  f   
    
dia  mnna  f   
    

n
 
n

 
hya  f   
 mx   x   mx    
 n  

 ma   a  
n
xa

n

    

a

in general  these expressions seem forbidding  but the immediate prediction models are
  na      
manageable enough  for any 
 
 
  
s  
   f  l a    f   
    
didm a c
f  x mx  
s
xa

 
s

p s  
 f
 
 
 
f  x mx  
min f for all f  l a  
    
idm a
ma   s
ma   s
xa

and

k
for all n  k  n  such that k  n 
n s
the corresponding imprecision function is given by 
sidm  n  k   

sidm  n  k   

s
for all n  k  n  such that k  n 
n s

and it is decreasing in its first and constant in its second argument  which implies that it
satisfies condition       this suggests that idmm inference systems are conservative when
little has been learned  and become more precise as more observations come in 
running example  as before  with mnn h  t    iht
       h t for all n     we find that 
using the results in appendix c 
  
 

di h  t   mnn h  t    iht
      

 h t
 
 h   t   h   t     

it is then not very difficult to verify using equation      that for any     s 
s n

p s n
       and p idm  h  t    iht
    
idm  h  t    iht

  s
 
   s

after observing the count vector         we find after some manipulations that 
       
      s 
 
 
   s             
     s      s 

p s n
            inf
idm  h  t    iht
and similarly 




 

  s
     s      s 
s n
p idm  h  t    iht
           

 
  s


   s

if s   
if s    

observe that for infinitely large s  we recover the inferences for the vacuous system 
  



fide cooman  de bock    diniz

interestingly  the immediate prediction models for our version of the idmm inference
system coincide with those of walleys original version  hence  in the many practical applications that are concerned with immediate prediction only  both approaches yield identical
results 
the idmm inference systems constitute an uncountably infinite family of coherent
inference systems  each of which satisfies the representation insensitivity and specificity
requirements 
theorem     for any s  r     the idmm inference system sidm is coherent  representation
insensitive and specific  sidm  crs  
s
since crs is closed under non empty infima  the infimum 
idm of all idm   s     is
still coherent  representation insensitive and specific  and more conservative than any of the
idmm inference systems  it is given by 
 
 
   

 a     p  v  a       ra
idm  a    v
     dia  p        

and although this set generally strictly includes the sets v    a  and v     a   the associated
immediate prediction models and predictive lower previsions can be shown to coincide with
the ones for the vacuous and nearly vacuous inference systems 

    the skeptical idmm inference systems
we now combine the ideas in the previous two sections  we suppose that our subject uses
the following system for making inferences based on a sequence of n     observations with
 in a category set a  as before in section     he is skeptical in that he
count vector  
believes that in the future  he will only observe categories that he has seen previously  so only
 but rather than being cautious in having completely vacuous
categories in the set a   
beliefs about which of these already observed categories will be observed in the future  he
uses an idmm like inference for them  as described in section    
it turns out this can be done quite simply by replacing  in the characterisation      of
the sets hsc a  of the skeptically cautious inference system  the nearly vacuous models
s
v     k  by the appropriate idmm models hidm k
crk     so we define  for any category
set a  any   na      and any s  r     the following set of polynomials 
 
 
s
s
   p  v  a           k  min sa   p  p k  hidm k
hsi a 
crk     
    
where we recall that if k  min sa   p   then a    k and therefore k rk      
a    k   a    so  and rk    are essentially the same count vectors  we also let
s
s
   hsi a 
hsi a
for       or in other words 
 
 
s
s
   p  v  a           k  min sa    p  p k  hidm k
hsi a
 
 
 
where  again  sa    p      
  k  a   p k        in the remainder of this section  we show
s
that the sets of polynomials hsi a
indeed lead to the definition of a reasonable and potentially
useful type of inference system  we begin with coherence 
s
proposition     hsi a
is a bernstein coherent set of polynomials on a  

  

ficoherent predictive inference under exchangeability

s
this shows that the inference system ssi   given by ssi  a     hsi a
for all category sets a 
s
is coherent  we call si the skeptical idmm inference system with hyperparameter s 
we now want to find out how updating works in this inference system  the following
proposition should not really come as a surprise 
s
s
proposition     for any   na       hsi a
c   hsi a 
 

by combining this with equation       we obtain the followingagain  rather involved
predictive sets of desirable gambles for the skeptical idmm inference systems  for any n  n
  na      
and any 
 
 
s n
s
   f  l an     mnna  f    hsi a 
dsi a
c
    
  

s
 are rather abstract  this is not the case for the
although the expressions for hsi a
c
  na  
corresponding lower previsions  for any 

h ssi a  p    min p x   for all p  v  a 
xa

    

and
  
h ssi a  p  

inf

sa  


    
dia  
 ra  
  p a  
   


 for all p  v  a  
  h sidm a  
 ra  
    
  p a  


    
    

combining this with equation       we immediately obtain the following predictive lower
  na  
previsions for the skeptical idmm inference systems  for any n  n and any 
n
p s n
si a  f     min f  x  x          x  for all f  l a  
xa

and
  
p s n
si a  f   

inf

sa  


n
    
dia  
  mna  
   
  f  a  
 n   ra  

 for all f  l an   
  p s n
    
 n  ra  
  f  a  
idm a  

    

the immediate prediction models of the skeptical idmm inference systems are surprisingly
more manageable 
s  
dsi a
  l    a  and p s  
si a  f     min f for all f  l a 

  na  
and  for any 
 
 
  
s  
   f  l a    f  a  
dsi a c
f  x mx  l    a 
  
s

    


xa  


 
s
s  
  
 f   
f  x mx  
min f  x  for all f  l a  
p si a
ma   s
ma   s xa  


xa  

  

    

fide cooman  de bock    diniz

the lower probability function is given by 
 
k
if k   n or n    
s
si  n  k    n s
 
if k   n    
and the corresponding imprecision function by 
 
s
if n     or     k   n
s
si  n  k    n s
 
otherwise

for all n  k  n  such that k  n 

for all n  k  n  such that k  n 

when we consider the case n      we see that ssi  n  n      but ssi  n      n   
this imprecision function does not satisfy condition      

s
n   s

     so

running example  because  h   t              h   t    we infer from equation      that the
 are the same as for the idmm inference systems 
inferences about the event ht

all the coherent inference systems ssi also satisfy both representation insensitivity and
specificity 
theorem     for each s  r     the corresponding skeptical idmm inference system is
coherent  representation insensitive  and specific  ssi  crs  
s
since crs is closed under non empty infima  the infimum 
si of all si   s     is still
coherent  representation insensitive and specific  and more conservative than any of the
skeptical idmm inference systems  it can be shown that the associated immediate prediction
models and predictive lower previsions coincide with the ones for the skeptically cautious
inference system 

    the haldane inference system
we already know from our discussion of near ignorance following theorem   that no representation insensitive coherent inference system can be fully precise  as its immediate prediction
models before observations have been made  must be completely vacuous  but we can ask
ourselves whether there are representation insensitive  and specific  inference systems whose
posterior predictive lower previsions become precise  linear  previsions  this is the problem we
address in this section  we shall first construct such an inference system  and then show that
this system is  in some definite sense  unique in having linear posterior predictive previsions 
we use the family of all idmm inference systems sidm   s  r     to define an inference
system h that is more committal than any of them 


s
hidm a
 
sidm  a  for all category sets a 
h  a    hh a   
sr  

sr  

we call this h the haldane inference system  for reasons that will become clear further on
in this section 
theorem     the haldane inference system h is coherent  representation insensitive and
specific  h  crs  
  

ficoherent predictive inference under exchangeability

due to its representation insensitivity  the haldane system satisfies prior near ignorance 
this implies that before making any observation  its immediate prediction model is vacuous 
and as far away from a precise probability model as possible  but we are about to show
that  after making even a single observation  its inferences become precise probabilistic  they
coincide with the inferences generated by the haldane  improper  prior 
to get there  we first take a look at the models involving sets of desirable gambles  for
  na      
any 

s
    p  v  a     s  r       sa   dia  p 
           

hh a c
hidm a
c 
sr  
    
the corresponding predictive models are easily derived by applying equation       for any
  na      
n  n and any 
 
 
n
   f  l an      s  r       sa   dia  mnna  f   
        
dh a
c

s n

 
didm a
c 
    
sr  

the immediate prediction models are obtained by combining equations      and       for
  na  
any 
 
 

 
 
   f  l a   
dh a   l    a  and dh a c
f  x mx      l    a  
xa

it turns out that the expressions for the corresponding lower previsions are much more
  na      
manageable  first of all  we find for any 
       lim h sidm a  p  
 for all p  v  a  
inf dia  p 

   lim
h h a  p  

s   sa

s  

    

      this simplifies to 
in particular  for 
h h a  p    min p x   for all p  v  a  
xa

    

  na   we find linear previsions   
whereas for any 
   h h a  p  
   hh a  p  
   dia  p  
 for all p  v  a  
h h a  p  

    

the corresponding predictive models are easily derived by applying equation       for any
  na      
n  n and any 
   lim
p nh a  f   

       lim p s n
 for all f  l an   
inf dia  mnna  f   
idm a  f   

s   sa

s  

    
     
in particular  for 
p nh a  f     min f  x  x          x  for all f  l an   
xa

    the dirichlet expectations dia     are strictly speaking defined for   ra
     but as we argue in
appendix c  they can be continuously extended to  with some components zero  and the others strictly
positive 

  

fide cooman  de bock    diniz

  na  
and for any 

p nh a  f   

 

n

p h a  f   

 

n

ph a
 f   

 


n
na

    
 nx  
n
xa mx
 
 n 

m

hyna  f   

    

a

  na  
for the immediate prediction models  we find that for any 

mx
 
  
p  h a  f     min f and ph a
 f   
f  x 
for all f  l a  
ma
xa

and the lower probability function is given by 
 
k
if n    
h  n  k    n
for all n  k  n  such that k  n 
 
if n    
the corresponding imprecision function is given by 
 
  if n    
h  n  k   
for all n  k  n  such that k  n 
  if n    
and it satisfies condition       which suggests that also the haldane inference system displays
albeit in an extreme and not very interesting mannerthe desirable behaviour mentioned in
the introduction  it is conservative when little has been learned  and it never become less
precise as more observations come in 
running example  we can use equation      and the results previously obtained for the
idmm inference systems to find that
n

n
p nh  h  t    iht
       and ph  h  t    iht
           
     p h  h  t    iht

 
 
  

we want to point out that the first equalities do not contradict the prior near ignorance of
the haldane inference system  as that only pertains to immediate predictions  predictions
about single future observations 

the precise posterior predictive previsions in equation      are exactly the ones that
would be found were we to formally apply bayess rule with a multinomial likelihood and
haldanes improper prior  haldane 
      jeffreys        jaynes         whose density is a
function on int a   proportional to xa x    this  of course  is why we use haldanes name
for the inference system that produces them  our argumentation shows that there is nothing
wrong with these posterior predictive previsions  as they are based on coherent inferences  in
fact  our analysis shows that there is an infinity of precise and proper priors on the simplex
a that  together with the multinomial likelihood  are coherent with these posterior predictive
previsions  every coherent prevision on v  a  that dominates the coherent lower prevision
h h a on v  a     for binomial parametric inferences under the haldane prior  walley       
section        comes to a related conclusion in a completely different manner 
    it is an immediate consequence of the f  riesz representation theorem that each such coherent prevision
is the restriction to polynomials of the expectation operator of some unique  additive probability measure
on the borel sets of a   see for instance the discussion by de cooman and miranda      a  and also
footnote   

  

ficoherent predictive inference under exchangeability

there is a simple argument to show that these haldane posterior predictive previsions
are the only precise ones that are compatible with representation insensitivity  indeed  it
can be shown that for any representation insensitive coherent inference system with precise
posterior predictive previsions  the lower probability function must satisfy  n  k    k n for
n     and    k  n    and then it is straightforward to prove  using bayess theorem to go
from immediate prediction to more general predictive inference  that the posterior predictive
previsions must be haldanes 

    characterisation of the idmm immediate predictions
the lower probability function  n  k  for a representation insensitive coherent inference
system gives the lower probability of observing a non trivial event that has been observed k
times before in n trials 
now suppose that a subject specifies a single lower probability  namely the value of
                the probability of observing something  again  that has been observed  once 
in a single trial  then we can ask ourselves what the most conservative consequences of such
an assessment are  if we take representation insensitivity and specificity for granted  in other
words  what is the most conservative representation insensitive and specific coherent inference
system that has  at least  this given value        for its lower probability function  this
question makes sense because the representation insensitive and specific coherent inference
systems constitute a complete meet semilattice by statement      and theorems   and      
clearly  if             this is the smallest representation insensitive and specific coherent
inference system  which as we know from the discussion in sections   and     must have the
same immediate prediction models and predictive lower previsions as the  nearly  vacuous
inference system  we consider the case that                   or in other words  to use a
parametrisation that will turn out to be more convenient for our purposes  that 
        

 
 
for some positive real number s   
   
  s
      

    

let us denote this most conservative inference system by s   and its lower probability
 
function by s   then by assumption s           s
  it now follows from theorem    l  that
n
s
  n  n   n s for all n  n    but since for the idmm inference system sidm   equation     
n
tells us that sidm  n  n    n s
  and since by assumption sidm  n  n   s  n  n   we conclude
that 
n
s  n  n    sidm  n  n   
for all n  n   
    
n s
it has been surmised  bernard        de cooman et al       a  that the idmm inference
system with hyperparameter s could be the smallest  most conservative  representation
 
insensitive and specific coherent inference system with a given value            s
  in
fact  trying to prove this was what made us start research on the present paper  but
this conjecture turns out to be false  apart from the lower bound      on the  n  n  
    it suffices to exploit the additivity of precise probabilities and the symmetry implied by representation
insensitivity  for an explicit proof  see the paper by de cooman et al       a  thm     
    see the discussion near the end of section   
    we surmise  but do not prove here  that the most conservative representation insensitive and specific
coherent inference system corresponding to            might be the skeptically cautious one 

  

fide cooman  de bock    diniz

representation insensitivity and specificity impose no lower bounds on the  n  k  for k   n 
to see this  consider the inference system smc    inf sc   sidm    which by statement     
and theorems           and    is coherent  representation insensitive and specific  smc  crs  
its lower probability function smc satisfies 
 
n
n
min    n s
    n s
if k   n    
smc  n  k    min sc  n  k   sidm  n  k    
k
min    n s      
otherwise 
substantiating the claim we made above  see also figure    where we have depicted lower  and
upper  probability functions for the haldane system h   the idmm system sidm   smc and
n
s
the inference system inf  s
si   idm    the latter three all share the same value n s for  n  n  
n     we conjecture that smc could be the smallest  most conservative  representation
 
insensitive and specific coherent inference system with a given value            s
  but offer
no proof for this 
 n  k 
 
n
n s

s
n s

 
 

 

 

   

n 

n

k

figure    lower and upper probability functions  h for the haldane system  dark grey 
s
    sidm for the idmm system with hyperparameter s  blue      min  s
si   idm  
 orange    and smc   min sc   sidm    red     this specific plot was made for
n      and s     
this means that if we want to characterise the idmm inference systems in any way as
the most conservative ones  we need to add  besides coherence  representation insensitivity
and specificity  another requirement that is preserved under taking infima  one possible
candidate for this  which we shall prove does the job and is inspired by figure    is the
following requirement 
let us define the subjects surprise of an event as his supremum rate for betting on the
opposite event  or in other words  his lower probability for the opposite event  this surprise
is highclose to onewhen the subject believes strongly that the event will not occur  and
lowclose to zerowhen the subject has no strong beliefs that it will not occur 
  

ficoherent predictive inference under exchangeability

this allows us to associate a so called surprise function  n  k      n  n  k  with a
lower probability function  where  n  k  is the subjects surprise when observing a non trivial
event that has been observed k out of n times before 
it follows from theorem    l  that for any representation insensitive system  the surprise
function is non increasing in its second argument 
 n  k      n  k        n  k     n  n  k       n  n  k     for    k  n    
this is a fairly intuitive property  the more often an event has been observed before  the
smaller the surprise is at seeing it again 
we shall say that a representation insensitive system has concave surprise if
   n  k      n  k        n  k     for    k  n    
where  of course     n  k     n  n  k        n  n  k        n  n  k   it is not
difficult to see that having concave surprise is preserved under taking non empty infima
of inference systems  so it makes sense to go looking for the smallest  most conservative 
coherent representation insensitive and specific coherent inference system that has concave
surprise  and satisfies some additional local assessments  such as      
looking at figure   makes us suspect that the idmm inference system sidm might be
this system  but again  we offer no proof for this conjecture  we can however provide a proof
for the following  related but  probably  weaker  statement  which focusses on immediate
prediction only 
theorem     the immediate prediction models p  a        na      for the smallest
 most conservative  coherent representation insensitive and specific coherent inference system
 that has concave surprise and satisfies       coincide with the ones for the idmm inference
system sidm with hyperparameter s 

    conclusion
we believe this is the first paper that tries to deal in a systematic fashion with principles for
predictive inference under exchangeability using imprecise probability models  two salient
features of our approach are  i  that we consistently use coherent sets of desirable gambles as
our uncertainty models of choice  and  ii  that our notion of an inference system allows us to
derive a conservative predictive inference method combining both local predictive probability
assessments and general inference principles 
the first feature is what allows us  in contradistinction with most other approaches in
probability theory  to avoid problems with determining unique conditional models from
unconditional ones when conditioning on events with  lower  probability zero  a set of
n c
 and
polynomials ha completely determines all prior and posterior predictive models da
n
n
 even when the  lower  prior probability p a     
   h a  ba 
p a     
   of observing the
 is zero  an approach using only lower previsions and probabilities would make
count vector 
this much more complicated and involved  if not impossible  interestingly  we can provide
a perfect illustration of this fact using the results in sections        and       the three
    something similarly dramatic happens in sections   and     the inference systems there have the same
immediate prediction models and the same  predictive  lower previsions  but one is specific and the other
is not 

  

fide cooman  de bock    diniz

inference systems that are described therethe skeptically cautious  the skeptical idmm and
the haldane systemshave  for any given category set a  three different sets of polynomials
ha   nevertheless  as we can gather from equations            and       they have the same
lower prevision h a and therefore the same prior predictive models p na   and any count vector
  na has the same prior lower probability 


   h a  ba 
p na     
     min ba 
  x       
xa

 and the
this zero lower probability makes sure that the posterior lower previsions h a    
 are not uniquely determined by the prior lower prevision
posterior predictive models p na    
h a   we infer from equations            and      that they are indeed very different for these
three types of inference systems  we fail to see how we could have come up withlet alone
proved the necessary results forthese three systems relying only on lower prevision or credal
set theory 
we canand musttake this line of argumentation even further  by theorem    any
inference system that satisfies  prior  representation insensitivity has near vacuous prior
predictive models  and therefore  by time consistency and coherence  monotonicity   we see
n
     for any
that its prior predictive lower previsions must satisfy h a  ba 
     p a     
  na as well  this simply means that it is impossible in a  prior  representation insensitive

coherent inference system for the lower prevision h a to uniquely determine the conditional
 and therefore any systematic way of dealing with such inference
lower previsions h a     
systems must be able to resolveor deal withthis non unicity in some way  we believe
our approach involving coherent sets of desirable gambles is one of the mathematically more
elegant ways of doing this 
the second feature has allowed us  as an example  to characterise the idmm immediate
predictions as the most conservative ones satisfying a number of inference principles  the
approach we follow canat least in principlealso be used for other types of inference
systems and other inference principles  the key requirement for an inference principle to
make it amenable to our approach is that  when formulated as a property of an inference
system  it should be preserved under taking arbitrary non empty infima  the three inference
principles that we have been considering aboverepresentation insensitivity  specificity and
having concave surprisehave this property  but there is nothing that prevents our analysis
and approach from being extended to any other inference principle that has it too  the only
complications we see  at this point  are of a technical mathematical nature  the reader will no
doubt have noticed that our proofs for the results in the later sections are quite involved and
technical  and rely quite heavily on properties of polynomials on a simplex  we feel that in
the present paper we have made some headway into this mathematical territory  for instance
with our new discussion about the bernstein positivity of polynomials near proposition   
in appendix b  in the conclusions of a paper by de cooman and quaeghebeur         a
characterisation of bernstein positivity was mentioned as an open problem with interesting
practical applications in doing inferencenatural extensionunder exchangeability  but
much remains open for further exploration  and a more determined study of the mathematical
structure and properties of such polynomials would certainly help in alleviating the technical
difficulties of working with inference principles in inference systems 
while this paper has only just opened up what we feel to be an interesting line of
research into the foundations of predictive inference  it nevertheless has provided answers to
  

ficoherent predictive inference under exchangeability

a number ofif not allopen problems formulated in the conclusions of an earlier paper by
de cooman et al       a   who tried to deal with representation insensitivity in immediate
prediction  as a first example  it was asked there whether there are representation insensitive
coherent inference systems whose lower probability functions are not additive in the second
argument  it suffices to look at figure   to see that the answer is  clearly  yes  another
question was  are there representation insensitive coherent inference systems that are not
mixing predictive systems    it follows from equation      that the answer is yes  each of
the skeptical idmm inference systems provides an example  finally  we can use the infimum
smc of the skeptically cautious inference system sc and an idmm inference system sidm  
mentioned briefly in section     to answer two more questions  are there representation
insensitive coherent inference systems for which the inequality in theorem    l  is strict 
and are there representation insensitive coherent inference systems whose behaviour on
gambles is not completely determined by their lower probability function  the inference
system smc provides a positive answer to both questions 
most of the inference systems mentioned above  apart from the idmm and the haldane
systems  appear here for the first time  some of them may appear contrived and perhaps
even artificial  but we have found them to be most useful in constructing  counter examples 
shaping intuition  and building new models  as figure   and the argumentation above clearly
indicate  we might also wonder whether there are other representation insensitive and or
specific coherent inference systems  which cannot be produced as appropriately chosen infima
of the examples we have introduced here  we suggest  as candidates for further consideration 
the inference systems that can be derived using walleys        bounded derivative model 
and inference systems that can be constructed using sets of infinitely divisible distributions 
as recently proposed by mangili and benavoli         the framework provided here  as well as
the simple characterisation results of theorems   and     should be quite useful in addressing
this and similar problems 
to end  we want to draw attention once again to a simple and direct  but quite appealing 
consequence of our argumentation in section     there is an infinity of precise and proper
priors that  together with the multinomial likelihood  are coherent with the haldane posterior
predictive previsions  so  there is no need for improper priors to justify these posteriors  as
there are proper priors that will do the job perfectly well  this  precise  probabilistic conclusion
follows easily when looking at the problem using the more general and powerful language
of imprecise probabilities  moreover  we have seen that properties such as representation
insensitivity cannot be satisfied by precise probabilistic models  finally  the entire framework
of conservative predictive inference using inference principles would be impossible to develop
within the more limitative context of precise probabilities  this shows that there are distinct
advantages to using imprecise probability models for dealing with predictive inference 

acknowledgements
gert de coomans research was partially funded through project number  g       of the
research foundation flanders  fwo   jasper de bock is a phd fellow of the research
    loosely speaking  that cannot be written as a  specific kind of  convex mixture of the haldane inference
system and an idmm inference system  see the paper by de cooman et al       a  section    for more
information 

  

fide cooman  de bock    diniz

foundation flanders and wishes to acknowledge its financial support  marcio diniz was
supported by fapesp  so paulo research foundation   under the project             
and wishes to thank the systems research group at ghent university for its hospitality and
support during his sabbatical visit there  the authors would like to thank three anonymous
reviewers for their many insightful comments and suggestions aimed at making this paper
easier to read and cleaning up misunderstandings  a special thank you also to the great
arthur van camp for his enthusiasm in everything and  in particular  in helping us check
little examples 

appendix a  notation
in this appendix  we provide a list of the most commonly used and most important notation 
and where it is defined or first introduced 
notation

meaning

introduced where

a  b  c  d
ib
x  xn
n
n
posi a 
l a 
l    a 
l   a 




n
da

category sets  events
indicator of an event b
variable  variable at time n
number of already observed variables
number of to be observed variables
cone generated by a
set of all gambles on a
set of all positive gambles on a
set of all non positive gambles on a
observed sample
observed count vector
prior predictive set of desirable gambles
for category set a and n future observations
posterior predictive set of desirable gambles
prior predictive lower prevision
posterior predictive lower prevision
pooling map or relabelling map
renaming bijection
category permutation
sample with observations outside b eliminated
counting map
set of count vectors for n observations
set of all count vectors  with zero
hypergeometric expectation operator
multinomial coefficient with count vector 
multinomial expectation operator
simplex of all probability mass functions on a
sum of components x of  over x  b
bernstein basis polynomial
set of polynomials of degree up to n on a

section  
section    
section  
section  
section  
equation    
section    
section    
section    
section  
section    
section  

n c 
n c
 da

da
n
p a   
 p na    

p na     


 
 b


nan
na   na     
hyna    
  
mnna    
a
b
ba 
v n  a 

  

section  
section  
section  
sections        
section    
sections    
section    
equation    
equation    
section    
equation     
equation     
equation     
equation     
equation     
equation     
section    

ficoherent predictive inference under exchangeability

v  a 
v    a 
v     a 
ha

ha c
ha

h a    
f

c
crs
r
rb
ia
irb a
sa



subscript
subscript
subscript
subscript
subscript
subscript
subscript

a  
  
v  
 a 

v
nv
sc
idm
si
h
oi

dia    
dia    
dimnna    
bnp

set of all polynomials on a
set of bernstein positive polynomials on a
set of polynomials on a
that are positive on int a  
representing set of polynomials
updated representing set of polynomials
lower prevision induced by ha

lower prevision induced by ha c
set of all category sets
inference system
set of all coherent inference systems
set of coherent inference systems that are
representation insensitive and specific
extended relabelling map
restriction map
injection map
extended injection map
sampling expectation
lower probability function
imprecision function
surprise function
related to vacuous inference system
related to nearly vacuous inference system
related to skeptically cautious inference system
related to idmm inference systems
related to skeptical idmm inference systems
related to haldane inference system
related to original idmm inference systems
categories in a already observed
set of polynomials on a
that are positive on int a    
dirichlet density
dirichlet expectation operator
dirichlet multinomial expectation operator
expansion of polynomial p
in bernstein basis of degree n

section    
section    
section   
theorem  
equation     
equation     
equation     
definition  
definition  
equation     
theorem   
equation     
equation     
equation     
equation     
section  
equation     
equation     
section   
section  
section   
section   
section   
section   
section   
appendix d
equation     
section   
appendix
appendix
appendix
appendix

c
c
c
b

appendix b  multivariate bernstein basis polynomials
with any n    and   nan there corresponds
bernstein basis polynomial
 a  multivariate 
m
x
  
of degree n on a   given by ba    
   xa x     a   these polynomials have a
number of very interesting properties  see for instance prautzsch  boehm    paluszny       
chapters    and      which we list here 
bb   the set  ba    
  nan   of all bernstein basis polynomials of fixed degree n is linearly
independent  if n n  ba       then      for all  in nan  
a

  

fide cooman  de bock    diniz

 nan   of all bernstein basis polynomials of fixed degree n forms a
bb   the set  ba    
partition of unity  n n ba      
a

bb   all bernstein basis polynomials are non negative  and strictly positive on the interior
int a   of a  
bb   the set  ba      nan   of all bernstein basis polynomials of fixed degree n forms a
basis for the linear space of all polynomials whose degree is at most n 
property bb  follows from bb  and bb     it follows from bb  that 
bb   any polynomial p has a unique expansion in terms of the bernstein basis polynomials
also called bernstein expansionof fixed degree n  deg p  
or in other words  there is a unique count gamble bnp on nan such that 

p    
bnp   ba     for all   a  

    

n
na

this tells us  also use bb  and bb   that each p   is a convex combination of the bernstein
coefficients bnp       nan whence 
min bnp  min p  p    max p  max bnp for all   a  

    

the following proposition adds more detail to this picture 
proposition     for any polynomial p on a  
lim  min bnp   max bnp      min p  max p    p a   

n 
ndeg p 

proof of proposition     since bnp converges uniformly to the polynomial p as n   
 trump   prautzsch         in the sense that
     



lim maxn p
 bnp        
n  na
n
ndeg p 

we find that
lim

n 
ndeg p 

min bnp  min p  

lim

 
 
minn bnp     min p

n  na
ndeg p 

 
     
minn bnp     p
n  na
n
ndeg p 
     



  lim maxn p
 bnp        
n  na
n



lim

ndeg p 

and therefore limn  ndeg p  min bnp  min p  furthermore  by statement       we see that
limn  ndeg p  min bnp  min p  hence indeed limn  ndeg p  min bnp   min p  the proof
for the other equality is completely analogous 
    to see how  clearly all polynomials are by definition linear combinations of bernstein basis polynomials 
of possibly different degrees  for each of the terms  use bb  to raise the degree to a common higher
degree nmultiply it by an appropriate version of    this shows that the bernstein basis polynomials of
fixed degree n are generating for all polynomials of lower degrees  they are also independent by bb  

  

ficoherent predictive inference under exchangeability

using the above results  we can prove a number of useful relations between the bernstein
positivity of a polynomial and its positivity on  the interior of  the simplex  they are related
to a property first proved by hausdorff in the univariate case  hausdorff        p       
proposition     let p be any polynomial on a   consider the following statements 
 i     a  p       
 ii  p  v    a   meaning that there is some n  deg p  such that bnp     
 iii  p  v     a   meaning that    int a   p       
 iv     a  p      
then  i  ii  iii  iv  
proof of proposition     the first implication is a direct consequence of proposition     we
infer from  i  and the continuity of p that min p     and therefore  by proposition     that
limn  ndeg p  min bnp   min p      which implies  ii  
to prove that  ii  iii   assume that there is some n  deg p  such that bnp      and
consider any   int a    then since ba         for all   nan  bb    and since by
assumption bnp    and bnp        for some   nan   we see that
p    



bnp   ba      bnp   ba         

n
na

the third implication is an immediate consequence of the continuity of p 
the following counterexample shows that not necessarily v    a    v     a  
running example  we go back to the polynomial q on  h  t  u   defined in equation      
 
 
q     h
 h t   t
   h  t      h t for all    h  t  u    

we have already argued that this polynomial is not bernstein positive  nevertheless  it is
obviously positive on the interior of  h  t  u    

it is also quite easy to trace the effect on the bernstein expansion of multiplying with a
bernstein basis polynomial 
proposition     for all polynomials p on a   all natural n  deg p   all   na     
and all   nan ma  

  
bn     
if   
p
n ma
 

   
bpba      

 
otherwise 
proof of proposition     observe that 
  
 

n
pba   
bp   ba  ba   
bnp   ba  ba 
n
na

n
na

  

fide cooman  de bock    diniz

 



bnp   

n
na

     
ba    
    

and use the uniqueness of the  bernstein  basis expansion 
this allows us to prove the following simple but interesting result about bernstein positivity 
proposition     consider any   na      and any polynomial p on a   then 
pba   v    a   p  v    a  
proof of proposition     first  assume that pba   v    a   so there is some natural n 
n
a
deg p  such that bn m
pba       then it follows from proposition    that also bp      and
therefore p  v    a  
assume  conversely  that p  v    a   so there is some n  deg p  such that bnp      then
 
a
it follows from proposition    that also bn m
pba       and therefore pba   v  a  

appendix c  the dirichlet distribution
the density dia     of the dirichlet distribution with hyperparameter   ra
   is given by 
dia        


 a  
xx   for all   int a   
 
 
x
xa
xa

and for any polynomial p on a we define the corresponding expectation as   


 a  
p   
dia  p     
xx   d 
 
 
x
a
xa
xa

in particular 


 

  


 a  
xx   d
 x  
a
xa
xa
xa
   
    

n
 a  
 mx   x  
 
n
 
x  mx    
 
  n   a  
 x  
a  n  

dia  ba      

n


xmx 

    

xa

xa

using the ascending factorial  r       r 
                      r      with   r and
r  n   
the dirichlet distribution can be used as a prior in combination with a multinomial
likelihood  leading to the so called dirichlet multinomial distribution  which can be described
as follows  the probability of observing  a sample with n    observations with  count vector
  na      in a multinomial process with dirichlet prior density dia     is given by 

n
dimna         
comnna       dia     d
a

    the integrals in this section can be interpreted as multiple riemann integrals 

  

ficoherent predictive inference under exchangeability


ba     dia     d   dia  ba     

 
a

where the second equality follows from equation       therefore  more generally  if we take
the expansion of the polynomial p in bernstein basis polynomials of degree n  deg p  
dia  p    



bnp    dia  ba      

n
na

  dimnna



bnp    dimnna  i     

n
na

  
n
na

  

n
bp   i      dimnna  bnp    

which is the dirichlet multinomial expectation of the count gamble bnp   this is the general
and useful relationship between the dirichlet expectation of a polynomial p  and the dirichlet
multinomial expectation of its bernstein expansion bnp   although these expectations are
strictly speaking only defined for   ra
     we can extend their definition continuously to
elements  of ra
 
   
by
taking
appropriate
limits  as equation      indicates 

c   special properties of the dirichlet distribution
we now recall a few interesting properties of the dirichlet distribution  we begin with the
updating property 
proposition     updating   for any category set a  any polynomial p  v  a   any count
vector   na      and any   ra
    
dia  pba       dia  ba     dia  p      
proof of proposition    

dia  pba      
p  ba     dia     d
a
 
 


ma  mx  a  
xx   d
 
p  
x 
 
 

x
a
xa
xa
xa
 
 


 mx   x  
ma
 a  
 
p   dia        d
  ma   a  
 x  
a
xa

  dia  ba     dia  p      
where the last equality follows from equation      
next  we turn to the so called renaming property 
proposition     renaming   for any category sets a and c such that there is some bijective
 one to one and onto  map    a  c  any polynomial p  v  c  and any   ra
    
dia  p  r      dic  p r     
  

fide cooman  de bock    diniz

proof of proposition     due to the linear nature of the dirichlet expectation  it clearly
suffices to prove the property for the bernstein basis polynomials p   bc    where  
nc       observe that r is a bijection too  then  using equation       if we let     r   
and     r      so z      z  and mz   n   z  for all z  c  and a   c and
na   mc   we get 
 
 
  mz   z  
mc
 c  
dic  bc   r       dic  bc      
  mc   c  
 z  
zc
   
  n   z       z   
 a  
na
 
  na   a  
    z   
zc
   
  nx   x  
 a  
na
 
  dia  ba     
  na   a  
 x  
xa

and if we take into account that for all   int a   
 bc   r       bc   r    
 
 
 
 
 
 
mc  mz
mc 
mc  n   z 
mz
 
   z 
   z   
r   z  



zc
zc
zc
    
na
xnx   ba     
 

xa

we see that indeed dic  bc   r       dia  bc   r    
the so called pooling property generalises the renaming property 
proposition     pooling   for any category sets a and d such that there is some onto
map    a  d  any polynomial p  v  d  and any   ra
    
dia  p  r      did  p r     
proof of proposition     due to the linear nature of the dirichlet expectation  it again suffices
to prove the property for the bernstein basis polynomials p   bd    where   nd      
also  if we take into account the renaming property of proposition     it is enough to consider
the following special case  where we have some non empty set do and different categories b 
c and d not belonging to it  let a    do   b  c  and d    do   d   and define  by letting
 x     x if x  do and  b     c     d 
then on the one hand  taking into account equation       letting     r     
 
 
  mz   z  
md
 d  
did  bd   r       did  bd      
  md   d  
 z  
zd
 
 
md
 d  
 md   d     mz   z  
 
 
    
  md   d    d  
 z  
zdo

on the other hand 

  

ficoherent predictive inference under exchangeability

dia  bd   r   
  
 
 
  
md
md
mz
 b   c  
z
dia     d
 

a
zdo
 
 


 a  
md

 
 b   c  md bb   cc  
zmz  z   d

xa  x   a
zdo
 
 
 
 

m
d
 md

 a  
md

bk b   cmd k c  
zmz  z   d
 
k
 
 

x
a
xa
zdo
k  

 
 
 
 
m
d

 a  
md  k   b   md  k   c   zdo  mz   z  
md

 
 

k
 md   a  
xa  x  
k  

so  if we compare both results and recall that d   a   z   z for z  do and d   b   c  
we see that we must prove that 
 
md  

 
md
 md   b   c  
 
 k   b   md  k   c  
 b   c  
 b   c  
k
k  

or equivalently  using ascending factorials 
 b   c   md    

 
md  

md
b  k  c  md k   
k

    

k  

so we see that proving the pooling property is essentially equivalent to proving equation      
which is the binomial theorem for ascending factorials  this is a well known result  and it
follows from the fact that ascending factorials are sheffer sequences of binomial type  sheffer 
       for completeness  we give a proof for it here  which is now very easy  because we have
just shown that it will hold if we can prove the pooling property in the particular case that
do    a   where a is a category different from b  c and d  so a    a  b  c  and d    a  d  
and in this case we can rewrite equation      as 
did  bd   r    
 
 
ma   md
 a   b   c  
 md   b   c    ma   a  
 
ma
 ma   md   a   b   c    b   c  
 a  
whereas
dia  bd   r     
where we let
     
i   
 
  

   

 

   

 

 a

a  md bb     

a  md ama  a  

  

 
 
ma   md  a   b   c  
i
ma
 a   b   c  

 a 

 a

b  c   ama  a   db

bb     

 a  b  

c  

 
da

 

db da
    
 
  
md  b  c   ma  a  
b  
c  
 
    a  
a
t
    t 
dt da
 

 

 

 

  

fide cooman  de bock    diniz

  b ma   a   md   b   c  b b   c    

 ma   a   md   b   c    b   c  
 
 ma   md   a   b   c    b   c  

using the well known evaluation of the beta function in terms of gamma functions 
finally  we look at properties related to restriction 
proposition     restriction   for any category sets a and b such that b  a  any
polynomial p  v  b   any   ra
   and any r  n   
dia  irb a  p     

 deg p    r   b  
 a  
dib  p rb     
 deg p    r   a  
 b  

proof of proposition     let n    deg p    r  then due to the linearity of the dirichlet
expectation operator  and equations      and      

dia  irb a  p     
bnp    dia  ba ia      
n
nb



   
n
 a  
xb  nx   x  
xa b  x  


 
  n   a  
n
xa b  x  
xb  x  
nb
   

n
 a     nx   x  
 
bnp   
  n   a  
 x  
n


bnp   

nb

 


n
nb

 

xb

bnp   

 a    n   b  
dib  bb   rb    
 n   a    b  

 a    n   b  
dib  p rb     
 n   a    b  

concluding the proof 

appendix d  the original idmm inference system by walley and
bernard
the idmm inference system sidm   as we introduced it in section     differs from the one
originally proposed by walley and bernard           in this appendix  we discuss the original
idmm inference system  which we denote by soi   explain how it is related to ours  and
illustrate some of the advantages our version has over the one by walley and bernard 
d   defining the original idmm inference system
for any s  r     and any category set a  consider the following set of polynomials 
s
    p  v  a       ksa   dia  p       
hoi a

   p  v  a       int a    dia  p s        
    strictly speaking  walley and bernard did not propose an inference system in our sense  but rather a
collection of prior and posterior predictive lower previsions for each category set a  the inference system
we call the original idmm inference system is one that produces these predictive lower previsions 

  

ficoherent predictive inference under exchangeability

for reasons that should become clear shortly  we call the inference system soi defined by
s
soi  a     hoi a
for all category sets a 

the original idmm inference system with hyperparameter s      updating is done in much
  na      
the same way as for the inference system sidm in section     for any 
s
    p  v  a       int a    dia  p 
   s        
hoi a
c

and this should be compared with equation       we leave it as an exercise to the reader to
check that soi is coherent and representation insensitive    however  as illustrated by the
counterexample in section d    soi is not specific 
the predictive models of soi are easily derived by mimicking the approach used in
section    to derive the predictive models of sidm   see equations      and       for any
  nan  
n  n    n  n and any 
 
 
s n
   f  l an        int a    dia  mnna  f   
   s       
doi a
c

    

  
p s n
oi a  f   

    

and
inf
int a  

   s  for all gambles f on an  
dia  mnna  f   

the latter expression motivates why we refer to soi as the original idmm inference system 
its predictive lower previsions coincide with those proposed by walley and bernard        
using equation      for n      and mimicking the argument in the proof of equation      in
appendix e    we see that
s  
  
doi a
c

 
f  l a    f   

 
  
s  
 for all 
  na      
f  x mx   didm a
c
s
xa

this tells us that the idmm and the original idmm have the same immediate prediction
models  the corresponding immediate predictive lower previsions for the original idmm are
well known and are of course identical to the ones produced by our version of the idmm
inference system  as given by equation       however  as the examples in the next section
illustrate  this equality does not extend beyond immediate prediction  the idmm and the
original idmm are different coherent inference systems  which leads us to the general and
important conclusion that coherent inference systems are not completely determined by their
immediate prediction models 
nevertheless  both approaches are closely related  by comparing equations      and      
  nan
we see that for any n  n    n  n and any 
 

 n
   inf  p soi a
 for all gambles f on an  
p s n
 f   
idm a  f   
  s  s

    the proof is very similar to the one for sidm  see theorem     

  

    

fide cooman  de bock    diniz

d   the original idmm inference system is not monotone in s
the hyperparameter s of the original idmm inference system is usually interpreted as
a degree of caution  higher values of s are often claimed to produce inferences that are
more cautious and less informative  the following quote from walley and bernard       
section      makes this explicit 
if b is any event concerning future observations  the idmm s  produces intervals
of posterior probabilities  p  b    p  b    which are nested and become wider as
s increases  this means that the inferences produced by two idmms with different
values of s are always consistent with each other  and the effect of increasing s is
simply to make inferences more cautious and less informative 
similar statements can be found in related papers by walley        section      and bernard
       section       although this is indeed true for many inferences  including many important
onesfor example  the immediate predictions  it does not hold for any event concerning
future observations  as illustrated by the following example  where the lower probability of
an event concerning two future observations is shown to initially increase with s 
example    consider a situation where the possibility space a consists of two elements
only  say heads  h  and tails  t    each of which has been observed once  so n     and
    mh   mt             we are interested in the predictive lower probability that during the

next two trials  heads and tails will each be observed once  so n     and we are looking for the
     h  t     t  h    with 
    mh   mt            
predictive lower probability of the event   
for the original idmm inference system  the following formula provides a closed form
expression 
  
p s n
   
oi a  i  

   s    inf dia  ba 
   s 
dia  mnna  i  
  
   
int a  
    
 
n
 mx   stx   mx  
  inf
 n 


int a    n   s 
xa
inf

int a  

  inf

  t  

      st      s    t  
      s 
 
 
     s      s 
     s      s 

 initially increases with s  see also figure   
we conclude that p s n
   
oi a  i  

    


for our version of the idmm inference system  the statement made in the aforementioned
quote does hold for any event concerning future observations  this follows trivially from
equation       we illustrate this in our next example 
example    consider again the problem in example    this time  we solve it using our version
of the idmm  the result is also depicted in figure    as a function of the hyperparameter s 
s n
 p idm a
 is a non increasing function of s  indeed 
in contrast with p s n
 i  
    
   
oi a  i  

   n
 

  
 i  
if     s    
p soi a

   
slim
   
 
s n
  
p idm a  i  
   

      s 

p s n
  
if s   
   
oi a  i  
     s      s 
is the closed form expression we find by combining equations      and      
  



ficoherent predictive inference under exchangeability

    


p s n
   
oi a  i  

    
    


p s n
   
idm a  i  

   
    
s
 

   

 

   

 

figure    lower probability of observing two different outcomes during the next two experiments  given that the possibility space consists of two categories  each of which
has already been observed once  solutions according to soi  solid line  and sidm
 dashed and solid line   see examples   and   for more information 

clearly  the inferences for soi and sidm can differ  it suffices to compare the results in
examples   and    see figure   as well  therefore  it seems clear that walleys        p     
statement that          s can be allowed to vary between   and s  and this produces exactly
the same inferences as the idm with s   s  or equivalently  that soi and sidm produce the
same inferences  should be taken to apply to immediate prediction only 
d   the original idmm inference system is not specific
as announced in theorem     our version of the idmm inference system is specific  we now
show that  at least for some values of the hyperparameter s  this is not true for the original
version 
  nbn   then for all a  b and all f  l b n   
consider any n  n    n  n and any 
s n
     int a    dia  mnna  f ib n   ia   
   s     
f ib n  doi a
cia   

   srb         
    int a    dib  mnnb  f   
where the last equivalence is a consequence of propositions    and    and the fact that
    
 if in particular b  a  it is not hard to see that sb    srb        int a    
rb  ia    
which implies that 
s n
s n
     sb   dib  mnnb  f   
          f  didm b

f ib n  doi a
cia   
c 

and therefore also 

 b n     inf s dib  mnnb  f   
       p s n
p s n
oi a  f  ia    
idm b  f    
b

  

fide cooman  de bock    diniz

on the other hand  due to equation  sp    if soi were specific  we would have that 
 b n     p s n


  p s n
p s n
oi a  f  ia    
oi b  f  rb  ia     
oi b  f    
 and p s n
 to
hence  in order for soi to be specific  it is necessary for p s n
oi b    
idm b    
coincide  as illustrated by the examples in the previous section  this is not necessarily the
case  therefore  soi is not always specific  in the counterexample we have provided  the
difference occurs for s     only  whereas in practice  s is usually chosen to be either   or
   walley   bernard        section       it would be interesting to see whether similar
counterexamples can be constructed for s    
that the original idmm inference systems are not specific  apparently contradicts theorem    by de cooman et al       a   which seems to state that they are  but in fact  what
that theorem states is that the original idmm immediate prediction models satisfy a weaker
specificity condition  tailored to immediate prediction only  since the immediate prediction
models for the original idmm and the idmm coincide  there is no contradiction 

appendix e  proofs and additional results that are more technical
e   proofs of results in section  
proof of theorem    for the sake of notational simplicity  we use the intuitive notation f  xk  
for extnk  f    we give the proof for the most general definition  in terms of sets of desirable
gambles  the proof for lower previsions then follows immediately 
consider any category set a  any n  n  any    k  n and any gamble f on a such
n we may assume without loss of generality that a is not a singleton  this
that f  xk    da
already implies that f      by coherence  d    hence in particular f      and max f     
assume ex absurdo that f       then there must be some a  a for which f  a       define
the gamble g on a by letting g a     f  a  and g x     max f     for all x  a    a   then
g  f and therefore g xk    f  xk    which implies  by coherence  use d  and d    that also
n   if we now let     max f  f  a      and     f  a        and define the
g xk    da
n   because      
gamble h    g       ia  a    then also  by coherence  d    h xk    da
now consider any natural number n     then it follows from repeatedly applying pooling
n
and renaming invariance in an appropriate manner that    i a     zk    d a
  where
       an  
zk is any variable that assumes the value a  when xk    a and that assumes some value in
 a            an   when xk   a  by repeatedly applying category permutation invariance  we find
n
that    i a     zk    d a
for all                n    coherence  d   then tells us that
       an  
n
n
n                i a     zk     d a
  this leads to a contradiction with coherence
       an  
 d   if we choose n large enough 
e   proofs of results in section  
proposition     for all n  n and   an         r       
proof of proposition     consider any z  d  then
tz        k              n     xk     z    


ya    y  z

  

  k              n    xk   y  

ficoherent predictive inference under exchangeability



 

ty      r      z  

ya    y  z

concluding the proof 
lemma     for all n  n  all   nan and all   dn  

 
 
i       
i
   
  
 r      r    
  


proof of lemma     consider the map m   dn  r defined by m       i     then
for any permutation  of the index set             n  and any   dn   we see that


m     
i       
i         
  

  



 



i       

  

i        m    

  

which tells us that m is permutation invariant and thereforeconstant on the atoms
      ndn   this means that  with obvious notations  m   n n m   i     now
d
m        implies that there is some      such that      and therefore  by
proposition                 r         r    and therefore    r      this tells
us that m        unless    r    and therefore m   m  r    i r       now if
we plug f      into equation       we see that


    
m     
m  r    i r          m  r     r     
dn

dn

lemma     for all n  n and all   ndn  


bd   r  

ba 

n   r    
na


proof of lemma     for any  in a   we have that
       
 nz
n
 bd   r      
x

zd x    z  
    
   

n
nz
 
z


nz
z
zd  n

   
n
 

 

    z  

  


n   r    
na



n 
na

r    

 

n


  

xa

z

xmx

x    z  

xmx

    

xa

  

concluding the proof 



xmx  

zd

 

nz

     z  


n 
na

r    

ba     

fide cooman  de bock    diniz

this lemma allows us to prove two related propositions 
proposition     for all n  n and all gambles f on dn   mnna  f      mnnd  f    r  
proof of proposition     first of all  we have for any count vector  in nan that
hyna  f      


 
 
 
f     
i     f   
  
  
n
   d

  



 

f   

dn

i     

    

  

 
i
  
f   
 r      r    
n



 

d

 

 
  



 
 r    



f      hynd  f  r     

 r    

where the fourth equality follows from lemma     therefore indeed 


mnna  f     
hyna  f    ba   
hynd  f  r    ba 
n
na

 



n
na

hynd  f   

n
nd

 





ba 

n   r    
na


hynd  f    bd   r     mnnd  f    r  

n
nd

where the fourth equality now follows from lemma    
proposition     for all polynomials p on d and all n  n  such that n  deg p  
bnpr   bnp  r  
proof of proposition     we find after expanding p in the appropriate bernstein basis 
  
 

n
p  r  
bp   bd   r  
bnp    bd   r  
n
nd

 



bnp   

n
nd

 



n
nd



ba   





bnp  r    ba 

n
n   r    
nd
na


n   r    
na


 bnp  r    ba   

n
na

where the third equality follows from lemma     the desired result now follows from the
uniqueness of an expansion in a  bernstein  basis 
proof of theorem    fix any category sets a and d such that there is an onto map    a  d 
  an and any gamble f on dn   we use the notation ha     a  and
any n  n  n  any 
  

ficoherent predictive inference under exchangeability

hd     d   and we transform condition  ri   using the equivalence in condition       on
        

the one hand  letting 
n
f    da
 mnna  f     ha  mnnd  f    r  ha
n
n
n
  ba 
f    da
c
 mna  f     ha  ba 
  mnd  f    r    ha  

where the second equivalences follow from proposition     on the other hand  recalling that
   r      
   r   
 by proposition    
   
n
f  dd
 mnnd  f    hd
n
n
  bd r   
f  dd
c
 mnd  f    hd  

this tells us that the equivalences in condition  ri   can be rewritten as 
mnnd  f    r  ha  mnnd  f    hd
n
n
ba 
  mnd  f    r    ha  bd r   
 mnd  f    hd  

the proof is complete if we observe  and recall from the discussion in section     and
appendix b  that by varying n  n and f  l dn    we can let p    mnnd  f   range over all
  an   we can let 
       
 range
polynomials on d   and that by varying n  n and 
over all count vectors in na  
proof of theorem    let  for ease of notation     inf ii i   then  is coherent using equation       consider any category sets a and d such that there is an onto map    a  d 
any p  v  d  and any   na       then  using the representation insensitivity of the
coherent i and theorem   
 p  r  ba    a    i  i  p  r  ba   i  a 
  i  i pbd r     i  d   pbd r      d  
and this concludes the proof 
proposition     for all   an     b     rb       
proof of proposition     immediate  since b is a sample whose components all belong to
b  and for each category in b  the number of times it occurs in b is exactly the same as
the number of times it occurs in  
proof of proposition    consider any   b   and let  for simplicity of notation    ia    
then since for any   nbn   with n    deg p    r
 
  
    
n
n
ia   x
ba ia        
x
 
nx x   bb     
ia   

xa

xb

we see that indeed 
irb a  p    



bnp   ba ia        

n
nb


n
nb

  

bnp   bb       p   

fide cooman  de bock    diniz

proof of proposition     when deg p    r      then r     and p   c  r  and trivially
irb a  p     i b a  c      c  so let us assume that deg p    r      first of all  observe that
deg p  r

for all   nb

and all   a  
 
 
 
 
deg p    r  ia   x
deg p    r  nx
ba ia        
x
 
x
ia   

xa
xb
 
deg p  r
b
bb     
if b    
b 
 
 
otherwise 

    

it therefore already follows from condition      that irb a  p       if b      let us therefore
assume that b      then condition      and equation      tell us that 

irb a  p    
bdeg p  r
  ba ia      
p
deg p  r

nb

deg p  r



 

bdeg p  r
  b
p

bb     
b 

deg p  r

nb

deg p  r

deg p  r



  b

bdeg p  r
  bb     
p
b     b

p   
b   

deg p  r

nb

which concludes the proof 
proposition     for all n  n and all gambles f on b n  
mnna  f ib n     irb a  mnnb  f     where r    n  deg mnnb  f    
proof of proposition     first of all  we have for any count vector  in nan thatwith some
slight abuse of notation 
hyna  f ib n     


 
 
 f ib n      
  
  
  



f   

  b n

is zero unless    ia    for some   nbn   in that case  since then obviously         
and    ia      b n      again with some slight abuse of notation 
hyna  f ib n  ia      

  
f      hynb  f    
  
  

therefore  if we recall condition      


hynb  f   ba ia   
mnna  f ib n    
hyna  f ib n  ia    ba ia     
n
nb

n
nb

  irb a  mnnb  f    
where r    n  deg mnnb  f    
  

ficoherent predictive inference under exchangeability

proof of theorem     fix any category sets a and b such that b  a  any n  n  n  any
  an and any gamble f on b n   we use the notation ha     a  and hb     b   and

we transform condition  sp   using the equivalence in condition       on the one hand 
       
 and r    n  deg mnnb  f    
letting 
n
f ib n  da
 mnna  f ib n    ha  irb a  mnnb  f     ha
n
n
r
n
  ba 
f ib n  da
c
 mna  f ib n    ha  ba 
 ib a  mnb  f     ha  

where the second equivalences follow from proposition     on the other hand  recalling that
 b     rb      
   rb   
 by proposition    
  
n
f  db
 mnnb  f    hb
n
n
 b  bb rb   
f  db
c
 mnb  f    hb  

this tells us that the equivalences in condition  sp   can be rewritten as 
irb a  mnnb  f     ha  mnnb  f    hb
r
n
n
ba 
 ib a  mnb  f     ha  bb rb   
 mnb  f    hb  

the proof is complete if we recall from the discussion in section     and appendix b that
by varying n  n and f  l b n    we can let p    mnnb  f     comnnb  hynb  f    range over
all polynomials on b and r   n  deg mnnb  f    range over all elements of n    and that by
  an   we can let 
       
 range over all count vectors in na  
varying n  n and 
proof of theorem     let  for ease of notation     inf ii i   then  is coherent using
equation       consider any category sets a and b such that b  a  any p  v  b   any
  na      and any r  n    then  using the specificity of the i  
irb a  p ba    a    i  i irb a  p ba   i  a 
  i  i pbb rb     i  b   pbb rb      b  
which concludes the proof 
e   proofs of results in section  
proof of proposition     for sufficiency  fix a category set a  a gamble g on a  and a count
vector   na       condition  ri   with d    g a       g  f    idd yields condition  ri   
for necessity  fix category sets a and d such that there is an onto map    a  d  a
gamble f on d  and a count vector   na       observe that  f    a    f  d  and that
for all r  f  d 



rf    r  
mx  
mx
xa    f   x  r

zd   f  z  r xa    x  z



 

zd   f  z  r

  

r   z   rf  r    r  

fide cooman  de bock    diniz

so rf    rf  r   we now infer by invoking condition  ri   twice that 
 
 
f    da
c  id f   a   d f
  a  crf    
 
 idf  d   df   d  crf  r      f  dd
cr    

concluding the proof 
proof of theorem     the arguments in this proof rely heavily on the following expression
for the lower probability function 
 
 
 
 n  k    sup   r   i a     d a b 
c k  n  k 
 
 
  sup   r   ak bnk  a       a  b  
    
and other related expressions that are equivalent to it by representation insensitivity and
bernstein coherence  b    both expressions follow from equations      and       bernstein
coherence  b   and representation insensitivity in its form  ri   
l   immediate from bernstein coherence and the fact that  n  k  is a lower probability 
use equation       b  and b  
l   fix any non negative integers n  k and   such that k      n  consider any real
    n  k  and     n      then it follows from applying equation      and condition  ri  
that both xk y  znk   x       x  y  z   and xk y  znk   y       x  y  z    whence 
by bernstein coherence  b    xk y  znk    x   y              x  y  z    applying
equation      and condition  ri   again tells us that uk   znk   u            u  z   
whence       n  k      
l   l  and l  are immediate consequences of l  and l  
l   consider the category set a     a  b  and the count vector  with ma    k and
mb    n  k  define the gamble g on a by g a      n      k      and g b      n      k  
then g a   g b  by l   and therefore the coherence  p  and p   of the predictive lower
prevision p  a     tells us that p  a  g     g b     g a   g b  p  a   a       n      k   
 n  k   n      k        n      k    see also equation        so it clearly suffices to prove
that p  a  g     n  k    p  a   a     consider any    p  a  g    then it follows using
equation      that 
ak bnk  g a a   g b b      a  
    
also  for any       both ak   bnk  a  g a        a  and ak bn  k  a  g b        a  
and therefore  by coherence  b    and recalling that a   b     
 a    ak   bnk  a  g a        ak bn  k  a  g b     
  ak bnk  a  g a a  g b b          
combining statements      and      using coherence  b    this leads to ak bnk  a       
 a   whence  n  k       and this completes the proof 
l   use l  and l  to find that  n  k   n      k        n      k       and then use l  
l   that sn    follows from l   so we only need to prove that sn    sn   or equivalently 
that  n       n              n       indeed 
 n       n            n      n           n         
  

ficoherent predictive inference under exchangeability

  n            n       n           n         
   n            n     n         
where the first inequality follows from l  with k      and the second from l  and l  
l   the inequalities hold trivially for n      due to l   so consider any n  n  and
category sets a     x  y  and b     x    x            xn   y   let         and            
since            we see that x  x      a   or equivalently  x  x        y     a  
since x   y      representation insensitivity  use equation      and condition  ri    then
tells us that xk  xk     
   y      xk   y    and specificity  use theorem     allows us
to infer from this that   nk   xk
  xk      
n y     b   for all k              n   now
n
infer from coherence  b   that   k   xk    k   xk        ny     b   and apply
representation insensitivity to get to xn  x        ny     a   since y      x   this
n
is equivalent to xn  x        n   n    a   this shows that  n  n     n
  using
equation       the rest of the proof is now immediate 
e   proofs of results in section  
proof of theorem     that v is coherent is obvious  because for each category set a  f 
v  a    v    a  is a bernstein coherent set of polynomials on a  
to prove representation insensitivity  we use theorem    consider any category sets a
and d such that there is an onto map    a  d  any p  v  d  and any   na      
then indeed
 p  r  ba   v    a   p  r  v    a   p  v    d   pbd r     v    d  
where the first and last equivalences follow from proposition     and the second one from
lemma    with k   a 
e   proofs of results in section   
proof of theorem     that v is coherent is obvious  because for each category set a  f 
v  a    v     a  is obviously a convex cone that includes v    a   proposition     and does
not contain the zero polynomial  v     a  is therefore a bernstein coherent set of polynomials
on a  
to prove representation insensitivity  we use theorem    consider any category sets a
and d such that there is an onto map    a  d  any p  v  d  and any   na      
then indeed
 p  r  ba   v     a      int a   p r    ba        
    int a   p r        
    int d   p      
    int d   p  bd r            pbd r     v     d  
where the second and fourth equivalences follow from the bernstein positivity of the bernstein
basis polynomials and proposition     and the third one from lemma    with k   a 
  

fide cooman  de bock    diniz

to prove specificity  we use theorem     consider any category sets a and b such that
b  a  any p  v  b   any   na      and any r  n    then indeed 
irb a  p ba   v     a      int a   irb a  p  ba        
    int a   irb a  p      
    int b   p      
    int b   p  bb rb            pbb rb     v     b  
where the second and fourth equivalences follow from the bernstein positivity of the bernstein
basis polynomials and proposition     and the third one from lemma    with k   a 
e   proofs of results in section   
below  we use the convenient device of identifying  for any proper subset b of a  an element
 of b with the unique corresponding element    ia    of a whose components outside
b are zero 
 x  b x   x and  x  a   b x     
also observe that  using this convention  we can identify int a     with a subset of a   and
then characterise it as follows 
for any   a     int a       x  a  x      mx      
proof of proposition     it clearly suffices to prove that v    a   hsc a and   
  hsc a  
the first statement is easy to prove because asc a trivially includes all non constant
bernstein basis polynomials  by proposition     since v    a  consists of finite  strictly positive
linear combinations of these non constant bernstein basis polynomials  we immediately have
that v    a   hsc a  
to prove the second statement  suppose ex absurdo that    hsc a   this implies that
  
there are finitely many nk      count vectors k in nank and pk  v 
 a  such that
k 

    k pk ba k   it is always possible to find  at least  one such count vector    say 
for which a k     a     for all k  in other words  we have either a k     a     or
a k     a          now consider any   int a        if a k     a          then
  
ba k         if a k     a      then ba k         and moreover  since pk  v 
 a  
k 

pk         hence     k pk   ba k         a contradiction 
lemma     consider any   na      and p  hsc a    so there are    n  nk  n 

  
such that ma   nk      k  nank and pk  v  
 a  such that p    k   pk ba k  
k 
then
sa   p     k  a   a    k    k for some k                 
and therefore
min sa   p    min  a    k     k                   
proof of lemma     the second statement is trivial  given the first  so we restrict our
attention to proving the first statement 
assume first that a    r    k  a for some r                  then clearly k     
since ma   nr      we may assume without loss of generality that a    r   is a minimal
  

ficoherent predictive inference under exchangeability

element of the set  a    k     k                   consider any   int a  r      whence
also   k   now for all k                 such that a    k     a    r  and there
  
clearly is at least one such kwe see that both pk        since pk  v  
 a   and
k 
ba k         whence  pk ba k          for all other k we must have that a    k    
a    r        and therefore  pk ba k         since ba k         this guarantees that

p      k    pk ba k          whence indeed k  sa   p   since we already know that
  k   a    a    r    k and k     
assume  conversely  that k  sa   p   which implies that     k  a and a    k 
and that there is some   k such that p         observe that a    k     a  a k   
and assume ex absurdo that a    k     k and therefore a k     k for all k                 
fix any k                  then there is some x  a 
  k  and therefore x     
 k   such that x 
whence ba k         this shows that p      k    pk ba k          a contradiction 
lemma     consider any   na      any p  v  a  and any n  n such that n  deg p  
then for all   nan  
bnp           k  min sa   p  k   a    a   
proof of lemma     fix any  in nan   we prove the contraposition  so suppose that for
all k in min sa   p   we have that k   a     a   and therefore k   a       since
a        a    a    hence  a      
  sa   p   since moreover     a      and
a    a       we infer from equation      that p b      where we let  for ease of
notation  b    a       we can rewrite this as  see also lemma     
    p b  


n
na

bnp   ba   b  


n
nb

bnp   ba   b  



bnp   bb   

n
nb

due to the uniqueness of the bernstein expansion  this is only possible if bnp        for all
n
n
  na   
  this concludes the proof since  clearly    na   
 

proof of proposition     first  assume that p  hsc a    implying that p    k   pk ba k
  
for some    n  nk  n  such that ma   nk      k  nank and pk  v  
 a   it already
k 
follows from lemma    that p  
    and that min sa   p    min  a    k     k                  
consider now any k  min  a    k     k                  and any   int k    then for all
k  we have that either a    k     k or a    k     k      if a    k     kwhich
happens for at least one k  due to our choice of kthen pk        and ba k         if
a    k     k      then since a    k  a k     k  
    implying that ba k        
hence  p        since this holds for all   int k    we find that p k  v     k  
assume  conversely  that p  v  a      and 
that p k  v     k  for
 all k n min sa   p  
n
fix any n  n such that n  deg p   then p   n n bp   ba    m bp   ba    with
a
 
 
m      nan   bnp           since p       we infer from equation      that min sa   p     
 observe that a  sa   p    we know from lemma    that for any   m   there is at least
one k  min sa   p  such that k   a    a    let us just pick any of these k  and call
it k   now let  for any k  min sa   p   mk       m   k   k   then we have found
a way to divide m into disjoint subsets mk   one for every k  min sa   p  and some of
  

fide cooman  de bock    diniz


which may be empty  such that k   a    a   for all   mk   m   kmin sa   p  mk


and therefore p   kmin sa   p  mk bnp   ba   
now fix any k  min sa   p   then we construct a count vector k by letting  mk  x     
if x  k   a   and  mk  x      otherwise  notice that k  nank   with nk the number of
elements  k   a    in the set k   a    and therefore nk  n  consider any   mk   then
since  mk  x     implies that x  a   and therefore x     we see that for all   a  
ba         



xx     

xa  



xx  mk  x

xa  



x mk  x

xa  

   k   ba k   ba k    

n
         hence  we can rewrite
where  k           

 
k
k
mk bp   ba 

   mk  k   bnp   ba k   in this way  we find that p  
as
 pk ba k   where pk
kmin sa   p  ba k pk  
hence  if we fix any k  min sa   p       then we are left to prove that ma   nk    
  
and pk  v  
 a   assume first  ex absurdo  that ma   nk      then in particular
k 
  
k     which contradicts k  sa   p   so it remains to prove that pk  v  
 a  
k 
consider any   int a  k      then we can derive from k  min sa   p   sa   p 
that a    k  since a k     k  a    this implies that a    k     a  a k    
a     k   a      k  and therefore also   int k    for all k    min sa   p     k  
k    k  
   and therefore ba k           hence  p     ba k   pk     we know that
p       because p k  v     k  and that ba k        because a k     k   a    k 
we conclude that indeed pk        
lemma     for all   na      and p  v  a  
sa   p    sa    pba    and therefore min sa   p    min sa    pba    
proof of lemma     first  assume that k  sa   p   then     k  a  a    k and
p k       from this last inequality and the continuity of polynomials  we infer that there is
some   int k   such that p         since a    k  we find that p  ba          and
therefore  pba    k      
assume  conversely  that k  sa    pba     then   
  k  a and  pba    k       this
last inequality implies that there is some   k such that  pba           and therefore
both ba          and p         from ba           we derive that a    k and from
p         we derive that p k      
proof of proposition     by the way hsc a and hsc a  are constructed  see the defining
expressions      and        it clearly suffices to prove that hsc a c  hsc a    consider
therefore any p  v  a  such that pba   hsc a   which by proposition     implies that
pba       and that  pba    k  v     k  for all k  min sa    pba     we now set out
to prove that p  hsc a    applying proposition    again  and since  clearly  p       we
see that it suffices to show that p k  v     k  for all k  min sa   p   so consider any
k  min sa   p   then  by lemma     k  min sa    pba     so we have already argued
above that  pba    k  v     k   hence indeed also p k  v     k  
  

ficoherent predictive inference under exchangeability

 
proof of equation       combining equations      and       we see that  for any 
na      
 
    f  l a    sa  f    hsc a 
dsc a
c
    
   
also  for any f  l a  and any   
  k  a 
sa  f        f      sa  f   k  v     k   f  k     and sa  f   k      f  k          
      for any f  l a  
we start with the case 
min sa    sa  f        x    x  a and f  x         
 
because of statement       hence  by proposition    and equations      and       dsc a
 
l    a  
  na   for all f  l a  
next  we consider any 

 

 a   
if f  a  
   
  
min sa 
  sa  f     
   x    x  a   a  
 and f  x        if f  a  
 a  
   

    

because of equation       now recall proposition    and equations      and      and
 
 if and
consider two cases  f  a  
      and f  a  
      if f  a  
       then f  dsc a c
only  if f       which is redundant 
and f  a  
     or  equivalently  since f  a  
        if
 
 
 if and only if
f  h  l a    h a  
      l    a   if f  a  
      then f  dsc a c
 or  equivalently  since f  a  
f      and f  x     for  
all x  a   a  
       again if
f  h  l a    h a  
      l    a  
proof of equation       we start with the first part of equation       due to equation     
and proposition     it suffices to prove that  for any p  v  a   minxa p x        p 
hsc a   and minxa p x        p 
  hsc a    

first  assume that minxa p x        then there is some y  a for which p y       
hence  since p  y    p y        we find that  y   min sa    p  and therefore also that
p
  hsc a     by proposition    
next  assume that minxa p x        then p  x    p x       for all x  a  implying
that min sa    p      x    x  a  and therefore also  since p       that p  hsc a     by
proposition    
we now turn to the second part of equation       due to equation      and proposition    
  na and any p  v  a   mina  
it suffices to prove that  for any 
p        p 

hsc a 
p        p 
  hsc a 
 and mina  
 

first  assume that mina  
p  
 
  
then
there is some   int a  
   for which

  

 
p        implying that p a  
     and p a  

  v  a    
hence  we find that a  


min sa 
  hsc a 
  p  and therefore also that p 
   by proposition    

next  assume that mina  
p  
 
  
then
p a  
 
    and p a  
 v     a    



 and therefore also  since p       that p  hsc a 
hence  we find that min sa 
  p     a   
 
by proposition    
  

fide cooman  de bock    diniz

proof of equation       the first part of equation      is a trivial consequence of equa  na and any f  l a   then  combining
tion       for the second part  consider any 
equations      and      


   min
f  x x   min f  x  
p  sc a  f   
f  x x   min
a  


xa

a  



xa  


xa  

lemma     consider any category sets a and d such that there is an onto map    a  d 
any p  v  d  and any   
  k  a  then  p  r   k       p  k       
proof of lemma     first  assume that p  k        so there is some    k  such that
p         now choose any   k such that r        then  clearly   p  r      
p r       p        and therefore  p  r   k      
assume  conversely  that  pr   k       so there is some   k such that  pr          
if we let     r     then    k  and p     p r        p  r           hence 
p  k       
lemma     consider any category sets a and d such that there is an onto map    a  d 
any p  v  d   and any   na       then
 
 
sa   p  r     k  a   a    k and  k   sd r     p   
and therefore
 sa   p  r      sd r     p  and  min sa   p  r      min sd r     p  
proof of lemma     we start by proving the first statement  first  assume that k  sa   p
r    implying that   
  k  a  a    k and  p  r   k       then      k   d 
d r        a      k  and  by lemma     p  k        hence   k   sd r     p  
conversely  assume that k  a  a    k and  k   sd r     p   then   
   k   which
implies that   
  k  and also p  k        which  by lemma     implies that  p  r   k      
hence  k  sa   p  r   
the first statement implies that  sa   p  r     sd r     p  and therefore  in order
to prove the second statement  it suffices to show that sd r     p    sa   p  r    or 
equivalently  that for every l  sd r     p   there is some k  sa   p  r   such that
 k    l  so choose any l  sd r     p  and let k     x  a    x   l       l   then
 k    l because  is onto  and since  a      d r      l  it follows that a    k 
hence  by the first statement  k  sa   p  r   
to prove the third statement  first assume that k  min sa   p  r    implying that
k  sa   p  r   and that  for all k    sa   p  r    k     k  by the second statement 
 k   sd r     p   to prove that  k   min sd r     p   assume ex absurdo that there
is some l  sd r     p  such that l   k   let k       x  k    x   l    k     l  
then k    k and  k       l  and therefore  by lemma      p  r   k         because
k       and p l  
     since l  sd r     p   we see that  a      d r      l and
therefore a       l   since k  sa   p  r    we also know that a    k  and
therefore a    k     l    k     this tells us that k    sa   p  r    a contradiction 
assume  conversely  that l  min sd r     p   implying that l  sd r     p   then  by
  

ficoherent predictive inference under exchangeability

the second statement  there is some k    sa   p  r   such that  k       l  hence  there
is some k  min sa   p  r   such that k  k   and therefore  k    k       l  since
l  min sd r     p  and since  due to the second statement   k   sd r     p   we also
have that  k    l and therefore  k    l 
lemma     let   
  k  a and let p be any polynomial on a   then for any n  deg p  
bnp    bnp  nkn  
k

proof of lemma     it follows from

p    
bnp   ba     for all   a
n
na

that for all   k  
p k     


n
na

 





bnp   ba   ia      

bnp   ba   ia    

n   a  k
na

bnp  nkn   bk     

n
nk

and this completes the proof 
lemma     consider any category sets a and d such that there is an onto map    a  d 
any p  v  d  and any   
  k  a  then 
 i   p  r   k  v    k   p  k   v     k   
 ii   p  r   k  v     k   p  k   v      k   
proof of lemma     the first statement follows from the fact that  for all n  deg p  
bn pr   

k

n
     bn pr    nkn       bnp  r   nkn      bnp  n k 
     bnp 

    
 k 

where the first and last equivalence are due to lemma     the second equivalence follows
n    nn
from proposition     and the third equivalence holds because r  nk
 k   
we now turn to the second statement  where we have to prove that the following statements
are equivalent 
 a     int k   p r         
 b     int  k    p       
first assume that  a  holds  and consider any   int  k     we have to prove that p       
 
we construct a   k as follows 
 consider any z   k   for all x     z    k  choose
the x     in such a way that xk    x  z x   z   in this way  we have found a   k
satisfying r        and such that moreover x     for all x  k  whence   int k    we
now infer from  a  that indeed p     p r         
assume  conversely  that  b  holds  and consider any   int k    then  for any z  d 
r   z     if z   k  and r   z     otherwise  this means that r     int  k    and
we infer from  b  that indeed p r         
  

fide cooman  de bock    diniz

proposition     sc is representation insensitive 
proof of proposition     we use the characterisation of representation insensitivity in theorem    consider any category sets a and d such that there is an onto map    a  d 
any p  v  d  and any   na       then  by proposition     we need to prove that
p  r  hsc a   p  hsc d r     
first  assume that p  hsc d r      which  by proposition     implies that p      and
that p l  v     l  for all l  min sd r     p   applying lemma    with k   a  we infer
from p  
    that p  r       consider now any k  min sa   p  r    then  by lemma    
 k   min sd r     p   implying that  due to the assumption  p  k   v      k    since
k      we can apply lemma    to find that  p  r   k  v     k   hence  by proposition    
p  r  hsc a   
assume  conversely  that pr  hsc a    which  by proposition     implies that pr     
and that  p  r   k  v     k  for all k  min sa   p  r    applying lemma     with
k   a  we infer from p  r      that p       now  consider any l  min sd r     p   then
by lemma     there is some k  min sa   p  r   such that  k    l  since k  
   and 
by assumption   p  r   k  v     k   we infer from lemma    that p l  v     l   hence 
by proposition     p  hsc d r     
lemma     consider any category sets a and b such that b  a  any p  v  b   any
k  a such that k  b     and any r  n    then irb a  p  k       p kb      
proof of lemma     we may assume without loss of generality that r   deg p       as the
proof is trivial otherwise 
first  assume that p kb  
     which means that there is some   kb such that
p         then     ia     k   and we infer from proposition   that irb a  p     p       
and therefore irb a  p  k      
assume  conversely  that irb a  p  k       which means  due to the continuity of polynomials  that there is some   int k   such that irb a  p         we now infer from k  b    
 
that b      so proposition    guarantees that p   
b         since  b  kb   we find that
p kb      
lemma     consider any category sets a and b such that b  a  any p  v  b  any
r  n  such that r   deg p       and any   na       then
 
 
sa   irb a  p     k  a   a    k and k  b  sb rb     p   
and therefore
 
 
sb rb     p    k  b   k  sa   irb a  p  
and
 
 
min sb rb     p    k  b   k  min sa   irb a  p    
proof of lemma     we begin with the first statement  first  assume that k  sa   irb a  p  
and therefore that     k  a  a    k and irb a  p  k  
     then k  a implies that
kb  b  a    k implies that b rb       a  b  kb  moreover  irb a  p  k  
   
together with proposition    and r   deg p      implies that k  b      which in turn  by
lemma     implies that p kb       hence  k  b  sb rb     p   conversely  assume that
  

ficoherent predictive inference under exchangeability

k  a  a    k and k  b  sb rb     p   then k  b      implying that k      and
p kb       which  by lemma     implies that irb a  p  k       hence  k  sa   irb a  p   
in order to prove the second statement  it clearly suffices to show that sb rb     p  
 k  b   k  sa   irb a  p     since the converse inclusion follows directly from the first
statement  so consider any l  sb rb     p  and let k    l  a    then k  a  a    k
and k  b   l   a    b    l  b rb       l  hence  by the first statement  indeed
k  sa   irb a  p   
to prove the third statement  first assume that k  min sa   irb a  p    implying that
in particular k  sa   irb a  p    then  by the second statement  k  b  sb rb     p   to
prove that k  b  min sb rb     p   consider any l  sb rb     p  such that l  k  b  and
let k      l  a    then  by an argument identical to the one used in the proof of the second
statement  k    b   l and k    sa   irb a  p    however  since k    b   l  k  b and
k     b   a     b  k   b  we find that k      k    b    k     b    k  b    k   b    k 
and therefore k     k  by assumption  hence indeed l   k    b   k  b  assume 
conversely  that l  min sb rb     p   implying that l  sb rb     p   then  by the second
statement  there is some k    sa   irb a  p   such that k    b   l  so there is some
k  min sa   irb a  p   such that k  k   and therefore k  b  k    b   l  since
l  min sb rb     p  and  by the second statement  k  b  sb rb     p   we also have that
k  b   l 
lemma     consider any category sets a and b such that b  a  any p  v  b   any k  a
such that k  b     and any r  n    then irb a  p  k  v     k   p kb  v     k  b  
proof of lemma     we may assume without loss of generality that r   deg p       as the
proof is trivial otherwise  using proposition     and considering that  since k  b  
    b    
for any   int k    it then suffices to prove that the following statements are equivalent 
 a     int k   p   
b       
 b     int kb   p       
first assume that  a  holds  and consider any   int kb    we have to prove that p       
we construct
a   k as follows  for any x  k   b  choose x     in such a way that

    xk b x      which is always possible  and for any x  k b  let x       x     
then it follows from this construction that b              
b    and   int k    so we
infer from  a  that indeed p     p   
 
 
  
b
assume  conversely  that  b  holds  and consider any   int k    then b     because
 
k b  
    and therefore  for all z  b     
b  z      z  k  b  hence  b  int kb   
 
so we infer from  b  that p  b       
proposition     sc is specific 
proof of proposition     we use the characterisation of specificity in theorem     consider
any category sets a and b such that b  a  any p  v  b   any   na      and any r  n   
then  by proposition     we need to prove that irb a  p   hsc a   p  hsc b rb     
first  assume that p  hsc b rb      which  by proposition     implies that p      and
that p l  v     l  for all l  min sb rb     p   applying lemma    with k   a  we infer
from p      that irb a  p        consider any k  min sa   irb a  p    then by lemma    
  

fide cooman  de bock    diniz

k  b  min sb rb     p   implying that  due to the assumption  p kb  v     k  b  
since k  b       we can apply lemma    to find that irb a  p  k  v     k   hence  again
by proposition     irb a  p   hsc a   
assume  conversely  that irb a  p   hsc a    which  by proposition     implies that
r
ib a  p       and that irb a  p  k  v     k  for all k  min sa   irb a  p    from lemma   
with k   a  and from irb a  p        we infer that p       consider any l  min sb rb     p  
then  by lemma     there is some k  min sa   irb a  p   such that kb   l  since therefore
k  b      and since  by assumption  irb a  p  k  v     k   we infer from lemma    that
p l  v     l   hence  by proposition     p  hsc b rb     
proof of theorem     this is an immediate consequence of propositions     coherence     
 representation insensitivity  and     specificity  
e   proofs of results in section   
  na      and any p  v  a   then
proof of equation       consider any 
s
s
  ba  p  hidm a
p  hidm a
c
    sa   dia  ba  p      

        
    sa   dia  ba     dia  p 
         
    sa   dia  p 
where the third equivalence follows from the updating property of the dirichlet expectation
 proposition     
  na       then  combining equations     
proof of equation       consider any 
and      for n     
 
 

mx   x
s  
   f  l a       sa  
didm a
c
f  x 
    
ma   a
xa

now consider any f  l a   then for all   sa  

xa

f  x 



mx   x
x
  
  
f  x  mx   x       
 
f  x 
f  x mx  
ma   a
s
s
xa

xa

combining the equations above  and letting c      s
find that 

xa



xa f  x mx

s  
   s       s     int a   
f  didm a
c

for ease of notation  we

s  
f  x tx   c 
s

    

xa

if f  c  then there is some y  a for which f  y    c and therefore  by statement      
s  
  choose s  and ty close enough to s and    respectively   if f   c  then due to
f
  didm a
c
s  
 finally  let us
the definition of c  f   c      hence  again by statement       f 
  didm a
c 
see what happens 
if f   c  then clearly c     consider any s       
s  and any   int a   
 
 
then since f   c  xa f  x tx   c and therefore also  since c     ss xa f  x tx   ss c  c 
s  
 by statement      
hence f  didm a
c
  

ficoherent predictive inference under exchangeability

  na      and any f  l a   then by combining
proof of equation       consider any 
equations      and      

mx   x
mx   s  tx
  inf
inf
f  x 
a
ma   a s     s  int a  
ma   s 
xa
xa
 
 


 
s 
  inf
f  x mx  
inf
f  x tx
ma   s  int a  
s     s  ma   s 
xa
xa
 
 

 
s 
  inf
f  x mx  
min f
ma   s 
s     s  ma   s 
xa

s
 
f  x mx  
min f 
 
ma   s
ma   s

   inf s
p s  
idm a  f   



f  x 

xa

where the last equality follows from min f 

mx
xa f  x  ma   a



property of convex combinations 

proof of theorem     for coherence  if we fix any category set a  then we must prove that
s
hidm a
satisfies the requirements b b  of bernstein coherence  this is trivial from the
s
definition of hidm a
  the linearity of the dirichlet expectation operator  and the fact that
the dirichlet expectation of any bernstein basis polynomial is positive 
next  we turn to representation insensitivity  and use its characterisation in theorem   
consider any category sets a and d such that there is an onto map    a  d  any p  v  d 
and any   na       then  using the pooling property  proposition     of the dirichlet
expectation and equation       we find that indeed 
s
 p  r  ba   hidm a
    sa   dia  p  r          

    sa   did  p r           
s
    sd   did  p r             pbd r     hidm d
 

where the third equivalence follows from the equality sd   r  sa   
finally  we turn to specificity  and use its characterisation in theorem     consider
any category sets a and b such that b  a  any p  v  b   any   na      and any
r  n    then  using the restriction property  proposition     of the dirichlet expectation
and equation       we find that indeed 
s
irb a  p ba   hidm a
    sa   dia  irb a  p          

    sa   dib  p rb           
s
    sb   dib  p rb             pbb rb     hidm b
 

where the third equivalence follows from sb   rb  sa   
e   proofs of results in section   
s
lemma     for any p    p   hsi a
  sa    p    p      sa    p     sa    p    

  

fide cooman  de bock    diniz

proof of lemma     first  consider any k  sa    p    p     meaning that     k  a and
 p    p    k       assume  ex absurdo  that k 
  sa    p    and k 
  sa    p     then p   k    
and p   k     and therefore  p    p    k      which is a contradiction  hence indeed
k  sa    p     sa    p    
next  consider any k  sa    p     sa    p     implying that   
  k  a  then there is
at least one k    min sa    p     sa    p     such that k    k  and we can assume without
loss of generality that k    sa    p     since k    min sa    p     sa    p      we have that
l   k   for all l  sa    p     sa    p     and therefore k    min sa    p     this already tells us
s
 
that p   k    hidm k
    there are now two possibilities  the first one is that k  sa    p    
s
and then  in very much the same way as as above  we find that p   k    hidm k     hence 
s
s
due to the bernstein coherence  b   of hidm k
     p    p         p        p       hidm k    
k
k
k
 
 
the second possibility is that k 
  sa    p     and then p   k       since k      so we find 
s
here too  that  p    p    k     p   k     p   k     p   k    hidm k
    in both cases  therefore 
s
s
 p    p    k    hidm k     and the bernstein coherence  b   of hidm k
  allows us to conclude
     since k    k  we find that also  p    p    k      and therefore that
that  p    p    k    
k  sa    p    p    
s
s
proof of proposition     since   
  hsi a
  we are left to prove that v    a   hsi a
and that 
s
s
s
for all      and p  p    p   hsi a   p  hsi a and p    p   hsi a  
s
first  consider any      and p  hsi a
  then  clearly  sa    p    sa    p  and therefore
min sa    p    min sa    p   then for any k  min sa    p   we have that k  min sa    p  
s
s
which  since p  hsi a
  implies that p k  hidm k
and therefore  due to the bernstein
s
s
coherence of hidm k   that  p  k    p k    hidm k
  furthermore  since p      also
s
p       and therefore p  hsi a  
s
next  consider any p    p   hsi a
  then p       and p        implying that sa    p       
and sa    p         and therefore sa    p     sa    p         applying lemma     we find
that sa    p    p         so there is some k such that     k  a   p    p    k  
    and
therefore p    p        then for any k    min sa    p    p     or equivalently  due to lemma    
k    min sa    p     sa    p      then  by applying the same reasoning as in the second part
s
s
of the proof of lemma     we find that  p    p    k    hidm k
    hence  p    p   hsi a  
s
since we have already shown that hsi a is closed under taking positive linear combinations 
and since v    a  consists of positive linear combinations of bernstein basis polynomials  we
s
only need to show that hsi a
contains all bernstein basis polynomials in order to prove that
 
s
v  a   hsi a   so consider any   na       then  for any k such that     k  a  we
have that ba   k   bk rk    if a    k  and ba   k     otherwise  this implies that
s
sa    ba         
  k  a   a    k  and that  due to the bernstein coherence of hidm k
 
s
s
ba   k   bk rk     hidm k for all k  sa    ba     hence  ba   k  hidm k for all
s
k  min sa    ba     since also ba        we find that indeed ba   hsi a
 
s
s
proof of proposition     we first prove that hsi a
c  hsi a 
  consider any p  v  a 
s
s
such that pba   hsi a   meaning that pba       and that  pba    k  hidm k
for all
s
k  min sa    pba     we set out to prove that p  hsi a    since  clearly  p       it suffices to
s
show that p k  hidm k
crk    for all k  min sa   p   so consider any k  min sa   p  
implying that a    k and therefore also that k rk       a    we also infer from
s
lemma    that k  min sa    pba     which tells us that  pba    k  hidm k
  since
s
 pba    k   p k ba   k   p k bk rk      we find that p k  hidm k crk    

  

ficoherent predictive inference under exchangeability

s
s
s
next  we prove that hsi a 
 hsi a
c  consider any p  hsi a 
  meaning that
s
p      and p k  hidm k crk    for all k  min sa   p   we set out to prove that
s
s
pba   hsi a
or  equivalently  that pba   
for all
    and that  pba    k  hidm k
k  min sa    pba     since p       the continuity of polynomials guarantees that there is
some   int a   such that p        and therefore also  pba            so we know already
that pba        consider any k  min sa    pba     then  by lemma     k  min sa   p  
s
s
implying that a    k and p k  hidm k
crk    and therefore p k bk rk     hidm k
 
s
since moreover p k bk rk       pba    k   we find that indeed  pba    k  hidm k  

proof of equation       due to equation       it suffices to prove that  for any p  v  a  
s
s
minxa p x        p  hsi a
and minxa p x        p 
  hsi a
 

first  assume that minxa p x        then there is some y  a for which p y       
hence  since p  y    p y        we find that  y   min sa    p  and therefore also  due to
s
s
the bernstein coherence of hidm  y 
 see theorem      that p  y  
  hidm  y 
  from which
s
we infer that p 
  hsi a  
next  assume that minxa p x        then p  x    p x       for all x  a  implying
s
that min sa    p      x    x  a  and that  for all x  a  p  x   hidm  x 
  again because
s
s
of the bernstein coherence of hidm  x    hence  since p       we find that p  hsi a
 
proof of equations      and       equation      follows directly from equation       we
prove equation       due to equation      and proposition     it suffices to prove that  for
  na and any p  v  a  
any 
s
s
      p  hsi a 
   p
c p   
  hsi a 
 and c p   
 



where  for ease of notation  we let
   
c p   

inf

sa  


     
dia  
 ra  
  p a  
   


      implying that dia  
         for
first  assume that c p   
 ra  
  p a  
   

s
some   a  
     and  by equation       that p a  

 
 and therefore also that p a  


s


hidm a  
  p  and therefore also that
     hence  we find that a    min sa 
 cra  
s
p
  hsi a 
 


      implying that p a  
next  assume that c p   
     and  by equation       that

s

 and therefore
p a  

h
cr
 
  
hence 
we
find
that
min
s
  p     a   
a 

a  

idm a  

s
also that p  hsi a 
 
  na  
proof of equation       by combining equations      and       we see that  for any 
 
 
s  
s
   f  l a    sa  f    hsi a 
dsi a
c
    
  

consider now any f  l a  and distinguish between two cases  f  a  
      and f  a  
     
if f  a  
       and therefore also f        then
s  
s
  sa  f   a  
f  dsi a
c
 hidm a 
   
 cra  
 

s
 sa  
  f  a  
    hidm a  
   
 cra  

  

fide cooman  de bock    diniz

s  
 f  a  
  didm a  
   
 cra  

 
  
 f  a  
 

f
 x 
m

f
 

f  x mx or f     
x


 a  
s
s

xa  


xa  

where the first equivalence is due to statement      and equations            and       the
second equivalence follows from the definition of sa and sa  
 and the third one is due to
equations      and       the fourth equivalence is a consequence of equation      and the
final equivalence holds because f     is redundant  given that f  a  
      
if f  a  
      then  again  using statement      and equations            and      
s  
 if and only if f      and if for all x  a   a   

f  dsi a
c
s
f  x      or sa  f   a   x 
 hidm a 
cra   x 
   


  x 

s
since f  a  
cra   x 
   is bernstein coherent  theorem      the
     and hidm a   x 


latter statement is equivalent to f  x       hence  we find that 
s  
  f      and  x  a   a   f

f  dsi a
c
 x    

f   
 f  a  
  

  
f  x mx or f     
s

xa  

where the second and third equivalences are consequences of f  a  
     
lemma     consider any category sets a and d such that there is an onto map    a  d 
any p  v  d   any   na       and any     k  a such that a    k  then
s
s
 p  r   k  hidm k
crk     p  k   hidm  k 
cr k   r     
proof of lemma     let a    k  d     k        k and p    p  k    then  is
an onto map from a to d   p  v  d   and p  r   p  k   r k    p  r   k  
ma
since a    k  we can identify  with an element     rk    of nk
and therefore
the result follows from the representation insensitivity of the idmm inference system with
hyperparameter s  because then also r       r k  rk       r k   r     
s


s

p  r  hidm a
 c  p  hidm d  cr     

proposition     ssi is representation insensitive 
proof of proposition     we use the characterisation of representation insensitivity in theorem    consider any category sets a and d such that there is an onto map    a  d 
any p  v  d  and any   na       then  by proposition     we need to prove that
s
s
p  r  hsi a 
 p  hsi d r
 
   
s
s
first  assume that p  hsi d r      meaning that p      and p l  hidm l
crl  r    
for all l  min sd r     p   applying lemma    with k   a  we infer from p      that
p  r       consider any k  min sa   p  r    so     k  a and a    k  then 
by lemma      k   min sd r     p   implying that  due to the assumption  p  k  
  

ficoherent predictive inference under exchangeability

s
s
hidm  k 
cr k   r      applying lemma     we find that  p  r   k  hidm k
crk    
s
hence  p  r  hsi a   
s
assume  conversely  that p  r  hsi a 
  meaning that p  r      and that  p  r   k 
s
hidm k crk    for all k  min sa   p  r    applying lemma    with k   a  we infer
from p  r      that p       consider any l  min sd r     p   by lemma     there
is some k  min sa   p  r   such that  k    l  since     k  a  a    k
s
and  by the assumption   p  r   k  hidm k
crk     we infer from lemma    that
s
s
p l  hidm l crl  r      hence  p  hsi d r     

lemma     consider any category sets a and b such that b  a  any p  v  b   any
  na       any r  n  and any k  a such that k  b     and a    k  then
s
s
irb a  p  k  hidm k
crk     p kb  hidm kb
crkb    
proof of lemma     let a    k  b     k  b  p    p kb and r   deg p   deg p     r 
then b   a   p  v  b     r  r     r   deg p     r   deg p   and


bdeg p  r
  ba ia    k  
bdeg p  r
  ba ia    k
irb a  p  k  
p
p
deg p  r

deg p  r

nb

 



nb
b  k


bdeg p  r
  bk ik      irb   a  p   
p 
kb

deg p  r

nkb

where the third equality follows from the unicity of the bernstein expansion of a polynomial 
since a    k  we can identify  with an element     rk    of nk     and therefore
the result follows from the specificity of the idmm inference system with hyperparameter s 
because then also rb        rkb    


s


s

irb   a  p    hidm a
 c  p  hidm b  crb      

proposition     ssi is specific 
proof of proposition     we use the characterisation of specificity in theorem     consider
any category sets a and b such that b  a  any p  v  b   any   na      and any
s
s
r  n    then  by proposition     we need to prove that irb a  p   hsi a 
 p  hsi b r
 
b   
it is clear from propositions    and    that we can assume without loss of generality that
r   deg p      
s
s
first  assume that p  hsi b r
  implying that p  
    and that p l  hidm l
crbl   
b   
for all l  min sb rb     p   applying lemma    with k   a  we infer from p      that
irb a  p        consider any k  min sa   irb a  p    then we infer from lemma    that k 
s
crkb    
b  min sb rb     p   implying that  due to our assumption  p kb  hidm kb
r
s
since k  b      and a    k  ib a  p  k  hidm k crk    because of lemma     hence 
s
irb a  p   hsi a 
 
s
assume  conversely  that irb a  p   hsi a 
  which implies that irb a  p       and that
r
s
ib a  p  k  hidm k crk    for all k  min sa   irb a  p    applying lemma    with
k   a  we infer from irb a  p       that p       consider any l  min sb rb     p   by
lemma     there is some k  min sa   irb a  p   such that k  b   l  since k  b      
  

fide cooman  de bock    diniz

s
a    k and  by assumption  irb a  p  k  hidm k
crk     we infer from lemma   
s
s
that p kb  hidm kb crkb     or in other words  p l  hidm l
crbl     hence 
s
p  hsi b rb     

proof of theorem     this is an immediate consequence of propositions     coherence     
 representation insensitivity  and     specificity  
e   proofs of results in section   
proof of theorem     we begin with coherence  consider any category set a  then we have
s
to prove that hh a is bernstein coherent  for b   recall that   
  hidm a
for all s      and
 
s
therefore also   
  hh a   similarly  for b   recall that v  a   hidm a for all s      and
 
therefore also v  a   hh a   for b   consider n  n and k  r   and pk  hh a for
s
all k             
n   then there is some s     such that pk  hidm a
for all k              n  
n
s
and
n therefore k   k pk  hidm a   by bernstein coherence  theorem      hence indeed
k   k pk  hh a  
next  we turn to representation insensitivity  and use its characterisation in theorem   
consider any category sets a and d such that there is an onto map    a  d  any p  v  d 
and any   na       then we find that indeed 
s
 p  r  ba   hh a   s  r     p  r  ba   hidm a
s
  s  r    pbd r     hidm d
 pbd r     hh d  

where the second equivalence follows from the representation insensitivity of the idmm
inference systems  theorem     
finally  we turn to specificity  and use its characterisation in theorem     consider any
category sets a and b such that b  a  any p  v  b   any   na      and any r  n   
then we find that indeed 
s
irb a  p ba   hh a   s  r    irb a  p ba   hidm a
s
  s  r    pbb rb     hidm b
 pbb rb     hh b  

where the second equivalence follows from the specificity of idmm inference systems  theorem     
  na      and any p  v  a  
proof of equation       for any 
s
  pba 
p  hh a c
  hh a   s  r    pba 
  hidm a
s

  s  r    p  hidm a
c 

combined with equation       this yields the desired result 
  na      and any p  v  a  
proof of equation       for any 
   sup    r   p    hh a c 

h h a  p  
 
 
s

  sup sup   r   p    hidm a
c
sr  

  

ficoherent predictive inference under exchangeability

       lim
inf s dia  p 

  sup

     
inf dia  p 

s   sa

sr   a

where the second equality is due to equation       and the third one due to equation      
proof of equation       consider any p  v  a  and apply equation      
h h a  p    lim

inf dia  p     lim

s   sa

inf dia  p s   

inf

s   int a   s     s 

    

now fix any n  max deg p      and any   int a    using equation       we find that for
all   nan  
 

 

 

dia  ba   s    

s   n 

 
    
n     mx  
n
 
 m  
 s tx  
   n 
 s  tx   x  
 


s
xa
xa  

where for all x  a   
 mx  

 s  tx  

   s  tx   s  tx             s  tx   mx       s  tx  mx          o s    

and similarly 
 
s   n 

 

s   n

 
     o s     
    

hence  we find that
  
 

dia  ba   s    

xa   tx  mx

    

 

 n     

s  a          o s     

we now consider two cases   a        and  a         since n     these cases are
exhaustive   if  a         then dia  ba   s      o s     if  a        or  equivalently  if
there is some x  a such that    nx   then dia  ba nx  s      tx      o s      if we combine
this with equation       we find that
dia  p s     



bnp    dia  ba   s     

n
na



bnp  nx  tx   o s    

xa

furthermore  again due to equation      
bnp  nx    



bnp   ba   x     p x   for all x  a 

n
na

hence  we conclude that
dia  p s     



p x  tx   o s    

xa

which  combined with equation       leads to the desired result 
  

fide cooman  de bock    diniz

  na and any p  v  a  and use equation      
proof of equation       consider any 
   lim
h h a  p  

   lim sup dia  p 
          
     and h h a  p  
inf dia  p 

s   sa

s   s

a

 is bernstein coherent  theorem      it follows that h h a    
 is a coherent
since hh a c
 is super additive  and that its conjugate upper
lower prevision  this implies that h h a    
 is sub additive  hence  it suffices to prove the equalities in equation     
prevision h h a    
for any bernstein basis polynomial p   ba    where   na       now for any   sa we
gather from equation      in appendix b that 
    
n
      
 mx   x   nx    
dia  ba   
 n  
 ma   a  
xa
 

observe that 
x 
 mx   x   nx      mx   x   mx   x             mx   x   nx       m n
     o x    
x

and similarly  since ma     
 
 ma   a  

 n 

 

 

 n 

ma

     o a   

therefore 
    
 nx  

n
xa mx
      
dia  ba   
  
 
o 
  
     o x    
a
 n 

m
xa

a

which  using equation       leads to   
    
 nx  
n
xa mx
   h h a  ba    
  

h h a  ba    
  dia  ba     
 n 

m
a

e    proofs of results in section   
proof of theorem     we have already argued above that there is a smallest such inference
system   and we shall denote its lower probability function by   first  assume that n    
if we denote  n  k      n  k        n  k   then it follows from the assumptions that
 n  k        n  k  for    k  n    

    

we are first going to prove by induction that this implies that
 n  k  

k
 n  n  for    k  n 
n

    see footnote    

  

     

ficoherent predictive inference under exchangeability

observe that this inequality holds trivially for k      theorem    l    so assume that the
inequality holds for k      where                n      then we must show that it also holds
for k          assume  ex absurdo  that it does not  and therefore
 n          

   
 
 n  n    n        n  n  
n
n

     

where the second inequality follows from the induction hypothesis  now we also have that
 n  n     n          

n 


 n  m    n            n        n    

m    

 

   
n  
 n  n   
 n  n     n  n  
n
n

where the first inequality follows from equation       and the second from the first and
second inequalities in equation        this is a contradiction  which completes our proof by
induction of       
we infer from        theorem    l  and assumption      that
 n  k  

k n
k
 
for    k  n 
nn s
n s

     

also observe that this inequality holds trivially for n          we then get for the predictive
lower prevision p  a  h   of any gamble h on a 

 h x   min h p  a  i x    
p  a  h     min h   p  a  h  min h    min h  
xa

  min h  



 h x   min h  n  mx  

xa

 min h  


xa

 h x   min h 

mx
  p s  
idm a  h   
n s

where the first equality and the first inequality follow from the coherence  p   p  and p  
of p  a      the second equality from representation insensitivity  equation        and the
second inequality from equation        for the converse inequality  observe that the idmm
inference system sidm is coherent  representation insensitive  and specific by theorem    
clearly has concave surprise  satisfies assumption       and therefore dominates the smallest
such inference system 

references
augustin  t   coolen  f  p  a   de cooman  g     troffaes  m  c  m   eds           
introduction to imprecise probabilities  john wiley   sons 
bernard  j  m          bayesian analysis of tree structured categorized data  revue internationale de systmique           
bernard  j  m          an introduction to the imprecise dirichlet model for multinomial
data  international journal of approximate reasoning             
  

fide cooman  de bock    diniz

bernard  j  m          in personal conversation  
boole  g         reprinted in        the laws of thought  dover publications  new york 
boole  g         reprint of the work originally published by watts   co   london  in       
studies in logic and probability  dover publications  mineola  ny 
carnap  r          the continuum of inductive methods  the university of chicago press 
cifarelli  d  m     regazzini  e          de finettis contributions to probability and statistics 
statistical science             
couso  i     moral  s          sets of desirable gambles  conditioning  representation  and
precise probabilities  international journal of approximate reasoning              
     
cozman  f  g          independence for full conditional probabilities  structure  factorization  non uniqueness  and bayesian networks  international journal of approximate
reasoning                   
de cooman  g     miranda  e          symmetry of models versus models of symmetry  in
harper  w  l     wheeler  g  r   eds    probability and inference  essays in honor of
henry e  kyburg  jr   pp         kings college publications 
de cooman  g     miranda  e       a   the f  riesz representation theorem and finite
additivity  in dubois  d   lubiano  m  a   prade  h   gil  m  a   grzegorzewski 
p     hryniewicz  o   eds    soft methods for handling variability and imprecision
 proceedings of smps        pp          springer 
de cooman  g     miranda  e       b   weak and strong laws of large numbers for coherent
lower previsions  journal of statistical planning and inference                    
de cooman  g     miranda  e          irrelevant and independent natural extension for
sets of desirable gambles   journal of artificial intelligence research             
de cooman  g   miranda  e     quaeghebeur  e       a   representation insensitivity in
immediate prediction under exchangeability  international journal of approximate
reasoning                 
de cooman  g     quaeghebeur  e          exchangeability and sets of desirable gambles 
international journal of approximate reasoning                  special issue in
honour of henry e  kyburg  jr 
de cooman  g   quaeghebeur  e     miranda  e       b   exchangeable lower previsions 
bernoulli                 
de finetti  b          la prvision  ses lois logiques  ses sources subjectives  annales de
linstitut henri poincar          english translation by kyburg jr  and smokler
       
de finetti  b          teoria delle probabilit  einaudi  turin 
de finetti  b              theory of probability  a critical introductory treatment  john
wiley   sons  chichester  english translation de finettis        book  two volumes 
dubins  l  e          finitely additive conditional probabilities  conglomerability and
disintegrations  the annals of probability          
  

ficoherent predictive inference under exchangeability

geisser  s          predictive inference  an introduction  chapman   hall 
goldstein  m          the prevision of a prevision  journal of the american statistical
society             
goldstein  m          temporal coherence  in bernardo  j  m   degroot  m  h   lindley 
d  v     smith  a  f  m   eds    bayesian statistics  vol     pp          north holland 
amsterdam  with discussion 
good  i  j          the estimation of probabilities  an essay on modern bayesian methods 
the mit press 
haldane  j  b  s          on a method of estimating frequencies  biometrika             
hausdorff  f          momentprobleme fr ein endliches intervall  mathematische zeitschrift 
           
jaynes  e  t          probability theory  the logic of science  cambridge university press 
jeffreys  h          theory of probability  oxford classics series  oxford university press 
reprint of the third edition         with corrections 
johnson  n  l   kotz  s     balakrishnan  n          discrete multivariate distributions 
wiley series in probability and statistics  john wiley and sons  new york 
johnson  w  e          logic  part iii  the logical foundations of science  cambridge
university press  reprinted by dover publications in      
keynes  j  m          a treatise on probability  macmillan  london 
koopman  b  o          the axioms and algebra of intuitive probability  the annals of
mathematics  second series                 
kyburg jr   h  e     smokler  h  e   eds            studies in subjective probability  wiley 
new york  second edition  with new material       
lad  f          operational subjective statistical methods  a mathematical  philosophical
and historical introduction  john wiley   sons 
levi  i          the enterprise of knowledge  mit press  london 
mangili  f     benavoli  a          new prior near ignorance models on the simplex 
in cozman  f   denux  t   destercke  s     seidenfeld  t   eds    isipta    
proceedings of the eighth international symposium on imprecise probability  theories
and applications  pp          sipta 
miranda  e          updating coherent lower previsions on finite spaces  fuzzy sets and
systems                    
miranda  e     de cooman  g          introduction to imprecise probabilities  chap  lower
previsions  john wiley   sons 
miranda  e     zaffalon  m          notes on desirability and conditional lower previsions 
annals of mathematics and artificial intelligence                   
moral  s          epistemic irrelevance on sets of desirable gambles  annals of mathematics
and artificial intelligence             
  

fide cooman  de bock    diniz

moral  s     wilson  n          revision rules for convex sets of probabilities  in coletti 
g   dubois  d     scozzafava  r   eds    mathematical models for handling partial
knowledge in artificial intelligence  pp          plenum press  new york 
piatti  a   zaffalon  m   trojani  f     hutter  m          limits of learning about a categorical
latent variable under prior near ignorance  international journal of approximate
reasoning                 
prautzsch  h   boehm  w     paluszny  m          bzier and b spline techniques  springer 
berlin 
quaeghebeur  e          introduction to imprecise probabilities  chap  desirability  john
wiley   sons 
quaeghebeur  e   de cooman  g     hermans  f          accept   reject statement based
uncertainty models  international journal of approximate reasoning  accepted for
publication 
rouanet  h     lecoutre  b          specific inference in anova  from significance tests
to bayesian procedures  british journal of mathematical and statistical psychology 
               
seidenfeld  t   schervish  m  j     kadane  j  b          a representation of partially ordered
preferences  the annals of statistics                reprinted in the collection by
seidenfeld et al         pp         
seidenfeld  t   schervish  m  j     kadane  j  b          rethinking the foundations of
statistics  cambridge university press  cambridge 
sheffer  i  m          some properties of polynomial sets of type zero  duke mathematical
journal            
smith  c  a  b          consistency in statistical inference and decision  journal of the
royal statistical society  series a          
troffaes  m  c  m     de cooman  g          lower previsions  wiley 
trump  w     prautzsch  h          arbitrary degree elevation of bzier representations 
computer aided geometric design             
walley  p          statistical reasoning with imprecise probabilities  chapman and hall 
london 
walley  p          inferences from multinomial data  learning about a bag of marbles  journal
of the royal statistical society  series b           with discussion 
walley  p          a bounded derivative model for prior ignorance about a real valued
parameter  scandinavian journal of statistics                 
walley  p          towards a unified theory of imprecise probability  international journal
of approximate reasoning             
walley  p     bernard  j  m          imprecise probabilistic prediction for categorical data 
tech  rep  caf       laboratoire cognition et activites finalises  universit de
paris   
  

ficoherent predictive inference under exchangeability

williams  p  m       a   coherence  strict coherence and zero probabilities  in proceedings
of the fifth international congress on logic  methodology and philosophy of science 
vol  vi  pp        dordrecht  proceedings of a      conference held in warsaw 
williams  p  m       b   notes on conditional previsions  tech  rep   school of mathematical
and physical science  university of sussex  uk  see also the revised journal version by
williams        
williams  p  m          indeterminate probabilities  in przelecki  m   szaniawski  k    
wojcicki  r   eds    formal methods in the methodology of empirical sciences  pp 
        reidel  dordrecht  proceedings of a      conference held in warsaw 
williams  p  m          notes on conditional previsions  international journal of approximate
reasoning             
zabell  s  l          w  e  johnsons sufficientness postulate  the annals of statistics     
          reprinted in the collection by zabell        
zabell  s  l          symmetry and its discontents  essays on the history of inductive probability  cambridge studies in probability  induction  and decision theory  cambridge
university press  cambridge  uk 
zaffalon  m     miranda  e          probability and time  artificial intelligence           

  

fi