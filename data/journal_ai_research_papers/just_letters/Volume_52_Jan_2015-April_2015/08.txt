journal of artificial intelligence research                 

submitted        published      

computing convex coverage sets
for faster multi objective coordination
diederik m  roijers
shimon whiteson
frans a  oliehoek

d m roijers uva nl
s a whiteson uva nl
f a oliehoek uva nl

informatics institute
university of amsterdam
amsterdam  the netherlands

abstract
in this article  we propose new algorithms for multi objective coordination graphs  mocogs   key to the efficiency of these algorithms is that they compute a convex coverage
set  ccs  instead of a pareto coverage set  pcs   not only is a ccs a sufficient solution
set for a large class of problems  it also has important characteristics that facilitate more
efficient solutions  we propose two main algorithms for computing a ccs in mo cogs 
convex multi objective variable elimination  cmove  computes a ccs by performing a
series of agent eliminations  which can be seen as solving a series of local multi objective
subproblems  variable elimination linear support  vels  iteratively identifies the single
weight vector w that can lead to the maximal possible improvement on a partial ccs
and calls variable elimination to solve a scalarized instance of the problem for w  vels
is faster than cmove for small and medium numbers of objectives and can compute
an  approximate ccs in a fraction of the runtime  in addition  we propose variants of
these methods that employ and or tree search instead of variable elimination to achieve
memory efficiency  we analyze the runtime and space complexities of these methods  prove
their correctness  and compare them empirically against a naive baseline and an existing
pcs method  both in terms of memory usage and runtime  our results show that  by
focusing on the ccs  these methods achieve much better scalability in the number of
agents than the current state of the art 

   introduction
in many real world problem domains  such as maintenance planning  scharpff  spaan 
volker    de weerdt        and traffic light control  pham et al          multiple agents
need to coordinate their actions in order to maximize a common utility  key to coordinating
efficiently in these domains is exploiting loose couplings between agents  guestrin  koller 
  parr        kok   vlassis         each agents actions directly affect only a subset of the
other agents 
multi agent coordination is complicated by the fact that  in many domains  agents need
to balance multiple objectives  roijers  vamplew  whiteson    dazeley      a   for example  agents might have to maximize the performance of a computer network while minimizing
power consumption  tesauro  das  chan  kephart  lefurgy  levine    rawson         or
maximize the cost efficiency of maintenance tasks on a road network while minimizing traffic
delays  roijers  scharpff  spaan  oliehoek  de weerdt    whiteson        
c
    
ai access foundation  all rights reserved 

firoijers  whiteson    oliehoek

figure    mining company example 
however  the presence of multiple objectives does not per se necessitate the use of
specialized multi objective solution methods  if the problem can be scalarized  i e   the
utility function can be converted to a scalar utility function  the problem may be solvable
with existing single objective methods  such a conversion involves two steps  roijers et al  
    a   the first step is to specify a scalarization function 
definition    a scalarization function f   is a function that maps a multi objective utility
of a solution a of a decision problem  u a   to a scalar utility uw  a  
uw  a    f  u a   w  
where w is a weight vector that parameterizes f  
the second step is to define a single objective version of the decision problem such that the
utility of each solution a equals the scalarized utility of the original problem uw  a  
unfortunately  scalarizing the problem before solving it is not always possible because
w may not be known in advance  for example  consider a company that mines different
resources  in figure    we depict the problem this company faces  in the morning one van
per village needs to transport workers from that village to a nearby mine  where various
resources will be mined  different mines yield different quantities of resource per worker 
the market prices per resource vary through a stochastic process and every price change
can alter the optimal assignment of vans  the expected price variation increases with
the passage of time  to maximize performance  it is thus critical to act based on the latest
possible price information  since computing the optimal van assignment takes time  redoing
this computation for every price change is highly undesirable 
in such settings  we need a multi objective method that computes  in advance  an optimal solution for all possible prices  w  we call such a set a coverage set  cs   in many
cases  w is revealed before a solution must be executed  in which case that solution can
be automatically selected from the cs given w  in other cases  w is never made explicit
but instead a human is involved in the decision making and selects one solution from the
cs  perhaps on the basis of constraints or preferences that were too difficult to formalize in
the objectives themselves  roijers et al       a   in both cases  because the cs is typically
much smaller than the complete set of solutions  selecting the optimal joint action from the
cs is typically much easier than selecting it directly from the complete set of solutions 
   

ficomputing ccss for faster multi objective coordination

in this article  we consider how multi objective methods can be made efficient for problems that require the coordination of multiple  loosely coupled agents  in particular  we address multi objective coordination graphs  mo cogs   one shot multi agent decision problems in which loose couplings are expressed using a graphical model  mo cogs form an
important class of decision problems  not only can they be used to model a variety of realworld problems  delle fave  stranders  rogers    jennings        marinescu        rollon 
       but many sequential decision problems can be modeled as a series of mo cogs  as is
common in single objective problems  guestrin et al         kok   vlassis        oliehoek 
spaan  dibangoye    amato        
key to the efficiency of the mo cog methods we propose is that they compute a convex
coverage set  ccs  instead of a pareto coverage set  pcs   the ccs is a subset of the
pcs that is a sufficient solution for any multi objective problem with a linear scalarization
function  for example  in the mining company example of figure    f is linear  since the
total revenue is simply the sum of the quantity of each resource mined times its price per
unit  however  even if f is nonlinear  if stochastic solutions are allowed  then a ccs is
again sufficient  
the ccs has not previously been considered as a solution concept for mo cogs because
computing a ccs requires running linear programs  whilst computing a pcs requires only
pairwise comparisons of solutions  however  a key insight of this article  is that  in loosely
coupled systems  ccss are easier to compute than pcss  for two reasons  first  the ccs is
a  typically much smaller  subset of the pcs  in loosely coupled settings  efficient methods
work by solving a series of local subproblems  focusing on the ccs can greatly reduce the size
of these subproblems  second  focusing on the ccs makes solving a mo cog equivalent to
finding an optimal piecewise linear and convex  pwlc  scalarized value function  for which
efficient techniques can be adapted  for these reasons  we argue that the ccs is often the
concept of choice for mo cogs 
we propose two approaches that exploit these insights to solve mo cogs more efficiently
than existing methods  delle fave et al         dubus  gonzales    perny        marinescu 
razak    wilson        rollon   larrosa         the first approach deals with the multiple
objectives on the level of individual agents  while the second deals with them on a global
level 
the first approach extends an algorithm by rollon and larrosa        which we refer
to as pareto multi objective variable elimination  pmove      that computes local pareto
sets at each agent elimination  to compute a ccs instead  we call the resulting algorithm
convex multi objective variable elimination  cmove  
the second approach is a new abstract algorithm that we call optimistic linear support
 ols  and is much faster for small and medium numbers of objectives  furthermore  ols
   to be precise  in the case of stochastic strategies a ccs of deterministic strategies is always sufficient
 vamplew  dazeley  barker    kelarev         in the case of deterministic strategies  linearity of the
scalarization function makes the ccs sufficient  roijers et al       a  
   this article synthesizes and extends research already reported in two conference papers  specifically 
the cmove algorithm  section    was previously published at adt  roijers  whiteson    oliehoek 
    b  and the vels algorithm  section    at aamas  roijers  whiteson    oliehoek         the
memory efficient methods for computing ccss  section    are a novel contribution of this article 
   in the original article  this algorithm is called multi objective bucket elimination  mobe   however  we
use pmove to be consistent with the names of the other algorithms mentioned in this article 

   

firoijers  whiteson    oliehoek

can be used to produce a bounded approximation of the ccs  an  ccs  if there is not
enough time to compute a full ccs  ols is a generic method that employs single objective
solvers as a subroutine  in this article  we consider two implementations of this subroutine 
using variable elimination  ve  as a subroutine yields variable elimination linear support
 vels   which is particularly fast for small and moderate numbers of objectives and is more
memory efficient than cmove  however  when memory is highly limited  this reduction
in memory usage may not be enough  in such cases  using and or search  mateescu  
dechter        instead of ve yields and or tree search linear support  tsls   which is
slower than vels but much more memory efficient 
we prove the correctness of both cmove and ols  we analyze the runtime and space
complexities of both methods and show that our methods have better guarantees than
pcs methods  we show cmove and ols are complementary  i e   various trade offs exist
between them and their variants 
furthermore  we demonstrate empirically  on both randomized and more realistic problems  that cmove and vels scale much better than previous algorithms  we also empirically confirm the trade offs between cmove and ols  we show that ols  when used as
a bounded approximation algorithm  can save additional orders of magnitude of runtime 
even for small   finally  we show that  even when memory is highly limited  tsls can
still solve large problems 
the rest of this article is structured as follows  first  we provide a formal definition
of our model  as well as an overview of existing solution methods in section    after
presenting a naive approach in section    in sections      and    we analyze the runtime
and space complexities of each algorithm  and compare them empirically  against each other
and existing algorithms  at the end of each section  finally  we conclude in section   with
an overview of our contributions and findings  and suggestions for future research 

   background
in this section  we formalize the multi objective coordination graph  mo cog   before doing
so however  we describe the single objective version of this problem  the coordination graph
 cog   of which the mo cog is an extension  and the variable elimination  ve  algorithm
for solving cogs  the methods we present in section   and   build on ve in different ways 
     single objective  coordination graphs
a coordination graph  cog   guestrin et al         kok   vlassis        is a tuple hd  a  ui 
where
 d            n  is the set of n agents 
 a   ai       an is the joint action space  the cartesian product of the finite action
spaces of all agents  a joint action is thus a tuple containing an action for each agent
a   ha         an i  and

 
 u   u         u is the set of  scalar local payoff functions  each of which has limited
scope  i e   it depends on onlypa subset of the agents  the total team payoff is the sum
of the local payoffs  u a    e   ue  ae   
   

ficomputing ccss for faster multi objective coordination

figure     a  a cog with   agents and   local payoff functions  b  after eliminating agent   by
adding u   c  after eliminating agent   by adding u   

a 
a 

a 
    
    

a 
 
    

a 
a 

a 
   
 

a 
   
 

table    the payoff matrices for u   a    a     left  and u   a    a     right   there are two possible
actions per agent  denoted by a dot  a    and a bar  a    

all agents share the payoff function u a   we abuse the notation e to both index a local
payoff function ue and to denote the subset of agents in its scope  ae is thus a local joint
action  i e   a joint action of this subset of agents 
the decomposition of u a  into local payoff functions can be represented as a factor
graph  bishop         a bipartite graph containing two types of vertices  agents  variables 
and local payoff functions  factors   with edges connecting local payoff functions to the
agents in their scope 
figure  a shows the factor graph of an example cog in which the team payoff function
decomposes into two local payoff functions  each with two agents in scope 
u a   


x

ue  ae     u   a    a      u   a    a    

e  

the local payoff functions are defined in table    the factor graph illustrates the loose
couplings that result from the decomposition into local payoff functions  in particular  each
agents choice of action directly depends only on those of its immediate neighbors  e g   once
agent   knows agent  s action  it can choose its own action without considering agent   
    variable elimination
we now discuss the variable elimination  ve  algorithm  on which several multi objective
extensions  rollon   larrosa        rollon        build  including our own cmove algorithm  section     we also use ve as a subroutine in the ols algorithm  section    
ve exploits the loose couplings expressed by the local payoff functions to efficiently
compute the optimal joint action  i e   the joint action maximizing u a   first  in the forward
   

firoijers  whiteson    oliehoek

pass  ve eliminates each of the agents in turn by computing the value of that agents best
response to every possible joint action of its neighbors  these values are used to construct a
new local payoff function that encodes the value of the best response and replaces the agent
and the payoff functions in which it participated  in the original algorithm  once all agents
are eliminated  a backward pass assembles the optimal joint action using the constructed
payoff functions  here  we present a slight variant in which each payoff is tagged with the
action that generates it  obviating the need for a backwards pass  while the two algorithms
are equivalent  this variant is more amenable to the multi objective extension we present in
section   
ve eliminates agents from the graph in a predetermined order  algorithm   shows
pseudocode for the elimination of a single agent i  first  ve determines the set of local
payoff functions connected to i  ui   and the neighboring agents of i  ni  lines      
definition    the set of neighboring local payoff functions ui of i is the set of all local
payoff functions that have agent i in scope 
definition    the set of neighboring agents of i  ni   is the set of all agents that are in
scope of one or more of the local payoff functions in ui  
then  it constructs a new payoff function by computing the value of agent is best
response to each possible joint action ani of the agents in ni  lines        to do so  it
loops over all these joint actions ani  line     for each ani   it loops over all the actions ai
available to agent i  line     for each ai  ai   it computes the local payoff when agent i
responds to ani with ai  line     ve tags the total payoff with ai   the action that generates
it  line    in order to be able to retrieve the optimal joint action later  if there are already
tags present  ve appends ai to them  in this way  the entire joint action is incrementally
constructed  ve maintains the value of the best response by taking the maximum of these
payoffs  line      finally  it eliminates the agent and all payoff functions in ui and replaces
them with the newly constructed local payoff function  line     
algorithm    elimve u  i 
 
 
 
 
 
 
 

input  a cog u  and an agent i
ui  set of local payoff functions involving i
ni  set of neighboring agents of i
unew  a new factor taking joint actions of ni   ani   as input
foreach ani  ani do
s
foreach ax
i  ai do
v
uj  ani   ai  
uj ui

 
 
  
  
  
  

tag v with ai
s  s   v 
end
unew  ani    max s 
end
return  u   ui     unew  

   

ficomputing ccss for faster multi objective coordination

consider the example in figure  a and table    the optimal payoff maximizes the sum
of the two payoff functions 
max u a    max u   a    a      u   a    a    
a

a   a   a 

if ve eliminates agent   first  then it pushes the maximization over a  inward such that
goes only over the local payoff functions involving agent    in this case just u   


 
 
max u a    max u  a    a      max u  a    a     
a

a   a 

a 

ve solves the inner maximization and replaces it with a new local payoff function u  that
depends only on agent  s neighbors  thereby eliminating agent   

max u a    max u   a    a      u   a     
a

a   a 

which leads to the new factor graph depicted in figure  b  the values of u   a    are u   a     
     using a    and u   a        using a    as these are the optimal payoffs for the actions of
agent    given the payoffs shown in table    because we ultimately want the optimal joint
action  not just the optimal payoff  ve tags each payoff of u  with the action of agent  
that generates it  i e   we can think of u   a    as a  value  tag  pair  we denote such a pair
with parentheses and a subscript  u   a           a    and u   a         a   
ve next eliminates agent    yielding the factor graph shown in figure  c 


 
 
max u a    max max u  a    a      u  a      max u   a    
a

a 

a 

a 

ve appends the new tags for agent   to the existing tags for agent    yielding the following
tagged payoff values  u   a      maxa  u   a    a      u   a           a         a  a          a  a 
and u   a            a       a  a          a  a    finally  maximizing over a  yields the optimal
payoff of       a  a  a    with the optimal action contained in the tags 
the runtime complexity of ve is exponential  not in the number of agents  but only in
the induced width  which is often much less than the number of agents 
theorem    the computational complexity of ve is o n amax  w   where  amax   is the
maximal number of actions for a single agent and w is the induced width  i e   the maximal
number of neighboring agents of an agent plus one  the agent itself    at the moment when
it is eliminated  guestrin et al         
theorem    the space complexity of ve is o  n  amax  w   
this space complexity arises because  for every agent elimination  a new local payoff
function is created with o  amax  w   fields  possible input actions   since it is impossible
to tell a priori how many of these new local payoff functions exist at any given time during
the execution of ve  this need to be multiplied by the total number of new local payoff
functions created during a ve execution  which is n 
while ve is designed to minimize runtime  other methods focus on memory efficiency
instead  mateescu   dechter         we discuss memory efficiency further in section     
   in fact  ve is proven to have the best runtime guarantees within a large class of algorithms  rosenthal 
      

   

firoijers  whiteson    oliehoek

a 
a 

a 
     
     

a 
     
     

a 
a 

a 
     
     

a 
     
     

table    the two dimensional payoff matrices for u   a    a     left  and u   a    a     right  
    multi objective coordination graphs
a multi objective coordination

 graph  mo cog  is a tuple hd  a  ui in which d and a are
as before but  u   u         u is now a set of   d dimensional local p
payoff functions  the
total team payoff is the sum of local vector valued payoffs  u a    e   ue  ae    we use
ui to indicate the value of the i th objective  we denote the set of all possible joint action
values as v  table   shows a two dimensional mo cog with the same structure as the
single objective example in section      but with multi objective payoffs 
the solution to a mo cog is a coverage set  cs  of joint actions a and associated values
u a  that contains at least one optimal joint action for each possible parameter vector w
of the scalarization function f  definition     a cs is a subset of the undominated set 
definition    the undominated set  u  of a mo cog  is the set of all joint actions and
associated payoff values that are optimal for some w of the scalarization function f  

 
u  v    u a    u a  v  wa  uw  a   uw  a     
because we care about having at least one optimal joint action for every w  rather than all
optimal joint actions  a lossless subset of u suffices 
definition    a coverage set  cs   cs v   is a subset of u   such that for each possible w 
there is at least one optimal solution in the cs  i e  
wa


u a   cs v   a  uw  a   uw  a     

note that the cs is not necessarily unique  typically we seek the smallest possible cs  for
convenience  we assume that payoff vectors in the cs contain both the values and associated
joint actions  as suggested by the tagging scheme described in section     
which payoff vectors from v should be in the cs depends on what we know about the
scalarization function f   a minimal assumption is that f is monotonically increasing  i e  
if the value for one objective ui   increases while all uj  i stay constant  the scalarized value
u a  cannot decrease  this assumption ensures that objectives are desirable  i e   all else
being equal  having more of them is always better 
definition    the pareto front is the undominated set for arbitrary strictly monotonically
increasing scalarization functions f  

 
p f  v    u a    u a  v  a  u a    p u a   
where p indicates pareto dominance  p dominance   greater or equal in all objectives and
strictly greater in at least one objective 
   

ficomputing ccss for faster multi objective coordination

in order to have all optimal scalarized values  it is not necessary to compute the entire
pf  e g   if two joint actions have equal payoffs we need to retain only one of those 
definition    a pareto coverage set  pcs   p cs v   p f  v   is a coverage set for
arbitrary strictly monotonically increasing scalarization functions f   i e  

a  a u a   p cs v    u a  p u a     u a    u a      
computing p dominance requires only pairwise comparison of payoff vectors  feng  
zilberstein         
a highly prevalent scenario is that  in addition to f being monotonically increasing 
we also know that it is linear  that is  the parameter vectors w are weights by which the
values of the individual objectives are multiplied  f   w  u a   in the mining example from
figure    resources are traded on an open market and all resources have a positive unit
price  in this case  the scalarization is a linear combination of the amount of each resource
mined  where the weights correspond to the price per unit of each resource  many more
examples of linear scalarization functions exist in the literature  e g    lizotte  bowling   
murphy         because we assume the linear scalarization is monotonically increasing  we
can represent it without loss of generality as a convex combination of the objectives  i e  
the weights are positive and sum to    in this case  only a convex coverage set  ccs  is
needed  which is a subset of the convex hull  ch     
definition    the convex hull  ch  is the undominated set for linear non decreasing
scalarizations f  u a   w    w  u a  

 
ch v    u a    u a  v  wa  w  u a   w  u a     
that is  the ch contains all solutions that attain the optimal value for at least one weight 
vectors not in the ch are c dominated  in contrast to p domination  c domination cannot
be tested for with pairwise comparisons because it can take two or more payoff vectors to
c dominate a payoff vector  note that the ch contains more solutions than needed to guarantee an optimal scalarized value value  it can contain multiple solutions that are optimal
for one specific weight  a lossless subset of the ch with respect to linear scalarizations is
called a convex coverage set  ccs   i e   a ccs retains at least one u a  that maximizes
the scalarized payoff  w  u a   for every w 
definition    a convex coverage set  ccs   ccs v   ch v   is a cs for linear nondecreasing scalarizations  i e  

wa u a   ccs v   a  w  u a   w  u a     
since linear non decreasing functions are a specific type of monotonically increasing function  there is always a ccs that is a subset of the smallest possible pcs 
as previously mentioned  css like the pcs and ccs  may not be unique  for example 
if there are two joint actions with equal payoff vectors  we need at most one of them to
make a pcs or ccs 
   p dominance is often called pairwise dominance in the pomdp literature 
   note that the term convex hull is overloaded  in graphics  the convex hull is a superset of what we mean
by the convex hull in this article 

   

firoijers  whiteson    oliehoek

figure    the ccs  filled circles at left  and solid black lines at right  versus the pcs  filled circles
and squares at left  and both dashed and solid black lines at right  for twelve random
  dimensional payoff vectors 

in practice  the pcs and the ccs are often equal to the pf and ch  however  the
algorithms proposed in this article are guaranteed to produce a pcs or a ccs  and not
necessarily the entire pf or the ch  because pcss and the ccss are sufficient solutions in
terms of scalarized value  we say that these algorithms solve the mo cogs 
in figure    left  the values of joint actions  u a   are represented as points in valuespace  for a two objective mo cog  the joint action value a is in both the ccs and the
pcs  b  however  is in the pcs  but not the ccs  because there is no weight for which a
linear scalarization of bs value would be optimal  as shown in figure    right   where the
scalarized value of the strategies are plotted as a function of the weight on the first objective
 w       w     c is in neither the ccs nor the pcs  it is pareto dominated by a 
many multi objective methods  e g    delle fave et al         dubus et al         marinescu et al         rollon        simply assume that the pcs is the appropriate solution
concept  however  we argue that the choice of cs depends on what one can assume about
how utility is defined with respect to the multiple objectives  i e   which scalarization function is used to scalarize the vector valued payoffs  we argue that in many situations the
scalarization function is linear  and that in such cases one should use the ccs 
in addition to the shape of f   the choice of solution concept depends on whether only
deterministic joint actions are considered or whether stochastic strategies are also permitted  a stochastic strategy  assigns a probabilitypto each joint action a          the
probabilities for all joint actions together sum to    aa  a       the value of a stochastic strategy is a linear
p combination of the value vectors of the joint actions of which it is

a mixture  u  
aa  a u a   therefore  the optimal values  for any monotonically
increasing f   lie on the convex upper surface spanned by the strategies in the ccs  as
indicated by the lines in figure    left   therefore  all optimal values for monotonically
increasing f   including nonlinear ones  can be constructed by taking mixture policies from
the ccs  vamplew et al         
this article considers methods for computing ccss  which  as we show in sections  
and    can be computed more efficiently than pcss  furthermore  ccss are typically much
   

ficomputing ccss for faster multi objective coordination

smaller  this is particularly important when the final selection of the joint is done by  a
group of  humans  who have to compare all possible alternatives in the solution set 
the methods presented in this article are based on variable elimination  ve   sections
  and    and and or tree search  ts   section     these algorithms are exact solution
methods for cogs 
the cmove algorithm we propose in section   is based on ve  it differs from another
multi objective algorithm based on ve  which we refer to as pmove  rollon   larrosa 
       in that it produces a ccs rather than a pcs  an alternative to ve are messagepassing algorithms  like max plus  pearl        kok   vlassis      a   however  these are
guaranteed to be exact only for tree structured cogs  multi objective methods that build
on max plus such as that of delle fave et al          have this same limitation  unless
they preprocess the cog to form a clique tree or gai network  dubus et al          on
tree structured graphs  both message passing algorithms and ve produce optimal solutions
with similar runtime guarantees  note that  like pmove  existing multi objective methods
based on message passing produce a pcs rather than a ccs 
in section    we take a different approach to multi objective coordination based on an
outer loop approach  as we explain  this approach is applicable only for computing a ccs 
not a pcs  but has considerable advantages in terms of runtime and memory usage 

   non graphical approach
a naive way to compute a ccs is to ignore the graphical structure  calculate the set of all
possible payoffs for all joint actions v  and prune away the c dominated joint actions  we
first translate the problem to a set of value set factors  vsfs   f  each vsf f is a function
mapping local joint actions to sets of payoff vectors  the initial vsfs are constructed from
the local payoff functions such that
f e  ae      ue  ae    
i e   each vsf maps a local joint action to the singleton set containing only that actions
local payoff  we can now define v in terms of f using the cross sum operator over all vsfs
in f for each joint action a 
 m
v f   
f e  ae   
a f e f

where the cross sum of two sets a and b contains all possible vectors that can be made by
summing one payoff vector from each set 
a  b    a   b   a  a  b  b   
the ccs can now be calculated by applying a pruning operator cprune  described below 
that removes all c dominated vectors from a set of value vectors  to v 
 m
ccs v f     cprune v f     cprune 
f e  ae    
   
a f e f

the non graphical ccs algorithm simply computes the righthand side of equation    i e  
it computes v f  explicitly by looping over all actions  and for each action looping over all
local vsfs  and then pruning that set down to a ccs 
   

firoijers  whiteson    oliehoek

a ccs contains at least one payoff vector that maximizes the scalarized value for every
w 
w



a   arg max w  u a 



  a 

u a     ccs v f    w  u a    w  u a        

aa

that is  for every w there is an solution a  that is part of the ccs and that achieves the
same value as a maximizing solution a  moreover the value of such solutions is given by the
dot product  thus  finding the ccs is analogous to the problem faced in partially observable
markov decision processes  pomdps   feng   zilberstein         where optimal  vectors
 corresponding to the value vectors u a   for all beliefs  corresponding to the weight vectors
w  must be found  therefore  we can employ pruning operators from pomdp literature 
algorithm   describes our implementation of cprune  which is based on that of feng and
zilberstein        with one modification  in order to improve runtime guarantees  cprune
first pre prunes the candidate solutions u to a pcs using the pprune  algorithm    at line
   pprune computes a pcs in o d u  p cs   by running pairwise comparisons  next  a
partial ccs  u   is constructed as follows  a random vector u from u is selected at line   
for u the algorithm tries to find a weight vector w for which u is better than the vectors
in u   line     by solving the linear program in algorithm    if there is such a w  cprune
finds the best vector v for w in u and moves it to u   line        if there is no weight for
which u is better it is c dominated and thus removed u from u  line    
algorithm    cprune u 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  

input  a set of payoff vectors u
u  pprune u 
u  
while notempty u  do
select random u from u
w  findweight u  u   
if w null then
  did not find a weight where u is optimal
remove u from u
end
else
v  arg maxuu w  u
u  u    v 
u   u    v 
end
end
return u 

the runtime of cprune as defined by algorithm   is
o d u  p cs     p cs p  d ccs    

   

where p  d ccs   is a polynomial in the size of the ccs and the number of objectives d 
which is the runtime of the linear program that tests for c domination  algorithm    
   

ficomputing ccss for faster multi objective coordination

algorithm    pprune u 
 
 
 
 
 
 
 
 
 
  
  
  

input  a set of payoff vectors u
u  
while u     do
u  the first element of u
foreach v  u do
if v p u then
u  v    continue with v instead of u
end
end
remove u  and all vectors p dominated by u  from u
add u to u 
end
return u 

algorithm    findweight u  u 
max x
x w

subject to w   u  u     x     u   u
d
x

wi    

i  

if x     return w else return null

the key downside of the non graphical approach is that it requires explicitly enumerating
all possible joint actions and calculating the payoffs associated with each one  consequently 
it is intractable for all but small numbers of agents  as the number of joint actions grows
exponentially in the number of agents 
theorem    the time complexity of computing a ccs of a mo cog containing  local
payoff functions  following the non graphical approach  equation    is 
o d amax  n   d amax  n  p cs     p cs p  d ccs  
proof  first  v is computed by looping over all  vsfs for each joint action a  summing
vectors of length d  if the maximum size of the action space of an agent is amax there are
o  amax  n   joint actions  v contains one payoff vector for each joint action  v is the input
of cprune 
in the next two sections  we present two approaches to compute ccss more efficiently 
the first approach pushed the cprune operator in equation   into the cross sum and
union  just as the max operator is pushed into the summation in ve  we call this the
inner loop approach  as it uses pruning operators during agent eliminations  which is the
inner loop of the ve algorithm  the second approach is inspired by linear support  cheng 
   

firoijers  whiteson    oliehoek

       a pomdp pruning operator that only requires finding the optimal solution for certain w  instead of performing maximization over the entire set v  as in the original linear
support algorithm  we show that we can use ve on a finite number of scalarized instances
of the mo cog  avoiding explicit calculation of v  we call this approach the outer loop
approach  as this it creates an outer loop around a single objective method  like ve   which
it calls as a subroutine 

   convex variable elimination for mo cogs
in this section we show how to exploit loose couplings and calculate a ccs using an inner loop approach  i e   by pushing the pruning operators into the cross sum and union
operators of equation    the result is cmove  an extension to rollon and larrosas
pareto based extension of ve  which we refer to as pmove  rollon   larrosa        
by analyzing cmoves complexity in terms of local convex coverage sets  we show that
this approach yields much better runtime complexity guarantees than the non graphical
approach to computing ccss that was presented in section   
    exploiting loose couplings in the inner loop
in the non graphical approach  computing a ccs is more expensive than computing a pcs 
as we have shown in section    we now show that  in mo cogs  we can compute a ccs
much more efficiently by exploiting the mo cogs graphical structure  in particular  like in
ve  we can solve the mo cog as a series of local subproblems  by eliminating agents and
manipulating the set of vsfs f which describe the mo cog  the key idea is to compute
local ccss  lccss  when eliminating an agent instead of a single best response  as in ve  
when computing an lccs  the algorithm prunes away as many vectors as possible  this
minimizes the number of payoff vectors that are calculated at the global level  which can
greatly reduce computation time  here we describe the elim operator for eliminating agents
used by cmove in section     
we first need to update our definition of neighboring local payoff functions  definition
    to neighboring vsfs 
definition     the set of neighboring vsfs fi of i is the set of all local payoff functions
that have agent i in scope 
the neighboring agents ni of an agent i are now the agents in the scope of a vsf in
fi   except for i itself  corresponding to definition    for each possible local joint action of
ni   we now compute an lccs that contains the payoffs of the c undominated responses of
agent i  as the best response values of i  in other words  it is the ccs of the subproblem that
arises when considering only fi and fixing a specific local joint action ani   to compute the
lccs  we must consider all payoff vectors of the subproblem  vi   and prune the dominated
ones 
definition     if we fix all actions
in ani   but not ai   the set of all payoff vectors for
s l
this subproblem is  vi  fi   ani     ai f e fi f e  ae    where ae is formed from ai and the
appropriate part of ani  
using definition     we can now define the lccs as the ccs of vi  
   

ficomputing ccss for faster multi objective coordination

definition     a local ccs  an lccs  is the c undominated subset of vi  fi   ani   
lccsi  fi   ani     ccs vi  fi   ani    

using these lccss  we can create a new vsf  f new   conditioned on the actions of the
agents in ni  
ani f new  ani     lccs i  fi   ani   
the elim operator replaces the vsfs in fi in f by this new factor 
elim f  i     f   fi     f new  ani    
theorem    elim preserves the ccs  i f ccs v f     ccs v elim f  i    
proof  we show this by using the implication of equation    i e   for all joint actions a for
which there is a w at which the scalarized value of a is maximal  a vector valued payoff
u a    for which w  u a      w  u a    is in the ccs  we show that the maximal scalarized
payoff cannot be lost as a result of elim 
the
function distributes over the local payoff functions  w  u a   
p linear scalarization
p
w  e ue  ae     e w  ue  ae    thus  when eliminating agent i  we divide the set of vsfs
into non neighbors  nn   in which agent i does not participate  and neighbors  ni   such
that 
x
x
w  u a   
w  ue  ae    
w  ue  ae   
enn

eni

now  following equation    the ccs contains maxaa w  u a  for all w  elim pushes this
maximization in 
max w  u a    max
aa

ai ai

x

w  ue  ae     max

ai ai

enn

x

w  ue  ae   

eni

elim
the agent i factors by a term f new  ani   that satisfies w  f new  ani     maxai
p replaces
e
eni w  u  ae   per definition  thus preserving the maximum scalarized value for all w and
thereby preserving the ccs 
instead of an lccs  we could compute a local pcs  lpcs   that is  using a pcs
computation on vi instead of a ccs computation  note that  since lccs  lpcs  vi  
elim not only reduces the problem size with respect to vi   it can do so more than would
be possible if we only considered p dominance  therefore  focusing on the ccs can greatly
reduce the sizes of local subproblems  since the solution of a local subproblem is the input
for the next agent elimination  the size of subsequent local subproblems is also reduced 
which can lead to considerable speed ups 
   

firoijers  whiteson    oliehoek

    convex multi objective variable elimination
we now present the convex multi objective variable elimination  cmove  algorithm  which
implements elim using cprune  like ve  cmove iteratively eliminates agents until none
are left  however  our implementation of elim computes a ccs and outputs the correct
joint actions for each payoff vector in this ccs  rather than a single joint action  cmove
is an extension to rollon and larrosas pareto based extension of ve  which we refer to as
pmove  rollon   larrosa        
the most important difference between cmove and pmove is that cmove computes a ccs  which typically leads to much smaller subproblems and thus much better
computational efficiency  in addition  we identify three places where pruning can take
place  yielding a more flexible algorithm with different trade offs  finally  we use the tagging scheme instead of the backwards pass  as in section     
algorithm   presents an abstract version of cmove that leaves the pruning operators
unspecified  as in section    cmove first translates the problem into a set of vector set
factors  vsfs   f on line    next  cmove iteratively eliminates agents using elim  line
     the elimination order can be determined using techniques devised for single objective
ve  koller   friedman        
algorithm    cmove u  prune   prune   prune   q 

 
 
 
 
 
 
 
 

input  a set of local payoff functions u and an elimination order q  a queue containing all
agents 
f  create one vsf for every local payoff function in u
while ani  ani do
i  q dequeue  
f  elim f  i  prune   prune  
end
f  retrieve final factor from f
s  f  a  
return prune  s 

algorithm   shows our implementation of elim  parameterized with two pruning operators  prune  and prune   corresponding to two different pruning locations inside the
operator that computes lccsi   computelccsi  fi   ani   prune   prune   
algorithm    elim f  i  prune   prune  
 
 
 
 
 
 
 
 

input  a set of vsfs f  and an agent i
ni  the set of neighboring agents of i
fi  the subset of vsf that have i in scope
f new  ani    a new vsf
foreach ani  ani do
f new  ani    computelccs i  fi   ani   prune   prune  
end
f  f   fi   f new  
return f

   

ficomputing ccss for faster multi objective coordination

computelccsi is implemented as follows  first we define a new cross sum and prune
   prune  a  b   lccsi applies this operator sequentially 
operator ab
 m

f e  ae    
   
computelccsi  fi   ani   prune   prune     prune  
e
ai

f fi

 operator  leading to incremental
prune  is applied to each cross sum of two sets  via the 
pruning  cassandra  littman    zhang         prune  is applied at a coarser level  after
the union  cmove applies elim iteratively until no agents remain  resulting in a ccs 
note that  when there are no agents left  f new on line   has no agents to condition on  in
this case  we consider the actions of the neighbors to be a single empty action  a  
pruning can also be applied at the very end  after all agents have been eliminated 
which we call prune   in increasing level of coarseness  we thus have three pruning operators  incremental pruning  prune    pruning after the union over actions of the eliminated
agent  prune    and pruning after all agents have been eliminated  prune    as reflected in
algorithm    after all agents have been eliminated  the final factor is taken from the set
of factors  line     and the single set  s contained in that factor is retrieved  line     note
that we use the empty action a to denote the field in the final factor  as it has no agents
in scope  finally prune  is called on s 
consider the example in figure  a  using the payoffs defined by table    and apply
cmove  first  cmove creates the vsfs f   and f   from u  and u    to eliminate agent   
it creates a new vsf f    a    by computing the lccss for every a  and tagging each element
of each set with the action of agent   that generates it  for a    cmove first generates
the set        a          a     since both of these vectors are optimal for some w  neither is
removed by pruning and thus f    a             a          a     for a    cmove first generates
       a          a     cprune determines that       a  is dominated and consequently removes
it  yielding f    a             a     cmove then adds f   to the graph and removes f   and
agent    yielding the factor graph shown in figure  b 
cmove then eliminates agent   by combining f   and f   to create f     for f    a    
cmove must calculate the lccs of 
 f    a    a     f    a       f    a    a     f    a     
the first cross sum yields        a  a          a  a    and the second yields        a  a     pruning
their union yields f    a             a  a          a  a     similarly  for a  taking the union yields
       a  a          a  a          a  a     of which the lccs is f    a             a  a     adding f  
results in the graph in figure  c 
finally  cmove eliminates agent    since there are no neighboring agents left  ai
contains only the empty action  cmove takes the union of f    a    and f    a     since
       a  a  a    and        a  a  a    dominate        a  a  a      the latter is pruned  leaving ccs  
        a  a  a             a  a  a      
    cmove variants
there are several ways to implement the pruning operators that lead to correct instantiations of cmove  both pprune  algorithm    and cprune  algorithm    can be used  as
long as either prune  or prune  is cprune  note that if prune  computes the ccs  prune 
is not necessary 
   

firoijers  whiteson    oliehoek

in this article  we consider basic cmove  which does not use prune  and prune  and
only prunes at prune  using cprune  as well as incremental cmove  which uses cprune at
both prune  and prune   the latter invests more effort in intermediate pruning  which can
result in smaller cross sums  and a resulting speedup  however  when only a few vectors
can be pruned in these intermediate steps  this additional speedup may not occur  and
the algorithm creates unnecessary overhead   we empirically investigate these variants in
section    
one could also consider using pruning operators that contain prior knowledge about
the range of possible weight vectors  if such information is available  it could be easily
incorporated by changing the pruning operators accordingly  leading to even smaller lccss 
and thus a faster algorithm  in this article however  we focus on the case in which such
prior knowledge is not available 
    analysis
we now analyze the correctness and complexity of cmove 
theorem    move correctly computes a ccs 
proof  the proof works by induction on the number of agents  the base case is the original
mo cog  where each f e  ae   from f is a singleton set  then  since elim preserves the ccs
 see theorem     no necessary vectors are lost  when the last agent is eliminated  only
one factor remains  since it is not conditioned on any agent actions and is the result of an
lccs computation  it must contain one set  the ccs 
theorem    the computational complexity of cmove is
o  n  amax  wa  wf r    r      r    

   

where wa is the induced agent width  i e   the maximum number of neighboring agents  connected via factors  of an agent when eliminated  wf is the induced factor width  i e   the
maximum number of neighboring factors of an agent when eliminated  and r    r  and r 
are the cost of applying the prune   prune  and prune  operators 
proof  cmove eliminates n agents and for each one computes an lccs for each joint
action of the eliminated agents neighbors  in a field in a new vsf  cmove computes
o  amax  wa   fields per iteration  calling prune   equation    for each adjacent factor  and
prune  once after taking the union over actions of the eliminated agent  prune  is called
exactly once  after eliminating all agents  line   of algorithm    
unlike the non graphical approach  cmove is exponential only in wa   not the number
of agents  in this respect  our results are similar to those for pmove  rollon        
however  those earlier complexity results do not make the effect of pruning explicit  instead 
the complexity bound makes use of additional problem constraints  which limit the total
number of possible different value vectors  specifically  in the analysis of pmove  the
payoff vectors are integer valued  with a maximum value for all objectives  in practice 
   we can also compute a pcs first  using prune  and prune   and then compute the ccs with prune  
however  this is useful only for small problems for which a pcs is cheaper to compute than a ccs 

   

ficomputing ccss for faster multi objective coordination

such bounds can be very loose or even impossible to define  e g   when the payoff values
are real valued in one or more objectives   therefore  we instead give a description of
the computational complexity that makes explicit the dependence on the effectiveness of
pruning  even though such complexity bounds are not better in the worst case  i e   when
no pruning is possible   they allow greater insight into the runtimes of the algorithms we
evaluate  as is apparent in our analysis of the experimental results in section     
theorem   demonstrates that the complexity of cmove depends heavily on the runtime
of its pruning operators  which in turn depends on the sizes of the input sets  the input
set of prune  is the union of what is returned by a series of applications of prune   while
prune  uses the output of the last application of prune   we therefore need to balance
the effort of the lower level pruning with that of the higher level pruning  which occurs less
often but is dependent on the output of the lower level  the bigger the lccss  the more
can be gained from lower level pruning 
theorem    the space complexity of cmove is
o  d n  amax  wa  lccsmax     d   amax   emax     
where  lccsmax   is maximum size of a local ccs   is the original number of vsfs  and
 emax   is the maximum scope size of the original vsfs 
proof  cmove computes a local ccs for each new vsf for each joint action of the eliminated agents neighbors  there are maximally wa neighbors  there are maximally n new
factors  each payoff vector stores d real numbers 
there are  vsfs created during the initialization of cmove  all of these vsfs have
exactly one payoff vector containing d real numbers  per joint action of the agents in scope 
there are maximally  amax   emax   such joint actions 
for pmove  the space complexity is the same but with  p ccsmax   instead of  lccsmax   
because the lccs is a subset of the corresponding lpcs  cmove is thus strictly more
memory efficient than pmove 
note that theorem   is a rather loose upper bound on the space complexity  as not all
vsfs  original or new  exist at the same time  however  it is not possible to to predict
a priori how many of these vsfs exist at the same time  resulting in a space complexity
bound on the basis of all vsfs that exist at some point during the execution of cmove 
    empirical evaluation
to test the efficiency of cmove  we now compare its runtimes to those of pmove  and
the non graphical approach for problems with varying numbers of agents and objectives 
we also analyze how these runtimes correspond to the sizes of the pcs and ccs 
we use two types of experiments  the first experiments are done with random mocogs in which we can directly control all variables  in the second experiment  we use
mining day  a more realistic benchmark  that is more structured than random mo cogs
but still randomized 
   we compare to pmove using only prune    pprune  rather than prune    prune    pprune  as was
proposed in the original article  rollon   larrosa        because we found the former option slightly
but consistently faster 

   

firoijers  whiteson    oliehoek

 a 

 b 

 c 

figure     a  runtimes  ms  in log scale for the nongraphical method  pmove and cmove with
standard deviation of mean  error bars    b  the corresponding number of vectors in the
pcs and ccs  and  c  the corresponding spread of the induced width 

      random graphs
to generate random mo cogs  we employ a procedure that takes as input  n  the number
of agents  d  the number of payoff dimensions   the number of local payoff functions  and
 ai    the action space size of the agents  which is the same for all agents  the procedure
then starts with a fully connected graph with local payoff functions connecting to two agents
each  then  local payoff functions are randomly removed  while ensuring that the graph
remains connected  until only  local payoff functions remain  the values for the different
objectives in each local payoff function are real numbers that are drawn independently and
uniformly from the interval          we compare algorithms on the same set of randomly
generated mo cogs for each separate value of n  d    and  ai   
to compare basic cmove  incremental cmove  pmove  and the non graphical
method  we test them on random mo cogs with the number of agents ranging between
   and     the average number of factors per agent held at       n  and the number of
objectives d      this experiment was run on a     ghz intel core i  computer  with   gb
memory  figure   shows the results  averaged over    mo cogs for each number of agents 
the runtime  figure  a  of the non graphical method quickly explodes  both cmove
variants are slower than pmove for small numbers of agents  but the runtime grows much
more slowly than that of pmove  at    agents  both cmove variants are faster than
pmove on average  for    agents  one of the mo cogs generated caused pmove to
time out at     s  while basic cmove had a maximum runtime of    s  and incremental
cmove    s  this can be explained by the differences in the size of the solutions  i e  
the pcs and the ccs  figure  b   the pcs grows much more quickly with the number of
agents than the ccs does  for two objective problems  incremental cmove seems to be
consistently slower than basic cmove 
while cmoves runtime grows much more slowly than that of the nongraphical method 
it is still exponential in the number of agents  a counterintuitive result since the worst case
complexity is linear in the number of agents  this can be explained by the induced width
of the mo cogs  in which the runtime of cmove is exponential  in figure  c  we see that
the induced width increases linearly with the number of agents for random graphs 
   

ficomputing ccss for faster multi objective coordination

figure    runtimes  ms  for the non graphical method  pmove and cmove in log scale with the
standard deviation of mean  error bars   left  and the corresponding number of vectors
in the pcs and ccs  right   for increasing numbers of agents and   objectives 

we therefore conclude that  in two objective mo cogs  the non graphical method is
intractable  even for small numbers of agents  and that the runtime of cmove increases
much less with the number of agents than pmove does 
to test how the runtime behavior changes with a higher number of objectives  we run
the same experiment with the average number of factors per agent held at       n and
increasing numbers of agents again  but now for d      this and all remaining experiments
described in this section were executed on a xeon l          ghz computer with    gb
memory  figure    left  shows the results of this experiment  averaged over    mo cogs
for each number of agents  note that we do not plot the induced widths  as this does not
change with the number of objectives  these results demonstrate that  as the number of
agents grows  using cmove becomes key to containing the computational cost of solving
the mo cog  cmove outperforms the nongraphical method from    agents onwards  at
   agents  basic cmove is    times faster  cmove also does significantly better than
pmove  though it is one order of magnitude slower with    agents     ms  basic  and
   ms  incremental  versus   ms on average   its runtime grows much more slowly than
that of pmove  at    agents  both cmove variants are faster than pmove and at
   agents  basic cmove is almost one order of magnitude faster     s versus       s on
average   and the difference increases with every agent 
as before  the runtime of cmove is exponential in the induced width  which increases
with the number of agents  from     at n      to     at n      on average  as a result of
the random mo cog generation procedure  however  cmoves runtime is polynomial in
the size of the ccs  and this size grows exponentially  as shown in figure    right   the
fact that cmove is much faster than pmove can be explained by the sizes of the pcs
and ccs  as the former grows much faster than the latter  at    agents  the average pcs
size is     and the average ccs size is     at    agents  the average pcs size has risen to
        while the average ccs size is only        
figure    left  compares the scalability of the algorithms in the number of objectives 
on random mo cogs with n      and        averaged over     mo cogs  cmove
always outperforms the nongraphical method  interestingly  the nongraphical method is
   

firoijers  whiteson    oliehoek

figure    runtimes  ms  for the non graphical method  pmove and cmove in logscale with the
standard deviation of mean  error bars   left  and the corresponding number of vectors
in the pcs and ccs  right   for increasing numbers of objectives 

several orders of magnitude slower at d      grows slowly until d      and then starts to
grow with about the same exponent as pmove  this can be explained by the fact that the
time it takes to enumerate of all joint actions and payoffs remains approximately constant 
while the time it takes to prune increases exponentially with the number of objectives 
when d      cmove is an order of magnitude slower than pmove     ms  basic  and
     incremental  versus   ms   however  when d      both cmove variants are already
faster than pmove and at   dimensions they are respectively     and     times faster 
this happens because the ccs grows much more slowly than the pcs  as shown in figure
   right   the difference between incremental and basic cmove decreases as the number
of dimensions increases  from a factor     at d     to     at d      this trend indicates
that pruning after every cross sum  i e   at prune   becomes  relatively  better for higher
numbers of objectives  although we were unable to solve problem instances with many more
objectives within reasonable time  we expect this trend to continue and that incremental
cmove would be faster than basic cmove for problems with very many objectives 
overall  we conclude that  for random graphs  cmove is key to solving mo cogs
within reasonable time  especially when the problem size increases in either the number of
agents  the number of objectives  or both 
      mining day
in mining day  a mining company mines gold and silver  objectives  from a set of mines
 local payoff functions  located in the mountains  see figure     the mine workers live in
villages at the foot of the mountains  the company has one van in each village  agents 
for transporting workers and must determine every morning to which mine each van should
go  actions   however  vans can only travel to nearby mines  graph connectivity   workers
are more efficient if there are more workers at the mine  there is a    efficiency bonus per
worker such that the amount of each resource mined per worker is x      w   where x is
the base rate per worker and w is the number of workers at the mine  the base rate of
gold and silver are properties of a mine  since the company aims to maximize revenue  the
best strategy depends on the fluctuating prices of gold and silver  to maximize revenue 
   

ficomputing ccss for faster multi objective coordination

figure    runtimes  ms  for basic and incremental cmove  and pmove  in log scale with the
standard deviation of mean  error bars   left  and the corresponding number of vectors
in the pcs and ccs  right   for increasing numbers of agents 

the mining company wants to use the latest possible price information  and not lose time
recomputing the optimal strategy with every price change  therefore  we must calculate a
ccs 
to generate a mining day instance with v villages  agents   we randomly assign    
workers to each village and connect it to     mines  each village is only connected to mines
with a greater or equal index  i e   if village i is connected to m mines  it is connected to
mines i to i   m     the last village is connected to   mines and thus the number of mines
is v      the base rates per worker for each resource at each mine are drawn uniformly and
independently from the interval         
in order to compare the runtimes of basic and incremental cmove against pmove
on a more realistic benchmark  we generate mining day instances with varying numbers
of agents  note that we do not include the non graphical method  as its runtime mainly
depends on the number of agents  and is thus not considerably faster for this problem
than for random graphs  the runtime results are shown in figure    left   both cmove
and pmove are able to tackle problems with over     agents  however  the runtime of
pmove grows much more quickly than that of cmove  in this two objective setting 
basic cmove is better than incremental cmove  basic cmove and pmove both have
runtimes of around    s at    agents  but at     agents  basic cmove runs in about    s
and pmove in   s  even though incremental cmove is worse than basic cmove  its
runtime still grows much more slowly than that of pmove  and it beats pmove when
there are many agents 
the difference between pmove and cmove results from the relationship between the
number of agents and the sizes of the ccs  which grows linearly  and the pcs  which grows
polynomially  as shown in figure    right   the induced width remains around   regardless
of the number of agents  these results demonstrate that  as the ccs grows more slowly
than the pcs with the number of agents  cmove can solve mo cogs more efficiently
than pmove as the number of agents increases 
   

firoijers  whiteson    oliehoek

   linear support for mo cogs
in this section  we present variable elimination linear support  vels   vels is a new
method for computing the ccs in mo cogs that has several advantages over cmove 
for moderate numbers of objectives  its runtime complexity is better  it is an anytime
algorithm  i e   over time  vels produces intermediate results which become better and
better approximations of the ccs and therefore  when provided with a maximum scalarized
error   vels can compute an  optimal ccs 
rather than dealing with the multiple objectives in the inner loop  like cmove   vels
deals with them in the outer loop and employs ve as a subroutine  vels thus builds the
ccs incrementally  with each iteration of its outer loop  vels adds at most one new
vector to a partial ccs  to find this vector  vels selects a single w  the one that offers
the maximal possible improvement   and passes that w to the inner loop  in the inner loop 
vels uses ve  section      to solve the single objective coordination graph  cog  that
results from scalarizing the mo cog using the w selected by the outer loop  the joint
action that is optimal for this cog and its multi objective payoff are then added to the
partial ccs 
the departure point for creating vels is chengs linear support  cheng         chengs
linear support was originally designed as a pruning algorithm for pomdps  unfortunately 
this algorithm is rarely used for pomdps in practice  as its runtime is exponential in the
number of states  however  the number of states in a pomdp corresponds to the number
of objectives in a mo cog  and while realistic pomdps typically have many states  many
mo cogs have only a handful of objectives  therefore  for mo cogs  the scalability in the
number of agents is more important  making chengs linear support an attractive starting
point for developing an efficient mo cog solution method 
building on chengs linear support  in section     we create an abstract algorithm that
we call optimistic linear support  ols   which builds up the ccs incrementally  because
ols takes an arbitrary single objective problem solver as input  it can be seen as a generic
multi objective method  we show that ols chooses a w at each iteration such that  after
a finite number of iterations  no further improvements to the partial ccs can be made
and ols can terminate  furthermore  we bound the maximum scalarized error of the
intermediate results  so that they can be used as bounded approximations of the ccs 
then  in section      we instantiate ols by using ve as its single objective problem solver 
yielding vels  an effective mo cog algorithm 
    optimistic linear support
ols constructs the ccs incrementally  by adding vectors to an initially empty partial ccs  
definition     a partial ccs  s  is a subset of the ccs  which is in turn a subset of v 
s  ccs  v 
we define the scalarized value function over s  corresponding to the convex upper surface
 shown in bold  in figure  b d 
definition     a scalarized value function over a partial ccs  s  is a function that takes
a weight vector w as input  and returns the maximal attainable scalarized value with any
   

ficomputing ccss for faster multi objective coordination

 a 

 b 

 c 

 d 

figure     a  all possible payoff vectors for a   objective mo cog   b  ols finds two payoff
vectors at the extrema  red vertical lines   a new corner weight wc              is
found  with maximal possible improvement   ccs is shown as the dotted line 
 c  ols finds a new vector at             and adds two new corner weights to q 
 d  ols calls solvecog for both corner weights  in two iterations   and finds no
new vectors  ensuring s   ccs   ccs 
payoff vector in s 
us  w    max w  u a  
u a s

similarly  we define the set of maximizing joint actions 
definition     the optimal joint action set function with respect to s is a function that
gives the joint actions that maximize the scalarized value 
as  w    arg max w  u a  
u a s

note that as  w  is a set because for some w there can be multiple joint actions that provide
the same scalarized value 
using these definitions  we can describe optimistic linear support  ols   ols adds
vectors to a partial ccs  s  finding new vectors for so called corner weights  these corner
weights are the weights where us  w   definition     changes slope in all directions  these
must thus be weights where as  w   definition     consists of multiple payoff vectors  every
corner weight is prioritized by the maximal possible improvement of finding a new payoff
vector at that corner weight  when the maximal possible improvement is    ols knows
that the partial ccs is complete  an example of this process is given in figure    where
the  corner  weights where the algorithm has searched for new payoff vectors are indicated
by red vertical lines 
ols is shown in algorithm    to find the optimal payoff for a corner weight  ols
assumes access to a function called solvecog that computes the best payoff vector for a
given w  for now  we leave the implementation of solvecog abstract  in section      we
discuss how to implement solvecog  ols also takes as input m  the mo cog to be solved 
and   the maximal tolerable error in the result 
we first describe how ols is initialized  section         then  we define corner weights
formally and describe how ols identifies them  section         finally  we describe how
   

firoijers  whiteson    oliehoek

algorithm    ols m  solvecog   
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

input 
a coggccs 
and an agent i to eliminate 
s
   partial
w     set of checked weights
q  an empty priority queue
foreach extremum of the weight simplex we do
q add we        add extrema with infinite priority
end
while q isempty    timeout do
w  q pop  
u  solvecog m  w 
if u   s then
wdel  remove the corner weights made obsolete by u from q  and store them
wdel   w   wdel   corner weights which are removed because of adding u
wu  newcornerweights u  wdel   s 
s  s   u 
foreach w  wu do
r  w   calculate improvement using maxvaluelp w  s  w 
if r  w     then
q add w  r  w  
end
end
end
w  w   w 
end
return s and the highest r  w  left in q

ols prioritizes corner weights and how this can also be used to bound the error when
stopping ols before it is done finding a full ccs  section        
      initialization
ols starts by initializing the partial ccs  s  which will contain the payoff vectors in the
ccs discovered so far  line   of algorithm     as well as the set of visited weights w  line
    then  it adds the extrema of the weight simplex  i e   those points where all of the
weight is on one objective  to a priority queue q  with infinite priority  line    
these extrema are popped off the priority queue when ols enters the main loop  line
    in which the w with the highest priority is selected  line     solvecog is then called
with w  line    to find u  the best payoff vector for that w 
for example  figure  b shows s after two payoff vectors of a   dimensional mocog have been found by applying solvecog to the extrema of the weight simplex  s  
                  each of these vectors must be part of the ccs because it is optimal for
at least one w  the one for which solvecog returned it as a solution  the extrema of the
weight simplex   the set of weights w that ols has tested so far are marked with vertical
red line segments 
   

ficomputing ccss for faster multi objective coordination

      corner weights
after having evaluated the extrema  s consists of d  the number of objectives  payoff vectors
and associated joint actions  however  for many weights on the simplex  it does not yet
contain the optimal payoff vector  therefore  after identifying a new vector u to add to s
 line     ols must determine what new weights to add to q  like chengs linear support 
ols does so by identifying the corner weights  the weights at the corners of the convex
upper surface  i e   the points where the pwlc surface us  w  changes slope  to define
the corner weights precisely  we must first define p   the polyhedral subspace of the weight
simplex that is above us  w   bertsimas   tsitsiklis         the corner weights are the
vertices of p  which can be defined by a set of linear inequalities 
definition     if s is the set of known payoff vectors  we define a polyhedron
x
p    x   d     s   x      i  wi     
wi      
i

where s   is a matrix with the elements of s as row vectors  augmented by a column vector
of  s  the setpof linear inequalities s   x      is supplemented by the simplex constraints 
i wi     and i wi      the vector x    w         wd   u  consists of a weight vector and
a scalarized value at those weights  the corner weights are the weights contained in the
vertices of p   which are also of the form  w         wd   u  
note that  due to the simplex constraints  p is only d dimensional  furthermore  the
extrema of the weight simplex are special cases of corner weights 
after identifying u  ols identifies which corner weights change in the polyhedron p by
adding u to s  fortunately  this does not require recomputation of all the corner weights 
but can be done incrementally  first  the corner weights in q for which u yields a better
value than currently known are deleted from the queue  line     and then the function
newcornerweights u  wdel   s  at line    calculates the new corner weights that involve u
by solving a system of linear equations to see where u intersects with the boundaries and
the relevant subset of the present vectors in s 
newcornerweights u  wdel   s   line     first calculates the set of all relevant payoff
vectors  arel   by taking the union of all the maximizing vectors of the weights in wdel    
arel  

 

as  w  

wwdel

if any as  w  contains fewer than d payoff vectors  then a boundary of the weight simplex
is involved  these boundaries are also stored  all possible subsets of size d     of vectors
and boundaries  are taken  for each subset the weight where these d    payoff vectors
 and or boundaries  intersect with each other and u is computed by solving a system of
linear equations  the intersection weights for all subsets together form the set of candidate
corner weights  wcan   newcornerweights u  wdel   s  returns the subset of wcan which are
inside of the weight simplex and for which u has a higher scalarized value than any payoff
   in fact  in our implementation  we optimize this step by caching as  w  for all w in q 

   

firoijers  whiteson    oliehoek

vector already in s  figure  b shows one new corner weight labelled wc               in
practice   arel   is very small  so only a few systems of linear equations need to be solved   
after calculating the new corner weights wu at line     u is added to s at line    
cheng showed that finding the best payoff vector for each corner weight and adding it to
the partial ccs  i e   s  s   solvecog w    guarantees the best improvement to s 
theorem     cheng       the maximum value of 
max

min w  u  w  v 

w uccs vs

i e   the maximal improvement to s by adding a vector to it  is at one of the corner weights
 cheng        
theorem   guarantees the correctness of ols  after all corner weights are checked  there
are no new payoff vectors  thus the maximal improvement must be   and ols has found
the full ccs 
      prioritization
chengs linear support assumes that all corner weights can be checked inexpensively  which
is a reasonable assumption in a pomdp setting  however  since solvecog is an expensive
operation  testing all corner weights may not be feasible in mo cogs  therefore  unlike
chengs linear support  ols pops only one w off q to be tested per iteration  making
ols efficient thus critically depends on giving each w a suitable priority when adding it
to q  to this end  ols prioritizes each corner weight w according to its maximal possible
improvement  an upper bound on the improvement in us  w   this upper bound is computed
with respect to ccs  the optimistic hypothetical ccs  i e   the best case scenario for the
final ccs given that s is the current partial ccs and w is the set of weights already
tested with solvecog  the key advantage of ols over chengs linear support is that these
priorities can be computed without calling solvecog  obviating the need to run solvecog
on all corner weights 
definition     an optimistic hypothetical ccs  ccs is a set of payoff vectors that yields
the highest possible scalarized value for all possible w consistent with finding the vectors s
at the weights in w 
figure  b denotes the ccs                            with a dotted line  note that ccs is a
superset of s and the value of uccs  w  is the same as us  w  at all the weights in w  for

a given w  maxvaluelp finds the the scalarized value of uccs
 w  by solving 
max w  v
subject to w v  us w  
    however  in theory it is possible to construct a partial ccs  s that has a corner weight for which all
payoff vectors in s are in adel  

   

ficomputing ccss for faster multi objective coordination

where us w is a vector containing us  w    for all w   w  note that we abuse the notation
w  which in this case is a matrix whose rows consist of all the weight vectors in the set
w   
using ccs  we can define the maximal possible improvement 
 w    uccs  w   us  w  
figure  b shows  wc   with a dashed line  we use the maximal relative possible improvement  r  w     w  uccs  w   as the priority of each new corner weight w  wu   in

figure  b  r  wc                         
       when a corner weight w is identified  line     
   
it is added to q with priority r  w  as long as r  w      lines        
after wc in figure  b is added to q  it is popped off again  as it is the only element
of q   solvecog wc   generates a new vector         yielding s                             as
illustrated in figure  c  the new corner weights                and                are the
points at which        intersects with        and         testing these weights  as illustrated in
figure  d  does not result in new payoff vectors  causing ols to terminate  the maximal
improvement at these corner weights is   and thus  due to theorem    s   ccs upon
termination  ols called solvecog for only   weights resulting exactly in the   payoff
vectors of the ccs  the other   payoff vectors in v  displayed as grey and dashed black
lines in figure  a  were never generated 
    variable elimination linear support
any exact cog algorithm can be used to implement solvecog  a naive approach is to
explicitly compute the values of all joint actions v and select the joint action that maximizes
this value 
solvecog m  w    arg max w  u a  
u a v

this implementation of solvecog in combination with ols yields an algorithm that we
refer to as non graphical linear support  ngls   because it ignores the graphical structure 
flattening the cog into a standard multi objective cooperative normal form game  the
main downside is that the computational complexity of solvecog is linear in  v   which
is equal to  a    which is exponential in the number of agents  making it feasible only for
mo cogs with very few agents 
by contrast  if we use ve  section      to implement solvecog  we can do better  we
call the resulting algorithm variable elimination linear support  vels   having dealt with
the multiple objectives in the outer loop of ols  vels relies on ve to exploit the graphical
structure in the inner loop  yielding a much more efficient method than ngls 
    analysis
we now analyze the computational complexity of vels 
    our implementation of ols reduces the size of the lp by using only the subset of weights in w for
which the joint actions involved in w  as  w   have been found to be optimal  this can lead to a slight

overestimation of uccs
 w  

   

firoijers  whiteson    oliehoek

theorem    the runtime of vels with      is
o   ccs     wccs    n amax  w   cnw   cheur    
where w is the induced width when running ve   ccs  is the size of the ccs   wccs   is
the number of corner weights of uccs  w   cnw the time it costs to run newcornerweights 
and cheur the cost of the computation of the value of the optimistic ccs using maxvaluelp 
proof  since n amax  w is the runtime of ve  theorem     the runtime of vels is this
quantity  plus the overhead per corner weight cnw   cheur   multiplied by the number of
calls to ve  to count these calls  we consider two cases  calls to ve that result in adding
a new vector to s and those that do not result in a new vector but instead confirm the
optimality of the scalarized value at that weight  the former is the size of the final ccs 
 ccs   while the latter is the number of corner weights for the final ccs   wccs   
the overhead of ols itself  i e   computing new corner weights  cnw   and calculating
the maximal relative improvement  cheur   is very small compared to the solvecog calls 
in practice  newcornerweights u  wdel   s  computes the solutions to only a small set of
linear equations  of d equations each   maxvaluelp w  s  w  computes the solutions to
linear programs  which is polynomial in the size of its inputs   
for d      the number of corner weights is smaller than  ccs  and the runtime of
vels is thus o n amax  w  ccs    for d      the number of corner weights is twice  ccs 
 minus a constant  because  when solvecog finds a new payoff vector  one corner weight is
removed and three new corner weights are added  for d      a loose bound on  wccs   is

the total number of possible combinations of d payoff vectors or boundaries  o   ccs  d
  
d
however  we can obtain a tighter bound by observing that counting the number of corner
weights given a ccs is equivalent to vertex enumeration  which is the dual problem of
facet enumeration  i e   counting the number of vertices given the corner weights  kaibel  
pfetsch        
theorem     for arbitrary d   wccs   is bounded by o 
 avis   devroye        

 ccs b d  
c
 
 ccs d

 

 ccs b d  
c
 
 
 ccs d

proof  this result follows directly from mcmullens upper bound theorem for facet enumeration  henk  richter gebert    ziegler        mcmullen        
the same reasoning used to prove theorems   can also be used to establish the following 
corollary    the runtime of vels with     is
o    ccs     wccs    n amax  w   cnw   cheur    where   ccs  is the size of the  ccs 
and  wccs   is the number of corner weights of uccs  w  
in practice  vels will often not test all the corner weights of the polyhedron spanned
by the  ccs  but this cannot be guaranteed in general  in section      we show empirically
that   ccs  decreases rapidly as  increases 
    when the reduction in footnote    is used  only a very small subset of w is used  making it even smaller 

   

ficomputing ccss for faster multi objective coordination

figure     left  the runtimes of pmove  cmove and vels with different values of  
for varying numbers of agents  n  and       n factors    actions per agent  and
  objectives and  right  the corresponding sizes of the  ccss 
theorem     the space complexity of vels is o d  ccs  d wccs   n amax  w   with
    
proof  ols needs to store every corner weight  a vector of length d  in the queue  which
is at most  wccs    ols also needs to store every vector in s  also vectors of length d  
furthermore  when solvecog is called  the memory usage of ve is added to the memory
usage of the outer loop of ols  the memory usage of ve is n amax  w  theorem    
because ols adds few memory requirements to that of ve  vels is almost as memory
efficient as ve and thus considerably more memory efficient than cmove  theorem    
    empirical evaluation
we now empirically evaluate vels  in comparison to cmove and pmove  we no longer
compare against the non graphical method as this is clearly dominated by cmove and
pmove  where we refer to cmove in this section  we mean basic cmove  as this was
fastest for the tested scenarios  like before  we use both random graphs and the mining day
benchmark  all experiments in this section were run on a     ghx intel core i  computer 
with   gb memory 
      random graphs
to test vels on randomly generated mo cogs  we use the same mo cog generation
procedure as in section    to determine how the scalability of exact and approximate
vels compares to that of pmove and cmove  we tested them on random mo cogs
with increasing numbers of agents  the average number of factors per agent was held at
      n and the number of objectives at d      figure   shows the results  which are
averaged over    mo cogs for each number of agents  note that the runtimes on the left 
on the y axis  are in log scale but the set sizes on the right are not 
these results demonstrate that vels is more efficient than cmove for two objective
random mo cogs  the runtime of exact vels        is on average    times less than
   

firoijers  whiteson    oliehoek

that of cmove  cmove solves random mo cogs with    agents in   s on average  whilst
exact vels can handle     agents in   s 
while this is already a large gain  we can achieve an even lower growth rate by permitting
a small   for     agents  permitting a       error margin yields a gain of more than an
order of magnitude  reducing the runtime to    s  permitting a      error reduces the
runtime to only    s  we can thus reduce the runtime of vels by a factor of     while
retaining     accuracy  compared to cmove at    agents  vels with         is    
times faster 
these speedups can be explained by the slower growth of the  ccs  figure    right   
for small numbers of agents  the size of the  ccs grows only slightly more slowly than
the size of the full ccs  however  from a certain number of agents onwards  the size of the
 ccs grows only marginally while the size of the full ccs keeps on growing  for         
the  ccs grew from      payoff vectors to      payoff vectors between   and    agents 
and then only marginally to      at     agents  by contrast  the full ccs grew from     
to      vectors between   and    agents  but then keeps on growing to       at     agents 
a similar picture holds for the       ccs  which grows rapidly from      vectors at   to
      vectors at    agents  then grows slowly to       at    agents  and then stabilizes  to
reach       vectors at     agents  between    and     agents  the full ccs grows from
      vectors to       vectors  making it almost   times as large as the       ccs and  
times larger than the      ccs  
to test the scalability of vels with respect to the number of objectives  we tested it
on random mo cogs with a constant number of agents and factors n      and       n 
but increased the number of objectives  for      and         we compare this to the
scalability of cmove  we kept the number of agents  n       and the number of local
payoff functions         small in order to test the limits of scalability in the number of
objectives  the number of actions per agent was    figure     left  plots the number of
objectives against the runtime  in log scale   because the ccs grows exponentially with
the number of objectives  figure     right    the runtime of cmove is also exponential in
the number of objectives  vels however is linear in the number of corner weights  which is
exponential in the size of the ccs  making vels doubly exponential  exact vels       
is faster than cmove for d     and d      and for d     approximate vels with       
is more than    times faster  however for d     even approximate vels with        is
slower than cmove 
unlike when the number of agents grows  the size of the  ccs  figure     right   does
not stabilize when the number of objectives grows  as can be seen in the following table 
 ccs 
d  
d  
d  

  
    
    
     

        
   
    
     

       
   
    
     

      
   
    
     

we therefore conclude that vels can compute a ccs faster than cmove for   objectives
or less  but that cmove scales better in the number of objectives  vels however  scales
better in the number of agents 
   

ficomputing ccss for faster multi objective coordination

figure      left  the runtimes of cmove and vels       and          for varying numbers of objectives  right  the size of the  ccs for varying numbers of objectives 

figure      left  plot of the runtimes of cmove and vels with different values of   for
varying n  up to        right  loglogplot of the runtime of vels on          
and      agent mining day instances  for varying values of  
      mining day
we now compare cmove and vels on the mining day benchmark using the same generation procedure as in section        we generated    mining day instances for increasing n
and averaged the runtimes  figure     left    at     agents  cmove has reached a runtime
of   s  exact vels        can compute the complete ccs for a mo cog with     agents
in the same time  this indicates that vels greatly outperforms cmove on this structured
  objective mo cog  moreover  when we allow only      error             it takes only
   s to compute an  ccs for     agents  a speedup of over an order of magnitude 
to measure the additional speedups obtainable by further increasing   and to test vels
on very large problems  we generated mining day instances with n                    we
averaged over    instances per value of   on these instances  exact vels runs in    s
for n          s for n       and    s for n        on average  as expected  increasing
 leads to greater speedups  figure     right    however  when  is close to    i e   the
   

firoijers  whiteson    oliehoek

 ccs is close to the full ccs  the speedup is small  after  has increased beyond a certain
value  dependent on n   the decline becomes steady  shown as a line in the log log plot  if
 increases by a factor     the runtime decreases by about a factor     
thus  these results show that vels can compute an exact ccs for unprecedented
numbers of agents        in well structured problems  in addition  they show that small
values of  enable large speedups  and that increasing  leads to even bigger improvements
in scalability 

   memory efficient methods
both cmove and vels are designed to minimize the runtime required to compute a ccs 
however  in some cases  the bottleneck may be memory instead  memory efficient methods
for cogs and related problems have recently received considerable attention  dechter  
mateescu        marinescu              mateescu   dechter         in this section  we
show that  because it is an outer loop method  vels is naturally memory efficient and
can therefore solve much larger mo cogs than an inner loop method like cmove when
memory is restricted  in addition  we show how both cmove and vels can be modified
to produce even more memory efficient variants 
    and or tree search
we begin with some background on and or tree search  dechter   mateescu       
marinescu        mateescu   dechter        yeoh  felner    koenig         a class of
algorithms for solving single objective cogs that can be tuned to provide better space
complexity guarantees than ve  however  the improvement in space complexity comes at
a price  i e   the runtime complexity is worse  mateescu   dechter         the background
we provide is brief  for a broader overview of and or tree search for cogs and related
models please see the work of dechter        and marinescu         and for multi objective
versions the work of marinescu              
and or tree search algorithms work by converting the graph to a pseudo tree  pt 
such that each agent need only know which actions its ancestors and descendants in the pt
take in order to select its own action  for example  if an agent i  a node  in the pt has two
subtrees  t  and t    under it  all the agents in t  are conditionally independent of all the
agents in t  given i and the ancestors of i  figure   a shows the pt for the coordination
graph in figure  a 
next  and or tree search algorithms perform a tree search that results in an and or
search tree  aost   each agent i in an aost is an or node  its children are and nodes 
each corresponding to one of agent is actions  in turn  the children of these and nodes are
or nodes corresponding to agent is children in the pt  because each action  and nodes 
of agent i has the same agents under it as or nodes  the agents and actions can appear in
the tree multiple times  figure   b shows an aost for the graph of figure  a 
a specific joint action can be constructed by traversing the tree  starting at the root and
selecting one alternative from the childen of each or node  i e   one action for each agent 
and continuing down all children of each and node  for example  in figure   b  the joint
action   a    a    a    is indicated in grey  to retrieve the value of a joint action  we must
first define the value of and nodes 
   

ficomputing ccss for faster multi objective coordination

figure      a  a pseudo tree   b  a corresponding and or search tree 
definition     the value of an and node vai   representing an action ai of an agent i is
the sum of the local payoff functions that have i in scope  ai   together with its and node
ancestors actions  specifies an action for each agent in scope of these local payoff functions 
for example  in figure   b  the total payoff of the cog is u a    a    a      u   a    a     
u   a    a     the value of the grey and node a  is u   a    a     as u  is the only payoff function
that has agent   in scope and  together with its ancestral and nodes  the grey a   node  a 
completes a joint local action for u   
to retrieve the optimal action  we must define the value of a subtree in the aost 
definition     the value of a subtree v ti   rooted by an or node i in an aost is the
maximum of the value of the subtrees rooted by the  and node  children of i  the value of
a subtree v tai   rooted by an and node ai in an aost is the value of ai itself  definition
    plus the sum of the value of the subtrees rooted by the  or node  children of ai  
the most memory efficient way to retrieve the optimal joint action using an aost
is euler touring it  i e   performing a depth first search and computing the values of the
subtrees  by generating nodes on the fly and deleting them after they are evaluated  memory
usage is minimized  we refer to this algorithm simply as and or tree search  ts   as
in earlier sections  our implementation employs a tagging scheme  tagging the value of a
subtree with the actions that maximize it 
while ts is a single objective method  it has been extended to compute the pcs 
yielding an algorithm we call pareto ts  pts   marinescu         to define pts  we must
update definition    to be a set of pareto optimal payoffs  we refer to such a subtree value
set as an intermediate pcs  ipcs  
definition     the intermediate pcs of a subtree  ip cs ti   rooted by an or node i is
the pcs of the union of the intermediate pcss of the children  ch i   of i 
ip cs ti     pprune 

 

aj ch i 

   

ip cs taj    

firoijers  whiteson    oliehoek

the intermediate pcs of a subtree  ip cs tai   rooted by an and node ai is the pcs of the
value of ai itself  definition     plus the cross sum of the intermediate pcss of the subtrees
rooted by the  or node  children of ai  


m
ip cs tj     vai    
ip cs tai     pprune 
jch ai  

thus  pts replaces the max operator in ts by a pruning operator  just as pmove replaces
the max operator in ve by a pruning operator 
    memory efficient ccs algorithms
we now propose two memory efficient algorithms for computing a ccs  both are straightforward variants of cmove and vels 
the first algorithm  which we call convex ts  cts   simply replaces pprune by cprune
in definition     thus  cts is like pts but with a different pruning operator  it can
also be seen as cmove but with ve replaced with ts  the advantage of cts over pts
is analogous to that of cmove over pmove  it is highly beneficial to compute local
ccss instead of local pcss because the intermediate coverage sets are input to the next
subproblem in a sequential search scheme  regardless of whether that scheme is ve or ts 
while cts is more memory efficient than cmove  it still requires computing intermediate
coverage sets that take up space  while these are typically only about as large as the ccs 
their size is bounded only by the total number of joint actions 
the second algorithm addresses this problem by employing ols with ts as the singleobjective solver subroutine  solvecog  yielding tree search linear support  tsls   thus 
tsls is like vels but with ve replaced by ts  because tsls is an outer loop method  it
runs ts in sequence  requiring only the memory used by ts itself and the overhead of the
outer loop  which consists only of the partial ccs  definition     and the priority queue 
consequently  tsls is even more memory efficient than cts 
    analysis
ts has much better space complexity than ve  i e   only linear in the number of agents n 
theorem     the time complexity of ts is o n amax  m    where n is the number of agents 
 amax   is the maximal number of actions of a single agent and m is the depth of the pseudo
tree  and uses linear space  o n  
proof  the number of nodes in an aost is bounded by o n amax  m    the tree creates
maximally  amax   children at each or node  if every and node had exactly one child  the
number of nodes would be bounded by o  amax  m    as the pt is m deep  however  if there
is branching in the pt  an and node can have multiple children  each branch increases
the size of the aost by at most o  amax  m   nodes  because there are exactly n agents in
the pt  this can happen at most n times  at each node in the aost  ts performs either
a summation of scalars  or a maximization over scalars  because ts performs depth first
search  at most o n  nodes need to exist at any point during execution 
   

ficomputing ccss for faster multi objective coordination

tss memory usage is usually lower than that required to store the original  singleobjective  problem in memory  o  amax  emax    where  is the number of local payoff
functions in the problem   amax   is the maximal size of the action space of a single agent 
and emax is the maximal size of the scope of a single local payoff function 
the pt depth m is a different constant than the induced width w  and is typically
larger  however  m can be bounded in w 
theorem     given a mo cog with induced width w  there there exists a pseudo tree for
which the depth m  w log n  dechter   mateescu        
thus  combining theorems    and    shows that  when there are few agents  ts can
be much more memory efficient than ve with a relatively small runtime penalty 
using the time and space complexity results for ts  we can establish the following
corollaries about the time and space complexity of cts and tsls 
corollary    the time complexity of cts is o n amax  m r   where r is the runtime of
cprune 
proof  o n amax  m   bounds the number of nodes in the aost  for each node in the aost
cprune is called 
the runtime of cprune in terms of the size of its input is given in equation    note
that the size of the input of cprune depends on the size of the intermediate ccss of
the children of a node  in the case of an and node  this input size is o  iccsmax  c   
where c is the maximum number of children of an and node    for or nodes this is
o  amax   iccsmax    
corollary    the space complexity of cts is o n iccsmax     where  iccsmax   is the
maximum size of an intermediate ccs during the execution of cts 
proof  like in ts  only o n  nodes of the aost need to exist during any point during
execution  and each node contains an intermediate ccs 
cts is thus much more memory efficient than cmove  which has a space complexity
that is exponential in the induced width  theorem    
corollary    the time complexity of tsls is o    ccs   w  ccs     n  amax  m  cnw  
cheur     where m  w log n and     
proof  the proof is the same as that of theorem   but with the time complexity of ve
replaced by that of ts 
in terms of memory usage  the outer loop approach  ols  has a large advantage over
the inner loop approach  because the overhead of the outer loop consists only of the partial
ccs  definition     and the priority queue  vels  theorem     thus has much better
space complexity than cmove  theorem     tsls has the same advantage over cts as
vels over cmove  therefore  tsls has very low memory usage  since it requires only
the memory used by ts itself plus the overhead of the outer loop 
    note that c is in turn upper bounded by n but this is a very loose bound 

   

firoijers  whiteson    oliehoek

corollary    the space complexity of tsls is o d  ccs    d w  ccs     n    where
m  w log n and     
proof  the proof is the same as that of theorem    but with the space complexity of ve
replaced by that of ts 
as mentioned in section      ts is the most memory efficient member of the class of
and or tree search algorithms  other members of this class offer different trade offs
between time and space complexity  it is possible to create inner loop algorithms and
corresponding outer loop algorithms on the basis of these other algorithms  the time
and space complexity analyses of these algorithms can be performed in a similar manner to
corollaries     the advantages of the outer loop methods compared to their corresponding
inner loop methods will however remain the same as for tsls and cts  therefore  in this
article we focus on comparing the most memory efficient inner loop method against the
most memory efficient outer loop method 
    empirical evaluation
in this section  we compare cts and tsls to cmove and vels  as before  we use both
random graphs and the mining day benchmark  to obtain the pts for cts and tsls 
we use the same heuristic as cmove and vels to generate an elimination order and
then transform it into a pt for which m  w log n holds  whose existence is guaranteed by
theorem      using the procedure suggested by bayardo and miranker        
      random graphs
first  we test our algorithms on random graphs  employing the same generation procedure
as in section        because connections between agents in these graphs are generated
randomly  the induced width varies between different problems  on average  the induced
width increases with the number of local payoff functions  even when the ratio between
local payoff factors and the number of agents remains constant 
in order to test the sizes of problems that the different mo cog solution methods can
handle within limited memory  we generate random graphs with two objectives  a varying
number of agents n  and with       n local payoff functions  as in previous sections  we
limited the maximal available memory to  kb and imposed a timeout of     s 
figure   a shows that vels can scale to more agents within the given memory constraints than the other non memory efficient methods  in particular  pmove and cmove
can handle only    and    agents  respectively  because  for a given induced width w  they
must store o  amax  w   local css  at    agents  the induced width  figure   c  is at most
   while at    agents the induced width is at most    vels can handle    agents  with an
induced width of at most     because most of its memory demands come from running ve
in the inner loop  while the outer loop adds little overhead  ve need only store one payoff
in each new local payoff function that results from an agent elimination  whereas pmove
and cmove must store local coverage sets  thus  using an outer loop approach  vels 
instead of the inner loop approach  cmove  already yields a significant improvement in
the problem sizes that can be tackled with limited memory 
   

ficomputing ccss for faster multi objective coordination

 a 

 b 

 c 

figure      a  runtimes in ms of tsls  vels  cts  cmove and pmove on random  objective mo cogs with varying numbers of agents n and       n local payoff
factors   b  runtimes of approximate tsls for varying amounts of allowed error
  compared to  exact  vels  for the same problem parameters as in  a    c 
the corresponding induced widths of the mo cogs in  b  

however  scaling beyond    agents requires a memory efficient approach  figure   a
also shows that  while cts and tsls require more runtime  they can handle more agents
within the memory constraints  in fact  we were unable to generate a mo cog with enough
agents to cause these methods to run out of memory  tsls is faster than cts  in this case
    times faster  for the same reasons that vels is faster than cmove 
however  speed is not the only advantage of the outer loop approach  when we allow
a bit of error in scalarized value    we can trade accuracy for runtime  figure   b   at   
agents  exact tsls         had an average runtime of    s  which is    times slower than
vels  however  for            the runtime was only   s     times slower   for         it
is   s      times slower   and for        it is only  s      times slower   furthermore  the
relative increase in runtime as the number of agents increases is less for higher   thus  an
approximate version of tsls is a highly attractive method for cases in which both memory
and runtime are limited 
      mining field
we compare the performance of cmove and vels against tsls on a variation of mining
day that we call mining field  we no longer consider cls because it has consistently higher
runtime than tsls and worse space complexity  we use mining field in order to ensure
an interesting problem for the memory restricted setting  in mining day  see section    
the induced width depends only on the parameter specifying the connectivity of the villages
and does not increase with the number of agents and factors  therefore  whether or not
vels is memory efficient enough to handle a particular instance depends primarily on this
parameter and not on the number of agents 
in mining field  the villages are not situated along a mountain ridge but are placed on
an s  s grid  the number of agents is thus n   s    we use random placement of mines 
while ensuring that the graph is connected  because the induced width of a connected grid
is s and we generate grid like graphs  larger instances have a higher induced width  the
   

firoijers  whiteson    oliehoek

village

 a 

mine

 b 

 c 

figure      a  an example of a   by   mining field instance  the additional mines m are
marked with a     b  runtimes in ms of tsls  for varying amounts of allowed
error    vels         and cmove on   objective mining field instances with
varying numbers of additional mines m          and a grid size of s       c 
the corresponding induced widths of the mining field instances 

induced width thus no longer depends only on the connectivity parameter but also increases
with the number of agents and factors in the graph 
an example mining field instance is provided in figure   a  we choose the distance
between adjacent villages on the grid to be unit length  on this map  we then place the
mines  local payoff functions   we connect all agents using an arbitrary tree using   agent
local payoff functions  mines   in the figures  the mines that span this tree are unmarked
and connected to the mines with black edges  we require s     factors to build the tree 
then we add m additional mines  by  independently  placing them on a random point on
the map inside the grid  when a mine is placed  we connect it to the villages that are within
a r         radius of that mine on the map  we chose         therefore  the maximum
connectivity of a factor  mine  created in this fashion is    in the figure  these mines are
marked with a    the rewards per mine per worker  as well as the number of workers per
village  are generated in the same way as in mining day 
to compare the runtimes and memory requirements of cmove  vels  and tsls on
mining field  we tested them on a      instance     agents   with  mb available memory 
for tsls  we use three different values of      exact        and      we use a time limit of
         s     minutes   we increase the number of additional mines m from       factors
in total  onwards  by steps of   
using this setup  it was not possible to solve any of the problem instances using pmove 
which ran out of memory for all problems  in fact  pmove succeeded only a tree shaped
problem  i e   one without any additional factors  figures   b and   c  show the results for
the remaining methods  cmove runs out of memory at   additional factors     factors in
total   by contrast  vels runs out of memory only at    additional factors  at an induced
width of   
compared to the random graph results in section        the induced widths of the
problems that cmove and vels can handle are lower in mining field  we suspect that
   

ficomputing ccss for faster multi objective coordination

this is because  on a grid shaped problem  the number of factors with the highest induced
width that need to exist in parallel during the execution of the algorithms is higher 
tsls does not run out of memory on any of the tested instances  in face  we were
unable to generate instances for which tsls does run out of memory  however  it does
run out of time  for       tsls first exceeds the time limit at m      additional mines 
for          this happens at m       for         tsls ran out of time at m      
the differences in runtime between tsls and vels are larger than for random graphs
and therefore it is more difficult to compensate for the slower runtime of tsls by choosing
a higher   how much slower tsls is compared to vels thus seems to depend on the
structure of the mo cog 
these mining field results confirm the conclusion of the random graph experiments that
using an outer loop approach  vels  instead of the inner loop approach  cmove  yields
a significant improvement in the problem sizes that can be tackled with limited memory 
futhermore  tsls can be used to solve problem sizes beyond those that vels can handle
within limited memory  an approximate version of tsls is an appealing choice for cases
in which both memory and runtime are limited 

   conclusions and future work
in this article  we proposed new algorithms that exploit loose couplings to compute a ccs
for multi objective coordination graphs  we showed that exploiting these loose couplings
is key to solving mo cogs with many agents  in particular  we showed  both theoretically
and empirically  that computing a ccs has considerable advantages over computing a pcs
in terms of both runtime and memory usage  our experiments have consistently shown
that the runtime of pcs methods grows a lot faster than that of our ccs methods 
cmove deals with multiple objectives in the inner loop  i e   it computes local ccss
while looping over the agents  by contrast  vels deals with multiple objectives in the
outer loop  i e   it identifies weights where the maximal improvement upon a partial ccs
can be made and solves scalarized  single objective  problems using these weights  yielding
an anytime approach  in addition  cts and tsls are memory efficient variants of these
methods  we proved the correctness of these algorithms and analyzed their complexity 
cmove and vels are complementary methods  cmove scales better in the number
of objectives  while vels scales better in the number of agents and can compute an ccs  leading to large additional speedups  furthermore  vels is more memory efficient
than cmove  in fact  vels uses little more memory than single objective ve 
however  if memory is very restricted and vels cannot be applied  tsls provides a
memory efficient alternative  while tsls is considerably slower than vels  some of this
loss can be compensated by allowing some error    
there are numerous possibilities for future work  as mentioned in section    ols is a
generic method that can also be applied to other multi objective problems  in fact   together
with other authors  we already applied ols to large multi objective mdps and showed that
ols can be extended to permit non exact single objective solvers  roijers et al          in
future work  we intend to investigate  approximate methods for mo cogs  by using approximate single objective solvers for cogs  using  e g   lp relaxation methods  sontag 
globerson    jaakkola         we will attempt to find the optimal balance between the
   

firoijers  whiteson    oliehoek

levels of approximation in the inner and outer loop  with respect to runtime guarantees and
empirical runtimes 
many methods exist for single objective coordination graphs in which a single parameter
controls the trade off between memory usage and runtime  furcy   koenig        rollon 
       for some of these algorithms  a corresponding multi objective inner loop version
that computes a pcs  marinescu              has been devised  it would be interesting
to create inner and outer loop methods based on these methods that compute a ccs
instead and compare performance  in particular  we have shown that ols requires very
little extra memory usage compared to single objective solvers  it would be interesting to
investigate how much extra memory could be used by a single objective solver inside ols 
in comparison to the corresponding inner loop method 
in addition to further work on mo cogs  we also aim to extend our work to sequential
settings  in particular  we will look at developing an efficient planning method for multiagent multi objective mdps by better exploiting loosely couplings  first  we will try to
develop an  approximate planning version of sparse cooperative q learning  kok   vlassis 
    b   however  this may not be possible in general because the effects of an agent on
other agents via the state is impossible to bound in general  therefore  we hope to identify a
broadly applicable subclass of multi agent momdps for which an  approximate planning
method yields a substantial speed up compared to exact planning methods 

acknowledgements
we thank rina dechter for introducing us to memory efficient methods for cogs and
mo cogs  and radu marinescu for his tips on memory efficient methods and their implementation  also  we would like to thank maarten inja  as well as the anonymous reviewers  for their valuable feedback  this research is supported by the nwo dtc ncap
               and nwo catch                projects and nwo innovational research incentives scheme veni                 frans oliehoek is affiliated with both the
university of amsterdam and the university of liverpool 

references
avis  d     devroye  l          estimating the number of vertices of a polyhedron  information processing letters                 
bayardo  r  j  j     miranker  d  p          on the space time trade off in solving constraint
satisfaction problems  in ijcai       proceedings of the fourteenth international
joint conference on artificial intelligence 
bertsimas  d     tsitsiklis  j          introduction to linear optimization  athena scientific 
bishop  c  m          pattern recognition and machine learning  springer 
cassandra  a   littman  m     zhang  n          incremental pruning  a simple  fast  exact
method for partially observable markov decision processes  in uai       proceedings
of the thirteenth conference on uncertainty in artificial intelligence  pp       
   

ficomputing ccss for faster multi objective coordination

cheng  h  t          algorithms for partially observable markov decision processes  ph d 
thesis  university of british columbia  vancouver 
dechter  r          reasoning with probabilistic and deterministic graphical models  exact algorithms  vol    of synthesis lectures on artificial intelligence and machine
learning  morgan   claypool publishers 
dechter  r     mateescu  r          and or search spaces for graphical models  artificial
intelligence                 
delle fave  f   stranders  r   rogers  a     jennings  n          bounded decentralised
coordination over multiple objectives  in proceedings of the tenth international joint
conference on autonomous agents and multiagent systems  pp         
dubus  j   gonzales  c     perny  p          choquet optimization using gai networks
for multiagent multicriteria decision making  in adt       proceedings of the first
international conference on algorithmic decision theory  pp         
feng  z     zilberstein  s          region based incremental pruning for pomdps  in uai
      proceedings of the twentieth conference on uncertainty in artificial intelligence  pp         
furcy  d     koenig  s          limited discrepancy beam search  in ijcai       proceedings of the nineteenth international joint conference on artificial intelligence  pp 
       
guestrin  c   koller  d     parr  r          multiagent planning with factored mdps  in
advances in neural information processing systems     nips    
henk  m   richter gebert  j     ziegler  g  m          basic properties of convex polytopes 
in handbook of discrete and computational geometry  ch     pp          crc
press  boca 
kaibel  v     pfetsch  m  e          some algorithmic problems in polytope theory  in
algebra  geometry and software systems  pp        springer 
kok  j  r     vlassis  n          sparse cooperative q learning  in proceedings of the
twenty first international conference on machine learning  icml     new york  ny 
usa  acm 
kok  j  r     vlassis  n       a   using the max plus algorithm for multiagent decision
making in coordination graphs  in robocup       robot soccer world cup ix  pp 
    
kok  j     vlassis  n       b   collaborative multiagent reinforcement learning by payoff
propagation  journal of machine learning research              
koller  d     friedman  n          probabilistic graphical models  principles and techniques  mit press 
lizotte  d   bowling  m     murphy  s          efficient reinforcement learning with multiple
reward functions for randomized clinical trial analysis  in proceedings of the   th
international conference on machine learning  icml      pp         
   

firoijers  whiteson    oliehoek

marinescu  r   razak  a     wilson  n          multi objective influence diagrams  in
uai       proceedings of the twenty eighth conference on uncertainty in artificial
intelligence 
marinescu  r          and or search strategies for combinatorial optimization in graphical models  ph d  thesis  university of california  irvine 
marinescu  r          exploiting problem decomposition in multi objective constraint optimization  in principles and practice of constraint programming cp       pp     
     springer 
marinescu  r          efficient approximation algorithms for multi objective constraint
optimization  in adt       proceedings of the second international conference on
algorithmic decision theory  pp          springer 
mateescu  r     dechter  r          the relationship between and or search and variable
elimination  in uai       proceedings of the twenty first conference on uncertainty
in artificial intelligence  pp         
mcmullen  p          the maximum numbers of faces of a convex polytope  mathematika 
               
oliehoek  f  a   spaan  m  t  j   dibangoye  j  s     amato  c          heuristic search
for identical payoff bayesian games  in aamas       proceedings of the ninth international joint conference on autonomous agents and multiagent systems  pp 
         
pearl  j          probabilistic reasoning in intelligent systems  networks of plausible inference  morgan kaufmann 
pham  t  t   brys  t   taylor  m  e   brys  t   drugan  m  m   bosman  p  a   cock 
m  d   lazar  c   demarchi  l   steenhoff  d   et al          learning coordinated
traffic light control  in proceedings of the adaptive and learning agents workshop  at
aamas      vol      pp           
roijers  d  m   scharpff  j   spaan  m  t  j   oliehoek  f  a   de weerdt  m     whiteson 
s          bounded approximations for linear multi objective planning under uncertainty  in icaps       proceedings of the twenty fourth international conference
on automated planning and scheduling  pp         
roijers  d  m   vamplew  p   whiteson  s     dazeley  r       a   a survey of multiobjective sequential decision making  journal of artificial intelligence research     
      
roijers  d  m   whiteson  s     oliehoek  f       b   computing convex coverage sets for
multi objective coordination graphs  in adt       proceedings of the third international conference on algorithmic decision theory  pp         
roijers  d  m   whiteson  s     oliehoek  f  a          linear support for multi objective
coordination graphs  in aamas       proceedings of the thirteenth international
joint conference on autonomous agents and multi agent systems  pp           
rollon  e          multi objective optimization for graphical models  ph d  thesis  universitat politecnica de catalunya  barcelona 
   

ficomputing ccss for faster multi objective coordination

rollon  e     larrosa  j          bucket elimination for multiobjective optimization problems  journal of heuristics             
rosenthal  a          nonserial dynamic programming is optimal  in proceedings of the
ninth annual acm symposium on theory of computing  pp         acm 
scharpff  j   spaan  m  t  j   volker  l     de weerdt  m          planning under uncertainty for coordinating infrastructural maintenance  in proceedings of the  th annual
workshop on multiagent sequencial decision making under certainty 
sontag  d   globerson  a     jaakkola  t          introduction to dual decomposition for
inference  optimization for machine learning            
tesauro  g   das  r   chan  h   kephart  j  o   lefurgy  c   levine  d  w     rawson  f 
        managing power consumption and performance of computing systems using
reinforcement learning  in advances in neural information processing systems   
 nips    
vamplew  p   dazeley  r   barker  e     kelarev  a          constructing stochastic mixture policies for episodic multiobjective reinforcement learning tasks  in advances in
artificial intelligence  pp         
yeoh  w   felner  a     koenig  s          bnb adopt  an asynchronous branch andbound dcop algorithm  journal of artificial intelligence research            

   

fi