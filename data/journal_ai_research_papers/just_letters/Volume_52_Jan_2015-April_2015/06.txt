journal of artificial intelligence research                  

submitted        published      

scheduling conservation designs for maximum flexibility
via network cascade optimization
shan xue
alan fern

xue eecs oregonstate edu
afern eecs oregonstate edu

school of eecs  oregon state university
corvallis  or       usa

daniel sheldon

sheldon cs umass edu

school of computer science  university of massachusetts
amherst  ma        usa

abstract
one approach to conserving endangered species is to purchase and protect a set of land
parcels in a way that maximizes the expected future population spread  unfortunately 
an ideal set of parcels may have a cost that is beyond the immediate budget constraints
and must thus be purchased incrementally  this raises the challenge of deciding how to
schedule the parcel purchases in a way that maximizes the flexibility of budget usage while
keeping population spread loss in control  in this paper  we introduce a formulation of this
scheduling problem that does not rely on knowing the future budgets of an organization  in
particular  we consider scheduling purchases in a way that achieves a population spread no
less than desired but delays purchases as long as possible  such schedules offer conservation
planners maximum flexibility and use available budgets in the most efficient way  we
develop the problem formally as a stochastic optimization problem over a network cascade
model describing a commonly used model of population spread  our solution approach is
based on reducing the stochastic problem to a novel variant of the directed steiner tree
problem  which we call the set weighted directed steiner graph problem  we show that this
problem is computationally hard  motivating the development of a primal dual algorithm
for the problem that computes both a feasible solution and a bound on the quality of an
optimal solution  we evaluate the approach on both real and synthetic conservation data
with a standard population spread model  the algorithm is shown to produce near optimal
results and is much more scalable than more generic off the shelf optimizers  finally  we
evaluate a variant of the algorithm to explore the trade offs between budget savings and
population growth 

   introduction
reserve site selection is a key problem in conservation planning in which planners select land
regions to be designated as nature reserves  either to achieve general conservation goals such
as preserving biodiversity  or to achieve specific goals such as supporting the recovery of an
endangered species  in general  the problem is extremely complex as it involves reasoning
about the interplay between uncertain population spread  uncertain future budgets  and
other problem specific factors  in particular  properly assessing population spread involves
reasoning about spatial aspects of landscapes such as their sizes  shapes  and connectivity 
further  the decision space is huge  consisting of all possible land investment combinations
over time 
c
    
ai access foundation  all rights reserved 

fixue  fern    sheldon

given the above factors  it would be highly desirable for conservation practitioners to
enhance their decision making via automated  or semi automated  planning and scheduling
algorithms  unfortunately  this problem is beyond the scope of existing off the shelf stochastic planners and schedulers  this is largely due to the combination of enormous state and
action spaces  the highly uncertain  exogenous dynamics  and the need for spatio temporal
reasoning  the main contribution of this paper is to make progress toward handling these
complexities by studying a useful subproblem of conservation planning that can be used by
practitioners on realistic scenarios  the general schema used to develop our algorithm is
more widely applicable  see section    and one that has not received significant attention
in the ai community  thus  we hope that this work will also inspire new specialized and
general purpose approaches for complex stochastic planning scheduling problems 
recently  sheldon et al         studied a restricted  but still challenging  version of
the conservation planning problem  which we will refer to as upfront conservation design
optimization  in this problem  the planner is given an upfront budget and a stochastic
metapopulation model  hanski   ovaskainen        that describes how the species under
consideration will spread throughout a landscape of available habitat  in addition the
system is given information about the costs of potential land parcels that are available for
purchase and conservation  the objective is to select a set of land parcels to immediately
purchase and conserve  subject to the budget constraint  that will maximize the spread of
the population within a specified time horizon 
a key simplification present in this problem is that all land parcels are assumed to be
purchased upfront with the currently available budget  an advantage of this simplification
is that it allows for a reasonably efficient and near optimal solution approach  sheldon
et al          however  the upfront simplification limits the utility of the approach in a
number of ways  first  conservation budgets generally arrive in increments over time  so it
is unrealistic to purchase a large set of parcels in advance and restricting to a small set of
parcels using the current budget may be very suboptimal in the long run  moreover  it is
often unnecessary to purchase parcels that are spatially remote from the current population
until the species has spread enough to make them relevant to further population growth 
second  this upfront simplification requires planners to commit in advance to conservation
strategies that may take many years to play out  which ignores the potential advantage of
observing and responding to the stochastic outcomes of the population spreading process as
it unfolds  for example  as the population spread is observed  it may be beneficial to divert
money from failed subpopulations to purchase more parcels near thriving populations 
in contrast to upfront planning  an ideal approach would be fully adaptive planning 
where  at regular decision epochs  the planner would make purchase decisions based on the
most recent population and budgetary information  unfortunately  no currently available
adaptive planning tools can scale to realistic conservation scenarios  this is due to the
combination of an enormous state space  possible population and purchase configurations  
enormous action space  possible subsets of land parcels to purchase   long horizons  tens to
hundreds of years   and high degree of stochasticity in the population spread model 
given the challenge of arriving at a fully adaptive solution  a main contribution of
this paper is to introduce a problem that strikes an important middle ground between
the upfront and fully adaptive approaches  in particular  we consider conservation design
scheduling for exploring the trade off between future population and cost  where we are
   

fischeduling conservation designs for maximum flexibility

given an initial conservation design  i e  a set of parcels to purchase  and are asked to
schedule the purchase time of each parcel in a way that     achieves a population spread
over the time horizon within an arbitrary tolerance of population loss  and     maximizes
purchase flexibility by delaying the specified purchase time  i e  purchase deadline  for each
parcel as long as possible 
this problem formulation simplifies over the fully adaptive problem in a number of ways 
first  the set of parcels to be purchased is provided as input  which removes this degree
of freedom from the planning problem  second  and more significantly  in order to select
an action for the current time step  the general case of fully adaptive planning requires
computing a policy that dictates what to do at each possible future contingency  or at least
the reasonably likely ones  in contrast  the space of possible schedules  our focus here  is
much smaller than the space of policies or even partial policies  this allows for a compact
encoding of the scheduling problem  which does not appear possible for the problem of
computing full  or even  partial policies  this distinction between the fully adaptive and
scheduling setting is akin to the distinction between closed loop and open loop planning 
where generally computing closed loop plans is considered to be more difficult than openloop plans for large stochastic problems 
a solution to the above scheduling problem yields a useful tool to conservation planners 
who can first develop conservation designs that capture their own complex decision making
objectives  perhaps with optimization software  and then schedule the purchases to obtain
the most efficient and cost effective implementation of that design  the conservation planner
then has the flexibility to purchase the parcels at any time before the schedule specified
deadlines  knowing that the population spread will not be hurt too much by such purchase
delays 
in addition  our scheduling problem can potentially be used as a component of an
adaptive planner  a common and successful approach for many adaptive planning problems
is replanning  where at each decision epoch a non adaptive plan is computed from the current
state and its first actions are executed  our work enables a replanning approach that at
each decision epoch first computes an upfront design using existing work  e g  sheldon
et al          and then computes a schedule and purchases only the parcels scheduled to
be purchased immediately  this purchase strategy would spend the minimum amount of
budget at each step while guaranteeing a limited loss in population spread 
in addition to introducing and formalizing the problem of conservation design scheduling  the second contribution of this paper is to develop a principled algorithm for solving
it  the key idea is to apply the sample average approximation  saa  approach  shapiro 
      in order to arrive at a novel deterministic optimization problem  for which we develop a principled solution with a motivation from its special case  in particular  when
the approximated loss tolerance ratio is    our deterministic optimization problem is one of
network cascade optimization  which we show is equivalent to a novel variant of the directed
steiner tree problem  in the traditional steiner tree problem  graph edges are associated
with costs  and the objective is to compute a steiner tree with minimum cumulative edge
cost  in our variant  the set weighted directed steiner graph problem  costs are associated with sets of edges  possibly non disjoint  rather than individual edges  we show that
this problem is computationally hard even under restrictions where the traditional problem
admits an efficient solution  we then present an efficient primal dual algorithm  which is
   

fixue  fern    sheldon

guaranteed to compute both a feasible solution and a bound on the quality of the optimal
solution  then an early stopping version of the algorithm provides a natural approach to
explore the trade off between future population and budget flexibility 
our experiments on both real and synthetic data from a red cockaded woodpecker
conservation problem show that our primal dual algorithm produces near optimal results
and is much more scalable than standard optimization tools  cplex   we also show that
the trade off between population and budget allows for a flexibility of purchasing land
parcels 
in what follows  section   first presents related work  followed by our problem formulation in section    section   then shows how to reduce our subproblem to the set weighted
directed steiner graph problem  section   derives the corresponding primal dual algorithm
and a natural extension for the trade off problem  experiments are presented in section   
finally we conclude and discuss future work 

   related work
previously  many different algorithms have been proposed to select reserve sites by formulating a numerical measure of reserve quality  together with the possible addition of
constraints the reserve must satisfy  and then solving for the optimal set of sites under the
proposed model  e g  see the review article  williams  revelle    levin         although
the earliest reserve site selection algorithms largely ignored spatial considerations  many
newer models incorporate spatial objectives or constraints directly into the optimization
problems  williams  revelle  and levin argue that a primary reason for the importance of
spatial attributes is the fact that they capture properties of the landscape that are favorable
for the underlying population dynamics  and that an important  but computationally difficult  research direction is to directly optimize with respect to a model for the population
dynamics instead of using spatial attributes as a proxy  this is the direction that we are
following in this paper by addressing the problem of spatial conservation planning with
respect to a specific and widely adopted model of population dynamics 
a recent approach that explicitly reasons about a population dynamics model is the
work of sheldon et al         on the upfront conservation design problem  as described
in section    in order to cope with the stochasticity of the model  the popular sample
average approximation  saa  approach was employed to transform the stochastic problem
into a deterministic combinatorial optimization problem  this problem was then encoded
as a mixed integer program  mip  and solved using state of the art solvers  while that
approach was able to solve reasonably large problems via various speedup techniques  the
scalability is still limited to a relatively small number of sample scenarios used by saa 
which controls the accuracy of the approach  kumar et al         have addressed this
aspect of the approach  lagrangian relaxation was used to decompose the saa problem
into independent subproblems that could each be solved in a practical time frame  possibly
in parallel  by standard optimizers  this was shown to significantly reduce the runtime
dependence on the number of saa samples used 
unfortunately  directly extending the above approach to compute multi stage adaptive
solutions  where the budget arrives in increments over time  does not seem practical  one
attempt at this for two stage problems was considered by ahmadizadeh et al          they
   

fischeduling conservation designs for maximum flexibility

explore re planning using a two stage non adaptive problem formulation  and find that it
can indeed offer advantages over upfront planning  in their setting  the budget split and
decision epochs are manually fixed  unlike that work  our work explicitly separates the
decision of which parcels to buy from the decision of when to buy them  the focus of this
work   so that we may develop efficient special purpose algorithms for the latter problem
that scale much more easily to bigger problems and more stages 
there are several existing approaches that might be considered for a fully adaptive
solution to the conservation problem  for example  the fully adaptive problem can be
encoded as a markov decision process  mdp   but the resulting state and action spaces
would be far too big for state of the art solvers  for instance  recent advances in solving
large spatio temporal mdps  crowley   poole        require significant restrictions to
the solution space  which are not acceptable in our application  as an existing approach
for stochastic planning that has been successfully applied by bent et al          chang
et al          chong et al         and yoon et al          hindsight optimization
samples the future outcomes and optimistically estimates the state value based on the
determined futures  however  when the action space is huge  this approach would have
computational problems as current algorithms require enumeration of all the candidate
actions when approximating the state value  another approach would be to formulate the
adaptive planning problem as a multi stage stochastic integer program  however  the size of
such a problem formulation scales exponentially with the number of stages  and the running
time is already very costly for a single stage  sheldon et al          or for a two stage problem
in a simpler setting that is not fully adaptive  ahmadizadeh et al         
recently  golovin et al         proved that a simple greedy planning strategy provides
near optimal solutions in an adaptive conservation setting that at first appears similar
to ours  however  in order to provide approximation guarantees  the authors restrict the
population dynamics so that no spread occurs between distinct land parcels  while this may
be a reasonable assumption for slow moving species such as certain insects  which were the
focus of that work  it ignores critical aspects of the population dynamics of highly mobile
animals such as birds  including the red cockaded woodpecker on which our experiments
are based 

   problem formulation
in this section  we first introduce the basic terminology of conservation design planning and
define our main stochastic optimization problem  next  we describe how the sample average
approximation  saa  is used to transform this problem into a deterministic optimization
problem  which is the focus of the remainder of the paper 
    basic concepts
we largely follow the formulation of sheldon et al          our conservation problems
will involve a  large  land region of interest that is divided into land parcels that are the
smallest land units available for purchase  each parcel contains some number of distinct
habitat patches  which are the atomic units in the population dynamics model and can
either be occupied or unoccupied by the species of interest  for example  in the redcockaded woodpecker problem considered in our experiments  habitat patches correspond
   

fixue  fern    sheldon

to particular trees that have been prepared by humans  or existing birds  to facilitate
nesting  each parcel p has a cost c p   which denotes the cost of purchasing the land and
restoring or conserving all of its habitat patches so that they are suitable for the species to
occupy 
a conservation design is a set of parcels that are intended to be purchased and conserved 
given a conservation design d  a purchase schedule  for d is a mapping from parcels in d
to purchase times in                h     where h is the time horizon of interest and purchasing
a parcel at time t    means this parcel is not going to be purchased  thus the scheduler
may choose not to purchase some parcels even though they are part of the design so as to
realize the best tradeoff between budget flexibility and population spread  although the
species population dynamics have a yearly time step in our model  described below   the
allowed purchase times  i e  decision epochs  may be less frequent depending on the specific
problem  an upfront schedule is one that assigns all parcels to purchase time t     
it is worth noting that the purchase times specified by a schedule are best viewed as
purchase deadlines  that is  we interpret the schedule as constraining the purchases to occur
before or at the specified times  this view is justified by the fact that in the setup below 
purchasing a parcel at an earlier time than specified will never result in worse population
spread 

      population dynamics model
we use the same stochastic dynamics model as sheldon et al          which is an instance of
a popular metapopulation model from the ecology literature  hanski   ovaskainen        
a patch a has two possible states at each time step  either unoccupied or occupied  and
only conserved patches may be occupied  the population dynamics consists of two types
of stochastic events  colonization events occur when a population from patch a colonizes
an unoccupied patch b  which happens with probability pab   extinction events occur when
a patch a that is occupied at time t becomes unoccupied at time t      which happens with
probability    paa   all events are independent  the details of the probabilities used in our
experiments are given in section   
the single step colonization probability pab in our experiments typically decays with
the distance between patches a and b  which encodes spatio temporal dynamics in which
populations slowly spread from a source population when new habitat is made available 
thus  in long term planning for population spread  it is often unnecessary to purchase
parcels that are distant from a source population at time t      since the probability of the
population spreading to such distant patches in the near future is negligible  by delaying
such purchases until they become relevant to the design  i e  the population has spread
nearby   a conservation organization can use limited funds much more flexibly  however 
it is non trivial to decide how much to delay purchases so as not to harm the spread  since
this decision depends very much on the spatio temporal details of the population spread
model  it is this decision that the optimization problem defined below is designed to make 
   

fischeduling conservation designs for maximum flexibility

    stochastic optimization problem
our problem statement will rely on two important concepts     the reward of a schedule 
and    the flexibility of a schedule  we first define these two concepts and then formulate
the optimization problem in terms of them 
the reward of schedule   denoted by r    is a random variable that encodes the
amount of population spread at time h  which is simply a count of the number of occupied
patches at time h  it is easy to show in our model that the upfront schedule always
achieves at least as much reward as any other schedule and thus maximizes the expected
reward  thus  we define the maximum expected reward as r   e r upfront     our
optimization goal is to find a schedule  that almost achieves this optimal expected reward 
i e  e r          r where  is a positive real number and indicates the percentage
tolerance of reward loss but has maximum purchase flexibility  we know that the
upfront schedule achieves      r   however  it requires commitment to all expenditures
at the first time step and is thus the least flexible  indeed  we now formalize the notion of
flexibility in terms of expenditures over time 
given any schedule  we can define its corresponding cost curve c to be a function
from purchase times to accumulated cost  so that c  t  is equal to the total cost of parcels
purchased under  from time   up to and including time t  this curve is non decreasing
and provides a view of a schedules spending profile over the time horizon  in particular 
if the profile of cost curve c  is never above that of c    i e  the total expenditures of  
never exceed those of     then we can say that   offers more flexibility in terms of budget
management compared to   and should be preferred if all else is equal 
now we define a surrogate cost function over schedules as
costf     

x

c p   f   p  

p

which is parameterized by a function f from times in             h    to real numbers  we
require f        for any f so that any parcel that is not purchased within the time horizon
would not contribute to the surrogate cost  we can see that this surrogate cost function is
simply a weighted sum of the parcel costs  where the weight is determined by f based on
the parcels purchase time  although our algorithm can work with any real valued function 
we assume henceforth that f is strictly decreasing  there are two reasons for this  first 
discounting future costs makes sense due to economic factors such as inflation  second 
intuitively  since f decreases with purchase times  minimizing with respect to costf would
favor schedules that delay purchasing  in particular  if policy   has a cost profile that is
never greater than that of     then   will be assigned a lower surrogate cost when f is
strictly decreasing 
if all parcels have positive costs  then the upfront schedule is the unique element that
maximizes the surrogate cost  and the schedule that defers all purchases until time h gives
the unique minimum as f is a strictly decreasing function  however  if we restrict to
schedules that achieve at least reward      r   the latter schedule will be excluded and
there may no longer be a unique minimum 
   

fixue  fern    sheldon

we can now specify the problem of conservation design scheduling  which is to find a
schedule   from the set of all possible schedules such that 
   arg min costf    s t  e r          r


   

that is  out of all schedules that achieve reward      r we want to return one that is
minimal in terms of its surrogate cost  i e  it has maximal flexibility   thus   controls
the trade off between flexibility and reward  in particular  using larger  increases the
set of feasible schedules and allows the potential for returning a more flexible schedule by
sacrificing some reward 
note that by varying the choice of f it may be possible to generate different solutions to
equation    each of which is minimal in the sense that no other feasible policy has a strictly
lower cost curve  in our experiments  we use a simple discounted f given by f  t     t for
a discount factor          
in practice  it is likely that a conservation manager will not have a particular value of 
in mind at design time  rather   is best viewed as a parameter that will be varied in order
to observe the different flexibility reward trade offs that are possible  the final selection of
a schedule would then be based on an assessment of those possibilities 
finally  it is worth noting that for       no reward approximation   the upfront solution
will be the only feasible solution under typical population spread models  thus  using
     is necessary for achieving any additional flexibility  this is because requiring a
policy to achieve expected reward exactly r  i e         requires it to make purchases to
accommodate very unlikely outcomes of the population spread model that contribute a tiny 
but positive  amount to the expected reward  for example  consider an outcome where the
population jumps from its initial location to a very distant location in the first year  and
then undergoes no further spread  this has vanishingly small  but positive  probability  the
upfront schedule will support this population spread  since the distant location is purchased
at the first step  thus  any schedule that does not purchase the distant parcel in the first
step will suffer a tiny loss in reward compared to the upfront schedule  so it does not
achieve       however  such a purchase will tend to be useless for the vast majority of the
probability mass 
    deterministic optimization problem
the above optimization problem is stochastic in the sense that its constraint is defined in
terms of an expectation over a complicated population spread distribution  this greatly
complicates the direct solution of this problem  as in prior work on stochastic optimization
of upfront schedules  sheldon et al          we address this complication by converting the
stochastic problem into an approximately equivalent deterministic optimization problem 
this is done via the very common sample average approximation  saa  approach  see
shapiro       for a survey of some results   the key idea is to approximate a stochastic
optimization problem using a collection of samples from the probability distribution  which
are used to approximate expectations or probabilities via averages over samples 
in our problem formulation  each sample corresponds to a so called cascade scenario 
which is a particular realization of the population spread process over the time horizon 
the main idea behind our application of saa is to generate a set of such cascade scenarios
   

fischeduling conservation designs for maximum flexibility

from the probabilistic population spread model  and to approximate the expected reward of
schedules as the average reward over the scenarios  the scenarios are combined into a single
scenario graph  which is illustrated in figure   and explained in detail in the remainder of
this section 
more concretely  a cascade scenario is a layered graph  where layers correspond to time
steps  with a vertex va t for each patch a and each time step t  for each pair of patches
 a  b  and time step t  a coin is flipped with probability pab to determine if the directed
edge  va t   vb t     is present or not  if this edge is present and patch a is occupied at time
t  through previous colonizations or non extinctions   then patch b will be colonized and
become occupied at time t      as long as it is conserved  that is  the presence of edge
 va t   vb t     is interpreted as meaning that if a is occupied at time t and b is conserved at
or before time t      then b will be occupied at time t     in the particular scenario  in this
way  a cascade scenario graph encodes occupancy as reachability  in particular  assuming
 for now  that all patches are conserved  then patch b is occupied at time t exactly when
vb t is reachable from a vertex va   corresponding to an initially occupied patch a 
to approximate the probabilistic spread model  we sample a set of n i i d  cascade
n    these scenarios
scenarios  c            cn    where we will denote the vertices in cn by  va t
are combined into a single scenario graph  which has an additional root vertex r with
n   to each vertex representing an initially occupied patch a  figure  
directed edges  r  va  
shows an example scenario graph that has three scenarios over a range of five time steps
involving three patches a  b  and c  and two parcels  one containing a and b and the other
containing just c  in this example  a is the only initially occupied patch and hence is
connected at time step zero to the root node r across all three scenarios  assuming that all
parcels are conserved  i e  purchased upfront   if a vertex is connected to the root node r 
then the corresponding patch is considered to be occupied at the corresponding time in the
particular scenario  this is because we defined r so that it can only be connected to other
vertices through initially occupied patches 
scenario graphs will be used in our work to estimate the reward of schedules as follows 
n and all of its incoming
given a scenario graph  a schedule  is said to purchase node va t
edges if patch a is purchased no later than time t  that is   p   t where a belongs to
parcel p  thus  purchasing a parcel p at time t can be viewed as purchasing all vertices in
the scenario graph  along with their incoming edges that involve patches in p that occur at
layer t or later  this reflects the fact that once a patch is purchased and conserved  it is
considered to be conserved and hence eligible for occupancy for the remainder of the time
horizon  in figure    an example schedule is shown that purchases parcel p   containing a
and b  at time    and parcel p   containing c  at time    the vertices that are purchased
by this schedule are shown in the shaded region and their  purchased  incoming edges are
shown in bold 
we can now define under what conditions a vertex in a scenario graph is considered
n becomes occupied under  if there is a path
to be occupied given a schedule  vertex va t
n   we define the variable x n  a  t  to be equal to  
through purchased edges from r to va t

n
if va t is occupied under  and   otherwise  in figure    we have shaded in red the set of
vertices that are occupied under the example policy  as an example  note that in scenario   
  is not occupied since there is no path from r to it through purchased edges 
the vertex vc  
this is despite the fact that there is a path in the graph from r  since that path involves
   

fixue  fern    sheldon

figure    example scenario graph  n      for problem with parcels p     a  b   p     c  
the schedule   p          p         is also illustrated  using shaded boxes to
indicate purchased nodes and heavy line weights to indicate purchased edges 
vertices representing occupied patches under this schedule are colored red 

some unpurchased edges  note that the upfront schedule would purchase this node  since
all vertices and edges would be considered to be purchased under that schedule 
the average reward of a schedule  relative to a scenario graph built from scenarios
 c            cn   is denoted as follows 
r    

n
  xx n
x  a  h 
n
a
n  

this is just the average across scenarios of the number of occupied patches at time h 
in figure   the average reward for the example schedule would be    a key property of
scenario graphs is that as n   we have that r   converges to e r    for any fixed
  this implies that the set of schedules     r         r upfront    converges to
the set     r         r   as n grows  which is the set of policies that we wish
to optimize flexibility over  further  for any policy   one can use standard probability
concentration bounds  e g  chernoff bounds  to show that the event  r    r      has
a probability mass that decreases exponentially fast as n grows  this suggests that only a
relatively small number of scenarios are required to reliably obtain a tight approximation
to the true expected reward of a policy  in practice  however  it is important to empirically
validate that the approximation errors are reasonable for the number of scenarios used in
the approximation 
   

fischeduling conservation designs for maximum flexibility

the above motivates a deterministic saa formulation of our original stochastic optimization problem     where flexibility is optimized subject to a constraint based on the
empirical reward r  that is  our deterministic problem is to solve 
   arg min costf    s t  r         r


   

where r   r upfront   
    overview of solution approach
recall that for the stochastic optimization problem     setting      resulted in an optimization problem that would typically have only the upfront schedule as a feasible solution 
rather  here  for the approximate saa formulation  there will typically be non upfront
solutions that are feasible  even when using       this is because even for large  but practical  values of n   the set of scenarios used for the approximation will not tend to include
highly unlikely scenarios  which need to be accounted for in the stochastic solution when
using       this observation motivates our solution approach for      in particular  in
section    we first consider the problem when       which turns out to be a new variant
of the classic steiner tree problem  we then derive an incremental primal dual algorithm
for the problem  section    that can be used to approximately solve the      case or the
     case through early stopping  our experiments will show that this approach is able to
provide significant flexibility with little loss in reward  with the flexibility reward trade off
being controlled by      

   set weighted directed steiner graph formulation
as motivated above  here we focus on optimization problem     for the case when      
that is  we must optimize flexibility subject to the constraint that we obtain the optimal
empirical reward as measured by r  in this section  we show how to formulate this problem
as a novel variant of the steiner tree problem 
    set weighted directed steiner graph
from      we arrive at our final optimization problem for      
   arg min costf    s t  r     r  


   

we can view this problem as a type of steiner tree problem on the scenario graph  in
particular  we say that any vertex at time t   h is a terminal vertex if it is reachable from
the root r  which is the set of nodes with xnupfront  a  h      and hence contribute to the
upfront reward r   the only way for  to satisfy the constraint r     r is to purchase
a set of edges in the scenario graph that connect all of those target nodes to r  thus  the
constraint in equation   corresponds to purchasing edges such that r has a path to each
terminal  as in the steiner tree problem 
as an example  consider again the scenario graph in figure    the terminal nodes in
  and v     which are the only two
this example are all nodes at layer t     except for vb  
b  
nodes that are not connected to r by a directed path  a schedule that satisfies the constraint
   

fixue  fern    sheldon

r     r must purchase edges such that all of these terminals are reachable from r  note
that for the example schedule of figure    the set of purchased edges does not satisfy this
   
constraint since there is no path of purchased edges to the terminal vertex vc  
while our problem is very similar to the traditional steiner tree problem  there is a
significant difference  in the traditional problem  each edge is associated with a distinct
weight and can be purchased individually  with the goal of connecting all terminals using a
set of edges of minimum total weight  which always forms a tree   rather  our situation is
more complicated because we purchase parcels  which correspond to subsets of edges in the
scenario graph  in particular  purchasing a parcel p at time t  which incurs cost c p f  t  in
equation      corresponds to purchasing an edge set ep t with cost c p f  t  that contains
n   that come from any vertex u and arrive at any vertex v n with a  p 
all the edges  u  va t
 
a t 
t   t and n              n    note that under this cost model  the total cost of edge sets
purchased by a schedule  exactly equals our surrogate objective costf    
from the above we see that our problem is an instance of a problem that we will call
the set weighted directed steiner graph  sw dsg  problem  a novel variant of the steiner
tree problem  the goal of which is to select a set of vertices with minimal total cost in order
to connect all the terminal vertices to the root  for the remainder of the paper we will
discuss this problem in its general form to simplify notation  the input for sw dsg is a
directed graph g    v  e  with a single root vertex r  a set of terminal vertices t  v 
a set of m edge sets e    e            em   where each es  e  and a non negative cost
cs for each es   in particular  our conservation problem has edge sets e    ep t   with
n      u  v n    e  a  p  t   t  n              n    and cost c
ep t     u  va t
 
p t   c p f  t   a
a t 
subset of e forms a steiner graph if the union of the edges connect r to all vertices in t  
the desired output is a minimum cost subset of e that forms a steiner graph  note that
the optimal steiner graph need not be a tree in sw dsg  unlike in the traditional steiner
tree problem 
it is clear that sw dsg is more general than the original deterministic optimization
problem since the latter has a specific edge set structure  for instance  ep t   ep t 
if t    t    however  this structure does not make it an easier problem than sw dsg 
in the following sections  we prove that both problems are np complete and our primaldual algorithm is the same for either setting  the special structure does not lead to any
algorithmic advantages in deriving a primal dual algorithm  therefore  we mainly discuss
the problem in the form of sw dsg to simplify notation 
while the sw dsg problem was motivated by our particular conservation application 
it is relevant to other problems that have steiner style objectives  but where the edge
resources are best considered as groups  for example  steiner trees are often used for the
design of communication networks where edges correspond to existing or potentially new
communication links  for situations where those links must be purchased as coherent sets
 e g  the communication infrastructure of different companies organizations   our sw dsg
problem would be the appropriate formulation 
    computational complexity
to our knowledge  the sw dsg generalization of the steiner tree problem has not been
previously studied and hence we now consider its computational complexity 
   

fischeduling conservation designs for maximum flexibility

the sw dsg problem is a generalization of the traditional directed steiner tree  dst 
problem  which is known to be np complete  hwang  richards    winter         further 
under standard complexity assumptions  dst is hard to approximate by a factor better
than log  t     charikar  chekuri  cheung  dai  guha    li         note that these results
hold even for acyclic directed graphs  there are a number of effective heuristic algorithms
for dst  drummond   santos         with many of the most successful relying on shortest path computations as a subroutine  while shortest paths can be computed in edge
weighted graphs efficiently  this turns out to not be the case for our set weighted problem 
in particular  note that the shortest path problem is a special case of dst  or sw dsg 
where there is a single terminal vertex  this problem turns out to be np hard for swdsg  even when restricted to acyclic graphs and the special edge set structure shown in
our original deterministic optimization problem  which is the case for the scenario graphs
from our conservation problem 
theorem    the sw dsg problem is np hard even when restricted to acyclic graphs with
a single terminal vertex and the edge set structure in scenario graph 
proof  we prove the hardness by reducing the weighted set cover problem to the subclass of
sw dsg problems restricted to a scenario graph with one scenario and exactly one terminal 
note that here we consider the decision version of the sw dsg problem  which asks if there
is a feasible steiner graph whose cost is less than a specified threshold c    an instance of
the weighted set cover problem specifies a ground set of elements s    e            en    a set
s    s            sm   of m subsets sj  s  a cost cj for each subset  and a cost bound c   
 

the problem
s asks whether there is a collection s  s with total cost no more than c
such that sj s     s 
given a set cover instance  we first describe how to construct a scenario graph as illustrated in figure   and later describe the corresponding sw dsg instance  the graph
contains  n layers  which alternate between set layers and element layers starting with a
set layer  n layers of each  hence  n layers   each layer has m vertices labeled s            sm
to represent the sets in s and n vertices labeled e            en to represent the elements in s 
in addition we include a root vertex r  each vertex can also be seen as a parcel with a
single patch  the edges in the graph only go from one layer to the immediate next layer
as follows  the root vertex has an edge going to each sj in the first set layer  for the ith
element layer  i e  layer  i in the graph   we include an edge from a vertex with label sj in
the previous layer to a vertex with label ei in the current layer whenever ei  sj   finally 
vertex ei at the ith element layer has an edge from it to each sj in the next layer 
the corresponding sw dsg  conservation problem  instance on this graph is specified
as follows  the root node is r and the single terminal vertex is en at the final element layer 
the edge sets are specified as follows  which is the same as the setting in a scenario graph 
there are edge sets ej t for each sj at time t  in particular  ej t contains every incoming
edge that is from any vertex and to any vertex labeled as sj at layers t   t  we let the
strictly decreasing f  t  be sufficiently close to   for all ts  the cost of each ej t is equal to
cj  f  t  where cj is the cost of sj in the original set cover problem  in other words  the
cost of ej t is almost cj   similarly  there are edge sets ei t for each ei at time t  we set
their costs as    the cost threshold for the sw dsg problem is equal to the threshold c 
of the set cover problem 
   

fixue  fern    sheldon

to see that this reduction is correct  consider the case where the resulting sw dsg
instance has a feasible solution  the solution provides a path from r to en through purchased
edge sets that have total cost at most c   
since the edge set ei t for each ei has zero cost  the edge set cost is the result of
purchasing edge sets ej t   by the construction the path must go through a sequence of
alternating element nodes and set nodes  in particular  the path must traverse each element
node ei for i              n  the only way for this to happen is to purchase for each of those
ei at least one edge leading to ei from one of the immediately preceding sj at layer t   i 
which is only possible when ei  sj   this can only happen by purchasing the corresponding
edge set ej t   corresponding to sj   which has a cost of  almost  cj   from this we see that
the collection of sj corresponding to purchased edge sets must cover all of the elements and
that their total cost is no more than c    thus  the collection of sets is a solution to the set
cover problem 
conversely consider an instance of the set cover problem with a feasible solution  it is
easy to verify that a feasible solution to the corresponding sw dsg problem is to purchase
edge sets ej   corresponding to any sj in the set cover solution  combining the above we
see that there is a feasible solution to the sw dsg instance if and only if there is a feasible
solution to the set cover instance 

figure    description of the reduction from set cover to sw dsg with a single terminal
vertex and a scenario graph 

the above result proves that the shortest  or least cost  path problem is also np hard
for sw dsg  i e  the problem of finding a least cost path when edges are purchased as
sets  thus  it is difficult to extend prior shortest path based heuristics for the steiner tree
problem to our problem  given that sw dsg is in np  it is np complete  this motivates
our derivation of an efficient heuristic solution approach in the next section  which computes
both a feasible solution along with a bound on the cost of the optimal solution  importantly
this bound provides a sense of how good the computed solution is compared to the optimal 
   

fischeduling conservation designs for maximum flexibility

   primal dual algorithm
a potential solution approach to the sw dsg problem is to encode it as a mixed integer
program  mip   which is straightforward  and then to use an off the shelf mip optimizer 
while this approach produced non trivial results for the upfront conservation problem  sheldon et al          as our experiments will demonstrate  it does not scale well for our problem 
a related approach could be to consider a rounding procedure for the mips lp relaxation 
while solving the lp relaxation is easier than solving the mip  our experiments show that
the scalability of lp solvers is also poor for the problem sizes of interest to us  instead 
we exploit the mip encoding in another way  by following the primal dual schema  vazirani        to derive a scalable algorithm that performs near optimally in our experiments 
our work can be considered as a non trivial generalization of previous work  wong        
where the primal dual schema was applied to dst  moreover  an early stopping version of
our primal dual algorithm provides a way to trade off the schedule flexibility and reward
        note that the primal dual algorithms for sw dsg and our original deterministic
conservation problem only differ in the notations  in other words  the edge set structure in
conservation problem does not offer further improvements for the algorithm  thus in the
following  we only present the approach for sw dsg 
    primal dual algorithm for sw dsg
to apply the primal dual schema  we start by giving a primal mip for the sw dsg problem
along with the dual of its lp relaxation in figure    the primal mip includes a binary
variable y es   for each edge set in e  which indicates whether es was purchased  y es       
or not  y es         the objective of the primal is then simply the sum of these variables
weighted by the costs of the corresponding edge sets  the steiner graph constraint  requiring
that all terminals be connected to the root node by purchased edges  is encoded using a
standard network flow encoding  lines     involving flow variables xki j   the flow variable
xki j encodes the flow on edge  i  j  destined for terminal k  the flow balance constraints
    guarantee that one unit of flow is carried on a path from the root node r to terminal k 
the lp relaxation of the primal simply replaces the integer constraints on the y es  
variables with a positivity constraint  the dual of this relaxed problem  lines     includes
k corresponding to the primal flow constraints  note that the
dual variables uki and wi j
constraint that one unit of flow leaves the root is implied by the other flow constraints  by
omitting this constraint  one could simplify the dual by eliminating the ukr variables  or 
equivalently  set ukr     for all k  t   
given the primal and dual formulations of our problem  we can now apply the primaldual schema for designing optimization algorithms  in particular  our primal dual algorithm
is iterative where each iteration increases the value of the dual objective and purchases
a single edge set es   which corresponds to setting the primal variable y es        the
iteration stops when the purchased edges form a steiner graph  i e  the primal becomes
feasible   the value of the dual objective at the end of the iteration serves as a lower bound
on the optimal primal objective  which provides a worst case indication of how far from
optimal the returned solution is  at a high level  this algorithm is a simple greedy heuristic
that continuously purchases the most beneficial edge set in order to build paths for an
   

fixue  fern    sheldon

 primal  min

m
x

y es    cs  

subject to 

   

s  

x
 i h e

xki h 

x

xkj i

 j i e

xki j



if i   r
  
     if i   k   k  t  i  v


  
if i  
  r  k
x
y es    k  t   i  j   e


   

   

s  i j es

xki j      i  j   e  k  t
y es          

 dual  max

x

 ukk  ukr   

   
   

subject to 

   

kt

x

k
wi j
 cs   s              m  

   

k
ukj  uki  wi j
    k  t   i  j   e

   

k  i j es

k
wi j

 

   

figure    mip for the sw dsg problem and the corresponding dual lp of the mips lprelaxation  the sw dsg problem is defined by a graph g    v  e   a root vertex
r  a set of terminal vertices t   and a set of edges sets e    es   s              m   
where each es  e 

unconnected terminal  the primal dual schema provides a principled way of incrementally
computing this heuristic and at the same time computing a lower bound 
algorithm   gives pseudo code for the algorithm  the main data structure is an auxiliary
graph g     v  a  with the same vertices as the input graph g  the auxiliary graph edge
set a is initially empty and then each iteration adds the newly purchased edges es  e 
the algorithm terminates when the edges in a form a steiner graph  the edge sets used
to construct this graph are then returned as the solution  following a pruning step that
removes obviously redundant edge sets  which the algorithm can sometimes include during
the iteration process 
in order to describe the algorithm in detail  we first introduce some terminology  given
a current auxiliary graph a  we let c k  denote the set of all vertices that have directed
paths to terminal node k via only edges in a  note that we consider k to be included in
c k   also  we define the cut set of a terminal node k  denoted by cut k   to be the set of
all edges  i  j  such that j  c k  and i   c k   intuitively  if k is not already reachable
from the root  we know that at least one edge in cut k  must be added to a in order to
arrive at a steiner graph 
   

fischeduling conservation designs for maximum flexibility

algorithm   primal dual algorithm for sw dsg 
    inputs  graph g    v  e   edge sets e    e            em    costs  c         cm    terminals
t  v 
   initialize 
k      for each  i  j   e  k  t
uki      for each k  t  i  v  wi j
 
g    v  a  with a   
lowerbound      solution   
   while g  is not a steiner graph do
  
let k be random vertex in t not connected to r in g 
  
s    s   es  cut k       s   solution 
  
s   arg minss  s 
 k p

k 
where  s  k    cs  k  t  m n es wm n
  es  cut k  
  
  
  
   
   
   
   

ukj   ukj    s   k   for each j  c k 
k   w k    s   k   for each  i  j   cut k 
wi j
i j
a   a  es
lowerbound   lowerbound    s   k 
solution   solution   s  
end while
pruning  solution   solution   s   s   solution  es  es   

the algorithm first initializes all dual variables to zeros and the auxiliary graph a to
include all vertices and no edges  each iteration then proceeds by first randomly selecting
a terminal vertex k that is not connected to r in the auxiliary graph  at an intuitive level 
the algorithm will then select an edge set es that contains a cutset edge of k according to
a heuristic  s  k  that is derived by applying the primal dual schema  more concretely 
the aim of each iteration is to raise the dual objective value by increasing the value of ukk
while maintaining feasibility  increasing ukk by itself will violate constraints of type     in
the dual and lines   through   of the algorithm maintain feasibility by selecting an edge set
es among those that intersects the cut set of k and then raising all variables corresponding
to vertices in c k  and edges in cut k  by a value  s   k   including ukk    this is done in
a way that causes the dual constraint of type     corresponding to edge set es to become
tight  since this constraint corresponds to primal variable y es    the algorithm effectively
sets y es        indicating a purchase  by adding the edges in es to a  the dual objective
value at termination is the sum across iterations of  s   k  and is returned as the lower
bound 
the key property of our algorithm is that each iteration increases the dual objective 
while also maintaining feasibility of the dual  this guarantees that at each iteration the
dual objective value corresponds to a true lower bound on the optimal value of the primal 
theorem    each iteration of the primal dual algorithm produces a feasible dual solution
with increased objective 
proof  as the base case  the initialization assigns all dual variables to be zeros  which is a
l   
feasible solution  now suppose that iteration q    starts with a feasible solution  uli   wi j
which satisfies the dual constraints of type     and      now if the algorithm terminates 
   

fixue  fern    sheldon

we get a feasible solution  otherwise let k be the terminal vertex selected  for all variables
l   with l    k the values are not changed  so     is satisfied  for the remaining
 uli   wi j
variables with l   k  there are three cases  case    for j   c k   the variables ukj and
k are unchanged  so they cannot contribute to a violation of     or      case    for any
wi j
edge  i  j  with both j  i  c k   we increase both ukj and uki by  s   k  and continue to
satisfy the corresponding constraint of      case    for any cut set edge  i  j   cut k   we
k by  s   k  so that     remains satisfied  since the w k for edges in the
increase ukj and wi j
i j
cut set are increased  we must ensure that constraints of type     do not become violated 
the choice of  s   k  made by the algorithm can be verified to never violate any of those
constraints and makes at least one of them tight 
after the main portion of the algorithm terminates  a pruning step is conducted to
remove any edge set that is a subset of some other edge set in the solution  which decreases
the total cost while maintaining feasibility  in particular  in the context of our conservation
scheduling problem  the pruning step ensures that each parcel is purchased no more than
once in the final solution  there are other more aggressive and computationally expensive
pruning techniques that could also be used  for example  one could consider removing each
one of the selected edge sets from the final solution and then test for feasibility  if the
solution is still feasible  then the edge set can be eliminated  we did not find this more
aggressive style of pruning to be necessary in our experiments 
      implementation and running time
k dual variables 
note that our pseudo code stores and updates values for the ukj and wi j
then a naive implementation of the above algorithm would result in o  e  t    runtime for
initialization as well as the computation per iteration  where  e  is the number of edges
in the graph and  t   is the number of terminals  this could be too much for sw dsg
problems with a large network such as the one in our conservation application  however 
the algorithm is described in this way only for presentation purposes  it turns out that for
the purposes of running the algorithm  it can be implemented significantly more efficiently 
k values for
in particular  we only need to store and update the sum of corresponding wi j
each edge set  i e  the sum term that appears inside of the definition of  s  k  on line
    and maintain the current objective value  stored as lowerbound in the pseudo code  
which is updated on line     therefore  before the iterations only those m     variables
need to be initialized  where m is the number of edge sets and is much smaller than the
size of the network  in our implementation the dominant computation per iteration is the
computation of the cut set for the selected terminal k  we find the cut set by a backward
traversal from terminal k toward the root  the time for this computation is acceptable
when terminals are only connected to relatively small parts of the overall graph  this is
the case in our conservation problem  where terminals are only connected to nodes in the
same cascade and among those only ones that are spatially close enough to be reached  in
other applications where terminals are possibly connected to a large portion of the graph 
it may be preferable to incrementally maintain a cut set for every terminal at each iteration
to reduce the computation  then the memory needed is o c t    where c is the maximum
size of a cut set and presumably c   e   after getting the cut set  the algorithm takes
o m c  time to identify the best edge set and update the solution 

   

fischeduling conservation designs for maximum flexibility

    early stopping for fractional connection
in our primal dual algorithm  the computation continues until all of the terminals in the
scenario graph are connected by paths from the root  in the context of our conservation
problem this corresponds to having no reward approximation loss         here we modify
the above primal dual algorithm to allow for reward approximation loss where       this
case corresponds to only modifying the sw dsg feasibility constraint to only require a
fraction     of the terminals to be connected  leading to a natural way of exploring the
trade off between reward and flexibility 
given the incremental  greedy nature of our primal dual algorithm  which adds one
edge set each iteration  a natural choice for this fractional connection problem is to stop
the algorithm whenever at least a fraction     of the terminals are connected  while this
basic early stopping approach will lead to some improvement in the cost of the returned
solution  compared to       the savings are often quite minimal  this is due to the fact
that the primal dual algorithm grows paths from the terminal nodes to the root and is
unaware of the early stopping condition  as a result  some of the paths that were being
grown are never actually connected to the root at the point that the algorithm is stopped 
these unconnected paths can be considered to be a waste of resources with respect to
meeting the fractional connection constraint  thus  to make this early stopping algorithm
viable  it is necessary to perform pruning on the early stopping result  our algorithm for
fractional coverage then has two stages     generation  where early stopping is used to
generate an initial solution that meets the fractional connection constraint  and    pruning 
where the solution produced in stage   is pruned while maintaining the fractional connection
constraint 
for the pruning stage we use a simple but effective greedy strategy  the idea is to
iterate through the purchased parcels in the schedule returned by the early stopping stage
and to delay the purchase of each parcel as long as possible while ensuring that the number
of connected terminal nodes is almost always within the required fractional connection
tolerance 
we have found that for our conservation application  where the sw dsg problem corresponds to a set of cascades  it is beneficial to prune using an independently generated and
larger set of scenarios than those used to create the initial solution  this is analogous to using validation data to tune algorithm parameters in machine learning prediction problems 
and it is beneficial for the same reasons  we found that pruning based on the original set of
scenarios was often overly aggressive and hurt empirical performance due to over fitting of
the saa scenarios  since we can easily generate independent scenarios to estimate the true
expected reward of the pruned policies  it is better to prune based on that criterion instead 
also  since the computational complexity of evaluating the reward of pruned policies is low
compared with the saa optimization  we can afford to use a larger set of scenarios  in
particular  in our experiments we formed the initial schedules based on a set of    cascade
scenarios and conducted the pruning step with respect to    cascade scenarios 
this approach for pruning can also be viewed as directly enforcing a threshold on the
 independently estimated  expected reward from the original stochastic problem  equation
   instead of enforcing a threshold on the objective value of the saa problem  equation
    since we cant calculate the correct threshold value  the rhs of equation    without
   

fixue  fern    sheldon

knowing the true optimum r of the stochastic problem  we use the saa optimum r
in its place  the saa optimum is a stochastic upper bound to r   so this is generally a
conservative approach for enforcing equation   

   experiments
in this section  we first evaluate our primal dual algorithm by applying it to a real  full scale
conservation problem  next  to verify the robustness of our approach to other problems  we
present results using synthetic conservation data from a problem generator used in several
recent studies  we focus the first two parts of the experimentation on the case of      
which we will see provides substantial gains in flexibility  in order to explore the trade off
between flexibility and reward  population spread   at the end of this section  we evaluate
the early stopping approach for      
    evaluation of primal dual algorithm on real conservation map
the real map we use is the same dataset as in prior work by sheldon et al         on
computing upfront conservation designs  the data is derived from a conservation problem
involving the red cockaded woodpecker  rcw  in a large land region of the southeastern
united states that was of interest to the conservation fund  the region was divided into
    non overlapping parcels  each with an area of at least     acres  and      patches
serving as potential habitat sites  parcel costs were based on land prices and some land
parcels were already conserved and thus had cost zero  we use the same population spread
model as sheldon et al          which was based on individual based models of the rcw 
since our approach requires a conservation design as input  we use the design computed
by sheldon et al         using a total budget constraint of     m  the map of the area is
shown in the left cell of figure    with parcels making up the design shaded green and free
parcels  with cost    shaded grey  red   marks indicate initially occupied patches  our
method also requires specifying a strictly decreasing function for defining the surrogate cost
function  for which we use f  t     t with          we found that the results are not very
sensitive to the value of  
      comparing to optimal solutions
here we compare the solutions of our primal dual algorithm to optimal solutions found using
the cplex solver applied to a mip encoding of the sw dsg problem  the mip encodings
become very large as the horizon and number of scenarios increase  in particular  there are
     h         h  n variables and the number of constraints grows with the number of
edges  in the cascade network  which becomes impractical as n and h grow  since the
optimal solver cant scale to large versions of the problem  we consider problems involving
cascade networks with just   scenarios and horizons ranging from just    to    years  we
also use cplex to compute solutions to the lp relaxation of the mip  the objective value
returned for the lp provides an alternative approach to computing a lower bound on the
optimal solution and thus is interesting to compare to our lower bound in terms of tightness
and runtime  since our primal dual algorithm is stochastic due to the random selection of
   

fischeduling conservation designs for maximum flexibility

h
h
h
h
h
h

 
 
 
 
 
 

  
  
  
  
  
  

cost
mip
     
     
     
     

 m  
pd
      
     
     
     
     
     

lower
lp
     
     
     
     
     

bound
pd
    
    
    
    
    
    

run
mip
   
   
  
    

time
lp
    
   
  
  
  

 s 
pd
   
   
   
  
  
  

table    comparison of primal dual  pd  with mip and lp 

terminal nodes  we report averages over    runs  noting that the standard deviations are
negligible 
the first two data columns of table       show the  surrogate  cost of the solutions
found by cplex solving mip and our algorithm  pd  for increasing horizons  where larger
horizons correspond to larger problems  when a method fails to return a solution due to
memory constraints no value is shown in the table  we see that for horizons where mip is
able to yield solutions by cplex  our algorithm produces solutions that have very similar
costs  here lower cost is better   we also see that the mip solver runs out of memory and
is unable to return solutions for the   largest problem instances  which are already scaled
down versions of the problem  small number of cascades and horizon  
the next two columns of table       provide results for the lower bound computed by
cplex solving the lp and by the pd algorithm  we see that the lower bound produced
by the lp is significantly tighter than the bound produced by our algorithm  however  the
lp cannot be solved by cplex for the largest problem  while our approach still yields a
lower bound  overall  though our lower bound is not as good as the lp  when it can be
computed   it is generally within a factor of two of the optimal solution  which provides a
non trivial assurance about the quality of the returned solution for very large problems 
the final three columns of table       present the time used by the approaches for each
problem  where blank cells indicate that the method ran out of memory  our algorithm is
significantly faster than the mip approach  which fails for the two largest problems  and
is comparable with the lp approach  which only provides a lower bound and fails for the
largest problem  this later result indicates that a solution based on lp rounding would
face difficulty  since even solving the lp for these large problems     time steps with     
patches each  is computationally demanding  an advantage of the primal dual algorithm
is that it avoids encoding the lp and rather works directly with a graph 

   here the pd cost is less than the optimal mip cost returned by cplex  after investigating  we found
that cplex correctly evaluates the solution it returns  but thinks that the solution is optimal when it
is not  it appears that this issue is due to the small error tolerance allowed by the cplex solver 
   here mip takes less time than lp  we think this is possibly because cplex uses different algorithms
for lp and mip  especially  mip is solved by branch and bound algorithm which uses modern features
like cutting planes and heuristics  making cplex a powerful mip solver 

   

fixue  fern    sheldon

      number of cascades in the scenario graph
according to saa  the optimal solution over a finite set of cascade scenarios will converge
to true optimum with more scenarios  previously only two cascades are used due to the
poor scalability of cplex  now we study the number of cascade scenarios we should use
to ensure a good solution  recall that as n increases  the original stochastic problem is
approximated more accurately  yet a larger n corresponds to more computation  more
importantly  here with       a larger n leaves the schedule less space for flexibility  in the
extreme case of n    the only possible schedule is the upfront schedule that has minimal
flexibility  to find a good value of n in practice  we study the primal dual solutions with
different number of cascade scenarios by validating the reward r   that the each primaldual schedule can achieve  given that the population spread is stochastic  we compute the
reward r   by running    simulations of the stochastic population spread model  each
simulation provides a reward value  number of occupied patches at the horizon  and we
average the results  we do this for the schedules produced by our primal dual algorithm
and for the upfront schedule  recall that the intention is to nearly match the reward of
the upfront schedule  figure   presents the results when the time horizon is h       we
observe that the primal dual schedule achieves more and more reward when n increases 
and the reward is converging towards the expected reward of the upfront schedule  we also
see that    cascades is quite close to get the best performance and the rate of improvement
is slowing down  thus  the remainder of our experiments use    cascades for the saa 

figure    rewards of pd solutions w r t the number of cascade scenarios 

      quality of conservation schedules
we now evaluate our algorithm on problems of more realistic sizes  here  we consider
problems based on    cascades and horizons ranging from    to     years  which are well
beyond the range approachable for the mip and lp  the solution times for our algorithm
ranged from    seconds for h      to    minutes for h       
first  we evaluate the average accumulated reward of the schedule returned by our
method for each horizon in figure    the average reward of the upfront schedule ranged
from     for h      to     at h       and for all time horizons the primal dual solution
attained average reward at least       of optimal  with negligible error bars about the
   

fischeduling conservation designs for maximum flexibility

averages  the small gap indicates that for    scenarios the saa approximation is quite
goodthe gap could be further reduced by increasing the number of scenarios 

figure    rewards of pd schedules w r t 
time horizon h 

figure    cost curves of pd schedules for
horizons    to     

of course  we must also consider the cost curves corresponding to the schedules  since
that is what affords the flexibility criterion of our problem  figure   presents the cost
curves for our schedules  note that as defined in section      a cost curve shows the  nondiscounted  accumulated cost of a schedule over time  then the cost curve for a schedule
produced for horizon h will only increase until time h and then remain flat  reflecting that
no purchases are made after that time  we see that for all horizons the cost curves show
a fairly gradual increase in cost expenditures over time  indicating that the schedules are
indeed providing a significant amount of flexibility regarding purchase times  particularly
when compared to the upfront schedule  the cost curve of which is the flat black line in
figure   since all the parcels are purchased at time    in experiments not shown  we found
that the cost curves vary by a small amount for different values of   but the same general
trend is present  interestingly  in all curves there is a sudden jump in cost at around   
years  to understand this in figure   we show both the parcel purchases made by our
schedule and the population spread on the map over the     year horizon  we see that at
t      the sharp increase in cost is due to the purchase of some relatively expensive and
vast parcels in the southern part of the design  looking at the population spread dynamics 
it is apparent that those parcels are a critical gateway for ensuring reliable spread to the
southwestern part of the design in later years  delaying the purchase any longer significantly
increases the probability that such spread does not occur  which our approach discovers 
another interesting observation can be seen by comparing the expected population
spread under the pd computed schedule and the expected population spread of the upfront
schedule  figure     the most striking difference between the spreads is seen at time steps
t              in the northeastern part of the map  for the upfront schedule the entire
northeastern part is occupied in large part  while for the pd schedule there is a hole in
the northeastern part near where the initial bird populations are located  note  however 
that this hole is finally occupied by the horizon of the problem  t         at that time  the
spread in the upfront and pd schedules are visually very similar  which agrees with the fact
that their measured rewards were also similar  the reason for the difference in population
spread is that the pd schedule delays the purchase of some of the northeastern parts of
   

fixue  fern    sheldon

t     

t     

t     

t      

figure     left  original conservation design used for scheduling shown as green shaded
parcels  free  zero cost  parcels are also shaded in dark grey and red   indicates
initially occupied patches   right  the top row shows the parcels purchased
 shaded green  by our pd schedule over a horizon of     years  the middle
row shows the population spread over the same horizon for the schedule  where
lighter red shading of a patch indicates a smaller probability of being occupied
 as measured by    simulations   the bottom row shows the population spread
of the upfront schedule over a horizon of     years 

the map near the initial bird population until about    years before the time horizon ends 
from the population spread process of both schedules  we found that these parcels are very
closely connected and hence can become occupied in a fairly short time if a bird population
is nearby  this is apparent for the upfront schedule  where those areas are already occupied
by t       thus  the purchase of such parcels can be delayed as long as there is enough time
left for the population to spread over these landscapes  therefore  the purchasing can be
delayed not only for parcels far away from current population  but also for parcels that can
be covered quickly and reliably  note that such flexibility is mainly due to our definition of
the reward function  which only takes the population at time h into account  if we count
the population at every time step  presumably a good schedule would purchase the hole
area very soon for more population 
    evaluation of primal dual on synthetic maps
to evaluate the primal dual algorithm more thoroughly  we randomly selected    synthetic
maps generated and used in prior work  ahmadizadeh et al          all the maps consist of
the same region of     non overlapping parcels and     patches  with different configurations
of parcel costs and the initial population  for each map  we considered problems involving
   

fischeduling conservation designs for maximum flexibility

different conservation designs  where each design corresponded to the upfront solution when
the budget is limited to a factor b of the total parcel cost of the map  where b ranged from    
to      in this section  we present a very similar analysis as in section     and show consistent
results  indicating that our primal dual algorithm is stable across different problems 
      comparing to optimal solutions
we first compare the upper and lower bounds returned by our primal dual algorithm to the
optimal objective values of mip and lp as computed using cplex  since cplex still has
scalability issues when solving the larger synthetic problems  we restrict the comparison to
problems with   and   scenarios and a horizon of    years 
results on all of the    maps are very similar  as an example  figure   shows the
surrogate cost  pd ub  and dual objective value  pd lb  of the primal dual solution on
map      together with the optimal surrogate cost  cplex mip  and the lower bound
computed for the lp by cplex  we see that compared to optimal  our algorithm can still
produce solutions with similar costs  especially when the problem is easy  b is smaller  
also  we again see that the lower bound computed by cplex is better than the pd lower
bound  however  the pd lower bound is still within a factor of   

figure    cost and objective value of problems on map      the horizontal axis varies
the amount of budget used to compute the upfront solution that is used as the
conservation design given to the scheduling algorithms 

      quality of conservation schedules
we now consider larger problems based on    cascade scenarios and horizon h       for
which the mip and lp are not practically solvable 
we first compare the average accumulated reward of the schedule returned by our primaldual algorithm and the upfront schedule  figure   shows results for one of our maps 
indicating that the rewards achieved by our primal dual schedules are always very close to
those of the upfront schedules  as desired  the results for other maps are very similar 
we also study the cost curves of the schedules in order to illustrate their flexibility
compared to upfront schedules  figure    presents the average cost curves for our schedules
across all the    maps  it is noted that when the budget is very limited  the purchase is not
delayed very much  for example  when b        most parcels are purchased before t      
long ahead of the time horizon  on further analysis this can be explained by the fact that
   

fixue  fern    sheldon

figure    rewards of primal dual schedule and upfront schedule on
map      

figure     average cost curves of pd
schedules on    maps 

for many of the maps and such small budgets  the sets of affordable parcels are fairly spread
out and loosely connected  this means that the population requires more time in order to
reliably spread across such sets  thus  the parcels must be purchased quite early in the
horizon to support that spread  rather for larger budgets  the sets of parcels that must
be purchased are spread out but also more tightly coupled  which allows for easier  more
reliable population spread  thus  it is possible to delay purchases to a much larger extent
as seen by the cost curves for the larger budgets  this shows that our algorithm is able
to afford considerable flexibility when the initial conservation design supports reasonably
reliable population spread 
    early stopping for trading off flexibility and reward
we now consider the early stopping variant of our primal dual algorithm  referred to as
pd es  for producing schedules that trade off flexibility for reward using       the
dataset we use here is the real conservation map  figure    illustrates the cost curves of
the early stopping schedules with                                and h                   which
demonstrates the possible budget saving over time if a corresponding fraction of reward loss
is allowed  figure    shows the average simulated rewards of these schedules 
first we notice that the average reward achieved by the early stopping schedules is
almost always within the specified error tolerance  which shows that the pruning step is
generalizing effectively  we also see that the cost curves of the early stopping schedules
show significant improvement for even small values of  compared to no early stopping
        for example  when h      and          there is almost no cost during the first
several years  and for several decades the cost is approximately half of the cost required for
      these results show that our approach is able to provide a set of schedules that spans
a spectrum of trade offs  which can then be considered by conservation managers 

   summary and future work
in this work  we addressed the problem of scheduling purchases of parcels in a conservation
design  we formulated the problem as a network cascade optimization problem which was
   

fischeduling conservation designs for maximum flexibility

h     

h     

h     

h     

figure     cost curves of pd es schedules with pruning  the red line        shows the
cost curve of non early stopping pd schedule 
reduced to a novel variant of the classic directed steiner tree problem  we showed that
this problem is computationally hard and then developed a primal dual algorithm for the
problem  our experiments showed that this algorithm produces close to optimal results
and is much more scalable than a state of the art mip solver  we also showed that an
early stopping variant of the algorithm is able to explore the possible trade offs between
flexibility and reward  which is an important consideration in practice 
the scheduling problem considered in this work poses considerable challenges to generic
off the shelf schedulers and planners  the complicating factors include     highly stochastic 
exogenous dynamics that arise from the population spread model     the need to reason
about spatio temporal processes     the long horizons that must be considered  and    the
combinatorial space of potential investment options at each point in time  the general
solution schema we pursued in this work is likely to be applicable to other problems that
pose similar challenges to existing techniques  in particular  this general schema suggests
approximating the problem via the saa and then studying the resulting deterministic
optimization problem  often the resulting deterministic problem will correspond to an
existing well studied problems  for which state of the art approximation algorithms can
be used  in other cases  such as in this work  the resulting problem will be related to
an existing well studied problem and a solution can be designed by extending existing
solution frameworks  we expect that this generic saa schema will be particularly useful
   

fixue  fern    sheldon

h     

h     

h     

h     

figure     rewards of primal dual schedules with early stopping  the number below each
data point indicates the percentage of pd es reward over pd reward 

for problems involving stochastic spread of populations or information across networks 
since the deterministic problems will typical map to graph theoretic problems  for which
there is a vast literature 
in future work  we plan to consider several improvements to the primal dual algorithm 
currently  at each iteration  the algorithm randomly picks an unconnected terminal to
grow a path from  it is likely that more intelligent selection mechanisms can improve the
overall results  we are also interested in developing a primal dual algorithm that directly
incorporates the error tolerance constraint of our early stopping approach  this would
provide a more direct method for trading off reward for improved flexibility  furthermore 
we intend to pursue fully adaptive approaches to this and other conservation problems 
one idea is to incorporate our scheduling approach into a replanning algorithm that selects
purchases for the current decision epoch based on the most up to date information  in
particular  at each decision epoch a schedule would be formed and those parcels scheduled to
be purchased immediately  those with no flexibility  would be purchased or a subset of those
in cases where the immediate budget would be exceeded  considering more sophisticated
approaches that take into account the immediate budget would be a natural and useful
extension  it would also be interesting to consider the conservation problem with other
variants of the reward function  for some species  rather than caring only the population
   

fischeduling conservation designs for maximum flexibility

in the end  the ecological goal may value the spread during the whole period the same or
even more  presumably such models would have different properties and complexities from
the one we study in this paper 

acknowledgements
parts of the material in this paper appeared in an earlier work by xue  fern  and sheldon
        this work was supported by nsf under grant iis         

references
ahmadizadeh  k   dilkina  c   gomes  c  p     sabharwal  a          an empirical study
of optimization for maximizing diffusion in network  in   th international conference
on principles and practice of constraint programming 
bent  r     van hentenryck  p          regret only  online stochastic optimization under
time constraints  in nineteenth aaai conference on artificial intelligence 
chang  h  s   givan  r  l     chong  e  k          on line scheduling via sampling  in
artificial intelligence planning and scheduling 
charikar  m   chekuri  c   cheung  t   dai  a   guha  s     li  m          approximation algorithm for directed steiner tree problems  in the ninth annual acm siam
symposium on discrete algorithms 
chong  e  k   givan  r  l     chang  h  s          a framework for simulation based
network control via hindsight optimization  in ieee cdc conference 
crowley  m     poole  d          policy gradient planning for environmental decision making
with existing simulators  in twenty fifth aaai conference on artificial intelligence 
drummond  l  m     santos  m          a distributed dual ascent algorithm for steiner
problems in multicast routing  networks             
golovin  d   krause  a   gardner  b   converse  s  j     morey  s          dynamic resource
allocation in conservation planning  in twenty fifth aaai conference on artificial
intelligence 
hanski  i     ovaskainen  o          the metapopulation capacity of a fragmented landscape  nature                     
hwang  f  k   richards  d  s     winter  p          the steiner tree problem  springer 
kumar  a   wu  x     zilberstein  s          lagrangian relaxation techniques for scalable spatial conservation planning  in twenty sixth aaai conference on artificial
intelligence 
shapiro  a          monte carlo sampling methods  in stochastic programming  handbooks
in operations research and management science  vol      pp         
sheldon  d   dilkina  b   elmachtoub  a   finseth  r   sabharwal  a   conrad  j   gomes 
c   shmoys  d   allen  w   amundsen  o     vaughan  b          maximizing the
   

fixue  fern    sheldon

spread of cascades using network design  in uncertainty in artificial intelligence
 uai  
vazirani  v  v          approximation algorithms  springer  berlin 
williams  j   revelle  c     levin  s          spatial attributes and reserve design models 
a review  environmental modeling and assessment                 
wong  r  t          a dual ascent approach for steiner tree problems on a directed graph 
mathematical programming             
xue  s   fern  a     sheldon  d          scheduling conservation designs via network
cascade optimization  in twenty sixty aaai conference on artificial intelligence 
yoon  s   fern  a   givan  r  l     kambhampati  s          probabilistic planning via
determinization in hindsight  in twenty third aaai conference on artificial intelligence 

   

fi