journal artificial intelligence research                  

submitted       published     

agnostic pointwise competitive selective classification
yair wiener
ran el yaniv

wyair tx technion ac il
rani cs technion ac il

computer science department
technion israel institute technology
haifa        israel

abstract
pointwise competitive classifier class f required classify identically
best classifier hindsight f  noisy  agnostic settings present strategy
learning pointwise competitive classifiers finite training sample provided
classifier abstain prediction certain region choice  interesting hypothesis classes families distributions 
measure

rejected region
  
shown diminishing rate    polylog m  log     m   
  high probability  sample size  standard confidence parameter       
smoothness parameters bernstein type condition associated excess loss class
 related f     loss   exact implementation proposed learning strategy
dependent erm oracle hard compute agnostic case  thus
consider heuristic approximation procedure based svms  show empirically
algorithm consistently outperforms traditional rejection mechanism based
distance decision boundary 

   introduction
given labeled training set class models f  possible select f  based
finite training sample  model whose predictions always identical best model
hindsight  classical results statistical learning theory surely preclude
possibility within standard model  changing rules game possible 
indeed  consider game classifier allowed abstain prediction without
penalty region choice  a k a classification reject option   game 
assuming noise free realizable setting  shown el yaniv wiener       
one train perfect classifier never errs whenever willing predict 
always abstaining render perfect classification vacuous  shown
quite broad set problems  each specified underlying distribution family
hypothesis class   perfect realizable classification achievable rejection rate
diminishes quickly zero training sample size 
general  perfect classification cannot achieved noisy setting  paper 
objective achieve pointwise competitiveness  property ensuring prediction
every non rejected test point identical prediction best predictor hindsight
class  consider pointwise competitive selective classification
generalize results el yaniv wiener        agnostic case  particular 
show pointwise competitive classification achievable high probability
learning strategy called low error selective strategy  less   given training sample sm

c
    
ai access foundation  rights reserved 

fiwiener   el yaniv

hypothesis class f  less outputs pointwise competitive selective classifier  f  g  
f  x  standard classifier  g x  selection function qualifies
predictions dont knows  see definitions section     classifier f simply
taken empirical risk minimizer  erm  classifier  f  pointwise competitiveness
achieved g follows  using standard concentration inequalities  show
true risk minimizer  f   achieves empirical error close f  thus  high
probability f belongs class low empirical error hypotheses  left
set g x  allows prediction label x  f x  
hypotheses low error class unanimously agree label x 
simpler  realizable setting  el yaniv   wiener         low error class simply reduces
version space 
bulk analysis  in sections         concerns coverage bounds less 
namely  showing measure region classifier  f  g  refuses classify 
diminishes quickly  high probability  training sample size grows  see section  
formal definition   provide several general distribution dependent coverage
bounds  particular  show  in corollaries        respectively  high probability
bounds coverage  f  g  classifier  f  g    form 


 f  g       polylog m  log     m       

linear models  unknown  distribution p  x     x feature space points
labels  whose marginal p  x  finite mixture gaussians  axis
aligned rectangles p  x    whose marginal p  x  product distribution 
      bernstein class smoothness parameters depending hypothesis class
underlying distribution  and loss function      case  
outset  efficient implementation less seems reach
required track supremum empirical error possibly infinite hypothesis
subset  general might intractable  overcome computational difficulty 
propose reduction problem problem calculating  two  constrained erms 
given test point x  calculate erm training sample sm
constraint label x  one positive label constraint one negative   show
thresholding difference empirical error two constrained erms
equivalent tracking supremum entire  infinite  hypothesis subset  based
reduction introduce section   disbelief principle motivates heuristic implementation less  relies constrained svms  mimics optimal
behavior 
section   present numerical examples medical classification problems
examine empirical performance new algorithm compare performance
widely used selective classification method rejection  based distance
decision boundary 

   pointwise competitive selective classification  preliminary
definitions
let x feature space  example  d dimensional vectors rd  
output space  standard classification  goal learn classifier f   x y  using
   

fiagnostic pointwise competitive selective classification

finite training sample labeled examples  sm     xi   yi   m
i     assumed sampled
i i d  unknown underlying distribution p  x    x y  classifier
selected hypothesis class f  let          bounded loss function 
selective classification  el yaniv   wiener         learning algorithm receives sm
required output selective classifier  defined pair  f  g   f f
classifier  g   x        selection function  serving qualifier f follows 
x x    f  g  x    f  x  iff g x       otherwise  classifier outputs dont know 
general performance selective predictor characterized terms two quantities  coverage risk  coverage  f  g   f  g    ep  g x     true risk  f  g  
respect loss function   average loss f restricted region activity
qualified g  normalized coverage  r f  g    ep   f  x   y  g x     f  g  
easy verify g    and therefore  f  g        r f  g  reduces
famil  pm
iar risk functional r f     ep   f  x   y    classifier f   let r f     i    f  xi    yi   
standard empirical error f sample sm   let f   argminf f r f  
empirical risk minimizer  erm   let f   argminf f r f   true risk minimizer
respect unknown distribution p  x      clearly  true risk minimizer f unknown  selective classifier  f  g  called pointwise competitive x x  
g x       f  x    f  x  

   low error selective strategy  less 
hypothesis class f  hypothesis f f  distribution p   sample sm   real number
r      define true empirical low error sets 


v f  r    f f   r f   r f     r
   


n

v f  r    f f   r f   r f     r  

   

throughout paper denote  m    d  slack standard uniform deviation
bound  given terms training sample size  m  confidence parameter   
vc dimension  d  class f 


  ln  
 d ln  me

 m    d     
 
   

following theorem slight extension statement made bousquet  boucheron 
lugosi        p       
theorem    bousquet et al          let     loss function f  hypothesis
class whose vc dimension d           probability least  
choice sm p   hypothesis f f satisfies
r f   r f      m    d  
similarly  r f   r f      m    d  conditions 
 

formally  f classifier r f     inf f f r f   inf f f p   x  y    f  x     f  x       
existence  measurable  f guaranteed sufficient considerations  see hanneke       
pp          

   

fiwiener   el yaniv

remark    use theorem   and  particular  vc bounds classification problems
     loss  mandatory developing theory presented paper  similar
theories developed using types bounds  e g   rademacher compression
bounds  learning problems 
let g f  disagreement set  hanneke      a  el yaniv   wiener        w r t  g
defined
dis g     x x   f    f  g s t  f   x     f   x    
   
let us motivate low error selective strategy  less  whose pseudo code appears
strategy    strategy define whenever empirical risk minimizer  erm  exists 
example  case     loss  using standard uniform deviation bound 
one theorem    one show training error true risk minimizer  f  
cannot far training error empirical risk minimizer  f  therefore 

guarantee  high probability  empirical low error class v f  r  applied
appropriately chosen r  includes true risk minimizer f   selection function
g constructed accept subset domain x   hypotheses
empirical low error set unanimously agree  strategy   formulates idea  call
strategy rather algorithm lacks implementation details  indeed 
clear outset strategy implemented 
strategy   agnostic low error selective strategy  less 
input  sm   m   
output  pointwise competitive selective classifier  h  g  w p   

   set f   erm
 f  sm    i e   f empirical risk minimizer f w r t  sm
   set g   v f    m      d   see eq          
   construct g g x      x  x   dis  g  
   f   f
begin analysis less  following lemma establishes pointwise competitiveness  section   develop general coverage bounds terms undetermined
disagreement coefficient  then  section   present distribution dependent bounds
rely disagreement coefficient 
lemma    pointwise competitiveness   let     loss function f  hypothesis
class whose vc dimension d  let     given let  f  g  selective classifier
chosen less  then  probability least        f  g  pointwise competitive
selective classifier 
proof  theorem    probability least      
r f   r f      m      d   
clearly  since f minimizes true error  r f   r f   applying theorem   
know probability least      
r f  r f     m      d   
   

fiagnostic pointwise competitive selective classification

using union bound  follows probability least      
r f   r f       m      d   
hence  probability least      


f v f     m      d    g 

definition  less constructs selection function g x  equals one iff x
x  dis  g    thus  x x   g x       hypotheses g agree 
particular f f agree  therefore  f  g  pointwise competitive high probability 

   general coverage bounds less terms disagreement
coefficient
require following definitions facilitate coverage analysis  f f
r      define set b f  r  hypotheses reside ball radius r around f  





b f  r    f f   pr f  x     f  x  r  
xp

g f  distribution p   denote g volume disagreement set
g  see       g   pr  dis g    let r     disagreement coefficient  hanneke 
      hypothesis class f respect target distribution p
 r      f  r      sup

r r 

b f   r 
 
r

   

disagreement coefficient utilized later analysis  see discussion
characteristics corollary    associated excess loss class class f
loss function  massart        mendelson        bartlett  mendelson    philips       
defined
xl f    x  y      f  x   y   f  x   y    f f   
whenever f fixed abbreviate xl   xl f    x  y   xl said
         bernstein class respect p  where               every h xl
satisfies
eh     eh    
   
bernstein classes arise many natural situations  see  e g   koltchinskii        bartlett  
mendelson        bartlett   wegkamp         example  conditional probability
p  y  x  bounded away      satisfies tsybakovs noise conditions   
excess loss function bernstein class  bartlett   mendelson        tsybakov         
 

data generated unknown deterministic hypothesis limited noise p  y  x 
bounded away     

 

specifically      loss  assumption proposition   work tsybakov         equivalent

bernstein class condition equation           
  tsybakov noise
parameter 

   

fiwiener   el yaniv

following sequence lemmas theorems assume binary hypothesis class f
vc dimension d  underlying distribution p x          loss
function  also  xl denotes associated excess loss class  results extended
loss functions     similar techniques used beygelzimer  dasgupta 
langford        
figure   schematically depict hypothesis class f  the gray area   target
hypothesis  filled black circle outside f   best hypothesis class f  
distance two points diagram relates distance two hypothesis
marginal distribution p  x   first observation excess loss class
         bernstein class  set low true error  depicted figure    a   resides
within larger ball centered around f  see figure    b   

figure    set low true error  a  resides within ball around f  b  
lemma    xl          bernstein class respect p   r    


v f   r  b f     r   

proof  f v f   r  then  definition  e  i f  x        e  i f  x          r 
linearity expectation have 
e  i f  x       i f  x        r 
since xl          bernstein 
e  i f  x     f  x      e   i f  x       i f  x        
n

  e   f  x      f  x         eh     eh  
     e  i f  x       i f  x           


     e  i f  x     f  x      r    therefore  definition  f b f     r   
   

   

fiagnostic pointwise competitive selective classification

far seen set low true error resides within ball around f  
would prove high probability set low empirical error  depicted
figure    a   resides within set low true error  see figure    b    emphasize
distance hypotheses figure    a  based empirical error 
distance figure    b  based true error 

figure    set low empirical error  a  resides within set low true error  b  
lemma    r               probability least    
v f  r  v  f      m      d    r   
proof  f v f  r   then  definition  r f   r f    r  since f minimizes empirical
error  know r f  r f    using theorem   twice  applying union bound 
see probability least    
r f   r f      m      d 



r f   r f      m      d  

therefore 
r f   r f        m      d    r 

f v  f      m      d    r   

shown that  high probability  set low empirical error subset
certain ball around f   therefore  probability least two hypotheses set
low empirical error disagree bounded probability
least two hypotheses ball around f disagree other  fortunately 
latter bounded disagreement coefficient established following lemma 

   

fiwiener   el yaniv

lemma    r              probability least    
v f  r        m      d    r    r    
 r    disagreement coefficient f respect p   applied r   
   m      d     see      
proof  applying lemmas     get probability least    


v f  r  b f         m      d    r    

therefore 



v f  r  b f         m      d    r    

definition disagreement coefficient      r   r    b f   r    r   r  
recalling     thus observing r         m      d    r         m      d     
r    proof complete 
position state first coverage bound selective classifier
constructed less  bound given terms disagreement coefficient 
corollary    let f hypothesis class theorem    assume xl
         bernstein class w r t  p   let  f  g  selective classifier constructed less 
then  probability least      f  g  pointwise competitive selective classifier

 f  g          m      d     r    

 r    disagreement coefficient f respect p   r       m      d     

proof 
lemma   
probability least       f  g  pointwise competitive  set

g   v f      m      d    construction  f   f  selection function g x  equals
one iff x x   dis  g   thus  definition coverage   f  g    e g x       g 
therefore  applications lemma   union bound imply probability
least      f  g  pointwise competitive coverage satisfies 
 f  g    e g x       g         m      d     r    

noting  r  monotone non increasing r  know coverage bound
corollary   clearly applies      quantity     discussed numerous papers shown finite various settings including thresholds r
distribution            hanneke         linear
separators origin
rd uniform distribution sphere      d   hanneke         linear
separators rd smooth data distribution bounded away zero      c f  d 
c f   unknown constant depends target hypothesis   friedman 
       cases  application corollary   sufficient guarantee pointwisecompetitiveness bounded coverage converges one  unfortunately many
hypothesis classes distributions disagreement coefficient     infinite  hanneke 
       fortunately  disagreement coefficient  r  grows slowly respect   r  as
shown wang        sufficient smoothness conditions   corollary   sufficient
guarantee bounded coverage 

   

fiagnostic pointwise competitive selective classification

   distribution dependent coverage bounds less
section establish distribution dependent coverage bounds less  starting
point bounds following corollary 
corollary    let f hypothesis class theorem    assume f disagreement coefficient
 r       polylog    r    
   
w r t  distribution p   xl          bernstein class w r t  distribution 
let  f  g  selective classifier chosen less  then  probability least    
 f  g  pointwise competitive coverage satisfies 
 


polylog m 
      
 f  g     
log
 


proof  plugging     coverage bound corollary   immediately yields result 

corollary   states fast coverage bounds less cases disagreement coefficient grows slowly respect   r     recent results disagreement based active
learning selective prediction  wiener et al         wiener        established tight relations disagreement coefficient empirical quantity called version space
compression set size  quantity analyzed el yaniv wiener       
context realizable selective classification  known distribution dependent
bounds it  plan rest section introduce version space compression set size  discuss relation disagreement coefficient  show
apply results agnostic setting 
interested solving agnostic case  consider moment
realizable setting utilize known results used analysis  specifically 
assume f f p y   f  x  x   x      x x  
 x    p   given training sample sm   let vsf  s induced version space  i e  
set hypotheses consistent given sample sm   version space compression
set size  denoted n sm     n f  sm    defined size smallest subset
sm inducing version space  hanneke      b  el yaniv   wiener        
function sm   clearly n sm   random variable  specific realization sm
value unique 
        define version space compression set size minimal bound
bn  m      min  b n   p n sm   b       

   

rely following lemma  wiener et al          sake self containment
provide proof appendix 
 

disagreement coefficient grow ploy logarithmically   r  still o   r    
still possible prove lower bound coverage  specifically   r          r           one

show  f  g    o     m         

   

fiwiener   el yaniv

lemma    wiener et al          realizablecase  bn  m 
     polylog m  log       

 
 
bn m        polylog m     r      polylog r   

obviously  statement lemma   holds  and well defined  within realizable
setting  the version space compression set size defined setting   turn
back agnostic setting consider arbitrary underlying distribution p x y 
recall agnostic setting  let f   x denote  measurable  classifier
r f     inf f f r f   inf f f p   x  y    f  x     f  x        guaranteed
exist sufficient assumptions  see hanneke        section       call f infimal
 best  hypothesis  of f  w r t  p    clearly several different infimal hypotheses 
note  however  xl          bernstein class respect p  as assume
paper   lemma   ensures infimal hypotheses identical measure
zero 
definitions version space version space compression set size naturally
generalized agnostic setting respect infimal hypothesis  wiener et al  
      follows  let f infimal hypothesis f w r t  p   agnostic version space
sm
vsf  sm  f    f f    x  y  sm   f  x    f  x   

agnostic version space compression set size  denoted n sm     n f  sm   f    defined
size smallest subset sm inducing agnostic version space vsf  sm  f  
finally  extend definition version space compression set minimal bound
agnostic setting follows 
bn  m    f     min b n   p n f  sm   f   b      

key observation allows surprisingly easy utilization lemma  
agnostic setting disagreement coefficient depends hypothesis class
f marginal distribution p  x   using infimal hypothesis f therefore
take agnostic learning problem consider realizable projection  whereby points
labeled f marginal distribution p  x   two problems
 essentially  disagreement coefficients  idea initially observed
hanneke        wiener         formulate slight variation
formulation work wiener  hanneke  el yaniv        
define disagreement agnostic setting     respect infimal hypothesis f   agnostic learning problem  f  p   define realizable
projection  f   p   follows  let f   f  f   f infimal hypothesis
agnostic problem  define p distribution marginal p  x    p  x  
p y   f  x  x   x      x x   easy verify  f   p   realizable
learning problem  i e   f f pp  x y    y   f  x  x   x      x x  
lemma     realizable projection   given agnostic learning problem   f  p    let
 f   p   realizable projection  let  r     r    associated disagreement coefficients agnostic realizable projection problems  respectively  then   r     r    

proof  first note depend  respectively  p p via f
marginal distributions p  x    p  x   since f f  f     f   readily get
 r     r    
   

fiagnostic pointwise competitive selective classification

let us summarize derivation  given agnostic problem  f  p    consider
realizable projection  f   p    bn  m       polylog m  log        or bn  m         
 polylog m    realizable problem  lemma     r       polylog    r     
which  lemma     holds original agnostic problem  therefore  corollary  
applies obtain fast coverage bound less w r t   f  p   
new agnostic coverage bounds less obtained using following known bounds
 realizable  version space compression set size  first one  el yaniv wiener
        applies problem learning linear separators mixture gaussian
distributions  following theorem direct application lemma    work
el yaniv wiener        
theorem     el yaniv   wiener         d  n n  let x rd   f space
linear separators rd   p distribution marginal rd mixture
n multivariate normal distributions  then  constant cd n      depending
d  n  otherwise independent p     
bn  m        cd n  log m  d   
applying theorem     together lemma     lemma   corollary    immediately
yields following result 
corollary     assume conditions theorem     assume xl         bernstein class w r t  p  x     let  f  g  selective classifier constructed less 
then  probability least      f  g  pointwise competitive selective classifier



 f  g       polylog m  log     m       
second version space compression set size bound concerns realizable learning
axis aligned rectangles product densities rn   bounds previously
proposed wiener  hanneke  el yaniv        el yaniv wiener              
state  without proof  recent bound  wiener  hanneke    el yaniv        giving
version space compression set size bound learning problem  whose positive class
bounded away zero  

theorem     wiener et al          d  n           let x rd   p
marginal distribution rd product densities rd marginals
continuous cdfs  f space axis aligned rectangles f rd
p   x  y    f  x        

 d
 d
bn  m   
ln
 


again  application theorem     together lemma     lemma   corollary   yields following corollary 
corollary     d  n           let x rd   let p  x    underlying
distribution marginal p  x  product densities rd marginals
continuous cdfs  let f space axis aligned rectangles f rd p   x  y    f  x   
     assume xl          bernstein class w r t  p  x     let  f  g 
   

fiwiener   el yaniv

selective classifier constructed less  then  probability least      f  g 
pointwise competitive selective classifier


 f  g       polylog m  log     m       

   erm oracles disbelief principle
outset  efficient construction selection function g prescribed less seems
reach required verify  point x question  whether
hypotheses low error class agree label  moreover  g computed
entire domain  luckily  possible compute g lazy manner show
compute g x  calculating  two  constrained erms  given test point x 
calculate erm training sample sm constraint label x  one
positive label constraint one negative   show thresholding difference
empirical error two constrained erms equivalent tracking supremum
entire  infinite  hypothesis subset  following lemma establishes reduction 
lemma     let  f  g  selective classifier chosen less observing training
sample sm   let f empirical risk minimizer sm   let x point x
define


n
fx   argmin r f     f  x    sign f x   
f f

i e   empirical risk minimizer forced label x opposite f x  
g x     



r fx   r f     m      d   

proof  first note according definition v  see eq      


r fx   r f     m      d  fx v f     m      d   

    

    

prove first direction           assume rhs      holds       
get f  fx v  however  construction  f x    fx  x   x dis v 
g x      
prove direction      assume r fx   r f       m      d  
assumption  prove f v  f  x    f x   therefore  x
x   dis v   entailing g x       indeed  assume contradiction exists
f v f  x    fx  x     f x   construction  holds
r f   r fx     r f       m      d   
f   v  contradiction 
lemma    tells us order decide point x rejected need measure
empirical error r fx   special empirical risk minimizer  fx   constrained
label x opposite h x   error sufficiently close r h   classifier cannot
sure label x must reject it  thus  provided compute
erms  decide whether predict reject individual test point x x  
   

fiagnostic pointwise competitive selective classification

without actually constructing g entire domain x   figure   illustrates principle
  dimensional example  hypothesis class class linear classifiers r 
source distribution two normal distributions  negative samples represented
blue circles positive samples red squares  usual  f denotes empirical

figure    constrained erm 
risk minimizer  let us assume want classify point x    point classified
positive f  therefore  force point negative calculate restricted
erm  depicted doted line marked fx     difference empirical risk f
fx  large enough  point x  rejected  however  want classify
point x    difference empirical risk f fx  quite large point
classified positive 
equation      motivates following definition disbelief index df  x  sm  
individual point x   specifically  x x   define disbelief index w r t  sm
f 
d x    df  x  sm     r fx   r f  
observe d x  large whenever model sensitive label x sense
forced bend best model fit opposite label x  model
substantially deteriorates  giving rise large disbelief index  large d x 
interpreted disbelief possibility x labeled differently 
case definitely predict label x using unforced model  conversely 
d x  small  model indifferent label x sense  committed
label  case abstain prediction x  notice less
specific application thresholded disbelief index 
note similar technique using erm oracle enforce arbitrary
number example based constraints used dasgupta  hsu  monteleoni      a 
beygelzimer  hsu  langford  zhang         context active learning 
disbelief index  difference empirical risk  or importance weighted
empirical risk  see beygelzimer et al         two erm oracles  with different constraints 
used estimate prediction confidence 

   

fiwiener   el yaniv

   

    
    

    
test error

test error

    
   
    

    
    

    
    

    

    
 

   

   

   

   

    
   

 

c

   

   

   

   

   

c

figure    rc curve technique  depicted red  compared rejection based
distance decision boundary  depicted dashed green line   rc curve
right figure zooms lower coverage regions left curve 

practical applications selective prediction desirable allow control
trade off risk coverage  words  desirable able
develop entire risk coverage  rc  curve classifier hand  see  e g   el yaniv  
wiener        let user choose cutoff point along curve accordance
practical considerations constraints  disbelief index facilitates exploration
risk coverage trade off curve classifier follows  given pool test points
rank test points according disbelief index  points low index
rejected first  thus  ranking provides means constructing risk coverage
trade off curve  ignoring moment implementation details  which discussed
section     typical rc curve generated less depicted figure    red curve    
dashed green rc curve computed using traditional distance based techniques
rejection  see discussion common technique section    right graph zoom
section entire rc curve  depicted left graph   dashed horizontal line
test error f entire domain dotted line bayes error 
high coverage values two techniques statistically indistinguishable  coverage
less     get significant advantage less  clear case
estimation error reduced  test error goes significantly optimal
test error f low coverage values 
interestingly  disbelief index generates rejection regions fundamentally different obtained traditional distance based techniques rejection  see
section     illustrate point  and still ignoring implementation details   consider
figure   depict rejection regions training sample     points sampled
mixture two identical normal distributions  centered different locations  
height map figure  correspond disbelief index magnitude  a   distance
decision boundary  b   reflect confidence regions technique according
confidence measure 
 

learning problem synthetic problem used generating figure   

   

fiagnostic pointwise competitive selective classification

 a 

 b 

figure    linear classifier  confidence height map using  a  disbelief index   b  distance
decision boundary 

figure    svm polynomial kernel  confidence height map using  a  disbelief index 
 b  distance decision boundary 

intuitively explain height map figure   a   recall disbelief index
difference empirical error erm restricted erm  test
point resides high density region  expect forcing wrong label point
result large increase training error  result  denser area is 
larger disbelief index  therefore  higher classification confidence 
second synthetic  d source distribution consider even striking  x
distributed uniformly               labels sampled according
following conditional distribution

      x  sin x    
p  y     x    x    x      
      else 
thick red line depicts decision boundary bayes classifier  hight maps
figure   depict rejection regions obtained  our approximation of  less

   

fiwiener   el yaniv

traditional  distance decision boundary  technique training sample   
points sampled distribution  averaged     iterations   hypothesis
class used training svm polynomial kernel degree    qualitative
difference two techniques  particular  nice fit disbelief
principle technique compared svm quite surprising 

figure    rc curves svm linear kernel  method solid red  rejection
based distance decision boundary dashed green  horizontal axis  c 
represents coverage 

   heuristic procedure using svm empirical performance
computation  constrained  erm oracle efficiently achieved case
realizable learning linear models  see  e g   el yaniv   wiener        case
linear regression  wiener   el yaniv         however  noisy setting computation
linear erm oracle reduced variant max fls c max fls
problems  with strict non strict inequalities   amaldi   kann         unfortunately 

   

fiagnostic pointwise competitive selective classification

max fls apx complete  within factor     c max fls max ind set hard 
cannot approximated efficiently all  moreover  extensions results
classes  including axis aligned hyper rectangles  showing approximating erm
classes np hard  ben david et al         
present known hardness results  and related lower
bounds  hold half spaces nice distributions gaussian  mixtures   note
tauman kalai et al         studied problem agnostically learning halfspaces
distributional assumptions  particular  showed data distribution
uniform d dimensional unit sphere  or hyper cube  related distributions  
 
possible agnostically learn  accurate halfspaces time poly d      however 
known particular distributions elicit effective pointwise competitive
learning  contrary  uniform distribution unit sphere among
worst possible distributions pointwise competitive classification  and disagreement based
active learning  unless one utilizes homogeneous halfspaces  see discussion in  e g   el yaniv
  wiener        

figure    svm linear kernel  maximum coverage distance based rejection
technique allows error rate method specific coverage 

   

fiwiener   el yaniv

discussed computational hurdles  recall much applied
machine learning research many applications quite well heuristic
approximations  rather formal ones   practical performance objective 
clever heuristics tricks sometimes make difference  point paper
therefore switch theory practice  aiming implementing rejection method
inspired disbelief principle see well work real world problems 
approximate erm follows  using support vector machines  svms  use
high c value      experiments  penalize training errors small
margin  see definitions svm parameters in  e g  chang   lin         way
solution optimization problem tend get closer erm  order estimate
r fx   restrict svm optimizer consider hypotheses classify
point x specific way  accomplish use weighted svm unbalanced data 
add point x another training point weight    times larger weight
training points combined  thus  penalty misclassification x large
optimizer finds solution doesnt violate constraint 
another problem face disbelief index noisy statistic highly
depends sample sm   overcome noise use robust statistics  first
              k   using bootstrap sampling
generate odd number k different samples  sm


 we used k        sample calculate disbelief index test points
point take median measurements final index  note
finite training sample disbelief index discrete variable  often case
several test points share disbelief index  cases use confidence
measure tie breaker  experiments use distance decision boundary
break ties  focusing svms linear kernel compared rc  risk coverage 
curves achieved proposed method achieved svm rejection based
distance decision boundary  latter approach common practical
applications selective classification  implementation used libsvm  chang  
lin        
tested algorithm standard medical diagnosis problems uci repository  including datasets used grandvalet  rakotomamonjy  keshet  canu        
transformed nominal features numerical ones standard way using binary indicator attributes  normalized attribute independently dynamic
range         preprocessing employed  iteration choose uniformly
random non overlapping training set      samples  test set      samples 
dataset   svm trained entire training set  test samples sorted
according confidence  either using distance decision boundary disbelief index  
figure   depicts rc curves technique  red solid line  rejection based
distance decision boundary  green dashed line  linear kernel   datasets 
results averaged     iterations  error bars show standard error   exception
hepatitis dataset  methods statistically indistinguishable 
datasets proposed method exhibits significant advantage traditional
approach  would highlight performance proposed method
pima dataset  traditional approach cannot achieve error less   
 

due size hepatitis dataset test set limited    samples 

   

fiagnostic pointwise competitive selective classification

figure    rc curves svm rbf kernel  method solid red rejection
based distance decision boundary dashed green 

rejection rate  approach test error decreases monotonically zero rejection
rate  furthermore  clear advantage method large range rejection rates
evident haberman dataset    
sake fairness  note running time algorithm  as presented
here  substantially longer traditional technique  performance algorithm substantially improved many unlabeled samples available 
case rejection function evaluated unlabeled samples generate new
labeled sample  new rejection classifier trained sample 
figure   depicts maximum coverage distance based rejection technique
allows error rate method specific coverage  example  let us
assume method error rate     coverage    
 

haberman dataset contains survival data patients undergone surgery breast cancer 
estimated         new cases breast cancer united states       society       
improvement    affects lives      women 

   

fiwiener   el yaniv

figure     svm rbf kernel  maximum coverage distance based rejection
technique allows error rate method specific coverage 

distance based rejection technique achieves error maximum coverage     
point            red line  thus  red line bellow diagonal
technique advantage distance based rejection visa versa 
example  consider haberman dataset  observe regardless rejection rate 
distance based technique cannot achieve error technique coverage
lower     
figures      depict results obtained rbf kernel  case statistically
significant advantage technique observed datasets 

   related work
pointwise competitive classification unique extreme instance classification
abstention option  idea emerged pattern recognition community 
first proposed studied    years ago chow               generated lots interest

   

fiagnostic pointwise competitive selective classification

 fumera et al         tortorella        santos pereira   pires        fumera   roli       
pietraszek        bounsiar et al         landgrebe et al         herbei   wegkamp       
hellman        el yaniv   pidan        bartlett   wegkamp        wegkap        freund
et al          taking broader perspective  pointwise competitive selective prediction  and
particular  classification  particular instance broader concept confidencerated learning  whereby learner must formally quantify confidence prediction 
achieving effective confidence rated prediction  including abstention  longstanding
challenging goal number disciplines research communities  let us first discuss
prominent approaches confidence rated prediction note
related present work 
knows what it knows  kwik  framework studied reinforcement learning  li 
littman    walsh        strehl   littman        li   littman        similar notion
pointwise competitiveness studied  coverage rates analyzed  li et al        
li         however  kwik limited realizable model concerned
adversarial setting target hypothesis training data selected
adversary  positive results kwik adversarial setting apply
statistical pointwise competitive prediction setting  where training examples sampled
i i d    adversarial setting precludes non trivial coverage interesting hypothesis
classes currently addressed pointwise competitive prediction  deficiency comes
surprise kwik adversarial setting much challenging statistical
pointwise competitive prediction assumptions 
conformal prediction framework  vovk  gammerman    shafer        provides
hedged predictions allowing possibility multi labeled predictions guarantees
user desired confidence rate asymptotic sense  conformal prediction mainly concerned online probabilistic setting  rather predicting single label
sample point  conformal predictor assign multiple labels  user defined confidence level error rate asymptotically guaranteed  interpreting
multi labeled predictions rejection  compare pointwise competitive prediction  sense  conformal prediction construct online predictors reject option
asymptotic performance guarantees  important differences conformal predictions pointwise competitive prediction pointed out 
approaches provide hedged predictions  use different notions hedging  whereas
pointwise competitive prediction goal guarantee high probability
training sample predictor agrees best predictor class
points accepted domain  goal conformal predictions provide guarantees
average error rate  average taken possible samples test
points   sense  conformal prediction cannot achieve pointwise competitiveness 
addition  conformal prediction utilizes different notion error one used
pointwise competitive model  pointwise competitive prediction focused performance guarantees error rate covered  accepted  examples  conformal
prediction provides guarantee examples  including multiple predictions none all   increasing multi labeled prediction rate  uncertain prediction  
 

noted vovk et al   impossible achieve conditional probability error equal given
observed examples  unconditional probability error equals   therefore  implicitly
involves averaging different data sequences     vovk et al         p       

   

fiwiener   el yaniv

error rate decreased arbitrarily small value  case
pointwise competitive prediction error notion covered examples  bounded
bayes error covered region  finally  conformal prediction mentions
notion efficiency  similar coverage but  best knowledge 
finite sample results established  another interesting scheme vicinity
confidence rated learning guaranteed error machine  gem   campi        
gem model reject option considered correct answer  means risk
reduced arbitrarily  as conformal prediction  
pointwise competitive classification special case pointwise competitive prediction
 el yaniv   wiener              wiener   el yaniv        el yaniv   wiener       
wiener        wiener et al          pointwise competitive selective classification first
addressed el yaniv wiener        realizable case studied  in
paper pointwise competitiveness termed perfect classification   present article
extends pointwise competitive classification noisy problems
number theoretical studies  general  selective classification  not
pointwise competitive   freund et al         studied simple ensemble method binary
classification  given hypothesis class f  method outputs weighted average
hypotheses f  weight hypothesis exponentially depends
individual training error  algorithm abstains prediction whenever weighted
average individual predictions close zero  able bound probability
misclassification  r f      m  and  conditions  proved bound
 r f      f  m  rejection rate  less strategy viewed extreme
variation freund et al  method  include ensemble hypotheses
sufficiently low empirical error abstain weighted average predictions
definitive          risk coverage bounds asymptotically tighter 
excess risk bounds developed herbei wegkamp        model
rejection incurs cost           bound applies empirical risk minimizer
hypothesis class ternary hypotheses  whose output     reject    see
various extensions wegkap        bartlett wegkamp        
rejection mechanism svms based distance decision boundary perhaps
widely known used rejection technique  routinely used medical applications  mukherjee et al         guyon et al         mukherjee         papers proposed
alternative techniques rejection case svms  include taking reject
area account optimization  fumera   roli         training two svm classifiers
asymmetric cost  sousa  mora    cardoso         using hinge loss  bartlett  
wegkamp         grandvalet et al         proposed efficient implementation svm
reject option using double hinge loss  empirically compared results
two selective classifiers  one proposed bartlett wegkamp       
traditional rejection based distance decision boundary  experiments
statistically significant advantage either method compared traditional
approach high rejection rates 
pointwise selective classification strongly tied disagreement based active learning 
realizable case  el yaniv wiener        presented reduction stream based
active learning cal algorithm cohn et al         pointwise competitive
classification  reduction roughly states rejection rate  the reciprocal

   

fiagnostic pointwise competitive selective classification

coverage  less o polylog m   m  problem  f  p   actively learnable
cal exponential speedup  consequence reduction resulted first
exponential speedup bounds cal general linear models finite mixture
gaussians  direction  showing exponential speedup cal implies
rejection rate less  in realizable setting  recently established wiener
       wiener  hanneke  el yaniv         using two different techniques  
version space compression set size  extensively utilized present
work  introduced implicitly hanneke      b  special case extended
teaching dimension  context  version space compression set called
minimal specifying set  introduced explicitly el yaniv wiener       
context selective classification  proved el yaniv wiener       
special case extended teaching dimension hanneke      b   relations
disagreement coefficient version space compression set size first discussed
el yaniv wiener         sharp ties two quantities  stated
lemma    others recently developed wiener  hanneke  el yaniv
       

   concluding remarks
find existence pointwise competitive classification quite fascinating  striking
feature classifier that  definition  pointwise competitive predictor free
estimation error cannot overfit  means hypothesis class
expressive still protected overfitting  however  without
effective coverage bounds pointwise competitive classifier may refuse predict
times 
current paper  recent studies selective prediction  el yaniv   wiener 
      active learning  wiener  hanneke    el yaniv         place version space
compression set size center stage  leading quantity drive results
intuition domains  present  known technique able prove fast
coverage pointwise competitive classification exponential label complexity speedup
disagreement based active learning general linear models fixed mixture
gaussians axis aligned rectangles product distributions   possible
extend results beyond linear classifiers axis aligned rectangles interesting
distribution families  example  plausible existing results axis aligned
rectangles extended decision trees 
formal relationship active learning pointwise competitive classification
 el yaniv   wiener        wiener        wiener et al         created powerful synergy
allows migrating results two models  currently  formal connection manifested via two links  first  within realizable setting  equivalence
less based classification fast coverage cal based active learning exponential speedup  second link consists bounds relate underlying complexity
measures  disagreement coefficient active learning  version space compression
set size pointwise competitive classification  number non established relations significantly substantiate interaction two problems could
considered  example  possible prove direct equivalence less based

   

fiwiener   el yaniv

pointwise competitive agnostic classification fast coverage rates less based active
learning exponential speedup  expect resolution question
various interesting implications  example  relationship could potentially facilitate
migration interesting algorithms techniques devised active learning
pointwise competitive framework  immediate candidate algorithm beygelzimer et al          builds ideas dasgupta et al       b  beygelzimer et al 
        resembling implementation proposed less via calls  a constrained  erm
oracle  algorithm works without tracking version space final choice
hypothesis well querying component  instead  querying  relies
erm oracle enforces one example based constrain  thus  importanceweighting technique based resembles disbelief principle outline here 
regard  interesting consider migrate ideas active
learning algorithms emerging online learning branch  orabona   cesa bianchi 
      cesa bianchi et al         dekel et al         using  required  online batch
conversion techniques  zhang        kakade   tewari        cesa bianchi   gentile       
dekel        
less strategy requires unanimous vote among hypotheses low empirical
error subset hypothesis class  considering  e g   linear models  subset
hypotheses uncountable  case  even finite  size huge  clearly 
less extremely radical defensive strategy  immediate question arises
whether less unanimity requirement relaxed majority vote 
achieve pointwise competitiveness  strong  majority vote instead unanimity 
besides greater flexibility general voting scheme  may lead different types
interesting learning algorithms  relaxation potentially ease computational
complexity implementing less  which  discussed above  bottleneck agnostic
classification   example  relaxed voting scheme might utilize hypothesis
sampling  classical example related context celebrated query bycommittee  qbc  strategy  seung et al         freund et al         fine et al         giladbachrach        gilad bachrach et al          however  strict pointwise competitiveness
advocated  easy see strong majority vote sufficient  indeed  consider
f differs hypotheses f single point x   unless probability
point large  not typical case   high probability point part
training set sm   therefore  majority vote  even strong  label
opposite f   hence  worst case  even strong majority sufficient pointwise
competitiveness  natural compromise pointwise competitiveness objective  one
revert standard excess risk bounds  bartlett et al         whereby compare
overall average performance predictor  r f    optimal predictor  r f  
 not pointwise   regard  work freund  mansour  schapire        discussed
section    result excess risk bound r f  g   r f
       m    

  hyper parameter  coverage bound  f  g     r f   ln  f   m     
considering excess risk bounds f   possible beat risk coverage
bounds using relaxed voting scheme rejection  would optimal bounds
fully agnostic setting  better bounds devised specific distributions
gaussian mixtures  note freund et al  strategy interesting

   

fiagnostic pointwise competitive selective classification

final aggregated predictor general outside f can  principle  significantly
outperform f f  the bound elicit behavior   emphasizes
potential usefulness ensembles  applied rejection scheme 
final predictor  recall less strategy final predictor always belongs f 
thus  considering ensembles allowing excess risk bounds  even
ambitious goals  strictly beating f average 

acknowledgments
thank anonymous referees good comments  grateful steve hanneke helpful insightful discussions  also  warmly thank intel collaborative
research institute computational intelligence  icri ci   israel science foundation
 isf  generous support 

appendix a  proofs
proof lemma   relies following lemma     wiener et al          whose
proof provided sake self containment 

lemma     wiener et al          realizable case  r         



 
 
 
       
 r    max max   bn
r   
r r     


proof  prove that  r        




b f   r 
 
 
max   bn
 
       
r
r   

    

result follows taking supremum sides r  r       
fix r         let     r              m   define sm i   sm     xi   yi    
define dm i   dis vsf  sm i b f   r   m i   p xi dm i  sm i     p  dm i y  
b f   r m           clearly holds  otherwise  suppose b f   r m        xi
dis vsf  sm i    must  xi   yi   csm  

n sm  


x
i  

 

dis vsf  sm i    xi   

   

fiwiener   el yaniv

therefore 
p  n sm         b f   r m 
 
 m
x

p
dis vsf  s
   xi         b f   r m
 

p
 p

m i

i  
 m
x

dm i  xi  

 

i  
 m
x

      b f   r m

dis b f  r    xi  

 

i  

 p

 


x



dis b f  r    xi  



 

 

 p

 

i  

x

p



dis b f  r    xi  
i  

x

i  
 m
x

i  
 m
x
 

 

 



 

 

dm i  xi  

dm i  xi  




x

 p

dis b f  r    xi  

i  


x

 
b f   r m 
dis b f  r    xi  
  
 

dm i  xi  


x

 

 

i  

      b f   r m

 
 

dis b f  r    xi     b f   r m
 



 
b f   r m 
dis b f  r    xi  
  

dis b f  r    xi  

 




x
i  

 

 

 
 

dis b f  r    xi   b f   r m
 

       b f   r m

i  

 

 



dis b f  r    xi  

i  



 

dm i  xi  



 

       b f   r m

 

since considering case b f   r m        chernoff bound implies
 

x

exp  b f   r m        e   
p
dis b f  r    xi          b f   r m
 

i  

furthermore  markovs inequality implies
p


x

 

dis b f  r    xi  

i  



 

dm i  xi  

 

       b f   r m


mb f   r  e

hp


i  

 

dm i  xi  

       mb f   r 

since xi values exchangeable 
 
 m



h h
ii x
x
x





e e dm i  xi  fism i  
e m i   m m  
e
dm i  xi    
 

i  

 

i  

i  

   



 

fiagnostic pointwise competitive selective classification

shown  hanneke        least
m   r m  b f   r  
particular  b f   r m        must r                implies
   r   r      
 
 m
x

e
dm i  xi        mb f   r  
 

i  

altogether  established
p  n sm         b f   r m   

mb f   r       mb f   r 
  e 
       mb f   r 
  
 
  e          
  


 
thus  since n sm   bn m    
probability least        must


b f   r 
 
 
bn m 
        b f   r m       
  
r

proof lemma    assuming bn  m      polylog m  log   holds  exists
constant            
bn  m    non bn  m         polylog m   

 
 
bn  m       thus bn m    
   polylog m    therefore 
increasing   bn m    






 
 
max bn m 
  max polylog m    polylog
 
  
r 
m  r 
m  r 
using lemma    have 




 
 r    max
max   bn m 
     
  
m  r 





 
 
 
  polylog
         max bn m 
  
r 
m  r 

references
amaldi  e     kann  v          complexity approximability finding maximum
feasible subsystems linear relations  theoretical computer science                  
bartlett  p  l   jordan  m  i     mcauliffe  j  d          convexity  classification  risk
bounds  journal american statistical association                    
bartlett  p     mendelson  s          discussion      ims medallion lecture  local
rademacher complexities oracle inequalities risk minimization v  koltchinskii  annals statistics               

   

fiwiener   el yaniv

bartlett  p   mendelson  s     philips  p          local complexities empirical risk
minimization  colt  proceedings workshop computational learning
theory  morgan kaufmann publishers 
bartlett  p     wegkamp  m          classification reject option using hinge loss 
journal machine learning research              
ben david  s   eiron  n     long  p          difficulty approximately maximizing
agreements  journal computer system sciences                 
beygelzimer  a   dasgupta  s     langford  j          importance weighted active learning 
proceedings   th annual international conference machine learning  pp 
      acm 
beygelzimer  a   hsu  d   langford  j     zhang  t          agnostic active learning without
constraints  advances neural information processing systems    
beygelzimer  a   dasgupta  s     langford  j          importance weighted active learning 
proceedings   th annual international conference machine learning  pp 
      acm 
beygelzimer  a   hsu  d   langford  j     zhang  t          agnostic active learning without
constraints  arxiv preprint arxiv           
bounsiar  a   grall  e     beauseroy  p          kernel based rejection method supervised classification  international journal computational intelligence            
bousquet  o   boucheron  s     lugosi  g          introduction statistical learning
theory  advanced lectures machine learning  vol       lecture notes
computer science  pp          springer 
campi  m          classification guaranteed probability error  mach  learn          
     
cesa bianchi  n     gentile  c          improved risk tail bounds on line algorithms 
information theory  ieee transactions on                 
cesa bianchi  n   gentile  c     orabona  f          robust bounds classification via
selective sampling  proceedings   th annual international conference
machine learning  pp          acm 
chang  c     lin  c          libsvm  library support vector machines  acm
transactions intelligent systems technology                software available
http   www csie ntu edu tw  cjlin libsvm 
chow  c          optimum character recognition system using decision function  ieee
trans  computer                
chow  c          optimum recognition error reject trade off  ieee trans 
information theory           
cohn  d   atlas  l     ladner  r          improving generalization active learning 
machine learning                 
dasgupta  s   hsu  d     monteleoni  c       a   general agnostic active learning algorithm  nips 

   

fiagnostic pointwise competitive selective classification

dasgupta  s   monteleoni  c     hsu  d  j       b   general agnostic active learning
algorithm  advances neural information processing systems  pp         
dekel  o          online batch learning cutoff averaging   nips 
dekel  o   gentile  c     sridharan  k          robust selective sampling single
multiple teachers   colt  pp         
el yaniv  r     pidan  d          selective prediction financial trends hidden
markov models  nips  pp         
el yaniv  r     wiener  y          foundations noise free selective classification 
journal machine learning research               
el yaniv  r     wiener  y          agnostic selective classification  neural information
processing systems  nips  
el yaniv  r     wiener  y          active learning via perfect selective classification 
journal machine learning research             
el yaniv  r     wiener  y          version space compression set size
applications  vovk  v   papadopoulos  h     gammerman  a   eds    measures
complexity  festschrift alexey chervonenkis  springer  berlin 
fine  s   gilad bachrach  r     shamir  e          query committee  linear separation
random walks  theoretical computer science                
freund  y   mansour  y     schapire  r          generalization bounds averaged classifiers  annals statistics                   
freund  y   seung  h   shamir  e     tishby  n          selective sampling using query
committee algorithm  machine learning             
friedman  e          active learning smooth problems  proceedings   nd
annual conference learning theory 
fumera  g     roli  f          support vector machines embedded reject option 
pattern recognition support vector machines  first international workshop  pp 
       
fumera  g   roli  f     giacinto  g          multiple reject thresholds improving
classification reliability  lecture notes computer science       
gilad bachrach  r          pac beyond  ph d  thesis  hebrew university
jerusalem 
gilad bachrach  r   navot  a     tishby  n          query committee made real 
nips 
grandvalet  y   rakotomamonjy  a   keshet  j     canu  s          support vector machines reject option  nips  pp          mit press 
guyon  i   weston  j   barnhill  s     vapnik  v          gene selection cancer classification using support vector machines   machine learning         
hanneke  s       a   bound label complexity agnostic active learning  icml 
pp         

   

fiwiener   el yaniv

hanneke  s       b   teaching dimension complexity active learning  proceedings   th annual conference learning theory  colt   vol       lecture
notes artificial intelligence  pp       
hanneke  s          theoretical foundations active learning  ph d  thesis  carnegie
mellon university 
hanneke  s          statistical theory active learning  unpublished 
hanneke  s          activized learning  transforming passive active improved label
complexity  journal machine learning research                  
hellman  m          nearest neighbor classification rule reject option  ieee
trans  systems sc  cyb             
herbei  r     wegkamp  m          classification reject option  canadian journal
statistics                 
kakade  s     tewari  a          generalization ability online strongly convex
programming algorithms  advances neural information processing systems
 nips   pp         
koltchinskii  v               ims medallion lecture  local rademacher complexities
oracle inequalities risk minimization  annals statistics               
landgrebe  t   tax  d   paclk  p     duin  r          interaction classification
reject performance distance based reject option classifiers  pattern recognition
letters                 
li  l     littman  m  l          reducing reinforcement learning kwik online regression 
annals mathematics artificial intelligence         
li  l   littman  m     walsh  t          knows knows  framework self aware
learning  proceedings   th international conference machine learning  pp 
        acm 
li  l          unifying framework computational reinforcement learning theory  ph d 
thesis  rutgers  state university new jersey 
massart  p          applications concentration inequalities statistics  annales
de la faculte des sciences de toulouse  vol     pp          universite paul sabatier 
mendelson  s          improving sample complexity using global data  information
theory  ieee transactions on                   
mukherjee  s          chapter    classifying microarray data using support vector machines 
scientists university pennsylvania school medicine school
engineering applied science  kluwer academic publishers 
mukherjee  s   tamayo  p   slonim  d   verri  a   golub  t   mesirov  j  p     poggio  t 
        support vector machine classification microarray data  tech  rep   ai memo
      massachusetts institute technology 
orabona  f     cesa bianchi  n          better algorithms selective sampling 
proceedings   th international conference machine learning  icml     
pp         

   

fiagnostic pointwise competitive selective classification

pietraszek  t          optimizing abstaining classifiers using roc analysis  proceedings twenty second international conference machine learning icml   pp 
       
santos pereira  c     pires  a          optimal reject rules roc curves  pattern
recognition letters                 
seung  h   opper  m     sompolinsky  h          query committee  proceedings
fifth annual workshop computational learning theory  colt   pp         
society  a  c          cancer facts   figures       
sousa  r   mora  b     cardoso  j          ordinal data method classification
reject option  icmla  pp          ieee computer society 
strehl  a  l     littman  m  l          online linear regression application
model based reinforcement learning  advances neural information processing
systems  pp           
tauman kalai  a   klivans  a   mansour  y     servedio  r          agnostically learning
halfspaces  siam j  comput                    
tortorella  f          optimal reject rule binary classifiers  lecture notes computer
science               
tsybakov  a          optimal aggregation classifiers statistical learning  annals
mathematical statistics             
vovk  v   gammerman  a     shafer  g          algorithmic learning random world 
springer  new york 
wang  l          smoothness  disagreement coefficient  label complexity agnostic
active learning  jmlr           
wegkap  m          lasso type classifiers reject option  electronic journal
statistics            
wiener  y          theoretical foundations selective prediction  ph d  thesis  technion
israel institute technology 
wiener  y     el yaniv  r          pointwise tracking optimal regression function 
advances neural information processing systems     pp           
wiener  y   hanneke  s     el yaniv  r          compression technique analyzing
disagreement based active learning  arxiv preprint arxiv           
zhang  t          data dependent concentration bounds sequential prediction algorithms  learning theory  pp          springer 

   


