journal artificial intelligence research                 

submitted        published      

computing convex coverage sets
faster multi objective coordination
diederik m  roijers
shimon whiteson
frans a  oliehoek

d m roijers uva nl
s a whiteson uva nl
f a oliehoek uva nl

informatics institute
university amsterdam
amsterdam  netherlands

abstract
article  propose new algorithms multi objective coordination graphs  mocogs   key efficiency algorithms compute convex coverage
set  ccs  instead pareto coverage set  pcs   ccs sufficient solution
set large class problems  important characteristics facilitate
efficient solutions  propose two main algorithms computing ccs mo cogs 
convex multi objective variable elimination  cmove  computes ccs performing
series agent eliminations  seen solving series local multi objective
subproblems  variable elimination linear support  vels  iteratively identifies single
weight vector w lead maximal possible improvement partial ccs
calls variable elimination solve scalarized instance problem w  vels
faster cmove small medium numbers objectives compute
 approximate ccs fraction runtime  addition  propose variants
methods employ and or tree search instead variable elimination achieve
memory efficiency  analyze runtime space complexities methods  prove
correctness  compare empirically naive baseline existing
pcs method  terms memory usage runtime  results show that 
focusing ccs  methods achieve much better scalability number
agents current state art 

   introduction
many real world problem domains  maintenance planning  scharpff  spaan 
volker    de weerdt        traffic light control  pham et al          multiple agents
need coordinate actions order maximize common utility  key coordinating
efficiently domains exploiting loose couplings agents  guestrin  koller 
  parr        kok   vlassis         agents actions directly affect subset
agents 
multi agent coordination complicated fact that  many domains  agents need
balance multiple objectives  roijers  vamplew  whiteson    dazeley      a   example  agents might maximize performance computer network minimizing
power consumption  tesauro  das  chan  kephart  lefurgy  levine    rawson        
maximize cost efficiency maintenance tasks road network minimizing traffic
delays  roijers  scharpff  spaan  oliehoek  de weerdt    whiteson        
c
    
ai access foundation  rights reserved 

firoijers  whiteson    oliehoek

figure    mining company example 
however  presence multiple objectives per se necessitate use
specialized multi objective solution methods  problem scalarized  i e  
utility function converted scalar utility function  problem may solvable
existing single objective methods  conversion involves two steps  roijers et al  
    a   first step specify scalarization function 
definition    scalarization function f   function maps multi objective utility
solution decision problem  u a   scalar utility uw  a  
uw  a    f  u a   w  
w weight vector parameterizes f  
second step define single objective version decision problem
utility solution equals scalarized utility original problem uw  a  
unfortunately  scalarizing problem solving always possible
w may known advance  example  consider company mines different
resources  figure    depict problem company faces  morning one van
per village needs transport workers village nearby mine  various
resources mined  different mines yield different quantities resource per worker 
market prices per resource vary stochastic process every price change
alter optimal assignment vans  expected price variation increases
passage time  maximize performance  thus critical act based latest
possible price information  since computing optimal van assignment takes time  redoing
computation every price change highly undesirable 
settings  need multi objective method computes  advance  optimal solution possible prices  w  call set coverage set  cs   many
cases  w revealed solution must executed  case solution
automatically selected cs given w  cases  w never made explicit
instead human involved decision making selects one solution
cs  perhaps basis constraints preferences difficult formalize
objectives  roijers et al       a   cases  cs typically
much smaller complete set solutions  selecting optimal joint action
cs typically much easier selecting directly complete set solutions 
   

ficomputing ccss faster multi objective coordination

article  consider multi objective methods made efficient problems require coordination multiple  loosely coupled agents  particular  address multi objective coordination graphs  mo cogs   one shot multi agent decision problems loose couplings expressed using graphical model  mo cogs form
important class decision problems  used model variety realworld problems  delle fave  stranders  rogers    jennings        marinescu        rollon 
       many sequential decision problems modeled series mo cogs 
common single objective problems  guestrin et al         kok   vlassis        oliehoek 
spaan  dibangoye    amato        
key efficiency mo cog methods propose compute convex
coverage set  ccs  instead pareto coverage set  pcs   ccs subset
pcs sufficient solution multi objective problem linear scalarization
function  example  mining company example figure    f linear  since
total revenue simply sum quantity resource mined times price per
unit  however  even f nonlinear  stochastic solutions allowed  ccs
sufficient  
ccs previously considered solution concept mo cogs
computing ccs requires running linear programs  whilst computing pcs requires
pairwise comparisons solutions  however  key insight article  that  loosely
coupled systems  ccss easier compute pcss  two reasons  first  ccs
 typically much smaller  subset pcs  loosely coupled settings  efficient methods
work solving series local subproblems  focusing ccs greatly reduce size
subproblems  second  focusing ccs makes solving mo cog equivalent
finding optimal piecewise linear convex  pwlc  scalarized value function 
efficient techniques adapted  reasons  argue ccs often
concept choice mo cogs 
propose two approaches exploit insights solve mo cogs efficiently
existing methods  delle fave et al         dubus  gonzales    perny        marinescu 
razak    wilson        rollon   larrosa         first approach deals multiple
objectives level individual agents  second deals global
level 
first approach extends algorithm rollon larrosa        refer
pareto multi objective variable elimination  pmove      computes local pareto
sets agent elimination  compute ccs instead  call resulting algorithm
convex multi objective variable elimination  cmove  
second approach new abstract algorithm call optimistic linear support
 ols  much faster small medium numbers objectives  furthermore  ols
   precise  case stochastic strategies ccs deterministic strategies always sufficient
 vamplew  dazeley  barker    kelarev         case deterministic strategies  linearity
scalarization function makes ccs sufficient  roijers et al       a  
   article synthesizes extends research already reported two conference papers  specifically 
cmove algorithm  section    previously published adt  roijers  whiteson    oliehoek 
    b  vels algorithm  section    aamas  roijers  whiteson    oliehoek        
memory efficient methods computing ccss  section    novel contribution article 
   original article  algorithm called multi objective bucket elimination  mobe   however 
use pmove consistent names algorithms mentioned article 

   

firoijers  whiteson    oliehoek

used produce bounded approximation ccs   ccs 
enough time compute full ccs  ols generic method employs single objective
solvers subroutine  article  consider two implementations subroutine 
using variable elimination  ve  subroutine yields variable elimination linear support
 vels   particularly fast small moderate numbers objectives
memory efficient cmove  however  memory highly limited  reduction
memory usage may enough  cases  using and or search  mateescu  
dechter        instead yields and or tree search linear support  tsls  
slower vels much memory efficient 
prove correctness cmove ols  analyze runtime space
complexities methods show methods better guarantees
pcs methods  show cmove ols complementary  i e   various trade offs exist
variants 
furthermore  demonstrate empirically  randomized realistic problems  cmove vels scale much better previous algorithms  empirically confirm trade offs cmove ols  show ols  used
bounded approximation algorithm  save additional orders magnitude runtime 
even small   finally  show that  even memory highly limited  tsls
still solve large problems 
rest article structured follows  first  provide formal definition
model  well overview existing solution methods section   
presenting naive approach section    sections         analyze runtime
space complexities algorithm  compare empirically 
existing algorithms  end section  finally  conclude section  
overview contributions findings  suggestions future research 

   background
section  formalize multi objective coordination graph  mo cog  
however  describe single objective version problem  coordination graph
 cog   mo cog extension  variable elimination  ve  algorithm
solving cogs  methods present section     build different ways 
     single objective  coordination graphs
coordination graph  cog   guestrin et al         kok   vlassis        tuple hd  a  ui 

           n  set n agents 
  ai     joint action space  cartesian product finite action
spaces agents  joint action thus tuple containing action agent
  ha         i 


u   u         u set scalar local payoff functions  limited
scope  i e   depends onlypa subset agents  total team payoff sum
local payoffs  u a    e   ue  ae   
   

ficomputing ccss faster multi objective coordination

figure     a  cog   agents   local payoff functions  b  eliminating agent  
adding u   c  eliminating agent   adding u   

a 
a 

a 
    
    

a 
 
    

a 
a 

a 
   
 

a 
   
 

table    payoff matrices u   a    a     left  u   a    a     right   two possible
actions per agent  denoted dot  a    bar  a    

agents share payoff function u a   abuse notation e index local
payoff function ue denote subset agents scope  ae thus local joint
action  i e   joint action subset agents 
decomposition u a  local payoff functions represented factor
graph  bishop         bipartite graph containing two types vertices  agents  variables 
local payoff functions  factors   edges connecting local payoff functions
agents scope 
figure  a shows factor graph example cog team payoff function
decomposes two local payoff functions  two agents scope 
u a   


x

ue  ae     u   a    a      u   a    a    

e  

local payoff functions defined table    factor graph illustrates loose
couplings result decomposition local payoff functions  particular 
agents choice action directly depends immediate neighbors  e g  
agent   knows agent  s action  choose action without considering agent   
    variable elimination
discuss variable elimination  ve  algorithm  several multi objective
extensions  rollon   larrosa        rollon        build  including cmove algorithm  section     use subroutine ols algorithm  section    
exploits loose couplings expressed local payoff functions efficiently
compute optimal joint action  i e   joint action maximizing u a   first  forward
   

firoijers  whiteson    oliehoek

pass  eliminates agents turn computing value agents best
response every possible joint action neighbors  values used construct
new local payoff function encodes value best response replaces agent
payoff functions participated  original algorithm  agents
eliminated  backward pass assembles optimal joint action using constructed
payoff functions  here  present slight variant payoff tagged
action generates it  obviating need backwards pass  two algorithms
equivalent  variant amenable multi objective extension present
section   
eliminates agents graph predetermined order  algorithm   shows
pseudocode elimination single agent i  first  determines set local
payoff functions connected i  ui   neighboring agents i  ni  lines      
definition    set neighboring local payoff functions ui set local
payoff functions agent scope 
definition    set neighboring agents i  ni   set agents
scope one local payoff functions ui  
then  constructs new payoff function computing value agent best
response possible joint action ani agents ni  lines        so 
loops joint actions ani  line     ani   loops actions ai
available agent  line     ai ai   computes local payoff agent
responds ani ai  line     tags total payoff ai   action generates
 line    order able retrieve optimal joint action later  already
tags present  appends ai them  way  entire joint action incrementally
constructed  maintains value best response taking maximum
payoffs  line      finally  eliminates agent payoff functions ui replaces
newly constructed local payoff function  line     
algorithm    elimve u  i 
 
 
 
 
 
 
 

input  cog u  agent
ui set local payoff functions involving
ni set neighboring agents
unew new factor taking joint actions ni   ani   input
foreach ani ani

foreach ax
ai
v
uj  ani   ai  
uj ui

 
 
  
  
  
  

tag v ai
 v 
end
unew  ani   max s 
end
return  u   ui    unew  

   

ficomputing ccss faster multi objective coordination

consider example figure  a table    optimal payoff maximizes sum
two payoff functions 
max u a    max u   a    a      u   a    a    


a   a   a 

eliminates agent   first  pushes maximization a  inward
goes local payoff functions involving agent    case u   


 
 
max u a    max u  a    a      max u  a    a     


a   a 

a 

solves inner maximization replaces new local payoff function u 
depends agent  s neighbors  thereby eliminating agent   

max u a    max u   a    a      u   a     


a   a 

leads new factor graph depicted figure  b  values u   a    u   a     
     using a    u   a        using a    optimal payoffs actions
agent    given payoffs shown table    ultimately want optimal joint
action  optimal payoff  tags payoff u  action agent  
generates it  i e   think u   a     value  tag  pair  denote pair
parentheses subscript  u   a           a    u   a         a   
next eliminates agent    yielding factor graph shown figure  c 


 
 
max u a    max max u  a    a      u  a      max u   a    


a 

a 

a 

appends new tags agent   existing tags agent    yielding following
tagged payoff values  u   a      maxa  u   a    a      u   a           a         a  a          a  a 
u   a            a       a  a          a  a    finally  maximizing a  yields optimal
payoff       a  a  a    optimal action contained tags 
runtime complexity exponential  number agents 
induced width  often much less number agents 
theorem    computational complexity o n amax  w    amax  
maximal number actions single agent w induced width  i e   maximal
number neighboring agents agent plus one  the agent    moment
eliminated  guestrin et al         
theorem    space complexity o  n  amax  w   
space complexity arises because  every agent elimination  new local payoff
function created o  amax  w   fields  possible input actions   since impossible
tell priori many new local payoff functions exist given time
execution ve  need multiplied total number new local payoff
functions created execution  n 
designed minimize runtime  methods focus memory efficiency
instead  mateescu   dechter         discuss memory efficiency section     
   fact  proven best runtime guarantees within large class algorithms  rosenthal 
      

   

firoijers  whiteson    oliehoek

a 
a 

a 
     
     

a 
     
     

a 
a 

a 
     
     

a 
     
     

table    two dimensional payoff matrices u   a    a     left  u   a    a     right  
    multi objective coordination graphs
multi objective coordination

graph  mo cog  tuple hd  a  ui
but  u   u         u set   d dimensional local p
payoff functions 
total team payoff sum local vector valued payoffs  u a    e   ue  ae    use
ui indicate value i th objective  denote set possible joint action
values v  table   shows two dimensional mo cog structure
single objective example section      multi objective payoffs 
solution mo cog coverage set  cs  joint actions associated values
u a  contains least one optimal joint action possible parameter vector w
scalarization function f  definition     cs subset undominated set 
definition    undominated set  u  mo cog  set joint actions
associated payoff values optimal w scalarization function f  


u  v    u a    u a  v wa  uw  a  uw  a     
care least one optimal joint action every w  rather
optimal joint actions  lossless subset u suffices 
definition    coverage set  cs   cs v   subset u   possible w 
least one optimal solution cs  i e  
wa


u a  cs v  a  uw  a  uw  a     

note cs necessarily unique  typically seek smallest possible cs 
convenience  assume payoff vectors cs contain values associated
joint actions  suggested tagging scheme described section     
payoff vectors v cs depends know
scalarization function f   minimal assumption f monotonically increasing  i e  
value one objective ui   increases uj  i stay constant  scalarized value
u a  cannot decrease  assumption ensures objectives desirable  i e   else
equal  always better 
definition    pareto front undominated set arbitrary strictly monotonically
increasing scalarization functions f  


p f  v    u a    u a  v a  u a    p u a   
p indicates pareto dominance  p dominance   greater equal objectives
strictly greater least one objective 
   

ficomputing ccss faster multi objective coordination

order optimal scalarized values  necessary compute entire
pf  e g   two joint actions equal payoffs need retain one those 
definition    pareto coverage set  pcs   p cs v  p f  v   coverage set
arbitrary strictly monotonically increasing scalarization functions f   i e  

a  u a  p cs v   u a  p u a    u a    u a      
computing p dominance requires pairwise comparison payoff vectors  feng  
zilberstein         
highly prevalent scenario that  addition f monotonically increasing 
know linear  is  parameter vectors w weights
values individual objectives multiplied  f   w u a   mining example
figure    resources traded open market resources positive unit
price  case  scalarization linear combination amount resource
mined  weights correspond price per unit resource  many
examples linear scalarization functions exist literature  e g    lizotte  bowling   
murphy         assume linear scalarization monotonically increasing 
represent without loss generality convex combination objectives  i e  
weights positive sum    case  convex coverage set  ccs 
needed  subset convex hull  ch     
definition    convex hull  ch  undominated set linear non decreasing
scalarizations f  u a   w    w u a  


ch v    u a    u a  v wa  w u a  w u a     
is  ch contains solutions attain optimal value least one weight 
vectors ch c dominated  contrast p domination  c domination cannot
tested pairwise comparisons take two payoff vectors
c dominate payoff vector  note ch contains solutions needed guarantee optimal scalarized value value  contain multiple solutions optimal
one specific weight  lossless subset ch respect linear scalarizations
called convex coverage set  ccs   i e   ccs retains least one u a  maximizes
scalarized payoff  w u a   every w 
definition    convex coverage set  ccs   ccs v  ch v   cs linear nondecreasing scalarizations  i e  

wa u a  ccs v  a  w u a  w u a     
since linear non decreasing functions specific type monotonically increasing function  always ccs subset smallest possible pcs 
previously mentioned  css pcs ccs  may unique  example 
two joint actions equal payoff vectors  need one
make pcs ccs 
   p dominance often called pairwise dominance pomdp literature 
   note term convex hull overloaded  graphics  convex hull superset mean
convex hull article 

   

firoijers  whiteson    oliehoek

figure    ccs  filled circles left  solid black lines right  versus pcs  filled circles
squares left  dashed solid black lines right  twelve random
  dimensional payoff vectors 

practice  pcs ccs often equal pf ch  however 
algorithms proposed article guaranteed produce pcs ccs 
necessarily entire pf ch  pcss ccss sufficient solutions
terms scalarized value  say algorithms solve mo cogs 
figure    left  values joint actions  u a   represented points valuespace  two objective mo cog  joint action value ccs
pcs  b  however  pcs  ccs  weight
linear scalarization bs value would optimal  shown figure    right  
scalarized value strategies plotted function weight first objective
 w      w     c neither ccs pcs  pareto dominated a 
many multi objective methods  e g    delle fave et al         dubus et al         marinescu et al         rollon        simply assume pcs appropriate solution
concept  however  argue choice cs depends one assume
utility defined respect multiple objectives  i e   scalarization function used scalarize vector valued payoffs  argue many situations
scalarization function linear  cases one use ccs 
addition shape f   choice solution concept depends whether
deterministic joint actions considered whether stochastic strategies permitted  stochastic strategy assigns probabilitypto joint action        
probabilities joint actions together sum    aa  a       value stochastic strategy linear
p combination value vectors joint actions

mixture  u  
aa  a u a   therefore  optimal values  monotonically
increasing f   lie convex upper surface spanned strategies ccs 
indicated lines figure    left   therefore  optimal values monotonically
increasing f   including nonlinear ones  constructed taking mixture policies
ccs  vamplew et al         
article considers methods computing ccss  which  show sections  
   computed efficiently pcss  furthermore  ccss typically much
   

ficomputing ccss faster multi objective coordination

smaller  particularly important final selection joint done  a
group of  humans  compare possible alternatives solution set 
methods presented article based variable elimination  ve   sections
     and or tree search  ts   section     algorithms exact solution
methods cogs 
cmove algorithm propose section   based ve  differs another
multi objective algorithm based ve  refer pmove  rollon   larrosa 
       produces ccs rather pcs  alternative messagepassing algorithms  max plus  pearl        kok   vlassis      a   however 
guaranteed exact tree structured cogs  multi objective methods build
max plus delle fave et al          limitation  unless
preprocess cog form clique tree gai network  dubus et al         
tree structured graphs  message passing algorithms produce optimal solutions
similar runtime guarantees  note that  pmove  existing multi objective methods
based message passing produce pcs rather ccs 
section    take different approach multi objective coordination based
outer loop approach  explain  approach applicable computing ccs 
pcs  considerable advantages terms runtime memory usage 

   non graphical approach
naive way compute ccs ignore graphical structure  calculate set
possible payoffs joint actions v  prune away c dominated joint actions 
first translate problem set value set factors  vsfs   f  vsf f function
mapping local joint actions sets payoff vectors  initial vsfs constructed
local payoff functions
f e  ae      ue  ae    
i e   vsf maps local joint action singleton set containing actions
local payoff  define v terms f using cross sum operator vsfs
f joint action a 
 m
v f   
f e  ae   
f e f

cross sum two sets b contains possible vectors made
summing one payoff vector set 
b    a   b   b b   
ccs calculated applying pruning operator cprune  described below 
removes c dominated vectors set value vectors  v 
 m
ccs v f     cprune v f     cprune 
f e  ae    
   
f e f

non graphical ccs algorithm simply computes righthand side equation    i e  
computes v f  explicitly looping actions  action looping
local vsfs  pruning set ccs 
   

firoijers  whiteson    oliehoek

ccs contains least one payoff vector maximizes scalarized value every
w 
w



  arg max w u a 



  a 

u a    ccs v f   w u a    w u a        

aa

is  every w solution a  part ccs achieves
value maximizing solution a  moreover value solutions given
dot product  thus  finding ccs analogous problem faced partially observable
markov decision processes  pomdps   feng   zilberstein         optimal  vectors
 corresponding value vectors u a   beliefs  corresponding weight vectors
w  must found  therefore  employ pruning operators pomdp literature 
algorithm   describes implementation cprune  based feng
zilberstein        one modification  order improve runtime guarantees  cprune
first pre prunes candidate solutions u pcs using pprune  algorithm    line
   pprune computes pcs o d u  p cs   running pairwise comparisons  next 
partial ccs  u   constructed follows  random vector u u selected line   
u algorithm tries find weight vector w u better vectors
u  line     solving linear program algorithm    w  cprune
finds best vector v w u moves u  line        weight
u better c dominated thus removed u u  line    
algorithm    cprune u 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  

input  set payoff vectors u
u pprune u 
u
notempty u 
select random u u
w findweight u  u  
w null
  did find weight u optimal
remove u u
end
else
v arg maxuu w u
u u    v 
u u  v 
end
end
return u

runtime cprune defined algorithm  
o d u  p cs     p cs p  d ccs    

   

p  d ccs   polynomial size ccs number objectives d 
runtime linear program tests c domination  algorithm    
   

ficomputing ccss faster multi objective coordination

algorithm    pprune u 
 
 
 
 
 
 
 
 
 
  
  
  

input  set payoff vectors u
u
u   
u first element u
foreach v u
v p u
u v    continue v instead u
end
end
remove u  vectors p dominated u  u
add u u
end
return u

algorithm    findweight u  u 
max x
x w

subject w  u u    x    u  u

x

wi    

i  

x     return w else return null

key downside non graphical approach requires explicitly enumerating
possible joint actions calculating payoffs associated one  consequently 
intractable small numbers agents  number joint actions grows
exponentially number agents 
theorem    time complexity computing ccs mo cog containing local
payoff functions  following non graphical approach  equation    is 
o d amax  n   d amax  n  p cs     p cs p  d ccs  
proof  first  v computed looping vsfs joint action a  summing
vectors length d  maximum size action space agent amax
o  amax  n   joint actions  v contains one payoff vector joint action  v input
cprune 
next two sections  present two approaches compute ccss efficiently 
first approach pushed cprune operator equation   cross sum
union  max operator pushed summation ve  call
inner loop approach  uses pruning operators agent eliminations 
inner loop algorithm  second approach inspired linear support  cheng 
   

firoijers  whiteson    oliehoek

       pomdp pruning operator requires finding optimal solution certain w  instead performing maximization entire set v  original linear
support algorithm  show use finite number scalarized instances
mo cog  avoiding explicit calculation v  call approach outer loop
approach  creates outer loop around single objective method  like ve  
calls subroutine 

   convex variable elimination mo cogs
section show exploit loose couplings calculate ccs using inner loop approach  i e   pushing pruning operators cross sum union
operators equation    result cmove  extension rollon larrosas
pareto based extension ve  refer pmove  rollon   larrosa        
analyzing cmoves complexity terms local convex coverage sets  show
approach yields much better runtime complexity guarantees non graphical
approach computing ccss presented section   
    exploiting loose couplings inner loop
non graphical approach  computing ccs expensive computing pcs 
shown section    show that  mo cogs  compute ccs
much efficiently exploiting mo cogs graphical structure  particular 
ve  solve mo cog series local subproblems  eliminating agents
manipulating set vsfs f describe mo cog  key idea compute
local ccss  lccss  eliminating agent instead single best response  as ve  
computing lccs  algorithm prunes away many vectors possible 
minimizes number payoff vectors calculated global level 
greatly reduce computation time  describe elim operator eliminating agents
used cmove section     
first need update definition neighboring local payoff functions  definition
    neighboring vsfs 
definition     set neighboring vsfs set local payoff functions
agent scope 
neighboring agents ni agent agents scope vsf
  except itself  corresponding definition    possible local joint action
ni   compute lccs contains payoffs c undominated responses
agent i  best response values i  words  ccs subproblem
arises considering fixing specific local joint action ani   compute
lccs  must consider payoff vectors subproblem  vi   prune dominated
ones 
definition     fix actions
ani   ai   set payoff vectors
l
subproblem is  vi  fi   ani     ai f e f e  ae    ae formed ai
appropriate part ani  
using definition     define lccs ccs vi  
   

ficomputing ccss faster multi objective coordination

definition     local ccs  lccs  c undominated subset vi  fi   ani   
lccsi  fi   ani     ccs vi  fi   ani    

using lccss  create new vsf  f new   conditioned actions
agents ni  
ani f new  ani     lccs  fi   ani   
elim operator replaces vsfs f new factor 
elim f  i     f      f new  ani    
theorem    elim preserves ccs  f ccs v f     ccs v elim f  i    
proof  show using implication equation    i e   joint actions
w scalarized value maximal  vector valued payoff
u a    w u a      w u a    ccs  show maximal scalarized
payoff cannot lost result elim 

function distributes local payoff functions  w u a   
p linear scalarization
p
w e ue  ae     e w ue  ae    thus  eliminating agent i  divide set vsfs
non neighbors  nn   agent participate  neighbors  ni  
that 
x
x
w u a   
w ue  ae    
w ue  ae   
enn

eni

now  following equation    ccs contains maxaa w u a  w  elim pushes
maximization in 
max w u a    max
aa

ai ai

x

w ue  ae     max

ai ai

enn

x

w ue  ae   

eni

elim
agent i factors term f new  ani   satisfies w f new  ani     maxai
p replaces
e
eni w u  ae   per definition  thus preserving maximum scalarized value w
thereby preserving ccs 
instead lccs  could compute local pcs  lpcs   is  using pcs
computation vi instead ccs computation  note that  since lccs lpcs vi  
elim reduces problem size respect vi   would
possible considered p dominance  therefore  focusing ccs greatly
reduce sizes local subproblems  since solution local subproblem input
next agent elimination  size subsequent local subproblems reduced 
lead considerable speed ups 
   

firoijers  whiteson    oliehoek

    convex multi objective variable elimination
present convex multi objective variable elimination  cmove  algorithm 
implements elim using cprune  ve  cmove iteratively eliminates agents none
left  however  implementation elim computes ccs outputs correct
joint actions payoff vector ccs  rather single joint action  cmove
extension rollon larrosas pareto based extension ve  refer
pmove  rollon   larrosa        
important difference cmove pmove cmove computes ccs  typically leads much smaller subproblems thus much better
computational efficiency  addition  identify three places pruning take
place  yielding flexible algorithm different trade offs  finally  use tagging scheme instead backwards pass  section     
algorithm   presents abstract version cmove leaves pruning operators
unspecified  section    cmove first translates problem set vector set
factors  vsfs   f line    next  cmove iteratively eliminates agents using elim  line
     elimination order determined using techniques devised single objective
 koller   friedman        
algorithm    cmove u  prune   prune   prune   q 

 
 
 
 
 
 
 
 

input  set local payoff functions u elimination order q  a queue containing
agents 
f create one vsf every local payoff function u
ani ani
q dequeue  
f elim f  i  prune   prune  
end
f retrieve final factor f
f  a  
return prune  s 

algorithm   shows implementation elim  parameterized two pruning operators  prune  prune   corresponding two different pruning locations inside
operator computes lccsi   computelccsi  fi   ani   prune   prune   
algorithm    elim f  i  prune   prune  
 
 
 
 
 
 
 
 

input  set vsfs f  agent
ni set neighboring agents
subset vsf scope
f new  ani   new vsf
foreach ani ani
f new  ani   computelccs  fi   ani   prune   prune  
end
f f    f new  
return f

   

ficomputing ccss faster multi objective coordination

computelccsi implemented follows  first define new cross sum and prune
  prune  a b   lccsi applies operator sequentially 
operator ab
 m

f e  ae    
   
computelccsi  fi   ani   prune   prune     prune  
e
ai

f

operator  leading incremental
prune  applied cross sum two sets  via
pruning  cassandra  littman    zhang         prune  applied coarser level 
union  cmove applies elim iteratively agents remain  resulting ccs 
note that  agents left  f new line   agents condition on 
case  consider actions neighbors single empty action   
pruning applied end  agents eliminated 
call prune   increasing level coarseness  thus three pruning operators  incremental pruning  prune    pruning union actions eliminated
agent  prune    pruning agents eliminated  prune    reflected
algorithm    agents eliminated  final factor taken set
factors  line     single set  contained factor retrieved  line     note
use empty action denote field final factor  agents
scope  finally prune  called s 
consider example figure  a  using payoffs defined table    apply
cmove  first  cmove creates vsfs f   f   u  u    eliminate agent   
creates new vsf f    a    computing lccss every a  tagging element
set action agent   generates it  a    cmove first generates
set        a          a     since vectors optimal w  neither
removed pruning thus f    a             a          a     a    cmove first generates
       a          a     cprune determines       a  dominated consequently removes
it  yielding f    a             a     cmove adds f   graph removes f  
agent    yielding factor graph shown figure  b 
cmove eliminates agent   combining f   f   create f     f    a    
cmove must calculate lccs of 
 f    a    a    f    a      f    a    a    f    a     
first cross sum yields        a  a          a  a    second yields        a  a     pruning
union yields f    a             a  a          a  a     similarly  a  taking union yields
       a  a          a  a          a  a     lccs f    a             a  a     adding f  
results graph figure  c 
finally  cmove eliminates agent    since neighboring agents left  ai
contains empty action  cmove takes union f    a    f    a     since
       a  a  a           a  a  a    dominate        a  a  a      latter pruned  leaving ccs  
        a  a  a             a  a  a      
    cmove variants
several ways implement pruning operators lead correct instantiations cmove  pprune  algorithm    cprune  algorithm    used 
long either prune  prune  cprune  note prune  computes ccs  prune 
necessary 
   

firoijers  whiteson    oliehoek

article  consider basic cmove  use prune  prune 
prunes prune  using cprune  well incremental cmove  uses cprune
prune  prune   latter invests effort intermediate pruning 
result smaller cross sums  resulting speedup  however  vectors
pruned intermediate steps  additional speedup may occur 
algorithm creates unnecessary overhead   empirically investigate variants
section    
one could consider using pruning operators contain prior knowledge
range possible weight vectors  information available  could easily
incorporated changing pruning operators accordingly  leading even smaller lccss 
thus faster algorithm  article however  focus case
prior knowledge available 
    analysis
analyze correctness complexity cmove 
theorem    move correctly computes ccs 
proof  proof works induction number agents  base case original
mo cog  f e  ae   f singleton set  then  since elim preserves ccs
 see theorem     necessary vectors lost  last agent eliminated 
one factor remains  since conditioned agent actions result
lccs computation  must contain one set  ccs 
theorem    computational complexity cmove
o  n  amax  wa  wf r    r      r    

   

wa induced agent width  i e   maximum number neighboring agents  connected via factors  agent eliminated  wf induced factor width  i e  
maximum number neighboring factors agent eliminated  r    r  r 
cost applying prune   prune  prune  operators 
proof  cmove eliminates n agents one computes lccs joint
action eliminated agents neighbors  field new vsf  cmove computes
o  amax  wa   fields per iteration  calling prune   equation    adjacent factor 
prune  taking union actions eliminated agent  prune  called
exactly once  eliminating agents  line   algorithm    
unlike non graphical approach  cmove exponential wa   number
agents  respect  results similar pmove  rollon        
however  earlier complexity results make effect pruning explicit  instead 
complexity bound makes use additional problem constraints  limit total
number possible different value vectors  specifically  analysis pmove 
payoff vectors integer valued  maximum value objectives  practice 
   compute pcs first  using prune  prune   compute ccs prune  
however  useful small problems pcs cheaper compute ccs 

   

ficomputing ccss faster multi objective coordination

bounds loose even impossible define  e g   payoff values
real valued one objectives   therefore  instead give description
computational complexity makes explicit dependence effectiveness
pruning  even though complexity bounds better worst case  i e  
pruning possible   allow greater insight runtimes algorithms
evaluate  apparent analysis experimental results section     
theorem   demonstrates complexity cmove depends heavily runtime
pruning operators  turn depends sizes input sets  input
set prune  union returned series applications prune  
prune  uses output last application prune   therefore need balance
effort lower level pruning higher level pruning  occurs less
often dependent output lower level  bigger lccss 
gained lower level pruning 
theorem    space complexity cmove
o  n  amax  wa  lccsmax      amax   emax     
 lccsmax   maximum size local ccs  original number vsfs 
 emax   maximum scope size original vsfs 
proof  cmove computes local ccs new vsf joint action eliminated agents neighbors  maximally wa neighbors  maximally n new
factors  payoff vector stores real numbers 
vsfs created initialization cmove  vsfs
exactly one payoff vector containing real numbers  per joint action agents scope 
maximally  amax   emax   joint actions 
pmove  space complexity  p ccsmax   instead  lccsmax   
lccs subset corresponding lpcs  cmove thus strictly
memory efficient pmove 
note theorem   rather loose upper bound space complexity 
vsfs  original new  exist time  however  possible predict
priori many vsfs exist time  resulting space complexity
bound basis vsfs exist point execution cmove 
    empirical evaluation
test efficiency cmove  compare runtimes pmove 
non graphical approach problems varying numbers agents objectives 
analyze runtimes correspond sizes pcs ccs 
use two types experiments  first experiments done random mocogs directly control variables  second experiment  use
mining day  realistic benchmark  structured random mo cogs
still randomized 
   compare pmove using prune    pprune  rather prune    prune    pprune 
proposed original article  rollon   larrosa        found former option slightly
consistently faster 

   

firoijers  whiteson    oliehoek

 a 

 b 

 c 

figure     a  runtimes  ms  log scale nongraphical method  pmove cmove
standard deviation mean  error bars    b  corresponding number vectors
pcs ccs   c  corresponding spread induced width 

      random graphs
generate random mo cogs  employ procedure takes input  n  number
agents  d  number payoff dimensions  number local payoff functions 
 ai    action space size agents  agents  procedure
starts fully connected graph local payoff functions connecting two agents
each  then  local payoff functions randomly removed  ensuring graph
remains connected  local payoff functions remain  values different
objectives local payoff function real numbers drawn independently
uniformly interval          compare algorithms set randomly
generated mo cogs separate value n  d     ai   
compare basic cmove  incremental cmove  pmove  non graphical
method  test random mo cogs number agents ranging
       average number factors per agent held      n  number
objectives      experiment run     ghz intel core i  computer    gb
memory  figure   shows results  averaged    mo cogs number agents 
runtime  figure  a  non graphical method quickly explodes  cmove
variants slower pmove small numbers agents  runtime grows much
slowly pmove     agents  cmove variants faster
pmove average     agents  one mo cogs generated caused pmove
time     s  basic cmove maximum runtime    s  incremental
cmove    s  explained differences size solutions  i e  
pcs ccs  figure  b   pcs grows much quickly number
agents ccs does  two objective problems  incremental cmove seems
consistently slower basic cmove 
cmoves runtime grows much slowly nongraphical method 
still exponential number agents  counterintuitive result since worst case
complexity linear number agents  explained induced width
mo cogs  runtime cmove exponential  figure  c  see
induced width increases linearly number agents random graphs 
   

ficomputing ccss faster multi objective coordination

figure    runtimes  ms  non graphical method  pmove cmove log scale
standard deviation mean  error bars   left  corresponding number vectors
pcs ccs  right   increasing numbers agents   objectives 

therefore conclude that  two objective mo cogs  non graphical method
intractable  even small numbers agents  runtime cmove increases
much less number agents pmove does 
test runtime behavior changes higher number objectives  run
experiment average number factors per agent held      n
increasing numbers agents again       remaining experiments
described section executed xeon l          ghz computer    gb
memory  figure    left  shows results experiment  averaged    mo cogs
number agents  note plot induced widths 
change number objectives  results demonstrate that  number
agents grows  using cmove becomes key containing computational cost solving
mo cog  cmove outperforms nongraphical method    agents onwards 
   agents  basic cmove    times faster  cmove significantly better
pmove  though one order magnitude slower    agents     ms  basic 
   ms  incremental  versus   ms average   runtime grows much slowly
pmove     agents  cmove variants faster pmove
   agents  basic cmove almost one order magnitude faster     s versus       s
average   difference increases every agent 
before  runtime cmove exponential induced width  increases
number agents      n          n      average  result
random mo cog generation procedure  however  cmoves runtime polynomial
size ccs  size grows exponentially  shown figure    right  
fact cmove much faster pmove explained sizes pcs
ccs  former grows much faster latter     agents  average pcs
size     average ccs size        agents  average pcs size risen
        average ccs size        
figure    left  compares scalability algorithms number objectives 
random mo cogs n            averaged     mo cogs  cmove
always outperforms nongraphical method  interestingly  nongraphical method
   

firoijers  whiteson    oliehoek

figure    runtimes  ms  non graphical method  pmove cmove logscale
standard deviation mean  error bars   left  corresponding number vectors
pcs ccs  right   increasing numbers objectives 

several orders magnitude slower      grows slowly      starts
grow exponent pmove  explained fact
time takes enumerate joint actions payoffs remains approximately constant 
time takes prune increases exponentially number objectives 
     cmove order magnitude slower pmove     ms  basic 
     incremental  versus   ms   however       cmove variants already
faster pmove   dimensions respectively         times faster 
happens ccs grows much slowly pcs  shown figure
   right   difference incremental basic cmove decreases number
dimensions increases  factor                  trend indicates
pruning every cross sum  i e   prune   becomes  relatively  better higher
numbers objectives  although unable solve problem instances many
objectives within reasonable time  expect trend continue incremental
cmove would faster basic cmove problems many objectives 
overall  conclude that  random graphs  cmove key solving mo cogs
within reasonable time  especially problem size increases either number
agents  number objectives  both 
      mining day
mining day  mining company mines gold silver  objectives  set mines
 local payoff functions  located mountains  see figure     mine workers live
villages foot mountains  company one van village  agents 
transporting workers must determine every morning mine van
go  actions   however  vans travel nearby mines  graph connectivity   workers
efficient workers mine     efficiency bonus per
worker amount resource mined per worker x     w   x
base rate per worker w number workers mine  base rate
gold silver properties mine  since company aims maximize revenue 
best strategy depends fluctuating prices gold silver  maximize revenue 
   

ficomputing ccss faster multi objective coordination

figure    runtimes  ms  basic incremental cmove  pmove  log scale
standard deviation mean  error bars   left  corresponding number vectors
pcs ccs  right   increasing numbers agents 

mining company wants use latest possible price information  lose time
recomputing optimal strategy every price change  therefore  must calculate
ccs 
generate mining day instance v villages  agents   randomly assign    
workers village connect     mines  village connected mines
greater equal index  i e   village connected mines  connected
mines      last village connected   mines thus number mines
v      base rates per worker resource mine drawn uniformly
independently interval         
order compare runtimes basic incremental cmove pmove
realistic benchmark  generate mining day instances varying numbers
agents  note include non graphical method  runtime mainly
depends number agents  thus considerably faster problem
random graphs  runtime results shown figure    left   cmove
pmove able tackle problems     agents  however  runtime
pmove grows much quickly cmove  two objective setting 
basic cmove better incremental cmove  basic cmove pmove
runtimes around    s    agents      agents  basic cmove runs    s
pmove   s  even though incremental cmove worse basic cmove 
runtime still grows much slowly pmove  beats pmove
many agents 
difference pmove cmove results relationship
number agents sizes ccs  grows linearly  pcs  grows
polynomially  shown figure    right   induced width remains around   regardless
number agents  results demonstrate that  ccs grows slowly
pcs number agents  cmove solve mo cogs efficiently
pmove number agents increases 
   

firoijers  whiteson    oliehoek

   linear support mo cogs
section  present variable elimination linear support  vels   vels new
method computing ccs mo cogs several advantages cmove 
moderate numbers objectives  runtime complexity better  anytime
algorithm  i e   time  vels produces intermediate results become better
better approximations ccs therefore  provided maximum scalarized
error   vels compute  optimal ccs 
rather dealing multiple objectives inner loop  like cmove   vels
deals outer loop employs subroutine  vels thus builds
ccs incrementally  iteration outer loop  vels adds one new
vector partial ccs  find vector  vels selects single w  the one offers
maximal possible improvement   passes w inner loop  inner loop 
vels uses  section      solve single objective coordination graph  cog 
results scalarizing mo cog using w selected outer loop  joint
action optimal cog multi objective payoff added
partial ccs 
departure point creating vels chengs linear support  cheng         chengs
linear support originally designed pruning algorithm pomdps  unfortunately 
algorithm rarely used pomdps practice  runtime exponential
number states  however  number states pomdp corresponds number
objectives mo cog  realistic pomdps typically many states  many
mo cogs handful objectives  therefore  mo cogs  scalability
number agents important  making chengs linear support attractive starting
point developing efficient mo cog solution method 
building chengs linear support  section     create abstract algorithm
call optimistic linear support  ols   builds ccs incrementally 
ols takes arbitrary single objective problem solver input  seen generic
multi objective method  show ols chooses w iteration that 
finite number iterations  improvements partial ccs made
ols terminate  furthermore  bound maximum scalarized error
intermediate results  used bounded approximations ccs 
then  section      instantiate ols using single objective problem solver 
yielding vels  effective mo cog algorithm 
    optimistic linear support
ols constructs ccs incrementally  adding vectors initially empty partial ccs  
definition     partial ccs  s  subset ccs  turn subset v 
ccs v 
define scalarized value function s  corresponding convex upper surface
 shown bold  figure  b d 
definition     scalarized value function partial ccs  s  function takes
weight vector w input  returns maximal attainable scalarized value
   

ficomputing ccss faster multi objective coordination

 a 

 b 

 c 

 d 

figure     a  possible payoff vectors   objective mo cog   b  ols finds two payoff
vectors extrema  red vertical lines   new corner weight wc             
found  maximal possible improvement   ccs shown dotted line 
 c  ols finds new vector             adds two new corner weights q 
 d  ols calls solvecog corner weights  in two iterations   finds
new vectors  ensuring   ccs   ccs 
payoff vector s 
us  w    max w u a  
u a s

similarly  define set maximizing joint actions 
definition     optimal joint action set function respect function
gives joint actions maximize scalarized value 
 w    arg max w u a  
u a s

note  w  set w multiple joint actions provide
scalarized value 
using definitions  describe optimistic linear support  ols   ols adds
vectors partial ccs  s  finding new vectors so called corner weights  corner
weights weights us  w   definition     changes slope directions 
must thus weights  w   definition     consists multiple payoff vectors  every
corner weight prioritized maximal possible improvement finding new payoff
vector corner weight  maximal possible improvement    ols knows
partial ccs complete  example process given figure   
 corner  weights algorithm searched new payoff vectors indicated
red vertical lines 
ols shown algorithm    find optimal payoff corner weight  ols
assumes access function called solvecog computes best payoff vector
given w  now  leave implementation solvecog abstract  section     
discuss implement solvecog  ols takes input m  mo cog solved 
  maximal tolerable error result 
first describe ols initialized  section         then  define corner weights
formally describe ols identifies  section         finally  describe
   

firoijers  whiteson    oliehoek

algorithm    ols m  solvecog   
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

input 
coggccs 
agent eliminate 

  partial
w   set checked weights
q empty priority queue
foreach extremum weight simplex
q add we        add extrema infinite priority
end
q isempty   timeout
w q pop  
u solvecog m  w 
u  
wdel remove corner weights made obsolete u q  store
wdel  w  wdel   corner weights removed adding u
wu newcornerweights u  wdel   s 
 u 
foreach w wu
r  w  calculate improvement using maxvaluelp w  s  w 
r  w   
q add w  r  w  
end
end
end
w w  w 
end
return highest r  w  left q

ols prioritizes corner weights used bound error
stopping ols done finding full ccs  section        
      initialization
ols starts initializing partial ccs  s  contain payoff vectors
ccs discovered far  line   algorithm     well set visited weights w  line
    then  adds extrema weight simplex  i e   points
weight one objective  priority queue q  infinite priority  line    
extrema popped priority queue ols enters main loop  line
    w highest priority selected  line     solvecog called
w  line    find u  best payoff vector w 
example  figure  b shows two payoff vectors   dimensional mocog found applying solvecog extrema weight simplex   
                  vectors must part ccs optimal
least one w  one solvecog returned solution  the extrema
weight simplex   set weights w ols tested far marked vertical
red line segments 
   

ficomputing ccss faster multi objective coordination

      corner weights
evaluated extrema  consists  the number objectives  payoff vectors
associated joint actions  however  many weights simplex  yet
contain optimal payoff vector  therefore  identifying new vector u add
 line     ols must determine new weights add q  chengs linear support 
ols identifying corner weights  weights corners convex
upper surface  i e   points pwlc surface us  w  changes slope  define
corner weights precisely  must first define p   polyhedral subspace weight
simplex us  w   bertsimas   tsitsiklis         corner weights
vertices p  defined set linear inequalities 
definition     set known payoff vectors  define polyhedron
x
p    x  d       x     i  wi     
wi      


  matrix elements row vectors  augmented column vector
 s  setpof linear inequalities   x     supplemented simplex constraints 
wi     wi      vector x    w         wd   u  consists weight vector
scalarized value weights  corner weights weights contained
vertices p   form  w         wd   u  
note that  due simplex constraints  p d dimensional  furthermore 
extrema weight simplex special cases corner weights 
identifying u  ols identifies corner weights change polyhedron p
adding u s  fortunately  require recomputation corner weights 
done incrementally  first  corner weights q u yields better
value currently known deleted queue  line     function
newcornerweights u  wdel   s  line    calculates new corner weights involve u
solving system linear equations see u intersects boundaries
relevant subset present vectors s 
newcornerweights u  wdel   s   line     first calculates set relevant payoff
vectors  arel   taking union maximizing vectors weights wdel    
arel  

 

 w  

wwdel

 w  contains fewer payoff vectors  boundary weight simplex
involved  boundaries stored  possible subsets size    of vectors
boundaries  taken  subset weight   payoff vectors
 and or boundaries  intersect u computed solving system
linear equations  intersection weights subsets together form set candidate
corner weights  wcan   newcornerweights u  wdel   s  returns subset wcan
inside weight simplex u higher scalarized value payoff
   fact  implementation  optimize step caching  w  w q 

   

firoijers  whiteson    oliehoek

vector already s  figure  b shows one new corner weight labelled wc              
practice   arel   small  systems linear equations need solved   
calculating new corner weights wu line     u added line    
cheng showed finding best payoff vector corner weight adding
partial ccs  i e    solvecog w    guarantees best improvement s 
theorem     cheng       maximum value of 
max

min w u w v 

w uccs vs

i e   maximal improvement adding vector it  one corner weights
 cheng        
theorem   guarantees correctness ols  corner weights checked 
new payoff vectors  thus maximal improvement must   ols found
full ccs 
      prioritization
chengs linear support assumes corner weights checked inexpensively 
reasonable assumption pomdp setting  however  since solvecog expensive
operation  testing corner weights may feasible mo cogs  therefore  unlike
chengs linear support  ols pops one w q tested per iteration  making
ols efficient thus critically depends giving w suitable priority adding
q  end  ols prioritizes corner weight w according maximal possible
improvement  upper bound improvement us  w   upper bound computed
respect ccs  optimistic hypothetical ccs  i e   best case scenario
final ccs given current partial ccs w set weights already
tested solvecog  key advantage ols chengs linear support
priorities computed without calling solvecog  obviating need run solvecog
corner weights 
definition     optimistic hypothetical ccs  ccs set payoff vectors yields
highest possible scalarized value possible w consistent finding vectors
weights w 
figure  b denotes ccs                            dotted line  note ccs
superset value uccs  w  us  w  weights w 

given w  maxvaluelp finds scalarized value uccs
 w  solving 
max w v
subject w v us w  
    however  theory possible construct partial ccs  corner weight
payoff vectors adel  

   

ficomputing ccss faster multi objective coordination

us w vector containing us  w    w  w  note abuse notation
w  case matrix whose rows consist weight vectors set
w   
using ccs  define maximal possible improvement 
 w    uccs  w  us  w  
figure  b shows  wc   dashed line  use maximal relative possible improvement  r  w     w  uccs  w   priority new corner weight w wu  

figure  b  r  wc                         
       corner weight w identified  line     
   
added q priority r  w  long r  w     lines        
wc figure  b added q  popped  as element
q   solvecog wc   generates new vector         yielding                            
illustrated figure  c  new corner weights                              
points        intersects                testing weights  illustrated
figure  d  result new payoff vectors  causing ols terminate  maximal
improvement corner weights   thus  due theorem      ccs upon
termination  ols called solvecog   weights resulting exactly   payoff
vectors ccs    payoff vectors v  displayed grey dashed black
lines figure  a  never generated 
    variable elimination linear support
exact cog algorithm used implement solvecog  naive approach
explicitly compute values joint actions v select joint action maximizes
value 
solvecog m  w    arg max w u a  
u a v

implementation solvecog combination ols yields algorithm
refer non graphical linear support  ngls   ignores graphical structure 
flattening cog standard multi objective cooperative normal form game 
main downside computational complexity solvecog linear  v   which
equal  a    exponential number agents  making feasible
mo cogs agents 
contrast  use  section      implement solvecog  better 
call resulting algorithm variable elimination linear support  vels   dealt
multiple objectives outer loop ols  vels relies exploit graphical
structure inner loop  yielding much efficient method ngls 
    analysis
analyze computational complexity vels 
    implementation ols reduces size lp using subset weights w
joint actions involved w   w   found optimal  lead slight

overestimation uccs
 w  

   

firoijers  whiteson    oliehoek

theorem    runtime vels    
o   ccs     wccs    n amax  w   cnw   cheur    
w induced width running ve   ccs  size ccs   wccs  
number corner weights uccs  w   cnw time costs run newcornerweights 
cheur cost computation value optimistic ccs using maxvaluelp 
proof  since n amax  w runtime  theorem     runtime vels
quantity  plus overhead per corner weight cnw   cheur   multiplied number
calls ve  count calls  consider two cases  calls result adding
new vector result new vector instead confirm
optimality scalarized value weight  former size final ccs 
 ccs   latter number corner weights final ccs   wccs   
overhead ols itself  i e   computing new corner weights  cnw   calculating
maximal relative improvement  cheur   small compared solvecog calls 
practice  newcornerweights u  wdel   s  computes solutions small set
linear equations  of equations each   maxvaluelp w  s  w  computes solutions
linear programs  polynomial size inputs   
     number corner weights smaller  ccs  runtime
vels thus o n amax  w  ccs         number corner weights twice  ccs 
 minus constant  because  solvecog finds new payoff vector  one corner weight
removed three new corner weights added       loose bound  wccs  

total number possible combinations payoff vectors boundaries  o   ccs  d
  

however  obtain tighter bound observing counting number corner
weights given ccs equivalent vertex enumeration  dual problem
facet enumeration  i e   counting number vertices given corner weights  kaibel  
pfetsch        
theorem     arbitrary d   wccs   bounded o 
 avis   devroye        

 ccs b d  
c
 
 ccs d

 

 ccs b d  
c
 
 
 ccs d

proof  result follows directly mcmullens upper bound theorem facet enumeration  henk  richter gebert    ziegler        mcmullen        
reasoning used prove theorems   used establish following 
corollary    runtime vels  
o    ccs     wccs    n amax  w   cnw   cheur      ccs  size  ccs 
 wccs   number corner weights uccs  w  
practice  vels often test corner weights polyhedron spanned
 ccs  cannot guaranteed general  section      show empirically
  ccs  decreases rapidly increases 
    reduction footnote    used  small subset w used  making even smaller 

   

ficomputing ccss faster multi objective coordination

figure     left  runtimes pmove  cmove vels different values  
varying numbers agents  n       n factors    actions per agent 
  objectives  right  corresponding sizes  ccss 
theorem     space complexity vels o d  ccs  d wccs   n amax  w  
  
proof  ols needs store every corner weight  a vector length d  queue 
 wccs    ols needs store every vector  also vectors length d  
furthermore  solvecog called  memory usage added memory
usage outer loop ols  memory usage n amax  w  theorem    
ols adds memory requirements ve  vels almost memory
efficient thus considerably memory efficient cmove  theorem    
    empirical evaluation
empirically evaluate vels  comparison cmove pmove  longer
compare non graphical method clearly dominated cmove
pmove  refer cmove section  mean basic cmove 
fastest tested scenarios  before  use random graphs mining day
benchmark  experiments section run     ghx intel core i  computer 
  gb memory 
      random graphs
test vels randomly generated mo cogs  use mo cog generation
procedure section    determine scalability exact approximate
vels compares pmove cmove  tested random mo cogs
increasing numbers agents  average number factors per agent held
     n number objectives      figure   shows results 
averaged    mo cogs number agents  note runtimes left 
y axis  log scale set sizes right not 
results demonstrate vels efficient cmove two objective
random mo cogs  runtime exact vels        average    times less
   

firoijers  whiteson    oliehoek

cmove  cmove solves random mo cogs    agents   s average  whilst
exact vels handle     agents   s 
already large gain  achieve even lower growth rate permitting
small       agents  permitting       error margin yields gain
order magnitude  reducing runtime    s  permitting      error reduces
runtime    s  thus reduce runtime vels factor    
retaining     accuracy  compared cmove    agents  vels           
times faster 
speedups explained slower growth  ccs  figure    right   
small numbers agents  size  ccs grows slightly slowly
size full ccs  however  certain number agents onwards  size
 ccs grows marginally size full ccs keeps growing         
 ccs grew      payoff vectors      payoff vectors      agents 
marginally          agents  contrast  full ccs grew     
     vectors      agents  keeps growing           agents 
similar picture holds       ccs  grows rapidly      vectors  
      vectors    agents  grows slowly          agents  stabilizes 
reach       vectors     agents         agents  full ccs grows
      vectors       vectors  making almost   times large       ccs  
times larger      ccs  
test scalability vels respect number objectives  tested
random mo cogs constant number agents factors n           n 
increased number objectives             compare
scalability cmove  kept number agents  n       number local
payoff functions         small order test limits scalability number
objectives  number actions per agent    figure     left  plots number
objectives runtime  in log scale   ccs grows exponentially
number objectives  figure     right    runtime cmove exponential
number objectives  vels however linear number corner weights 
exponential size ccs  making vels doubly exponential  exact vels       
faster cmove              approximate vels      
   times faster  however     even approximate vels      
slower cmove 
unlike number agents grows  size  ccs  figure     right  
stabilize number objectives grows  seen following table 
 ccs 
d  
d  
d  

  
    
    
     

       
   
    
     

      
   
    
     

     
   
    
     

therefore conclude vels compute ccs faster cmove   objectives
less  cmove scales better number objectives  vels however  scales
better number agents 
   

ficomputing ccss faster multi objective coordination

figure      left  runtimes cmove vels               varying numbers objectives  right  size  ccs varying numbers objectives 

figure      left  plot runtimes cmove vels different values  
varying n  up        right  loglogplot runtime vels          
     agent mining day instances  varying values  
      mining day
compare cmove vels mining day benchmark using generation procedure section        generated    mining day instances increasing n
averaged runtimes  figure     left        agents  cmove reached runtime
  s  exact vels        compute complete ccs mo cog     agents
time  indicates vels greatly outperforms cmove structured
  objective mo cog  moreover  allow      error             takes
   s compute  ccs     agents  speedup order magnitude 
measure additional speedups obtainable increasing   test vels
large problems  generated mining day instances n                  
averaged    instances per value   instances  exact vels runs    s
n          s n          s n        average  expected  increasing
leads greater speedups  figure     right    however  close    i e  
   

firoijers  whiteson    oliehoek

 ccs close full ccs  speedup small  increased beyond certain
value  dependent n   decline becomes steady  shown line log log plot 
increases factor     runtime decreases factor     
thus  results show vels compute exact ccs unprecedented
numbers agents        well structured problems  addition  show small
values enable large speedups  increasing leads even bigger improvements
scalability 

   memory efficient methods
cmove vels designed minimize runtime required compute ccs 
however  cases  bottleneck may memory instead  memory efficient methods
cogs related problems recently received considerable attention  dechter  
mateescu        marinescu              mateescu   dechter         section 
show that  outer loop method  vels naturally memory efficient
therefore solve much larger mo cogs inner loop method cmove
memory restricted  addition  show cmove vels modified
produce even memory efficient variants 
    and or tree search
begin background and or tree search  dechter   mateescu       
marinescu        mateescu   dechter        yeoh  felner    koenig         class
algorithms solving single objective cogs tuned provide better space
complexity guarantees ve  however  improvement space complexity comes
price  i e   runtime complexity worse  mateescu   dechter         background
provide brief  broader overview and or tree search cogs related
models please see work dechter        marinescu         multi objective
versions work marinescu              
and or tree search algorithms work converting graph pseudo tree  pt 
agent need know actions ancestors descendants pt
take order select action  example  agent  a node  pt two
subtrees  t  t    it  agents t  conditionally independent
agents t  given ancestors i  figure   a shows pt coordination
graph figure  a 
next  and or tree search algorithms perform tree search results and or
search tree  aost   agent aost or node  children and nodes 
corresponding one agent actions  turn  children and nodes
or nodes corresponding agent children pt  action  and nodes 
agent agents or nodes  agents actions appear
tree multiple times  figure   b shows aost graph figure  a 
specific joint action constructed traversing tree  starting root
selecting one alternative childen or node  i e   one action agent 
continuing children and node  example  figure   b  joint
action   a    a    a    indicated grey  retrieve value joint action  must
first define value and nodes 
   

ficomputing ccss faster multi objective coordination

figure      a  pseudo tree   b  corresponding and or search tree 
definition     value and node vai   representing action ai agent
sum local payoff functions scope  ai   together and node
ancestors actions  specifies action agent scope local payoff functions 
example  figure   b  total payoff cog u a    a    a      u   a    a     
u   a    a     value grey and node a  u   a    a     u  payoff function
agent   scope and  together ancestral and nodes  grey a   node  a 
completes joint local action u   
retrieve optimal action  must define value subtree aost 
definition     value subtree v ti   rooted or node aost
maximum value subtrees rooted  and node  children i  value
subtree v tai   rooted and node ai aost value ai  definition
    plus sum value subtrees rooted  or node  children ai  
memory efficient way retrieve optimal joint action using aost
euler touring it  i e   performing depth first search computing values
subtrees  generating nodes fly deleting evaluated  memory
usage minimized  refer algorithm simply and or tree search  ts  
earlier sections  implementation employs tagging scheme  tagging value
subtree actions maximize it 
ts single objective method  extended compute pcs 
yielding algorithm call pareto ts  pts   marinescu         define pts  must
update definition    set pareto optimal payoffs  refer subtree value
set intermediate pcs  ipcs  
definition     intermediate pcs subtree  ip cs ti   rooted or node
pcs union intermediate pcss children  ch i   i 
ip cs ti     pprune 

 

aj ch i 

   

ip cs taj    

firoijers  whiteson    oliehoek

intermediate pcs subtree  ip cs tai   rooted and node ai pcs
value ai  definition     plus cross sum intermediate pcss subtrees
rooted  or node  children ai  



ip cs tj    vai    
ip cs tai     pprune 
jch ai  

thus  pts replaces max operator ts pruning operator  pmove replaces
max operator pruning operator 
    memory efficient ccs algorithms
propose two memory efficient algorithms computing ccs  straightforward variants cmove vels 
first algorithm  call convex ts  cts   simply replaces pprune cprune
definition     thus  cts pts different pruning operator 
seen cmove replaced ts  advantage cts pts
analogous cmove pmove  highly beneficial compute local
ccss instead local pcss intermediate coverage sets input next
subproblem sequential search scheme  regardless whether scheme ts 
cts memory efficient cmove  still requires computing intermediate
coverage sets take space  typically large ccs 
size bounded total number joint actions 
second algorithm addresses problem employing ols ts singleobjective solver subroutine  solvecog  yielding tree search linear support  tsls   thus 
tsls vels replaced ts  tsls outer loop method 
runs ts sequence  requiring memory used ts overhead
outer loop  consists partial ccs  definition     priority queue 
consequently  tsls even memory efficient cts 
    analysis
ts much better space complexity ve  i e   linear number agents n 
theorem     time complexity ts o n amax  m    n number agents 
 amax   maximal number actions single agent depth pseudo
tree  uses linear space  o n  
proof  number nodes aost bounded o n amax  m    tree creates
maximally  amax   children or node  every and node exactly one child 
number nodes would bounded o  amax  m    pt deep  however 
branching pt  and node multiple children  branch increases
size aost o  amax  m   nodes  exactly n agents
pt  happen n times  node aost  ts performs either
summation scalars  maximization scalars  ts performs depth first
search  o n  nodes need exist point execution 
   

ficomputing ccss faster multi objective coordination

tss memory usage usually lower required store original  singleobjective  problem memory  o  amax  emax    number local payoff
functions problem   amax   maximal size action space single agent 
emax maximal size scope single local payoff function 
pt depth different constant induced width w  typically
larger  however  bounded w 
theorem     given mo cog induced width w  exists pseudo tree
depth w log n  dechter   mateescu        
thus  combining theorems       shows that  agents  ts
much memory efficient relatively small runtime penalty 
using time space complexity results ts  establish following
corollaries time space complexity cts tsls 
corollary    time complexity cts o n amax  m r   r runtime
cprune 
proof  o n amax  m   bounds number nodes aost  node aost
cprune called 
runtime cprune terms size input given equation    note
size input cprune depends size intermediate ccss
children node  case and node  input size o  iccsmax  c   
c maximum number children and node    or nodes
o  amax   iccsmax    
corollary    space complexity cts o n iccsmax      iccsmax  
maximum size intermediate ccs execution cts 
proof  ts  o n  nodes aost need exist point
execution  node contains intermediate ccs 
cts thus much memory efficient cmove  space complexity
exponential induced width  theorem    
corollary    time complexity tsls o    ccs   w  ccs     n  amax  m  cnw  
cheur     w log n   
proof  proof theorem   time complexity
replaced ts 
terms memory usage  outer loop approach  ols  large advantage
inner loop approach  overhead outer loop consists partial
ccs  definition     priority queue  vels  theorem     thus much better
space complexity cmove  theorem     tsls advantage cts
vels cmove  therefore  tsls low memory usage  since requires
memory used ts plus overhead outer loop 
    note c turn upper bounded n loose bound 

   

firoijers  whiteson    oliehoek

corollary    space complexity tsls o d  ccs    d w  ccs     n   
w log n   
proof  proof theorem    space complexity
replaced ts 
mentioned section      ts memory efficient member class
and or tree search algorithms  members class offer different trade offs
time space complexity  possible create inner loop algorithms
corresponding outer loop algorithms basis algorithms  time
space complexity analyses algorithms performed similar manner
corollaries     advantages outer loop methods compared corresponding
inner loop methods however remain tsls cts  therefore 
article focus comparing memory efficient inner loop method
memory efficient outer loop method 
    empirical evaluation
section  compare cts tsls cmove vels  before  use
random graphs mining day benchmark  obtain pts cts tsls 
use heuristic cmove vels generate elimination order
transform pt w log n holds  whose existence guaranteed
theorem      using procedure suggested bayardo miranker        
      random graphs
first  test algorithms random graphs  employing generation procedure
section        connections agents graphs generated
randomly  induced width varies different problems  average  induced
width increases number local payoff functions  even ratio
local payoff factors number agents remains constant 
order test sizes problems different mo cog solution methods
handle within limited memory  generate random graphs two objectives  varying
number agents n       n local payoff functions  previous sections 
limited maximal available memory  kb imposed timeout     s 
figure   a shows vels scale agents within given memory constraints non memory efficient methods  particular  pmove cmove
handle       agents  respectively  because  given induced width w 
must store o  amax  w   local css     agents  induced width  figure   c 
      agents induced width    vels handle    agents 
induced width     memory demands come running
inner loop  outer loop adds little overhead  need store one payoff
new local payoff function results agent elimination  whereas pmove
cmove must store local coverage sets  thus  using outer loop approach  vels 
instead inner loop approach  cmove  already yields significant improvement
problem sizes tackled limited memory 
   

ficomputing ccss faster multi objective coordination

 a 

 b 

 c 

figure      a  runtimes ms tsls  vels  cts  cmove pmove random  objective mo cogs varying numbers agents n      n local payoff
factors   b  runtimes approximate tsls varying amounts allowed error
  compared  exact  vels  problem parameters  a    c 
corresponding induced widths mo cogs  b  

however  scaling beyond    agents requires memory efficient approach  figure   a
shows that  cts tsls require runtime  handle agents
within memory constraints  fact  unable generate mo cog enough
agents cause methods run memory  tsls faster cts  case
    times faster  reasons vels faster cmove 
however  speed advantage outer loop approach  allow
bit error scalarized value    trade accuracy runtime  figure   b     
agents  exact tsls         average runtime    s     times slower
vels  however            runtime   s     times slower         
  s      times slower          s      times slower   furthermore 
relative increase runtime number agents increases less higher   thus 
approximate version tsls highly attractive method cases memory
runtime limited 
      mining field
compare performance cmove vels tsls variation mining
day call mining field  longer consider cls consistently higher
runtime tsls worse space complexity  use mining field order ensure
interesting problem memory restricted setting  mining day  see section    
induced width depends parameter specifying connectivity villages
increase number agents factors  therefore  whether
vels memory efficient enough handle particular instance depends primarily
parameter number agents 
mining field  villages situated along mountain ridge placed
grid  number agents thus n   s    use random placement mines 
ensuring graph connected  induced width connected grid
generate grid like graphs  larger instances higher induced width 
   

firoijers  whiteson    oliehoek

village

 a 

mine

 b 

 c 

figure      a  example     mining field instance  additional mines
marked     b  runtimes ms tsls  for varying amounts allowed
error    vels         cmove   objective mining field instances
varying numbers additional mines         grid size       c 
corresponding induced widths mining field instances 

induced width thus longer depends connectivity parameter increases
number agents factors graph 
example mining field instance provided figure   a  choose distance
adjacent villages grid unit length  map  place
mines  local payoff functions   connect agents using arbitrary tree using   agent
local payoff functions  mines   figures  mines span tree unmarked
connected mines black edges  require s    factors build tree 
add additional mines   independently  placing random point
map inside grid  mine placed  connect villages within
r        radius mine map  chose        therefore  maximum
connectivity factor  mine  created fashion    figure  mines
marked    rewards per mine per worker  well number workers per
village  generated way mining day 
compare runtimes memory requirements cmove  vels  tsls
mining field  tested     instance     agents    mb available memory 
tsls  use three different values      exact             use time limit
            minutes   increase number additional mines       factors
total  onwards  steps   
using setup  possible solve problem instances using pmove 
ran memory problems  fact  pmove succeeded tree shaped
problem  i e   one without additional factors  figures   b   c  show results
remaining methods  cmove runs memory   additional factors     factors
total   contrast  vels runs memory    additional factors  induced
width   
compared random graph results section        induced widths
problems cmove vels handle lower mining field  suspect
   

ficomputing ccss faster multi objective coordination

because  grid shaped problem  number factors highest induced
width need exist parallel execution algorithms higher 
tsls run memory tested instances  face 
unable generate instances tsls run memory  however 
run time       tsls first exceeds time limit      additional mines 
        happens              tsls ran time      
differences runtime tsls vels larger random graphs
therefore difficult compensate slower runtime tsls choosing
higher   much slower tsls compared vels thus seems depend
structure mo cog 
mining field results confirm conclusion random graph experiments
using outer loop approach  vels  instead inner loop approach  cmove  yields
significant improvement problem sizes tackled limited memory 
futhermore  tsls used solve problem sizes beyond vels handle
within limited memory  approximate version tsls appealing choice cases
memory runtime limited 

   conclusions future work
article  proposed new algorithms exploit loose couplings compute ccs
multi objective coordination graphs  showed exploiting loose couplings
key solving mo cogs many agents  particular  showed  theoretically
empirically  computing ccs considerable advantages computing pcs
terms runtime memory usage  experiments consistently shown
runtime pcs methods grows lot faster ccs methods 
cmove deals multiple objectives inner loop  i e   computes local ccss
looping agents  contrast  vels deals multiple objectives
outer loop  i e   identifies weights maximal improvement upon partial ccs
made solves scalarized  single objective  problems using weights  yielding
anytime approach  addition  cts tsls memory efficient variants
methods  proved correctness algorithms analyzed complexity 
cmove vels complementary methods  cmove scales better number
objectives  vels scales better number agents compute ccs  leading large additional speedups  furthermore  vels memory efficient
cmove  fact  vels uses little memory single objective ve 
however  memory restricted vels cannot applied  tsls provides
memory efficient alternative  tsls considerably slower vels 
loss compensated allowing error    
numerous possibilities future work  mentioned section    ols
generic method applied multi objective problems  fact   together
authors  already applied ols large multi objective mdps showed
ols extended permit non exact single objective solvers  roijers et al         
future work  intend investigate  approximate methods mo cogs  using approximate single objective solvers cogs  using  e g   lp relaxation methods  sontag 
globerson    jaakkola         attempt find optimal balance
   

firoijers  whiteson    oliehoek

levels approximation inner outer loop  respect runtime guarantees
empirical runtimes 
many methods exist single objective coordination graphs single parameter
controls trade off memory usage runtime  furcy   koenig        rollon 
       algorithms  corresponding multi objective inner loop version
computes pcs  marinescu              devised  would interesting
create inner outer loop methods based methods compute ccs
instead compare performance  particular  shown ols requires
little extra memory usage compared single objective solvers  would interesting
investigate much extra memory could used single objective solver inside ols 
comparison corresponding inner loop method 
addition work mo cogs  aim extend work sequential
settings  particular  look developing efficient planning method multiagent multi objective mdps better exploiting loosely couplings  first  try
develop  approximate planning version sparse cooperative q learning  kok   vlassis 
    b   however  may possible general effects agent
agents via state impossible bound general  therefore  hope identify
broadly applicable subclass multi agent momdps  approximate planning
method yields substantial speed up compared exact planning methods 

acknowledgements
thank rina dechter introducing us memory efficient methods cogs
mo cogs  radu marinescu tips memory efficient methods implementation  also  would thank maarten inja  well anonymous reviewers  valuable feedback  research supported nwo dtc ncap
               nwo catch                projects nwo innovational research incentives scheme veni                 frans oliehoek affiliated
university amsterdam university liverpool 

references
avis  d     devroye  l          estimating number vertices polyhedron  information processing letters                 
bayardo  r  j  j     miranker  d  p          space time trade off solving constraint
satisfaction problems  ijcai       proceedings fourteenth international
joint conference artificial intelligence 
bertsimas  d     tsitsiklis  j          introduction linear optimization  athena scientific 
bishop  c  m          pattern recognition machine learning  springer 
cassandra  a   littman  m     zhang  n          incremental pruning  simple  fast  exact
method partially observable markov decision processes  uai       proceedings
thirteenth conference uncertainty artificial intelligence  pp       
   

ficomputing ccss faster multi objective coordination

cheng  h  t          algorithms partially observable markov decision processes  ph d 
thesis  university british columbia  vancouver 
dechter  r          reasoning probabilistic deterministic graphical models  exact algorithms  vol    synthesis lectures artificial intelligence machine
learning  morgan   claypool publishers 
dechter  r     mateescu  r          and or search spaces graphical models  artificial
intelligence                 
delle fave  f   stranders  r   rogers  a     jennings  n          bounded decentralised
coordination multiple objectives  proceedings tenth international joint
conference autonomous agents multiagent systems  pp         
dubus  j   gonzales  c     perny  p          choquet optimization using gai networks
multiagent multicriteria decision making  adt       proceedings first
international conference algorithmic decision theory  pp         
feng  z     zilberstein  s          region based incremental pruning pomdps  uai
      proceedings twentieth conference uncertainty artificial intelligence  pp         
furcy  d     koenig  s          limited discrepancy beam search  ijcai       proceedings nineteenth international joint conference artificial intelligence  pp 
       
guestrin  c   koller  d     parr  r          multiagent planning factored mdps 
advances neural information processing systems     nips    
henk  m   richter gebert  j     ziegler  g  m          basic properties convex polytopes 
handbook discrete computational geometry  ch     pp          crc
press  boca 
kaibel  v     pfetsch  m  e          algorithmic problems polytope theory 
algebra  geometry software systems  pp        springer 
kok  j  r     vlassis  n          sparse cooperative q learning  proceedings
twenty first international conference machine learning  icml     new york  ny 
usa  acm 
kok  j  r     vlassis  n       a   using max plus algorithm multiagent decision
making coordination graphs  robocup       robot soccer world cup ix  pp 
    
kok  j     vlassis  n       b   collaborative multiagent reinforcement learning payoff
propagation  journal machine learning research              
koller  d     friedman  n          probabilistic graphical models  principles techniques  mit press 
lizotte  d   bowling  m     murphy  s          efficient reinforcement learning multiple
reward functions randomized clinical trial analysis  proceedings   th
international conference machine learning  icml      pp         
   

firoijers  whiteson    oliehoek

marinescu  r   razak  a     wilson  n          multi objective influence diagrams 
uai       proceedings twenty eighth conference uncertainty artificial
intelligence 
marinescu  r          and or search strategies combinatorial optimization graphical models  ph d  thesis  university california  irvine 
marinescu  r          exploiting problem decomposition multi objective constraint optimization  principles practice constraint programming cp       pp     
     springer 
marinescu  r          efficient approximation algorithms multi objective constraint
optimization  adt       proceedings second international conference
algorithmic decision theory  pp          springer 
mateescu  r     dechter  r          relationship and or search variable
elimination  uai       proceedings twenty first conference uncertainty
artificial intelligence  pp         
mcmullen  p          maximum numbers faces convex polytope  mathematika 
               
oliehoek  f  a   spaan  m  t  j   dibangoye  j  s     amato  c          heuristic search
identical payoff bayesian games  aamas       proceedings ninth international joint conference autonomous agents multiagent systems  pp 
         
pearl  j          probabilistic reasoning intelligent systems  networks plausible inference  morgan kaufmann 
pham  t  t   brys  t   taylor  m  e   brys  t   drugan  m  m   bosman  p  a   cock 
m  d   lazar  c   demarchi  l   steenhoff  d   et al          learning coordinated
traffic light control  proceedings adaptive learning agents workshop  at
aamas      vol      pp           
roijers  d  m   scharpff  j   spaan  m  t  j   oliehoek  f  a   de weerdt  m     whiteson 
s          bounded approximations linear multi objective planning uncertainty  icaps       proceedings twenty fourth international conference
automated planning scheduling  pp         
roijers  d  m   vamplew  p   whiteson  s     dazeley  r       a   survey multiobjective sequential decision making  journal artificial intelligence research     
      
roijers  d  m   whiteson  s     oliehoek  f       b   computing convex coverage sets
multi objective coordination graphs  adt       proceedings third international conference algorithmic decision theory  pp         
roijers  d  m   whiteson  s     oliehoek  f  a          linear support multi objective
coordination graphs  aamas       proceedings thirteenth international
joint conference autonomous agents multi agent systems  pp           
rollon  e          multi objective optimization graphical models  ph d  thesis  universitat politecnica de catalunya  barcelona 
   

ficomputing ccss faster multi objective coordination

rollon  e     larrosa  j          bucket elimination multiobjective optimization problems  journal heuristics             
rosenthal  a          nonserial dynamic programming optimal  proceedings
ninth annual acm symposium theory computing  pp         acm 
scharpff  j   spaan  m  t  j   volker  l     de weerdt  m          planning uncertainty coordinating infrastructural maintenance  proceedings  th annual
workshop multiagent sequencial decision making certainty 
sontag  d   globerson  a     jaakkola  t          introduction dual decomposition
inference  optimization machine learning            
tesauro  g   das  r   chan  h   kephart  j  o   lefurgy  c   levine  d  w     rawson  f 
        managing power consumption performance computing systems using
reinforcement learning  advances neural information processing systems   
 nips    
vamplew  p   dazeley  r   barker  e     kelarev  a          constructing stochastic mixture policies episodic multiobjective reinforcement learning tasks  advances
artificial intelligence  pp         
yeoh  w   felner  a     koenig  s          bnb adopt  asynchronous branch andbound dcop algorithm  journal artificial intelligence research            

   


