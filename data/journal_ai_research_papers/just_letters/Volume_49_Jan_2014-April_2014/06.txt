journal of artificial intelligence research                  

submitted        published      

large scale optimization for evaluation functions with
minimax search
kunihito hoki

hoki cs uec ac jp

department of communication engineering and informatics
the university of electro communications

tomoyuki kaneko

kaneko acm org

department of graphics and computer sciences
the university of tokyo

abstract
this paper presents a new method  minimax tree optimization  mmto   to learn
a heuristic evaluation function of a practical alpha beta search program  the evaluation
function may be a linear or non linear combination of weighted features  and the weights
are the parameters to be optimized  to control the search results so that the move decisions agree with the game records of human experts  a well modeled objective function
to be minimized is designed  moreover  a numerical iterative method is used to find local
minima of the objective function  and more than forty million parameters are adjusted by
using a small number of hyper parameters  this method was applied to shogi  a major
variant of chess in which the evaluation function must handle a larger state space than
in chess  experimental results show that the large scale optimization of the evaluation
function improves the playing strength of shogi programs  and the new method performs
significantly better than other methods  implementation of the new method in our shogi
program bonanza made substantial contributions to the programs first place finish in the
     world computer shogi championship  additionally  we present preliminary evidence
of broader applicability of our method to other two player games such as chess 

   introduction
heuristic search is a powerful method in artificial intelligence  in       the chess playing
computer deep blue defeated the world chess champion garry kasparov  campbell  hoane 
  hsu         the computer decided its moves after making a large number of searches
of the minimax game tree and using heuristic evaluation functions  in this framework of
artificial intelligence  the heuristic evaluation functions  as well as the search methods  are
crucial for making strong computer players  thus  researchers working on various games
have made substantial efforts in a quest to create effective evaluation functions by using machine learning techniques  furnkranz         however  fully automated learning of
the heuristic evaluation functions remains a challenging goal in chess variants  for example  developers have reported that the majority of the features and weights in deep blue
were created tuned by hand  campbell et al          it is said that recent top level chess
programs tune some of their parameters automatically  although we have yet to find any
publication describing the methods they use  moreover  reinforcement learning has been
applied to chess  baxter  tridgell    weaver        veness  silver  uther    blair        
c
    
ai access foundation  all rights reserved 

fihoki   kaneko

however  to the best of the authors knowledge  the evaluation functions learned by the
methods reported in the literature are still weaker than the best hand crafted functions in
terms of chess playing strength 
in this paper  we revisit the idea behind earlier research on learning chess evaluation
functions  marsland        hsu  anantharaman  campbell    nowatzyk        tesauro 
      and reformulate the task as an optimization problem using an alternative learning
method  called minimax tree optimization  mmto   the objective here is to optimize
a full set of parameters in the evaluation function so that the search results match the
desired move decisions  e g   the recorded moves of grandmaster games  the evaluation
functions are learned through iteration of two procedures      a shallow heuristic search for
all training positions using the current parameters and     a parameter update guided by an
approximation of the gradient of the objective function  to achieve scalability and stability 
we introduce a new combination of optimization techniques  a simplified loss function  gridadjacent update  equality constraint  and l   regularization  one of the resulting merits is
that mmto can ensure the existence of a local minimum within a convenient range of
parameters 
this study demonstrates the performance of mmto in shogi  a variant of chess where
evaluation functions need to handle a wider variety of features and positions than in western
chess  implementation of mmto in our shogi program bonanza  described in section     
made substantial contribution to the programs first place finish in the      world computer shogi championship  the rules of shogi  as well as a survey of approaches in artificial
intelligence  are described in the literature  iida  sakuta    rollason         basic techniques  such as a minimax search guided by heuristic evaluation functions  are as effective
in shogi as in chess  however  the drop rule that allows a player to reuse captured pieces
significantly changes a few properties      the number of legal moves  as well as average
game length  is greater than in chess      endgame databases are not available      and
the material balance is less important than in chess  especially in the endgame  thus  the
performance of a shogi program is more dependent on the quality of its evaluation function 
through experiments  we first show that the full set of parameters in the evaluation
functions can be optimized with respect to the rate of agreement with the training set 
after that  we examine the performance of various learned evaluation functions in terms of
their rates of agreement with the test positions and win rates against references  scalability
is demonstrated up to about forty million parameters  which is far too many to tune by
hand  the features we used are piece values and extended versions of piece square tables
that are commonly used to learn evaluation functions in chess  tesauro        baxter et al  
      veness et al          we also briefly examine the performance of mmto in chess to
catch a glimpse of the applicability of mmto to other games 
the rest of this paper is organized as follows  the next section reviews related research 
the third section presents the mmto method  the fourth section shows our experimental
results  where forty million of parameters are adjusted for better performance  and compares
the performance of our method with that of existing methods  the last section presents
our concluding remarks  this paper incorporates and extends our previous work  hoki  
kaneko        kaneko   hoki        
   

filarge scale optimization for evaluation functions with minimax search

   related work
this section reviews related research on learning evaluation functions  first  we describe
supervised learning methods that use the desired moves  second  we discuss other learning
methods  including regression and reinforcement learning  third  we briefly discuss the difficulty of supervised learning in terms of numerical optimization  although machine learning
of other components besides evaluation functions in game programs would be an interesting
research topic  bjornsson   marsland        tsuruoka  yokoyama    chikayama       
coulom        silver   tesauro         this review only focuses on research that has been
done on learning evaluation functions 
    learning from desired moves in chess
grandmaster games are a popular source of information for learning chess  let us say
that we have a set of positions p and the desired moves for each position in p  typically 
such positions and moves are sampled from grandmaster games  a chess program has an
evaluation function e p  w    where p is a game position and the feature weight vector w
contains the parameters to be adjusted 
let us assume that the evaluation function e p  w   is partially differentiable with respect
to wi for any i  here  wi is the i th component of w   for
p example  the function could be
a linear combination of weighted features  i e   e p  w     i wi fi  p   where fi  p  is the i th
feature value of position p  the aim of learning is to find a better weight vector w for
strengthening the play of the program  the hypothesis behind this kind of learning is that
the more the computer play agrees with the desired moves  the better it plays 
let us begin with a simple intuitive goal  make the results of a one ply search agree
with the desired moves  for simplicity  let us assume that the maximizing player moves first
at the root position p  in a one ply search  the move with the highest evaluation value is
selected  thus  w should be adjusted so that the desired move has the highest evaluation of
all the moves  this goal can formally be written as a mathematical minimization problem
with an objective function 
p
w   
jh
 w

x x

h  e p m  w    e p dp   w     

   

pp mm p

here  p m is the position after move m in position p  dp is the desired move in position p  m p
is the set of all legal moves in p excluding dp   and h x  is the heaviside step function  i e  
h x  equals   if x     and   otherwise  because this objective function counts the number
of moves that have an evaluation value greater than or equal to that of the desired move 
a better w can be found by minimizing eq       although several studies have attempted
machine learning on the basis of this framework  nitsche        van der meulen       
anantharaman         their numerical procedures were complicated  and the adjustment of
a large scale vector w seemed to present practical difficulties 
marsland        presented a notable extension wherein a continuous function is used
so that conventional optimization techniques can be exploited  here  a continuous function
of difference is substituted for the non continuous step function in eq       an interesting
   

fihoki   kaneko

modified function is
w   
j p  w

x x

 max     e p m  w    e p dp   w       

   

pp mm p

the meaning of the function value is different from that in eq       i e   the function does
not count the number of moves that have an evaluation value greater than or equal to that
w   helps to reduce the function
of the desired move  however  the gradient vector w j p  w
value numerically  marsland also introduced inequality constraints in order to keep the
evaluation in the right range  however  the literature does not provide any experimental
results on practical chess programs 
a second notable extension was proposed early in the development of chess machines
deep thought  nowatzyk        hsu et al          here  the positions being compared are
not p m  but rather wp m   that is  one of the leaves of the principal variations  pvs   possibly
several plies from p m  this extension carries out a least square fitting of the evaluation
w   does  instead  it biases
values  therefore  it does not have the max function that j p  w
p dp
the value of e w   w   before it is used in each least square fitting  if the evaluation value
p  d
p  m
of the desired move dp   e w p   w   is lower than that of another move m  e w
  w   
a third notable extension is the comparison training proposed by tesauro        
tesauro modified the objective function to
x x
p  d
p
p  m
w   
jct
 w
tct  e w p   w    e w
  w    
pp mm p

tct  x      r x         

   

where  is the standard sigmoid function  and r is a heuristic rescaling factor for positive
differences  i e   r x    x when x     and r x    cx for a constant c     otherwise  note
that r x  is still a continuous function  the important property of this modified objective
function is that the value and the derivative are zero in the limit as the difference x goes
to positive infinity  and they are respectively one and zero in the limit as the difference
x goes to negative infinity  therefore  tct  x  in eq      is a continuous approximation of
h x  in eq       note that this property is not explicitly stated by tesauro  but it is
notably distinct from the other work  the number of feature weights adjusted with his
method was less than two hundred  tesauro also mentioned an application for small bit
integers  which was used to adjust some of the weights in deep blue  however  he neither
clarified its procedure nor mentioned whether the weights were automatically adjusted in
that experiment 
table   summarizes the related work  each of the existing methods possesses at least
one of three important properties for optimization  i e   continuity  minimax searches  and
assured local minimum  however  none of them have all three properties  also  some
of the existing methods  nowatzyk        hsu et al         tesauro        do not try to
decrease the functions through iteration as much as possible  we will revisit these issues in
section      on the other hand  our method  mmto  has scalability in high dimensional
learning  moreover  we empirically show that a decrease in the objective function value
leads to an increase in playing strength  the existing methods have not been shown to have
this property 
   

filarge scale optimization for evaluation functions with minimax search

method
 nitsche       
 marsland       
 van der meulen       
 hsu et al        
 anantharaman       
comparison training
mmto

continuity

search

assured local minimum

no

no
no
no

no
no
no
no

yes
no

yes

yes
yes
yes
yes

no

yes
yes

yes
no

yes

table    summary of learning methods using the desired moves in training positions to
adjust the feature weights in the evaluation functions  the first column is the name
of the method or piece of literature  the second column describes the continuity
of the objective functions with respect to the feature weights  yes means that
continuity depends on the kind of search method used  the third column indicates
whether the objective functions use minimax searches with depths more than   
instead of comparisons of legal moves at the root position  the fourth column
shows whether the hyper parameters of the objective functions can assure a local
minimum can be found 

    other methods of learning evaluation functions
many researchers have utilized information sources other than the desired moves  for
example  some studies on othello dating from the     s compare the desired moves with
other moves  fawcett         however  the most practical and famous machine learning
method that has yielded strong programs is based on regression of the desired value by
using     million features  buro               in othello  different evaluation functions are
used for game stages determined on the basis of the number of discs in play  thus  the
desired values of the training positions are obtained through a complete endgame search
as well as a heuristic search with evaluation functions learned in later game stages  this
method has also been successfully applied to card games  buro  long  furtak    sturtevant 
       but not to chess variants  to the best of the authors knowledge  learning based on
regression with win loss labeled data has not yielded decent evaluation functions in chess
variants  except for not using the desired moves  buros method has properties that are
similar to those listed in table    his objective function has continuity as well as an assured
local minimum  and his method is scalable  gomboc  buro  and marsland        proposed
to learn from game records annotated by human experts  however  the feature weights that
were adjusted in their experiments were only a small part of the full evaluation functions 
reinforcement learning  sutton   barto         especially temporal difference learning 
of which a famous success is backgammon  tesauro         is considered to be promising
way to avoid the difficulty in finding the desired values for regression  this approach has
been applied to chess and has been shown to improve the strength of programs  baxter
et al         levinson   weber        veness et al          the knightcap program
achieved a rating of about        points at the free internet chess server  fics    and
   free internet chess server  http   www freechess org  last access      

   

fihoki   kaneko

easy
 a 

single minimum

dicult
 b 

 c 

 d 

smooth non dieren able

 e 

narrow trough non con nuous

figure    example illustrating the difficulties facing any minimization procedure 
       points at its highest peak at the internet chess club  icc   baxter et al         
another program achieved        points at its highest peak at icc  veness et al         
however  strong human players have ratings of more than        points at icc  and this
difference means these programs have not reached the top level of chess programs  that
is  evaluation functions tuned by reinforcement learning have not yet reached the level
of the best handcrafted evaluation functions in chess  moreover  the number of feature
weights to be adjusted is on the order of thousands  in checkers  evaluation functions
trained by temporal difference learning are reportedly comparable to the best handcrafted
efforts  schaeffer  hlynka    jussila         it has also been reported that a player stronger
than expert human checker players was created by using neural networks trained with
an evolutionary strategy  chellapilla   fogel         here  no features beyond the piece
differentials were given to the neural network a priori 
many machine learning techniques  baxter et al         veness et al         have been
applied to shogi  however  despite efforts by many programmers and researchers  the adjustment of the full weight vector in the evaluation function remains a challenging goal 
the studies published so far have adjusted only piece values or a small part of the feature
weights in the evaluation functions  beal   smith        ugajin   kotani        
    learning and numerical optimization
some learning methods reviewed in section     have objective functions to decrease  the
learning process can be extended into a numerical optimization using these functions  the
performance of numerical optimization is sensitive to the surface of the objective function 
figure   shows the properties of particular sorts of functions and their difficulties regarding
numerical minimization  the easiest one among them is the convex function  a   if a local
minimum exists  then it is a global minimum  function  b  has multiple local minima  however  it can still be thought of as an easy problem  because various minimization algorithms
using gradients and hessian matrices are effective on it  it would be desirable to design a
learning method using  say  linear or logistic regression  which uses one of these two types
of objective function  buro        
in contrast  non differentiable functions such as  c  and  e  are often more difficult to
minimize than differentiable ones  this is because a quadratic model  such as the hessian
approximation of the conjugated gradient method  bertsekas   bertsekas         is not
always appropriate for these functions  function  d  is also a difficult target  because an
important local minimum is hidden inside a deep narrow trough  and it is quite difficult to
find it by using numerical iteration methods  the most difficult example is minimization of
   

filarge scale optimization for evaluation functions with minimax search

the non continuous function  e   even primitive iterative methods such as gradient decent
are not capable of finding its minimum  the extreme case would be a function for which
an analytical formula for the gradient is unavailable  in that case  a learning method would
not be able to use partial derivatives  and the minima would have to be obtained using
derivative free methods  e g   sampling methods  bjornsson   marsland        coulom 
      
theorems in appendix a show that the minimax value is continuous but not always
partially differentiable  thus  the existing methods that incorporate a minimax search
 hsu et al         tesauro        and mmto listed in table   are type  c   moreover 
certain forward pruning techniques may cause discontinuities  therefore  even these learning
methods can be type  e   to overcome this difficulty  mmto has a well modeled objective
function and updates the feature weights in a careful manner 

   minimax tree optimization
minimax tree optimization  mmto  is an extension of comparison training to reach the
first intuitive goal embodied in eq       the purpose of this extension is to overcome the
practical difficulties and stabilize the mathematical optimization procedure with a largescale feature weight vector w   given a set of training positions p and the desired move dp
for each position p  mmto optimizes the weight vector w so that the minimax search with
w better agrees with the desired moves 
the weight vector w is improved through iteration of sub procedures  see figure    
for each iteration t  the first step consists of tree searches to identify one of the leaves of
pvs w  t  for all legal moves in the training positions p  because pv leaf w  t  depends
on the feature weights w  t  in an evaluation function  a new pv will be obtained when
w  t  is updated  we discuss this issue in section       the second step is the calculation
of the approximate partial derivatives  which depends on both pv and the weight vector 
the last step is the update of the weight vector  for numerical stability  the difference
w  t       w  t   must be kept small so that it will not be distorted by drastic changes in
 w
the partial derivatives  section     shows that a grid adjacent update ensures this 
    objective function to be minimized
the objective function is
p
w     j p  w     jc  w
w     jr  w
w   
jmmto
 w

   

where the first term j p  w   on the right side is the main part  the other terms jc and
jr are constraint and regularization terms  respectively  which are defined in section     
the first term is
x x
j p  w    
t  s p dp   w    s p m  w     
   
pp mm p

where s p  w   is the minimax value identified by the tree search for position p  t  x  is
       exp ax    which is a horizontally mirrored sigmoid function  the slope of t  x  is
controlled by a constant parameter a      in the large a limit  t  x  becomes the heaviside
   

fihoki   kaneko



fi
m
wp  t 

   perform a game tree search to identify pv leaves
for all child positions
p m of position p in training set p  where w  t  is the weight vector at the
t th iteration and w     is the initial guess 
   calculate a partial derivative approximation of the well modeled objective
p  m
w
function defined in section     by using both w
 t  and  t   the objective
function employs a differentiable approximation of h x   see section      
as well as a constraint and regularization term  see section      



   obtain a new weight vector w  t    from w  t  by using a grid adjacent update
guided by the partial derivatives computed in step    see section       go
back to step    or terminate the optimization when the objective function
value converges  see section    



figure    minimax tree optimization  iteration of searches and update using partial
derivatives

step function h x   thus  the main differences from the first intuitive objective function
p  w
w   in eq      are the use of t  x  for a smooth approximation of h x  and the use of
jh
w   in
the search result s p  w   instead of the raw evaluation e p  w    the difference from j p  w
p
w
w
eq      and jct  w   in eq      is that j p    is simpler and closer to the first intuitive one
w   or jr  w
w   in eq     
in eq       moreover  none of the existing studies incorporate jc  w
the minimax value s p  w   equals the raw evaluation value e wp   w    where e p  w   is the
evaluation of position p and wp is one of the pv leaves identified by the tree search rooted
at p with a weight vector w   in most cases  the derivatives of s p  w   equal the derivatives
of e wp   w    for these reasons  the pv leaves are identified in step   in figure   
    constraint and regularization terms
in the computer programs of chess variants  the evaluation values are typically represented
by integers  signed    bit integers are especially preferred because the corresponding transposition tables will be memory efficient  thus  we will restrict the range of the absolute
value of the evaluation function e p  w    moreover  because the search results do not change
w with a constant factor       this restriction
when one uses a scaled weight vector w
stabilizes the numerical optimization procedure if the value of  is uncertain 
w       g w
w     in eq       where
for this restriction  we introduce a constraint term jc  w
 
w       is an equality constraint  and   is a lagrange multiplier 
is a subset of w   g w
w    see
in addition to the constraint term  we also introduce a regularization term jr  w
w        w
w       where       is a
the last term in eq        we use l   regularization jr  w
constant variable  and w    is a subset of w   l   regularization is widely used to deal with highdimensional parameters  whereas l   regularization is used to avoid over fitting  tibshirani 
      

w 

   

filarge scale optimization for evaluation functions with minimax search

p  w
w   exists
the constraint and regularization terms ensure that a local minimum of jmmto
in a finite range of w   on the other hand  depending on p and the distribution of dp   this
p  w
w   in eq       or for jct
w   in eq      
property is not always true for j p  w   itself  for j p  w
the constraint and l   regularization terms have similar functionalities  i e   both restrict
the range of the absolute value of the evaluation function e p  w    however  their distinctions are important in practice because l   regularization makes the weight vector w    sparse
whereas the constraint term does not  thus  the regularization term is suitable for minor
features that are rarely seen  whereas the constraint term is suitable for major features
that appear often in the training set  moreover  both terms are useful for controlling the
strength of the restriction  because major feature values usually change more often than
minor feature values  the magnitudes of the partial derivatives with respect to major feature
weights are usually greater than those with respect to minor feature weights  we can adjust
the strength of l   regularization term so that it is weaker than the constraint term 
for example  our experiments used the constraint term for the piece values because their
feature values  i e   the number of pieces owned by black white  change in most single games
of shogi  the many other weights were penalized by l   regularization  each weight was
w     w       because the
controlled by either the constraint or l   regularization term  i e   w    w
partial derivatives with respect to the major and minor feature weights differed by several
orders of magnitude  it was difficult to stabilize the optimization procedure by means of a
single hyper parameter    

    partial derivative approximation
in each iteration  feature weights are updated on the basis of the partial derivatives of the
p  w
w   defined by eq       the partial derivative  if it exists  is
objective function jmmto
 p



w   
w   
w   
jmmto  w
j p  w    
jc  w
jr  w
wi
wi
wi
wi

   


w   on the right side is treated in an intuitive manner  sgn wi    for
the last term w
jr  w
i
  
wi  w   and   otherwise  function sgn x  is   for x        for x      and   for x     

w   is   for wi 
jc  w
  w     the case of wi  w  
the partial derivative of the constraint term w
i
is discussed in section     
the partial derivative of j p  w   does not always exist  because the minimax value
s p  w   is not always differentiable  instead  we can use an approximation 


j p  w    
wi


 x x
t  s p dp   w    s p m  w   
wi
 

   



 x x
p  d
t e w p   w    e wp m   w  
wi
 

   

pp mmp

pp mmp

 

x x

p d

t    e w p   w    e wp m   w    

pp mm p


  p  d p
e w   w    e wp m   w    
wi

where t    x    ddx t  x   the approximation of eq      by eq      makes the computation
tractable  because we identify the pv leaves in step   in figure    as stated in appendix a 
   

fihoki   kaneko

minimax value s p  w   found by the  search is continuous  and therefore  the function
j p  w   is also continuous  moreover  the approximate value is equivalent to the partial
derivative when a unique pv exists for each position  appendix a also discusses the con
ditions under which w
s p  w   exists  note that we have found that the errors caused by
i
this approximation are sufficiently small for the shogi application  kaneko   hoki        
previous studies  baxter et al         tesauro        use this approximation as well 
    grid adjacent update
for numerical stability  the grid adjacent update in step    see figure    is used to get
w  t      from w  t   consider a simple n dimensional grid in which the distance between two
adjacent points is h  suppose that h is an integer  e g   h      in the grid adjacent update 
the feature vector w  t  is always one of the points of the grid  and the i th component
wi  t      is adjacent to wi  t  
wi  t        wi  t   h sgn 

p  w
w  t  
jmmto
  
wi

thus   wi  t   wi  t      wi     h or   for all i  this update should decrease the objective
p  w
p  w
w   because w
w  w jmmto
w      and the errors in the approximation  see
function jmmto
eq       are negligible  moreover  h must be small enough so that the update does not
p
p
change the pv  i e   w
 t    w  t    for the majority of positions p searched in step   
although mmto focuses on optimization of weight vectors represented by integers  it
should be noted that the gradient descent update is not suitable even when one uses floatingpoint feature weights  our preliminary experiments indicate that the partial derivatives of
j p  w   with respect to major and minor feature weights differ by more than seven orders
w that is proportional to the gradient vector may
of magnitude  thus  an update vector w
not be appropriate for updating the minor feature weights with a small step  thus  the step
size of each component in a weight vector should be fixed as in the grid adjacent update 
or it might be able to be controlled in other ways  see  e g   duchi  hazan    singer        
    combination of techniques and practical issues
mmto is a combination of the above described techniques  this subsection discusses the
practical issues of this combination and its alternatives  some relate to external constraints
on learning  e g   how many weeks we can wait for results   and some depend on the properties of the domain to which mmto is applied 
      lagrange multiplier in grid adjacent update
for numerical stability  mmto explores a restricted parameter space where the constraint
w        to do this  the lagrange multiplier   in jc  w
w   is set to the
is satisfied  i e   jc  w
w 
j p w
 
median of the partial derivatives   wi   wi  w   in order to maintain the constraint
w         in each iteration  as a result  wi  is h for n feature weights  h for n feature
g w
weights  and   in one feature weight  where the number of feature weights in w   is  n     
w   is constant in all iterations 
on the other hand    in the regularization term jr  w
   

filarge scale optimization for evaluation functions with minimax search

      search depth
the game tree searches in step   in figure   are the most time consuming step in mmto 
tesauro        has shown that the use of a quiescence search yields better evaluation
functions  thus  it is expected that deeper searches in mmto will yield better evaluation
functions  on the other hand  we must handle a large amount of training positions  and the
search time tends to grow exponentially when we increase the search depth  therefore  most
of our experiments use a   ply standard search together with a quiescence search  here  the
quiescence search is called at every frontier node of the standard search  we observed that
evaluation functions learned with shallow searches are still effective for playing games with
deep searches  see section       similar results were reported by tesauro 
      reuse of pv for efficiency of learning
because step   in figure   is the most time consuming part  it is worth considering omitting
it by assuming wp  t    wp  t   with a certain frequency  in our experiments  steps   and  
were repeated    times without running step    we counted the number of iterations in the
run of step    that is  each iteration ran a single step   and    pairs of steps   and    the
number    would be domain dependent and should be set small enough so that the update
does not change the pv for most positions 
      pruning of trees
pruning techniques can dramatically reduce the number of searched nodes and hence speed
up learning  fortunately   pruning does not introduce any discontinuities in the objective
function  on the other hand  other pruning methods  including futility pruning  schaeffer 
       may introduce discontinuities  see appendix a     therefore  the robustness of the
whole learning procedure should be examined when such pruning techniques are used  as
far as the authors experience goes  the objective function with futility pruning seems to be
continuous  see section      
      convergence and performance measurement
the termination criteria is usually difficult to determine in iterative computations  in the
case of learning the shogi evaluation function  the convergence of the objective function in
mmto seems to be a significant criteria  because the rate of agreement with the test set
and the elo rating of the learned evaluation function also converge when it converges  note
that the rate of agreement should be measured on a separate test set from the training set
in order to detect overfitting  see section      
      duplication in positions and alternative moves
game records usually have duplications in the positions and desired moves in the opening
phase  although the ideal distributions of these positions and desired moves are unknown 
we decided to remove the duplications from the training and test sets for simplicity  that
is  we use each pair of hposition  movei at most once in each iteration  these duplications
are detected by zobrist hashing         note that two or more different moves may be
suggested for the same position in the training and test sets  and the objective function
   

fihoki   kaneko

becomes smaller if the tree search rooted at the position matches one of these moves  as
a result  conflicting goals such as move a should be better than move b and vice versa
are independently augmented to the objective function and cancel each other when both
moves can be played in the same position  in our experience  this adaptation seems to work
reasonably well in shogi  but the best solution may depend on the target game 

   experiments
we evaluated the effectiveness of mmto in experiments in which the number of feature
weights in the evaluation function was varied from thirteen to about forty million  we
found that mmto works better than comparison training and its intuitive modifications
in terms of the rate of agreement  speed of convergence  and game playing strength  we
w   and regularization term jr  w
w   help to inalso observed that the constraint term jc  w
crease the performance of the evaluation functions in terms of the rate of agreement with
the test set  to see numerical convergence  we investigated the surfaces of the objective
function in mmto with a limited number of feature weights and experimentally found that
mmto finds local minima in a reasonable range of feature weights  finally  we carried out
preliminary experiments on chess as well as experiments on data quality dependence 
    setup  evaluation functions  features  and game records
most of the experiments described in this section used bonanza  whose source code is
available online  hoki   muramatsu         the performance of bonanza in major tournaments is discussed in section      bonanza uses techniques such as mmto  pvs  pearl 
      marsland   campbell        reinefeld         a capture search at frontier nodes as
a quiescence search  transposition tables  zobrist        russell   norvig         static exchange evaluation  reul         killer and history heuristics  akl   newborn        schaeffer         null move pruning  adelson velskiy  arlazarov    donskoy        heinz        
futility pruning  schaeffer        heinz         and late move reductions  romstad        
it also uses an opening book database from which we randomly chose the opening lines
of self play experiments  the game records in the training and test sets were exclusively
chosen from games played in famous tournaments    there were         game records in
total  more than         of the games were played by professional players using standard
time controls  i e   from one to ten hours for a side with a byoyomi period  once the time
   the abbreviated tournament name  number of games we used  and date range of the games are  juni 
                 kisei                  ryuo                  osho                  oui       
          ouza                  nhk cup                  ginga                  kio       
          shinjino                  zen nihon proshogi                  hayazashi shogi senshuken 
               judan                 meisho                 joryu meijin                 meijin 
               all star kachinuki                 rating senshuken                 asahi open 
               heisei saikyo                 teno                 joryu osho                
kurashiki touka                 nihon series                   dan league                 ladiesopen                 joryu oui                 shoureikai                 gakusei osho           
      hayazashi shinei                 gakusei ouza                 asahi amashogi                
wakajishi                 kudan                 gakusei meijin                 shogi renmei cup 
               tatsujin                 kinsho cup                 amateur meijin                
kashima cup                 grand champion                 saikyosya kettei                 miscellaneous                 

   

filarge scale optimization for evaluation functions with minimax search

evaluation function
  
x
 a

a
e
fi  p   fia  p  wia
i   
x
 b
b
b
eb
fkj
 p   fkj
 p  wkj
ec
ed

k j
x

   l
k k
x

dimension
  
       

 c
c
c
fkk
  l  p   fkk   l  p  wkk   l

           

 d
 d
d
fkjj    p   fkjj
   p  wkjj  

            

k jj  

table    dimensions of evaluation functions  each evaluation function is a linear combination of weighted features  ea evaluates the material balance  and the others
evaluate a variety of positional scores by using extended piece square tables 

had expired  a player had to move within sixty seconds   some tournaments employed rapid
time controls such as    seconds per move and had top level amateur players as participants 
table   shows the four basic evaluation functions  where ea is for material balance and
the others are for positional scores  our experiments used the sum of these functions  i e  
ea   eab   ea   eb   eabc   eab   ec   and eabcd   eabc   ed   all evaluation functions are
anti symmetric with respect to the exchange of black and white  e p  w      e p  w    here 
p is a complete reversal of the black and white sides at position p  that is  black plays for
white and white plays for black    after this reversal  the pieces owned by black and white in
p are regarded as white and black pieces in p  respectively  also  all evaluation functions are
symmetric with respect to right and left mirroring of a position  e p  w     e p  w    where p
is the mirror image of p along file e 
the function ea  p  w a   was used to evaluate the material balance  there are    types
of pieces in shogi  iida et al          each feature fia  p  represents the number of the i th
type owned by black in position p  and wia is the relative value of the i th type of piece 
the partial derivative of the evaluation function with respect to wia is  ea  p  w a   wia  
fia  p   fia  p  
the function eb  p  w b   is a linear combination of weighted two piece square features 
these are natural extensions of one piece square features that were employed in recent
machine learning studies of chess evaluations  baxter et al         tesauro        veness
et al          these two piece square features were used to evaluate all conditions of the
b  p  is an indicator function that returns one if
king and another piece  each feature fkj
both of the conditions k and j exist in position p  otherwise  it returns zero  condition
k represents the location of the black king  there are    squares   and j represents the
type  owner  black or white   and location of the other piece  there were        different
conditions for j after some of the minor conditions were merged  thus  the total number of
the kingpiece conditions was                       before the mirror symmetric conditions
were merged 
   following shogi notation  black and white refer to the players who plays first and second  respectively 

   

fihoki   kaneko

similarly  the functions ec  p  w c   and ed  p  w d   were used to evaluate the kingking
c  p 
piece features and kingpiecepiece features  respectively  the indicator function fkk
 l
represents the location of the two kings  k  k     and the condition  type and location  of a
d  p  represents the location of black king k and
black piece l  the indicator function fkjj
 
the conditions of the other two black or white pieces  j  j     
game tree searches are required to identify pv leaf positions for mmto and to obtain
best moves to measure the rate of agreement  for these purposes  a nominal depth  
search was used together with the quiescence search  to normalize the objective function
values  the objective function values were divided by the total number of move pairs  z p  
p
 
pp  mp    the constraint function was set to
a

w   
g w

  
x

 
wia

        

   

i  

also  in accordance with the magnitude of the constraint  a in the horizontally mirrored
sigmoid function t  x           exp ax   was set to        so that t  x  would vary signifiw                w
wb   
cantly if x changed by a hundred  the regularization term was jr  w
c
d
w      w
w     an intuitive explanation of the penalty strength is that the absolute value
 w
of wi can be increased to     if doing so improves the relationship between the evaluation
values of a desired move and another legal move  the sums in eb   ec   and ed were computed
using    bit integers  and they were divided by    in order to fit the evaluation value into
a    bit integer  step h in the grid adjacent update was set to the smallest integer value   
    learning the piece values
first  the feature weights w   w a of the evaluation function ea were adjusted by mmto
or by comparison training  starting from the same initial value wia       for all i  tesauro
       used floating point feature weights and the conventional gradient descent method 
that is  the weight vector w was updated by
p
w   
w  t    w  t   rw jct
 w

    

where r is a constant training rate hand tuned to      the components in w used in the tree
search were rounded to the nearest integer values  the rescaling factor r in eq      was set
p  d
to        in accordance with the range of the difference   e w p   w    e wp m   w    from    to
        because this experiment had only    piece values to be adjusted in w a   only       
game records were used to compose the training set p  the set had         desired moves
and z p               move pairs after removing duplications and handicapped games 
one problem observed in the comparison training was slow learning  as shown in figure    phase i of the iterative procedure  from iteration   to     is mainly for adjusting
the pawn value  because the partial differential value of eq      for pawns is the largest
in this phase  after a good pawn value is found  phase ii  from iteration    to      is
mainly for adjusting the promoted rook and promoted bishop values  these values should
be the highest and second highest for reasonable game play  the long period of time taken
w   in eq      scales poorly  this is a general problem in
by phase ii indicates that jctp  w
gradient descent methods with multiple degrees of freedom  nocedal   wright         and
   

filarge scale optimization for evaluation functions with minimax search

pro rook

    

phase ii

phase i

phase iii
pro bishop

piece weight

    
gold
bishop
rook
pro pawn
pro knight
silver
pro silver
pro lance
knight
lance

    

    

   

pawn
 

 

 

 

     

 

 

     

  

 

 

     

   

    

iteration

figure    results of comparison training of the piece weights in shogi  the horizontal axis
plots the number of iterations on a logarithmic scale 

to cope with it  the learning rate r cannot be greater than     in accordance with the largest
partial derivative in these experiments 
the second problem was about convergence  in phase iii  after iteration      of figure   
all piece values keep increasing without changing the ratio of piece values  even though the
relative ratios of the piece values have room for improvement  this problem is inherent
to the objective function of comparison training  because eq      has no explicit term
to avoid it  in an extreme case where the training data satisfy the inequality condition
p  d
e w p   w    e wp m   w   for all moves m in any position p  all piece values diverge to infinity
w   is minimized  in fact  it was found that the training data in this
when the value jctp  w
experiment satisfied the condition for     of the pairs of the best and another legal move 
moreover  in the other extreme case  where the training does not satisfy the inequality
condition for any move m in any position p in eq       all piece values shrink to zero 
mmto deals with these problems by making grid adjacent updates and keeping the
w    the weighted vector w a converged
magnitudes constant through its constraint term jc  w
in    iterations  see figure     the value of the promoted rook was     and that of the
pawn was      note that the number of iterations was counted as the number of step    s
throughout the experiments 
    scalability in learning practical evaluation functions
after learning the piece values  we adjusted the weight vectors for positional scores  this
time  a large number of training records were used to cope with high dimensional weight
vectors  the main training set p had             desired moves and z p                
   

fihoki   kaneko

pro rook
pro bishop
rook
bishop
gold
pro knight
silver
pro pawn
pro lance
pro silver
knight
lance
pawn

piece weight

   

   

   

   
 

 

 

 

       

 

  
iteration

 

 

       

   

figure    results of mmto for the piece weights 
move pairs after removing duplications and handicap games from the         game records 
the test set had          desired moves after removing duplications and handicap games
from another        game records  the feature weights for eab were adjusted with mmto
and comparison training and its intuitive modifications  the same initial feature weights
w a      w b      were used in all three methods  w a     was optimized by mmto from the
 w
previous experiment  and w b          after that  to show scalability  the feature weights for
eabc and eabcd were optimized by mmto in this order  to adjust the feature weights for
eabc   the optimized feature weights of eab were used for the initial feature weights w a    
and w b      and   was used for w c      similarly  in eabcd   the optimized feature weights
in eabc were used for the initial feature weights w a      w b      and w c      and   was used
for w d     
comparison training with eabc and eabcd was not tested because learning eab yielded
only small improvements  the rate r in eq       was hand tuned to        as an example
of the intuitive modifications to stabilize the iterative procedure  a constant step update
was also tested for learning eab   in this case  the training rate r   t  was substituted for r
and
p
w  t    
r   t    rc   w  t  jct
 w
where this constant step modification conservatively updated w by using a constant step rc
that was hand tuned to         each value of r and rc was the best of five trials  another
intuitive modification was the reuse of pv  as explained in section      where the same pvs
were used    times and the rate r in eq       was       the rescaling factor r in eq     
was set to         because this value was satisfactory in the previous experiment shown in
figure    although the three methods are different  their iterations consumed almost the
same amount of time  this is because the most time consuming step of these experiments
was the game tree search to identify the pv leaf positions 
the rate of agreement with the test set is shown in figure    here  agreement means that
the legal move that obtained the highest value in the tree search is the desired move  the
   

filarge scale optimization for evaluation functions with minimax search

 

b  c  h 
b  c  b 

  

   
d  b  f 
   

b  c  b 
d  f  c 

   

positional weight

agreement    

  

  

abcd

mmto  e
 
abc
mmto  e  
ab
mmto  e  
ab
ct  e   reuse of pv 
ab
ct  e   constant step 
ab
ct  e  

  

  

 

 

 

 

       

 

  

 

 

       

b  c  a 
d  f  b 

   

b  b  c 
    

b  c  a 

    

    
b  d  c 

 

   

iteration

 

  

   
   
iteration

   

figure     left panel  improvement in rate of agreement with the test set for mmto
and comparison training  ct    right panel  improvement in feature weights for
positional features in ed   feature weight b  c  b  indicates the black king is at
b  and two gold generals are at c  and b   similarly  feature weights b  c  h  
b  c  b   b  c  a   b  b  c   b  c  a   and b  d  c  indicate two gold generals
with the king at b   feature weight d  b  f  indicates the black king is at d 
and the opponents two gold generals are at b  and f   similarly  feature weights
d  f  b  and d  f  c  indicate the opponents two gold generals with the king at
d   here  each value has been divided by     

rate is calculated by excluding positions in which there is only one legal move  or positions
in which there is an easy checkmate sequence that can be identified in the shallow depth
search  tied values are not counted  either 
the performances of mmto  comparison training  and its variations were compared in
the case of learning eab   we can see from the figure that the agreement rates of comparison
training and constant step modification are unstable and substantially lower than that of
mmto  we can also see that the reuse of pv modification increases stability because it
reduces the step length from       to      and reduces the computation time of learning by
almost    times because it reduces the number of time consuming pv updates 
mmto with the full evaluation function eabcd had the highest rate        the largescale optimization of the weight vector wd increased the level of agreement in     iterations 
   

fihoki   kaneko

without constraint
with constraint

pawn value

   
   
   
 

 

       

 

  
iteration

       

 

   

figure    effect of the constraint term in mmto  eab   
this computation took about a week using an intel x     workstation  the agreement ratio
with the test set converged in     iterations  however  the feature weights did not converge 
w   in eq      improves the stability of mmto in
figure   shows how the constraint jc  w
response to pawn value changes during the eab learning  we can see that the value keeps
w   is turned off and that it converges in     iterations with the
on increasing when jc  w
constraint turned on  one of the feature weights overflowed in the comparison training of
eabc   and this is another reason why the results of eabc are not shown for comparison
w   has little effect on the learning of eab   the
training  as the regularization term jr  w
improvement in the agreement rates of mmto is mainly due to the use of the constraint
w   and grid adjacent updates 
jc  w
w   is important for optimizing larger weight vectors  figthe regularization term jr  w
w   in eq      improves the weight vector in the enlarged evaluation
ure   shows how jr  w
function eabcd   without a regularization term  the objective function value and the rate
of agreement with the training set increase with the number of iterations  however  there
is also a linear increment in the absolute value of the weight vectors  and it distorts the
rate of agreement with the test set after the    th iteration  after the     th iteration 
only      of the components in w are zero  on the other hand        of the components
in w are zero with the regularization term  these results indicate that mmto without the
regularization suffers from overfitting of the training set when a large scale weight vector
is used  a similar effect of regularization also occurs when mmto is used in the eabc
learning  though the effect is smaller than that of eabcd  
    improvements in strength
to analyze the relationship between the agreement rate and the strength  we had the programs learned by mmto and by comparison training  figure    play games against a
reference shogi program many times  the reference program was a version of gps shogi
released in       kaneko         it is an open source program and was a finalist in past
world computer shogi championships  it has a completely different evaluation function in
which the majority of the parameters have been hand tuned  this version of gps shogi
serves as a reference program on a popular game server for shogi programs    the matches
were as follows  the reference program         nodes move  vs  all four learned programs 
   http   wdoor c u tokyo ac jp shogi   last access        in japanese  

   

fiobjective function

large scale optimization for evaluation functions with minimax search

    
    
    
    
    

agreement    

  

with l  regularization
without l  regularization

  
  

with training set

  

with test set

  

d

  

b

           

  

c

  
  
  

  

 

 

 

 

     

 

  

 

     

 

   

iteration

figure    effect of the regularization term in mmto  eabcd   
the reference program       nodes move  vs  the two learned programs with the evaluation
function ea and eab   and the reference program         nodes move  vs  the two learned
programs with the evaluation functions eabc and eabcd       games were played in each
match  the weight vectors obtained after                                               and    
iterations were tested for each learning configuration  thus  a total of                     
games were played  the learned programs searched        nodes per move  all programs
ran on a single thread and searched similar numbers of nodes in a second 
we measured the playing strength in terms of the elo rating  which is a popular way
to represent relative strength in two player games  the winning probability between two
players is estimated as          d        where d is the difference between their ratings  for
example  if the rating of player a is higher than that of player b by      the winning
percentage of player a is about      here  the ratings were determined by using maximum
likelihood estimation on all the games 
figure   shows the elo rating of each player  we can see that mmto with eab significantly outperformed comparison training with the same initial feature weights  when
mmto used eab   the winning percentage against the reference     k move  stably increased from               elo points  to               elo points   in contrast  comparison
   

fielo rating

hoki   kaneko

    
    
    
    
    
    
    
    
    
    
    

mmto  abcd 
mmto  abc 
mmto  ab 
comparison training  ab 
reference     k 
reference     k 
reference     k 

 

  
iteration

   

figure    improvements in strength  elo rating  achieved by mmto and comparison training 

opponent
player  
player  

depth  
      
      

depth  
      
      

depth  
      
      

depth  
      
      

depth  
      
      

depth  
      
      

depth  
      
      

table    winning percentages of program learned with game tree search having various
depths  opponent player   is the same program but the search depth is reduced
by    and opponent player   is also the same program but it uses the weight vector
before the learning 

training won at most               elo points  of its games  the results shown in figs   
and   indicate that mmto outperforms comparison training 
the large number of features also contributed to the playing strength of the programs
learned by mmto  although eabc showed a small improvement in terms of the agreement
rate and elo rating  eabcd consistently yielded significant improvements in these two criteria  thus  we concluded that mmto scales well to forty million features  note that the
computational cost of eabcd was reasonably small for practical game play  this is because
the number of features that appear in a position is only        or less even when the total
number of features is about forty million  also  the summations in table   can be maintained in an incremental manner when the program makes or unmakes a move  this sort
of feature design is similar to that of a famous othello program  buro         as a result 
bonanza using eabcd searched about        nodes sec on an intel xeon x     with   
threads  the speed itself is slower than that of many chess programs  but about average
for strong shogi programs  in addition  we found that bonanza using eabcd trained by
mmto played better than or was comparable to any of the top shogi programs in actual
tournaments  the details are discussed in section     
two additional fixed depth self play experiments were conducted to see if evaluation
functions trained by using shallow searches  depth   with quiescence search  are effective on
deep searches  table   shows the winning percentages of the learned program with various
   

filarge scale optimization for evaluation functions with minimax search

search depths of game play  the learned program had the eabcd evaluation function yielded
after the     th iteration  figure     the winning percentages against the same program
 player    with the search depth reduced by   were around      thus  we see that the
deeper the learned program searched  the stronger the program was  tesauro        also
reported similar results by using comparison training  in addition  the winning percentage
was about     against a program  player    that searched to the same depth but used eabc
after the     th iteration  thus  the use of eabcd trained by     iterations was effective
even when the program searched deeper  here  the winning percentages were computed for
a thousand games  seventy six games or less ending in draws or exceeding     moves were
not counted   fifty megabytes of memory were assigned to the transposition table of each
program  the uncertainties indicated as   was estimated by conducting a two sided test
at a significance level of    on the one thousand games 
    numerical stability and convergence
we investigated the continuity and partial differentiability of the objective function and
convergence of the feature weights in an empirical manner  while forward pruning techniques in game tree searches can speed up mmto in practical applications  such methods
do not always maintain continuous search values  as is shown in appendix a    moreover 
the objective function contains a large number of search values  this means it is difficult
to estimate its properties in a theoretical manner 
to make the empirical investigation manageable  we used only the smallest evaluation
function ea that deals with thirteen shogi piece values  moreover  we reduced the number
of game records to         the game records had         desired moves and z p              
move pairs after removing duplications and handicapped games 
      surface of the objective function
we investigated the function surface of the main part of the objective function j p  w a  
of mmto in eq      by generating contour maps from millions of sampling vectors w  
note that a contour line  isovalue surface  is a curve along which the functions take the
same value  the contour lines have certain properties  the gradient of the function is
perpendicular to the lines  and the magnitude of the gradient is large when two lines are
close together  in addition  a closed loop contour line indicates the location of the local
minimum or maximum 
two of the thirteen piece values in the weight vector w a were sampled in order to draw
the contour maps of two dimensional functions at an interval of   for each piece value 
the remaining eleven pieces were assigned reasonable values       pawn        lance      
 knight        silver general        gold general        bishop        rook        promoted
pawn        promoted lance        promoted knight        promoted silver        promoted
w a   was ignored so that w a
bishop   and      promoted rook   the constraint term jc  w
w   was turned off for piece values 
could be freely changed and the regularization term jr  w
a nominal depth   search  together with the quiescence search  was used 
we analyzed two pairs  hgold  bishopi  and hpawn  promoted lancei  figure   shows
an enlargement of the contour map of j p  wgold   wbishop    the contour interval is         
the map was computed with the ranges of             for the bishop and            for
   

fihoki   kaneko

    bishop pro bishop

    bishop dragon
    gold bishop

    bishop rook

    bishop silver

gold general weight

    gold pro bishop

    gold rook
   
   

x x

   
   
   

objective function

    gold pro rook

   

     

    gold silver
   

       

           
bishop weight

   

   

     
     

   

       
   

   

       

     
                   
bishop weight

                   
gold general weight

figure     upper panel  enlarged contour map of j p  wgold   wbishop    the dashed lines
indicate the critical boundaries at which the two dimensional function is not
partially differentiable  the two minima are indicated by x   bottom panel 
cross sections of the contour map  the left one shows the intersection of the
map with the line wgold        and the other shows that of wbishop       

the gold general  note that the function simply increases and there are no interesting
structures outside of the enlarged map  figure    shows an enlargement of the contour
map of j p  wpawn   wpro lance    the contour interval is          the map was computed
with the ranges of           for a pawn and            for a promoted lance 
we can see from these maps that there are local minima within reasonable ranges
and no sudden changes in the function values  although the function depends on a large
number of empirical search values  s p  w    it is approximately continuous and amenable to
optimization on the basis of gradients approximated by mmto 
on the other hand  the maps illustrate three difficulties  the first difficulty is the clear
edges of the contour lines  they indicate that the function is not partially differentiable at
the points on these edges  the dashed lines in these maps are critical boundaries at which
the profit and loss ratio of material exchanges inverts itself  for example  a silver is usually
   

filarge scale optimization for evaluation functions with minimax search

    pro lance lance     pro lance knight     pro lance silver
    pawn silver

pawn weight

   
    pawn knight
   

    pawn lance
    lance promotion pawn

   
x

   

   

   

   

x

   

   

   

objective function

promoted lance weight
      
      
           
      
          
      
      
                       
promoted lance weight

    
    
    
    
    

           
   

                   
pawn weight

figure      upper panel  enlarged contour map of j p  wpawn   wpro lance    the dashed
lines indicate critical boundaries at which the two dimensional function is not
partially differentiable  the two minima are indicated by x   bottom panel 
cross sections of the contour map  the left one shows the intersection of the
map with the line of wpawn        and the other shows that of wpro lance       

less valuable than a bishop  but capturing a silver becomes more profitable than capturing a
bishop when the bishop value is smaller than      this boundary is labeled bishop silver
in figure    as discussed in appendix a  the function is not always partially differentiable at
these critical boundaries  where multiple moves share the same best value  note that there
can be more boundaries in theory  e g   a bishop promoted knight boundary  whether a
boundary is visible or not depends on the training set and evaluation features  in addition 
the boundaries become winding curves when a non linear evaluation function is used instead
of a linear weighted sum 
the second difficulty  the scaling problem  is illustrated in figure     in this map  we
can see that the scales of the two piece values differ by two orders of magnitude  that
is  a pawn value variation of five hundred changes the function value by       whereas a
promoted lance value variation of five hundred changes the function value by only         
because of the difference in scaling  the surface along a promoted lance is almost flat  this
property explains why the pawn value is optimized earlier than those of the other pieces in
comparison training  as shown in figure    this property of ill scaling is disadvantageous
when it comes to optimizing the promoted lance value using a naive gradient decent method 
   

fihoki   kaneko

methods based on second order partial derivatives or approximations of the hessian matrix
can resolve this problem  however  they behave poorly at non partially differentiable points
on many boundaries  these two difficulties point to why the grid adjacent update in mmto
is effective 
the third difficulty is that there are multiple local minima in the two maps  this
means the results of mmto depend on the initial values and there is a chance of ending
up with a local rather than global minimum  we will investigate this problem in the next
subsection       
      empirical convergence and local minima properties
in the previous subsection  we examined two dimensional cross sections of the function
j p  w a    in this subsection  we loosen the restriction from two to thirteen dimensions 
which is sufficiently large to express all piece values in shogi  the aim of this experiment
is to catch a glimpse of the global map and numerical convergences for arbitrary initial
guesses about the values of all of the pieces 
for this purpose  a monte carlo sampling of the initial guess  w a      was carried out to
enumerate the local minima and analyze the optimized vectors  we ran     mmto with
randomized initial values  here  a uniformly distributed integer in the range of           
was assigned to each vector component  and the resulting vector was scaled to satisfy the
w a       
equality condition g w
figure    shows the cosine similarity and objective function value of a hundred of the    
runs  here  the cosine similarity of a weight vector is measured relative to the best vector
whose objective function is the smallest among those of     vectors after     iterations 
in the majority of the runs  we can see that function values and weight vectors converged
numerically in    iterations  here  we regard the iteration procedure to have converged
when the function values and similarities oscillate and show neither steady increase nor
decrease from the    th to     th iteration  although convergence is almost assured for
mmto with thirteen piece values  it would be difficult to achieve if more feature weights
were to be optimized  for example  figure   shows there was no convergence after twothousand iterations using eabcd   because     iterations took about a week on an intel
x     workstation  we could not afford to investigate the convergence of eabcd with the
current hardware  however      iterations nonetheless achieved a significant improvement
in strength  as shown in figure   
we can also see that these trials of mmto ended up with multiple local minima 
although a multiplicity of minima is generally undesirable in an optimization  there were
other  more favorable properties  the first property is that each run of mmto changed
the weight vector components by a sufficient amount  that is  the cosine similarity of the
    optimized vectors was localized in the range of             while that of the random
initial vectors were widely spread  see the top panel of figure      the second property is
that there was a weak correlation between the cosine similarities of the initial and optimized
vectors  this means that starting from a better initial vector in terms of the cosine similarity
should be beneficial  see the top panel of figure      however  starting from a better initial
vector in terms of the objective function value is not beneficial  see the middle panel of
figure      the third is that the distribution of local minima formed structures  see the
   

filarge scale optimization for evaluation functions with minimax search

    
    
    
    
    

similarity

cosine similarity of weight vector

    

    
    

    
    
    

objective function

    

objective function

    

     
     

    
     
     
iteration

    

 

 

 

 

        

  
iteration

 

 

 

        

   

figure     a hundred runs of mmto for a weight vector w a consisting of thirteen piece
values  the initial vectors were set using pseudo random numbers  the inset
is an enlargement showing the appearance of the numerical convergences  the
top panel shows the cosine similarities relative to the best weight vector  the
bottom panel shows the values of the objective function 

bottom panel of figure      that is  the lower the local minimum is  the more similar it
becomes to the best vector  moreover  the number of local minima decreases as the weight
vector gets farther away from the best 
we also investigated the dependence of the performance on the nominal search depth
of step     shown in figure    similar results in terms of convergence and the distribution
of local minima were obtained using a deeper search with a nominal depth of    because
mmto with a depth of   consumes more time than mmto with a depth of    the number
   

fiinitial objective function

cosine similarity of initial vector

hoki   kaneko

   
   
   
   
   
    

corr       

    

    

    
corr        

optimized objective function

     
     
     
     
     
corr        
    

    

    

    

cosine similarity of optimized vector

figure     scatter plots for     trials of thirteen dimensional weight vectors  the vector
expresses thirteen piece values  the cosine similarity of the vector is measured
relative to the best vector  the initial vector consists of uniform pseudo random
numbers  and the optimized one is the     th vector of the mmto iterations
starting from the initial one  the inset shows the correlation coefficient of each
scatter plot 

of random initial vectors was reduced to     and the number of iterations was reduced
to sixty for the sake of speed  in the majority of runs  the function values and weight
vectors converged in    iterations  figure    shows the strength  elo rating  and objective   

filarge scale optimization for evaluation functions with minimax search

   
depth    corr         
depth    corr         

elo rating

  
 
   
    
    
     

     

     

     

     

     

     

objective function

figure     scatter plots for thirteen dimensional weight vectors  the     vectors indicated
by crosses were learned with the nominal depth   search of step      and the   
vectors indicated by squares were learned with a depth   search 

function value of the    runs with depth    squares  and     runs with depth    crosses  
here  the elo ratings were identified by using maximum likelihood estimation on         
random pairing games         nodes move   the elo rating with depth   was    on
average and that with depth   was    on average  also  the correlation coefficient between
the elo rating and objective function value with depth   was      and that with depth
  was       moreover  we compared the performance of two best vectors that gave the
smallest objective function values  here  we computed the winning probability between the
best results of depth   and    each player was allowed to use one second for each move 
and one core of an intel xeon x     and fifty megabytes of memory were assigned to the
transposition table  after excluding two drawn games and two games exceeding a thousand
moves  we obtained a       winning rate against the program using the best results of
depth    these results indicate that mmto is better with depth   than with depth   
    performance of mmto under tournament conditions
mmto was invented by the developer of bonanza and made it one of the best programs
in shogi  moreover  the ideas behind earlier versions of mmto published in japanese
 hoki        have been adopted by many developers and have dramatically changed shogi
programs 
one of the authors started developing bonanza in       published program files on
the web in       and published source codes on the web in       hoki         this paper
gives detailed descriptions of the evaluation function learning  whereas the literature  hoki
  muramatsu        gives detailed descriptions of the game tree pruning of bonanza  in
addition to the learning method mmto  bonanza uses the evaluation function eabcd
shown in table    the earlier versions until      used a subset of eabcd with a modified
   

fihoki   kaneko

 
 
 
 
 

     may
bonanza
yss
kcc shogi
tacos
gekisashi

     may
yss
tanase shogi
gekisashi
bonanza
bingo shogi

     may
gekisashi
tanase shogi
bonanza
yss
bingo shogi

     may
gps shogi
otsuki shogi
monju
kcc shogi
bonanza

 
 
 
 
 

     may
gekisashi
shueso
gps shogi
bonkras
bonanza feliz

     may
bonkras
bonanza
shueso
gekisashi
ponanza

     may
gps shogi
puella 
tsutsukana
ponanza
shueso

     may
bonanza
ponanza
gps shogi
gekisashi
ninedayfever

table    program names and results of the recent world computer shogi championship 
 mmto   an earlier version or a variant of mmto  or  a learning method
influenced by mmto is used 

l  regularization  hoki         subsequent versions fully evaluate eabcd learned with l regularization 
table   shows the results of the world computer shogi championships  since      
the performance of bonanza has been examined in several computer shogi tournaments 
where each participant connects to a server program and plays shogi under a time control
of    minutes a side  bonanza received the first prize twice  second prize once  and third
prize once  moreover  players entitled bonanza feliz and monju used the same evaluation functions as obtained by mmto  thus  we claim that when bonanza uses mmto  it
plays better than or is comparable to any of the top programs in shogi  including commercial ones  this method clearly plays at the level of handcrafted shogi programs  moreover 
descriptions of the learning shogi evaluation functions and the earlier version of mmto
were published by hoki        in japanese and were quickly recognized as significant advances  in fact  no shogi program with conventional handcrafted evaluation functions has
broken into the top five in during the last five years of tournaments  one interesting case is
the results of gps shogi  kaneko         the winner of the      and      tournaments 
and source codes are available online  tanaka and kaneko         from      to       this
program uses a handcrafted evaluation function but in      it used a variant of mmto
and its results dramatically improved  the variants of mmto used in each program differ
in accordance with the content and policy of each program  for example  tanase shogi 
the runner up program in       used a learning method based on mmto and handcrafted
evaluation functions  bonkras  ponanza  puella   and ninedayfever also used variants of mmto  these excellent results make it clear that mmto outperforms conventional
programs that use handcrafted evaluation functions and has played extremely well in recent
shogi tournaments 
   

filarge scale optimization for evaluation functions with minimax search

it should be noted that some versions of bonanza add a small amount of randomness
to the grid adjacent updates  however  we omitted any discussion of using randomness in
this paper because it is not clear whether the added randomness improved the quality of the
evaluation function or not  the source codes of various versions of bonanza are available
online  hoki        and the source code of mmto are in two files  learn  c and learn  c 
    preliminary experiments on chess
so far  we have discussed the performance of mmto in shogi  we expect that mmto
would be effective in other two player perfect information games provided that certain
conditions are met      a sufficient number of game records are available      minimax
searches guided by heuristic evaluations are effective  and     the analytic partial derivatives
of the evaluation function with respect to the variables are available  for example  mmto
would not yield interesting results were it to be applied to a game that has been solved by
other means  e g   van den herik  uiterwijk    van rijswijck         also  it would not yield
interesting results in the game of go because monte carlo tree searches are more effective
than minimax searches guided by a heuristic evaluation function  kocsis   szepesvari       
gelly   silver        browne  powley  whitehouse  lucas  cowling  rohlfshagen  tavener 
perez  samothrakis    colton        gelly  kocsis  schoenauer sebag  silver  szepesvari   
teytaud         moreover  a simpler learning method  e g   a regression method in othello 
buro        would be preferable to mmto  if it is sufficiently effective 
we conducted preliminary experiments on chess to catch a glimpse of the applicability
of mmto to other games  note that there already are evaluation functions in chess that
can outplay grandmasters  whereas there are none in shogi  thus  it might be difficult to
improve well crafted chess evaluation functions  for this experiment  we chose an opensource program  crafty  as a fair implementation of a chess program  hyatt        
the original evaluation function had been tightly tuned and is not a simple multivariable
function  thus  for the sake of simplicity  we did not modify it in any way except to add
a new linear combination of weighted two pieces square features  the features were used
to evaluate all conditions of the king and another piece  such as in eb in section      the
mirror symmetric property described in section     was not applied and features in which
a pawn exists at the eighth rank were not counted  as a results  the total number of added
weights w b was          because a chess position possesses only thirty or fewer two piecessquare features  the additional computational time due to the above modification became
almost negligible with the help of a pawn hash table and the lazy evaluation technique that
had come with the original 
the training and test sets were composed by using game records at the free internet
chess server  fics   these games were played using the standard time control of the server
by two players with ratings of        or more  the training set p had             desired
moves and z p                move pairs after removing duplications from the         game
records  whereas the test set p had          desired moves and z p               move pairs
after removing duplications from the        game records 
figure    shows the rate of agreement with the test set and the number of correct answers
of chess problems through iteration  here   in the sigmoid function was set to          the
w b           w
w b   
equality constraint was not used  and the regularization term was jr  w
   

fihoki   kaneko

agreement    

    

    

    

    
    

agreement
number of correct answers

    
 

 

 

 

 

     

 

 

 

 

 

  

 

    

     

   

number of correct answers

    

iteration

figure     improvement in rate of agreement with the test set  solid line  and the number of
correct answers of        problems  dashed line  in chess  the two piece square
weights w b were adjusted using mmto 

rating
win

        
      

        
      

        
      

        
      

    
      

table    dependence of the strength  winning percentages  of learned programs on the
quality  ratings of players  of the training set  the uncertainty indicated as  
was estimated by conducting a two sided test at a significance level of    on       
games 

a total of        chess problems from the encyclopedia of chess middlegames  the second
section of the     problems   win at chess      problems   and winning chess sacrifices
        problems  were used  krogius  livsic  parma    taimanov        reinfeld       
       the learned program searched        nodes per problem and eight megabytes of
memory were assigned to the transposition table  we see that the agreement rate as well as
the number of correct answers tends to improve as the number of iterations grows  though
the differences are moderate  it means that mmto found room for improvement in a
well implemented chess program  these results indicate that mmto can be a useful way
to learn heuristic evaluation functions in chess  especially when one can design evaluation
features suitable for learning 
    data quality dependence
to assess the importance of the quality of the game records  we conducted additional experiments using game records of players with various levels of experience in shogi  here 
eabcd was learned by using the results of eabc in figure   as the initial value  the results
are summarized in table    each training set was composed from the records of        
rapid time control     seconds per move  games played by amateurs on a popular internet
shogi site  shogi club       the first line in the table shows the ratings of the amateur
players  the second line shows the winning percentages of the learned evaluation function
   shogi club     http   www shogidojo com  last access       

   

filarge scale optimization for evaluation functions with minimax search

against the evaluation function trained with grandmaster game records  here  each evaluation function was learned in     iterations  the winning percentages were computed by
averaging the results of a thousand games  about    drawn games and games exceeding
    moves were not counted   each player was allowed to use one second on one core of
an intel xeon x     for each move  and fifty megabytes of memory were assigned to the
transposition table  table   shows the significance of the quality of the training set  the use
of game records of stronger players made the program stronger 

   conclusion
we presented a method  minimax tree optimization  mmto   that uses game records to
adjust a full set of feature weights of the evaluation function in a two player game  the
learning of mmto has been designed so that the search results match the desired moves 
e g   the recorded moves of grandmaster games  mmto consists of two procedures     
a shallow heuristic search for all training positions using the current feature weights and
    an update guided by an approximation of the gradient of the objective function  a
new combination of a simple smooth approximation of the step function and grid adjacent
updates with standard techniques  i e   gradient guided optimization  constraints  and regularization  contributed to the scalability and stability of mmto and led to it showing
substantial improvements over existing methods 
the performance of mmto was demonstrated in experiments on shogi  a variant of chess
that has a larger number of legal moves  mmto clearly outperformed the existing methods 
in addition  the experimental results on the rate of agreement and playing strength indicate
that mmto can adjust forty million parameters  possible future work would be automated
adjustment of the step length and a theoretical convergence analysis 

acknowledgments
we are grateful to dr  masakazu muramatsu for his support of this work 

appendix a  notes on the continuity and partial differentiability of the
minimax value
we saw in section     that the objective function of mmto has a piecewise smooth surface 
in this appendix  we theoretically discuss the continuity and partial differentiability of the
w   with respect to w  rn   where w is the vector of parameters in the
minimax value vp  w
evaluation function e p  w   and p is the position  the continuity of the minimax value
ensures the continuity of the main part of objective function of mmto defined in eq      
the partial differentiability analysis gives conditions under which the approximation inside
mmto described in section     is valid  we first analyze a single minimax tree  assuming
that the tree is known and fixed  then  we extend our discussion to game tree search
methods that possibly explore different trees for different w  
definition    the evaluation function e     is a  p  rn     r function  where p is the set
of all positions in a target game  r is the set of real numbers  and rn is an n  dimensional
   

fihoki   kaneko

euclidean space  the evaluation function e p  w   is continuous with respect to the parameters w for any position p  p and for any w  rn   moreover  the evaluation function
e p  w   is partially differentiable with respect to any component of w at any w  rn  
the continuity and partial differentiability of the evaluation function are feasible assumptions  note that an evaluation based on an ordinary piece square table has these
properties  and all recent machine learning of evaluation functions have them  baxter et al  
      veness et al         buro        
definition    the theoretical game graph g is a finite  directed acyclic  connected graph
representing all possible transitions of states in the target game  where a node  resp  edge 
represents a position  resp  move   the set of nodes in g corresponds to p  v  g    p  a
minimax graph t is a finite connected sub graph of g  by convention  we use the term
minimax tree for the minimax graph even when it is not a tree  we denote the set of minimax
trees in g by t  a node is called a maximizing  resp  minimizing  node if the corresponding
position is the maximizing  resp  minimizing  player to move  the destination of an edge is
a maximizing  resp  minimizing  node if and only if the source of the edge is a minimizing
 resp  maximizing  node  we can clearly assume any node n to be a single position p  and
we will denote the evaluation function as e n  w   
let lr t be the set of leaf nodes of the entire sub tree tr of t and tr is rooted at node r 
we will omit tree t and use lr if it is obvious  we denote the set of immediate successors
 or children  at node n in tree t by cn t or by cn   note that cn    if n is a leaf  in the
standard notation  a node  or vertex  in a graph t is denoted by n  v  t    however  in
this appendix  we will omit v     and write n  t because it is obvious 
w   is a value associated with each node n in a minimax
definition    a minimax value vn t  w
tree t  t and it is defined recursively by a tree structure and by a static evaluation function
e n  w    as follows 

if n is a leaf 
 e n  w  
w   if n is a non leaf maximizing node 
w   
maxccn t vc  w
vn t  w
    

w
minccn t vc  w   if n is a non leaf minimizing node 
w   if it is obvious  for two minimax values a and b of a
we will omit tree t and use vn  w
maximizing  resp  minimizing  node  we say a is better than b if a   b  resp  b   a  
a   continuity of minimax value
the continuity of the minimax value follows from the continuity of the evaluation function 
w   is continuous with respect to w for any minimax
theorem    the minimax value vn t  w
w     vn t  w
w      or equivalently  for
tree t  t and for any w  rn   that is  limw w
w   vn t  w
w  w        logically implies
any w    rn and for any       there exists      such that  w
 
w    vn t  w
w       
 vn t  w
the following assertion about the ordinary properties of the basic functions max and
min and is common sense in analysis  it is rather difficult  however  to find a suitable
reference containing it  we therefore give a proof that will be useful in the subsequent
discussion 
   

filarge scale optimization for evaluation functions with minimax search

x        fk  x
x  be a continuous function
proposition    let k be a natural number and each f   x
x   is a continuous function on rn   similarly  mini  fi  x
x   is a
rn   r  then  maxi  fi  x
n
continuous function on r  
x  is continuous  for any x    rn and for any       there exists
proof  because each fi  x
x  x      i implies  fi  x
x   fi  x
x         hence  if we choose    mini i  
i     such that  x
 
 
x  x      implies  fi  x
x   fi  x
x       for any i              k  that is 
then  x
x        fi  x
x    fi  x
x       
fi  x

for any i              k 

note that ai   bi for any i              k obviously implies maxi ai   maxi bi   thus  from the
above inequalities we obtain
x        max fi  x
x    max fi  x
x       
max fi  x
i

i

i

that is 

x   max fi  x
x        
  max fi  x
i

i

x   the proof is similar for mini fi  x
x  
this implies the continuity of maxi fi  x
let r be the root of a given tree t   now  we prove theorem   on the basis of
mathematical induction from the leaf nodes lr t to root r  that is  at any leaf node
n  lr t   the minimax value is continuous because of the continuity of the evaluation
w     e n  w    for an internal node n  we assume that continuity holds for
function  vn t  w
any child c in cn t   this induction hypothesis and proposition   ensure the continuity of
w   
vn t  w
a   stability of principal variations
in the above subsection  we showed the continuity of minimax values through the continuity
of min and max functions  here  we show that the best moves and principal variations are
stable when the changes in the leaves are small enough  we analyze the stability in order
to discuss partial differentiability 
 
w    hereafter called the best children  denotes the set of
 w
definition    the symbol cn t
such children at node n in tree t that have the same minimax value as that of n 
 
w      c  cn t  vc  w
w     vn  w
w    
 w
cn t
 

w    here  a   b denotes
w    that is  cn t   cn t
 w
 w
we denote the rest of the children as cn t
the set difference  i e    e e  a  e 
  b  

a child is considered to be the best choice in its parent node if the minimax value of the
child is the same as that of the parent node  when no two children share the same value 
w   contains only one child  otherwise  the number of nodes in cn   w
w   can be greater
cn   w
than one 
definition    let r be the root of a tree t  t  the principal variation  abbreviated pv
w   of tree t is the sub tree of t obtained as the closure of the best children
for short  t   w
from the root 
w      r  
t    w
 
i
w      c  cn t
w     n  t i   w
w    for i     
t  w
 w
w   
t   w


 

w   
t i  w

i  

   

fihoki   kaneko

n   

 
 
n    n    n   

  

n    n    n   

figure     example of a minimax tree  graph  with a transposition at n 
 
 

w     cn t
w   and cn t
w      for any n  t   w
w    also  we denote leaves
note that cn t
 w
  w
  w



w   by l  w
w    that is  t  w
w    lr t  
in t  w

example    figure    shows a small minimax tree t that has two best children at root n   
the maximizing and minimizing nodes are denoted by boxes and circles  respectively  here 
cn      n    n    and cn      n     the principal variation t  of this tree is  n    n    n    n    
lemma    for any internal node n in any tree t  t and for any w    rn   there exists
w    w       n   the set of the best
a positive number n such that for any w   satisfying  w
 
 
children at node n for w is a subset of the one for w  
 
 
w      for any w   s t   w
w    w       n  
w      cn t
 w
 w
cn t

w     is empty  the assertion is trivial 
proof  when all child values are the same  i e   cn  w
otherwise  let   be the minimum absolute difference between the best value and any of the
w      vc  w
w           the continuity of the minimax
other values  i e       minccn  w
w      vn  w
w    w       n   we have
values ensures the existence of n such that for any w   satisfying  w
 
 
 
 
w    vc  w
w           and also  vn  w
w    vn  w
w            from the definition of
maxccn  vc  w

 
w   satisfies
n and triangle inequalities  any c  cn t  w
w      vn  w
w     
    vc  w
w      vc  w
w         vc  w
w      vn  w
w     
  vc  w
w      vc  w
w         vc  w
w      vn  w
w         vn  w
w      vn  w
w     
  vc  w


w      vn  w
w             
   vc  w
 
 
 
 
w    vn  w
w         
   vc  w
w      vn  w
w                  namely  vc  w
w        vn  w
w      this implies by definition
thus   vc  w
 
w     
 irrespective of whether n is a max or min node  that c   cn t
 w
definition     the tree stability t of a tree t is the minimum value of n among all the
nodes n  t   where n is a positive number satisfying lemma    note that the minimum
value t     exists because t is finite 
example     in reference to figure     suppose that each leaf value changes by at most     
w      vn  w
w           for each internal node n of heights      
then  it will be proven that  vn  w
and   in order  it is obvious for n    n    and n    and it can be proven for n    n  and n    and
finally for n    we can see that neither n  nor n  can become a new best node as a result of
this change 
   

filarge scale optimization for evaluation functions with minimax search

o
p

vn


u
 





w  
vn  w

w  
 w
j

w  
vc  w
 c  cn   
  h
w      c 
vc  w
  cn   
q



w     at maximizing node n  where w   changes along the i th comfigure     sketch of vn  w
 
w     at n equals vc  w
w     at one of the old best
ponent wi   h from w     here  vn  w
 
 
w   
children c  cn  w

a   partial differentiability
we show that the partial differentiability  as well as the partial derivative  of the minimax
value at a node in tree t depends only on its principal variations  we denote the right and
left partial derivatives of a function rn   r at point x   as
f  
x    
 x
x 
i

f  x             x i   h          x n    f  x             x n  
 
h  
h

    

f  
x    
 x
x
i

f  x             x i   h          x n    f  x             x n  
 
h 
h

    

lim
lim

let us pay attention to the single parameter xi that changes by h under these limit opf
 
erations  hereafter  the other parameters held constant will often be omitted as x
   x   
where x is the one dimensional parameter of interest  we use the symbol  because of its
analogy to the partial derivative in order not to forget that the other parameters have been
omitted 
w   of tree t  t and for any
theorem     for any node n in the principal variation t   w
w   at which the partial derivative of the evaluation
w  rn   there exists such a leaf la  l  w

w    vn   w
w     w
function equals the right partial derivative of vn  w
e la   w    similarly  there
i
w
i

w   at which the partial derivative of the evaluation function equals
exists such a leaf lb  l  w

w    vn  w
w     w
the left partial derivative of vn  w
e lb   w   
i
w
i

the proof of the theorem  given at the end of this subsection  is based on the stability
    and  w
w    w        h  are
of the best moves  we assume that w      w             wi    h          wn
sufficiently small in appendix a    consequently  we have  h    t   and for any node n in
tree t and for any w    rn  

 
 n leaf 

 e n  w  
     n maximizing node 
 
w
max
v
 w
 
 
c
w   
w  
vn  w
ccn t  w

 min  
w      n minimizing node  
w     vc  w
cc
 w
n t

   

    

fihoki   kaneko

w     changing with h  where n is a maxexample     figure    sketches an example of vn  w
w     when h      each value
imizing node  there are three best children with value vn  w
continuously  not always linearly  changes with h  while the best child depends on the sign
w     when h is less than t   this is because the minimax
of h  it is always one of c  cn   w
w     are sufficiently less  by at least     than vn  w
w     at
values of the other children c  cn  w
h     
w   are given by
the next goal is to show that the right and left partial derivatives of vn  w
w    respectively  the
the right and left partial derivatives at one of the best children cn   w
following propositions describe the ordinary properties of the right and left limits and the
basic functions max and min  similar arguments can be found in a comprehensive textbook
of calculus  we will give a detailed proof here  however  because it is rather difficult to find
the precisely same assertion in a textbook 
proposition     let k be a natural number and any of f   x        fk  x  be a continuous
function r   r  suppose that these functions have the same value at point x    i e  
fi
 
maxi fi  x      mini fi  x     and all of them have a right partial derivative x
   x    then 
 
the right partial derivative of the minimum or maximum of fi  x  at point x exists and is
equal to the minimum or maximum of the right partial derivatives of fi  x     respectively 
 maxi fi  
fi
 mini fi  
fi
 x     max    x    
 x     min    x    
 
 
i x
i x
x
x
proof  let o h  be landaus symbol  and let us use it to denote residual terms converging
 
 
to   faster than h  i e   limh   o h 
h      recall that fi  x     f   x   for any i              k 
for positive h  we have


fi  
 
max fi  x   h   max fi  x     max fi  x     h    x     o h   max fi  x   
i
i
i
i
x


fi
  max f   x      h    x      o h   f   x   
i
x


fi  
  h max    x     o h 
i x
 

 

from eq        the function maxi fi  x  at point x  has a right partial derivative maxi
the same argument applies to the right partial derivative of mini fi  x  

fi
 x    
x 

proposition     suppose that functions have the same value at point x  and all of these
fi
 
functions have a left derivative x
  x    then  the left partial derivative of the minimum
or maximum of fi  x  at point x  is equal to the maximum or minimum of the left partial
derivatives of fi  x    
 maxi fi  
fi
 mini fi  
fi
 x     min   x    
 x     max   x    


i x
i x
x
x
   

filarge scale optimization for evaluation functions with minimax search

proof  using similar algebra as in the proof of proposition     we find for negative h 


fi  
 
 
max fi  x   h   max fi  x     h min   x     o h  
i
i
i x
fi
 
from eq        the function maxi fi  x  at point x  has the left partial derivative mini x
  x   
note that min and max are switched in the algebra above because of the negativity of h 
the same argument applies to the left partial derivative of mini fi  x  

lemma     let gi   n  w    

vn
w 
 w
wi 

 resp  gi  n  w    

vn
w    be the right  resp 
 w
wi
w  rn and for any internal

left 

w    for any
partial derivative of the minimax value vn  w
node
w   of tree t  t  there exist right and left partial derivatives
n in principal variation t   w
w   with respect to any i              n   the right and left partial
gi   n  w   and gi  n  w   of vn  w
derivatives are 

 
 maxcc    w
w   gi  c  w    n  maximizing node 
n t 
 
gi  n  w    
 
 mincc     w
w   gi  c  w    n  minimizing node 
n t


 mincc    w
w   gi  c  w    n  maximizing node 
n t 

gi  n  w    

 n  minimizing node  
 maxcc     w
w   gi  c  w  
n t

proof  we prove these equalities on the basis of mathematical induction from the leaf nodes
w    by the definition of the evaluation function  the
lr t to the root r  for each leaf n in l  w
w   is clearly continuous and partially differentiable with respect to any
minimax value vn  w
component in w  rn   for any internal node n  we assume  as an induction hypothesis 
that the right partial derivative gi   c  w   and left partial derivative gi  c  w   exist for any
w      cn   w
w     for any  h    t and eq        from the
child c  cn t   recall that cn   w
induction hypothesis with proposition     we have
 maxccn   w
w   vc
wi 

 minccn   w
vc
vc
w   vc
w   
w     min
w   
 w
 w
 
 
   w
 
 
w
w
w
w 
w 
ccn  w
ccn  w
i
i
i

w     max
 w

similarly  from proposition     we have
 maxccn   w
w   vc
wi

 minccn   w
vc
vc
w   vc
w   
w     max
w   
 w
 w


  w
 
 
w
w
w
w 
w 
ccn  w
ccn  w
i
i
i

w     min
 w

w    it is obvious that gi   n  w    
now  we prove theorem     for any leaf n  l  w

w    lemma    ensures that the left
  w
e n  w    for any internal node n  t   w
i
and right partial derivatives gi   n  w   and gi  n  w   are given by one of the best children 
w   such that
thus  for root r  there always exist leaves la and lb  l  w
gi  n  w  

gi   r  w    


wi

gi  r  w    

e la   w   
   


wi

e lb   w   

    

fihoki   kaneko


g    n  w        
g   n  w        

wi

e a  w        
e a  w        

n


 

a 

g    r  w       g   r  w        
w      
vr  w

r

 c
  b


wi

e c  w        
e c  w        


wi

e b  w        
e b  w        

w   exists at w     it is not equal to the partial
figure     although the partial derivative of vr  w

 
derivative at a pv leaf wi e a  w   

w   with
remark     by definition  if gi   n  w       gi  n  w      the partial derivative of vn  w
 

 
w   satisfying
respect to wi exists at the point w and there is a leaf l  l  w


w    
vn  w
e l  w     
wi
wi

    

w   with respect
remark     for any i              n   the partial derivative of minimax value vn  w

 
 

w     
to wi exists at w and equals wi e l  w    if l is the unique element of l  w
w   has a partial derivative
remark     there exists a tree tr for which the minimax value vn  w
 
w           and
with respect to wi at w   even when the leaves l in pv are not unique   l  w

 
give different partial derivatives wi e l  w    an example is sketched in figure     where
the partial derivative is   for a and   for b and c 
a   game tree search and pruning techniques
consider a game tree search s be a function that takes the root position r and the evaluationw   with minimax values
function parameters w as inputs  and yields a minimax tree trs  w
s  w
w
w
vn tr  w
 w
 
for
all
n

t
  
we
call
a
game tree
search
s
static 
provided that it yields
w 
r
s
s
 
w      v  tr  w
w     for any root r  then 
a constant tree with respect to w   i e   v  tr  w
w   yielded by such a static game tree
theorems   and    apply to the minimax value vr trs  w
search  for example  a fixed depth minimax search or a minimax search considering limited
types of moves  e g   capture and promotion  is a static game tree search  a minimax search
with stand pat used in the quiescence search  beal        is static  too  note that stand
pat at node n is equivalent to a virtual move adding an evaluation function e n  w   as a
w   in eq        even when n is not a leaf node 
candidate of the node value vn  w
when pruning techniques are incorporated  part of the tree is pruned and not explored 
 
w    trs  w
w   yielded by
consider a static search s  that with a pruning s     and tree trs  w
 
s   we call a pruning conservative  provided that it yields the same minimax value at
w     vr t s   w
w    theorem   applies to the minimax
any root r for any w  rn   vr trs  w
w    w
r
w
value at the root r  vr t s   w
 w
  
yielded
by
such
a
static
game tree search with conservative
w 
r
pruning  standard  pruning  knuth   moore        is a conservative pruning  however 
many pruning techniques  e g   static exchange evaluation  reul          extended  futility
pruning  heinz         null move pruning  adelson velskiy et al          and late move
reductions  romstad         can prune a sub tree without having to prove that the sub   

filarge scale optimization for evaluation functions with minimax search

tree is irrelevant to the minimax value at the root  thus  these pruning techniques are
generally not conservative 
a   summary
the minimax value of the root of the tree explored by a game tree search with wellconfigured pruning techniques is continuous  this result suggests the continuity of the
objective function of mmto in eq       as was empirically observed in section      as for
partial differentiability  theorem    suggest that it is feasible to consider the leaves of the
principal variations in a search tree  when there is only one principal variation  as stated in
remark     the use of the partial derivative at the unique leaf introduced in section     is
correct  otherwise  i e   when there are multiple principal variations  the partial derivative
may not exist or be different from the partial derivative at one of the leaves  as stated in
remark     although the frequency of such cases depends on the target game and on the
evaluation features  it is almost negligible in the experiments discussed in our previous work
 kaneko   hoki        

references
adelson velskiy  g  m   arlazarov  v  l     donskoy  m  v          some methods of
controlling the tree search in chess programs  artificial intelligence                  
akl  s  g     newborn  m  m          the principal continuation and the killer heuristic 
in proceedings of the      annual conference  acm     pp          new york  ny 
usa  acm 
anantharaman  t          evaluation tuning for computer chess  linear discriminant methods  icca journal                 
baxter  j   tridgell  a     weaver  l          learning to play chess using temporaldifferences  machine learning                 
beal  d  f          a generalised quiescence search algorithm  artificial intelligence     
     
beal  d  f     smith  m  c          temporal difference learning applied to game playing
and the results of application to shogi  theoretical computer science                
    
bertsekas  d  p     bertsekas  d  p          nonlinear programming   nd edition   athena
scientific 
bjornsson  y     marsland  t  a          learning control of search extensions  in caulfield 
h  j   chen  s  h   cheng  h  d   duro  r  j   honavar  v   kerre  e  e   lu  m  
romay  m  g   shih  t  k   ventura  d   wang  p  p     yang  y   eds    jcis  pp 
        jcis   association for intelligent machinery  inc 
browne  c   powley  e   whitehouse  d   lucas  s   cowling  p   rohlfshagen  p   tavener 
s   perez  d   samothrakis  s     colton  s          a survey of monte carlo tree
search methods  computational intelligence and ai in games  ieee transactions
on             
   

fihoki   kaneko

buro  m          improving heuristic mini max search by supervised learning  artificial
intelligence                 
buro  m   long  j  r   furtak  t     sturtevant  n  r          improving state evaluation 
inference  and search in trick based card games  in ijcai  pp           
buro  m          statistical feature combination for the evaluation of game positions 
journal of artificial intelligence research            
campbell  m   hoane  jr   a  j     hsu  f  h          deep blue  artificial intelligence 
               
chellapilla  k     fogel  d          evolving neural networks to play checkers without
relying on expert knowledge  neural networks  ieee transactions on              
     
coulom  r          computing elo ratings of move patterns in the game of go  icga
journal                 
coulom  r          clop  confident local optimization for noisy black box parameter tuning 
in herik  h     plaat  a   eds    advances in computer games     no       in lncs 
pp          springer verlag 
duchi  j   hazan  e     singer  y          adaptive subgradient methods for online learning
and stochastic optimization  journal of machine learning research               
fawcett  t  e          feature discovery for problem solving systems  ph d  thesis  department of computer science  university of massachusetts  amherst 
furnkranz  j          machine learning in games  a survey  in machines that learn to play
games  pp        nova science publishers  commack  ny  usa 
gelly  s   kocsis  l   schoenauer m   sebag  m   silver  d   szepesvari  c     teytaud  o 
        the grand challenge of computer go  monte carlo tree search and extensions 
commun  acm                 
gelly  s     silver  d          monte carlo tree search and rapid action value estimation in
computer go  artificial intelligence                     
gomboc  d   buro  m     marsland  t  a          tuning evaluation functions by maximizing concordance  theoretical computer science                  
heinz  e  a          extended futility pruning  icca journal               
heinz  e  a          adaptive null move pruning  icca journal                 
hoki  k  bonanza  the computer shogi program   http   www geocities jp bonanza 
shogi  last access        in japanese 
hoki  k          optimal control of minimax search results to learn positional evaluation  in
the   th game programming workshop  gpw       pp        kanagawa  japan 
in japanese 
hoki  k     kaneko  t          the global landscape of objective functions for the optimization of shogi piece values with game tree search  in van den herik  h  j    
plaat  a   eds    advances in computer games     no       in lncs  pp         
springer verlag 
   

filarge scale optimization for evaluation functions with minimax search

hoki  k     muramatsu  m          efficiency of three forward pruning techniques in shogi 
futility pruning  null move pruning  and late move reduction  lmr   entertainment
computing              
hsu  f  h   anantharaman  t  s   campbell  m  s     nowatzyk  a          deep thought 
in marsland  t  a     schaeffer  j   eds    computers  chess  and cognition  pp 
      springer verlag 
iida  h   sakuta  m     rollason  j          computer shogi  artificial intelligence        
           
kaneko  t          recent improvements on computer shogi and gps shogi  ipsj magazine                  in japanese 
kaneko  t     hoki  k          analysis of evaluation function learning by comparison of
sibling nodes  in van den herik  h  j     plaat  a   eds    advances in computer
games     no       in lncs  pp          springer verlag 
knuth  d  e     moore  r  w          an analysis of alpha beta pruning  artificial
intelligence                
kocsis  l     szepesvari  c          bandit based monte carlo planning  in machine learning  ecml       vol        pp          springer 
krogius  n   livsic  a   parma  b     taimanov  m          encyclopedia of chess middlegames  combinations  chess informant 
levinson  r     weber  r          chess neighborhoods  function combination  and reinforcement learning  in marsland  t  a     frank  i   eds    computer and games 
no       in lncs  pp          springer verlag 
marsland  t  a          evaluation function factors  icca journal              
marsland  t  a     campbell  m          parallel search of strongly ordered game trees 
acm computing surveys                 
nitsche  t          a learning chess program  in advances in computer chess    pp 
        pergamon press 
nocedal  j     wright  s          numerical optimization  springer verlag 
nowatzyk  a          http   tim mann org dt eval tune txt 
pearl  j          scout  a simple game searching algorithm with proven optimal properties 
in in proceedings of the first annual national conference on artificial intelligence 
pp         
reinefeld  a          an improvement to the scout tree search algorithm  icca journal 
           
reinfeld  f               winning chess sacrifices and combinations  wilshire book
company 
reinfeld  f          win at chess  dover books on chess   dover publications 
reul  f          static exchange evaluation with  approach  icga journal              
   

fihoki   kaneko

romstad  t  an introduction to late move reductions  http   www glaurungchess com 
lmr html  last access       
russell  s  j     norvig  p          artificial intelligence  a modern approach   nd edition  
prentice hall 
schaeffer  j          experiments in search and knowledge  ph d  thesis  department of
computing science  university of waterloo  canada 
schaeffer  j          the history heuristic and alpha beta search enhancements in practice 
ieee transactions on pattern analysis and machine intelligence  pami             
     
schaeffer  j   hlynka  m     jussila  v          temporal difference learning applied to
a high performance game playing program  in ijcai    proceedings of the   th
international joint conference on artificial intelligence  pp          san francisco 
ca  usa  morgan kaufmann publishers inc 
silver  d     tesauro  g          monte carlo simulation balancing  in icml     proceedings of the   th annual international conference on machine learning  pp         
acm 
sutton  r  s     barto  a  g          reinforcement learning  an introduction  adaptive
computation and machine learning   the mit press 
tanaka  t     kaneko  t  gps shogi   http   gps tanaka ecc u tokyo ac jp 
gpsshogi  last access        in japanese 
tesauro  g          comparison training of chess evaluation functions  in machines that
learn to play games  pp          nova science publishers 
tesauro  g          programming backgammon using self teaching neural nets  artificial
intelligence                   
tibshirani  r          regression shrinkage and selection via the lasso  j  royal  statist 
soc b                 
tsuruoka  y   yokoyama  d     chikayama  t          game tree search algorithm based
on realization probability  icga journal                 
ugajin  t     kotani  y          learning evaluation function based on tree strap in shogi 
in the   th game programming workshop  pp          in japanese 
van den herik  h  j   uiterwijk  j  w  h  m     van rijswijck  j          games solved 
now and in the future  artif  intell                     
van der meulen  m          weight assessment in evaluation functions  in beal  d   ed   
advances in  computer chess    pp       
veness  j   silver  d   uther  w     blair  a          bootstrapping from game tree search 
in advances in neural information processing systems     pp           
zobrist  a  l          a new hashing method with application for game playing  icca
journal               

   

fi