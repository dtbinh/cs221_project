journal of artificial intelligence research                

submitted        published      

robustness and stability in constraint programming under
dynamism and uncertainty
laura climent

lcliment   dsic   upv  es

instituto de automatica e informatica industrial
universidad politecnica de valencia  spain 

richard j  wallace

r   wallace    c   ucc   ie

insight center for data analytics
department of computer science  university college cork  ireland 

miguel a  salido

msalido   dsic   upv  es

instituto de automatica e informatica industrial
universidad politecnica de valencia  spain 

federico barber

fbarber   dsic   upv  es

instituto de automatica e informatica industrial
universidad politecnica de valencia  spain 

abstract
many real life problems that can be solved by constraint programming  come from uncertain
and dynamic environments  because of the dynamism  the original problem may change over time 
and thus the solution found for the original problem may become invalid  for this reason  dealing
with such problems has become an important issue in the fields of constraint programming  in some
cases  there is extant knowledge about the uncertain and dynamic environment  in other cases  this
information is fragmentary or unknown  in this paper  we extend the concept of robustness and
stability for constraint satisfaction problems  csps  with ordered domains  where only limited
assumptions need to be made as to possible changes  we present a search algorithm that searches
for both robust and stable solutions for csps of this nature  it is well known that meeting both
criteria simultaneously is a desirable objective for constraint solving in uncertain and dynamic
environments  we also present compelling evidence that our search algorithm outperforms other
general purpose algorithms for dynamic csps using random instances and benchmarks derived
from real life problems 

   introduction
constraint programming is a powerful tool for solving many artificial intelligence problems that
can be modeled as csps  much effort has been spent on increasing the efficiency of algorithms for
solving csps  as reflected in the literature  however  most of these techniques assume that the set
of variables  domains and constraints involved in the csp are known and fixed when the problem is
modeled  this is a strong limitation when we deal with real life situations because these problems
may come from uncertain and dynamic environments  due to the dynamism in the environment 
both the original problem and its corresponding modeled csp may evolve  in addition  since the
c
    
ai access foundation  all rights reserved 

fic liment  wallace   s alido   barber

real world is uncertain in its nature  information about the dynamism of the environment may be
incomplete  erroneous or even may not exist  in such situations  a solution that holds for the original
model can become invalid after changes in the original problem 
the approaches that deal with this situation can be classified as   i  reactive approaches  whose
main objective is to obtain a new solution as similar as possible to the previous solution  the solution
found before the changes occurred  in a efficient way  and  ii  proactive approaches  which use
knowledge about possible future changes in order to avoid or minimize their effects  for a survey
see verfaillie   jussien         thus  proactive approaches are applied before the changes occur 
while reactive approaches are only applied when the changes invalidate the original solution 
reactive approaches re solve the csp after each solution loss  which consumes computational
time  that is a clear inconvenience  especially when we deal with short term changes  where solution loss is very frequent  in addition  in many applications  such as online planning and scheduling 
the time required to calculate a new solution may be too long for actions to be taken to redress the
situation  in addition  the loss of a solution can have several negative effects in the modeled situation  for example  in a task assignment of a production system with several machines  it could cause
the shutdown of the production system  the breakage of machines  the loss of the material object
in production  etc  in a transport timetabling problem  a solution loss  due to some disruption at a
point  may produce a delay that propagates through the entire schedule  all these negative effects
will probably entail an economic loss as well 
proactive approaches try to avoid the drawbacks just stated and  therefore  they are highly valued for dealing with problems in uncertain and dynamic environments  given the advantages that
proactive approaches potentially offer  in this paper we restrict ourselves to this approach  heretofore two main types of proactive approaches have been considered  which can be distinguished on
the basis of the characteristics of the solutions that they obtain  which are called robust and flexible
 see section     in an important survey on constraint solving in uncertain and dynamic environments
 verfaillie   jussien         the authors mention the possibility of developing proactive strategies
that combine the solution features of robustness and flexibility  they state  the production of
solutions that are at the same time robust and flexible  that have every chance to resist changes and
can be easily adapted when they did not resist  is obviously a desirable objective  in this paper  we
present an algorithm that meets the objective of combining solution robustness and stability  the
solution feature of stability is a special case of flexibility 
many proactive approaches proposed in the literature assume the existence of knowledge about
the uncertain and dynamic environment  see section     in these cases it is difficult to characterize
the robustness of the solutions when detailed information about possible future changes is not available  we consider situations where there is an added difficulty stemming from the fact that the only
limited assumptions about changes can be made  our discussion focuses on csps with ordered
and discrete domains that model problems for which the order over the elements of the domain
is significant  in these cases  a common type of change that problems may undergo is restrictive
modifications over the bounds of the solution space  these assumptions and their motivations were
introduced by climent et al          moreover  examples of real life problems that exhibit this
type of dynamism were described  specifically  temporal reasoning based problems  spatial and geometric reasoning problems  and design problems  in temporal problems  delays are an inherent
feature  which implies restrictive modifications of the bounds involved with such disruptions  for
instance  fu  lau  varakantham  and xiao        stated that unexpected external events such as
manpower availability  weather changes  etc  lead to delays or advances in completion of activities
  

firobustness and s tability in c onstraint p rogramming under dynamism and u ncertainty

in scheduling problems  in spatial and geometric reasoning problems  the constraints can be readjusted due to measurement errors  the latter can also occur in design problems  in which the data is
not completely certain 
in this paper  we present an algorithm that searches for solutions to csps with ordered domains 
which are robust and are also stable because they can often be repaired using a value of similar
magnitude if they undergo a value loss  the paper is organized as follows  the next section recalls
some general definitions  section   gives a brief account of earlier proactive procedures  section  
presents a new conception of robustness and stability when there exists an order over the elements of
the domain  sections   and   describe the main objective for finding solutions that meet the stability
and robustness criteria simultaneously  then  in section   the search algorithm that meets these
objectives is explained  section   presents a case study of scheduling problems  section   describes
experiments with various types of csps  showing the effectiveness of the present approach for
finding solutions that are both stable and robust  section    gives conclusions 

   technical background
in this section we give some basic definitions that are used in the rest of the paper  following standard
notations and definitions in the literature 
definition     a constraint satisfaction problem  csp  is represented as a triple p   hx   d  ci
where x is a finite set of variables x    x    x         xn    d is a set of domains d    d    d         dn  
such that for each variable xi  x there is a set of values that the variable can take  and c is a
finite set of constraints c    c    c         cm   which restrict the values that the variables can simultaneously take  we denote by dc the set of unary constraints associated with d 
definition     a tuple t is an assignment of values to a subset of variables xt  x  
if a tuple t is feasible we call it s  this means that s is an assignment of the domain values to
some variables that does not violate any constraint  if s is a complete assignment  it involves all the
variables of the csp   then it is a solution of the csp  xs is the subset of variables that are involved
in s  then x  xs is the set of unassigned variables in s  the value assigned to a variable x in s
is denoted as s x   in addition  we denote ds  x   d x  to the subset of domain values of the
variable x that are consistent with s 
the number of possible tuples of a constraint
q ci  c is composed of the elements of the
cartesian product of the domains of var ci    xj var ci   dj   where var ci    x is the set of
variables involved in ci  scope of ci   
definition     the tightness of a constraint is the ratio of the number of forbidden tuples to the
number of possible tuples  tightness is defined within the interval       
inferential processes for csps narrow the search space of possible partial solutions  in this work
we use one of the most known and used consistency procedure  arc consistency 
definition     a csp is arc consistent  mackworth      a  iff for any pair of constrained variables
xi and xj   for each value a in di there exists at least one value b in dj such that the partial
assignment  xi   a  xj   b  satisfies all the constraints related to both xi and xj   any value in
  

fic liment  wallace   s alido   barber

the domain of a variable which is not arc consistent can be eliminated as they can not be part of
any solution  the domain of a variable is arc consistent iff all values are arc consistent  thus  a
problem is arc consistent iff all its arcs are arc consistent 
 cij  c  a  d xi    b  d xj    a and b satisfy cij  
in the following  several properties associated with the solutions of problems that come from
dynamic environments are defined 
definition     the most robust solution of a csp within a set of solutions is the one with the highest
likelihood of remaining a solution after a given set of changes in the csp 
definition     a flexible solution is anything  a partial solution  complete solution  conditional
solution  set of solutions  etc   that  in case of change  can be easily modified to produce a solution
to the new problem  verfaillie   jussien        
a more specific concept of flexibility is the concept of stability 
definition     a solution s  is more stable than another solution s  if and only if  in the event of
a change that invalidates them  a closer alternative to s  than to s  exists  modified from the work
presented by hebrard        
the main difference between definition     and definition     is that the former introduces the
concept of closer solution  the measurement of this closeness is made by calculating distances
between solutions  more concrete information about the distance equations is explained in following
sections  we would like to remark that definition     does not consider the alterations in the original
solution but only its resistance to changes in the problem  on the other hand  definition     and
definition     do consider changes to the original solution when a new solution is produced after a
change in the problem 

   related work  proactive approaches
several approaches have been proposed in the past for handling this type of problem  which can be
classified based on the kind of solutions they obtain  thus  there are techniques that search for robust
solutions and others that search for flexible solutions  for a survey see verfaillie   jussien        
in this section we describe some techniques that search for robust solutions and their limitations 
then we discuss a technique that searches a certain type of stable solutions  super solutions 
    searching for robust solutions
many earlier approaches that search for robust solutions use additional information about the uncertain and dynamic environment in which the problem occurs  and most often this involves probabilistic representations  in one example of this type  information is gathered in the form of penalties
when values have been invalidated after changes in the problem  wallace   freuder         nevertheless  in the probabilistic csp model  pcsp   fargier   lang         there exists information
associated with each constraint  expressing its probability of existence  other techniques focus on
the dynamism of the variables of the csp  for instance  the mixed csp model  mcsp   fargier 
  

firobustness and s tability in c onstraint p rogramming under dynamism and u ncertainty

lang    schiex        and the subsequent uncertain csp model  ucsp   yorke smith   gervet 
      consider the dynamism of certain uncontrollable variables that can take on different values
of their uncertain domains  a related model  uses simple temporal networks  but adds data about
the time uncertainties and preferences  which represent the starting or ending times of events  stppus   rossi  venable    yorke smith         the stochastic csp model  scsp   walsh       
also considers probability distributions associated with the uncontrollable variables  the branching
csp model  bcsp  considers the possible addition of variables  with a certain associated gain  to
the current problem  fowler   brown        
in most of these models  the form of the algorithm is dependent on detailed knowledge about the
dynamic environment  for this purpose  a list of possible changes is required or there is an explicit
representation of uncertainty  often in the form of an associated probability distribution  as a result 
these approaches cannot be used if the necessary information is unknown  in many real problems 
however  knowledge about possible further changes is either limited or non existent  hence  there
is an important need for techniques that find robust solutions in this kind of environment 
for instance  climent et al         cope with csps that model problems for which the order over
the domain elements is significant  specifically  these csps are modeled as weighted constraint
satisfaction problems  wcsps   larrosa   schiex        by penalizing valid tuples based on their
coverings  instead of requiring extra detailed dynamism information  the authors only make limited
assumptions concerning the changes that might occur  which is related to the nature of csps with
ordered domains  specifically  dynamism is assumed to take the form of restrictions on the bounds
of the solution space  in this paper  we make the same assumptions about the dynamism  the
previous wcsp modeling approach computes robustness based on feasible neighbours that compose
a covering which surrounds the analyzed value with respect to each constraint boundary  thus  in
cases in which a neighbour is feasible with respect to one bound but is not for another bound  this
neighbour is not feasible in the solution space  for this reason  this approach obtains robustness
approximations in problems in which there is a high relation between constraints  on the other
hand  the algorithm described in this paper computes feasible assignments with respect to the entire
solution space  which avoids the weakness of the wcsp modeling approach explained above  a
comparison between these approaches is found in section   
    searching for super solutions
techniques that search for stable solutions of a certain type  which are denoted as super solutions 
were presented by hebrard         the goal is to be able to repair an invalid solution after changes
occur  with minimal changes that can be specified in advance  since this is another approach that
does not require detailed additional information about changes in a problem  it is of interest to
compare it to the search algorithm introduced in this paper 
definition     a solution is an  a  b  super solution if the loss of values of at most a variables can
be repaired by assigning other values to these variables and changing the values of at most b other
variables  hebrard        
for csps  a major focus has been on finding        super solutions  this is because of the high
computational cost of computing b     or a      this is one of the reasons why we analyze
this particular super solution case in this paper  the other reason is given by verfaillie and jussien
        where the authors state that a desirable objective is to limit as much as possible changes
  

fic liment  wallace   s alido   barber

in the produced solution  which motives the search of  a     super solutions  in general  it is unusual to find        super solutions where all variables can be repaired  for this reason  hebrard
       also developed a branch and bound based algorithm for finding solutions that are close to
       super solutions  i e   where the number of repairable variables is maximized  also called maximizing the        repairability  

   extending robustness and stability to csps with ordered domains
in this section we extend the original definition of solution robustness  definition      and solution
stability  definition      to consider csps with ordered domains  where only limited assumptions
are made about changes in the problem that are derived from their inherent structure  given this
framework and therefore the existence of a significant order over the values in the domains  it is
reasonable to assume that the original bounds of the solution space can only be restricted or relaxed 
even if this does not cover all possible changes  the bounds of the solution space are delimited
by the domains and constraints of the csp  note that the possibility of solution loss only exists
when changes over the original bounds of the solution space are restrictive  for this reason  a
solution that is located farther away from the bounds is more likely to remain a solution  given
these assumptions  we specialize definition     for this framework as follows 
definition     the most robust solution of a csp with ordered domains without detailed dynamism
data is the solution that maximizes the distance from all the dynamic bounds of the solution space 
furthermore  the definition of stable solutions for csps with ordered domains can be made more
precise because it is possible to define a more specific notion of closeness between two solutions
due to the existent order over the domain values  hebrard        measures the level of dissimilarity
of two solutions by counting
pn the number of variables that take different values in both solutions  i e  
the hamming distance   i    s i    s i     later  hebrard  osullivan  and walsh        consider
another similarity measure  the manhattan distance  this
p measure uses the sum of the absolute difference of values  of each variable  for both solutions   ni    s i s i     note that unlike hamming
distance  manhattan distance requires an order over the elements in order to calculate the absolute
difference of the values  in the following definition  we apply the manhattan distance to the notion
of stable solutions for csps with ordered domains 
definition     given an order relationship over the values of a set of solutions  a solution s  is
more stable than another solution s  iff  in the event of a change that invalidates them  there exists
an alternative solution to s  with lower manhattan distance than the manhattan distance of any
alternative solution to s  
furthermore  we present an extension of definition     for csps with ordered domains by fixing
a maximum manhattan distance between the original solution and the repaired solution  which is
called c 
definition     a solution is an  a  b  c  super solution if the loss of values of a variables at most 
can be repaired by assigning other values whose manhattan distance with respect the original
values is lower or equal to c  and this involves changing the values of b variables at most 
the above definition also holds for the        c  super solutions and the        c  repairability 
which are the main focus of the stability analysis in this paper 
  

firobustness and s tability in c onstraint p rogramming under dynamism and u ncertainty

   searching for robust and stable solutions  general main objective
in order to find robust and stable solutions for csps with ordered domains under our assumptions 
we combine the robustness and stability criteria presented in section    as mentioned  calculating
distances is required for the search of robust solutions in this framework  however  a measure of
the distance from the dynamic bounds of the solution space is not always obvious or easy to derive 
since the bounds are delimited by the domains and the constraints of the csp  and the latter may
be extensionally expressed  however  some deductions about minimum distances to the bounds can
be made based on the feasibility of the neighbours of a solution  this idea is first motivated with a
very simple example and then it is formalized 
example     figure   shows two solution spaces  one convex and the other non convex  whose
dynamic bounds are marked by contiguous lines  the most robust solutions according to definition
    are highlighted  note that there are two contiguous feasible neighbours on both sides of each
assignment  discontinuous lines  

 a  convex solution space

 b  non convex solution space

figure    most robust solutions for different solution spaces 
from this example  we can conclude that we can only ensure that a solution s is located at
least at a distance d from a bound in a certain direction of the n dimensional space if all the tuples
at distances lower or equal to d from s in this direction are feasible  therefore  the number of
feasible contiguous surrounding neighbours of the solution is a measure of the robustness of the
solution in the face of restrictive changes that affect the original bounds of the solution space  see
definition       in addition to fulfilling the main objective of finding solutions whose values have
a high number of feasible neighbours close to each assignment  this criterion can be used to obtain
solutions with high stability  this is because if the value assigned to a variable has at least one
of these feasible neighbour values  then this variable is repairable  that is  if its assigned value is
lost  it can easily be repaired by assigning the neighbour value  since this value is consistent with
the rest of the values of the assignment   regarding the stability notion of definition      note that
the difference between the lost value and the repairable value is very low  since they are immediate
neighbours  in fact  their value difference is one  which is the minimum possible 
  

fic liment  wallace   s alido   barber

the set of feasible contiguous neighbour values of the value v that have differences not greater
than k with respect to v in increasing  or decreasing  or both directions with respect to the order
relationship is denoted as nk  x  v  s     value v is a feasible value for variable x in the feasible
partial complete assignment s  here  when we say that other values are feasible  we mean that they
are also feasible with respect to s   recall that we use ds  x   d x  the subset of domain values
that are consistent with the feasible partial assignment s   the list of operators  is composed of a
set of paired elements  or operator pairs  each operator pair is denoted as i                  
the operator pairs fix the order directions to analyze  thus  the set        refers to values greater
than v  increasing direction  and the set       refers to values lower than v  decreasing direction  
for each operator pair  the operator in position j is referenced as ij   for instance  if the list of
operators is                     the operator pair   references        and the operator   
references the operator    given this notation  we define nk  x  v  s    as 
nk  x  v  s      w  ds  x    i   w i  v   v  w   k 

   

 z j             v  w         v z  j   ds  x  
the first condition of equation   ensures that the value w is greater or lower than v according
to the operator i          and that the distance between these values is less or equal to k  the
second condition ensures that all values that are closer to v than w are also feasible values for s  if
at least one of them is not  the value w cannot belong to nk  x  v  s     as mentioned previously 
the set of feasible neighbours of a value has to be contiguous  otherwise  there is an infeasible
space between this value and another feasible value  for instance  in figure   b  the value   does
not belong to nk  y      x         for any  or k because the value   is not a feasible value and
therefore it is outside the bounds of the solution space 
for the general case of csps with ordered domains in which we assume that all the bounds
are dynamic  the desirable objective is to find contiguous surrounding feasible neighbours on both
sides  for this reason                     for this list of operator pairs  the last condition of
equation   checks that all the values in both directions that are closer to v than w  are also feasible
values for s  for instance  in figure   b   nk  y      x                                 for any k
value  note that these neighbours are on both sides the value   with respect to the y axis  in section
   we will show a specific case for which it is desirable to apply only one operator pair due to the
nature of the problem 
to apply equation   to domains that are not ordered in z  a monotonic and order preserving
function has to be applied  for instance  if we consider d    f reezing  cold  mild  warm  hot 
boiling   a monotonic function that assigns greater values to values with higher temperatures could
be defined  for example  f  f reezing       f  cold       f  mild       f  warm       f  hot   
  and f  boiling      

   objective function
in section   we stated that the main desirable objective for a selected value is to have as many contiguous feasible neighbours in a certain direction  because they determine the minimum distance of
this value from the bound in this direction  for approximating the distance of several values assigned
 partial or complete assignment   we compute the number of neighbours of each value  therefore 
  

firobustness and s tability in c onstraint p rogramming under dynamism and u ncertainty

we define as an objective function of our search algorithm the sum of the size of nk  x  v  s   
 denoted  nk  x  v  s      for each variable x  x   if s is an incomplete assignment  we calculate
the maximum  nk  x  v  s     for each v  ds  x  of each unassigned variable x  x  xs  upper
bound   note that the maximum size of the set of neighbour values for each variable is       k 
where      is the number of pair operators  thus  the maximum size of the set of neighbour values
is  k if  is composed of two operator pairs or k if  is composed of only one operator pair  note
also that it is not necessary to check all the values of ds  x  if  for at least one of them  the size of
the set is the maximum possible  in the following equation  we formalize the objective function that
is used by our search algorithm 

f  s  k       

x

max  nk  x  v  s      v  ds  x    

x

 nk  y  s y   s     

   

yxs

xx  xs

for example      for the most robust solutions of both figures   a  and   b   highlighted solutions  f  s  k                        for k     since every value assigned to each solution has
two contiguous neighbours on both sides 
next  we give a formal rationale for using the total number of neighbours of the solution  sum
of feasible surrounding neighbours of each value of the solution  as a measure of robustness 
for k      in a convex solution space  each value has either zero  one or two feasible neighbours  here we can discount the case of zero neighbours because if an assignment has zero feasible
neighbours  then it must be part of a singleton domain  and it will be part of all solutions  so we
need only consider values with one or two feasible neighbours 
in this case  a solution with a greater sum is one whose assignments have more feasible neighbour pairs  this can be easily seen if we consider the difference between a solution all of whose
values have only one feasible neighbour and any other solution  this difference will be equal to the
number of feasible neighbour pairs associated with the latters assignments 
proposition    if we assume that having two feasible neighbours confers greater robustness
than having one and that the probabilities of single changes are independent  then a solution with a
greater feasible neighbour sum than another will also be more robust  and vice versa 
in the non convex case  it is unfortunately possible for one assignment to have zero feasible
neighbours  while other assignments to the same variable have one or two  in this case  we cannot
assume proposition    however  as the number of variables in the problem increases  it becomes
increasingly unlikely that a variable with an assignment having zero feasible neighbours will be
associated with the largest neighbour sum for the remaining variables 
regarding the measure of stability  a solution that maximizes the        k  repairability  see
definition      also maximizes the number of variables that can be repaired by a neighbour value
at a distance less or equal to k  without modifying any other variable   however  to obtain robust
solutions we maximize the sum of neighbour values of each value of the solution  note that even
if both maximization criteria are not identical  as mentioned  when the number of variables in the
problem increases  it becomes increasingly unlikely that a non repairable variable will be associated
with the largest neighbour sum for the remaining variables  so in this work we will use the same
technique for finding robust and stable solutions for csps with ordered domains  nevertheless  the
basic units of measure for both criteria are different 
  

fic liment  wallace   s alido   barber

   search algorithm
in this section we present an algorithm for finding robust and stable solutions according to the main
objective described in section    for this purpose  we have incorporated this optimization criterion
into a branch   bound algorithm  algorithm    that maximizes the objective function f  s  k   
 see equation     as mentioned  this function sums  nk   of each assigned variable and the maximum
possible  nk   of each unassigned variable  note that this computation is an upper bound of the final
total number of feasible contiguous neighbours of the solution 
algorithm    b b nk   is an anytime algorithm that uses an inference process and prunes
the branches whose objective function value is lower or equal to the current maximum function
value obtained  referred to as lb  lower bound   the process stops when all the branches have been
explored or pruned  providing the solution s with the maximum f  s  k     on the other hand  we
can limit the search time and therefore the quality of the best solution found by fixing a time cutoff 
of course  the more time algorithm   spends searching  the more robust and stable the solution
provided can be  in addition  we compute the maximum possible objective function value  which is
the maximum number of neighbours for each variable multiplied by the number of variables of the
csp  denoted as ub  upper bound   thus  if the objective function value of a new solution found is
equal to ub  the algorithm stops  since this solution is optimal 
algorithm    b b nk   branch   bound anytime algorithm
data  p   hx   d  ci    k  scale  m  time cutoff  optional 
result  s  nk   lb
s       partial assignment
xs       set of variables assigned
nk       set of contiguous surrounding neighbours
lb        maximum f  s  k    for the solutions
ub        k   x   
i    
gac  nk  p  s  xs   nk     k  lb  
repeat
if restarting scratch  new solution found then
i    
c  scale  mi     number of fails cutoff
i  i     
until time cutoff  not mgac  nk  p  s  xs   nk     k  lb     c  ub  
we have implemented the branch   bound algorithm using a geometric restart strategy  walsh 
      in order to reduce the repetition of fails in the search due to very early wrong assignments
 thrashing   thus  each time the number of failures  referenced as nbf   reaches the number offails cutoff value condition  c  that is checked in algorithm    the algorithm restarts the search
from scratch  except for the constraint weights stored by the dom wdeg heuristic variable selection
 boussemart et al          the value of the number of fails cutoff is increased geometrically in algorithm   according to the scale factor  referred to as scale  and the multiplicative factor  referred to
as m   we have implemented two different options to carry out after a solution is found  in the first 
called restarting completion  when the first solution is found  the algorithm continues to search until
  

firobustness and s tability in c onstraint p rogramming under dynamism and u ncertainty

completion  this is done by assigning a huge number representing  to the number of fails cutoff  
in the second option  called restarting scratch  after each solution found  the algorithm restarts the
search from scratch and also restarts the number of fails cutoff computation  the constraint weighs
remain the same   for instances with very large domain sizes  this restarting option can be effective
because it avoids spending a large amount of time in a specific branch  the latter happens when
algorithm   checks many domain values of variables located at low levels of the search tree  because the objective function of the partial assignment is better than the current maximum  lb   in
this case  if there exists a time cutoff  algorithm   could not analyze other branches of the tree that
may contain solutions of better quality 
the inference process is carried out by algorithm    gac  nk    which is an extension of the
well known ac   mackworth      b  that performs generalized arc consistency  gac   mohr
  henderson        bessiere         some specific notation has been included  as var c   which
is the scope of c  c  the original seeksupport function of gac  searches for a support
for each domain value  we have modified this function slightly by providing the set of values to
be analysed as a parameter of the function  thus  if any of these values is deleted because there
does not exist any consistent support with respect the partial assignment  seeksupport returns
false  this function is first called with the values of the domain of the variables  for checking if
the partial assignment s is gac   and later with nk just for assigned variables  for checking if each
nk  x  s x   s    is gac  with respect s   in order to ensure the contiguity of the values in nk  
algorithm   checks the consistency of subsets of ni  nk   where i is equal to one initially  and it
is increased by one unit until at least one of the values of ni is inconsistent or i reaches the value
of k  the complexity of updating ni can be reduced to       i if the domains are ordered  note
that in the case where both greater and lower values are candidates to be in the set  the updating
cost is    i  after composing the set of contiguous neighbour values that are gac  with respect s 
algorithm   analyzes if the objective function f  s  k    is greater than lb  if it is not  or s is not
gac   it returns false 
algorithm    mgac  nk   performs a maintaining gac  procedure by assigning to each variable x  x a new value v  d x   until the value selected is gac  nk with respect s  we have
implemented two value selection heuristics  lexicographical order and selection of the value that
maximizes  nk  x  v  s      starting from intermediate values  there are some real life problems
for which the lexicographical selection order is effective in finding feasible solutions quickly  an
example is scheduling problems  whose domain values represent time units  hence the importance
of selecting low values in order not to exceed the maximum fixed makespan  however  if it is not
important to select low values  the heuristic that starts with intermediate values may offer better
results because it is selecting values that maximize the objective function at the current node of the
search tree  furthermore  since search starts with intermediate values  the likelihood of selecting
values located far from the domain bounds is higher 
algorithm   is also responsible for updating the set of assigned variables xs   the partial assignment s and the maximum objective function value lb  for each solution found   furthermore  it
stores the domains and the set of neighbours of all the variables before making an assignment  note
that after a variable x is assigned  d x  contains a single value that is the value assigned to x  if algorithm    gac  nk   returns false  then algorithm    mgac  nk   carries out the backtracking
process and also restores the domains and set of neighbours of all the variables 
to reduce computational time when we deal with csps with convex domains  we have implemented bounds arc consistency for discrete csps  lhomme         the main feature of this
  

fic liment  wallace   s alido   barber

algorithm    gac  nk   global arc consistency algorithm
data  p  s  xs   nk     k  lb  nbf
result  d  nk   nbf
q    x  c   c  c  x  var c      var c  is the scope of c
while q     do
 x  c   takeelement q  
seekd  seeksupport x  d x   c      found support for all d x  for c 
if d x     then
nbf  nbf         number of failures
return false
if not seekd then
q  q    y  c    c  c  c    c  x  y  var c    x    y 
if x  xs then
i    
repeat
update ni  x  s x   s    applying equation   
seekn  seeksupport x  ni  x  s x   s     c  
i  i     
until seekn   false  i   k 
nk  x  s x   s     ni  x  s x   s   
return f  s  k      lb    see equation  

consistency technique is that the arc consistency is restricted with respect to the bounds of each
convex domain  thus  including it in the search algorithm only affects to the seeksupport function  which instead of seeking for a support for all the set of values  just checks the minimum and
maximum bounds  note that this implementation is not necessary for the search of robust and stable solutions  however it allows a significant reduction of the search time  we only apply bounds
consistency to the tentative values of the assignment but not to their set of neighbours  since they
require a complete consistency check  otherwise there could exist infeasible gaps  which would
break the contiguity requirement that ensures minimum distances to the bounds 

   case study  searching for robust and stable schedules
there are some types of real life problems whose structure can provide us with specific information
about their dynamism  in this section we analyze a well known type of problem from the literature 
scheduling problems  these problems can be converted into satisfiability problems by fixing a
maximum makespan  and they can then be modeled as csps  the csp modeling usually consists
of associating the start or end time of each task with a particular variable  in this paper we use the
start time   the domain associated with a variable represents the possible time units  and by means
of them it is possible to fix a maximum desired makespan  finally  the duration of the tasks and
their order  if it exists  can be fixed by means of csp constraints 
in this section  we will first explain some robustness scheduling measurement units  and then
we describe the objective function for csps that model scheduling problems and give an example
of its application 
  

firobustness and s tability in c onstraint p rogramming under dynamism and u ncertainty

algorithm    mgac  nk   maintaining global arc consistency
data  p  s  xs   nk     k  lb  nbf  c  ub
result  s  nk   lb
select x  x  xs      dom wdeg heuristic
xs  xs  x 
save d and nk  
while d x       nbf   c do
select min v   d x      heuristic    lexicographical value order
select v  d x   max  nk  x  v  s      starting by intermediate values     heuristic  
s  s   x   v 
d x   v 
if gac  nk  p  s  xs   nk   k  lb  nbf   then
if xs   x then
   new solution found
lb  f  s  k    
if lb   ub then
return true    best possible sum achieved
c       restarting completion
return false    restarting scratch
if mgac  nk  p  s  xs   nk   k  lb  nbf  c  ub  then
return true
restore d d x  and nk  
s  s  x   v  
xs  xs  x 
return false

    robustness measurement in scheduling
in this section  we introduce several criteria for measuring the scheduling robustness  for such
purpose  we use the terms buffers and slack to refer to the spare time between related tasks  there
are two main factors that enhance the capability of a schedule to absorb unexpected delays in its
activities  the number of buffers and their duration  ideally  according to the robustness criterion 
a buffer time should be as long as possible because the longer it is  the longer are the delays that
is able to absorb  for this reason another straight forward robustness measurement was proposed
by leon et al         as the slack average in the schedule  the combination of the duration of
the buffers and their distribution across the schedule provides a more accurate robustness measure
s
denoted as rslack
  it is a slight variant of a measure introduced by surico et al         that consists
in maximizing the slack average  shorted as avg  and minimizing their standard deviation  shorted
as std  for a schedule s  for regulating the importance of the standard deviation term  the authors
use a parameter called   which can take any value in the interval             according to the authors
considerations 

s
rslack
  avg slack    std slack 

  

   

fic liment  wallace   s alido   barber

another means of measuring the robustness of a system  defined by kitano        is related
to its resistance to perturbations having a certain probability of occurrence  this approach was
extended by escamilla et al         to scheduling problems in which the probabilities of task delays
s   where z is the discrete set of unexpected
are unknown  the robustness measure is denoted as rf z
delays in the duration of tasks  f measures whether the schedule s is still feasible after the disruption
 
  z  z is the probability for
 f  z      when is satisfiable  otherwise f  z       and p z     z 
an instance z  z  i e   all delays are considered to have the same probability of occurrence  
s
rf z
 

x

p z   f  z 

   

zz

    objective function for scheduling
in csp models of scheduling problems  the fact that domain values represent time units has implications with respect to measures of robustness and stability  for these problems  when a value of the
solution is lost  lower values cannot be used for replacing this unfeasible value because they represent time units that have already taken place  thus  if there is an incident  and the time point t is not
available  neither are the values lower than t  therefore  having lower feasible neighbours does not
improve the robustness nor the stability of a solution of a csp that models a scheduling problem
 since they cannot absorb delays nor be used as repairable values   given these characteristics  the
main desirable objective is to search for neighbours greater than the value assigned  to do this  we
fix the set of operators to             for scheduling problems  this is illustrated below 
example     we consider a toy scheduling problem with two tasks  t  and t    both have a duration of two time units and they must be executed in the order listed  the maximum makespan
allowed is six time units  in figure   we can see the associated csp model and its solution space 
the variables x  and x  represent the start times of tasks t  and t    respectively  the domain
of both variables  represented by discontinuous lines  is              which preserves the maximum
makespan of six time units  the maximum start time of a task is the maximum makespan minus the
duration of the aforesaid task   there is one constraint controlling the execution order of the tasks
 t  must start before t     which is c    x   x       the solution space is represented by a dark
gray area  where there are six solutions  black dots  
if no specific information is given about the dynamic environment  which schedule is the most
robust  as stated in section      the greater the number of time buffers and the greater their duration  the more robust the schedule is  but how can we determine which solution of the modeled
csp meets these requirements  the answer is obtained by determining the feasible contiguous
neighbours with greater values  located at distances less or equal to k from the solution  however 
depending on the value of k  we will either prioritize the selection of schedules with a large number
of short time buffers or we will prioritize the selection of schedules with lower number of long time
buffers  the number of greater feasible neighbours associated with a value of a variable corresponds
to the total amount of slack that is located after the task represented by this variable  thus  the slack
is able to absorb a delay in the previous task as long as itself  without modifying the other tasks of
the present schedule  robustness feature   furthermore  if the slack following a task is not sufficient
to absorb a delay  the start of the following task can be delayed  after repairing the broken assigned
value  if there is a long enough buffer associated with this later task  stability feature  
  

firobustness and s tability in c onstraint p rogramming under dynamism and u ncertainty

 

 

x 

 

 

 

c 

 

 

 

 

 

x 

figure    csp model associated with example     and its solution space 
for the above example  which is a two dimensional csp representing a scheduling problem
with two tasks  there are three schedules that are most robust according to the criteria stated above 
if we maximize the sum of distances for greater values located at a distance one  k      from
each value of the assignment  we obtain the solution shown in figure   a   whose sum is f  s    k  
                    the first number is nk  x    v    s          and second is nk  x    v    s          
where v  and v  are the values assigned to the variables x  and x  respectively  note that the sums
for the neighbours greater than the solution values and located at a distance one from them are
f  s    k                      and f  s    k                       respectively  in the following  a 
figures  the greater neighbours are indicated by an elipse  with an arrow pointing to the solution  the
circled dot   in the associated  b  figures  the schedules equivalent to the solutions marked in  a 
are shown  note that the greater neighbours indicated in the  a  figures correspond to the slack in
the  b  figures  for instance  in figure   b  each task has an associated slack of duration one  which
corresponds to the existence of one greater neighbour for each value assignment in figure   a  
on the other hand  if we maximize the sum of greater neighbors values for k      the three
solutions represented in figures   a     a  and   a  are all classified as best solutions according to
our objective function  the computation of the sum of neighbours located at a distance lower or
equal to k  for k     is  f  s    k                       figure   a    f  s    k                     
 figure   a   and f  s    k                       figure   a    note that the schedules in figures
  b  and   b  each have only one time buffer  but its duration is two time units  unlike the schedule
represented in figure   b  that has two time buffers of one unit each  thus  by fixing k     we
prioritize the seek of a high number of time buffers  however  for greater k values  we prioritize
their duration  even if in this case their distribution may not be optimal 
if we consider the stability of the solutions  small modifications in the solutions are always
preferred  in this case  if it is not possible that a task starts at the scheduled time  by reassigning its
start time to a closer greater neighbour  we are composing another schedule that is very similar to the
original one  therefore  the search of the feasible greater neighbours  which introduces buffers into
the tasks of the schedule  are improving both  the robustness and stability of the obtained schedules 
the search of schedules with buffers that are up to k time units can also be achieved with model
reformulation techniques  this is achieved by adding two variables to each original variable  the
variables that represent the start time of the tasks   one variable represents the slack that is following
the task and the other variable represents the sum of this slack and the original starting time  for
  

fic liment  wallace   s alido   barber

 

 

 

x 

 
 

slack

t 

 

x 

 
 a  solution space 

 

 

t 
 

 

slack
 

 

 b  schedule of the marked solution 

figure    robust schedule s     x       x       for example     and its greater neighbours for
k    

 

 

 

x 

 
 

slack

t 

 

x 

 
 a  solution space 

 

 

 

t 
 

 

 

 b  schedule of the marked solution 

figure    robust schedule s     x       x       for example     and its greater neighbours for
k     

 

 

 

x 

 
 

t 

 

x 

 
 a  solution space 

 

slack

t 
 

 

 

 

 

 b  schedule of the marked solution 

figure    robust schedule s     x       x       for example     and its greater neighbours for
k     

  

firobustness and s tability in c onstraint p rogramming under dynamism and u ncertainty

instance  let pi be the starting time of the task xi   thus  we would add the constraint pi   pi   si  
where si represents the slack associated to task xi   in addition  depending on the maximum desired
duration of the buffers  another constraint may be added  such as si  k  in this case  the delay is
up to k timep
units  in addition  an objective function that express the goal of maximizing the total
slack  max ni   si   must be defined 
furthermore  other proactive specific approaches for scheduling problems that do not involve
csp can be found in the herroelen and leus        survey  the main advantage of the approach
presented in this paper over these proactive alternatives for scheduling problems is that our approach
can be applied when all slack values require a consistency check  this requirement is necessary in
scheduling problems where intermediate non valid slack values are possible  examples of this type
of problem are scheduling problems with limited machine availability  see  for instance schmidt 
       in these cases  some machines are unavailable in certain time intervals  for this reason  tasks
that require these resources cannot be executed in such time units  the same happens with some
scheduling with operators  where the workers have some breaks during the day  moreover  there
also exist reactive approaches  which re schedule the activities when a disruption invalidates the
original schedule found  for example  solving dynamic resource constrained project scheduling
problems  rcpsp   elkhyari  gueret    jussien        

   experimental results
in this section  we present results from experiments designed to evaluate the performance of algorithm    solutions obtained by the restarting completion procedure are referred to as neighbour
solutions in the graphs and tables throughout this section  solutions obtained by restarting scratch
are referred to as neighbour solutions r   experiments were done with random problems and
benchmarks presented in the literature  the random instances generator  rbgenerator       the
benchmarks and the parser for the xcsp instances can be found on christophe lecoutres web
page    
in addition to assessing search algorithm    we also evaluated two other proactive methods
that do not require specific additional information about the dynamism  one of them is the wcsp
modeling technique  climent et al          which is based on the same dynamism assumptions
as in the present work  the solutions obtained by this technique are referred to as wcsp mod
solutions  we have not evaluated this approach with scheduling problems because it does not
consider the adaptation for scheduling problems that we have presented in section      neighbouring values that are lower in magnitude are not considered   the other proactive approach
maximizes the        repairability  see section       to implement this technique  we have modified algorithm    b b nk   by exchanging mgac  nk and gac  nk algorithms for mac  and
gac   hebrard         respectively  the solutions obtained by this technique are referred to as
       super solutions  in addition  solutions of an ordinary csp solver have been analyzed  referred to as simple solutions   in order to detect whether there are cases in which all solutions
have similar robustness and or stability 
in addition  we added the geometric restart  restarting completion  and bounds consistency techniques explained in section   to the ordinary csp solver and the super solutions solver in order to
provide them with these computational advantages  for the approach that models csps as wcsp 
   http   www cril univ artois fr  lecoutre index html

  

fic liment  wallace   s alido   barber

we have used the same solver as the one used by climent et al          toulbar     it was necessary
to use a different solver for the evaluation of this technique because this approach requires a wcsp
solver  for these other approaches evaluated  values were selected in lexicographical order and a
time cutoff was fixed to     seconds  experiments were run on an intel core i      processor      
ghz   in addition  for the geometric restart  the scale factor was fixed to    and the multiplicative
factor to     
the evaluation is based on the two main features of solutions obtained by proactive approaches 
stability and robustness  in all the tables of this section  the best robustness stability results obtained
are marked in bold  in accordance with the assumptions laid out in the previous sections  we use
the robustness and stability measures described in section    here  we note that the ordinary csp
solver and the super solutions solver do not consider the same dynamism assumptions as the wcsp
modeling technique and the approach presented in this paper  that is to say  they do not consider
possible future restrictive modifications over the bounds of the solution space of csps with ordered
domains  regarding stability  only the technique that maximizes the        repairability searches for
stable solutions according to definition      however  as mentioned above  in this paper we analyze
a more precise concept of stability for csps with ordered domains  the        c  repairability  see
definition      
    robustness analysis with general csps
in this section we analyze the robustness and stability of solutions obtained over a wide range of
tightness values  for this purpose  random csps were generated by the rbgenerator      which
have non convex constraints represented extensionally  because of the non convexity of the domains  the bounds consistency technique cannot be used  the csps generated have    variables
with domain size    and     binary constraints  domain values are integer values in the interval
         the tightness values analyzed are           and       note       is the critical value of the
tightness of this csp typology   for each tightness we generated    random instances that were
solved by algorithm   for k      because in this analysis we deal with the general case of csps
with ordered domains  see section      we have fixed the set of operators for our search algorithm
to                    and the value selection is heuristic    see section     which maximizes
 nk  x  v  s     starting from intermediate values 
as mentioned previously  it is not usually feasible to compute the complete set of solutions of
a csp  for this reason  in order to measure the robustness of the solutions obtained with the four
approaches  we have sampled the closest surrounding neighbours  k       thus  if a closest surrounding neighbour is not a solution of the csp  it means that the analyzed solution could become
infeasible after a change of magnitude one or greater to the original bound s that invalidate such
neighbour  on the other hand  if the neighbour is a solution of the csp  this means that this restrictive modification would not invalidate the analyzed solution  therefore  satisfiability checking of a
random sample of the neighbours of the solutions provides an estimation of the likelihood that the
solutions will remain valid  that is to say  an estimation of their robustness 
for sampling the feasibility of the neighbourhood of the solutions  we made a certain number
of random modifications of magnitude k over the values assigned to the variables of the solutions 
the number of values assigned to the variables of the solutions that are modified  is denoted as
nbv arm od                for each value of nbv arm od  we sampled     neighbours over the
   http   carlit toulouse inra fr cgi bin awki cgi toolbarintro

  

firobustness and s tability in c onstraint p rogramming under dynamism and u ncertainty

solution analyzed and checked their feasibility  the average number of feasible neighbours for each
type of solution are shown in table      it can be observed that algorithm   with either restarting
option dramatically outperformed the ordinary csp solver and the technique that maximizes the
       repairability  it also outperformed the wcsp modeling approach for tightness     and     
the weakness of the latter approach is that it obtains robustness approximations in problems in
which there is a high relation between constraints  because it computes feasible neighbours for
each constraint boundary  thus  the higher the tightness  the higher the likelihood of the existence
of neighbour tuples that are feasible for one constraint domain but not for another one  these
conflicting situations are less frequent in very unconstrained instances  for this reason  for tightness
    the performance of the modeling approach is better  however  it only obtains better robustness
results than algorithm   for highly unrestricted instances for high nbv arm od values  in regard
to our algorithm    the restarting completion option provides better results than restarting scratch
 differentiated with r  for very unconstrained instances  while they preform similarly for higher
tightness values  in figure   b  we selected the nbv arm od     to emphasize trends in robustness
and stability as a function of varying tightness 
tightness

   

   

nbv arm od

 

approach

average number of feasible neighbours in the sample

simple
super
wcsp m
neigh
neigh r 

   
   
     
     
     

 

   
   
    
  
    

 

 
 
    
    
    

 

 
 
    
    
   

  

 
 
   
   
   

 

   
 
   
    
    

   
 

 
 
   
   
 

 

 
 
 
 
   

 

  

 

 

 

 
 
 
 
   

 
 
 
 
 

 
   
   
   
   

 
 
   
   
 

 
 
 
 
 

table    robustness analysis based on the tightness                    tightness    
for the stability measurement            repairability is used  see definition       which measures the number of variables that can be replaced by a value located at a distance of one from the
value assigned without modifying the rest of values in the solution  stability results are shown in
figure   a   as mentioned  if a solution value is lost  the objective is to find the closest repairable
values  for this reason  our algorithm does not consider feasible values that are k units greater
or smaller than the value assigned  since this could result in future solutions where the manhattan
distance between the new solution and the original one would be exaggeratedly great  see section
    on the other hand  the technique that maximizes the        repairability considers any value
as a repairable value  this fact represents a disadvantage when searching for repairable values in
ordered domains  this can be observed in figure   a   where we can see the poor performance of
super solutions for the           repairability 
we would like to note that for csps that are very highly restricted  the stability and robustness
of the solutions obtained by all the evaluated methods are very similar  this is due to the fact that in
these cases the csps have very few solutions and consequently the distances of all solutions from
the bounds is very low  for most of these instances  the number of solutions is so low that the
solutions are scattered within the tuple space  so the likelihood of a solution being located on the
bounds of the solution space is very high  for the same reason  the likelihood that a variable has a
  

fic liment  wallace   s alido   barber

  
neighbours solution
neighbours  r  solution
simple solution
      super solution
wcsp mod solution

          repairability

  

  

  

 

 
   

   
tightness

   

average sampling of the number of neighbour solutions

 a  stability analysis
neighbours solution
neighbours  r  solution
simple solution
      super solution
wcsp mod solution

   

   

   

  

 
   

   
tightness

   

 b  robustness analysis for nbv arm od    

figure    combined robustness stability based on the tightness                    tightness     the
curves are shifted for improving the clarity of the graph 

  

firobustness and s tability in c onstraint p rogramming under dynamism and u ncertainty

feasible repairable value that is near by is very low  it can even be the case that none of the solutions
has an assignment with feasible neighbours located at a distance k  in this case  all the solutions are
equally robust and stable for this k value 
    scheduling benchmarks evaluation
in this section  we evaluate the approaches with scheduling benchmarks from the literature in order
to determine the robustness of schedules obtained over a wide range of k values  we analyzed five
sets of    job shop csp instances  studied by sadeh and fox         each instance is composed
of    jobs of five tasks each and there are five resources  each job has a random linear sequence
of resources to visit  with the exception of the bottleneck resources  which are visited after a fixed
number of operations  in order to further increase resource contention  
because this analysis deals with scheduling problems  see section    we have fixed the set of
operators for our search algorithm   to             and the value selection is done with heuristic
   in which values are selected in lexicographical order  regarding the other proactive technique
evaluated  the author of the        repairability approach  hebrard        made an extension to the
concept of breakage  the loss of an assigned value  for scheduling problems  a breakage for this
kind of problem was considered a delay of duration d in a task  therefore  only values that are
greater than the value assigned in d time units are considered repairable values  for this reason  for
the evaluation of scheduling problems  we have incorporated this condition to the        repairability
approach  for a proper comparison of this approach with our approaches  we used the same values
for k and d parameters  in the following  in order to avoid term repetition  we assume that d   k 
note that an ordinary csp solver does not use this parameter  so it obtains the same schedule for
any value of k 
for measuring the robustness of the schedules obtained  we used the robustness measures introduced in section      a first robustness assessment is made by measuring the total slack whose
duration does not exceed k  which is denoted as ts k   in addition  a more accurate measure is
s
 k   see equation     which measures the average total slack  minus the standard
also used  rslack
deviation multiplied by the  parameter  the  parameter was fixed to       which is inside the
interval that the authors consider appropriate for this parameter  another robustness measure used
s
is based on the resistance of a schedule faced with perturbations  and is denoted as rf z
 see equation     where z is the set of incidents that consist in delays of durations up to maxd over the tasks 
we have used   different values for maxd     and k  in each case  we independently simulated    
delays up to maxd units with equal probability over the entire schedule and checked if the schedule
remained valid  for the stability measurement  again            repairability is used  see definition
      which is equivalent to the measurement of the number of buffers of the schedule  denoted as
nbb  note that the desired objective is that in cases where repairs are necessary  the start time of a
task is delayed for as short a time as possible 
the following figures and tables show the evaluation for two of the sadeh problem sets  for the
other problem sets we obtained similar results  we show results for the e ddr  and e ddr  benchmarks in order to compare robustness and stability of schedules obtained with different numbers of
bottlenecks in the problem  other parameters are fixed   sadeh stated that the e ddr  benchmark
contained just one bottleneck and on the other hand  e ddr  benchmark contained two bottlenecks 
tables     and     show the means for the robustness and stability measures for scheduling problems  in addition  other measurements are shown  including the number of schedules obtained nbs 
  

fic liment  wallace   s alido   barber

total number of restarts done by the search algorithm nbr  the total number of nodes explored nbn
and the total number of failures nbf   figure   shows the stability and robustness measurements
s
 k  for the e ddr   the horizontal axis
 vertical axis   the mean number of buffers and mean rslack
of the figures represents the value of the ratio of parameters k 
k

approach

nbs

nbr

nbn

nbf

nbb

ts k 

s
rslack
 k 

s
rf z
   

s
rf z
 k 

 

simple
super
neigh
neigh r 

 
   
    
    

   
   
   
    

   
      
       
      

  
      
      
     

    
    
    
    

    
    
    
    

     
     
     
     

     
     
     
     

     
    
     
     

 

simple
super
neigh
neigh r 

 
   
    
    

   
   
   
    

   
      
      
      

  
      
      
     

    
    
    
    

    
    
    
    

     
     
     
     

     
     
     
     

     
    
     
     

 

simple
super
neigh
neigh r 

 
   
  
    

   
   
   
    

   
      
      
      

  
      
      
     

    
    
    
    

    
    
    
    

     
     
     
     

     
     
     
     

     
     
     
    

 

simple
super
neigh
neigh r 

 
   
    
    

   
   
   
    

   
      
      
      

  
      
      
     

    
    
    
    

    
     
     
     

     
     
   
     

     
     
     
     

     
     
     
     

 

simple
super
neigh
neigh r 

 
   
    
    

   
   
   
    

   
      
      
      

  
      
     
     

    
    
    
    

     
     
     
   

     
     
     
     

     
     
     
     

     
     
     
     

  

simple
super
neigh
neigh r 

 
 
  
   

   
   
   
    

   
      
      
      

  
      
     
     

    
    
    
    

     
     
     
     

     
     
    
     

     
    
     
     

     
     
    
     

table    evaluation of e ddr  benchmark 
as expected  schedules obtained by all of the approaches for the e ddr  benchmark are more
robust and stable than those for the e ddr  benchmark  see tables     and      from the robustness
analysis  we see that our algorithm for k       for both restarting options  increased the robustness
s  k  by more than     units for problems with only one bottleneck  therefore  as
measure rf z
expected  the fewer bottlenecks a scheduling problem has  the more robust the schedule obtained
by our algorithm  detailed results for all robustness measures are found under columns ts k  
s
s     and rs  k  in the tables  for instance  for the largest k value analyzed  k  
rslack
 k   rf z
f z
     the total sum of all the buffer times of duration up to k of the schedule obtained by algorithm
  for restarting completion is       time units for the e ddr  benchmark and        time units for
the e ddr  benchmark  more than    time units difference   regarding the stability analysis  our
algorithm for k     restarting scratch  differentiated with r  found schedules with four mean
number of buffers  nbb  more for the problems with one bottleneck than for the problems with two
  

firobustness and s tability in c onstraint p rogramming under dynamism and u ncertainty

k

approach

nbs

nbr

nbn

nbf

nbb

ts k 

s
rslack
 k 

s
rf z
   

s
rf z
 k 

 

simple
super
neigh
neigh r 

   
   
    
    

 
   
 
    

   
       
        
       

     
       
       
      

     
     
     
     

     
     
     
     

    
    
    
    

    
   
    
    

    
    
    
    

 

simple
super
neigh
neigh r 

   
   
    
    

 
   
   
    

   
       
       
       

     
       
       
      

     
     
     
     

     
     
  
     

    
    
    
    

    
    
   
   

    
    
    
    

 

simple
super
neigh
neigh r 

   
   
    
   

 
   
   
    

   
       
       
       

     
      
       
      

     
     
     
     

     
     
     
     

    
    
    
    

    
    
    
    

    
    
    
   

 

simple
super
neigh
neigh r 

   
   
    
   

 
   
   
    

   
       
    
       

     
      
       
      

     
     
     
     

     
     
     
     

    
    
    
    

    
    
    
    

   
    
    
    

 

simple
super
neigh
neigh r 

   
   
    
   

 
   
   
    

   
    
       
       

     
      
       
      

     
     
     
     

     
     
      
      

    
    
   
    

    
    
    
    

    
    
    
    

  

simple
super
neigh
neigh r 

   
   
    
   

 
   
   
    

   
        
       
       

     
       
       
   

     
     
     
     

     
     
      
      

    
    
    
    

    
   
    
    

    
    
    
   

table    evaluation of e ddr  benchmark 

  

fic liment  wallace   s alido   barber

  
neighbours solution
neighbours solution  r 
simple solution
      super solution

  

mean number of buffers

  
  
  
  
  
  
  
 

 

 

 

 

  

k

 a  stability analysis
   
neighbours solution
neighbours solution  r 
simple solution
      super solution

   

mean of rslacks k 

   
   
 
   
   
   
   
 

 

 

 

 

  

k

 b  robustness analysis

figure    combined robustness stability for k parameter  mean measures for the e ddr  benchmark 

  

firobustness and s tability in c onstraint p rogramming under dynamism and u ncertainty

bottlenecks for the best case  therefore  as expected  the fewer bottlenecks a scheduling problem
has  the more stable the schedule obtained by our algorithm 
in both tables and the figure  we can see that algorithm   with either restarting option outperformed both the ordinary csp solver and the technique that maximizes the        repairability 
furthermore  the analysis of the k parameter shows that when these parameters have the lowest values  the number of buffers of the schedules found by our algorithm are markedly greater than these
two techniques  see figure   a    in contrast  the improvement in robustness for our algorithm with
respect to the ordinary solver is a little more marked for greater k values  the comparison with the
       repairability technique shows the same tendency for the e ddr  benchmark  see nbb in table
     
regarding the other robustness measures that are not plotted in the figure but are shown in
s     measure and the number of
tables     and      we see that there is a correlation between the rf z
s     were
buffers  this relation is expected  since the random incidents generated for measuring rf z
delays of one unit time  therefore  the more buffers there are  whatever is their duration  the greater
s
the likelihood that a schedule can absorb delays of one time unit  in addition  the ts k   rslack
 k 
s
and rf z  k  measures are correlated  recall that ts k  is the total slack whose duration does not
s
 k  is its average minus the standard deviation multiplied by an  parameter 
exceed k and rslack
therefore  unless the distribution of the slack is very poor  the two values must be proportional 
s
 k   the greater the proportionality with respect the
note that the lower the  parameter for rslack
s
other two robustness measures  the rf z  k  measure is calculated by generating random delays
up to duration k over the schedule  for this reason  this robustness measure is strongly related
with the two aforementioned  a example of the relation of all the aforementioned measurement
units can be observed in table     for k       where the schedules obtained with restarting scratch
s     values  and schedules
option  differentiated with r  have greater numbers of buffers and rf z
s
s  k  values  this
obtained with restarting completion option have greater ts k   rslack
 k  and rf z
means that the latter has a greater total slack whose duration does not exceed k  but its distribution
is more limited 
in tables     and     we also observe measurements that are not correlated with robustness or
stability  but important information can still be extracted from them  for k      the restartingcompletion for our algorithm finds the greater mean number of solutions  nbs   only for k    
does the restarting scratch  differentiated with r  find more solutions  the greater k is  the easier
it is to find new solutions whose objective function is better than the maximum one  if the instance
is not highly restricted   for this reason  the mean number of solutions found is greater for high
k values  for both restarting options  the mean number of solutions is considerably higher than
for the approach that maximizes the        repairability  this effect is stronger for greater values
of k because the condition of a repairable value for the latter technique becomes more restrictive 
moreover  this technique considers all feasible values in the domains as repairable values  as a result 
feasibility checking is slower than for techniques that assume only k neighbours  as our technique
does   as expected  the mean number of restarts  nbr  is much greater for the restarting scratch
option because the other techniques only restart until finding the first solution  as a consequence 
their mean number of nodes explored  nbn   and mean number of failures  nbf   is lower 
the schedules obtained by algorithm   for the lowest k value had the highest number of buffers 
on the other hand  the robustness measures are greater for the greater k values  depending on the
dynamic nature of the problem  it would be desirable to prioritize between a higher number of
buffers of short duration and a lower number of buffers of long duration  if the two features cannot
  

fic liment  wallace   s alido   barber

both be maximized   thus  if it is known that the possible future delays will have a duration of at
least d time units  it does not make sense to compute k values lower than d because the obtained
time buffers could not absorb the delay  on the other hand  if it is known that possible future delays
cannot have a duration greater than d  then it does not make sense to compute k values greater than
d time units because this may decrease the number of buffers  hence  the more information about
possible future changes we have  the better the robustness results we can obtain  however  even
if this information is unknown  we can obtain a schedule with certain level of both robustness and
stability by setting k to an intermediate value in algorithm   
  
neighbours solution
neighbours solution  r 
simple solution
      super solution

  

mean number of buffers

  
  
  
  
  
  
  
  
  

  

  

  

  
  
time s 

  

  

  

   

  

  

  

   

 a  k    
  
neighbours solution
neighbours solution  r 
simple solution
      super solution

  

mean number of buffers

  
  
  
  
  
  
  
  
  

  

  

  

  
  
time s 

 b  k    

figure    mean number of buffers over the time intervals for the e ddr  benchmark 
the above evaluation consists of analyzing the best results obtained for each technique for the
fixed cutoff time  however  we also wanted to analyze the change in the degree of robustness and
stability of the schedules found over the time  for this evaluation  we used the e ddr  benchmark
and determined the mean for    instances for each interval of time with a discretization of   
  

firobustness and s tability in c onstraint p rogramming under dynamism and u ncertainty

seconds  figures   a  and   b  show the mean number of buffers found by each approach for k    
and k      other measures are not shown since similar trends were found in these cases  we would
like to highlight that after    seconds the simple solution technique does not find better schedules
because it only searches for one schedule for each instance  which is done in less or equal to   
seconds   the most remarkable aspect is that for k     algorithm   for both restarting options
obtains a greater number of buffer times than the approach that maximizes the        repairability 
for k     for all time intervals  see figure   a   
on the other hand  figure   b   which represents k      shows more unstable results  since it is
difficult to find buffers with up to seven time units  it may happen than our algorithm sacrifices some
shorter buffers in order to find one buffer of seven time units  thus  even if the overall tendency
is for the measure to increase over the time  it is not entirely uniform  on the other hand  the
upward shape of the trend for the approach that searches for super solutions is due to the fact that
it considers values as repairable if there is any possible alternative for the start time of a task that
follows a task sharing the same resource  which is not equivalent to have a slack associated to this
task in the schedule  for this reason  schedules that are better for this technique may contain lower
number of buffers  this feature is more marked for greater values of k  since the repairable values
have to be at least k unit times greater than the assigned values  and therefore it is more unlikely to
find repairable values that are close to the assigned ones 
it can be concluded that in general the approach that maximizes the        repairability finds
solutions with lower robustness and stability  considering the closest repairable values  than our
approach for the aforementioned reason  another disadvantage is that it only assumes that delays
are of duration d  thus  only values greater than this value are considered as repairable values 
however  we consider up to k neighbours and therefore  slacks of duration lower than k are also
valued by our objective function in contrast to the        repairability objective function 
on the basis of this evaluation  we can conclude that the difference in performance between the
two restarting options  restarting completion and restarting scratch  is not very significant  sometimes  the time needed to restart from scratch after each solution makes this option less effective
than restarting completion  in other cases  the restarting completion option loses time in branches
in which there are no better solutions  while restarting scratch explores other branches  for instance 
for the random experiments  restarting completion provided slightly better results generally  see table     and figure   b    while for the scheduling problems  restarting scratch obtained schedules
that were a bit more robust and stable for lower k values  see k         in figure     for greater k
values  both restarting options gave similar results 

    conclusions
in this paper we extend the concept of robustness and stability for csps with discrete and ordered
domains where only limited assumptions can be made about changes in these problems  in particular  there are no uncertainty statistics nor probabilities about the incidences that can occur in the
original problem  in this context  it is reasonable to assume that the original bounds of the solution
space may undergo restrictive modifications  such as introduced by climent et al          therefore  the main objective in searching for robust solutions is to find solutions that maximize all the
euclidean distances from the dynamic bounds of the solution space  on the other hand  the main
objective in searching for stable solutions in terms of repairable variables is to find solutions whose
repairable values are as close as possible to the broken assignments 
  

fic liment  wallace   s alido   barber

in this paper  we present a new search algorithm that combines criteria for both robustness
and stability in this framework  the algorithm developed in this paper searches for a solution
that maximizes the sum of contiguous feasible surrounding neighbours at distances of k or less
from the values of the solution  the obtained solutions have a high probability of remaining valid
after possible future restrictive changes over the constraints and domains of the original problem
 robustness criterion   and they also have a high number of variables that can be easily repaired with
a value at a distance lower or equal to k if they undergo a value loss  stability criterion  
we have evaluated the new algorithm in experiments on well known scheduling benchmarks
as well as random csps  we have shown that both versions of the new algorithm outperform
three other approaches evaluated  the ordinary csp solvers  the technique that maximizes the
       repairability  and the approach that models csps as wcsps under many conditions where
there are real differences in the robustness of solutions that might be obtained  the latter occurs
when the problem is not so constrained that there are only a few valid solutions  with respect
to the two restarting options developed for our algorithm  we found that their performance is not
significantly different  although in certain situations there is an advantage of one over the other 
for slightly constrained csps  our algorithm obtains solutions with the greatest number of closer
neighbour solutions  the greatest           repairability and highest values for specific measures of
scheduling robustness  furthermore  we have shown that by increasing k for large problems  we
can also increase the robustness  although it may happen that           repairability decreases  for
instance  with scheduling problems the schedules obtained with lower k values tend to maximize
the number of buffers even if their size is small  however  the computation of higher k values tends
to give priority to the duration of the buffers and as consequence  the number of buffers obtained
can be lower  therefore  depending on the dynamic nature of the problem  it would be desirable to
prioritize between a higher number of short buffers or a lower number of long buffers  if it is not
possible to maximize both features  
the extension of the robustness and stability definition for csps with discrete and ordered
domains and the development of a search algorithm for finding robust and stable solutions in this
context  are useful and practical in many real life situations where problems can undergo restrictive
changes and there is the added difficulty that information about the possible future changes is limited
or non existent  even under these difficult conditions  our search algorithm is able to provide stable
and robust solutions  finding solutions located far away from the dynamic bounds is important
when we face restrictive modifications over the bounds of the solution space  moreover  in cases
where a value is lost  it is important to replace it by a nearby value in order to have a solution as
similar as possible to the original one  this closeness feature is handled by our algorithm but is not
by the approach that searches for super solutions 

acknowledgments
this work has been partially supported by the research project tin           c      and fpu
program fellowship  min  de ciencia e innovacion  spain   we wish to thank dr  christophe
lecoutre and dr  diarmuid grimes for their assistance 
  

firobustness and s tability in c onstraint p rogramming under dynamism and u ncertainty

references
bessiere  c          constraint propagation  foundations of artificial intelligence          
boussemart  f   hemery  f   lecoutre  c     sais  l          boosting systematic search by weighting constraints  in proceedings of the   th european conference on artificial intelligence
 ecai      vol      p      
climent  l   wallace  r  j   salido  m  a     barber  f          modeling robustness in csps as
weighted csps  in proceedings of the   th international conference on integration of artificial intelligence and operations research techniques in constraint programming  cpaior     pp       
elkhyari  a   gueret  c     jussien  n          constraint programming for dynamic scheduling
problems  hiroshi kise  editor  iss   international scheduling symposium  pp       
escamilla  j   rodriguez molins  m   salido  m   sierra  m   menca  c     barber  f          robust solutions to job shop scheduling problems with operators  in   th ieee international
conference on tools with artificial intelligence  ictai      pp         
fargier  h     lang  j          uncertainty in constraint satisfaction problems  a probabilistic approach  in proceedings of the symbolic and quantitative approaches to reasoning and uncertainty  ec sqaru      pp        
fargier  h   lang  j     schiex  t          mixed constraint satisfaction  a framework for decision
problems under incomplete knowledge  in proceedings of the   th national conference on
artificial intelligence  aaai      pp         
fowler  d     brown  k          branching constraint satisfaction problems for solutions robust
under likely changes  in proceedings of the international conference on principles and
practice of constraint programming  cp        pp         
fu  n   lau  h   varakantham  p     xiao  f          robust local search for solving rcpsp max
with durational uncertainty  journal of artificial intelligence research           
hebrard  e          robust solutions for constraint satisfaction and optimisation under uncertainty  ph d  thesis  university of new south wales 
hebrard  e   osullivan  b     walsh  t          distance constraints in constraint satisfaction  in
proceedings of the   th international joint conference on artificial intelligence  ijcai     
pp         
herroelen  w     leus  r          project scheduling under uncertainty  survey and research potentials  european journal of operational research                 
kitano  h          towards a theory of biological robustness  molecular systems biology       
larrosa  j     schiex  t          solving weighted csp by maintaining arc consistency  artificial
intelligence           
leon  v   wu  s     robert  h          robustness measures and robust scheduling for job shops 
iie transactions              
lhomme  o          consistency techniques for numeric csps  in proceedings of   th the international joint conference on artificial intelligence  ijcai      vol      pp         
  

fic liment  wallace   s alido   barber

mackworth  a       a   consistency in network of relations  artificial intelligence           
mackworth  a       b   on reading sketch maps  in proceedings of the  th international joint
conference on artificial intelligence  ijcai      pp         
mohr  r     henderson  t  c          arc and path consistency revisited  artificial intelligence 
              
rossi  f   venable  k     yorke smith  n          uncertainty in soft temporal constraint problems 
a general framework and controllability algorithms for the fuzzy case  journal of artificial
intelligence research                
sadeh  n     fox  m          variable and value ordering heuristics for the job shop scheduling
constraint satisfaction problem  artificial intelligence             
schmidt  g          scheduling with limited machine availability  european journal of operational
research              
surico  m   kaymak  u   naso  d     dekker  r          hybrid meta heuristics for robust
scheduling  erim report series reference no  ers          lis  available at ssrn 
http   ssrn com abstract        
verfaillie  g     jussien  n          constraint solving in uncertain and dynamic environments  a
survey  constraints                
wallace  r     freuder  e          stable solutions for dynamic constraint satisfaction problems 
in proceedings  th international conference on principles and practice of constraint programming  cp      pp         
walsh  t          search in a small world  in proceedings of the international joint conference on
artificial intelligence  vol      pp           
walsh  t          stochastic constraint programming  in proceedings of the   th european conference on artificial intelligence  ecai      pp         
yorke smith  n     gervet  c          certainty closure  reliable constraint reasoning with incomplete or erroneous data  journal of acm transactions on computational logic  tocl  
         

  

fi