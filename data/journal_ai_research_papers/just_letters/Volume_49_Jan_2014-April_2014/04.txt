journal of artificial intelligence research                  

submitted        published      

comparative evaluation of link based approaches for
candidate ranking in link to wikipedia systems
norberto fernandez garca
jesus arias fisteus
luis sanchez fernandez

berto it uc m es
jaf it uc m es
luiss it uc m es

telematics engineering department
universidad carlos iii de madrid
avda  universidad      e      
leganes  madrid  spain 

abstract
in recent years  the task of automatically linking pieces of text  anchors  mentioned in
a document to wikipedia articles that represent the meaning of these anchors has received
extensive research attention  typically  link to wikipedia systems try to find a set of
wikipedia articles that are candidates to represent the meaning of the anchor and  later 
rank these candidates to select the most appropriate one  in this ranking process the
systems rely on context information obtained from the document where the anchor is
mentioned and or from wikipedia  in this paper we center our attention in the use of
wikipedia links as context information  in particular  we offer a review of several candidate
ranking approaches in the state of the art that rely on wikipedia link information  in
addition  we provide a comparative empirical evaluation of the different approaches on
five different corpora  the tac      corpus and four corpora built from actual wikipedia
articles and news items 

   introduction
due to the important volume of information contained in wikipedia  but also to the open
nature of this content  the on line encyclopedia has been adopted in recent times as a useful
resource for computational linguistics tasks like name translation  lin  snover    ji        
named entity recognition  nothman  murphy    curran         etc 
the development of automatic link discovery systems  erbs  zesch    gurevych       
is another area of research where wikipedia has had an important impact  the task of
discovering links to wikipedia articles has been addressed  with slight variants and under
different names  by different communities  for instance  hachey et al         distinguish
between named entity linking  addressed in the context of the knowledge base population
 kbp  track  national institute of standards and technology      b  at the text analysis
conference  tac   national institute of standards and technology      a   and wikification  addressed in the link the wiki track at the initiative for the evaluation of xml
retrieval  inex   inex         in both cases the goal is to automatically find wikipedia
articles that represent the meaning of a certain piece of text in the document and define
a link to wikipedia using as anchor that piece of text  however  there are differences in
aspects like the anchors considered  only named entities in named entity linking  named
c
    
ai access foundation  all rights reserved 

fifernandez garca  arias fisteus   sanchez fernandez

entities and common terms in wikification  or in whether the wikipedia is considered as a
complete source of knowledge  wikification  or not  named entity linking  
henceforth  we will simply refer as link to wikipedia to the general task of discovering
links to wikipedia  which includes both wikification and named entity linking as particular
cases 
according to erbs et al          the task of discovering links can be divided into a series
of steps  they include  identifying the anchors to be linked  searching for candidate link
targets for each anchor  and selecting the best candidate from the results of the searching
step  it is common that link to wikipedia approaches address these steps independently
and sequentially  though there are also examples where the steps are not independent 
like cucerzan       or sil        
due to its important role  ji  grishman    dang         in the context of this paper
we will center our attention in the last of the aforementioned processes  it is referred
to as disambiguation by hachey et al          however  as selecting the best link target
usually involves creating a ranking of all the candidates to choose the one with the highest
rank  other authors refer to the process as target ranking  erbs et al         or candidate
ranking  guo  tang  che  liu    li        ploch  hennig  de luca    albayrak        ji
et al          in this article  we will also adopt the term candidate ranking 
in order to select the best wikipedia article to link from a given anchor  the candidate
ranking process relies on the context information provided by a set of features  these
features are extracted from the document where the anchor is placed  the context document 
and or the different wikipedia articles considered as candidates to become the link target 
according to erbs et al         the features can be classified into three groups      those
extracted from the text of the document articles      those extracted from their titles  and
    those based on existing links  the latter are the subject of study in this paper 
traditional research in the area of computational linguistics has shown the effectiveness of using wordnet  miller        graph links for tasks like computing semantic relatedness  budanitsky   hirst        or performing word sense disambiguation  navigli  
lapata         in the case of link discovery  erbs et al         indicate that  when enough
training information is available  link based approaches can outperform text based ones 
taking this into account  it is not surprising to find many link to wikipedia approaches
that use features for candidate ranking based only on information about links  some examples are the work of milne and witten      b   pilz         radford et al          fernandez
et al          guo et al          ploch et al         and ratinov et al         
given the ample variety of link based features for candidate ranking described in the
state of the art  a comparative analysis of the different alternatives can be useful to decide
which approach  or approaches  should be considered when designing link to wikipedia
systems  however  using only the results published in the state of the art it is difficult
to compare across systems the ranking performance of the different link based approaches 
first  link to wikipedia systems are usually evaluated in an end to end setup  that is  the
evaluation involves not only the ranking stage  but also the candidate searching and the
candidate selection processes  thus  the impact in performance of the different system
components is mixed  second  in general  link to wikipedia systems do not only rely on
link based features to rank the candidates  but also combine them with features of the other
types  thus  the effects of the different contributions to the ranking process are also mixed 
   

fievaluation of link based approaches for candidate ranking in link to wikipedia

taking this into account  the main goals of this paper are twofold      offer an overview of
link based approaches for candidate ranking in link to wikipedia systems  and      perform
an empirical evaluation to compare these approaches  in order to address the aforementioned difficulties  we      focus our analysis on the candidate ranking stage  isolating it
as much as possible from the candidate search selection stages  and     consider only linkbased features  not combined with those based on text or titles  a similar comparison is 
to the knowledge of the authors  not available at the time of writing 
the rest of this paper is organized as follows  section   presents some definitions and
a formal description of the problem to be addressed  section   outlines the different linkbased approaches to be compared  section   describes the setup of the empirical evaluation
we have carried out  as well as its results  section   offers an overview of related work 
finally  section   closes this paper with concluding remarks and future lines of work 

   definitions and problem formalization
in this section we introduce some definitions and nomenclature that will be helpful in the
rest of the article 
the textual document that mentions the anchor a that is going to be linked to wikipedia
is named context document and will be represented by dc  
the set of all the wikipedia articles will be denoted by w   whereas particular wikipedia
articles will be represented by wi   i               w    in our case we will consider in the set w
only the wikipedia pages that belong to the main namespace  wikipedia      b  and that
represent non ambiguous concepts  that is  disambiguation pages will be filtered out  
we will denote as c dc   a  the set of wikipedia articles  c    c            c c dc  a      ck  w
that are selected as candidates to fit the meaning of a in dc  
a link l can be defined as a duple l    src l   dest l    where src l  represents the
document that is the source of the link and dest l  is the document that is pointed by the
link  that is  its destination 
we will denote as f  d  the set of documents that are destination of the forward links
from d  that is  f  d     f   l  src l    d  dest l    f    similarly  we will represent as
b d  the set of documents that are the source of the backward links of d  that is  b d   
 b   l  src l    b  dest l    d   in this paper  we use only the information provided
by wikipedia links  thus  we will only consider wikipedia articles as members of f  d 
and b d   note also that both f  d  and b d  are sets and  thus  they do not consider
duplicates  however  it might happen that a document has several links pointing to the
same destination  in order to represent this information  we will denote the number of links
that have as source the document s and as destination the document d as n s  d   
taking into account the aforementioned definitions  the candidate ranking process to be
addressed in the context of this paper may be formalized as follows 
definition    given a context document dc   which mentions an anchor a  and a set of
candidate wikipedia articles c dc   a   the candidate ranking task consists on ordering the
members of c dc   a  according to a rating  this rating measures the suitability of each
candidate to represent the meaning of the anchor  the candidate ci  c dc   a  with the
highest rank  which fits best with the meaning of the anchor a in the context of the document
dc   is then selected to define a new link  dc   ci   
   

fifernandez garca  arias fisteus   sanchez fernandez

a few aspects should be stressed from this definition 
 we do not introduce any restriction on the nature of the anchors to be linked  in
particular  they may represent either named entities  persons  organizations  etc   or
common terms 
 as in some previous related work  cucerzan        mihalcea   csomai        han 
sun    zhao         we do not address in this paper the scenario where an adequate
wikipedia article to be linked to a does not exist 

   overview of approaches
we describe here the different candidate ranking approaches that are evaluated in this
paper  they have in common that their only source of context information are links  in
particular  the links considered are those available in the context document  dc   as well
as those that constitute the link structure of wikipedia  including the links to from the
candidate wikipedia articles  ci  c dc   a  
however  not all the approaches that are considered use the link information in the
same manner  in particular  we classify them in two families  which we name     bag oflinks approaches  and     graph approaches  the main difference between them is that in
the second group the links are used to build a graph structure  which is later analyzed to
select the best candidate  this is not the case of the approaches in the first group 
    bag of links approaches
in this section we present a set of approaches with a characteristic in common  they do not
rely on building a graph with the link context information to rank the candidates 
nevertheless  the approaches in this family have also differences in the way they address
the task  in particular  we can distinguish at least three alternative groups 
 some approaches rely on similarity metrics that compute a similarity score between
the context document and each candidate  to later select the candidate with the
highest score  the most similar one  
 another alternative is to rely on popularity metrics  which simply try to compute a
popularity score for each candidate  the most popular candidate is selected  these
approaches do not rely on the information provided by the context document 
 the ranking process can also be modeled as a statistical problem  statistical methods
are used to select the most likely candidate  given the context information 
in accordance with this classification  the following sections describe each group of approaches 
      similarity metrics
the first similarity metrics we consider is the relatedness  computed on the basis of the
wikipedia link based measure  milne   witten      a   the relatedness is used as feature
   

fievaluation of link based approaches for candidate ranking in link to wikipedia

in link to wikipedia approaches such as that of milne and witten      b   han and zhao
        kulkarni et al          pilz         fahrni et al          han et al         and ratinov
et al         
basically  the relatedness allows to compute the similarity between two wikipedia documents wi   wj from the links they have in common  in the original definition by milne and
witten      a   it can be computed as 
rb  wi   wj    

log max  b wi      b wj       log  b wi    b wj    
log  w     log min  b wi      b wj     

   

according to milne and witten      a   the relatedness metrics is based on the normalized google distance  ngd   defined by cilibrasi and vitanyi         the ngd is based
on the intuition that terms that have a similar or related meaning co occur frequently in
documents  thus  given a pair of terms  the google search engine can be used to obtain
pages which mention these terms  pages that mention both of them indicate relatedness 
while pages with only one of them suggest unrelatedness  as indicated by milne and witten
     a   the relatedness metrics  as defined in equation    shares the same inspiring principle  but uses wikipedia links instead of google search results to account for mentions 
being a distance metrics  relatedness values are expected to be smaller the more similar
the wikipedia articles are  however  it is easy to transform the distance metrics into a
similarity metrics following the approach of gracia and mena         which requires the
computation of 
simrb  wi   wj     e rb  wi  wj  

   

a second similarity metrics between wikipedia articles to be considered is based on
computing the pointwise mutual information  pmi  between the sets of links in the articles
to be compared  for instance  it is used by ratinov et al         for the the link to wikipedia
task  it is defined in that work as 
pb  wi   wj    

 b wi    b wj     w  
  b wi     w     b wj     w   

   

note that the definitions of equations     and     rely on backlinks  b x   for computation  however  as indicated by ratinov et al          both the relatedness and the pmi
can also be computed using the outgoing links from a document  in this paper we explore
and compare both alternatives and denote the relatedness similarity and the pmi computed
with forward links as simrf and pf respectively 
taking the aforementioned definitions into account  for each wikipedia article  c            ck  
 c dc   a  we can compute its relatedness and pmi with each of the wikipedia articles linked
from dc   that is  with the fj  f  dc    combining these different values we obtain the final
relatedness and pmi between ci and dc   according to ratinov et al         several ways
to combine the values may be followed  such as taking their average or the maximum  we
explore these different possibilities in the paper  in particular  for the relatedness 
   

fifernandez garca  arias fisteus   sanchez fernandez

relfa  ci   dc    
a
relb
 ci   dc    

 
 f  dc   
 
 f  dc   

relfm  ci   dc  

 

m
relb
 ci   dc    

x

simrf  ci   fj  

   

x

simrb  ci   fj  

   

max

simrf  ci   fj  

   

max

simrb  ci   fj  

   

fj f  dc  

fj f  dc  
fj f  dc  
fj f  dc  

whereas the pointwise mutual information can be computed as 
p m ifa  ci   dc    

 
 f  dc   

 
a
p m ib
 ci   dc    
 f  dc   
p m ifm  ci   dc  

 

m
p m ib
 ci   dc    

x

pf  ci   fj  

   

x

pb  ci   fj  

   

max

pf  ci   fj  

    

max

pb  ci   fj  

    

fj f  dc  

fj f  dc  
fj f  dc  
fj f  dc  

another well known approach to compute document similarity within the natural language processing and information retrieval communities is the cosine similarity  basically  a
vector is built to represent the context document and each candidate article  then  the similarity between the context document and each candidate is computed as the cosine of the
angle between the respective vectors  several approaches in the state of the art  bunescu
  pasca        fader  soderland    etzioni        nguyen   cao        fahrni et al  
      ploch et al         ratinov et al         use the cosine similarity  however  there are
differences between them in the features used to build the vector representations for the
documents 
in our case  we have the requirement of considering solely links as context information 
thus  each document will be represented using only the links that are mentioned in that
document  a similar approach to model documents and compute their cosine similarity is
used  for instance  by fahrni et al         and ploch et al         
in particular  a document d is represented as a vector vd  r w     each component
vd i   i               w   of the vector vd can be computed with the traditional term frequency
 tf   inverse document frequency  idf  product  manning  raghavan    schtze        as
follows 
vd i   t f  d  wi    idf  wi     p

 w  
n d  wi  
 log
 b wi   
wj f  d  n d  wj  

    

note that if f  d  does not contain a certain wikipedia article wi   then n d  wi       
t f  d  wi       and  thus  vd i      due to this  the vector vd is expected to be sparse 
   

fievaluation of link based approaches for candidate ranking in link to wikipedia

given two documents to be compared  for instance  dc and a wikipedia article ci 
c dc   a    the cosine similarity metrics is computed as the cosine of the angle between the
vectors of the two documents  as follows 
simcos  vci   vdc    

vdc  vci
  vdc       vci    

    

finally  radford et al         suggest a metrics based on the wikipedia link structure 
which can also be interpreted as a similarity metrics  in order to compute this metrics  the
following equation is computed for each candidate ci  c dc   a  
simr  ci   dc     log  b ci    ldc           

    

where ldc is the set built by the union of all the backlinks of all the wikipedia articles
linked from dc  
ldc  

 

b fi  

    

fi f  dc  

all the aforementioned similarity metrics can be trivially used to address the candidate
ranking process  the candidate ci  c dc   a  to be selected as link destination has a
maximal similarity with the context document dc  
arg max simf  ci   dc   
ci

    

a   relm   p m i a   p m i m  
where simf represents one the functions  relfa   relfm   relb
b
f
f
a   p m i m   sim
p m ib
cos   simr  
b

      popularity metrics
algorithms based on popularity metrics constitute the second group of the bag of links
family 
a first approach that could be used to compute the popularity of a certain candidate 
ci  c dc   a   is simply counting the number of wikipedia articles that link to it  that is 
its indegree   b ci    or  alternatively  the number of wikipedia articles linked from it  its
outdegree   f  ci     these metrics are considered  for instance  in the work of dredze et al 
        guo et al         and cao et al         
fader et al         describe a popularity score that is also based on the incoming links
from wikipedia to a candidate ci  
 b ci   
  
    

where  is a parameter that is set to        fader et al         
finally  the degree centrality of a certain wikipedia candidate article ci can also be
considered as a bag of links popularity metrics  hachey et al         define the degree
centrality as 
popf  ci          log    

d ci    

 b ci   
 w     

   

    

fifernandez garca  arias fisteus   sanchez fernandez

note that  as indicated at the beginning of this section  the aforementioned popularity
metrics do not take into account the information provided by the context document  all of
them depend only on information obtained from the candidates 
all the aforementioned metrics can be used to rank the candidates by popularity  then 
the most popular candidate ci  c dc   a  is selected as the link destination  taking into
account that all the functions of  b ci    involved in the aforementioned approaches  linear 
logarithm  are monotonically increasing functions  the order  ranking  provided in all the
cases should be the same  due to this  in the context of this paper  we consider for evaluation
the indegree and outdegree only 

arg max indegree ci   

    

arg max outdegree ci   

    

ci

ci

      statistical techniques
the candidate ranking process that we are addressing in the context of this paper can also
be mathematically modeled using statistical techniques  as has been suggested in the work
of fader et al         and han and sun        
in our particular case  considering the set of wikipedia articles linked from dc   f  dc    
 f            f f  dc      as input features  the destination for a can be computed by selecting the
wikipedia article ci  c dc   a  that maximizes the conditional probability 
p  ci  f            f f  dc      where fi  f  dc  

    

if the number of features  f  dc    to be considered is relatively large  estimating the values
of the conditional probability in equation      for each ci would be a complex problem  due
to this  in practice  the problem is reformulated to make it more treatable  in particular 
    the bayes rule is used to reverse the conditional probability in equation       and     
it is assumed that the features  links in f  dc   in our case  are conditionally independent
 naive bayes assumption  
the result of this problem reformulation is known in the state of the art as the naive
bayes classifier  manning et al          in our specific scenario  this classifier should be
able to distinguish which of the classes  the different ci  c dc   a   is the most likely for
the anchor a 
mathematically  the expression that we use to select the best ci using the maximum a
posteriori decision rule  manning et al         is 
 f  dc   

arg max n b ci   dc      arg max log p  ci    
ci

ci

x

n dc   fj   log p  fj  ci   

    

j  

where the logarithm function is used to avoid underflows  as suggested in manning et al  
      
in order to compute the values in equation      for each ci   we need to know the value
of two probabilities      the prior probability of class ci   p  ci    and      the conditional
   

fievaluation of link based approaches for candidate ranking in link to wikipedia

probabilities p  fj  ci    to estimate these two probabilities we follow the approach described
by manning et al         
p
bj b ci   n bj   ci  
p
    
p  ci     p
wi w
fj f  wi   n wi   fj  

that is  p  ci   represents the maximum likelihood estimate  mle  of the probability
that a certain document contains a link to ci   computed by dividing the number of actual
links to ci by the total number of links in wikipedia 
p
    bi b ci   n bi   fj  
p
p  fj  ci     p
    
wj w     
bi b ci   n bi   wj   

in this case  p  fj  ci   represents the probability of having an anchor linking to fj when
the document already contains a link to ci   again  the mle is also used for the conditional
probabilities and  thus  the probabilities are computed by dividing the number of links to
fj in documents that contain a link to ci by the total number of links in documents that
contain a link to ci   it can be seen that the mle is smoothed using the laplace smoothing
to avoid zeros 
    graph approaches
the second family of link based approaches for candidate ranking we consider are the graph
approaches  which rely on building a graph and processing it to select the best candidate 
      pagerank and personalized pagerank
the first algorithm that we consider within the graph family is pagerank  first defined
by page et al          and widely known due to its use as part of the google search engine 
examples of application of pagerank as a method for candidate ranking can be found for
instance in the work of fernandez et al          dredze et al         and hachey et al 
       
basically  pagerank is an algorithm that can be used to compute the popularity of a
certain page  taking into account the popularity and number of pages that link to it  using
the mathematical formulation described by brin and page        in the particular scenario
that we are addressing in this paper  to compute the popularity p r wi   of a wikipedia
article wi   we need to solve the following equation 
p r wi    

    d 
 d 
 w  

x

wj b wi  

 
p r wj    
 f  wj   

    

where d is a damping factor which can be set between   and    but is typically set to
     according to brin and page        and hachey et al         
note that  according to equation       we are only taking into account links to from
wikipedia  because computing pagerank in the general scenario requires complete information of the link structure of the web  which is a computationally expensive problem  the
same simplification is also assumed by fernandez et al         and hachey et al         
   

fifernandez garca  arias fisteus   sanchez fernandez

note also that  as it happens with the popularity metrics described in section        the
pagerank metrics does not depend on the context information in dc   but only on the graph
built from the link structure of wikipedia  however  the context information in dc can be
included in the process by using a variant of the algorithm known as personalized pagerank
or topic sensitive pagerank  haveliwala         this algorithm is used for instance by yeh
et al         to define a semantic relatedness metrics 
the main difference between classical pagerank and personalized pagerank is that 
instead of relying on a uniform damping vector  it is biased to give more relevance to a
given set of resources  haveliwala         in our particular case  these resources are the
articles linked from dc   that is  the members of f  dc    in practice  equation      is adapted
as follows to compute the personalized pagerank 

p p r wi   dc    









d 

x

 
p p r wj   dc    if wi 
  f  dc  
 f  wj   

x

 
p p r wj   dc    if wi  f  dc  
 f  wj   

wj b wi  






     d   t f  dc   wi     d   

wj b wi  

where t f  dc   wi   represents the term frequency of the link to wi in the context of the
document dc   computed as indicated in equation      
once the pagerank and personalized pagerank values are computed  they can be used
to rank the candidates  the article ci  c dc   a  with the highest p r ci   or p p r ci   dc  
value is then selected as the link destination 

arg max p r ci   

    

arg max p p r ci   dc   

    

ci

ci

      random walk
several works in the state of the art  gentile et al         fernandez et al         han et al  
      ploch et al         jimenez et al         define techniques to link several anchors in the
same context document at the same time  usually  these approaches address the candidate
ranking process by building a graph and computing a random walk  spitzer        over that
graph to rank its nodes 
though these approaches share the same underlying principle  there are differences
between them mainly in two aspects  the nature of the nodes to be considered as part of
the graph and the nature of the edges  for instance  gentile et al         indicate that
the nodes represent either concepts  candidates  or features  like words in the title of a
certain candidate   and the edges link the candidates with their specific features  han et al 
       define nodes for each anchor to be linked and for its candidates  the edges link each
anchor with its candidates but also the candidates among themselves on the basis of their
relatedness  see section         in the work of fernandez et al         the nodes include only
the candidates  and the edges are defined on the basis of information about co occurrence
   

fievaluation of link based approaches for candidate ranking in link to wikipedia

of candidates in wikipedia articles  a similar approach is used by ploch et al         
including nodes for the candidates and edges defined on the basis of wikipedia links 
note that the pagerank metrics can also be interpreted as a random walk  page et al  
       however  while pagerank  as described in section        operates on the graph of the
whole link structure of wikipedia  the approaches in this section build their own graphs 
typically much smaller and tailored to the concrete scenario to be addressed 
the approaches of gentile et al         and han et al         rely on text based features
to build their graphs  in the work of gentile et al         these features are used as nodes
in the graph  whereas han et al         use a text based similarity metrics to compute the
weights of the edges connecting each anchor with its candidates  due to this  in the context
of this paper we evaluate the approaches of fernandez et al         and ploch et al         
which rely only on link information 
as indicated above  fernandez et al         and ploch et al         designed their
approaches to link at the same time several anchors in the same context document  thus 
we need to adapt these approaches to the scenario addressed in this paper  where only
an anchor a is considered  to do so  each element in f  dc   is treated as a single element
pseudo candidate set for an anchor ai in dc with i               f  dc    
to compute the score for each candidate ci  c dc   a  according to ploch et al        
 that we name rwp  ci   dc     we build a graph having as nodes all the candidates and
pseudo candidates  that is  the elements in c dc   a  plus the wikipedia articles linked in
f  dc     an edge between two nodes appears when there is a link in wikipedia between
the articles represented by the nodes  once the graph is built  the pagerank algorithm is
applied to this graph  the score assigned to each node is its pagerank value 
a similar approach is used in the case of fernandez et al          again  the nodes
include all the candidates and pseudo candidates  but in this case the edges represent cooccurrences  in particular  there is an edge from node wi to node wj when 
   there is at least a third wikipedia article wk that links both to wi and wj   that is 
wi   wj  f  wk    these edges are assigned weights according to 
weightc  wi  wj    

 b wi    b wj   
 b wi   

    

   a direct link exists between wi and wj   that is  wj  f  wi    these edges are assigned
weights as follows 
weightl  wi  wj     t fij idfj   p

n wi   wj  
 w  
log
 b wj   
wk f  wi   n wi   wk  

    

when two nodes wi and wj match both the conditions above  that is  they are directly
linked and they co occur in a third article wk   a single edge is created that combines the
contributions as follows 

weight wi  wj    

kl
kc
weightc  wi  wj    
weightl  wi  wj  
kc   kl
kc   kl
   

    

fifernandez garca  arias fisteus   sanchez fernandez

where kl and kc are configuration parameters  in this case we will use the values
kl        and kc        as suggested by fernandez et al         
once the weighted  directed graph is built  the pagerank is computed for this graph 
the score for each candidate ci  c dc   a   named rwf  ci   dc    is the pagerank value of
the candidate node in the graph  when the scores of all the candidates are computed  the
candidate with highest score is selected as the best one 
arg max rwp  ci   dc   

    

arg max rwf  ci   dc   

    

ci

ci

note that  in all the approaches listed in section    it might happen that different
members of the candidates set obtain the same weight and  thus  there would be a tie in the
ranking  we have used the most frequently linked  mfl  algorithm to break these potential
ties  this algorithm simply assigns a weight to each candidate according to its total number
of incoming links  that is 
m f l ci   dc    

x

n bj   ci  

    

bj b ci  

   comparative evaluation
this section reports the results of the evaluation of the different approaches described in
section    it is organized as follows  the experimental setup  wikipedia dataset  corpora 
etc   used for the evaluation is outlined in section      whereas section     reports the
quantitative results as well as some analysis and interpretation of these results 
    experimental setup
in order to evaluate the approaches described in section    we need a set of elements     
corpora of queries to evaluate the approaches      information about the wikipedia link
structure that will be used as input by the different approaches  and     adequate metrics
to measure and compare the performance of each approach  the next sections describe
these three elements briefly 
      corpora of queries
in order to evaluate the different approaches  we need corpora containing link to wikipedia
queries  according to the definition of the problem  see section    each of the queries in
these corpora should provide 
 the anchor a that is going to be linked 
 the context document dc in which the anchor a appears  the links in this document
provide the context information used by some algorithms 
 a set of candidates  c dc   a   with wikipedia articles that are potential targets for
the anchor 
   

fievaluation of link based approaches for candidate ranking in link to wikipedia

 a golden standard that indicates the correct answer  candidate in c dc   a  to be ranked
at the top  for each query  this golden standard is used to compute the performance
of the algorithms evaluated 
in the state of the art  we distinguish different approaches regarding the corpora they
use for empirical evaluation  a first approach is to build specific corpora  it is followed by
early work  bunescu   pasca        cucerzan        mihalcea   csomai         as well
as by more recent work  milne   witten      b  nguyen   cao        pilz         a
common approach within this first group is to use as corpus a subset of wikipedia articles
and compare the links suggested by automatic algorithms with those provided by wikipedia
editors  see for instance bunescu   pasca        cucerzan        milne   witten      b 
nguyen   cao       and pilz         this methodology has also been used in the context of
the inex link the wiki track  huang  xu  trotman    geva         a second alternative
is to use already available corpora  like the tac kbp corpus  used for instance in han  
sun       and hachey et al          or the corpora defined by cucerzan         used for
instance in gentile et al        and ratinov et al         
in the context of this paper  we adopt both approaches  in particular  we use the
following corpora in our comparative evaluation 
 cucerzan in the work of cucerzan        the authors use two different corpora  one
built from wikipedia articles and the other from manually annotated msnbc  msnbc 
      news items  we have used these corpora to build our own  in order to do so 
we proceeded as follows 
   the documents in the cucerzan corpora contain a set of pairs  anchor  wikipedia
article   each one representing a potential link to wikipedia query  we select
randomly     pairs from each of the cucerzans corpora  these pairs provide us
with the anchor a to be linked and the golden standard  correct answer to the
query  
   a typical approach among the systems in tac kbp to generate the candidate
set  c dc   a   is to rely on information retrieval techniques  ji et al          in
this paper we adopt this approach  however  as a difference with the tac kbp
scenario  where the evaluation involves all the stages of entity linking  we center
our evaluation only on the candidate ranking stage  due to this  we are interested
in isolating as much as possible this stage from the potential bad performance
of a particular candidate search implementation  that is  we are interested in
analyzing the performance of different candidate ranking approaches assuming
that the candidate search stage is ideal  in the sense that it always returns the
correct candidate among the candidate set  obviously  there does not exist
an ideal candidate searcher  thus  in practice  we rely on a state of the art
search engine  google  and append the correct answer to the candidate set in
case it is not found by the search engine  in particular  we query the google
search engine with the text of the anchor and a site en wikipedia org restriction 
filtering out from the top    google results the wikipedia pages not included in
the main namespace  in case the correct wikipedia article to be linked is not
   

fifernandez garca  arias fisteus   sanchez fernandez

included within the google result set  it is appended at the end  though this
only happens in a very limited number of queries  in the curcerzan wikipedia
corpus the correct candidate was added in   out of     queries         while in
the curcerzan news corpus it was added in    out of     queries        
   finally  from the rest of the pairs  anchor  wikipedia article  included in the
document where the query has been selected  we obtain the wikipedia article
component to be used as context information  links in f  dc     filtering out the
links to articles that are included in the candidate set in order to avoid bias 
 ad hoc corpora two ad hoc corpora have been used in the evaluation  one corpus
 wikipedia random corpus  was built by following the methodology suggested by
previous works in the state of the art  that is  selecting a set of     wikipedia articles
using the random article page  wikipedia      a  
the second ad hoc corpus  wikinews corpus  was built using documents from the
english wikinews site  wikinews      b   these documents represent news items 
they are usually annotated by human editors with wikipedia links  in this case    
news items were selected with the random article functionality of wikinews  wikinews 
    a  
for each document in the total set of      documents in the two ad hoc corpora  we
built a link to wikipedia query by using the following procedure 
   a wikipedia link is randomly selected from the document 
   from the selected link we obtain the anchor a and the golden standard  link
destination in wikipedia  
   we use the anchor and google search engine to build the candidate set  as it
was indicated in the case of the cucerzan corpora  again  we append the correct
candidate when it is not included in the google result set  in particular  the
correct candidate was added in    out of     queries        in the case of the
wikinews corpus and in    out of     queries        in the wikipedia random
corpus 
   the query context information is obtained from the rest of the links in the
document  filtering out those linking to members of the candidate set in order
to avoid bias 
 tac     the tac      dataset includes a total of      entity linking queries and 
for each one  provides the anchor a to be linked  the context document dc and the
golden standard  we have used this dataset as basis to build the last corpus involved
in our evaluation  in order to do so  we proceeded as follows 
   from the total set of      queries       have as golden standard the nil answer 
that is  there is no wikipedia article to link in these cases  thus  no correct
candidate instance exists and  due to this  it is difficult to take advantage of these
queries to evaluate the candidate ranking process  taking this into account  we
discard these queries and keep the remaining      
   

fievaluation of link based approaches for candidate ranking in link to wikipedia

   the documents in the tac      corpus do not contain links  thus  we use
the following procedure in order to obtain the context links needed by some
algorithms 
 for each query  we analyze its context document dc by using natural language
processing techniques  in particular  we extract named entities  persons 
locations and organizations  from text using the stanford ner tool  finkel 
grenager    manning        
 then  we link the detected entities to wikipedia using google  in particular 
we query the google search engine with the text of the named entity and a
site en wikipedia org restriction  filtering out from the top    google results
the wikipedia pages not included in the main namespace  and assigning as
link the top result in the filtered list  we discard the named entities where no
google results are found  the links from named entities to wikipedia defined
with this procedure are used as context information for candidate ranking 
filtering out from the context those links to members of the candidate set in
order to avoid bias 
we discard those queries where no context information is available  that is  where
the ner tool does not find named entities in dc   or when they are filtered in
the process of linking them to wikipedia  this results in a total of      valid
queries 
   the candidate set c dc   a  for each query is obtained by using the same procedure
as in previous corpora  using google to search the anchor a and appending the
correct candidate in case it is not found  which happens in    out of      queries
        
to summarize  we carry out our evaluation in five different corpora  cucerzan news 
cucerzan wikipedia  wikipedia random  wikinews and tac          which add up to a
total of      link to wikipedia queries  boxplot diagrams representing the distributions in
each corpus of the number of candidates   c dc   a    per query  and the number of links
  f  dc     per query  are shown in figures   and   respectively 
note that appending the correct candidate to the candidates set was needed in only    
of the      queries  this indicates that google performs quite well as a candidate searcher
in our case  with a candidate recall near to      considering only the first    results  
to put this result in context  we can indicate that hachey et al         compare several
candidate search approaches in the tac      dataset and report that their candidate recall
is below the     when limited to a maximum of    results  however our results are similar
to those by lehmann et al          where the authors use google search combined with a
set of additional techniques and report a     candidate recall in the tac      dataset 
      wikipedia link structure
all the approaches described in section   require information from the wikipedia link structure to carry out the candidate ranking process  in our case  that information has been
   these corpora are available to download at  http   www it uc m es berto link to wikipedia survey 

   

fifernandez garca  arias fisteus   sanchez fernandez

 

 

 

 

  

distribution of the number of candidates per query in each corpus

cucerzan news

cucerzan wiki 

wiki  random

wikinews

tac    

figure    boxplot diagram of the number of candidates   c dc   a    per query for each
corpus 

 

  

   

   

   

   

   

   

distribution of the number of context links per query for each corpus

cucerzan news

cucerzan wiki 

wiki  random

wikinews

tac    

figure    boxplot diagram of the number of links in the context   f  dc     per query for
each corpus 

   

fievaluation of link based approaches for candidate ranking in link to wikipedia

obtained from a dump of wikipedia page links provided by dbpedia  bizer et al        
version        which was generated from a full wikipedia dump dated in june      
the links dump was preprocessed as follows 
 the redirections were resolved  using the redirections mapping table from dbpedia
      
 as indicated in section    we consider only the wikipedia pages that belong to the
main namespace  thus  the links from to pages in other namespaces  like talk pages 
user pages  etc   were removed 
 using the information provided by the disambiguation map from dbpedia        the
links from to disambiguation pages were also removed 
 the inner links  from an article to itself  were also filtered out 
 the evaluation corpora described in section       include in some cases wikipedia
articles  to separate the input data from the evaluation data  all the links with source
or destination in one of the wikipedia articles included in the evaluation corpora were
filtered out 
      performance metrics
to measure and compare the performance of each of the considered approaches  we need
adequate metrics  a well known evaluation metrics for link to wikipedia approaches is the
accuracy  used for instance by bunescu and pasca         cucerzan         hachey et al 
        ratinov et al         and hachey et al          the accuracy can be computed as
the percentage of queries where the candidate selected by the algorithm is the correct one 
or  more formally 
accuracy  

  x
s q 
 q 

    

qq

where q represents a set of evaluation queries  q a particular query in the set  and s q 
a function so that s q      if the candidate article ranked at the top for query q is the
correct answer or s q      otherwise 
however  as we center the evaluation in the candidate ranking stage of the link towikipedia task  the accuracy presents a limitation  it does not take into account the actual
position of the correct answer within the ranking produced by each algorithm  for example 
if one algorithm ranks the correct answer for a query in the  nd position  whereas another
algorithm ranks it in the  th position  the contribution from this query to the accuracy is
zero in both cases  despite the first algorithm having ranked the correct answer higher 
in scenarios where link to wikipedia approaches work under human supervision  for instance  if these systems are used within the production process of a news agency  fernandez
   http   downloads dbpedia org     en page links en nt bz   april       
   http   downloads dbpedia org     en redirects transitive en nt bz   april       
   http   downloads dbpedia org     en disambiguations en nt bz   april       

   

fifernandez garca  arias fisteus   sanchez fernandez

et al         to add metadata to news items  the particular order of the candidates is relevant  because in case the top ranked candidate is not the correct one  the human supervisor
can continue reading the ranked list of candidates and select another option  obviously 
the nearer to the top the correct candidate is in the list of suggestions  the better 
taking this into account  we decided to report performance using not only accuracy  but
also two position based discounting schemes to measure the overall quality of the ranked
list of results 
 the mean reciprocal rank  mrr  is used for instance in the evaluation of question
answering systems  voorhees         the mrr of a set of evaluation queries q can
be computed as 
m rr q   

  x  
 q 
r q 

    

qq

where r q  represents the position of the correct candidate in the rank for a query
q  q 
 as shown in equation     the mrr penalizes the differences in position severely 
taking this into account  we also report the results of the discounted cumulative
gain at a certain level k  dcg k   which introduces a smoother penalization with
position  the dcg k can be computed as 
k
  x x  r q i    
dcg k q   
 q 
log       i 

    

qq i  

where q represents a set of evaluation queries  q a particular query in the set  and
r q  i  the relevance score given to the candidate article in position i for query q  we
adopt a binary relevance model and  thus  r q  i      if the candidate in position
i is the correct answer and r q  i      otherwise  furthermore  we only consider a
single candidate as relevant for each query  taking this into account  the dcg k is
equivalent to its normalized version  the normalized discounted cumulative gain at
k  ndcg k   manning et al          and equation    can be simplified into 

  x
dcg k q   
f  q  k  with f  q  k   
 q 
qq

 

 
log     r q  

if r q     k

 

if r q    k

    

where r q  represents the position of the correct candidate in the rank for query q 
as it can be seen in equation     the bigger the value of r q   that is  the farther away
the correct candidate from the top of the rank  the lesser the value of the term added
to dcg k  note also that dcg   would be equivalent to the accuracy as defined
in equation    
   

fievaluation of link based approaches for candidate ranking in link to wikipedia

figure    taxonomy of the different approaches considered for evaluation 
    evaluation
this section reports the results of the empirical evaluation of the approaches  we have
structured the presentation of results in three parts  section       compares the individual
algorithms described in section    section       analyzes the combination of the different approaches through the use of machine learning techniques and  finally  section       evaluates
the impact of changing the search stage on the performance of the algorithms 
      comparison of individual approaches
all the approaches described in section    summarized in the taxonomy shown in figure   
were evaluated in the corpora described in section        table   reports the accuracy
obtained by each approach in the different evaluation corpora  we have highlighted in
boldface the best accuracy among the link based evaluated approaches for each particular
corpus 
table   includes a column  overall  that reports the results obtained in the corpus
generated by aggregating all the queries  the last column  confidence interval  overall  
reports the     confidence interval for the accuracy in the overall case  computed using
bootstrap methods as suggested by adibi  cohen  and morrison        
as it can be seen in the approach column in table    apart from the approaches considered in section    we include the results of two naive algorithms as reference baselines 
    the random algorithm  which simply ranks all the candidates randomly  and      the
most frequently linked  mfl  algorithm  described in section    see equation        we also
report  see row google  the accuracy obtained by using a trivial ranker that simply returns
   

fifernandez garca  arias fisteus   sanchez fernandez

the candidates in the same order as they are defined in the corpus  that is  in the same
order as returned by google  with the correct candidate at the end if it was not found by
google    
cucerzan
approach
random
mfl
google
relfa
relfm
a
relb
m
relb
p m ifa
p m ifm
a
p m ib
m
p m ib
simcos
simr
indegree
outdegree
nb
pr
ppr
rwp
rwf

wiki 

news

wiki

random

     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

tac
wikinews
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

    
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

overall
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

confidence
interval
 overall 
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              

table    accuracy obtained by the different approaches in each of the evaluation corpora 
figure   shows the dcg k for different values of k in the overall aggregated corpus 
mrr values for the different evaluation corpora are reported in table    where  again 
we have highlighted in boldface the best mrr among the link based evaluated approaches
for each particular corpus  furthermore  in order to provide a more detailed idea of the
differences among methods  we show in figure   the percentage of queries in which the
correct candidate is ranked at position k  with k from   to     for each algorithm 
we can also provide some empirical results about the run time of the different algorithms  in particular  the average run time per query  in seconds  measured on a linux
        intel core i      ghz pc with   gb ram was under one second for all the approaches except rwf and p p r  which run closer to   and     seconds per query respectively  the relatively large response time of p p r is due to the fact that this algorithm uses
context information to personalize the pagerank damping vector  taking into account that 
in general  each query has a different context  this means that we need to run a pagerank
   note that in the google case  the queries where the correct candidate is appended to the result set are
accounted as errors when computing accuracy 

   

fievaluation of link based approaches for candidate ranking in link to wikipedia

approach
random
mfl
google
relfa
relfm
a
relb
m
relb
p m ifa
p m ifm
a
p m ib
m
p m ib
simcos
simr
indegree
outdegree
nb
pr
ppr
rwp
rwf

cucerzan
news
wiki
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
     
    
           
           
           

wiki 
random
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

wikinews
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

tac
    
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

overall
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table    mrr obtained by the different approaches in each of the evaluation corpora 
computation on the whole wikipedia graph for each query in the corpus  a process that is
time consuming    note  however  that we have used a python implementation which was
not optimized and  thus  these results are provided only as a reference 
to contextualize the results reported in table    we can indicate that the tac     
corpus that we are using in our evaluation is practically equivalent  apart from   queries
removed due to the lack of context information  as indicated in section        to the non nil
queries in the tac      dataset  due to this  the results reported in the column tac     
of table   can be roughly compared  less than    of error  with the tac      non nil
accuracy reported by some papers in the state of the art  for instance  the best performing
approach in tac       lehmann et al         reported an accuracy on the non nil queries
of        note  however  that we have to be cautious with these comparisons  as the results
we are reporting would be equivalent to those obtained with an end to end system using
an ideal candidate search stage  we always append the correct candidate  and without a
candidate selection process  we report the results of the candidate ranking stage  
analyzing the results reported in table    a first conclusion that may be drawn is that
the overall accuracy achieved by using the google ranking is better than that obtained
by any of the evaluated approaches  however  if we observe the results obtained for each
   according to bianchini  gori  and scarselli         this computation depends linearly on the number of
edges on the wikipedia graph 

   

fifernandez garca  arias fisteus   sanchez fernandez

algorithms
relaf

    

relm
f
relab
relm
b
pmiaf

dcg k

pmim
f
pmiab
pmim
b
simcos

    

simr
indegree
outdegree
nb
pr
ppr
rwp
rwf

    

 

 

 

 

 

 

 

 

 

  

k

figure    dcg k values for the different algorithms considered when evaluated on the
overall corpus generated by aggregating all the queries 

individual corpus  we can also note that using google is not always the best approach  in
particular  the accuracy of n b in the tac      corpus is slightly better than that achieved
by google 
to interpret these findings  we have to take into account that previous work in the area
 notably that of chang et al         had already pointed out that using google produces
relatively good results for the entity linking task  accuracy near     in tac      experimental setup   in that sense  the overall performance obtained by google is not completely
unexpected  the degradation in performance in the tac      case is partially explained
   

fievaluation of link based approaches for candidate ranking in link to wikipedia

    

   

   
position  k 

percentage of queries

   

  
 

   

 
 
 

   

 
 

   

 
 

   

 

   

   

rwf

rwp

ppr

pr

nb

outdegree

indegree

simr

simcos

pmim
b

pmiab

pmim
f

pmiaf

relm
b

relab

relm
f

relaf

  

algorithms

figure    percentage of queries where the correct candidate is ranked at position k  with
k from   to     for each of the different algorithms compared 

   

fifernandez garca  arias fisteus   sanchez fernandez

by the fact that this corpus is specifically built for the entity linking task by using a careful
targeted process    due to this  the queries in the tac      corpus are expected to be
challenging  for instance  a common case  within this corpus is to have groups of queries
sharing the same anchor to be linked  but with different correct answers depending on the
particular query  in this case  as the queries share the anchor  they also share the google
ranking and  thus  the top ranked candidate but  as the correct answer changes between
queries  using always the google top ranked candidate as answer introduces some errors 
note also that by using google we are not taking advantage of the context information 
which is expected to be valuable to decide the best link for an anchor  especially when the
queries are challenging 
a second conclusion is that naive popularity metrics like the indegree  or the m f l 
whose performance is very similar to the indegree  produce reasonably accurate results 
this aspect is consistent with previous work in the state of the art  ji   grishman       
where the authors indicate that naive candidate ranking approaches based on web popularity
can achieve accuracies around     on the tac      corpus 
another aspect to be highlighted is that  consistently across evaluation corpora  the
indegree metrics produces better accuracy than the outdegree  which indicates that the
number of backlinks offers a better representation of the popularity of a wikipedia article
than the number of its outgoing links  one aspect that may explain  at least partially  this
difference in performance is the fact that the number of outgoing links may be high due
to several reasons  when a wikipedia article is long  which indicates that it has received
extensive attention by wikipedia contributors and is  in that sense  popular  we can expect
it to have more links than shorter articles  however  there are other cases in which a
wikipedia article can contain many outgoing links  it is the case  for instance  of articles
that represent a hub of links  such as list articles 
to test the hypothesis that the outdegree metrics introduces bias to favor hubs like
list articles  we compared the number of queries where the candidate that is ranked at the
top by indegree and outdegree is a list  its title starts with list   in the different corpora 
these results are shown in table    where x  is the proportion of queries where the candidate
ranked at the top is a list  whereas y is the proportion of the queries where the candidate
ranked at the top is a list and this is not the correct answer  that is 
queries where the top ranked candidate is a list
total number of queries in the corpus

    

queries where the top ranked candidate is a list and is not correct
queries where the top ranked candidate is a list

    

x 

y 

as it can be seen in table    the outdegree metrics ranks list pages more frequently
at the top than the indegree metrics  it can also be seen that  in most cases  when the
candidate ranked at the top is a list  it is not the correct answer  this particularity explains
a significant part of the difference between the overall results of indegree and outdegree 
   as indicated in the tac kbp      task definition document  available at 
http   www it uc m es berto link to wikipedia survey kbp     taskdefinition pdf  april       
   we have identified a total of     queries  approximately a      following this pattern 

   

fievaluation of link based approaches for candidate ranking in link to wikipedia

approach
name
param
x
indegree
y
x
outdegree
y

cucerzan
news wiki
           
   
   
          
   
   

wiki 
random
     
     
     
     

wikinews
     
     
     
     

tac
    
     
   
     
   

overall
     
     
     
     

table    comparison between indegree and outdegree regarding the tendency to rank a
wikipedia list at the top 

the difference in performance between using backlinks and forward links can also be
noticed in the similarity metrics  where those approaches relying on backlink information
a   relm   p m i a   p m i m   produce better results than the corresponding metrics work relb
b
b
b
ing on forward links  relfa   relfm   p m ifa   p m ifm   
according to the results in table    it can also be pointed out that  among the linkbased approaches being evaluated  taking advantage of context information is  in general 
beneficial  to support this conclusion we can compare the results of m f l and n b  note
that n b uses the prior p  ci    see equations      and        which is basically a normalized
version of m f l  however  n b combines this prior with the probabilities p  fj  ci    which
capture context information  as it can be seen in tables   and    the result of this combination is that n b produces better results than m f l  note also that none of the alternatives
which use only popularity information are included among the top   link based evaluated
a    however  using context
approaches with higher accuracy  n b  rwf   simr   rwp   relb
information is not a sufficient condition to ensure a good performance  as reflected by the
results of the pmi variants 
another conclusion that can be reached is that  in the cases of relatedness and pmi
metrics  averaging the pairwise similarities between the candidate and the articles in f  dc  
a   p m i a   rela and p m i a   produces  consistently across corpora  better
 as is done in relb
b
f
f
m   p m i m   relm and p m i m   
accuracy than relying on the maximum  as is done in relb
b
f
f
a possible explanation to this result is that by relying on the maximum similarity we
just take into account one of the elements in f  dc    the one that maximizes similarity 
to represent the semantics of the document dc   whereas  when averaging  all the elements
in f  dc   contribute to the final similarity value  it is reasonable to think that the set of
forward links in dc provides a more accurate representation of the semantics of the context
document than a single link in the document 
m
with the objective of testing this intuition  we implemented two new variants of relb
m   that we name relm  p   and p m i m  p    in order to obtain the relm  p   c   d  
and p m ib
i c
b
b
b
scores we compute the simrb  ci   fj   fj  f  dc   as in equation      however  instead of just
selecting the maximum value  as it is done in equation      we select a certain percentage
p of the top values and average them  for instance  if  f  dc         and p        we
select the top   simrb  ci   fj   values and average them  note that  with this approach 
a   to obtain
when p        the scores would be equivalent to those obtained with relb
m  p   c   d   scores we proceed in a similar way  but using the p  c   f   values
the p m ib
i c
b i j
m  p   and
 see equation       instead of the simrb  ci   fj   ones  we evaluated the relb
   

fifernandez garca  arias fisteus   sanchez fernandez

   

relab

accuracy

   

relm
b
   

algorithms
pmim
b   p 
relm
b   p 

   

pmiab
   

pmim
b

   

   

   

   

   

percentage
m  p   and p m i m  p   approaches for different values
figure    accuracy values for the relb
b
of percentage p when evaluated on the overall corpus generated by aggregating
all the queries 

m  p   variants on the overall aggregated corpus for different values of percentage p  
p m ib
figure   reports the accuracy obtained by these new variants  we have also included as
m   p m i m   rela and
references horizontal lines representing the overall accuracy of relb
b
b
a
p m ib   as it can be seen  increasing the context information improves results 
also related with the relatedness and the pmi metrics is the fact that the results obtained
by pmi variants are quite poor when compared with the equivalent relatedness variants  as
a         and p m i a         
an example  see the difference in overall accuracy between relb
b

   

fievaluation of link based approaches for candidate ranking in link to wikipedia

an inspection of the results of p m i revealed that  because of using absolute values instead
of logarithmic values  as in relatedness   the pmi is more sensitive to outliers  in order to
verify and quantify this observation  we decided to compare the results of pmi with two
other alternatives 
 we implemented a logarithmically smoothed version of the averaging pmi variants
by adapting equations     and     as follows 

logp m ifa  ci   dc    
a
logp m ib
 ci   dc    

 
 f  dc   
 
 f  dc   

x

log pf  ci   fj   

    

x

log pb  ci   fj   

    

fj f  dc  

fj f  dc  

 we used the symmetric conditional probability  scp   introduced by da silva and
lopes         the scp of two wikipedia documents wi   wj can be computed as 
sb  wi   wj    

 b wi    b wj    
 b wi    b wj   

    

that can also be adapted to use forward links as 
sf  wi   wj    

 f  wi    f  wj    
 f  wi    f  wj   

    

using equations      and      the following two metrics were implemented 
scpfa  ci   dc    
scpba  ci   dc    

 
 f  dc   
 
 f  dc   

x

 sf  ci   fj   

x

 sb  ci   fj   

    

fj f  dc  

    

fj f  dc  

we run these approaches on all the corpora  and report the results in table    accuracies 
and table    mrr   as it can be seen comparing the results in table   with those for
a in table    a significant increase in performance is achieved by using the
p m ifa and p m ib
logarithmically smoothed version of p m i  it can also be seen that the accuracies reported
a and more similar to the
for scpfa and scpba are better than those for p m ifa and p m ib
a   respectively 
results of the relatedness variants relfa and relb
      combining individual approaches
we want also to explore the possibility of combining the results of the different link based
approaches to test whether better results can be obtained or not  the approach that we
follow to combine the alternatives described in section   is based on supervised machine
learning techniques  in particular  we use a learning to rank  joachims        method 
   

fifernandez garca  arias fisteus   sanchez fernandez

cucerzan
approach
logp m ifa
a
logp m ib
scpfa
scpba

wiki 

news

wiki

random

     
     
     
     

     
     
     
     

     
     
     
     

tac
wikinews
     
     
     
     

    
     
     
     
     

overall
     
     
     
     

confidence
interval
 overall 
              
              
              
              

table    accuracy obtained by the logarithmically smoothed p m i variants and the scp based metrics in each of the evaluation corpora 

approach
logp m ifa
a
logp m ib
a
scpf
scpba

cucerzan
news wiki
           
           
           
           

wiki 
random
     
     
     
     

wikinews
     
     
     
     

tac
    
     
     
     
     

overall
     
     
     
     

table    mrr obtained by the logarithmically smoothed p m i variants and the scp  based
metrics in each of the evaluation corpora 

though several learning to rank algorithms are available in the state of the art  liu        
we decided to rely on the listn et method described by cao et al          our decision
is backed on the results reported by chen and ji         where several alternatives are
evaluated and compared in the context of the entity linking problem  in particular  we
took advantage of the open source implementation of listn et provided by the university
of massachusetts ranklib package  van b  dang        
basically  we use the scores returned by the individual approaches in section   as features
to be taken into account by the listn et algorithm  the values of the features are normalized
in the range        to avoid any bias that might favor some of them 
we have tested three different combinations of approaches  the first variant  that
we name listn etall   combines all the link based approaches under evaluation  that is 
all the algorithms included in table   except google and the naive references m f l and
random   the second variant  listn ett op   combines just the top   best performing linkbased algorithms under evaluation  according to overall accuracy in table    that is  n b 
a    finally  the third case  listn et
rwf   simr   rwp   relb
t op google   combines the top  best performing link based algorithms with the google baseline  in all the cases  we
have used the configuration parameters for listn et that are suggested in the ranklib
implementation       epochs and a learning rate of       
in order to report the accuracy  mrr and dcg k of the listn et variants  we use
the results obtained by averaging    repetitions of    fold cross validation on the particular
corpus being analyzed  table   reports the accuracy in the different corpora for the combinations that have been considered  while table   reports the mrr results for the same
   

fievaluation of link based approaches for candidate ranking in link to wikipedia

approach
listn etall
listn ett op
listn ett op google

cucerzan
news wiki
           
           
           

wiki 
random
     
     
     

wikinews
     
     
     

tac
    
     
     
     

overall
     
     
     

table    accuracy obtained when combining approaches in section   with listn et in each
of the evaluation corpora 

approach
listn etall
listn ett op
listn ett op google

cucerzan
news wiki
           
           
           

wiki 
random
     
     
     

wikinews
     
     
     

tac
    
     
     
     

overall
     
     
     

table    mrr obtained when combining approaches in section   with listn et in each of
the evaluation corpora 

combinations  figure   compares the dcg k achieved by listn et variants in the overall
case with those of the top   link based evaluated approaches 
we can compare the accuracy values reported in table   with those in table    in the
overall case  the best result is obtained by listn ett op google   the listn ett op combination
shows a lower performance than the google reference  but outperforms n b  the best of the
individual algorithms under comparison   regarding the listn etall variant  its accuracy is
lower than that obtained by both google and n b  similar conclusions can also be reached
from figure   for the dcg k metrics in the overall case  these conclusions suggest that
some particular combinations of features can have a positive impact on results 
however  we have to be cautious with these results  because  as indicated by vanwinckelen         repeated cross validation should not be assumed to provide perfectly precise
estimates of a models predictive accuracy  in fact  vanwinckelen        does not recommend reporting confidence intervals or making significance claims from repeated cross validation  they report that  though popular among researchers  this practice can contribute
to misleading interpretations 
      effect of changes in the search stage
as indicated in section        in order to isolate the results of the candidate ranking algorithms being evaluated from the potential bad performance of a particular candidate search
implementation  we would need to rely on an ideal candidate search stage  in the sense that
it always returns the correct answer among the candidate set  obviously  there does not
exist an ideal candidate searcher  thus  in practice  to try to mimic this behavior  we have
relied on a state of the art search engine  google  and we have appended the correct answer
to the candidate set in case it is not found by the search engine 
   

fifernandez garca  arias fisteus   sanchez fernandez

    

algorithms
relab

    

simr

dcg k

nb
rwp
rwf
listnetall

    

listnettop
listnettop google

    

    
 

 

 

 

 

 

 

 

 

  

k

figure    dcg k values for the top   link based evaluated approaches and listn et variants when evaluated on the overall corpus generated by aggregating all the
queries 

   

fievaluation of link based approaches for candidate ranking in link to wikipedia

however  we are also interested in evaluating the impact on the results achieved by
the different algorithms when the aforementioned conditions change into a more restrictive
setup  in order to do so  we proceeded as follows 
 we used the information retrieval library apache lucene  apache software foundation        to build an index with the titles of the dbpedia     pages  each title was
processed by the standardanalyzer of lucene 
 for each of the      queries in the tac      corpus we carried out the following
process 
 the anchor a of the query is used to search into the lucene index for the candidates  c dc   a   the result set was limited to the top    entries  like in previous experiments  however  on the contrary to our previous experiments  when
lucene does not return the correct answer within its result set  we do not append
it 
 as the documents in the tac      corpus do not contain context links  these
are automatically generated using a similar approach to the one described in
section        the named entities obtained from the context documents using
stanford ner are resolved into links by querying lucene with the text of the
named entity and assigning as link destination the top result from the search
engine  as usual  filtering out the links to articles that are included within the
candidate set  
using the aforementioned procedure we built a new version of the tac      corpus
annotated with lucene  thus  we have now two variants of tac      
 tac      google  where google has been used as candidate searcher and the correct
candidate is appended to the google results set in case it is not found  this is the
version used in the experiments of previous sections 
 tac      lucene  which is the version built following the procedure described in this
section 
we run all the evaluated approaches  as well as the references random and m f l  in
the tac      lucene corpus  the accuracies achieved by the different algorithms and their
    confidence intervals are shown in figure    to ease comparison  we have depicted in
the same figure the accuracies for the tac      google corpus  we have also included
 with the name search  the accuracy achieved when the candidate ranking provided by the
search engine  either google or lucene  is directly used 
a first aspect to be noted from the results reported in figure   is that  not surprisingly 
the accuracies obtained by the different approaches when using lucene search are  in general 
lower  note that in the lucene case we are not including the correct candidate in the
candidates set  thus  there are many queries      cases  almost a     of the total queries 
where it is impossible for the candidate rankers to rank the correct candidate at the top 
another issue to be highlighted is that  if we look at the top   best performing link based
a   all of them have greatly reduced their
approaches in table    n b  rwf   simr   rwp   relb
   

fifernandez garca  arias fisteus   sanchez fernandez

   

accuracy

   

case
google

   

lucene

   

rwf

rwp

ppr

pr

nb

indegree

outdegree

simr

simcos

pmim
b

pmiab

pmim
f

pmiaf

relm
b

relab

relm
f

relaf

search

mfl

random

   

algorithms

figure    accuracy obtained by the different approaches on the tac      google and tac
     lucene corpora 

performance in the tac      lucene corpus  in fact  though n b is the top performing in
that corpus  there is no statistically significant difference with popularity approaches like
indegree or pagerank  p r   a possible explanation for this result is that in the tac     
corpus the context information is automatically generated from the search engine and not
supervised  thus  we can expect it to be noisy  this noise affects all n b  rwf   simr  
a   which rely on context information to take their decisions  but does not impact
rwp   relb
indegree and p r  which do not rely on context information  note that  though the noise in
the context information affects both the cases where google and lucene are used as search
   

fievaluation of link based approaches for candidate ranking in link to wikipedia

engines  the better performance of google  note results for search  makes it much worse in
the lucene case 

   related work
the foundations of the link to wikipedia task can be found in two different research communities  first  this area is related with traditional computational linguistics tasks like
cross document co reference resolution  bagga   baldwin         or word sense disambiguation  navigli         the main difference with respect to these traditional tasks is that
wikipedia is used as a source of knowledge instead of lexicons such as wordnet  miller 
      typically used in former work  see for instance li  szpakowicz    matwin        
second  the link to wikipedia task is also related to the link prediction task in link mining  getoor   diehl         though in this case the goal is mainly to decide whether two
objects  for instance  two actors in a social network  or an actor and an event  are linked
or not  instead of finding the best link destination in wikipedia for a particular anchor in
a text document 
traditionally  the works of bunescu and pasca        and cucerzan        have been
considered as seminal in this area  since the publication of these papers the problem of
linking anchors in a text document to wikipedia articles has been addressed by several
other works  like those referenced in section   
two initiatives are also especially relevant in this sense  the knowledge base population  kbp  track at the text analysis conference  tac   and link the wiki track of the
initiative for the evaluation of xml retrieval  inex   both initiatives share the same goal 
offer a common environment  corpora  performance metrics  etc  to allow a fair comparative evaluation of different techniques and  thus  foster this area of research  however  as
indicated in the introductory section  they approach the link to wikipedia task with slight
differences  in the case of kbp  the final aim is to automatically populate a knowledge
base  kb  built from wikipedia with information about named entities  thus  the linkto wikipedia variant  named entity linking  is focused on these entities and covers the case
where no good wikipedia target exists for the link  as this case indicates the need to add
a new entry to the kb  in the case of the link the wiki inex track  the focus is set in
keeping the links up to date in a rich and dynamic hypermedia document collection  such as
a wiki   therefore  the link to wikipedia variant  wikification  covers both common terms
and named entities as anchors to be linked  and does not pay special attention to the case
where no good wikipedia target exists  as in this case no link needs to be created 
in both cases  the overview papers published by the organizers of these events  huang
et al                     ji et al               ji   grishman        offer a good source
of references in this area  however  the comparisons provided by these works refer only to
systems taking part in tac kbp or inex  and not to other external work  furthermore 
as indicated in the introductory section  link to wikipedia systems combine  in general  a
variety of techniques and features of different types  based on text  on links  etc   to address
the task  because the results reported in the overviews refer usually to full systems  it is
difficult to analyze and compare the performance of the individual techniques that are part
of these systems  the goal of our work is doing this analysis and comparison for link based
techniques 
   

fifernandez garca  arias fisteus   sanchez fernandez

other surveys related with the task of link to wikipedia and  thus  relevant for the purposes of this paper are that of navigli and lapata         chen and ji         and hachey
et al         
chen and ji        evaluate several supervised candidate rankers for named entity
linking  and compare them with reference unsupervised approaches  a naive algorithm and
three different similarity metrics based on textual features  the main goal of this comparison
was to assess which machine learning mechanism  maximum entropy  svm  sv m rank and
listnet  was the top performing  thus  the results reported by chen and ji        and
those in our paper are complementary  because  as we indicated in section        the different
approaches analyzed here can be used as features in supervised systems  obviously  this
requires to know which supervised techniques work better  chen   ji         but also to
know which link based techniques are better  that is the goal of our paper 
hachey et al         re implement and compare three different named entity linking
systems in the state of the art  however  the main goal of their work was different to ours 
as the aim of hachey et al         was to analyze the impact of the candidate searching and
candidate ranking stages in the final performance of the entity linking system 
navigli and lapata        compare several metrics based on graph connectivity  including some that we have also considered in our paper  like pagerank and indegree  however 
their work has a different scope to ours  it is centered on a different task  word sense
disambiguation   and uses a different data source  wordnet  
to the knowledge of the authors  a previous overview and comparison of different linkbased approaches for candidate ranking in link to wikipedia systems  as proposed in this
paper  is not available at the time of writing 

   conclusions and future lines
in this paper we have presented an overview of link based approaches for candidate ranking
in link to wikipedia systems  apart from this overview  a comparative analysis of the
different approaches is also carried out  we have structured this analysis into three parts 
 the first part was devoted to compare the performance of the individual approaches
according to three metrics  accuracy  dcg k and mrr  in five different corpora
 cucerzan news  cucerzan wikipedia  random wikipedia articles  random wikinews
articles and tac        the results in this part of the analysis indicate that  though
naive approaches based only on the popularity of the candidates perform reasonably
well  taking advantage of the context information is  in general  beneficial in linkbased approaches  we have also found that by using information from backlinks we
can obtain better results than by using forward links with the same techniques 
 in the second part of the analysis we have combined different approaches by using
listnet  the main conclusion of this part is that  according to the results obtained 
combining algorithms can produce positive effects in performance 
 finally  the third part of the analysis was devoted to evaluate the impact of the
candidate search stage in the candidate ranking results  an impact that was found to
be very significant 
   

fievaluation of link based approaches for candidate ranking in link to wikipedia

regarding potential future lines of development of the work described in this paper  a
first aspect to consider is to evaluate the impact of the quality of the context links in the
performance of the algorithms  we also want to analyze the effect of ignoring links that
might be introducing some noise into the ranking process  like lists  on the opposite case 
we are interested in measuring the impact of including links to pages in other namespaces 
like categories  that have not been considered in this paper  in this sense  taking categories
into account will open the door to the use of semantic relatedness measures based on this
information  like those described by ponzetto and strube        
according to the results in the paper  using listnet to combine algorithms can produce
positive effects in performance in some cases  however  an exhaustive analysis of different
combinations has not been carried out  thus  another potential line of development could be
exploring further combinations of algorithms  either by taking advantage of some proposals
of mechanisms for feature selection in learning to rank  geng  liu  qin    li        or
empirically 
we have analyzed the different algorithms from the perspective of their performance on
the link to wikipedia task  the computational complexity aspects have not been addressed 
an exhaustive analysis of the different algorithms along this line is left for future work 
as suggested in section        link to wikipedia systems can be integrated into content
production workflows  where they have to interact with human supervisors  assessing the
impact of this human factor on the final performance of the systems can also constitute an
area for future research 
finally  though in this paper we have centered our attention on the candidate ranking
stage  link to wikipedia systems usually include other processing stages  identifying the
anchors to be linked  searching the candidate links for these anchors  and deciding whether
a link is to be suggested or not  detect nil answers   an end to end evaluation including
these additional processing stages is also an interesting line to continue the work reported
in this paper 

acknowledgements
in memoriam of concepcion garca alonso             and all the passengers that passed
away in the angrois railway accident     jul       

references
adibi  j   cohen  p  r     morrison  c  t          measuring confidence intervals in link
discovery  a bootstrap approach  in proceedings of the acm special interest group
on knowledge discovery and data mining  acm sigkdd    
apache software foundation         apache lucene   welcome to apache lucene  available
at  http   lucene apache org  
bagga  a     baldwin  b          entity based cross document coreferencing using the
vector space model  in proceedings of the   th international conference on computational linguistics   volume    coling     pp        stroudsburg  pa  usa 
association for computational linguistics 
   

fifernandez garca  arias fisteus   sanchez fernandez

bianchini  m   gori  m     scarselli  f          inside pagerank  acm trans  internet
technol                
bizer  c   lehmann  j   kobilarov  g   auer  s   becker  c   cyganiak  r     hellmann  s 
        dbpedia   a crystallization point for the web of data  web semant         
       
brin  s     page  l          the anatomy of a large scale hypertextual web search engine 
comput  netw  isdn syst                    
budanitsky  a     hirst  g          evaluating wordnet based measures of lexical semantic relatedness  comput  linguist                
bunescu  r  c     pasca  m          using encyclopedic knowledge for named entity
disambiguation  in proceedings of the   st conference of the european chapter of
the association for computational linguistics  eacl 
cao  y   lin  c     zheng  g          msra at tac       entity linking  in proceedings
of the knowledge base population  kbp  track of the  th text analysis conference
 tac   national institute of standards and technololgy  nist  
cao  z   qin  t   liu  t  y   tsai  m  f     li  h          learning to rank  from pairwise
approach to listwise approach  in icml     proceedings of the   th international
conference on machine learning  pp          new york  ny  usa  acm 
chang  a   spitkovsky  v   yeh  e   aguirre  e     manning  c          stanford ubc entity
linking at tac kbp  in proceedings of the knowledge base population  kbp  track
of the  rd text analysis conference  tac  
chen  z     ji  h          collaborative ranking  a case study on entity linking  in proceedings of the conference on empirical methods in natural language processing  emnlp
    pp          stroudsburg  pa  usa  association for computational linguistics 
cilibrasi  r  l     vitanyi  p  m  b          the google similarity distance  ieee trans 
on knowl  and data eng                  
cucerzan  s          large scale named entity disambiguation based on wikipedia data 
in proceedings of emnlp conll       pp         
cucerzan  s          the msr system for entity linking at tac       in proceedings
of the knowledge base population  kbp  track of the  th text analysis conference
 tac  
da silva  j  f     lopes  g  p          a local maxima method and a fair dispersion normalization for extracting multi word units from corpora  in sixth meeting of mathematics
of language 
dredze  m   mcnamee  p   rao  d   gerber  a     finin  t          entity disambiguation
for knowledge base population  in proceedings of the   rd international conference
on computational linguistics  coling     pp          stroudsburg  pa  usa 
association for computational linguistics 
erbs  n   zesch  t     gurevych  i          link discovery  a comprehensive analysis  in
proceedings of the      ieee fifth international conference on semantic computing 
icsc     pp        washington  dc  usa  ieee computer society 
   

fievaluation of link based approaches for candidate ranking in link to wikipedia

fader  a   soderland  s     etzioni  o          scaling wikipedia based named entity
disambiguation to arbitrary web text  in proceedings of the wikiai      ijcai
workshop  user contributed knowledge and artificial intelligence  an evolving synergy  pasadena  ca  usa 
fahrni  a   nastase  v     strube  m          hits cross lingual entity linking system
at tac       one model for all languages  in proceedings of the knowledge base
population  kbp  track of the  th text analysis conference  tac   national institute
of standards and technololgy  nist  
fernandez  n   fisteus  j   sanchez  l     martin  e          webtlab  a cooccurence
based approach to kbp      entity linking task  in proceedings of the knowledge
base population  kbp  track of the  rd text analysis conference  tac   national
institute of standards and technololgy  nist  
fernandez  n   blazquez  j  m   fisteus  j  a   sanchez  l   sintek  m   bernardi  a   fuentes 
m   marrara  a     ben asher  z          news  bringing semantic web technologies into news agencies  in the semantic web   iswc       vol       of lecture
notes in computer science  pp          springer berlin heidelberg 
finkel  j  r   grenager  t     manning  c          incorporating non local information
into information extraction systems by gibbs sampling  in proceedings of the   nd
annual meeting of the association for computational linguistics  acl        pp 
       
geng  x   liu  t  y   qin  t     li  h          feature selection for ranking  in proceedings
of the   th annual international acm sigir conference on research and development
in information retrieval  sigir     pp          new york  ny  usa  acm 
gentile  a   zhang  z   xia  l     iria  j          graph based semantic relatedness for
named entity disambiguation  in proceeding of the  st international conference on
software  services and semantic technologies  s t  
getoor  l     diehl  c  p          link mining  a survey  sigkdd explor  newsl         
    
gracia  j     mena  e          web based measure of semantic relatedness  in proceedings
of the  th international conference on web information systems engineering  wise
    pp         
guo  y   tang  g   che  w   liu  t     li  s          hit approaches to entity linking at
tac       in proceedings of the knowledge base population  kbp  track of the  th
text analysis conference  tac   national institute of standards and technololgy
 nist  
hachey  b   radford  w     curran  j  r          graph based named entity linking with
wikipedia  in proceedings of the   th international conference on web information
system engineering  wise    pp          berlin  heidelberg  springer verlag 
hachey  b   radford  w   nothman  j   honnibal  m     curran  j  r          evaluating
entity linking with wikipedia  artificial intelligence                    
han  x     sun  l          a generative entity mention model for linking entities with
knowledge base  in proceedings of the   th annual meeting of the association for
   

fifernandez garca  arias fisteus   sanchez fernandez

computational linguistics  human language technologies   volume    hlt     pp 
        stroudsburg  pa  usa  association for computational linguistics 
han  x   sun  l     zhao  j          collective entity linking in web text  a graph based
method  in proceedings of the   th international acm sigir conference on research
and development in information retrieval  sigir     pp          new york  ny 
usa  acm 
han  x     zhao  j          named entity disambiguation by leveraging wikipedia semantic
knowledge  in proceedings of the   th acm conference on information and knowledge
management  cikm     pp          new york  ny  usa  acm 
haveliwala  t  h          topic sensitive pagerank  a context sensitive ranking algorithm for web search  ieee trans  on knowl  and data eng                  
huang  d  w   xu  y   trotman  a     geva  s          overview of inex      link
the wiki track  in fuhr  n   kamps  j   lalmas  m     trotman  a   eds    focused
access to xml documents  pp          springer verlag  berlin  heidelberg 
huang  d  w  c   geva  s     trotman  a          overview of the inex      link the
wiki track  in geva  s   kamps  j     trotman  a   eds    advances in focused
retrieval  vol       of lecture notes in computer science  pp          springer
berlin heidelberg 
huang  w   geva  s     trotman  a          overview of the inex      link the wiki
track  in geva  s   kamps  j     trotman  a   eds    focused retrieval and evaluation  vol       of lecture notes in computer science  pp          springer berlin
heidelberg 
inex         inex      main page  available at 
https   inex mmci uni saarland de  
ji  h     grishman  r          knowledge base population  successful approaches and
challenges  in proceedings of the   th annual meeting of the association for computational linguistics  acl   pp           
ji  h   grishman  r     dang  h  t          overview of the tac     knowledge base
population track  in proceedings of the knowledge base population  kbp  track of
the  th text analysis conference  tac  
ji  h   grishman  r   dang  h  t   griffitt  k     ellis  j          overview of the tac    
knowledge base population track  in proceedings of the knowledge base population
 kbp  track of the  rd text analysis conference  tac  
jimenez  m   fernandez  n   fisteus  j     sanchez  l          wikiidrank    extensions
and improvements of the wikiidrank system for entity linking  international journal
on artificial intelligence tools         
joachims  t          optimizing search engines using clickthrough data  in proceedings of
the eighth acm sigkdd international conference on knowledge discovery and data
mining  kdd     pp          new york  ny  usa  acm 
kulkarni  s   singh  a   ramakrishnan  g     chakrabarti  s          collective annotation
of wikipedia entities in web text  in proceedings of the   th acm sigkdd inter   

fievaluation of link based approaches for candidate ranking in link to wikipedia

national conference on knowledge discovery and data mining  kdd     pp         
new york  ny  usa  acm 
lehmann  j   monahan  s   nezda  l   jung  a     shi  y          lcc approaches to
knowledge base population at tac       in proceedings of the knowledge base
population  kbp  track of the  rd text analysis conference  tac  
li  x   szpakowicz  s     matwin  s          a wordnet based algorithm for word sense
disambiguation  in proceedings of the   th international joint conference on artificial
intelligence   volume    ijcai    pp            san francisco  ca  usa  morgan
kaufmann publishers inc 
lin  w  p   snover  m     ji  h          unsupervised language independent name translation mining from wikipedia infoboxes  in proceedings of the first workshop on
unsupervised learning in nlp  emnlp     pp        stroudsburg  pa  usa  association for computational linguistics 
liu  t  y          learning to rank for information retrieval  found  trends inf  retr  
              
manning  c  d   raghavan  p     schtze  h          introduction to information retrieval 
cambridge university press  new york  ny  usa 
mihalcea  r     csomai  a          wikify   linking documents to encyclopedic knowledge 
in proceedings of the sixteenth acm conference on conference on information and
knowledge management  cikm     pp          new york  ny  usa  acm 
miller  g  a          wordnet  a lexical database for english  commun  acm          
     
milne  d     witten  i  h       a   an effective  low cost measure of semantic relatedness obtained from wikipedia links  in proceedings of the first aaai workshop on
wikipedia and artificial intelligence  wikiai    
milne  d     witten  i  h       b   learning to link with wikipedia  in proceedings of
the   th acm conference on information and knowledge management  cikm     pp 
        new york  ny  usa  acm 
msnbc         msnbc  news  video and progressive community  available at 
http   www msnbc msn com 
national institute of standards and technology      a   text analysis conference  tac  
available at  http   www nist gov tac  
national institute of standards and technology      b   text analysis conference  tac 
kbp      tracks  available at  http   www nist gov tac      kbp  
navigli  r          word sense disambiguation  a survey  acm comput  surv          
          
navigli  r     lapata  m          an experimental study of graph connectivity for
unsupervised word sense disambiguation  ieee trans  pattern anal  mach  intell  
               
   

fifernandez garca  arias fisteus   sanchez fernandez

nguyen  h     cao  t          exploring wikipedia and text features for named entity
disambiguation  in nguyen  n   le  m     swiatek  j   eds    intelligent information
and database systems  vol       of lecture notes in computer science  pp       
springer berlin   heidelberg 
nothman  j   murphy  t     curran  j  r          analysing wikipedia and gold standard
corpora for ner training  in proceedings of the   th conference of the european
chapter of the association for computational linguistics  eacl     pp         
stroudsburg  pa  usa  association for computational linguistics 
page  l   brin  s   motwani  r     winograd  t          the pagerank citation ranking 
bringing order to the web  technical report          stanford infolab 
pilz  a          entity disambiguation using link based relations extracted from
wikipedia  in first workshop on automated knowledge base construction  akbc
       grenoble  france 
ploch  d   hennig  l   de luca  e  w     albayrak  s          dai approaches to the
tac kbp      entity linking task  in proceedings of the knowledge base population  kbp  track of the  th text analysis conference  tac   national institute of
standards and technololgy  nist  
ponzetto  s  p     strube  m          knowledge derived from wikipedia for computing
semantic relatedness  j  artif  int  res                  
radford  w   hachey  b   nothma  j   honnibal  m     curran  j          cmcrc at
tac       document level entity linking with graph based re ranking  in proceedings of the  rd text analysis conference  tac   national institute of standards and
technology  nist  maryland  usa 
ratinov  l   roth  d   downey  d     anderson  m          local and global algorithms
for disambiguation to wikipedia  in proceedings of the   th annual meeting of the
association for computational linguistics  human language technologies   volume
   hlt     pp            stroudsburg  pa  usa  association for computational
linguistics 
sil  a          exploring re ranking approaches for joint named entityrecognition and
linking  in proceedings of the sixth workshop on ph d  students in information and
knowledge management  pikm     pp       
spitzer  f          principles of random walk   nd edition   springer 
van b  dang         ranklib  software package   available at 
http   people cs umass edu vdang ranklib html 
vanwinckelen  gitte  blockeel  h          on estimating model accuracy with repeated
cross validation  in proceedings of the   st belgian dutch conference on machine
learning  pp       
voorhees  e          trec   question answering track report  in proceedings of the  th
text retrieval conference  pp       
wikinews      a   wikinews random page generator  available at 
http   en wikinews org wiki special random 
   

fievaluation of link based approaches for candidate ranking in link to wikipedia

wikinews      b   wikinews  the free news source  available at 
http   en wikinews org  
wikipedia      a   wikipedia random page generator  available at 
http   en wikipedia org wiki special random 
wikipedia      b   wikipedia namespace   wikipedia  the free encyclopedia  available at 
http   en wikipedia org wiki wikipedia namespace 
yeh  e   ramage  d   manning  c  d   aguirre  e     soroa  a          wikiwalk  random
walks on wikipedia for semantic relatedness  in proceedings of the      workshop
on graph based methods for natural language processing  textgraphs    pp       
stroudsburg  pa  usa  association for computational linguistics 

   

fi