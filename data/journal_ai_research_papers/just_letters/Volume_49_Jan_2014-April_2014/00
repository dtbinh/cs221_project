journal artificial intelligence research               

submitted       published     

multimodal distributional semantics
elia bruni

elia bruni unitn it

center mind brain sciences 
university trento  italy

nam khanh tran

ntran l s de

l s research center 
hannover  germany

marco baroni

marco baroni unitn it

center mind brain sciences 
university trento  italy
department information engineering computer science 
university trento  italy

abstract
distributional semantic models derive computational representations word meaning
patterns co occurrence words text  models success
story computational linguistics  able provide reliable estimates semantic
relatedness many semantic tasks requiring them  however  distributional models
extract meaning information exclusively text  extremely impoverished
basis compared rich perceptual sources ground human semantic knowledge 
address lack perceptual grounding distributional models exploiting computer
vision techniques automatically identify discrete visual words images 
distributional representation word extended encompass co occurrence
visual words images associated with  propose flexible architecture
integrate text  image based distributional information  show set
empirical tests integrated model superior purely text based approach 
provides somewhat complementary semantic information respect latter 

   introduction
distributional hypothesis states words occur similar contexts semantically similar  claim multiple theoretical roots psychology  structuralist linguistics  lexicography possibly even later writings wittgenstein  firth       
harris        miller   charles        wittgenstein         however  distributional hypothesis huge impact computational linguistics last two decades mainly
empirical reasons  is  suggests simple practical method harvest
word meaning representations large scale  record contexts words
occur easy to assemble large collections texts  corpora  use contextual profiles surrogates meaning  nearly contemporary corpus based approaches
semantics rely contextual evidence one way another  systematic
extensive application distributional methods found call distributional
semantic models  dsms   known literature vector space semantic space
     ai access foundation  rights reserved 

fibruni  tran   baroni

models meaning  landauer   dumais        sahlgren        schtze        turney  
pantel        
dsms  meaning word approximated vector keeps track
patterns co occurrence word text corpora  degree semantic
similarity or  generally  relatedness  budanitsky   hirst        two words
precisely quantified terms geometric distance vectors representing
them  example  car automobile might occur terms street  gas
driver  thus distributional vectors likely close  cuing fact
words synonyms  extended empirical evidence shown distributional
semantics good harvesting effective meaning representations large scale 
confirming validity distributional hypothesis  see references section    
below  
still  successes  distributional semantics suffers obvious limitation
represents meaning word entirely terms connections words  long
tradition studies cognitive science philosophy stressed models
meaning symbols  e g   words  entirely accounted terms symbols  e g  
words  without links outside world  e g   via perception  deeply problematic  issue often referred symbol grounding problem  harnad        
dsms come attack lack grounding  glenberg   robertson 
        although specific criticisms vented might entirely well founded
 burgess         little doubt limitation textual contexts makes
dsms dissimilar humans  who  thanks senses  access rich sources
perceptual knowledge learning meaning words much cognitive scientists argued meaning directly embodied sensory motor processing
 see work de vega  glenberg    graesser        different views embodiment
cognitive science   indeed  last decades large amount behavioural neuroscientific evidence amassed indicating knowledge words concepts
inextricably linked perceptual motor systems  example  perceiving actiondenoting verbs kick lick involves activation areas brain controlling
foot tongue movements  respectively  pulvermueller         hansen  olkkonen  walter  gegenfurtner        asked subjects adjust color fruit images objects
appeared achromatic  objects generally adjusted color shifted
away subjects gray point direction opposite typical color fruit 
e g   bananas shifted towards blue subjects overcorrected typical
yellow color  typical color influences lexical access  example  subjects faster
naming pumpkin picture presented orange grayscale
representation  slowest another color  therriault  yaxley    zwaan        
final example  kaschak  madden  therriault  yaxley  aveyard  blanchard  zwaan
       found subjects slower processing sentence describing action
sentence presented concurrently visual stimulus depicting motion opposite
   harnard  original paper  discussing formal symbols  postulated fodors language
thought  fodor         rather words natural language  however  latter
represented terms connections words  case dsms  grounding problem
arises  follow recent literature issue referring symbol grounding 
symbols natural language words 

 

fimultimodal distributional semantics

direction described  e g   car approached harder process concurrently
perception motion away you   see review barsalou        review
evidence conceptual linguistic competence strongly embodied 
one might argue concerns dsms grounded embodied
exaggerated  overlook fact patterns linguistic co occurrence
exploited dsms reflect semantic knowledge acquired perception 
linguistic perceptual information strongly correlated  louwerse        
dogs often brown pink  likely talk brown dogs
pink dogs  consequently  child learn useful facts meaning concept
denoted dog direct perception linguistic input  this explains  among
things  congenitally blind subjects excellent knowledge color terms 
see  e g   connolly  gleitman    thompson schill         one could hypothesize
meaning representations extracted text corpora indistinguishable
derived perception  making grounding redundant  however  fairly
extensive literature showing case  many studies  andrews  vigliocco 
  vinson        baroni  barbu  murphy    poesio        baroni   lenci        riordan
  jones        underlined text derived dsms capture encyclopedic  functional
discourse related properties word meanings  tend miss concrete aspects 
intuitively  might harvest text information bananas tropical eatable 
yellow  because authors write obvious statements
bananas yellow   hand  studies show how  humans
asked describe concepts  features produce  equivalent sense contextual
features exploited dsms  preponderantly perceptual nature  bananas yellow 
tigers stripes  on  
discrepancy dsms humans not  per se  proof dsms
face empirical difficulties computational semantic models  however  interested
potential implications dsms models humans acquire use language
case many dsm developers  e g   griffiths  steyvers    tenenbaum       
landauer   dumais        lund   burgess        many others  complete
lack grounding perception serious blow psychological plausibility 
exposes criticism classic ungrounded symbolic models received 
even empirical level  reasonable expect dsms enriched perceptual
information would outperform purely textual counterparts  useful computational semantic models must capture human semantic knowledge  human semantic knowledge
strongly informed perception 
accept grounding dsms perception desirable avenue research 
must ask find practical source perceptual information embed dsms 
several interesting recent experiments use features produced human subjects concept
description tasks  so called semantic norms  surrogate true perceptual features
 andrews et al         johns   jones        silberer   lapata        steyvers        
reasonable first step  integration methods proposed studies
   perfectly fair  tendency might part triggered fact that  subjects asked
describe concepts  might encouraged focus perceptual aspects experimenters
instructions  example mcrae  cree  seidenberg  mcnorgan        asked subjects list first
physical properties  internal external parts   the object  looks 

 

fibruni  tran   baroni

quite sophisticated  using subject produced features unsatisfactory practically
theoretically  see however work reported kievit kylar   jones       
crowdsourcing project addressing kinds concerns   practically  using subjectgenerated properties limits experiments words denote concepts described
semantic norms  even large norms contain features hundred concepts 
theoretically  features produced subjects concept description tasks far removed
sort implicit perceptual features supposed stand for  example 
since expressed words  limited conveyed verbally 
moreover  subjects tend produce salient distinctive properties 
state dogs head  since thats hardly distinctive feature animal 
article  explore direct route integrate perceptual information
dsms  exploit recent advances computer vision  grauman   leibe       
availability documents combine text images automatically extract visual
features naturally co occurring words multimodal corpora  imagebased features combined standard text based features obtain perceptuallyenhanced distributional vectors  this  rely natural extension distributional hypothesis  encompasses similarity linguistic context 
similarity visual context  interestingly  landauer dumais  one classic papers
laid groundwork distributional semantics  already touched grounding
issue proposed  speculatively  solution along lines one implementing
here   i f one judiciously added numerous pictures scenes without rabbits
context columns          corpus matrix  filled handful appropriate cells
rabbit hare word rows   a dsm  could easily learn words rabbit hare
go pictures containing rabbits ones without  forth   landauer  
dumais        p        
although vision one source perceptual data  reasonable starting point 
convenience  availability suitable data train models 
probably dominating modality determining word meaning  one piece
evidence claim  widely used subject generated semantic norms mcrae et al 
       contain       distinct perceptual features total  and  these             
visual nature 
relatively low level noisy features extract images multimodal corpora contribute meaningful information distributional representation
word meaning  report results systematic comparison network semantic relations entertained set concrete nouns traditional text based
novel image based distributional spaces confirming image based features are  indeed 
semantically meaningful  moreover  expected  provide somewhat complementary
information respect text based features  thus found practical effective way extract perceptual information  must consider next combine textand image derived features build multimodal distributional semantic model 
propose general parametrized architecture multimodal fusion that  given appropriate
sample data  automatically determines optimal mixture text  image based features used target semantic task  finally  evaluate multimodal dsms
   thank mike jones pointing interesting historical connection us 

 

fimultimodal distributional semantics

two separate semantic tasks  namely predicting degree semantic relatedness assigned
word pairs humans  categorizing nominal concepts classes  show
tasks multimodal dsms consistently outperform purely textual models  confirming
supposition that  humans  performance computational models
meaning improves meaning grounded perception 
article structured follows  section   provides relevant background
computational linguistics image analysis  discusses related work  lay
general architecture multimodal fusion distributional semantics section   
necessary implementation details provided section    section   presents experiments tested approach  section   concludes summarizing current
results well sketching come next 

   background related work
section first give brief introduction traditional distributional semantic models
 i e   based solely textual information   then  describe image analysis
techniques adopt extract manipulate visual information  next  discuss earlier
attempts construct multimodal distributional representation meaning  finally 
describe relevant strategies combine information coming text images
proposed inside computer vision community 
    distributional semantics
last decades  number different distributional semantic models  dsms  word
meaning proposed computational linguistics  relying assumption
word meaning learned directly linguistic environment 
semantic space models one common types dsm  approximate
meaning words vectors record distributional history corpus
 turney   pantel         distributional semantic model encoded matrix whose
rows semantic vectors representing meanings set target words 
component semantic vector function occurrence counts corresponding
target word certain context  see lowe        formal treatment   definitions
context range simple ones  such documents occurrence another word
inside fixed window target word  linguistically sophisticated ones  such
occurrence certain words connected target special syntactic relations 
 pad   lapata        sahlgren        turney   pantel         raw targetcontext counts collected  transformed association scores typically
discount weights components whose corresponding word context pairs high
probability chance co occurrence  evert         rank matrix containing
semantic vectors rows optionally decreased dimensionality reduction 
might provide beneficial smoothing getting rid noise components and or allow
efficient storage computation  landauer   dumais        sahlgren        schtze 
       finally  distributional semantic similarity pair target words estimated
similarity function takes semantic vectors input returns scalar
similarity score output 
 

fibruni  tran   baroni

many different semantic space models literature  probably best
known latent semantic analysis  lsa  landauer   dumais         highdimensional semantic space words derived use co occurrence information
words passages occur  another well known example
hyperspace analog language model  hal  lund   burgess         word
represented vector containing weighted co occurrence values word
words fixed window  semantic space models rely syntactic relations instead
windows  grefenstette        curran   moens        pad   lapata         general
overviews semantic space models provided clark         erk         manning
schtze         sahlgren        turney pantel        
recently  probabilistic topic models receiving increasing attention
alternative implementation dsms  blei  ng    jordan        griffiths et al         
probabilistic topic models rely co occurrence information large corpora derive meaning but  differently semantic space models  based assumption
words corpus exhibit probabilistic structure connected topics  words
represented points high dimensional space probability distribution
set topics  conversely  topic defined probability distribution
different words  probabilistic topic models tackle problem meaning representation
means statistical inference  use word corpus infer hidden topic structure 
distributional semantic models  whether geometric probabilistic kind 
ultimately mainly used provide similarity score arbitrary pairs words 
employ them  indeed  models shown effective
modeling wide range semantic tasks including judgments semantic relatedness
word categorization 
several data sets assess well dsm captures human intuitions
semantic relatedness  rubenstein goodenough set  rubenstein   goodenough        wordsim     finkelstein  gabrilovich  matias  rivlin  solan  wolfman 
  ruppin         usually constructed asking subjects rate set word
pairs according similarity scale  then  average rating pair taken
estimate perceived relatedness words  e g   dollar buck        cord smile 
       measure well distributional model approximates human semantic intuitions 
usually correlation measure similarity scores generated model
human ratings computed  highest correlation aware wordsim   
set employ      obtained model called temporal
semantic analysis  captures patterns word usage time concepts
represented time series corpus temporally ordered documents  radinsky 
agichtein  gabrilovich    markovitch         temporal knowledge could integrated
perceptual knowledge encode model  direct comparison point 
agirre  alfonseca  hall  kravalova  pasa  soroa        presented extensive evaluation distributional wordnet based semantic models wordsim  achieving
maximum correlation      across various parameters  
   wordnet  available http   wordnet princeton edu   large computational lexicon english
nouns  verbs  adjectives adverbs grouped sets synonyms  synsets   expressing
distinct concept 

 

fimultimodal distributional semantics

humans good grouping together words  or concepts denote 
classes based semantic relatedness  murphy         therefore cognitive aware
representation meaning must show proficiency categorization  e g   poesio
  almuhareb        baroni et al          concept categorization moreover useful
applications automated ontology construction recognizing textual entailment 
unlike similarity ratings  categorization requires discrete decision group coordinates cohyponyms class performed applying standard clustering techniques
model generated vectors representing words categorized  example 
almuhareb poesio data set  almuhareb   poesio         employ below 
includes     concepts wordnet  balanced terms frequency degree ambiguity  distributional model rothenhusler schtze        exploits syntactic
information reach state of the art performance almuhareb poesio data set  maximum clustering purity across various parameter         window based distributional
approach baroni lenci         directly comparable text based models 
achieves      purity 
semantic tasks dsms applied include semantic priming  generation
salient properties concepts intuitions thematic fit verb arguments  see 
e g   baroni   lenci        baroni et al         mcdonald   brew        pad   lapata 
      pad  pad    erk         distributional semantic vectors used wide
range applications require representation word meaning  particular
objective measure meaning relatedness  including document classification  clustering
retrieval  question answering  automatic thesaurus generation  word sense disambiguation 
query expansion  textual advertising areas machine translation  dumais       
turney   pantel        
    visual words
ideally  build multimodal dsm  would extract visual information
images way similar text  thanks well known image
analysis technique  namely bag of visual words  bovw   indeed possible discretize
image content produce visual units somehow comparable words text  known
visual words  bosch  zisserman    munoz        csurka  dance  fan  willamowski   
bray        nister   stewenius        sivic   zisserman        yang  jiang  hauptmann 
  ngo         therefore  semantic vectors extracted corpus images
associated target  textual  words using similar pipeline commonly
used construct text based vectors  collect co occurrence counts target words
discrete image based contexts  visual words   approximate semantic relatedness
two words similarity function visual words representing them 
bovw technique extract visual word representations documents inspired
traditional bag of words  bow  method information retrieval  bow turn
dictionary based method represent  textual  document bag  i e   order
considered   contains words dictionary  bovw extends idea visual
documents  namely images   describing collection discrete regions  capturing
appearance ignoring spatial structure  the visual equivalent ignoring word
order text   bag of visual word representation image convenient image 

fibruni  tran   baroni






ffffff

fififi






figure    representing images bovw   i  salient image patches keypoints contain rich local information detected represented vectors low level
features called descriptors   ii  descriptors mapped visual words
basis distance centers clusters corresponding visual words


 the preliminary clustering step shown
figure    iii  images finally
represented bag of visual words feature vector according distribution
visual words contain  images depicting things rotations 
occlusions  small differences low level descriptors might still similar
distribution visual words  hence object traced robustly
across images conditions change 

analysis point view translates usually large set high dimensional local
descriptors single sparse vector representation across images  importantly  size
original set varies image image  bag of visual word representation
fixed dimensionality  therefore  machine learning algorithms default expect
fixed dimensionality vectors input  e g   supervised classification unsupervised
clustering  used tackle typical image analysis tasks object recognition 
image segmentation  video tracking  motion detection  etc 
specifically  similarly terms text document  image local interest
points keypoints defined salient image patches contain rich local information
image  however keypoint types images come off the shelf word
 

fimultimodal distributional semantics

types text documents  local interest points grouped types  i e   visual
words  within across images  image represented number
occurrences type it  analogously bow  following pipeline typically
followed  every image data set  keypoints automatically detected  note
recent approaches dense  pixelwise sampling keypoints preferred
detecting salient ones only  solution adopt 
explained section        represented vectors low level features called descriptors 
keypoint vectors grouped across images number clusters based
similarity descriptor space  cluster treated discrete visual word 
keypoints mapped onto visual words  image represented bovw feature
vector recording many times visual word occurs it  way  move
representing image varying number high dimensional keypoint descriptor vectors
representation terms single visual word count vector fixed dimensionality
across images  advantages discussed above  visual word assignment
use represent image content exemplified figure    two images
similar content described terms bag of visual word vectors 
kind image content visual word captures exactly depends number
factors  including descriptors used identify represent keypoints  clustering
algorithm number target visual words selected  general  local interest points
assigned visual word tend patches similar low level appearance 
local patterns need correlated object level parts present images
 grauman   leibe        
    multimodal distributional semantics
availability large amounts mixed media web  one hand 
discrete representation images visual words  other  escaped attention
computational linguists interested enriching distributional representations word
meaning visual features 
feng lapata        propose first multimodal distributional semantic model 
generative probabilistic setting requires extraction textual visual features
mixed media corpus  latent dimensions estimated
probabilistic process assumes document generated sampling textual
visual words  words represented distribution set latent
multimodal dimensions topics  griffiths et al         derived surface textual
visual features  feng lapata experiment collection documents downloaded
bbc news website corpus  test semantic representations
free association norms nelson  mcevoy  schreiber        subset    
pairs wordsim  obtaining gains performance visual information taken
account  correlations human judgments           respectively   compared
textual modality standalone            respectively   even performance still
well state of the art wordsim  see section     above  
main drawbacks approach textual visual data must
extracted corpus  thus limiting choice corpora used 
generative probabilistic approach  elegant  allow much flexibility
 

fibruni  tran   baroni

two information channels combined  below  re implement feng
lapata method  mixlda  training esp game data set  source labeled
images adopt model  possible data set contains images
textual labels describing them  general  recapture feng lapatas
idea common latent semantic space latent multimodal mixing step pipeline
 see section       below  
leong mihalcea        exploit textual visual information obtain multimodal distributional semantic model  feng lapata merge two sources
information learning joint semantic model  leong mihalcea propose strategy
akin call scoring level fusion below  come separate text 
image based similarity estimates  combine obtain multimodal score 
particular  use two combination methods  summing scores computing
harmonic mean  differently feng lapata  leong mihalcea extract visual information corpus manually coded resource  namely imagenet
database  deng  dong  socher  li    fei fei         large scale ontology images   using
handcoded annotated visual resource imagenet faces sort problems
using manually developed lexical database wordnet faces respect
textual information  is  applications severely limited imagenet coverage  for
example  imagenet currently restricted nominal concepts   interest
model computational simulation word meaning acquisition naturally occurring language visual data somewhat reduced  humans learn meaning
mountain set carefully annotated images mountains little else crowding
occluding scene   evaluation  leong mihalcea experiment small subsets wordsim  obtaining improvements  although level report
 the highest reported correlation         word pairs   furthermore use
data set tune test models 
bruni  tran  baroni        propose instead directly concatenate textand image based vectors produce single multimodal vector represent words 
call feature level fusion below  text based distributional vector representing word  taken state of the art distributional semantic model  baroni  
lenci         concatenated vector representing word visual features  extracted images esp game collection use here 
obtain promising performance wordsim test sets  although appreciably lower
results report  we obtain maximum correlation      text 
image based features used together  compare table   below  
attempts use multimodal models derived text images perform
specific semantic tasks reported  bergsma goebel        use textual
image based cues model selectional preferences verbs  which nouns likely
arguments verbs   experiment shows several cases visual information
useful text task  example  looking textual corpora words
carillon  migas mamey  much useful information obtained guess
three plausible argument verb eat  hand  show
   http   image net org 

  

fimultimodal distributional semantics

that  exploiting google image search functionality   enough images words
found vision based model edible things classify correctly 
finally  evaluate multimodal models task discovering color concrete objects  showing relation words denoting concrete things
typical color better captured visual information taken account  bruni 
boleda  baroni    tran         moreover  show multimodality helps distinguishing literal nonliteral uses color terms 
    multimodal fusion
textual information used image analysis  mostly done different
aims ours  text used improve image related tasks  typically
attempt model relation specific images specific words textual passages
 e g   barnard  duygulu  forsyth  de freitas  blei    jordan        berg  berg    shih 
      farhadi  hejrati  sadeghi  young  rashtchian  hockenmaier    forsyth        griffin 
wahab    newell        kulkarni  premraj  dhar  li  choi  berg    berg        
contrast   i  want use image derived features improve representation word
meaning  ii  interested capturing meaning word types basis
sets images connected word  model specific word image relations 
despite differences  challenges addressed image analysis literature deals exploiting textual cues similar ones face  particular 
problem merging  fusing  textual visual cues common representational
space exactly face construct multimodal semantic space 
traditionally  image analysis community distinguishes two classes fusion
schemes  namely early fusion late fusion  former fuses modalities feature space 
latter fuses modalities semantic similarity space  analogously call
feature level scoring level fusion  respectively  example  escalante  hrnadez 
sucar  montes        propose image retrieval system multimodal documents 
early late fusion strategies combination image textual
channels considered  early fusion settings include weighted linear combination
two channels global strategy different retrieval systems used contemporarily
entire  joint data set  late fusion strategies include per modality strategy 
documents retrieved using one channel hierarchical setting
first text  image combination used independently query database
results aggregated four weighted combinations  vreeswijk  huurnink 
smeulders        train visual concept classifier abstract subject categories
biology history using late fusion approach image text information
combined output level  is  first obtaining classification scores image 
text based models separately joining them  similarly multimodal mixing
step  pham  maillot  lim  chevallet        caicedo  ben abdallah  gonzlez 
nasraoui        propose early fusion two inputs mapped onto
latent space using dimensionality reduction techniques  e g   singular value decomposition  
multimodal representation obtained way directly used retrieve image
documents 
   http   images google com 

  

fibruni  tran   baroni

   framework multimodal distributional semantics
section  general flexible architecture multimodal semantics presented 
architecture makes use distributional semantic models based textual visual
information build multimodal representation meaning  merge two sources 
uses parameter based pipeline able capture previously proposed combination
strategies  advantage explored within single system 
    input multimodal architecture
construct multimodal representation meaning  semantic model single
modality implemented  independently actual parameters chosen
creation  that  point view  black box   requirements
model satisfy order guarantee good functioning framework 
first place  modality must provide separate representation  leave room
various fusion strategies afterwards  then  modality must encode semantic
information pertaining word interest fixed size vectorial representation 
moreover  assume text  image based vectors normalized arranged
matrices words rows co occurring elements columns 
follows  assume harvested matrix text based semantic vectors 
one image based semantic vectors set target words  representing 
respectively  verbal visual information words  section   give
details specific implementation construct matrices 
    multimodal fusion
pipeline based two main steps 
    latent multimodal mixing  text vision matrices concatenated  obtaining single matrix whose row vectors projected onto single  common space
make interact 
    multimodal similarity estimation  information text  image based
matrices combined two ways obtain similarity estimates pairs target
words  feature level scoring level 
figure   describes infrastructure propose fusion  first  introduce mixing
phase promote interaction modalities call latent multimodal mixing 
step part approaches would consider feature level fusion  see
below   keep separated might benefit scoring level fusion well 
mixing performed  proceed integrate textual visual features 
reviewed section     above  literature fusion performed two main levels 
feature level scoring level  first case features first combined
considered single input operations  second case task performed separately
different sets features separate results combined  approach
advantages limitations incorporated
multimodal infrastructure together constitute call multimodal similarity
  

fimultimodal distributional semantics






































figure    multimodal fusion combining textual visual information semantic
model 



estimation  feature level approach requires one learning step  i e   determining
parameters feature vector combination  offers richer vector based representation
combined information  used purposes  e g   image text
features could used together train classifier   benefits scoring level approach
include possibility different representations  in principle  even vectorial 
different similarity scores different modalities ease increasing  or decreasing 
number different modalities used representation 
      latent multimodal mixing
preparatory step textual visual components projected
onto common representation lower dimensionality discover correlated latent factors 
result new connections made source matrix taking account
information connections present matrix  originating patterns covariance overlap  importantly  assume mixing done via dimensionality
reduction technique following characteristics  parameter k determines
  

fibruni  tran   baroni

dimensionality reduced space fact k equals rank
original matrix reduced matrix identical considered good approximation
original one  commonly used singular value decomposition reduction method
adopt mixing step satisfies constraints 
toy example mixing might beneficial  consider concepts pizza
coin  could use features text based semantic vectors  i e   record cooccurrences target words concepts part vector dimensions  
words likely occur similar contexts text  obviously visually similar  so  original text features pizza coin might highly correlated 
however  mixing multimodal space  might associated  have high
weights on  reduced space component  similar distributions
visual features cue roundness  consequently  two textual features originally
uncorrelated might drawn closer multimodal mixing  corresponding concepts visually similar  resulting mixed textual features are  sense 
visually enriched  vice versa mixed visual features  interestingly  psychologists
shown that  certain conditions  words pizza coin  strongly
associated perceptually similar  prime other  e g   pecher  zeelenberg    raaijmakers        
note matrices obtained splitting reduced rank matrix back
original textual visual blocks number feature columns original
textual visual blocks  values smoothed dimensionality
reduction  we explain details achieved specific implementation
next paragraph   matrices used calculate similarity score word
pair  re  merging information feature scoring levels 
mixing svd implementation  perform mixing across text  imagebased features applying singular value decomposition  svd   matrix obtained concatenating two feature types row wise  so row concatenated
matrix describes target word textual visual space   svd widely used technique
find best approximation original data points space lower underlying
dimensionality whose basis vectors  principal components latent dimensions 
selected capture much variance original space possible  manning 
raghavan    schtze        ch       performing svd concatenated textual
visual matrices  project two types information space 
described linear combinations principal components  following description
pham et al          svd matrix rank r factorization form
  u v




u   matrix eigenvectors derived




  r r diagonal matrix singular values

  square roots eigenvalues







v   matrix eigenvectors derived

   computed svdlibc  http   tedlab mit edu  dr svdlibc 

  

fimultimodal distributional semantics

context  matrix given normalizing two feature matrices separately
concatenating  selecting k largest values matrix keeping
corresponding columns matrices u v   reduced matrix mk given
mk   uk k vkt
k   r dimensionality latent space  mk keeps number
columns dimensions   rank k  k free parameter tune
development sets  note k equals rank original matrix  trivially
mk     thus consider performing svd reduction special case
svd  helps searching optimal parameters 
note that  n columns  vkt k n matrix  mk
number columns   first j columns contain textual features  columns
j     n contain visual features  hold mk   although latter
values features affected global svd smoothing  thus 
current implementation pipeline figure    block splitting attained simply
dividing mk textual mixed matrix containing first j columns  visual mixed
matrix containing remaining columns 
      multimodal similarity estimation
similarity function following distributional hypothesis  dsms describe word
terms contexts occurs  therefore  measure similarity two words
dsms need function capable determining similarity two descriptions  i e  
two semantic vectors   literature  many different similarity functions
used compare two semantic vectors  including cosine similarity  euclidean distance  l 
norm  jaccards coefficient  jensen shannon divergence  lins similarity  extensive
evaluation different similarity measures  see work weeds        
focus cosine similarity since shown effective measure
many semantic benchmarks  bullinaria   levy        pad   lapata         also 
given system based geometric principles  cosine  together euclidean
distance  principled choice measure similarity  example 
measures listed above  developed probabilistic considerations 
applicable vectors encode well formed probability distributions  typically
case  for example  multimodal mixing  vectors might contain negative
values  
cosine two semantic vectors b dot product divided product
lengths 
pi n

i   ai bi
qp
i n  
i n  


i  
i   bi

cos a  b    qp

cosine ranges    orthogonal vectors       parallel vectors pointing
opposite directions cosine values       respectively  
  

fibruni  tran   baroni

feature level fusion feature level fusion  fl   use linear weighted fusion
method combine text  image based feature vectors words single representation use latter estimate similarity pairs  linear weighted
combination function defined
f   ft      fv
vector concatenate operator 
scoring level fusion scoring level fusion  sl   text  image based matrices
used estimate similarity pairs independently  scores combined obtain
final estimate using linear weighted scoring function 
  st        sv
general form special cases given fixed normalized text  image based
matrices  multimodal approach parametrized k  dimensionality latent space  
fl vs  sl   weight text component fl similarity estimation   weight text
component sl  
note k r  r rank original combined matrix  latent multimodal
mixing returns original combined matrix  no actual mixing   picking sl   
   corresponds using textual visual matrix only  respectively  thus derive
special cases models text  k r  sl      images  k r  sl     
used  called text image models results section below   simple approach
bruni et al          two matrices concatenated without mixing 
parametrization k r  fl        called naivefl model  below   summing approach
leong mihalcea        corresponds k r  sl        naivesl  below   picking
k r  sl     amounts performing latent multimodal mixing  using textual
features only  reverse mixed image features      textmixed
imagemixed   respectively   reducing models parametrized
approach means that  given development set specific task requires similarity
measurements  discover data driven way various models best
task hand  for example  certain task might discover better
using text only  another mixed text features  yet another text image
features  on  
formally  given set k         kn r n dimensionalities latent space  with kn
equal original dimensionality  arbitrary steps chosen values  
sets          r potential weights text block fl  with           
         l r l weights text block sl  with       l      
calculate number possible configurations explore totc   n m   l   unless n 
l large  i e   consider small intervals values tested  
completely feasible perform full search best parameters certain task
without approximate optimization methods  experiments  n        l      
consequently totc       
  

fimultimodal distributional semantics

   implementation details
implementation multimodal framework visual feature extraction
procedure publicly available open source   moreover visual feature extraction
procedure presented bruni  bordignon  liska  uijlings  sergienya        
    construction text based semantic matrix
reviewed section     above  text based distributional model encoded matrix whose rows semantic vectors representing meaning set target words 
important parameters model choice target contextual elements 
source corpora used extract co occurrence information  context delimiting
scope co occurrence  function transform raw counts statistical association scores downplaying impact frequent elements 
source corpora collect co occurrence counts concatenation two corpora 
ukwac wackypedia  size     b    m running words  tokens  respectively  
ukwac collection web pages based linguistically controlled crawl  uk
domain conducted mid     s  wackypedia built mid      dump
english wikipedia  corpora automatically annotated lemma  dictionary form  part of speech  pos  category information using treetagger  
freely publicly available    widely used linguistic research 
target context elements since source corpora annotated lemma
part of speech information  take account extracting target context
words  e g   string sang treated instance verb lemma sing   collect
semantic vectors set   k target words  lemmas   namely top   k frequent
nouns   k frequent adjectives  k frequent verbs combined corpora 
  k lemmas employed contextual elements  consequently  textbased semantic models encoded   k  k matrix   note combine
text matrices image based ones  preserve rows  target words 
image based vector  trimming matrix size         k 
context define context terms words co occur within window fixed
width  tradition popular hal model  lund   burgess         window based
models attractive simplicity fact require resourceintensive advanced linguistic annotation  moreover reported
state art various semantic tasks  rapp        sahlgren         bruni 
uijlings  baroni  sebe        show window based methods use
outperform document as context model sophisticated syntax  lexicalpattern based model men wordsim test sets introduced section    
 see post hoc analysis using document based model discussed end
section       below   consider two variants  window  window    we chose
particular variants arbitrarily  representatives narrow wide windows  respectively  
   see https   github com s m fuse  https   github com vsem   respectively 
   http   www ims uni stuttgart de projekte corplex treetagger 
    http   wacky sslmit unibo it 

  

fibruni  tran   baroni

window  records sentence internal co occurrence nearest   content words left
right target word  function words articles prepositions ignored  
window   considers larger window    content words left right target 
narrower window expected capture narrower kind semantic similarity 
one exists terms closely taxonomically related  example coordinate
concepts  dog cat  pairs superordinate subordinate concepts  animal
dog   rationale behind expectation terms share many narrow window
collocates similar  semantically syntactically 
hand  broader window capture broader kind topical similarity  one
would expect words tend occur paragraphs  for example  war oil 
rather distant concepts taxonomic sense  might easily occur
discourse   see work sahlgren        discussion effects context
width distributional semantic models 
association score transform raw co occurrence counts nonnegative local mutual information  lmi  association scores  lmi scores obtained multiplying raw
counts pointwise mutual information  nonnegative case close approximation log likelihood ratio scores  one widely used weighting
schemes computational linguistics  evert         nonnegative lmi target element
context element c defined as 


p t  c 
  
p t p c 



lm i t  c    max count t  c  log

worth observing that  extensive study parameters affect quality
semantic vectors  bullinaria levy        bullinaria levy        found
model similar window   co occurrence statistics ukwac  narrow window 
lemmatized content word collocates  nonnegative pointwise mutual information instead
lmi  performs near top variety semantic tasks  thus  independent
grounds claim using state of the art text based model 
    construction image based semantic matrix
given image based semantic vectors novelty respect text based ones 
next subsections dedicate space constructed them  including
full details source corpus utilize input pipeline  section        
particular image analysis technique choose extract visual collocates finally
arrange semantic vectors constitute visual block distributional
semantic matrix  section        
      image source corpus
adopt source corpus esp game data set   contains    k images  labeled
famous game purpose developed louis von ahn  two
    http   www cs cmu edu  biglou resources 

  

fimultimodal distributional semantics


fffi fiff
fifi fffi
fifi

fffi
fififf







fififffi

fifi


fifi






figure
   samples images tags esp game data set

people partnered online must independently rapidly agree appropriate word
label random selected images  word entered partners certain number
game rounds  word added tag image  becomes taboo term
next rounds game involving image  encourage players produce
terms describing image  von ahn         tags images data set form
vocabulary        distinct word types  images    tags average       standard
deviation   word tag    images average         standard deviation  
words format text based models  tags lemmatized pos tagged  annotate words parts speech  could
run pos tagger  since words context  i e   tag appears alphabetically
within small list words labeling image within ordinary sentence
required pos tagger   thus used heuristic method  assigned words
esp game vocabulary frequent tag textual corpora 
esp game corpus interesting data set point view since 
one hand  rather large know tags contains related images 
hand  product experts labelling representative images 
noisy annotation process often poor quality uninteresting images  e g   logos  randomly
downloaded web  thus  analogously characteristics textual corpus 
algorithms must able exploit large scale statistical information  robust
noise  cleaner illustrative examples concept available
carefully constructed databases imagenet  see section       noisy tag annotations
  

fibruni  tran   baroni

available massive scale sites flickr   facebook    want
eventually exploit data important methods work noisy input 
advantage esp game respect imagenet images associated
concrete noun categories adjectives  verbs nouns related
events  e g   vacation  party  travel  etc   practical point view  clean
data sets imagenet still relatively small  making experimentation standard
benchmarks difficult  concrete  looking benchmarks experiment with  mid
      imagenet covers half pairs wordsim    test set  less
    almuhareb poesio words  future want explore
extent higher quality data sources improve image based models  require larger
databases  benchmarks relying restricted vocabulary 
image samples figure   exemplify different kinds noise characterize
esp game data set  top bottom left top right images
scene cluttered partially occluded  top center image hardly good representative accompanying words building  tower s  square  similarly  center
bottom image partially good illustration coin  certainly good
example man  finally  bottom right image useless visual feature extraction
perspective 
      image based semantic vector construction
collect co occurrence counts target words image based contexts adopting
bovw pipeline that  already explained      particularly convenient order
discretize visual information visual collocates  adopting currently
considered standard implementation bovw  future  could explore
cutting edge ways build image based semantic vectors  local linear encoding
 wang  yang  yu  lv  huang    gong        fisher encoding  perronnin  sanchez   
mensink         chatfield  lempitsky  vedaldi  zisserman        present systematic
evaluation several recent methods 
current implementation composed following steps   i  extraction
local descriptors  is  vectors low level features encode geometric
information area around keypoint  i e   pixel interest  here  sift descriptors    ii  constructing vector representation image assigning
local descriptors clusters corresponding visual words  recording distribution
across clusters vector  this presupposes preliminary step clustering
algorithm applied whole image collection sample  determine visual word vocabulary   iii  including spatial information representation
spatial binning   iv  summing visual word occurrences across list images associated
word label obtain co occurrence counts associated word label
transforming counts association scores  analogously done text
analysis  process  without spatial binning  schematically illustrated figure   
hypothetical example three images collection labeled
word monkey  details follow 
    http   www flickr com
    http   www facebook com

  

fimultimodal distributional semantics

dense sampling
pixels
interest

mapping sift
descriptors visual
word clusters

extracting
local
descriptors

sift  x 

monkey 

 

 

monkey

 

 

 

 

 

labeled
images

monkey 

 

  

instance
counts

 

monkey
monkey 

 

 

 

 

monkey 

 

  

  

 

monkey
total
counts

figure    procedure build image based semantic vector target word  first 
bag of visual word representation image labeled target word
computed  in case  three images labeled target word monkey  
then  visual word occurrences across instance counts summed obtain
co occurrence counts associated target word 

local descriptors construct local descriptors pixels interest use scaleinvariant feature transform  sift   lowe               chose sift invariance
image scale  orientation  noise  distortion partial invariance illumination changes 
sift vector formed measuring local image gradients region around
location orientation feature multiple scales  particular  contents
    sampling subregions explored around keypoint  resulting
   samples  magnitude gradients   orientations calculated  would
already result sift feature vector     components  however  extract color
sift descriptors hsv  hue  saturation value  space  bosch  zisserman    munoz 
       use hsv encodes color information similar way humans
  

fibruni  tran   baroni

do  compute sift descriptors hsv component  gives      dimensions
per descriptor      per channel  color channels averaged obtain final
    dimensional descriptors  experimented different color scales 
luv  lab rgb  obtaining significantly worse performance compared hsv
development set introduced        therefore conduct experiments
them  van de sande  gevers  snoek        present systematic evaluation color
features 
instead searching interesting keypoints salient patch detection algorithm 
use computationally intensive thorough dense keypoint sampling
approach  patches fixed size localized regular grid covering whole image
repeated multiple scales  sift descriptors computed regular grid every
five pixels  four scales                 pixel radii  zeroing low contrast descriptors 
extraction use vl phow command included vlfeat toolbox  vedaldi
  fulkerson         implementation shown close lowes original
much faster dense feature extraction  nowak  jurie  triggs        report
systematic evaluation different patch sampling strategies 
importantly  sift feature vectors extracted large corpus representative
images populate feature space  subsequently quantized discrete number
visual words clustering  step performed  every sift vector  local descriptor 
original new images translated visual word determining
cluster nearest quantized space 
visual vocabulary map sift descriptors visual words  first cluster local
descriptors extracted images training image corpus      dimensional
space using k means clustering algorithm  encode descriptor index
cluster  visual word  belongs  k means common way constructing
visual vocabularies  grauman   leibe         given set x         xn rd n training
descriptors  k means aims partition n descriptors k sets  k n  minimize
p
cumulative approximation error ni     xi qi       k centroids          k rd
data to means assignments q         qn          k   use approximated version
k means called lloyds algorithm        implemented vlfeat toolbox 
construct visual vocabulary extracted sift descriptors    k
images esp game data set  tune parameter k used men development
set  see section         varying k          steps      found
optimal k       likely performance peaked even     
visual words enhancements could attained adopting larger visual vocabularies
via efficient implementations bovw pipeline  example chatfield et al 
       
image representation given set descriptors x         xn sampled image  let
qi assignment descriptor xi corresponding visual word  bag ofvisual words representation image nonnegative vector v rk vk     i  
qi   k    q ranging   number visual words vocabulary  in
case         representation vector visual words obtained via hard quantization
 i e   assignment local descriptor vector single nearest codeword  
  

fimultimodal distributional semantics

spatial binning consolidated way introducing weak geometry bovw use
spatial histograms  grauman   darrell        lazebnik  schmid    ponce        
main idea divide image several  spatial  regions perform entire visual
word extraction counting pipeline region concatenate vectors 
experiments spatial regions obtained dividing image      total
   regions  therefore  crossing values k spatial region  increase
feature dimensions    times  total        components vectors 
co occurrence counts weighting bovw representations built 
target  textual  word associated list images labeled it 
visual word occurrences across list images summed obtain co occurrence
counts associated target  textual  word  total         target words  those
constitute esp game tags  image based semantic vector associated 
image based semantic matrix  text based one  raw counts
transformed nonnegative lmi  difference lmi computed
target element textual word context element c visual word instead 
note that  standard textual approach  accumulating visual words
images contain word without taking account fact words might
denote concepts multiple appearances  polysemous even hide homonyms
 our bank vector include visual words extracted river well building pictures  
interesting direction research would cluster images associated
word order distinguish visual senses word  e g   along lines
done textual models reisinger mooney        
    multimodal fusion tuning
performed two separate parameter optimizations  one specifically semantic relatedness task  using men development  see section        specifically
clustering task  using battig  see section         determined best model
performing exhaustive search across svd k  from        powers     fl sl
varying      inclusive  steps     similarly   total      models explored one highest performance development data
chosen  note tuning performed separately window  window   models 

    mixlda
reimplement feng lapatas approach  discussed section      comparable
setting ours  treat esp game data set mixed media corpus
image together associated tags constitutes document  image  extract
image based features procedure described       use words
labeling image obtain text based features  features stored
term by document matrix  image treated document term
either textual tag visual word extracted image  obtain matrix size
  k   k    k textual words  the word list resulting intersection
words used experimental data sets     k visual words    k documents  images  
  

fibruni  tran   baroni

latent dirichlet allocation  mixlda  model trained matrix tuned
men development set varying number topics kt     optimal value find
kt        mixlda  target word evaluation set represented
vector giving distribution     latent topics 

   experiments
test semantic representation three different tasks  is  evaluating distribution different kinds semantic relations among words neighbours        modeling
word relatedness judgments       clustering words superordinate concepts       
together  tasks give us clear idea general quality models
relative contribution visual information meaning representation 
    differentiation semantic relations
acquire qualitative insight well text  image based models capturing word meaning  test bless  baroni lenci evaluation semantic similarity   benchmark recently introduced baroni lenci        analyze specific
aspects lexico semantic knowledge  rather focusing point estimate quality
model specific semantic task  bless allows us assess overall pattern
semantic relations model tends capture  run bless evaluation
combining textual visual channels together sanity check semantic
meaningfulness image based vectors  looking potential complementary information respect text motivate fusion  note since
combining textual visual sources  tuning parameters report 
      benchmark method
bless contains set     pivot words denoting concrete concepts  we use     pivots 
since remaining    sufficiently large set related words covered
models   pivots  data set contains number related words 
relata  instantiating following   common semantic relations pivots  coord 
relatum noun co hyponym  coordinate  pivot  alligator lizard  
hyper  relatum noun hypernym  superordinate  pivot  alligatorreptile   mero  relatum noun referring meronym  is  part material
pivot  alligator teeth   attri  relatum adjective expressing attribute
pivot  alligator ferocious   event  relatum verb referring action
event involving concept  alligator swim   ran n  ran j ran v  finally  control
cases pivot matched set random nouns  alligator trombone   adjectives
 alligator electronic  verbs  alligator conclude   respectively 
pivot  bless contains set relata category  ranging   hypernyms    random nouns per pivot average   way  bless highlight
broader semantic properties model independently specific preferences 
example  model assigns high score alligator ferocious model
assigns high score alligator green correctly treated models picked
    lda computed gensim  http   radimrehurek com gensim 

  

fimultimodal distributional semantics

relevant attribute alligators  time  comparison specific relata
selected models allows granular qualitative analysis differences 
following guidelines baroni lenci         analyze semantic model
follows  compute cosine model vectors representing    
pivots relata  picking relatum highest cosine
  relations  the nearest hypernym  nearest random noun  etc    transform
  similarity scores collected way pivot onto standardized z scores  to
get rid pivot specific effects   produce boxplot summarizing distribution
scores per relation across     pivots  for example  leftmost box first panel
figure   reports distribution     standardized cosines nearest coordinate relata
respective pivots   besides analyzing distributions qualitatively  discuss
significant differences cosines different relation types obtained via
tukeys honestly significance tests  thus correcting multiple pairwise comparisons  abdi
  williams        
      results
fig     report bless nearest relata distributions purely textual model window    the window  distribution shows even stronger skew favour coordinate
neighbours  purely visual model call image next sections  patterns
produced text based model  left panel  illustrate sensible word meaning profile
look like  coordinates similar terms  an alligator maximally similar
crocodile   followed superordinates  reptile  parts  teeth   semantically related
adjectives  attri  ferocious  verbs  event  swim  less close pivots  still
random item 
right panel shows distribution relata image based semantic vectors 
overall pattern quite similar one observed text based vectors 
clear preference coordinates  followed hypernyms parts  attributes
events  random relata away pivots semantically
meaningful categories  models  coordinates significantly closer relata
hypernyms meronyms  significantly closer attributes events 
turn significantly closer random category  although difference
hypernyms parts significant either representation  intriguingly
image based vectors show slight preference imageable parts  teeth 
abstract hypernyms  reptile   difference statistical import one
events attributes  text based model shows significant preference
events  whereas two categories statistically indistinguishable image based
model  as see shortly  relative preference latter attributes probably
due tendency pick perceptual adjectives denoting color size  
looking closely specific relata picked text  image based models 
striking differences pertain  again  attributes  text  image based
models picked attribute pivot     cases  compare    
overlap across non random relation types   table   reports attributes picked
text  vs  image based models    random cases two mismatch 
  

fibruni  tran   baroni

image based semantic vectors

  

  

  

  

 

 

 

 

 

 

text based semantic vectors

coord

hyper

mero

attri

event

ran n

ran j

ran v

coord

hyper

mero

attri

event

ran n

ran j

ran v

figure    distribution z normalized cosines words instantiating various relations across
bless pivots  text based vectors window   model 

pivot
cabbage
carrot
cherry
deer
dishwasher
elephant
glider
gorilla
hat
hatchet

text
leafy
fresh
ripe
wild
electric
wild
heavy
wild
white
sharp

image
white
orange
red
brown
white
white
white
black
old
short

pivot
helicopter
onion
oven
plum
sofa
sparrow
stove
tanker
toaster
trout

text
heavy
fresh
electric
juicy
comfortable
wild
electric
heavy
electric
fresh

image
old
white
new
red
old
little
hot
grey
new
old

table    attributes preferred text   window    vs  image based models 

  

fimultimodal distributional semantics

immediately clear table that  despite fact pivots nouns
denoting concrete concepts  text based model almost never picks adjectives denoting
salient perceptual properties  and particular visual properties  white hat leafy
cabbage   text based model focuses instead encyclopedic properties fresh 
ripe  wild  electric comfortable  line earlier analyses ungrounded
semantics provided text based models  andrews et al         baroni et al         baroni
  lenci        riordan   jones         differs greatly trend found
image based model        cases  closest attribute latter model color 
remaining cases  size  short  little   one instance hot and  surprisingly  four
old 
conclude  analysis presented confirms  one hand  hypothesis
image based distributional vectors contain sufficient information capture network
sensible word meaning relations  other  intriguing differences relations picked text  image based models  pointing complementarity 
    word relatedness
standard distributional semantics literature  budanitsky   hirst        sahlgren 
       assess performance models task predicting degree semantic relatedness two words rated human judges  test models
ws men benchmarks 
      benchmarks method
ws  is  wordsim       see section      widely used benchmark constructed
asking    subjects rate set     word pairs    point meaning similarity scale
averaging ratings  e g   dollar buck gets high average rating  professor cucumber
low one   target words cover     ws pairs  thus  correlations reported
directly comparable reported studies used ws   however 
text based models much higher ws coverage        evaluated larger ws
set cover  window  window   achieve           correlations  respectively 
thus comparing multimodal approach purely textual models
state art ws  see results reported section     above  
second benchmark use  men  for marco  elia nam  resource creators 
developed us  specifically purpose testing multimodal models  created
large data set that  comparable ws benchmarks commonly used
computational semantics community  contains words appear image
labels esp game mirflickr  m   collections  thus ensuring full coverage
researchers train visual models resources  men consists       word pairs
       normalized semantic relatedness ratings provided amazon mechanical turk
workers  via crowdflower   interface   example  beach sand men score
      bakery zebra received   score 
    http   www cs technion ac il  gabr resources data wordsim    
    http   press liacs nl mirflickr 
    http   crowdflower com 

  

fibruni  tran   baroni

compared ws  men sufficiently large allow us separate development
test data  avoiding issues overfitting  use indeed       men pairs  development set 
model tuning       pairs evaluation  test set   importantly  development
set used find best configuration men test set ws 
thus  ws evaluation illustrates well parameters learned training data
specific data set generalize applied semantic task different
data set 
models evaluated follows  pair data set  compute cosine
model vectors representing words pair  calculate spearman
correlation cosines  pooled  human ratings pairs  idea
higher correlation better model simulate relatedness
scores 
men construction earlier version men used first time
authors bruni et al         since current article first major publication
focus specifically it  recently improved benchmark
extending ratings  provide details constructed 
word pairs constitute men randomly selected words occur
least     times concatenated ukwac wackypedia text corpora least   
times tags esp game mirflickr  m tagged image collections  order
avoid picking pairs weakly related  would happen sample
random word pairs list  ranked possible pairs cosines according
text based model window    gather      word pairs needed construction
men  subsequently picked first      word pairs  another      sampled
pairs placed           cosine ranked list last block      pairs
remaining items 
acquire human semantic relatedness judgments  decided ask comparative
judgments two pair exemplars time rather absolute scores single pairs 
done creators ws  constitute natural way evaluate
target pairs  since human judgments comparative nature  person evaluates
given target  vacuum  relation certain context 
moreover  binary choices preferred make construction right
wrong control items straightforward  see footnote      operationally  word pair
randomly matched comparison pair coming set      items
rated single turker either less related comparison item 
validity approach confirmed high annotation accuracy observe
control set    high correlation men scores ratings collected
likert scale report below 
    control items correct annotations created prior running job amazon mechanical turk 
act hidden tests randomly shown turkers complete job  way 
calculate quality contributors performance reject annotations accuracy
drops certain percentage  we set required minimum precision equal      obtained
almost      average accuracy overall   control items great help train quickly new workers
perform required task  create control items harvested two equally sized sets word
pairs ws  one containing pairs high relatedness score  one containing pairs
low relatedness score  control item obtained juxtaposing high score pair
low score pair treating pair higher score one selected

  

fimultimodal distributional semantics

instructions  annotators warned sometimes candidate pairs could
contain words related meaning cases asked pick pair
strongly related words  e g   wheels car dog race somewhat related pairs 
first one preferred every car wheels every dog involved
race   cases  annotators could find neither pair contains closely related
words  cases instructed pick pair contained slightly
related words  e g   neither auction car cup asphalt closely related words 
first pair picked fancy vintage cars sold auctions   requested
participants native speakers accepted connecting english
speaking country  cannot guarantee non natives take part study 
subject filtering techniques based control pairs  see footnote     ensures
data speakers good command english retained 
transform binary preference data relatedness scores retrieved pairs 
evaluated    randomly picked comparison pairs  thus received score
   point scale  given number times    pair picked
related two   score subsequently normalized     dividing
number times pair picked related     example  fun night
chosen related comparison pair    times  thus normalized score
given              note that  comparison  recorded preference
assigned one two pairs  avoid dependencies final scores assigned
different pairs  that is  times pair selected random comparison item
another pair counted ratings pair  
raters saw men pairs matched different random items  number
pairs varying rater rater  possible compute annotator agreement
scores men  however  get sense human agreement  first third author
rated       pairs  presented different random orders  standard     likert scale 
spearman correlation two authors       correlation average
ratings men scores       one hand  high correlation suggests
men contains meaningful semantic ratings  other  taken
upper bound computational models realistically achieve simulating
human men judgments 
high score men pairs include pairs terms strictly taxonomically close  cathedral church        terms connected broader semantic
relations  whole part  flower petal         item related event  boat fishing       
etc  reason  prefer refer men semantic relatedness rather
similarity score data set  note ws capturing broader notion relatedness  agirre et al          men publicly available downloaded from 
http   clic cimec unitn it  elia bruni men 
      results
table   reports correlations men testing ws data sets using either
window  window   textual model  automated tuning method selected k     
annotators related  control items manually checked  examples control items
hotel word vs  psychology depression  telephone communication vs  face locomotive 

  

fibruni  tran   baroni

model
text
image
naivefl
naivesl
mixlda
textmixed
imagemixed
tunedfl
tunedsl

window 
men ws
    
    
    
    
    
    
    
    
    
    
         
    
    
         
         

window  
men ws
    
    
    
    
    
    
    
    
    
    
         
    
    
         
         

table    spearman correlation models men wordsim  all coefficients significant p           tunedfl model selected automatically men
development data  tunedsl automatically tuned fixing sl similarity estimation 

 when textual information comes window   k        with window    optimal 
feature level  fl  similarity estimation       cases  since input
matrices row normalized  latter setting assigns equal weights textual
visual components   models called tunedfl table  scoring level
 sl  strategy  again similar weights assigned two channels  k values
tunedfl  performed slightly worse tunedfl  report results
best sl based models tuned development men data well  tunedsl  
models reported table  naivefl  naivesl  mixlda  textmixed imagemixed   
parameters tuned manually order gain insights combination strategies
representing ideas earlier literature   
first two rows table show results text  image based models 
mixing  text shows comparable performances data sets  image correlates
significantly better men ws correlations lower text 
accordance found earlier studies  next three rows find
results earlier multimodal approaches took consideration  bruni et al        
feng   lapata        leong   mihalcea         naivefl approach  analogous
bruni et al s method   textual visual matrices concatenated without
mixing  performs slightly better text men  attains lower performance ws 
naivesl  equivalent leong mihalceas summing approach   text
image sources combined scoring level  obtains improvements men  loosing
several correlation points ws compared text 
implementation mixlda achieves poor results men ws  one
might attribute fact feng lapatas approach constrained using
source textual visual model image data set poor source
    textmixed imagemixed   best k values found development data 
set     textual sources 

  

fimultimodal distributional semantics

textmixed
tunedfl
tunedsl

window 
    
    
    

window  
    
    
    

table    pearson correlation best multimodal combinations wordsim
subset covered feng lapata         all coefficients significant p  
       pearson used instead spearman full comparability feng
lapata   models assigned   similarity        pairs
missing vector  feng lapata        report      correlation mixlda 

textual data  approach however outperforming original mixlda
large margin latter ws test set  strongly disfavoured  particular 
feng lapata        report correlation      subset     ws pairs covered
model  tested system subset  despite fact
missing one vectors    pairs  almost one third   models
forced assign   cosines cases  despite huge handicap  models
still attaining much higher correlations original mixlda feng lapata
pairs  illustrated interesting fusion strategies table   
analyzing effects fusion strategies  first see uniform enhancement men ws textmixed imagemixed  the models obtained first
performing latent multimodal mixing combined matrix  using textual features textmixed visual features imagemixed    textmixed reaches best
performance overall ws source textual models  significantly better
text men according two tailed paired permutation test  moore   mccabe 
       looking automatically selected tunedfl model  reaches best performance overall  significantly outperforms text models data sets 
significantly better textmixed men window    the difference approaching significance window  well  p          tunedsl competitive 
significantly better text window sizes textmixed window   
noticeably worse tunedfl ws window   only  actually
slight advantage men window    the difference tunedfl tunedsl
never significant  
worth remarking textmixed bit worse full fusion models 
still achieves high correlations human judgments extremely high
correlation tunedfl best model            suggests
benefits multimodality already captured latent mixing  textmixed attractive
model less parameters whole pipeline compact
tunedfl  since discards visual features using mixing 
validating results shown significant improvements visual features added distributional models  one could object improvements due
fact using information  larger number features  higher dimensional
  

fibruni  tran   baroni

vectors  feature level fusion  complex model  two similarity scores
independent variables predict human judgments  scoring level fusion  experiments provide evidence respond objection 
first  built purely textual models number features multimodal
models is  instead collecting co occurrence target terms   k
frequent content lemmas corpus  see section     above   extended list
context items    k frequent content lemmas  results larger
textual models virtually identical   k dimensional vectors reported
table    correlation window   model men      instead        thus 
least using large corpus window based approach    k features
pretty much exhausted useful textual information  nature  simply
quantity extra visual features add matters 
answer objection scoring level approach using complex model 
two independent variables  text  image base similarities  instead one  casted
problem standard inferential statistical terms  see  e g  baayen        ch      specifically  fitted ordinary linear regression models predict men ws ratings
text based similarities vs  text  image based similarities  for comparability
spearman correlation results reported above  analyses replicated
transforming ratings similarities ranks   variables highly significant
experiments  and  importantly  sequential f tests nested models revealed
cases adding image based similarities explains significantly variance
would expected chance given extra parameter  p         
qualitative analysis acquire qualitative insights multimodality contributing meaning representation  first picked top     related pairs
combined men ws norms  would confident indeed highly
related pairs humans  looked  within subset  pairs
pronounced difference cosines text tunedfl  using window  
textual source  is  first column table   presents pairs considered
related humans relatedness better captured text  second column
pairs relatedness better captured tunedfl 
notice      relations better captured tunedfl coordinates
synonyms pertaining concrete objects  candy chocolate  bicycle bike  apple cherry 
military soldier  paws whiskers  stream waterfall cheetah lion   indeed
maximally visually similar  either objects or  case paws whiskers 
surrounds   purely text based model  hand  captures relations
times day  that  imageable  well delimited concrete objects
 dawn dusk  sunrise sunset   captures properties concepts expressed adjectives
 dog canine  skyscraper tall  cat feline  pregnancy pregnant  rain misty   least one
case spotting relation requires encyclopedic knowledge  grape wine   thus
hypothesize added value multimodally enhanced model derives
power vision finding relations concrete objects taxonomic level 
results detecting particularly tight forms relatedness  synonymy
coordination 
  

fimultimodal distributional semantics

text
dawn dusk
sunrise sunset
canine dog
grape wine
foliage plant
foliage petal
skyscraper tall
cat feline
pregnancy pregnant
misty rain

tunedfl
pet puppy
candy chocolate
paw pet
bicycle bike
apple cherry
copper metal
military soldier
paws whiskers
stream waterfall
cheetah lion

table    top    pairs whose relatedness better captured text  window   
vs  tunedfl 

observed one reviewer  given taxonomic nature information captured
multimodal approach  interesting compare future work features
directly extracted linguistic taxonomy  wordnet  observe passing
manually constructed resource  unlike extracted textual corpora  likely
reflect linguistic perceptual knowledge lexicographers built
it 
going opposite direction  another reviewer observed might get
mileage combining visual features textual models less taxonomic nature 
hypothesis partially confirmed fact obtain larger relative improvement mixing vision window   window   look back table    see
section     think narrower window mainly captures taxonomic
relations  larger one broader topical themes   explore conjecture 
re ran men ws experiments combining visual vectors document based
textual model  i e   semantic space whose dimensions record number occurrences
words documents   space expected capture mostly topical information 
estimates relatedness basis tendency words occur
documents  sahlgren         document based model alone good
window based models  it obtained spearman correlation      men     
ws   combining image based models led relative improvements comparable
inferior attained window    the best combined model correlations
     men      ws   conclude that  looking textual models
complementary respect visual information seems reasonable direction
develop multimodal systems cover broader range semantic phenomena  simply
emphasizing topical side textual models evidently suffice 
  

fibruni  tran   baroni

      concreteness factor modeling relatedness ratings  pilot
study
previous experiments  observed trend towards division labour text  image based models  latter apt capturing similarity
among concrete concepts properties  one strongest limitations current
version framework fact every target word assumed equally perceptually salient consequently uniformly enriched visual information  intuitively 
might want distinguish instead concrete words  chair cat  require
integration perceptual information representation  abstract words 
consequence absurd  represented purely symbolic linguistic basis 
indeed  recchia jones        recently presented evidence that  lexical decision
naming tasks  rich physical contexts favour activation concrete concepts  whereas rich
linguistic contexts facilitate activation abstract concepts  follow up pilot
experiment presented section want pave way systematic introduction concreteness factor multimodal meaning representation  operationally 
separate abstract concrete word pairs semantic relatedness benchmark
men  assessing contribution textual visual information approximating word
meaning two domains independently  importantly  use automated method
determine word concrete abstract  eye future integration
automatically determined abstractness score fusion algorithm 
particular  use abstractness scores automatically assigned algorithm recently introduced turney  neuman  assaf  cohen         scores calculated
computing difference sum text based semantic similarities target
word set concrete paradigm words sum semantic similarities
set abstract paradigm words  words  i e   paradigm words words
abstractness score computed  represented co occurrence based
matrix gathered large corpus university websites  co occurrence counts
transformed positive pointwise mutual information scores  church   hanks       
resulting matrix smoothed svd  pairwise semantic similarity measured
cosines  paradigm words turn selected supervised learning method
trained subject rated words mrc psycholinguistic database machine usable
dictionary  coltheart         examples highly abstract words automatically rated
list purvey        sense       improbable        examples highly concrete
words  i e   words low abstractness score  donut        bullet       shoe 
     
abstractness score assigned men testing words  divided
data set two subsets  one containing concrete word pairs  men conc     
pairs   containing abstract pairs mixed pairs  pairs formed one
concrete one abstract word  men abst      pairs   word considered concrete
abstract score      abstract otherwise  example  word pair arm bicycle
considered concrete  with scores           respectively   fun relax considered
abstract  with scores          respectively  design orange considered mixed
 with scores           respectively   experimented window   purely
  

fimultimodal distributional semantics

model
window  
image
tunedfl

men conc
    
    
    

men abst
    
    
    

men full
    
    
    

table    spearman correlation models men divided concrete abstract
subsets  results full data set repeated  coefficients significant
p         

textual model  image usual visual model tunedfl trained men development
multimodal model 
table   show correlation scores three models two men subsets
 as well repeating correlations attain full set   first all  worth
noticing models higher correlations men conc men abst  suggesting
approximating similarity judgments pairs concrete pairs general easier
task distributional semantics  and  suspect  humans well    besides broad
effect  observe clear interaction added value visual component
men abst men conc  fact  tunedfl gains     performance
men conc compared window    performance essentially
text only model case men abst  indicates visual information
mostly beneficial concrete domain  maintains neutral  timidly positive 
impact abstract domain  recall that  case  men abst contains mixed
pairs  
conclude  section followed qualitative analysis main
relatedness results pilot experiment focusing concreteness factor  showed
divide men benchmark concrete abstract subsets  visual
information enhances text based model concrete domain  impact
strong  exploited automatic scoring function divide data set
concrete abstract subsets  thus see results reporting
validation turney et al s algorithm  and  importantly purposes 
encouragement incorporate automated abstractness concreteness scoring way
model mixes textual visual information word by word basis 
    concept categorization
verify conclusions reached ws men extend different semantic tasks and 
particular  assess whether multimodal approach able capture organize
meaning humans do  use two existing concept categorization benchmarks
call battig almuhareb poesio  ap   respectively  goal cluster set
 nominal  concepts broader categories  already discussed section     
particular  use battig exclusively tuning  in way used men
development set previous section  ap testing  results ap
reported  word relatedness task tuning testing sets quite similar
  

fibruni  tran   baroni

 men development men testing two subsets data set words
ws similar men   task challenging since battig ap
two independent data sets built following different strategies populated
different kinds concepts  namely concrete unambiguous concepts battig 
vs  mixture concrete abstract  possibly ambiguous concepts ap  adopted
present challenging training testing regime felt neither data set
sufficient size allow split development testing data  details follow 
      benchmarks method
battig benchmark introduced baroni et al         based battig
montague norms van overschelde  rawson  dunlosky         consists
   highly prototypical concepts    common concrete categories  up    concepts
per class   battig contains basic level concepts belonging categories bird  eagle 
owl         kitchenware  bowl  spoon        vegetable  broccoli  potato         version
cover    concepts    different classes 
ap introduced almuhareb poesio        made     nouns
   different wordnet classes  version cover  ap contains     concepts
clustered    classes vehicle  airplane  car         time  aeon  future        social
unit  brigade  nation   data set contains many difficult cases unusual ambiguous
instances class  casuarina samba trees 
sets  following original proponents others  cluster words based
pairwise cosines semantic space defined model using cluto toolkit
 karypis         use clutos built in repeated bisections global optimization
method  accepting clutos default values  cluster quality often evaluated
percentage purity  zhao   karypis         nir number items i th true
 gold standard  class assigned r th cluster  n total number items 
k number clusters 
purity  

x
  i n
max  nri  
n i  

words  number items belonging majority true class  i e   represented
class cluster  summed across clusters divided total number items 
best scenario purity   approach   cluster quality deteriorates 
since lack full ap coverage  results report directly comparable
studies used it  however  text based models perfect coverage 
evaluated full set achieve purities       window         window   
state of the art levels comparable models  reported section     above 
so  again  confidently claim improvements achieved multimodality
obtained comparing approach competitive purely textual models 
      results
table   reports percentage purities ap clustering task  best automatically selected model  tunedfl  uses fl similarity estimation previous task 
similar svd k     window     window          parameters
  

fimultimodal distributional semantics

model
text
image
naivefl
naivesl
mixlda
textmixed
imagemixed
tunedfl
tunedsl

window 
ap
    
    
    
    
    
    
    
    
    

window  
ap
    
    
    
    
    
    
    
    
    

table    percentage purities models ap  tunedfl model automatically
selected battig data  tunedsl automatically tuned fixing sl similarity estimation 

ones found relatedness  suggesting particular parameter choice robust
could used out of the box tasks well  tunedsl best sl based method
tuning battig set  same ks tunedfl        window        
window   
analogously previous semantic task  see image model alone
level text models  although ap purities significantly chance
 p        based simulated distributions random cluster assignment   thus 
confirmation fact image based vectors capture important aspects
meaning  previous task  mixlda achieves poor results 
looking text based models enhanced visual information  see general
improvement performance almost multimodal combination strategies  except
naivefl window   naivesl window   even textmixed benefits
visual smoothing cases  outperformed tunedfl  whose performance
similar tunedsl  actually slightly better window   interestingly  tunedsl outperforms text window  despite fact single combination
strongly unbalanced towards textual similarity           indicating visual information beneficial even textual information accounts lions share
composed estimate 
relatedness task  adding equal amount textual features instead
image based ones help window         purity    k textual features 
even lowers performance window        purity   thus  improvement brought
visual features must attributed quality  quantity 
according two tailed permutation test  even largest difference tunedfl
text window   significant  might due brittleness purity
statistics leading high variance permutations  possibly suboptimal tuning 
recall  respect  tuning phase performed rather different data
set  battig  compared data set eventually evaluated models  ap  
  

fibruni  tran   baroni

however  overall trends encouraging  line found
relatedness study 

   conclusion
paper provided extensive introduction new approach distributional semantics named multimodal distributional semantics  multimodal
distributional semantic model integrates traditional text based representation meaning
information coming vision  way  tries answer critique distributional models lack grounding  since base representation meaning entirely
linguistic input  neglecting statistical information inherent perceptual experience 
humans instead exploit  course  truly multimodal representation meaning
account entire spectrum human senses  hand  line
research still embryonic stage still shortage perceptual data
available techniques automatize processing  why  article 
focused analysis visual perceptual channel  disposal
large data sets effective methods analyze them 
particular  exploited esp game data set  image documents
tagged words describing content  harvest visual information adopted
bag of visual words technique  discretizes image content ways analogous
standard text based distributional representations  introduced multimodal framework
optimizes text image fusion data driven fashion development data 
conducted number experiments assess quality obtained models 
first investigated general semantic properties purely image based model 
assess overall quality well look information complementary present
text  found systematic differences two modalities  preference
encyclopedic properties text based model perceptual properties case
image based model  proceeded test selection models obtained combination
text  image based representations via multimodal framework  used two
benchmarks word relatedness one benchmark word categorization
cases obtained systematic improvement performance multimodal models
compared models based standalone channels 
still  looking numerical results  cannot deny improvement performance attained including visual information dramatic  indeed  pessimistic
interpretation experiments could confirm hypothesis louwerse
others  e g   louwerse        louwerse   connell        tillman  datla  hutchinson   
louwerse        perceptual information already encoded  sufficient degree 
linguistic data  direct visual features dont bring much table  however  showed
various statistical validation tests important result  namely
adding visual information improves using text alone  robust reliable  think
realistic take home message experiments reported  establishing
basic result mentioned  drawbacks overcome
work 
first all  deliberately used general semantic benchmarks state of the art text
models  performance computational methods might getting close
  

fimultimodal distributional semantics

ceiling       correlation  best models still percentage points go
men  estimated upper bound based raters agreement        see section        
improvements bound quite small  concerning ap benchmark  consider
difficult would even humans categorize casuarina samba among
trees  indeed  error analysis tunedfl clustering results suggests factors
might lead better performance little vision  example 
model wrongly clusters branch  a social unit according ap  trees  merges
concepts melon peach  fruit ap  mandarin lime  trees   lack
contextual information  hard dispute model choices  similarly  tunedfl
splits ap animal class cluster small domestic mammals  cats  dogs  kittens 
mice  puppies rats  cluster containing everything else  mostly larger mammals
cows elephants   again  clustering procedure information
classes searching  e g   animals general  small animals  
hard see performance could improved thanks better semantic features  visual
kinds  moreover  data sets include abstract terms  specifically
designed test grounded aspects meaning  visual features might help
most  think made sense start investigation general benchmarks
semantics  opposed ad hoc test sets  show viability multimodal approach 
however  future want focus experimental challenges strengths
visually enhanced models might emerge clearly  took first step direction
bruni et al          focused specifically visual features help
processing literal metaphorical colours 
another factor take account large scale image data sets
techniques extract features infancy  might able
improve performance developing better image based models  regarding data
sets  explained section       chose esp game  obviously
sub optimal many respects  discuss there  regarding features 
mentioned beginning section        recent advances image processing 
fisher encoding  might lead better ways extract information contained images 
experiments  compared automatically tuned multimodal model
settings  showing overall stability superiority  two important caveats 
first  experiments good results already obtained using visual information
smooth text features  without using visual features directly  what called
textmixed approach   note already multimodal approach  visual
information crucially used improve quality textual dimensions  indeed
weve seen consistently outperforms using non multimodally smoothed text features 
textmixed good full tuned model  simplicity makes
attractive approach 
second  although automated tuning led us prefer feature level scoring level
fusion development sets  tunedsl clearly worse tunedfl one case
 with window   ws   suggesting that  least evaluation settings considered 
difference two fusion strategies crucial  however  comparing
naive versions strategies tuned ones across results  clear
tuning important obtain consistently good performance  confirming usefulness
general fusion architecture 
  

fibruni  tran   baroni

conducted pilot experiment concreteness abstractness factor  assess
impact meaning representation check good candidate new
weighted fusion strategy plan investigate future  fact  current version
multimodal framework  parametrization combination strategy works
global level  i e  words   could productive combine textual
visual information word by word basis  tune two modality contributions
meaning representation depending particular nature single word  concrete
vs  abstract constitute neat binary distinction words  rather
thought ideal distinction offset less abrupt  real world formulation 
takes account degree according certain word considered concrete
abstract  doubt words backdrop  squalor sharp evoke
perceptual cues gathered experience them  time
unequivocal amount abstractness accompanying them  plan refine
concreteness scoring method order make focus specifically imageable
components concreteness  expect relevant visual channel 
developments focus techniques extract image based semantic
models  example  pilot study  bruni et al          exploit new methods developed
computer vision improve object recognition capturing object location  felzenszwalb 
girshick  mcallester    deva ramanan        de sande  uijlings  gevers    smeulders 
       show possible extract better image based semantic vectors first
localizing objects denoted words extracting visual information
object location surround independently  interestingly  discovered
image based semantic vectors extracted object surround effective
based object location tested word relatedness task  example 
fact pictures containing deers wolves depict similar surrounds tells us
creatures live similar environments  thus likely somewhat
related  seen distributional hypothesis transposed images  objects
semantically similar occur similar visual contexts  nevertheless  work
considered proof concept  since experimented    words only  future
studies test larger number words 
obviously much room improvement  many exciting routes
explore  hope framework empirical results presented study
convinced reader multimodal distributional semantics promising avenue
pursue development human like models meaning 

acknowledgments
thank jasper uijlings valuable suggestions image analysis pipeline 
lot code many ideas came giang binh tran  owe gemma boleda many
ideas useful comments  peter turney kindly shared abstractness score list
used section       yair neuman generously helped preliminary analysis
impact abstractness multimodal models  mirella lapata kindly made
wordsim    set used experiments feng lapata        available us 
thank jair associated editor reviewers helpful suggestions constructive
  

fimultimodal distributional semantics

criticism  google partially funded project google research award third
author  bless study section       first presented bruni et al         

references
abdi  h     williams  l          newman keuls tukey test  salkind  n   frey  b    
dougherty  d   eds    encyclopedia research design  pp          sage  thousand
oaks  ca 
agirre  e   alfonseca  e   hall  k   kravalova  j   pasa  m     soroa  a          study
similarity relatedness using distributional wordnet based approaches 
proceedings hlt naacl  pp        boulder  co 
almuhareb  a     poesio  m          concept learning categorization web 
proceedings cogsci  pp          stresa  italy 
andrews  m   vigliocco  g     vinson  d          integrating experiential distributional
data learn semantic representations  psychological review                  
baayen  h          analyzing linguistic data  practical introduction statistics using
r  cambridge university press  cambridge  uk 
barnard  k   duygulu  p   forsyth  d   de freitas  n   blei  d     jordan  m          matching words pictures  journal machine learning research              
baroni  m   barbu  e   murphy  b     poesio  m          strudel  distributional semantic
model based properties types  cognitive science                 
baroni  m     lenci  a          concepts properties word spaces  italian journal
linguistics               
baroni  m     lenci  a          distributional memory  general framework corpusbased semantics  computational linguistics                 
baroni  m     lenci  a          blessed distributional semantic evaluation 
proceedings emnlp gems workshop  pp       edinburgh  uk 
barsalou  l          grounded cognition  annual review psychology             
berg  t   berg  a     shih  j          automatic attribute discovery characterization
noisy web data  eccv  pp          crete  greece 
bergsma  s     goebel  r          using visual information predict lexical preference 
proceedings ranlp  pp          hissar  bulgaria 
blei  d  m   ng  a  y     jordan  m  i          latent dirichlet allocation  journal
machine learning research             
bosch  a   zisserman  a     munoz  x          image classification using random forests
ferns  proceedings iccv  pp      rio de janeiro  brazil 
bosch  a   zisserman  a     munoz  x          scene classification using hybrid generative discriminative approach  ieee transactions pattern analysis machine
intelligence         
bruni  e   boleda  g   baroni  m     tran  n  k          distributional semantics
technicolor  proceedings acl  pp          jeju island  korea 
  

fibruni  tran   baroni

bruni  e   bordignon  u   liska  a   uijlings  j     sergienya  i          vsem  open
library visual semantics representation  proceedings acl  sofia  bulgaria 
bruni  e   tran  g  b     baroni  m          distributional semantics text images 
proceedings emnlp gems workshop  pp        edinburgh  uk 
bruni  e   uijlings  j   baroni  m     sebe  n          distributional semantics eyes 
using image analysis improve computational representations word meaning 
proceedings acm multimedia  pp            nara  japan 
budanitsky  a     hirst  g          evaluating wordnet based measures lexical semantic
relatedness  computational linguistics               
bullinaria  j     levy  j          extracting semantic representations word cooccurrence statistics  computational study  behavior research methods         
    
bullinaria  j     levy  j          extracting semantic representations word cooccurrence statistics  stop lists  stemming svd  behavior research methods 
           
burgess  c          theory operational definitions computational memory models 
response glenberg robertson  journal memory language         
       
caicedo  j   ben abdallah  j   gonzlez  f     nasraoui  o          multimodal representation  indexing  automated annotation retrieval image collections via nonnegative matrix factorization  neurocomputing               
chatfield  k   lempitsky  v   vedaldi  a     zisserman  a          devil
details  evaluation recent feature encoding methods  proceedings bmvc 
dundee  uk 
church  k     hanks  p          word association norms  mutual information  lexicography  computational linguistics               
clark  s          vector space models lexical meaning  lappin  s     fox  c   eds   
handbook contemporary semantics   nd ed  blackwell  malden  ma  press 
coltheart  m          mrc psycholinguistic database  quarterly journal experimental psychology     
connolly  a   gleitman  l     thompson schill  s          effect congenital blindness
semantic representation everyday concepts  proceedings national
academy sciences                     
csurka  g   dance  c   fan  l   willamowski  j     bray  c          visual categorization
bags keypoints  workshop statistical learning computer vision 
eccv  pp       prague  czech republic 
curran  j     moens  m          improvements automatic thesaurus extraction 
proceedings acl workshop unsupervised lexical acquisition  pp       
philadelphia  pa 
  

fimultimodal distributional semantics

de sande  k  v   uijlings  j   gevers  t     smeulders  a          segmentation selective
search object recognition  proceedings iccv  pp            barcelona 
spain 
de vega  m   glenberg  a     graesser  a   eds            symbols embodiment  debates
meaning cognition  oxford university press  oxford  uk 
deng  j   dong  w   socher  r   li  l  j     fei fei  l          imagenet  large scale
hierarchical image database  proceedings cvpr  pp          miami beach 
fl 
dumais  s          data driven approaches information access  cognitive science     
       
erk  k          vector space models word meaning phrase meaning  survey  
language linguistics compass                 
escalante  h  j   hrnadez  c  a   sucar  l  e     montes  m          late fusion heterogeneous methods multimedia image retrieval  proceedings icmr  vancouver 
canada 
evert  s          statistics word cooccurrences  dissertation  stuttgart university 
farhadi  a   hejrati  m   sadeghi  m  a   young  p   rashtchian  c   hockenmaier  j    
forsyth  d          every picture tells story  generating sentences images 
proceedings eccv  crete  greece 
felzenszwalb  p   girshick  r   mcallester  d     deva ramanan  d          object detection discriminatively trained part based models  ieee transactions pattern
analysis machine intelligence               
feng  y     lapata  m          visual information semantic representation  proceedings hlt naacl  pp        los angeles  ca 
finkelstein  l   gabrilovich  e   matias  y   rivlin  e   solan  z   wolfman  g     ruppin 
e          placing search context  concept revisited  acm transactions
information systems                 
firth  j  r          papers linguistics             oxford university press  oxford 
uk 
fodor  j          language thought  crowell press  new york 
glenberg  a     robertson  d          symbol grounding meaning  comparison
high dimensional embodied theories meaning  journal memory language                 
grauman  k     darrell  t          pyramid match kernel  discriminative classification
sets image features  proceedings iccv  pp            beijing  china 
grauman  k     leibe  b          visual object recognition  morgan   claypool  san
francisco 
grefenstette  g          explorations automatic thesaurus discovery  kluwer  boston 
ma 
  

fibruni  tran   baroni

griffin  l   wahab  h     newell  a          distributional learning appearance  plos
one         published online  http   www plosone org article info doi    
     journal pone         
griffiths  t   steyvers  m     tenenbaum  j          topics semantic representation 
psychological review              
hansen  t   olkkonen  m   walter  s     gegenfurtner  k          memory modulates color
appearance  nature neuroscience              
harnad  s          symbol grounding problem  physica d  nonlinear phenomena 
                 
harris  z          distributional structure  word                     
johns  b     jones  m          perceptual inference global lexical similarity  topics
cognitive science                
karypis  g          cluto  clustering toolkit  tech  rep          university minnesota
department computer science 
kaschak  m   madden  c   therriault  d   yaxley  r   aveyard  m   blanchard  a     zwaan 
r          perception motion affects language processing  cognition      b  b   
kievit kylar  b     jones  m          semantic pictionary project  proceedings
cogsci  pp            austin  tx 
kulkarni  g   premraj  v   dhar  s   li  s   choi  y   berg  a  c     berg  t  l         
baby talk  understanding generating simple image descriptions  proceedings
cvpr  colorado springs  msa 
landauer  t     dumais  s          solution platos problem  latent semantic analysis theory acquisition  induction  representation knowledge  psychological
review                  
lazebnik  s   schmid  c     ponce  j          beyond bags features  spatial pyramid
matching recognizing natural scene categories  proceedings cvpr  pp      
      washington  dc 
leong  c  w     mihalcea  r          going beyond text  hybrid image text approach
measuring word relatedness  proceedings ijcnlp  pp           
lloyd  s          least squares quantization pcm  ieee transactions information
theory             
louwerse  m          symbol interdependency symbolic embodied cognition  topics
cognitive science            
louwerse  m     connell  l          taste words  linguistic context perceptual
simulation predict modality words  cognitive science             
lowe  d          object recognition local scale invariant features  proceedings
iccv  pp           
lowe  d          distinctive image features scale invariant keypoints  international
journal computer vision         
  

fimultimodal distributional semantics

lowe  w          towards theory semantic space  proceedings cogsci  pp         
edinburgh  uk 
lund  k     burgess  c          producing high dimensional semantic spaces lexical
co occurrence  behavior research methods             
manning  c   raghavan  p     schtze  h          introduction information retrieval 
cambridge university press  cambridge  uk 
manning  c     schtze  h          foundations statistical natural language processing 
mit press  cambridge  ma 
mcdonald  s     brew  c          distributional model semantic context effects
lexical processing  proceedings acl  pp        barcelona  spain 
mcrae  k   cree  g   seidenberg  m     mcnorgan  c          semantic feature production
norms large set living nonliving things  behavior research methods         
       
miller  g     charles  w          contextual correlates semantic similarity  language
cognitive processes             
moore  d     mccabe  g          introduction practice statistics    edition  
freeman  new york 
murphy  g          big book concepts  mit press  cambridge  ma 
nelson  d   mcevoy  c     schreiber  t          university south florida word association  rhyme  word fragment norms  http   www usf edu freeassociation  
nister  d     stewenius  h          scalable recognition vocabulary tree  proceedings      ieee computer society conference computer vision pattern
recognition   volume    cvpr     pp           
nowak  e   jurie  f     triggs  b          sampling strategies bag of features image
classification  proceedings eccv  pp          graz  austria 
pad  s     lapata  m          dependency based construction semantic space models 
computational linguistics                 
pad  u   pad  s     erk  k          flexible  corpus based modelling human plausibility
judgements  proceedings emnlp  pp          prague  czech republic 
pecher  d   zeelenberg  r     raaijmakers  j          pizza prime coin  perceptual
priming lexical decision pronunciation  journal memory language     
       
perronnin  f   sanchez  j     mensink  t          improving fisher kernel large scale
image classification  proceedings eccv  pp          berlin  heidelberg 
pham  t  t   maillot  n   lim  j  h     chevallet  j  p          latent semantic fusion
model image retrieval annotation  proceedings cikm  pp         
lisboa  portugal 
poesio  m     almuhareb  a          identifying concept attributes using classifier 
proceedings acl workshop deep lexical semantics  pp        ann arbor 
mi 
  

fibruni  tran   baroni

pulvermueller  f          brain mechanisms linking language action  nature reviews
neuroscience            
radinsky  k   agichtein  e   gabrilovich  e     markovitch  s          word time 
computing word relatedness using temporal semantic analysis  proceedings
www  pp          hyderabad  india 
rapp  r          word sense discovery based sense descriptor dissimilarity  proceedings  th mt summit  pp          new orleans  la 
recchia  g     jones  m          semantic richness abstract concepts  frontiers
human neuroscience          
reisinger  j     mooney  r  j          multi prototype vector space models word meaning  proceedings naacl  pp          los angeles  ca 
riordan  b     jones  m          redundancy perceptual linguistic experience 
comparing feature based distributional models semantic representation  topics
cognitive science             
rothenhusler  k     schtze  h          unsupervised classification dependency
based word spaces  proceedings eacl gems workshop  pp        athens 
greece 
rubenstein  h     goodenough  j          contextual correlates synonymy  communications acm                 
sahlgren  m          introduction random indexing  http   www sics se  mange 
papers ri intro pdf 
sahlgren  m          word space model  dissertation  stockholm university 
sahlgren  m          distributional hypothesis  italian journal linguistics         
     
schtze  h          ambiguity resolution natural language learning  csli  stanford 
ca 
silberer  c     lapata  m          grounded models semantic representation  proceedings emnlp conll  pp            jeju  korea 
sivic  j     zisserman  a          video google  text retrieval approach object matching videos  proceedings iccv  pp            nice  france 
steyvers  m          combining feature norms text data topic models  acta
psychologica                  
therriault  d   yaxley  r     zwaan  r          role color diagnosticity object
recognition representation  cognitive processing                 
tillman  r   datla  v   hutchinson  s     louwerse  m          head toe  embodiment statistical linguistic frequencies  proceedings cogsci  pp 
          austin  tx 
turney  p   neuman  y   assaf  d     cohen  y          literal metaphorical sense
identification concrete abstract context  proceedings emnlp  pp 
        edinburgh  uk 
  

fimultimodal distributional semantics

turney  p     pantel  p          frequency meaning  vector space models semantics  journal artificial intelligence research             
van de sande  k   gevers  t     snoek  c          evaluating color descriptors object
scene recognition  ieee transactions pattern analysis machine intelligence 
                 
van overschelde  j   rawson  k     dunlosky  j          category norms  updated
expanded version battig montague        norms  journal memory
language             
vedaldi  a     fulkerson  b          vlfeat open portable library computer
vision algorithms  proceedings acm multimedia  pp            firenze  italy 
von ahn  l          games purpose  computer               
vreeswijk  d  t   huurnink  b     smeulders  a  w          text image subject
classifiers  dense works better  proceedings acm multimedia  pp           
scottsdale  az 
wang  j   yang  j   yu  k   lv  f   huang  t     gong  y          locality constrained
linear coding image classification  proceedings cvpr  pp            san
francisco  ca 
weeds  j          measures applications lexical distributional similarity  ph d 
thesis  department informatics  university sussex 
wittgenstein  l          philosophical investigations  blackwell  oxford  uk  translated
g e m  anscombe 
yang  j   jiang  y  g   hauptmann  a     ngo  c  w          evaluating bag of visualwords representations scene classification  wang  j  z   boujemaa  n   bimbo 
a  d     li  j   eds    multimedia information retrieval  pp          acm 
zhao  y     karypis  g          criterion functions document clustering  experiments
analysis  tech  rep         university minnesota department computer
science 

  


