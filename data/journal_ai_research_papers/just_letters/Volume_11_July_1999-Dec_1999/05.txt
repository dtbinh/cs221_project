journal of artificial intelligence research                  

submitted       published     

evolutionary algorithms for reinforcement learning
david e  moriarty

moriarty isi edu

university of southern california  information sciences institute
     admiralty way  marina del rey  ca      

alan c  schultz

navy center for applied research in artificial intelligence
naval research laboratory  washington dc           

schultz aic nrl navy mil

john j  grefenstette

institute for biosciences  bioinformatics and biotechnology
george mason university  manassas  va      

gref ib  gmu edu

abstract

there are two distinct approaches to solving reinforcement learning problems  namely 
searching in value function space and searching in policy space  temporal difference methods and evolutionary algorithms are well known examples of these approaches  kaelbling 
littman and moore recently provided an informative survey of temporal difference methods  this article focuses on the application of evolutionary algorithms to the reinforcement
learning problem  emphasizing alternative policy representations  credit assignment methods  and problem specific genetic operators  strengths and weaknesses of the evolutionary
approach to reinforcement learning are presented  along with a survey of representative
applications 

   introduction
kaelbling  littman  and moore        and more recently sutton and barto        provide informative surveys of the field of reinforcement learning  rl   they characterize two
classes of methods for reinforcement learning  methods that search the space of value functions and methods that search the space of policies  the former class is exemplified by
the temporal difference  td  method and the latter by the evolutionary algorithm  ea 
approach  kaelbling et al  focus entirely on the first set of methods and they provide an
excellent account of the state of the art in td learning  this article is intended to round
out the picture by addressing evolutionary methods for solving the reinforcement learning
problem 
as kaelbling et al  clearly illustrate  reinforcement learning presents a challenging array
of diculties in the process of scaling up to realistic tasks  including problems associated
with very large state spaces  partially observable states  rarely occurring states  and nonstationary environments  at this point  which approach is best remains an open question  so
it is sensible to pursue parallel lines of research on alternative methods  while it is beyond
the scope of this article to address whether it is better in general to search value function
space or policy space  we do hope to highlight some of the strengths of the evolutionary
approach to the reinforcement learning problem  the reader is advised not to view this
c     


ai access foundation and morgan kaufmann publishers  all rights reserved 

fimoriarty  schultz    grefenstette

article as an ea vs  td discussion  in some cases  the two methods provide complementary
strengths  so hybrid approaches are advisable  in fact  our survey of implemented systems
illustrates that many ea based reinforcement learning systems include elements of tdlearning as well 
the next section spells out the reinforcement learning problem  in order to provide a
specific anchor for the later discussion  section   presents a particular td method  section   outlines the approach we call evolutionary algorithms for reinforcement learning
 earl   and provides a simple example of a particular earl system  the following three
sections focus on features that distinguish eas for rl from eas for general function optimization  including alternative policy representations  credit assignment methods  and
rl specific genetic operators  sections   and   highlight some strengths and weaknesses
of the ea approach  section    briey surveys some successful applications of ea systems
on challenging rl tasks  the final section summarizes our presentation and points out
directions for further research 

   reinforcement learning

all reinforcement learning methods share the same goal  to solve sequential decision tasks
through trial and error interactions with the environment  barto  sutton    watkins       
grefenstette  ramsey    schultz         in a sequential decision task  an agent interacts
with a dynamic system by selecting actions that affect state transitions to optimize some
reward function  more formally  at any given time step t  an agent perceives its state
st and selects an action at  the system responds by giving the agent some  possibly zero 
numerical reward r st  and changing into state st       st   at   the state transition may be
determined solely by the current state and the agent s action or may also involve stochastic
processes 
the agent s goal is to learn a policy     s   a  which maps states to actions  the
optimal policy     can be defined in many ways  but is typically defined as the policy that
produces the greatest cumulative reward over all states s 

   argmax
v   s     s 


   

where v   s  is the cumulative reward received from state s using policy    there are also
many ways to compute v   s   one approach uses a discount rate  to discount rewards
over time  the sum is then computed over an infinite horizon 

v

 

 
x
s      ir

t

i  

t i

   

where rt is the reward received at time step t  alternatively  v   s  could be computed by
summing the rewards over a finite horizon h 

v   st   

xh r
i  

t i

   

the agent s state descriptions are usually identified with the values returned by its
sensors  which provide a description of both the agent s current state and the state of the
   

fievolutionary algorithms for reinforcement learning

world  often the sensors do not give the agent complete state information and thus the
state is only partially observable 
besides reinforcement learning  intelligent agents can be designed by other paradigms 
notably planning and supervised learning  we briey note some of the major differences
among these approaches  in general  planning methods require an explicit model of the
state transition function   s  a   given such a model  a planning algorithm can search
through possible action choices to find an action sequence that will guide the agent from
an initial state to a goal state  since planning algorithms operate using a model of the
environment  they can backtrack or  undo  state transitions that enter undesirable states 
in contrast  rl is intended to apply to situations in which a suciently tractable action
model does not exist  consequently  an agent in the rl paradigm must actively explore
its environment in order to observe the effects of its actions  unlike planning  rl agents
cannot normally undo state transitions  of course  in some cases it may be possible to
build up an action model through experience  sutton         enabling more planning as
experience accumulates  however  rl research focuses on the behavior of an agent when it
has insucient knowledge to perform planning 
agents can also be trained through supervised learning  in supervised learning  the agent
is presented with examples of state action pairs  along with an indication that the action
was either correct or incorrect  the goal in supervised learning is to induce a general policy
from the training examples  thus  supervised learning requires an oracle that can supply
correctly labeled examples  in contrast  rl does not require prior knowledge of correct
and incorrect decisions  rl can be applied to situations in which rewards are sparse  for
example  rewards may be associated only with certain states  in such cases  it may be
impossible to associate a label of  correct  or  incorrect  on particular decisions without
reference to the agent s subsequent decisions  making supervised learning infeasible 
in summary  rl provides a exible approach to the design of intelligent agents in situations for which both planning and supervised learning are impractical  rl can be applied
to problems for which significant domain knowledge is either unavailable or costly to obtain 
for example  a common rl task is robot control  designers of autonomous robots often
lack sucient knowledge of the intended operational environment to use either the planning
or the supervised learning regime to design a control policy for the robot  in this case  the
goal of rl would be to enable the robot to generate effective decision policies as it explores
its environment 
figure   shows a simple sequential decision task that will be used as an example later
in this paper  the task of the agent in this grid world is to move from state to state by
selecting among two actions  right  r  or down  d   the sensor of the agent returns the
identity of the current state  the agent always starts in state a  and receives the reward
indicated upon visiting each state  the task continues until the agent moves off the grid
world  e g   by taking action d from state a    the goal is to learn a policy that returns
the highest cumulative rewards  for example  a policy which results in the sequences of
actions r  d  r  d  d  r  r  d starting from from state a  gives the optimal score of    
   

fimoriarty  schultz    grefenstette

a

b

c

d

e

 

 

 

 

  

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

figure    a simple grid world sequential decision task  the agent starts in state a  and
receives the row and column of the current box as sensory input  the agent moves
from one box to another by selecting between two moves  right or down   and the
agent s score is increased by the payoff indicated in each box  the goal is to find
a policy that maximizes the cumulative score 

    policy space vs  value function space
given the reinforcement learning problem as described in the previous section  we now
address the main topic  how to find an optimal policy      we consider two main approaches 
one involves search in policy space and the other involves search in value function space 
policy space search methods maintain explicit representations of policies and modify
them through a variety of search operators  many search methods have been considered 
including dynamic programming  value iteration  simulated annealing  and evolutionary
algorithms  this paper focuses on evolutionary algorithms that have been specialized for
the reinforcement learning task 
in contrast  value function methods do not maintain an explicit representation of a
policy  instead  they attempt learn the value function v    which returns the expected
cumulative reward for the optimal policy from any state  the focus of research on value
function approaches to rl is to design algorithms that learn these value functions through
experience  the most common approach to learning value functions is the temporal difference  td  method  which is described in the next section 

   temporal difference algorithms for reinforcement learning
as stated in the introduction  a comprehensive comparison of value function search and
direct policy space search is beyond the scope of this paper  nevertheless  it will be useful
to point out key conceptual differences between typical value function methods and typical
evolutionary algorithms for searching policy space  the most common approach for learning
a value function v for rl problems is the temporal difference  td  method  sutton        
   

fievolutionary algorithms for reinforcement learning

the td learning algorithm uses observations of prediction differences from consecutive
states to update value predictions  for example  if two consecutive states i and j return
payoff prediction values of   and    respectively  then the difference suggests that the payoff
from state i may be overestimated and should be reduced to agree with predictions from
state j   updates to the value function v are achieved using the following update rule 

v  st    v  st    ff v  st      v  st    rt 
   
where ff represents the learning rate and rt any immediate reward  thus  the difference in
predictions  v  st     v  st    from consecutive states is used as a measure of prediction error 
consider a chain of value predictions v  s     v  sn   from consecutive state transitions with
the last prediction v  sn   containing the only non zero reward from the environment  over

many iterations of this sequence  the update rule will adjust the values of each state so that
they agree with their successors and eventually with the reward received in v  sn    in other
words  the single reward is propagated backwards through the chain of value predictions 
the net result is an accurate value function that can be used to predict the expected reward
from any state of the system 
as mentioned earlier 
the goal
of td methods is to learn the value function for the




optimal policy  v   given v   the optimal action    s   can be computed using the
following equation 
    s  a  
 s    argmax
v
a

   

of course  we have already stated that in rl the state transition function   s  a  is unknown
to the agent  without this knowledge  we have no way of evaluating      an alternative
value function that can be used to compute   s  is called a q function  q s  a   watkins 
      watkins   dayan         the q function is a value function that represents the
expected value of taking action a in state s and acting optimally thereafter 

q s  a    r s    v    s  a  

   
where r s  represents any immediate reward received in state s  given the q function 
actions from the optimal policy can be directly computed using the following equation 

 s    argmax
q s  a 
a

   

q st  at    q st  at    ff max
q st    at      q st  at    r st  
a   

   

table   shows the q function for the grid world problem of figure    this table based
representation of the q function associates cumulative future payoffs for each state action
pair in the system   the letter number pairs at the top represent the state given by the row
and column in figure    and r and d represent the actions right and down  respectively  
the td method adjusts the q values after each decision  when selecting the next action 
the agent considers the effect of that action by examining the expected value of the state
transition caused by the action 
the q function is learned through the following td update equation 
t

   

fimoriarty  schultz    grefenstette

a  a  a  a  a  b  b  b  b  b  c  c  c  c  c  d  d  d  d  d  e  e  e  e  e 
r                                                        
d                                                         

table    a q function for the simple grid world  a value is associated with each state action
pair 
essentially  this equation updates q st   at  based on the current reward and the predicted
reward if all future actions are selected optimally  watkins and dayan        proved that
if updates are performed in this fashion and if every q value is explicitly represented 
the estimates will asymptotically converge to the correct values  a reinforcement learning
system can thus use the q values to select the optimal action in any state  because qlearning is the most widely known implementation of temporal difference learning  we will
use it in our qualitative comparisons with evolutionary approaches in later sections 

   evolutionary algorithms for reinforcement learning  earl 
the policy space approach to rl searches for policies that optimize an appropriate objective
function  while many search algorithms might be used  this survey focuses on evolutionary
algorithms  we begin with a brief overview of a simple ea for rl  followed by a detailed
discussion of features that characterize the general class of eas for rl 

    design considerations for evolutionary algorithms

evolutionary algorithms  eas  are global search techniques derived from darwin s theory
of evolution by natural selection  an ea iteratively updates a population of potential
solutions  which are often encoded in structures called chromosomes  during each iteration 
called a generation  the ea evaluates solutions and generates offspring based on the fitness
of each solution in the task environment  substructures  or genes  of the solutions are then
modified through genetic operators such as mutation and recombination  the idea is that
structures that are associated with good solutions can be mutated or combined to form
even better solutions in subsequent generations  the canonical evolutionary algorithm is
shown in figure    there have been a wide variety of eas developed  including genetic
algorithms  holland        goldberg         evolutionary programming  fogel  owens   
walsh         genetic programming  koza         and evolutionary strategies  rechenberg 
      
eas are general purpose search methods and have been applied in a variety of domains
including numerical function optimization  combinatorial optimization  adaptive control 
adaptive testing  and machine learning  one reason for the widespread success of eas is
that there are relatively few requirements for their application  namely 
   an appropriate mapping between the search space and the space of chromosomes  and
   an appropriate fitness function 
   

fievolutionary algorithms for reinforcement learning

procedure ea
begin
t     
initialize p t  
evaluate structures in p t  
while termination condition not satisfied do
begin
t   t     
select p t  from p t    
alter structures in p t  
evaluate structures in p t  
end
end 
figure    pseudo code evolutionary algorithm 
for example  in the case of parameter optimization  it is common to represent the list of
parameters as either a vector of real numbers or a bit string that encodes the parameters 
with either of these representations  the  standard  genetic operators of mutation and
cut and splice crossover can be applied in a straightforward manner to produce the genetic
variations required  see figure     the user must still decide on a  rather large  number
of control parameters for the ea  including population size  mutation rates  recombination
rates  parent selection rules  but there is an extensive literature of studies which suggest
that eas are relatively robust over a wide range of control parameter settings  grefenstette 
      schaffer  caruana  eshelman    das         thus  for many problems  eas can be
applied in a relatively straightforward manner 
however  for many other applications  eas need to be specialized for the problem domain  grefenstette         the most critical design choice facing the user is the representation  that is  the mapping between the search space of knowledge structures  or  the
phenotype space  and the space of chromosomes  the genotype space   many studies have
shown that the effectiveness of eas is sensitive to the choice of representations  it is not
sucient  for example  to choose an arbitrary mapping from the search space into the space
of chromosomes  apply the standard genetic operators and hope for the best  what makes a
good mapping is a subject for continuing research  but the general consensus is that candidate solutions that share important phenotypic similarities must also exhibit similar forms
of  building blocks  when represented as chromosomes  holland         it follows that the
user of an ea must carefully consider the most natural way to represent the elements of
the search space as chromosomes  moreover  it is often necessary to design appropriate
mutation and recombination operators that are specific to the chosen representation  the
end result of this design process is that the representation and genetic operators selected
for the ea comprise a form of search bias similar to biases in other machine learning meth   

fimoriarty  schultz    grefenstette

parent   

a

b

c

d

e

f

g

parent   

a

b

c

d

e

f

g

offspring   

a

b

c

d

e

f

g

offspring   

a

b

c

d

e

f

g

figure    genetic operators on fixed position representation  the two offspring are generated by crossing over the selected parents  the operation shown is called one point
crossover  the first offspring inherits the initial segment of one parent and the
final segment of the other parent  the second offspring inherits the same pattern
of genes from the opposite parents  the crossover point is position    chosen at
random  the second offspring has also incurred a mutation in the shaded gene 
ods  given the proper bias  the ea can quickly identify useful  building blocks  within the
population  and converge on the most promising areas of the search space  
in the case of rl  the user needs to make two major design decisions  first  how will the
space of policies be represented by chromosomes in the ea  second  how will the fitness of
population elements be assessed  the answers to these questions depend on how the user
chooses to bias the ea  the next section presents a simple earl that adopts the most
straightforward set of design decisions  this example is meant only to provide a baseline
for comparison with more elaborate designs 

    a simple earl

as the remainder of this paper shows  there are many ways to use eas to search the space
of rl policies  this section provides a concrete example of a simple earl  which we call
earl    the pseudo code is shown in figure    this system provides the ea counterpart
to the simple table based td system described in section   
the most straightforward way to represent a policy in an ea is to use a single chromosome per policy with a single gene associated with each observed state  in earl    each
gene s value  or allele in biological terminology  represents the action value associated with
the corresponding state  as shown in figure    table   shows part of an earl  population
of policies for the sample grid world problem  the number of policies in a population is
usually on the order of     to      
the fitness of each policy in the population must reect the expected accumulated fitness
for an agent that uses the given policy  there are no fixed constraints on how the fitness of
an individual policy is evaluated  if the world is deterministic  like the sample grid world 
   other ways to exploit problem specific knowledge in eas include the use of heuristics to initialize the
population and the hybridization with problem specific search algorithms  see  grefenstette        for
further discussions of these methods 

   

fievolutionary algorithms for reinforcement learning

procedure earl  
begin
t     
initialize a population of policies  p t  
evaluate policies in p t  
while termination condition not satisfied do
begin
t   t     
select high payoff policies  p t   from policies in p t    
update policies in p t  
evaluate policies in p t  
end
end 
figure    pseudo code for evolutionary algorithm reinforcement learning system 
policy i 

s 
a 

s 
a 

s 
a 

   

sn
an

figure    table based policy representation  each observed state has a gene which indicates
the preferred action for that state  with this representation  standard genetic
operators such as mutation and crossover can be applied 
the fitness of a policy can be evaluated during a single trial that starts with the agent in the
initial state and terminates when the agent reaches a terminal state  e g   falls off the grid
in the grid world   in non deterministic worlds  the fitness of a policy is usually averaged
over a sample of trials  other options include measuring the total payoff achieved by the
agent after a fixed number of steps  or measuring the number of steps required to achieve
a fixed level of payoff 
once the fitness of all policies in the population has been determined  a new population
is generated according to the steps in the usual ea  figure     first  parents are selected
for reproduction  a typical selection method is to probabilistically select individuals based
on relative fitness 
 pi  
pr pi     pnfitness
j    fitness pj  

   

where pi represents individual i and n is the total number of individuals  using this selection
rule  the expected number of offspring for a given policy is proportional to that policy s
fitness  for example  a policy with average fitness might have a single offspring  whereas
   

fimoriarty  schultz    grefenstette

policy
 
 
 
 
 

a 
d
d
r
d
r

a 
r
d
d
d
d

a 
d
d
d
d
d

a 
d
d
r
d
d

a 
r
r
r
r
r

b 
r
r
d
d
d

b 
r
r
r
r
r

b 
r
r
d
r
r

b 
r
r
r
r
d

b 
r
r
r
r
r

c 
d
d
d
r
r

c 
r
d
d
d
d

c 
d
r
d
r
r

c 
d
r
r
r
r

c 
r
d
d
r
d

d 
r
r
r
d
r

d 
d
d
d
r
d

d 
r
r
r
r
r

d 
r
r
r
d
r

d 
r
r
r
r
d

e 
d
d
d
d
d

e 
r
r
r
r
r

e 
r
d
d
d
d

e 
d
d
d
d
d

e  fitness
r  
r  
d   
r   
d   

table    an ea population of five decision policies for the sample grid world  this simple
policy representation specifies an action for each state of the world  the fitness
corresponds to the payoffs that are accumulated using each policy in the grid
world 
a policy with twice the average fitness would have two offspring   offspring are formed
by cloning the selected parents  then new policies are generated by applying the standard
genetic operators of crossover and mutation to the clones  as shown in figure    the process
of generating new populations of strategies can continue indefinitely or can be terminated
after a fixed number of generations or once an acceptable level of performance is achieved 
for simple rl problems such as the grid world  earl  may provide an adequate approach  in later sections  we will point out some ways in which even earl  exhibits
strengths that are complementary to td methods for rl  however  as in the case of td
methods  earl methods have been extended to handle the many challenges inherent in
more realistic rl problems  the following sections survey some of these extensions  organized around three specific biases that distinguish eas for reinforcement learning  earl 
from more generic eas  policy representations  fitness credit assignment models  and rlspecific genetic operators 

   policy representations in earl

perhaps the most critical feature that distinguishes classes of eas from one another is the
representation used  for example  eas for function optimization use a simple string or
vector representation  whereas eas for combinatorial optimization use distinctive representations for permutations  trees or other graph structures  likewise  eas for rl use a
distinctive set of representations for policies  while the range of potential policy representations is unlimited  the representations used in most earl systems to date can be
largely categorized along two discrete dimensions  first  policies may be represented either by condition action rules or by neural networks  second  policies may be represented
by a single chromosome or the representation may be distributed through one or more
populations 

    single chromosome representation of policies
      rule based policies

for most rl problems of practical interest  the number of observable states is very large 
and the simple table based representation in earl  is impractical  for large scale state
   many other parent selection rules have been explored  grefenstette      a      b  

   

fievolutionary algorithms for reinforcement learning

policy i 

c i   ai 

c i   ai 

c i   ai 

   

c ik  aik

figure    rule based policy representation  each gene represents a condition action rule
that maps a set of states to an action  in general  such rules are independent
of the position along the chromosome  conict resolution mechanisms may be
needed if the conditions of rules are allowed to intersect 
w 
w k 
policy i 

w 

w 

w 

   

wk

  
   

wk
wj

figure    a simple parameter representation of weights for a neural network  the fitness
of the policy is the payoff when the agent uses the corresponding neural net as
its decision policy 
spaces  it is more reasonable to represent a policy as a set of condition action rules in which
the condition expresses a predicate that matches a set of states  as shown in figure    early
examples of this representation include the systems ls    smith        and ls    schaffer
  grefenstette         followed later by samuel  grefenstette et al         
      neural net representation of policies

as in td based rl systems  earl systems often employ neural net representations as
function approximators  in the simplest case  see figure     a neural network for the
agent s decision policy is represented as a sequence of real valued connection weights  a
straightforward ea for parameter optimization can be used to optimize the weights of
the neural network  belew  mcinerney    schraudolph        whitley  dominic  das   
anderson        yamauchi   beer         this representation thus requires the least
modification of the standard ea  we now turn to distributed representations of policies in
earl systems 

    distributed representation of policies

in the previous section we outlined earl approaches that treat the agent s decision policy
as a single genetic structure that evolves over time  this section addresses earl approaches
that decompose a decision policy into smaller components  such approaches have two
potential advantages  first  they allow evolution to work at a more detailed level of the task 
e g   on specific subtasks  presumably  evolving a solution to a restricted subtask should be
   

fimoriarty  schultz    grefenstette

sensors

message list

rewards

classifiers

decision

evolutionary
algorithm

figure    holland s learning classifier system 
easier than evolving a monolithic policy for a complex task  second  decomposition permits
the user to exploit background knowledge  the user might base the decomposition into
subtasks on a prior analysis of the overall performance task  for example  it might be known
that certain subtasks are mutually exclusive and can therefore be learned independently 
the user might also decompose a complex task into subtasks such that certain components
can be explicitly programmed while other components are learned 
in terms of knowledge representation in earl  the alternative to the single chromosome
representation is to distribute the policy over several population elements  by assigning a
fitness to these individual elements of the policy  evolutionary selection pressure can be
brought to bear on more detailed aspects of the learning task  that is  fitness is now a
function of individual subpolicies or individual rules or even individual neurons  this general
approach is analogous to the classic td methods that take this approach to the extreme of
learning statistics concerning each state action pair  as in the case of single chromosome
representations  we can partition distributed earl representations into rule based and
neural net based classes 
      distributed rule based policies

the most well known example of a distributed rule based approach to earl is the learning classifier systems  lcs  model  holland   reitman        holland        wilson 
       an lcs uses an evolutionary algorithm to evolve if then rules called classifiers that
map sensory input to an appropriate action  figure   outlines holland s lcs framework
 holland         when sensory input is received  it is posted on the message list  if the left
hand side of a classifier matches a message on the message list  its right hand side is posted
on the message list  these new messages may subsequently trigger other classifiers to post
messages or invoke a decision from the lcs  as in the traditional forward chaining model
of rule based systems 
in an lcs  each chromosome represents a single decision rule and the entire population
represents the agent s policy  in general  classifiers map a set of observed states to a set of
messages  which may be interpreted as either internal state changes or actions  for example 
   

fievolutionary algorithms for reinforcement learning

condition
action strength
a 
  r
    
  
  d
    
d 

   
 

d

    

table    lcs population for grid world  the   is a don t care symbol which allows for
generality in conditions  for example  the first rule says  turn right in column
a   the strength of a rule is used for conict resolution and for parent selection in
the genetic algorithm 

lcs
lcs

lcs

environment

figure    a two level hierarchical alecsys system  each lcs learns a specific behavior 
the interactions among the rule sets are pre programmed 
if the learning agent for the grid world in figure   has two sensors  one for the column and
one for the row  then the population in an lcs might appear as shown in table    the
first classifier matches any state in the column a and recommends action r  each classifier
has a statistic called strength that estimates the utility of the rule  the strength statistics
are used in both conict resolution  when more than one action is recommended  and as
fitness for the genetic algorithm  genetic operators are applied to highly fit classifiers to
generate new rules  generally  the population size  i e   the number of rules in the policy 
is kept constant  thus classifiers compete for space in the policy 
another way that earl systems distribute the representation of policies is to partition
the policy into separate modules  with each module updated by its own ea  dorigo and
colombetti        describe an architecture called alecsys in which a complex reinforcement learning task is decomposed into subtasks  each of which is learned via a separate
lcs  as shown in figure    they provide a method called behavior analysis and training
 bat  to manage the incremental training of agents using the distributed lcs architecture 
the single chromosome representation can also be extended by partitioning the policy across multiple co evolving populations  for example  in the cooperative co evolution
model  potter         the agent s policy is formed by combining chromosomes from several independently evolving populations  each chromosome represents a set of rules  as
in figure    but these rules address only a subset of the performance task  for example 
separate populations might evolve policies for different components of a complex task  or
   

fimoriarty  schultz    grefenstette

ea i
ea  

domain
model
collaboration

fitness

evolutionary
algorithm

population

representative

merge

representative

individual
to be
evaluated

representative

ea  

ea n

representative

figure     cooperative coevolutionary architecture from the perspective of the ith ea instance  each ea contributes a representative  which is merged with the others 
representatives to form a collaboration  or policy for the agent  the fitness of
each representative reects the average fitness of its collaborations 

might address mutually exclusive sets of observed states  the fitness of each chromosome is
computed based on the overall fitness of the agents that employ that chromosome as part of
its combined chromosomes  the combined chromosomes represent the decision policy and
are called a collaboration  figure     
      distributed network based policies

distributed earl systems using neural net representations have also been designed  in
 potter   de jong         separate populations of neurons evolve  with the evaluation of
each neuron based on the fitness of a collaboration of neurons selected from each population 
in sane  moriarty   miikkulainen      a         two separate populations are maintained
and evolved  a population of neurons and a population of network blueprints  the motivation for sane comes from our a priori knowledge that individual neurons are fundamental
building blocks in neural networks  sane explicitly decomposes the neural network search
problem into several parallel searches for effective single neurons  the neuron level evolution provides evaluation and recombination of the neural network building blocks  while the
population of blueprints search for effective combinations of these building blocks  figure   
gives an overview of the interaction of the two populations 
each individual in the blueprint population consists of a set of pointers to individuals
in the neuron population  during each generation  neural networks are constructed by
combining the hidden neurons specified in each blueprint  each blueprint receives a fitness
according to how well the corresponding network performs in the task  each neuron receives
a fitness according to how well the top networks in which it participates perform in the
task  an aggressive genetic selection and recombination strategy is used to quickly build
and propagate highly fit structures in both the neuron and blueprint populations 
   

fievolutionary algorithms for reinforcement learning

network blueprint population

neuron population

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

figure     an overview of the two populations in sane  each member of the neuron population specifies a series of connections  connection labels and weights  to be
made within a neural network  each member of the network blueprint population specifies a series of pointers to specific neurons which are used to build a
neural network 

   fitness and credit assignment in earl

evolutionary algorithms are all driven by the concept of natural selection  population
elements that have higher fitness leave more offspring to later generations  thus inuencing
the direction of search in favor of high performance regions of the search space  the concept
of fitness is central to any ea  in this section  we discuss features of the fitness model that
are common across most earl systems  we specifically focus on ways in which the fitness
function reects the distinctive structure of the rl problem 

    the agent model

the first common features of all earl fitness models is that fitness is computed with
respect to an rl agent  that is  however the policy is represented in the ea  it must be
converted to a decision policy for an agent operating in a rl environment  the agent is
assumed to observe a description of the current state  select its next action by consulting
its current policy  and collect whatever reward is provided by the environment  in earl
systems  as in td systems  the agent is generally assumed to perform very little additional
computation when selecting its next action  while neither approach limits the agent to
strict stimulus response behavior  it is usually assumed that the agent does not perform
extensive planning or other reasoning before acting  this assumption reects the fact that
rl tasks involve some sort of control activity in which the agent must respond to a dynamic
environment within a limited time frame 
   

fimoriarty  schultz    grefenstette

    policy level credit assignment

as shown in the previous section  the meaning of fitness in earl systems may vary depending on what the population elements represent  in a single chromosome representation 
fitness is associated with entire policies  in a distributed representation  fitness may be associated with individual decision rules  in any case  fitness always reects accumulated
rewards received by the agent during the course of interaction with the environment  as
specified in the rl model  fitness may also reect effort expended  or amount of delay 
it is worthwhile considering the different approaches to credit assignment in the td
and ea methods  in a reinforcement learning problem  payoffs may be sparse  that is 
associated only with certain states  consequently  a payoff may reect the quality of an
extended sequence of decisions  rather than any individual decision  for example  a robot
may receive a reward after a movement that places it in a  goal  position within a room 
the robot s reward  however  depends on many of its previous movements leading it to
that point  a dicult credit assignment problem therefore exists in how to apportion the
rewards of a sequence of decisions to individual decisions 
in general  ea and td methods address the credit assignment problem in very different ways  in td approaches  credit from the reward signal is explicitly propagated to
each decision made by the agent  over many iterations  payoffs are distributed across a
sequence of decisions so that an appropriately discounted reward value is associated with
each individual state and decision pair 
in simple earl systems such as earl    rewards are associated only with sequences
of decisions and are not distributed to the individual decisions  credit assignment for an
individual decision is made implicitly  since policies that prescribe poor individual decisions
will have fewer offspring in future generations  by selecting against poor policies  evolution
automatically selects against poor individual decisions  that is  building blocks consisting
of particular state action pairs that are highly correlated with good policies are propagated
through the population  replacing state action pairs associated with poorer policies 
figure    illustrates the differences in credit assignment between td and earl  in the
grid world of figure    the q learning td method explicitly assigns credit or blame to each
individual state action pair by passing back the immediate reward and the estimated payoff
from the new state  thus  an error term becomes associated with each action performed by
the agent  the ea approach does not explicitly propagate credit to each action but rather
associates an overall fitness with the entire policy  credit is assigned implicitly  based on the
fitness evaluations of entire sequences of decisions  consequently  the ea will tend to select
against policies that generate the first and third sequences because they achieve lower fitness
scores  the ea thus implicitly selects against action d in state b   for example  which is
present in the bad sequences but not present in the good sequences 

    subpolicy credit assignment

besides the implicit credit assignment performed on building blocks  earl systems have
also addressed the credit assignment problem more directly  as shown in section    the
individuals in an earl system might represent either entire policies or components of
a policy  e g   component rule sets  individual decision rules  or individual neurons   for
distributed representation earls  fitness is explicitly assigned to individual components 
   

fievolutionary algorithms for reinforcement learning

td explicit credit assignment
  max q b  a  

a  r

b  d

  max q b  a  

a  r

  max q a  a  

a  d

  max q b  a  

a  d

a  r

b  d

b  d

b  d

c 

 

c 

a  r

b  d

b  r

c  d

c 

 

c 

a  d

a  r

b  d

b  d

c 

 

d 

a  d

a  d

b  r

c  d

d 

 

  max q c  a  

  max q c  a  

b  d

   max q c  a  

b  r

c 

c  d

   max q b  a  

b  d

fitness

  max q c  a  

b  d

   max q c  a  

b  r

  max q b  a  

a  r

   max q b  a  

b  d

  max q b  a  

b  d

  max q a  a  

a  d

  max q b  a  

ea implicit credit assignment

  max q d  a  

c  d

figure     explicit vs  implicit credit assignment  the q learning td method assigns credit
to each state action pair based on the immediate reward and the predicted future
rewards  the ea method assigns credit implicitly by associating fitness values
with entire sequences of decisions 
in cases in which a policy is represented by explicit components  different fitness functions
can be associated with different evolving populations  allowing the implementer to  shape 
the overall policy by evolving subpolicies for specific subtasks  dorigo   colombetti       
potter  de jong    grefenstette         the most ambitious goal is to allow the system to
manage the number of co evolving species as well as the form of interactions  potter        
this exciting research is still at an early stage 
for example  in the lcs model  each classifier  decision rule  has a strength which is
updated using a td like method called the bucket brigade algorithm  holland         in the
bucket brigade algorithm  the strength of a classifier is used to bid against other classifiers
for the right to post messages  bids are subtracted from winning classifiers and passed back
to the classifiers that posted the enabling message on the previous step  classifier strengths
are thus reinforced if the classifier posts a message that triggers another classifier  the
classifier that invokes a decision from the lcs receives a strength reinforcement directly
from the environment  the bucket brigade bid passing mechanism clearly bears a strong
relation to the method of temporal differences  sutton         the bucket brigade updates
a given classifier s strength based on the strength of the classifiers that fire as a direct result
of its activation  the td methods differ slightly in this respect because they assign credit
based strictly on temporal succession and do not take into account causal relations of steps 
it remains unclear which is more appropriate for distributing credit 
even for single chromosome representations  td like methods have been adopted in
some earl systems  in samuel  each gene  decision rule  also maintains a quantity called
strength that is used to resolve conict when more than one rule matches the agent s current
sensor readings  when payoff is obtained  thereby terminating the trial   the strengths of
   

fimoriarty  schultz    grefenstette

all rules that fired during the trial are updated  grefenstette         in addition to resolving
conicts  a rule s strength also plays a role in triggering mutation operations  as described
in the next section 

   rl specific genetic operators

the creation of special genetic operators provides another avenue for imposing an rlspecific bias on eas  specialized operators in earl systems first appeared in  holland 
       in which so called triggered operators were responsible for creating new classifiers
when the learning agent found that no classifier in its existing population matched the
agent s current sensor readings  in this case  a high strength rule was explicitly generalized
to cover the new set of sensor readings  a similar rule creation operator was included in
early versions of samuel  grefenstette et al          later versions of samuel included
a number of mutation operators which created altered rules based on an agent s early
experiences  for example  samuel s specialization mutation operator is triggered when
a low strength  general rule fires during an episode that results in high payoff  in such a
case  the rule s conditions are reduced in generality to more closely match the agent s sensor
readings  for example  if the agent has a sensor readings  range       bearing       
and the original rule is 
if range            and bearing            then set turn       strength
    
then the new rule would be 
if range            and bearing             then set turn       strength
    
since the episode triggering the operator resulted in high payoff  one might suspect that
the original rule was over generalized  and that the new  more specific version might lead
to better results   the strength of the new rule is initialized to the payoff received during
the triggering episode   this is considered a lamarckian operator because the agent s
experience is causing a genetic change which is passed on to later offspring  
samuel also uses an rl specific crossover operator to recombine policies  in particular 
crossover in samuel attempts to cluster decision rules before assigning them to offspring 
for example  suppose that the traces of the most previous evaluations of the parent strategies are as follows  ri j denotes the j th decision rule in policy i  
trace for parent    
episode 
  
 
   r      r      r      r    high payoff
   r      r      r   
low payoff
   jean baptiste lamarck developed an evolutionary theory that stressed the inheritance of acquired characteristics  in particular acquired characteristics that are well adapted to the surrounding environment 
of course  lamarck s theory was superseded by darwin s emphasis on two stage adaptation  undirected
variation followed by selection  research has generally failed to substantiate any lamarckian mechanisms
in biological systems  gould        

   

fievolutionary algorithms for reinforcement learning

  
 
trace for parent    
  
 
   r      r   
   r      r      r   
  
 
then one possible offspring would be 

low payoff
high payoff

fr             r      r      r      r             r      r      r             r   g
the motivation here is that rules that fire in sequence to achieve a high payoff should be
treated as a group during recombination  in order to increase the likelihood that the offspring
policy will inherit some of the better behavior patterns of its parents  rules that do not
fire in successful episodes  e g   r     are randomly assigned to one of the two offspring 
this form of crossover is not only lamarckian  since it is triggered by the experiences
of the agent   but is directly related to the structure of the rl problem  since it groups
components of policies according to the temporal association among the decision rules 

   strengths of earl

the ea approach represents an interesting alternative for solving rl problems  offering
several potential advantages for scaling up to realistic applications  in particular  earl
systems have been developed that address dicult challenges in rl problems  including 
 large state spaces 
 incomplete state information  and
 non stationary environments 
this section focuses on ways that earl address these challenges 

    scaling up to large state spaces

many early papers in the rl literature analyze the eciency of alternative learning methods
on toy problems similar to the grid world shown in figure    while such studies are useful
as academic exercises  the number of observed states in realistic applications of rl is likely
to preclude any approach that requires the explicit storage and manipulation of statistics
associated with each observable state action pair  there are two ways that earl policy
representations help address the problem of large state spaces  generalization and selectivity 
      policy generalization

most earl policy representations specify the policy at a level of abstraction higher than an
explicit mapping from observed states to actions  in the case of rule based representations 
the rule language allows conditions to match sets of states  thus greatly reducing the storage
   

fimoriarty  schultz    grefenstette

a  a  a  a  a  b  b  b  b  b  c  c  c  c  c  d  d  d  d  d  e  e  e  e  e 
r                                                                  
l                                                                    

table    an approximated value function from the population in table    the table displays the average fitness for policies that select each state action pair and reects
the estimated impact each action has on overall fitness  given the tiny population
size in this example  the estimates are not particularly accurate  note the question
marks in states where actions have converged  since no policies select the alternative action  the population has no statistics on the impact of these actions on
fitness  this is different from simple td methods  where statistics on all actions
are maintained 

required to specify a policy  it should be noted  however  that the generality of the rules
within a policy may vary considerably  from the level of rules that specify an action for
a single observed state all the way to completely general rules that recommend an action
regardless of the current state  likewise  in neural net representations  the mapping function
is stored implicitly in the weights on the connections of the neural net  in either case  a
generalized policy representation facilitates the search for good policies by grouping together
states for which the same action is required 
      policy selectivity

most earl systems have selective representations of policies  that is  the ea learns mappings from observed states to recommended actions  usually eliminating explicit information
concerning less desirable actions  knowledge about bad decisions is not explicitly preserved 
since policies that make such decisions are selected against by the evolutionary algorithm
and are eventually eliminated from the population  the advantage of selective representations is that attention is focused on profitable actions only  reducing space requirements for
policies 
consider our example of the simple earl operating on the grid world  as the population evolves  policies normally converge to the best actions from a specific state  because of
the selective pressure to achieve high fitness levels  for example  the population shown in
table   has converged alleles  actions  in states a   a   b   b   d   e   and e   each of these
converged state action pairs is highly correlated with fitness  for example  all policies have
converged to action r in state b   taking action r in state b  achieves a much higher
expected return than action d     vs    from table     policies that select action d from
state b  achieve lower fitness scores and are selected against  for this simple earl  a snapshot of the population  table    provides an implicit estimate of a corresponding td value
function  table     but the distribution is biased toward the more profitable state actions
pairs 
   

fievolutionary algorithms for reinforcement learning

  
l

   
l

red

r

blue

r
   

green

l

blue

l

     

r
r
   
   

figure     an environment with incomplete state information  the circles represent the
states of the world and the colors represent the agent s sensory input  the agent
is equally likely to start in the red state or the green state

    dealing with incomplete state information

clearly  the most favorable condition for reinforcement learning occurs when the agent can
observe the true state of the dynamic system with which it interacts  when complete state
information is available  td methods make ecient use of available feedback by associating
reward directly with individual decisions  in real world situations  however  the agent s
sensors are more likely to provide only a partial view that may fail to disambiguate many
states  consequently  the agent will often be unable to completely distinguish its current
state  this problem has been termed perceptual aliasing or the hidden state problem  in
the case of limited sensory information  it may be more useful to associate rewards with
larger blocks of decisions  consider the situation in figure     in which the agent must
act without complete state information  circles represent the specific states of the world 
and the colors represent the sensor information the agent receives within the state  square
nodes represent goal states with the corresponding reward shown inside  in each state  the
agent has a choice of two actions  l or r   we further assume that the state transitions
are deterministic and that the agent is equally likely to start in either the state with the
red or green sensor readings 
in this example  there are two different states that return a sensor reading of blue 
and the agent is unable to distinguish between them  moreover  the actions for each blue
state return very different rewards  a q function applied to this problem treats the sensor
reading of blue as one observable state  and the rewards for each action are averaged over
both blue states  thus  q blue  l  and q blue  r  will converge to      and    respectively 
since the reward from q blue  r  is higher than the alternatives from observable states red
and green  the agent s policy under q learning will choose to enter observable state blue
each time  the final decision policy under q learning is shown in table    this table also
shows the optimal policy with respect to the agent s limited view of its world  in other
   

fimoriarty  schultz    grefenstette

value function policy optimal policy
r
r
l
r
r
l
expected reward
   
     

red
green
blue

table    the policy and expected reward returned by a converged q function compared to
the optimal policy given the same sensory information 
words  the policy reects the optimal choices if the agent cannot distinguish the two blue
states 
by associating values with individual observable states  the simple td methods are
vulnerable to hidden state problems  in this example  the ambiguous state information
misleads the td method  and it mistakenly combines the rewards from two different states
of the system  by confounding information from multiple states  td cannot recognize that
advantages might be associated with specific actions from specific states  for example  that
action l from the top blue state achieves a very high reward 
in contrast  since ea methods associate credit with entire policies  they rely more on
the net results of decision sequences than on sensor information  that may  after all  be
ambiguous  in this example  the evolutionary algorithm exploits the disparity in rewards
from the different blue states and evolves policies that enter the good blue state and avoid
the bad one  the agent itself remains unable to distinguish the two blue states  but the evolutionary algorithm implicitly distinguishes among ambiguous states by rewarding policies
that avoid the bad states 
for example  an ea method can be expected to evolve an optimal policy in the current
example given the existing  ambiguous state information  policies that choose the action
sequence r l when starting in the red state will achieve the highest levels of fitness  and
will therefore be selected for reproduction by the ea  if agents using these policies are
placed in the green state and select action l  they receive the lowest fitness score  since
their subsequent action  l from the blue sensors  returns a negative reward  thus  many of
the policies that achieve high fitness when started in the red state will be selected against if
they choose l from the green state  over the course of many generations  the policies must
choose action r from the green state to maximize their fitness and ensure their survival 
we confirmed these hypotheses in empirical tests  a q learner using single step updates
and a table based representation converged to the values in table   in every run  an
evolutionary algorithm  consistently converged     of its population on the optimal policy 
figure    shows the average percentage of the optimal policy in the population as a function
of time  averaged over     independent runs 
thus even simple ea methods such as earl  appear to be more robust in the presence
of hidden states than simple td methods  however  more refined sensor information could
still be helpful  in the previous example  although the ea policies achieve a better average
reward than the td policy  the evolved policy remains unable to procure both the    
   we used a binary tournament selection  a    policy population      crossover probability  and     
mutation rate 

   

fievolutionary algorithms for reinforcement learning

   

percentage optimal

  

  

  

  

 
 

  

  

  

  

  
generation

  

  

  

  

   

figure     the optimal policy distribution in the hidden state problem for an evolutionary
algorithm  the graph plots the percentage of optimal policies in the population 
averaged over     runs 
and     rewards from the two blue states  these rewards could be realized  however  if
the agent could separate the two blue states  thus  any method that generates additional
features to disambiguate states presents an important asset to ea methods  kaelbling
et al         describe several promising solutions to the hidden state problem  in which
additional features such as the agent s previous decisions and observations are automatically
generated and included in the agent s sensory information  chrisman        lin   mitchell 
      mccallum        ring         these methods have been effective at disambiguating
states for td methods in initial studies  but further research is required to determine the
extent to which similar methods can resolve significant hidden state information in realistic
applications  it would be useful to develop ways to use such methods to augment the sensory
data available in ea methods as well 

    non stationary environments

if the agent s environment changes over time  the rl problem becomes even more dicult 
since the optimal policy becomes a moving target  the classic trade off between exploration
and exploitation becomes even more pronounced  techniques for encouraging exploration
in td based rl include adding an exploration bonus to the estimated value of state action
pairs that reects how long it has been since the agent has tried that action  sutton        
and building a statistical model of the agent s uncertainty  dayan   sejnowski        
simple modifications of standard evolutionary algorithms offer an ability to track nonstationary environments  and thus provide a promising approach to rl for these dicult
cases 
the fact that evolutionary search is based on competition within a population of policies
suggest some immediate benefits for tracking non stationary environments  to the extent
that the population maintains a diverse set of policies  changes in the environment will bias
   

fimoriarty  schultz    grefenstette

selective pressure in favor of the policies that are most fit for the current environment  as
long as the environment changes slowly with respect to the time required to evaluate a
population of policies  the population should be able to track a changing fitness landscape
without any alteration of the algorithm  empirical studies show that maintaining the
diversity within the population may require a higher mutation rate than those usually
adopted for stationary environments  cobb   grefenstette        
in addition  special mechanisms have been explored in order to make eas more responsive to rapidly changing environments  for example   grefenstette        suggests
maintaining a random search within a restricted portion of the population  the random
population elements are analogous to immigrants from other populations with uncorrelated
fitness landscapes  maintaining this source of diversity permits the ea to respond rapidly
to large  sudden changes in the fitness landscape  by keeping the randomized portion of
the population to less than about     of the population  the impact on search eciency in
stationary environments is minimized  this is a general approach that can easily be applied
in earl systems 
other useful algorithms that have been developed to ensure diversity in evolving popultions include fitness sharing  goldberg   richardson         crowding  de jong        
and local mating  collins   jefferson         in goldberg s fitness sharing model  for example  similar individuals are forced to share a large portion of a single fitness value from
the shared solution point  sharing decreases the fitness of similar individuals and causes
evolution to select against individuals in overpopulated niches 
earl methods that employ distributed policy representations achieve diversity automatically and are well suited for adaptation in dynamic environments  in a distributed
representation  each individual represents only a partial solution  complete solutions are
built by combining individuals  because no individual can solve the task on its own  the
evolutionary algorithm will search for several complementary individuals that together can
solve the task  evolutionary pressures are therefore present to prevent convergence of the
population  moriarty and miikkulainen        showed how the inherent diversity and specialization in sane allow it to adapt much more quickly to changes in the environment
than standard  convergent evolutionary algorithms 
finally  if the learning system can detect changes in the environment  even more direct
response is possible  in the anytime learning model  grefenstette   ramsey         an
earl system maintains a case base of policies  indexed by the values of the environmental
detectors corresponding to the environment in which a given policy was evolved  when
an environmental change is detected  the population of policies is partially reinitialized 
using previously learned policies selected on the basis of similarity between the previously
encountered environment and the current environment  as a result  if the environment
changes are cyclic  then the population can be immediately seeded with those policies in
effect during the last occurrence of the current environment  by having a population of
policies  this approach is protected against some kinds of errors in detecting environmental
changes  for example  even if a spurious environmental change is mistakenly detected 
learning is not unduly affected  since only a part of the current population of policies is
replaced by previously learned policies  zhou        explored a similar approach based on
lcs 
   

fievolutionary algorithms for reinforcement learning

in summary  earl systems can respond to non stationary environments  both by techniques that are generic to evolutionary algorithms and by techniques that have been specifically designed with rl in mind 

   limitations of earl
although the ea approach to rl is promising and has a growing list of successful applications  as outlined in the following section   a number of challenges remain 

    online learning
we can distinguish two broad approaches to reinforcement learning  online learning and
oine learning  in online learning  an agent learns directly from its experiences in its
operational environment  for example  a robot might learn to navigate in a warehouse by
actually moving about its physical environment  there are two problems with using earl
in this situation  first  it is likely to require a large number of experiences in order to
evaluate a large population of policies  depending on how quickly the agent performs tasks
that result in some environmental feedback  it may take an unacceptable amount of time
to run hundreds of generations of an ea that evaluates hundreds or thousands of policies 
second  it may be dangerous or expensive to permit an agent to perform some actions in
its actual operational environment that might cause harm to itself or its environment  yet
it is very likely that at least some policies that the ea generates will be very bad policies 
both of these objections apply to td methods as well  for example  the theoretical results
that prove the optimality of q learning require that every state be visited infinitely often 
which is obviously impossible in practice  likewise  td methods may explore some very
undesirable states before an acceptable value function is found 
for both td and earl  practical considerations point toward the use of oine learning 
in which the rl system performs its exploration on simulation models of the environment 
simulation models provide a number of advantages for earl  including the ability to
perform parallel evaluations of all the policies in a population simultaneously  grefenstette 
      

    rare states
the memory or record of observed states and rewards differs greatly between ea and td
methods  temporal difference methods normally maintain statistics concerning every stateaction pair  as states are revisited  the new reinforcement is combined with the previous
value  new information thus supplements previous information  and the information content of the agent s reinforcement model increases during exploration  in this manner  td
methods sustain knowledge of both good and bad state action pairs 
as pointed out previously  ea methods normally maintain information only about good
policies or policy components  knowledge of bad decisions is not explicitly preserved  since
policies that make such decisions are selected against by the evolutionary algorithm and
are eventually eliminated from the population  for example  refer once again to table   
which shows the implicit statistics of the population from table    note the question
   

fimoriarty  schultz    grefenstette

marks in states where actions have converged  since no policies in the population select the
alternative action  the ea has no statistics on the impact of these actions on fitness 
this reduction in information content within the evolving population can be a disadvantage with respect to states that are rarely visited  in any evolutionary algorithm  the value
of genes that have no real impact on the fitness of the individual tends to drift to random
values  since mutations tend to accumulate in these genes  if a state is rarely encountered 
mutations may freely accumulate in the gene that describes the best action for that state 
as a result  even if the evolutionary algorithm learns the correct action for a rare state  that
information may eventually be lost due to mutations  in contrast  since table based td
methods permanently record information about all state action pairs  they may be more
robust when the learning agent does encounter a rare state  of course  if a td method
uses a function approximator such as a neural network as its value function  then it too
can suffer from memory loss concerning rare states  since many updates from frequently
occurring states can dominate the few updates from the rare states 

    proofs of optimality

one of the attractive features of td methods is that the q learning algorithm has a proof
of optimality  watkins   dayan         however  the practical importance of this result is
limited  since the assumptions underlying the proof  e g   no hidden states  all state visited
infinitely often  are not satisfied in realistic applications  the current theory of evolutionary
algorithms provide a similar level of optimality proofs for restricted classes of search spaces
 vose   wright         however  no general theoretical tools are available that can be
applied to realistic rl problems  in any case  ultimate convergence to an optimal policy
may be less important in practice than eciently finding a reasonable approximation 
a more pragmatic approach may be to ask how ecient alternative rl algorithms are 
in terms of the number of reinforcements received before developing a policy that is within
some tolerance level of an optimal policy  in the model of probably approximately correct
 pac  learning  valiant         the performance of a learner is measured by how many
learning experiences  e g   samples in supervised learning  are required before converging
to a correct hypothesis within specified error bounds  although developed initially for
supervised learning  the pac approach has been extended recently to both td methods
 fiechter        and to general ea methods  ros         these analytic methods are
still in an early stage of development  but further research along these lines may one day
provide useful tools for understanding the theoretical and practical advantages of alternative
approaches to rl  until that time  experimental studies will provide valuable evidence for
the utility of an approach 

    examples of earl methods

finally  we take a look at a few significant examples of the earl approach and results
on rl problems  rather than attempt an exhaustive survey  we have selected four earl
systems that are representative of the diverse policies representations outlined in section   
samuel represents the class of single chromosome rule based earl systems  alecsys is
an example of a distributed rule based earl method  genitor is a single chromosome
neural net system  and sane is a distributed neural net system  this brief survey should
   

fievolutionary algorithms for reinforcement learning

provide a starting point for those interested in investigating the evolutionary approach to
reinforcement learning 

    

samuel
samuel  grefenstette et al         is an earl system that combines darwinian and lamarckian evolution with aspects of temporal difference reinforcement learning  samuel has

been used to learn behaviors such as navigation and collision avoidance  tracking  and herding  for robots and other autonomous vehicles 
samuel uses a single chromosome  rule based representation for policies  that is  each
member of the population is a policy represented as a rule set and each gene is a rule that
maps the state of the world to actions to be performed  an example rule might be 
if range            and bearing           then set turn       strength
    
the use of a high level language for rules offers several advantages over low level binary
pattern languages typically adopted in genetic learning systems  first  it makes it easier to
incorporate existing knowledge  whether acquired from experts or by symbolic learning programs  second  it is easier to transfer the knowledge learned to human operators  samuel
also includes mechanisms to allow coevolution of multiple behaviors simultaneously  in
addition to the usual genetic operators of crossover and mutation  samuel uses more traditional machine learning techniques in the form of lamarckian operators  samuel keeps a
record of recent experiences and will allow operators such as generalization  specialization 
covering  and deletion to make informed changes to the individual genes  rules  based on
these experiences 
samuel has been used successfully in many reinforcement learning applications  here
we will briey describe three examples of learning complex behaviors for real robots  in
these applications of samuel  learning is performed under simulation  reecting the fact
that during the initial phases of learning  controlling a real system can be expensive or
dangerous  learned behaviors are then tested on the on line system 
in  schultz   grefenstette        schultz        schultz   grefenstette         samuel
is used to learn collision avoidance and local navigation behaviors for a nomad     mobile
robot  the sensors available to the learning task were five sonars  five infrared sensors 
and the range and bearing to the goal  and the current speed of the vehicle  samuel
learned a mapping from those sensors to the controllable actions   a turning rate and a
translation rate for the wheels  samuel took a human written rule set that could reach
the goal within a limited time without hitting an obstacle only    percent of the time  and
after    generations was able to obtain a      percent success rate 
in  schultz   grefenstette         the robot learned to herd a second robot to a  pasture   in this task  the learning system used the range and bearing to the second robot  the
heading of the second robot  and the range and bearing to the goal  as its input sensors 
the system learned a mapping from these sensors to a turning rate and steering rate  in
these experiments  success was measured as the percentage of times that the robot could
maneuver the second robot to the goal within a limited amount of time  the second robot
implemented a random walk  plus a behavior that made it avoid any nearby obstacles  the
first robot learned to exploit this to achieve its goal of moving the second robot to the goal 
   

fimoriarty  schultz    grefenstette

samuel was given an initial  human designed rule set with a performance of    percent 
and after     generations was able to move the second robot to the goal    percent of the
time 
in  grefenstette        the samuel ea system is combined with case based learning to
address the adaptation problem  in this approach  called anytime learning  grefenstette  
ramsey         the learning agent interacts both with the external environment and with
an internal simulation  the anytime learning approach involves two continuously running
and interacting modules  an execution module and a learning module  the execution
module controls the agent s interaction with the environment and includes a monitor that
dynamically modifies the internal simulation model based on observations of the actual agent
and the environment  the learning module continuously tests new strategies for the agent
against the simulation model  using a genetic algorithm to evolve improved strategies  and
updates the knowledge base used by the execution module with the best available results 
whenever the simulation model is modified due to some observed change in the agent or the
environment  the genetic algorithm is restarted on the modified model  the learning system
operates indefinitely  and the execution system uses the results of learning as they become
available  the work with samuel shows that the ea method is particularly well suited
for anytime learning  previously learned strategies can be treated as cases  indexed by the
set of conditions under which they were learned  when a new situation is encountered  a
nearest neighbor algorithm is used to find the most similar previously learned cases  these
nearest neighbors are used to re initialize the genetic population of policies for the new case 
grefenstette        reports on experiments in which a mobile robot learns to track another
robot  and dynamically adapts its policies using anytime learning as its encounters a series
of partial system failures  this approach blurs the line between online and oine learning 
since the online system is being updated whenever the oine learning system develops an
improved policy  in fact  the oine learning system can even be executed on board the
operating mobile robot 

    

alecsys

as described previously  alecsys  dorigo   colombetti        is a distributed rule based
ea that supports an approach to the design of autonomous systems called behavioral engineering  in this approach  the tasks to be performed by a complex autonomous systems are
decomposed into individual behaviors  each of which is learned via a learning classifier systems module  as shown in figure    the decomposition is performed by the human designer 
so the fitness function associated with each lcs can be carefully designed to reect the role
of the associated component behavior within the overall autonomous system  furthermore 
the interactions among the modules is also preprogrammed  for example  the designer may
decide that the robot should learn to approach a goal except when a threatening predator
is near  in which case the robot should evade the predator  the overall architecture of the
set of behaviors can then be set such that the evasion behavior has higher priority than
the goal seeking behavior  but the individual lcs modules can evolve decision rules for
optimally performing the subtasks 
alecsys has been used to develop behavioral rules for a number of behaviors for
autonomous robots  including complex behavior groups such as chase feed escape
   

fievolutionary algorithms for reinforcement learning

 dorigo   colombetti         the approach has been implemented and tested on both
simulated robots and on real robots  because it exploits both human design and earl
methods to optimize system performance  this method shows much promise for scaling up
to realistic tasks 

    

genitor

genitor  whitley   kauth        whitley        is an aggressive  general purpose genetic

algorithm that has been shown effective when specialized for use on reinforcement learning
problems  whitley et al         demonstrated how genitor can eciently evolve decision
policies represented as neural networks using only limited reinforcement from the domain 
genitor relies solely on its evolutionary algorithm to adjust the weights in neural
networks  in solving rl problems  each member of the population in genitor represents a
neural network as a sequence of connection weights  the weights are concatenated in a realvalued chromosome along with a gene that represents a crossover probability  the crossover
gene determines whether the network is to be mutated  randomly perturbed  or whether a
crossover operation  recombination with another network  is to be performed  the crossover
gene is modified and passed to the offspring based on the offspring s performance compared
to the parent  if the offspring outperforms the parent  the crossover probability is decreased 
otherwise  it is increased  whitley et al  refer to this technique as adaptive mutation 
which tends to increase the mutation rate as populations converge  essentially  this method
promotes diversity within the population to encourage continual exploration of the solution
space 
genitor also uses a so called  steady state  genetic algorithm in which new parents are
selected and genetic operators are applied after each individual is evaluated  this approach
contrasts with  generational  gas in which the entire population is evaluated and replaced
during each generation  in a steady state ga  each policy is evaluated just once and retains
this same fitness value indefinitely  since policies with lower fitness are more likely to be
replaced  it is possible that a fitness based on a noisy evaluation function may have an
undesirable inuence on the direction of the search  in the case of the pole balancing rl
application  the fitness value depends on the length of time that the policy can maintain
a good balance  given a randomly chosen initial state  the fitness is therefore a random
variable that depends on the initial state  the authors believe that noise in the fitness
function had little negative impact on learning good policies  perhaps because it was more
dicult for poor networks to obtain a good fitness than for good networks  of which there
were many copies in the population  to survive an occasional bad fitness evaluation  this
is an interesting general issue in earl that needs further analysis 
genitor adopts some specific modification for its rl applications  first  the representation uses a real valued chromosome rather than a bit string representation for the weights 
consequently  genitor always recombines policies between weight definitions  thus reducing potentially random disruption of neural network weights that might result if crossover
operations occurred in the middle of a weight definition  the second modification is a very
high mutation rate which helps to maintain diversity and promote rapid exploration of the
policy space  finally  genitor uses unusually small populations in order to discourage
different  competing neural network  species  from forming within the population  whit   

fimoriarty  schultz    grefenstette

ley et al         argue that speciation leads to competing conventions and produces poor
offspring when two dissimilar networks are recombined 
whitley et al         compare genitor to the adaptive heuristic critic  anderson 
      ahc   which uses the td method of reinforcement learning  in several different
versions of the common pole balancing benchmark task  genitor was found to be comparable to the ahc in both learning rate and generalization  one interesting difference
whitley et al  found was that genitor was more consistent than the ahc in solving the
pole balancing problem when the failure signals occurs at wider pole bounds  make the
problem much harder   for ahc  the preponderance of failures appears to cause all states
to overpredict failure  in contrast  the ea method appears more effective in finding policies
that obtain better overall performance  even if success is uncommon  the difference seems
to be that the ea tends to ignore those cases where the pole cannot be balanced  and concentrate on successful cases  this serves as another example of the advantages associated
with search in policy space  based on overall policy performance  rather than paying too
much attention to the value associated with individual states 

    

sane

the sane  symbiotic  adaptive neuro evolution  system was designed as a ecient method
for building artificial neural networks in rl domains where it is not possible to generate
training data for normal supervised learning  moriarty   miikkulainen      a         the
sane system uses an evolutionary algorithm to form the hidden layer connections and
weights in a neural network  the neural network forms a direct mapping from sensors to
actions and provides effective generalization over the state space  sane s only method of
credit assignment is through the ea  which allows it to apply to many problems where
reinforcement is sparse and covers a sequence of decisions  as described previously  sane
uses a distributed representation for policies 
sane offers two important advantages for reinforcement learning that are normally not
present in other implementations of neuro evolution  first  it maintains diverse populations 
unlike the canonical function optimization ea that converge the population on a single solution  sane forms solutions in an unconverged population  because several different types
of neurons are necessary to build an effective neural network  there is inherent evolutionary
pressure to develop neurons that perform different functions and thus maintain several different types of individuals within the population  diversity allows recombination operators
such as crossover to continue to generate new neural structures even in prolonged evolution 
this feature helps ensure that the solution space will be explored eciently throughout the
learning process  sane is therefore more resilient to suboptimal convergence and more
adaptive to changes in the domain 
the second feature of sane is that it explicitly decomposes the search for complete solutions into a search for partial solutions  instead of searching for complete neural networks
all at once  solutions to smaller problems  good neurons  are evolved  which can be combined to form an effective full solution  a neural network   in other words  sane effectively
performs a problem reduction search on the space of neural networks 
sane has been shown effective in several different large scale problems  in one problem 
sane evolved neural networks to direct or focus a minimax game tree search  moriarty
   

fievolutionary algorithms for reinforcement learning

  miikkulainen         by selecting which moves should be evaluated from a given game
situation  sane guides the search away from misinformation in the search tree and towards
the most effective moves  sane was tested in a game tree search in othello using the
evaluation function from the former world champion program bill  lee   mahajan        
tested against a full width minimax search  sane significantly improved the play of bill 
while examining only a subset of the board positions 
in a second application  sane was used to learn obstacle avoidance behaviors in a
robot arm  moriarty   miikkulainen      b   most approaches for learning robot arm
control learn hand eye coordination through supervised training methods where examples
of correct behavior are explicitly given  unfortunately in domains with obstacles where the
arm must make several intermediate joint rotations before reaching the target  generating
training examples is extremely dicult  a reinforcement learning approach  however  does
not require examples of correct behavior and can learn the intermediate movements from
general reinforcements  sane was implemented to form neuro control networks capable of
maneuvering the oscar   robot arm among obstacles to reach random target locations 
given both camera based visual and infrared sensory input  the neural networks learned to
effectively combine both target reaching and obstacle avoidance strategies 
for further related examples of evolutionary methods for learning neural net control
systems for robotics  the reader should see  cliff  harvey    husbands        husbands 
harvey    cliff        yamauchi   beer        

    summary
this article began by suggesting two distinct approaches to solving reinforcement learning
problems  one can search in value function space or one can search in policy space  td
and earl are examples of these two complementary approaches  both approaches assume
limited knowledge of the underlying system and learn by experimenting with different policies and using reinforcement to alter those policies  neither approach requires a precise
mathematical model of the domain  and both may learn through direct interactions with
the operational environment 
unlike td methods  earl methods generally base fitness on the overall performance
of a policy  in this sense  ea methods pay less attention to individual decisions than td
methods do  while at first glance  this approach appears to make less ecient use of
information  it may in fact provide a robust path toward learning good policies  especially
in situations where the sensors are inadequate to observe the true state of the world 
it is not useful to view the path toward practical rl systems as a choice between ea
and td methods  we have tried to highlight some of the strengths of the evolutionary
approach  but we have also shown that earl and td  while complementary approaches 
are by no means mutually exclusive  we have cited examples of successful earl systems
such as samuel and alecsys that explicitly incorporate td elements into their multilevel credit assignment methods  it is likely that many practical applications will depend
on these kinds of multi strategy approaches to machine learning 
we have also listed a number of areas that need further work  particularly on the theoretical side  in rl  it would be highly desirable to have a better tools for predicting the
amount of experience needed by a learning agent before reaching a specified level of per   

fimoriarty  schultz    grefenstette

formance  the existing proofs of optimality for both q learning and ea are of extremely
limited practical use in predicting how well either approach will perform on realistic problems  preliminary results have shown that the tools of pac analysis can be applied to both
ea an td methods  but much more effort is needed in this direction 
many serious challenges remain in scaling up reinforcement learning methods to realistic applications  by pointing out the shared goals and concerns of two complementary
approaches  we hope to motivate further collaboration and progress in this field 

references

anderson  c  w          learning to control an inverted pendulum using neural networks 
ieee control systems magazine           
barto  a  g   sutton  r  s     watkins  c  j  c  h          learning and sequential
decision making  in gabriel  m     moore  j  w   eds    learning and computational
neuroscience  mit press  cambridge  ma 
belew  r  k   mcinerney  j     schraudolph  n  n          evolving networks  using
the genetic algorithm with connectionist learning  in farmer  j  d   langton  c  
rasmussen  s     taylor  c   eds    artificial life ii reading  ma  addison wesley 
chrisman  l          reinforcement learning with perceptual aliasing  the perceptual
distinctions approach  in proceedings of the tenth national conference on artificial
intelligence  pp          san jose  ca 
cliff  d   harvey  i     husbands  p          explorations in evolutionary robotics  adaptive
behavior            
cobb  h  g     grefenstette  j  j          genetic algorithms for tracking changing environments  in proc  fifth international conference on genetic algorithms  pp          
collins  r  j     jefferson  d  r          selection in massively parallel genetic algorithms 
in proceedings of the fourth international conference on genetic algorithms  pp 
        san mateo  ca  morgan kaufmann 
dayan  p     sejnowski  t  j          exploration bonuses and dual control  machine
learning               
de jong  k  a          an analysis of the behavior of a class of genetic adaptive systems 
ph d  thesis  the university of michigan  ann arbor  mi 
dorigo  m     colombetti  m          robot shaping  an experiment in behavioral engineering  mit press  cambridge  ma 
fiechter  c  n          ecient reinforcement learning  in proceedings of the seventh
annual acm conference on computational learning theory  pp         association
for computing machinery 
fogel  l  j   owens  a  j     walsh  m  j          artificial intelligence through simulated
evolution  wiley publishing  new york 
   

fievolutionary algorithms for reinforcement learning

goldberg  d  e          genetic algorithms in search  optimization  and machine learning  addison wesley  reading  ma 
goldberg  d  e     richardson  j          genetic algorithms with sharing for multimodal
function optimization  in proceedings of the second international conference on genetic algorithms  pp          san mateo  ca  morgan kaufmann 
grefenstette  j  j          optimization of control parameters for genetic algorithms  ieee
transactions on systems  man   cybernetics  smc                 
grefenstette  j  j          incorporating problem specific knowledge into genetic algorithms 
in davis  l   ed    genetic algorithms and simulated annealing  pp        san mateo 
ca  morgan kaufmann 
grefenstette  j  j          credit assignment in rule discovery system based on genetic
algorithms  machine learning                   
grefenstette  j  j          genetic algorithms for changing environments  in manner  r  
  manderick  b   eds    parallel problem solving from nature     pp          
grefenstette  j  j          robot learning with parallel genetic algorithms on networked
computers  in proceedings of the      summer computer simulation conference
 scsc       pp          
grefenstette  j  j          genetic learning for adaptation in autonomous robots  in robotics
and manufacturing  recent trends in research and applications  volume    pp      
     asme press  new york 
grefenstette  j  j       a   proportional selection and sampling algorithms  in handbook of
evolutionary computation  chap  c     iop publishing and oxford university press 
grefenstette  j  j       b   rank based selection  in handbook of evolutionary computation  chap  c     iop publishing and oxford university press 
grefenstette  j  j     ramsey  c  l          an approach to anytime learning  in proc 
ninth international conference on machine learning  pp          san mateo  ca 
morgan kaufmann 
grefenstette  j  j   ramsey  c  l     schultz  a  c          learning sequential decision
rules using simulation models and competition  machine learning             
holland  j  h          adaptation in natural and artificial systems  an introductory
analysis with applications to biology  control and artificial intelligence  university
of michigan press  ann arbor  mi 
holland  j  h          escaping brittleness  the possibilities of general purpose learning
algorithms applied to parallel rule based systems  in machine learning  an artificial
intelligence approach  vol     morgan kaufmann  los altos  ca 
   

fimoriarty  schultz    grefenstette

holland  j  h          genetic algorithms and classifier systems  foundations and future
directions  in proceedings of the second international conference on genetic algorithms  pp        hillsdale  new jersey 
holland  j  h     reitman  j  s          cognitive systems based on adaptive algorithms 
in pattern directed inference systems  academic press  new york 
husbands  p   harvey  i     cliff  d          circle in the round  state space attractors for
evolved sighted robots  robot  autonomous systems             
kaelbling  l  p   littman  m  l     moore  a  w          reinforcement learning  a survey 
journal of artificial intelligence research             
koza  j  r          genetic programming  on the programming of computers by means
of natural selection  mit press  cambridge  ma 
lee  k  f     mahajan  s          the development of a world class othello program 
artificial intelligence            
lin  l  j     mitchell  t  m          memory approaches to reinforcement learning in nonmarkovian domains  tech  rep  cmu cs         carnegie mellon university  school
of computer science 
mccallum  a  k          reinforcement learning with selective perception and hidden
state  ph d  thesis  the university of rochester 
moriarty  d  e     miikkulainen  r          evolving neural networks to focus minimax
search  in proceedings of the twelfth national conference on artificial intelligence
 aaai      pp            seattle  wa  mit press 
moriarty  d  e     miikkulainen  r       a   ecient reinforcement learning through
symbiotic evolution  machine learning            
moriarty  d  e     miikkulainen  r       b   evolving obstacle avoidance behavior in a
robot arm  in from animals to animats  proceedings of the fourth international
conference on simulation of adaptive behavior  sab      pp          cape cod 
ma 
moriarty  d  e     miikkulainen  r          forming neural networks through ecient and
adaptive co evolution  evolutionary computation                 
potter  m  a          the design and analysis of a computational model of cooperative
coevolution  ph d  thesis  george mason university 
potter  m  a     de jong  k  a          evolving neural networks with collaborative
species  in proceedings of the      summer computer simulation conference ottawa 
canada 
potter  m  a   de jong  k  a     grefenstette  j          a coevolutionary approach to
learning sequential decision rules  in eshelman  l   ed    proceedings of the sixth
international conference on genetic algorithms pittsburgh  pa 
   

fievolutionary algorithms for reinforcement learning

rechenberg  i          cybernetic solution path of an experimental problem  in library
translation       royal aircraft establishment  farnborough  hants  aug       
ring  m  b          continual learning in reinforcement environments  ph d  thesis  the
university of texas at austin 
ros  j  p          probably approximately correct  pac  learning analysis  in handbook of
evolutionary computation  chap  b     iop publishing and oxford university press 
schaffer  j  d   caruana  r  a   eshelman  l  j     das  r          a study of control
parameters affecting online performance of genetic algorithms for function optimization  in proceedings of the third international conference on genetic algorithms 
pp         morgan kaufmann 
schaffer  j  d     grefenstette  j  j          multi objective learning via genetic algorithms 
in proceedings of the ninth international joint conference on artificial intelligence 
pp           morgan kaufmann 
schultz  a  c          learning robot behaviors using genetic algorithms  in intelligent
automation and soft computing  trends in research  development  and applications 
pp           tsi press  albuquerque 
schultz  a  c     grefenstette  j  j          using a genetic algorithm to learn behaviors for
autonomous vehicles  in proceedings of the aiaa guidance  navigation  and control
conference hilton head  sc 
schultz  a  c     grefenstette  j  j          robo shepherd  learning complex robotic behaviors  in robotics and manufacturing  recent trends in research and applications 
volume    pp           asme press  new york 
smith  s  f          flexible learning of problem solving heuristics through adaptive search 
in proceedings of the eighth international joint conference on artificial intelligence 
pp           morgan kaufmann 
sutton  r          integrated architectures for learning  planning  and reacting based on
approximate dynamic programming  in machine learning  proceedings of the seventh
international conference  pp          
sutton  r  s          learning to predict by the methods of temporal differences  machine
learning          
sutton  r  s     barto  a          reinforcement learning  an introduction  mit press 
cambridge  ma 
valiant  l  g          a theory of the learnable  communications of the acm           
     
vose  m  d     wright  a  h          simple genetic algorithms with linear fitness  evolutionary computation             
   

fimoriarty  schultz    grefenstette

watkins  c  j  c  h          learning from delayed rewards  ph d  thesis  university of
cambridge  england 
watkins  c  j  c  h     dayan  p          q learning  machine learning                 
whitley  d          the genitor algorithm and selective pressure  in proceedings of the
third international conference on genetic algorithms  pp          san mateo  ca 
morgan kaufman 
whitley  d     kauth  j          genitor  a different genetic algorithm  in proceedings
of the rocky mountain conference on artificial intelligence  pp          denver  co 
whitley  d   dominic  s   das  r     anderson  c  w          genetic reinforcement
learning for neurocontrol problems  machine learning              
wilson  s  w          zcs  a zeroth level classifier system  evolutionary computation 
            
yamauchi  b  m     beer  r  d          sequential behavior and learning in evolved
dynamical neural networks  adaptive behavior             
zhou  h          csm  a computational model of cumulative learning  machine learning 
               

   

fi