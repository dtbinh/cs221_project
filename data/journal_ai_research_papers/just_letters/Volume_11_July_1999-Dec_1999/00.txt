journal of artificial intelligence research               

submitted        published      

decision theoretic planning  structural assumptions and
computational leverage
craig boutilier

cebly cs ubc ca

department of computer science  university of british columbia
vancouver  bc  v t  z   canada

thomas dean

tld cs brown edu

department of computer science  brown university
box       providence  ri         usa

steve hanks

hanks cs washington edu

department of computer science and engineering  university of washington
seattle  wa         usa

abstract

planning under uncertainty is a central problem in the study of automated sequential
decision making  and has been addressed by researchers in many different fields  including
ai planning  decision analysis  operations research  control theory and economics  while
the assumptions and perspectives adopted in these areas often differ in substantial ways 
many planning problems of interest to researchers in these fields can be modeled as markov
decision processes  mdps  and analyzed using the techniques of decision theory 
this paper presents an overview and synthesis of mdp related methods  showing how
they provide a unifying framework for modeling many classes of planning problems studied
in ai  it also describes structural properties of mdps that  when exhibited by particular classes of problems  can be exploited in the construction of optimal or approximately
optimal policies or plans  planning problems commonly possess structure in the reward
and value functions used to describe performance criteria  in the functions used to describe
state transitions and observations  and in the relationships among features used to describe
states  actions  rewards  and observations 
specialized representations  and algorithms employing these representations  can achieve
computational leverage by exploiting these various forms of structure  certain ai techniques 
in particular those based on the use of structured  intensional representations can be
viewed in this way  this paper surveys several types of representations for both classical
and decision theoretic planning problems  and planning algorithms that exploit these representations in a number of different ways to ease the computational burden of constructing
policies or plans  it focuses primarily on abstraction  aggregation and decomposition techniques based on ai style representations 

   introduction

planning using decision theoretic notions to represent domain uncertainty and plan quality
has recently drawn considerable attention in artificial intelligence  ai    decision theoretic
planning  dtp  is an attractive extension of the classical ai planning paradigm because it
allows one to model problems in which actions have uncertain effects  the decision maker has
   see  for example  the recent texts  dean  allen    aloimonos        dean   wellman        russell  
norvig        and the research reported in  hanks  russell    wellman        

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fiboutilier  dean    hanks

incomplete information about the world  where factors such as resource consumption lead to
solutions of varying quality  and where there may not be an absolute or well defined  goal 
state  roughly  the aim of dtp is to form courses of action  plans or policies  that have
high expected utility rather than plans that are guaranteed to achieve certain goals  when
ai planning is viewed as a particular approach to solving sequential decision problems of
this type  the connections between dtp and models used in other fields of research such
as decision analysis  economics and operations research  or  become more apparent  at
a conceptual level  most sequential decision problems can be viewed as instances of markov
decision processes  mdps   and we will use the mdp framework to make the connections
explicit 
much recent research on dtp has explicitly adopted the mdp framework as an underlying model  barto  bradtke    singh        boutilier   dearden        boutilier  dearden 
  goldszmidt        dean  kaelbling  kirman    nicholson        koenig        simmons
  koenig        tash   russell         allowing the adaptation of existing results and algorithms for solving mdps  e g   from the field of or  to be applied to planning problems  in
doing so  however  this work has departed from the traditional definition of the  planning
problem  in the ai planning community one goal of this paper is to make explicit the
connection between these two lines of work 
adopting the mdp framework as a model for posing and solving planning problems
has illuminated a number of interesting connections among techniques for solving decision
problems  drawing on work from ai planning  reasoning under uncertainty  decision analysis
and or  one of the most interesting insights to emerge from this body of work is that many
dtp problems exhibit considerable structure  and thus can be solved using special purpose
methods that recognize and exploit that structure  in particular  the use of feature based
representations to describe problems  as is the typical practice in ai  often highlights the
problem s special structure and allows it to be exploited computationally with little effort 
there are two general impediments to the more widespread acceptance of mdps within
ai as a general model of planning  the first is the absence of explanations of the mdp model
that make the connections to current planning research explicit  at either the conceptual
or computational level  this may be due in large part to the fact that mdps have been
developed and studied primarily in or  where the dominant concerns are  naturally  rather
different  one aim of this paper is to make the connections clear  we provide a brief
description of mdps as a conceptual model for planning that emphasizes the connection
to ai planning  and explore the relationship between mdp solution algorithms and ai
planning algorithms  in particular  we emphasize that most ai planning models can be
viewed as special cases of mdps  and that classical planning algorithms have been designed
to exploit the problem characteristics associated with these cases 
the second impediment is skepticism among ai researchers regarding the computational
adequacy of mdps as a planning model  can the techniques scale to solve planning problems
of reasonable size  one diculty with solution techniques for mdps is the tendency to rely
on explicit  state based problem formulations  this can be problematic in ai planning
since state spaces grow exponentially with the number of problem features  state space size
and dimensionality are of somewhat lesser concern in or and decision analysis  in these
fields  an operations researcher or decision analyst will often hand craft a model that ignores
certain problem features deemed irrelevant  or will define other features that summarize a
 

fidecision theoretic planning  structural assumptions

wide class of problem states  in ai  the emphasis is on the automatic solution of problems
posed by users who lack the expertise of a decision analyst  thus  assuming a well crafted 
compact state space is often not appropriate 
in this paper we show how specialized representations and algorithms from ai planning
and problem solving can be used to design ecient mdp solution techniques  in particular 
ai planning methods assume a certain structure in the state space  in the actions  or
operators   and in the specification of a goal or other success criteria  representations and
algorithms have been designed that make the problem structure explicit and exploit that
structure to solve problems effectively  we demonstrate how this same process of identifying
structure  making it explicit  and exploiting it algorithmically can be brought to bear in
the solution of mdps 
this paper has several objectives  first  it provides an overview of dtp and mdps
suitable for readers familiar with traditional ai planning methods and makes connections
with this work  second  it describes the types of structure that can be exploited and how
ai representations and methods facilitate computationally effective planning with mdps 
as such  it is a suitable introduction to ai methods for those familiar with the classical
presentation of mdps  finally  it surveys recent work on the use of mdps in ai and
suggests directions for further research in this regard  and should therefore be of interest to
researchers in dtp 

    general problem definition
roughly speaking  the class of problems we consider are those involving systems whose
dynamics can be modeled as stochastic processes  where the actions of decision maker 
referred to here as the agent   can inuence the system s behavior  the system s current
state and the choice of action jointly determine a probability distribution over the system s
possible next states  the agent prefers to be in certain system states  e g   goal states  over
others  and therefore must determine a course of action also called a  plan  or  policy  in
this paper that is likely to lead to these target states  possibly avoiding undesirable states
along the way  the agent may not know the system s state exactly in making its decision
on how to act  however it may have to rely on incomplete and noisy sensors and be forced
to base its choice of action on a probabilistic estimate of the state 
to help illustrate the types of problems in which we are interested  consider the following
example  imagine that we have a robot agent designed to help someone  the  user   in an
oce environment  see figure     there are three activities it might undertake  picking up
the user s mail  getting coffee  or tidying up the user s research lab  the robot can move
from location to location and perform various actions that tend to achieve certain target
states  e g   bringing coffee to the user on demand  or maintaining a minimal level of tidiness
in the lab  
we might associate a certain level of uncertainty with the effects of the robot s actions
 e g   when it tries to move to an adjacent location it might succeed     of the time and fail
to move at all the other     of the time   the robot might have incomplete access to the
true state of the system in that its sensors might supply it with incomplete information  it
cannot tell whether mail is available for pickup if it is not in the mail room  and incorrect
 

fiboutilier  dean    hanks

my office

hallway

lab
mailroom

coffee

figure    a decision theoretic planning problem
information  even when in the mail room its sensors occasionally fail to detect the presence
of mail  
finally  the performance of the robot might be measured in various ways  do its actions
guarantee that a goal will be achieved  do they maximize some objective function defined
over possible effects of its actions  do they achieve a goal state with sucient probability while avoiding  disastrous  states with near certainty  the stipulation of optimal or
acceptable behavior is an important part of the problem specification 
the types of problems that can be captured using this general framework include classical  goal oriented  deterministic  complete knowledge  planning problems and extensions
such as conditional and probabilistic planning problems  as well as other more general
problem formulations 
the discussion to this point has assumed an extensional representation of the system s
states one in which each state is explicitly named  in ai research  intensional representations are more common  an intensional representation is one in which states or sets of
states are described using sets of multi valued features  the choice of an appropriate set
of features is an important part of the problem design  these features might include the
current location of the robot  the presence or absence of mail  and so on  the performance
metric is also typically expressed intensionally  figure   serves as a reference for our example problem  which we use throughout the paper  it lists the basic features used to describe
the states of the system  the actions available to the robot and the exogenous events that
might occur  together with an intuitive description of the features  actions and events 
the remainder of the paper is organized as follows  in section    we present the mdp
framework in the abstract  introducing basic concepts and terminology and noting the relationship between this abstract model and the classical ai planning problem  section   surveys common solution techniques algorithms based on dynamic programming for general
mdp problems and search algorithms for planning problems and points out the relationship between problem assumptions and solution techniques  section   turns from algorithms
to representations  showing various ways in which the structured representations commonly
used by ai algorithms can be used to represent mdps compactly as well  section   surveys
 

fidecision theoretic planning  structural assumptions

features
location

denoted
description
loc m    etc  location of robot  five possible locations  mailroom  m   coffee room
 c   user s oce  o   hallway  h   laboratory  l 
tidiness
t      etc 
degree of lab tidiness  five possible values  from    messiest  to  
 tidiest 
mail present
m  m
is there mail is user s mail box  true  m   or false  m  
robot has mail
rhm  rhm does the robot have mail in its possession 
coffee request
cr  cr
is there an outstanding  unfulfilled  request for coffee by the user 
robot has coffee rhc  rhc does the robot have coffee in its possession 
actions
denoted
description
move clockwise
clk
move to adjacent location  clockwise direction 
counterclockwise cclk
move to adjacent location  counterclockwise direction 
tidy lab
tidy
if the robot is in the lab  the degree of tidiness is increased by  
pickup mail
pum
if the robot is in the mailroom and there is mail present  the robot
takes the mail  rhm becomes true and m becomes false 
get coffee
getc
if the robot is in the coffee room  it gets coffee  rhc becomes true 
deliver mail
delm
if the robot is in the oce and has mail  it hands the mail to the user
 rhm becomes false 
deliver coffee
delc
if the robot is in the oce and has coffee  it hands the coffee to the
user  rhc and cr both become false 
events
denoted
description
mail arrival
arrm
mail arrives causing m to become true
request coffee
reqc
user issues coffee request causing cr to become true
untidy the lab
mess
the lab becomes messier  one degree less tidy 

figure    elements of the robot domain 
some recent work on abstraction  aggregation and problem decomposition methods  and
shows the connection to more traditional ai methods such as goal regression  this last
section demonstrates that representational and computational methods from ai planning
can be used in the solution of general mdps  section   also points out additional ways in
which this type of computational leverage might be developed in the future 

   markov decision processes  basic problem formulation
in this section we introduce the mdp framework and make explicit the relationship between
this model and classical ai planning models  we are interested in controlling a stochastic
dynamical system  a system that at any point in time can be in one of a number of distinct
states  and in which the system s state changes over time in response to events  an action
is a particular kind of event instigated by an agent in order to change the system s state 
we assume that the agent has control over what actions are taken and when  though the
effects of taking an action might not be perfectly predictable  in contrast  exogenous events
are not under the agent s control  and their occurrence may be only partially predictable 
this abstract view of an agent is consistent both with the  ai  view where the agent is an
autonomous decision maker and the  control  view where a policy is determined ahead of
time  programmed into a device  and executed without further deliberation 
 

fiboutilier  dean    hanks

    states and state transitions

we define a state to be a description of the system at a particular point in time  how one
defines states can vary with particular applications  some notions being more natural than
others  however  it is common to assume that the state captures all information relevant
to the agent s decision making process  we assume a finite state space s   fs            sn g
of possible system states   in most cases the agent will not have complete information
about the current state  this uncertainty or incomplete information can be captured using
a probability distribution over the states in s  
a discrete time stochastic dynamical system consists of a state space and probability
distributions governing possible state transitions how the next state of the system depends
on past states  these distributions constitute a model of how the system evolves over time
in response to actions and exogenous events  reecting the fact that the effects of actions
and events may not be perfectly predictable even if the prevailing state is known 
although we are generally concerned with how the agent chooses an appropriate course
of action  for the remainder of this section we assume that the agent s course of action is
fixed  concentrating on the problem of predicting the system s state after the occurrence of
a predetermined sequence of actions  we discuss the action selection problem in the next
section 
we assume the system evolves in stages  where the occurrence of an event marks the
transition from one stage t to the next stage t      since events define changes in stage 
and since events often  but not necessarily  cause state transitions  we often equate stage
transitions with state transitions  of course  it is possible for an event to occur but leave
the system in the same state 
the system s progression through stages is roughly analogous to the passage of time 
the two are identical if we assume that some action  possibly a no op  is taken at each
stage  and that every action takes unit time to complete  we can thus speak loosely as if
stages correspond to units of time  and we refer to t interchangeably as the set of all stages
and the set of all time points  
we can model uncertainty by regarding the system s state at some stage t as a random
variable s t that takes values from s   an assumption of  forward causality  requires that the
variable s t does not depend directly on the value of future variable s k  k   t   roughly 
it requires that we model our system such that the past history  directly  determines
the distribution over current states  whereas knowledge of future states can inuence the
estimate of the current state only indirectly by providing evidence on what the current state
may have been so as to lead to these future states  figure   a  shows a graphical perspective
on a discrete time  stochastic dynamical system  the nodes are random variables denoting
the state at a particular time  and the arcs indicate the direct probabilistic dependence
of states on previous states  to describe this system completely we must also supply the
conditional distributions pr s t js     s        s t     for all times t 
states should be thought of as descriptions of the system being modeled  so the question arises of how much detail about the system is captured in a state description  more
   most of the discussion in this paper also applies to cases where the state space is countably infinite  see
 puterman        for a discussion of infinite and continuous state problems 
   while we do not deal with such topics here  there is a considerable literature in the or community on
continuous time markov decision processes  puterman        

 

fidecision theoretic planning  structural assumptions

 a 

s

s

s

 b 

s

s

s

 c 

 

 

 

 

s

 

s

 

s

t  

t  

s

t

s

t

s

t  

t

figure    a general stochastic process  a   a markov chain  b   and a stationary markov
chain  c  
detail implies more information about the system  which in turn often translates into better
predictions of future behavior  of course  more detail also implies a larger set s   which can
increase the computational cost of decision making 
it is commonly assumed that a state contains enough information to predict the next
state  in other words  any information about the history of the system relevant to predicting
its future is captured explicitly in the state itself  formally  this assumption  the markov
assumption  says that knowledge of the present state renders information about the past
irrelevant to making predictions about the future 
pr s t   js t   s t             s       pr s t   js t  
markovian models can be represented graphically using a structure like that in figure   b  
reecting the fact that the present state is sucient to predict future state evolution  
finally  it is common to assume that the effects of an event depend only on the prevailing
state  and not the stage or time at which the event occurs   if the distribution predicting
the next state is the same regardless of stage  the model is said to be stationary and can
be represented schematically using just two stages  as in figure   c   in this case only a
single conditional distribution is required  in this paper we generally restrict our attention
to discrete time  finite state  stochastic dynamical systems with the markov property  commonly called markov chains  furthermore  most of our discussion is restricted to stationary
chains 
to complete the model we must provide a probability distribution over initial states 
reecting the probability of being in any state at stage    this distribution can be repre   it is worth mentioning that the markov property applies to the particular model and not to the system
itself  indeed  any non markovian model of a system  of finite order  i e   whose dynamics depend on at
most the k previous states for some k  can be converted to an equivalent though larger markov model 
in control theory  this is called conversion to state form  luenberger        
   of course  this is also a statement about model detail  saying that the state carries enough information
to make the stage irrelevant to predicting transitions 

 

fiboutilier  dean    hanks

  

 

  

  
  

  

 
  

 

  

 

   

  

 

  

 

  

 
   

  

figure    a state transition diagram 
sented as a real valued  row  vector of size n   js j  one entry for each state   we denote
this vector p   and use p i to denote its ith entry  that is  the probability of starting in state
si  
we can represent a t  stage nonstationary markov chain with t transition matrices 
each of size n  n   where matrix p t captures the transition probabilities governing the
system as it moves from stage t to stage t      each matrix consists of probabilities ptij  
where ptij   pr s t     sj js t   si    if the process is stationary  the transition matrix is
the same at all stages and one matrix  whose entries are denoted pij   will suce  given an
initial
over states p     the probability distribution over states after n stages is
q  pdistribution
i 
i n
a stationary markov process can also be represented using a state transition diagram
as in figure    here nodes correspond to particular states and the stage is not represented
explicitly  arcs denote possible transitions  those with non zero probability  and are labeled
with the transition probabilities pij   pr s t     sj js t   si    the arc from node i to node
j is labeled with pij if pij       the size of such a diagram is at least o n   and at most
o n      depending on the number of arcs  this is a useful representation when the transition
graph is relatively sparse  for example  when most states have immediate transitions to only
few neighbors 

example     to illustrate these notions  imagine that the robot in figure   is executing

the policy of moving counterclockwise repeatedly  we restrict our attention to two
variables  location loc and presence of mail m   giving a state space of size     we
suppose that the robot always moves to the adjacent location with probability     
in addition  mail can arrive at the mailroom with probability     at any time  independent of the robot s location   causing the variable m to become true  once m
becomes true  the robot cannot move to a state where m is false  since the action of
moving does not inuence the presence of mail  the state transition diagram for this
example is illustrated in figure    the transition matrix is also shown   

the structure of a markov chain is occasionally of interest to us in planning  a subset

c  s is closed if pij     for all i   c and j    c   it is a proper closed set if no proper
subset of c enjoys this property  we sometimes refer to proper closed sets as recurrent
classes of states  if a closed set consists of a single state  then that state is called an
absorbing state  once an agent enters a closed set or absorbing state  it remains there
   it is important to note that the nodes here do not represent random variables as in the earlier figures 

 

fidecision theoretic planning  structural assumptions

s 
s 

   

om

   

   
   

om

s 

hm

lm

   

   

   

s  
   

   
cm

mm

mm

   

s 

hm

s     

s 

   

s 
s 
s 
s 
s 
s 
s 
s 
s 
s  

s 

   

s 

lm

   

s 

cm

   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
    
   
   
   
   
   
   
   
    

s 
   
   
   
   
   
   
   
   
   
    

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
    
   
    
   
    
    
    
    
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s  
   
   
   
   
   
   
   
   
   
   

figure    the state transition diagram and transition matrix for a moving robot 
forever with probability    in the example above  figure     the set of states where m
holds forms a recurrent class  there are no absorbing states in the example  but should we
program the robot to stay put whenever it is in the state hm  loc o i  then this would be
an absorbing state in the altered chain  finally  we say a state is transient if it does not
belong to a recurrent class  in figure    each state where m holds is transient eventually
 with probability     the agent leaves the state and never returns  since there is no way to
remove mail once it arrives 

    actions
markov chains can be used to describe the evolution of a stochastic system  but they do
not capture the fact that an agent can choose to perform actions that alter the state of the
system  a key element of mdps is the set of actions available to the decision maker  when
an action is performed in a particular state  the state changes stochastically in response to
the action  we assume that the agent takes some action at each stage of the process  and
then the system changes state accordingly 
at each stage t of the process and each state s  the agent has available a set of actions
ats  this is called the feasible set for s at stage t  to describe the effects of a   ats  we must
supply the state transition distribution pr s t   js t   s  at   a  for all actions a  states s 
and stages t  unlike the case of a markov chain  the terms pr s t   js t   s  at   a  are not
true conditional distributions  but rather a family of distributions parameterized by s t and
at   since the probability of at is not part of the model  we retain this notation  however 
for its suggestive nature 
we often assume that the feasible set of actions is the same for all stages and states  in
which case the set of actions is a   fa            ak g and each can be executed at any time 
this contrasts with the ai planning practice of assigning preconditions to actions defining
the states in which they can meaningfully be executed  our model takes the view that any
action can be executed  or  attempted   in any state  if the action has no effect when
executed in some state  or its execution leads to disastrous effects  this can be noted in
the action s transition matrix  action preconditions are often a computational convenience
rather than a representational necessity  they can make the planning process more ecient
by identifying states in which the planner should not even consider selecting that action 
preconditions can be represented in mdps by relaxing the assumption that the set of
 

fiboutilier  dean    hanks

s 
s 
s 
s 
s 
s 
s 
s 
s 
s  

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s  
   
   
   
   
   
   
   
   
   
   

s 
   

   

om

   

s 

lm

s 

s 

s  

   

s 

   
   

cm

mm

mm

s 

s 

   
   

   

s 
hm

s 

   
   

   

hm

   

lm

om

   

cm

   

figure    the transition matrix for clk and the induced transition diagram for a two action
policy 
feasible actions is the same for all states  to illustrate planning concepts below  however 
we sometimes assume actions do have preconditions 
we again restrict our attention to stationary processes  which in this case means that
the effects of each action depends only on the state and not on the stage  our transition
matrices thus take the form pkij   pr s t     sj js t   si   at   ak    capturing the probability
that the system moves to state sj when ak is executed in state si   in stationary models an
action is fully described by a single n  n transition matrix p k   it is important to note
that the transition matrix for an action includes not only the direct effects of executing the
action but also the effects of any exogenous events that might occur at the same stage  

example     the example in figure   can be extended so the agent has two available

actions  moving clockwise and moving counterclockwise  the transition matrix for
cclk  with the assumption that mail arrives with probability      is shown in figure   
the matrix for clk appears on the left in figure    suppose the agent fixes its behavior
so that it moves clockwise in locations m and c and counterclockwise in locations h  
o and l  we address below how the agent might come to know its location so that it
can actually implement this behavior   this defines the markov chain illustrated in
the transition diagram on the right in figure     

    exogenous events

exogenous events are those events that stochastically cause state transitions  much like
actions  but beyond the control of the decision maker  these might correspond to the
evolution of a natural process or the action of another agent  notice that the effect of
the action cclk in figure    combines  the effects of the robot s action with that of the
exogenous event of mail arrival  state transition probabilities incorporate both the motion
of the robot  causing a change in location  and the possible change in mail status due
to mail arrival  for the purposes of decision making  it is precisely this combined effect
   it is possible to assess the effects of actions and exogenous events separately  then combine them into
a single transition matrix in certain cases  boutilier   puterman         we discuss this later in this
section 

  

fidecision theoretic planning  structural assumptions

that is important when predicting the distribution over possible states resulting when an
action is taken  we call such models of actions implicit event models  since the effects of
the exogenous event are folded into the transition probabilities associated with the action 
however  it is often natural to view these transitions as comprised of these two separate
events  each having its own effect on the state  more generally  we often think of transitions
as determined by the effects of the agent s chosen action and those of certain exogenous
events beyond the agent s control  each of which may occur with a certain probability 
when the effects of actions are decomposed in this fashion  we call the action model an
explicit event model 
specifying a transition function for an action and zero or more exogenous events is not
generally easy  for actions and events can interact in complex ways  for instance  consider
specifying the effect of action pum  pickup mail  at a state where no mail is present but
there is the possibility of  simultaneous  mail arrival  i e   during the  same unit  of discrete
time   if the event arrm occurs  does the robot obtain the newly arrived mail  or does the
mail remain in the mailbox  intuitively  this depends on whether the mail arrived before or
after the pickup was completed  albeit within the same time quantum   the state transition
in this case can be viewed as the composition of two transitions where the precise description
of the composition depends on the ordering of the agent s action and the exogenous event 
if mail arrives first  the transition might be s   s    s     where s  is a state where mail
is waiting and s   is a state where no mail is waiting and the robot is holding mail  but if
the pickup action is completed first  the transition would be s   s   s   i e   pum has no
effect  then mail arrives and remains in the box  
the picture is more complicated if the actions and events can truly occur simultaneously
over some interval in this case the resulting transition need not be a composition of the
individual transitions  as an example  if the robot lifts the side of a table on which a glass
of water is situated  the water will spill  similarly if an exogenous event causes the other side
to be raised  but if the action and event occur simultaneously  the result is qualitatively
different  the water is not spilled   thus  the  interleaving  semantics described above is
not always appropriate 
because of such complications  modeling exogenous events and their combination with
actions or other events can be approached in many ways  depending on the modeling assumptions one is willing to make  generally  we specify three types of information  first 
we provide transition probabilities for all actions and events under the assumption that
these occur in isolation these are standard transition matrices  the transition matrix in
figure   can be decomposed into the two matrices shown in figure    one for clk and one
for arrm   second  for each exogenous event  we must specify its probability of occurrence 
since this can vary with the state  we generally require a vector of length n indicating the
probability of occurrence at each state  the occurrence vector for arrm would be
                                         
   the fact that these individual matrices are deterministic is an artifact of the example  in general  the
actions and events will each be represented using genuinely stochastic matrices 

  

fiboutilier  dean    hanks

s 
s 
s 
s 
s 
s 
s 
s 
s 
s  

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s  
   
   
   
   
   
   
   
   
   
   

s 
s 
s 
s 
s 
s 
s 
s 
s 
s  

action clk

s 

    
    
    
    
    
    
    
    
    
    

s 

   
   
   
   
   
   
   
   
   
   

s 

    
    
    
    
    
    
    
    
    
    

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s  
   
   
   
   
   
   
   
   
   
   

event arrm

figure    the transition matrices for an action and exogenous event in an explicit event
model 
where we assume  for illustration  that mail arrives only when none is present   the final
requirement is a combination function that describes how to  compose  the transitions of an
action with any subset of event transitions  as indicated above  this can be very complex 
sometimes almost unrelated to the individual action and event transitions  however  under
certain assumptions combination functions can be specified reasonably concisely 
one way of modeling the composition of transitions is to assume an interleaving semantics of the type alluded to above  in this case  one needs to specify the probability that
the action and events that take place occur in a specific order  for instance  one might
assume that each event occurs at a time within the discrete time unit according to some
continuous distribution  e g   an exponential distribution with a given rate   with this information  the probability of any particular ordering of transitions  given that certain events
occur  can be computed  as can the resulting distribution over possible next states  in the
example above  the probability of  composed  transitions s    s    s  and s    s    s 
would be given by the probabilities with which mail arrived first or last  respectively 
in certain cases  the probability of this ordering is not needed  to illustrate another
combination function  assume that the action always occurs before the exogenous events 
furthermore  assume that events are commutative   a  for any initial state s and any pair
of events e  and e    the distribution that results from applying event sequence e   e  to s is
identical to that obtained from the sequence e   e    and  b  the occurrence probabilities at
intermediate states are identical  intuitively  the set of events in our domain  arrm  reqc
and mess  has this property  under these conditions the combined transition distribution
for any action a is computed by considering the probability of any subset of events and
applying that subset in any order to the distribution associated with a 
generally  we can construct an implicit event model from the various components of the
explicit event model  thus  the  natural  specification can be converted to the form usually
used by mdp solution algorithms  under the two assumptions above  for instance  we can
form an implicit event transition matrix pr si   a  sj   for any action a  given the matrix
pcra  si   sj   for a  which assumes no event occurrences   the matrices pre  si   sj   for events
e  and the occurrence vector pre si   for each event e  the effective transition matrix for
   the probability of different events may be correlated  possibly at particular states   if this is the case 
then it is necessary to specify occurrence probabilities for subsets of events  we will treat event occurrence
probabilities as independent for ease of exposition 

  

fidecision theoretic planning  structural assumptions

event e is defined as follows 
pcre  si   sj     pre  si  pre  si   sj    

 

    pre  si     i   j
    i    j

this equation captures the event transition probabilities with the probability of event occurrence factored in  if we let e  e   denote the diagonal matrices with entries ekk   pre  sk  
        pre  sk    then pr
c e si  sj     e pre  e    under the assumptions above  the
and ekk
implicit event matrix pr si   a  sj   for action a is then given by pr   pcre     pcren pra for
any ordering of the n possible events 
naturally  different procedures for constructing implicit event matrices will be required
given different assumptions about action and event interaction  whether such implicit models are constructed or specified directly without explicit mention of the exogenous events  we
will always assume unless stated otherwise that action transition matrices take into account
the effects of exogenous events as well  and thus represent the agent s best information
about what will happen if it takes a particular action 

    observations

although the effects of an action can depend on any aspect of the prevailing state  the
choice of action can depend only on what the agent can observe about the current state
and remember about its prior observations  we model the agent s observational or sensing
capabilities by introducing a finite set of observations o   fo            oh g  the agent receives
an observation from this set at each stage prior to choosing its action at that stage  we
can model this observation as a random variable ot whose value is taken from o  the
probability that a particular ot is generated can depend on 

 the state of the system at t    
 the action taken at t    
 the state of the system at t after taking the action at t     and after the effects of any
exogenous events at t     are realized  but before the action at t is taken 
we let pr ot   oh js t     si   at     ak   s t   sj   be the probability that the agent observes

oh at stage t given that it performs ak in state si and ends up in state sj   as with actions 

we assume that observational distributions are stationary  independent of the stage   using

phi j k   pr ohjsi  ak   sj   to denote this quantity  we can view the probabilistic dependencies

among state  action and observation variables as a graph in which the time indexed variables
are shown as nodes and one variable is directly probabilistically dependent on another if
there is an edge from the latter to the former  see figure   
this model allows a wide variety of assumptions about the agent s sensing capabilities 
at one extreme are fully observable mdps  fomdps   in which the agent knows exactly
what state it is in at each stage t  we model this case by letting o   s and setting
pr oh jsi   ak   sj    
  

 

  iff oh   sj
  otherwise

fiboutilier  dean    hanks

a

t  

s

s

t  

t

o

t

figure    graph showing the dependency relationships among states  actions and observations at different times 
in the example above  this means the robot always knows its exact location and whether or
not mail is waiting in the mailbox  even if it is not in the mailroom when the mail arrives 
the agent thus receives perfect feedback about the results of its actions and the effects
of exogenous events it has noisy effectors but complete  noise free  and  instantaneous 
sensors  most recent ai research that adopts the mdp framework explicitly assumes full
observability 
at the other extreme we might consider non observable systems  nomdps  in which
the agent receives no information about the system s state during execution  we can model
this case by letting o   fog  here the same observation is reported at each stage  revealing
no information about the state  so that pr sj jsi   ak   o    pr sj jsi   ak    in these open loop
systems  the agent receives no useful feedback about the results of its actions  the agent has
noisy effectors and no sensors  in this case an agent chooses its actions according to a plan
consisting of a sequence of actions executed unconditionally  in effect  the agent is relying
on its predictive model to determine good action choices before execution time 
traditionally  ai planning work has implicitly made the assumption of non observability 
often coupled with an omniscience assumption that the agent knows the initial state with
certainty  can predict the effects of its actions perfectly  and can precisely predict the occurrence of any exogenous events and their effects  under these circumstances  the agent
can predict the exact outcome of any plan  thus obviating the need for observation  such
an agent can build a straight line plan a sequence of actions to be performed without
feedback that is as good as any plan whose execution might depend on information gathered at execution time 
these two extremes are special cases of the general observation model described above 
which allows the agent to receive incomplete or noisy information about the system state
 i e   partially observable mdps  or pomdps   for example  the robot might be able to
determine its location exactly  but might not be able to determine whether mail arrives
unless it is in the mailroom  furthermore  its  mail  sensors might occasionally report
inaccurately  leading to an incorrect belief as to whether there is mail waiting 

example     suppose the robot has a  checkmail  action that does not change the system

state but generates an observation that is inuenced by the presence of mail  provided
  

fidecision theoretic planning  structural assumptions

loc m    m
loc m    m
loc m    m
loc m    m

pr obs   mail  pr obs   nomail 
    
    
    
    
    
    
    
    

figure    observation probabilities for checking mailbox 
the robot is in the mailroom at the time the action is performed  if the robot is not
in the mailroom  the sensor always reports  no mail   a noisy  checkmail  sensor
can be described by a probability distribution like the one shown in figure    we can
view these error probabilities as the probability of  false positives         and  false
negatives           

    system trajectories and observable histories

we use the terms trajectory and history interchangeably to describe the system s behavior
during the course of a problem solving episode  or perhaps some initial segment thereof 
the complete system history is the sequence of states  actions  and observations generated
from stage   to some time point of interest  and can be of finite or infinite length  complete
histories can be represented by a  possibly infinite  sequence of tuples of the form

hhs     o    a  i  hs     o    a  i        hs t   ot   at ii
we can define two alternative notions of history that contain less complete information 
for some arbitrary stage t we define the observable history as the sequence

hho    a  i          hot     at   ii
where o  is the observation of the initial state  the observable history at stage t comprises
all information available to the agent about its history when it chooses its action at stage t 
a third type of trajectory is the system trajectory  which is the sequence

hhs     a  i          hs t     at   i  s t i
describing the system s behavior in  objective  terms  independent of the agent s particular
view of the system 
in evaluating an agent s performance  we will generally be interested in the system
trajectory  an agent s policy must be defined in terms of the observable history  since the
agent does not have access to the system trajectory  except in the fully observable case 
when the two are equivalent 

    reward and value

the problem facing the decision maker is to select an action to be performed at each stage
of the decision problem  making this decision on the basis of the observable history  the
agent still needs some way to judge the quality of a course of action  this is done by defining
  

fiboutilier  dean    hanks

a

t  

s

s

t  

t

c

t

r

t

figure     decision process with rewards and action costs 
a value function v   as a function mapping the set of system histories hs into the reals 
that is  v   hs   r    the agent prefers system history h to h  just in case v h    v h    
thus  the agent judges its behavior to be good or bad depending on its effect on the
underlying system trajectory  generally  the agent cannot predict with certainty which
system trajectory will occur  and can at best generate a probability distribution over the
possible trajectories caused by its actions  in that case  it computes the expected value of
each candidate course of action and chooses a policy that maximizes that quantity 
just as with system dynamics  specifying a value function over arbitrary trajectories
can be cumbersome and unintuitive  it is therefore important to identify structure in the
value function that can lead to a more parsimonious representation 
two assumptions about value functions commonly made in the mdp literature are
time separability and additivity  a time separable value function is defined in terms of
more primitive functions that can be applied to component states and actions  the reward
function r   s   r associates a reward with being in a state s  costs can be assigned
to taking actions by defining a cost function c   s  a   r that associates a cost with
performing an action a in state s  rewards are added to the value function  while costs are
subtracted   
a value function is time separable if it is a  simple combination  of the rewards and costs
accrued at each stage   simple combination  means that value is taken to be a function of
costs and rewards at each stage  where the costs and rewards can depend on the stage t  but
the function that combines these must be independent of the stage  most commonly a linear
combination or a product    a value function is additive if the combination function is a
sum of the reward and cost function values accrued over the history s stages  the addition
of rewards and action costs in a system with time separable value can be viewed graphically
as shown in figure    
the assumption of time separability is restrictive  in our example  there might be
certain goals involving temporal deadlines  have the workplace tidy as soon as possible
after      tomorrow morning  and maintenance  do not allow mail to sit in the mailroom
    technically  the set of histories of interest also depends on the horizon chosen  as described below 
    the term  reward  is somewhat of a misnomer in that the reward could be negative  in which case
 penalty  might be a better word  likewise   costs  can be either positive  punitive  or negative  beneficial   thus  they admit great exibility in defining value functions 
    see  luenberger        for a more precise definition of time separability 

  

fidecision theoretic planning  structural assumptions

undelivered for more than    minutes  that require value functions that are non separable
given our current representation of the state  note  however  that separability like the
markov property is a property of a particular representation  we could add additional
information to the state in our example  the clock time  the interval of time between     
and the time at which tidiness is achieved  the length of time mail sits in the mail room
before the robot picks it up  and so on  with this additional information we could reestablish a time separable value function  but at the expense of an increase in the number
of states and a more ad hoc and cumbersome action representation   

    horizons and success criteria

in order to evaluate a particular course of action  we need to specify how long  in how
many stages  it will be executed  this is known as the problem s horizon  in finite horizon
problems  the agent s performance is evaluated over a fixed  finite number of stages t  
commonly  our aim is to maximize the total expected reward associated with a course of
action  we therefore define the  finite horizon  value of any length t history h as  bellman 
      

v  h   

tx
  
t  

fr st     c  st  at  g   r st  

an infinite horizon problem  on the other hand  requires that the agent s performance
be evaluated over an infinite trajectory  in this case the total reward may be unbounded 
meaning that any policy could be arbitrarily good or bad if it were executed for long enough 
in this case it may be necessary to adopt a different means of evaluating a trajectory  the
most common is to introduce a discount factor  ensuring that rewards or costs accrued at
later stages are counted less than those accrued at earlier stages  the value function for
an expected total discounted reward problem is defined as follows  bellman        howard 
      
 
x
v  h     t  r st    c  st  at   
t  

where  is a fixed discount rate            this formulation is a particularly simple
and elegant way to ensure a bounded measure of value over an infinite horizon  though it is
important to verify that discounting is in fact appropriate  economic justifications are often
provided for discounted models a reward earned sooner is worth more than one earned
later provided the reward can somehow be invested  discounting can also be suitable for
modeling a process that terminates with probability      at at any point in time  e g   a
robot that can break down   in which case discounted models correspond to expected total
reward over a finite but uncertain horizon  for these reasons  discounting is sometimes used
for finite horizon problems as well 
another technique for dealing with infinite horizon problems is to evaluate a trajectory
based on the average reward accrued per stage  or gain  the gain of a history is defined as
n
x
g h    lim   fr st     c  st   at  g
n   n t  

    see  bacchus  boutilier    grove               however  for a systematic approach to handling certain
types of history dependent reward functions 

  

fiboutilier  dean    hanks

refinements of this criterion have also been proposed  puterman        
sometimes the problem itself ensures that total reward over any infinite trajectory is
bounded  and thus the expected total reward criterion is well defined  consider the case
common in ai planners in which the agent s task is to bring the system to a goal state  a
positive reward is received only when the goal is reached  all actions incur a non negative
cost  and when a goal is reached the system enters an absorbing state in which no further
rewards or costs are accrued  as long as the goal can be reached with certainty  this
situation can be formulated as an infinite horizon problem where total reward is bounded
for any desired trajectory  bertsekas        puterman         in general  such problems
cannot be formulated as  fixed  finite horizon problems unless an a priori bound on the
number of steps needed to reach the goal can be established  these problems are sometimes
called indefinite horizon problems  from a practical point of view  the agent will continue to
execute actions for some finite number of stages  but the exact number cannot be determined
ahead of time 

    solution criteria

to complete our definition of the planning problem we need to specify what constitutes
a solution to the problem  here again we see a split between explicit mdp formulations
and work in the ai planning community  classical mdp problems are generally stated as
optimization problems  given a value function  a horizon  and an evaluation metric  e g  
expected total reward  expected total discounted reward  expected average reward per stage 
the agent seeks a behavioral policy that maximizes the objective function 
work in ai often seeks satisficing solutions to such problems  in the planning literature 
it is generally taken that any plan that satisfies the goal is equally preferred to any other
plan that satisfies the goal  and that any plan that satisfies the goal is preferable to any
plan that does not    in a probabilistic framework  we might seek the plan that satisfies
the goal with maximum probability  an optimization   but this can lead to situations in
which the optimal plan has infinite length if the system state is not fully observable  the
satisficing alternative  kushmerick  hanks    weld        is to seek any plan that satisfies
the goal with a probability exceeding a given threshold 

example     we extend our running example to demonstrate an infinite horizon  fully
observable  discounted reward situation  we begin by adding one new dimension to
the state description  the boolean variable rhm  does the robot have mail   giving us
a system with    states  we also provide the agent with two additional actions  pum
 pickup mail  and delm  deliver mail  as described in figure    we can now reward
the agent in such a way that mail delivery is encouraged  we associate a reward of
   with each state in which rhm and m are both false and   with all other states 
if actions have no cost  the agent gets a total reward of    for this six stage system
trajectory 
hloc m    m  rhmi  stay  hloc m    m  rhmi  pum  hloc m    m  rhmi 
clk  hloc h    m  rhmi  clk  hloc o   m  rhmi  delm  hloc o   m  rhmi

    though see  haddawy   hanks        williamson   hanks        for a restatement of planning as an
optimization problem 

  

fidecision theoretic planning  structural assumptions

if we assign an action cost of    for each action except stay  which has   cost  
the total reward becomes     if we use a discount rate of     to discount future
rewards and costs  this initial segment of an infinite horizon history would contribute
                                                                      to the total
value of the trajectory  as subsequently extended   furthermore  we can establish a
bound on the total expected value of this trajectory  in the best case  all subsequent
stages will yield a reward of     so the expected total discounted reward is bounded
by
 
x
                                                       i     
i  

a similar effect on behavior can be achieved by penalizing states  i e   having negative
rewards  in which either m or rhm is true   

    policies

we have mentioned policies  or courses of action  or plans  informally to this point  and
now provide a precise definition  the decision problem facing an agent can be viewed most
generally as deciding which action to perform given the current observable history  we
define a policy  to be a mapping from the set of observable histories ho to actions  that
is     ho   a  intuitively  the agent executes action

at    hho    a  i          hot     at   i  ot i 
at stage t if it has performed the actions a       at   and made observations o       ot   at
earlier stages  and has just made observation ot at the current stage 
a policy induces a distribution pr hj  over the set of system histories hs   this probability distribution depends on the initial distribution p     we define the expected value of a
policy to be 
x
ev    
v h  pr hj 
h hs

we would like the agent to adopt a policy that either maximizes this expected value or  in
a satisficing context  has an acceptably high expected value 
the general form of a policy  depending as it does on an arbitrary observation history 
can lead to very complicated policies and policy construction algorithms  in special cases 
however  assumptions about observability and the structure of the value function can result
in optimal policies that have a much simpler form 
in the case of a fully observable mdp with a time separable value function  the optimal
action at any stage can be computed using only information about the current state and
the stage  that is  we can restrict policies to have the simpler form    s  t   a without
danger of acting suboptimally  this is due to the fact that full observability allows the state
to be observed completely  and the markov assumption renders prior history irrelevant 
in the non observable case  the observational history contains only vacuous observations
and the agent must choose its actions using only knowledge of its previous actions and the
stage  however  since  incorporates previous actions  it takes the form    t   a  this
  

fiboutilier  dean    hanks

form of policy corresponds to a linear  unconditional sequence of actions ha    a            at i  or
a straight line plan in ai nomenclature   

     model summary  assumptions  problems  and computational
complexity

this concludes our exposition of the mdp model for planning under uncertainty  its generality allows us to capture a wide variety of the problem classes that are currently being
studied in the literature  in this section we review the basic components of the model 
describe problems commonly studied in the dtp literature with respect to this model  and
summarize known complexity results for each  in section    we describe some of the specialized computational techniques used to solve problems in each of these problem classes 
       model summary and assumptions

the mdp model consists of the following components 
 the state space s   a finite or countable set of states  we generally make the markov
assumption  which requires that each state convey all information necessary to predict
the effects of all actions and events independent of any further information about
system history 
 the set of actions a  each action ak is represented by a transition matrix of size
js jjs j representing the probability pkij that performing action ak in state si will move
the system into state sj   we assume throughout that the action model is stationary 
meaning that transition probabilities do not vary with time  the transition matrix
for an action is generally assumed to account for any exogenous events that might
occur at the stage at which the action is executed 
 the set of observation variables o  this is the set of  messages  sent to the agent after
an action is performed  that provide execution time information about the current
system state  with each action ak and pair of states si   sj   such that pkij     
we associate a distribution over possible observations  pkm
ij denotes the probability
of obtaining observation om given that action ak was taken in si and resulted in a
transition to state sj  
 the value function v   the value function maps a state history into a real number
such that v  h     v  h    just in case the agent considers history h  at least as good
as h    a state history records the progression of states the system assumes along
with the actions performed  assumptions such as time separability and additivity are
common for v   in particular  we generally use a reward function r and cost function
c when defining value 
 the horizon t   this is the number of stages over which the state histories should be
evaluated using v  
    many algorithms in the ai literature produce a partially ordered sequence of actions  these plans do
not  however  involve conditional or nondeterministic execution  rather  they represent the fact that
any linear sequence consistent with the partial order will solve the problem  thus  a partially ordered
plan is a concise representation for a particular set of straight line plans 

  

fidecision theoretic planning  structural assumptions

 an optimality criterion  this provides a criterion for evaluating potential solutions
to planning problems 

       common planning problems

we can use this general framework to classify various problems commonly studied in the
planning and decision making literature  in each case below  we note the modeling assumptions that define the problem class 

planning problems in the or decision sciences tradition
 fully observable markov decision processes  fomdps    there is an ex 

tremely large body of research studying fomdps  and we present the basic algorithmic techniques in some detail in the next section  the most commonly used formulation of fomdps assumes full observability and stationarity  and uses as its optimality
criterion the maximization of expected total reward over a finite horizon  maximization of expected total discounted reward over an infinite horizon  or minimization of
the expected cost to a goal state 
fomdps were introduced by bellman        and have been studied in depth in the
fields of decision analysis and or  including the seminal work of howard         recent texts on fomdps include  bertsekas        and  puterman         average reward optimality has also received attention in this literature  blackwell        howard 
      puterman         in the ai literature  discounted or total reward models have
been most popular as well  barto et al         dearden   boutilier        dean  kaelbling  kirman    nicholson        koenig         though the average reward criterion
has been proposed as more suitable for modeling ai planning problems  boutilier  
puterman        mahadevan        schwartz        

 partially observable markov decision processes  pomdps    pomdps

are closer than fomdps to the general model of decision processes we have described 
pomdps have generally been studied with the assumption of stationarity and optimality criteria identical to those of fomdps  though the average reward criterion
has not been widely considered  as we discuss below  a pomdp can be viewed as
a fomdp with a state space consisting of the set of probability distributions over
s   these probability distributions represent states of belief  the agent can  observe 
its state of belief about the system although it does not have exact knowledge of the
system state itself 
pomdps have been widely studied in or and control theory  astrom        lovejoy      b  smallwood   sondik        sondik         and have drawn increasing
attention in ai circles  cassandra  kaelbling    littman        hauskrecht       
littman        parr   russell        simmons   koenig        thrun  fox    burgard        zhang   liu         inuence diagrams  howard   matheson       
shachter        are a popular model for decision making in ai and are  in fact  a
structured representational method for pomdps  see section      

planning problems in the ai tradition
  

fiboutilier  dean    hanks

 classical deterministic planning   the classical ai planning model assumes

deterministic actions  any action ak taken at any state si has at most one successor sj  
the other important assumptions are non observability and that value is determined
by reaching a goal state  any plan that leads to a goal state is preferred to any
that does not  often there is a preference for shorter plans  this can be represented
by using a discount factor to  encourage  faster goal achievement or by assigning a
cost to actions  reward is associated only with transitions to goal states  which are
absorbing  action costs are typically ignored  except as noted above 
in classical models it is usually assumed that the initial state is known with certainty 
this contrasts with the general specification of mdps above  which does not assume
knowledge of or even distributional information about the initial state  policies are
defined to be applicable no matter what state  or distribution over states  one finds
oneself in action choices are defined for every possible state or history  knowledge of
the initial state and determinism allow optimal straight line plans to be constructed 
with no loss in value associated with non observability  but unpredictable exogenous
events and uncertain action effects cannot be modeled consistently if these assumptions are adopted 
for an overview of early classical planning research and the variety of approaches
adopted  see  allen  hendler    tate        as well as yang s        recent text 

 optimal deterministic planning   a separate body of work retains the classical

assumptions of complete information and determinism  but tries to recast the planning
problem as an optimization that relaxes the implicit assumption of  achieve the goal
at all costs   at the same time  these methods use the same sort of representations
and algorithms applied to satisficing planning 
haddawy and hanks        present a multi attribute utility model for planners that
keeps the explicit information about the initial state and goals  but allows preferences to be stated about the partial satisfaction of the goals as well as the cost of the
resources consumed in satisfying them  the model also allows the expression of preferences over phenomena like temporal deadlines and maintenance intervals that are
dicult to capture using a time separable additive value function  williamson       
 see also williamson   hanks         implements this model by extending a classical planning algorithm to solve the resulting optimization problem  haddawy and
suwandi        also implement this model in a complete decision theoretic framework 
their model of planning  refinement planning  differs somewhat from the generative
model discussed in this paper  in their model the set of all possible plans is pre stored
in an abstraction hierarchy  and the problem solver s job is to find in the hierarchy
the optimal choice of concrete actions for a particular problem 
perez and carbonell s        work also incorporates cost information into the classical
planning framework  but maintains the split between a classical satisficing planner
and additional cost information provided in the utility model  the cost information is
used to learn search control rules that allow the classical planner to generate low cost
goal satisfying plans 
  

fidecision theoretic planning  structural assumptions

 conditional deterministic planning   the classical planning assumption of

omniscience can be relaxed somewhat by allowing the state of some aspects of the
world to be unknown  the agent is thus in a situation where it is certain that the
system is one of a particular set of states  but does not know which one  unknown
truth values can be included in the initial state specification  and taking actions can
cause a proposition to become unknown as well 
actions can provide the agent with information while the plan is being executed  conditional planners introduce the idea of actions providing runtime information about
the prevailing state  distinguishing between an action that makes proposition p true
and an action that will tell the agent whether p is true when the action is executed 
an action can have both causal and informational effects  simultaneously changing
the world and reporting on the value of one or more propositions  this second sort
of information is not useful at planning time except that it allows steps in the plan
to be executed conditionally  depending on the runtime information provided by prior
information producing steps  the value of such actions lies in the fact that different
courses of action may be appropriate under different conditions these informational
effects allow runtime selection of actions based on the observations produced  much
like the general pomdp model 
examples of conditional planners in the classical framework include early work by
warren        and the more recent cnlp  peot   smith         cassandra  pryor
  collins         plynth  goldman   boddy         and uwl  etzioni  hanks 
weld  draper  lesh    williamson        systems 

 probabilistic planning without feedback   a direct probabilistic extension

of the classical planning problem can be stated as follows  kushmerick et al         
take as input  a  a probability distribution over initial states   b  stochastic actions
 explicit or implicit transition matrices    c  a set of goal states  and  d  a probability
success threshold    the objective is to produce a plan that reaches any goal state
with probability at least    given the initial state distribution  no provision is made
for execution time observation  thus straight line plans are the only form of policy
possible  this is a restricted case of the infinite horizon nomdp problem  one in
which actions incur no cost and goal states offer positive reward and are absorbing 
it is also a special case in that the objective is to find a satisficing policy rather than
an optimal one 

 probabilistic planning with feedback   draper et al       a  have proposed

an extension of the probabilistic planning problem in which actions provide feedback 
using exactly the observation model described in section      again  the problem is
posed as that of building a plan that leaves the system in a goal state with sucient
probability  but a plan is no longer a simple sequence of actions it can contain conditionals and loops whose execution depends on the observations generated by sensing
actions  this problem is a restricted case of the general pomdp problem  absorbing goal states and cost free actions are used  and the objective is to find any policy
 conditional plan  that leaves the system in a goal state with sucient probability 
  

fiboutilier  dean    hanks

comparing the frameworks  task oriented versus process oriented problems

it is useful at this point to pause and contrast the types of problems considered in the classical planning literature with those typically studied within the mdp framework  although
problems in the ai planning literature have emphasized a goal pursuit or  one shot  view of
problem solving  in some cases viewing the problem as an infinite horizon decision problem
results in a more satisfying formulation  consider our running example involving the oce
robot  it is simply not possible to model the problem of responding to coffee requests  mail
arrival and keeping the lab tidy as a strict goal satisfaction problem while capturing the
possible nuances of intuitively optimal behavior 
the primary diculty is that no explicit and persistent goal states exist  if we were
simply to require that the robot attain a state where the lab is tidy  no mail awaits  and no
unfilled coffee requests exist  no  successful  plan could anticipate possible system behavior
after a goal state was reached  the possible occurrence of exogenous events after goal
achievement requires that the robot bias its methods for achieving its goals in a way that
best suits the expected course of subsequent events  for instance  if coffee requests are
very likely at any point in time and unmet requests are highly penalized  the robot should
situate itself in the coffee room in order to satisfy an anticipated future request quickly 
most realistic decision scenarios involve both task oriented and process oriented behavior 
and problem formulations that take both into account will provide more satisfying models
for a wider range of situations 
       the complexity of policy construction

we have now defined the planning problem in several different ways  each having a different
set of assumptions about the state space  system dynamics and actions  deterministic or
stochastic   observability  full  partial  or none   value function  time separable  goal only 
goal rewards and action costs  partially satisfiable goals with temporal deadlines   planning
horizon  finite  infinite  or indefinite   and optimality criterion  optimal or satisficing solutions   each set of assumptions puts the corresponding problem in a particular complexity
class  which defines worst case time and space bounds on any representation and algorithm
for solving that problem  here we summarize known complexity results for each of the
problem classes defined above 
fully observable markov decision processes fully observable mdps  fomdps 
with time separable  additive value functions can be solved in time polynomial in the size
of the state space  the number of actions  and the size of the inputs    the most common algorithms for solving fomdps are value iteration and policy iteration  which are
described in the next section  both finite horizon and discounted infinite horizon problems
require a polynomial amount of computation per iteration o js j  jaj  and o js j  jaj js j    
respectively and converge in a polynomial number of iterations  with factor      in the
discounted case   on the other hand  these problems have been shown to be p complete
 papadimitriou   tsitsiklis         which means that an ecient parallel solution algorithm
is unlikely    the space required to store the policy for an n stage finite horizon problem
    more precisely  the maximum number of bits required to represent any of the transition probabilities or
costs 
    see  littman  dean    kaelbling        for a summary of these complexity results 

  

fidecision theoretic planning  structural assumptions

is o js jn   for most interesting classes of infinite horizon problems  specifically those involving discounted models with time separable additive reward  the optimal policy can be
shown to be stationary  and the policy can be stored in o js j  space 
bear in mind that these are worst case bounds  in many cases  better time bounds and
more compact representations can be found  sections   and   explore ways to represent
and solve these problems more eciently 
partially observable markov decision processes pomdps are notorious for their
computational diculty  as mentioned above  a pomdp can be viewed as a fomdp
with an infinite state space consisting of probability distributions over s   each distribution
representing the agent s state of belief at a point in time  astrom        smallwood  
sondik         the problem of finding an optimal policy for a pomdp with the objective
of maximizing expected total reward or expected total discounted reward over a finite
horizon t has been shown to be exponentially hard both in js j and in t  papadimitriou
  tsitsiklis         the problem of finding a policy that maximizes or approximately
maximizes the expected discounted total reward over an infinite horizon is shown to be
undecidable  madani  condon    hanks        
even restricted cases of the pomdp problem are computationally dicult in the worst
case  littman        considers the special case of boolean rewards  determining whether
there is an infinite horizon policy with nonzero total reward given that the rewards associated with all states are non negative  he shows that the problem is exptime complete if
the transitions are stochastic  and pspace hard if the transitions are deterministic 
deterministic planning recall that the classical planning problem is defined quite
differently from the mdp problems above  the agent has no ability to observe the state but
has perfect predictive powers  knowing the initial state and the effects of all actions with
certainty  in addition  rewards come only from reaching a goal state  and any plan that
achieves the goal suces 
planning problems are typically defined in terms of a set p of boolean features or
propositions  a complete assignment of truth values to features describes exactly one state 
and a partial assignment of truth values describes a set of states  a set of propositions p
induces a state space of size  jpj  thus  the space required to represent a planning problem
using a feature based representation can be exponentially smaller than that required by a
at representation for the same problem  see section    
the ability to represent planning problems compactly has a dramatic impact on worstcase complexity  bylander        shows that the deterministic planning problem without
observation is pspace complete  roughly speaking  this means that at worst planning
time will increase exponentially with p and a  and further  that the size of a solution plan
can grow exponentially with the problem size  these results hold even when the action
space a is severely restricted  for example  the planning problem is np complete even
in cases where each action is restricted to one precondition feature and one postcondition
feature  conditional and optimal planning are pspace complete as well  these results
are for inputs that are generally more compact  generally exponentially so  than those in
terms of which the complexity of the fomdp and pomdp problems are phrased 
probabilistic planning in probabilistic goal oriented planning  as for pomdps  we
typically search for a solution in a space of probability distributions over states  or over
  

fiboutilier  dean    hanks

formulas that describe states   even the simplest problem in probabilistic planning one
that admits no observability is undecidable at worst  madani et al          the intuition
is that even though the set of states is finite  the set of distributions over those states is not 
and at worst the agent may have to search an infinite number of plans before being able
to determine whether or not a solution exists  an algorithm can be guaranteed to find a
solution plan eventually if one exists  but cannot be guaranteed to terminate in finite time
if there is no solution plan  conditional probabilistic planning is a generalization of the
non observable probabilistic planning problem  and thus is undecidable as well 
it is interesting to note a connection between conditional probabilistic planning and
pomdps  the actions and observations of the two problems have equivalent expressive
power  but the reward structure of the conditional probabilistic planning problem is quite
restrictive  goal states have positive rewards  all other states have no reward  and goal states
are absorbing  since we cannot put an a priori bound on the length of a solution plan 
conditional probabilistic planning must be viewed as an infinite horizon problem where the
objective is to maximize total expected undiscounted reward  note  however  that since goal
states are absorbing  we can guarantee that total expected reward will be non negative and
bounded  even over an infinite horizon  technically this means that the conditional probabilistic planning problem is a restricted case of an infinite horizon positive bounded problem
 puterman        section       we can therefore conclude that the problem of solving an
arbitrary infinite horizon undiscounted positive bounded pomdp is also undecidable  the
more commonly studied problem is the infinite horizon pomdp with a criterion of maximizing expected discounted total reward  finding optimal or near optimal solutions to that
problem is also undecidable  as noted above 
       conclusion

we end this section by noting again that these results are algorithm independent and describe worst case behavior  in effect  they indicate how badly any algorithm can be made to
perform on an  arbitrarily unfortunate  problem instance  the more interesting question
is whether we can build representations  techniques  and algorithms that typically perform
well on problem instances that typically arise in practice  this concern leads us to examine
the problem characteristics with an eye toward exploiting the restrictions placed on the
states and actions  on observability  and on the value function and optimality criterion  we
begin with algorithmic techniques that focus on the value function particularly those that
take advantage of time separability and goal orientation  then in the following section we
explore complementary techniques for building compact problem representations 

   solution algorithms  dynamic programming and search
in this section we review standard algorithms for solving the problems described above in
terms of the  unstructured  or  at  problem representations  as noted in the analysis
above  fully observable markov decision processes  fomdps  are by far the most widely
studied models in this general class of stochastic sequential decision problems  we begin by
describing techniques for solving fomdps  focusing on techniques that exploit structure in
the value function like time separability and additivity 
  

fidecision theoretic planning  structural assumptions

    dynamic programming approaches

suppose we are given a fully observable mdp with a time separable  additive value function 
in other words  we are given the state space s   action space a  a transition matrix pr s  js  a 
for each action a  a reward function r  and a cost function c   we start with the problem
of finding the policy that maximizes expected total reward for some fixed  finite horizon t  
suppose we are given a policy  such that  s  t  is the action to be performed by the agent
in state s with t stages remaining to act  for    t  t      bellman        shows that the
expected value of such a policy at any state can be computed using the set of t stage to go
value functions vt   we define v   s  to be r s   then define 

vt  s    r s    c   s  t    

x

s  s
 

fpr s j s  t   s vt    s  g

   

this definition of the value function for  makes its  dependence on the initial state clear 
we say a policy  is optimal if vt  s   vt  s  for all policies   and all s   s  
the optimal t  stage to go value function  denoted vt   is simply the value function of any
optimal t  horizon policy  bellman s principle of optimality  bellman        forms the basis
of the stochastic dynamic programming algorithms used to solve mdps  establishing the
following relationship between the optimal value function at tth stage and the optimal value
function at the previous stage 

vt  s    r s    max
fc  a   
a a

x

s   s

pr s  ja  s vt    s   g

   

      value iteration

equation   forms the basis of the value iteration algorithm for finite horizon problems 
value iteration begins with the value function v    r  and uses equation   to compute in
sequence the value functions for longer time intervals  up to the horizon t   any action that
maximizes the right hand side of equation   can be chosen as the policy element  s  t  
the resulting policy is optimal for the t  stage  fully observable mdp  and indeed for any
shorter horizon t   t  
it is important to note that a policy describes what should be done at every stage and
for every state of the system  even if the agent cannot reach certain states given the system s
initial configuration and its available actions  we return to this point below 

example     consider a simplified version of the robot example  in which we have four

state variables m   cr  rhc  and rhm  movement to various locations is ignored  
and four actions getc  pum  delc  and delm  actions getc and pum make rhc
and rhm  respectively  true with certainty  action delm  when rhm holds  makes
both m and rhm false with probability      delc makes both cr and rhc false with
probability      leaving the state unchanged with probability      a reward of   is
associated with cr and a reward of   is associated with m   the reward for any state
is the sum of the rewards for each objective satisfied in that state  figure    shows
the optimal   stage    stage and   stage value functions for various states  along with

    recall that for fomdps other aspects of history are not relevant 

  

fiboutilier  dean    hanks

state
s    hm  rhm  cr  rhci
s    hm  rhm  cr  rhci
s    hm  rhm  cr  rhci
s    hm  rhm  cr  rhci
s    hm  cr  rhci
s    hm  cr  rhci
s    hm  rhm  cri
s    hm  rhm  cri
s    hm  cri

v 

 
 
 
 
 
 
 
 
 

v 

 
 
   
 
   
 
 
 
 

    

    
any
  pum
delm   delm
delc      delc
delm     delm
delc      delc
any     getc
delm    delm
any    pum
any    any
v 

figure     finite horizon optimal value and policy 
the optimal choice of action at each state stage pairing  the values for any  state 
with missing variables hold for all instantiations of those variables   note that v   s 
is simply r s  for each state s 
to illustrate the application of equation    first consider the calculation of v   s    
the robot has the choice of delivering coffee or delivering mail  and the expected value
of each option  with one stage remaining  is given by 
ev   s   delc       v  s        v   s        
ev  s   delm   
   v   s   
     
thus   s         delm and v   s    is the value of this maximizing choice  notice that
the robot with one action to perform will aim for the  lesser  objective m due to the
risk of failure inherent in delivering coffee  with two stages remaining at the same
state  the robot will again deliver mail  but with certainty will move to s  with one
stage to go  where it will attempt to deliver coffee    s         delc  
to illustrate the effects a fixed finite horizon can have on policy choice  note that
  s        pum  with two stages remaining and the choice of getting mail or coffee 
the robot will get mail because subsequent delivery  in the last stage  is guaranteed
to succeed  whereas subsequent coffee delivery may fail  however  if we compute
  s       we see 
ev  s    getc       v  s          
ev   s   pum       v  s         
with three stages to go  the robot will instead retrieve coffee at s    once it has
coffee  it has two chances at successful delivery  the expected value of this course
of action is greater than that of  guaranteed  mail delivery  note that three stages
does not allow sucient time to try to achieve both objectives at s    in fact  the
larger reward associated with coffee delivery ensures that with any greater number of
stages remaining  the robot should focus first on coffee retrieval and delivery  and then
attempt mail retrieval and delivery once coffee delivery is successfully completed   
often we are faced with tasks that do not have a fixed finite horizon  for example  we
may want our robot to perform the tasks of keeping the lab tidy  picking up mail whenever it
  

fidecision theoretic planning  structural assumptions

arrives  responding to coffee requests  and so on  there is no fixed time horizon associated
with these tasks they are to be performed as the need arises  such problems are best
modeled as infinite horizon problems 
we consider here the problem of building a policy that maximizes the discounted sum
of expected rewards over an infinite horizon    howard        showed that there always
exists an optimal stationary policy for such problems  intuitively  this is the case because
no matter what stage the process is in  there are still an infinite number of stages remaining 
so the optimal action at any state is independent of the stage  we can therefore restrict
our attention to policies that choose the same action for a state regardless of the stage of
the process  under this restriction  the policy will have the same size jsj regardless of the
number of stages over which the policy is executed the policy  has the form    s   a 
in contrast  optimal policies for finite horizon problems are generally nonstationary  as
illustrated in example     
howard also shows that the value of policy  satisfies the following recurrence 

v   s    r s    fc   s     

x

s   s

and that the optimal value function v  satisfies 

v  s    r s    max
fc  a    
a a

pr s  j s   s v   s   g

x
s  s
 

pr s  ja  s v   s   g

   
   

the value of a fixed policy  can be evaluated using the method of successive approximation  which is almost identical to the procedure described in equation   above  we begin
with an arbitrary assignment of values to v   s   then define 

vt  s    r s    c   s  t     

x

s  s
 

fpr s j s  t   s vt    s  g

   

the sequence of functions vt converges linearly to the true value function v   
one can also alter the value iteration algorithm slightly so it builds optimal policies for
infinite horizon discounted problems  the algorithm starts with a value function v  that
assigns an arbitrary value to each s   s   given value estimate vt  s  for each state s  vt    s 
is calculated as 

vt    s    r s    max
fc  a    
a a

x

s   s

pr s  ja  s   vt  s   g

   

the sequence of functions vt converges linearly to the optimal value function v   s   after
some finite number of iterations n  the choice of maximizing action for each s forms an
optimal policy  and vn approximates its value   
    this is by far the most commonly studied problem in the literature  though it is argued in  boutilier  
puterman        mahadevan        schwartz        that such problems are often best modeled using
average reward per stage as the optimality criterion  for a discussion of average reward optimality and
its many variants and refinements  see  puterman        
    the number of iterations n is based on a stopping criterion that generally involves measuring the difference between vt and vt     for a discussion of stopping criteria and convergence of the algorithm  see
 puterman        

  

fiboutilier  dean    hanks

      policy iteration

howard s        policy iteration algorithm is an alternative to value iteration for infinitehorizon problems  rather than iteratively improving the estimated value function  it instead
modifies the policies directly  it begins with an arbitrary policy     then iterates  computing
i   from i 
each iteration of the algorithm comprises two steps  policy evaluation and policy improvement 
    policy evaluation  for each s   s   compute the value function v i  s  based on the
current policy i  
    policy improvement  for each s   s   find the action a that maximizes

qi   a  s    r s    c  a    

x

s   s

pr s  ja  s   v i  s   

   

if qi    a   s    v i  s   let i     a   otherwise i    s    i  s    
the algorithm iterates until i    s    i  s  for all states s  step   evaluates the current
policy by solving the n  n linear system represented by equation    one equation for
each s   s    and can be computationally expensive  however  the algorithm converges to
an optimal policy at least linearly and under certain conditions converges superlinearly or
quadratically  puterman         in practice  policy iteration tends to converge in many
fewer iterations than does value iteration  policy iteration thus spends more computational
time at each individual stage  with the result that fewer stages need be computed   
modified policy iteration  puterman   shin        provides a middle ground between
policy iteration and value iteration  the structure of the algorithm is exactly the same as
that of policy iteration  alternating evaluation and improvement phases  the key insight is
that one need not evaluate a policy exactly in order to improve it  therefore  the evaluation
phase involves some  usually small  number of iterations of successive approximation  i e  
setting v    vt for some small t  using equation     with some tuning of the value
of t used at each iteration  modified policy iteration can work extremely well in practice
 puterman         both value iteration and policy iteration are special cases of modified
policy iteration  corresponding to setting t     and t      respectively 
a number of other variants of both value and policy iteration have been proposed  for
instance  asynchronous versions of these algorithms do not require that the value function
be constructed  or policy improved  at each state in lockstep  in the case of value iteration
for infinite horizon problems  as long as each state is updated suciently often  convergence
can be assured  similar guarantees can be provided for asynchronous forms of policy iteration  such variants are important tools for understanding various online approaches to
solving mdps  bertsekas   tsitsiklis         for a nice discussion of asynchronous dynamic
programming  see  bertsekas        bertsekas   tsitsiklis        
    the q function defined by equation    and so called because of its use in q learning  watkins   dayan 
       gives the value of performing action a at state s assuming the value function v  accurately reects
future value 
    see  littman et al         for a discussion of the complexity of the algorithm 

  

fidecision theoretic planning  structural assumptions

      undiscounted infinite horizon problems

the diculty with finding optimal solutions to infinite horizon problems is that total reward
can grow without limit over time  thus  the problem definition must provide some way to
ensure that the value metric is bounded over arbitrarily long horizons  the use of expected
total discounted reward as the optimality criterion offers a particularly elegant way to
guarantee a bound  since the infinite sum of discounted rewards is finite  however  although
discounting is appropriate for certain classes of problems  e g   economic problems  or those
where the system may terminate at any point with a certain probability   for many realistic
ai domains it is dicult to justify counting future rewards less than present rewards  and
the discounted reward criterion is not appropriate 
there are a variety of ways to bound total reward in undiscounted problems  in some
cases the problem itself is structured so that reward is bounded  in planning problems  for
example  the goal reward can be collected at most once  and all actions incur a cost  in
that case total reward is bounded from above and the problem can legitimately be posed
in terms of maximizing total expected undiscounted reward in many cases  e g   if the goal
can be reached with certainty  
in cases where discounting is inappropriate and total reward is unbounded  different
success criteria can be employed  for example  the problem can instead be posed as one
in which we wish to maximize expected average reward per stage  or gain  computational
techniques for constructing gain optimal policies are similar to the dynamic programming
algorithms described above  but are generally more complicated  and the convergence rate
tends to be quite sensitive to the communicating structure and periodicity of the mdp 
refinements to gain optimality have also been studied  for example  bias optimality can
be used to distinguish two gain optimal polices by giving preference to the policy whose total
reward over some initial segment of policy execution is larger  again  while the algorithms
are more complicated than those for discounted problems  they are variants of standard
policy or value iteration  see  puterman        for details 
      dynamic programming and pomdps

dynamic programming techniques can be applied in partially observable settings as well
 smallwood   sondik         the main diculty in building policies for situations in which
the state is not fully observable is that  since past observations can provide information
about the system s current state  decisions must be based on information gleaned in the
past  as a result  the optimal policy can depend on all observations the agent has made since
the beginning of execution  these history dependent policies can grow in size exponential
in the length of the horizon  while history dependence precludes dynamic programming 
the observable history can be summarized adequately with a probability distribution over s
 astrom         and policies can be computed as a function of these distributions  or belief
states 
a key observation of sondik  smallwood   sondik        sondik        is that when
one views a pomdp with a time separable value function by taking the state space to be
the set of probability distributions over s   one obtains a fully observable mdp that can
be solved by dynamic programming  the  computational  problem with this approach is
  

fiboutilier  dean    hanks

that the state space for this fomdp is an n  dimensional continuous space    and special
techniques must be used to solve it  smallwood   sondik        sondik        
we do not explore these techniques here  but note that they are currently practical
only for very small problems  cassandra et al         cassandra  littman    zhang       
littman        lovejoy      b   a number of approximation methods  developed both in
or  lovejoy      a  white iii   scherer        and ai  brafman        hauskrecht       
parr   russell        zhang   liu         can be used to increase the range of solvable
problems  but even these techniques are presently of limited practical value 
pomdps play a key role in reinforcement learning as well  where the  natural state
space  consisting of agent observations provides incomplete information about the underlying system state  see  e g   mccallum        

    ai planning and state based search
we noted in section     that the classical ai planning problem can be formulated as an
infinite horizon mdp and can therefore be solved using an algorithm like value iteration 
recall that two assumptions in classical planning specialize the general mdp model  namely
determinism of actions and the use of goal states instead of a more general reward function 
a third assumption that we want to construct an optimal course of action starting from a
known initial state does not have a counterpart in the fomdp model as presented above 
since the policy dictates the optimal action from any state at any stage of the plan  as we
will see below  the interest in online algorithms within ai has led to revised formulations
of fomdps that do take initial and current states into account 
though we defined the classical planning problem earlier as a non observable process
 nomdp   it can be solved as if it were fully observable  we let g be the set of goal states
and sinit be the initial state  applying value iteration to this type of problem is equivalent
to determining the reachability of goal states from all system states  for instance  if we
make goal states absorbing  assign a reward of   to all transitions from any s   s   g
to some g   g and   to all others  then the set of all states where vk  s      is exactly
the set of states that can lead to a goal state    in particular  if vk  sinit        then a
successful plan can be constructed by extracting actions from the k stage  finite horizon 
policy produced by value iteration  the determinism assumption means that the agent can
predict the state perfectly at every stage of execution  the fact that it cannot observe the
state is unimportant 
the assumptions commonly made in classical planning can be exploited computationally in value iteration  first  we can terminate the process at the first iteration k where
vk  sinit       because we are interested only in plans that begin at sinit  not in acting
optimally from every possible start state  second  we can terminate value iteration after js j
iterations  if vjs j sinit       at that point  the algorithm will have searched every possible
state and can guarantee that no solution plan exists  therefore  we can view classical planning as a finite horizon decision problem with a horizon of js j  this use of value iteration
    more accurately  it is an n  dimensional simplex  or  n      dimensional space 
    specifically  vk  s  indicates the probability with which one reaches the goal region under the optimal
policy from s   s   g in stochastic settings  in the deterministic case being discussed  this value must
be   or   

  

fidecision theoretic planning  structural assumptions

is equivalent to using the floyd warshall algorithm to find a minimum cost path through
a weighted graph  floyd        
      planning and search

while value iteration can  in theory  be used for classical planning  it does not take advantage
of the fact that the goal and initial states are known  in particular  it computes the value
and policy assignment for all states at all stages  this can be very wasteful since optimal
actions will be computed for states that cannot be reached from sinit or that cannot possibly
lead to any state g   g  it is also problematic when js j is large  since each iteration of
value iteration requires o js jjaj  computations  for this reason dynamic programming
approaches have not been used extensively in ai planning 
the restricted form of the value function  especially the fact that initial and goal states
are given  makes it more advantageous to view planning as a graph search problem  unlike
general fomdps  where it is generally not known a priori which states are most desirable
with respect to  long term  value  the well defined set of target states in a classical planning
problem makes search based algorithms appropriate  this is the approach taken by most
ai planning algorithms 
one way to formulate the problem as a graph search is to make each node of the graph
correspond to a state in s   the initial state and goal states can then be identified  and
the search can proceed either forward or backward through the graph  or in both directions
simultaneously 
in forward search  the initial state is the root of the search tree  a node is then chosen
from the tree s fringe  the set of all leaf nodes   and all feasible actions are applied  each
action application extends the plan by one step  or one stage  and generates a unique new
successor state  which is a new leaf node in the tree  this node can be pruned if the state it
defines is already in the tree  the search ends when a state is identified as a member of the
goal set  in which case a solution plan can be extracted from the tree   or when all branches
have been pruned  in which case no solution plan exists   forward search attempts to build
a plan from beginning to end  adding actions to the end of the current sequence of actions 
forward search never considers states that cannot be reached from the sinit  
backward search can be viewed in several different ways  we could arbitrarily select
some g   g as the root of the search tree  and expand the search tree at the fringe by
selecting a state on the fringe and adding to the tree all states such that some action would
cause the system to enter the chosen state  in general  an action can give rise to more than
one predecessor vertex  even if actions are deterministic  a state can again be pruned if it
appears in the search tree already  the search terminates when the initial state is added to
the tree  and a solution plan can again be extracted from the tree  this search is similar
to dynamic programming based algorithms for finding the shortest path through a graph 
the difference is that backward search considers only those states at a depth k in the search
tree that can reach the chosen goal state within k steps  dynamic programming algorithms 
in contrast  visit every state at every stage of the search 
one diculty with the backward approach as described above is the commitment to
a particular goal state  of course  this assumption can be relaxed  as an algorithm could
 simultaneously  search for paths to all goal states by adding at the first level of the search
  

fiboutilier  dean    hanks

tree any vertex that can reach some g   g  we will see in section   that goal regression
can be viewed as doing this  at least implicitly 
it is generally thought that regression  or backward  techniques are more effective in
practice than progression  or forward  methods  the reasoning is that the branching factor
in the forward graph  which is the number of actions that can feasibly be applied in a given
state  is substantially larger than the branching factor in the reverse graph  which is the
number of operators that could bring the system into a given state    this is especially true
when goal sets are represented by a small set of propositional literals  section     the two
approaches are not mutually exclusive  however  one can mix forward and backward expansions of the underlying problem graph and terminate when a forward path and backward
path meet 
the important thing to observe about these algorithms is that they restrict their attention to the relevant and reachable states  in forward search  only those states that can be
reached from sinit are ever considered  this can provide benefit over dynamic programming
methods if few states are reachable  since unreachable states cannot play a role in constructing a successful plan  in backward approaches  similarly  only states lying on some path
to the goal region g are considered  and this can have significant advantages over dynamic
programming if only a fraction of the state space is connected to the goal region 
in contrast  dynamic programming methods  with the exception of asynchronous methods  must examine the entire state space at every iteration  of course  the ability to ignore
parts of the state space comes from planning s stringent definition of what is relevant  states
in g have positive reward  no other states matter except to the extent they move the agent
closer to the goal  and the choice of action at states unreachable from sinit is not of interest 
while state based search techniques use knowledge of a specific initial state and a specific
goal set to constrain the search process  forward search does not exploit knowledge of the
goal set  nor does backward search exploit knowledge of the initial state  the graphplan
algorithm  blum   furst        can be viewed as a planning method that integrates the
propagation of forward reachability constraints with backward goal informed search  we
describe this approach in section    furthermore  work on partial order planning  pop 
can be viewed as a slightly different approach to this form of search  it too is described
in section    after we discuss feature based or intensional representations for mdps and
planning problems 
      decision trees and real time dynamic programming

state based search techniques are not limited to deterministic  goal oriented domains  knowledge of the initial state can be exploited in more general mdps as well  forming the basis of
decision tree search algorithms  assume we have been given a finite horizon fomdp with
horizon t and initial state sinit   a decision tree rooted at sinit is constructed in much the
same way as a search tree for a deterministic planning problem  french         each action
applicable at sinit forms level   of the tree  the states s  that result with positive probability when any of those actions occur are applied at sinit are placed at level    with an arc
    see bacchus et al               for some recent work that makes the case for progression with good
search control  and bonet et al         who argue that progression in deterministic planning is useful
when integrating planning and execution 

  

fidecision theoretic planning  structural assumptions

s init
v   max v    v   
a 
p 

a 
p 

s 
a 

p 

s 
a 

a 

v 
p 

s 
a 

a 

v    p v     p v 
 
 

s 
a 

a 

a 

v 

v 

figure     the initial stages of a decision tree for evaluating action choices at sinit   the
value of an action is the expected value of its successor states  while the value
of a state is the maximum of the values of its successor actions  as indicated by
dashed arrows at selected nodes  
labeled with probability pr s  ja  sinit   relating s  with a  level   has the actions applicable
at the states at level    and so on  until the tree is grown to depth  t   at which point each
branch of the tree is a path consisting of a positive probability length t trajectory rooted
at sinit  see figure     
the relevant part of the optimal t  stage value function and the optimal policy can easily
be computed using this tree  we say that value of any node in the tree labeled with an
action is the expected value of its successor states in the tree  using the probabilities labeling
the arcs   while the value of any node in the tree labeled with state s is the sum of r s  and
the maximum value of its successor actions    the rollback procedure  whereby value at the
leaves of the tree are first computed and then values at successively higher levels of the tree
are determined using the preceding values  is  in fact  a form of value iteration  the value
of any state s at level  t is precisely vt t  s  and the maximizing actions form the optimal
finite horizon policy  this form of value iteration is directed   t   t  stage to go values
are computed only for states that are reachable from sinit within t steps  infinite horizon
problems can be solved in an analogous fashion if one can determine a priori the depth
required  i e   the number of iterations of value iteration needed  to ensure convergence to
an optimal policy 
unfortunately  the branching factor for stochastic problems is generally much greater
than that for deterministic problems  more troublesome still is the fact that one must
construct the entire decision tree to be sure that the proper values are computed  and hence
the optimal policy constructed  this stands in contrast with classical planning search 
where attention can be focused on a single branch  if a goal state is reached  the path
constructed determines a satisfactory plan  while worst case behavior for planning may
require searching the whole tree  decision tree evaluation is especially problematic because
    states at level  t are given value r s  

  

fiboutilier  dean    hanks

the entire tree must be generated in general to ensure optimal behavior  furthermore 
infinite horizon problems pose the diculty of determining a suciently deep tree 
one way around this diculty is the use of real time search  korf         in particular 
real time dynamic programming  or rtdp  has been proposed in  barto et al         as a
way of approximately solving large mdps in an online fashion  one can interleave search
with execution of an approximately optimal policy using a form of rtdp similar to decisiontree evaluation as follows  imagine the agent finds itself in a particular state sinit   it can
then build a partial search tree to some depth  perhaps uniformly or perhaps with some
branches expanded more deeply than others  partial tree construction may be halted due to
time pressure or due to an assessment by the agent that further expansion of the tree may
not be fruitful  when a decision to act must be made  the rollback procedure is applied to
this partial  possibly unevenly expanded decision tree 
reward values can be used to evaluate the leaves of the tree  but this may offer an
inaccurate picture of the value of nodes higher in the tree  heuristic information can be
used to estimate the long term value of states labeling leaves  as with value iteration  the
deeper the tree  the more accurate the estimated value at the root  generally speaking 
for a fixed heuristic  we will see in section   that structured representations of mdps can
provide a means to construct such heuristics  dearden   boutilier               specifically 
with admissible heuristics or upper and lower bounds on the true values of leaf nodes in the
tree  methods such as a  or branch and bound search can be used 
a key advantage of integrating search with execution is that the actual outcome of the
action taken can be used to prune from the tree the branches rooted at the unrealized
outcomes  the subtree rooted at the realized state can then be expanded further to make
the next action choice  the algorithm of hansen and zilberstein        can be viewed as
a variant of these methods in which stationary policies  i e   state action mappings  can be
extracted during the search process 
rtdp is formulated by barto et al         more generally as a form of online  asynchronous value iteration  specifically  the values  rolled backed  can be cached and used
as improved heuristic estimates of the value function at the states in question  this technique is also investigated in  bonet et al         dearden   boutilier              koenig
  simmons         and is closely tied to korf s        lrta  algorithm  these value
updates also need not proceed strictly using a decision tree to determine the states  the key
requirement of rtdp is simply that the actual state sinit be one of the states whose value
is updated at each decision action iteration 
a second way to avoid some of the computational diculties that arise in large search
spaces is to use sampling methods  these methods sample from the space of possible trajectories and use this sampled information to provide estimates of the values of specific courses
of action  this approach is quite common in reinforcement learning  sutton   barto        
where simulation models are often used to generate experience from which a value function
can be learned  in the present context  kearns  mansour and ng  kearns  mansour   
ng        have investigated search methods for infinite horizon mdps where the successor
states of any specific action are randomly sampled according to the transition distribution 
thus  rather than expand all successor states  only sampled states are searched  though
this method is exponential in the  effective  horizon  or mixing rate  of the mdp and is
required to expand all actions  the number of states expanded can be less than that required
  

fidecision theoretic planning  structural assumptions

by full search  even if the underlying transition graph is not sparse  they are able to provide polynomial bounds  ignoring action branching and horizon effects  on the number of
trajectories that need to be sampled in order to generate approximately optimal behavior
with high probability 

    summary

we have seen that dynamic programming methods and state based search methods can
both be used for fully observable and non observable mdps  with forward search methods interpretable as  directed  forms of value iteration  dynamic programming algorithms
generally require explicit enumeration of the state space at each iteration  while search
techniques enumerate only reachable states  but the branching factor may require that 
at sucient depth in the search tree  search methods enumerate individual states multiple
times  whereas they are considered only once per stage in dynamic programming  overcoming this diculty in search requires the use of cycle checking and multiple path checking
methods 
we note that search techniques can be applied to partially observable problems as well 
one way to do this is to search through the space of belief states  just as dynamic programming can be applied to the belief space mdp see section          specifically  belief
states play the role of system states and the stochastic effects of actions on belief states are
induced by specific observation probabilities  since each observation has a distinct  but fixed
effect on any belief state  this type of approach has been pursued in  bonet   geffner 
      koenig   simmons        

   factored representations

to this point our discussion of mdps has used an explicit or extensional representation for
the set of states  and actions  in which states are enumerated directly  in many cases it
is advantageous  from both the representational and computational point of view  to talk
about properties of states or sets of states  the set of possible initial states  the set of
states in which action a can be executed  and so on  it is generally more convenient and
compact to describe sets of states based on certain properties or features than to enumerate
them explicitly  representations in which descriptions of objects substitute for the objects
themselves are called intensional and are the technique of choice in ai systems 
an intensional representation for planning systems is often built by defining a set of
features that are sucient to describe the state of the dynamic system of interest  in the
example in figure    the state was described by a set of six features  the robot s location  the
lab s tidiness  whether or not mail is present  whether or not the robot has mail  whether or
not there is a pending coffee request  and whether or not the robot has coffee  the first and
second features can each take one of five values  and the last four can each take one of two
values  true or false   an assignment of values to the six features completely defines a state 
the state space thus comprises all possible combinations of feature values  with jsj       
each feature  or factor  is typically assigned a unique symbolic name  as indicated in the
second column of figure    the fundamental tradeoff between extensional and intensional
representations becomes clear in this example  an extensional representation of the coffee
example views the space of possible states as a single variable that takes on     possible
  

fiboutilier  dean    hanks

values  whereas the intensional or factored representation views a state as the cross product
of six variables  each of which takes on substantially fewer values  generally  the state space
grows exponentially in the number of features required to describe a system 
the fact that the state of a system can be described using a set of features allows one
to adopt factored representations of actions  rewards and other components of an mdp  in
a factored action representation  for instance  one generally describes the effect of an action
on specific state features rather than on entire states  this often provides considerable representational economy  for instance  in the strips action representation  fikes   nilsson 
       the state transitions induced by actions are represented implicitly by describing the
effects of actions on only those features that change value when the action is executed 
factored representations can be very compact when individual actions affect relatively few
features  or when their effects exhibit certain regularities  similar remarks apply to the
representation of reward functions  observation models  and so on  the regularities that
make factored representations suitable for many planning problems can often be exploited
by planning and decision making algorithms 
while factored representations have long been used in classical ai planning  similar
representations have also been adopted in the recent use of mdp models within ai  in
this section  section     we focus on the economy of representation afforded by exploiting
the structure inherent in many planning domains  in the following section  section     we
describe how this structure when made explicit by the factored representations can be
exploited computationally in plan and policy construction 

    factored state spaces and markov chains

we begin by examining structured states  or systems whose state can be described using a
finite set of state variables whose values change over time    to simplify our illustration of
the potential space savings  we assume that these state variables are boolean  if there are
m such variables  then the size of the state space is jsj   n    m   for large m   specifying
or representing the dynamics explicitly using state transition diagrams or n  n matrices
is impractical  furthermore  representing a reward function as an n  vector  and specifying
the observational probabilities  is similarly infeasible  in section      we define a class of
problems in which the dynamics can be represented in o m   space in many cases  we begin
by considering how to represent markov chains compactly and then consider incorporating
actions  observations and rewards 
we let a state variable x take on a finite number of values and let 
x stand for the
set of possible values  we assume that 
x is finite  though much of what follows can be
applied to countable state and action spaces as well  we say the state space is at if it is
specified using one state variable  this variable is denoted s as in the general model  taking
values from s    the state space is factored if there is more than one state variable  a state
is any possible assignment of values to these variables  letting xi represent the ith state
variable  the state space is the cross product of the value spaces for the individual state
t
variables  that is  s   m
i   
xi   just as s denotes the state of the process at stage t  we
let xit be the random variable representing the value of the ith state variable at stage t 
    these variables are often called uents in the ai literature  mccarthy   hayes         in classical
planning  these are the atomic propositions used to describe the domain 

  

fidecision theoretic planning  structural assumptions

a bayesian network  pearl        is a representational framework for compactly representing a probability distribution in factored form  although these networks have most typically been used to represent atemporal problem domains  we can apply the same techniques
to represent markov chains  encoding the chain s transition probabilities in the network
structure  dean   kanazawa        
formally  a bayes net is a directed acyclic graph in which vertices correspond to random
variables and an edge between two variables indicates a direct probabilistic dependency
between them  a network so constructed also reects implicit independencies among the
variables  the network must be quantified by specifying a probability for each variable
 vertex  conditioned on all possible values of its immediate parents in the graph  in addition 
the network must include a marginal distribution  an unconditional probability for each
vertex that has no parents  this quantification is captured by associating a conditional
probability table  cpt  with each variable in the network  together with the independence
assumptions defined by the graph  this quantification defines a unique joint distribution
over the variables in the network  the probability of any event over this space can then be
computed using algorithms that exploit the independencies represented within the graphical
structure  we refer to pearl        for details 
figures   a   c   page    are special cases of bayes nets called  temporal  bayesian
networks  in these networks  vertices in the graph represent the system s state at different
time points and arcs represent dependencies across time points  in these temporal networks 
each vertex s parent is its temporal predecessor  the conditional distributions are transition
probability distributions  and the marginal distributions are distributions over initial states 
the networks in figure   reect an extensional representation scheme in which states are
explicitly enumerated  but techniques for building and performing inference in probabilistic temporal networks are designed especially for application to factored representations 
figure    illustrates a two stage temporal bayes net   tbn  describing the state transition
probabilities associated with the markov chain induced by the fixed policy of executing
the action cclk  repeatedly moving counterclockwise   in a  tbn  the set of variables is
partitioned into those corresponding to state variables at a given time  or stage  t and those
corresponding to state variables at time t      directed arcs indicate probabilistic dependencies between those variables in the markov chain  diachronic arcs are those directed
from time t variables to time t     variables  while synchronic arcs are directed between
variables at time t      figure    contains only diachronic arcs  synchronic arcs will be
discussed later in this section 
given any state at time t  the network induces a unique distribution over states at t    
the quantification of the network describes how the state of any particular variable changes
as a function of certain state variables  the lack of a direct arc  or more generally a directed
path if there are synchronic arcs among the t     variables  from a variable xt to another
variable yt   means that knowledge of xt is irrelevant to the prediction of the  immediate 
or one stage  evolution of variable y in the markov process 
figure    shows how compact this representation can be in the best of circumstances  as
many of the potential links between one stage and the next can be omitted  the graphical
representation makes explicit the fact that the policy  i e   the action cclk  can affect only
the state variable loc  and the exogenous events arrm  reqc  and mess can affect only
  

fiboutilier  dean    hanks

loc

loc

t

t

cr

cr

p loc t    
loc t o l c m h
o              
             
l
             
c
m              
h              
p cr t   
cr t
t
f

rhc

t f
     
       

rhc

rhm

rhm

m

m

time t

time t  

p rhc t   
rhc t t f
t      
f
     

figure     a factored  tbn for the markov chain induced by moving counterclockwise
 with selected cpts shown  
the variables m   cr  and tidy  respectively    furthermore  the dynamics of loc  and
the other variables  can be described using only knowledge of the state of their parent
variables  for instance  the distribution over loc at t    depends only on the value of loc at
the previous stage  e g   if loct   o  then loct     m with probability     and loct     o
with probability       similarly  cr can become true with probability      due to a reqc
event   but once true  cannot become false  under this simple policy   and rhc remains
true  or false  with certainty if it was true  or false  at the previous stage  finally  the
effects on the relevant variables are independent  for any instantiation of the variables at
time t  the distribution over next states can be computed by multiplying the conditional
probabilities of relevant t     variables 
the ability to omit arcs from the graph based on the locality and independence of action
effects has a strong effect on the number of parameters that must be supplied to complete
the model  although the full transition matrix for cclk would be of size               
the transition model in figure    requires only    parameters   
the example above shows how  tbns exploit independence to represent markov chains
compactly  but the example is extreme in that there is effectively no relationship between the
variables the chain can be viewed as the product of six independently evolving processes 
    we show only some of the cpts for brevity 
    in fact  we can exploit the fact that probabilities sum to one and leave one entry unspecified per row of
any cpt or explicit transition matrix  in this case  the  tbn requires only    explicit parameters  while
the transition matrix requires                     entries  we generally ignore this fact when comparing
the sizes of representations 

  

fidecision theoretic planning  structural assumptions

loc

loc

t

t

cr

cr

rhc

rhc

rhm

rhm

m

m

time t

time t  

loc t rhc t
o
t
l
t
c
t
m
t
h
t
o
f
l
f
c
f
m
f
h
f

p loc t    
o l c m h
           
             
             
             
             
             
             
             
             
p cr t   
             
loc t rhc t cr t t f

p rhc t   
loc t rhc t
t f
t        
o
f        
o
t        
l
f        
l
t        
c
f        
c
etc 
etc 

o
o
o
o
l
l
l
l

t
t
f
f
t
t
f
f
etc 

t
f
t
f
t
f
t
f

       
       
       
       
       
       
       
       
etc 

figure     a  tbn for the markov chain induced by moving counterclockwise and delivering coffee 

  

fiboutilier  dean    hanks

in general  these  subprocesses  will interact  but still exhibit certain independencies and
regularities that can be exploited by a  tbn representation  we consider two distinct
markov chains that exhibit different types of dependencies 
figure    illustrates a  tbn representing the markov chain induced by the following
policy  the robot consistently moves counterclockwise unless it is in the oce and has
coffee  in which case it delivers coffee to the user  notice that different variables are now
dependent  for instance  predicting the value of rhc at t     requires knowing the values
of loc and rhc at t  the cpt for rhc shows that the robot retains coffee at stage t    
with certainty  if it has it at stage t  in all locations except o  where it executes delc 
thus losing the coffee   the variable loc also depends on the value of rhc  the location
will change as in figure    with one exception  if the robot is in the oce with coffee  the
location remains the same  since the robot does not move  but executes delc   the effect
on the variable cr is explained as follows  if the robot is in the oce and delivers coffee in
its possession  it will fulfill any outstanding coffee request  however  the      chance of cr
remaining true under these conditions indicates a    chance of spilling the coffee 
even though there are more dependencies  i e   additional diachronic arcs  in this  tbn 
it still requires only     parameters  again  the distribution over resulting states is determined by multiplying the conditional distributions for the individual variables  even though
the variables are  related   when the state s t is known  the variables at time t      loct    
rhct     etc   are independent  in other words 
pr loct     t t     crt     rhct     rhmt     m t   js t    
t 
pr loct   js t   pr t t   js t   pr crt   js t   pr rhct   js t   pr rhmt   js t   pr m t   js   
figure    illustrates a  tbn representing the markov chain induced by the same policy
as above  but where we assume that the act of moving counterclockwise has a slightly
different effect  we now suppose that  when the robot moves from the hallway into some
adjacent location  it has a     chance of spilling any coffee it has in its possession  the
fragment of the cpt for rhc in figure    illustrates this possibility  furthermore  should
the robot be carrying mail whenever it loses coffee  whether  accidentally  or  intentionally 
via the delc action   there is a     chance it will lose the mail  notice that the effects of this
policy on the variables rhc and rhm are correlated  one cannot accurately predict the
probability of rhmt   without determining the probability of rhct     this correlation is
modeled by the synchronic arc between rhc and rhm at the t     slice of the network 
the independence of the t    variables given s t does not hold in  tbns with synchronic
arcs  determining the probability of a resulting state requires some simple probabilistic
reasoning  for example  application of the chain rule  in this example  we can write
pr rhct     rhmt   js t     pr rhmt   jrhct     s t   pr rhct   js t  
the joint distribution over t     variables given s t can then be computed as in equation   above  with this term replacing the pr rhct   js t   pr rhmt   js t   while these two
variables are correlated  the remaining variables are independent 
we refer to  tbns with no synchronic arcs  like the one in figure     as simple  tbns 
general  tbns allow synchronic as well as diachronic arcs  as in figure    
  

fidecision theoretic planning  structural assumptions

loc

loc

t

t

cr

cr

rhc

rhc

rhm

rhm

m

m

time t

time t  

pr rhc t    
loc t rhc t t f
t        
o
f        
o
t        
h
f        
h
t        
c
f        
c
etc 
etc 
pr rhmt    
rhc t rhc t   rhmt t f
       
t
t
t
       
f
t
t
       
t
f
t
       
f
f
t
       
t
t
f
       
f
t
f
       
t
f
f
       
f
f
f

figure     a  tbn for the markov chain induced by moving counterclockwise and delivering coffee with correlated effects 

    factored action representations

just as we extended markov chains to account for different actions  we must extend the
 tbn representation to account for the fact that the state transitions are inuenced by
the agent s choice of action  we discuss a variety of techniques for specifying the transition
matrices that exploit the factored state representation to produce representations that are
more natural and compact than explicit transition matrices 
      implicit event models

we begin with the implicit event model from section     in which the effects of actions
and exogenous events are combined in a single transition matrix  we will consider explicitevent models in section        as we saw in the previous section  algorithms such as value
and policy iteration require the use of transition models that reect the ultimate transition
probabilities  including the effects of any exogenous events 
one way to model the dynamics of a fully observable mdp is to represent each action
by a separate  tbn  the  tbn shown above in figure    can be seen as a representation
of the action cclk  since the policy inducing the markov chain in that example consists
of the repeated application of that action alone   the network fragment in figure    a 
illustrates the interesting aspects of the  tbn for the delc action including the effects of
exogenous events  as above  the robot satisfies an outstanding coffee request if it delivers
coffee while it is in the oce and has coffee  with a      chance of spillage   as shown in the
conditional probability table for cr  the effect on rhc can be explained as follows  the
  

fiboutilier  dean    hanks

loc

rhc

cr
time t

loc

rhc

cr
time t  

 a 

pr rhc t    
loc t rhc t t f
o
t        
o
f        
l
t        
f        
l
t        
c
f        
c
etc 
etc 
pr cr t    
loc t rhc t cr t t f
t        
t
o
f        
t
o
t        
f
o
f        
f
o
t        
t
l
f        
t
l
t        
f
l
f        
f
l
etc 
etc 

off
t
t

cr

    

rhc

loc

else

f

f

t
t

cr

       

cr

   

f

f

   

   

 b 
off
t
t

cr

    

rhc

f

loc
else

f
f

   

cr

t

   

 c 

figure     a factored  tbn for action delc  a  and structured cpt representations  b c  
robot loses the coffee  to the user or to spillage  if it delivers it in the oce  if it attempts
delivery elsewhere  there is a     chance that a random passerby will take the coffee from
the robot 
as in the case of markov chains  the effects of actions on different variables can be
correlated  in which case we must introduce synchronic arcs  such correlations can be
thought of as ramifications  baker        finger        lin   reiter        
      structured cpts

the conditional probability table  cpt  for the node cr in figure    a  has    rows  one
for each assignment to its parents  however  the cpt contains a number of regularities 
intuitively  this reects the fact that the coffee request will be met successfully  i e   the
variable becomes false      of the time when delc is executed  if the robot has coffee and
is in the right location  the user s oce   otherwise  cr remains true if it was true and
becomes true with probability     if it was not  in other words  there are three distinct cases
to be considered  corresponding to three  rules  governing the  stochastic  effect of delc
on cr  this can be represented more compactly by using a decision tree representation
 with  else  branches to summarize groups of cases involving multivalued variables such
as loc  like that shown in figure    b   or more compactly still using a decision graph
 figure    c    in tree  and graph based representations of cpts  interior nodes are labeled
by parent variables  edges by values of the variables  and leaves or terminals by distributions
over the child variable s values   
decision tree and decision graph representations are used to represent actions in fully
observable mdps in  boutilier et al         hoey  st aubin  hu    boutilier        and
    when the child is boolean  we label the leaves with only the probability of that variable being true  the
probability of the variable being false is one minus this value  

  

fidecision theoretic planning  structural assumptions

are described in detail in  poole        boutilier   goldszmidt           intuitively  trees
and graphs embody the rule like structure present in the family of conditional distributions
represented by the cpt  and in the settings we consider often yield considerable representational compactness  rule based representations have been used directly by poole       
    a  in the context of decision processes and can often be more compact than trees  poole 
    b   we generically refer to representations of this type as  tbns with structured cpts 
      probabilistic strips operators

the  tbn representation can be viewed as oriented toward describing the effects of actions
on distinct variables  the cpt for each variable expresses how it  stochastically  changes
 or persists   perhaps as a function of the state of certain other variables  however  it
has long been noted in ai research on planning and reasoning about action that most
actions change the state in limited ways  that is  they affect a relatively small number of
variables  one diculty with variable oriented representations such as  tbns is that one
must explicitly assert that variables unaffected by a specific action persist in value  e g  
see the cpt for rhc in figure     this is an instance of the infamous frame problem
 mccarthy   hayes        
another form of representation for actions might be called an outcome oriented representation  one explicitly describes the possible outcomes of an action or the possible joint
effects over all variables  this was the idea underlying the strips representation from
classical planning  fikes   nilsson        
a classical strips operator is described by a precondition and a set of effects  the
former identifies the set of states in which the action can be executed  and the latter
describes how the input state changes as a result of taking the action  a probabilistic
strips operator  pso   hanks        hanks   mcdermott        kushmerick et al        
extends the strips representation in two ways  first  it allows actions to have different
effects depending on context  and second  it recognizes that the effects of actions are not
always known with certainty   
formally  a pso consists of a set of mutually exclusive and exhaustive logical formulae 
called contexts  and a stochastic effect associated with each context  intuitively  a context discriminates situations under which an action can have differing stochastic effects 
a stochastic effect is itself a set of change sets a simple list of variable values with a
probability attached to each change set  with the requirement that these probabilities sum
to one  the semantics of a stochastic effect can be described as follows  when the stochastic
effect of an action is applied at state s  the possible resulting states are determined by the
change sets  each occurring with the corresponding probability  the resulting state associated with a change set is constructed by changing variable values at state s to match those
in the change set  while all unmentioned variables persist in value  note that since only one
    the fact that certain direct dependencies among variables in a bayes net are rendered irrelevant under
specific variable assignments has been studied more generally in the guise of context specific independence
 boutilier  friedman  goldszmidt    koller         see  geiger   heckerman        shimony        for
related notions 
    the conditional nature of effects is also a feature of a deterministic extension of strips known as adl
 pednault        

  

fiboutilier  dean    hanks

rhc
t

f

loc
o

 cr  rhc  m
 cr  rhc
 rhc  m
 rhc

 cr  m
 cr
 m
nil

else

 rhc  cr  m
 rhc  cr
 rhc  m
 rhc
 cr  m
 cr
 m
nil

    
    
    
    

    
    
    
    

     
     
     
     
     
     
     
     

figure     a pso representation for the delc action 
loc

o
l

 loc l     
nil
   

 loc c     
nil
   

h

c

m

 loc m     
nil
   

 loc h     
nil
   

 loc o   rhc  rhm
 loc o   rhc
 loc o 
 rhc  rhm
 rhc
nil

     
     
    
     
     
    

figure     a pso representation of a simplified cclk action 
context can hold in any state s  the transition distribution for the action at any state s is
easily determined 
figure    gives a graphical depiction of the pso for the delc action  shown as a  tbn
in figure      the three contexts  rhc  rhc   loc o  and rhc   loc o  are represented
using a decision tree  at the leaf of each branch in the decision tree is the stochastic effect
 set of change sets and associated probabilities  determined by the corresponding context 
for example  when rhc   loc o  holds  the action has four possible effects  the robot loses
the coffee  it may or may not satisfy the coffee request  due to the      chance of spillage  
and mail may or may not arrive  notice that each outcome is spelled out completely  the
number of outcomes in the other two contexts is rather large due to possible exogenous
events  we discuss this further in section          
a key difference between psos and  tbns lies in their treatment of persistence  all
variables that are unaffected by an action must be given cpts in the  tbn model  while
such variables are not mentioned at all in the pso model  e g   compare the variable loc in
both representations of delc   in this way  psos can be said to  solve  the frame problem 
since unaffected variables need not be mentioned in an action s description   
    to keep figure    manageable  we ignore the effect of the exogenous event mess on variable t  
    for a discussion of the frame problem in  tbns  see  boutilier   goldszmidt        

  

fidecision theoretic planning  structural assumptions

arrm

mess

loc

loc

loc

loc

rhc

rhc

rhc

rhc

rhm

rhm

rhm

rhm

cr

cr

cr

cr

m

m

m

m

t

t

t

t

t

t   

t   

t  

figure     an simplified explicit event model for delc 
psos can provide an effective means for representing actions with correlated effects 
recall the description of the cclk action captured in figure     where the robot may
drop its coffee as it moves from the hallway  and may drop its mail only if it drops the
coffee  in the  tbn representation of cclk  one must have both rhct and rhct   as
parents of rhmt     we must model the dependence of rhm on a change in value in the
variable rhc  figure    shows the cclk action in pso format  for simplicity  we ignore
the occurrence of exogenous events   the pso representation can offer an economical
representation of correlated effects such as this since the possible outcomes of moving in the
hallway are spelled out explicitly  specifically  the  possible  simultaneous change in values
of the variables in question is made clear 
      explicit event models

explicit event models can also be represented using  tbns in a somewhat different form 
as in our discussion in section      the form taken by explicit event models depends crucially on one s assumptions about the interplay between the effects of the action itself and
exogenous events  however  under certain assumptions even explicit event models can be
rather concise 
to illustrate  figure    shows the deliver coffee action represented as a  tbn with
exogenous events explicitly represented  the first  slice  of the network shows the effects of
the action delc without the presence of exogenous events  the subsequent slices describe
the effects of the events arrm and mess  we use only two events for illustration   notice the
presence of the extra random variables representing the occurrence of the events in question 
the cpts for these nodes reect the occurrence probabilities for the events under various
  

fiboutilier  dean    hanks

conditions  while the directed arcs from the event variables to state variables indicate the
effects of these events  these probabilities do not depend on all state variables in general 
thus  this  tbn represents the occurrence vectors  see section      in a compact form  also
notice that  in contrast to the event occurrence variables  we do not explicitly represent the
action occurrence as a variable in the network  since we are modeling the effect on the
system given that the action was taken   
this example reects the assumptions described in section      namely  that events
occur after the action takes place and that event effects are commutative  and for this
reason the ordering of the events arrm and mess in the network is irrelevant  under this
model  the system actually passes through two intermediate though not necessarily distinct
states as it goes from stage t to stage t      we use subscripts    and    to suggest this
process  of course  as described earlier  not all actions and events can be combined in such a
decomposable way  more complex combination functions can also be modeled using  tbns
 for one example  see boutilier   puterman        
      equivalence of representations

an obvious question one might ask concerns the extent to which certain representations are
inherently more concise than others  here we focus on the standard implicit event models 
describing some of the domain features that make the different representations more or less
suitable 
both  tbn and pso representations are oriented toward representing the changes in
the values of the state variables induced by an action  a key distinction lies in the fact that
 tbns model the inuence on each variable separately  while the pso model explicitly
represents complete outcomes  a simple  tbn a network with no synchronic arcs can
be used to represent an action in cases where there are no correlations among the action s
effect on different state variables  in the worst case  when the effect on each variable
differs at each state  each time t     variable must have all time t variables as parents 
if there are no regularities that can be exploited in structured cpt representations  then
such an action requires the specification of o n n   parameters  assuming boolean variables  
compared with the   n entries required by an explicit transition matrix  when the number of
parents of any variable is bounded by k  then we need specify no more than n k conditional
probabilities  this can be further reduced if the cpts exhibit structure  e g   can be
represented concisely in a decision tree   for instance  if the cpt can be captured by the
representation of choice with no more than f  k  entries  where f is a polynomial function of
the number of parents of a variable  then the representation size  o n  f  k    is polynomial
in the number of state variables  this is often the case  for instance  in actions where one
of its  stochastic  effects on a variable requires that some number of  pre   conditions hold 
if any of them do not  a different effect comes into play 
a pso representation may not be as concise as a  tbn when an action has multiple
independent stochastic effects  a pso requires that each possible change list be enumerated with its corresponding probability of occurrence  the number of such changes grows
exponentially with the number of variables affected by the action  this fact is evident in
    sections       and     discuss representations that model the choice of action explicitly as a variable in
the network 

  

fidecision theoretic planning  structural assumptions

rhc
t

f

loc
o

 rhc  cr
 rhc

nil

 m    
nil    
   

else
    
    

 rhc
nil

   
   

figure     a  factored  pso representation for the delc action 
figure     where the impact of exogenous events affects a number of variables stochastically and independently  the problem can arise with respect to  direct  action effects  as
well  consider an action in which a set of    unpainted parts is spray painted  each part is
successfully painted with probability      and these successes are uncorrelated  ignoring the
complexity of representing different conditions under which the action could take place  a
simple  tbn can represent such an action with    parameters  one success probability per
part   in contrast  a pso representation might require one to list all     distinct change
lists and their associated probabilities  thus  a pso representation can be exponentially
larger  in the number of affected variables  than a simple  tbn representation 
fortunately  if certain variables are affected deterministically  these do not cause the
pso representation to blow up  furthermore  pso representations can also be modified
to exploit the independence of an action s effects on different state variables  boutilier  
dearden        dearden   boutilier         thus escaping this combinatorial diculty  for
instance  we might represent the delc action shown in figure    in the more  factored
form  illustrated in figure     for simplicity  we show only the effect of the action and
the exogenous event arrm   much like a  tbn  we can determine an overall effect by
combining the change sets  in the appropriate contexts  and multiplying the corresponding
probabilities 
simple  tbns defined over the original set of state variables are not sucient to represent all actions    correlated action effects require the presence of synchronic arcs  in
the worst case  this means that time t     variables can have up to  n     parents  in
fact 
p the acyclicity condition assures that in the worst case  the total number of parents
is nk    k      thus  we end up specifying o   n   entries  the same as required by an
explicit transition matrix  however  if the number of parents  whether occurring within
the time slice t or t      can be bounded  or if regularities in the cpts allow a compact
representation  then  tbns can still be profitably used 
pso representations compare more favorably to  tbns in cases in which most of an
action s effects on different variables are correlated  in this case  psos can provide a
somewhat more economical representation of action effects  primarily because one needn t
worry about frame conditions  the main advantage of psos is that one need not enlist the
aid of probabilistic reasoning procedures to determine the transitions induced by actions
with correlated effects  contrast the explicit specification of outcomes in psos with the
type of reasoning required to determine the joint effects of an action represented in  tbn
    however  section       discusses certain problem transformations that do render simple  tbns sucient
for any mdp 

  

fiboutilier  dean    hanks

form with synchronic arcs  as described in section      essentially  correlated effects are
 compiled  into explicit outcomes in psos 
recent results by littman        have shown that simple  tbns and psos can both
be used to represent any action represented as a  tbn without an exponential blowup
in representation size  this is effected by a clever problem transformation in which new
sets of actions and propositional variables are introduced  using either a simple  tbn or
pso representation   the structure of the original  tbn is reected in the new planning
problem  incurring no more than a polynomial increase in the size of the input action
descriptions and the description of any policy  though the resulting policy consists of actions
that do not exist in the underlying domain  extracting the true policy is not dicult  it
should be noted  however  that while such a representation can automatically be constructed
from a general  tbn specification  it is unlikely that it could be provided directly  since
the actions and variables in the transformed problem have no  physical  meaning in the
original mdp 
      transformations to eliminate synchronic constraints

the discussion above has assumed that the variables or propositions used in the  tbn or
pso action descriptions are the original state variables  however  certain problem transformations can be used to ensure that one can represent any action using simple  tbns  as
long as one does not require the original state variables to be used  one such transformation
simply clusters all variables on which some action has a correlated effect  a new compound
variable which takes as values assignments to the clustered variables can then be used
in the  tbn  removing the need for synchronic arcs  of course  this variable will have a
domain size exponential in the number of clustered variables 
some of the intuitions underlying psos can be used to convert general  tbn action descriptions to simple  tbn descriptions with explicit  events  dictating the precise outcome
of the action  intuitively  this event can occur in k different forms  each corresponding to a
different change list induced by the action  or a change list with respect to the variables in
question   as an example  we can convert the  action  description for cclk in figure   
into the explicit event model shown in figure       notice that the  event  takes on values
corresponding to the possible effects on the correlated variables rhc and rhm  specifically  a denotes the event of the robot escaping the hallway successfully without losing its
cargo  b denotes the event of the robot losing only its coffee  and c denotes the event of losing
both the coffee and the mail  in effect  the event space represents all possible  combined 
effects  obviating the need for synchronic arcs in the network 
      actions as explicit nodes in the network

one diculty with the  tbn and pso approach to action description is that each action
is represented separately  offering no opportunity to exploit patterns across actions  for
instance  the fact that location persists in all actions except moving clockwise or counterclockwise means that the  frame axiom  is duplicated in the  tbn for all other actions
 this is not the case for psos  of course   in addition  ramifications  or correlated action
    while figure    describes a markov chain induced by a policy  the representation of cclk can easily be
extracted from it 

  

fidecision theoretic planning  structural assumptions

loc

hall

a     
b     
c     

event

t

rhc

rhm
t

loc

else

f

a     a    
b      b    
c      c    

loc

f

a     
b     
c     

t

a

rhc

rhc
   
t
off

rhm
time t

rhm
time t  

loc

   
   

a

rhm

b
   

b
   

f

   

c
   

f

   

else

event

event

rhc

c
   

figure     an explicit event model that removes correlations 
effects  are duplicated across actions as well  for instance  if a coffee request occurs  with
probability      only when the robot ends up in the oce  then this correlation is duplicated
across all actions  a more compelling example might be one in which the robot can move
a briefcase to a new location in one of a number of ways  we d like to capture the fact  or
ramification  that the contents of the briefcase move to the same location as the briefcase
regardless of the action that moves the briefcase 
to circumvent this diculty  we can introduce the choice of action as a  random variable  in the network  conditioning the distribution over state variable transitions on the
value of this variable  unlike state variables  or event variables in explicit event models  
we do not generally require a distribution over this action variable the intent is simply
to model schematically the conditional state transition distributions given any particular
choice of action  this is because the choice of action will be dictated by the decision maker
once a policy is determined  for this reason  anticipating terminology used for inuence
diagrams  see section       we call these nodes decision nodes and depict them in our network diagrams with boxes  such a variable can take as its value any action available to the
agent 
a  tbn with an explicit decision node is shown in figure     in this restricted example 
we might imagine the decision node can take one of two values  clk or cclk  the fact that
the issuance of a coffee request at t   depends on whether the robot successfully moved from
 or remained in  the oce is now represented  once  by the arc between loct   and crt    
rather than repeated across multiple action networks  furthermore  the noisy persistence
of m under both actions is also represented only once  adding the action pum  however 
undercuts this advantage as we will see when we try to combine actions  
one diculty with this straightforward use of decision nodes  which is the standard
representation in the inuence diagram literature  is that adding candidate actions can
cause an explosion in the network s dependency structure  for example  consider the two
  

fiboutilier  dean    hanks

act

loc

loc

cr

cr

m

m

time t

time t  

figure     an inuence diagram for a restricted process 

act
x

x

x

x

x

act

x

else

a 
a 

y

y

y

y

y

y

t
   

y

f
t

x

   

z

z
 a  action a 

z

z

z

 b  action a 

   

f
 

y

f
t

   

t
z

   

f
 

z

 c  influence diagram

figure     unwanted dependencies in inuence diagrams 

  

t

 d  cpt for y

y

f
 

fidecision theoretic planning  structural assumptions

action networks shown in figure    a  and  b   action a  makes y true with probability
    if x is true  having no effect otherwise   while a  makes y true if z is true 
combining these actions in a single network in the obvious way produces the inuence
diagram shown in figure    c   notice that y now has four parent nodes  inheriting the
union of all its parents in the individual networks  plus the action node  and requiring a
cpt with    entries for actions a  and a  together with eight additional entries for each
action that does not affect y   the individual networks reect the fact that y depends
on x only when a  is performed and on z only when a  is performed  this fact is lost
in the naively constructed inuence diagram  however  structured cpts can be used to
recapture this independence and compactness of representation  the tree of figure    d 
captures the distribution much more concisely  requiring only eight entries  this structured
representation also allows us concisely to express that y persists under all other actions  in
large domains  we expect variables to generally be unaffected by a substantial number of
 perhaps most  actions  thus requiring representations such as this for inuence diagrams 
see  boutilier   goldszmidt        for a deeper discussion of this issue and its relationship
to the frame problem 
while we provide no distributional information over the action choice  it is not hard to
see that a  tbn with an explicit decision node can be used to represent the markov chain
induced by a particular policy in a very natural way  specifically  by adding arcs from state
variables at time t to the decision node  the value of the decision node  i e   the choice of
action at that point  can be dictated by the prevailing state   

    inuence diagrams
inuence diagrams  howard   matheson        shachter        extend bayesian networks
to include special decision nodes to represent action choices  and value nodes to represent
the effect of action choice on a value function  the presence of decision nodes means that
action choice is treated as a variable under the decision maker s control  value nodes treat
reward as a variable inuenced  usually deterministically  by certain state variables 
inuence diagrams have not typically been associated with the schematic representation
of stationary systems  instead being used as a tool for decision analysts where the sequential
decision problem is carefully handcrafted  this more generic use of inuence diagrams has
been discussed by tatman and shachter         in any event  there is no theory of plan
construction associated with inuence diagrams  the choice of all possible actions at each
stage must be explicitly encoded in the model  inuence diagrams are  therefore  usually
used to model finite horizon decision problems by explicitly describing the evolution of the
process at each stage in terms of state variables 
as in section        decision nodes take as values specific actions  though the set of
possible actions can be tailored to the particular stage  in addition  an analyst will generally
include at each stage only state variables that are thought relevant to the decision at that
or subsequent stages  value nodes are also a key feature of inuence diagrams and are
discussed section      usually  a single value node is specified  with arcs indicating the
    more generally  a randomized policy can be represented by specifying a distribution over possible actions
conditioned on the state 

  

fiboutilier  dean    hanks

t
rhm
m

rhm
rew

cr

m

etc 

t
 

cr

etc 

   

t
 

 

                  

 

   

 

 

                  

figure     the representation of a reward function in an inuence diagram 
inuence of particular state and decision variables  often over multiple stages  on the overall
value function 
inuence diagrams are typically used to model partially observable problems  an arc
from a state variable to a decision node reects the fact that the value of that state variable
is available to the decision maker at the time the action is to be chosen  in other words 
this variable s value forms part of the observation made at time t prior to the action being
selected at time t     and the policy constructed can refer to this variable  once again  this
allows a compact specification of the observation probabilities associated with a system  the
fact that the probability of a given observation depends directly only on certain variables
and not on others can mean that far fewer model parameters are required 

    factored reward representation

we have already noted that it is very common in formulating mdp problems to adopt a
simplified value function  assigning rewards to states and costs to actions  and evaluating histories by combining these factors according to some simple function like addition 
this simplification alone allows a representation for the value function significantly more
parsimonious than one based on a more complex comparison of complete histories  even
this representation requires an explicit enumeration of the state and action space  however 
motivating the need for more compact representations for these parameters  factored representations for rewards and action costs can often obviate the need to enumerate state and
action parameters explicitly 
like an action s effect on a particular variable  the reward associated with a state often
depends only on the values of certain features of the state  for example  in our robot
domain  we can associate rewards or penalties with undelivered mail  with unfulfilled coffee
requests and with untidiness in the lab  this reward or penalty is independent of other
variables  and individual rewards can be associated with the groups of states that differ on
the values of the relevant variables  the relationship between rewards and state variables is
represented in value nodes in inuence diagrams  represented by the diamond in figure    
the conditional reward table  crt  for such a node is a table that associates a reward with
every combination of values for its parents in the graph  this table  not shown in figure    
is locally exponential in the number of relevant variables  although figure    shows the
case of a stationary markovian reward function  inuence diagrams can be used to represent
  

fidecision theoretic planning  structural assumptions

nonstationary or history dependent rewards and are often used to represent value functions
for finite horizon problems 
although in the worst case the crt will take exponential space to store  in many
cases the reward function exhibits structure  allowing it to be represented compactly using
decision trees or graphs  boutilier et al          strips like tables  boutilier   dearden 
       or logical rules  poole            a   figure    shows a fragment of one possible
decision tree representation for the reward function used in the running example 
the independence assumptions studied in multiattribute utility theory  keeney   raiffa 
      provide yet another way in which reward functions can be represented compactly  if
we assume that the component attributes of the reward function make independent contributions to a state s total reward  the individual contributions can be combined functionally 
for instance  we might imagine penalizing states where cr holds with a  partial  reward
of     penalizing situations where there is undelivered mail  m   rhm  with     and
penalizing untidiness t  i  with i      i e   in proportion to how untidy things are   the
reward for any state can then be determined simply by adding the individual penalties associated with each feature  the individual component rewards along with the combination
function constitute a compact representation of the reward function  the tree fragment in
figure     which reects the additive independent structure just described  is considerably
more complex than a representation that defines the  independent  rewards for individual
propositions separately  the use of additive reward functions for mdps is considered in
 boutilier  brafman    geib        meuleau  hauskrecht  kim  peshkin  kaelbling  dean 
  boutilier        singh   cohn        
another example of structured rewards is the goal structure studied in classical planning 
goals are generally specified by a single proposition  or a set of literals  to be achieved 
as such  they can generally be represented very compactly  haddawy and hanks       
explore generalizations of goal oriented models that permit extensions such as partial goal
satisfaction  yet still admit compact representations 

    factored policy and value function representation

the techniques studied so far have been concerned with the input specification of the mdp 
the states  actions  and reward function  the components of a problem s solution the policy and optimal value function are also candidates for compact structured representation 
in the simplest case  that of a stationary policy for a fully observable problem  a policy
must associate an action with every state  nominally requiring a representation of size
o jsj   the problem is exacerbated for nonstationary policies and pomdps  for example 
the policy for a finite horizon fomdp with t stages generates a policy of size o t jsj  
for a finite horizon pomdp  each possible
p observable history of length t   t might require
a different action choice  as many as tk   bk such histories can be generated by a fixed
policy  where b is the maximum number of possible observations one can make following an
action   
the fact that policies require too much space motivates the need to find compact functional representations  and standard techniques like the tree structures discussed above for
    other methods of dealing with pomdps  by conversion to fomdps over belief space  see section         
are more complex still 

  

fiboutilier  dean    hanks

cr
rhc

etc 

loc
o

l c

loc
m

h

h

o l

c

m

delc clk clk m cclk cclk hrm clk getc m
pum cclk

delm cclk

pum cclk

figure     a tree representation of a policy 
actions and reward functions can be used to represent policies and value functions as well 
here we focus on stationary policies and value functions for fomdps  for which any logical
function representation may be used  for example  schoppers        uses a strips style
representation for universal plans  which are deterministic  plan like policies  decision trees
have also been used for policies and value functions  boutilier et al         chapman  
kaelbling         an example policy for the robot domain specified with a decision tree is
given in figure     this policy dictates that  for instance  if cr and rhc are true   a  the
robot deliver the coffee to the user if it is in the oce  and  b  it move toward the oce
if it is not in the oce  unless  c  there is mail and it is in the mailroom  in which case it
should pickup the mail on its way 

    summary

in this section we discussed a number of compact factored representations for components of
an mdp  we began by discussing intensional state representations  then temporal bayesian
networks as a device for representing the system dynamics  tree structured conditional
probability tables  cpts  and probabilistic strips operators  psos  were introduced as an
alternative to transition matrices  similar tree structures and other logical representations
were introduced for representing reward functions  value functions  and policies 
while these representations can often be used to describe a problem compactly  by
themselves they offer no guarantee that the problem can be solved effectively  in the next
section we explore algorithms that use these factored representations to avoid iterating
explicitly over the entire set of states and actions 

   abstraction  aggregation  and decomposition methods

the greatest challenge in using mdps as the basis for dtp lies in discovering computationally feasible methods for the construction of optimal  approximately optimal or satisficing
policies  of course  arbitrary decision problems are intractable even producing satisficing
or approximately optimal policies is generally infeasible  however  the previous sections
suggest that many realistic application domains may exhibit considerable structure  and
furthermore that the structure can be modeled explicitly and exploited so that typical
problems can be solved effectively  for instance  structure of this type can lead to compact
  

fidecision theoretic planning  structural assumptions

factored representations of both input data and output policies  often polynomial sized with
respect to the number of variables and actions describing the problem  this suggests that
for these compact problem representations  policy construction techniques can be developed that exploit this structure and are tractable for many commonly occurring problem
instances 
both the dynamic programming and state based search techniques described in section   exploit structure of a different kind  value functions that can be decomposed into
state dependent reward functions  or state based goal functions  can be tackled by dynamic
programming and regression search  respectively  these algorithms exploit the structure
in decomposable value functions to prevent having to search explicitly through all possible
policies  however  while these algorithms are polynomial in the size of the state space 
the curse of dimensionality makes even these algorithms infeasible for practical problems 
though compact problem representations aid in the specification of large problems  it is
clear that a large system can be specified compactly only if the representation exploits
 regularities  found in the domain  recent ai research on dtp has stressed using the
regularities implicit in compact representations to speed up the planning process  these
techniques focus on both optimal and approximately optimal policy construction 
in the following subsection we focus on abstraction and aggregation techniques  especially those that manipulate factored representations  roughly  these techniques allow the
explicit or implicit grouping of states that are indistinguishable with respect to certain characteristics  e g   value or optimal action choice   we refer to a set of states grouped in this
manner as an aggregate or abstract state   or sometimes as a cluster  and assume that the
set of abstract states constitutes a partition of the state space  that is to say  every state is
in exactly one abstract state and the union of all abstract states comprises the entire state
space    by grouping similar states  each abstract state can be treated as a single state  thus
alleviating the need to perform computations for each state individually  these techniques
can be used for approximation if the elements of an abstract state are only approximately
indistinguishable  e g   if the values of those states lie within some small interval  
we then look at the use of problem decomposition techniques in which an mdp is
broken into various pieces  each of which is solved independently  the solutions are then
pieced together or used to guide the search for a global solution  if subprocesses whose
solutions interact minimally are treated as independent  we might expect an approximately
optimal global solution  furthermore  if the structure of the problem requires a solution
to a particular subproblem only  then the solutions to other subproblems can be ignored
altogether 
related is the use of reachability analysis to restrict attention to  relevant  regions of
state space  indeed  reachability analysis and the communicating structure of an mdp
can be used to form certain types of decompositions  specifically  we distinguish serial
decompositions from parallel decompositions 
the result of a serial decomposition can be viewed as a partitioning of the state space
into blocks  each representing a  more or less  independent subprocess to be solved  in
serial decomposition  the relationship between blocks is generally more complicated than
in the case of abstraction or aggregation  in a partition resulting from decomposition  the
    we might also group states into non disjoint sets that cover the entire state space  we do not consider
such soft state aggregation here  but see  singh  jaakkola    jordan        

  

fiboutilier  dean    hanks

states within a particular block may behave quite differently with respect to  say  value or
dynamics  the important consideration in choosing a decomposition is that it is possible to
represent each block compactly and to compute eciently the consequences of moving from
one block to another and  further  that the subproblems corresponding to the subprocesses
can themselves be solved eciently 
a parallel decomposition is somewhat more closely related to an abstract mdp  an
mdp is divided into  parallel sub mdps  such that each decision or action causes the
state to change within each sub mdp  thus  the mdp is the cross product or join of the
sub mdps  in contrast to the union  as in serial decomposition   we briey discuss several
methods that are based on parallel mdp decomposition 

    abstraction and aggregation
one way problem structure can be exploited in policy construction relies on the notion
of aggregation grouping states that are indistinguishable with respect to certain problem
characteristics  for example  we might group together all states that have the same optimal
action  or that have the same value with respect to the k stage to go value function  these
aggregates can be constructed during the solution of the problem 
in ai  emphasis has generally been placed on a particular form of aggregation  namely
abstraction methods  in which states are aggregated by ignoring certain problem features 
the policy in figure    illustrates this type of abstraction  those states in which cr 
rhc and loc o  are true are grouped  and the same action is selected for each such
state  intuitively  when these three propositions hold  other problem features are ignored
and abstracted away  i e   they are deemed irrelevant   a decision tree representation of a
policy or a value function partitions the state space into a distinct cluster for each leaf of
the tree  other representations  e g   strips like rules  abstract the state space similarly 
it is precisely this type of abstraction that is used in the compact  factored representations of actions and goals discussed in section    in the  tbn shown in figure     the
effect of the action delc on the variable cr is given by the cpt for crt     however 
this  stochastic  effect is the same at any state for which the parent variables have the
same value  this representation abstracts away other variables  combining states that have
distinct values for the irrelevant  non parent  variables  intensional representations often
make it easy to decide which features to ignore at a certain stage of problem solving  and
thus  implicitly  how to aggregate the state space 
there are at least three dimensions along which abstractions of this type can be compared  the first is uniformity  a uniform abstraction is one in which variables are deemed
relevant or irrelevant uniformly across the state space  while a nonuniform abstraction allows certain variables to be ignored under certain conditions and not under others  the
distinction is illustrated schematically in figure     the tabular representation of a cpt
can be viewed as a form of uniform abstraction the effect of an action on a variable is
distinguished for all clusters of states that differ on the value of a parent variable  and is
not distinguished for states that agree on parent variables but disagree on others while a
decision tree representation of a cpt embodies a nonuniform abstraction 
a second dimension of comparison is accuracy  states are grouped together on the
basis of certain characteristics  and the abstraction is called exact if all states within a
  

fidecision theoretic planning  structural assumptions

uniform
abc
abc

abc
abc

abc
abc

abc
abc

nonuniform
a
b

   
   

ab

 

abc

c

abc

exact
   
   

a

approximate
   
   

   
   

   
   

   

   
   

   
   

   

adaptive

fixed

figure     different forms of state space abstraction 
cluster agree on this characteristic  a non exact abstraction is called approximate  this
is illustrated schematically in figure     the exact abstraction groups together states that
agree on the value assigned to them by a value function  while the approximate abstraction
allows states to be grouped together that differ in value  the extent to which these states
differ is often used as a measure of the quality of an approximate abstraction 
a third dimension is adaptivity  technically  this is a property not of an abstraction
itself  but of how abstractions are used by a particular algorithm  an adaptive abstraction
technique is one in which the abstraction can change during the course of computation  while
a fixed abstraction scheme groups together states once and for all  again  see figure     
for example  one can imagine using an abstraction in the representation of a value function
v k   then revising this abstraction to represent v k   more accurately 
abstraction and aggregation techniques have been studied in the or literature on
mdps  bertsekas and castanon        develop an adaptive aggregation  as opposed to
abstraction  technique  the proposed method operates on at state spaces  however  and
therefore does not exploit implicit structure in the state space itself  an adaptive  uniform
abstraction method is proposed by schweitzer et al         for solving stochastic queuing models  these methods  often referred to as aggregation disaggregation procedures  are
typically used to accelerate the calculation of the value function for a fixed policy  valuefunction calculation requires computational effort at least quadratic in the size of the state
space  which is impractical for very large state spaces  in aggregation disaggregation procedures  the states are first aggregated into clusters  a system of equations is then solved 
or a series of summations performed  requiring effort no more than cubic in the number of
clusters  next  a disaggregation step is performed for each cluster  requiring effort at least
linear in the size of the cluster  the net result is that the total work  while at least linear
in the total number of states  is at worst cubic in the size of the largest cluster 
in dtp it is generally assumed that computations even linear in the size of the full
state space are infeasible  therefore it is important to develop methods that perform
  

fiboutilier  dean    hanks

work polynomial in the log of the size of the state space  not all problems are amenable
to such reductions without some  perhaps unacceptable  sacrifice in solution quality  in
the following section  we review some recent techniques for dtp aimed at achieving such
reductions 
      goal regression and classical planning

in section     we introduced the general technique of regression  or backward  search
through state space to solve classical planning problems  those involving deterministic actions and performance criteria specified in terms of reaching a goal satisfying state  one
diculty is that such a search requires that any branch of the search tree lead to a particular
goal state  this commitment to a goal state may have to be retracted  by backtracking
the search process  if no sequence of actions can lead to that particular goal state from the
initial state  however  a goal is usually specified as a set of literals g representing a set of
states  where reaching any state in g is equally suitable it may  therefore  be wasteful to
restrict the search to finding a plan that reaches a particular element of g 
goal regression is an abstraction technique that avoids the problem of choosing a particular goal state to pursue  a regression planner works by searching for a sequence of actions
as follows  the current set of subgoals sg  is initialized as g  at each iteration an action
ff is selected that achieves one or more of the current subgoals of sgi without deleting
the others  and whose preconditions do not conict with the  unachieved subgoals   the
subgoals so achieved are removed from the current subgoal set and replaced by a formula
representing the context under which ff will achieve the current subgoals  forming sgi    
this process is known as regressing sgi through ff  the process is repeated until one of
two conditions holds   a  the current subgoal set is satisfied by the initial state  in which
case the current sequence of actions so selected is a successful plan  or  b  no action can be
applied  in which case the current sequence cannot be extended into a successful plan and
some earlier action choice must be reconsidered 
example     as an example  consider the simplified version of the robot planning example used in section     to illustrate value iteration  the robot has only four actions
pum  getc  delc and delm  which we make deterministic in the obvious way  the
initial state sinit is hcr  m  rhc  rhmi and the goal set g is fcr  m g  regressing g through delm results in sg    fcr  m  rhmg  regressing sg  through
delc results in sg    frhc  m  rhmg  regressing sg  through pum results in
sg    frhc  m g  regressing sg  through getc results in sg    fm g  note that
sinit   sg   so the sequence of actions getc  pum  delc  delm will successfully reach
a goal state   
to see how this algorithm implements a form of abstraction  first note that the goal
itself provides an initial partition of the state space  dividing it into one set of states in
which the goal is satisfied  g  and a second set in which it is not  g   viewed as a partition
of a zero stage to go value function  g represents those states whose value is positive while
g represents those states whose value is zero 
every regression step can be thought of as revising this partition  when the planning
algorithm attempts to satisfy the current subgoal set sgi by applying action ff  it uses
  

fidecision theoretic planning  structural assumptions

getc

rhc

m

s

 

pum

rhc

delc

cr

delm

cr

m

m

m

rhm

rhm

m

s

s

s

goal

 

 

 

figure     an example of goal regression 
regression to compute the  largest  set of states such that  after executing ff  all subgoals
are satisfied  in particular  the state space is repartitioned into two abstract states  sgi  
and sgi     in this way  the abstraction mechanism implemented by goal regression should
be considered adaptive  this can be viewed as an  i      stage value function  any state
satisfying sgi   can reach a goal state in i    steps using the action sequence that produced
sgi       the regression process can be stopped when the initial state is a member of the
abstract state sgi     figure    illustrates the repartitioning of the state space into the
different regions sgi   for each of the steps in the example above 
while regression produces a compact representation of something like a value function
 as in our discussion of deterministic  goal based dynamic programming in section       the
analogy is not exact in that the regions produced by regression record only the property of
goal reachability contingent on a particular choice of action or action sequence 
standard dynamic programming methods can be implemented in a structured way by
simply noticing that a number of different regions can be produced at the ith iteration
by considering all actions that can be regressed at that stage  the union of all of these
regressions form the states that have positive values in vi   thus making the representation of
the i stage to go value function exact  notice that each iteration is now more costly  since
regression through all actions must be attempted  but this approach obviates the need for
backtracking and can ensure that a shortest plan is found  standard regression does not
provide such guarantees without commitment to a particular search strategy  e g   breadthfirst   this use of dynamic programming using strips action descriptions forms the basic
idea of schoppers s universal planning method  schoppers        
another general technique for solving classical planning problems is partial order planning  pop   chapman        sacerdoti         embodied in such popular planning algorithms as snlp  mcallester   rosenblitt        and ucpop  penberthy   weld          
the main motivation for the least commitment approach comes from the realization that
regression techniques are incrementally building a plan from the end to the beginning  in
the temporal dimension   thus  each iteration must commit to inserting a step last in the
plan 
in many cases it can be determined that a particular step must appear somewhere in the
plan  but not necessarily as the last step in the plan  and  indeed  in many cases the step
    it is not the case  however  that states in sgi   cannot reach the goal region in i     steps  it is only
the case that they cannot do so using the specific sequence of actions chosen so far 
    this type of planning is also sometimes called nonlinear or least commitment planning  see weld s
       survey for a nice overview 

  

fiboutilier  dean    hanks

under consideration cannot appear last  but this fact cannot be recognized until later choices
reveal an inconsistency  in these cases  a regression algorithm will prematurely commit to
the incorrect ordering and will eventually have to backtrack over that choice  for example 
suppose in the problem scenario above that the robot can hold only one item at a time 
coffee or mail  picking up mail causes the robot to spill any coffee in its possession  and
similarly grasping the coffee makes it drop the mail  the plan generated by regression would
no longer be valid  once the first two actions  delc and delm  have been inserted into the
plan  no action can be added to achieve rhc or rhm without making the other one false 
the search for a plan would have to backtrack  ultimately it would be discovered that no
successful plan can end with these two actions performed in sequence 
partial order planning algorithms proceed much like regression algorithms  choosing
actions to achieve unachieved subgoals and using regression to determine new subgoals 
but leaving actions unordered to whatever extent possible  strictly speaking  subgoal sets
aren t regressed  rather  each unachieved goal or action precondition is addressed separately 
and actions are ordered relative to one another only if one action threatens to negate the
desired effect of another  in the example above  the algorithm might first place actions
delc and delm into the plan  but leave them unordered  pum can be added to the plan
to achieve the requirement rhm of delm  it is ordered before delm but is still unordered
with respect to delc  when getc is finally added to the plan so as to achieve rhc for
action delc  two threats arise  first  getc threatens the desired effect rhm of pum  this
can be resolved by ordering getc before pum or after delm  assume the former ordering
is chosen  second  pum threatens the desired effect rhc of getc  this threat can also
be resolved by placing pum before getc or after delc  since the first threat was resolved
by ordering getc before pum  the latter ordering is the only consistent one  the result
is the plan getc  delc  pum  delm  no backtracking was required to generate the plan 
because the actions were initially unordered  and orderings were introduced only when the
discovery of threats required them 
in terms of abstraction  any incomplete  partially ordered plan that is threat free 
but perhaps has certain  open conditions   unachieved preconditions or subgoals   can be
viewed in much the same way as a partially completed regression plan  any state satisfying
the open conditions can reach a goal state by executing any total ordering of the plan s
actions consistent with current set of ordering constraints  see  kambhampati        for a
framework that unifies various approaches to solving classical plan generation problems 
while techniques relying on regression have been studied extensively in the deterministic
setting  they have only recently been applied to probabilistic unobservable  kushmerick
et al         and partially observable  draper  hanks    weld      b  domains  for the
most part  these techniques assume a goal based performance criterion and attempt to
construct plans whose probability of reaching a goal state exceeds some threshold  these
augment standard pop methods with techniques for evaluating a plan s probability of
achieving the goal  and techniques for improving this probability by adding further structure
to the plan  in the next section  we consider how to use regression related techniques to
solve mdps with performance criteria more general than goals 
  

fidecision theoretic planning  structural assumptions

      stochastic dynamic programming with structured representations

a key idea underlying propositional goal regression that one need only regress the relevant propositions through an action can be extended to stochastic dynamic programming
methods  like value iteration and policy iteration  and used to solve general mdps  there
are  however  two key diculties to overcome  the lack of a specific goal region and the
uncertainty associated with action effects 
instead of viewing the state space as partitioned into goal and non goal clusters  we
consider grouping states according to their expected values  ideally  we might want to
group states according to their value with respect to the optimal policy  here we consider
a somewhat less dicult task  that of grouping states according to their value with respect
to a fixed policy  this is essentially the task performed by the policy evaluation step in
policy iteration  and the same insights can be used to construct optimal policies 
for a fixed policy  we want to group states that have the same value under that policy 
generalizing the goal versus non goal distinction  we begin with a partition that groups
states according their immediate rewards  then  using an analogue of regression developed
for the stochastic case  we reason backward to construct a new partition in which states
are grouped according to their value with respect to the one stage to go value function  we
iterate in this manner so that on the kth iteration we produce a new partition that groups
states according the k stage to go value function 
on each iteration  we perform work polynomial in the number of abstract states  and
the size of the mdp representation  and  if we are lucky  the total number of abstract states
will be bounded by some logarithmic factor of the size of the state space  to implement this
scheme effectively  we have to perform operations like regression without ever enumerating
the set of all states  and this is where the structured representations for state transition 
value  and policy functions play a role 
for fomdps  approaches of this type are taken in  boutilier        boutilier   dearden        boutilier et al         boutilier  dearden    goldszmidt        dietterich  
flann        hoey et al          we illustrate the basic intuitions behind this approach
by describing how value iteration for discounted infinite horizon fomdps might work  we
assume that the mdp is specified using a compact representation of the reward function
 such as a decision tree  and actions  such as  tbns  
in value iteration  we produce a sequence of value functions v    v         vn   each vk
representing the utility of the optimal k stage policy  our aim is to produce a compact
representation of each value function and  using vn for some suitable n  produce a compact
representation of the optimal stationary policy  given a compact representation of the
reward function r  it is clear that this constitutes a compact representation of v    as
usual  we think of each leaf of the tree as a cluster of states having identical utility  to
produce v  in compact form  we can proceed in two phases 
each branch of the tree for v  provides an intensional description namely  the conjunction of variable values labeling the branch of an abstract state  or region  comprising
states with identical value with respect to the initial value function v    for any deterministic action ff  we can perform a regression step using this description to determine the
conditions under which  should we perform ff  we would end up in this cluster  this would 
furthermore  determine a region of the state space containing states of identical future value
  

fiboutilier  dean    hanks

x

x

x

       
x

y

y

   

y

       

z

z

y
   

time t

z

time t  
       

figure     an example action 
with respect to the execution of ff with one stage to go    unfortunately  nondeterministic
actions cannot be handled in quite this way  at any given state  the action might lead to
several different regions of v  with non zero probability  however  for each leaf in the tree
representing v   i e   for each region of v     we can regress the conjunction x describing
that region through action ff to produce the conditions under which x becomes true or
false with a specified probability  in other words  instead of regressing in the standard fashion to determine the conditions under which x becomes true  we produce a set of distinct
conditions under which x becomes true with different probabilities  by piecing together
the regions produced for the different labels in the description of v    we can construct a
set of regions such that each state in a given region   a  transitions  under action ff  to a
particular part of v  with identical probability  and hence  b  has identical expected future
value  boutilier et al          we can view this as a generalization of propositional goal
regression suitable for decision theoretic problems 
example     to illustrate  consider the example action a shown in figure    and the value
function v   shown to the left of figure     in order to generate the set of regions
consisting of states whose future value  w r t  v     under a is identical  we proceed in
two steps  see figure      we first determine the conditions under which a has a fixed
probability of making y true  hence we have a fixed probability of moving to the left
or right subtree of v      these conditions are given by the tree representing the cpt
for node y   which makes up the first portion of the tree representing v    see step  
of figure     notice that this tree has leaves labeled with the probability of making
y true or  implicitly  false 
if a makes y true  then we know that its future value  i e   value with zero stages
to go  is      but if y becomes false  we need to know whether a makes z true  to
    we ignore immediate reward and cost distinctions within the region so produced in our description 
recall that the value of performing ff at any state s is given by r s   c  ff  s  and expected future value 
we simply focus on abstract states whose elements have identical future expected value  differences in
immediate reward and cost can be added after the fact 

  

fidecision theoretic planning  structural assumptions

y

x

   

z

   

y    

   

x

y

y    

y

y    

y    
z    

z

y    
z    
 

v

step  

y

y    

y    
z    

z

y    
z    

y    
z    

step  

figure     an iteration of decision theoretic regression  step   produces the portion of the
tree with dashed lines  while step   produces the portion with dotted lines 
determine whether the future value is   or       the probability with which z becomes
true is given by the tree representing the cpt for node z   in step   in figure    
the conditions in that cpt are conjoined to the conditions required for predicting
y  s probability  by  grafting  the tree for z to the tree for y given by the first step  
this grafting is slightly different at each of the three leaves of the tree for y    a  the
full tree for z is attached to the leaf x   t   b  the tree for z is simplified where it is
attached to to the leaf x   f   y   f by removal of the redundant test on variable
y    c  notice that there is no need to attach the tree for z to the leaf x   f   y   t 
since a makes y true with probability   under those conditions  and z is relevant to
the determination of v   only when y is false  
at each of the leaves of the newly formed tree we have both pr y   and pr z    each of
these joint distributions over y and z  the effect of a and these variables is independent by the semantics of the network  tells us the probability of having y and z true
with zero stages to go given that the conditions labeling the appropriate branch of the
tree hold with one stage to go  in other words  the new tree uniquely determines  for
any state with one stage remaining  the probability of making any of the conditions
labeling the branches of v   true  the computation of expected future value obtained
by performing a with one stage to go can then be placed at the leaves of this tree by
taking expectation over the values at the leaves of v      
the new set of regions produced this way describes the function qff    where qff   s  is the
value associated with performing ff at state s with one stage to go and acting optimally
thereafter  these functions  for each action ff  can be pieced together  i e    maxed  see
section      to determine v    of course  the process can be repeated some number of times
to produce vn for some suitable n  as well as the optimal policy with respect to vn  
this basic technique can be used in a number of different ways  dietterich and flann
       propose ideas similar to these  but restrict attention to mdps with goal regions
  

fiboutilier  dean    hanks

and deterministic actions  represented using strips operators   thus rendering true goalregression techniques directly applicable    boutilier et al         develop a version of
modified policy iteration to produce tree structured policies and value functions  while
boutilier and dearden        develop the version of value iteration described above  these
algorithms are extended to deal with correlations in action effects  i e   synchronic arcs in the
 tbns  in  boutilier         these abstraction schemes can be categorized as nonuniform 
exact and adaptive 
the utility of such exact abstraction techniques has not been tested on real world problems to date  in  boutilier et al          results on a series of abstract process planning
examples are reported  and the scheme is shown to be very useful  especially for larger
problems  for example  in one specific problem with     million states  the tree representation of the value function has only        leaves  indicating a tremendous amount of
regularity in the value function  schemes like this exploit such regularity to solve problems
more quickly  in this example  in much less than half the time required by modified policy iteration  and with much lower memory demands  however  these schemes do involve
substantial overhead in tree construction  and for smaller problems with little regularity 
the overhead is not repaid in time savings  simple vector matrix representations methods
are faster   though they still generally provide substantial memory savings  what might be
viewed as best  and worst case behavior is also described in  boutilier et al          in a
series of  linear  examples  i e   problems with value functions that can be represented with
trees whose size is linear in the number of problem variables   the tree based scheme solves
problems many orders of magnitude faster than classical state based techniques  in contrast  problems with exponentially many distinct values are also tested  i e   with a distinct
value at each state   here tree construction methods are required to construct a complete
decision tree in addition to performing the same number of expected value and maximization
computations as classical methods  in this worst case  tree construction overhead makes
the algorithm run about     times slower than standard modified policy iteration 
in  hoey et al          a similar algorithm is described that uses algebraic decision
diagrams  adds   bahar  frohm  gaona  hachtel  macii  pardo    somenzi        rather
than trees  adds are a simple generalization of boolean decision diagrams  bdds   bryant 
      that allow terminal nodes to be labeled with real values instead of just boolean values 
essentially  add based algorithms are similar to the tree based algorithms except that
isomorphic subtrees can be shared  this lets adds provide more compact representations
of certain types of value functions  highly optimized add manipulation and evaluation
software developed in the verification community can also be applied to solving mdps 
initial results provided in  hoey et al         are encouraging  showing considerable savings
over tree based algorithms on the same problems  for example  the add algorithm applied
to the     million state example described above revealed the value function to have only
    distinct values  cf  the        tree leaves required  and produced an add description
of the value function with less than      internal nodes  it also solved the same problem
in seven minutes  about    times faster than earlier reported timing results using decision
trees  though some of this improvement was due to the use of optimized add software
packages   similar results obtain with other problems  problems of up to     million states
    dietterich and flann        also describe their work in the context of reinforcement learning rather than
as a method for solving mdps directly 

  

fidecision theoretic planning  structural assumptions

were solved in about four hours   most encouraging is the fact that on the worst case
 exponential  examples  the overhead associated with using adds compared to classical 
vector based methods is much less than with trees  about a factor of    compared to  at 
modified policy iteration with    state variables   and lessens as problems become larger 
like tree based algorithms  these methods have yet to be applied to real world problems 
with these exact abstraction schemes it is clear that  while in some examples the resulting policies and value functions may be compact  in others the set of regions may get very
large  even reaching the level of individual states boutilier et al          thus precluding
any computational savings  boutilier and dearden        develop an approximation scheme
that exploits the tree structured nature of the value functions produced  at each stage k 
the value function vk can be pruned to produce a smaller  less accurate tree that approximates vk   specifically  approximate value functions are represented using trees whose leaves
are labeled with an upper and lower bound on the value function in that region  decisiontheoretic regression is performed on these bounds  certain subtrees of the value tree can
be pruned when leaves of the subtree are very close in value or when the tree is too large
given computational constraints  this scheme is nonuniform  approximate and adaptive 
this approximation scheme can be tailored to provide  roughly  the most accurate value
function of a given maximum tree size  or the smallest value function  with respect to tree
size  of some given minimum accuracy  results reported in  boutilier   dearden       
show that approximation on a small set of examples  including the worst case examples for
tree based algorithms  allows substantial reduction in computational cost  for instance  in
a    variable worst case example  a small amount of pruning introduced an average error of
only      but reduced computation time by a factor of     more aggressive pruning tends
to increase error and decrease computation time very rapidly  making appropriate tradeoffs
in these two dimensions is still to be addressed  this method too remains to be tested and
evaluated on realistic problems 
structured representations and solution algorithms can be applied to problems other
than fomdps  methods for solving inuence diagrams  shachter        exploit structure
in a natural way  tatman and shachter        explore the connection between inuence diagrams and fomdps and the relationship between inuence diagram solution techniques and
dynamic programming  boutilier and poole        show how classic history independent
methods for solving pomdps  based on conversion to a fomdp with belief states  can exploit the types of structured representations described here  however  exploiting structured
representations of pomdps remains to be explored in depth 
      abstract plans

one of the diculties with the adaptive abstraction schemes suggested above is the fact
that different abstractions must be constructed repeatedly  incurring substantial computational overhead  if this overhead is compensated by the savings obtained during policy
construction e g   by reducing the number of backups then it is not problematic  but in
many cases the savings can be dominated by the time and space required to generate the
abstractions  and thus motivates the development of cheaper but less accurate approximate
clustering schemes 
  

fiboutilier  dean    hanks

another way to reduce this overhead is to adopt a fixed abstraction scheme so that
only one abstraction is ever produced  this approach has been adopted in classical planning in hierarchical or abstraction based planners  pioneered by sacerdoti s abstrips system  sacerdoti         a similar form of abstraction is studied by knoblock         see also
knoblock  tenenberg    yang         in this work  variables  in this case propositional  are
ranked according to criticality  roughly  how important such variables are to the solution
of the planning problem  and an abstraction is constructed by deleting from the problem
description a set of propositions of low criticality  a solution to this abstract problem is a
plan that achieves the elements of the original goal that have not been deleted  however 
preconditions and effects of actions that have been deleted are not accounted for in this solution  so it might not be a solution to the original problem  even so  the abstract solution
can be used to restrict search for a solution in the underlying concrete space  very often
hierarchies of more and more refined abstractions are used and propositions are introduced
back into the domain in stages 
this form of abstraction is uniform  propositions are deleted uniformly  and fixed  since
the abstract solution need not be a solution to the problem  we might be tempted to view
it as an approximate abstraction method  however  it is best not to think of the abstract
plan as a solution at all  rather as a form of heuristic information that can help solve the
true problem more quickly 
the intuitions underlying knoblock s scheme are applied to dtp by boutilier and dearden               variables are ranked according to their degree of inuence on the reward
function and a subset of the most important variables is deemed relevant  once this subset
is determined  those variables that inuence the relevant variables through the effects of
actions  which can be determined easily using strips or  tbn action descriptions  are
also deemed relevant  and so on  all remaining variables are deemed irrelevant and are
deleted from the description of the problem  both action and reward descriptions   this
leaves an abstract mdp with a smaller state space  i e   fewer variables  that can be solved
by standard methods  recall that the state space reduction is exponential in the number of
variables removed  we can view this method as an uniform fixed approximate abstraction
scheme  unlike the output of classical abstraction methods  the abstract policy produced
can be implemented and has a value  the degree to which the optimal abstract policy and
the true optimal policy differ in value can be bounded a priori once the abstraction is fixed 

example     as a simple illustration  suppose that the reward for satisfying coffee requests

 or penalty for not satisfying them  is substantially greater than that for keeping the
lab tidy or for delivering mail  suppose that time pressure requires our agent to focus
on a specific subset of objectives in order to produce a small abstract state space  in
this case  of the four reward laden variables in our problem  see figure      only cr
will be judged to be important  when the action descriptions are used to determine
the variables that can  directly or indirectly  affect the probability of achieving cr 
only cr  rhc and loc will be deemed relevant  allowing t   m   and rhm to be
ignored  the state space is thus reduced from size     to size     in addition  several
of the action descriptions  e g   tidy  become trivial and can be deleted   
  

fidecision theoretic planning  structural assumptions

the advantage of these abstractions is that they are easily computed and incur little
overhead  the disadvantages are that the uniform nature of such abstractions is restrictive 
and the relevant  reward variables  are determined before the policy is constructed and
without knowledge of the agent s ability to control these variables  as a result  important
variables those that have a large impact on reward but over which the agent has no
control  may be taken into account  while less important variables that the agent can actually
inuence are ignored  however  a series of such abstractions can be used that take into
account objectives of decreasing importance  and the a posteriori most valuable objectives
can be dealt with once risk and controllability are taken into account  boutilier et al  
       the policies generated at more abstract levels can also be used to  seed  value or
policy iteration at less abstract levels  in certain cases reducing the time to convergence
 dearden   boutilier         it has also been suggested  dearden   boutilier             
that the abstract value function be used as a heuristic in an online search for policies that
improve the abstract policy so constructed  as discussed in section        thus  the error
in the approximate value function is overcome to some extent by search  and the heuristic
function can be improved by asynchronous updates 
a different use of abstraction is adopted in the drips planner  haddawy   suwandi 
      haddawy   doan         actions can be abstracted by collapsing  branches   or possible outcomes  and maintaining probabilistic intervals over the abstract  disjunctive effects 
actions are also combined in an decomposition hierarchy  much like those in hierarchical
task networks  planning is done by evaluating abstract plans in the decomposition network  producing ranges of utility for the possible instantiations of those plans  and refining
only those plans that are possibly optimal  the use of task networks means that search is
restricted to finite horizon  open loop plans with action choice restricted to possible refinements of the network  such task networks offer a useful way to encode a priori heuristic
knowledge about the structure of good plans 
      model minimization and reduction methods

the abstraction techniques defined above can be recast in terms of minimizing a stochastic
automaton  providing a unifying view of the different methods and offering new insights
into the abstraction process  dean   givan         from automata theory we know that
for any given finite state machine m recognizing a language l there exists a unique minimal
finite state machine m   that also recognizes l  it could be that m   m     but it might also
be that m   is exponentially smaller than m   this minimal machine  called the minimal
model for the language l  captures every relevant aspect of m and so the machines are
said to be equivalent  we can define similar notions of equivalence for mdps  since we are
primarily concerned with planning  it is important that equivalent mdps agree on the value
functions for all policies  from a practical standpoint  it may not be necessary to find the
minimal model if we can find a reduced model that is suciently small but still equivalent 
we apply the idea of model minimization  or model reduction  to planning as follows 
we begin by using an algorithm that takes as input an implicit mdp model in factored form
and produces  if we are lucky  an explicit  reduced model whose size is within a polynomial
factor of the size of the factored representation  we then use our favorite state based
dynamic programming algorithms to solve the explicit model 
  

fiboutilier  dean    hanks

we can think of the dynamic programming techniques that rely on structured representations discussed earlier as operating on a reduced model without ever explicitly constructing
that model  in some cases  building the reduced model once and for all may be appropriate 
in other cases  one might save considerable effort by explicitly constructing only those parts
of the reduced model that are absolutely necessary 
there are some potential computational problems with the model minimization techniques sketched above  a small minimal model may exist  but it may be hard to find 
instead  we might look for a reduced model that is easier to find but not necessarily minimal  this too could fail  in which case we might look for a model small enough to be useful
but only approximately equivalent to the original factored model  we have to be careful
what we mean by  approximate   but intuitively two mdps are approximately equivalent
if the corresponding optimal value functions are within some small factor of one another 
in order to be practical  mdp model reduction schemes operate directly on the implicit
or factored representation of the original mdp  lee and yannakakis        call this online
model minimization  online model minimization starts with an initial partition of the states 
minimization then iteratively refines the partition by splitting clusters into smaller clusters 
a cluster is split if and only if the states in the cluster behave differently with respect to
transitions to states in the same or other clusters  if this local property is satisfied by all
clusters in a given partition  then the model consisting of aggregate states that correspond
to the clusters of this partition is equivalent to the original model  in addition  if the
initial partition and the method of splitting clusters satisfy certain properties    then we
are guaranteed to find the minimal model  in the case of mdp reduction  the initial partition
groups together states that have the same reward  or nearly the same reward in the case of
approximation methods 
the clusters of the partitions manipulated by online model reduction methods are represented intensionally as formulas involving the state variables  for instance  the formula
rhc   loc m   represents the set of all states such that the robot has coffee and is located
in the mail room  the operations performed on these clusters require conjoining  complementing  simplifying  and checking for satisfiability  in the worst case  these operations are
intractable  and so the successful application of these methods depends critically on the
problem and the way in which it is represented  we illustrate the basic idea on a simple
example 

example     figure    depicts a simple version of our running example with a single
action  there are three boolean state variables corresponding to rhc the robot has
coffee  or not  rhc   cr there is an outstanding request for coffee  or not  cr  
and  considering only two location possibilities  loc c   the robot is in the coffee
room  or not  loc c     whether there is an outstanding coffee request depends on
whether there was a request in the previous stage and whether the robot was in the
coffee room  location depends only on the location at the previous stage  and the
reward depends only on whether or not there is an outstanding coffee request 

    the property required of the initial partition is that  if two states are in the same cluster of the partition
defining the minimal model  recall that the minimal model is unique   then they must be in same cluster
in the initial partition 

  

fidecision theoretic planning  structural assumptions

st   

st

cr

cr

pr cr s t    
cr
cr
loc c 
loc c 
   
   
   

loc

loc

pr loc c  s t          

rhc

pr rhc s t    
loc c 
loc c 
rhc
rhc
   
   
   

r


r s t       if cr
   else

rhc

figure     factored model illustrating model reduction techniques 
cr  loc c 

cr
cr

cr
cr  loc c 
 a 

 b 

figure     models involving aggregate states   a  the model corresponding to the initial
partition and  b  the minimal model 
the initial partition shown in figure    a  is defined in terms of immediate rewards 
we say that all the states in a particular starting cluster behave the same with respect
to a particular destination cluster if the probability of ending up in the destination
cluster is the same for all states in the starting cluster  this property is not satisfied
for starting cluster cr and destination cluster cr in figure    a   and so we split the
cluster labeled cr to obtain the model in figure    b   now the property is satisfied
for all pairs of clusters and the model in figure    b  is the minimal model   
the lee and yannakakis algorithm for non deterministic finite state machines has been
extended by givan and dean to handle classical strips planning problems  givan   dean 
      and mdps  dean   givan         the basic step of splitting a cluster is closely
related to goal regression  a relationship explored in  givan   dean         variants of
the model reduction approach apply when the action space is large and represented in a
factored form  dean  givan    kim         for example  when each action is specified
by a set of parameters such as those corresponding to the allocations of several different
resources in an optimization problem  there also exist algorithms for computing approxi  

fiboutilier  dean    hanks

a

a

a

r

g

c

g
p

b

b

d

e

b

 a 

 b 

 c 

figure     reachability and serial problem decomposition 
mate models  dean  givan    leach        and ecient planning algorithms that use these
approximate models  givan  leach    dean        

    reachability analysis and serial problem decomposition
      reachability analysis

the existence of goal states can be exploited in different settings  for instance  in deterministic classical planning problems  regression can be viewed as a form of directed dynamic
programming  without uncertainty  a certain policy either reaches a goal state or does not 
and the dynamic programming backups need be performed only from goal states  not from
all possible states  regression  therefore  implicitly exploits certain reachability characteristics of the domain along with the special structure of the value function 
reachability analysis applied much more broadly forms the basis for various types of
problem decomposition  in decomposition problem solving  the mdp is broken into several
subprocesses that can be solved independently  or roughly independently  and the solutions
can be pieced together  if subprocesses whose solutions interact marginally are treated as
independent  we might expect a good but nonoptimal global solution to result  furthermore 
if the structure of the problem requires that only a solution to a particular subproblem is
needed  then the solutions to other subproblems can be ignored or need not be computed
at all  for instance  in regression analysis  the optimal action for states that cannot reach
a goal region is irrelevant to the solution of a classical ai planning problem  this is shown
schematically in figure    a   where regions a and b are never explored in the backward
search through state space  only states that can reach the goal within the search horizon
are ever deemed relevant  while regions a and b may be reachable from the start state  the
fact that they do not reach the goal state means they are known to be irrelevant  should
the system dynamics be stochastic  such a scheme can form the basis of an approximately
optimal solution method  regions a and b can be ignored if they are unlikely to transition
to the regression of the goal region  region r   similar remarks using progression or forward
search from the start state apply  as illustrated in figure    b  
  

fidecision theoretic planning  structural assumptions

several schemes have been proposed in the ai literature for exploiting such reachability
constraints  apart from the usual forward  or backward search approaches  peot and smith
       introduce the operator graph  a structure computed prior to problem solving that
caches reachability relationships among propositions  the graph can be consulted during
the planning process in deciding which actions to insert into the plan and how to resolve
threats 
the graphplan algorithm of blum and furst        attempts to blend considerations
of both forward and backward reachability in a deterministic planning context  one of the
diculties with regression is that we may regress the goal region through a sequence of
operators only to find ourselves in a region that cannot be reached from the initial state 
in figure    a   for example  not all states in region r may be reachable from the initial
state  graphplan constructs a variant of the operator graph called the planning graph  in
which certain forward reachability constraints are posted  regression is then implemented
as usual  but if the current subgoal set violates the forward reachability constraints at any
point  this subgoal set is abandoned and the regression search backtracks 
conceptually  one might think of graphplan as constructing a forward search tree
through state space with the initial state as the root  then doing a backward search from
the goal region backward through this tree  of course  the process is not state based 
instead  constraints on the possible variable values that can hold simultaneously at different
planning stages are recorded  and regression is used to search backward through the planning
graph  in a sense  graphplan can be viewed as constructing an abstraction in which
forward reachable states are distinguished from unreachable states at each planning stage 
and using this distinction among abstract states quickly to identify infeasible regression
paths  note  however  the graphplan approximates this distinction by overestimating
the set of reachable states  overestimation  as opposed to underestimation  ensures that
the regression search space contains all legitimate plans 
reachability has also been exploited in the solution of more general mdps  dean
et al         propose an envelope method for solving  goal based  mdps approximately 
assuming some path can be generated quickly from a given start state to the goal region 
an mdp consisting of the states on this path and perhaps neighboring states is solved  to
deal with transitions that lead out of this envelope  a heuristic method estimates a value for
these states    as time permits  the set of neighboring states can be expanded  increasing
solution quality by more accurately evaluating the quality of alternative actions 
some of the ideas underlying graphplan have been applied to more general mdps in
 boutilier  brafman    geib         where the construction of a planning graph is generalized to deal with the stochastic  conditional action representation offered by  tbns  given
an initial state  or set of initial states   this algorithm discovers reachability constraints
that have a form like those in graphplan   for instance  that two variable values x   x 
and y   y  cannot both obtain simultaneously  that is  no action sequence starting at the
given initial state can lead to a state in which these values both hold    the reachability
constraints discovered by this process are then used to simplify the action and reward representation of an mdp so that it refers only to reachable states  in this case  any action that
    the approximate abstraction techniques described in section       might be used to generate such
heuristic information 
    general k ary constraints of this type are considered in  boutilier et al         

  

fiboutilier  dean    hanks

requires an unreachable set of values to hold is effectively deleted  in some cases  certain
variables are discovered to be immutable given the initial conditions and can themselves be
deleted  leading to much smaller mdps  this simplified representation retains the original
propositional structure so standard abstraction methods can be applied to the reachable
mdp  it is also suggested that a strong synergy exists between abstraction and reachability analysis such that together these techniques reduce the size of the  effective  mdp to
be solved much more dramatically than either does in isolation  just as reachability constraints can be used to prune regression paths in deterministic domains  they can be used
to prune value function and policy estimates generated by decision theoretic regression and
abstraction algorithms  boutilier et al         
the results reported in  boutilier et al         are limited to a single process planning
domain  but show that reachability analysis together with abstraction can provide substantial reductions in the size of the effective mdp that must be solved  at least in some domains 
in a domain with    binary variables  reachability considerations generally eliminated on
the order of    to    variables  depending on the initial state and the arity binary or
ternary of the constraints considered   reducing the state space from size     to anywhere
from     to       incorporating abstraction on the reachable mdp provided considerably
more reduction  reducing the mdp to sizes ranging from    to effectively zero states  the
latter case would occur if it is discovered that no values of variables that impact reward
can be altered in which case every course of action has the same expected utility and the
mdp needn t be solved  or can be solved by applying null actions with zero cost  
      serial problem decomposition and communicating structure

the communicating or reachability structure of an mdp provides a way to formalize different types of problem decomposition  we can classify an mdp according to the markov
chains induced by the stationary policies it admits  for a fixed markov chain  we can group
states into maximal recurrent classes and transient states  as described in section      an
mdp is recurrent if each policy induces a markov chain with a single recurrent class  an
mdp is unichain if each policy induces a single recurrent class with  possibly  some transient states  an mdp is communicating if for any pair of states s  t  there is some policy
under which s can reach t  an mdp is weakly communicating if there exists a closed set
of states that is communicating plus  possibly  a set of states transient under every policy 
we call other mdps noncommunicating 
these notions are crucial in the construction of optimal average reward policies  but can
also be exploited in problem decomposition  suppose an mdp is discovered to consist of a
set of recurrent classes c       cn  i e   no matter what policy is adopted  the agent cannot
leave any such class once it enters that class  and a set of transient states    it is clear that
optimal policy restricted to any class ci can be constructed without reference to the policy
decisions made at any states outside of ci or even their values  essentially  each ci can be
viewed as an independent subprocess 
    a simple way to view these classes is to think of the agent adopting a randomized policy where each action
is adopted at any state with positive probability  the classes of the induced markov chain correspond
to the classes of the mdp 

  

fidecision theoretic planning  structural assumptions

this observation leads to the following suggestion for optimal policy construction    we
solve the subprocesses consisting of the recurrent classes for the mdps  we then remove
these states from the mdp  forming a reduced mdp consisting only of the transient states 
we then break the reduced mdp into its recurrent classes and solve these independently 
the key to doing this effectively is to use the value function for the original recurrent
states  computed in solving the independent subproblems in step    to take into account
transitions out of the recurrent classes in the reduced mdp  figure    c  shows an mdp
broken into the classes that might be constructed this way  in the original mdp  classes c
and e are recurrent and can be solved independently  once removed from the mdp  class
d is recurrent in the reduced mdp  it can  of course  be solved without reference to classes
a and b   but does rely on the value of the states that it transitions to in class e   however 
the value function for e is available for this purpose  and can be used to solve for d as
if d consisted only of jdj states  with this in hand  b can then be solved  and finally a
can be solved  lin and dean        provide a version of this type of decomposition that
also employs a factored representation  the factored representation allows dimensionality
reduction in different state subspaces by aggregating states that differ only in the values of
the irrelevant variables in their subspaces 
a key to such a decomposition is the discovery of the recurrent classes of an mdp 
puterman        suggests an adaptation of the fox landi algorithm  fox   landi       
for discovering the structure of markov chains that is o n      recall n   jsj     to alleviate
the diculties of algorithms that work with an explicit state based representation  boutilier
and puterman        propose a variant of the algorithm that works with a factored  tbn
representation 
one diculty with this form of decomposition is its reliance on strongly independent
subproblems  i e   recurrent classes  within the mdp  others have explored exact and approximate techniques that work under less restrictive assumptions  one simple method of
approximation is to construct  approximately recurrent classes   in figure    c  we might
imagine that c and e are nearly independent in the sense that all transitions between them
are very low probability or high cost  treating them as independent might lead to approximately optimal policies whose error can be bounded  if the solutions to c and e interact
strongly enough that the solutions should not be constructed completely independently  a
different approach to solving the decomposed problem can be taken 
if we have the optimal value function for e then  as pointed out  we can calculate the
optimal value function for d  the first thing to note is that we don t need to know the
value function for all of the states in e   just the value of every state in e that is reachable
from some state in d in a single step  the set of all states outside d reachable in a single
step from a state inside d is referred to as the states in the periphery of d  the values of
the states in the intersection of e and the periphery of d summarize the value of exiting d
and ending up in e   we refer to the set of all states that are in the periphery of some block
as the kernel of the mdp  all of the different blocks interact with one another through
states in the kernel 
    ross and varadarajan        make a related suggestion for solving average reward problems 
    a slight correction is made to the suggested algorithm in  boutilier   puterman        

  

fiboutilier  dean    hanks

loc c 

loc l 

loc m  

loc o 

figure     decomposition based on location 

loc c 

loc l 
kernel

loc m  

loc o 

figure     kernel based decomposition depicting the kernel states 

  

fidecision theoretic planning  structural assumptions

example     spatial features often provide a natural dimension along which to decom 

pose a domain  in our running example  the location of the robot might be used to
decompose the state space into blocks of states  one block for each of the possible locations  figure    shows such a decomposition superimposed over the state transition
diagram for the mdp  states in the kernel are shaded and might correspond to the
entrances and exits of locations  the star shaped topology  induced by the kernel
decomposition used in  kushner   chen        and  dean   lin         is illustrated
in figure     in figure     the hallway location is not explicitly represented  this
simplification may be reasonable if the hallway is only a conduit for moving from
one room to another  in this case the function of the hallway is accounted for in the
dynamics governing states in the kernel  figures    and    are idealized in that  given
the full set of features in our running example  the kernel would contain many more
states   

one technique for computing the optimal policy for the entire mdp involves repeatedly
solving the mdps corresponding to the individual blocks  the techniques works as follows 
initially  we guess the value of every state in the kernel    given a current estimate for the
values of the kernel states  we solve the component mdps  this solution produces a new
estimate for the states in the kernel  we adjust the values of the states in the kernel by
considering the difference between the current and the new estimates and iterate until this
difference is negligible 
this iterative method for solving a decomposed mdp is a special case of the lagrangian
method for finding the extrema of a function  the or literature is replete with such
methods for both linear and nonlinear systems of equations  winston         it is possible
to formulate an mdp as a linear program  d epenoux        puterman         dantzig and
wolfe        developed a method of decomposing a system of equations involving a very
large number of variables into a set of smaller systems of equations interacting through a set
of coupling variables  variables shared by more two or more blocks   in the dantzig wolfe
decomposition method  the original  very large system of equations is solved by iteratively
solving the smaller systems and adjusting the coupling variables on each iteration until no
further adjustment is required  in the linear programming formulation of an mdp  the
values of the states are encoded as variables 
kushner and chen        exploit the fact that mdps can be modeled as linear programs
by using the dantzig wolfe decomposition method to solve mdps involving a large number
of states  dean and lin        describe a general framework for solving decomposed mdps
pointing to the work of kushner and chen as a special case  but neither work addresses
the issue of where the decompositions come from  dean et al         investigate methods
for decomposing the state space into two blocks  those reachable in k steps or fewer and
those not reachable in k steps  see the discussion of reachability above   the set of states
reachable in k or fewer steps is used to construct an mdp that is the basis for a policy that
approximates the optimal policy  as k increases  the size of the block of states reachable in
k steps increases  ensuring a better solution  but the amount of time required to compute a
    ideally we would aggregate kernel states with the same value so as to provide a compact representation 
in the remainder of this section  however  we won t consider this or any other opportunities for combining
aggregation and decomposition methods 

  

fiboutilier  dean    hanks

solution also increases  dean et al         discuss methods for solving mdps in time critical
problems by trading off quality against time 
we have ignored the issue of how to obtain decompositions that expedite our calculations  ideally  each component of the decomposition would yield to simplification via
aggregation and abstraction  reducing the dimensionality in each component and thereby
avoiding explicit enumeration of all the states  lin        presents methods for exploiting
structure for certain special cases in which the communicating structure is revealed by a
domain expert  in general  however  finding a decomposition so as to minimize the effort
spent in solving the component mdps is quite hard  at least as hard as finding the smallest circuit consistent with a given input output behavior  and so the best we can hope for
are good heuristic methods  unfortunately  we are not aware of any particularly useful
heuristics for finding serial decompositions for markov decision processes  developing such
heuristics is clearly an area for investigation 
related to this form of decomposition is the development of macro operators for mdps
 sutton         macros have a long history in classical planning and problem solving  fikes 
hart    nilsson        korf         but only recently have they been generalized to mdps
 hauskrecht  meuleau  kaelbling  dean    boutilier        parr        parr   russell       
precup  sutton    singh        stone   veloso        sutton        thrun   schwartz 
       in most of this work  a macro is taken to be a local policy over a region of state
space  or block in the above terminology   given an mdp comprising these blocks and a
set of macros defined for each block  the mdp can be solved by selecting a macro action
for each block such that the global policy induced by the set of macros so picked is close
to optimal  or at the very least is the best combination of macros from the set available 
in  sutton        precup et al          macros are treated as temporally abstract actions
and models are defined by which a macro can be treated as if it were a single action and
used in policy or value iteration  along with concrete actions   in  hauskrecht et al        
parr        parr   russell         these models are exploited in a hierarchical fashion  with
a high level mdp consisting only of states lying on the boundaries of blocks  and macros
the only  actions  that can be chosen at these states  the issue of macro generation 
constructing a set of macros guaranteed to provide the exibility to select close to optimal
global behavior is addressed in  hauskrecht et al         parr         the relationship
to serial decomposition techniques is quite close  thus  the problems of discovering good
decompositions  constructing good sets of macros  and exploiting intensional representations
are areas in which clearer  compelling solutions are required  to date  work in this area has
not provided much computational utility in the solution of mdps except in cases where
good  hand crafted  region based decompositions and macros can be provided and little
of this work has taken into account the factored nature of many mdps  for this reason  we
do not discuss it in detail  however  the general notion of serial decomposition continues to
develop and shows great promise 

    multiattribute reward and parallel decomposition
another form of decomposition is parallel decomposition  in which an mdp is broken into
a set of sub mdps that are  run in parallel   specifically  at each stage of the  global 
decision process  the state of each subprocess in affected  for instance  in figure     action
  

fidecision theoretic planning  structural assumptions

a

mdp 

a

mdp 

a

mdp 

figure     parallel problem decomposition 

a affects the state of each subprocess  intuitively  an action is suitable for execution in the

original mdp at some state if it is reasonably good in each of the sub mdps 
generally  the sub mdps form either a product or join decomposition of the original
state space  contrast this with the union decompositions of state space determined by serial
decompositions   the state space is formed by taking the cross product of the sub mdp state
spaces  or the join if certain states in the subprocesses cannot be linked  the subprocesses
may have identical action spaces  as in figure      or each may have its own action space 
with the global action choice being factored into a choice for each subprocess  in the latter
case  the sub mdps may be completely independent  in which case the  global  mdp can be
solved exponentially faster  a more challenging problem arises when there are constraints
on the legal action combinations  for example  if the actions in the subprocesses each
require certain shared resources  interactions in the global choice may arise 
in a parallel mdp decomposition  we wish to solve the sub mdps and use the policies
or value functions generated to help construct an optimal or approximately optimal solution
to the original mdp  highlighting the need to find appropriate decompositions for mdps
and to develop suitable merging techniques  recent parallel decomposition methods have
all involved decomposing an mdp into subprocesses suitable for distinct objectives  since
reward functions often deal with multiple objectives  each associated with an independent
reward  and whose rewards can be summed to determine a global reward  this is often a
very natural way to decompose mdps  thus  ideas from multiattribute utility theory can
be seen to play a role in the solution of mdps 
boutilier et al         decompose an mdp specified using  tbns and an additive reward
function using the abstraction technique described in section        for each component
of the reward function  abstraction is used to generate an mdp referring only to variables
relevant to that component    since certain state variables may be present in multiple
sub mdps  i e   relevant to more than one objective   the original state space in the join of
the subspaces  thus  decomposition is tackled automatically  merging is tackled in several
ways  one involves using the sum of the value functions obtained by solving the sub mdps
as a heuristic estimate of the true value function  this heuristic is then used to guide online 
state based search  see section         if the sub mdps do not interact  then this heuristic
is perfect and leads to backtrack free optimal action selection  if they interact  search is
    note that the existence of a factored mdp representation is crucial for this abstraction method 

  

fiboutilier  dean    hanks

required to detect conicts  note that each sub mdp has identical sets of actions  if the
action space is large  the branching factor of the search process may be prohibitive 
singh and cohn        also deal with parallel decomposition  though they assume the
global mdp is specified explicitly as a set of parallel mdps  thus generating decompositions
of a global mdp is not at issue  the global mdp is given by the cross product of the state
and action spaces of these sub mdps and the reward functions are summed  however 
constraints on the feasible action combinations couple the solutions of these sub mdps  to
solve the global mdp  the sum of the sub mdp value functions is used as an upper bound
on the optimal global value function  while the maximum of these  at any global state  is
used as a lower bound  these bounds then form the basis of an action elimination procedure
in a value iteration algorithm for solving the global mdp    unfortunately  value iteration
is run over the explicit state space of the global mdp  since the action space is also a cross
product  this is a potential computational bottleneck for value iteration  as well 
meuleau et al         use parallel decomposition to approximate the solution of stochastic resource allocation problems with very large state and action spaces  much like singh
and cohn         an mdp is specified in terms of a number of independent mdps  each
involving a distinct objective  whose action choices are linked through shared resource constraints  the value functions for the individual mdps are constructed oine and then used
in set of online action selection procedures  unlike many of the approximation procedures
we have discussed  this approach makes no attempt to construct a policy explicitly  and
is similar to real time search or rtdp in this respect  nor to construct the value function
explicitly  this method has been applied to very large mdps  with state spaces of size
      and actions spaces that are even larger  and can solve such problems in roughly half
an hour  the solutions produced are approximate  but the size of the problem precludes
exact solution  so good estimates of solution quality are hard to derive  however  when the
same method is applied to smaller problems of the same nature whose exact solution can
be computed  the approximations have very high quality  meuleau et al          while able
to solve very large mdps  with large  but factored  state and action spaces   the model
relies on somewhat restrictive assumptions about the nature of the local value functions
that ensure good solution quality  however  the basic approach appears to be generalizable 
and offers great promise for solving very large factored mdps 
the algorithms in both  singh   cohn        and  meuleau et al         can be seen
to rely at least implicitly on structured mdp representations involving almost independent
subprocesses  it seems likely that such approaches could take further advantage of automatic
mdp decomposition algorithms such as that of  boutilier et al          where factored
representations explicitly play a part 

    summary

we have seen a number of ways in which intensional representations can be exploited to
solve mdps effectively without enumeration of the state space  these include techniques
for abstraction of mdps  including those based on relevance analysis  goal regression and
decision theoretic regression  techniques relying on reachability analysis and serial decomposition  and methods for parallel mdp decomposition exploiting the multiattribute nature
    singh and cohn        also incorporate methods for removing unreachable states during value iteration 

  

fidecision theoretic planning  structural assumptions

of reward functions  many of these methods can  in fortunate circumstances  offer exponential reduction is solution time and space required to represent a policy and value function 
but none come with guarantees of such reductions except in certain special cases  while
most of the methods described provide approximate solutions  often with error bounds provided   some of them offer optimality guarantees in general  and most can provide optimal
solutions under suitable assumptions 
one avenue that has not been explored in detail is the relationship between the structured solution methods developed for mdps described above and techniques used for solving
bayesian networks  since many of the algorithms discussed in this section rely on the structure inherent in the  tbn representation of the mdp  it is natural to ask whether they
embody some of the intuitions that underlie solution algorithms for bayes nets  and thus
whether the solution techniques for bayes nets can be  directly or indirectly  applied to
mdps in ways that give rise to algorithms similar to those discussed here  this remains an
open question at this point  but undoubtedly some strong ties exist  tatman and shachter
       have explored the connections between inuence diagrams and mdps  kjaerulff
       has investigated computational considerations involved in applying join tree methods
for reasoning tasks such as monitoring and prediction in temporal bayes nets  the abstraction methods discussed in section       can be interpreted as a form of variable elimination
 dechter        zhang   poole         elimination of variables occurs in temporal order 
but good orderings within a time slice must also exploit the tree or graph structure of the
cpts  approximation schemes based on variable elimination  dechter        poole       
may also be related to certain of the approximation methods developed for mdps  the
independence based decompositions of mdps discussed in section     can clearly be viewed
as exploiting the independence relations made explicit by  unrolling  a  tbn  the development of these and other connections to bayes net inference algorithms will no doubt prove
very useful in enhancing our understanding of existing methods  increasing their range of
applicability and pointing to new algorithms 

   concluding remarks
the search for effective algorithms for controlling automated agents has a long and important history  and the problem will only continue to grow in importance as more decisionmaking functionality is automated  work in several disciplines  among them ai  decision
analysis  and or  has addressed the problem  but each has carried with it different problem definitions  different sets of simplifying assumptions  different viewpoints  and hence
different representations and algorithms for problem solving  more often than not  the assumptions seem to have been made for historical reasons or reasons of convenience  and it
is often dicult to separate the essential assumptions from the accidental  it is important
to clarify the relationships among problem definitions  crucial assumptions  and solution
techniques  because only then can a meaningful synthesis take place 
in this paper we analyzed various approaches to a particular class of sequential decision problems that have been studied in the or  decision analysis  and ai literature  we
started with a general  reasonably neutral statement of the problem  couched  for convenience  in the language of markov decision processes  from there we demonstrated how
various disciplines define the problem  i e   what assumptions they make   and the effect
  

fiboutilier  dean    hanks

of these assumptions on the worst case time complexity of solving the problem so defined 
assumptions regarding two main factors seem to distinguish the most commonly studied
classes of decision problems 

 observation or sensing  does sensing tend to be fast  cheap  and accurate or laborious 
costly and noisy 

 the incentive structure for the agent  is its behavior evaluated on its ability to perform
a particular task  or on its ability to control a system over an interval of time 

moving beyond the worst case analysis  it is generally assumed that  although pathological cases are inevitably dicult  the agent should be able to solve  typical  or  easy 
cases effectively  to do so  the agent needs to be able to identify structure in the problem
and to exploit that structure algorithmically 
we identified three ways in which structural regularities can be recognized  represented 
and exploited computationally  the first is structure induced by domain level simplifying
assumptions like full observability  goal satisfaction or time separable value functions  and
so on  the second is structure exploited by compact domain specific encodings of states 
actions  and rewards  the designer can use these techniques to make structure explicit  and
decision making algorithms can then exploit the structural regularities as they apply to the
particular problem at hand  the third involves aggregation  abstraction and decomposition techniques  whereby structural regularities can be discovered and exploited during the
problem solving process itself  in developing this framework one that allows comparison
of domains  assumptions  problems  and techniques drawn from different disciplines we
discover the essential problem structure required for specific representations and algorithms
to prove effective  and we do so in such a way that the insights and techniques developed
for certain problems  or within certain disciplines  can be evaluated and potentially applied
to new problems  or within other disciplines 
a main focus of this work has been the elucidation of various forms of structure in
decision problems and of how each can be exploited representationally or computationally 
for the most part  we have focused on propositional structure  which is most commonly associated with planning in ai circles  a more complete treatment would also have included
other compact representations of dynamics  rewards  policies  and value functions often
considered in continuous  real valued domains  for instance  we have not discussed linear
dynamics and quadratic cost functions  often used in control theory  caines         or the
use of neural network representations of value functions  as frequently adopted within the
reinforcement learning community  bertsekas   tsitsiklis        tesauro           nor have
we discussed the partitioning of continuous state spaces often addressed in reinforcement
learning  moore   atkeson         neither have we addressed the relational or quantificational structure used in first order planning representations  however  even these techniques
can be cast within the framework described here  for example  the use of piecewise linear
value functions can be seen as a form of abstraction in which different linear components
are applied to different regions or clusters of state space 
    bertsekas and tsitsiklis        provide an in depth treatment of neural network and linear function
approximators for mdps and reinforcement learning 

  

fidecision theoretic planning  structural assumptions

although in certain cases we have indicated how to devise methods that exploit several
types of structure at once  research along these lines has been limited  to some extent 
many of the representations and algorithms described in this paper are complementary and
should pose few obstacles to combination  it remains to be seen how they interact with
techniques developed for other forms of structure  such as those used for continuous state
and action spaces 
so our analysis raises opportunities and challenges  by understanding the assumptions 
the techniques  and their relationships  a designer of decision making agents has many more
tools with which to build effective problem solvers  and the challenges lie in the development
of additional tools and the integration of existing ones 

acknowledgments

many thanks to the careful comments of the referees  thanks to ron parr and robert
st aubin for their comments on an earlier draft of this paper  the students taking cs    
 spring       taught by martha pollack at the university of pittsburgh and cpsc   
 winter       at the university of british columbia also deserve thanks for their detailed
comments 
boutilier was supported by nserc research grant ogp         and the nce irisii program project ic    dean was supported in part by a national science foundation
presidential young investigator award iri         and by the air force and the advanced
research projects agency of the department of defense under contract no  f         c      hanks was supported in part by arpa   rome labs grant f                and
in part by nsf grant iri         

references

allen  j   hendler  j     tate  a   eds            readings in planning  morgan kaufmann 
san mateo 
astrom  k  j          optimal control of markov decision processes with incomplete state
estimation  j  math  anal  appl               
bacchus  f   boutilier  c     grove  a          rewarding behaviors  in proceedings of
the thirteenth national conference on artificial intelligence  pp            portland 
or 
bacchus  f   boutilier  c     grove  a          structured solution methods for nonmarkovian decision processes  in proceedings of the fourteenth national conference
on artificial intelligence  pp          providence  ri 
bacchus  f     kabanza  f          using temporal logic to control search
in a forward chaining planner 
in proceedings of the third european
workshop on planning  ewsp     assisi  italy  available via the url
ftp   logos uwaterloo ca  pub tlplan tlplan ps z 
bacchus  f     teh  y  w          making forward chaining relevant  in proceedings of the
fourth international conference on ai planning systems  pp        pittsburgh  pa 
  

fiboutilier  dean    hanks

bahar  r  i   frohm  e  a   gaona  c  m   hachtel  g  d   macii  e   pardo  a     somenzi 
f          algebraic decision diagrams and their applications  in international conference on computer aided design  pp           ieee 
baker  a  b          nonmonotonic reasoning in the framework of the situation calculus 
artificial intelligence           
barto  a  g   bradtke  s  j     singh  s  p          learning to act using real time dynamic
programming  artificial intelligence                   
bellman  r          dynamic programming  princeton university press  princeton  nj 
bertsekas  d  p     castanon  d  a          adaptive aggregation for infinite horizon
dynamic programming  ieee transactions on automatic control                  
bertsekas  d  p          dynamic programming  prentice hall  englewood cliffs  nj 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena  belmont 
ma 
blackwell  d          discrete dynamic programming  annals of mathematical statistics 
            
blum  a  l     furst  m  l          fast planning through graph analysis  in proceedings
of the fourteenth international joint conference on artificial intelligence  pp       
     montreal  canada 
bonet  b     geffner  h          learning sorting and decision trees with pomdps  in
proceedings of the fifteenth international conference on machine learning  pp       
madison  wi 
bonet  b   loerincs  g     geffner  h          a robust and fast action selection mechanism 
in proceedings of the fourteenth national conference on artificial intelligence  pp 
        providence  ri 
boutilier  c          correlated action effects in decision theoretic regression  in proceedings of the thirteenth conference on uncertainty in artificial intelligence  pp       
providence  ri 
boutilier  c   brafman  r  i     geib  c          prioritized goal decomposition of markov
decision processes  toward a synthesis of classical and decision theoretic planning  in
proceedings of the fifteenth international joint conference on artificial intelligence 
pp            nagoya  japan 
boutilier  c   brafman  r  i     geib  c          structured reachability analysis for markov
decision processes  in proceedings of the fourteenth conference on uncertainty in
artificial intelligence  pp        madison  wi 
boutilier  c     dearden  r          using abstractions for decision theoretic planning
with time constraints  in proceedings of the twelfth national conference on artificial
intelligence  pp            seattle  wa 
  

fidecision theoretic planning  structural assumptions

boutilier  c     dearden  r          approximating value trees in structured dynamic
programming  in proceedings of the thirteenth international conference on machine
learning  pp        bari  italy 
boutilier  c   dearden  r     goldszmidt  m          exploiting structure in policy construction  in proceedings of the fourteenth international joint conference on artificial
intelligence  pp            montreal  canada 
boutilier  c   dearden  r     goldszmidt  m          stochastic dynamic programming
with factored representations   manuscript  
boutilier  c   friedman  n   goldszmidt  m     koller  d          context specific independence in bayesian networks  in proceedings of the twelfth conference on uncertainty
in artificial intelligence  pp          portland  or 
boutilier  c     goldszmidt  m          the frame problem and bayesian network action
representations  in proceedings of the eleventh biennial canadian conference on
artificial intelligence  pp        toronto 
boutilier  c     poole  d          computing optimal policies for partially observable
decision processes using compact representations  in proceedings of the thirteenth
national conference on artificial intelligence  pp            portland  or 
boutilier  c     puterman  m  l          process oriented planning and average reward optimality  in proceedings of the fourteenth international joint conference on artificial
intelligence  pp            montreal  canada 
brafman  r  i          a heuristic variable grid solution method for pomdps  in proceedings of the fourteenth national conference on artificial intelligence  pp         
providence  ri 
bryant  r  e          graph based algorithms for boolean function manipulation  ieee
transactions on computers  c                 
bylander  t          the computational complexity of propositional strips planning 
artificial intelligence              
caines  p  e          linear stochastic systems  wiley  new york 
cassandra  a  r   kaelbling  l  p     littman  m  l          acting optimally in partially
observable stochastic domains  in proceedings of the twelfth national conference on
artificial intelligence  pp            seattle  wa 
cassandra  a  r   littman  m  l     zhang  n  l          incremental pruning  a simple  fast  exact method for pomdps  in proceedings of the thirteenth conference on
uncertainty in artificial intelligence  pp        providence  ri 
chapman  d          planning for conjunctive goals  artificial intelligence                  
  

fiboutilier  dean    hanks

chapman  d     kaelbling  l  p          input generalization in delayed reinforcement
learning  an algorithm and performance comparisons  in proceedings of the twelfth
international joint conference on artificial intelligence  pp          sydney  australia 
dantzig  g     wolfe  p          decomposition principle for dynamic programs  operations
research                 
dean  t   allen  j     aloimonos  y          artificial intelligence  theory and practice 
benjamin cummings 
dean  t     givan  r          model minimization in markov decision processes  in
proceedings of the fourteenth national conference on artificial intelligence  pp      
    providence  ri  aaai 
dean  t   givan  r     kim  k  e          solving planning problems with large state and
action spaces  in proceedings of the fourth international conference on ai planning
systems  pp          pittsburgh  pa 
dean  t   givan  r     leach  s          model reduction techniques for computing approximately optimal solutions for markov decision processes  in proceedings of the
thirteenth conference on uncertainty in artificial intelligence  pp          providence  ri 
dean  t   kaelbling  l   kirman  j     nicholson  a          planning with deadlines in
stochastic domains  in proceedings of the eleventh national conference on artificial
intelligence  pp          
dean  t   kaelbling  l   kirman  j     nicholson  a          planning under time constraints in stochastic domains  artificial intelligence                 
dean  t     kanazawa  k          a model for reasoning about persistence and causation 
computational intelligence                 
dean  t     lin  s  h          decomposition techniques for planning in stochastic domains  in proceedings of the fourteenth international joint conference on artificial
intelligence  pp            
dean  t     wellman  m          planning and control  morgan kaufmann  san mateo 
california 
dearden  r     boutilier  c          integrating planning and execution in stochastic
domains  in proceedings of the tenth conference on uncertainty in artificial intelligence  pp          washington  dc 
dearden  r     boutilier  c          abstraction and approximate decision theoretic planning  artificial intelligence              
dechter  r          bucket elimination  a unifying framework for probabilistic inference 
in proceedings of the twelfth conference on uncertainty in artificial intelligence  pp 
        portland  or 
  

fidecision theoretic planning  structural assumptions

dechter  r          mini buckets  a general scheme for generating approximations in
automated reasoning in probabilistic inference  in proceedings of the fifteenth international joint conference on artificial intelligence  pp            nagoya  japan 
d epenoux  f          sur un probleme de production et de stockage dans l aleatoire 
management science             
dietterich  t  g     flann  n  s          explanation based learning and reinforcement
learning  a unified approach  in proceedings of the twelfth international conference
on machine learning  pp          lake tahoe  nv 
draper  d   hanks  s     weld  d       a   a probabilistic model of action for leastcommitment planning with information gathering  in proceedings of the tenth conference on uncertainty in artificial intelligence  pp          washington  dc 
draper  d   hanks  s     weld  d       b   probabilistic planning with information gathering and contingent execution  in proceedings of the second international conference
on ai planning systems  pp        
etzioni  o   hanks  s   weld  d   draper  d   lesh  n     williamson  m          an
approach to planning with incomplete information  in proceedings of the third international conference on principles of knowledge representation and reasoning  pp 
        boston  ma 
fikes  r   hart  p     nilsson  n          learning and executing generalized robot plans 
artificial intelligence             
fikes  r     nilsson  n  j          strips  a new approach to the application of theorem
proving to problem solving  artificial intelligence             
finger  j          exploiting constraints in design synthesis  ph d  thesis  stanford university  stanford 
floyd  r  w          algorithm     shortest path   communications of the acm        
    
fox  b  l     landi  d  m          an algorithm for identifying the ergodic subchains and
transient states of a stochastic matrix  communications of the acm             
french  s          decision theory  halsted press  new york 
geiger  d     heckerman  d          advances in probabilistic reasoning  in proceedings
of the seventh conference on uncertainty in artificial intelligence  pp          los
angeles  ca 
givan  r     dean  t          model minimization  regression  and propositional strips
planning  in proceedings of the fifteenth international joint conference on artificial
intelligence  pp            nagoya  japan 
  

fiboutilier  dean    hanks

givan  r   leach  s     dean  t          bounded parameter markov decision processes  in
proceedings of the fourth european conference on planning  ecp      pp         
toulouse  france 
goldman  r  p     boddy  m  s          representing uncertainty in simple planners 
in proceedings of the fourth international conference on principles of knowledge
representation and reasoning  pp          bonn  germany 
haddawy  p     doan  a          abstracting probabilistic actions  in proceedings of the
tenth conference on uncertainty in artificial intelligence  pp          washington 
dc 
haddawy  p     hanks  s          utility models for goal directed decision theoretic
planners  computational intelligence         
haddawy  p     suwandi  m          decision theoretic refinement planning using inheritence abstraction  in proceedings of the second international conference on ai planning systems  pp          chicago  il 
hanks  s          projecting plans for uncertain worlds  ph d  thesis      yale university 
department of computer science  new haven  ct 
hanks  s     mcdermott  d  v          modeling a dynamic and uncertain world i  symbolic
and probabilistic reasoning about change  artificial intelligence               
hanks  s   russell  s     wellman  m   eds            decision theoretic planning  proceedings of the aaai spring symposium  aaai press  menlo park 
hansen  e  a     zilberstein  s          heuristic search in cyclic and or graphs  in
proceedings of the fifteenth national conference on artificial intelligence  pp      
    madison  wi 
hauskrecht  m          a heuristic variable grid solution method for pomdps  in proceedings of the fourteenth national conference on artificial intelligence  pp         
providence  ri 
hauskrecht  m          planning and control in stochastic domains with imperfect information  ph d  thesis  massachusetts institute of technology  cambridge 
hauskrecht  m   meuleau  n   kaelbling  l  p   dean  t     boutilier  c          hierarchical
solution of markov decision processes using macro actions  in proceedings of the
fourteenth conference on uncertainty in artificial intelligence  pp          madison 
wi 
hoey  j   st aubin  r   hu  a     boutilier  c          spudd  stochastic planning
using decision diagrams  in proceedings of the fifteenth conference on uncertainty
in artificial intelligence stockholm  to appear 
howard  r  a          dynamic programming and markov processes  mit press  cambridge  massachusetts 
  

fidecision theoretic planning  structural assumptions

howard  r  a     matheson  j  e          inuence diagrams  in howard  r  a     matheson  j  e   eds    the principles and applications of decision analysis  strategic
decisions group  menlo park  ca 
kambhampati  s          refinement planning as a unifying framework for plan synthesis 
ai magazine  summer             
kearns  m   mansour  y     ng  a  y          a sparse sampling algorithm for nearoptimal planning in large markov decision processes  in proceedings of the sixteenth
international joint conference on artificial intelligence stockholm  to appear 
keeney  r  l     raiffa  h          decisions with multiple objectives  preferences and
value tradeoffs  john wiley and sons  new york 
kjaerulff  u          a computational scheme for reasoning in dynamic probabilistic networks  in proceedings of the eighth conference on uncertainty in ai  pp         
stanford 
knoblock  c  a          generating abstraction hierarchies  an automated approach to
reducing search in planning  kluwer  boston 
knoblock  c  a   tenenberg  j  d     yang  q          characterizing abstraction hierarchies for planning  in proceedings of the ninth national conference on artificial
intelligence  pp          anaheim  ca 
koenig  s          optimal probabilistic and decision theoretic planning using markovian
decision theory  m sc  thesis ucb csd         university of california at berkeley 
computer science department 
koenig  s     simmons  r          real time search in nondeterministic domains  in
proceedings of the fourteenth international joint conference on artificial intelligence 
pp            montreal  canada 
korf  r          macro operators  a weak method for learning  artificial intelligence     
      
korf  r  e          real time heuristic search  artificial intelligence              
kushmerick  n   hanks  s     weld  d          an algorithm for probabilistic planning 
artificial intelligence              
kushner  h  j     chen  c  h          decomposition of systems governed by markov
chains  ieee transactions on automatic control                  
lee  d     yannakakis  m          online minimization of transition systems  in proceedings
of   th annual acm symposium on the theory of computing  pp          victoria 
bc 
lin  f     reiter  r          state constraints revisited  journal of logic and computation 
               
  

fiboutilier  dean    hanks

lin  s  h          exploiting structure for planning and control  ph d  thesis  department
of computer science  brown university 
lin  s  h     dean  t          generating optimal policies for high level plans with conditional branches and loops  in proceedings of the third european workshop on
planning  ewsp      pp          
littman  m  l          probabilistic propositional planning  representations and complexity  in proceedings of the fourteenth national conference on artificial intelligence 
pp          providence  ri 
littman  m  l   dean  t  l     kaelbling  l  p          on the complexity of solving
markov decision problems  in proceedings of the eleventh conference on uncertainty
in artificial intelligence  pp          montreal  canada 
littman  m  l          algorithms for sequential decision making  ph d  thesis cs       
brown university  department of computer science  providence  ri 
lovejoy  w  s       a   computationally feasible bounds for partially observed markov
decision processes  operations research                  
lovejoy  w  s       b   a survey of algorithmic methods for partially observed markov
decision processes  annals of operations research            
luenberger  d  g          introduction to linear and nonlinear programming  addisonwesley  reading  massachusetts 
luenberger  d  g          introduction to dynamic systems  theory  models and applications  wiley  new york 
madani  o   condon  a     hanks  s          on the undecidability of probabilistic planning
and infinite horizon partially observable markov decision problems  in proceedings of
the sixteenth national conference on artificial intelligence orlando  fl  to appear 
mahadevan  s          to discount or not to discount in reinforcement learning  a case
study in comparing r learning and q learning  in proceedings of the eleventh international conference on machine learning  pp          new brunswick  nj 
mcallester  d     rosenblitt  d          systematic nonlinear planning  in proceedings of
the ninth national conference on artificial intelligence  pp          anaheim  ca 
mccallum  r  a          instance based utile distinctions for reinforcement learning with
hidden state  in proceedings of the twelfth international conference on machine
learning  pp          lake tahoe  nevada 
mccarthy  j     hayes  p  j          some philosophical problems from the standpoint of
artificial intelligence  machine intelligence             
  

fidecision theoretic planning  structural assumptions

meuleau  n   hauskrecht  m   kim  k   peshkin  l   kaelbling  l   dean  t     boutilier  c 
        solving very large weakly coupled markov decision processes  in proceedings
of the fifteenth national conference on artificial intelligence  pp          madison 
wi 
moore  a  w     atkeson  c  g          the parti game algorithm for variable resolution
reinforcement learning in multidimensional state spaces  machine learning          
    
papadimitriou  c  h     tsitsiklis  j  n          the complexity of markov chain decision
processes  mathematics of operations research                  
parr  r          flexible decomposition algorithms for weakly coupled markov decision
processes  in proceedings of the fourteenth conference on uncertainty in artificial
intelligence  pp          madison  wi 
parr  r     russell  s          approximating optimal policies for partially observable
stochastic domains  in proceedings of the fourteenth international joint conference
on artificial intelligence  pp            montreal 
parr  r     russell  s          reinforcement learning with hierarchies of machines  in
jordan  m   kearns  m     solla  s   eds    advances in neural information processing
systems     pp             mit press  cambridge 
pearl  j          probabilistic reasoning in intelligent systems  networks of plausible
inference  morgan kaufmann  san mateo 
pednault  e          adl  exploring the middle ground between strips and the situation calculus  in proceedings of the first international conference on principles of
knowledge representation and reasoning  pp          toronto  canada 
penberthy  j  s     weld  d  s          ucpop  a sound  complete  partial order planner for
adl  in proceedings of the third international conference on principles of knowledge
representation and reasoning  pp          boston  ma 
peot  m     smith  d          conditional nonlinear planning  in proceedings of the first
international conference on ai planning systems  pp          college park  md 
perez  m  a     carbonell  j  g          control knowledge to improve plan quality  in
proceedings of the second international conference on ai planning systems  pp      
    chicago  il 
poole  d          exploiting the rule structure for decision making within the independent
choice logic  in proceedings of the eleventh conference on uncertainty in artificial
intelligence  pp          montreal  canada 
poole  d       a   the independent choice logic for modelling multiple agents under uncertainty  artificial intelligence                 
  

fiboutilier  dean    hanks

poole  d       b   probabilistic partial evaluation  exploiting rule structure in probabilistic
inference  in proceedings of the fifteenth international joint conference on artificial
intelligence  pp            nagoya  japan 
poole  d          context specific approximation in probabilistic inference  in proceedings
of the fourteenth conference on uncertainty in artificial intelligence  pp         
madison  wi 
precup  d   sutton  r  s     singh  s          theoretical results on reinforcement learning
with temporally abstract behaviors  in proceedings of the tenth european conference
on machine learning  pp          chemnitz  germany 
pryor  l     collins  g          cassandra  planning for contingencies  technical
report     northwestern university  the institute for the learning sciences 
puterman  m  l          markov decision processes  john wiley   sons  new york 
puterman  m  l     shin  m          modified policy iteration algorithms for discounted
markov decision problems  management science                
ross  k  w     varadarajan  r          multichain markov decision processes with a
sample path constraint  a decomposition approach  mathematics of operations research                  
russell  s     norvig  p          artificial intelligence  a modern approach  prentice hall 
englewood cliffs  nj 
sacerdoti  e  d          planning in a hierarchy of abstraction spaces  artificial intelligence 
           
sacerdoti  e  d          the nonlinear nature of plans  in proceedings of the fourth
international joint conference on artificial intelligence  pp          
schoppers  m  j          universal plans for reactive robots in unpredictable environments 
in proceedings of the tenth international joint conference on artificial intelligence 
pp            milan  italy 
schwartz  a          a reinforcement learning method for maximizing undiscounted rewards  in proceedings of the tenth international conference on machine learning 
pp          amherst  ma 
schweitzer  p  l   puterman  m  l     kindle  k  w          iterative aggregationdisaggregation procedures for discounted semi markov reward processes  operations
research              
shachter  r  d          evaluating inuence diagrams  operations research              
    
shimony  s  e          the role of relevance in explanation i  irrelevance as statistical
independence  international journal of approximate reasoning                 
  

fidecision theoretic planning  structural assumptions

simmons  r     koenig  s          probabilistic robot navigation in partially observable
environments  in proceedings of the fourteenth international joint conference on
artificial intelligence  pp            montreal  canada 
singh  s  p     cohn  d          how to dynamically merge markov decision processes  in
advances in neural information processing systems     pp             mit press 
cambridge 
singh  s  p   jaakkola  t     jordan  m  i          reinforcement learning with soft state
aggregation  in hanson  s  j   cowan  j  d     giles  c  l   eds    advances in neural
information processing systems    morgan kaufmann  san mateo 
smallwood  r  d     sondik  e  j          the optimal control of partially observable
markov processes over a finite horizon  operations research                
smith  d     peot  m          postponing threats in partial order planning  in proceedings
of the eleventh national conference on artificial intelligence  pp          washington  dc 
sondik  e  j          the optimal control of partially observable markov processes over the
infinite horizon  discounted costs  operations research              
stone  p     veloso  m          team partitioned  opaque transition reinforcement learning 
in asada  m   ed    robocup     robot soccer world cup ii  springer verlag  berlin 
sutton  r  s          td models  modeling the world at a mixture of time scales  in
proceedings of the twelfth international conference on machine learning  pp      
    lake tahoe  nv 
sutton  r  s     barto  a  g          reinforcement learning  an introduction  mit press 
cambridge  ma 
tash  j     russell  s          control strategies for a stochastic planner  in proceedings
of the twelfth national conference on artificial intelligence  pp            seattle 
wa 
tatman  j  a     shachter  r  d          dynamic programming and inuence diagrams 
ieee transactions on systems  man  and cybernetics                  
tesauro  g  j          td gammon  a self teaching backgammon program  achieves masterlevel play  neural computation             
thrun  s   fox  d     burgard  w          a probabilistic approach to concurrent mapping
and localization for mobile robots  machine learning            
thrun  s     schwartz  a          finding structure in reinforcement learning  in tesauro 
g   touretzky  d     leen  t   eds    advances in neural information processing
systems   cambridge  ma  mit press 
warren  d          generating conditional plans and programs  in proceedings of aisb
summer conference  pp          university of edinburgh 
  

fiboutilier  dean    hanks

watkins  c  j  c  h     dayan  p          q learning  machine learning             
weld  d  s          an introduction to least commitment planning  ai magazine  winter
            
white iii  c  c     scherer  w  t          solutions procedures for partially observed
markov decision processes  operations research                  
williamson  m          a value directed approach to planning  ph d  thesis          
university of washington  department of computer science and engineering 
williamson  m     hanks  s          optimal planning with a goal directed utility model 
in proceedings of the second international conference on ai planning systems  pp 
        chicago  il 
winston  p  h          artificial intelligence  third edition  addison wesley  reading 
massachusetts 
yang  q          intelligent planning   a decomposition and abstraction based approach 
springer verlag 
zhang  n  l     liu  w          a model approximation scheme for planning in partially
observable stochastic domains  journal of artificial intelligence research             
zhang  n  l     poole  d          exploiting causal independence in bayesian network
inference  journal of artificial intelligence research             

  

fi