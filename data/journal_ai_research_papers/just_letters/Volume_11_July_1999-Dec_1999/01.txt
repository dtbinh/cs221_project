journal of artificial intelligence research                 

submitted       published     

semantic similarity in a taxonomy  an information based
measure and its application to problems of ambiguity in
natural language
philip resnik

resnik umiacs umd edu

department of linguistics and
institute for advanced computer studies
university of maryland
college park  md       usa

abstract

this article presents a measure of semantic similarity in an is a taxonomy based on
the notion of shared information content  experimental evaluation against a benchmark
set of human similarity judgments demonstrates that the measure performs better than
the traditional edge counting approach  the article presents algorithms that take advantage of taxonomic similarity in resolving syntactic and semantic ambiguity  along with
experimental results demonstrating their effectiveness 

   introduction
evaluating semantic relatedness using network representations is a problem with a long
history in artificial intelligence and psychology  dating back to the spreading activation
approach of quillian        and collins and loftus         semantic similarity represents a
special case of semantic relatedness  for example  cars and gasoline would seem to be more
closely related than  say  cars and bicycles  but the latter pair are certainly more similar 
rada et al   rada  mili  bicknell    blettner        suggest that the assessment of similarity
in semantic networks can in fact be thought of as involving just taxonomic  is a  links  to
the exclusion of other link types  that view will also be taken here  although admittedly
links such as part of can also be viewed as attributes that contribute to similarity  cf 
richardson  smeaton    murphy        sussna        
although many measures of similarity are defined in the literature  they are seldom
accompanied by an independent characterization of the phenomenon they are measuring 
particularly when the measure is proposed in service of a computational application  e g  
similarity of documents in information retrieval  similarity of cases in case based reasoning  
rather  the worth of a similarity measure is in its utility for the given task  in the cognitive
domain  similarity is treated as a property characterized by human perception and intuition 
in much the same way as notions like  plausibility  and  typicality   as such  the worth
of a similarity measure is in its fidelity to human behavior  as measured by predictions
of human performance on experimental tasks  the latter view underlies the work in this
article  although the results presented comprise not only direct comparison with human
performance but also practical application to problems in natural language processing 
a natural  time honored way to evaluate semantic similarity in a taxonomy is to measure
the distance between the nodes corresponding to the items being compared   the shorter

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

firesnik

the path from one node to another  the more similar they are  given multiple paths  one
takes the length of the shortest one  lee  kim    lee        rada   bicknell        rada
et al         
a widely acknowledged problem with this approach  however  is that it relies on the
notion that links in the taxonomy represent uniform distances  unfortunately  uniform
link distance is dicult to define  much less to control  in real taxonomies  there is wide
variability in the  distance  covered by a single taxonomic link  particularly when certain
sub taxonomies  e g   biological categories  are much denser than others  for example  in
wordnet  miller        fellbaum         a widely used  broad coverage semantic network
for english  it is not at all dicult to find links that cover an intuitively narrow distance
 rabbit ears is a television antenna  or an intuitively wide one  phytoplankton
is a living thing   the same kinds of examples can be found in the collins cobuild
dictionary  sinclair  ed          which identifies superordinate terms for many words  e g  
safety valve is a valve seems much narrower than knitting machine is a machine  
in the first part of this article  i describe an alternative way to evaluate semantic similarity in a taxonomy  based on the notion of information content  like the edge counting
method  it is conceptually quite simple  however  it is not sensitive to the problem of
varying link distances  in addition  by combining a taxonomic structure with empirical
probability estimates  it provides a way of adapting a static knowledge structure to multiple contexts  section   sets up the probabilistic framework and defines the measure of
semantic similarity in information theoretic terms  and section   presents an evaluation of
the similarity measure against human similarity judgments  using the simple edge counting
method as a baseline 
in the second part of the article  sections   and    i describe two applications of semantic
similarity to problems of ambiguity in natural language  the first concerns a particular
case of syntactic ambiguity that involves both coordination and nominal compounds  each
of which is a pernicious source of structural ambiguity in english  consider the phrase food
handling and storage procedures  does it represent a conjunction of food handling and storage
procedures  or does it refer to the handling and storage of food  the second application
concerns the resolution of word sense ambiguity   not for words in running text  which is a
large open problem  though cf  wilks   stevenson         but for groups of related words
as are often discovered by distributional analysis of text corpora or found in dictionaries
and thesauri  finally  section   discusses related work 

   similarity and information content

let c be the set of concepts in an is a taxonomy  permitting multiple inheritance  intuitively 
one key to the similarity of two concepts is the extent to which they share information  indicated in an is a taxonomy by a highly specific concept that subsumes them both  the
edge counting method captures this indirectly  since if the minimal path of is a links between two nodes is long  that means it is necessary to go high in the taxonomy  to more
abstract concepts  in order to find a least upper bound  for example  in wordnet  nickel
and dime are both subsumed by coin  whereas the most specific superclass that nickel
and credit card share is medium of exchange  see figure     in a feature based setting
 e g   tversky         this would be reected by explicit shared features  nickels and dimes
  

fiinformation based semantic similarity

medium of exchange
money
cash

credit

coin
nickel

dime

credit card

figure    fragment of the wordnet taxonomy  solid lines represent is a links  dashed lines
indicate that some intervening nodes were omitted to save space 
are both small  round  metallic  and so on  these features are captured implicitly by the
taxonomy in categorizing nickel and dime as subordinates of coin 
by associating probabilities with concepts in the taxonomy  it is possible to capture
the same idea as edge counting  but avoiding the unreliability of edge distances  let the
taxonomy be augmented with a function p   c           such that for any c   c   p c  is the
probability of encountering an instance of concept c  this implies that p is monotonically
nondecreasing as one moves up the taxonomy  if c  is a c   then p c     p c    moreover 
if the taxonomy has a unique top node then its probability is   
following the standard argumentation of information theory  ross         the information content of a concept c can be quantified as negative the log likelihood    log p c  
notice that quantifying information content in this way makes intuitive sense in this setting  as probability increases  informativeness decreases  so the more abstract a concept 
the lower its information content  moreover  if there is a unique top concept  its information
content is   
this quantitative characterization of information provides a new way to measure semantic similarity  the more information two concepts share  the more similar they are  and the
information shared by two concepts is indicated by the information content of the concepts
that subsume them in the taxonomy  formally  define
max    log p c    
sim c   c    
   
c   s  c    c  

where s  c   c   is the set of concepts that subsume both c  and c   a class that achieves the
maximum value in equation   will be termed a most informative subsumer  most often there
is a unique most informative subsumer  although this need not be true in the general case 
taking the maximum with respect to information content is analogous to taking the first
intersection in semantic network marker passing or the shortest path with respect to edge
distance  cf  quillian        rada et al          a generalization from taking the maximum
to taking a weighted average is introduced in section     
notice that although similarity is computed by considering all upper bounds for the two
concepts  the information measure has the effect of identifying minimal upper bounds  since
no class is less informative than its superordinates  for example  in figure    coin  cash 
etc  are all members of s  nickel  dime   but the concept that is structurally the minimal
  

firesnik

person
p      
info      

adult
p      
info      

female person
p      
info      

professional
p      
info      

actor 
p      
info      

intellectual
p      
info      

doctor 
p      
info      

nurse 
p      
info      

health professional
p      
info      

doctor 
p      
info      

guardian
p      
info      

lawyer
p      
info      

nurse 
p      
info      

figure    another fragment of the wordnet taxonomy
upper bound  coin  will also be the most informative  this can make a difference in cases
of multiple inheritance  two distinct ancestor nodes may both be minimal upper bounds 
as measured using distance in the graph  but those two nodes might have very different
values for information content  also notice that in is a taxonomies such as wordnet 
where there are multiple sub taxonomies but no unique top node  asserting zero similarity
for concepts in separate sub taxonomies  e g   liberty  aorta  is equivalent to unifying
the sub taxonomies by creating a virtual topmost concept 
in practice  one often needs to measure word similarity   rather than concept similarity 
using s w  to represent the set of concepts in the taxonomy that are senses of word w 
define
wsim w   w     cmax
   
   c   sim c   c     

where c  ranges over s w   and c  ranges over s w    this is consistent with rada et al  s
       treatment of  disjunctive concepts  using edge counting  they define the distance
between two disjunctive sets of concepts as the minimum path length from any element of
the first set to any element of the second  here  the word similarity is judged by taking
the maximal information content over all concepts of which both words could be an instance  to take an example  consider how the word similarity wsim doctor  nurse  would
be computed  using the taxonomic information in figure     note that only noun senses
are considered here   by equation    we must consider all pairs of concepts hc   c i  where
c    fdoctor   doctor g and c    fnurse   nurse g  and for each such pair we must
compute the semantic similarity sim c   c   according to equation    table   illustrates the
computation 
  

fiinformation based semantic similarity

c   description 

c   description 

sim c   c  
doctor   medical  nurse   medical  health professional
     
doctor   medical  nurse   nanny 
person
     
doctor   ph d   nurse   medical 
person
     
doctor   ph d   nurse   nanny 
person
     
subsumer

table    computation of similarity for doctor and nurse
as the table shows  when all the senses for doctor are considered against all the senses
for nurse  the maximum value is        via health professional as a most informative
subsumer  this is  therefore  the value of word similarity for doctor and nurse  

   evaluation

this section describes a simple  direct method for evaluating semantic similarity  using
human judgments as the basis for comparison 

    implementation

the work reported here used wordnet s taxonomy of concepts represented by nouns  and
compound nominals  in english   frequencies of concepts in the taxonomy were estimated
using noun frequencies from the brown corpus of american english  francis   kucera 
       a large            word  collection of text across genres ranging from news articles
to science fiction  each noun that occurred in the corpus was counted as an occurrence of
each taxonomic class containing it   for example  in figure    an occurrence of the noun
dime would be counted toward the frequency of dime  coin  cash  and so forth  formally 
x
freq c   
count n  
   
n words c 
where words c  is the set of words subsumed by concept c  concept probabilities were
computed simply as relative frequency 
   
p  c    freq c   

n

where n was the total number of nouns observed  excluding those not subsumed by any
wordnet class  of course   naturally the frequency estimates in equation   would be
   the taxonomy in figure   is a fragment of wordnet version      showing real quantitative information
computed using the method described below  the  nanny  sense of nurse  nursemaid  a woman who is
the custodian of children  is primarily a british usage  the example omits two other senses of doctor in
wordnet  a theologian in the roman catholic church  and a game played by children  wordnet does
not use node labels like doctor   but i have created such labels here for the sake of readability 
   concept as used here refers to what miller et al         call a synset  essentially a node in the taxonomy 
the experiment reported in this section used the noun taxonomy from wordnet version      which has
approximately        nodes 
   plural nouns counted as instances of their singular forms 

  

firesnik

improved by taking into account the intended sense of each noun in the corpus   for
example  an instance of crane can be a bird or a machine  but not both  sense tagged
corpora are generally not available  however  and so the frequency estimates are done using
this weaker but more generally applicable technique 
it should be noted that the present method of associating probabilities with concepts in
a taxonomy is not based on the notion of a single random variable ranging over all concepts
  were that the case  the  credit  for each noun occurrence would be distributed over all
concepts for the noun  and the counts normalized across the entire taxonomy to sum to   
 that is the approach taken in resnik      a  also see resnik      b for discussion   in
assigning taxonomic probabilities for purposes of measuring semantic similarity  the present
model associates a separate  binomially distributed random variable with each concept  
that is  from the perspective of any given concept c  an observed noun either is or is
not an instance of that concept  with probabilities p c  and     p c   respectively  unlike a
model in which there is a single multinomial variable ranging over the entire set of concepts 
this formulation assigns probability   to the top concept of the taxonomy  leading to the
desirable consequence that its information content is zero 

    task

although there is no standard way to evaluate computational measures of semantic similarity  one reasonable way to judge would seem to be agreement with human similarity ratings 
this can be assessed by using a computational similarity measure to rate the similarity of
a set of word pairs  and looking at how well its ratings correlate with human ratings of the
same pairs 
an experiment by miller and charles        provided appropriate human subject data
for the task  in their study     undergraduate subjects were given    pairs of nouns that
were chosen to cover high  intermediate  and low levels of similarity  as determined using
a previous study  rubenstein   goodenough         and those subjects were asked to
rate  similarity of meaning  for each pair on a scale from    no similarity  to    perfect
synonymy   the average rating for each pair thus represents a good estimate of how similar
the two words are  according to human judgments  
in order to get a baseline for comparison  i replicated miller and charles s experiment 
giving ten subjects the same    noun pairs  the subjects were all computer science graduate
students or postdoctoral researchers at the university of pennsylvania  and the instructions
were exactly the same as used by miller and charles  the main difference being that in
this replication the subjects completed the questionnaire by electronic mail  though they
were instructed to complete the whole task in a single uninterrupted sitting   five subjects
received the list of word pairs in a random order  and the other five received the list in the
reverse order  the correlation between the miller and charles mean ratings and the mean
ratings in my replication was      quite close to the     correlation that miller and charles
obtained between their results and the ratings determined by the earlier study 
   this is similar in spirit to the way probabilities are used in a bayesian network 
   an anonymous reviewer points out that human judgments on this task may be inuenced by prototypicality  e g   the pair bird robin would likely yield higher ratings than bird crane  issues of this kind are
briey touched on in section    but for the most part they are ignored here since prototypicality  like
topical relatedness  is not captured in most is a taxonomies 

   

fiinformation based semantic similarity

for each subject in my replication  i computed how well his or her ratings correlated
with the miller and charles ratings  the average correlation over the    subjects was
r         with a standard deviation of        this value represents an upper bound on
what one should expect from a computational attempt to perform the same task 
for purposes of evaluation  three computational similarity measures were used  the
first is the similarity measurement using information content proposed in the previous section  the second is a variant on the edge counting method  converting it from distance to
similarity by subtracting the path length from the maximum possible path length 


wsimedge  w   w         max    cmin
len c   c  
 c
 

 



   

where c  ranges over s w    c  ranges over s w    max is the maximum depth of the taxonomy  and len c   c   is the length of the shortest path from c  to c     recall that s w 
denotes the set of concepts in the taxonomy that represent senses of word w   if all senses
of w  and w  are in separate sub taxonomies of wordnet their similarity is taken to be
zero  note that because correlation is used as the evaluation metric  the conversion from a
distance to a similarity can be viewed as an expository convenience  and does not affect the
results  although the sign of the correlation coecient changes from positive to negative 
its magnitude turns out to be just the same regardless of whether or not the minimum path
length is subtracted from     max  
the third point of comparison is a measure that simply uses the probability of a concept 
rather than the information content  to define semantic similarity of concepts
max      p c  
simp c  c   c    
   
c   s  c    c  

and the corresponding measure of word similarity 

h

i

wsimp c  w   w     cmax
simp c  c   c    
 c
 

 

   

where c  ranges over s w   and c  ranges over s w   in equation    the probability based
similarity score is included in order to assess the extent to which similarity judgments might
be sensitive to frequency per se rather than information content  again  the difference
between maximizing     p c  and minimizing p c  turns out not to affect the magnitude of
the correlation  it simply ensures that the value can be interpreted as a similarity value 
with high values indicating similar words 

    results

table   summarizes the experimental results  giving the correlation between the similarity
ratings and the mean ratings reported by miller and charles  note that  owing to a noun
missing from the wordnet     taxonomy  it was only possible to obtain computational
similarity ratings for    of the    noun pairs  hence the proper point of comparison for
human judgments is not the correlation over all    items  r         but rather the correlation
over the    included pairs  r         the similarity ratings by item are given in table   
   inter subject correlation in the replication  estimated using leaving one out resampling  weiss   kulikowski         was r        stdev        

   

firesnik

similarity method
correlation
human judgments  replication  r        
information content
r        
probability
r        
edge counting
r        
table    summary of experimental results 
word pair
car
gem
journey
boy
coast
asylum
magician
midday
furnace
food
bird
bird
tool
brother
crane
lad
journey
monk
food
coast
forest
monk
coast
lad
chord
glass
noon
rooster

automobile
jewel
voyage
lad
shore
madhouse
wizard
noon
stove
fruit
cock
crane
implement
monk
implement
brother
car
oracle
rooster
hill
graveyard
slave
forest
wizard
smile
magician
string
voyage

miller and charles replication
wsim wsimedge wsimp c 
means
means
    
          
         
    
           
         
    
          
         
    
          
         
    
           
         
    
           
         
    
           
         
    
           
         
    
          
         
    
          
         
    
          
         
    
          
         
    
          
         
    
          
         
    
          
         
    
          
         
    
          
        
    
          
         
    
          
         
    
          
         
    
          
        
    
          
         
    
          
        
    
          
         
    
          
         
    
          
         
    
          
        
    
          
        
table    semantic similarity by item 
   

fiinformation based semantic similarity

n 
tobacco
tobacco
tobacco

n 
wsim n   n   subsumer
alcohol
    
drug
sugar
     substance
horse
     narcotic

table    similarity with tobacco computed by maximizing information content

    discussion

the experimental results in the previous section suggest that measuring semantic similarity
using information content provides results that are better than the traditional method of
simply counting the number of intervening is a links 
the measure is not without its problems  however  like simple edge counting  the
measure sometimes produces spuriously high similarity measures for words on the basis of
inappropriate word senses  for example  table   shows the word similarity for several words
with tobacco  tobacco and alcohol are similar  both being drugs  and tobacco and sugar are
less similar  though not entirely dissimilar  since both can be classified as substances  the
problem arises  however  in the similarity rating for tobacco with horse  the word horse can
be used as a slang term for heroin  and as a result information based similarity is maximized 
and path length minimized  when the two words are both categorized as narcotics  this is
contrary to intuition 
cases like this are probably relatively rare  however  the example illustrates a more
general concern  in measuring similarity between words  it is really the relationship among
word senses that matters  and a similarity measure should be able to take this into account 
in the absence of a reliable algorithm for choosing the appropriate word senses  the most
straightforward way to do so in the information based setting is to consider all concepts
to which both nouns belong rather than taking just the single maximally informative class 
this suggests defining a measure of weighted word similarity as follows 
wsimff w   w    

x

i

ff ci    log p ci   

   

where fcig is the set of concepts dominating both w  and w  in any sense of either word  and
ff is a weighting function over concepts such that pi ff ci       this measure of similarity

takes more information into account than the previous one  rather than relying on the
single concept with maximum information content  it allows each class representing shared
properties to contribute information content according to the value of ff ci   intuitively 
these ff values measure relevance  for example  in computing wsimff  tobacco horse   the
ci would range over all concepts of which tobacco and horse are both instances  including
narcotic  drug  artifact  life form  etc  in an everyday context one might expect low
values for ff narcotic  and ff drug   but in the context of  say  a newspaper article about
drug dealers  the weights of these concepts might be quite high  although it is not possible
to include weighted word similarity in the comparison of section    since the noun pairs are
judged without context  section   provides further discussion and a weighting function ff
designed for a particular natural language processing task 
   

firesnik

   using taxonomic similarity in resolving syntactic ambiguity

having considered a direct evaluation of the information based semantic similarity measure 
i now turn to the application of the measure in resolving syntactic ambiguity 

    clues for resolving coordination ambiguity

syntactic ambiguity is a pervasive problem in natural language  as church and patil
       point out  the class of  every way ambiguous  syntactic constructions   those for
which the number of analyses is the number of binary trees over the terminal elements  
includes such frequent constructions as prepositional phrases  coordination  and nominal
compounds  in the last several years  researchers in natural language have made a great
deal of progress in using quantitative information from text corpora to provide the needed
constraints  progress on broad coverage prepositional phrase attachment ambiguity has
been particularly notable  now that the dominant approach has shifted from structural
strategies to quantitative analysis of lexical relationships  whittemore  ferrara    brunner 
      hindle   rooth        brill   resnik        ratnaparkhi   roukos        li   abe 
      collins   brooks        merlo  crocker    berthouzoz         noun compounds have
received comparatively less attention  kobayasi  takunaga    tanaka        lauer       
       as has the problem of coordination ambiguity  agarwal   boggess        kurohashi
  nagao        
in this section  i investigate the role of semantic similarity in resolving coordination
ambiguities involving nominal compounds  i began with noun phrase coordinations of the
form n  and n  n   which admit two structural analyses  one in which n  and n  are the
two noun phrase heads being conjoined   a  and one in which the conjoined heads are n 
and n    b  
    a  a  bank and warehouse  guard
b  a  policeman  and  park guard 
identifying which two head nouns are conjoined is necessary in order to arrive at a correct
interpretation of the phrase s content  for example  analyzing   b  according to the structure of   a  could lead a machine translation system to produce a noun phrase describing
somebody who guards both policemen and parks  analyzing   a  according to the structure of   b  could lead an information retrieval system to miss this phrase when looking for
queries involving the term bank guard 
kurohashi and nagao        point out that similarity of form and similarity of meaning
are important cues to conjoinability  in english  similarity of form is to a great extent
captured by agreement in number  singular vs  plural  
   

a  several business and university groups
b  several businesses and university groups
similarity of form between candidate conjoined heads can thus be thought of as a boolean
variable  number agreement is either satisfied by the candidate heads or it is not 
similarity of meaning of the conjoined heads also appears to play an important role 
   

a  a television and radio personality
   

fiinformation based semantic similarity

b  a psychologist and sex researcher
clearly television and radio are more similar than television and personality  correspondingly for psychologist and researcher  this similarity of meaning is captured well by semantic
similarity in a taxonomy  and thus a second variable to consider when evaluating a coordination structure is semantic similarity as measured by overlap in information content between
the two head nouns 
in addition  for the constructions considered here  the appropriateness of noun noun
modification is relevant 
   

a  mail and securities fraud
b  corn and peanut butter

one reason we prefer to conjoin mail and securities is that mail fraud is a salient compound
nominal phrase  on the other hand  corn butter is not a familiar concept  compare to the
change in perceived structure if the phrase were corn and peanut crops  in order to measure
the appropriateness of noun noun modification  i use a quantitative measure of selectional
fit called selectional association  resnik         which takes into account both lexical cooccurrence frequencies and semantic class membership in the wordnet taxonomy  briey 
the selectional association of a word w with a wordnet class c is given by
p cjw  log p cjw 
a w  c    d p c jw  k p p cc    

   

where d p  k p   is the kullback leibler distance  relative entropy  between probability
distributions p  and p   intuitively  a w  c  is measuring the extent to which class c is
predicted by word w  for example  a wool  clothing  would have a higher value than 
say  a wool  person   the selectional association a w   w   of two words is defined as
the maximum of a w   c  taken over all classes c to which w  belongs  for example 
a wool  glove  would most likely be equal to a wool  clothing   as compared to  say 
a wool  sports equipment    the latter value corresponding to the sense of glove as
something used in baseball or in boxing   see li   abe        for an approach in which
selectional relationships are modeled using conditional probability   a simple way to treat
selectional association as a variable in resolving coordination ambiguities is to prefer analyses that include noun noun modifications with very strong anities  e g   bank as a modifier
of guard  and to disprefer very weak noun noun relationships  e g   corn as a modifier of
butter   thresholds defining  strong  and  weak  are parameters of the algorithm  defined
below 

    resolving coordination ambiguity  first experiment

i investigated the roles of these sources of evidence by conducting a straightforward disambiguation experiment using naturally occurring linguistic data  two sets of     noun
phrases of the form  np n  and n  n   were extracted from the parsed wall street journal
 wsj  corpus  as found in the penn treebank  marcus  santorini    marcinkiewicz        
these were disambiguated by hand  with one set used for development and the other for
   

firesnik

source of evidence conjoined condition
number agreement n  and n  number n     number n   and number n      number n  
n  and n  number n     number n   and number n      number n  
undecided otherwise
semantic similarity n  and n  wsim n  n     wsim n  n  
n  and n  wsim n  n     wsim n  n  
undecided otherwise
noun noun
n  and n  a n  n      or a n  n     
modification
n  and n  a n  n      or a n  n     
undecided otherwise

table    rules for number agreement  semantic similarity  and noun noun modification in
resolving syntactic ambiguity of noun phrases n  and n  n 
testing   a set of simple transformations were applied to all wsj data  including the mapping of all proper names to the token someone  the expansion of month abbreviations  and
the reduction of all nouns to their root forms 
number agreement was determined using a simple analysis of suxes in combination
with wordnet s lists of root nouns and irregular plurals   semantic similarity was determined using the information based measure of equation       the noun class probabilities
of equation     were estimated using a sample of approximately         noun occurrences
in associated press newswire stories   for the purpose of determining semantic similarity 
nouns not in wordnet were treated as instances of the class hthingi  appropriateness of
noun noun modification was determined by computing selectional association  equation    
using co occurrence frequencies taken from a sample of approximately        noun noun
compounds extracted from the wsj corpus   this sample did not include the test data  
both selection of the modifier for the head and selection of the head for the modifier were
considered by the disambiguation algorithm  table   provides details of the decision rule
for each source of evidence when used independently   
in addition  i investigated several methods for combining the three sources of information  these included   a  a simple form of  backing off   specifically  given the number
agreement  noun noun modification  and semantic similarity strategies in that order  use
the choice given by the first strategy that isn t undecided    b  taking a vote among the
three strategies and choosing the majority   c  classifying using the results of a linear re   hand disambiguation was necessary because the penn treebank does not encode np internal structure 
these phrases were disambiguated using the full sentence in which they occurred  plus the previous and
following sentence  as context 
   the experiments in this section used wordnet version     
   i am grateful to donald hindle for making these data available 
    thresholds        and        were fixed manually based on experience with the development set
before evaluating the test data 

   

fiinformation based semantic similarity

strategy

default
number agreement
noun noun modification
semantic similarity
backing off
voting
number agreement   default
noun noun modification   default
semantic similarity   default
backing off   default
voting   default
regression
id  tree

coverage     accuracy    

     
    
    
    
    
    
     
     
     
     
     
     
     

    
    
    
    
    
    
    
    
    
    
    
    
    

table    syntactic disambiguation for items of the form n  and n  n 
gression  and  d  constructing a decision tree classifier  the latter two methods are forms of
supervised learning  in this experiment the development set was used as the training data   
the results are shown in table    the development set contained a bias in favor of
conjoining n  and n   therefore a  default  strategy  always choosing that bracketing  was
used as a baseline for comparison  the default was also used for resolving undecided cases
in order to make comparisons of individual strategies at      coverage  for example 
 number agreement   default  shows the figures obtained when number agreement is
used to make the choice and the default is selected if that choice is undecided 
not surprisingly  the individual strategies perform reasonably well on the instances they
can classify  but coverage is poor  the strategy based on similarity of form is the most highly
accurate  but arrives at an answer only half the time  however  the heavy a priori bias makes
up the difference   to such an extent that even though the other forms of evidence have
some value  no combination beats the number agreement plus default combination  on the
positive side  this shows that the ambiguity can be resolved reasonably well using a very
simple algorithm  viewed in terms of how many errors are made  number agreement makes
it possible to cut the baseline     error rate nearly in half to     incorrect analyses  a
    reduction   on the negative side  the results fail to make a strong case that semantic
similarity can add something useful 
before taking up this issue  let us assess the contributions of the individual strategies to
the results when evidence is combined  by further analyzing the behavior of the unsupervised
evidence combination strategies  when combining evidence by voting  a choice was made
in    cases  the number agreement strategy agreed with the majority vote in    cases 
of which            were correct  the noun noun modification strategy agreed with the
majority in    cases  of which            were correct  and the semantic similarity strategy
    what i am calling  backing off  is related in spirit to katz s well known smoothing technique  katz 
       but the  backing off  strategy used here is not quantitative  i retain the double quotes in order
to highlight the distinction 

   

firesnik

agreed with the majority in    cases  of which            were correct  in the  backing off 
form of evidence combination  number agreement makes a choice for    cases and is correct
for             then  of those remaining undecided  noun noun modification makes a choice
for    cases and is correct for             then  of those still undecided  semantic similarity
makes a choice for   cases of which   are correct          and the remaining   cases are
undecided 
this analysis and the above baseline performance of the semantic similarity plus default
strategy show that semantic similarity does contain information about the correct answer 
it agrees with the majority vote a substantial portion of the time  and it selects correct
answers more often than one would expect by default for the cases it receives through
 backing off   however  because the default is correct two thirds of the time  and because
the number agreement strategy is correct nine out of ten times for the cases it can decide 
the potential contribution of semantic similarity remains suggestive rather than conclusive 
in a second experiment  therefore  i investigated a more dicult formulation of the problem
in order to obtain a better assessment 

    resolving coordination ambiguity  second experiment

in the second experiment using the same data sources  i investigated a more complex set
of coordinations  looking at noun phrases of the form n  n  and n  n   the syntactic
analyses of such phrases are characterized by the same top level binary choice as the data
in the previous experiment  either conjoining heads n  and n  as in     or conjoining n 
and n  as in       
    a  freshman   business and marketing  major 
b   food  handling and storage   procedures
c    mail fraud  and bribery  charges
   

a  clorets  gum and  breath mints  
b   baby food  and  puppy chow 

for this experiment  one set of    items was extracted from the penn treebank wsj data
for development  and another set of    items was set aside for testing  the development
set showed significantly less bias than the data in the previous experiment  with       of
items conjoining n  and n  
the disambiguation strategies in this experiment were a more refined version of those
used in the previous experiment  as illustrated in table    number agreement was used just
as before  however  rather than employing semantic similarity and noun noun modification
as independent strategies   something not clearly warranted given the lackluster performance of the modification strategy   the two were combined in a measure of weighted
semantic similarity as defined in equation      selectional association was used as the basis
for ff  in particular  ff    c  was the greater of a n  c  and a n  c   capturing the fact that
when n  and n  are conjoined  the combined phrase potentially stands in a head modifier
relationship with n  and a modifier head relationship with n    correspondingly  ff    c 
was the greater of a n  c  and a n  c   capturing the fact that the coordination of n 
    the full   way classification problem for the structures in     and     was not investigated 

   

fiinformation based semantic similarity

source of evidence conjoined condition
number agreement n  and n  number n     number n   and number n      number n  
n  and n  number n     number n   and number n      number n  
undecided otherwise
weighted semantic n  and n  wsimff     n  n     wsimff     n  n  
similarity
n  and n  wsimff     n  n     wsimff     n  n  
undecided otherwise
 

 

 

 

table    rules for number agreement and weighted semantic similarity in resolving syntactic ambiguity of noun phrases n  n  and n  n 
strategy

default
number agreement
weighted semantic similarity
backing off

coverage     accuracy    

     
    
    
    

    
    
    
    

table    syntactic disambiguation for items of the form n  n  and n  n 
and n  takes place in the context of n  modifying n  and of n   or a coordinated phrase
containing it  being modified by n  
for example  consider an instance of the ambiguous phrase 
   

telecommunications products and services units 

it so happens that a high information content connection exists between product in its
sense as  a quantity obtained by multiplication  and unit in its sense as  a single undivided
whole   as a result  although neither of these senses is relevant for this example  nouns n 
and n  would be assigned a high value for  unweighted  semantic similarity and be chosen
incorrectly as the conjoined heads for this example  however  the unweighted similarity computation misses an important piece of context  in any syntactic analysis conjoining product
and unit  cf  examples  a and  b   the word telecommunications is necessarily a modifier
of the concept identified by products  but the selectional association between telecommunications and products in its  multiplication  sense is weak or nonexistent  weighting by
selection association  therefore  provides a way to reduce the impact of the spurious senses
on the similarity computation 
in order to combine sources of evidence  i used  backing off   from number agreement
to weighted semantic similarity  to combine the two individual strategies  as a baseline 
results were evaluated against a simple default strategy of always choosing the group that
was more common in the development set  the results are shown in table   
in this case  the default strategy defined using the development set was misleading 
yielding worse than chance accuracy  for this reason  strategy plus default figures are not
reported  however  even if default choices were made using the bias found in the test set 
   

firesnik

accuracy would be only        in contrast to the equivocal results in the first experiment 
this experiment demonstrates a clear contribution of semantic similarity  by employing
semantic similarity in those cases where the more accurate number agreement strategy
cannot apply  it is possible to obtain equivalent or even somewhat better accuracy than
number agreement alone while at the same time more than doubling the coverage 
comparison with previous algorithms is unfortunately not possible  since researchers on
coordination ambiguity have not established a common data set for evaluation or even a
common characterization of the problem  in contrast to the now standard  v  n   prep  n  
contexts used in work on propositional phrase attachment  with that crucial caveat  it
is nonetheless interesting to note that the results obtained here are broadly consistent
with kurohashi and nagao         who report accuracy results in the range of        at
     coverage when analyzing a broad range of conjunctive structures in japanese using
a combination of string matching  syntactic similarity  and thesaurus based similarity  and
with agarwal and boggess         who use syntactic types and structure  along with partly
domain dependent semantic labels  to obtain accuracies in a similar range for identifying
conjuncts in english 

   using taxonomic similarity in word sense selection
this section considers the application of the semantic similarity measure in resolving another
form of ambiguity  selecting the appropriate sense of a noun when it appears in the context
of other nouns that are related in meaning 

    associating word senses with noun groupings
knowledge about groups of related words plays a role in many natural language applications 
as examples  query expansion using related words is a well studied technique in information
retrieval  e g   harman        grefenstette         clusters of similar words can play a
role in smoothing stochastic language models for speech recognition  brown  della pietra 
desouza  lai    mercer         classes of verbs that share semantic structure form the
basis for an approach to interlingual machine translation  dorr         and clusterings of
related words can be used in characterizing subgroupings of retrieved documents in largescale web searches  e g   digital equipment corporation         there is a wide body
of research on the use of distributional methods for measuring word similarity in order to
obtain groups of related words  e g   bensch   savitch        brill        brown et al        
grefenstette              mckeown   hatzivassiloglou        pereira  tishby    lee       
schutze         and thesauri such as wordnet are another source of word relationships  e g  
voorhees        
distributional techniques can sometimes do a good job of identifying groups of related
words  see resnik      b  for an overview and critical discussion   but for some tasks the
relevant relationships are not among words  but among word senses  for example  brown
et al         illustrate the notion of a distributionally derived   semantically sticky  cluster
using an automatically derived word group containing attorney  counsel  trial  court  and
judge  although the semantic coherence of this cluster  pops out  for a human reader  a
naive computational system has no defense against word sense ambiguity  using this cluster
   

fiinformation based semantic similarity

for query expansion could result in retrieving documents involving advice  one sense of
counsel  and royalty  as one sense of court    
resnik      a  introduces an algorithm that uses taxonomically defined semantic similarity in order to derive grouping relationships among word senses from grouping relationships among words  formally  the problem can be stated as follows  consider a set of words
w   fw          wng  with each word wi having an associated
set si   fsi            si mg of poss
sible senses  assume that there exists some set w    si   representing the set of word
senses that an ideal human judge would conclude belong to the group of senses corresponding to the word grouping w    it follows that w   must contain at least one representative
from each si    the goal is then to define a membership function   that takes si j   wi   and
w as its arguments and computes a value in         representing the confidence with which
one can state that sense si j belongs in sense grouping w     note that  in principle  nothing
precludes the possibility that multiple senses of a word are included in w   
for example  consider again the group
attorney  counsel  trial  court  judge 
restricting attention to noun senses in wordnet  every word but attorney is polysemous 
treating this word group as w   a good algorithm for computing   should assign a value
of   to the unique sense of attorney  and it should assign a high value to the sense of counsel
as
a lawyer who pleads cases in court 
similarly  it should assign high values to the senses of trial as
legal proceedings consisting of the judicial examination of issues by a competent tribunal
the determination of a person s innocence or guilt by due process of law 
it should also assign high values to the senses of court as
an assembly to conduct judicial business
a room in which a law court sits 
and it should assign a high value to the sense of judge as
a public ocial authorized to decide questions brought before a court of justice 
it should assign low values of   to the various word senses of words in this cluster that are
associated with the group to a lesser extent or not at all  these would include the sense of
counsel as
direction or advice as to a decision or course of action 
similarly  a low value of   should be assigned to other senses of court such as
    see krovetz and kroft       and voorhees       for experimentation and discussion of the effects of
word sense ambiguity in information retrieval 

   

firesnik

algorithm  resnik      a   given w   fw            w g  a set of nouns 
n

for i and j     to n  with i   j
f

v   wsim w   w  
c   the most informative subsumer for w and w
i j

i

j

i j

i

j

for k     to num senses w  
if c is an ancestor of sense
increment support i  k  by v
i

i j

i k

i j

for k     to num senses w  
if c is an ancestor of sense
increment support j  k   by v
 

j

j k  

i j

i j

increment normalization i  by v
increment normalization j  by v

i j

g

i j

for i     to n
for k     to num senses w  
f

i

if  normalization i        
    support i  k    normalization i 
else
        num senses w  
i k

g

i k

i

figure    disambiguation algorithm for noun groupings
a yard wholly or partly surrounded by walls or buildings 
the disambiguation algorithm for noun groups is given in figure    intuitively  when two
polysemous words are similar  their most informative subsumer provides information about
which sense of each word is the relevant one  this observation is similar in spirit to other
approaches to word sense disambiguation based on maximizing relatedness of meaning  e g  
lesk        sussna         the key idea behind the algorithm is to consider the nouns in a
word group pairwise  for each pair the algorithm goes through all possible combinations of
the words  senses  and assigns  credit  to senses on the basis of shared information content 
as measured using the information content of the most informative subsumer   
as an example  wordnet lists doctor as meaning either a medical doctor or someone
holding a ph d   and lists nurse as meaning either a health professional or a nanny  but
when the two words are considered together  the medical sense of each word is obvious to
the human reader  this effect finds its parallel in the operation of the algorithm  given a
taxonomy like that of figure    consider a case in which the set w of words contains w   
doctor  w    nurse  and w    actor  in the first pairwise comparison  for doctor and nurse 
    in figure    the square bracket notation highlights the fact that support is a matrix and normalization
is an array  conceptually v and c are  triangular  matrices also  however  i use subscripts rather than
square brackets because at implementation time there is no need to implement them as such since the
values v and c are used and discarded on each pass through the double loop 
i j

i j

   

fiinformation based semantic similarity

the most informative subsumer is c      health professional  which has information
content v             therefore the support for doctor  and nurse  is incremented
by        neither doctor  nor nurse  receives any increment in support based on this
comparison  since neither has health professional as an ancestor  in the second pairwise
comparison  the most informative subsumer for doctor and actor is c      person  with
information content v             and so there is an increment by that amount to the
support for doctor   doctor   and actor   all of which have person as an ancestor 
similarly  in the third pairwise comparison  the most informative subsumer for nurse and
actor is also person  so nurse   nurse   and actor  all have their support incremented
by        in the end  therefore  doctor  has received support               out of a
possible               for all the pairwise comparisons in which it participated  so for that
word sense        in contrast  doctor  received support in the amount of       out of a
possible               for the comparisons in which it was involved  so the value of   for
              
doctor  is        
    
resnik      a  illustrates the algorithm of figure   using word groupings from a variety
of sources  including several of the sources on distributional clustering cited above  and
evaluates the algorithm more rigorously on the task of associating wordnet senses with
nouns in roget s thesaurus  based on their thesaurus category membership  on average  the
algorithm achieved approximately     of the performance of human annotators performing
the same task    in the remainder of this section i describe a new application of the
algorithm  and evaluate its performance 

    linking to wordnet using a bilingual dictionary
multilingual resources for natural language processing can be dicult to obtain  although
some promising efforts are underway in projects like eurowordnet  vossen         for
many languages  however  such large scale resources are unlikely to be available in the
near future  and individual research efforts will have to continue to build from scratch or
to adapt existing resources such as bilingual dictionaries  e g   klavans   tzoukermann 
       in this section i describe an application of the algorithm of figure   to the english
definitions in the ceta chinese english dictionary  ceta         the ultimate task 
being undertaken in the context of a chinese english machine translation project  will be
to associate chinese vocabulary items with nodes in wordnet  much in the same way that
vocabulary in spanish  dutch  and italian are associated with interlingual taxonomy nodes
derived from the american wordnet  in the eurowordnet project  the task is also similar
to attempts to relate dictionaries and thesauri monolingually  e g   see section     and
ji  gong    huang         the present study investigates the extent to which semantic
similarity might be useful in partially automating the process 
    the task was performed independently by two human judges  treating judge   as the benchmark the
accuracies achieved by judge    the algorithm  and random selection were respectively              
and        treating judge   as the benchmark the accuracies achieved by judge    the algorithm  and
random selection were respectively               and        as the relatively low accuracies for human
judges demonstrate  disambiguation using wordnet s fine grained senses is quite a bit more dicult than
disambiguation to the level of homographs  hearst        cowie  guthrie    guthrie         resnik and
yarowsky              discuss the implications of wordnet s fine grainedness for evaluation of word
sense disambiguation  and consider alternative evaluation methods 

   

firesnik

for example  consider the following dictionary entries 
 a 
     hliti brother in law  husband s elder brother     hregi father   
hregi uncle  father s elder brother     uncle  form of address to an older
man 
  actress  player of female roles 
 b 
in order to associate chinese terms such as these with the wordnet noun taxonomy  it
is important to avoid associations with inappropriate senses   for example  the word in
  should clearly not be associated with father in its wordnet senses as church
entry  a  
father  priest  god the father  or founding father   
although one traditional approach to using dictionary entries has been to compute word
overlap with respect to dictionary definitions  e g   lesk         the english glosses in the
ceta dictionary are generally too short to take advantage of word overlap in this fashion 
however  many of the definitions do have a useful property  they possess multiple subdefinitions that are similar in meaning  as in the cases illustrated above  although one
cannot always assume that this is so  e g  
 c 
     case  i e   upper case or lower case     dial  of a watch  etc   
inspection of the dictionary confirms that when multiple definitions are present they tend
more toward polysemy than homonymy 
based on this observation  i conducted an experiment to assess the extent to which the
word sense disambiguation algorithm of figure   can be used to identify relevant noun senses
in wordnet for chinese words in the ceta dictionary  using the english definitions as the
source of similar nouns to disambiguate  nouns heading definitional noun phrases were
extracted automatically via simple heuristic methods  for a randomly selected sample of
    dictionary entries containing multiple definitions to be used as a test set  for example 
the noun groups associated with the definitions above would be
 a   uncle  brother in law  father
 b   actress  player 
wordnet s noun database was used to automatically identify compound nominals where
possible  so  for example  a word defined as  record player  would have the compound
record player rather than player as its head noun because record player is a compound
noun known to wordnet   
it should be noted that no attempt was made to exclude dictionary entries like  c  when
creating the test set  since in general there is no way to automatically identify alternative
definitions distinguished by synonymy from those distinguished by homonymy  such entries
must be faced by any disambiguation algorithm for this task 
two independent judges were recruited for assistance in annotating the test set  one a
native chinese speaker  and the second a chinese language expert for the united states
government  these judges independently annotated the     test items  for each item 
    annotations within the dictionary entries such as
ignored by the algorithm described in this section 
    wordnet version     was used for this experiment 

 lit 

   

 literary  

 reg 

 regional   and the like are

fiinformation based semantic similarity

for each wordnet definition  you will see   boxes                 and is a  for each definition 
 if you think the chinese word can have that meaning  select the number corresponding to your
confidence in that choice  where   is lowest confidence and   is highest confidence 
 if the chinese word cannot have that meaning  but can have a more specific meaning  select is a 
for example  if the chinese word means  truck  and the wordnet definition is  automotive vehicle 
self propelled wheeled vehicle   you would select this option   that is  it makes sense to say that
this chinese word describes a concept that is a kind of automotive vehicle   then pick   
         or   as your confidence in this decision  again with   as lowest confidence and   as highest
confidence 
 if neither of the above cases apply for this wordnet definition  don t check off anything for this
definition 

figure    instructions for human judges selecting senses associated with chinese words
the judge was given the chinese word  its full ceta dictionary definition  as in examples
a c   and a list of all the wordnet sense descriptions associated with any sense of any head
noun in the associated noun group  for example  the list corresponding to the following
dictionary definition
 d 
  urgent message  urgent dispatch
would contain the following wordnet sense descriptions  as generated via the head nouns
message and dispatch 
 message  content  subject matter  substance  what a communication that
is about something is about
 dispatch  expedition  expeditiousness  fastness  subconcept of celerity  quickness  rapidity
 dispatch  despatch  communique  an ocial report  usually sent in haste 
 message  a communication  usually brief  that is written or spoken or
signaled   he sent a three word message 
 dispatch  despatch  shipment  the act of sending off something
 dispatch  despatch  the murder or execution of someone
for each item  the judge was first asked whether he knew that chinese word in that meaning 
if the response was negative  he was instructed to proceed to the next item  for items with
known words  the judges were instructed as in figure   
although the use of the is a selection was not used in the analysis of the results  it
was important to include it because it provided the judges with a way to indicate where a
chinese word could best be classified in the wordnet noun taxonomy  without having to
assert translational equivalence between the chinese concept and a close wordnet  english 
concept  so  for example  a judge could classify the word
 the spring festival  lunar
new year  chinese new year  as belonging under the wordnet sense glossed as
festival  a day or period of time set aside for feasting and celebration 
   

firesnik

the most sensible choice given that  chinese new year  does not appear as a wordnet
concept  annotating the is a relationship for the set was also important because the algorithm being evaluated was working on groups of head nouns  thereby potentially losing
information pointing to a more specific concept reading  for example  the definition
  steel tube  steel pipe
 e 
would be given to the algorithm as a group containing head nouns tube and pipe 
once the test set was annotated  evaluation was done according to two paradigms 
selection and filtering  in both paradigms we assume that for each entry in the test set  an
annotator has correctly specified which wordnet senses are to be considered correct  and
which are incorrect  an algorithm being tested against this set must identify  for each listed
sense  whether that sense should be included for that item or whether it should be excluded 
for example  the wordnet sense corresponding to  the murder or execution of someone 
would be identified by an annotator as incorrect for  d   and so an algorithm marking it as
 included  should be penalized 
for the selection paradigm  the goal is to identify wordnet senses to include  we can
therefore define precision in that paradigm as
of correctly included senses
pselection   number
    
number of included senses
and recall as
of correctly included senses  
rselection   number
    
number of correct senses
these correspond directly to the use of precision and recall in information retrieval  precision begins with the set of senses included by some method  and computes the proportion of
these that are correct  recall begins with the set of senses that should have been included 
and computes the proportion of these that the method actually managed to choose 
since the number of potential wordnet senses for an item can be quite large  an equally
valid alternative to the selection paradigm is what i will call the filtering paradigm  according
to which the goal is to identify wordnet senses to exclude  one can easily imagine this
being the more relevant paradigm   for example  in a semi automated setting where one
wishes to reduce the burden of a user selecting among alternatives  in the filtering paradigm
one can define filtering precision as
of correctly excluded senses
pfiltering   number
number of excluded senses

    

of correctly excluded senses  
rfiltering   number
number of senses labeled incorrect

    

and filtering recall as

in the filtering paradigm  precision begins with the set of senses that the method filtered
out and computes the proportion that were correctly filtered out  and recall in filtering
begins with the set of senses that should have been excluded  i e  the incorrect ones  and
computes the proportion of these that the method actually managed to exclude 
   

fiinformation based semantic similarity

sense selection
sense filtering
precision     recall     precision     recall    
random
    
    
    
    
algorithm
    
    
    
    
judge  
    
    
    
    
table    evaluation using judge   as the reference standard  considering items selected
with confidence   and above 
judge  
algorithm
random
include exclude include exclude include exclude
judge   include
  
  
  
  
  
  
exclude
  
   
  
   
  
   
table     agreement and disagreement with judge  
table   shows the precision recall figures using the judgments of judge    the native
chinese speaker  as a reference standard  considering only known items selected with confidence   and above    the algorithm recorded all     items as known  and its confidence
values were scaled linearly from continuous values in range       to discrete values from  
to    the table shows the algorithm s results with its choice thresholded at confidence   
and figure   shows how recall and precision vary as the confidence threshold changes  as
a lower bound for comparison  an algorithm was implemented that considered each word
sense for each item  selecting that sense probabilistically  with complete confidence  in such
a way as to make the average number of senses per item as close as possible to the average
number of senses per item in the reference standard      senses   figures for the random
baseline are the average over    runs  table    illustrates the choices underlying those
figures  for example  there were    senses that the random procedure chose to include that
were also included by judge   
the fact that judge   has such low precision and recall for selection indicates that
matching the choices of an independent judge is indeed a dicult task  this is unsurprising  given previous experience with the problem of selecting among wordnet s fine grained
senses  resnik      a  resnik   yarowsky         the results clearly show that the algorithm is better than the baseline  but also indicate that it is overgenerating senses  which
hurts selection precision  in terms of filtering  when the algorithm chooses to filter out a
sense it tends to do so reliably  filtering precision   however  its propensity toward overgeneration is reected in its below baseline performance on filtering recall  that is  the algorithm
is choosing to allow in senses that it should be filtering out 
    judge    the native speaker of chinese  identified    of the words as known to him  judge   identified
    this on line dictionary was constructed from a large variety of lexical resources  and includes a great
many uncommon words  archaic usages  regionalisms  and the like 

   

firesnik

filtering  algorithm
human
random
selection  algorithm
human
random

precision

 
   
   
   
   

   

   

   

   
recall

 

figure    precision recall curves using judge   as the reference standard  varying the confidence threshold
this pattern of results suggests that the best use of this algorithm at its present level
of performance would be as a filter for a lexical acquisition process with a human in the
loop  dividing candidate wordnet senses for dictionary entries according to higher and
lower priority  for chinese english dictionary entries that serve as appropriate input to the
algorithm  of which there are approximately       in the ceta dictionary   if a wordnet
sense is not selected by the algorithm with a confidence at least equal to   it should be
demoted to the lower priority group in the presentation of alternatives  since the algorithm s
choice to exclude a sense is correct approximately     of the time  those senses that are
selected by the algorithm are not necessarily to be included   the human judge is still
needed to make the selection  since selection precision is low   but the algorithm tends to
err on the side of caution  and so correct senses will be found in the higher priority group
some     of the time 

    linking to wordnet from an english dictionary thesaurus

the results on wordnet sense selection using a bilingual dictionary demonstrate that the
algorithm of figure   does a good job of assigning low scores to wordnet senses that should
be filtered out  even if it should probably not be trusted to make categorical decisions  one
application proposed as suitable  therefore  was helping to identify which senses should be
filtered out within a semi automated process of lexical acquisition  here i describe a closely
related  real world application for which the algorithm has been deployed  adding pointers
into wordnet from an on line dictionary thesaurus on the web 
the context of this application is the wordsmyth english dictionary thesaurus  wedt 
http   www wordsmyth net    an on line educational dictionary aliated with the artfl
text database project  http   humanities uchicago edu artfl   morrissey         it
has been designed to be useful in educational contexts  and  as part of that design  it
integrates a thesaurus within the structure of the dictionary  as illustrated in figure   
   

fiinformation based semantic similarity

bar

syl 
pro 
pos 
def 
exa 
exa 
exa 
syn 
sim 
def 
syn 
sim 
  
 

bar 
bar
noun
   a length of solid material  usu  rectangular or cylindrical 
a bar of soap 
a candy bar 
an iron bar 
rod      stick         
pole    shaft  stake    ingot  block  rail    railing 
crowbar  jimmy  lever
   anything that acts as a restraint or hindrance 
block       hindrance      obstruction      impediment     
obstacle  barrier        stop    
barricade  blockade  deterrent  hurdle  curb  stumbling
block  snag  jam    shoal    reef    sandbar

figure    example from the wordsmyth english dictionary thesaurus  wedt 
wedt contains traditional dictionary information  such as part of speech  pronunciation 
and definitional information  but in many cases also includes pointers to synonyms  syn 
or similar words  sim   within the on line dictionary  these thesaurus items are hyperlinks
  for example  stake  is a link to the first wedt entry for stake   and parenthetical
numbers refer to specific definitions within an entry 
the thesaurus like grouping of similar words provides an opportunity to exploit the
algorithm for disambiguating noun groupings by automatically linking wedt entries to
wordnet  the value in linking these two resources comes from their compatability  in that
both have properties of both a thesaurus and a dictionary  as well as from their complementarity  beyond being an alternative source of definitional information and lists of synonyms 
wordnet provides ordering of word senses by frequency  estimates of word familiarity  partof relationships  and of course the overall taxonomic organization illustrated in figures  
and    figure   shows how taxonomic information is presented using the wordnet web
server  http   www cogsci princeton edu cgi bin webwn   
in a collaboration with wedt and artfl  i have taken the noun entries from the
wedt dictionary and  for each grouping of similar words  added a set of experimental
hyperlinks to wordnet entries on the wordnet web server  figure   shows how the experimental wordnet links  xwn  look to the wedt user  links to wordnet senses  such as
pole   appear together with the confidence level assigned by the sense disambiguation algorithm  senses with confidence less than a threshold are not presented    when an xwn
hyperlink is selected by the user  wordnet taxonomic information for the selected sense
appears in a parallel browser window  as in figure   
from this window  the user has an entry point into the other capabilities of the wordnet
web server  for example  one might choose to look at all the wordnet senses for pole as
    the current threshold       was chosen manually  it may be sub optimal but i have found that it works
well in practice 

   

firesnik

sense  
pole
 a long  usually round  rod of wood or metal or plastic 
   rod
 a long thin implement made of metal or wood 
   implement
 a piece of equipment or tool used to effect an end 
   instrumentality  instrumentation
 an artifact  or system of artifacts  that is
instrumental in accomplishing some end 
   artifact  artefact
 a man made object 
   object  physical object
 a physical  tangible and visible  entity    it was
full of rackets  balls and other objects   
   entity  something
 anything having existence  living or nonliving  

figure    wordnet entry  hypernyms  for pole 
bar

syl 
pro 
pos 
def 
exa 
exa 
exa 
syn 
sim 
xwn 
def 
syn 
sim 
xwn 

  
 

bar 
bar
noun
   a length of solid material  usu  rectangular or cylindrical 
a bar of soap 
a candy bar 
an iron bar 
rod      stick         
pole    shaft  stake    ingot  block  rail    railing 
crowbar  jimmy  lever
pole         ingot         block         rail        
railing         crowbar        
jimmy         lever         lever        lever       
   anything that acts as a restraint or hindrance 
block       hindrance      obstruction      impediment     
obstacle  barrier        stop    
barricade  blockade  deterrent  hurdle  curb  stumbling
block  snag  jam    shoal    reef    sandbar
barricade         barricade        blockade        
blockade         deterrent         hurdle        
hurdle         curb         curb        
curb         curb         stumbling block        
snag         jam         shoal         shoal       
reef         sandbar        

figure    example from wedt with experimental wordnet links
   

fiinformation based semantic similarity

   pole    a long  usually round  rod of wood or metal or plastic 
   pole    a native or inhabitant of poland 
   pole    one of two divergent or mutually exclusive opinions   they are at opposite poles  or  they are
poles apart  
   perch  rod  pole     british  a linear measure of      feet 
   perch  rod  pole    a square rod of land 
   pole  celestial pole    one of two points of intersection of the earth s axis and the celestial sphere 
   pole    one of two antipodal points where the earth s axis of rotation intersects the earth s surface 
   terminal  pole    a point on an electrical device  such as a battery  at which electric current enters
or leaves 
   pole    a long fiberglass implement used for pole vaulting 
    pole  magnetic pole    one of the two ends of a magnet where the magnetism seems to be concentrated 

figure    list of wordnet senses for pole
a noun  displayed as in figure    notice that if a user of wedt had simply gone directly
to the wordnet server to look up pole  the full list of    senses would have appeared
with no indication of which are most potentially related to the wedt dictionary entry
under consideration  in contrast  the wedt hyperlinks  introduced via the sense selection
algorithm  filter out the majority of the irrelevant senses and provide the user a measure of
confidence in selecting among those that remain 
although no formal evaluation of the wedt wordnet connection has been attempted 
the results of the bilingual dictionary experiment suggest that this application of word
sense disambiguation   filtering out the least relevant senses  and then leaving the user
in the loop   is a task for which the sense disambiguation algorithm is well suited  this
is supported by user feedback on the xwn feature of wedt  which has been favorable
 robert parks  personal communication   the site has been growing in popularity  with a
current estimate of           hits per day 

   related work
there is an extensive literature on measuring similarity in general  and on word similarity
in particular  for a classic paper see tversky         recent work in information retrieval
and computational linguistics has emphasized a distributional approach  in which words
are represented as vectors in a space of features and similarity measures are defined in
terms of those vectors  see resnik      b  for discussion  and lee        for a good recent
example  common to the traditional and the distributional approaches is the idea that word
or concept representations include explicit features  whether those features are specified in
a knowledge based fashion  e g   dog might have features like mammal  loyal  or defined
in terms of distributional context  e g   dog might have features like  observed within  
words of howl   this representational assumption contrasts with the assumptions embodied
in a taxonomic representation  where most often the is a relation stands between nondecomposed concepts  the two are not inconsistent  of course  since concepts in a taxonomy
   

firesnik

sometimes can be decomposed into explicit features  and the is a relation  as it is usually
interpreted  implies inheritance of features whether they are explicit or implicit  in that
respect  the traditional approach of counting edges can be viewed as a particularly simple
approximation to a similarity measure based on counting feature differences  under the
assumption that an edge exists to indicate a difference of at least one feature 
information theoretic concepts and techniques have  in recent years  emerged from the
speech recognition community to find wide application in natural language processing  e g  
see church and mercer         the information of an event is a fundamental notion in
stochastic language modeling for speech recognition  where the contribution of a correct
word prediction based on its conditional probability  p wordjcontext   is measured as the
information conveyed by that prediction    log p wordjcontext   this forms the basis for
standard measures of language model performance  such as cross entropy  frequency of
shared and unshared features has also long been a factor in computing similarity over vector representations  the inverse document frequency  idf  for term weighting in information
retrieval makes use of logarithmic scaling  and serves to identify terms that do not discriminate well among different documents  a concept very similar in spirit to the idea that such
terms have low information content  salton        
although the counting of edges in is a taxonomies seems to be something many people
have tried  there seem to be few published descriptions of attempts to directly evaluate
the effectiveness of this method  a number of researchers have attempted to make use of
conceptual distance in information retrieval  for example  rada et al               and lee
et al         report experiments using conceptual distance  implemented using the edgecounting metric  as the basis for ranking documents by their similarity to a query  sussna
       uses semantic relatedness measured with wordnet in word sense disambiguation 
defining a measure of distance that weights different types of links and also explicitly takes
depth in the taxonomy into account 
following the original proposal to measure semantic similarity in a taxonomy using
information content  resnik      b      a   a number of related proposals have been explored  leacock and chodorow        define a measure resembling information content 
but using the normalized path length between the two concepts being compared rather than
the probability of a subsuming concept  specifically  they define
 
min
len c    c    
c
 
c
    
wsimndist  w   w       log         max     
 the notation above is the same as for equation       in addition to this definition  they
also include several special cases  most notably to avoid infinite similarity when c  and
c  are exact synonyms and thus have a path length of    leacock and chodorow have
experimented with this measure and the information content measure described here in the
context of word sense disambiguation  and found that they yield roughly similar results 
implementing their method and testing it on the task reported in section    i found that
it actually outperformed the information based measure slightly on that data set  however 
in a follow up experiment using a different and larger set of noun pairs      items   the
information based measure performed significantly better  table     
analyzing the differences between the two studies is illuminating  in the follow up experiment  i used netnews archives to gather highly frequent nouns within related topic areas
   

fiinformation based semantic similarity

similarity method
correlation
information content
r        
leacock and chodorow r        
edge counting
r        
table     summary of experimental results in follow up study 
 to ensure that similar noun pairs occurred  and then selected noun pairings at random  in
order to avoid biasing the follow up study in favor of either algorithm   there is  therefore 
a predominance of low similarity noun pairs in the test data  looking at the distribution
of ratings for the noun pairs  as given by the two measures  it is evident that the leacock
and chodorow measure is overestimating semantic similarity for many of the predominantly
non similar pairs  this stands to reason since the measure is identical whenever the edge
distance is identical  regardless of whether the pair is high or low in the taxonomy  e g   the
distance between plant and animal is the same as the distance between white oak and red
oak   in contrast  the information based measure is sensitive to the difference  and better
at avoiding spuriously high similarity values for non similar pairs  on a related note  the
edge counting measure used in the follow up study was a variant that computes path length
through a virtual top node  rather than asserting zero similarity between words with no path
connecting them in the existing wordnet taxonomy  as was done previously  using the data
set in the follow up study  the information based measure  at r          does significantly
better than either of the edge counting variants  r         and r           but going back
to the original miller and charles data  the virtual top node variant does significantly better
than the assert zero edge distance measure  with its correlation of r         approaching
that of the measure based on information content  this comparison between the follow up
study and the original miller and charles data illustrates quite clearly how the utility of a
similarity measure can depend upon the distribution of items given by the task 
lin              has recently proposed an alternative information theoretic similarity
measure  derived from a set of basic assumptions about similarity in a style reminiscent of
the way in which entropy information itself has a formal definition derivable from a set of
basic properties  khinchin         formally  lin defines similarity in a taxonomy as 
t

 log p  i ci 
simlin c   c     log  p 
c     log p c  

    

where the ci are the  maximally specific superclasses  oft both c  and c    although the
possibility of multiple inheritance makes the intersection i ci necessary in principle  multiple inheritance is in fact so rare in wordnet that in practice one computes equation     
separately for each common ancestor ci   using p ci  in the numerator  and then takes
the maximum  dekang lin  p c    other than the multiplicative constant of    therefore 
lin s method for determining similarity in a taxonomy is essentially the information based
similarity measure of equation    but normalized by the combined information content of
the two concepts assuming their independence  put another way  lin s measure is taking
   

firesnik

similarity method correlation
information content r        
simwu palmer
r        
simlin
r        
table     summary of lin s results comparing alternative similarity measures
into account not only commonalities but differences between the items being compared 
expressing both in information theoretic terms 
lin s measure is theoretically well motivated and elegantly derived  moreover  lin points
out that his measure will by definition yield the same value for simlin x  x  regardless of
the identity of x   unlike information content  which has been criticized on the grounds
that the value of self similarity depends on how specific a concept x is  and that two nonidentical items x and y can be rated more similar to each other than a third item z is to itself
 richardson et al          from a cognitive perspective  however  similarity comparisons
involving self similarity   robins are similar to robins    as well as subclass relationships
  robins are similar to birds    have themselves been criticized by psychologists as anomalous  medin  goldstone    gentner         moreover  experimental evidence with human
judgments suggests that not all identical objects are judged equally similar  consistent with
the information content measure proposed here but contrary to lin s measure  for example  objects that are identical and complex  such as twins  can seem more similar to each
other than objects that are identical and simple  such as two instances of a simple geometric shape  goldstone        tversky         it would appear  therefore  that insofar as
fidelity to human judgments is relevant  further experimentation is needed to evaluate the
competing predictions of alternative similarity measures 
wu and palmer        propose a similarity measure that is based on edge distances  but
related to lin s measure in the way it takes into account the most specific node dominating
c  and c   characterizing their commonalities  while normalizing in a way that accounts for
their differences  revising wu and palmer s notation slightly  their measure is 
c  
    
simwu palmer c   c     d  c    d d 
c  
 
where c  is the maximally specific superclass of c  and c    d c   is its depth  i e  distance
from the root of the taxonomy  and d c   and d c   are the depths of c  and c  on the path
through c  
lin        repeats the experiment of section   for the information content measure 
simlin  and simwu palmer  reporting the results that appear in table     lin uses a sensetagged corpus to estimate frequencies  and smoothed probabilities rather than simple relative frequency  his results show a somewhat higher correlation for simlin than the other
measures  further experimentation is needed in order to assess the alternative measures 
particularly with respect to their competing predictions and the variability of performance
across data sets  what seems clear  however  is that all these measures perform better than
the traditional edge counting measure 
   

fiinformation based semantic similarity

   conclusions
this article has presented a measure of semantic similarity in an is a taxonomy  based on
the notion of information content  experimental evaluation was performed using a large 
independently constructed corpus  an independently constructed taxonomy  and previously
existing and new human subject data  and the results suggest that the measure performs
encouragingly well and can be significantly better than the traditional edge counting approach  semantic similarity  as measured using information content  was shown to be useful
in resolving cases of two pervasive kinds of linguistic ambiguity  in resolving coordination
ambiguity  the measure was employed to capture the intuition that similarity of meaning is
one indicator that two words are being conjoined  suggestive results of a first experiment
were bolstered by unequivocal results in a second study  demonstrating significant improvements over a disambiguation strategy based only on syntactic agreement  in resolving word
sense ambiguity  the semantic similarity measure was used to assign confidence values to
word senses of nouns within thesaurus like groupings  a formal evaluation provided evidence that the technique can produce useful results but is better suited for semi automated
sense filtering than categorical sense selection  application of the technique to a dictionary thesaurus on the world wide web provides a demonstration of the method in action
in a real world setting 

acknowledgements
sections     of this article comprise a revised and extended version of resnik        
section   describes previously presented algorithms and data  resnik      b      a   extended by further discussion and analysis  section   summarizes an algorithm described
in resnik      a   and then extends previous results by presenting new applications of the
algorithm  with section     containing a formal evaluation in a new setting and section    
giving a real world illustration where the approach has been put into practice  section  
adds a substantial discussion of related work by other authors that has taken place since
the information based similarity measure was originally proposed 
parts of this research were done at the university of pennsylvania with the partial
support of an ibm graduate fellowship and grants aro daal       c       darpa
n         j       nsf iri           and ben franklin   s     c    parts of this research
were also done at sun microsystems laboratories in chelmsford  massachusetts  and parts
of this work were supported at the university of maryland by department of defense
contract mda     c      darpa ito contract n         c       army research
laboratory contract daal      c      through battelle  and a research grant from sun
microsystems laboratories  the author gratefully acknowledges the comments of three
anonymous jair reviewers and helpful discussions with john kovarik  claudia leacock 
dekang lin  johanna moore  mari broman olsen  and jin tong  as well as comments and
criticism received during various presentations of this work 

references
agarwal  r     boggess  l          a simple but useful approach to conjunct identifica   

firesnik

tion  in proceedings of the   th annual meeting of the association for computational
linguistics  pp         association for computational linguistics 
bensch  p  a     savitch  w  j          an occurrence based model of word categorization 
presented at  rd meeting on mathematics of language  mol   
brill  e          discovering the lexical features of a language  in proceedings of the   th
annual meeting of the association for computational linguistics  berkeley  ca 
brill  e     resnik  p          a rule based approach to prepositional phrase attachment disambiguation  in proceedings of the   th international conference on computational
linguistics  coling     
brown  p  f   della pietra  v  j   desouza  p  v   lai  j  c     mercer  r  l         
class based n gram models of natural language  computational linguistics         
        
ceta         chinese dictionaries  an extensive bibliography of dictionaries in chinese
and other languages  chinese english translation assistance group  greenwood
publishing 
church  k  w     mercer  r          introduction to the special issue on computational
linguistics using large corpora  computational linguistics               
church  k  w     patil  r          coping with syntactic ambiguity or how to put the
block in the box on the table  american journal of computational linguistics          
        
collins  a     loftus  e          a spreading activation theory of semantic processing 
psychological review              
collins  m     brooks  j          prepositional phrase attachment through a backed off
model  in third workshop on very large corpora  association for computational
linguistics  cmp lg         
cowie  j   guthrie  j     guthrie  l          lexical disambiguation using simulated annealing  in proceedings of the   th international conference on computational linguistics
 coling      pp          nantes  france 
digital equipment corporation        

altavista web page  refine  or cow    
http   altavista digital com av content about our technology cow  htm 

dorr  b  j          large scale dictionary construction for foreign language tutoring and
interlingual machine translation  machine translation                  
fellbaum  c   ed            wordnet  an electronic lexical database  mit press 
francis  w  n     kucera  h          frequency analysis of english usage  lexicon and
grammar  houghton miin  boston 
   

fiinformation based semantic similarity

goldstone  r  l          similarity  in mit encyclopedia of the cognitive sciences  mit
press  cambridge  ma 
grefenstette  g          use of syntactic context to produce term association lists for text
retrieval  in proceedings of the fifteenth annual international acm sigir conference on research and development in information retrieval  pp        
grefenstette  g          explorations in automatic thesaurus discovery  kluwer  boston 
harman  d          relevance feedback revisited  in proceedings of the fifteenth annual
international acm sigir conference on research and development in information
retrieval  pp       
hearst  m          noun homograph disambiguation using local context in large corpora 
in proceedings of the  th annual conference of the university of waterloo centre for
the new oed and text research oxford 
hindle  d     rooth  m          structural ambiguity and lexical relations  computational
linguistics                  
ji  d   gong  j     huang  c          combining a chinese thesaurus with a chinese
dictionary  in coling acl      pp           universite de montreal 
katz  s  m          estimation of probabilities from sparse data for the language model
component of a speech recognizer  ieee transactions on acoustics  speech and signal
processing  assp                 
khinchin  a  i          mathematical foundations of information theory  new york  dover
publications  translated by r  a  silverman and m  d  friedman 
klavans  j  l     tzoukermann  e          dictionaries and corpora  combining corpus
and machine readable dictionary data for building bilingual lexicons  machine
translation              
kobayasi  y   takunaga  t     tanaka  h          analysis of japanese compound nouns
using collocational information  in proceedings of the   th international conference
on computational linguistics  coling     
krovetz  r     croft  w  b          lexical ambiguity and information retrieval  acm
transactions on information systems                  
kurohashi  s     nagao  m          dynamic programming method for analyzing conjunctive structures in japanese  in proceedings of the   th international conference on
computational linguistics  coling     nantes  france 
lauer  m          conceptual association for compound noun analysis  in proceedings of the
  nd annual meeting of the association for computational linguistics las cruces 
new mexico  student session 
lauer  m          designing statistical language learners  experiments on noun compounds  ph d  thesis  macquarie university  sydney  australia 
   

firesnik

leacock  c     chodorow  m          filling in a sparse training space for word sense
identification  ms 
lee  j  h   kim  m  h     lee  y  j          information retrieval based on conceptual
distance in is a hierarchies  journal of documentation                  
lee  l          similarity based approaches to natural language processing  tech  rep 
tr        harvard university  doctoral dissertation  cmp lg         
lesk  m          automatic sense disambiguation using machine readable dictionaries 
how to tell a pine cone from an ice cream cone  in proceedings of the      sigdoc
conference  pp        
li  h     abe  n          generalizing case frames using a thesaurus and the mdl principle 
in proceedings of the international conference on recent advances in nlp velingrad 
bulgaria 
lin  d          using syntactic dependency as local context to resolve word sense ambiguity  in proceedings of the   th annual meeting of the association for computational
linguistics and  th conference of the european chapter of the association for computational linguistics madrid  spain 
lin  d          an information theoretic definition of similarity  in proceedings of the
fifteenth international conference on machine learning  icml     madison  wisconsin 
marcus  m  p   santorini  b     marcinkiewicz  m          building a large annotated
corpus of english  the penn treebank  computational linguistics              
mckeown  k     hatzivassiloglou  v          augmenting lexicons automatically  clustering semantically related adjectives  in bates  m   ed    arpa workshop on human
language technology  morgan kaufmann 
medin  d   goldstone  r     gentner  d          respects for similarity  psychological
review                   
merlo  p   crocker  m     berthouzoz  c          attaching multiple prepositional phrases 
generalized backed off estimation  in proceedings of the second conference on empirical methods in natural language processing  emnlp     cmp lg         
miller  g          wordnet  an on line lexical database  international journal of lexicography          special issue  
miller  g  a     charles  w  g          contextual correlates of semantic similarity  language and cognitive processes              
morrissey 
r 
        texts and contexts  the artfl database in french studies  profession    
       http   humanities uchicago edu homes publications romoart html 
   

fiinformation based semantic similarity

pereira  f   tishby  n     lee  l          distributional clustering of english words  in proceedings of the   st annual meeting of the association for computational linguistics
 acl     morristown  new jersey  association for computational linguistics 
quillian  m  r          semantic memory  in minsky  m   ed    semantic information
processing  mit press  cambridge  ma 
rada  r     bicknell  e          ranking documents with a thesaurus  jasis         
        
rada  r   mili  h   bicknell  e     blettner  m          development and application of
a metric on semantic nets  ieee transaction on systems  man  and cybernetics 
              
ratnaparkhi  a     roukos  s          a maximum entropy model for prepositional phrase
attachment  in proceddings of the arpa workshop on human language technology
plainsboro  nj 
resnik  p       a   selection and information  a class based approach to lexical relationships 
ph d 
thesis 
university
of
pennsylvania 
 ftp   ftp cis upenn edu pub ircs tr       ps z  
resnik  p       b   semantic classes and syntactic ambiguity  in proceedings of the     
arpa human language technology workshop  morgan kaufmann 
resnik  p          using information content to evaluate semantic similarity in a taxonomy 
in proceedings of the   th international joint conference on artificial intelligence
 ijcai       cmp lg          
resnik  p          selectional constraints  an information theoretic model and its computational realization  cognition              
resnik  p       a   disambiguating noun groupings with respect to wordnet senses  in
armstrong  s   church  k   isabelle  p   tzoukermann  e     yarowsky  d   eds   
natural language processing using very large corpora  kluwer 
resnik  p       b   wordnet and class based probabilities  in fellbaum  c   ed    wordnet 
an electronic lexical database  mit press 
resnik  p     yarowsky  d          a perspective on word sense disambiguation methods
and their evaluation  in anlp workshop on tagging text with lexical semantics
washington  d c 
resnik  p     yarowsky  d          distinguishing systems and distinguishing senses  new
evaluation methods for word sense disambiguation  natural language engineering 
 to appear  
richardson  r   smeaton  a  f     murphy  j          using wordnet as a knowledge base for measuring semantic similarity between words  working paper ca      dublin city university  school of computer applications  dublin  ireland 
ftp   ftp compapp dcu ie pub w papers      ca     ps z 
   

firesnik

ross  s          a first course in probability  macmillan 
rubenstein  h     goodenough  j          contextual correlates of synonymy  cacm 
                
salton  g          automatic text processing  addison wesley 
schutze  h          word space  in hanson  s  j   cowan  j  d     giles  c  l   eds    advances in neural information processing systems    pp           morgan kaufmann
publishers  san mateo ca 
sinclair  ed    j          collins cobuild english language dictionary  collins  london 
sussna  m          word sense disambiguation for free text indexing using a massive semantic network  in proceedings of the second international conference on information
and knowledge management  cikm     arlington  virginia 
tversky  a          features of similarity  psychological review              
voorhees  e  m          using wordnet to disambiguate word senses for text retrieval  in
korfhage  r   rasmussen  e     willett  p   eds    proceedings of the sixteenth annual
international acm sigir conference on research and development in information
retrieval  pp          
voorhees  e  m          query expansion using lexical semantic relations  in   th international conference on research and development in information retrieval  sigir
     dublin  ireland 
vossen  p          special issue on eurowordnet  computers and the humanities           
weiss  s  m     kulikowski  c  a          computer systems that learn  classification and
prediction methods from statistics  neural nets  machine learning  and expert systems 
morgan kaufmann  san mateo  ca 
whittemore  g   ferrara  k     brunner  h          empirical study of predictive powers of
simple attachment schemes for post modifier prepositional phrases  in proceedings of
the   th annual meeting of the association for computational linguistics  pp        
pittsburgh  pennsylvania 
wilks  y     stevenson  m          the grammar of sense  is word sense tagging much
more than part of speech tagging    technical report cs        cmp lg         
wu  z     palmer  m          verb semantics and lexical selection  in proceedings of the
  nd annual meeting of the association for computational linguistics las cruces 
new mexico 

   

fi