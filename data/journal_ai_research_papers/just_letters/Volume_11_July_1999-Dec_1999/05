journal artificial intelligence research                  

submitted       published     

evolutionary algorithms reinforcement learning
david e  moriarty

moriarty isi edu

university southern california  information sciences institute
     admiralty way  marina del rey  ca      

alan c  schultz

navy center applied research artificial intelligence
naval research laboratory  washington dc           

schultz aic nrl navy mil

john j  grefenstette

institute biosciences  bioinformatics biotechnology
george mason university  manassas  va      

gref ib  gmu edu

abstract

two distinct approaches solving reinforcement learning problems  namely 
searching value function space searching policy space  temporal difference methods evolutionary algorithms well known examples approaches  kaelbling 
littman moore recently provided informative survey temporal difference methods  article focuses application evolutionary algorithms reinforcement
learning problem  emphasizing alternative policy representations  credit assignment methods  problem specific genetic operators  strengths weaknesses evolutionary
approach reinforcement learning presented  along survey representative
applications 

   introduction
kaelbling  littman  moore        recently sutton barto        provide informative surveys field reinforcement learning  rl   characterize two
classes methods reinforcement learning  methods search space value functions methods search space policies  former class exemplified
temporal difference  td  method latter evolutionary algorithm  ea 
approach  kaelbling et al  focus entirely first set methods provide
excellent account state art td learning  article intended round
picture addressing evolutionary methods solving reinforcement learning
problem 
kaelbling et al  clearly illustrate  reinforcement learning presents challenging array
diculties process scaling realistic tasks  including problems associated
large state spaces  partially observable states  rarely occurring states  nonstationary environments  point  approach best remains open question 
sensible pursue parallel lines research alternative methods  beyond
scope article address whether better general search value function
space policy space  hope highlight strengths evolutionary
approach reinforcement learning problem  reader advised view
c     


ai access foundation morgan kaufmann publishers  rights reserved 

fimoriarty  schultz    grefenstette

article ea vs  td discussion  cases  two methods provide complementary
strengths  hybrid approaches advisable  fact  survey implemented systems
illustrates many ea based reinforcement learning systems include elements tdlearning well 
next section spells reinforcement learning problem  order provide
specific anchor later discussion  section   presents particular td method  section   outlines approach call evolutionary algorithms reinforcement learning
 earl   provides simple example particular earl system  following three
sections focus features distinguish eas rl eas general function optimization  including alternative policy representations  credit assignment methods 
rl specific genetic operators  sections     highlight strengths weaknesses
ea approach  section    brie surveys successful applications ea systems
challenging rl tasks  final section summarizes presentation points
directions research 

   reinforcement learning

reinforcement learning methods share goal  solve sequential decision tasks
trial error interactions environment  barto  sutton    watkins       
grefenstette  ramsey    schultz         sequential decision task  agent interacts
dynamic system selecting actions affect state transitions optimize
reward function  formally  given time step t  agent perceives state
st selects action at  system responds giving agent  possibly zero 
numerical reward r st  changing state st      st   at   state transition may
determined solely current state agent s action may involve stochastic
processes 
agent s goal learn policy      a  maps states actions 
optimal policy    defined many ways  typically defined policy
produces greatest cumulative reward states s 

  argmax
v  s     s 


   

v  s  cumulative reward received state using policy  
many ways compute v  s   one approach uses discount rate discount rewards
time  sum computed infinite horizon 

v

 

 
x
    ir



i  

t i

   

rt reward received time step t  alternatively  v  s  could computed
summing rewards finite horizon h 

v  st   

xh r
i  

t i

   

agent s state descriptions usually identified values returned
sensors  provide description agent s current state state
   

fievolutionary algorithms reinforcement learning

world  often sensors give agent complete state information thus
state partially observable 
besides reinforcement learning  intelligent agents designed paradigms 
notably planning supervised learning  brie note major differences
among approaches  general  planning methods require explicit model
state transition function  s  a   given model  planning algorithm search
possible action choices find action sequence guide agent
initial state goal state  since planning algorithms operate using model
environment  backtrack  undo  state transitions enter undesirable states 
contrast  rl intended apply situations suciently tractable action
model exist  consequently  agent rl paradigm must actively explore
environment order observe effects actions  unlike planning  rl agents
cannot normally undo state transitions  course  cases may possible
build action model experience  sutton         enabling planning
experience accumulates  however  rl research focuses behavior agent
insucient knowledge perform planning 
agents trained supervised learning  supervised learning  agent
presented examples state action pairs  along indication action
either correct incorrect  goal supervised learning induce general policy
training examples  thus  supervised learning requires oracle supply
correctly labeled examples  contrast  rl require prior knowledge correct
incorrect decisions  rl applied situations rewards sparse 
example  rewards may associated certain states  cases  may
impossible associate label  correct   incorrect  particular decisions without
reference agent s subsequent decisions  making supervised learning infeasible 
summary  rl provides exible approach design intelligent agents situations planning supervised learning impractical  rl applied
problems significant domain knowledge either unavailable costly obtain 
example  common rl task robot control  designers autonomous robots often
lack sucient knowledge intended operational environment use either planning
supervised learning regime design control policy robot  case 
goal rl would enable robot generate effective decision policies explores
environment 
figure   shows simple sequential decision task used example later
paper  task agent grid world move state state
selecting among two actions  right  r   d   sensor agent returns
identity current state  agent always starts state a  receives reward
indicated upon visiting state  task continues agent moves grid
world  e g   taking action state a    goal learn policy returns
highest cumulative rewards  example  policy results sequences
actions r  d  r  d  d  r  r  starting state a  gives optimal score    
   

fimoriarty  schultz    grefenstette



b

c



e

 

 

 

 

  

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

figure    simple grid world sequential decision task  agent starts state a 
receives row column current box sensory input  agent moves
one box another selecting two moves  right down  
agent s score increased payoff indicated box  goal find
policy maximizes cumulative score 

    policy space vs  value function space
given reinforcement learning problem described previous section 
address main topic  find optimal policy    consider two main approaches 
one involves search policy space involves search value function space 
policy space search methods maintain explicit representations policies modify
variety search operators  many search methods considered 
including dynamic programming  value iteration  simulated annealing  evolutionary
algorithms  paper focuses evolutionary algorithms specialized
reinforcement learning task 
contrast  value function methods maintain explicit representation
policy  instead  attempt learn value function v   returns expected
cumulative reward optimal policy state  focus research value
function approaches rl design algorithms learn value functions
experience  common approach learning value functions temporal difference  td  method  described next section 

   temporal difference algorithms reinforcement learning
stated introduction  comprehensive comparison value function search
direct policy space search beyond scope paper  nevertheless  useful
point key conceptual differences typical value function methods typical
evolutionary algorithms searching policy space  common approach learning
value function v rl problems temporal difference  td  method  sutton        
   

fievolutionary algorithms reinforcement learning

td learning algorithm uses observations prediction differences consecutive
states update value predictions  example  two consecutive states j return
payoff prediction values      respectively  difference suggests payoff
state may overestimated reduced agree predictions
state j   updates value function v achieved using following update rule 

v  st    v  st    ff v  st      v  st    rt 
   
represents learning rate rt immediate reward  thus  difference
predictions  v  st     v  st    consecutive states used measure prediction error 
consider chain value predictions v  s     v  sn   consecutive state transitions
last prediction v  sn   containing non zero reward environment 

many iterations sequence  update rule adjust values state
agree successors eventually reward received v  sn   
words  single reward propagated backwards chain value predictions 
net result accurate value function used predict expected reward
state system 
mentioned earlier 
goal
td methods learn value function




optimal policy  v   given v   optimal action   s   computed using
following equation 
   s  a  
 s    argmax
v


   

course  already stated rl state transition function  s  a  unknown
agent  without knowledge  way evaluating      alternative
value function used compute  s  called q function  q s  a   watkins 
      watkins   dayan         q function value function represents
expected value taking action state acting optimally thereafter 

q s  a    r s    v   s  a  

   
r s  represents immediate reward received state s  given q function 
actions optimal policy directly computed using following equation 

 s    argmax
q s  a 


   

q st  at    q st  at    ff max
q st    at      q st  at    r st  
  

   

table   shows q function grid world problem figure    table based
representation q function associates cumulative future payoffs state action
pair system   the letter number pairs top represent state given row
column figure    r represent actions right down  respectively  
td method adjusts q values decision  selecting next action 
agent considers effect action examining expected value state
transition caused action 
q function learned following td update equation 


   

fimoriarty  schultz    grefenstette

a  a  a  a  a  b  b  b  b  b  c  c  c  c  c  d  d  d  d  d  e  e  e  e  e 
r                                                        
                                                        

table    q function simple grid world  value associated state action
pair 
essentially  equation updates q st   at  based current reward predicted
reward future actions selected optimally  watkins dayan        proved
updates performed fashion every q value explicitly represented 
estimates asymptotically converge correct values  reinforcement learning
system thus use q values select optimal action state  qlearning widely known implementation temporal difference learning 
use qualitative comparisons evolutionary approaches later sections 

   evolutionary algorithms reinforcement learning  earl 
policy space approach rl searches policies optimize appropriate objective
function  many search algorithms might used  survey focuses evolutionary
algorithms  begin brief overview simple ea rl  followed detailed
discussion features characterize general class eas rl 

    design considerations evolutionary algorithms

evolutionary algorithms  eas  global search techniques derived darwin s theory
evolution natural selection  ea iteratively updates population potential
solutions  often encoded structures called chromosomes  iteration 
called generation  ea evaluates solutions generates offspring based fitness
solution task environment  substructures  genes  solutions
modified genetic operators mutation recombination  idea
structures associated good solutions mutated combined form
even better solutions subsequent generations  canonical evolutionary algorithm
shown figure    wide variety eas developed  including genetic
algorithms  holland        goldberg         evolutionary programming  fogel  owens   
walsh         genetic programming  koza         evolutionary strategies  rechenberg 
      
eas general purpose search methods applied variety domains
including numerical function optimization  combinatorial optimization  adaptive control 
adaptive testing  machine learning  one reason widespread success eas
relatively requirements application  namely 
   appropriate mapping search space space chromosomes 
   appropriate fitness function 
   

fievolutionary algorithms reinforcement learning

procedure ea
begin
    
initialize p t  
evaluate structures p t  
termination condition satisfied
begin
      
select p t  p t    
alter structures p t  
evaluate structures p t  
end
end 
figure    pseudo code evolutionary algorithm 
example  case parameter optimization  common represent list
parameters either vector real numbers bit string encodes parameters 
either representations   standard  genetic operators mutation
cut and splice crossover applied straightforward manner produce genetic
variations required  see figure     user must still decide  rather large  number
control parameters ea  including population size  mutation rates  recombination
rates  parent selection rules  extensive literature studies suggest
eas relatively robust wide range control parameter settings  grefenstette 
      schaffer  caruana  eshelman    das         thus  many problems  eas
applied relatively straightforward manner 
however  many applications  eas need specialized problem domain  grefenstette         critical design choice facing user representation  is  mapping search space knowledge structures  or 
phenotype space  space chromosomes  the genotype space   many studies
shown effectiveness eas sensitive choice representations 
sucient  example  choose arbitrary mapping search space space
chromosomes  apply standard genetic operators hope best  makes
good mapping subject continuing research  general consensus candidate solutions share important phenotypic similarities must exhibit similar forms
 building blocks  represented chromosomes  holland         follows
user ea must carefully consider natural way represent elements
search space chromosomes  moreover  often necessary design appropriate
mutation recombination operators specific chosen representation 
end result design process representation genetic operators selected
ea comprise form search bias similar biases machine learning meth   

fimoriarty  schultz    grefenstette

parent   



b

c



e

f

g

parent   



b

c



e

f

g

offspring   



b

c



e

f

g

offspring   



b

c



e

f

g

figure    genetic operators fixed position representation  two offspring generated crossing selected parents  operation shown called one point
crossover  first offspring inherits initial segment one parent
final segment parent  second offspring inherits pattern
genes opposite parents  crossover point position    chosen
random  second offspring incurred mutation shaded gene 
ods  given proper bias  ea quickly identify useful  building blocks  within
population  converge promising areas search space  
case rl  user needs make two major design decisions  first 
space policies represented chromosomes ea  second  fitness
population elements assessed  answers questions depend user
chooses bias ea  next section presents simple earl adopts
straightforward set design decisions  example meant provide baseline
comparison elaborate designs 

    simple earl

remainder paper shows  many ways use eas search space
rl policies  section provides concrete example simple earl  call
earl    pseudo code shown figure    system provides ea counterpart
simple table based td system described section   
straightforward way represent policy ea use single chromosome per policy single gene associated observed state  earl   
gene s value  or allele biological terminology  represents action value associated
corresponding state  shown figure    table   shows part earl  population
policies sample grid world problem  number policies population
usually order          
fitness policy population must ect expected accumulated fitness
agent uses given policy  fixed constraints fitness
individual policy evaluated  world deterministic  sample grid world 
   ways exploit problem specific knowledge eas include use heuristics initialize
population hybridization problem specific search algorithms  see  grefenstette       
discussions methods 

   

fievolutionary algorithms reinforcement learning

procedure earl  
begin
    
initialize population policies  p t  
evaluate policies p t  
termination condition satisfied
begin
      
select high payoff policies  p t   policies p t    
update policies p t  
evaluate policies p t  
end
end 
figure    pseudo code evolutionary algorithm reinforcement learning system 
policy i 

s 
a 

s 
a 

s 
a 

   

sn


figure    table based policy representation  observed state gene indicates
preferred action state  representation  standard genetic
operators mutation crossover applied 
fitness policy evaluated single trial starts agent
initial state terminates agent reaches terminal state  e g   falls grid
grid world   non deterministic worlds  fitness policy usually averaged
sample trials  options include measuring total payoff achieved
agent fixed number steps  measuring number steps required achieve
fixed level payoff 
fitness policies population determined  new population
generated according steps usual ea  figure     first  parents selected
reproduction  typical selection method probabilistically select individuals based
relative fitness 
 pi  
pr pi     pnfitness
j    fitness pj  

   

pi represents individual n total number individuals  using selection
rule  expected number offspring given policy proportional policy s
fitness  example  policy average fitness might single offspring  whereas
   

fimoriarty  schultz    grefenstette

policy
 
 
 
 
 

a 


r

r

a 
r





a 






a 


r



a 
r
r
r
r
r

b 
r
r




b 
r
r
r
r
r

b 
r
r

r
r

b 
r
r
r
r


b 
r
r
r
r
r

c 



r
r

c 
r





c 

r

r
r

c 

r
r
r
r

c 
r


r


d 
r
r
r

r

d 



r


d 
r
r
r
r
r

d 
r
r
r

r

d 
r
r
r
r


e 






e 
r
r
r
r
r

e 
r





e 






e  fitness
r  
r  
  
r   
  

table    ea population five decision policies sample grid world  simple
policy representation specifies action state world  fitness
corresponds payoffs accumulated using policy grid
world 
policy twice average fitness would two offspring   offspring formed
cloning selected parents  new policies generated applying standard
genetic operators crossover mutation clones  shown figure    process
generating new populations strategies continue indefinitely terminated
fixed number generations acceptable level performance achieved 
simple rl problems grid world  earl  may provide adequate approach  later sections  point ways even earl  exhibits
strengths complementary td methods rl  however  case td
methods  earl methods extended handle many challenges inherent
realistic rl problems  following sections survey extensions  organized around three specific biases distinguish eas reinforcement learning  earl 
generic eas  policy representations  fitness credit assignment models  rlspecific genetic operators 

   policy representations earl

perhaps critical feature distinguishes classes eas one another
representation used  example  eas function optimization use simple string
vector representation  whereas eas combinatorial optimization use distinctive representations permutations  trees graph structures  likewise  eas rl use
distinctive set representations policies  range potential policy representations unlimited  representations used earl systems date
largely categorized along two discrete dimensions  first  policies may represented either condition action rules neural networks  second  policies may represented
single chromosome representation may distributed one
populations 

    single chromosome representation policies
      rule based policies

rl problems practical interest  number observable states large 
simple table based representation earl  impractical  large scale state
   many parent selection rules explored  grefenstette      a      b  

   

fievolutionary algorithms reinforcement learning

policy i 

c i  ai 

c i  ai 

c i  ai 

   

c ik aik

figure    rule based policy representation  gene represents condition action rule
maps set states action  general  rules independent
position along chromosome  con ict resolution mechanisms may
needed conditions rules allowed intersect 
w 
w k 
policy i 

w 

w 

w 

   

wk

  
   

wk
wj

figure    simple parameter representation weights neural network  fitness
policy payoff agent uses corresponding neural net
decision policy 
spaces  reasonable represent policy set condition action rules
condition expresses predicate matches set states  shown figure    early
examples representation include systems ls    smith        ls    schaffer
  grefenstette         followed later samuel  grefenstette et al         
      neural net representation policies

td based rl systems  earl systems often employ neural net representations
function approximators  simplest case  see figure     neural network
agent s decision policy represented sequence real valued connection weights 
straightforward ea parameter optimization used optimize weights
neural network  belew  mcinerney    schraudolph        whitley  dominic  das   
anderson        yamauchi   beer         representation thus requires least
modification standard ea  turn distributed representations policies
earl systems 

    distributed representation policies

previous section outlined earl approaches treat agent s decision policy
single genetic structure evolves time  section addresses earl approaches
decompose decision policy smaller components  approaches two
potential advantages  first  allow evolution work detailed level task 
e g   specific subtasks  presumably  evolving solution restricted subtask
   

fimoriarty  schultz    grefenstette

sensors

message list

rewards

classifiers

decision

evolutionary
algorithm

figure    holland s learning classifier system 
easier evolving monolithic policy complex task  second  decomposition permits
user exploit background knowledge  user might base decomposition
subtasks prior analysis overall performance task  example  might known
certain subtasks mutually exclusive therefore learned independently 
user might decompose complex task subtasks certain components
explicitly programmed components learned 
terms knowledge representation earl  alternative single chromosome
representation distribute policy several population elements  assigning
fitness individual elements policy  evolutionary selection pressure
brought bear detailed aspects learning task  is  fitness
function individual subpolicies individual rules even individual neurons  general
approach analogous classic td methods take approach extreme
learning statistics concerning state action pair  case single chromosome
representations  partition distributed earl representations rule based
neural net based classes 
      distributed rule based policies

well known example distributed rule based approach earl learning classifier systems  lcs  model  holland   reitman        holland        wilson 
       lcs uses evolutionary algorithm evolve if then rules called classifiers
map sensory input appropriate action  figure   outlines holland s lcs framework
 holland         sensory input received  posted message list  left
hand side classifier matches message message list  right hand side posted
message list  new messages may subsequently trigger classifiers post
messages invoke decision lcs  traditional forward chaining model
rule based systems 
lcs  chromosome represents single decision rule entire population
represents agent s policy  general  classifiers map set observed states set
messages  may interpreted either internal state changes actions  example 
   

fievolutionary algorithms reinforcement learning

condition
action strength
a 
  r
    
  
 
    
d 

   
 



    

table    lcs population grid world    don t care symbol allows
generality conditions  example  first rule says  turn right column
a   strength rule used con ict resolution parent selection
genetic algorithm 

lcs
lcs

lcs

environment

figure    two level hierarchical alecsys system  lcs learns specific behavior 
interactions among rule sets pre programmed 
learning agent grid world figure   two sensors  one column
one row  population lcs might appear shown table   
first classifier matches state column recommends action r  classifier
statistic called strength estimates utility rule  strength statistics
used con ict resolution  when one action recommended 
fitness genetic algorithm  genetic operators applied highly fit classifiers
generate new rules  generally  population size  i e   number rules policy 
kept constant  thus classifiers compete space policy 
another way earl systems distribute representation policies partition
policy separate modules  module updated ea  dorigo
colombetti        describe architecture called alecsys complex reinforcement learning task decomposed subtasks  learned via separate
lcs  shown figure    provide method called behavior analysis training
 bat  manage incremental training agents using distributed lcs architecture 
single chromosome representation extended partitioning policy across multiple co evolving populations  example  cooperative co evolution
model  potter         agent s policy formed combining chromosomes several independently evolving populations  chromosome represents set rules 
figure    rules address subset performance task  example 
separate populations might evolve policies different components complex task 
   

fimoriarty  schultz    grefenstette

ea
ea  

domain
model
collaboration

fitness

evolutionary
algorithm

population

representative

merge

representative

individual

evaluated

representative

ea  

ea n

representative

figure     cooperative coevolutionary architecture perspective ith ea instance  ea contributes representative  merged others 
representatives form collaboration  policy agent  fitness
representative ects average fitness collaborations 

might address mutually exclusive sets observed states  fitness chromosome
computed based overall fitness agents employ chromosome part
combined chromosomes  combined chromosomes represent decision policy
called collaboration  figure     
      distributed network based policies

distributed earl systems using neural net representations designed 
 potter   de jong         separate populations neurons evolve  evaluation
neuron based fitness collaboration neurons selected population 
sane  moriarty   miikkulainen      a         two separate populations maintained
evolved  population neurons population network blueprints  motivation sane comes priori knowledge individual neurons fundamental
building blocks neural networks  sane explicitly decomposes neural network search
problem several parallel searches effective single neurons  neuron level evolution provides evaluation recombination neural network building blocks 
population blueprints search effective combinations building blocks  figure   
gives overview interaction two populations 
individual blueprint population consists set pointers individuals
neuron population  generation  neural networks constructed
combining hidden neurons specified blueprint  blueprint receives fitness
according well corresponding network performs task  neuron receives
fitness according well top networks participates perform
task  aggressive genetic selection recombination strategy used quickly build
propagate highly fit structures neuron blueprint populations 
   

fievolutionary algorithms reinforcement learning

network blueprint population

neuron population

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

figure     overview two populations sane  member neuron population specifies series connections  connection labels weights 
made within neural network  member network blueprint population specifies series pointers specific neurons used build
neural network 

   fitness credit assignment earl

evolutionary algorithms driven concept natural selection  population
elements higher fitness leave offspring later generations  thus uencing
direction search favor high performance regions search space  concept
fitness central ea  section  discuss features fitness model
common across earl systems  specifically focus ways fitness
function ects distinctive structure rl problem 

    agent model

first common features earl fitness models fitness computed
respect rl agent  is  however policy represented ea  must
converted decision policy agent operating rl environment  agent
assumed observe description current state  select next action consulting
current policy  collect whatever reward provided environment  earl
systems  td systems  agent generally assumed perform little additional
computation selecting next action  neither approach limits agent
strict stimulus response behavior  usually assumed agent perform
extensive planning reasoning acting  assumption ects fact
rl tasks involve sort control activity agent must respond dynamic
environment within limited time frame 
   

fimoriarty  schultz    grefenstette

    policy level credit assignment

shown previous section  meaning fitness earl systems may vary depending population elements represent  single chromosome representation 
fitness associated entire policies  distributed representation  fitness may associated individual decision rules  case  fitness always ects accumulated
rewards received agent course interaction environment 
specified rl model  fitness may ect effort expended  amount delay 
worthwhile considering different approaches credit assignment td
ea methods  reinforcement learning problem  payoffs may sparse  is 
associated certain states  consequently  payoff may ect quality
extended sequence decisions  rather individual decision  example  robot
may receive reward movement places  goal  position within room 
robot s reward  however  depends many previous movements leading
point  dicult credit assignment problem therefore exists apportion
rewards sequence decisions individual decisions 
general  ea td methods address credit assignment problem different ways  td approaches  credit reward signal explicitly propagated
decision made agent  many iterations  payoffs distributed across
sequence decisions appropriately discounted reward value associated
individual state decision pair 
simple earl systems earl    rewards associated sequences
decisions distributed individual decisions  credit assignment
individual decision made implicitly  since policies prescribe poor individual decisions
fewer offspring future generations  selecting poor policies  evolution
automatically selects poor individual decisions  is  building blocks consisting
particular state action pairs highly correlated good policies propagated
population  replacing state action pairs associated poorer policies 
figure    illustrates differences credit assignment td earl 
grid world figure    q learning td method explicitly assigns credit blame
individual state action pair passing back immediate reward estimated payoff
new state  thus  error term becomes associated action performed
agent  ea approach explicitly propagate credit action rather
associates overall fitness entire policy  credit assigned implicitly  based
fitness evaluations entire sequences decisions  consequently  ea tend select
policies generate first third sequences achieve lower fitness
scores  ea thus implicitly selects action state b   example 
present bad sequences present good sequences 

    subpolicy credit assignment

besides implicit credit assignment performed building blocks  earl systems
addressed credit assignment problem directly  shown section   
individuals earl system might represent either entire policies components
policy  e g   component rule sets  individual decision rules  individual neurons  
distributed representation earls  fitness explicitly assigned individual components 
   

fievolutionary algorithms reinforcement learning

td explicit credit assignment
  max q b  a  

a  r

b  d

  max q b  a  

a  r

  max q a  a  

a  d

  max q b  a  

a  d

a  r

b  d

b  d

b  d

c 

 

c 

a  r

b  d

b  r

c  d

c 

 

c 

a  d

a  r

b  d

b  d

c 

 

d 

a  d

a  d

b  r

c  d

d 

 

  max q c  a  

  max q c  a  

b  d

   max q c  a  

b  r

c 

c  d

   max q b  a  

b  d

fitness

  max q c  a  

b  d

   max q c  a  

b  r

  max q b  a  

a  r

   max q b  a  

b  d

  max q b  a  

b  d

  max q a  a  

a  d

  max q b  a  

ea implicit credit assignment

  max q d  a  

c  d

figure     explicit vs  implicit credit assignment  q learning td method assigns credit
state action pair based immediate reward predicted future
rewards  ea method assigns credit implicitly associating fitness values
entire sequences decisions 
cases policy represented explicit components  different fitness functions
associated different evolving populations  allowing implementer  shape 
overall policy evolving subpolicies specific subtasks  dorigo   colombetti       
potter  de jong    grefenstette         ambitious goal allow system
manage number co evolving species well form interactions  potter        
exciting research still early stage 
example  lcs model  classifier  decision rule  strength
updated using td like method called bucket brigade algorithm  holland        
bucket brigade algorithm  strength classifier used bid classifiers
right post messages  bids subtracted winning classifiers passed back
classifiers posted enabling message previous step  classifier strengths
thus reinforced classifier posts message triggers another classifier 
classifier invokes decision lcs receives strength reinforcement directly
environment  bucket brigade bid passing mechanism clearly bears strong
relation method temporal differences  sutton         bucket brigade updates
given classifier s strength based strength classifiers fire direct result
activation  td methods differ slightly respect assign credit
based strictly temporal succession take account causal relations steps 
remains unclear appropriate distributing credit 
even single chromosome representations  td like methods adopted
earl systems  samuel  gene  decision rule  maintains quantity called
strength used resolve con ict one rule matches agent s current
sensor readings  payoff obtained  thereby terminating trial   strengths
   

fimoriarty  schultz    grefenstette

rules fired trial updated  grefenstette         addition resolving
con icts  rule s strength plays role triggering mutation operations  described
next section 

   rl specific genetic operators

creation special genetic operators provides another avenue imposing rlspecific bias eas  specialized operators earl systems first appeared  holland 
       so called triggered operators responsible creating new classifiers
learning agent found classifier existing population matched
agent s current sensor readings  case  high strength rule explicitly generalized
cover new set sensor readings  similar rule creation operator included
early versions samuel  grefenstette et al          later versions samuel included
number mutation operators created altered rules based agent s early
experiences  example  samuel s specialization mutation operator triggered
low strength  general rule fires episode results high payoff 
case  rule s conditions reduced generality closely match agent s sensor
readings  example  agent sensor readings  range       bearing       
original rule is 
range            bearing            set turn       strength
    
new rule would be 
range            bearing             set turn       strength
    
since episode triggering operator resulted high payoff  one might suspect
original rule over generalized  new  specific version might lead
better results   the strength new rule initialized payoff received
triggering episode   considered lamarckian operator agent s
experience causing genetic change passed later offspring  
samuel uses rl specific crossover operator recombine policies  particular 
crossover samuel attempts cluster decision rules assigning offspring 
example  suppose traces previous evaluations parent strategies follows  ri j denotes j th decision rule policy i  
trace parent    
episode 
  
 
   r      r      r      r    high payoff
   r      r      r   
low payoff
   jean baptiste lamarck developed evolutionary theory stressed inheritance acquired characteristics  particular acquired characteristics well adapted surrounding environment 
course  lamarck s theory superseded darwin s emphasis two stage adaptation  undirected
variation followed selection  research generally failed substantiate lamarckian mechanisms
biological systems  gould        

   

fievolutionary algorithms reinforcement learning

  
 
trace parent    
  
 
   r      r   
   r      r      r   
  
 
one possible offspring would be 

low payoff
high payoff

fr             r      r      r      r             r      r      r             r   g
motivation rules fire sequence achieve high payoff
treated group recombination  order increase likelihood offspring
policy inherit better behavior patterns parents  rules
fire successful episodes  e g   r     randomly assigned one two offspring 
form crossover lamarckian  since triggered experiences
agent   directly related structure rl problem  since groups
components policies according temporal association among decision rules 

   strengths earl

ea approach represents interesting alternative solving rl problems  offering
several potential advantages scaling realistic applications  particular  earl
systems developed address dicult challenges rl problems  including 
large state spaces 
incomplete state information 
non stationary environments 
section focuses ways earl address challenges 

    scaling large state spaces

many early papers rl literature analyze eciency alternative learning methods
toy problems similar grid world shown figure    studies useful
academic exercises  number observed states realistic applications rl likely
preclude approach requires explicit storage manipulation statistics
associated observable state action pair  two ways earl policy
representations help address problem large state spaces  generalization selectivity 
      policy generalization

earl policy representations specify policy level abstraction higher
explicit mapping observed states actions  case rule based representations 
rule language allows conditions match sets states  thus greatly reducing storage
   

fimoriarty  schultz    grefenstette

a  a  a  a  a  b  b  b  b  b  c  c  c  c  c  d  d  d  d  d  e  e  e  e  e 
r                                                                  
l                                                                    

table    approximated value function population table    table displays average fitness policies select state action pair ects
estimated impact action overall fitness  given tiny population
size example  estimates particularly accurate  note question
marks states actions converged  since policies select alternative action  population statistics impact actions
fitness  different simple td methods  statistics actions
maintained 

required specify policy  noted  however  generality rules
within policy may vary considerably  level rules specify action
single observed state way completely general rules recommend action
regardless current state  likewise  neural net representations  mapping function
stored implicitly weights connections neural net  either case 
generalized policy representation facilitates search good policies grouping together
states action required 
      policy selectivity

earl systems selective representations policies  is  ea learns mappings observed states recommended actions  usually eliminating explicit information
concerning less desirable actions  knowledge bad decisions explicitly preserved 
since policies make decisions selected evolutionary algorithm
eventually eliminated population  advantage selective representations attention focused profitable actions only  reducing space requirements
policies 
consider example simple earl operating grid world  population evolves  policies normally converge best actions specific state 
selective pressure achieve high fitness levels  example  population shown
table   converged alleles  actions  states a   a   b   b   d   e   e  
converged state action pairs highly correlated fitness  example  policies
converged action r state b   taking action r state b  achieves much higher
expected return action     vs    table     policies select action
state b  achieve lower fitness scores selected against  simple earl  snapshot population  table    provides implicit estimate corresponding td value
function  table     distribution biased toward profitable state actions
pairs 
   

fievolutionary algorithms reinforcement learning

  
l

   
l

red

r

blue

r
   

green

l

blue

l

     

r
r
   
   

figure     environment incomplete state information  circles represent
states world colors represent agent s sensory input  agent
equally likely start red state green state

    dealing incomplete state information

clearly  favorable condition reinforcement learning occurs agent
observe true state dynamic system interacts  complete state
information available  td methods make ecient use available feedback associating
reward directly individual decisions  real world situations  however  agent s
sensors likely provide partial view may fail disambiguate many
states  consequently  agent often unable completely distinguish current
state  problem termed perceptual aliasing hidden state problem 
case limited sensory information  may useful associate rewards
larger blocks decisions  consider situation figure     agent must
act without complete state information  circles represent specific states world 
colors represent sensor information agent receives within state  square
nodes represent goal states corresponding reward shown inside  state 
agent choice two actions  l r   assume state transitions
deterministic agent equally likely start either state
red green sensor readings 
example  two different states return sensor reading blue 
agent unable distinguish them  moreover  actions blue
state return different rewards  q function applied problem treats sensor
reading blue one observable state  rewards action averaged
blue states  thus  q blue  l  q blue  r  converge         respectively 
since reward q blue  r  higher alternatives observable states red
green  agent s policy q learning choose enter observable state blue
time  final decision policy q learning shown table    table
shows optimal policy respect agent s limited view world 
   

fimoriarty  schultz    grefenstette

value function policy optimal policy
r
r
l
r
r
l
expected reward
   
     

red
green
blue

table    policy expected reward returned converged q function compared
optimal policy given sensory information 
words  policy ects optimal choices agent cannot distinguish two blue
states 
associating values individual observable states  simple td methods
vulnerable hidden state problems  example  ambiguous state information
misleads td method  mistakenly combines rewards two different states
system  confounding information multiple states  td cannot recognize
advantages might associated specific actions specific states  example 
action l top blue state achieves high reward 
contrast  since ea methods associate credit entire policies  rely
net results decision sequences sensor information  may  all 
ambiguous  example  evolutionary algorithm exploits disparity rewards
different blue states evolves policies enter good blue state avoid
bad one  agent remains unable distinguish two blue states  evolutionary algorithm implicitly distinguishes among ambiguous states rewarding policies
avoid bad states 
example  ea method expected evolve optimal policy current
example given existing  ambiguous state information  policies choose action
sequence r l starting red state achieve highest levels fitness 
therefore selected reproduction ea  agents using policies
placed green state select action l  receive lowest fitness score  since
subsequent action  l blue sensors  returns negative reward  thus  many
policies achieve high fitness started red state selected
choose l green state  course many generations  policies must
choose action r green state maximize fitness ensure survival 
confirmed hypotheses empirical tests  q learner using single step updates
table based representation converged values table   every run 
evolutionary algorithm  consistently converged     population optimal policy 
figure    shows average percentage optimal policy population function
time  averaged     independent runs 
thus even simple ea methods earl  appear robust presence
hidden states simple td methods  however  refined sensor information could
still helpful  previous example  although ea policies achieve better average
reward td policy  evolved policy remains unable procure    
   used binary tournament selection     policy population      crossover probability      
mutation rate 

   

fievolutionary algorithms reinforcement learning

   

percentage optimal

  

  

  

  

 
 

  

  

  

  

  
generation

  

  

  

  

   

figure     optimal policy distribution hidden state problem evolutionary
algorithm  graph plots percentage optimal policies population 
averaged     runs 
    rewards two blue states  rewards could realized  however 
agent could separate two blue states  thus  method generates additional
features disambiguate states presents important asset ea methods  kaelbling
et al         describe several promising solutions hidden state problem 
additional features agent s previous decisions observations automatically
generated included agent s sensory information  chrisman        lin   mitchell 
      mccallum        ring         methods effective disambiguating
states td methods initial studies  research required determine
extent similar methods resolve significant hidden state information realistic
applications  would useful develop ways use methods augment sensory
data available ea methods well 

    non stationary environments

agent s environment changes time  rl problem becomes even dicult 
since optimal policy becomes moving target  classic trade off exploration
exploitation becomes even pronounced  techniques encouraging exploration
td based rl include adding exploration bonus estimated value state action
pairs ects long since agent tried action  sutton        
building statistical model agent s uncertainty  dayan   sejnowski        
simple modifications standard evolutionary algorithms offer ability track nonstationary environments  thus provide promising approach rl dicult
cases 
fact evolutionary search based competition within population policies
suggest immediate benefits tracking non stationary environments  extent
population maintains diverse set policies  changes environment bias
   

fimoriarty  schultz    grefenstette

selective pressure favor policies fit current environment 
long environment changes slowly respect time required evaluate
population policies  population able track changing fitness landscape
without alteration algorithm  empirical studies show maintaining
diversity within population may require higher mutation rate usually
adopted stationary environments  cobb   grefenstette        
addition  special mechanisms explored order make eas responsive rapidly changing environments  example   grefenstette        suggests
maintaining random search within restricted portion population  random
population elements analogous immigrants populations uncorrelated
fitness landscapes  maintaining source diversity permits ea respond rapidly
large  sudden changes fitness landscape  keeping randomized portion
population less     population  impact search eciency
stationary environments minimized  general approach easily applied
earl systems 
useful algorithms developed ensure diversity evolving popultions include fitness sharing  goldberg   richardson         crowding  de jong        
local mating  collins   jefferson         goldberg s fitness sharing model  example  similar individuals forced share large portion single fitness value
shared solution point  sharing decreases fitness similar individuals causes
evolution select individuals overpopulated niches 
earl methods employ distributed policy representations achieve diversity automatically well suited adaptation dynamic environments  distributed
representation  individual represents partial solution  complete solutions
built combining individuals  individual solve task own 
evolutionary algorithm search several complementary individuals together
solve task  evolutionary pressures therefore present prevent convergence
population  moriarty miikkulainen        showed inherent diversity specialization sane allow adapt much quickly changes environment
standard  convergent evolutionary algorithms 
finally  learning system detect changes environment  even direct
response possible  anytime learning model  grefenstette   ramsey        
earl system maintains case base policies  indexed values environmental
detectors corresponding environment given policy evolved 
environmental change detected  population policies partially reinitialized 
using previously learned policies selected basis similarity previously
encountered environment current environment  result  environment
changes cyclic  population immediately seeded policies
effect last occurrence current environment  population
policies  approach protected kinds errors detecting environmental
changes  example  even spurious environmental change mistakenly detected 
learning unduly affected  since part current population policies
replaced previously learned policies  zhou        explored similar approach based
lcs 
   

fievolutionary algorithms reinforcement learning

summary  earl systems respond non stationary environments  techniques generic evolutionary algorithms techniques specifically designed rl mind 

   limitations earl
although ea approach rl promising growing list successful applications  as outlined following section   number challenges remain 

    online learning
distinguish two broad approaches reinforcement learning  online learning
oine learning  online learning  agent learns directly experiences
operational environment  example  robot might learn navigate warehouse
actually moving physical environment  two problems using earl
situation  first  likely require large number experiences order
evaluate large population policies  depending quickly agent performs tasks
result environmental feedback  may take unacceptable amount time
run hundreds generations ea evaluates hundreds thousands policies 
second  may dangerous expensive permit agent perform actions
actual operational environment might cause harm environment  yet
likely least policies ea generates bad policies 
objections apply td methods well  example  theoretical results
prove optimality q learning require every state visited infinitely often 
obviously impossible practice  likewise  td methods may explore
undesirable states acceptable value function found 
td earl  practical considerations point toward use oine learning 
rl system performs exploration simulation models environment 
simulation models provide number advantages earl  including ability
perform parallel evaluations policies population simultaneously  grefenstette 
      

    rare states
memory record observed states rewards differs greatly ea td
methods  temporal difference methods normally maintain statistics concerning every stateaction pair  states revisited  new reinforcement combined previous
value  new information thus supplements previous information  information content agent s reinforcement model increases exploration  manner  td
methods sustain knowledge good bad state action pairs 
pointed previously  ea methods normally maintain information good
policies policy components  knowledge bad decisions explicitly preserved  since
policies make decisions selected evolutionary algorithm
eventually eliminated population  example  refer table   
shows implicit statistics population table    note question
   

fimoriarty  schultz    grefenstette

marks states actions converged  since policies population select
alternative action  ea statistics impact actions fitness 
reduction information content within evolving population disadvantage respect states rarely visited  evolutionary algorithm  value
genes real impact fitness individual tends drift random
values  since mutations tend accumulate genes  state rarely encountered 
mutations may freely accumulate gene describes best action state 
result  even evolutionary algorithm learns correct action rare state 
information may eventually lost due mutations  contrast  since table based td
methods permanently record information state action pairs  may
robust learning agent encounter rare state  course  td method
uses function approximator neural network value function 
suffer memory loss concerning rare states  since many updates frequently
occurring states dominate updates rare states 

    proofs optimality

one attractive features td methods q learning algorithm proof
optimality  watkins   dayan         however  practical importance result
limited  since assumptions underlying proof  e g   hidden states  state visited
infinitely often  satisfied realistic applications  current theory evolutionary
algorithms provide similar level optimality proofs restricted classes search spaces
 vose   wright         however  general theoretical tools available
applied realistic rl problems  case  ultimate convergence optimal policy
may less important practice eciently finding reasonable approximation 
pragmatic approach may ask ecient alternative rl algorithms are 
terms number reinforcements received developing policy within
tolerance level optimal policy  model probably approximately correct
 pac  learning  valiant         performance learner measured many
learning experiences  e g   samples supervised learning  required converging
correct hypothesis within specified error bounds  although developed initially
supervised learning  pac approach extended recently td methods
 fiechter        general ea methods  ros         analytic methods
still early stage development  research along lines may one day
provide useful tools understanding theoretical practical advantages alternative
approaches rl  time  experimental studies provide valuable evidence
utility approach 

    examples earl methods

finally  take look significant examples earl approach results
rl problems  rather attempt exhaustive survey  selected four earl
systems representative diverse policies representations outlined section   
samuel represents class single chromosome rule based earl systems  alecsys
example distributed rule based earl method  genitor single chromosome
neural net system  sane distributed neural net system  brief survey
   

fievolutionary algorithms reinforcement learning

provide starting point interested investigating evolutionary approach
reinforcement learning 

    

samuel
samuel  grefenstette et al         earl system combines darwinian lamarckian evolution aspects temporal difference reinforcement learning  samuel

used learn behaviors navigation collision avoidance  tracking  herding  robots autonomous vehicles 
samuel uses single chromosome  rule based representation policies  is 
member population policy represented rule set gene rule
maps state world actions performed  example rule might be 
range            bearing           set turn       strength
    
use high level language rules offers several advantages low level binary
pattern languages typically adopted genetic learning systems  first  makes easier
incorporate existing knowledge  whether acquired experts symbolic learning programs  second  easier transfer knowledge learned human operators  samuel
includes mechanisms allow coevolution multiple behaviors simultaneously 
addition usual genetic operators crossover mutation  samuel uses traditional machine learning techniques form lamarckian operators  samuel keeps
record recent experiences allow operators generalization  specialization 
covering  deletion make informed changes individual genes  rules  based
experiences 
samuel used successfully many reinforcement learning applications 
brie describe three examples learning complex behaviors real robots 
applications samuel  learning performed simulation  ecting fact
initial phases learning  controlling real system expensive
dangerous  learned behaviors tested on line system 
 schultz   grefenstette        schultz        schultz   grefenstette         samuel
used learn collision avoidance local navigation behaviors nomad     mobile
robot  sensors available learning task five sonars  five infrared sensors 
range bearing goal  current speed vehicle  samuel
learned mapping sensors controllable actions   turning rate
translation rate wheels  samuel took human written rule set could reach
goal within limited time without hitting obstacle    percent time 
   generations able obtain      percent success rate 
 schultz   grefenstette         robot learned herd second robot  pasture   task  learning system used range bearing second robot 
heading second robot  range bearing goal  input sensors 
system learned mapping sensors turning rate steering rate 
experiments  success measured percentage times robot could
maneuver second robot goal within limited amount time  second robot
implemented random walk  plus behavior made avoid nearby obstacles 
first robot learned exploit achieve goal moving second robot goal 
   

fimoriarty  schultz    grefenstette

samuel given initial  human designed rule set performance    percent 
    generations able move second robot goal    percent
time 
 grefenstette        samuel ea system combined case based learning
address adaptation problem  approach  called anytime learning  grefenstette  
ramsey         learning agent interacts external environment
internal simulation  anytime learning approach involves two continuously running
interacting modules  execution module learning module  execution
module controls agent s interaction environment includes monitor
dynamically modifies internal simulation model based observations actual agent
environment  learning module continuously tests new strategies agent
simulation model  using genetic algorithm evolve improved strategies 
updates knowledge base used execution module best available results 
whenever simulation model modified due observed change agent
environment  genetic algorithm restarted modified model  learning system
operates indefinitely  execution system uses results learning become
available  work samuel shows ea method particularly well suited
anytime learning  previously learned strategies treated cases  indexed
set conditions learned  new situation encountered 
nearest neighbor algorithm used find similar previously learned cases 
nearest neighbors used re initialize genetic population policies new case 
grefenstette        reports experiments mobile robot learns track another
robot  dynamically adapts policies using anytime learning encounters series
partial system failures  approach blurs line online oine learning 
since online system updated whenever oine learning system develops
improved policy  fact  oine learning system even executed on board
operating mobile robot 

    

alecsys

described previously  alecsys  dorigo   colombetti        distributed rule based
ea supports approach design autonomous systems called behavioral engineering  approach  tasks performed complex autonomous systems
decomposed individual behaviors  learned via learning classifier systems module  shown figure    decomposition performed human designer 
fitness function associated lcs carefully designed ect role
associated component behavior within overall autonomous system  furthermore 
interactions among modules preprogrammed  example  designer may
decide robot learn approach goal except threatening predator
near  case robot evade predator  overall architecture
set behaviors set evasion behavior higher priority
goal seeking behavior  individual lcs modules evolve decision rules
optimally performing subtasks 
alecsys used develop behavioral rules number behaviors
autonomous robots  including complex behavior groups chase feed escape
   

fievolutionary algorithms reinforcement learning

 dorigo   colombetti         approach implemented tested
simulated robots real robots  exploits human design earl
methods optimize system performance  method shows much promise scaling
realistic tasks 

    

genitor

genitor  whitley   kauth        whitley        aggressive  general purpose genetic

algorithm shown effective specialized use reinforcement learning
problems  whitley et al         demonstrated genitor eciently evolve decision
policies represented neural networks using limited reinforcement domain 
genitor relies solely evolutionary algorithm adjust weights neural
networks  solving rl problems  member population genitor represents
neural network sequence connection weights  weights concatenated realvalued chromosome along gene represents crossover probability  crossover
gene determines whether network mutated  randomly perturbed  whether
crossover operation  recombination another network  performed  crossover
gene modified passed offspring based offspring s performance compared
parent  offspring outperforms parent  crossover probability decreased 
otherwise  increased  whitley et al  refer technique adaptive mutation 
tends increase mutation rate populations converge  essentially  method
promotes diversity within population encourage continual exploration solution
space 
genitor uses so called  steady state  genetic algorithm new parents
selected genetic operators applied individual evaluated  approach
contrasts  generational  gas entire population evaluated replaced
generation  steady state ga  policy evaluated retains
fitness value indefinitely  since policies lower fitness likely
replaced  possible fitness based noisy evaluation function may
undesirable uence direction search  case pole balancing rl
application  fitness value depends length time policy maintain
good balance  given randomly chosen initial state  fitness therefore random
variable depends initial state  authors believe noise fitness
function little negative impact learning good policies  perhaps
dicult poor networks obtain good fitness good networks  of
many copies population  survive occasional bad fitness evaluation 
interesting general issue earl needs analysis 
genitor adopts specific modification rl applications  first  representation uses real valued chromosome rather bit string representation weights 
consequently  genitor always recombines policies weight definitions  thus reducing potentially random disruption neural network weights might result crossover
operations occurred middle weight definition  second modification
high mutation rate helps maintain diversity promote rapid exploration
policy space  finally  genitor uses unusually small populations order discourage
different  competing neural network  species  forming within population  whit   

fimoriarty  schultz    grefenstette

ley et al         argue speciation leads competing conventions produces poor
offspring two dissimilar networks recombined 
whitley et al         compare genitor adaptive heuristic critic  anderson 
      ahc   uses td method reinforcement learning  several different
versions common pole balancing benchmark task  genitor found comparable ahc learning rate generalization  one interesting difference
whitley et al  found genitor consistent ahc solving
pole balancing problem failure signals occurs wider pole bounds  make
problem much harder   ahc  preponderance failures appears cause states
overpredict failure  contrast  ea method appears effective finding policies
obtain better overall performance  even success uncommon  difference seems
ea tends ignore cases pole cannot balanced  concentrate successful cases  serves another example advantages associated
search policy space  based overall policy performance  rather paying
much attention value associated individual states 

    

sane

sane  symbiotic  adaptive neuro evolution  system designed ecient method
building artificial neural networks rl domains possible generate
training data normal supervised learning  moriarty   miikkulainen      a        
sane system uses evolutionary algorithm form hidden layer connections
weights neural network  neural network forms direct mapping sensors
actions provides effective generalization state space  sane s method
credit assignment ea  allows apply many problems
reinforcement sparse covers sequence decisions  described previously  sane
uses distributed representation policies 
sane offers two important advantages reinforcement learning normally
present implementations neuro evolution  first  maintains diverse populations 
unlike canonical function optimization ea converge population single solution  sane forms solutions unconverged population  several different types
neurons necessary build effective neural network  inherent evolutionary
pressure develop neurons perform different functions thus maintain several different types individuals within population  diversity allows recombination operators
crossover continue generate new neural structures even prolonged evolution 
feature helps ensure solution space explored eciently throughout
learning process  sane therefore resilient suboptimal convergence
adaptive changes domain 
second feature sane explicitly decomposes search complete solutions search partial solutions  instead searching complete neural networks
once  solutions smaller problems  good neurons  evolved  combined form effective full solution  a neural network   words  sane effectively
performs problem reduction search space neural networks 
sane shown effective several different large scale problems  one problem 
sane evolved neural networks direct focus minimax game tree search  moriarty
   

fievolutionary algorithms reinforcement learning

  miikkulainen         selecting moves evaluated given game
situation  sane guides search away misinformation search tree towards
effective moves  sane tested game tree search othello using
evaluation function former world champion program bill  lee   mahajan        
tested full width minimax search  sane significantly improved play bill 
examining subset board positions 
second application  sane used learn obstacle avoidance behaviors
robot arm  moriarty   miikkulainen      b   approaches learning robot arm
control learn hand eye coordination supervised training methods examples
correct behavior explicitly given  unfortunately domains obstacles
arm must make several intermediate joint rotations reaching target  generating
training examples extremely dicult  reinforcement learning approach  however 
require examples correct behavior learn intermediate movements
general reinforcements  sane implemented form neuro control networks capable
maneuvering oscar   robot arm among obstacles reach random target locations 
given camera based visual infrared sensory input  neural networks learned
effectively combine target reaching obstacle avoidance strategies 
related examples evolutionary methods learning neural net control
systems robotics  reader see  cliff  harvey    husbands        husbands 
harvey    cliff        yamauchi   beer        

    summary
article began suggesting two distinct approaches solving reinforcement learning
problems  one search value function space one search policy space  td
earl examples two complementary approaches  approaches assume
limited knowledge underlying system learn experimenting different policies using reinforcement alter policies  neither approach requires precise
mathematical model domain  may learn direct interactions
operational environment 
unlike td methods  earl methods generally base fitness overall performance
policy  sense  ea methods pay less attention individual decisions td
methods do  first glance  approach appears make less ecient use
information  may fact provide robust path toward learning good policies  especially
situations sensors inadequate observe true state world 
useful view path toward practical rl systems choice ea
td methods  tried highlight strengths evolutionary
approach  shown earl td  complementary approaches 
means mutually exclusive  cited examples successful earl systems
samuel alecsys explicitly incorporate td elements multilevel credit assignment methods  likely many practical applications depend
kinds multi strategy approaches machine learning 
listed number areas need work  particularly theoretical side  rl  would highly desirable better tools predicting
amount experience needed learning agent reaching specified level per   

fimoriarty  schultz    grefenstette

formance  existing proofs optimality q learning ea extremely
limited practical use predicting well either approach perform realistic problems  preliminary results shown tools pac analysis applied
ea td methods  much effort needed direction 
many serious challenges remain scaling reinforcement learning methods realistic applications  pointing shared goals concerns two complementary
approaches  hope motivate collaboration progress field 

references

anderson  c  w          learning control inverted pendulum using neural networks 
ieee control systems magazine           
barto  a  g   sutton  r  s     watkins  c  j  c  h          learning sequential
decision making  gabriel  m     moore  j  w   eds    learning computational
neuroscience  mit press  cambridge  ma 
belew  r  k   mcinerney  j     schraudolph  n  n          evolving networks  using
genetic algorithm connectionist learning  farmer  j  d   langton  c  
rasmussen  s     taylor  c   eds    artificial life ii reading  ma  addison wesley 
chrisman  l          reinforcement learning perceptual aliasing  perceptual
distinctions approach  proceedings tenth national conference artificial
intelligence  pp          san jose  ca 
cliff  d   harvey  i     husbands  p          explorations evolutionary robotics  adaptive
behavior            
cobb  h  g     grefenstette  j  j          genetic algorithms tracking changing environments  proc  fifth international conference genetic algorithms  pp          
collins  r  j     jefferson  d  r          selection massively parallel genetic algorithms 
proceedings fourth international conference genetic algorithms  pp 
        san mateo  ca  morgan kaufmann 
dayan  p     sejnowski  t  j          exploration bonuses dual control  machine
learning               
de jong  k  a          analysis behavior class genetic adaptive systems 
ph d  thesis  university michigan  ann arbor  mi 
dorigo  m     colombetti  m          robot shaping  experiment behavioral engineering  mit press  cambridge  ma 
fiechter  c  n          ecient reinforcement learning  proceedings seventh
annual acm conference computational learning theory  pp         association
computing machinery 
fogel  l  j   owens  a  j     walsh  m  j          artificial intelligence simulated
evolution  wiley publishing  new york 
   

fievolutionary algorithms reinforcement learning

goldberg  d  e          genetic algorithms search  optimization  machine learning  addison wesley  reading  ma 
goldberg  d  e     richardson  j          genetic algorithms sharing multimodal
function optimization  proceedings second international conference genetic algorithms  pp          san mateo  ca  morgan kaufmann 
grefenstette  j  j          optimization control parameters genetic algorithms  ieee
transactions systems  man   cybernetics  smc                 
grefenstette  j  j          incorporating problem specific knowledge genetic algorithms 
davis  l   ed    genetic algorithms simulated annealing  pp        san mateo 
ca  morgan kaufmann 
grefenstette  j  j          credit assignment rule discovery system based genetic
algorithms  machine learning                   
grefenstette  j  j          genetic algorithms changing environments  manner  r  
  manderick  b   eds    parallel problem solving nature     pp          
grefenstette  j  j          robot learning parallel genetic algorithms networked
computers  proceedings      summer computer simulation conference
 scsc       pp          
grefenstette  j  j          genetic learning adaptation autonomous robots  robotics
manufacturing  recent trends research applications  volume    pp      
     asme press  new york 
grefenstette  j  j       a   proportional selection sampling algorithms  handbook
evolutionary computation  chap  c     iop publishing oxford university press 
grefenstette  j  j       b   rank based selection  handbook evolutionary computation  chap  c     iop publishing oxford university press 
grefenstette  j  j     ramsey  c  l          approach anytime learning  proc 
ninth international conference machine learning  pp          san mateo  ca 
morgan kaufmann 
grefenstette  j  j   ramsey  c  l     schultz  a  c          learning sequential decision
rules using simulation models competition  machine learning             
holland  j  h          adaptation natural artificial systems  introductory
analysis applications biology  control artificial intelligence  university
michigan press  ann arbor  mi 
holland  j  h          escaping brittleness  possibilities general purpose learning
algorithms applied parallel rule based systems  machine learning  artificial
intelligence approach  vol     morgan kaufmann  los altos  ca 
   

fimoriarty  schultz    grefenstette

holland  j  h          genetic algorithms classifier systems  foundations future
directions  proceedings second international conference genetic algorithms  pp        hillsdale  new jersey 
holland  j  h     reitman  j  s          cognitive systems based adaptive algorithms 
pattern directed inference systems  academic press  new york 
husbands  p   harvey  i     cliff  d          circle round  state space attractors
evolved sighted robots  robot  autonomous systems             
kaelbling  l  p   littman  m  l     moore  a  w          reinforcement learning  survey 
journal artificial intelligence research             
koza  j  r          genetic programming  programming computers means
natural selection  mit press  cambridge  ma 
lee  k  f     mahajan  s          development world class othello program 
artificial intelligence            
lin  l  j     mitchell  t  m          memory approaches reinforcement learning nonmarkovian domains  tech  rep  cmu cs         carnegie mellon university  school
computer science 
mccallum  a  k          reinforcement learning selective perception hidden
state  ph d  thesis  university rochester 
moriarty  d  e     miikkulainen  r          evolving neural networks focus minimax
search  proceedings twelfth national conference artificial intelligence
 aaai      pp            seattle  wa  mit press 
moriarty  d  e     miikkulainen  r       a   ecient reinforcement learning
symbiotic evolution  machine learning            
moriarty  d  e     miikkulainen  r       b   evolving obstacle avoidance behavior
robot arm  animals animats  proceedings fourth international
conference simulation adaptive behavior  sab      pp          cape cod 
ma 
moriarty  d  e     miikkulainen  r          forming neural networks ecient
adaptive co evolution  evolutionary computation                 
potter  m  a          design analysis computational model cooperative
coevolution  ph d  thesis  george mason university 
potter  m  a     de jong  k  a          evolving neural networks collaborative
species  proceedings      summer computer simulation conference ottawa 
canada 
potter  m  a   de jong  k  a     grefenstette  j          coevolutionary approach
learning sequential decision rules  eshelman  l   ed    proceedings sixth
international conference genetic algorithms pittsburgh  pa 
   

fievolutionary algorithms reinforcement learning

rechenberg  i          cybernetic solution path experimental problem  library
translation       royal aircraft establishment  farnborough  hants  aug       
ring  m  b          continual learning reinforcement environments  ph d  thesis 
university texas austin 
ros  j  p          probably approximately correct  pac  learning analysis  handbook
evolutionary computation  chap  b     iop publishing oxford university press 
schaffer  j  d   caruana  r  a   eshelman  l  j     das  r          study control
parameters affecting online performance genetic algorithms function optimization  proceedings third international conference genetic algorithms 
pp         morgan kaufmann 
schaffer  j  d     grefenstette  j  j          multi objective learning via genetic algorithms 
proceedings ninth international joint conference artificial intelligence 
pp           morgan kaufmann 
schultz  a  c          learning robot behaviors using genetic algorithms  intelligent
automation soft computing  trends research  development  applications 
pp           tsi press  albuquerque 
schultz  a  c     grefenstette  j  j          using genetic algorithm learn behaviors
autonomous vehicles  proceedings aiaa guidance  navigation  control
conference hilton head  sc 
schultz  a  c     grefenstette  j  j          robo shepherd  learning complex robotic behaviors  robotics manufacturing  recent trends research applications 
volume    pp           asme press  new york 
smith  s  f          flexible learning problem solving heuristics adaptive search 
proceedings eighth international joint conference artificial intelligence 
pp           morgan kaufmann 
sutton  r          integrated architectures learning  planning  reacting based
approximate dynamic programming  machine learning  proceedings seventh
international conference  pp          
sutton  r  s          learning predict methods temporal differences  machine
learning          
sutton  r  s     barto  a          reinforcement learning  introduction  mit press 
cambridge  ma 
valiant  l  g          theory learnable  communications acm           
     
vose  m  d     wright  a  h          simple genetic algorithms linear fitness  evolutionary computation             
   

fimoriarty  schultz    grefenstette

watkins  c  j  c  h          learning delayed rewards  ph d  thesis  university
cambridge  england 
watkins  c  j  c  h     dayan  p          q learning  machine learning                 
whitley  d          genitor algorithm selective pressure  proceedings
third international conference genetic algorithms  pp          san mateo  ca 
morgan kaufman 
whitley  d     kauth  j          genitor  different genetic algorithm  proceedings
rocky mountain conference artificial intelligence  pp          denver  co 
whitley  d   dominic  s   das  r     anderson  c  w          genetic reinforcement
learning neurocontrol problems  machine learning              
wilson  s  w          zcs  zeroth level classifier system  evolutionary computation 
            
yamauchi  b  m     beer  r  d          sequential behavior learning evolved
dynamical neural networks  adaptive behavior             
zhou  h          csm  computational model cumulative learning  machine learning 
               

   


