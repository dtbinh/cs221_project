journal of artificial intelligence research                  

submitted       published      

committee based sample selection
for probabilistic classifiers
shlomo argamon engelson

department of computer science
jerusalem college of technology  machon lev
p o b       
jerusalem        israel

argamon mail jct ac il

ido dagan

department of mathematics and computer science
bar ilan university
      ramat gan  israel

dagan cs biu ac il

abstract
in many real world learning tasks it is expensive to acquire a sucient number of labeled
examples for training  this paper investigates methods for reducing annotation cost by
sample selection  in this approach  during training the learning program examines many
unlabeled examples and selects for labeling only those that are most informative at each
stage  this avoids redundantly labeling examples that contribute little new information 
our work follows on previous research on query by committee  and extends the
committee based paradigm to the context of probabilistic classification  we describe a
family of empirical methods for committee based sample selection in probabilistic classification models  which evaluate the informativeness of an example by measuring the degree
of disagreement between several model variants  these variants  the committee  are drawn
randomly from a probability distribution conditioned by the training set labeled so far 
the method was applied to the real world natural language processing task of stochastic part of speech tagging  we find that all variants of the method achieve a significant
reduction in annotation cost  although their computational eciency differs  in particular 
the simplest variant  a two member committee with no parameters to tune  gives excellent
results  we also show that sample selection yields a significant reduction in the size of the
model used by the tagger 

   introduction
algorithms for supervised concept learning build classifiers for a concept based on a given
set of labeled examples  for many real world concept learning tasks  however  acquiring
such labeled training examples is expensive  hence  our objective is to develop automated
methods that reduce training cost within the framework of active learning  in which the
learner has some control over the choice of examples which will be labeled and used for
training 
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fiargamon   dagan

there are two main types of active learning  the first uses membership queries  in which
the learner constructs examples and asks a teacher to label them  angluin        mackay 
    b  plutowski   white         while this approach provides proven computational
advantages  angluin         it is not always applicable since it is not always possible to
construct meaningful and informative unlabeled examples for training  this diculty may
be overcome when a large set of unlabeled training data is available  in this case the second
type of active learning  sample selection  can often be applied  the learner examines many
unlabeled examples  and selects only the most informative ones for learning  seung  opper 
  sompolinsky        freund  seung  shamir    tishby        cohn  atlas    ladner 
      lewis   catlett        lewis   gale        
in this paper  we address the problem of sample selection for training a probabilistic
classifier  classification in this framework is performed by a probability based model which 
given an input example  assigns a score to each possible classification and selects that with
the highest score 
our research follows theoretical work on sample selection in the query by committee
 qbc  paradigm  seung et al         freund et al          we propose a novel empirical
scheme for applying the qbc paradigm to probabilistic classification models  allowing label
noise   which were not addressed in the original qbc framework  see section       in this
committee based selection scheme  the learner receives a stream of unlabeled examples as
input and decides for each of them whether to ask for its label or not  to that end  the
learner constructs a  committee  of  two or more  classifiers based on the statistics of the
current training set  each committee member then classifies the candidate example  and the
learner measures the degree of disagreement among the committee members  the example
is selected for labeling depending on this degree of disagreement  according to some selection
protocol 
in previous work  dagan   engelson        engelson   dagan      b  we presented
a particular selection protocol for probabilistic concepts  this paper extends our previous
work mainly by generalizing the selection scheme and by comparing a variety of different
selection protocols  a preliminary version appeared as engelson   dagan      a  

    application to natural language processing
much of the early work in sample selection has either been theoretical in nature  or has
been tested on toy problems  we  however  are motivated by complex  real world problems
in the area of statistical natural language and text processing  our work here addresses
the task of part of speech tagging  a core task for statistical natural language processing
 nlp   other work on sample selection for natural language tasks has mainly focused on
text categorization problems  such as the works of lewis and catlett         liere and
tadepalli         and mccallum and nigam        
in statistical nlp  probabilistic classifiers are often used to select a preferred analysis
of the linguistic structure of a text  such as its syntactic structure  black  jelinek  lafferty 
magerman  mercer    roukos         word categories  church         or word senses  gale 
   

ficommittee based sample selection for probabilistic classifiers

church    yarowsky         the parameters of such a classification model are estimated
from a training corpus  a collection of text  
in the common case of supervised training  the learner uses a corpus in which each
sentence is manually annotated with the correct analysis  manual annotation is typically
very expensive  as a consequence  few large annotated corpora exist  mainly for the english language  covering only a few genres of text  this situation makes it dicult to apply
supervised learning methods to languages other than english  or to adapt systems to different genres of text  furthermore  it is infeasible in many cases to develop new supervised
methods that require annotations different from those which are currently available 
in some cases  manual annotation can be avoided altogether  using self organized methods  such as was shown for part of speech tagging of english by kupiec         even in
kupiec s tagger  though  manual  and somewhat unprincipled  biasing of the initial model
was necessary to achieve satisfactory convergence  elworthy        and merialdo       
have investigated the effect of self converging re estimation for part of speech tagging and
found that some initial manual training is needed  more generally  the more supervised
training is provided  the better the results  in fact  fully unsupervised methods are not
applicable for many nlp tasks  and perhaps not even for part of speech tagging in some
languages  sample selection is an appropriate way to reduce the cost of annotating corpora 
as it is easy to obtain large volumes of raw text from which smaller subsets will be selected
for annotation 
we have applied committee based selection to learning hidden markov models  hmms 
for part of speech tagging of english sentences  part of speech tagging is the task of labeling
each word in the sentence with its appropriate part of speech  for example  labeling an
occurrence of the word  hand  as a noun or a verb   this task is non trivial since determining
a word s part of speech depends on its linguistic context  hmms have been used extensively
for this task  e g   church        merialdo         in most cases trained from corpora
which have been manually annotated with the correct part of speech for each word  our
experiments on part of speech tagging  described in section      show that using committeebased selection results in substantially faster learning rates  enabling the learner to achieve
a given level of accuracy using far fewer training examples than by sequential training using
all of the text 

   background
the objective of sample selection is to select those examples which will be most informative in the future  how might we determine the informativeness of an example  one
approach is to derive an explicit measure of the expected amount of information gained
by using the example  cohn  ghahramani    jordan        mackay      b      a   for
example  mackay      b  assesses the informativeness of an example  in a neural network
learning task  by the expected decrease in the overall variance of the model s prediction 
after training on the example  explicit measures can be appealing  since they attempt to
give a precise characterization of the information content of an example  also  for membership querying  an explicit formulation of information content sometimes enables finding
   

fiargamon   dagan

the most informative examples analytically  saving the cost of searching the example space 
the use of explicit methods may be limited  however  since explicit measures are generally
 a  model specific   b  complex  often requiring various approximations to be practical  and
 c  depend on the accuracy of the current hypothesis at any given step 
the alternative to measuring the informativeness of an example explicitly is to measure
it implicitly  by quantifying the amount of uncertainty in the classification of the example
given the current training data  the informativeness of an example is evaluated with respect
to models derived from the training data at each stage of learning  one approach is to use
a single model based on the training data seen so far  this approach is taken by lewis
and gale         for training a binary classifier  they select for training those examples
whose classification probability is closest to      i e  those examples for which the current
best model is most uncertain 
in order to better evaluate classification uncertainty with respect to the entire space of
possible models  one may instead measure the classification disagreement among a sample
set of possible models  a committee   using the entire model space enables measuring the
degree to which the training entails a single  best  classification for the example  on the
other hand  referring to a single model measures only the degree to which that model is
certain of its classification  for example  a classifier with sucient training for predicting ips of a coin with heads probability      will always predict heads  and hence will
make mistakes     of the time  however  although this classifier is quite uncertain of the
correctness of its classification  additional training will not improve its accuracy 
there are two main approaches for generating a committee in order to evaluate example
uncertainty  the version space approach and the random sampling approach  the version
space approach  pursued by cohn et al         seeks to choose committee members on
the border of the space of all the models allowed by the training data  the version space 
mitchell         thus models are chosen for the committee which are as far from each other
as possible while being consistent with the training data  this ensures that the models will
disagree on an example whenever training on the example would restrict the version space 
the version space approach can be dicult to apply since finding models on the edge
of the version space is non trivial in general  furthermore  the approach is not directly
applicable in the case of probabilistic classification models  where almost all models are
possible  though not equally probable  given the training  the alternative is random sampling  as exemplified by the query by committee algorithm  seung et al         freund
et al          which inspired this paper  in this approach  models are sampled randomly
from the set of all possible models  according to the probability of the models given the
training data  our work applies the random sampling approach to probabilistic classifiers
by computing an approximation to the posterior model distribution given the training data 
and generating committee members from that distribution  mccallum and nigam       
use a similar approach for sample selection on text categorization using a naive bayes classifier  the primary difference is that they skew example selection using density weighted
sampling  such that documents that are similar to many other documents in the training
set will be selected for labeling with a higher probability 

   

ficommittee based sample selection for probabilistic classifiers

matan        presents two other methods for random sampling  in the first method  he
trains committee members on different subsets of the training data  in his second method 
for neural network models  matan generates committee members by backpropagation training using different initial weights in the networks so that they reach different local minima 
a similar approach is taken by liere and tadepalli         who applied a committee based
selection approach to text categorization using the winnow learning algorithm  littlestone 
      which learns linear classifiers  they represented the model space by a set of classifiers  the model set   each classifier in the model set learns independently from labeled
examples  having been initialized with a different initial hypothesis  thus at any point the
set gives a selection of the possible hypotheses given the training data   labeling decisions
are performed based on two models chosen at random from the model set  if the models
disagree on a document s class  the document s label is requested  and all models in the
space are updated 

    query by committee
as mentioned above  this paper follows theoretical work on sample selection in the query
by committee  qbc  paradigm  seung et al         freund et al          this method was
proposed for learning binary  non probabilistic  concepts in cases where there exists a prior
probability distribution measure over the concept class  qbc selects  informative  training
examples out of a stream of unlabeled examples  when an example is selected the learner
queries the teacher for its correct label and adds it to the training set  as examples are
selected for training  they restrict the set of consistent concepts  i e  the set of concepts that
label all the training examples correctly  the version space  
a simple version of qbc  which was analyzed by freund et al          see also the
summary in freund         uses the following selection algorithm 
   draw an unlabeled input example at random from the probability distribution of the example space 
   select at random two hypotheses according to the prior probability distribution of the concept class  restricted to the set of consistent concepts 
   select the example for training if the two hypotheses disagree on its classification 
freund et al  prove that  under some assumptions  this algorithm achieves an exponential
reduction in the number of labeled examples required to achieve a desired classification
accuracy  compared with random selection of training examples  this speedup is achieved
because the algorithm tends to select examples that split the version space into two parts
of similar size  one of these parts is eliminated from the version space after the example
and its correct label are added to the training set 

    selection for probabilistic classifiers
we address here the problem of sample selection for training a probabilistic classifier  classification in this framework is performed by a probabilistic model which  given an input
   

fiargamon   dagan

example  assigns a probability  or a probability based score  to each possible classification
and selects the best classification  probabilistic classifiers do not fall within the framework
addressed in the theoretical qbc work  training a probabilistic classifier involves estimating the values of model parameters which determine a probability estimate for each possible
classification of an example  while we expect that in most cases the optimal classifier will
assign the highest probability to the correct class  this is not guaranteed to always occur 
accordingly  the notion of a consistent hypothesis is generally not applicable to probabilistic
classifiers  thus  the posterior distribution over classifiers given the training data cannot
be defined as the restriction of the prior to the set of consistent hypotheses  rather  within
a bayesian framework  the posterior distribution is defined by the statistics of the training
set  assigning higher probability to those classifiers which are more likely given the statistics 
we now discuss some desired properties of examples that are selected for training  generally speaking  a training example contributes data to several statistics  which in turn
determine the estimates of several parameter values  an informative example is therefore
one whose contribution to the statistics leads to a useful improvement of parameter estimates  assuming the existence of an optimal classification model for the given concept
 such as a maximum likelihood model   we identify three properties of parameters for which
acquiring additional statistics is most beneficial 
   the current estimate of the parameter is uncertain due to insucient statistics in
the training set  an uncertain estimate is likely to be far from the true value of the
parameter and can cause incorrect classification  additional statistics would bring the
estimate closer to the true value 
   classification is sensitive to changes in the current estimate of the parameter  otherwise  acquiring additional statistics is unlikely to affect classification and is therefore
not beneficial 
   the parameter takes part in calculating class probabilities for a large proportion of
examples  parameters that are only relevant for classifying few examples  as determined by the probability distribution of the input examples  have low utility for future
estimation 
the committee based selection scheme  as we describe further below  tends to select
examples that affect parameters with the above three properties  property   is addressed by
randomly picking parameter values for committee members from the posterior distribution
of parameter estimates  given the current statistics   when the statistics for a parameter
are insucient the variance of the posterior distribution of the estimates is large  and hence
there will be large differences in the values of the parameter picked for different committee
members  note that property   is not addressed when uncertainty in classification is only
judged relative to a single model  as in  e g   lewis   gale         such an approach
captures uncertainty with respect to given parameter values  in the sense of property    but
it does not model uncertainty about the choice of these values in the first place  the use of
a single model is criticized by cohn et al         
property   is addressed by selecting examples for which committee members highly disagree in classification  thus  the algorithm tends to acquire statistics where uncertainty in
   

ficommittee based sample selection for probabilistic classifiers

parameter estimates entails uncertainty in actual classification  this is analogous to splitting
the version space in qbc   finally  property   is addressed by independently examining
input examples which are drawn from the input distribution  in this way  we implicitly
model the expected utility of the statistics in classifying future examples 

    paper outline
the following section defines the basic concepts and notation that we will use in the rest
of the paper  section   presents a general selection scheme along with variant selection
algorithms  the next two sections demonstrate the effectiveness of the sample selection
scheme  section   presents results on an artificial  colorful coin ipper  problem  providing
a simple illustration of the operation of the proposed system  section   presents results for
the task of stochastic part of speech tagging  demonstrating the usefulness of committeebased sample selection in the real world 

   definitions
the concern of this paper is how to minimize the number of labeled examples needed to
learn a classifier which accurately classifies input examples e by classes c   c   where c
is a known set of possible classes  during learning  a stream of unlabeled examples is
supplied for free  with examples drawn from an unknown probability distribution  there is
a cost  however  for the learning algorithm to obtain the true label of any given example 
our objective is to reduce this cost as much as possible  while still learning an accurate
classifier 
we address the specific case of probabilistic classifiers  where classification is done on
the basis of a score function  fm  c  e   which assigns a score to each possible class of an
input example  the classifier assigns the input example to the class with the highest score 
fm is determined by a probabilistic model m   in many applications  fm is the conditional
probability function  pm  cje   specifying the probability of each class given the example 
alternatively  other score functions that denote the likelihood of the class may be used
 such as an odds ratio   the particular type of model used for classification determines the
specific form of the score  as a function of features of the example 
a probabilistic model m   and thus the score function fm   is defined by a set of parameters  fffi g  giving the probabilities of various possible events  for example  a model
for part of speech tagging contains parameters such as the probability of a particular word
being a verb or a noun  during training  the values of the parameters are estimated from
a set of statistics  s   extracted from a training set of labeled examples  a particular model
is denoted by m   faig  where each ai is a specific value for the corresponding ffi  

   committee based sample selection
this section describes the algorithms which apply the committee based approach for evaluating classification uncertainty of each input example  the learning algorithm evaluates
   

fiargamon   dagan

an example by giving it to a committee containing several versions  or copies  of the classifier  all  consistent  with the training data seen so far  the greater the agreement of the
committee members on the classification of the example  the greater our certainty in its
classification  this is because if the training data entails a specific classification with high
certainty  then most  in a probabilistic sense  versions of the classifier consistent with the
data will produce that classification  an example is selected for labeling  therefore  when
the committee members disagree on its appropriate classification 

    generating a committee
to generate a committee with k members  we randomly choose k models according to
the posterior distribution p  m js   of possible models given the current training statistics 
how this sampling is performed depends on the form of this distribution  which in turn
depends on the form of the model  thus when implementing committee based selection for
a particular problem  an appropriate sampling procedure must be devised  as an illustration
of committee generation  the rest of this section describes the sampling process for models
consisting of independent binomial parameters or multinomial parameter groups 
consider first a model containing a single binomial parameter ff  the probability of a
success   with estimated value a  the statistics s for such a model are given by n   the
number of trials  and x  the number of successes in those trials 
given n and x  the  best  model parameter value can be estimated by any of several
estimation methods  for example  the maximum likelihood estimate  mle  for ff is a   nx  
giving the model m   fff   nx g  when generating a committee of models  however  we are
not interested in the  best  model  but rather in sampling the distribution of models given
the statistics  for our example  we need to sample the posterior density of estimates for
ff  namely p ff   ajs    in the binomial case  this density is the beta distribution  johnson 
       sampling this distribution yields a set of estimates scattered around nx  assuming a
uniform prior   where the variance of these estimates gets smaller as n gets larger  each
such estimate participates in a different member of the committee  thus  the more statistics
there are for estimating the parameter  the closer are the estimates used by different models
in the committee 
now consider a model consisting of a single group of interdependent parameters defining a multinomial  in this case  the posterior is a dirichlet distribution  johnson        
committee members are generated by sampling from this joint distribution  giving values
for all the model parameters 
for models consisting of a set of independent binomials or multinomials  sampling
p  m js   amounts to sampling each of the parameters independently  for models with
more complex dependencies among parameters sampling may be more dicult  in practice 
though  it may be possible to make enough independence assumptions to make sampling
feasible 
sampling the posterior generates committee members whose parameter estimates differ
most when they are based on low training counts and tend to agree when based on high
counts  if the classification of an example relies on parameters whose estimates by com   

ficommittee based sample selection for probabilistic classifiers

for each unlabeled input example e 
   draw   models randomly from p  m js    where s are statistics acquired
from previously labeled examples 
   classify e by each model  giving classifications c  and c  
   if c     c   select e for annotation 
   if e is selected  get its correct label and update s accordingly 
figure    the two member sequential selection algorithm 
mittee members differ  and these differences affect classification  then the example would
be selected for learning  this leads to selecting examples which contribute statistics to
currently unreliable estimates that also have an effect on classification  thus we address
properties   and   discussed in section     

    selection algorithms
within the committee based paradigm there exist different methods for selecting informative examples  previous research in sample selection has used either sequential selection
 seung et al         freund et al         dagan   engelson         or batch selection  lewis
  catlett        lewis   gale         we present here general algorithms for both sequential and batch committee based selection  in all cases  we assume that before any
selection algorithm is applied a small amount of labeled initial training is supplied  in order
to initialize the training statistics 
      two member sequential selection

sequential selection examines unlabeled examples as they are supplied  one by one  and
estimates their expected information gain  those examples determined to be suciently
informative are selected for training  most simply  we can choose a committee of size two
from the posterior distribution of the models  and select an example when the two models
disagree on its classification  this gives the parameter free  two member sequential selection
algorithm  shown in figure    this basic algorithm has no parameters 
      general sequential selection

a more general selection algorithm results from 

 using a larger number k of committee members  in order to evaluate example informativeness more precisely 

 more refined example selection criteria  and
   

fiargamon   dagan

for each unlabeled input example e 
   draw k models fmi g randomly from p  m js    possibly using a temperature t  
   classify e by each model mi giving classifications fci g 
   measure the disagreement d e  based on fci g 
   decide whether or not to select e for annotation  based on the value
of d e  
   if e is selected  get its correct label and update s accordingly 
figure    the general sequential selection algorithm 

 tuning the frequency of selection by replacing p  m js   with a distribution with a
different variance  this has the effect of adjusting the variability among the committee
members chosen  in many cases  eg   hmms  as described in section   below  this
can be implemented by a parameter t  called the temperature   used as a multiplier
of the variance of the posterior parameter distribution 

this gives the general sequential selection algorithm  shown in figure   
it is easy to see that two member sequential selection is a special case of general sequential selection  in order to instantiate the general algorithm for larger committees  we need
to fix a general measure d e  for disagreement  step     and a decision method for selecting
examples according to this disagreement  step    
we measure disagreement by the entropy of the distribution of classifications  voted for 
by the committee members  this vote entropy is a natural measure for quantifying the
uniformity of classes assigned to an example by the different committee members    we
further normalize this entropy by a bound on its maximum possible value  log min k  jcj   
giving a value between   and    denoting the number of committee members assigning a
class c for input example e by v  c  e   the normalized vote entropy is 
x v  c  e  v  c  e 
 
d e     
log min k  jc j  c k log k
normalized vote entropy has the value one when all committee members disagree  and the
value zero when they all agree  taking on intermediate values in cases with partial agreement 
we consider here two alternatives for the selection criterion  step     the simplest is
thresholded selection  in which an example is selected for annotation if its normalized vote
entropy exceeds some threshold   another alternative is randomized selection  in which
an example is selected for annotation based on the ip of a coin biased according to the
vote entropy a higher vote entropy corresponding to a higher probability of selection  we
   mccallum and nigam        have suggested an alternative measure  the kl divergence to the mean
 pereira  tishby    lee         it is not clear whether that measure has an advantage over the simpler
entropy function 

   

ficommittee based sample selection for probabilistic classifiers

for a batch b of n examples 
   for each example e in b  
 a  draw k models randomly from p  m js   
 b  classify e by each model  giving classifications fcig 
 c  measure the disagreement d e  for e based on fcig 
   select for annotation the m examples with the highest d e  
   update s by the statistics of the selected examples 
figure    the batch selection algorithm 
use a simple model where the selection probability is a linear function of normalized vote
entropy  p  e    gd e   calling g the entropy gain   
      batch selection

an alternative to sequential selection is batch selection  rather than evaluating examples
individually for their informativeness a large batch of n examples is examined  and the m
best are selected for annotation  the batch selection algorithm is given in figure   
this procedure is repeated sequentially for successive batches of n examples  returning
to the start of the corpus at the end  if n is equal to the size of the corpus  batch selection
selects the m globally best examples in the corpus at each stage  as in lewis   catlett        
batch selection has certain theoretical drawbacks  freund et al          particularly that
it does not consider the distribution of input examples  however  as shown by mccallum
and nigam         the distribution of the input examples can be modeled and taken into
account during selection  they do this by combining their disagreement measure with a
measure of example density  which produces good results with batch selection  this work is
discussed in more detail below in section       a separate diculty with batch selection is
that it has the computational disadvantage that it must look at a large number of examples
before selecting any  as the batch size is decreased  batch selection behaves similarly to
sequential selection 

   example  colorful coin flipper
as an illustrative example of a learning task  we define a colorful coin ipper  ccf  as a
machine which contains an infinite number of coins of various colors  the machine chooses
coins to ip  one by one  where each color of coin has a fixed  unknown  probability of being
chosen  when a coin is ipped  it comes up heads with probability determined solely by its
color  before it ips a coin  the machine tells the learner which color of coin it has chosen to
   the selection method used in  dagan   engelson        is randomized sequential selection using this
linear selection probability model  with parameters k  t and g 

   

fiargamon   dagan

ip  in order to know the outcome of the ip  however  the learner must pay the machine 
in training  the learner may choose the colors of coins whose outcomes it will examine  the
objective of selective sampling is to choose so as to minimize the training cost  number of
ips examined  required to attain a given prediction accuracy for ip outcomes 
for the case of the ccf  an example e is a coin ip  characterized by its color  and its
class c is either heads or tails  note that we do not require that ips of a given color always
have the same class  therefore the best that we can hope to do is classify according to the
most likely class for each color 
for a ccf  we can define a model whose parameters are the heads probabilities for the
coins of each particular color  so  for a ccf with three colors  one possible model would be
m   fr        g         b      g  giving the probabilities of heads for red  green  and blue
coins  respectively  a coin of a given color will then be classified  heads  if its score  given
directly by the appropriate model parameter  is        and  tails  otherwise 

    implementation of sample selection
training a model for a ccf amounts to counting the proportion of heads for each color 
providing estimates of heads probabilities  in complete training every coin ip in the training
sequence is examined and added to the counts  in sample selection we seek to label and
count only training ips of those colors for which additional counts are likely to improve
the model s accuracy  useful colors to train on are either those for which few training
examples have so far been seen  or those whose current probability estimates are near    
 cf  section      
recall that for sample selection we build a committee by sampling models from p  m js   
in the case of a ccf  all of the model parameters ffi  the heads probabilities for different
colors  are independent  and so sampling from p  m js   amounts to sampling independently
for each of the parameters 
while the form of the posterior distribution p  ffi   ai js   is given by the beta distribution  we found it technically easier to use a normal approximation  which was found
satisfactory in practice  let ni be the number of coin ips of color i seen so far  and ni be
the number of those ips which came up heads  we approximate p  ffi   ai js   as a truncated normal distribution  restricted to         with estimated mean i   nn and variance
i       n     this approximation made it easy to also incorporate a  temperature  parameter t  as in section         which is used as a multiplier for the variance estimate i    thus 
we actually approximate p  ffi   aijs   as a truncated normal distribution with mean i and
variance i  t  sampling from this distribution was done using the algorithm given by press 
flannery  teukolsky  and vetterling        for sampling from a normal distribution 
i

i

i

i

i

    vote entropy
the ccf is useful to illustrate the importance of determining classification uncertainty
using the vote entropy over a committee of models rather than using the entropy of the
class distribution given by a single model  as discussed in section     consider a ccf with
   

ficommittee based sample selection for probabilistic classifiers

model
 
 
 
 

red
      heads 
      heads 
      heads 
      heads 

blue
      tail 
      tail 
      heads 
      heads 
 a 

green
      heads 
      tail 
      tail 
      tail 

color d e  acde
red
        
blue
        
green          
 b 

figure     a  a committee of ccf models   b  the resultant vote entropy for each color 
ccf results for    colors

ccf results for     colors

   
ptm
committee based sampling
complete training

ptm
committee based sampling
complete training

   

   

selected training

selected training

   

   

   

  
  

 
   

 
    

   

    
desired accuracy

   

    

   

   

 a 

    

   

    
desired accuracy

   

    

   

 b 

figure    ccf results for random ccfs with    and     different coin colors  results
are averaged over   different such ccfs  comparing complete training with two
member sample selection  the figures show the amount of training required for
a desired classification accuracy   a  for    colors   b  for     colors 

three coin colors  red  blue  and green  suppose the   member committee in figure   a  is
generated  from this committee  we estimate for each color its vote entropy d e   as well
as the average of the class distribution entropies given by each of the individual models
 acde   given in figure   b  
if we compare the entropies of red and blue  for example  we see that their entropies
over the expected class probability distribution are both quite high  since both estimated
class probabilities are near       however  when we consider their vote entropies  over the
assigned classes   blue has maximal entropy  since the range of possible models straddles a
class boundary        while red has minimal entropy  since the range of possible models does
not straddle a class boundary  that is  it is quite certain that the optimal classification
for red is  heads   we also see how green has a higher vote entropy than red  although its
average class distribution entropy is lower  this shows the importance of using vote entropy
for selection 
   

fiargamon   dagan

ccf frequency of selection
 
   color ccf
    color ccf

   
   

selection frequency

   
   
   
   
   
   
   
 
 

  

  

  

  

   
   
selected training

   

   

   

   

figure    frequency of selection vs  amount of selected training for ccfs with    and    
colors  averaged for   different ccfs 

    results
we simulated sample selection for the simple ccf model in order to illustrate some of its
properties  in the following  we generated random ccfs with a fixed number of coins by
randomly generating occurrence probabilities and heads probabilities for each coin color 
we then generated learning curves for complete training  on all input examples  and for two
member sample selection  using    coin ips for initial training  both complete training
and sample selection were run on the same coin ip sequences  accuracy was measured
by computing the expected accuracy  assuming an infinite test set  of the mle model
generated by the selected training  the figures also show the accuracy for the theoretical
perfectly trained model  ptm  which knows all of the parameters perfectly 
figure   summarizes the average results for   comparison runs of complete vs  sample
selection for ccfs of    and     coins  in figures   a  and  b   we compare the amount
of selected training required to reach a given desired accuracy  we see in both cases that
as soon as sample selection starts operating  its eciency is higher than complete training 
and the gap increases in size as greater accuracy is desired  in figure    we examine the
cumulative frequency of selection  ratio between the number of selected examples and the
total number of examples seen  as learning progresses  we see here an exponential decrease
in the frequency of selection  as expected in the case of qbc for non probabilistic models
 analyzed in seung et al         freund et al         

   application  stochastic part of speech tagging
we have applied committee based selection to the real world task of learning hidden markov
models  hmms  for part of speech tagging of english sentences  part of speech tagging is
the task of labeling each word in the sentence with its appropriate part of speech  for
example  labeling an occurrence of the word  hand  as a noun or a verb   this task is nontrivial since determining a word s part of speech depends on its linguistic context  hmms
   

ficommittee based sample selection for probabilistic classifiers

have been used extensively for this task  e g   church        merialdo         in most cases
trained from corpora which have been manually annotated with the correct part of speech
for each word 

    hmms and part of speech tagging
a first order hidden markov model  hmm  is a probabilistic finite state string generator
 rabiner         defined as a set of states q   fqi g  a set of output symbols   a set of
transition probabilities p  qi  qj   of each possible transition between states qi and qj   a set of
output probabilities p  ajq  for each state q to output each symbol a     and a distinguished
start state q    the probability of a string s   a a     an being generated by an hmm is
given by
 
n
x
y
p  qi    qi  p  aijqi    
q  qn  qn i  

the sum  for all paths through the hmm  of the joint probability that the path was traversed
and that it output the given string  in contrast with ordinary markov models  in an hmm
it is not known which sequence of states generated a given string  hence the term  hidden   
hmms have been used widely in speech and language processing  in particular  an hmm
can be used to provide a classification model for sequence elements  if we need to classify
each element in a sequence  we encode each possible class by a state in an hmm  training
the hmm amounts to estimating the values of the transition and output probabilities 
then  given a sequence for classification  we assume that it was generated by the hmm and
compute the most likely state sequence for the string  using the viterbi algorithm   viterbi 
      
an hmm can be used for part of speech tagging of words by encoding each possible partof speech tag  t  noun  verb  adjective  etc    as an hmm state  the output probabilities 
p  wjt   give the probability of producing each word w in the language conditioned on the
current tag t  the transition probabilities  p  t  t    give the probability of generating
a word with the tag t  given that the previous word s tag is t    this constitutes a weak
syntactic model of the language  this model is often termed the tag bigram model   
given an input word sequence w   w     wn   we seek the most likely tag sequence
t   t     tn  
 
arg maxt p  t jw     arg maxt pp t w
 w  
  arg maxt p  t  w  
   an alternative classification scheme is to compute the most likely state for each individual element
 instead of the most likely state sequence  by the forward backward algorithm  rabiner         also
called the baum welch algorithm baum         we do not address here this alternative  which is
computationally more expensive and is typically not used for part of speech tagging  it is possible 
however  to apply the committee based selection method also for this type of classification 
   it should be noted that practical implementations of part of speech tagging often employ a tag trigram
model  in which the probability of a tag depends on the last two tags rather than just the last one  the
committee based selection method which we apply here to the bigram model can easily be applied also
to the trigram case 

   

fiargamon   dagan

since p  w   is a constant  thus we seek the t which maximizes

p  t  w    

n
y
i  

p  ti   ti  p  wijti  

for technical convenience  we use bayes  theorem to replace each p  wijti   term by the term
p  t jw  p  w  
  noting that p  wi  does not effect the maximization over tag sequences and can
p  t  
therefore be omitted  following church         the parameters of a part of speech model 
then  are  tag probabilities p  ti   transition probabilities p  ti   ti    and lexical probabilities
p  tjw  
supervised training of the tagger is performed using a tagged corpus  text collection  
which was manually labeled with the correct part of speech for each word  maximum likelihood estimates  mles  for the parameters are easily computed from word and tag counts
from the corpus  for example  the mle of p  t  is the fraction of tag occurrences in the corpus that were the tag t  whereas p  tjw  is the ratio between the count for the word w being
labeled with the tag t and the total count for w  in our committee based selection scheme 
the counts are used also to compute the posterior distributions for parameter estimates  as
discussed below in section     
we next describe the application of our committee based selection scheme to the hmm
classification framework  first we will discuss how to sample from the posterior distributions over the hmm parameters p  ti  tj   and p  tjw   given training statistics   we then
discuss the question of how to define an example for training an hmm deals with  in
principle  infinite strings  on what substrings do we make decisions about labeling  finally 
we describe how to measure the amount of disagreement between committee members 
i

i

i

i

    posterior distributions for multinomial parameters
in this section  we consider how to select committee members based on the posterior parameter distributions p  ffi   aijs   for an hmm  assuming a uniform prior  first note that the
parameters of an hmm define a set of multinomial probability distributions  each multinomial corresponds to a conditioning event and its values are given by the corresponding
set of conditioned events  for example  a transition probability parameter p  ti  tj   has
conditioning event ti and conditioned event tj  
let fui g denote the set of possible values of a given multinomial variable  e g   the
possible tags for a given word   and let s   fni g denote a set of statistics extracted from
the training set  where ni is the number of times that the value ui appears in the training
p
set  we denote the total number of appearances of the multinomial variable as n   i ni  
the parameters whose distributions we wish to estimate are ffi   p  ui   
the maximum likelihood estimate for each of the multinomial s distribution parameters 
ffi   is ff i   nn   in practice  this estimator is usually smoothed in some way to compensate
for data sparseness  such smoothing typically reduces the estimates for values with positive
i

   we do not sample the model space over the tag probability parameters  since the amount of data for tag
frequencies is large enough to make their mles quite definite 

   

ficommittee based sample selection for probabilistic classifiers

counts and gives small positive estimates for values with a zero count  for simplicity  we
first describe here the approximation of p  ffi   aijs   for the unsmoothed estimator   
the posterior p  ffi   ai js   is a dirichlet distribution  johnson         for ease of
implementation  we used a generalization of the normal approximation described above
 section      for binomial parameters  we assume first that a multinomial is a collection of
independent binomials  each of which corresponds to a single value ui of the multinomial  we
then separately apply the constraint that the parameters of all these binomials should sum
to    for each such binomial  we sample from the approximate distribution  possibly with
a temperature t   then  to generate a particular multinomial distribution  we renormalize
the sampled parameters so they sum to   
to sample for the smoothed estimator  we first note that the estimator for the smoothed
model  interpolating with the uniform  is

  
ff si           nni  
where     is a smoothing parameter controlling the amount of smoothing  in our experiments           and  is the number of possible values for the given multinomial  we
then sample for each i from the truncated normal approximation  as in section    for the
smoothed estimate  i e  with mean    ff si and variance       n     normalization for the
multinomial is then applied as above 
finally  to generate a random hmm given statistics s   we note that all of its parameters
p  ti  tj   and p  tjw  are independent of each other  we thus independently choose values
for the hmm s parameters from each multinomial distribution 

    examples for hmm training
typically  concept learning problems are formulated such that there is a set of training
examples that are independent of each other  when training an hmm  however  each
state output pair is dependent on the previous state  so we are presented  in principle 
with a single infinite input string for training  in order to perform sample selection  we
must divide this infinite string into  short  finite strings 
for part of speech tagging  this problem may be solved by considering each sentence as
an individual example  more generally  we can break the text at any point where tagging
is unambiguous  in particular  it is common to have a lexicon which specifies which partsof speech are possible for each word  i e  which of the parameters p  tjw  are positive   in
bigram tagging  we can use unambiguous words  those with only one possible part of speech 
as example boundaries  similar natural breakpoints occur in other hmm applications  for
example  in speech recognition we can consider different utterances separately  in other
cases of hmm learning  where such natural breakpoints do not occur  some heuristic will
have to be applied  preferring to break at  almost unambiguous  points in the input 
   in the implementation we smooth the mle by interpolation with a uniform probability distribution 
following merialdo         adaptation of p  ffi   ai s   to the smoothed version of the estimator is given
below 
j

   

fiargamon   dagan

    quantifying disagreement
recall that our selection algorithms decide whether or not to select an example based on
how much the committee members disagree on its labeling  as discussed in section       
we suggest the use of vote entropy for measuring classification disagreement between committee members  this idea is supported by the fact that we found empirically that the
average normalized vote entropy for words which the tagger  after some training  classified correctly was       whereas the average entropy for incorrectly classified words was
      this demonstrates that vote entropy is a useful measure of classification uncertainty
 likelihood of error  based on the training data 
in bigram tagging  each example consists of a sequence of several words  in our implementation  we measured vote entropy separately for each word in the sequence  and use
the average vote entropy over the sequence as our measurement of disagreement for the
example  we use the average entropy rather than the entropy over the entire sequence 
because the number of committee members is small with respect to the total number of
possible tag sequences 

    results
we now present our results on applying committee based sample selection to bigram part ofspeech tagging  comparing it with complete training on all examples in the corpus  evaluation was performed using the university of pennsylvania tagged corpus from the acl dci
cd rom i  for ease of implementation  we used a complete  closed  lexicon which contains
all the words in the corpus   approximately     of the word occurrences in the corpus are
ambiguous in the lexicon  have more than one possible part of speech  
each committee based selection algorithm was initialized using the first       words
from the corpus  and then examined the following examples in the corpus for possible
labeling  the training set consisted of the first million words in the corpus  with sentence
ordering randomized to compensate for inhomogeneity in corpus composition  the test set
was a separate portion of the corpus consisting of        words  starting just after the first
          
we compared the amount of training required by different selection methods to achieve
a given tagging accuracy on the test set  where both the amount of training and tagging
accuracy are measured over ambiguous words  
      labeling efficiency
   we use the lexicon provided with brill s part of speech tagger  brill         while in an actual application
a complete lexicon would not be available  our results using a complete lexicon are valid  as the evaluation
of complete training and committee based selection is comparative 
   most other work on tagging has measured accuracy over all words  not just ambiguous ones  complete
training of our system on           words gave us an accuracy of       over ambiguous words  which
corresponds to an accuracy of       over all words in the test set  comparable to other published results
on bigram tagging 

   

ficommittee based sample selection for probabilistic classifiers

     
batch selection  m    n     
thresholded selection  th     
randomized selection  g     
two member selection
complete training

     

selected training

     
     
     
     
     
    
 
    

    

    

    

    
accuracy

   

    

    

    

figure    labeled training versus classification accuracy  in batch  random  and thresholded runs  k     and t      

figure   presents a comparison of the results of several selection methods  the reported parameter settings are the best found for each selection method by manual tuning 
figure   shows the advantage that sample selection gives with regard to annotation cost 
for example  complete training requires annotated examples containing        ambiguous
words to achieve a       accuracy  while the selection methods require only              
ambiguous words to achieve this accuracy  we also find that  to a first approximation  all
of the methods considered give similar results  thus  it seems that a refined choice of the
selection method is not crucial for achieving large reductions in annotation cost 
      computational efficiency

figure   plots classification accuracy versus number of words examined  instead of those
selected  complete training is clearly the most ecient in these terms  as it learns from all
examples examined  the selective methods are similar  though two member selection seems
to require somewhat fewer examples for examination than the other methods  furthermore 
since only two committee members are used this method is computationally more ecient
in evaluating each examined example 
      model size

the ability of committee based selection to focus on the more informative parts of the
training corpus is analyzed in figure    here we examined the number of lexical and bigram
   

fiargamon   dagan

      
batch selection  m    n     
thresholded selection  th     
randomized selection  g     
two member selection
complete training

      

examined training

      
      
      
      
      
     
 
    

    

    

    

    
accuracy

   

    

    

    

figure    examined training  both labeled and unlabeled  versus classification accuracy 
in batch  random  and thresholded runs  k     and t      

     

    
two member selection
complete training

     

two member selection
complete training

    

     

bigram model size

lexical model size

     

     
     
    
    

    
    
   
   

    
   

    
 
    

    

    

    

    
accuracy

   

    

    

   
    

    

 a 

    

    

    

    
   
accuracy

    

    

    

    

 b 

figure    numbers of frequency counts      plotted  y axis  versus classification accuracy
 x axis    a  lexical counts  freq t  w    b  bigram counts  freq t   t     

   

ficommittee based sample selection for probabilistic classifiers

 
two member selection
batch selection  m    n    
batch selection  m    n     
batch selection  m    n     
batch selection  m    n      

    

accuracy

    
    
    
   
    
    
 

      

      

             
examined training

      

      

figure     evaluating batch selection  for m      classification accuracy versus number of
words examined from the corpus for different batch sizes 
counts that were stored  i e  were non zero  during training  using the two member selection
algorithm and complete training  as the graphs show  committee based selection achieves
the same accuracy as complete training with fewer lexical and bigram counts  to achieve
    accuracy  two member selection requires just      lexical counts and     bigram counts 
as compared with        lexical counts and      bigram counts for complete training  this
implies that many counts in the data are not needed for correct tagging  since smoothing
estimates the probabilities equally well   committee based selection ignores these counts 
focusing its efforts on parameters which improve the model s performance  this behavior
has an additional practical advantage of reducing the size of the model significantly  also 
the average count is lower in a model constructed by selective training than in a fully trained
model  suggesting that the selection method tends to avoid using examples which increase
the counts for already known parameters 
      batch selection

we investigated the properties of batch selection  varying batch size from    to     
examples  fixing the number of examples selected from each batch at    we found that
in terms of the number of labeled examples required to attain a given accuracy  selection
for these different batch sizes performed similarly  this means that increased batch size
   as mentioned above  in the tagging phase we smooth the mle estimates by interpolation with a uniform
probability distribution  following merialdo        

   

fiargamon   dagan

does not seem to improve the effectiveness of selection  on the other hand  we did not see
a decrease in performance with increased batch size  which we might have expected due
to poorer modeling of the input distribution  as noted in section       this may indicate
that even a batch size of       selecting just       of the examples seen  is small enough
to let us model the input distribution with reasonable accuracy  however  the similarity
in performance of the different batch sizes to each other and to sequential selection does
not hold with respect to the amount of unlabeled training used  figure    shows accuracy
attained as a function of the amount of unlabeled training used  we see quite clearly that 
as expected  using larger batch sizes required examining a far larger number of unlabeled
training examples in order to obtain the same accuracy 

   discussion
    committee based selection as a monte carlo technique
we can view committee based selection as a monte carlo method for estimating the probability distribution of classes assigned to an example over all possible models  given the
training data  the proportion of votes among committee members for a class c on an example e is a sample based estimate of the probability  for a model chosen randomly from
the posterior model distribution  of assigning c to e  that is  the the proportion of votes
for c given e  v  kc e    is a monte carlo estimate of
z

p  cje  s     tm  cje p  m js  dm
m

where m ranges over possible models  vectors of parameter values  in the model space m 
p  m js   is the posterior probability density of model m given statistics s   and tm  cje     
if c is the highest probability class for e based on m  i e  if c   arg maxc pm  ci je   where
pm  cje  is the class probability distribution for e given by model m    and   otherwise  vote
entropy  as defined in section        is thus an approximation of the entropy of p   this
entropy is a direct measure of uncertainty in example classification over the possible models 
note that we measure entropy over the final classes assigned to an example by possible
models  i e  tm    not over the class probabilities given by a single model  i e  pm    as
illustrated by the ccf example of section      measuring entropy over pm  say  by looking
at the expected probability over all models  would not properly address properties   and  
discussed in section     
i

    batch selection
property   discussed in section     states that parameters that affect only few examples have
low overall utility  and so atypical examples are not very useful for learning  in sequential
selection  this property is addressed by independently examining input examples which are
drawn from the input distribution  in this way  we implicitly model the distribution of model
parameters used for classifying input examples  such modeling  however  is not inherent in
the basic form of batch selection  which can lead to it being less effective  freund et al  
      
   

ficommittee based sample selection for probabilistic classifiers

this diculty of batch selection is addressed directly by mccallum and nigam        
who describe a version of batch selection  called pool based sampling   which differs from
the basic batch selection scheme presented in section       in two ways  first  they quantify disagreement between committee members by the kl divergence to the mean  pereira
et al          rather than vote entropy  more significantly  their disagreement measure is
combined with an explicit density measure in density weighted sampling  such that documents that are similar to many other documents in the training set will be more probably
selected for labeling  this is intended to address property   in section      the authors
found empirically that for text classification using naive bayes  their density weighted poolbased selection method using kl divergence to the mean improved learning eciency over
complete training  they also found that sequential selection using vote entropy was worse
than complete training for their problem 
we hypothesize that this is due to the high degree of sparseness of the example space
 text documents   which leads to a large proportion of the examples being atypical  even
though documents similar to a given atypical document are rare  many different atypical
documents occur   since this is the case  the sequential variant may tend to select many
atypical documents for labeling  which would degrade learner performance by skewing the
statistics  this problem can be remedied by adding density weighting to sequential selection
in future research  this may yield an ecient sequential selection algorithm that also works
well in highly sparse domains 

   conclusions
labeling large training sets for supervised classification is often a costly process  especially
for complicated domain areas such as natural language processing  we have presented an
approach for reducing this cost significantly using committee based sample selection  which
reduces redundant annotation of examples that contribute little new information  the
method is applicable whenever it is possible to estimate a posterior distribution over the
model space given the training data  we have shown how to apply it to training hidden
markov models  and demonstrated its effectiveness for the complex task of part of speech
tagging  implicit modeling of uncertainty makes the selection system generally applicable
and relatively simple to implement  in practical settings  the method may be applied in a
semi interactive process  in which the system selects several new examples for annotation
at a time and updates its statistics after receiving their labels from the user 
the committee based sampling method addresses the three factors which relate the
informativeness of a training example to the model parameters that it affects  these factors
are      the statistical significance of the parameter s estimate      the parameter s effect
on classification  and     the probability that the parameter will be used for classification
in the future  the use of a committee models the uncertainty in classification relative to
the entire model space  while sequential selection implicitly models the distribution of the
examples 
our experimental study of variants of the selection method suggests several practical
conclusions  first  it was found that the simplest version of the committee based method 
   

fiargamon   dagan

using a two member committee  yields reduction in annotation cost comparable to that
of the multi member committee  the two member version is simpler to implement  has
no parameters to tune and is computationally more ecient  second  we generalized the
selection scheme giving several alternatives for optimizing the method for a specific task 
for bigram tagging  comparative evaluation of the different variants of the method showed
similar large reductions in annotation cost  suggesting the robustness of the committeebased approach  third  sequential selection  which implicitly models the expected utility
of an example relative to the example distribution  worked in general better than batch
selection  recent results on improving batch selection by modeling explicitly the  typicality 
of examples suggest further comparison of the two approaches  as discussed in the previous
section   finally  we studied the effect of sample selection on the size of the trained model 
showing a significant reduction in model size for selectively trained models 
in future research we propose to investigate the applicability and effectiveness of committeebased sample selection for additional probabilistic classification tasks  furthermore  the
generality obtained by implicitly modeling information gain suggests using variants of
committee based sampling also in non probabilistic contexts  where explicit modeling of
information gain may be impossible  in such contexts  committee members might be generated by randomly varying some of the decisions made in the learning algorithm 

acknowledgments
discussions with yoav freund  yishai mansour  and wray buntine greatly enhanced this
work  the first author was at bar ilan university while this work was performed  and was
supported by the fulbright foundation during part of the work 

references
angluin  d          learning regular sets from queries and counterexamples  information
and computation                 
angluin  d          queries and concept learning  machine learning             
baum  l  e          an inequality and an associated maximization technique in statistical
estimation of probabilistic functions of a markov process  inequalities        
black  e   jelinek  f   lafferty  j   magerman  d   mercer  r     roukos  s          towards
history based grammars  using richer models for probabilistic parsing  in proc  of the
annual meeting of the acl  pp        
brill  e          a simple rule based part of speech tagger  in proc  of acl conference on
applied natural language processing 
church  k  w          a stochastic parts program and noun phrase parser for unrestricted
text  in proc  of acl conference on applied natural language processing 
cohn  d   atlas  l     ladner  r          improving generalization with active learning 
machine learning              
   

ficommittee based sample selection for probabilistic classifiers

cohn  d  a   ghahramani  z     jordan  m  i          active learning with statistical
models  in tesauro  g   touretzky  d     alspector  j   eds    advances in neural
information processing  vol     morgan kaufmann  san mateo  ca 
dagan  i     engelson  s          committee based sampling for training probabilistic
classifiers  in proceedings of the international conference on machine learning 
elworthy  d          does baum welch re estimation improve taggers   in proc  of acl
conference on applied natural language processing  pp        
engelson  s     dagan  i       a   minimizing manual annotation cost in supervised learning from corpora  in proceedings of the   th annual meeting of the association for
computational linguistics 
engelson  s     dagan  i       b   sample selection in natural language learning  in
wermter  s   riloff  e     scheler  g   eds    symbolic  connectionist  and statistical
approaches to learning for natural language processing  springer verlag 
freund  y   seung  h  s   shamir  e     tishby  n          selective sampling using the
query by committee algorithm  machine learning              
freund  y          sifting informative examples from a random source  in working notes
of the workshop on relevance  aaai fall symposium series  pp        
gale  w   church  k     yarowsky  d          a method for disambiguating word senses
in a large corpus  computers and the humanities              
johnson  n  l          continuous univariate distributions      john wiley   sons  new
york 
johnson  n  l          continuous multivariate distributions  john wiley   sons  new
york 
kupiec  j          robust part of speech tagging using a hidden makov model  computer
speech and language             
lewis  d  d     catlett  j          heterogeneous uncertainty sampling for supervised
learning  in proceedings of the international conference on machine learning 
lewis  d  d     gale  w  a          a sequential algorithm for training text classifiers  in
proceedings of the of the acm sigir conference 
liere  r     tadepalli  p          active learning with committees for text categorization 
in proceedings of the national conference on artificial intelligence 
littlestone  n          learning quickly when irrelevant features abound  a new linearthreshold algorithm  machine learning    
mackay  d  j  c       a   the evidence framework applied to classification networks 
neural computation    
   

fiargamon   dagan

mackay  d  j  c       b   information based objective functions for active data selection 
neural computation    
matan  o          on site learning  tech  rep  logic       stanford university 
mccallum  a  k     nigam  k          employing em and pool based active learning
for text classification  in proceedings of the international conference on machine
learning 
merialdo  b          tagging text with a probabilistic model  in proc  int l conf  on
acoustics  speech  and signal processing 
merialdo  b          tagging text with a probabilistic model  computational linguistics 
                
mitchell  t          generalization as search  artificial intelligence     
pereira  f   tishby  n     lee  l          distributional clustering of english words  in
proceedings of the annual meeting of the association for computational linguistics
 acl  
plutowski  m     white  h          selecting concise training sets from clean data  ieee
trans  on neural networks        
press  w  h   flannery  b  p   teukolsky  s  a     vetterling  w  t          numerical
recipes in c  cambridge university press 
rabiner  l  r          a tutorial on hidden markov models and selected applications in
speech recognition  proc  of the ieee         
seung  h  s   opper  m     sompolinsky  h          query by committee  in proceedings
of the acm workshop on computational learning theory 
viterbi  a  j          error bounds for convolutional codes and an asymptotically optimal
decoding algorithm  ieee trans  informat  theory  t    

   

fi