journal artificial intelligence research                  

submitted       published      

committee based sample selection
probabilistic classifiers
shlomo argamon engelson

department computer science
jerusalem college technology  machon lev
p o b       
jerusalem        israel

argamon mail jct ac il

ido dagan

department mathematics computer science
bar ilan university
      ramat gan  israel

dagan cs biu ac il

abstract
many real world learning tasks expensive acquire sucient number labeled
examples training  paper investigates methods reducing annotation cost
sample selection  approach  training learning program examines many
unlabeled examples selects labeling informative
stage  avoids redundantly labeling examples contribute little new information 
work follows previous research query committee  extends
committee based paradigm context probabilistic classification  describe
family empirical methods committee based sample selection probabilistic classification models  evaluate informativeness example measuring degree
disagreement several model variants  variants  the committee  drawn
randomly probability distribution conditioned training set labeled far 
method applied real world natural language processing task stochastic part of speech tagging  find variants method achieve significant
reduction annotation cost  although computational eciency differs  particular 
simplest variant  two member committee parameters tune  gives excellent
results  show sample selection yields significant reduction size
model used tagger 

   introduction
algorithms supervised concept learning build classifiers concept based given
set labeled examples  many real world concept learning tasks  however  acquiring
labeled training examples expensive  hence  objective develop automated
methods reduce training cost within framework active learning 
learner control choice examples labeled used
training 
c      ai access foundation morgan kaufmann publishers  rights reserved 

fiargamon   dagan

two main types active learning  first uses membership queries 
learner constructs examples asks teacher label  angluin        mackay 
    b  plutowski   white         approach provides proven computational
advantages  angluin         always applicable since always possible
construct meaningful informative unlabeled examples training  diculty may
overcome large set unlabeled training data available  case second
type active learning  sample selection  often applied  learner examines many
unlabeled examples  selects informative ones learning  seung  opper 
  sompolinsky        freund  seung  shamir    tishby        cohn  atlas    ladner 
      lewis   catlett        lewis   gale        
paper  address problem sample selection training probabilistic
classifier  classification framework performed probability based model which 
given input example  assigns score possible classification selects
highest score 
research follows theoretical work sample selection query committee
 qbc  paradigm  seung et al         freund et al          propose novel empirical
scheme applying qbc paradigm probabilistic classification models  allowing label
noise   addressed original qbc framework  see section      
committee based selection scheme  learner receives stream unlabeled examples
input decides whether ask label not  end 
learner constructs  committee   two more  classifiers based statistics
current training set  committee member classifies candidate example 
learner measures degree disagreement among committee members  example
selected labeling depending degree disagreement  according selection
protocol 
previous work  dagan   engelson        engelson   dagan      b  presented
particular selection protocol probabilistic concepts  paper extends previous
work mainly generalizing selection scheme comparing variety different
selection protocols  a preliminary version appeared engelson   dagan      a  

    application natural language processing
much early work sample selection either theoretical nature 
tested toy problems  we  however  motivated complex  real world problems
area statistical natural language text processing  work addresses
task part of speech tagging  core task statistical natural language processing
 nlp   work sample selection natural language tasks mainly focused
text categorization problems  works lewis catlett         liere
tadepalli         mccallum nigam        
statistical nlp  probabilistic classifiers often used select preferred analysis
linguistic structure text  syntactic structure  black  jelinek  lafferty 
magerman  mercer    roukos         word categories  church         word senses  gale 
   

ficommittee based sample selection probabilistic classifiers

church    yarowsky         parameters classification model estimated
training corpus  a collection text  
common case supervised training  learner uses corpus
sentence manually annotated correct analysis  manual annotation typically
expensive  consequence  large annotated corpora exist  mainly english language  covering genres text  situation makes dicult apply
supervised learning methods languages english  adapt systems different genres text  furthermore  infeasible many cases develop new supervised
methods require annotations different currently available 
cases  manual annotation avoided altogether  using self organized methods  shown part of speech tagging english kupiec         even
kupiec s tagger  though  manual  and somewhat unprincipled  biasing initial model
necessary achieve satisfactory convergence  elworthy        merialdo       
investigated effect self converging re estimation part of speech tagging
found initial manual training needed  generally  supervised
training provided  better results  fact  fully unsupervised methods
applicable many nlp tasks  perhaps even part of speech tagging
languages  sample selection appropriate way reduce cost annotating corpora 
easy obtain large volumes raw text smaller subsets selected
annotation 
applied committee based selection learning hidden markov models  hmms 
part of speech tagging english sentences  part of speech tagging task labeling
word sentence appropriate part speech  for example  labeling
occurrence word  hand  noun verb   task non trivial since determining
word s part speech depends linguistic context  hmms used extensively
task  e g   church        merialdo         cases trained corpora
manually annotated correct part speech word 
experiments part of speech tagging  described section      show using committeebased selection results substantially faster learning rates  enabling learner achieve
given level accuracy using far fewer training examples sequential training using
text 

   background
objective sample selection select examples informative future  might determine informativeness example  one
approach derive explicit measure expected amount information gained
using example  cohn  ghahramani    jordan        mackay      b      a  
example  mackay      b  assesses informativeness example  neural network
learning task  expected decrease overall variance model s prediction 
training example  explicit measures appealing  since attempt
give precise characterization information content example  also  membership querying  explicit formulation information content sometimes enables finding
   

fiargamon   dagan

informative examples analytically  saving cost searching example space 
use explicit methods may limited  however  since explicit measures generally
 a  model specific   b  complex  often requiring various approximations practical 
 c  depend accuracy current hypothesis given step 
alternative measuring informativeness example explicitly measure
implicitly  quantifying amount uncertainty classification example
given current training data  informativeness example evaluated respect
models derived training data stage learning  one approach use
single model based training data seen far  approach taken lewis
gale         training binary classifier  select training examples
whose classification probability closest      i e  examples current
best model uncertain 
order better evaluate classification uncertainty respect entire space
possible models  one may instead measure classification disagreement among sample
set possible models  a committee   using entire model space enables measuring
degree training entails single  best  classification example 
hand  referring single model measures degree model
certain classification  example  classifier sucient training predicting ips coin heads probability      always predict heads  hence
make mistakes     time  however  although classifier quite uncertain
correctness classification  additional training improve accuracy 
two main approaches generating committee order evaluate example
uncertainty  version space approach random sampling approach  version
space approach  pursued cohn et al         seeks choose committee members
border space models allowed training data  the version space 
mitchell         thus models chosen committee far
possible consistent training data  ensures models
disagree example whenever training example would restrict version space 
version space approach dicult apply since finding models edge
version space non trivial general  furthermore  approach directly
applicable case probabilistic classification models  almost models
possible  though equally probable  given training  alternative random sampling  exemplified query committee algorithm  seung et al         freund
et al          inspired paper  approach  models sampled randomly
set possible models  according probability models given
training data  work applies random sampling approach probabilistic classifiers
computing approximation posterior model distribution given training data 
generating committee members distribution  mccallum nigam       
use similar approach sample selection text categorization using naive bayes classifier  primary difference skew example selection using density weighted
sampling  documents similar many documents training
set selected labeling higher probability 

   

ficommittee based sample selection probabilistic classifiers

matan        presents two methods random sampling  first method 
trains committee members different subsets training data  second method 
neural network models  matan generates committee members backpropagation training using different initial weights networks reach different local minima 
similar approach taken liere tadepalli         applied committee based
selection approach text categorization using winnow learning algorithm  littlestone 
      learns linear classifiers  represented model space set classifiers  the model set   classifier model set learns independently labeled
examples  initialized different initial hypothesis  thus point
set gives selection possible hypotheses given training data   labeling decisions
performed based two models chosen random model set  models
disagree document s class  document s label requested  models
space updated 

    query committee
mentioned above  paper follows theoretical work sample selection query
committee  qbc  paradigm  seung et al         freund et al          method
proposed learning binary  non probabilistic  concepts cases exists prior
probability distribution measure concept class  qbc selects  informative  training
examples stream unlabeled examples  example selected learner
queries teacher correct label adds training set  examples
selected training  restrict set consistent concepts  i e  set concepts
label training examples correctly  the version space  
simple version qbc  analyzed freund et al          see
summary freund         uses following selection algorithm 
   draw unlabeled input example random probability distribution example space 
   select random two hypotheses according prior probability distribution concept class  restricted set consistent concepts 
   select example training two hypotheses disagree classification 
freund et al  prove that  assumptions  algorithm achieves exponential
reduction number labeled examples required achieve desired classification
accuracy  compared random selection training examples  speedup achieved
algorithm tends select examples split version space two parts
similar size  one parts eliminated version space example
correct label added training set 

    selection probabilistic classifiers
address problem sample selection training probabilistic classifier  classification framework performed probabilistic model which  given input
   

fiargamon   dagan

example  assigns probability  or probability based score  possible classification
selects best classification  probabilistic classifiers fall within framework
addressed theoretical qbc work  training probabilistic classifier involves estimating values model parameters determine probability estimate possible
classification example  expect cases optimal classifier
assign highest probability correct class  guaranteed always occur 
accordingly  notion consistent hypothesis generally applicable probabilistic
classifiers  thus  posterior distribution classifiers given training data cannot
defined restriction prior set consistent hypotheses  rather  within
bayesian framework  posterior distribution defined statistics training
set  assigning higher probability classifiers likely given statistics 
discuss desired properties examples selected training  generally speaking  training example contributes data several statistics  turn
determine estimates several parameter values  informative example therefore
one whose contribution statistics leads useful improvement parameter estimates  assuming existence optimal classification model given concept
 such maximum likelihood model   identify three properties parameters
acquiring additional statistics beneficial 
   current estimate parameter uncertain due insucient statistics
training set  uncertain estimate likely far true value
parameter cause incorrect classification  additional statistics would bring
estimate closer true value 
   classification sensitive changes current estimate parameter  otherwise  acquiring additional statistics unlikely affect classification therefore
beneficial 
   parameter takes part calculating class probabilities large proportion
examples  parameters relevant classifying examples  determined probability distribution input examples  low utility future
estimation 
committee based selection scheme  describe below  tends select
examples affect parameters three properties  property   addressed
randomly picking parameter values committee members posterior distribution
parameter estimates  given current statistics   statistics parameter
insucient variance posterior distribution estimates large  hence
large differences values parameter picked different committee
members  note property   addressed uncertainty classification
judged relative single model  as in  e g   lewis   gale         approach
captures uncertainty respect given parameter values  sense property   
model uncertainty choice values first place  the use
single model criticized cohn et al         
property   addressed selecting examples committee members highly disagree classification  thus  algorithm tends acquire statistics uncertainty
   

ficommittee based sample selection probabilistic classifiers

parameter estimates entails uncertainty actual classification  this analogous splitting
version space qbc   finally  property   addressed independently examining
input examples drawn input distribution  way  implicitly
model expected utility statistics classifying future examples 

    paper outline
following section defines basic concepts notation use rest
paper  section   presents general selection scheme along variant selection
algorithms  next two sections demonstrate effectiveness sample selection
scheme  section   presents results artificial  colorful coin ipper  problem  providing
simple illustration operation proposed system  section   presents results
task stochastic part of speech tagging  demonstrating usefulness committeebased sample selection real world 

   definitions
concern paper minimize number labeled examples needed
learn classifier accurately classifies input examples e classes c   c   c
known set possible classes  learning  stream unlabeled examples
supplied free  examples drawn unknown probability distribution 
cost  however  learning algorithm obtain true label given example 
objective reduce cost much possible  still learning accurate
classifier 
address specific case probabilistic classifiers  classification done
basis score function  fm  c  e   assigns score possible class
input example  classifier assigns input example class highest score 
fm determined probabilistic model   many applications  fm conditional
probability function  pm  cje   specifying probability class given example 
alternatively  score functions denote likelihood class may used
 such odds ratio   particular type model used classification determines
specific form score  function features example 
probabilistic model   thus score function fm   defined set parameters  fffi g  giving probabilities various possible events  example  model
part of speech tagging contains parameters probability particular word
verb noun  training  values parameters estimated
set statistics    extracted training set labeled examples  particular model
denoted   faig  ai specific value corresponding ffi  

   committee based sample selection
section describes algorithms apply committee based approach evaluating classification uncertainty input example  learning algorithm evaluates
   

fiargamon   dagan

example giving committee containing several versions  copies  classifier   consistent  training data seen far  greater agreement
committee members classification example  greater certainty
classification  training data entails specific classification high
certainty   in probabilistic sense  versions classifier consistent
data produce classification  example selected labeling  therefore 
committee members disagree appropriate classification 

    generating committee
generate committee k members  randomly choose k models according
posterior distribution p  m js   possible models given current training statistics 
sampling performed depends form distribution  turn
depends form model  thus implementing committee based selection
particular problem  appropriate sampling procedure must devised  illustration
committee generation  rest section describes sampling process models
consisting independent binomial parameters multinomial parameter groups 
consider first model containing single binomial parameter  the probability
success   estimated value a  statistics model given n  
number trials  x  number successes trials 
given n x   best  model parameter value estimated several
estimation methods  example  maximum likelihood estimate  mle    nx  
giving model   fff   nx g  generating committee models  however 
interested  best  model  rather sampling distribution models given
statistics  example  need sample posterior density estimates
ff  namely p ff   ajs    binomial case  density beta distribution  johnson 
       sampling distribution yields set estimates scattered around nx  assuming
uniform prior   variance estimates gets smaller n gets larger 
estimate participates different member committee  thus  statistics
estimating parameter  closer estimates used different models
committee 
consider model consisting single group interdependent parameters defining multinomial  case  posterior dirichlet distribution  johnson        
committee members generated sampling joint distribution  giving values
model parameters 
models consisting set independent binomials multinomials  sampling
p  m js   amounts sampling parameters independently  models
complex dependencies among parameters sampling may dicult  practice 
though  may possible make enough independence assumptions make sampling
feasible 
sampling posterior generates committee members whose parameter estimates differ
based low training counts tend agree based high
counts  classification example relies parameters whose estimates com   

ficommittee based sample selection probabilistic classifiers

unlabeled input example e 
   draw   models randomly p  m js    statistics acquired
previously labeled examples 
   classify e model  giving classifications c  c  
   c     c   select e annotation 
   e selected  get correct label update accordingly 
figure    two member sequential selection algorithm 
mittee members differ  differences affect classification  example would
selected learning  leads selecting examples contribute statistics
currently unreliable estimates effect classification  thus address
properties     discussed section     

    selection algorithms
within committee based paradigm exist different methods selecting informative examples  previous research sample selection used either sequential selection
 seung et al         freund et al         dagan   engelson         batch selection  lewis
  catlett        lewis   gale         present general algorithms sequential batch committee based selection  cases  assume
selection algorithm applied small amount labeled initial training supplied  order
initialize training statistics 
      two member sequential selection

sequential selection examines unlabeled examples supplied  one one 
estimates expected information gain  examples determined suciently
informative selected training  simply  choose committee size two
posterior distribution models  select example two models
disagree classification  gives parameter free  two member sequential selection
algorithm  shown figure    basic algorithm parameters 
      general sequential selection

general selection algorithm results from 

using larger number k committee members  order evaluate example informativeness precisely 

refined example selection criteria 
   

fiargamon   dagan

unlabeled input example e 
   draw k models fmi g randomly p  m js    possibly using temperature t  
   classify e model mi giving classifications fci g 
   measure disagreement d e  based fci g 
   decide whether select e annotation  based value
d e  
   e selected  get correct label update accordingly 
figure    general sequential selection algorithm 

tuning frequency selection replacing p  m js   distribution
different variance  effect adjusting variability among committee
members chosen  many cases  eg   hmms  described section   below 
implemented parameter  called temperature   used multiplier
variance posterior parameter distribution 

gives general sequential selection algorithm  shown figure   
easy see two member sequential selection special case general sequential selection  order instantiate general algorithm larger committees  need
fix general measure d e  disagreement  step     decision method selecting
examples according disagreement  step    
measure disagreement entropy distribution classifications  voted for 
committee members  vote entropy natural measure quantifying
uniformity classes assigned example different committee members   
normalize entropy bound maximum possible value  log min k  jcj   
giving value      denoting number committee members assigning
class c input example e v  c  e   normalized vote entropy is 
x v  c  e  v  c  e 
 
d e     
log min k  jc j  c k log k
normalized vote entropy value one committee members disagree 
value zero agree  taking intermediate values cases partial agreement 
consider two alternatives selection criterion  step     simplest
thresholded selection  example selected annotation normalized vote
entropy exceeds threshold   another alternative randomized selection 
example selected annotation based ip coin biased according
vote entropy a higher vote entropy corresponding higher probability selection 
   mccallum nigam        suggested alternative measure  kl divergence mean
 pereira  tishby    lee         clear whether measure advantage simpler
entropy function 

   

ficommittee based sample selection probabilistic classifiers

batch b n examples 
   example e b  
 a  draw k models randomly p  m js   
 b  classify e model  giving classifications fcig 
 c  measure disagreement d e  e based fcig 
   select annotation examples highest d e  
   update statistics selected examples 
figure    batch selection algorithm 
use simple model selection probability linear function normalized vote
entropy  p  e    gd e   calling g entropy gain   
      batch selection

alternative sequential selection batch selection  rather evaluating examples
individually informativeness large batch n examples examined 
best selected annotation  batch selection algorithm given figure   
procedure repeated sequentially successive batches n examples  returning
start corpus end  n equal size corpus  batch selection
selects globally best examples corpus stage  as lewis   catlett        
batch selection certain theoretical drawbacks  freund et al          particularly
consider distribution input examples  however  shown mccallum
nigam         distribution input examples modeled taken
account selection  combining disagreement measure
measure example density  produces good results batch selection  this work
discussed detail section       separate diculty batch selection
computational disadvantage must look large number examples
selecting any  batch size decreased  batch selection behaves similarly
sequential selection 

   example  colorful coin flipper
illustrative example learning task  define colorful coin  ipper  ccf 
machine contains infinite number coins various colors  machine chooses
coins ip  one one  color coin fixed  unknown  probability
chosen  coin ipped  comes heads probability determined solely
color  ips coin  machine tells learner color coin chosen
   selection method used  dagan   engelson        randomized sequential selection using
linear selection probability model  parameters k  g 

   

fiargamon   dagan

ip  order know outcome ip  however  learner must pay machine 
training  learner may choose colors coins whose outcomes examine 
objective selective sampling choose minimize training cost  number
ips examined  required attain given prediction accuracy ip outcomes 
case ccf  example e coin ip  characterized color 
class c either heads tails  note require ips given color always
class  therefore best hope classify according
likely class color 
ccf  define model whose parameters heads probabilities
coins particular color  so  ccf three colors  one possible model would
  fr        g         b      g  giving probabilities heads red  green  blue
coins  respectively  coin given color classified  heads  score  given
directly appropriate model parameter          tails  otherwise 

    implementation sample selection
training model ccf amounts counting proportion heads color 
providing estimates heads probabilities  complete training every coin ip training
sequence examined added counts  sample selection seek label
count training ips colors additional counts likely improve
model s accuracy  useful colors train either training
examples far seen  whose current probability estimates near    
 cf  section      
recall sample selection build committee sampling models p  m js   
case ccf  model parameters ffi  the heads probabilities different
colors  independent  sampling p  m js   amounts sampling independently
parameters 
form posterior distribution p  ffi   ai js   given beta distribution  found technically easier use normal approximation  found
satisfactory practice  let ni number coin ips color seen far  ni
number ips came heads  approximate p  ffi   ai js   truncated normal distribution  restricted         estimated mean   nn variance
i      n     approximation made easy incorporate  temperature  parameter  as section         used multiplier variance estimate i    thus 
actually approximate p  ffi   aijs   truncated normal distribution mean
variance i  t  sampling distribution done using algorithm given press 
flannery  teukolsky  vetterling        sampling normal distribution 










    vote entropy
ccf useful illustrate importance determining classification uncertainty
using vote entropy committee models rather using entropy
class distribution given single model  as discussed section     consider ccf
   

ficommittee based sample selection probabilistic classifiers

model
 
 
 
 

red
      heads 
      heads 
      heads 
      heads 

blue
      tail 
      tail 
      heads 
      heads 
 a 

green
      heads 
      tail 
      tail 
      tail 

color d e  acde
red
        
blue
        
green          
 b 

figure     a  committee ccf models   b  resultant vote entropy color 
ccf results    colors

ccf results     colors

   
ptm
committee based sampling
complete training

ptm
committee based sampling
complete training

   

   

selected training

selected training

   

   

   

  
  

 
   

 
    

   

    
desired accuracy

   

    

   

   

 a 

    

   

    
desired accuracy

   

    

   

 b 

figure    ccf results random ccfs        different coin colors  results
averaged   different ccfs  comparing complete training two
member sample selection  figures show amount training required
desired classification accuracy   a     colors   b      colors 

three coin colors  red  blue  green  suppose   member committee figure   a 
generated  committee  estimate color vote entropy d e   well
average class distribution entropies given individual models
 acde   given figure   b  
compare entropies red blue  example  see entropies
expected class probability distribution quite high  since estimated
class probabilities near       however  consider vote entropies  over
assigned classes   blue maximal entropy  since range possible models straddles
class boundary        red minimal entropy  since range possible models
straddle class boundary  is  quite certain optimal classification
red  heads   see green higher vote entropy red  although
average class distribution entropy lower  shows importance using vote entropy
selection 
   

fiargamon   dagan

ccf frequency selection
 
   color ccf
    color ccf

   
   

selection frequency

   
   
   
   
   
   
   
 
 

  

  

  

  

   
   
selected training

   

   

   

   

figure    frequency selection vs  amount selected training ccfs       
colors  averaged   different ccfs 

    results
simulated sample selection simple ccf model order illustrate
properties  following  generated random ccfs fixed number coins
randomly generating occurrence probabilities heads probabilities coin color 
generated learning curves complete training  input examples  two
member sample selection  using    coin  ips initial training  complete training
sample selection run coin  ip sequences  accuracy measured
computing expected accuracy  assuming infinite test set  mle model
generated selected training  figures show accuracy theoretical
perfectly trained model  ptm  knows parameters perfectly 
figure   summarizes average results   comparison runs complete vs  sample
selection ccfs        coins  figures   a   b   compare amount
selected training required reach given desired accuracy  see cases
soon sample selection starts operating  eciency higher complete training 
gap increases size greater accuracy desired  figure    examine
cumulative frequency selection  ratio number selected examples
total number examples seen  learning progresses  see exponential decrease
frequency selection  expected case qbc non probabilistic models
 analyzed seung et al         freund et al         

   application  stochastic part of speech tagging
applied committee based selection real world task learning hidden markov
models  hmms  part of speech tagging english sentences  part of speech tagging
task labeling word sentence appropriate part speech  for
example  labeling occurrence word  hand  noun verb   task nontrivial since determining word s part speech depends linguistic context  hmms
   

ficommittee based sample selection probabilistic classifiers

used extensively task  e g   church        merialdo         cases
trained corpora manually annotated correct part speech
word 

    hmms part of speech tagging
first order hidden markov model  hmm  probabilistic finite state string generator
 rabiner         defined set states q   fqi g  set output symbols   set
transition probabilities p  qi  qj   possible transition states qi qj   set
output probabilities p  ajq  state q output symbol     distinguished
start state q    probability string   a a  generated hmm
given
 
n
x

p  qi    qi  p  aijqi    
q  qn  qn i  

sum  paths hmm  joint probability path traversed
output given string  contrast ordinary markov models  hmm
known sequence states generated given string  hence term  hidden   
hmms used widely speech language processing  particular  hmm
used provide classification model sequence elements  need classify
element sequence  encode possible class state hmm  training
hmm amounts estimating values transition output probabilities 
then  given sequence classification  assume generated hmm
compute likely state sequence string  using viterbi algorithm   viterbi 
      
hmm used part of speech tagging words encoding possible partof speech tag   noun  verb  adjective  etc    hmm state  output probabilities 
p  wjt   give probability producing word w language conditioned
current tag t  transition probabilities  p  t  t    give probability generating
word tag t  given previous word s tag t    constitutes weak
syntactic model language  model often termed tag bigram model   
given input word sequence w   w  wn   seek likely tag sequence
  t  tn  
 
arg maxt p  t jw     arg maxt pp t w
 w  
  arg maxt p  t  w  
   alternative classification scheme compute likely state individual element
 instead likely state sequence  forward backward algorithm  rabiner         also
called baum welch algorithm baum         address alternative 
computationally expensive typically used part of speech tagging  possible 
however  apply committee based selection method type classification 
   noted practical implementations part of speech tagging often employ tag trigram
model  probability tag depends last two tags rather last one 
committee based selection method apply bigram model easily applied
trigram case 

   

fiargamon   dagan

since p  w   constant  thus seek maximizes

p  t  w    

n

i  

p  ti   ti  p  wijti  

technical convenience  use bayes  theorem replace p  wijti   term term
p  t jw  p  w  
  noting p  wi  effect maximization tag sequences
p  t  
therefore omitted  following church         parameters part of speech model 
then  are  tag probabilities p  ti   transition probabilities p  ti   ti    lexical probabilities
p  tjw  
supervised training tagger performed using tagged corpus  text collection  
manually labeled correct part of speech word  maximum likelihood estimates  mles  parameters easily computed word tag counts
corpus  example  mle p  t  fraction tag occurrences corpus tag t  whereas p  tjw  ratio count word w
labeled tag total count w  committee based selection scheme 
counts used compute posterior distributions parameter estimates 
discussed section     
next describe application committee based selection scheme hmm
classification framework  first discuss sample posterior distributions hmm parameters p  ti  tj   p  tjw   given training statistics  
discuss question define example training an hmm deals  in
principle  infinite strings  substrings make decisions labeling  finally 
describe measure amount disagreement committee members 








    posterior distributions multinomial parameters
section  consider select committee members based posterior parameter distributions p  ffi   aijs   hmm  assuming uniform prior  first note
parameters hmm define set multinomial probability distributions  multinomial corresponds conditioning event values given corresponding
set conditioned events  example  transition probability parameter p  ti  tj  
conditioning event ti conditioned event tj  
let fui g denote set possible values given multinomial variable  e g  
possible tags given word   let   fni g denote set statistics extracted
training set  ni number times value ui appears training
p
set  denote total number appearances multinomial variable n   ni  
parameters whose distributions wish estimate ffi   p  ui   
maximum likelihood estimate multinomial s distribution parameters 
ffi   ff i   nn   practice  estimator usually smoothed way compensate
data sparseness  smoothing typically reduces estimates values positive


   sample model space tag probability parameters  since amount data tag
frequencies large enough make mles quite definite 

   

ficommittee based sample selection probabilistic classifiers

counts gives small positive estimates values zero count  simplicity 
first describe approximation p  ffi   aijs   unsmoothed estimator   
posterior p  ffi   ai js   dirichlet distribution  johnson         ease
implementation  used generalization normal approximation described
 section      binomial parameters  assume first multinomial collection
independent binomials  corresponds single value ui multinomial 
separately apply constraint parameters binomials sum
   binomial  sample approximate distribution  possibly
temperature t   then  generate particular multinomial distribution  renormalize
sampled parameters sum   
sample smoothed estimator  first note estimator smoothed
model  interpolating uniform 

 
ff si           nni  
  smoothing parameter controlling amount smoothing  in experiments          number possible values given multinomial 
sample truncated normal approximation  as section   
smoothed estimate  i e  mean   ff si variance       n     normalization
multinomial applied above 
finally  generate random hmm given statistics   note parameters
p  ti  tj   p  tjw  independent other  thus independently choose values
hmm s parameters multinomial distribution 

    examples hmm training
typically  concept learning problems formulated set training
examples independent other  training hmm  however 
state output pair dependent previous state  presented  in principle 
single infinite input string training  order perform sample selection 
must divide infinite string  short  finite strings 
part of speech tagging  problem may solved considering sentence
individual example  generally  break text point tagging
unambiguous  particular  common lexicon specifies partsof speech possible word  i e  parameters p  tjw  positive  
bigram tagging  use unambiguous words  those one possible part speech 
example boundaries  similar natural breakpoints occur hmm applications 
example  speech recognition consider different utterances separately 
cases hmm learning  natural breakpoints occur  heuristic
applied  preferring break  almost unambiguous  points input 
   implementation smooth mle interpolation uniform probability distribution 
following merialdo         adaptation p  ffi   ai   smoothed version estimator given
below 
j

   

fiargamon   dagan

    quantifying disagreement
recall selection algorithms decide whether select example based
much committee members disagree labeling  discussed section       
suggest use vote entropy measuring classification disagreement committee members  idea supported fact found empirically
average normalized vote entropy words tagger  after training  classified correctly       whereas average entropy incorrectly classified words
      demonstrates vote entropy useful measure classification uncertainty
 likelihood error  based training data 
bigram tagging  example consists sequence several words  implementation  measured vote entropy separately word sequence  use
average vote entropy sequence measurement disagreement
example  use average entropy rather entropy entire sequence 
number committee members small respect total number
possible tag sequences 

    results
present results applying committee based sample selection bigram part ofspeech tagging  comparing complete training examples corpus  evaluation performed using university pennsylvania tagged corpus acl dci
cd rom i  ease implementation  used complete  closed  lexicon contains
words corpus   approximately     word occurrences corpus
ambiguous lexicon  have one possible part of speech  
committee based selection algorithm initialized using first       words
corpus  examined following examples corpus possible
labeling  training set consisted first million words corpus  sentence
ordering randomized compensate inhomogeneity corpus composition  test set
separate portion corpus consisting        words  starting first
          
compared amount training required different selection methods achieve
given tagging accuracy test set  amount training tagging
accuracy measured ambiguous words  
      labeling efficiency
   use lexicon provided brill s part of speech tagger  brill         actual application
complete lexicon would available  results using complete lexicon valid  evaluation
complete training committee based selection comparative 
   work tagging measured accuracy words  ambiguous ones  complete
training system           words gave us accuracy       ambiguous words 
corresponds accuracy       words test set  comparable published results
bigram tagging 

   

ficommittee based sample selection probabilistic classifiers

     
batch selection  m    n     
thresholded selection  th     
randomized selection  g     
two member selection
complete training

     

selected training

     
     
     
     
     
    
 
    

    

    

    

    
accuracy

   

    

    

    

figure    labeled training versus classification accuracy  batch  random  thresholded runs  k          

figure   presents comparison results several selection methods  reported parameter settings best found selection method manual tuning 
figure   shows advantage sample selection gives regard annotation cost 
example  complete training requires annotated examples containing        ambiguous
words achieve       accuracy  selection methods require              
ambiguous words achieve accuracy  find that  first approximation 
methods considered give similar results  thus  seems refined choice
selection method crucial achieving large reductions annotation cost 
      computational efficiency

figure   plots classification accuracy versus number words examined  instead
selected  complete training clearly ecient terms  learns
examples examined  selective methods similar  though two member selection seems
require somewhat fewer examples examination methods  furthermore 
since two committee members used method computationally ecient
evaluating examined example 
      model size

ability committee based selection focus informative parts
training corpus analyzed figure    examined number lexical bigram
   

fiargamon   dagan

      
batch selection  m    n     
thresholded selection  th     
randomized selection  g     
two member selection
complete training

      

examined training

      
      
      
      
      
     
 
    

    

    

    

    
accuracy

   

    

    

    

figure    examined training  both labeled unlabeled  versus classification accuracy 
batch  random  thresholded runs  k          

     

    
two member selection
complete training

     

two member selection
complete training

    

     

bigram model size

lexical model size

     

     
     
    
    

    
    
   
   

    
   

    
 
    

    

    

    

    
accuracy

   

    

    

   
    

    

 a 

    

    

    

    
   
accuracy

    

    

    

    

 b 

figure    numbers frequency counts      plotted  y axis  versus classification accuracy
 x axis    a  lexical counts  freq t  w    b  bigram counts  freq t   t     

   

ficommittee based sample selection probabilistic classifiers

 
two member selection
batch selection  m    n    
batch selection  m    n     
batch selection  m    n     
batch selection  m    n      

    

accuracy

    
    
    
   
    
    
 

      

      

             
examined training

      

      

figure     evaluating batch selection       classification accuracy versus number
words examined corpus different batch sizes 
counts stored  i e  non zero  training  using two member selection
algorithm complete training  graphs show  committee based selection achieves
accuracy complete training fewer lexical bigram counts  achieve
    accuracy  two member selection requires      lexical counts     bigram counts 
compared        lexical counts      bigram counts complete training 
implies many counts data needed correct tagging  since smoothing
estimates probabilities equally well   committee based selection ignores counts 
focusing efforts parameters improve model s performance  behavior
additional practical advantage reducing size model significantly  also 
average count lower model constructed selective training fully trained
model  suggesting selection method tends avoid using examples increase
counts already known parameters 
      batch selection

investigated properties batch selection  varying batch size        
examples  fixing number examples selected batch    found
terms number labeled examples required attain given accuracy  selection
different batch sizes performed similarly  means increased batch size
   mentioned above  tagging phase smooth mle estimates interpolation uniform
probability distribution  following merialdo        

   

fiargamon   dagan

seem improve effectiveness selection  hand  see
decrease performance increased batch size  might expected due
poorer modeling input distribution  as noted section       may indicate
even batch size       selecting       examples seen  small enough
let us model input distribution reasonable accuracy  however  similarity
performance different batch sizes sequential selection
hold respect amount unlabeled training used  figure    shows accuracy
attained function amount unlabeled training used  see quite clearly that 
expected  using larger batch sizes required examining far larger number unlabeled
training examples order obtain accuracy 

   discussion
    committee based selection monte carlo technique
view committee based selection monte carlo method estimating probability distribution classes assigned example possible models  given
training data  proportion votes among committee members class c example e sample based estimate probability  model chosen randomly
posterior model distribution  assigning c e  is  proportion votes
c given e  v  kc e    monte carlo estimate
z

p  cje      tm  cje p  m js  dm


ranges possible models  vectors parameter values  model space m 
p  m js   posterior probability density model given statistics   tm  cje     
c highest probability class e based  i e  c   arg maxc pm  ci je  
pm  cje  class probability distribution e given model      otherwise  vote
entropy  defined section        thus approximation entropy p  
entropy direct measure uncertainty example classification possible models 
note measure entropy final classes assigned example possible
models  i e  tm    class probabilities given single model  i e  pm   
illustrated ccf example section      measuring entropy pm  say  looking
expected probability models  would properly address properties    
discussed section     


    batch selection
property   discussed section     states parameters affect examples
low overall utility  atypical examples useful learning  sequential
selection  property addressed independently examining input examples
drawn input distribution  way  implicitly model distribution model
parameters used classifying input examples  modeling  however  inherent
basic form batch selection  lead less effective  freund et al  
      
   

ficommittee based sample selection probabilistic classifiers

diculty batch selection addressed directly mccallum nigam        
describe version batch selection  called pool based sampling   differs
basic batch selection scheme presented section       two ways  first  quantify disagreement committee members kl divergence mean  pereira
et al          rather vote entropy  significantly  disagreement measure
combined explicit density measure density weighted sampling  documents similar many documents training set probably
selected labeling  intended address property   section      authors
found empirically text classification using naive bayes  density weighted poolbased selection method using kl divergence mean improved learning eciency
complete training  found sequential selection using vote entropy worse
complete training problem 
hypothesize due high degree sparseness example space
 text documents   leads large proportion examples atypical  even
though documents similar given atypical document rare  many different atypical
documents occur   since case  sequential variant may tend select many
atypical documents labeling  would degrade learner performance skewing
statistics  problem remedied adding density weighting sequential selection
future research  may yield ecient sequential selection algorithm works
well highly sparse domains 

   conclusions
labeling large training sets supervised classification often costly process  especially
complicated domain areas natural language processing  presented
approach reducing cost significantly using committee based sample selection 
reduces redundant annotation examples contribute little new information 
method applicable whenever possible estimate posterior distribution
model space given training data  shown apply training hidden
markov models  demonstrated effectiveness complex task part speech
tagging  implicit modeling uncertainty makes selection system generally applicable
relatively simple implement  practical settings  method may applied
semi interactive process  system selects several new examples annotation
time updates statistics receiving labels user 
committee based sampling method addresses three factors relate
informativeness training example model parameters affects  factors
are      statistical significance parameter s estimate      parameter s effect
classification      probability parameter used classification
future  use committee models uncertainty classification relative
entire model space  sequential selection implicitly models distribution
examples 
experimental study variants selection method suggests several practical
conclusions  first  found simplest version committee based method 
   

fiargamon   dagan

using two member committee  yields reduction annotation cost comparable
multi member committee  two member version simpler implement 
parameters tune computationally ecient  second  generalized
selection scheme giving several alternatives optimizing method specific task 
bigram tagging  comparative evaluation different variants method showed
similar large reductions annotation cost  suggesting robustness committeebased approach  third  sequential selection  implicitly models expected utility
example relative example distribution  worked general better batch
selection  recent results improving batch selection modeling explicitly  typicality 
examples suggest comparison two approaches  as discussed previous
section   finally  studied effect sample selection size trained model 
showing significant reduction model size selectively trained models 
future research propose investigate applicability effectiveness committeebased sample selection additional probabilistic classification tasks  furthermore 
generality obtained implicitly modeling information gain suggests using variants
committee based sampling non probabilistic contexts  explicit modeling
information gain may impossible  contexts  committee members might generated randomly varying decisions made learning algorithm 

acknowledgments
discussions yoav freund  yishai mansour  wray buntine greatly enhanced
work  first author bar ilan university work performed 
supported fulbright foundation part work 

references
angluin  d          learning regular sets queries counterexamples  information
computation                 
angluin  d          queries concept learning  machine learning             
baum  l  e          inequality associated maximization technique statistical
estimation probabilistic functions markov process  inequalities        
black  e   jelinek  f   lafferty  j   magerman  d   mercer  r     roukos  s          towards
history based grammars  using richer models probabilistic parsing  proc 
annual meeting acl  pp        
brill  e          simple rule based part speech tagger  proc  acl conference
applied natural language processing 
church  k  w          stochastic parts program noun phrase parser unrestricted
text  proc  acl conference applied natural language processing 
cohn  d   atlas  l     ladner  r          improving generalization active learning 
machine learning              
   

ficommittee based sample selection probabilistic classifiers

cohn  d  a   ghahramani  z     jordan  m  i          active learning statistical
models  tesauro  g   touretzky  d     alspector  j   eds    advances neural
information processing  vol     morgan kaufmann  san mateo  ca 
dagan  i     engelson  s          committee based sampling training probabilistic
classifiers  proceedings international conference machine learning 
elworthy  d          baum welch re estimation improve taggers   proc  acl
conference applied natural language processing  pp        
engelson  s     dagan  i       a   minimizing manual annotation cost supervised learning corpora  proceedings   th annual meeting association
computational linguistics 
engelson  s     dagan  i       b   sample selection natural language learning 
wermter  s   riloff  e     scheler  g   eds    symbolic  connectionist  statistical
approaches learning natural language processing  springer verlag 
freund  y   seung  h  s   shamir  e     tishby  n          selective sampling using
query committee algorithm  machine learning              
freund  y          sifting informative examples random source  working notes
workshop relevance  aaai fall symposium series  pp        
gale  w   church  k     yarowsky  d          method disambiguating word senses
large corpus  computers humanities              
johnson  n  l          continuous univariate distributions      john wiley   sons  new
york 
johnson  n  l          continuous multivariate distributions  john wiley   sons  new
york 
kupiec  j          robust part of speech tagging using hidden makov model  computer
speech language             
lewis  d  d     catlett  j          heterogeneous uncertainty sampling supervised
learning  proceedings international conference machine learning 
lewis  d  d     gale  w  a          sequential algorithm training text classifiers 
proceedings acm sigir conference 
liere  r     tadepalli  p          active learning committees text categorization 
proceedings national conference artificial intelligence 
littlestone  n          learning quickly irrelevant features abound  new linearthreshold algorithm  machine learning    
mackay  d  j  c       a   evidence framework applied classification networks 
neural computation    
   

fiargamon   dagan

mackay  d  j  c       b   information based objective functions active data selection 
neural computation    
matan  o          on site learning  tech  rep  logic       stanford university 
mccallum  a  k     nigam  k          employing em pool based active learning
text classification  proceedings international conference machine
learning 
merialdo  b          tagging text probabilistic model  proc  int l conf 
acoustics  speech  signal processing 
merialdo  b          tagging text probabilistic model  computational linguistics 
                
mitchell  t          generalization search  artificial intelligence     
pereira  f   tishby  n     lee  l          distributional clustering english words 
proceedings annual meeting association computational linguistics
 acl  
plutowski  m     white  h          selecting concise training sets clean data  ieee
trans  neural networks        
press  w  h   flannery  b  p   teukolsky  s  a     vetterling  w  t          numerical
recipes c  cambridge university press 
rabiner  l  r          tutorial hidden markov models selected applications
speech recognition  proc  ieee         
seung  h  s   opper  m     sompolinsky  h          query committee  proceedings
acm workshop computational learning theory 
viterbi  a  j          error bounds convolutional codes asymptotically optimal
decoding algorithm  ieee trans  informat  theory  t    

   


