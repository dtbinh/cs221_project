journal artificial intelligence research               

submitted        published      

decision theoretic planning  structural assumptions
computational leverage
craig boutilier

cebly cs ubc ca

department computer science  university british columbia
vancouver  bc  v t  z   canada

thomas dean

tld cs brown edu

department computer science  brown university
box       providence  ri         usa

steve hanks

hanks cs washington edu

department computer science engineering  university washington
seattle  wa         usa

abstract

planning uncertainty central problem study automated sequential
decision making  addressed researchers many different fields  including
ai planning  decision analysis  operations research  control theory economics 
assumptions perspectives adopted areas often differ substantial ways 
many planning problems interest researchers fields modeled markov
decision processes  mdps  analyzed using techniques decision theory 
paper presents overview synthesis mdp related methods  showing
provide unifying framework modeling many classes planning problems studied
ai  describes structural properties mdps that  exhibited particular classes problems  exploited construction optimal approximately
optimal policies plans  planning problems commonly possess structure reward
value functions used describe performance criteria  functions used describe
state transitions observations  relationships among features used describe
states  actions  rewards  observations 
specialized representations  algorithms employing representations  achieve
computational leverage exploiting various forms structure  certain ai techniques 
particular based use structured  intensional representations can
viewed way  paper surveys several types representations classical
decision theoretic planning problems  planning algorithms exploit representations number different ways ease computational burden constructing
policies plans  focuses primarily abstraction  aggregation decomposition techniques based ai style representations 

   introduction

planning using decision theoretic notions represent domain uncertainty plan quality
recently drawn considerable attention artificial intelligence  ai    decision theoretic
planning  dtp  attractive extension classical ai planning paradigm
allows one model problems actions uncertain effects  decision maker
   see  example  recent texts  dean  allen    aloimonos        dean   wellman        russell  
norvig        research reported  hanks  russell    wellman        

c      ai access foundation morgan kaufmann publishers  rights reserved 

fiboutilier  dean    hanks

incomplete information world  factors resource consumption lead
solutions varying quality  may absolute well defined  goal 
state  roughly  aim dtp form courses action  plans policies 
high expected utility rather plans guaranteed achieve certain goals 
ai planning viewed particular approach solving sequential decision problems
type  connections dtp models used fields research such
decision analysis  economics operations research  or  become apparent 
conceptual level  sequential decision problems viewed instances markov
decision processes  mdps   use mdp framework make connections
explicit 
much recent research dtp explicitly adopted mdp framework underlying model  barto  bradtke    singh        boutilier   dearden        boutilier  dearden 
  goldszmidt        dean  kaelbling  kirman    nicholson        koenig        simmons
  koenig        tash   russell         allowing adaptation existing results algorithms solving mdps  e g   field or  applied planning problems 
so  however  work departed traditional definition  planning
problem  ai planning community one goal paper make explicit
connection two lines work 
adopting mdp framework model posing solving planning problems
illuminated number interesting connections among techniques solving decision
problems  drawing work ai planning  reasoning uncertainty  decision analysis
or  one interesting insights emerge body work many
dtp problems exhibit considerable structure  thus solved using special purpose
methods recognize exploit structure  particular  use feature based
representations describe problems  typical practice ai  often highlights
problem s special structure allows exploited computationally little effort 
two general impediments widespread acceptance mdps within
ai general model planning  first absence explanations mdp model
make connections current planning research explicit  either conceptual
computational level  may due large part fact mdps
developed studied primarily or  dominant concerns are  naturally  rather
different  one aim paper make connections clear  provide brief
description mdps conceptual model planning emphasizes connection
ai planning  explore relationship mdp solution algorithms ai
planning algorithms  particular  emphasize ai planning models
viewed special cases mdps  classical planning algorithms designed
exploit problem characteristics associated cases 
second impediment skepticism among ai researchers regarding computational
adequacy mdps planning model  techniques scale solve planning problems
reasonable size  one diculty solution techniques mdps tendency rely
explicit  state based problem formulations  problematic ai planning
since state spaces grow exponentially number problem features  state space size
dimensionality somewhat lesser concern decision analysis 
fields  operations researcher decision analyst often hand craft model ignores
certain problem features deemed irrelevant  define features summarize
 

fidecision theoretic planning  structural assumptions

wide class problem states  ai  emphasis automatic solution problems
posed users lack expertise decision analyst  thus  assuming well crafted 
compact state space often appropriate 
paper show specialized representations algorithms ai planning
problem solving used design ecient mdp solution techniques  particular 
ai planning methods assume certain structure state space  actions  or
operators   specification goal success criteria  representations
algorithms designed make problem structure explicit exploit
structure solve problems effectively  demonstrate process identifying
structure  making explicit  exploiting algorithmically brought bear
solution mdps 
paper several objectives  first  provides overview dtp mdps
suitable readers familiar traditional ai planning methods makes connections
work  second  describes types structure exploited
ai representations methods facilitate computationally effective planning mdps 
such  suitable introduction ai methods familiar classical
presentation mdps  finally  surveys recent work use mdps ai
suggests directions research regard  therefore interest
researchers dtp 

    general problem definition
roughly speaking  class problems consider involving systems whose
dynamics modeled stochastic processes  actions decision maker 
referred agent   uence system s behavior  system s current
state choice action jointly determine probability distribution system s
possible next states  agent prefers certain system states  e g   goal states 
others  therefore must determine course action also called  plan   policy 
paper that likely lead target states  possibly avoiding undesirable states
along way  agent may know system s state exactly making decision
act  however it may rely incomplete noisy sensors forced
base choice action probabilistic estimate state 
help illustrate types problems interested  consider following
example  imagine robot agent designed help someone  the  user  
oce environment  see figure     three activities might undertake  picking
user s mail  getting coffee  tidying user s research lab  robot move
location location perform various actions tend achieve certain target
states  e g   bringing coffee user demand  maintaining minimal level tidiness
lab  
might associate certain level uncertainty effects robot s actions
 e g   tries move adjacent location might succeed     time fail
move     time   robot might incomplete access
true state system sensors might supply incomplete information  it
cannot tell whether mail available pickup mail room  incorrect
 

fiboutilier  dean    hanks

office

hallway

lab
mailroom

coffee

figure    decision theoretic planning problem
information  even mail room sensors occasionally fail detect presence
mail  
finally  performance robot might measured various ways  actions
guarantee goal achieved  maximize objective function defined
possible effects actions  achieve goal state sucient probability avoiding  disastrous  states near certainty  stipulation optimal
acceptable behavior important part problem specification 
types problems captured using general framework include classical  goal oriented  deterministic  complete knowledge  planning problems extensions
conditional probabilistic planning problems  well general
problem formulations 
discussion point assumed extensional representation system s
states one state explicitly named  ai research  intensional representations common  intensional representation one states sets
states described using sets multi valued features  choice appropriate set
features important part problem design  features might include
current location robot  presence absence mail  on  performance
metric typically expressed intensionally  figure   serves reference example problem  use throughout paper  lists basic features used describe
states system  actions available robot exogenous events
might occur  together intuitive description features  actions events 
remainder paper organized follows  section    present mdp
framework abstract  introducing basic concepts terminology noting relationship abstract model classical ai planning problem  section   surveys common solution techniques algorithms based dynamic programming general
mdp problems search algorithms planning problems and points relationship problem assumptions solution techniques  section   turns algorithms
representations  showing various ways structured representations commonly
used ai algorithms used represent mdps compactly well  section   surveys
 

fidecision theoretic planning  structural assumptions

features
location

denoted
description
loc m    etc  location robot  five possible locations  mailroom  m   coffee room
 c   user s oce  o   hallway  h   laboratory  l 
tidiness
     etc 
degree lab tidiness  five possible values     messiest   
 tidiest 
mail present
m 
mail user s mail box  true  m   false  m  
robot mail
rhm  rhm robot mail possession 
coffee request
cr  cr
outstanding  unfulfilled  request coffee user 
robot coffee rhc  rhc robot coffee possession 
actions
denoted
description
move clockwise
clk
move adjacent location  clockwise direction 
counterclockwise cclk
move adjacent location  counterclockwise direction 
tidy lab
tidy
robot lab  degree tidiness increased  
pickup mail
pum
robot mailroom mail present  robot
takes mail  rhm becomes true becomes false 
get coffee
getc
robot coffee room  gets coffee  rhc becomes true 
deliver mail
delm
robot oce mail  hands mail user
 rhm becomes false 
deliver coffee
delc
robot oce coffee  hands coffee
user  rhc cr become false 
events
denoted
description
mail arrival
arrm
mail arrives causing become true
request coffee
reqc
user issues coffee request causing cr become true
untidy lab
mess
lab becomes messier  one degree less tidy 

figure    elements robot domain 
recent work abstraction  aggregation problem decomposition methods 
shows connection traditional ai methods goal regression  last
section demonstrates representational computational methods ai planning
used solution general mdps  section   points additional ways
type computational leverage might developed future 

   markov decision processes  basic problem formulation
section introduce mdp framework make explicit relationship
model classical ai planning models  interested controlling stochastic
dynamical system  system point time one number distinct
states  system s state changes time response events  action
particular kind event instigated agent order change system s state 
assume agent control actions taken when  though
effects taking action might perfectly predictable  contrast  exogenous events
agent s control  occurrence may partially predictable 
abstract view agent consistent  ai  view agent
autonomous decision maker  control  view policy determined ahead
time  programmed device  executed without deliberation 
 

fiboutilier  dean    hanks

    states state transitions

define state description system particular point time  one
defines states vary particular applications  notions natural
others  however  common assume state captures information relevant
agent s decision making process  assume finite state space   fs            sn g
possible system states   cases agent complete information
current state  uncertainty incomplete information captured using
probability distribution states  
discrete time stochastic dynamical system consists state space probability
distributions governing possible state transitions how next state system depends
past states  distributions constitute model system evolves time
response actions exogenous events  ecting fact effects actions
events may perfectly predictable even prevailing state known 
although generally concerned agent chooses appropriate course
action  remainder section assume agent s course action
fixed  concentrating problem predicting system s state occurrence
predetermined sequence actions  discuss action selection problem next
section 
assume system evolves stages  occurrence event marks
transition one stage next stage      since events define changes stage 
since events often  but necessarily  cause state transitions  often equate stage
transitions state transitions  course  possible event occur leave
system state 
system s progression stages roughly analogous passage time 
two identical assume action  possibly no op  taken
stage  every action takes unit time complete  thus speak loosely
stages correspond units time  refer interchangeably set stages
set time points  
model uncertainty regarding system s state stage random
variable takes values   assumption  forward causality  requires
variable depend directly value future variable k  k   t   roughly 
requires model system past history  directly  determines
distribution current states  whereas knowledge future states uence
estimate current state indirectly providing evidence current state
may lead future states  figure   a  shows graphical perspective
discrete time  stochastic dynamical system  nodes random variables denoting
state particular time  arcs indicate direct probabilistic dependence
states previous states  describe system completely must supply
conditional distributions pr s js         t     times t 
states thought descriptions system modeled  question arises much detail system captured state description 
   discussion paper applies cases state space countably infinite  see
 puterman        discussion infinite continuous state problems 
   deal topics here  considerable literature community
continuous time markov decision processes  puterman        

 

fidecision theoretic planning  structural assumptions

 a 







 b 







 c 

 

 

 

 



 



 



t  

t  











t  



figure    general stochastic process  a   markov chain  b   stationary markov
chain  c  
detail implies information system  turn often translates better
predictions future behavior  course  detail implies larger set  
increase computational cost decision making 
commonly assumed state contains enough information predict next
state  words  information history system relevant predicting
future captured explicitly state itself  formally  assumption  markov
assumption  says knowledge present state renders information past
irrelevant making predictions future 
pr s t   js   t                   pr s t   js  
markovian models represented graphically using structure figure   b  
ecting fact present state sucient predict future state evolution  
finally  common assume effects event depend prevailing
state  stage time event occurs   distribution predicting
next state regardless stage  model said stationary
represented schematically using two stages  figure   c   case
single conditional distribution required  paper generally restrict attention
discrete time  finite state  stochastic dynamical systems markov property  commonly called markov chains  furthermore  discussion restricted stationary
chains 
complete model must provide probability distribution initial states 
ecting probability state stage    distribution repre   worth mentioning markov property applies particular model system
itself  indeed  non markovian model system  of finite order  i e   whose dynamics depend
k previous states k  converted equivalent though larger markov model 
control theory  called conversion state form  luenberger        
   course  statement model detail  saying state carries enough information
make stage irrelevant predicting transitions 

 

fiboutilier  dean    hanks

  

 

  

  
  

  

 
  

 

  

 

   

  

 

  

 

  

 
   

  

figure    state transition diagram 
sented real valued  row  vector size n   js j  one entry state   denote
vector p   use p i denote ith entry  is  probability starting state
si  
represent  stage nonstationary markov chain transition matrices 
size n n   matrix p captures transition probabilities governing
system moves stage stage      matrix consists probabilities ptij  
ptij   pr s t     sj js   si    process stationary  transition matrix
stages one matrix  whose entries denoted pij   suce  given
initial
states p     probability distribution states n stages
q  pdistribution
i 
i n
stationary markov process represented using state transition diagram
figure    nodes correspond particular states stage represented
explicitly  arcs denote possible transitions  those non zero probability  labeled
transition probabilities pij   pr s t     sj js   si    arc node node
j labeled pij pij       size diagram least o n  
o n      depending number arcs  useful representation transition
graph relatively sparse  example  states immediate transitions
neighbors 

example     illustrate notions  imagine robot figure   executing

policy moving counterclockwise repeatedly  restrict attention two
variables  location loc presence mail   giving state space size    
suppose robot always moves adjacent location probability     
addition  mail arrive mailroom probability     time  independent robot s location   causing variable become true 
becomes true  robot cannot move state false  since action
moving uence presence mail  state transition diagram
example illustrated figure    transition matrix shown   

structure markov chain occasionally interest us planning  subset

c closed pij       c j    c   proper closed set proper
subset c enjoys property  sometimes refer proper closed sets recurrent
classes states  closed set consists single state  state called
absorbing state  agent enters closed set absorbing state  remains
   important note nodes represent random variables earlier figures 

 

fidecision theoretic planning  structural assumptions

s 
s 

   

om

   

   
   

om

s 

hm

lm

   

   

   

s  
   

   
cm

mm

mm

   

s 

hm

s     

s 

   

s 
s 
s 
s 
s 
s 
s 
s 
s 
s  

s 

   

s 

lm

   

s 

cm

   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
    
   
   
   
   
   
   
   
    

s 
   
   
   
   
   
   
   
   
   
    

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
    
   
    
   
    
    
    
    
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s  
   
   
   
   
   
   
   
   
   
   

figure    state transition diagram transition matrix moving robot 
forever probability    example  figure     set states
holds forms recurrent class  absorbing states example 
program robot stay put whenever state hm  loc o i  would
absorbing state altered chain  finally  say state transient
belong recurrent class  figure    state holds transient eventually
 with probability     agent leaves state never returns  since way
remove mail arrives 

    actions
markov chains used describe evolution stochastic system 
capture fact agent choose perform actions alter state
system  key element mdps set actions available decision maker 
action performed particular state  state changes stochastically response
action  assume agent takes action stage process 
system changes state accordingly 
stage process state s  agent available set actions
ats  called feasible set stage t  describe effects   ats  must
supply state transition distribution pr s t   js   s    a  actions a  states s 
stages t  unlike case markov chain  terms pr s t   js   s    a 
true conditional distributions  rather family distributions parameterized
  since probability part model  retain notation  however 
suggestive nature 
often assume feasible set actions stages states 
case set actions   fa            ak g executed time 
contrasts ai planning practice assigning preconditions actions defining
states meaningfully executed  model takes view
action executed  or  attempted   state  action effect
executed state  execution leads disastrous effects  noted
action s transition matrix  action preconditions often computational convenience
rather representational necessity  make planning process ecient
identifying states planner even consider selecting action 
preconditions represented mdps relaxing assumption set
 

fiboutilier  dean    hanks

s 
s 
s 
s 
s 
s 
s 
s 
s 
s  

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s 
   
   
   
   
   
   
   
   
   
   

s  
   
   
   
   
   
   
   
   
   
   

s 
   

   

om

   

s 

lm

s 

s 

s  

   

s 

   
   

cm

mm

mm

s 

s 

   
   

   

s 
hm

s 

   
   

   

hm

   

lm

om

   

cm

   

figure    transition matrix clk induced transition diagram two action
policy 
feasible actions states  illustrate planning concepts below  however 
sometimes assume actions preconditions 
restrict attention stationary processes  case means
effects action depends state stage  transition
matrices thus take form pkij   pr s t     sj js   si     ak    capturing probability
system moves state sj ak executed state si   stationary models
action fully described single n n transition matrix p k   important note
transition matrix action includes direct effects executing
action effects exogenous events might occur stage  

example     example figure   extended agent two available

actions  moving clockwise moving counterclockwise  transition matrix
cclk  with assumption mail arrives probability      shown figure   
matrix clk appears left figure    suppose agent fixes behavior
moves clockwise locations c counterclockwise locations h  
l  we address agent might come know location
actually implement behavior   defines markov chain illustrated
transition diagram right figure     

    exogenous events

exogenous events events stochastically cause state transitions  much
actions  beyond control decision maker  might correspond
evolution natural process action another agent  notice effect
action cclk figure    combines  effects robot s action
exogenous event mail arrival  state transition probabilities incorporate motion
robot  causing change location  possible change mail status due
mail arrival  purposes decision making  precisely combined effect
   possible assess effects actions exogenous events separately  combine
single transition matrix certain cases  boutilier   puterman         discuss later
section 

  

fidecision theoretic planning  structural assumptions

important predicting distribution possible states resulting
action taken  call models actions implicit event models  since effects
exogenous event folded transition probabilities associated action 
however  often natural view transitions comprised two separate
events  effect state  generally  often think transitions
determined effects agent s chosen action certain exogenous
events beyond agent s control  may occur certain probability 
effects actions decomposed fashion  call action model
explicit event model 
specifying transition function action zero exogenous events
generally easy  actions events interact complex ways  instance  consider
specifying effect action pum  pickup mail  state mail present
possibility  simultaneous  mail arrival  i e    same unit  discrete
time   event arrm occurs  robot obtain newly arrived mail 
mail remain mailbox  intuitively  depends whether mail arrived
pickup completed  albeit within time quantum   state transition
case viewed composition two transitions precise description
composition depends ordering agent s action exogenous event 
mail arrives first  transition might   s    s     s  state mail
waiting s   state mail waiting robot holding mail 
pickup action completed first  transition would     s   i e   pum
effect  mail arrives remains box  
picture complicated actions events truly occur simultaneously
interval in case resulting transition need composition
individual transitions  example  robot lifts side table glass
water situated  water spill  similarly exogenous event causes side
raised  action event occur simultaneously  result qualitatively
different  the water spilled   thus   interleaving  semantics described
always appropriate 
complications  modeling exogenous events combination
actions events approached many ways  depending modeling assumptions one willing make  generally  specify three types information  first 
provide transition probabilities actions events assumption
occur isolation these standard transition matrices  transition matrix
figure   decomposed two matrices shown figure    one clk one
arrm   second  exogenous event  must specify probability occurrence 
since vary state  generally require vector length n indicating
probability occurrence state  occurrence vector arrm would
                                         
   fact individual matrices deterministic artifact example  general 
actions events represented using genuinely stochastic matrices 

  

fiboutilier  dean    hanks

s 
s 
s 
s 
s 
s 
s 
s 
s 
s  

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s  
   
   
   
   
   
   
   
   
   
   

s 
s 
s 
s 
s 
s 
s 
s 
s 
s  

action clk

s 

    
    
    
    
    
    
    
    
    
    

s 

   
   
   
   
   
   
   
   
   
   

s 

    
    
    
    
    
    
    
    
    
    

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s 

   
   
   
   
   
   
   
   
   
   

s  
   
   
   
   
   
   
   
   
   
   

event arrm

figure    transition matrices action exogenous event explicit event
model 
assume  illustration  mail arrives none present   final
requirement combination function describes  compose  transitions
action subset event transitions  indicated above  complex 
sometimes almost unrelated individual action event transitions  however 
certain assumptions combination functions specified reasonably concisely 
one way modeling composition transitions assume interleaving semantics type alluded above  case  one needs specify probability
action events take place occur specific order  instance  one might
assume event occurs time within discrete time unit according
continuous distribution  e g   exponential distribution given rate   information  probability particular ordering transitions  given certain events
occur  computed  resulting distribution possible next states 
example above  probability  composed  transitions s    s    s  s    s    s 
would given probabilities mail arrived first last  respectively 
certain cases  probability ordering needed  illustrate another
combination function  assume action always occurs exogenous events 
furthermore  assume events commutative   a  initial state pair
events e  e    distribution results applying event sequence e  e 
identical obtained sequence e  e     b  occurrence probabilities
intermediate states identical  intuitively  set events domain  arrm  reqc
mess  property  conditions combined transition distribution
action computed considering probability subset events
applying subset order distribution associated a 
generally  construct implicit event model various components
explicit event model  thus   natural  specification converted form usually
used mdp solution algorithms  two assumptions above  instance 
form implicit event transition matrix pr si   a  sj   action a  given matrix
pcra  si   sj    which assumes event occurrences   matrices pre  si   sj   events
e  occurrence vector pre si   event e  effective transition matrix
   probability different events may correlated  possibly particular states   case 
necessary specify occurrence probabilities subsets events  treat event occurrence
probabilities independent ease exposition 

  

fidecision theoretic planning  structural assumptions

event e defined follows 
pcre  si   sj     pre  si  pre  si   sj    

 

    pre  si       j
       j

equation captures event transition probabilities probability event occurrence factored in  let e  e   denote diagonal matrices entries ekk   pre  sk  
        pre  sk    pr
c e si  sj     e pre  e    assumptions above 
ekk
implicit event matrix pr si   a  sj   action given pr   pcre  pcren pra
ordering n possible events 
naturally  different procedures constructing implicit event matrices required
given different assumptions action event interaction  whether implicit models constructed specified directly without explicit mention exogenous events 
always assume unless stated otherwise action transition matrices take account
effects exogenous events well  thus represent agent s best information
happen takes particular action 

    observations

although effects action depend aspect prevailing state 
choice action depend agent observe current state
remember prior observations  model agent s observational sensing
capabilities introducing finite set observations   fo            oh g  agent receives
observation set stage prior choosing action stage 
model observation random variable ot whose value taken o 
probability particular ot generated depend on 

state system    
action taken    
state system taking action     effects
exogenous events     realized  action taken 
let pr ot   oh js t     si   at     ak     sj   probability agent observes

oh stage given performs ak state si ends state sj   actions 

assume observational distributions stationary  independent stage   using

phi j k   pr ohjsi  ak   sj   denote quantity  view probabilistic dependencies

among state  action observation variables graph time indexed variables
shown nodes one variable directly probabilistically dependent another
edge latter former  see figure   
model allows wide variety assumptions agent s sensing capabilities 
one extreme fully observable mdps  fomdps   agent knows exactly
state stage t  model case letting   setting
pr oh jsi   ak   sj    
  

 

  iff oh   sj
  otherwise

fiboutilier  dean    hanks



t  





t  







figure    graph showing dependency relationships among states  actions observations different times 
example above  means robot always knows exact location whether
mail waiting mailbox  even mailroom mail arrives 
agent thus receives perfect feedback results actions effects
exogenous events it noisy effectors complete  noise free   instantaneous 
sensors  recent ai research adopts mdp framework explicitly assumes full
observability 
extreme might consider non observable systems  nomdps 
agent receives information system s state execution  model
case letting   fog  observation reported stage  revealing
information state  pr sj jsi   ak   o    pr sj jsi   ak    open loop
systems  agent receives useful feedback results actions  agent
noisy effectors sensors  case agent chooses actions according plan
consisting sequence actions executed unconditionally  effect  agent relying
predictive model determine good action choices execution time 
traditionally  ai planning work implicitly made assumption non observability 
often coupled omniscience assumption that agent knows initial state
certainty  predict effects actions perfectly  precisely predict occurrence exogenous events effects  circumstances  agent
predict exact outcome plan  thus obviating need observation 
agent build straight line plan a sequence actions performed without
feedback that good plan whose execution might depend information gathered execution time 
two extremes special cases general observation model described above 
allows agent receive incomplete noisy information system state
 i e   partially observable mdps  pomdps   example  robot might able
determine location exactly  might able determine whether mail arrives
unless mailroom  furthermore   mail  sensors might occasionally report
inaccurately  leading incorrect belief whether mail waiting 

example     suppose robot  checkmail  action change system

state generates observation uenced presence mail  provided
  

fidecision theoretic planning  structural assumptions

loc m   
loc m   
loc m   
loc m   

pr obs   mail  pr obs   nomail 
    
    
    
    
    
    
    
    

figure    observation probabilities checking mailbox 
robot mailroom time action performed  robot
mailroom  sensor always reports  no mail   noisy  checkmail  sensor
described probability distribution one shown figure   
view error probabilities probability  false positives          false
negatives           

    system trajectories observable histories

use terms trajectory history interchangeably describe system s behavior
course problem solving episode  perhaps initial segment thereof 
complete system history sequence states  actions  observations generated
stage   time point interest  finite infinite length  complete
histories represented  possibly infinite  sequence tuples form

hhs     o    a  i  hs     o    a  i        hs   ot   ii
define two alternative notions history contain less complete information 
arbitrary stage define observable history sequence

hho    a  i          hot     at   ii
o  observation initial state  observable history stage comprises
information available agent history chooses action stage t 
third type trajectory system trajectory  sequence

hhs     a  i          hs t     at   i 
describing system s behavior  objective  terms  independent agent s particular
view system 
evaluating agent s performance  generally interested system
trajectory  agent s policy must defined terms observable history  since
agent access system trajectory  except fully observable case 
two equivalent 

    reward value

problem facing decision maker select action performed stage
decision problem  making decision basis observable history 
agent still needs way judge quality course action  done defining
  

fiboutilier  dean    hanks



t  





t  



c



r



figure     decision process rewards action costs 
value function v   function mapping set system histories hs reals 
is  v   hs   r    agent prefers system history h h  case v h    v h    
thus  agent judges behavior good bad depending effect
underlying system trajectory  generally  agent cannot predict certainty
system trajectory occur  best generate probability distribution
possible trajectories caused actions  case  computes expected value
candidate course action chooses policy maximizes quantity 
system dynamics  specifying value function arbitrary trajectories
cumbersome unintuitive  therefore important identify structure
value function lead parsimonious representation 
two assumptions value functions commonly made mdp literature
time separability additivity  time separable value function defined terms
primitive functions applied component states actions  reward
function r     r associates reward state s  costs assigned
taking actions defining cost function c     r associates cost
performing action state s  rewards added value function  costs
subtracted   
value function time separable  simple combination  rewards costs
accrued stage   simple combination  means value taken function
costs rewards stage  costs rewards depend stage t 
function combines must independent stage  commonly linear
combination product    value function additive combination function
sum reward cost function values accrued history s stages  addition
rewards action costs system time separable value viewed graphically
shown figure    
assumption time separability restrictive  example  might
certain goals involving temporal deadlines  have workplace tidy soon possible
     tomorrow morning  maintenance  do allow mail sit mailroom
    technically  set histories interest depends horizon chosen  described below 
    term  reward  somewhat misnomer reward could negative  case
 penalty  might better word  likewise   costs  either positive  punitive  negative  beneficial   thus  admit great exibility defining value functions 
    see  luenberger        precise definition time separability 

  

fidecision theoretic planning  structural assumptions

undelivered    minutes  require value functions non separable
given current representation state  note  however  separability like
markov property is property particular representation  could add additional
information state example  clock time  interval time     
time tidiness achieved  length time mail sits mail room
robot picks up  on  additional information could reestablish time separable value function  expense increase number
states ad hoc cumbersome action representation   

    horizons success criteria

order evaluate particular course action  need specify long  in
many stages  executed  known problem s horizon  finite horizon
problems  agent s performance evaluated fixed  finite number stages  
commonly  aim maximize total expected reward associated course
action  therefore define  finite horizon  value length history h  bellman 
      

v  h   

tx
  
t  

fr st     c  st   g   r st  

infinite horizon problem  hand  requires agent s performance
evaluated infinite trajectory  case total reward may unbounded 
meaning policy could arbitrarily good bad executed long enough 
case may necessary adopt different means evaluating trajectory 
common introduce discount factor  ensuring rewards costs accrued
later stages counted less accrued earlier stages  value function
expected total discounted reward problem defined follows  bellman        howard 
      
 
x
v  h     r st    c  st    
t  

fixed discount rate          formulation particularly simple
elegant way ensure bounded measure value infinite horizon  though
important verify discounting fact appropriate  economic justifications often
provided discounted models a reward earned sooner worth one earned
later provided reward somehow invested  discounting suitable
modeling process terminates probability     point time  e g  
robot break down   case discounted models correspond expected total
reward finite uncertain horizon  reasons  discounting sometimes used
finite horizon problems well 
another technique dealing infinite horizon problems evaluate trajectory
based average reward accrued per stage  gain  gain history defined
n
x
g h    lim   fr st     c  st    g
n   n t  

    see  bacchus  boutilier    grove               however  systematic approach handling certain
types history dependent reward functions 

  

fiboutilier  dean    hanks

refinements criterion proposed  puterman        
sometimes problem ensures total reward infinite trajectory
bounded  thus expected total reward criterion well defined  consider case
common ai planners agent s task bring system goal state 
positive reward received goal reached  actions incur non negative
cost  goal reached system enters absorbing state
rewards costs accrued  long goal reached certainty 
situation formulated infinite horizon problem total reward bounded
desired trajectory  bertsekas        puterman         general  problems
cannot formulated  fixed  finite horizon problems unless priori bound
number steps needed reach goal established  problems sometimes
called indefinite horizon problems  practical point view  agent continue
execute actions finite number stages  exact number cannot determined
ahead time 

    solution criteria

complete definition planning problem need specify constitutes
solution problem  see split explicit mdp formulations
work ai planning community  classical mdp problems generally stated
optimization problems  given value function  horizon  evaluation metric  e g  
expected total reward  expected total discounted reward  expected average reward per stage 
agent seeks behavioral policy maximizes objective function 
work ai often seeks satisficing solutions problems  planning literature 
generally taken plan satisfies goal equally preferred
plan satisfies goal  plan satisfies goal preferable
plan not    probabilistic framework  might seek plan satisfies
goal maximum probability  an optimization   lead situations
optimal plan infinite length system state fully observable 
satisficing alternative  kushmerick  hanks    weld        seek plan satisfies
goal probability exceeding given threshold 

example     extend running example demonstrate infinite horizon  fully
observable  discounted reward situation  begin adding one new dimension
state description  boolean variable rhm  does robot mail   giving us
system    states  provide agent two additional actions  pum
 pickup mail  delm  deliver mail  described figure    reward
agent way mail delivery encouraged  associate reward
   state rhm false   states 
actions cost  agent gets total reward    six stage system
trajectory 
hloc m    m  rhmi  stay  hloc m    m  rhmi  pum  hloc m    m  rhmi 
clk  hloc h    m  rhmi  clk  hloc o   m  rhmi  delm  hloc o   m  rhmi

    though see  haddawy   hanks        williamson   hanks        restatement planning
optimization problem 

  

fidecision theoretic planning  structural assumptions

assign action cost    action except stay  which   cost  
total reward becomes     use discount rate     discount future
rewards costs  initial segment infinite horizon history would contribute
                                                                      total
value trajectory  as subsequently extended   furthermore  establish
bound total expected value trajectory  best case  subsequent
stages yield reward     expected total discounted reward bounded

 
x
                                                      i     
i  

similar effect behavior achieved penalizing states  i e   negative
rewards  either rhm true   

    policies

mentioned policies  or courses action  plans  informally point 
provide precise definition  decision problem facing agent viewed
generally deciding action perform given current observable history 
define policy mapping set observable histories ho actions 
is    ho   a  intuitively  agent executes action

   hho    a  i          hot     at   i  ot i 
stage performed actions a    at   made observations o    ot  
earlier stages  made observation ot current stage 
policy induces distribution pr hj  set system histories hs   probability distribution depends initial distribution p     define expected value
policy be 
x
ev    
v h  pr hj 
h hs

would agent adopt policy either maximizes expected value or 
satisficing context  acceptably high expected value 
general form policy  depending arbitrary observation history 
lead complicated policies policy construction algorithms  special cases 
however  assumptions observability structure value function result
optimal policies much simpler form 
case fully observable mdp time separable value function  optimal
action stage computed using information current state
stage  is  restrict policies simpler form     without
danger acting suboptimally  due fact full observability allows state
observed completely  markov assumption renders prior history irrelevant 
non observable case  observational history contains vacuous observations
agent must choose actions using knowledge previous actions
stage  however  since incorporates previous actions  takes form     a 
  

fiboutilier  dean    hanks

form policy corresponds linear  unconditional sequence actions ha    a            i 
straight line plan ai nomenclature   

     model summary  assumptions  problems  computational
complexity

concludes exposition mdp model planning uncertainty  generality allows us capture wide variety problem classes currently
studied literature  section review basic components model 
describe problems commonly studied dtp literature respect model 
summarize known complexity results each  section    describe specialized computational techniques used solve problems problem classes 
       model summary assumptions

mdp model consists following components 
state space   finite countable set states  generally make markov
assumption  requires state convey information necessary predict
effects actions events independent information
system history 
set actions a  action ak represented transition matrix size
js jjs j representing probability pkij performing action ak state si move
system state sj   assume throughout action model stationary 
meaning transition probabilities vary time  transition matrix
action generally assumed account exogenous events might
occur stage action executed 
set observation variables o  set  messages  sent agent
action performed  provide execution time information current
system state  action ak pair states si   sj   pkij     
associate distribution possible observations  pkm
ij denotes probability
obtaining observation om given action ak taken si resulted
transition state sj  
value function v   value function maps state history real number
v  h    v  h    case agent considers history h  least good
h    state history records progression states system assumes along
actions performed  assumptions time separability additivity
common v   particular  generally use reward function r cost function
c defining value 
horizon   number stages state histories
evaluated using v  
    many algorithms ai literature produce partially ordered sequence actions  plans
not  however  involve conditional nondeterministic execution  rather  represent fact
linear sequence consistent partial order solve problem  thus  partially ordered
plan concise representation particular set straight line plans 

  

fidecision theoretic planning  structural assumptions

optimality criterion  provides criterion evaluating potential solutions
planning problems 

       common planning problems

use general framework classify various problems commonly studied
planning decision making literature  case below  note modeling assumptions define problem class 

planning problems or decision sciences tradition
fully observable markov decision processes  fomdps    ex 

tremely large body research studying fomdps  present basic algorithmic techniques detail next section  commonly used formulation fomdps assumes full observability stationarity  uses optimality
criterion maximization expected total reward finite horizon  maximization expected total discounted reward infinite horizon  minimization
expected cost goal state 
fomdps introduced bellman        studied depth
fields decision analysis or  including seminal work howard         recent texts fomdps include  bertsekas         puterman         average reward optimality received attention literature  blackwell        howard 
      puterman         ai literature  discounted total reward models
popular well  barto et al         dearden   boutilier        dean  kaelbling  kirman    nicholson        koenig         though average reward criterion
proposed suitable modeling ai planning problems  boutilier  
puterman        mahadevan        schwartz        

partially observable markov decision processes  pomdps    pomdps

closer fomdps general model decision processes described 
pomdps generally studied assumption stationarity optimality criteria identical fomdps  though average reward criterion
widely considered  discuss below  pomdp viewed
fomdp state space consisting set probability distributions
  probability distributions represent states belief  agent  observe 
state belief system although exact knowledge
system state itself 
pomdps widely studied control theory  astrom        lovejoy      b  smallwood   sondik        sondik         drawn increasing
attention ai circles  cassandra  kaelbling    littman        hauskrecht       
littman        parr   russell        simmons   koenig        thrun  fox    burgard        zhang   liu         uence diagrams  howard   matheson       
shachter        popular model decision making ai are  fact 
structured representational method pomdps  see section      

planning problems ai tradition
  

fiboutilier  dean    hanks

classical deterministic planning   classical ai planning model assumes

deterministic actions  action ak taken state si one successor sj  
important assumptions non observability value determined
reaching goal state  plan leads goal state preferred
not  often preference shorter plans  represented
using discount factor  encourage  faster goal achievement assigning
cost actions  reward associated transitions goal states 
absorbing  action costs typically ignored  except noted above 
classical models usually assumed initial state known certainty 
contrasts general specification mdps above  assume
knowledge even distributional information initial state  policies
defined applicable matter state  or distribution states  one finds
oneself in action choices defined every possible state history  knowledge
initial state determinism allow optimal straight line plans constructed 
loss value associated non observability  unpredictable exogenous
events uncertain action effects cannot modeled consistently assumptions adopted 
overview early classical planning research variety approaches
adopted  see  allen  hendler    tate        well yang s        recent text 

optimal deterministic planning   separate body work retains classical

assumptions complete information determinism  tries recast planning
problem optimization relaxes implicit assumption  achieve goal
costs   time  methods use sort representations
algorithms applied satisficing planning 
haddawy hanks        present multi attribute utility model planners
keeps explicit information initial state goals  allows preferences stated partial satisfaction goals well cost
resources consumed satisfying them  model allows expression preferences phenomena temporal deadlines maintenance intervals
dicult capture using time separable additive value function  williamson       
 see williamson   hanks         implements model extending classical planning algorithm solve resulting optimization problem  haddawy
suwandi        implement model complete decision theoretic framework 
model planning  refinement planning  differs somewhat generative
model discussed paper  model set possible plans pre stored
abstraction hierarchy  problem solver s job find hierarchy
optimal choice concrete actions particular problem 
perez carbonell s        work incorporates cost information classical
planning framework  maintains split classical satisficing planner
additional cost information provided utility model  cost information
used learn search control rules allow classical planner generate low cost
goal satisfying plans 
  

fidecision theoretic planning  structural assumptions

conditional deterministic planning   classical planning assumption

omniscience relaxed somewhat allowing state aspects
world unknown  agent thus situation certain
system one particular set states  know one  unknown
truth values included initial state specification  taking actions
cause proposition become unknown well 
actions provide agent information plan executed  conditional planners introduce idea actions providing runtime information
prevailing state  distinguishing action makes proposition p true
action tell agent whether p true action executed 
action causal informational effects  simultaneously changing
world reporting value one propositions  second sort
information useful planning time except allows steps plan
executed conditionally  depending runtime information provided prior
information producing steps  value actions lies fact different
courses action may appropriate different conditions these informational
effects allow runtime selection actions based observations produced  much
general pomdp model 
examples conditional planners classical framework include early work
warren        recent cnlp  peot   smith         cassandra  pryor
  collins         plynth  goldman   boddy         uwl  etzioni  hanks 
weld  draper  lesh    williamson        systems 

probabilistic planning without feedback   direct probabilistic extension

classical planning problem stated follows  kushmerick et al         
take input  a  probability distribution initial states   b  stochastic actions
 explicit implicit transition matrices    c  set goal states   d  probability
success threshold   objective produce plan reaches goal state
probability least   given initial state distribution  provision made
execution time observation  thus straight line plans form policy
possible  restricted case infinite horizon nomdp problem  one
actions incur cost goal states offer positive reward absorbing 
special case objective find satisficing policy rather
optimal one 

probabilistic planning feedback   draper et al       a  proposed

extension probabilistic planning problem actions provide feedback 
using exactly observation model described section      again  problem
posed building plan leaves system goal state sucient
probability  plan longer simple sequence actions it contain conditionals loops whose execution depends observations generated sensing
actions  problem restricted case general pomdp problem  absorbing goal states cost free actions used  objective find policy
 conditional plan  leaves system goal state sucient probability 
  

fiboutilier  dean    hanks

comparing frameworks  task oriented versus process oriented problems

useful point pause contrast types problems considered classical planning literature typically studied within mdp framework  although
problems ai planning literature emphasized goal pursuit  one shot  view
problem solving  cases viewing problem infinite horizon decision problem
results satisfying formulation  consider running example involving oce
robot  simply possible model problem responding coffee requests  mail
arrival keeping lab tidy strict goal satisfaction problem capturing
possible nuances intuitively optimal behavior 
primary diculty explicit persistent goal states exist 
simply require robot attain state lab tidy  mail awaits 
unfilled coffee requests exist   successful  plan could anticipate possible system behavior
goal state reached  possible occurrence exogenous events goal
achievement requires robot bias methods achieving goals way
best suits expected course subsequent events  instance  coffee requests
likely point time unmet requests highly penalized  robot
situate coffee room order satisfy anticipated future request quickly 
realistic decision scenarios involve task oriented process oriented behavior 
problem formulations take account provide satisfying models
wider range situations 
       complexity policy construction

defined planning problem several different ways  different
set assumptions state space  system dynamics actions  deterministic
stochastic   observability  full  partial  none   value function  time separable  goal only 
goal rewards action costs  partially satisfiable goals temporal deadlines   planning
horizon  finite  infinite  indefinite   optimality criterion  optimal satisficing solutions   set assumptions puts corresponding problem particular complexity
class  defines worst case time space bounds representation algorithm
solving problem  summarize known complexity results
problem classes defined above 
fully observable markov decision processes fully observable mdps  fomdps 
time separable  additive value functions solved time polynomial size
state space  number actions  size inputs    common algorithms solving fomdps value iteration policy iteration 
described next section  finite horizon discounted infinite horizon problems
require polynomial amount computation per iteration o js j  jaj  o js j  jaj js j    
respectively and converge polynomial number iterations  with factor    
discounted case   hand  problems shown p complete
 papadimitriou   tsitsiklis         means ecient parallel solution algorithm
unlikely    space required store policy n stage finite horizon problem
    precisely  maximum number bits required represent transition probabilities
costs 
    see  littman  dean    kaelbling        summary complexity results 

  

fidecision theoretic planning  structural assumptions

o js jn   interesting classes infinite horizon problems  specifically involving discounted models time separable additive reward  optimal policy
shown stationary  policy stored o js j  space 
bear mind worst case bounds  many cases  better time bounds
compact representations found  sections     explore ways represent
solve problems eciently 
partially observable markov decision processes pomdps notorious
computational diculty  mentioned above  pomdp viewed fomdp
infinite state space consisting probability distributions   distribution
representing agent s state belief point time  astrom        smallwood  
sondik         problem finding optimal policy pomdp objective
maximizing expected total reward expected total discounted reward finite
horizon shown exponentially hard js j  papadimitriou
  tsitsiklis         problem finding policy maximizes approximately
maximizes expected discounted total reward infinite horizon shown
undecidable  madani  condon    hanks        
even restricted cases pomdp problem computationally dicult worst
case  littman        considers special case boolean rewards  determining whether
infinite horizon policy nonzero total reward given rewards associated states non negative  shows problem exptime complete
transitions stochastic  pspace hard transitions deterministic 
deterministic planning recall classical planning problem defined quite
differently mdp problems above  agent ability observe state
perfect predictive powers  knowing initial state effects actions
certainty  addition  rewards come reaching goal state  plan
achieves goal suces 
planning problems typically defined terms set p boolean features
propositions  complete assignment truth values features describes exactly one state 
partial assignment truth values describes set states  set propositions p
induces state space size  jpj  thus  space required represent planning problem
using feature based representation exponentially smaller required
representation problem  see section    
ability represent planning problems compactly dramatic impact worstcase complexity  bylander        shows deterministic planning problem without
observation pspace complete  roughly speaking  means worst planning
time increase exponentially p a  further  size solution plan
grow exponentially problem size  results hold even action
space severely restricted  example  planning problem np complete even
cases action restricted one precondition feature one postcondition
feature  conditional optimal planning pspace complete well  results
inputs generally compact  generally exponentially so 
terms complexity fomdp pomdp problems phrased 
probabilistic planning probabilistic goal oriented planning  pomdps 
typically search solution space probability distributions states  or
  

fiboutilier  dean    hanks

formulas describe states   even simplest problem probabilistic planning one
admits observability is undecidable worst  madani et al          intuition
even though set states finite  set distributions states not 
worst agent may search infinite number plans able
determine whether solution exists  algorithm guaranteed find
solution plan eventually one exists  cannot guaranteed terminate finite time
solution plan  conditional probabilistic planning generalization
non observable probabilistic planning problem  thus undecidable well 
interesting note connection conditional probabilistic planning
pomdps  actions observations two problems equivalent expressive
power  reward structure conditional probabilistic planning problem quite
restrictive  goal states positive rewards  states reward  goal states
absorbing  since cannot put priori bound length solution plan 
conditional probabilistic planning must viewed infinite horizon problem
objective maximize total expected undiscounted reward  note  however  since goal
states absorbing  guarantee total expected reward non negative
bounded  even infinite horizon  technically means conditional probabilistic planning problem restricted case infinite horizon positive bounded problem
 puterman        section       therefore conclude problem solving
arbitrary infinite horizon undiscounted positive bounded pomdp undecidable 
commonly studied problem infinite horizon pomdp criterion maximizing expected discounted total reward  finding optimal near optimal solutions
problem undecidable  noted above 
       conclusion

end section noting results algorithm independent describe worst case behavior  effect  indicate badly algorithm made
perform  arbitrarily unfortunate  problem instance  interesting question
whether build representations  techniques  algorithms typically perform
well problem instances typically arise practice  concern leads us examine
problem characteristics eye toward exploiting restrictions placed
states actions  observability  value function optimality criterion 
begin algorithmic techniques focus value function particularly
take advantage time separability goal orientation  following section
explore complementary techniques building compact problem representations 

   solution algorithms  dynamic programming search
section review standard algorithms solving problems described
terms  unstructured    at  problem representations  noted analysis
above  fully observable markov decision processes  fomdps  far widely
studied models general class stochastic sequential decision problems  begin
describing techniques solving fomdps  focusing techniques exploit structure
value function time separability additivity 
  

fidecision theoretic planning  structural assumptions

    dynamic programming approaches

suppose given fully observable mdp time separable  additive value function 
words  given state space   action space a  transition matrix pr s  js  a 
action a  reward function r  cost function c   start problem
finding policy maximizes expected total reward fixed  finite horizon  
suppose given policy  s  t  action performed agent
state stages remaining act  for        bellman        shows
expected value policy state computed using set t stage to go
value functions vt   define v   s  r s   define 

vt  s    r s    c   s  t    

x

 s
 

fpr s j s  t   s vt    s  g

   

definition value function makes its  dependence initial state clear 
say policy optimal vt  s  vt  s  policies      
optimal  stage to go value function  denoted vt   simply value function
optimal  horizon policy  bellman s principle optimality  bellman        forms basis
stochastic dynamic programming algorithms used solve mdps  establishing
following relationship optimal value function tth stage optimal value
function previous stage 

vt  s    r s    max
fc  a   
a a

x

s   s

pr s  ja  s vt    s   g

   

      value iteration

equation   forms basis value iteration algorithm finite horizon problems 
value iteration begins value function v    r  uses equation   compute
sequence value functions longer time intervals  horizon   action
maximizes right hand side equation   chosen policy element  s  t  
resulting policy optimal  stage  fully observable mdp  indeed
shorter horizon    
important note policy describes done every stage
every state system  even agent cannot reach certain states given system s
initial configuration available actions  return point below 

example     consider simplified version robot example  four

state variables   cr  rhc  rhm  movement various locations ignored  
four actions getc  pum  delc  delm  actions getc pum make rhc
rhm  respectively  true certainty  action delm  rhm holds  makes
rhm false probability      delc makes cr rhc false
probability      leaving state unchanged probability      reward  
associated cr reward   associated   reward state
sum rewards objective satisfied state  figure    shows
optimal   stage    stage   stage value functions various states  along

    recall fomdps aspects history relevant 

  

fiboutilier  dean    hanks

state
s    hm  rhm  cr  rhci
s    hm  rhm  cr  rhci
s    hm  rhm  cr  rhci
s    hm  rhm  cr  rhci
s    hm  cr  rhci
s    hm  cr  rhci
s    hm  rhm  cri
s    hm  rhm  cri
s    hm  cri

v 

 
 
 
 
 
 
 
 
 

v 

 
 
   
 
   
 
 
 
 

   

   

  pum
delm   delm
delc      delc
delm     delm
delc      delc
    getc
delm    delm
   pum
  
v 

figure     finite horizon optimal value policy 
optimal choice action state stage pairing  the values  state 
missing variables hold instantiations variables   note v   s 
simply r s  state s 
illustrate application equation    first consider calculation v   s    
robot choice delivering coffee delivering mail  expected value
option  one stage remaining  given by 
ev   s   delc       v  s        v   s        
ev  s   delm   
   v   s   
     
thus  s         delm v   s    value maximizing choice  notice
robot one action perform aim  lesser  objective due
risk failure inherent delivering coffee  two stages remaining
state  robot deliver mail  certainty move s  one
stage go  attempt deliver coffee    s         delc  
illustrate effects fixed finite horizon policy choice  note
 s        pum  two stages remaining choice getting mail coffee 
robot get mail subsequent delivery  in last stage  guaranteed
succeed  whereas subsequent coffee delivery may fail  however  compute
 s       see 
ev  s    getc       v  s          
ev   s   pum       v  s         
three stages go  robot instead retrieve coffee s   
coffee  two chances successful delivery  expected value course
action greater  guaranteed  mail delivery  note three stages
allow sucient time try achieve objectives s    fact 
larger reward associated coffee delivery ensures greater number
stages remaining  robot focus first coffee retrieval delivery 
attempt mail retrieval delivery coffee delivery successfully completed   
often faced tasks fixed finite horizon  example 
may want robot perform tasks keeping lab tidy  picking mail whenever
  

fidecision theoretic planning  structural assumptions

arrives  responding coffee requests  on  fixed time horizon associated
tasks they performed need arises  problems best
modeled infinite horizon problems 
consider problem building policy maximizes discounted sum
expected rewards infinite horizon    howard        showed always
exists optimal stationary policy problems  intuitively  case
matter stage process in  still infinite number stages remaining 
optimal action state independent stage  therefore restrict
attention policies choose action state regardless stage
process  restriction  policy size jsj regardless
number stages policy executed the policy form     a 
contrast  optimal policies finite horizon problems generally nonstationary 
illustrated example     
howard shows value policy satisfies following recurrence 

v  s    r s    fc   s    

x

s   s

optimal value function v satisfies 

v  s    r s    max
fc  a   
a a

pr s  j s   s v  s   g

x
 s
 

pr s  ja  s v  s   g

   
   

value fixed policy evaluated using method successive approximation  almost identical procedure described equation   above  begin
arbitrary assignment values v   s   define 

vt  s    r s    c   s  t    

x

 s
 

fpr s j s  t   s vt    s  g

   

sequence functions vt converges linearly true value function v  
one alter value iteration algorithm slightly builds optimal policies
infinite horizon discounted problems  algorithm starts value function v 
assigns arbitrary value     given value estimate vt  s  state s  vt    s 
calculated as 

vt    s    r s    max
fc  a   
a a

x

s   s

pr s  ja  s  vt  s   g

   

sequence functions vt converges linearly optimal value function v  s  
finite number iterations n  choice maximizing action forms
optimal policy vn approximates value   
    far commonly studied problem literature  though argued  boutilier  
puterman        mahadevan        schwartz        problems often best modeled using
average reward per stage optimality criterion  discussion average reward optimality
many variants refinements  see  puterman        
    number iterations n based stopping criterion generally involves measuring difference vt vt     discussion stopping criteria convergence algorithm  see
 puterman        

  

fiboutilier  dean    hanks

      policy iteration

howard s        policy iteration algorithm alternative value iteration infinitehorizon problems  rather iteratively improving estimated value function  instead
modifies policies directly  begins arbitrary policy     iterates  computing
i   i 
iteration algorithm comprises two steps  policy evaluation policy improvement 
    policy evaluation      compute value function v  s  based
current policy  
    policy improvement      find action maximizes

qi   a  s    r s    c  a   

x

s   s

pr s  ja  s  v  s   

   

qi    a   s    v  s   let i       otherwise i    s     s    
algorithm iterates i    s     s  states s  step   evaluates current
policy solving n n linear system represented equation    one equation
     computationally expensive  however  algorithm converges
optimal policy least linearly certain conditions converges superlinearly
quadratically  puterman         practice  policy iteration tends converge many
fewer iterations value iteration  policy iteration thus spends computational
time individual stage  result fewer stages need computed   
modified policy iteration  puterman   shin        provides middle ground
policy iteration value iteration  structure algorithm exactly
policy iteration  alternating evaluation improvement phases  key insight
one need evaluate policy exactly order improve it  therefore  evaluation
phase involves  usually small  number iterations successive approximation  i e  
setting v   vt small t  using equation     tuning value
used iteration  modified policy iteration work extremely well practice
 puterman         value iteration policy iteration special cases modified
policy iteration  corresponding setting          respectively 
number variants value policy iteration proposed 
instance  asynchronous versions algorithms require value function
constructed  policy improved  state lockstep  case value iteration
infinite horizon problems  long state updated suciently often  convergence
assured  similar guarantees provided asynchronous forms policy iteration  variants important tools understanding various online approaches
solving mdps  bertsekas   tsitsiklis         nice discussion asynchronous dynamic
programming  see  bertsekas        bertsekas   tsitsiklis        
    q function defined equation    called use q learning  watkins   dayan 
       gives value performing action state assuming value function v accurately ects
future value 
    see  littman et al         discussion complexity algorithm 

  

fidecision theoretic planning  structural assumptions

      undiscounted infinite horizon problems

diculty finding optimal solutions infinite horizon problems total reward
grow without limit time  thus  problem definition must provide way
ensure value metric bounded arbitrarily long horizons  use expected
total discounted reward optimality criterion offers particularly elegant way
guarantee bound  since infinite sum discounted rewards finite  however  although
discounting appropriate certain classes problems  e g   economic problems 
system may terminate point certain probability   many realistic
ai domains dicult justify counting future rewards less present rewards 
discounted reward criterion appropriate 
variety ways bound total reward undiscounted problems 
cases problem structured reward bounded  planning problems 
example  goal reward collected once  actions incur cost 
case total reward bounded problem legitimately posed
terms maximizing total expected undiscounted reward many cases  e g   goal
reached certainty  
cases discounting inappropriate total reward unbounded  different
success criteria employed  example  problem instead posed one
wish maximize expected average reward per stage  gain  computational
techniques constructing gain optimal policies similar dynamic programming
algorithms described above  generally complicated  convergence rate
tends quite sensitive communicating structure periodicity mdp 
refinements gain optimality studied  example  bias optimality
used distinguish two gain optimal polices giving preference policy whose total
reward initial segment policy execution larger  again  algorithms
complicated discounted problems  variants standard
policy value iteration  see  puterman        details 
      dynamic programming pomdps

dynamic programming techniques applied partially observable settings well
 smallwood   sondik         main diculty building policies situations
state fully observable that  since past observations provide information
system s current state  decisions must based information gleaned
past  result  optimal policy depend observations agent made since
beginning execution  history dependent policies grow size exponential
length horizon  history dependence precludes dynamic programming 
observable history summarized adequately probability distribution
 astrom         policies computed function distributions  belief
states 
key observation sondik  smallwood   sondik        sondik       
one views pomdp time separable value function taking state space
set probability distributions   one obtains fully observable mdp
solved dynamic programming   computational  problem approach
  

fiboutilier  dean    hanks

state space fomdp n  dimensional continuous space    special
techniques must used solve  smallwood   sondik        sondik        
explore techniques here  note currently practical
small problems  cassandra et al         cassandra  littman    zhang       
littman        lovejoy      b   number approximation methods  developed
 lovejoy      a  white iii   scherer        ai  brafman        hauskrecht       
parr   russell        zhang   liu         used increase range solvable
problems  even techniques presently limited practical value 
pomdps play key role reinforcement learning well   natural state
space  consisting agent observations provides incomplete information underlying system state  see  e g   mccallum        

    ai planning state based search
noted section     classical ai planning problem formulated
infinite horizon mdp therefore solved using algorithm value iteration 
recall two assumptions classical planning specialize general mdp model  namely
determinism actions use goal states instead general reward function 
third assumption that want construct optimal course action starting
known initial state does counterpart fomdp model presented above 
since policy dictates optimal action state stage plan 
see below  interest online algorithms within ai led revised formulations
fomdps take initial current states account 
though defined classical planning problem earlier non observable process
 nomdp   solved fully observable  let g set goal states
sinit initial state  applying value iteration type problem equivalent
determining reachability goal states system states  instance 
make goal states absorbing  assign reward   transitions     g
g   g   others  set states vk  s      exactly
set states lead goal state    particular  vk  sinit       
successful plan constructed extracting actions k stage  finite horizon 
policy produced value iteration  determinism assumption means agent
predict state perfectly every stage execution  fact cannot observe
state unimportant 
assumptions commonly made classical planning exploited computationally value iteration  first  terminate process first iteration k
vk  sinit       interested plans begin sinit  acting
optimally every possible start state  second  terminate value iteration js j
iterations  vjs j sinit       point  algorithm searched every possible
state guarantee solution plan exists  therefore  view classical planning finite horizon decision problem horizon js j  use value iteration
    accurately  n  dimensional simplex   n      dimensional space 
    specifically  vk  s  indicates probability one reaches goal region optimal
policy     g stochastic settings  deterministic case discussed  value must
    

  

fidecision theoretic planning  structural assumptions

equivalent using floyd warshall algorithm find minimum cost path
weighted graph  floyd        
      planning search

value iteration can  theory  used classical planning  take advantage
fact goal initial states known  particular  computes value
policy assignment states stages  wasteful since optimal
actions computed states cannot reached sinit cannot possibly
lead state g   g  problematic js j large  since iteration
value iteration requires o js jjaj  computations  reason dynamic programming
approaches used extensively ai planning 
restricted form value function  especially fact initial goal states
given  makes advantageous view planning graph search problem  unlike
general fomdps  generally known priori states desirable
respect  long term  value  well defined set target states classical planning
problem makes search based algorithms appropriate  approach taken
ai planning algorithms 
one way formulate problem graph search make node graph
correspond state   initial state goal states identified 
search proceed either forward backward graph  directions
simultaneously 
forward search  initial state root search tree  node chosen
tree s fringe  the set leaf nodes   feasible actions applied 
action application extends plan one step  or one stage  generates unique new
successor state  new leaf node tree  node pruned state
defines already tree  search ends state identified member
goal set  in case solution plan extracted tree   branches
pruned  in case solution plan exists   forward search attempts build
plan beginning end  adding actions end current sequence actions 
forward search never considers states cannot reached sinit  
backward search viewed several different ways  could arbitrarily select
g   g root search tree  expand search tree fringe
selecting state fringe adding tree states action would
cause system enter chosen state  general  action give rise
one predecessor vertex  even actions deterministic  state pruned
appears search tree already  search terminates initial state added
tree  solution plan extracted tree  search similar
dynamic programming based algorithms finding shortest path graph 
difference backward search considers states depth k search
tree reach chosen goal state within k steps  dynamic programming algorithms 
contrast  visit every state every stage search 
one diculty backward approach described commitment
particular goal state  course  assumption relaxed  algorithm could
 simultaneously  search paths goal states adding first level search
  

fiboutilier  dean    hanks

tree vertex reach g   g  see section   goal regression
viewed this  least implicitly 
generally thought regression  or backward  techniques effective
practice progression  or forward  methods  reasoning branching factor
forward graph  number actions feasibly applied given
state  substantially larger branching factor reverse graph 
number operators could bring system given state    especially true
goal sets represented small set propositional literals  section     two
approaches mutually exclusive  however  one mix forward backward expansions underlying problem graph terminate forward path backward
path meet 
important thing observe algorithms restrict attention relevant reachable states  forward search  states
reached sinit ever considered  provide benefit dynamic programming
methods states reachable  since unreachable states cannot play role constructing successful plan  backward approaches  similarly  states lying path
goal region g considered  significant advantages dynamic
programming fraction state space connected goal region 
contrast  dynamic programming methods  with exception asynchronous methods  must examine entire state space every iteration  course  ability ignore
parts state space comes planning s stringent definition relevant  states
g positive reward  states matter except extent move agent
closer goal  choice action states unreachable sinit interest 
state based search techniques use knowledge specific initial state specific
goal set constrain search process  forward search exploit knowledge
goal set  backward search exploit knowledge initial state  graphplan
algorithm  blum   furst        viewed planning method integrates
propagation forward reachability constraints backward goal informed search 
describe approach section    furthermore  work partial order planning  pop 
viewed slightly different approach form search  described
section    discuss feature based intensional representations mdps
planning problems 
      decision trees real time dynamic programming

state based search techniques limited deterministic  goal oriented domains  knowledge initial state exploited general mdps well  forming basis
decision tree search algorithms  assume given finite horizon fomdp
horizon initial state sinit   decision tree rooted sinit constructed much
way search tree deterministic planning problem  french         action
applicable sinit forms level   tree  states s  result positive probability actions occur applied sinit placed level    arc
    see bacchus et al               recent work makes case progression good
search control  bonet et al         argue progression deterministic planning useful
integrating planning execution 

  

fidecision theoretic planning  structural assumptions

init
v   max v    v   
a 
p 

a 
p 

s 
a 

p 

s 
a 

a 

v 
p 

s 
a 

a 

v    p v     p v 
 
 

s 
a 

a 

a 

v 

v 

figure     initial stages decision tree evaluating action choices sinit  
value action expected value successor states  value
state maximum values successor actions  as indicated
dashed arrows selected nodes  
labeled probability pr s  ja  sinit   relating s  a  level   actions applicable
states level    on  tree grown depth  t   point
branch tree path consisting positive probability length t trajectory rooted
sinit  see figure     
relevant part optimal  stage value function optimal policy easily
computed using tree  say value node tree labeled
action expected value successor states tree  using probabilities labeling
arcs   value node tree labeled state sum r s 
maximum value successor actions    rollback procedure  whereby value
leaves tree first computed values successively higher levels tree
determined using preceding values  is  fact  form value iteration  value
state level  t precisely vt t  s  maximizing actions form optimal
finite horizon policy  form value iteration directed   t   t  stage to go values
computed states reachable sinit within steps  infinite horizon
problems solved analogous fashion one determine priori depth
required  i e   number iterations value iteration needed  ensure convergence
optimal policy 
unfortunately  branching factor stochastic problems generally much greater
deterministic problems  troublesome still fact one must
construct entire decision tree sure proper values computed  hence
optimal policy constructed  stands contrast classical planning search 
attention focused single branch  goal state reached  path
constructed determines satisfactory plan  worst case behavior planning may
require searching whole tree  decision tree evaluation especially problematic
    states level  t given value r s  

  

fiboutilier  dean    hanks

entire tree must generated general ensure optimal behavior  furthermore 
infinite horizon problems pose diculty determining suciently deep tree 
one way around diculty use real time search  korf         particular 
real time dynamic programming  rtdp  proposed  barto et al        
way approximately solving large mdps online fashion  one interleave search
execution approximately optimal policy using form rtdp similar decisiontree evaluation follows  imagine agent finds particular state sinit  
build partial search tree depth  perhaps uniformly perhaps
branches expanded deeply others  partial tree construction may halted due
time pressure due assessment agent expansion tree may
fruitful  decision act must made  rollback procedure applied
partial  possibly unevenly expanded decision tree 
reward values used evaluate leaves tree  may offer
inaccurate picture value nodes higher tree  heuristic information
used estimate long term value states labeling leaves  value iteration 
deeper tree  accurate estimated value root  generally speaking 
fixed heuristic  see section   structured representations mdps
provide means construct heuristics  dearden   boutilier               specifically 
admissible heuristics upper lower bounds true values leaf nodes
tree  methods a  branch and bound search used 
key advantage integrating search execution actual outcome
action taken used prune tree branches rooted unrealized
outcomes  subtree rooted realized state expanded make
next action choice  algorithm hansen zilberstein        viewed
variant methods stationary policies  i e   state action mappings 
extracted search process 
rtdp formulated barto et al         generally form online  asynchronous value iteration  specifically  values  rolled backed  cached used
improved heuristic estimates value function states question  technique investigated  bonet et al         dearden   boutilier              koenig
  simmons         closely tied korf s        lrta  algorithm  value
updates need proceed strictly using decision tree determine states  key
requirement rtdp simply actual state sinit one states whose value
updated decision action iteration 
second way avoid computational diculties arise large search
spaces use sampling methods  methods sample space possible trajectories use sampled information provide estimates values specific courses
action  approach quite common reinforcement learning  sutton   barto        
simulation models often used generate experience value function
learned  present context  kearns  mansour ng  kearns  mansour   
ng        investigated search methods infinite horizon mdps successor
states specific action randomly sampled according transition distribution 
thus  rather expand successor states  sampled states searched  though
method exponential  effective  horizon  or mixing rate  mdp
required expand actions  number states expanded less required
  

fidecision theoretic planning  structural assumptions

full search  even underlying transition graph sparse  able provide polynomial bounds  ignoring action branching horizon effects  number
trajectories need sampled order generate approximately optimal behavior
high probability 

    summary

seen dynamic programming methods state based search methods
used fully observable non observable mdps  forward search methods interpretable  directed  forms value iteration  dynamic programming algorithms
generally require explicit enumeration state space iteration  search
techniques enumerate reachable states  branching factor may require that 
sucient depth search tree  search methods enumerate individual states multiple
times  whereas considered per stage dynamic programming  overcoming diculty search requires use cycle checking multiple path checking
methods 
note search techniques applied partially observable problems well 
one way search space belief states  just dynamic programming applied belief space mdp see section          specifically  belief
states play role system states stochastic effects actions belief states
induced specific observation probabilities  since observation distinct  fixed
effect belief state  type approach pursued  bonet   geffner 
      koenig   simmons        

   factored representations

point discussion mdps used explicit extensional representation
set states  and actions  states enumerated directly  many cases
advantageous  representational computational point view  talk
properties states sets states  set possible initial states  set
states action executed  on  generally convenient
compact describe sets states based certain properties features enumerate
explicitly  representations descriptions objects substitute objects
called intensional technique choice ai systems 
intensional representation planning systems often built defining set
features sucient describe state dynamic system interest 
example figure    state described set six features  robot s location 
lab s tidiness  whether mail present  whether robot mail  whether
pending coffee request  whether robot coffee  first
second features take one five values  last four take one two
values  true false   assignment values six features completely defines state 
state space thus comprises possible combinations feature values  jsj       
feature  factor  typically assigned unique symbolic name  indicated
second column figure    fundamental tradeoff extensional intensional
representations becomes clear example  extensional representation coffee
example views space possible states single variable takes     possible
  

fiboutilier  dean    hanks

values  whereas intensional factored representation views state cross product
six variables  takes substantially fewer values  generally  state space
grows exponentially number features required describe system 
fact state system described using set features allows one
adopt factored representations actions  rewards components mdp 
factored action representation  instance  one generally describes effect action
specific state features rather entire states  often provides considerable representational economy  instance  strips action representation  fikes   nilsson 
       state transitions induced actions represented implicitly describing
effects actions features change value action executed 
factored representations compact individual actions affect relatively
features  effects exhibit certain regularities  similar remarks apply
representation reward functions  observation models  on  regularities
make factored representations suitable many planning problems often exploited
planning decision making algorithms 
factored representations long used classical ai planning  similar
representations adopted recent use mdp models within ai 
section  section     focus economy representation afforded exploiting
structure inherent many planning domains  following section  section    
describe structure when made explicit factored representations can
exploited computationally plan policy construction 

    factored state spaces markov chains

begin examining structured states  systems whose state described using
finite set state variables whose values change time    simplify illustration
potential space savings  assume state variables boolean 
variables  size state space jsj   n    m   large   specifying
representing dynamics explicitly using state transition diagrams n n matrices
impractical  furthermore  representing reward function n  vector  specifying
observational probabilities  similarly infeasible  section      define class
problems dynamics represented o m   space many cases  begin
considering represent markov chains compactly consider incorporating
actions  observations rewards 
let state variable x take finite number values let
x stand
set possible values  assume
x finite  though much follows
applied countable state action spaces well  say state space
specified using one state variable  this variable denoted general model  taking
values    state space factored one state variable  state
possible assignment values variables  letting xi represent ith state
variable  state space cross product value spaces individual state

variables  is   
i  
xi   denotes state process stage t 
let xit random variable representing value ith state variable stage t 
    variables often called uents ai literature  mccarthy   hayes         classical
planning  atomic propositions used describe domain 

  

fidecision theoretic planning  structural assumptions

bayesian network  pearl        representational framework compactly representing probability distribution factored form  although networks typically used represent atemporal problem domains  apply techniques
represent markov chains  encoding chain s transition probabilities network
structure  dean   kanazawa        
formally  bayes net directed acyclic graph vertices correspond random
variables edge two variables indicates direct probabilistic dependency
them  network constructed ects implicit independencies among
variables  network must quantified specifying probability variable
 vertex  conditioned possible values immediate parents graph  addition 
network must include marginal distribution  unconditional probability
vertex parents  quantification captured associating conditional
probability table  cpt  variable network  together independence
assumptions defined graph  quantification defines unique joint distribution
variables network  probability event space
computed using algorithms exploit independencies represented within graphical
structure  refer pearl        details 
figures   a   c   page    special cases bayes nets called  temporal  bayesian
networks  networks  vertices graph represent system s state different
time points arcs represent dependencies across time points  temporal networks 
vertex s parent temporal predecessor  conditional distributions transition
probability distributions  marginal distributions distributions initial states 
networks figure   ect extensional representation scheme states
explicitly enumerated  techniques building performing inference probabilistic temporal networks designed especially application factored representations 
figure    illustrates two stage temporal bayes net   tbn  describing state transition
probabilities associated markov chain induced fixed policy executing
action cclk  repeatedly moving counterclockwise    tbn  set variables
partitioned corresponding state variables given time  or stage 
corresponding state variables time      directed arcs indicate probabilistic dependencies variables markov chain  diachronic arcs directed
time variables time     variables  synchronic arcs directed
variables time      figure    contains diachronic arcs  synchronic arcs
discussed later section 
given state time t  network induces unique distribution states    
quantification network describes state particular variable changes
function certain state variables  lack direct arc  or generally directed
path synchronic arcs among     variables  variable xt another
variable yt   means knowledge xt irrelevant prediction  immediate 
one stage  evolution variable markov process 
figure    shows compact representation best circumstances 
many potential links one stage next omitted  graphical
representation makes explicit fact policy  i e   action cclk  affect
state variable loc  exogenous events arrm  reqc  mess affect
  

fiboutilier  dean    hanks

loc

loc





cr

cr

p loc t    
loc l c h
             
             
l
             
c
             
h              
p cr t   
cr

f

rhc

f
     
       

rhc

rhm

rhm





time

time t  

p rhc t   
rhc f
     
f
     

figure     factored  tbn markov chain induced moving counterclockwise
 with selected cpts shown  
variables   cr  tidy  respectively    furthermore  dynamics loc  and
variables  described using knowledge state parent
variables  instance  distribution loc    depends value loc
previous stage  e g   loct   o  loct     probability     loct    
probability       similarly  cr become true probability      due reqc
event   true  cannot become false  under simple policy   rhc remains
true  or false  certainty true  or false  previous stage  finally 
effects relevant variables independent  instantiation variables
time t  distribution next states computed multiplying conditional
probabilities relevant     variables 
ability omit arcs graph based locality independence action
effects strong effect number parameters must supplied complete
model  although full transition matrix cclk would size               
transition model figure    requires    parameters   
example shows  tbns exploit independence represent markov chains
compactly  example extreme effectively relationship
variables the chain viewed product six independently evolving processes 
    show cpts brevity 
    fact  exploit fact probabilities sum one leave one entry unspecified per row
cpt explicit transition matrix  case   tbn requires    explicit parameters 
transition matrix requires                    entries  generally ignore fact comparing
sizes representations 

  

fidecision theoretic planning  structural assumptions

loc

loc





cr

cr

rhc

rhc

rhm

rhm





time

time t  

loc rhc


l

c



h


f
l
f
c
f

f
h
f

p loc t    
l c h
           
             
             
             
             
             
             
             
             
p cr t   
             
loc rhc cr f

p rhc t   
loc rhc
f
       

f        

       
l
f        
l
       
c
f        
c
etc 
etc 





l
l
l
l



f
f


f
f
etc 


f

f

f

f

       
       
       
       
       
       
       
       
etc 

figure      tbn markov chain induced moving counterclockwise delivering coffee 

  

fiboutilier  dean    hanks

general   subprocesses  interact  still exhibit certain independencies
regularities exploited  tbn representation  consider two distinct
markov chains exhibit different types dependencies 
figure    illustrates  tbn representing markov chain induced following
policy  robot consistently moves counterclockwise unless oce
coffee  case delivers coffee user  notice different variables
dependent  instance  predicting value rhc     requires knowing values
loc rhc t  cpt rhc shows robot retains coffee stage    
certainty  stage t  locations except  where executes delc 
thus losing coffee   variable loc depends value rhc  location
change figure    one exception  robot oce coffee 
location remains  since robot move  executes delc   effect
variable cr explained follows  robot oce delivers coffee
possession  fulfill outstanding coffee request  however       chance cr
remaining true conditions indicates    chance spilling coffee 
even though dependencies  i e   additional diachronic arcs   tbn 
still requires     parameters  again  distribution resulting states determined multiplying conditional distributions individual variables  even though
variables  related   state known  variables time      loct    
rhct     etc   independent  words 
pr loct     t     crt     rhct     rhmt     t   js    
t 
pr loct   js   pr t t   js   pr crt   js   pr rhct   js   pr rhmt   js   pr m t   js   
figure    illustrates  tbn representing markov chain induced policy
above  assume act moving counterclockwise slightly
different effect  suppose that  robot moves hallway
adjacent location      chance spilling coffee possession 
fragment cpt rhc figure    illustrates possibility  furthermore 
robot carrying mail whenever loses coffee  whether  accidentally   intentionally 
via delc action       chance lose mail  notice effects
policy variables rhc rhm correlated  one cannot accurately predict
probability rhmt   without determining probability rhct     correlation
modeled synchronic arc rhc rhm     slice network 
independence    variables given hold  tbns synchronic
arcs  determining probability resulting state requires simple probabilistic
reasoning  example  application chain rule  example  write
pr rhct     rhmt   js     pr rhmt   jrhct       pr rhct   js  
joint distribution     variables given computed equation   above  term replacing pr rhct   js   pr rhmt   js   while two
variables correlated  remaining variables independent 
refer  tbns synchronic arcs  one figure     simple  tbns 
general  tbns allow synchronic well diachronic arcs  figure    
  

fidecision theoretic planning  structural assumptions

loc

loc





cr

cr

rhc

rhc

rhm

rhm





time

time t  

pr rhc t    
loc rhc f
       

f        

       
h
f        
h
       
c
f        
c
etc 
etc 
pr rhmt    
rhc rhc t   rhmt f
       



       
f


       

f

       
f
f

       


f
       
f

f
       

f
f
       
f
f
f

figure      tbn markov chain induced moving counterclockwise delivering coffee correlated effects 

    factored action representations

extended markov chains account different actions  must extend
 tbn representation account fact state transitions uenced
agent s choice action  discuss variety techniques specifying transition
matrices exploit factored state representation produce representations
natural compact explicit transition matrices 
      implicit event models

begin implicit event model section     effects actions
exogenous events combined single transition matrix  consider explicitevent models section        saw previous section  algorithms value
policy iteration require use transition models ect ultimate transition
probabilities  including effects exogenous events 
one way model dynamics fully observable mdp represent action
separate  tbn   tbn shown figure    seen representation
action cclk  since policy inducing markov chain example consists
repeated application action alone   network fragment figure    a 
illustrates interesting aspects  tbn delc action including effects
exogenous events  above  robot satisfies outstanding coffee request delivers
coffee oce coffee  with      chance spillage   shown
conditional probability table cr  effect rhc explained follows 
  

fiboutilier  dean    hanks

loc

rhc

cr
time

loc

rhc

cr
time t  

 a 

pr rhc t    
loc rhc f

       

f        
l
       
f        
l
       
c
f        
c
etc 
etc 
pr cr t    
loc rhc cr f
       


f        


       
f

f        
f

       

l
f        

l
       
f
l
f        
f
l
etc 
etc 





cr

    

rhc

loc

else

f

f




cr

       

cr

   

f

f

   

   

 b 




cr

    

rhc

f

loc
else

f
f

   

cr



   

 c 

figure     factored  tbn action delc  a  structured cpt representations  b c  
robot loses coffee  to user spillage  delivers oce  attempts
delivery elsewhere      chance random passerby take coffee
robot 
case markov chains  effects actions different variables
correlated  case must introduce synchronic arcs  correlations
thought ramifications  baker        finger        lin   reiter        
      structured cpts

conditional probability table  cpt  node cr figure    a     rows  one
assignment parents  however  cpt contains number regularities 
intuitively  ects fact coffee request met successfully  i e  
variable becomes false      time delc executed  robot coffee
right location  the user s oce   otherwise  cr remains true true
becomes true probability     not  words  three distinct cases
considered  corresponding three  rules  governing  stochastic  effect delc
cr  represented compactly using decision tree representation
 with  else  branches summarize groups cases involving multivalued variables
loc  shown figure    b   compactly still using decision graph
 figure    c    tree  graph based representations cpts  interior nodes labeled
parent variables  edges values variables  leaves terminals distributions
child variable s values   
decision tree decision graph representations used represent actions fully
observable mdps  boutilier et al         hoey  st aubin  hu    boutilier       
    child boolean  label leaves probability variable true  the
probability variable false one minus value  

  

fidecision theoretic planning  structural assumptions

described detail  poole        boutilier   goldszmidt           intuitively  trees
graphs embody rule like structure present family conditional distributions
represented cpt  settings consider often yield considerable representational compactness  rule based representations used directly poole       
    a  context decision processes often compact trees  poole 
    b   generically refer representations type  tbns structured cpts 
      probabilistic strips operators

 tbn representation viewed oriented toward describing effects actions
distinct variables  cpt variable expresses  stochastically  changes
 or persists   perhaps function state certain variables  however 
long noted ai research planning reasoning action
actions change state limited ways  is  affect relatively small number
variables  one diculty variable oriented representations  tbns one
must explicitly assert variables unaffected specific action persist value  e g  
see cpt rhc figure     this instance infamous frame problem
 mccarthy   hayes        
another form representation actions might called outcome oriented representation  one explicitly describes possible outcomes action possible joint
effects variables  idea underlying strips representation
classical planning  fikes   nilsson        
classical strips operator described precondition set effects 
former identifies set states action executed  latter
describes input state changes result taking action  probabilistic
strips operator  pso   hanks        hanks   mcdermott        kushmerick et al        
extends strips representation two ways  first  allows actions different
effects depending context  second  recognizes effects actions
always known certainty   
formally  pso consists set mutually exclusive exhaustive logical formulae 
called contexts  stochastic effect associated context  intuitively  context discriminates situations action differing stochastic effects 
stochastic effect set change sets a simple list variable values with
probability attached change set  requirement probabilities sum
one  semantics stochastic effect described follows  stochastic
effect action applied state s  possible resulting states determined
change sets  occurring corresponding probability  resulting state associated change set constructed changing variable values state match
change set  unmentioned variables persist value  note since one
    fact certain direct dependencies among variables bayes net rendered irrelevant
specific variable assignments studied generally guise context specific independence
 boutilier  friedman  goldszmidt    koller         see  geiger   heckerman        shimony       
related notions 
    conditional nature effects feature deterministic extension strips known adl
 pednault        

  

fiboutilier  dean    hanks

rhc


f

loc


 cr  rhc  m
 cr  rhc
 rhc  m
 rhc

 cr  m
 cr
 m
nil

else

 rhc  cr  m
 rhc  cr
 rhc  m
 rhc
 cr  m
 cr
 m
nil

    
    
    
    

    
    
    
    

     
     
     
     
     
     
     
     

figure     pso representation delc action 
loc


l

 loc l     
nil
   

 loc c     
nil
   

h

c



 loc m     
nil
   

 loc h     
nil
   

 loc o   rhc  rhm
 loc o   rhc
 loc o 
 rhc  rhm
 rhc
nil

     
     
    
     
     
    

figure     pso representation simplified cclk action 
context hold state s  transition distribution action state
easily determined 
figure    gives graphical depiction pso delc action  shown  tbn
figure      three contexts  rhc  rhc   loc o  rhc   loc o  represented
using decision tree  leaf branch decision tree stochastic effect
 set change sets associated probabilities  determined corresponding context 
example  rhc   loc o  holds  action four possible effects  robot loses
coffee  may may satisfy coffee request  due      chance spillage  
mail may may arrive  notice outcome spelled completely 
number outcomes two contexts rather large due possible exogenous
events  we discuss section          
key difference psos  tbns lies treatment persistence 
variables unaffected action must given cpts  tbn model 
variables mentioned pso model  e g   compare variable loc
representations delc   way  psos said  solve  frame problem 
since unaffected variables need mentioned action s description   
    keep figure    manageable  ignore effect exogenous event mess variable  
    discussion frame problem  tbns  see  boutilier   goldszmidt        

  

fidecision theoretic planning  structural assumptions

arrm

mess

loc

loc

loc

loc

rhc

rhc

rhc

rhc

rhm

rhm

rhm

rhm

cr

cr

cr

cr



















t   

t   

t  

figure     simplified explicit event model delc 
psos provide effective means representing actions correlated effects 
recall description cclk action captured figure     robot may
drop coffee moves hallway  may drop mail drops
coffee   tbn representation cclk  one must rhct rhct  
parents rhmt     must model dependence rhm change value
variable rhc  figure    shows cclk action pso format  for simplicity  ignore
occurrence exogenous events   pso representation offer economical
representation correlated effects since possible outcomes moving
hallway spelled explicitly  specifically   possible  simultaneous change values
variables question made clear 
      explicit event models

explicit event models represented using  tbns somewhat different form 
discussion section      form taken explicit event models depends crucially one s assumptions interplay effects action
exogenous events  however  certain assumptions even explicit event models
rather concise 
illustrate  figure    shows deliver coffee action represented  tbn
exogenous events explicitly represented  first  slice  network shows effects
action delc without presence exogenous events  subsequent slices describe
effects events arrm mess  we use two events illustration   notice
presence extra random variables representing occurrence events question 
cpts nodes ect occurrence probabilities events various
  

fiboutilier  dean    hanks

conditions  directed arcs event variables state variables indicate
effects events  probabilities depend state variables general 
thus   tbn represents occurrence vectors  see section      compact form 
notice that  contrast event occurrence variables  explicitly represent
action occurrence variable network  since modeling effect
system given action taken   
example ects assumptions described section      namely  events
occur action takes place event effects commutative 
reason ordering events arrm mess network irrelevant 
model  system actually passes two intermediate though necessarily distinct
states goes stage stage      use subscripts       suggest
process  course  described earlier  actions events combined
decomposable way  complex combination functions modeled using  tbns
 for one example  see boutilier   puterman        
      equivalence representations

obvious question one might ask concerns extent certain representations
inherently concise others  focus standard implicit event models 
describing domain features make different representations less
suitable 
 tbn pso representations oriented toward representing changes
values state variables induced action  key distinction lies fact
 tbns model uence variable separately  pso model explicitly
represents complete outcomes  simple  tbn a network synchronic arcs can
used represent action cases correlations among action s
effect different state variables  worst case  effect variable
differs state  time     variable must time variables parents 
regularities exploited structured cpt representations 
action requires specification o n n   parameters  assuming boolean variables  
compared   n entries required explicit transition matrix  number
parents variable bounded k  need specify n k conditional
probabilities  reduced cpts exhibit structure  e g  
represented concisely decision tree   instance  cpt captured
representation choice f  k  entries  f polynomial function
number parents variable  representation size  o n f  k    polynomial
number state variables  often case  instance  actions one
 stochastic  effects variable requires number  pre   conditions hold 
not  different effect comes play 
pso representation may concise  tbn action multiple
independent stochastic effects  pso requires possible change list enumerated corresponding probability occurrence  number changes grows
exponentially number variables affected action  fact evident
    sections           discuss representations model choice action explicitly variable
network 

  

fidecision theoretic planning  structural assumptions

rhc


f

loc


 rhc  cr
 rhc

nil

 m    
nil    
   

else
    
    

 rhc
nil

   
   

figure      factored  pso representation delc action 
figure     impact exogenous events affects number variables stochastically independently  problem arise respect  direct  action effects 
well  consider action set    unpainted parts spray painted  part
successfully painted probability      successes uncorrelated  ignoring
complexity representing different conditions action could take place 
simple  tbn represent action    parameters  one success probability per
part   contrast  pso representation might require one list     distinct change
lists associated probabilities  thus  pso representation exponentially
larger  in number affected variables  simple  tbn representation 
fortunately  certain variables affected deterministically  cause
pso representation blow up  furthermore  pso representations modified
exploit independence action s effects different state variables  boutilier  
dearden        dearden   boutilier         thus escaping combinatorial diculty 
instance  might represent delc action shown figure     factored
form  illustrated figure     for simplicity  show effect action
exogenous event arrm   much  tbn  determine overall effect
combining change sets  in appropriate contexts  multiplying corresponding
probabilities 
simple  tbns defined original set state variables sucient represent actions    correlated action effects require presence synchronic arcs 
worst case  means time     variables  n     parents 
fact 
p acyclicity condition assures worst case  total number parents
nk    k      thus  end specifying o   n   entries  required
explicit transition matrix  however  number parents  whether occurring within
time slice      bounded  regularities cpts allow compact
representation   tbns still profitably used 
pso representations compare favorably  tbns cases
action s effects different variables correlated  case  psos provide
somewhat economical representation action effects  primarily one needn t
worry frame conditions  main advantage psos one need enlist
aid probabilistic reasoning procedures determine transitions induced actions
correlated effects  contrast explicit specification outcomes psos
type reasoning required determine joint effects action represented  tbn
    however  section       discusses certain problem transformations render simple  tbns sucient
mdp 

  

fiboutilier  dean    hanks

form synchronic arcs  described section      essentially  correlated effects
 compiled  explicit outcomes psos 
recent results littman        shown simple  tbns psos
used represent action represented  tbn without exponential blowup
representation size  effected clever problem transformation new
sets actions propositional variables introduced  using either simple  tbn
pso representation   structure original  tbn ected new planning
problem  incurring polynomial increase size input action
descriptions description policy  though resulting policy consists actions
exist underlying domain  extracting true policy dicult 
noted  however  representation automatically constructed
general  tbn specification  unlikely could provided directly  since
actions variables transformed problem  physical  meaning
original mdp 
      transformations eliminate synchronic constraints

discussion assumed variables propositions used  tbn
pso action descriptions original state variables  however  certain problem transformations used ensure one represent action using simple  tbns 
long one require original state variables used  one transformation
simply clusters variables action correlated effect  new compound
variable which takes values assignments clustered variables can used
 tbn  removing need synchronic arcs  course  variable
domain size exponential number clustered variables 
intuitions underlying psos used convert general  tbn action descriptions simple  tbn descriptions explicit  events  dictating precise outcome
action  intuitively  event occur k different forms  corresponding
different change list induced action  or change list respect variables
question   example  convert  action  description cclk figure   
explicit event model shown figure       notice  event  takes values
corresponding possible effects correlated variables rhc rhm  specifically  denotes event robot escaping hallway successfully without losing
cargo  b denotes event robot losing coffee  c denotes event losing
coffee mail  effect  event space represents possible  combined 
effects  obviating need synchronic arcs network 
      actions explicit nodes network

one diculty  tbn pso approach action description action
represented separately  offering opportunity exploit patterns across actions 
instance  fact location persists actions except moving clockwise counterclockwise means  frame axiom  duplicated  tbn actions
 this case psos  course   addition  ramifications  or correlated action
    figure    describes markov chain induced policy  representation cclk easily
extracted it 

  

fidecision theoretic planning  structural assumptions

loc

hall

a     
b     
c     

event



rhc

rhm


loc

else

f

a     a    
b      b    
c      c    

loc

f

a     
b     
c     





rhc

rhc
   



rhm
time

rhm
time t  

loc

   
   



rhm

b
   

b
   

f

   

c
   

f

   

else

event

event

rhc

c
   

figure     explicit event model removes correlations 
effects  duplicated across actions well  instance  coffee request occurs  with
probability      robot ends oce  correlation duplicated
across actions  compelling example might one robot move
briefcase new location one number ways  we d capture fact  or
ramification  contents briefcase move location briefcase
regardless action moves briefcase 
circumvent diculty  introduce choice action  random variable  network  conditioning distribution state variable transitions
value variable  unlike state variables  or event variables explicit event models  
generally require distribution action variable the intent simply
model schematically conditional state transition distributions given particular
choice action  choice action dictated decision maker
policy determined  reason  anticipating terminology used uence
diagrams  see section       call nodes decision nodes depict network diagrams boxes  variable take value action available
agent 
 tbn explicit decision node shown figure     restricted example 
might imagine decision node take one two values  clk cclk  fact
issuance coffee request t   depends whether robot successfully moved
 or remained in  oce represented  once  arc loct   crt    
rather repeated across multiple action networks  furthermore  noisy persistence
actions represented  adding action pum  however 
undercuts advantage see try combine actions  
one diculty straightforward use decision nodes  which standard
representation uence diagram literature  adding candidate actions
cause explosion network s dependency structure  example  consider two
  

fiboutilier  dean    hanks

act

loc

loc

cr

cr





time

time t  

figure     uence diagram restricted process 

act
x

x

x

x

x

act

x

else

a 
a 














   



f


x

   

z

z
 a  action a 

z

z

z

 b  action a 

   

f
 



f


   


z

   

f
 

z

 c  influence diagram

figure     unwanted dependencies uence diagrams 

  



 d  cpt



f
 

fidecision theoretic planning  structural assumptions

action networks shown figure    a   b   action a  makes true probability
    x true  having effect otherwise   a  makes true z true 
combining actions single network obvious way produces uence
diagram shown figure    c   notice four parent nodes  inheriting
union parents individual networks  plus action node  requiring
cpt    entries actions a  a  together eight additional entries
action affect   individual networks ect fact depends
x a  performed z a  performed  fact lost
naively constructed uence diagram  however  structured cpts used
recapture independence compactness representation  tree figure    d 
captures distribution much concisely  requiring eight entries  structured
representation allows us concisely express persists actions 
large domains  expect variables generally unaffected substantial number
 perhaps most  actions  thus requiring representations uence diagrams 
see  boutilier   goldszmidt        deeper discussion issue relationship
frame problem 
provide distributional information action choice  hard
see  tbn explicit decision node used represent markov chain
induced particular policy natural way  specifically  adding arcs state
variables time decision node  value decision node  i e   choice
action point  dictated prevailing state   

    uence diagrams
uence diagrams  howard   matheson        shachter        extend bayesian networks
include special decision nodes represent action choices  value nodes represent
effect action choice value function  presence decision nodes means
action choice treated variable decision maker s control  value nodes treat
reward variable uenced  usually deterministically  certain state variables 
uence diagrams typically associated schematic representation
stationary systems  instead used tool decision analysts sequential
decision problem carefully handcrafted  generic use uence diagrams
discussed tatman shachter         event  theory plan
construction associated uence diagrams  choice possible actions
stage must explicitly encoded model  uence diagrams are  therefore  usually
used model finite horizon decision problems explicitly describing evolution
process stage terms state variables 
section        decision nodes take values specific actions  though set
possible actions tailored particular stage  addition  analyst generally
include stage state variables thought relevant decision
subsequent stages  value nodes key feature uence diagrams
discussed section      usually  single value node specified  arcs indicating
    generally  randomized policy represented specifying distribution possible actions
conditioned state 

  

fiboutilier  dean    hanks


rhm


rhm
rew

cr



etc 


 

cr

etc 

   


 

 

                  

 

   

 

 

                  

figure     representation reward function uence diagram 
uence particular state decision variables  often multiple stages  overall
value function 
uence diagrams typically used model partially observable problems  arc
state variable decision node ects fact value state variable
available decision maker time action chosen  words 
variable s value forms part observation made time prior action
selected time     policy constructed refer variable  again 
allows compact specification observation probabilities associated system 
fact probability given observation depends directly certain variables
others mean far fewer model parameters required 

    factored reward representation

already noted common formulating mdp problems adopt
simplified value function  assigning rewards states costs actions  evaluating histories combining factors according simple function addition 
simplification alone allows representation value function significantly
parsimonious one based complex comparison complete histories  even
representation requires explicit enumeration state action space  however 
motivating need compact representations parameters  factored representations rewards action costs often obviate need enumerate state
action parameters explicitly 
action s effect particular variable  reward associated state often
depends values certain features state  example  robot
domain  associate rewards penalties undelivered mail  unfulfilled coffee
requests untidiness lab  reward penalty independent
variables  individual rewards associated groups states differ
values relevant variables  relationship rewards state variables
represented value nodes uence diagrams  represented diamond figure    
conditional reward table  crt  node table associates reward
every combination values parents graph  table  shown figure    
locally exponential number relevant variables  although figure    shows
case stationary markovian reward function  uence diagrams used represent
  

fidecision theoretic planning  structural assumptions

nonstationary history dependent rewards often used represent value functions
finite horizon problems 
although worst case crt take exponential space store  many
cases reward function exhibits structure  allowing represented compactly using
decision trees graphs  boutilier et al          strips like tables  boutilier   dearden 
       logical rules  poole            a   figure    shows fragment one possible
decision tree representation reward function used running example 
independence assumptions studied multiattribute utility theory  keeney   raiffa 
      provide yet another way reward functions represented compactly 
assume component attributes reward function make independent contributions state s total reward  individual contributions combined functionally 
instance  might imagine penalizing states cr holds  partial  reward
    penalizing situations undelivered mail  m   rhm     
penalizing untidiness  i       i e   proportion untidy things are  
reward state determined simply adding individual penalties associated feature  individual component rewards along combination
function constitute compact representation reward function  tree fragment
figure     ects additive independent structure described  considerably
complex representation defines  independent  rewards individual
propositions separately  use additive reward functions mdps considered
 boutilier  brafman    geib        meuleau  hauskrecht  kim  peshkin  kaelbling  dean 
  boutilier        singh   cohn        
another example structured rewards goal structure studied classical planning 
goals generally specified single proposition  or set literals  achieved 
such  generally represented compactly  haddawy hanks       
explore generalizations goal oriented models permit extensions partial goal
satisfaction  yet still admit compact representations 

    factored policy value function representation

techniques studied far concerned input specification mdp 
states  actions  reward function  components problem s solution the policy optimal value function are candidates compact structured representation 
simplest case  stationary policy fully observable problem  policy
must associate action every state  nominally requiring representation size
o jsj   problem exacerbated nonstationary policies pomdps  example 
policy finite horizon fomdp stages generates policy size o t jsj  
finite horizon pomdp  possible
p observable history length   might require
different action choice  many tk   bk histories generated fixed
policy  b maximum number possible observations one make following
action   
fact policies require much space motivates need find compact functional representations  standard techniques tree structures discussed
    methods dealing pomdps  conversion fomdps belief space  see section         
complex still 

  

fiboutilier  dean    hanks

cr
rhc

etc 

loc


l c

loc


h

h

l

c



delc clk clk cclk cclk hrm clk getc
pum cclk

delm cclk

pum cclk

figure     tree representation policy 
actions reward functions used represent policies value functions well 
focus stationary policies value functions fomdps  logical
function representation may used  example  schoppers        uses strips style
representation universal plans  deterministic  plan like policies  decision trees
used policies value functions  boutilier et al         chapman  
kaelbling         example policy robot domain specified decision tree
given figure     policy dictates that  instance  cr rhc true   a 
robot deliver coffee user oce   b  move toward oce
oce  unless  c  mail mailroom  case
pickup mail way 

    summary

section discussed number compact factored representations components
mdp  began discussing intensional state representations  temporal bayesian
networks device representing system dynamics  tree structured conditional
probability tables  cpts  probabilistic strips operators  psos  introduced
alternative transition matrices  similar tree structures logical representations
introduced representing reward functions  value functions  policies 
representations often used describe problem compactly 
offer guarantee problem solved effectively  next
section explore algorithms use factored representations avoid iterating
explicitly entire set states actions 

   abstraction  aggregation  decomposition methods

greatest challenge using mdps basis dtp lies discovering computationally feasible methods construction optimal  approximately optimal satisficing
policies  course  arbitrary decision problems intractable even producing satisficing
approximately optimal policies generally infeasible  however  previous sections
suggest many realistic application domains may exhibit considerable structure 
furthermore structure modeled explicitly exploited typical
problems solved effectively  instance  structure type lead compact
  

fidecision theoretic planning  structural assumptions

factored representations input data output policies  often polynomial sized
respect number variables actions describing problem  suggests
compact problem representations  policy construction techniques developed exploit structure tractable many commonly occurring problem
instances 
dynamic programming state based search techniques described section   exploit structure different kind  value functions decomposed
state dependent reward functions  state based goal functions  tackled dynamic
programming regression search  respectively  algorithms exploit structure
decomposable value functions prevent search explicitly possible
policies  however  algorithms polynomial size state space 
curse dimensionality makes even algorithms infeasible practical problems 
though compact problem representations aid specification large problems 
clear large system specified compactly representation exploits
 regularities  found domain  recent ai research dtp stressed using
regularities implicit compact representations speed planning process 
techniques focus optimal approximately optimal policy construction 
following subsection focus abstraction aggregation techniques  especially manipulate factored representations  roughly  techniques allow
explicit implicit grouping states indistinguishable respect certain characteristics  e g   value optimal action choice   refer set states grouped
manner aggregate abstract state   sometimes cluster  assume
set abstract states constitutes partition state space  say  every state
exactly one abstract state union abstract states comprises entire state
space    grouping similar states  abstract state treated single state  thus
alleviating need perform computations state individually  techniques
used approximation elements abstract state approximately
indistinguishable  e g   values states lie within small interval  
look use problem decomposition techniques mdp
broken various pieces  solved independently  solutions
pieced together used guide search global solution  subprocesses whose
solutions interact minimally treated independent  might expect approximately
optimal global solution  furthermore  structure problem requires solution
particular subproblem only  solutions subproblems ignored
altogether 
related use reachability analysis restrict attention  relevant  regions
state space  indeed  reachability analysis communicating structure mdp
used form certain types decompositions  specifically  distinguish serial
decompositions parallel decompositions 
result serial decomposition viewed partitioning state space
blocks  representing  more less  independent subprocess solved 
serial decomposition  relationship blocks generally complicated
case abstraction aggregation  partition resulting decomposition 
    might group states non disjoint sets cover entire state space  consider
soft state aggregation here  see  singh  jaakkola    jordan        

  

fiboutilier  dean    hanks

states within particular block may behave quite differently respect  say  value
dynamics  important consideration choosing decomposition possible
represent block compactly compute eciently consequences moving
one block another and  further  subproblems corresponding subprocesses
solved eciently 
parallel decomposition somewhat closely related abstract mdp 
mdp divided  parallel sub mdps  decision action causes
state change within sub mdp  thus  mdp cross product join
sub mdps  in contrast union  serial decomposition   brie discuss several
methods based parallel mdp decomposition 

    abstraction aggregation
one way problem structure exploited policy construction relies notion
aggregation grouping states indistinguishable respect certain problem
characteristics  example  might group together states optimal
action  value respect k stage to go value function 
aggregates constructed solution problem 
ai  emphasis generally placed particular form aggregation  namely
abstraction methods  states aggregated ignoring certain problem features 
policy figure    illustrates type abstraction  states cr 
rhc loc o  true grouped  action selected
state  intuitively  three propositions hold  problem features ignored
abstracted away  i e   deemed irrelevant   decision tree representation
policy value function partitions state space distinct cluster leaf
tree  representations  e g   strips like rules  abstract state space similarly 
precisely type abstraction used compact  factored representations actions goals discussed section     tbn shown figure    
effect action delc variable cr given cpt crt     however 
 stochastic  effect state parent variables
value  representation abstracts away variables  combining states
distinct values irrelevant  non parent  variables  intensional representations often
make easy decide features ignore certain stage problem solving 
thus  implicitly  aggregate state space 
least three dimensions along abstractions type compared  first uniformity  uniform abstraction one variables deemed
relevant irrelevant uniformly across state space  nonuniform abstraction allows certain variables ignored certain conditions others 
distinction illustrated schematically figure     tabular representation cpt
viewed form uniform abstraction the effect action variable
distinguished clusters states differ value parent variable 
distinguished states agree parent variables disagree others while
decision tree representation cpt embodies nonuniform abstraction 
second dimension comparison accuracy  states grouped together
basis certain characteristics  abstraction called exact states within
  

fidecision theoretic planning  structural assumptions

uniform
abc
abc

abc
abc

abc
abc

abc
abc

nonuniform

b

   
   

ab

 

abc

c

abc

exact
   
   



approximate
   
   

   
   

   
   

   

   
   

   
   

   

adaptive

fixed

figure     different forms state space abstraction 
cluster agree characteristic  non exact abstraction called approximate 
illustrated schematically figure     exact abstraction groups together states
agree value assigned value function  approximate abstraction
allows states grouped together differ value  extent states
differ often used measure quality approximate abstraction 
third dimension adaptivity  technically  property abstraction
itself  abstractions used particular algorithm  adaptive abstraction
technique one abstraction change course computation 
fixed abstraction scheme groups together states  again  see figure     
example  one imagine using abstraction representation value function
v k   revising abstraction represent v k   accurately 
abstraction aggregation techniques studied literature
mdps  bertsekas castanon        develop adaptive aggregation  as opposed
abstraction  technique  proposed method operates state spaces  however 
therefore exploit implicit structure state space itself  adaptive  uniform
abstraction method proposed schweitzer et al         solving stochastic queuing models  methods  often referred aggregation disaggregation procedures 
typically used accelerate calculation value function fixed policy  valuefunction calculation requires computational effort least quadratic size state
space  impractical large state spaces  aggregation disaggregation procedures  states first aggregated clusters  system equations solved 
series summations performed  requiring effort cubic number
clusters  next  disaggregation step performed cluster  requiring effort least
linear size cluster  net result total work  least linear
total number states  worst cubic size largest cluster 
dtp generally assumed computations even linear size full
state space infeasible  therefore important develop methods perform
  

fiboutilier  dean    hanks

work polynomial log size state space  problems amenable
reductions without  perhaps unacceptable  sacrifice solution quality 
following section  review recent techniques dtp aimed achieving
reductions 
      goal regression classical planning

section     introduced general technique regression  or backward  search
state space solve classical planning problems  involving deterministic actions performance criteria specified terms reaching goal satisfying state  one
diculty search requires branch search tree lead particular
goal state  commitment goal state may retracted  by backtracking
search process  sequence actions lead particular goal state
initial state  however  goal usually specified set literals g representing set
states  reaching state g equally suitable it may  therefore  wasteful
restrict search finding plan reaches particular element g 
goal regression abstraction technique avoids problem choosing particular goal state pursue  regression planner works searching sequence actions
follows  current set subgoals sg  initialized g  iteration action
selected achieves one current subgoals sgi without deleting
others  whose preconditions con ict  unachieved subgoals  
subgoals achieved removed current subgoal set replaced formula
representing context achieve current subgoals  forming sgi    
process known regressing sgi ff  process repeated one
two conditions holds   a  current subgoal set satisfied initial state 
case current sequence actions selected successful plan   b  action
applied  case current sequence cannot extended successful plan
earlier action choice must reconsidered 
example     example  consider simplified version robot planning example used section     illustrate value iteration  robot four actions
pum  getc  delc delm  make deterministic obvious way 
initial state sinit hcr  m  rhc  rhmi goal set g fcr  g  regressing g delm results sg    fcr  m  rhmg  regressing sg 
delc results sg    frhc  m  rhmg  regressing sg  pum results
sg    frhc  g  regressing sg  getc results sg    fm g  note
sinit   sg   sequence actions getc  pum  delc  delm successfully reach
goal state   
see algorithm implements form abstraction  first note goal
provides initial partition state space  dividing one set states
goal satisfied  g  second set  g   viewed partition
zero stage to go value function  g represents states whose value positive
g represents states whose value zero 
every regression step thought revising partition  planning
algorithm attempts satisfy current subgoal set sgi applying action ff  uses
  

fidecision theoretic planning  structural assumptions

getc

rhc





 

pum

rhc

delc

cr

delm

cr







rhm

rhm









goal

 

 

 

figure     example goal regression 
regression compute  largest  set states that  executing ff  subgoals
satisfied  particular  state space repartitioned two abstract states  sgi  
sgi     way  abstraction mechanism implemented goal regression
considered adaptive  viewed  i      stage value function  state
satisfying sgi   reach goal state    steps using action sequence produced
sgi       regression process stopped initial state member
abstract state sgi     figure    illustrates repartitioning state space
different regions sgi   steps example above 
regression produces compact representation something value function
 as discussion deterministic  goal based dynamic programming section      
analogy exact regions produced regression record property
goal reachability contingent particular choice action action sequence 
standard dynamic programming methods implemented structured way
simply noticing number different regions produced ith iteration
considering actions regressed stage  union
regressions form states positive values vi   thus making representation
i stage to go value function exact  notice iteration costly  since
regression actions must attempted  approach obviates need
backtracking ensure shortest plan found  standard regression
provide guarantees without commitment particular search strategy  e g   breadthfirst   use dynamic programming using strips action descriptions forms basic
idea schoppers s universal planning method  schoppers        
another general technique solving classical planning problems partial order planning  pop   chapman        sacerdoti         embodied popular planning algorithms snlp  mcallester   rosenblitt        ucpop  penberthy   weld          
main motivation least commitment approach comes realization
regression techniques incrementally building plan end beginning  in
temporal dimension   thus  iteration must commit inserting step last
plan 
many cases determined particular step must appear somewhere
plan  necessarily last step plan  and  indeed  many cases step
    case  however  states sgi   cannot reach goal region     steps 
case cannot using specific sequence actions chosen far 
    type planning sometimes called nonlinear least commitment planning  see weld s
       survey nice overview 

  

fiboutilier  dean    hanks

consideration cannot appear last  fact cannot recognized later choices
reveal inconsistency  cases  regression algorithm prematurely commit
incorrect ordering eventually backtrack choice  example 
suppose problem scenario robot hold one item time 
coffee mail  picking mail causes robot spill coffee possession 
similarly grasping coffee makes drop mail  plan generated regression would
longer valid  first two actions  delc delm  inserted
plan  action added achieve rhc rhm without making one false 
search plan would backtrack  ultimately would discovered
successful plan end two actions performed sequence 
partial order planning algorithms proceed much regression algorithms  choosing
actions achieve unachieved subgoals using regression determine new subgoals 
leaving actions unordered whatever extent possible  strictly speaking  subgoal sets
aren t regressed  rather  unachieved goal action precondition addressed separately 
actions ordered relative one another one action threatens negate
desired effect another  example above  algorithm might first place actions
delc delm plan  leave unordered  pum added plan
achieve requirement rhm delm  ordered delm still unordered
respect delc  getc finally added plan achieve rhc
action delc  two threats arise  first  getc threatens desired effect rhm pum 
resolved ordering getc pum delm  assume former ordering
chosen  second  pum threatens desired effect rhc getc  threat
resolved placing pum getc delc  since first threat resolved
ordering getc pum  latter ordering consistent one  result
plan getc  delc  pum  delm  backtracking required generate plan 
actions initially unordered  orderings introduced
discovery threats required them 
terms abstraction  incomplete  partially ordered plan threat free 
perhaps certain  open conditions   unachieved preconditions subgoals  
viewed much way partially completed regression plan  state satisfying
open conditions reach goal state executing total ordering plan s
actions consistent current set ordering constraints  see  kambhampati       
framework unifies various approaches solving classical plan generation problems 
techniques relying regression studied extensively deterministic
setting  recently applied probabilistic unobservable  kushmerick
et al         partially observable  draper  hanks    weld      b  domains 
part  techniques assume goal based performance criterion attempt
construct plans whose probability reaching goal state exceeds threshold 
augment standard pop methods techniques evaluating plan s probability
achieving goal  techniques improving probability adding structure
plan  next section  consider use regression related techniques
solve mdps performance criteria general goals 
  

fidecision theoretic planning  structural assumptions

      stochastic dynamic programming structured representations

key idea underlying propositional goal regression that one need regress relevant propositions action can extended stochastic dynamic programming
methods  value iteration policy iteration  used solve general mdps 
are  however  two key diculties overcome  lack specific goal region
uncertainty associated action effects 
instead viewing state space partitioned goal non goal clusters 
consider grouping states according expected values  ideally  might want
group states according value respect optimal policy  consider
somewhat less dicult task  grouping states according value respect
fixed policy  essentially task performed policy evaluation step
policy iteration  insights used construct optimal policies 
fixed policy  want group states value policy 
generalizing goal versus non goal distinction  begin partition groups
states according immediate rewards  then  using analogue regression developed
stochastic case  reason backward construct new partition states
grouped according value respect one stage to go value function 
iterate manner kth iteration produce new partition groups
states according k stage to go value function 
iteration  perform work polynomial number abstract states  and
size mdp representation  and  lucky  total number abstract states
bounded logarithmic factor size state space  implement
scheme effectively  perform operations regression without ever enumerating
set states  structured representations state transition 
value  policy functions play role 
fomdps  approaches type taken  boutilier        boutilier   dearden        boutilier et al         boutilier  dearden    goldszmidt        dietterich  
flann        hoey et al          illustrate basic intuitions behind approach
describing value iteration discounted infinite horizon fomdps might work 
assume mdp specified using compact representation reward function
 such decision tree  actions  such  tbns  
value iteration  produce sequence value functions v    v      vn   vk
representing utility optimal k stage policy  aim produce compact
representation value function and  using vn suitable n  produce compact
representation optimal stationary policy  given compact representation
reward function r  clear constitutes compact representation v   
usual  think leaf tree cluster states identical utility 
produce v  compact form  proceed two phases 
branch tree v  provides intensional description namely  conjunction variable values labeling branch of abstract state  region  comprising
states identical value respect initial value function v    deterministic action ff  perform regression step using description determine
conditions which  perform ff  would end cluster  would 
furthermore  determine region state space containing states identical future value
  

fiboutilier  dean    hanks

x

x

x

       
x





   



       

z

z


   

time

z

time t  
       

figure     example action 
respect execution one stage go    unfortunately  nondeterministic
actions cannot handled quite way  given state  action might lead
several different regions v  non zero probability  however  leaf tree
representing v   i e   region v     regress conjunction x describing
region action produce conditions x becomes true
false specified probability  words  instead regressing standard fashion determine conditions x becomes true  produce set distinct
conditions x becomes true different probabilities  piecing together
regions produced different labels description v    construct
set regions state given region   a  transitions  under action ff 
particular part v  identical probability  hence  b  identical expected future
value  boutilier et al          view generalization propositional goal
regression suitable decision theoretic problems 
example     illustrate  consider example action shown figure    value
function v   shown left figure     order generate set regions
consisting states whose future value  w r t  v     identical  proceed
two steps  see figure      first determine conditions fixed
probability making true  hence fixed probability moving left
right subtree v      conditions given tree representing cpt
node   makes first portion tree representing v    see step  
figure     notice tree leaves labeled probability making
true  implicitly  false 
makes true  know future value  i e   value zero stages
go       becomes false  need know whether makes z true  to
    ignore immediate reward cost distinctions within region produced description 
recall value performing state given r s   c  ff  s  expected future value 
simply focus abstract states whose elements identical future expected value  differences
immediate reward cost added fact 

  

fidecision theoretic planning  structural assumptions



x

   

z

   

   

   

x



   



   

   
z    

z

   
z    
 

v

step  



   

   
z    

z

   
z    

   
z    

step  

figure     iteration decision theoretic regression  step   produces portion
tree dashed lines  step   produces portion dotted lines 
determine whether future value         probability z becomes
true given tree representing cpt node z   step   figure    
conditions cpt conjoined conditions required predicting
 s probability  by  grafting  tree z tree given first step  
grafting slightly different three leaves tree    a 
full tree z attached leaf x   t   b  tree z simplified
attached leaf x   f     f removal redundant test variable
   c  notice need attach tree z leaf x   f     t 
since makes true probability   conditions  and z relevant
determination v   false  
leaves newly formed tree pr y   pr z   
joint distributions z  the effect variables independent semantics network  tells us probability z true
zero stages go given conditions labeling appropriate branch
tree hold one stage go  words  new tree uniquely determines 
state one stage remaining  probability making conditions
labeling branches v   true  computation expected future value obtained
performing one stage go placed leaves tree
taking expectation values leaves v      
new set regions produced way describes function qff    qff   s 
value associated performing state one stage go acting optimally
thereafter  functions  for action ff  pieced together  i e    maxed  see
section      determine v    course  process repeated number times
produce vn suitable n  well optimal policy respect vn  
basic technique used number different ways  dietterich flann
       propose ideas similar these  restrict attention mdps goal regions
  

fiboutilier  dean    hanks

deterministic actions  represented using strips operators   thus rendering true goalregression techniques directly applicable    boutilier et al         develop version
modified policy iteration produce tree structured policies value functions 
boutilier dearden        develop version value iteration described above 
algorithms extended deal correlations action effects  i e   synchronic arcs
 tbns   boutilier         abstraction schemes categorized nonuniform 
exact adaptive 
utility exact abstraction techniques tested real world problems date   boutilier et al          results series abstract process planning
examples reported  scheme shown useful  especially larger
problems  example  one specific problem     million states  tree representation value function        leaves  indicating tremendous amount
regularity value function  schemes exploit regularity solve problems
quickly  in example  much less half time required modified policy iteration  much lower memory demands  however  schemes involve
substantial overhead tree construction  smaller problems little regularity 
overhead repaid time savings  simple vector matrix representations methods
faster   though still generally provide substantial memory savings  might
viewed best  worst case behavior described  boutilier et al         
series  linear  examples  i e   problems value functions represented
trees whose size linear number problem variables   tree based scheme solves
problems many orders magnitude faster classical state based techniques  contrast  problems exponentially many distinct values tested  i e   distinct
value state   tree construction methods required construct complete
decision tree addition performing number expected value maximization
computations classical methods  worst case  tree construction overhead makes
algorithm run     times slower standard modified policy iteration 
 hoey et al          similar algorithm described uses algebraic decision
diagrams  adds   bahar  frohm  gaona  hachtel  macii  pardo    somenzi        rather
trees  adds simple generalization boolean decision diagrams  bdds   bryant 
      allow terminal nodes labeled real values instead boolean values 
essentially  add based algorithms similar tree based algorithms except
isomorphic subtrees shared  lets adds provide compact representations
certain types value functions  highly optimized add manipulation evaluation
software developed verification community applied solving mdps 
initial results provided  hoey et al         encouraging  showing considerable savings
tree based algorithms problems  example  add algorithm applied
    million state example described revealed value function
    distinct values  cf         tree leaves required  produced add description
value function less      internal nodes  solved problem
seven minutes     times faster earlier reported timing results using decision
trees  though improvement due use optimized add software
packages   similar results obtain problems  problems     million states
    dietterich flann        describe work context reinforcement learning rather
method solving mdps directly 

  

fidecision theoretic planning  structural assumptions

solved four hours   encouraging fact worst case
 exponential  examples  overhead associated using adds compared classical 
vector based methods is much less trees  about factor    compared   at 
modified policy iteration    state variables   lessens problems become larger 
tree based algorithms  methods yet applied real world problems 
exact abstraction schemes clear that  examples resulting policies value functions may compact  others set regions may get
large  even reaching level individual states boutilier et al          thus precluding
computational savings  boutilier dearden        develop approximation scheme
exploits tree structured nature value functions produced  stage k 
value function vk pruned produce smaller  less accurate tree approximates vk   specifically  approximate value functions represented using trees whose leaves
labeled upper lower bound value function region  decisiontheoretic regression performed bounds  certain subtrees value tree
pruned leaves subtree close value tree large
given computational constraints  scheme nonuniform  approximate adaptive 
approximation scheme tailored provide  roughly  accurate value
function given maximum tree size  smallest value function  with respect tree
size  given minimum accuracy  results reported  boutilier   dearden       
show approximation small set examples  including worst case examples
tree based algorithms  allows substantial reduction computational cost  instance 
   variable worst case example  small amount pruning introduced average error
     reduced computation time factor     aggressive pruning tends
increase error decrease computation time rapidly  making appropriate tradeoffs
two dimensions still addressed  method remains tested
evaluated realistic problems 
structured representations solution algorithms applied problems
fomdps  methods solving uence diagrams  shachter        exploit structure
natural way  tatman shachter        explore connection uence diagrams fomdps relationship uence diagram solution techniques
dynamic programming  boutilier poole        show classic history independent
methods solving pomdps  based conversion fomdp belief states  exploit types structured representations described here  however  exploiting structured
representations pomdps remains explored depth 
      abstract plans

one diculties adaptive abstraction schemes suggested fact
different abstractions must constructed repeatedly  incurring substantial computational overhead  overhead compensated savings obtained policy
construction e g   reducing number backups then problematic 
many cases savings dominated time space required generate
abstractions  thus motivates development cheaper less accurate approximate
clustering schemes 
  

fiboutilier  dean    hanks

another way reduce overhead adopt fixed abstraction scheme
one abstraction ever produced  approach adopted classical planning hierarchical abstraction based planners  pioneered sacerdoti s abstrips system  sacerdoti         similar form abstraction studied knoblock         see
knoblock  tenenberg    yang         work  variables  in case propositional 
ranked according criticality  roughly  important variables solution
planning problem  abstraction constructed deleting problem
description set propositions low criticality  solution abstract problem
plan achieves elements original goal deleted  however 
preconditions effects actions deleted accounted solution  might solution original problem  even so  abstract solution
used restrict search solution underlying concrete space  often
hierarchies refined abstractions used propositions introduced
back domain stages 
form abstraction uniform  propositions deleted uniformly  fixed  since
abstract solution need solution problem  might tempted view
approximate abstraction method  however  best think abstract
plan solution all  rather form heuristic information help solve
true problem quickly 
intuitions underlying knoblock s scheme applied dtp boutilier dearden               variables ranked according degree uence reward
function subset important variables deemed relevant  subset
determined  variables uence relevant variables effects
actions  which determined easily using strips  tbn action descriptions 
deemed relevant  on  remaining variables deemed irrelevant
deleted description problem  both action reward descriptions  
leaves abstract mdp smaller state space  i e   fewer variables  solved
standard methods  recall state space reduction exponential number
variables removed  view method uniform fixed approximate abstraction
scheme  unlike output classical abstraction methods  abstract policy produced
implemented value  degree optimal abstract policy
true optimal policy differ value bounded priori abstraction fixed 

example     simple illustration  suppose reward satisfying coffee requests

 or penalty satisfying them  substantially greater keeping
lab tidy delivering mail  suppose time pressure requires agent focus
specific subset objectives order produce small abstract state space 
case  four reward laden variables problem  see figure      cr
judged important  action descriptions used determine
variables  directly indirectly  affect probability achieving cr 
cr  rhc loc deemed relevant  allowing     rhm
ignored  state space thus reduced size     size     addition  several
action descriptions  e g   tidy  become trivial deleted   
  

fidecision theoretic planning  structural assumptions

advantage abstractions easily computed incur little
overhead  disadvantages uniform nature abstractions restrictive 
relevant  reward variables  determined policy constructed
without knowledge agent s ability control variables  result  important
variables those large impact reward but agent
control  may taken account  less important variables agent actually
uence ignored  however  series abstractions used take
account objectives decreasing importance  posteriori valuable objectives
dealt risk controllability taken account  boutilier et al  
       policies generated abstract levels used  seed  value
policy iteration less abstract levels  certain cases reducing time convergence
 dearden   boutilier         suggested  dearden   boutilier             
abstract value function used heuristic online search policies
improve abstract policy constructed  discussed section        thus  error
approximate value function overcome extent search  heuristic
function improved asynchronous updates 
different use abstraction adopted drips planner  haddawy   suwandi 
      haddawy   doan         actions abstracted collapsing  branches   possible outcomes  maintaining probabilistic intervals abstract  disjunctive effects 
actions combined decomposition hierarchy  much hierarchical
task networks  planning done evaluating abstract plans decomposition network  producing ranges utility possible instantiations plans  refining
plans possibly optimal  use task networks means search
restricted finite horizon  open loop plans action choice restricted possible refinements network  task networks offer useful way encode priori heuristic
knowledge structure good plans 
      model minimization reduction methods

abstraction techniques defined recast terms minimizing stochastic
automaton  providing unifying view different methods offering new insights
abstraction process  dean   givan         automata theory know
given finite state machine recognizing language l exists unique minimal
finite state machine   recognizes l  could       might
  exponentially smaller   minimal machine  called minimal
model language l  captures every relevant aspect machines
said equivalent  define similar notions equivalence mdps  since
primarily concerned planning  important equivalent mdps agree value
functions policies  practical standpoint  may necessary find
minimal model find reduced model suciently small still equivalent 
apply idea model minimization  or model reduction  planning follows 
begin using algorithm takes input implicit mdp model factored form
produces  if lucky  explicit  reduced model whose size within polynomial
factor size factored representation  use favorite state based
dynamic programming algorithms solve explicit model 
  

fiboutilier  dean    hanks

think dynamic programming techniques rely structured representations discussed earlier operating reduced model without ever explicitly constructing
model  cases  building reduced model may appropriate 
cases  one might save considerable effort explicitly constructing parts
reduced model absolutely necessary 
potential computational problems model minimization techniques sketched above  small minimal model may exist  may hard find 
instead  might look reduced model easier find necessarily minimal  could fail  case might look model small enough useful
approximately equivalent original factored model  careful
mean  approximate   intuitively two mdps approximately equivalent
corresponding optimal value functions within small factor one another 
order practical  mdp model reduction schemes operate directly implicit
factored representation original mdp  lee yannakakis        call online
model minimization  online model minimization starts initial partition states 
minimization iteratively refines partition splitting clusters smaller clusters 
cluster split states cluster behave differently respect
transitions states clusters  local property satisfied
clusters given partition  model consisting aggregate states correspond
clusters partition equivalent original model  addition 
initial partition method splitting clusters satisfy certain properties   
guaranteed find minimal model  case mdp reduction  initial partition
groups together states reward  nearly reward case
approximation methods 
clusters partitions manipulated online model reduction methods represented intensionally formulas involving state variables  instance  formula
rhc   loc m   represents set states robot coffee located
mail room  operations performed clusters require conjoining  complementing  simplifying  checking satisfiability  worst case  operations
intractable  successful application methods depends critically
problem way represented  illustrate basic idea simple
example 

example     figure    depicts simple version running example single
action  three boolean state variables corresponding rhc the robot
coffee  or not  rhc   cr there outstanding request coffee  or not  cr  
and  considering two location possibilities  loc c   the robot coffee
room  or not  loc c     whether outstanding coffee request depends
whether request previous stage whether robot
coffee room  location depends location previous stage 
reward depends whether outstanding coffee request 

    property required initial partition that  two states cluster partition
defining minimal model  recall minimal model unique   must cluster
initial partition 

  

fidecision theoretic planning  structural assumptions

st  

st

cr

cr

pr cr   
cr
cr
loc c 
loc c 
   
   
   

loc

loc

pr loc c          

rhc

pr rhc   
loc c 
loc c 
rhc
rhc
   
   
   

r


r s t      cr
  else

rhc

figure     factored model illustrating model reduction techniques 
cr loc c 

cr
cr

cr
cr loc c 
 a 

 b 

figure     models involving aggregate states   a  model corresponding initial
partition  b  minimal model 
initial partition shown figure    a  defined terms immediate rewards 
say states particular starting cluster behave respect
particular destination cluster probability ending destination
cluster states starting cluster  property satisfied
starting cluster cr destination cluster cr figure    a   split
cluster labeled cr obtain model figure    b   property satisfied
pairs clusters model figure    b  minimal model   
lee yannakakis algorithm non deterministic finite state machines
extended givan dean handle classical strips planning problems  givan   dean 
      mdps  dean   givan         basic step splitting cluster closely
related goal regression  relationship explored  givan   dean         variants
model reduction approach apply action space large represented
factored form  dean  givan    kim         example  action specified
set parameters corresponding allocations several different
resources optimization problem  exist algorithms computing approxi  

fiboutilier  dean    hanks







r

g

c

g
p

b

b



e

b

 a 

 b 

 c 

figure     reachability serial problem decomposition 
mate models  dean  givan    leach        ecient planning algorithms use
approximate models  givan  leach    dean        

    reachability analysis serial problem decomposition
      reachability analysis

existence goal states exploited different settings  instance  deterministic classical planning problems  regression viewed form directed dynamic
programming  without uncertainty  certain policy either reaches goal state not 
dynamic programming backups need performed goal states 
possible states  regression  therefore  implicitly exploits certain reachability characteristics domain along special structure value function 
reachability analysis applied much broadly forms basis various types
problem decomposition  decomposition problem solving  mdp broken several
subprocesses solved independently  roughly independently  solutions
pieced together  subprocesses whose solutions interact marginally treated
independent  might expect good nonoptimal global solution result  furthermore 
structure problem requires solution particular subproblem
needed  solutions subproblems ignored need computed
all  instance  regression analysis  optimal action states cannot reach
goal region irrelevant solution classical ai planning problem  shown
schematically figure    a   regions b never explored backward
search state space  states reach goal within search horizon
ever deemed relevant  regions b may reachable start state 
fact reach goal state means known irrelevant 
system dynamics stochastic  scheme form basis approximately
optimal solution method  regions b ignored unlikely transition
regression goal region  region r   similar remarks using progression forward
search start state apply  illustrated figure    b  
  

fidecision theoretic planning  structural assumptions

several schemes proposed ai literature exploiting reachability
constraints  apart usual forward  backward search approaches  peot smith
       introduce operator graph  structure computed prior problem solving
caches reachability relationships among propositions  graph consulted
planning process deciding actions insert plan resolve
threats 
graphplan algorithm blum furst        attempts blend considerations
forward backward reachability deterministic planning context  one
diculties regression may regress goal region sequence
operators find region cannot reached initial state 
figure    a   example  states region r may reachable initial
state  graphplan constructs variant operator graph called planning graph 
certain forward reachability constraints posted  regression implemented
usual  current subgoal set violates forward reachability constraints
point  subgoal set abandoned regression search backtracks 
conceptually  one might think graphplan constructing forward search tree
state space initial state root  backward search
goal region backward tree  course  process state based 
instead  constraints possible variable values hold simultaneously different
planning stages recorded  regression used search backward planning
graph  sense  graphplan viewed constructing abstraction
forward reachable states distinguished unreachable states planning stage 
using distinction among abstract states quickly identify infeasible regression
paths  note  however  graphplan approximates distinction overestimating
set reachable states  overestimation  as opposed underestimation  ensures
regression search space contains legitimate plans 
reachability exploited solution general mdps  dean
et al         propose envelope method solving  goal based  mdps approximately 
assuming path generated quickly given start state goal region 
mdp consisting states path perhaps neighboring states solved 
deal transitions lead envelope  heuristic method estimates value
states    time permits  set neighboring states expanded  increasing
solution quality accurately evaluating quality alternative actions 
ideas underlying graphplan applied general mdps
 boutilier  brafman    geib         construction planning graph generalized deal stochastic  conditional action representation offered  tbns  given
initial state  or set initial states   algorithm discovers reachability constraints
form graphplan   instance  two variable values x   x 
  y  cannot obtain simultaneously  is  action sequence starting
given initial state lead state values hold    reachability
constraints discovered process used simplify action reward representation mdp refers reachable states  case  action
    approximate abstraction techniques described section       might used generate
heuristic information 
    general k ary constraints type considered  boutilier et al         

  

fiboutilier  dean    hanks

requires unreachable set values hold effectively deleted  cases  certain
variables discovered immutable given initial conditions
deleted  leading much smaller mdps  simplified representation retains original
propositional structure standard abstraction methods applied reachable
mdp  suggested strong synergy exists abstraction reachability analysis together techniques reduce size  effective  mdp
solved much dramatically either isolation  reachability constraints used prune regression paths deterministic domains  used
prune value function policy estimates generated decision theoretic regression
abstraction algorithms  boutilier et al         
results reported  boutilier et al         limited single process planning
domain  show reachability analysis together abstraction provide substantial reductions size effective mdp must solved  least domains 
domain    binary variables  reachability considerations generally eliminated
order       variables  depending initial state arity binary
ternary of constraints considered   reducing state space size     anywhere
          incorporating abstraction reachable mdp provided considerably
reduction  reducing mdp sizes ranging    effectively zero states 
latter case would occur discovered values variables impact reward
altered in case every course action expected utility
mdp needn t solved  or solved applying null actions zero cost  
      serial problem decomposition communicating structure

communicating reachability structure mdp provides way formalize different types problem decomposition  classify mdp according markov
chains induced stationary policies admits  fixed markov chain  group
states maximal recurrent classes transient states  described section     
mdp recurrent policy induces markov chain single recurrent class 
mdp unichain policy induces single recurrent class  possibly  transient states  mdp communicating pair states s  t  policy
reach t  mdp weakly communicating exists closed set
states communicating plus  possibly  set states transient every policy 
call mdps noncommunicating 
notions crucial construction optimal average reward policies 
exploited problem decomposition  suppose mdp discovered consist
set recurrent classes c    cn  i e   matter policy adopted  agent cannot
leave class enters class  set transient states    clear
optimal policy restricted class ci constructed without reference policy
decisions made states outside ci even values  essentially  ci
viewed independent subprocess 
    simple way view classes think agent adopting randomized policy action
adopted state positive probability  classes induced markov chain correspond
classes mdp 

  

fidecision theoretic planning  structural assumptions

observation leads following suggestion optimal policy construction   
solve subprocesses consisting recurrent classes mdps  remove
states mdp  forming reduced mdp consisting transient states 
break reduced mdp recurrent classes solve independently 
key effectively use value function original recurrent
states  computed solving independent subproblems step    take account
transitions recurrent classes reduced mdp  figure    c  shows mdp
broken classes might constructed way  original mdp  classes c
e recurrent solved independently  removed mdp  class
recurrent reduced mdp  can  course  solved without reference classes
b   rely value states transitions class e   however 
value function e available purpose  used solve
consisted jdj states  hand  b solved  finally
solved  lin dean        provide version type decomposition
employs factored representation  factored representation allows dimensionality
reduction different state subspaces aggregating states differ values
irrelevant variables subspaces 
key decomposition discovery recurrent classes mdp 
puterman        suggests adaptation fox landi algorithm  fox   landi       
discovering structure markov chains o n      recall n   jsj     alleviate
diculties algorithms work explicit state based representation  boutilier
puterman        propose variant algorithm works factored  tbn
representation 
one diculty form decomposition reliance strongly independent
subproblems  i e   recurrent classes  within mdp  others explored exact approximate techniques work less restrictive assumptions  one simple method
approximation construct  approximately recurrent classes   figure    c  might
imagine c e nearly independent sense transitions
low probability high cost  treating independent might lead approximately optimal policies whose error bounded  solutions c e interact
strongly enough solutions constructed completely independently 
different approach solving decomposed problem taken 
optimal value function e then  pointed out  calculate
optimal value function d  first thing note don t need know
value function states e   value every state e reachable
state single step  set states outside reachable single
step state inside referred states periphery d  values
states intersection e periphery summarize value exiting
ending e   refer set states periphery block
kernel mdp  different blocks interact one another
states kernel 
    ross varadarajan        make related suggestion solving average reward problems 
    slight correction made suggested algorithm  boutilier   puterman        

  

fiboutilier  dean    hanks

loc c 

loc l 

loc m  

loc o 

figure     decomposition based location 

loc c 

loc l 
kernel

loc m  

loc o 

figure     kernel based decomposition depicting kernel states 

  

fidecision theoretic planning  structural assumptions

example     spatial features often provide natural dimension along decom 

pose domain  running example  location robot might used
decompose state space blocks states  one block possible locations  figure    shows decomposition superimposed state transition
diagram mdp  states kernel shaded might correspond
entrances exits locations  star shaped topology  induced kernel
decomposition used  kushner   chen         dean   lin         illustrated
figure     figure     hallway location explicitly represented 
simplification may reasonable hallway conduit moving
one room another  case function hallway accounted
dynamics governing states kernel  figures       idealized that  given
full set features running example  kernel would contain many
states   

one technique computing optimal policy entire mdp involves repeatedly
solving mdps corresponding individual blocks  techniques works follows 
initially  guess value every state kernel    given current estimate
values kernel states  solve component mdps  solution produces new
estimate states kernel  adjust values states kernel
considering difference current new estimates iterate
difference negligible 
iterative method solving decomposed mdp special case lagrangian
method finding extrema function  literature replete
methods linear nonlinear systems equations  winston         possible
formulate mdp linear program  d epenoux        puterman         dantzig
wolfe        developed method decomposing system equations involving
large number variables set smaller systems equations interacting set
coupling variables  variables shared two blocks   dantzig wolfe
decomposition method  original  large system equations solved iteratively
solving smaller systems adjusting coupling variables iteration
adjustment required  linear programming formulation mdp 
values states encoded variables 
kushner chen        exploit fact mdps modeled linear programs
using dantzig wolfe decomposition method solve mdps involving large number
states  dean lin        describe general framework solving decomposed mdps
pointing work kushner chen special case  neither work addresses
issue decompositions come from  dean et al         investigate methods
decomposing state space two blocks  reachable k steps fewer
reachable k steps  see discussion reachability above   set states
reachable k fewer steps used construct mdp basis policy
approximates optimal policy  k increases  size block states reachable
k steps increases  ensuring better solution  amount time required compute
    ideally would aggregate kernel states value provide compact representation 
remainder section  however  won t consider opportunities combining
aggregation decomposition methods 

  

fiboutilier  dean    hanks

solution increases  dean et al         discuss methods solving mdps time critical
problems trading quality time 
ignored issue obtain decompositions expedite calculations  ideally  component decomposition would yield simplification via
aggregation abstraction  reducing dimensionality component thereby
avoiding explicit enumeration states  lin        presents methods exploiting
structure certain special cases communicating structure revealed
domain expert  general  however  finding decomposition minimize effort
spent solving component mdps quite hard  at least hard finding smallest circuit consistent given input output behavior  best hope
good heuristic methods  unfortunately  aware particularly useful
heuristics finding serial decompositions markov decision processes  developing
heuristics clearly area investigation 
related form decomposition development macro operators mdps
 sutton         macros long history classical planning problem solving  fikes 
hart    nilsson        korf         recently generalized mdps
 hauskrecht  meuleau  kaelbling  dean    boutilier        parr        parr   russell       
precup  sutton    singh        stone   veloso        sutton        thrun   schwartz 
       work  macro taken local policy region state
space  or block terminology   given mdp comprising blocks
set macros defined block  mdp solved selecting macro action
block global policy induced set macros picked close
optimal  least best combination macros set available 
 sutton        precup et al          macros treated temporally abstract actions
models defined macro treated single action
used policy value iteration  along concrete actions    hauskrecht et al        
parr        parr   russell         models exploited hierarchical fashion 
high level mdp consisting states lying boundaries blocks  macros
 actions  chosen states  issue macro generation 
constructing set macros guaranteed provide exibility select close optimal
global behavior is addressed  hauskrecht et al         parr         relationship
serial decomposition techniques quite close  thus  problems discovering good
decompositions  constructing good sets macros  exploiting intensional representations
areas clearer  compelling solutions required  date  work area
provided much computational utility solution mdps except cases
good  hand crafted  region based decompositions macros provided and little
work taken account factored nature many mdps  reason 
discuss detail  however  general notion serial decomposition continues
develop shows great promise 

    multiattribute reward parallel decomposition
another form decomposition parallel decomposition  mdp broken
set sub mdps  run parallel   specifically  stage  global 
decision process  state subprocess affected  instance  figure     action
  

fidecision theoretic planning  structural assumptions



mdp 



mdp 



mdp 

figure     parallel problem decomposition 

affects state subprocess  intuitively  action suitable execution

original mdp state reasonably good sub mdps 
generally  sub mdps form either product join decomposition original
state space  contrast union decompositions state space determined serial
decompositions   state space formed taking cross product sub mdp state
spaces  join certain states subprocesses cannot linked  subprocesses
may identical action spaces  as figure      may action space 
global action choice factored choice subprocess  latter
case  sub mdps may completely independent  case  global  mdp
solved exponentially faster  challenging problem arises constraints
legal action combinations  example  actions subprocesses
require certain shared resources  interactions global choice may arise 
parallel mdp decomposition  wish solve sub mdps use policies
value functions generated help construct optimal approximately optimal solution
original mdp  highlighting need find appropriate decompositions mdps
develop suitable merging techniques  recent parallel decomposition methods
involved decomposing mdp subprocesses suitable distinct objectives  since
reward functions often deal multiple objectives  associated independent
reward  whose rewards summed determine global reward  often
natural way decompose mdps  thus  ideas multiattribute utility theory
seen play role solution mdps 
boutilier et al         decompose mdp specified using  tbns additive reward
function using abstraction technique described section        component
reward function  abstraction used generate mdp referring variables
relevant component    since certain state variables may present multiple
sub mdps  i e   relevant one objective   original state space join
subspaces  thus  decomposition tackled automatically  merging tackled several
ways  one involves using sum value functions obtained solving sub mdps
heuristic estimate true value function  heuristic used guide online 
state based search  see section         sub mdps interact  heuristic
perfect leads backtrack free optimal action selection  interact  search
    note existence factored mdp representation crucial abstraction method 

  

fiboutilier  dean    hanks

required detect con icts  note sub mdp identical sets actions 
action space large  branching factor search process may prohibitive 
singh cohn        deal parallel decomposition  though assume
global mdp specified explicitly set parallel mdps  thus generating decompositions
global mdp issue  global mdp given cross product state
action spaces sub mdps reward functions summed  however 
constraints feasible action combinations couple solutions sub mdps 
solve global mdp  sum sub mdp value functions used upper bound
optimal global value function  maximum  at global state 
used lower bound  bounds form basis action elimination procedure
value iteration algorithm solving global mdp    unfortunately  value iteration
run explicit state space global mdp  since action space cross
product  potential computational bottleneck value iteration  well 
meuleau et al         use parallel decomposition approximate solution stochastic resource allocation problems large state action spaces  much singh
cohn         mdp specified terms number independent mdps 
involving distinct objective  whose action choices linked shared resource constraints  value functions individual mdps constructed oine used
set online action selection procedures  unlike many approximation procedures
discussed  approach makes attempt construct policy explicitly  and
similar real time search rtdp respect  construct value function
explicitly  method applied large mdps  state spaces size
      actions spaces even larger  solve problems roughly half
hour  solutions produced approximate  size problem precludes
exact solution  good estimates solution quality hard derive  however 
method applied smaller problems nature whose exact solution
computed  approximations high quality  meuleau et al          able
solve large mdps  with large  factored  state action spaces   model
relies somewhat restrictive assumptions nature local value functions
ensure good solution quality  however  basic approach appears generalizable 
offers great promise solving large factored mdps 
algorithms  singh   cohn         meuleau et al         seen
rely least implicitly structured mdp representations involving almost independent
subprocesses  seems likely approaches could take advantage automatic
mdp decomposition algorithms  boutilier et al          factored
representations explicitly play part 

    summary

seen number ways intensional representations exploited
solve mdps effectively without enumeration state space  include techniques
abstraction mdps  including based relevance analysis  goal regression
decision theoretic regression  techniques relying reachability analysis serial decomposition  methods parallel mdp decomposition exploiting multiattribute nature
    singh cohn        incorporate methods removing unreachable states value iteration 

  

fidecision theoretic planning  structural assumptions

reward functions  many methods can  fortunate circumstances  offer exponential reduction solution time space required represent policy value function 
none come guarantees reductions except certain special cases 
methods described provide approximate solutions  often error bounds provided   offer optimality guarantees general  provide optimal
solutions suitable assumptions 
one avenue explored detail relationship structured solution methods developed mdps described techniques used solving
bayesian networks  since many algorithms discussed section rely structure inherent  tbn representation mdp  natural ask whether
embody intuitions underlie solution algorithms bayes nets  thus
whether solution techniques bayes nets  directly indirectly  applied
mdps ways give rise algorithms similar discussed here  remains
open question point  undoubtedly strong ties exist  tatman shachter
       explored connections uence diagrams mdps  kjaerulff
       investigated computational considerations involved applying join tree methods
reasoning tasks monitoring prediction temporal bayes nets  abstraction methods discussed section       interpreted form variable elimination
 dechter        zhang   poole         elimination variables occurs temporal order 
good orderings within time slice must exploit tree graph structure
cpts  approximation schemes based variable elimination  dechter        poole       
may related certain approximation methods developed mdps 
independence based decompositions mdps discussed section     clearly viewed
exploiting independence relations made explicit  unrolling   tbn  development connections bayes net inference algorithms doubt prove
useful enhancing understanding existing methods  increasing range
applicability pointing new algorithms 

   concluding remarks
search effective algorithms controlling automated agents long important history  problem continue grow importance decisionmaking functionality automated  work several disciplines  among ai  decision
analysis  or  addressed problem  carried different problem definitions  different sets simplifying assumptions  different viewpoints  hence
different representations algorithms problem solving  often not  assumptions seem made historical reasons reasons convenience 
often dicult separate essential assumptions accidental  important
clarify relationships among problem definitions  crucial assumptions  solution
techniques  meaningful synthesis take place 
paper analyzed various approaches particular class sequential decision problems studied or  decision analysis  ai literature 
started general  reasonably neutral statement problem  couched  convenience  language markov decision processes  demonstrated
various disciplines define problem  i e   assumptions make   effect
  

fiboutilier  dean    hanks

assumptions worst case time complexity solving problem defined 
assumptions regarding two main factors seem distinguish commonly studied
classes decision problems 

observation sensing  sensing tend fast  cheap  accurate laborious 
costly noisy 

incentive structure agent  behavior evaluated ability perform
particular task  ability control system interval time 

moving beyond worst case analysis  generally assumed that  although pathological cases inevitably dicult  agent able solve  typical   easy 
cases effectively  so  agent needs able identify structure problem
exploit structure algorithmically 
identified three ways structural regularities recognized  represented 
exploited computationally  first structure induced domain level simplifying
assumptions full observability  goal satisfaction time separable value functions 
on  second structure exploited compact domain specific encodings states 
actions  rewards  designer use techniques make structure explicit 
decision making algorithms exploit structural regularities apply
particular problem hand  third involves aggregation  abstraction decomposition techniques  whereby structural regularities discovered exploited
problem solving process itself  developing framework one allows comparison
domains  assumptions  problems  techniques drawn different disciplines we
discover essential problem structure required specific representations algorithms
prove effective  way insights techniques developed
certain problems  within certain disciplines  evaluated potentially applied
new problems  within disciplines 
main focus work elucidation various forms structure
decision problems exploited representationally computationally 
part  focused propositional structure  commonly associated planning ai circles  complete treatment would included
compact representations dynamics  rewards  policies  value functions often
considered continuous  real valued domains  instance  discussed linear
dynamics quadratic cost functions  often used control theory  caines        
use neural network representations value functions  frequently adopted within
reinforcement learning community  bertsekas   tsitsiklis        tesauro          
discussed partitioning continuous state spaces often addressed reinforcement
learning  moore   atkeson         neither addressed relational quantificational structure used first order planning representations  however  even techniques
cast within framework described here  example  use piecewise linear
value functions seen form abstraction different linear components
applied different regions clusters state space 
    bertsekas tsitsiklis        provide in depth treatment neural network linear function
approximators mdps reinforcement learning 

  

fidecision theoretic planning  structural assumptions

although certain cases indicated devise methods exploit several
types structure once  research along lines limited  extent 
many representations algorithms described paper complementary
pose obstacles combination  remains seen interact
techniques developed forms structure  used continuous state
action spaces 
analysis raises opportunities challenges  understanding assumptions 
techniques  relationships  designer decision making agents many
tools build effective problem solvers  challenges lie development
additional tools integration existing ones 

acknowledgments

many thanks careful comments referees  thanks ron parr robert
st aubin comments earlier draft paper  students taking cs    
 spring       taught martha pollack university pittsburgh cpsc   
 winter       university british columbia deserve thanks detailed
comments 
boutilier supported nserc research grant ogp         nce irisii program project ic    dean supported part national science foundation
presidential young investigator award iri         air force advanced
research projects agency department defense contract no  f         c      hanks supported part arpa   rome labs grant f               
part nsf grant iri         

references

allen  j   hendler  j     tate  a   eds            readings planning  morgan kaufmann 
san mateo 
astrom  k  j          optimal control markov decision processes incomplete state
estimation  j  math  anal  appl               
bacchus  f   boutilier  c     grove  a          rewarding behaviors  proceedings
thirteenth national conference artificial intelligence  pp            portland 
or 
bacchus  f   boutilier  c     grove  a          structured solution methods nonmarkovian decision processes  proceedings fourteenth national conference
artificial intelligence  pp          providence  ri 
bacchus  f     kabanza  f          using temporal logic control search
forward chaining planner 
proceedings third european
workshop planning  ewsp     assisi  italy  available via url
ftp   logos uwaterloo ca  pub tlplan tlplan ps z 
bacchus  f     teh  y  w          making forward chaining relevant  proceedings
fourth international conference ai planning systems  pp        pittsburgh  pa 
  

fiboutilier  dean    hanks

bahar  r  i   frohm  e  a   gaona  c  m   hachtel  g  d   macii  e   pardo  a     somenzi 
f          algebraic decision diagrams applications  international conference computer aided design  pp           ieee 
baker  a  b          nonmonotonic reasoning framework situation calculus 
artificial intelligence           
barto  a  g   bradtke  s  j     singh  s  p          learning act using real time dynamic
programming  artificial intelligence                   
bellman  r          dynamic programming  princeton university press  princeton  nj 
bertsekas  d  p     castanon  d  a          adaptive aggregation infinite horizon
dynamic programming  ieee transactions automatic control                  
bertsekas  d  p          dynamic programming  prentice hall  englewood cliffs  nj 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena  belmont 
ma 
blackwell  d          discrete dynamic programming  annals mathematical statistics 
            
blum  a  l     furst  m  l          fast planning graph analysis  proceedings
fourteenth international joint conference artificial intelligence  pp       
     montreal  canada 
bonet  b     geffner  h          learning sorting decision trees pomdps 
proceedings fifteenth international conference machine learning  pp       
madison  wi 
bonet  b   loerincs  g     geffner  h          robust fast action selection mechanism 
proceedings fourteenth national conference artificial intelligence  pp 
        providence  ri 
boutilier  c          correlated action effects decision theoretic regression  proceedings thirteenth conference uncertainty artificial intelligence  pp       
providence  ri 
boutilier  c   brafman  r  i     geib  c          prioritized goal decomposition markov
decision processes  toward synthesis classical decision theoretic planning 
proceedings fifteenth international joint conference artificial intelligence 
pp            nagoya  japan 
boutilier  c   brafman  r  i     geib  c          structured reachability analysis markov
decision processes  proceedings fourteenth conference uncertainty
artificial intelligence  pp        madison  wi 
boutilier  c     dearden  r          using abstractions decision theoretic planning
time constraints  proceedings twelfth national conference artificial
intelligence  pp            seattle  wa 
  

fidecision theoretic planning  structural assumptions

boutilier  c     dearden  r          approximating value trees structured dynamic
programming  proceedings thirteenth international conference machine
learning  pp        bari  italy 
boutilier  c   dearden  r     goldszmidt  m          exploiting structure policy construction  proceedings fourteenth international joint conference artificial
intelligence  pp            montreal  canada 
boutilier  c   dearden  r     goldszmidt  m          stochastic dynamic programming
factored representations   manuscript  
boutilier  c   friedman  n   goldszmidt  m     koller  d          context specific independence bayesian networks  proceedings twelfth conference uncertainty
artificial intelligence  pp          portland  or 
boutilier  c     goldszmidt  m          frame problem bayesian network action
representations  proceedings eleventh biennial canadian conference
artificial intelligence  pp        toronto 
boutilier  c     poole  d          computing optimal policies partially observable
decision processes using compact representations  proceedings thirteenth
national conference artificial intelligence  pp            portland  or 
boutilier  c     puterman  m  l          process oriented planning average reward optimality  proceedings fourteenth international joint conference artificial
intelligence  pp            montreal  canada 
brafman  r  i          heuristic variable grid solution method pomdps  proceedings fourteenth national conference artificial intelligence  pp         
providence  ri 
bryant  r  e          graph based algorithms boolean function manipulation  ieee
transactions computers  c                 
bylander  t          computational complexity propositional strips planning 
artificial intelligence              
caines  p  e          linear stochastic systems  wiley  new york 
cassandra  a  r   kaelbling  l  p     littman  m  l          acting optimally partially
observable stochastic domains  proceedings twelfth national conference
artificial intelligence  pp            seattle  wa 
cassandra  a  r   littman  m  l     zhang  n  l          incremental pruning  simple  fast  exact method pomdps  proceedings thirteenth conference
uncertainty artificial intelligence  pp        providence  ri 
chapman  d          planning conjunctive goals  artificial intelligence                  
  

fiboutilier  dean    hanks

chapman  d     kaelbling  l  p          input generalization delayed reinforcement
learning  algorithm performance comparisons  proceedings twelfth
international joint conference artificial intelligence  pp          sydney  australia 
dantzig  g     wolfe  p          decomposition principle dynamic programs  operations
research                 
dean  t   allen  j     aloimonos  y          artificial intelligence  theory practice 
benjamin cummings 
dean  t     givan  r          model minimization markov decision processes 
proceedings fourteenth national conference artificial intelligence  pp      
    providence  ri  aaai 
dean  t   givan  r     kim  k  e          solving planning problems large state
action spaces  proceedings fourth international conference ai planning
systems  pp          pittsburgh  pa 
dean  t   givan  r     leach  s          model reduction techniques computing approximately optimal solutions markov decision processes  proceedings
thirteenth conference uncertainty artificial intelligence  pp          providence  ri 
dean  t   kaelbling  l   kirman  j     nicholson  a          planning deadlines
stochastic domains  proceedings eleventh national conference artificial
intelligence  pp          
dean  t   kaelbling  l   kirman  j     nicholson  a          planning time constraints stochastic domains  artificial intelligence                 
dean  t     kanazawa  k          model reasoning persistence causation 
computational intelligence                 
dean  t     lin  s  h          decomposition techniques planning stochastic domains  proceedings fourteenth international joint conference artificial
intelligence  pp            
dean  t     wellman  m          planning control  morgan kaufmann  san mateo 
california 
dearden  r     boutilier  c          integrating planning execution stochastic
domains  proceedings tenth conference uncertainty artificial intelligence  pp          washington  dc 
dearden  r     boutilier  c          abstraction approximate decision theoretic planning  artificial intelligence              
dechter  r          bucket elimination  unifying framework probabilistic inference 
proceedings twelfth conference uncertainty artificial intelligence  pp 
        portland  or 
  

fidecision theoretic planning  structural assumptions

dechter  r          mini buckets  general scheme generating approximations
automated reasoning probabilistic inference  proceedings fifteenth international joint conference artificial intelligence  pp            nagoya  japan 
d epenoux  f          sur un probleme de production et de stockage dans l aleatoire 
management science             
dietterich  t  g     flann  n  s          explanation based learning reinforcement
learning  unified approach  proceedings twelfth international conference
machine learning  pp          lake tahoe  nv 
draper  d   hanks  s     weld  d       a   probabilistic model action leastcommitment planning information gathering  proceedings tenth conference uncertainty artificial intelligence  pp          washington  dc 
draper  d   hanks  s     weld  d       b   probabilistic planning information gathering contingent execution  proceedings second international conference
ai planning systems  pp        
etzioni  o   hanks  s   weld  d   draper  d   lesh  n     williamson  m         
approach planning incomplete information  proceedings third international conference principles knowledge representation reasoning  pp 
        boston  ma 
fikes  r   hart  p     nilsson  n          learning executing generalized robot plans 
artificial intelligence             
fikes  r     nilsson  n  j          strips  new approach application theorem
proving problem solving  artificial intelligence             
finger  j          exploiting constraints design synthesis  ph d  thesis  stanford university  stanford 
floyd  r  w          algorithm     shortest path   communications acm        
    
fox  b  l     landi  d  m          algorithm identifying ergodic subchains
transient states stochastic matrix  communications acm             
french  s          decision theory  halsted press  new york 
geiger  d     heckerman  d          advances probabilistic reasoning  proceedings
seventh conference uncertainty artificial intelligence  pp          los
angeles  ca 
givan  r     dean  t          model minimization  regression  propositional strips
planning  proceedings fifteenth international joint conference artificial
intelligence  pp            nagoya  japan 
  

fiboutilier  dean    hanks

givan  r   leach  s     dean  t          bounded parameter markov decision processes 
proceedings fourth european conference planning  ecp      pp         
toulouse  france 
goldman  r  p     boddy  m  s          representing uncertainty simple planners 
proceedings fourth international conference principles knowledge
representation reasoning  pp          bonn  germany 
haddawy  p     doan  a          abstracting probabilistic actions  proceedings
tenth conference uncertainty artificial intelligence  pp          washington 
dc 
haddawy  p     hanks  s          utility models goal directed decision theoretic
planners  computational intelligence         
haddawy  p     suwandi  m          decision theoretic refinement planning using inheritence abstraction  proceedings second international conference ai planning systems  pp          chicago  il 
hanks  s          projecting plans uncertain worlds  ph d  thesis      yale university 
department computer science  new haven  ct 
hanks  s     mcdermott  d  v          modeling dynamic uncertain world i  symbolic
probabilistic reasoning change  artificial intelligence               
hanks  s   russell  s     wellman  m   eds            decision theoretic planning  proceedings aaai spring symposium  aaai press  menlo park 
hansen  e  a     zilberstein  s          heuristic search cyclic and or graphs 
proceedings fifteenth national conference artificial intelligence  pp      
    madison  wi 
hauskrecht  m          heuristic variable grid solution method pomdps  proceedings fourteenth national conference artificial intelligence  pp         
providence  ri 
hauskrecht  m          planning control stochastic domains imperfect information  ph d  thesis  massachusetts institute technology  cambridge 
hauskrecht  m   meuleau  n   kaelbling  l  p   dean  t     boutilier  c          hierarchical
solution markov decision processes using macro actions  proceedings
fourteenth conference uncertainty artificial intelligence  pp          madison 
wi 
hoey  j   st aubin  r   hu  a     boutilier  c          spudd  stochastic planning
using decision diagrams  proceedings fifteenth conference uncertainty
artificial intelligence stockholm  appear 
howard  r  a          dynamic programming markov processes  mit press  cambridge  massachusetts 
  

fidecision theoretic planning  structural assumptions

howard  r  a     matheson  j  e          uence diagrams  howard  r  a     matheson  j  e   eds    principles applications decision analysis  strategic
decisions group  menlo park  ca 
kambhampati  s          refinement planning unifying framework plan synthesis 
ai magazine  summer             
kearns  m   mansour  y     ng  a  y          sparse sampling algorithm nearoptimal planning large markov decision processes  proceedings sixteenth
international joint conference artificial intelligence stockholm  appear 
keeney  r  l     raiffa  h          decisions multiple objectives  preferences
value tradeoffs  john wiley sons  new york 
kjaerulff  u          computational scheme reasoning dynamic probabilistic networks  proceedings eighth conference uncertainty ai  pp         
stanford 
knoblock  c  a          generating abstraction hierarchies  automated approach
reducing search planning  kluwer  boston 
knoblock  c  a   tenenberg  j  d     yang  q          characterizing abstraction hierarchies planning  proceedings ninth national conference artificial
intelligence  pp          anaheim  ca 
koenig  s          optimal probabilistic decision theoretic planning using markovian
decision theory  m sc  thesis ucb csd         university california berkeley 
computer science department 
koenig  s     simmons  r          real time search nondeterministic domains 
proceedings fourteenth international joint conference artificial intelligence 
pp            montreal  canada 
korf  r          macro operators  weak method learning  artificial intelligence     
      
korf  r  e          real time heuristic search  artificial intelligence              
kushmerick  n   hanks  s     weld  d          algorithm probabilistic planning 
artificial intelligence              
kushner  h  j     chen  c  h          decomposition systems governed markov
chains  ieee transactions automatic control                  
lee  d     yannakakis  m          online minimization transition systems  proceedings
  th annual acm symposium theory computing  pp          victoria 
bc 
lin  f     reiter  r          state constraints revisited  journal logic computation 
               
  

fiboutilier  dean    hanks

lin  s  h          exploiting structure planning control  ph d  thesis  department
computer science  brown university 
lin  s  h     dean  t          generating optimal policies high level plans conditional branches loops  proceedings third european workshop
planning  ewsp      pp          
littman  m  l          probabilistic propositional planning  representations complexity  proceedings fourteenth national conference artificial intelligence 
pp          providence  ri 
littman  m  l   dean  t  l     kaelbling  l  p          complexity solving
markov decision problems  proceedings eleventh conference uncertainty
artificial intelligence  pp          montreal  canada 
littman  m  l          algorithms sequential decision making  ph d  thesis cs       
brown university  department computer science  providence  ri 
lovejoy  w  s       a   computationally feasible bounds partially observed markov
decision processes  operations research                  
lovejoy  w  s       b   survey algorithmic methods partially observed markov
decision processes  annals operations research            
luenberger  d  g          introduction linear nonlinear programming  addisonwesley  reading  massachusetts 
luenberger  d  g          introduction dynamic systems  theory  models applications  wiley  new york 
madani  o   condon  a     hanks  s          undecidability probabilistic planning
infinite horizon partially observable markov decision problems  proceedings
sixteenth national conference artificial intelligence orlando  fl  appear 
mahadevan  s          discount discount reinforcement learning  case
study comparing r learning q learning  proceedings eleventh international conference machine learning  pp          new brunswick  nj 
mcallester  d     rosenblitt  d          systematic nonlinear planning  proceedings
ninth national conference artificial intelligence  pp          anaheim  ca 
mccallum  r  a          instance based utile distinctions reinforcement learning
hidden state  proceedings twelfth international conference machine
learning  pp          lake tahoe  nevada 
mccarthy  j     hayes  p  j          philosophical problems standpoint
artificial intelligence  machine intelligence             
  

fidecision theoretic planning  structural assumptions

meuleau  n   hauskrecht  m   kim  k   peshkin  l   kaelbling  l   dean  t     boutilier  c 
        solving large weakly coupled markov decision processes  proceedings
fifteenth national conference artificial intelligence  pp          madison 
wi 
moore  a  w     atkeson  c  g          parti game algorithm variable resolution
reinforcement learning multidimensional state spaces  machine learning          
    
papadimitriou  c  h     tsitsiklis  j  n          complexity markov chain decision
processes  mathematics operations research                  
parr  r          flexible decomposition algorithms weakly coupled markov decision
processes  proceedings fourteenth conference uncertainty artificial
intelligence  pp          madison  wi 
parr  r     russell  s          approximating optimal policies partially observable
stochastic domains  proceedings fourteenth international joint conference
artificial intelligence  pp            montreal 
parr  r     russell  s          reinforcement learning hierarchies machines 
jordan  m   kearns  m     solla  s   eds    advances neural information processing
systems     pp             mit press  cambridge 
pearl  j          probabilistic reasoning intelligent systems  networks plausible
inference  morgan kaufmann  san mateo 
pednault  e          adl  exploring middle ground strips situation calculus  proceedings first international conference principles
knowledge representation reasoning  pp          toronto  canada 
penberthy  j  s     weld  d  s          ucpop  sound  complete  partial order planner
adl  proceedings third international conference principles knowledge
representation reasoning  pp          boston  ma 
peot  m     smith  d          conditional nonlinear planning  proceedings first
international conference ai planning systems  pp          college park  md 
perez  m  a     carbonell  j  g          control knowledge improve plan quality 
proceedings second international conference ai planning systems  pp      
    chicago  il 
poole  d          exploiting rule structure decision making within independent
choice logic  proceedings eleventh conference uncertainty artificial
intelligence  pp          montreal  canada 
poole  d       a   independent choice logic modelling multiple agents uncertainty  artificial intelligence                 
  

fiboutilier  dean    hanks

poole  d       b   probabilistic partial evaluation  exploiting rule structure probabilistic
inference  proceedings fifteenth international joint conference artificial
intelligence  pp            nagoya  japan 
poole  d          context specific approximation probabilistic inference  proceedings
fourteenth conference uncertainty artificial intelligence  pp         
madison  wi 
precup  d   sutton  r  s     singh  s          theoretical results reinforcement learning
temporally abstract behaviors  proceedings tenth european conference
machine learning  pp          chemnitz  germany 
pryor  l     collins  g          cassandra  planning contingencies  technical
report     northwestern university  institute learning sciences 
puterman  m  l          markov decision processes  john wiley   sons  new york 
puterman  m  l     shin  m          modified policy iteration algorithms discounted
markov decision problems  management science                
ross  k  w     varadarajan  r          multichain markov decision processes
sample path constraint  decomposition approach  mathematics operations research                  
russell  s     norvig  p          artificial intelligence  modern approach  prentice hall 
englewood cliffs  nj 
sacerdoti  e  d          planning hierarchy abstraction spaces  artificial intelligence 
           
sacerdoti  e  d          nonlinear nature plans  proceedings fourth
international joint conference artificial intelligence  pp          
schoppers  m  j          universal plans reactive robots unpredictable environments 
proceedings tenth international joint conference artificial intelligence 
pp            milan  italy 
schwartz  a          reinforcement learning method maximizing undiscounted rewards  proceedings tenth international conference machine learning 
pp          amherst  ma 
schweitzer  p  l   puterman  m  l     kindle  k  w          iterative aggregationdisaggregation procedures discounted semi markov reward processes  operations
research              
shachter  r  d          evaluating uence diagrams  operations research              
    
shimony  s  e          role relevance explanation i  irrelevance statistical
independence  international journal approximate reasoning                 
  

fidecision theoretic planning  structural assumptions

simmons  r     koenig  s          probabilistic robot navigation partially observable
environments  proceedings fourteenth international joint conference
artificial intelligence  pp            montreal  canada 
singh  s  p     cohn  d          dynamically merge markov decision processes 
advances neural information processing systems     pp             mit press 
cambridge 
singh  s  p   jaakkola  t     jordan  m  i          reinforcement learning soft state
aggregation  hanson  s  j   cowan  j  d     giles  c  l   eds    advances neural
information processing systems    morgan kaufmann  san mateo 
smallwood  r  d     sondik  e  j          optimal control partially observable
markov processes finite horizon  operations research                
smith  d     peot  m          postponing threats partial order planning  proceedings
eleventh national conference artificial intelligence  pp          washington  dc 
sondik  e  j          optimal control partially observable markov processes
infinite horizon  discounted costs  operations research              
stone  p     veloso  m          team partitioned  opaque transition reinforcement learning 
asada  m   ed    robocup     robot soccer world cup ii  springer verlag  berlin 
sutton  r  s          td models  modeling world mixture time scales 
proceedings twelfth international conference machine learning  pp      
    lake tahoe  nv 
sutton  r  s     barto  a  g          reinforcement learning  introduction  mit press 
cambridge  ma 
tash  j     russell  s          control strategies stochastic planner  proceedings
twelfth national conference artificial intelligence  pp            seattle 
wa 
tatman  j  a     shachter  r  d          dynamic programming uence diagrams 
ieee transactions systems  man  cybernetics                  
tesauro  g  j          td gammon  self teaching backgammon program  achieves masterlevel play  neural computation             
thrun  s   fox  d     burgard  w          probabilistic approach concurrent mapping
localization mobile robots  machine learning            
thrun  s     schwartz  a          finding structure reinforcement learning  tesauro 
g   touretzky  d     leen  t   eds    advances neural information processing
systems   cambridge  ma  mit press 
warren  d          generating conditional plans programs  proceedings aisb
summer conference  pp          university edinburgh 
  

fiboutilier  dean    hanks

watkins  c  j  c  h     dayan  p          q learning  machine learning             
weld  d  s          introduction least commitment planning  ai magazine  winter
            
white iii  c  c     scherer  w  t          solutions procedures partially observed
markov decision processes  operations research                  
williamson  m          value directed approach planning  ph d  thesis          
university washington  department computer science engineering 
williamson  m     hanks  s          optimal planning goal directed utility model 
proceedings second international conference ai planning systems  pp 
        chicago  il 
winston  p  h          artificial intelligence  third edition  addison wesley  reading 
massachusetts 
yang  q          intelligent planning   decomposition abstraction based approach 
springer verlag 
zhang  n  l     liu  w          model approximation scheme planning partially
observable stochastic domains  journal artificial intelligence research             
zhang  n  l     poole  d          exploiting causal independence bayesian network
inference  journal artificial intelligence research             

  


