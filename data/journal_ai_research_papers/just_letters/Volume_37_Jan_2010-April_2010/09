journal artificial intelligence research                  

submitted        published      

training multilingual sportscaster 
using perceptual context learn language
david l  chen
joohyun kim
raymond j  mooney

dlcc   cs   utexas   edu
scimitar   cs   utexas   edu
mooney   cs   utexas   edu

department computer science
university texas austin
  university station c      austin tx        usa

abstract
present novel framework learning interpret generate language using perceptual context supervision  demonstrate capabilities developing system learns
sportscast simulated robot soccer games english korean without language specific
prior knowledge  training employs ambiguous supervision consisting stream descriptive textual comments sequence events extracted simulation trace  system
simultaneously establishes correspondences individual comments events
describe building translation model supports parsing generation 
present novel algorithm learning events worth describing  human evaluations
generated commentaries indicate reasonable quality cases even par
produced humans limited domain 

   introduction
current natural language processing  nlp  systems built using statistical learning algorithms trained large annotated corpora  however  annotating sentences requisite parse
trees  marcus  santorini    marcinkiewicz         word senses  ide   jeronis        semantic
roles  kingsbury  palmer    marcus        difficult expensive undertaking  contrast 
children acquire language exposure linguistic input context rich  relevant 
perceptual environment  also  connecting words phrases objects events world 
semantics language grounded perceptual experience  harnad         ideally  machine
learning system would able acquire language similar manner without explicit human supervision  step direction  present system describe events simulated
soccer game learning sample language commentaries paired traces simulated
activity without language specific prior knowledge  screenshot system generated
commentary shown figure   
fair amount research grounded language learning  roy       
bailey  feldman  narayanan    lakoff        barnard  duygulu  forsyth  de freitas  blei    jordan        yu   ballard        gold   scassellati         focus dealing
raw perceptual data rather language issues  many systems aimed learn meanings words phrases rather interpreting entire sentences  recent work dealt
fairly complex language data  liang  jordan    klein        branavan  chen  zettlemoyer   
c
    
ai access foundation  rights reserved 

fic hen   k im     ooney

figure    screenshot commentator system
barzilay        address three problems alignment  semantic parsing  natural
language generation  contrast  work investigates build complete language learning
system using parallel data perceptual context  study problem simulated environment retains many important properties dynamic world multiple agents
actions avoiding many complexities robotics computer vision  specifically 
use robocup simulator  chen  foroughi  heintz  kapetanakis  kostiadis  kummeneje  noda 
obst  riley  steffens  wang    yin        provides fairly detailed physical simulation
robot soccer  several groups constructed robocup commentator systems  andre 
binsted  tanaka ishii  luke  herzog    rist        provide textual natural language  nl 
transcript simulated game  systems use manually developed templates based
learning 
commentator system learns semantically interpret generate language robocup
soccer domain observing on going commentary game paired evolving simulator state  exploiting existing techniques abstracting symbolic description activity
field detailed states physical simulator  andre et al          obtain pairing
natural language symbolic description perceptual context uttered 
however  training data highly ambiguous comment usually co occurs several events game  integrate enhance existing methods learning semantic parsers
nl generators  kate   mooney        wong   mooney        order learn understand
generate language ambiguous training data  develop system that 
ambiguous training data  learns events worth describing  perform
strategic generation  is  deciding say well say  tactical generation    
   conciseness  use terminology early work generation  e g   mckeown         strategic tactical
generation commonly referred content selection surface realization  respectively

   

fit raining ultilingual portscaster

evaluate system demonstrate language independence training generate
commentaries english korean  experiments test data  annotated evaluation purposes only  demonstrate system learns accurately semantically parse sentences  generate
sentences  decide events describe  finally  subjective human evaluation commentated game clips demonstrate limited domain  system generates sportscasts
cases similar quality produced humans 
three main contributions make paper  first  explore possibility
learning grounded language models perceptual context form ambiguous parallel
data  second  investigate several different methods disambiguating data determined
using combined score includes tactical strategic generation scores performed
best overall  finally  built complete system learns sportscast multiple languages 
carefully verified automatic human evaluations system able perform
several tasks including disambiguating training data  semantic parsing  tactical strategic
generation  language involved work restricted compared handcrafted commercial sportscasting systems  goal demonstrate feasibility learning grounded
language system language specific prior knowledge 
remainder paper structured follows  section   provides background previous work utilize extend build system  section   describes sportscasting
data collected train test approach  section   section   present details
basic methods learning tactical strategic generation  respectively  initial experimental results  section   discusses extensions basic system incorporate information
strategic generation process disambiguating training data  section   presents experimental results initializing system data disambiguated recent method aligning
language facts may refer  section   discusses additions try detect superfluous sentences refer extracted event  section   presents human evaluation
automatically generated sportscasts  section    reviews related work  section    discusses future
work  section    presents conclusions 

   background
systems learning semantic parsers induce function maps natural language  nl  sentences
meaning representations  mrs  formal logical language  existing work focused
learning supervised corpus sentence manually annotated correct mr
 mooney        zettlemoyer   collins        lu  ng  lee    zettlemoyer        jurcicek  gasic 
keizer  mairesse  thomson    young         human annotated corpora expensive
difficult produce  limiting utility approach  kate mooney        introduced
extension one system  k risp  kate   mooney         learn ambiguous
training data requires little human annotation effort  however  system unable
generate language required sportscasting task  thus  enhanced another system
called wasp  wong   mooney        capable language generation well semantic
parsing similar manner allow learn ambiguous supervision  briefly describe
previous systems below  systems assume access formal deterministic
context free grammar  cfg  defines formal meaning representation language  mrl   since
mrls formal computer interpretable languages  grammar usually easily available 
   

fic hen   k im     ooney

    krisp krisper
k risp  kernel based robust interpretation semantic parsing   kate   mooney        uses
support vector machines  svms  string kernels build semantic parsers  svms state ofthe art machine learning methods learn maximum margin separators prevent over fitting
high dimensional data natural language text  joachims         extended
non linear separators non vector data exploiting kernels implicitly create even higher
dimensional space complex data  nearly  linearly separable  shawe taylor   cristianini 
       recently  kernels strings trees effectively applied variety problems
text learning nlp  lodhi  saunders  shawe taylor  cristianini    watkins        zelenko 
aone    richardella        collins        bunescu   mooney         particular  k risp uses
string kernel introduced lodhi et al         classify substrings nl sentence 
first  k risp learns classifiers recognize word phrase nl sentence indicates
particular concept mrl introduced mr  uses production rules
mrl grammar represent semantic concepts  learns classifiers production
classify nl substrings indicative production not  semantically parsing
sentence  classifier estimates probability production covering different substrings
sentence  information used compositionally build complete mr
sentence  given partial matching provided string kernels over fitting prevention
provided svms  k risp experimentally shown particularly robust noisy training
data  kate   mooney        
k risper  kate   mooney        extension k risp handles ambiguous training
data  sentence annotated set potential mrs  one correct 
psuedocode method shown algorithm    employs iterative approach analogous
expectation maximization  em   dempster  laird    rubin        improves upon selection
correct nlmr pairs iteration  first iteration  lines       assumes
mrs paired sentence correct trains k risp resulting noisy supervision 
subsequent iterations  lines         k risper uses currently trained parser score
potential nlmr pair  selects likely mr sentence  retrains parser
resulting disambiguated supervised data  manner  k risper able learn type
weak supervision expected grounded language learner exposed sentences ambiguous
contexts  however  system previously tested artificially corrupted generated
data 
    wasp wasp 
wasp  word alignment based semantic parsing   wong   mooney        uses state of the art
statistical machine translation  smt  techniques  brown  cocke  della pietra  della pietra  jelinek 
lafferty  mercer    roossin        yamada   knight        chiang        learn semantic
parsers  smt methods learn effective machine translators training parallel corpora consisting
human translations documents one alternative natural languages  resulting
translators typically significantly effective manually developed systems smt
become dominant approach machine translation  wong mooney        adapted
methods learn translate nl mrl rather one nl another 
first  smt word alignment system  giza    och   ney        brown  della pietra  della
pietra    mercer         used acquire bilingual lexicon consisting nl substrings coupled
   

fit raining ultilingual portscaster

algorithm   k risper
input sentences associated sets meaning representations r s 
output bestexamplesset  set nl mr pairs 
semanticmodel   k risp semantic parser
  
  
  
  
  
  
  
  
  

main
  initial training loop
sentence si
meaning representation mj r si  
add  si   mj   initialtrainingset
end
end
semanticmodel   train initialtrainingset 

   
   
   
   
   
   
   
   
   
   
   
   

  iterative retraining
repeat
sentence si
meaning representation mj r si  
mj  score   evaluate si   mj   semanticmodel  
end
end
bestexampleset
set consistent examples     s  m  s s  mr s  
p
m score maximized
semanticmodel   rain bestexamplesset 
convergence max iter reached
end main

   

function train trainingexamples 
train k risp unambiguous trainingexamples
   
return trained k risp semantic parser
    end function
   

   

   

function evaluate s  m  semanticmodel  
use k risp semantic parser semanticmodel find derivation meaning representation sentence
   
return parsing score
    end function

   

   

   

fic hen   k im     ooney

translations target mrl  formal languages  mrls frequently contain many
purely syntactic tokens parentheses brackets  difficult align words
nl  consequently  found much effective align words nl productions
mrl grammar used parse corresponding mr  therefore  giza   used
produce n   alignment words nl sentence sequence mrl
productions corresponding top down left most derivation corresponding mr 
complete mrs formed combining nl substrings translations using
grammatical framework called synchronous cfg  scfg   aho   ullman         forms
basis existing syntax based smt  yamada   knight        chiang         scfg 
right hand side production rule contains two strings  case one nl
mrl  derivations scfg simultaneously produce nl sentences corresponding mrs 
bilingual lexicon acquired word alignments training data used construct set
scfg production rules  probabilistic parser produced training maximum entropy
model using em learn parameters scfg productions  similar methods
used riezler  prescher  kuhn  johnson         zettlemoyer collins        
translate novel nl sentence mr  probabilistic chart parser  stolcke        used find
probable synchronous derivation generates given nl  corresponding mr
generated derivation returned 
since scfgs symmetric  used generate nl mr well parse nl
mr  wong   mooney         allows learned grammar used parsing
generation  elegant property important advantages  shieber         generation
system  wasp    uses noisy channel model  brown et al         
arg max pr e f     arg max pr e  pr f  e 
e

   

e

e refers nl string generated given input mr  f   pr e  language model 
pr f  e  parsing model provided wasps learned scfg  generation task find
sentence e     e good sentence priori      meaning input
mr  language model  use standard n gram model  useful ranking candidate
generated sentences  knight   hatzivassiloglou        

   sportscasting data
train test system  assembled human commentated soccer games robocup
simulation league  www robocup org   since focus language learning computer vision  chose use simulated games instead real game video simplify extraction
perceptual information  based rocco robocup commentators incremental event recognition module  andre et al         manually developed symbolic representations game events
rule based system automatically extract simulator traces  extracted
events mainly involve actions ball  kicking passing  include
game information whether current playmode kickoff  offside  corner kick 
events represented atomic formulas predicate logic timestamps  logical facts
constitute requisite mrs  manually developed simple cfg formal semantic
language  details events detected complete grammar found appendix a 
nl portion data  humans commentate games watching
simulator  collected commentaries english korean  english commentaries
   

fit raining ultilingual portscaster

total   comments
total   words
vocabulary size
avg  words per comment

english dataset
    
     
   
    

korean dataset
    
    
   
    

table    word statistics english korean datasets

number events

total

     final
     final
     final
     final

    
    
    
    

   
   
   
   

     final
     final
     final
     final

    
    
    
    

   
   
   
   

number comments
mrs correct mr
english dataset
   
   
   
   
   
   
   
   
korean dataset
   
   
   
   
   
   
   
   

events per comment
max average std  dev 
 
  
  
 

     
     
     
     

     
     
     
     

  
  
  
 

     
     
     
     

     
     
     
     

table    alignment statistics english korean datasets  comments correct meaning representations associated essentially noise training
data      english dataset    korean dataset   moreover  average
  possible events linked comment half links
incorrect 

produced two different people korean commentaries produced single person 
commentators typed comments text box  recorded timestamp 
construct final ambiguous training data  paired comment events
occurred five seconds less comment made  examples ambiguous training
data shown figure    edges connect sentences events might refer  english
translations korean commentaries included figure readers benefit
part actual data  note use english words predicates constants
mrs human readability only  system treats arbitrary conceptual tokens must
learn connection english korean words 
annotated total four games  namely  finals robocup simulation league
year            word statistics data shown table   
sentences fairly short due nature sportscasts  data provides challenges form
synonyms  e g  pink   pinkg pink goalie refer player  polysemes
 e g  kick kicks toward goal refers kick event whereas kicks pink  refers
pass event   alignment statistics datasets shown table         final almost
twice number events games went double overtime 
   

fic hen   k im     ooney

natural language commentary

meaning representation
badpass   purpleplayer   
pinkplayer   
turnover   purpleplayer   
pinkplayer   
kick   pinkplayer   
pass   pinkplayer    pinkplayer    
kick   pinkplayer    

purple goalie turns ball
pink 
purple team sloppy today
pink  passes pink  
pink   looks around teammate

kick   pinkplayer    
ballstopped
kick   pinkplayer    
pass   pinkplayer     pinkplayer   
kick   pinkplayer   
pass   pinkplayer    pinkplayer    

pink   makes long pass pink 

pink  passes back pink  

 a  sample trace ambiguous english training data

natural language commentary

meaning representation

       
 purple   passes purple    

kick   purpleplayer    

       
 purple   passes purple    

kick   purpleplayer    

      
 pink  steals ball purple    

steal   pinkplayer   

   
 pink  passes pink goalie 

kick   pinkplayer   

pass   purpleplayer     purpleplayer    

pass   purpleplayer     purpleplayer    

turnover   purpleplayer     pinkplayer   

playmode   free kick r  

 b  sample trace ambiguous korean training data

figure    examples training data  outgoing edges comments indicate
possibly associated meaning representations considered system  bold links
indicate correct matches comments meaning representations 

   

fit raining ultilingual portscaster

evaluation purposes only  gold standard matching produced examining comment manually selecting correct mr exists  matching approximate
sometimes comments contain information present mrs  example  comment might describe location length pass mr captures participants
pass  bold lines figure   indicate annotated correct matches sample data  notice sentences correct matches  about one fifth english data one tenth
korean data   example  sentence purple team sloppy today figure   a 
cannot represented mrl consequently corresponding correct mr 
another example  korean sentence translation pink  passes pink goalie figure   b  represented mrl  correct match due incomplete
event detection  free kick called pink  passing pink goalie pass event
retrieved  finally  case sentence pink   makes long pass pink  figure   a   correct mr falls outside   second window  game  table   shows
total number nl sentences  number least one recent extracted event
could refer  number actually refer one recent extracted
events  maximum  average  standard deviation number recent events paired
comment given 

   learning tactical generation ambiguous supervision
existing systems capable solving parts sportscasting problem  none
able perform whole task  need system deal ambiguous supervision
k risper generate language wasp  introduce three systems
both  overview differences existing systems new systems present
shown table   
three systems introduced based extensions wasp  underlying language
learner  main problem need solve disambiguate training data
train wasp create language generator  new system uses different
disambiguation criteria determine best matching nl sentences mrs 

    wasper
first system extension wasp manner similar k risp extended create
k risper  uses em like retraining handle ambiguously annotated data  resulting system
call wasper  general  system learns semantic parsers extended handle
ambiguous data long produce confidence levels given nlmr pairs  given set
sentences set mrs associated sentence r s   disambiguate
data finding pairs  s  m   r s    arg maxm p r m s   although
probability used here  ranking relative potential parses would suffice  pseudocode
wasper shown algorithm    difference compared k risper pseudocode
use wasp semantic parser instead k risp parser  also  produce wasp
language generator well desired final output task 
   

fic hen   k im     ooney

algorithm

underlying learner

k risp
k risper
wasp

svm
k risp
giza align words 
mr tokens 
learn probalistic scfg
wasp
first disambiguate
k risper 
train wasp
wasp

wasper
k risper  wasp

wasper  g en

generate 

disambiguation criteria



yes

ambiguous
data 

yes


yes
yes

yes
yes

wasps parsing score
k risps parsing score

yes

yes

nist score
best nl given mr

n a
k risps parsing score
n a

table    overview various learning systems presented  first three algorithms existing
systems  introduce last three systems able learn ambiguous
training data acquire language generator  differ disambiguate
training data 

algorithm   wasper
input sentences associated sets meaning representations r s 
output bestexamplesset  set nl mr pairs 
semanticmodel   wasp semantic parser language generator
   main
  
algorithm  
   end main
  

function train trainingexamples 
train wasp unambiguous trainingexamples
  
return trained wasp semantic parser language generator
   end function

  

  

  

function evaluate s  m  semanticmodel  
   
use wasp semantic parser semanticmodel find derivation meaning representation sentence
   
return parsing score
    end function
   

   

fit raining ultilingual portscaster

    krisper wasp
k risp shown quite robust handling noisy training data  kate   mooney        
important training noisy training data used initialize parser
k rispers first iteration  however  k risper cannot learn language generator  necessary sportscasting task  result  create new system called k risper wasp
good disambiguating training data capable generation  first use k risper
train ambiguous data produce disambiguated training set using prediction
likely mr sentence  unambiguous training set used train wasp
produce parser generator 
    wasper gen
k risper wasper  criterion selecting best nlmr pairs retraining based maximizing probability parsing sentence particular mr  however 
since wasper capable parsing generation  could alternatively select best
nlmr pairs evaluating likely generate sentence particular mr  thus 
built another version wasper called wasper g en disambiguates training data
order maximize performance generation rather parsing  pseudocode shown
algorithm    algorithm wasper except evaluation function  uses
generation based score rather parsing based score select best nlmr pairs 
specifically  nlmr pair  s  m  scored computing nist score  machine translation  mt  metric  sentence best generated sentence  lines        
formally  given set sentences set mrs associated sentence
r s   disambiguate data finding pairs  s  m   r s   
arg maxm n ist  s  argmaxs p r s  m   
nist measures precision translation terms proportion n grams shares
human translation  doddington         used evaluate nl generation  another
popular mt metric bleu score  papineni  roukos  ward    zhu         inadequate
purpose since comparing one short sentence another instead comparing whole
documents  bleu score computes geometric mean n gram precision value n 
means score   matching n gram found every value n  common
setting maximum n    two sentences matching   gram would
receive bleu score    consequently  bleu score unable distinguish quality
generated sentences since fairly short  contrast  nist uses additive score
avoids problem 
    experimental evaluation
section presents experimental results robocup data four systems  k risper  wasper 
k risper wasp  wasper g en  since aware existing systems could
learn semantically parse generate language using ambiguous supervision based perceptual context  constructed lower upper baselines using unmodified wasp  since
   natural way use generation based score would use probability nl given mr  p r s m   
however  initial experiments using metric produce good results  tried changing wasp
maximize joint probability instead parsing probability  however  improve results 

   

fic hen   k im     ooney

algorithm   wasper  g en
input sentences associated sets meaning representations r s 
output bestexamplesset  set nl mr pairs 
semanticmodel   wasp semantic parser language generator
   main
  
algorithm  
   end main
  

function train trainingexamples 
  
algorithm  
   end function

  

  

function evaluate s  m  semanticmodel  
generatedsentence use wasp language generator semanticmodel produce
sentence meaning representation
   
return nist score generatedsentence
    end function
  

   

wasp requires unambiguous training data  randomly pick meaning sentence
set potential mrs serve lower baseline  use wasp trained gold matching
consists correct nlmr pairs annotated human upper baseline  represents upper bound systems could achieve disambiguated training data
perfectly 
evaluate system three tasks  matching  parsing  generation  matching task
measures well systems disambiguate training data  parsing generation tasks
measure well systems translate nl mr  mr nl  respectively 
since four games total  trained using possible combinations one three
games  matching  measured performance training data since goal disambiguate data  parsing generation  tested games used training 
results averaged train test combinations  evaluated matching parsing using
f measure  harmonic mean recall precision  precision fraction systems
annotations correct  recall fraction annotations gold standard
system correctly produces  generation evaluated using bleu scores roughly estimates well produced sentences match target sentences  treat game
whole document avoid problem using bleu score sentence level comparisons mentioned earlier  also  increase number reference sentences mr using
sentences test data corresponding equivalent mrs  e g  pass pinkplayer  
pinkplayer   occurs multiple times test data  sentences matched mr
gold matchings used reference sentences mr 
      atching nl



mr

since handling ambiguous training data important aspect grounded language learning 
first evaluate well various systems pick correct nlmr pairs  figure   shows fmeasure identifying correct set pairs various systems  learning systems
   

fi   

   

    

    

   

   

    

    
f measure

f measure

raining ultilingual portscaster

   
    

   
    

   

   

    

    

wasper
wasper gen
krisper
random matching

   

wasper
wasper gen
krisper
random matching

   

    

    
 

 
number training games

 

 

 a  english

 
number training games

 

 b  korean

   

   

   

   

   

   

f measure

f measure

figure    matching results basic systems  wasper  g en performs best  outperforming
existing system k risper datasets 

   

   
wasp gold matching
krisper
krisper wasp
wasper
wasper gen
wasp random matching

   

   

   

   
wasp gold matching
krisper
krisper wasp
wasper
wasper gen
wasp random matching

   

   

   

   
 

 
number training games

 

 

 a  english

 
number training games

 

 b  korean

figure    semantic parsing results basic systems  results largely mirrors
matching results wasper  g en performing best overall 

perform significantly better random f measure      english
korean data  wasper  g en best system  wasper equals outperforms previous
system k risper well 
      emantic parsing
next  present results accuracy learned semantic parsers  trained system
used parse produce mr sentence test set correct mr
gold standard matching  parse considered correct matches gold standard
exactly  parsing fairly difficult task usually one way describe
event  example  player  passes player  refer event player  kicks
ball player   thus  accurate parsing requires learning different ways people describe
   

fi   

   

    

    

   

   

    

    
bleu

bleu

c hen   k im     ooney

   
    

   
    

wasp gold matching
krisper wasp
wasper
wasper gen
wasp random matching

   
    

wasp gold matching
krisper wasp
wasper
wasper gen
wasp random matching

   
    

   

   
 

 
number training games

 

 

 a  english

 
number training games

 

 b  korean

figure    tactical generation results basic systems  relative performances
various systems change  wasper  g en still best system 

event  synonymy limited verbs  data  pink   pinkg pink goalie
refer player  pink team  since providing systems prior knowledge 
learn different ways referring entity 
parsing results shown figure   generally correlate well matching results  systems better disambiguating training data better parsing
supervised training data less noisy  wasper g en best overall english korean data  interesting note k risper relatively well english data
compared matching performance  k risp robust noise wasp
 kate   mooney        even though trained noisier set data wasper  g en
still produced comparable parser 
      g eneration
third evaluation task generation  wasp based systems given mr test
set gold standard matching nl sentence asked generate nl description 
quality generated sentence measured comparing gold standard using bleu
scoring 
task tolerant noise training data parsing system
needs learn one way accurately describe event  property reflected results 
shown figure    even baseline system  wasp random matching  fairly well 
outperforming k risper wasp datasets wasper korean data  number
event types fairly small  relatively small number correct matchings required
perform task well long event type associated correct sentence pattern
often sentence pattern 
two tasks  wasper  g en best system task  one possible explanation wasper g ens superior performance stems disambiguation objective function 
systems wasper k risper wasp use parsing scores attempt learn good translation model sentence pattern  hand  wasper g en tries learn good
   

fit raining ultilingual portscaster

translation model mr pattern  thus  wasper g en likely converge good
model fewer mr patterns sentence patterns  however  argued learning
good translation models sentence pattern help producing varied commentaries 
quality captured bleu score  another possible advantage wasper  g en
uses softer scoring function  probabilities parsing particular sentence
mr sensitive noise training data  wasper  g en looks top generated
sentences mr  even noise data  top generated sentence remains relatively
constant  moreover  minor variations sentence change results dramatically since
nist score allows partial matching 

   learning strategic generation
language generator alone enough produce sportscast  addition tactical generation
deciding say something  sportscaster must preform strategic generation
choosing say  mckeown        
developed novel method learning events describe  event type  i e 
predicate pass  goal   system uses training data estimate probability
mentioned sportscaster  given gold standard nlmr matches  probability
easy estimate  however  learner know correct matching  instead  system
must estimate probabilities ambiguous training data  compare two basic methods
estimating probabilities 
first method uses inferred nlmr matching produced language learning system 
probability commenting event type  ei   estimated percentage events
type ei matched nl sentence 
second method  call iterative generation strategy learning  igsl   uses variant em  treating matching assignments hidden variables  initializing match
prior probability  iterating improve probability estimates commenting event
type  unlike first method  igsl uses information mrs explicitly associated
sentence training  algorithm   shows pseudocode  main loop alternates two
steps 
   calculating expected probability nlmr matching given current model
likely event commented  line   
   update prior probability event type mentioned human commentator based
matchings  line    
first iteration  nlmr match assigned
probability inversely proportional
p
amount ambiguity associated sentence   eevent s  pr  e     event s     example 
sentence associated five possible mrs assign match probability      prior
probability mentioning event type estimated average probability assigned
instances event type  notice process always guarantee proper probability since
mr associated multiple sentences  thus  limit probability one 
subsequent iterations  probabilities nlmr matchings updated according
new priors  assign match prior probability event type normalized across
associated mrs nl sentence  update priors event type using
   

fic hen   k im     ooney

algorithm   iterative generation strategy learning
input event types e    e         en    number occurrences event type otalcount ei  
entire game trace  sentences event types associated meaning representations event s 
output probabilities commenting event type p r ei  
   initialize pr  ei      
   repeat
  
event type ei e
  
matchcount    
  
sentence
p
pr  e 

p
  
probofmatch   eevent s e e
pr  e 
eevent s 

  
  
  
   
   

matchcount   matchcount   probofmatch
end
matchcount
pr  ei     min  totalcount e
     ensure proper probabilities 
i 
end
convergence max iter reached

       

                                             
             

            

             

           a 

       b c    de   f g 

   d  

    a 

 c      b c    de   f g 

      

  h h 

 c      b c    de   f g 

      
      

ij                                
k                           
    j                                                                 
                      

figure    example strategic generation component works  every timestep 
stochastically select event events occurring moment 
decide whether verbalize selected event based igsls estimated probability
commented upon 

new estimated probabilities matchings  process repeated probabilities
converge pre specified number iterations occurred 
generate sportscast  use learned probabilities determine events describe 
time step  first determine events occurring time  select one
randomly based normalized probabilities  avoid overly verbose  want
make comment every time something happening  especially event rarely commented on 
thus  stochastically decide whether comment selected event based probability 
example process shown figure   
   

fi   

   

   

   

   

   

f measure

f measure

raining ultilingual portscaster

   
inferred gold matching
igsl
inferred krisper
inferred wasper
inferred wasper gen
inferred random matching

   

   
inferred gold matching
igsl
inferred krisper
inferred wasper
inferred wasper gen
inferred random matching

   

   

   
 

 
number training games

 

 

 a  english

 
number training games

 

 b  korean

figure    strategic generation results various systems  novel algorithm igsl performs
best  almost par upper bound uses gold annotated matchings 

event
ballstopped
kick
pass
turnover
badpass

  occurrences
    
    
    
   
   

  commented
        
      
     
     
     

igsl
        
     
     
     
     

inferred wasper  g en
     
     
     
     
     

table    top   frequent events    times commented on  probabilities
learned top algorithms english data

    experimental evaluation
different methods learning strategic generation evaluated based often events
describe test data coincide human decided describe  first
approach  results using inferred matchings produced k risper  wasper  wasper g en
well gold random matching establishing baselines presented figure   
graph  clear igsl outperforms learning inferred matchings actually
performs level close using gold matching  however  important note
limiting potential learning gold matching using predicates decide
whether talk event 
english data  probabilities learned igsl inferred matchings wasper g en five frequently occurring events shown table    wasper  g en learns
fairly good probabilities general  well igsl frequent events 
igsl uses occurrences events associated possible comments
training iterations  rarely commented events ballstopped kick often occur without
comments uttered  consequently  igsl assigns low prior probabilities
lowers chances matched sentences  hand  wasper  g en
use priors sometimes incorrectly matches comments them  thus  using inferred
   

fic hen   k im     ooney

matches wasper  g en results learning higher probabilities commenting rarely
commented events 
methods use predicates mrs decide whether comment not 
perform quite well data collected  particular  igsl performs best  use
strategic generation rest paper 

   using strategic generation improve matching
section  explore knowledge learned strategic generation used improve
accuracy matching sentences mrs  previous section  described several ways
learn strategic generation  including igsl learns directly ambiguous training data 
knowing events people tend talk help resolve ambiguities training
data  events likely discussed likely matched
nl sentence disambiguating training data  therefore  section describes methods
integrate strategic generation scores  such table    scoring nlmr pairs used
matching process 
    wasper gen igsl
wasper  g en  igsl extension wasper  g en uses strategic generation scores
igsl  wasper  g en uses nist score pick best mr sentence finding mr
generates sentence closet actual nl sentence  wasper  g en  igsl combines tactical
 nist  strategic  igsl  generation scores pick best nlmr pairs  simply multiplies
nist score igsl score together form composite score  new score biases
selection matching pairs include events igsl determines are  priori  likely
discussed  helpful  especially beginning wasp produce
particularly good language generator  many instances  generated sentences
possible mrs equally bad overlap target sentence  even generation
produces perfectly good sentence  generation score unreliable comparing
single sentence single reference often short well  consequently  often
difficult wasper g en distinguish among several mrs equal scores  hand 
event types different strategic generation scores  default choosing
mr higher prior probability mentioned  algorithm   shows pseudocode
wasper  g en  igsl 
    variant wasper gen systems
although wasper  g en uses nist score estimate goodness nlmr pairs  could easily
use mt evaluation metric  already discussed unsuitability bleu comparing short individual sentences since assigns zero many pairs  however  nist score
limitations  example  normalized  may affect performance wasper  g en igsl combined igsl score  another limitation comes using higher order
n grams  commentaries domain often short  frequently higher order
n gram matches generated sentences target nl sentences 
meteor metric  banerjee   lavie        designed resolve various weaknesses
bleu nist metrics  focused word to word matches reference
   

fit raining ultilingual portscaster

algorithm   wasper  g en  igsl
input sentences associated sets meaning representations r s 
output bestexamplesset  set nl mr pairs 
semanticmodel   wasp semantic parser language generator
   main
  
algorithm  
   end main
  

function train trainingexamples 
  
algorithm  
   end function

  

  
  
   
   
   
   
   
   

function evaluate s  m  semanticmodel  
call algorithm   collect igsl scores
generatedsentence use wasp language generator semanticmodel produce
sentence meaning representation
tacticalgenerationscore nist score generatedsentence
strategicgenerationscore pr  event type m  result algorithm  
return tacticalgenerationscore strategicgenerationscore
end function

sentence test sentence  meteor first evaluates uni gram matches reference
test sentence determines well words ordered  meteor seems
appropriate domain good generated sentences missing adjectives adverbs critical meaning sentence prevent higher order n gram matches 
addition  meteor normalized always      may combine effectively igsl scores  which range     
    experimental evaluation
evaluated new systems  wasper g en i gsl nist meteor scoring using
methodology section      matching results shown figure    including results
wasper  g en  best system previous section  wasper g en igsl
either nist meteor scoring clearly outperforms wasper g en  indicates strategicgeneration information help disambiguate data  using different mt metrics produces
less noticeable effect  clear winner english data  however  meteor seems
improve performance korean data 
parsing results shown figure    previously noted  parsing results generally mirror
matching results  new systems outperform wasper  g en  previously best system 
again  english data show clear advantage using either nist meteor 
korean data gives slight edge using meteor metric 
results tactical generation shown figure     english korean
data  new systems come close performance wasper g en beat it  however 
new systems outperform k risper  wasp wasper shown figure 
   

fi   

   

    

    

   

   

    

    
f measure

f measure

c hen   k im     ooney

   
    
   

   
    
   

    

    
wasper gen
wasper gen igsl
wasper gen igsl using meteor

   

wasper gen
wasper gen igsl
wasper gen igsl using meteor

   

    

    
 

 
number training games

 

 

 a  english

 
number training games

 

 b  korean

   

   

    

    

   

   

    

    
f measure

f measure

figure    matching results  integrating strategic information improves results previously best system wasper  g en  choice mt metric used  however  makes
less impact 

   
    

   
    

   

   
wasp gold matching
wasper gen
wasper gen igsl
wasper gen igsl using meteor

    

wasp gold matching
wasper gen
wasper gen igsl
wasper gen igsl using meteor

    

   

   
 

 
number training games

 

 

 a  english

 
number training games

 

 b  korean

figure    semantic parsing results  results similar matching results integrating
strategic generation information improves performance 

   

fi   

   

    

    

   

   

    

    
bleu

bleu

raining ultilingual portscaster

   
    

   
    

   

   
wasp gold matching
wasper gen
wasper gen igsl
wasper gen igsl using meteor

    

wasp gold matching
wasper gen
wasper gen igsl
wasper gen igsl using meteor

    

   

   
 

 
number training games

 

 

 a  english

 
number training games

 

 b  korean

figure     tactical generation results  two new systems come close performance
wasper  g en  beat it  however  outperform systems
presented earlier shown figure 

overall  expected  using strategic information improves performance matching
semantic parsing tasks  english korean datasets  wasper g en igsl
variant using meteor metric clearly outperform wasper g en utilize
strategic information  however  strategic information improve tactical generation 
could due ceiling effect wasper  g en already performs level near upper
baseline  matching performance improved  generation performance little room
grow 

   using generative alignment model
recently  liang et al         developed generative model used match naturallanguage sentences facts corresponding database may refer  one
evaluation domains  used english robocup sportscasting data  method solves
matching  alignment  problem data  address tasks semantic parsing
language generation  however  generative model elegantly integrates simple strategic
tactical language generation models order find overall probable alignment sentences
events  demonstrated improved matching performance english data  generating
accurate nlmr pairs best system  thus  curious results could
used improve systems  perform semantic parsing generation 
ran code new korean data resulted much worse matching results compared
best system seen table   
simplest way utilizing results use nlmr pairs produced method
supervised data wasp  expected  improved nlmr pairs english data resulted
improved semantic parsers seen results table    even korean dataset 
training matchings produced system ended fairly well even though matching performance poor  tactical generation  using matching produced marginal
improvement english dataset surprisingly large improvement korean data
   

fic hen   k im     ooney

algorithm
liang et al        
wasper
wasper g en
wasper g en i gsl
wasper g en i gsl  m eteor

english dataset
initialization initialized
    
    
    
    
    
    
    
    
    

korean dataset
initialization initialized
    
    
    
    
    
    
    
    
    

table    matching results  f  scores    fold cross validation english korean
datasets  systems run initialization initialized matchings produced
liang et al s        system 

algorithm
wasp
wasper
wasper g en
wasper g en i gsl
wasper g en i gsl  m eteor

english dataset
initialization initialized
n a
    
     
     
     
     
     
     
     
     

korean dataset
initialization initialized
n a
     
     
     
     
     
     
     
     
     

table    semantic parsing results  f  scores    fold cross validation english
korean datasets  systems run initialization initialized matchings
produced liang et al s        system 

algorithm
wasp
wasper
wasper g en
wasper g en i gsl
wasper g en i gsl  m eteor

english dataset
initialization initialized
n a
      
      
      
      
      
      
      
      
      

korean dataset
initialization initialized
n a
      
      
      
      
      
      
      
      
      

table    tactical generation results  bleu score    fold cross validation english
korean datasets  systems run initialization initialized matchings
produced liang et al s        system 

shown table    overall  using alignments produced liang et al s system resulted good
semantic parsers tactical generators 
addition training wasp alignment  utilize output better
starting point systems  instead initializing iterative alignment methods
model trained ambiguous nlmr pairs  initialized disambiguated
nlmr pairs produced liang et al s system 
   

fit raining ultilingual portscaster

initializing systems manner almost always improved performance three
tasks  tables           moreover  results best systems exceed simply training wasp alignment cases except semantic parsing english data  thus 
combining liang et al s alignment disambiguation techniques seems produce best
overall results  english data  wasper initialization performs best matching generation  slightly worse semantic parsing task compared wasp trained
liang et al s alignment  korean data  systems better training wasp
alignment  wasper  g en  i gsl  m eteor initialization performs best matching
semantic parsing wasper  g en initialization performs best generation 
overall  initializing systems alignment output liang et al s generative model
improved performance expected  starting cleaner set data led better initial semantic
parsers language generators led better end results  furthermore  incorporating
semantic parser tactical generator  able improve liang et al s alignments
achieve even better results cases 

   removing superfluous comments
far  discussed handle ambiguity multiple possible mrs
nl sentence  training  methods assume nl sentence matches
exactly one potential mrs  however  comments superfluous  sense
refer currently extracted event represented set potential mrs  previously
shown tables    one fifth english sentences one tenth korean sentences
superfluous sense 
many reasons superfluous sentences  occur naturally language
people always talk current environment  domain  sportscasters often mention
past events general information particular teams players  moreover  depending
application  chosen mrl may represent things people talk about  example 
robocup mrl cannot represent information players actively engaged
ball  finally  even sentence represented chosen mrl  errors perceptual
system incorrect estimation event occurred lead superfluous sentences 
perceptual errors alleviated degree increasing size window used
capture potential mrs  the previous   seconds experiments   however  comes
cost increased ambiguity associates mrs sentence 
deal problem superfluous sentences  eliminate lowest scoring nlmr
pairs  e g  lowest parsing scores wasper lowest nist scores wasper g en   however 
order set pruning threshold  need automatically estimate amount superfluous
commentary absence supervised data  notice problem looks similar
strategic generation problem  estimating likely mr participates correct matching
opposed likely nl sentence participates correct matching   approaches used
cannot applied  first  cannot use matches inferred existing systems estimate
fraction superfluous comments since current systems match every sentence mr 
difficult develop algorithm similar igsl due imbalance nl sentences
mrs  since many mrs  examples events occurring without
commentaries vice versa 
   

fic hen   k im     ooney

    estimating superfluous rate using internal cross validation
propose using form internal  i e  within training set  cross validation estimate rate
superfluous comments  algorithm used conjunction systems 
chose implement k risper trains much faster systems  makes
tractable train many different semantic parsers choose best one  basic idea
use part ambiguous training data estimate accuracy semantic parser even though
know correct matchings  assuming reasonable superfluous sentence rate  know
time correct mr contained set mrs associated nl sentence 
thus  assume semantic parser parses nl sentence one mrs associated
better one parses mr set  approach estimating
accuracy  evaluate semantic parsers learned using various pruning thresholds pick
best one  algorithm briefly summarized following steps 
   split training set internal training set internal validation set 
   train k risper n times internal training set using n different threshold values  eliminating lowest scoring nlmr pairs threshold retraining iteration
algorithm    
   test n semantic parsers internal validation set determine parser able
parse largest number sentences one potential mrs 
   use threshold value produced best parser previous step train final parser
complete original training set 
    experiments
evaluated effect removing superfluous sentences three tasks  matching  parsing 
generation  present results k risper k risper  wasp  matching 
show results k risper responsible disambiguating training data
systems  so k risper  wasps results same   generation  show results
k risper wasp  since k risper cannot perform generation 
matching results shown figure    demonstrate removing superfluous sentences
improve performance english korean  although difference small absolute
terms  parsing results shown figure    indicate removing superfluous sentences usually
improves accuracy k risper k risper  wasp marginally  observed
many times  parsing results consistent matching results  finally  tactical generation results shown figure    suggest removing superfluous comments actually decreases
performance somewhat  again  potential explanation generation less sensitive
noisy training data  removing superfluous comments improves purity training data 
removes potentially useful examples  consequently  system learn generate sentences removed data  overall  generation  advantage
cleaner disambiguated training data apparently outweighed loss data 
   

fi   

   

    

    

   

   

    

    
f measure

f measure

raining ultilingual portscaster

   
    

   
    

   

   

    

    

   

   

krisper
krisper superfluous comment removal

    

krisper
krisper superfluous comment removal

    
 

 
number training games

 

 

 a  english

 
number training games

 

 b  korean

   

   

    

    

   

   

    

    
f measure

f measure

figure     matching results comparing effects removing superfluous comments 

   
    

   
    

   

   
krisper
krisper superfluous comment removal
krisper wasp
krisper wasp superfluous comment removal

    

krisper
krisper superfluous comment removal
krisper wasp
krisper wasp superfluous comment removal

    

   

   
 

 
number training games

 

 

 a  english

 
number training games

 

 b  korean

   

   

    

    

   

   

    

    
bleu

bleu

figure     semantic parsing results improved marginally superfluous comment removal 

   

   

    

    

   

   

    

    

krisper wasp
krisper wasp superfluous comment removal

   

krisper wasp
krisper wasp superfluous comment removal

   
 

 
number training games

 

 

 a  english

 
number training games

 

 b  korean

figure     tactical generation performance decreases removing superfluous comments 

   

fic hen   k im     ooney

   human subjective evaluation
best  automatic evaluation generation imperfect approximation human assessment 
moreover  automatically evaluating quality entire generated sportscast even difficult  consequently  used amazons mechanical turk collect human judgements
produced sportscasts  human judge shown three clips simulated game video one sitting    video clips total    clips use   game segments   minutes each  one
four games            robocup finals     game segments commentated
human system  use igsl determine events comment
wasper  g en  our best performing system tactical generation  produce commentaries 
make commentaries varied  took top   outputs wasper  g en chose
one stochastically weighted scores  system always trained three games  leaving game test segment extracted  video clips accompanied
commentaries appear subtitles screen well audio produced automated text speech system   videos shown random counter balanced order ensure
consistent bias toward segments shown earlier later  asked judges score
commentaries using following metrics 

score
 
 
 
 
 

fluency
flawless
good
non native
disfluent
gibberish

semantic
correctness
always
usually
sometimes
rarely
never

sportscasting
ability
excellent
good
average
bad
terrible

fluency semantic correctness  adequacy  standard metrics human evaluations nl
translations generations  fluency measures well commentaries structured  including
syntax grammar  semantic correctness indicates whether commentaries accurately describe
happening game  finally  sportscasting ability measures overall quality
sportscast  includes whether sportscasts interesting flow well  addition
metrics  asked whether thought sportscast composed human
computer  human   
since mechanical turk recruits judges internet  make sure judges
assigning ratings randomly  thus  addition asking rate video 
asked count number goals video  incorrect responses question caused
ratings discarded  ensure judges faithfully watched entire clip
assigning ratings  pruning  average    ratings  from    original ratings 
  videos english data  since difficult recruit korean judges
internet  recruited person collected   ratings average video
korean data  table     show results english korean data  respectively 
statistically significant results shown boldface 
results surprisingly good english data across categories machine actually
scoring higher human average  however  differences statistically significant
   sample video clips sound available web http   www cs utexas edu users ml 
clamp sportscasting  

   

fit raining ultilingual portscaster

     final
     final
     final
     final
average

commentator
human
machine
human
machine
human
machine
human
machine
human
machine

fluency
    
    
    
    
    
    
    
    
    
    

semantic
correctness
    
    
    
    
    
    
    
    
    
    

sportscasting
ability
    
    
    
    
    
    
    
    
    
    

human 
      
      
      
      
      
      
      
      
      
      

table    human evaluation overall sportscasts english data  bold numbers indicate statistical
significance 

     final
     final
     final
     final
average

commentator
human
machine
human
machine
human
machine
human
machine
human
machine

fluency
    
    
    
    
    
    
    
    
    
    

semantic
correctness
    
    
    
    
    
    
    
    
    
    

sportscasting
ability
    
    
    
    
    
    
    
    
    
    

human 
      
      
      
      
      
      
      
      
      
      

table    human evaluation overall sportscasts korean data  bold numbers indicate statistical
significance 

   

fic hen   k im     ooney

based unpaired t test  p          nevertheless  encouraging see machine
rated highly  variance humans performance since two different
commentators  notably  compared machine  humans performance      final
quite good commentary included many details position players 
types passes  comments overall flow game  hand 
humans performance      final quite bad human commentator
mechanical used sentence pattern repeatedly  machine performance
even throughout although sometimes gets lucky  example  machine serendipitously said
beginning exciting match  near start      final clip simply
statement incorrectly learned correspond extracted mr actually unrelated 
results korean impressive  human beats machine average
categories  however  largest difference scores category     
moreover  absolute scores indicate generated korean sportscast least acceptable
quality  judges even mistakenly thought produced humans one third time 
part reason worse performance compared english data korean commentaries fairly detailed included events extracted limited perceptual
system  thus  machine simply way competing limited expressing
information present extracted mrs 
elicited comments human judges get qualitative evaluation  overall 
judges thought generated commentaries good accurately described actions
field  picking top   generated sentences added variability machine generated
sportscasts improved results compared earlier experiments presented chen
mooney         however  machine still sometimes misses significant plays scoring
corner kicks  plays happen much less frequently often coincide
many events  e g  shooting ball kickoffs co occur scoring   thus  machine
harder time learning infrequent events  another issue concerns representation 
many people complain long gaps sportscasts lack details  event detector
concentrates ball possession positions elapsed time  thus  player holding onto
ball dribbling long time produce events detected simulated perceptual
system  also  short pass backfield treated exactly long pass across
field near goal  finally  people desired colorful commentary  background information 
statistics  analysis game  fill voids  somewhat orthogonal issue since
goal build play by play commentator described events currently happening 

    related work
section review related work semantic parsing  natural language generation
well grounded language learning 
     semantic parsing
mentioned section    existing work semantic parser learners focused supervised
learning sentence annotated semantic meaning  semantic parser learners additionally require either syntactic annotations  ge   mooney        prior syntactic knowledge target language  ge   mooney        zettlemoyer   collins               since
world never provides direct feedback syntactic structure  language learning methods
   

fit raining ultilingual portscaster

require syntactic annotation directly applicable grounded language learning  therefore 
methods learn semantic annotation critical learning language perceptual
context 
use logic formulas mrs  particular mrl use contains atomic
formulas equivalently represented frames slots  systems use
transformation based learning  jurcicek et al          markov logic  meza ruiz  riedel   
lemon        learn semantic parsers using frames slots  principle  framework
used semantic parser learner long provides confidence scores parse results 
     natural language generation
several existing systems sportscast robocup games  andre et al          given
game states provided robocup simulator  extract game events generate real time
commentaries  consider many practical issues timeliness  coherence  variability 
emotion needed produce good sportscasts  however  systems hand built
generate language using pre determined templates rules  contrast  concentrate
learning problem induce generation components ambiguous training data  nevertheless  augmenting system components systems could improve
final sportscasts produced 
prior work learning lexicon elementary semantic expressions corresponding natural language realizations  barzilay   lee         work uses multiple sequence
alignment datasets supply several verbalizations corresponding semantics extract
dictionary 
duboue mckeown        first propose algorithm learning strategic generation automatically data  using semantics associated texts  system learns classifier
determines whether particular piece information included presentation not 
recent work learning strategic generation using reinforcement learning
 zaragoza   li         work involves game setting speaker must aid listener
reaching given destination avoiding obstacles  game played repeatedly find
optimal strategy conveys pertinent information minimizing number
messages  consider different problem setting reinforcements available
strategic generation learner 
addition  work performing strategic generation collective task
 barzilay   lapata         considering strategic generation decisions jointly  captures
dependencies utterances  creates consistent overall output consistent
humans perform task  approach could potentially help system produce
better overall sportscasts 
     grounded language learning
one ambitious end to end visually grounded scene description system vitra  herzog   wazinski        comments traffic scenes soccer matches  system first
transforms raw visual data geometrical representations  next  set rules extract spatial relations interesting motion events representations  presumed intentions  plans  plan
   

fic hen   k im     ooney

interactions agents extracted based domain specific knowledge  however 
since system hand coded cannot adapted easily new domains 
srihari burhans        used captions accompanying photos help identify people
objects  introduced idea visual semantics  theory extracting visual information
constraints accompanying text  example  using caption information  system
determine spatial relationship entities mentioned  likely size shape
object interest  whether entity natural artificial  however  system based
hand coded knowledge 
siskind        performed earliest work learning grounded word meanings 
learning algorithm addresses problem ambiguous training referential uncertainty
semantic lexical acquisition  address larger problems learning complete semantic
parsers language generators 
several robotics computer vision researchers worked inferring grounded meanings
individual words short referring expressions visual perceptual context  e g   roy       
bailey et al         barnard et al         yu   ballard         however  complexity
natural language used existing work restrictive  many systems use pre coded
knowledge language  almost use static images learn language describing objects
relations  cannot learn language describing actions  sophisticated grammatical
formalism used learn syntax work finite state hidden markov model  contrast 
work exploits latest techniques statistical context free grammars syntax based statistical
machine translation handle complexities natural language 
recently  gold scassellati        built system called twig uses existing language knowledge help learn meaning new words  robot uses partial parses focus
attention possible meanings new words  playing game catch  robot able
learn meaning well identity relations 
variety work learning captions accompany pictures
videos  satoh  nakamura    kanade        berg  berg  edwards    forsyth         area
particular interest given large amount captioned images video available web
television  satoh et al         built system detect faces newscasts  however  use fairly
simple manually written rules determine entity picture language refers 
berg et al         used elaborate learning method cluster faces names  using
data  estimate likelihood entity appearing picture given context 
recent work video retrieval focused learning recognize events sports videos
connecting english words appearing accompanying closed captions  fleischman
  roy        gupta   mooney         however  work learns connection
individual words video events learn describe events using full grammatical
sentences  avoid difficult problems computer vision  work uses simulated world
perception complex events participants much simpler 
addition observing events passively  work grounded language learning interactive environments computer video games  gorniak   roy        
work  players cooperate communicate order accomplish certain task 
system learns map spoken instructions specific actions  however  relies existing statistical
parsers learn syntax semantics language perceptual environment
alone  kerr  cohen  chang        developed system learns grounded word meanings
nouns  adjectives  spatial prepositions human instructing perform tasks vir   

fit raining ultilingual portscaster

tual world  however  system assumes existing syntactic parser prior knowledge verb
semantics unable learn experience 
recently  interest learning interpret english instructions describing use particular website perform computer tasks  branavan et al         lau 
drews    nichols         systems learn predict correct computer action  pressing
button  choosing menu item  typing text field  etc   corresponding step instructions  instead using parallel training data perceptual context  systems utilize
direct matches words natural language instructions english words explicitly occurring menu items computer instructions order establish connection
language environment 
one core subproblems work addresses matching sentences facts world
refer  recent projects attempt align text english summaries american
football games database records contain statistics events game  snyder
  barzilay        liang et al          however  snyder barzilay        use supervised
approach requires annotating correct correspondences text semantic
representations  hand  liang et al         developed unsupervised approach
using generative model solve alignment problem  demonstrated improved results
matching sentences events robocup english sportscasting data  however  work
address semantic parsing language generation  section   presents results showing
methods improve nlmr matches produced approach well use
learn parsers generators 

    future work
previously discussed  limitations current system due inadequacies
perception events extracted robocup simulator  language commentary 
particularly korean data  refers information events currently represented
extracted mrs  example  player dribbling ball captured perceptual system 
event extractor could extended include information output representations 
commentaries always immediate actions happening field 
refer statistics game  background information  analysis game 
difficult obtain  would simple augment potential mrs include events
current score number turnovers  etc  may difficult learn correctly 
potentially would make commentaries much natural engaging 
statements commentaries specifically refer pattern activity across several
recent events rather single event  example  one english commentaries 
statement purple team sloppy today  appears series turn overs team 
simulated perception could extended extract patterns activity sloppiness 
however assumes concepts predefined  extracting many higher level
predicates would greatly increase ambiguity training data  current system assumes
already concepts words needs learn perceive concepts represent
mrs  however  would interesting include whorfian style language
learning  whorf        unknown word sloppiness could actually cause
creation new concept  content words seem consistently correlate
perceived event  system could collect examples recent activity word used try
   

fic hen   k im     ooney

learn new higher level concept captures regularity situations  example  given
examples situations referred sloppy  inductive logic programming system  lavrac  
dzeroski        able detect pattern several recent turnovers 
another shortcoming current system mr treated independently  fails
exploit fact many mrs related other  example  pass preceded kick 
bad pass followed turnover  natural way use graphical representation
represent entities events relationships them 
currently tactical strategic generation system loosely coupled  however 
conceptually much closely related  solving one problem help solve
other  initializing system output liang et al          uses generative model
includes strategic tactical components  produced somewhat better results  however 
interaction components loose tighter integration different
pieces could yield stronger results tasks 
obvious extension current work apply real robocup games rather
simulated ones  recent work rozinat  zickler  veloso  van der aalst  mcmillen       
analyzes games robocup small size league using video overhead camera 
using symbolic event trace extracted real perceptual system  methods could
applied real world games  using speech recognition accept spoken language input another
obvious extension 
currently exploring extending approach learn interpret generate nl instructions navigating virtual environment  system observe one person giving english
navigation instructions  e g  go hall turn left pass chair   another person follows directions get chosen destination  collecting examples sentences
paired actions executed together information local environment 
system construct ambiguous supervised dataset language learning  approach
could eventually lead virtual agents games educational simulations automatically
learn interpret generate natural language instructions 

    conclusion
presented end to end system learns generate natural language sportscasts
simulated robocup soccer games training sample human commentaries paired automatically extracted game events  learning semantically interpret generate language without
explicitly annotated training data  demonstrated system learn language simply
observing linguistic descriptions ongoing events  demonstrated systems language
independence successfully training produce sportscasts english korean 
dealing ambiguous supervision inherent training environment critical issue
learning language perceptual context  evaluated various methods disambiguating
training data order learn semantic parsers language generators  using generation
evaluation metric criterion selecting best nlmr pairs produced better results
using semantic parsing scores initial training data noisy  system learns
model strategic generation ambiguous training data estimating probability
event type evokes human commentary  moreover  using strategic generation information
help disambiguate training data shown improve results  demonstrated
system initialized alignments produced different system achieve better
   

fit raining ultilingual portscaster

results either system alone  finally  experimental evaluation verified overall system
learns accurately parse generate comments generate sportscasts competitive
produced humans 

acknowledgments
thank adam bossy work simulating perception robocup games 
thank percy liang sharing software experimental results us  finally  thank
anonymous reviewers jair editor  lillian lee  insightful comments
helped improve final presentation paper  work funded nsf grant iis
       x  experiments run mastodon cluster  provided nsf grant
eia         

appendix a  details meaning representation language
table    shows brief explanations different events detect simulated perception 
event
playmode
ballstopped
turnover
kick
pass
badpass
defense
steal
block

description
signifies current play mode defined game
ball speed minimum threshold
current possessor ball last possessor different teams
player possession ball one time interval next
player gains possession ball different player team
pass player gaining possession ball different team
transfer one player opposing player penalty area
player possession ball one time interval another player
different team next time interval
transfer one player opposing goalie 
table     description different events detected

include context free grammar developed meaning representation language  derivations start root symbol  s 

 s
 s
 s
 s
 s
 s
 s
 s
 s

  
  
  
  
  
  
  
  
  

playmode    playmode  
ballstopped
turnover    player    player  
kick    player  
pass    player    player  
badpass    player    player  
defense    player    player  
steal    player  
block    player  

   

fic hen   k im     ooney

 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

kick off l
kick off r
kick in l
kick in r
play on
offside l
offside r
free kick l
free kick r
corner kick l
corner kick r
goal kick l
goal kick r
goal l
goal r

pink 
pink 
pink 
pink 
pink 
pink 
pink 
pink 
pink 
pink  
pink  
purple 
purple 
purple 
purple 
purple 
purple 
purple 
purple 
purple 
purple  
purple  

   

fit raining ultilingual portscaster

references
aho  a  v     ullman  j  d          theory parsing  translation  compiling  prentice
hall  englewood cliffs  nj 
andre  e   binsted  k   tanaka ishii  k   luke  s   herzog  g     rist  t          three robocup
simulation league commentator systems  ai magazine              
bailey  d   feldman  j   narayanan  s     lakoff  g          modeling embodied lexical development  proceedings nineteenth annual conference cognitive science society 
banerjee  s     lavie  a          meteor  automatic metric mt evaluation improved
correlation human judgments  proceedings acl workshop intrinsic
extrinsic evaluation measures machine translation and or summarization  pp      
ann arbor  michigan  association computational linguistics 
barnard  k   duygulu  p   forsyth  d   de freitas  n   blei  d  m     jordan  m  i          matching
words pictures  journal machine learning research              
barzilay  r     lapata  m          collective content selection concept to text generation 
proceedings human language technology conference conference empirical
methods natural language processing  hlt emnlp     
barzilay  r     lee  l          bootstrapping lexical choice via multiple sequence alignment 
proceedings      conference empirical methods natural language processing
 emnlp     
berg  t  l   berg  a  c   edwards  j     forsyth  d  a          whos picture  advances
neural information processing systems     nips       
branavan  s   chen  h   zettlemoyer  l  s     barzilay  r          reinforcement learning
mapping instructions actions  proceedings joint conference   th annual
meeting association computational linguistics  th international joint
conference natural language processing asian federation natural language
processin  acl ijcnlp       
brown  p  f   cocke  j   della pietra  s  a   della pietra  v  j   jelinek  f   lafferty  j  d   mercer 
r  l     roossin  p  s          statistical approach machine translation  computational
linguistics              
brown  p  f   della pietra  v  j   della pietra  s  a     mercer  r  l          mathematics
statistical machine translation  parameter estimation  computational linguistics        
       
bunescu  r  c     mooney  r  j          subsequence kernels relation extraction  weiss 
y   scholkopf  b     platt  j   eds    advances neural information processing systems   
 nips       vancouver  bc 
chen  d  l     mooney  r  j          learning sportscast  test grounded language acquisition  proceedings   th international conference machine learning  icml      
helsinki  finland 
   

fic hen   k im     ooney

chen  m   foroughi  e   heintz  f   kapetanakis  s   kostiadis  k   kummeneje  j   noda  i   obst 
o   riley  p   steffens  t   wang  y     yin  x          users manual  robocup soccer server
manual soccer server version      later   available http   sourceforge 
net projects sserver  
chiang  d          hierarchical phrase based model statistical machine translation  proceedings   nd annual meeting association computational linguistics  acl     pp         ann arbor  mi 
collins  m          new ranking algorithms parsing tagging  kernels discrete structures  voted perceptron  proceedings   th annual meeting association
computational linguistics  acl        pp         philadelphia  pa 
dempster  a  p   laird  n  m     rubin  d  b          maximum likelihood incomplete data
via em algorithm  journal royal statistical society b          
doddington  g          automatic evaluation machine translation quality using n gram cooccurrence statistics  proceedings arpa workshop human language technology 
pp         san diego  ca 
duboue  p  a     mckeown  k  r          statistical acquisition content selection rules
natural language generation  proceedings      conference empirical methods
natural language processing  emnlp      pp         
fleischman  m     roy  d          situated models meaning sports video retrieval  proceedings human language technologies  conference north american chapter
association computational linguistics  naacl hlt     rochester  ny 
ge  r     mooney  r  j          statistical semantic parser integrates syntax semantics 
proceedings ninth conference computational natural language learning  conll       pp      ann arbor  mi 
ge  r     mooney  r  j          learning compositional semantic parser using existing syntactic parser  proceedings joint conference   th annual meeting association computational linguistics  th international joint conference natural
language processing asian federation natural language processin  acl ijcnlp
      
gold  k     scassellati  b          robot uses existing vocabulary infer non visual word
meanings observation  proceedings twenty second conference artificial
intelligence  aaai     
gorniak  p     roy  d          speaking sidekick  understanding situated speech
computer role playing games  proceedings  th conference artificial intelligence
interactive digital entertainment stanford  ca 
gupta  s     mooney  r          using closed captions train activity recognizers improve
video retrieval  proceedings cvpr    workshop visual contextual learning
annotated images videos  vcl  miami  fl 
   

fit raining ultilingual portscaster

harnad  s          symbol grounding problem  physica d             
herzog  g     wazinski  p          visual translator  linking perceptions natural language
descriptions  artificial intelligence review                 
ide  n  a     jeronis  j          introduction special issue word sense disambiguation 
state art  computational linguistics             
joachims  t          text categorization support vector machines  learning many relevant
features  proceedings tenth european conference machine learning  ecml     pp         berlin  springer verlag 
jurcicek  j   gasic  m   keizer  s   mairesse  f   thomson  b     young  s          transformationbased learning semantic parsing  interspeech brighton  uk 
kate  r  j     mooney  r  j          using string kernels learning semantic parsers  proceedings   st international conference computational linguistics   th annual
meeting association computational linguistics  coling acl      pp        
sydney  australia 
kate  r  j     mooney  r  j          learning language semantics ambiguous supervision 
proceedings twenty second conference artificial intelligence  aaai      pp 
       vancouver  canada 
kerr  w   cohen  p  r     chang  y  h          learning playing wubble world  proceedings fourth artificial intelligence interactive digital entertainment conference
 aiide  palo alto  ca 
kingsbury  p   palmer  m     marcus  m          adding semantic annotation penn treebank 
proceedings human language technology conference san diego  ca 
knight  k     hatzivassiloglou  v          two level  many paths generation  proceedings
  rd annual meeting association computational linguistics  acl      pp 
       cambridge  ma 
lau  t   drews  c     nichols  j          interpreting written how to instructions  proceedings
twenty first international joint conference artificial intelligence  ijcai       
lavrac  n     dzeroski  s          inductive logic programming  techniques applications 
ellis horwood 
liang  p   jordan  m  i     klein  d          learning semantic correspondences less supervision  proceedings joint conference   th annual meeting association
computational linguistics  th international joint conference natural language
processing asian federation natural language processin  acl ijcnlp       
lodhi  h   saunders  c   shawe taylor  j   cristianini  n     watkins  c          text classification
using string kernels  journal machine learning research            
   

fic hen   k im     ooney

lu  w   ng  h  t   lee  w  s     zettlemoyer  l  s          generative model parsing natural
language meaning representations  proceedings      conference empirical
methods natural language processing  emnlp     honolulu  hi 
marcus  m   santorini  b     marcinkiewicz  m  a          building large annotated corpus
english  penn treebank  computational linguistics                
mckeown  k  r          discourse strategies generating natural language text  artificial intelligence             
meza ruiz  i  v   riedel  s     lemon  o          spoken language understanding dialogue
systems  using   layer markov logic network  improving semantic accuracy  proceedings
londial 
mooney  r  j          learning semantic parsing  gelbukh  a   ed    computational linguistics intelligent text processing  proceedings  th international conference 
cicling       mexico city  pp          springer verlag  berlin 
och  f  j     ney  h          systematic comparison various statistical alignment models 
computational linguistics              
papineni  k   roukos  s   ward  t     zhu  w  j          bleu  method automatic evaluation
machine translation  proceedings   th annual meeting association
computational linguistics  acl        pp         philadelphia  pa 
riezler  s   prescher  d   kuhn  j     johnson  m          lexicalized stochastic modeling
constraint based grammars using log linear measures em training  proceedings
  th annual meeting association computational linguistics  acl        pp 
       hong kong 
roy  d          learning visually grounded words syntax scene description task  computer speech language                
rozinat  a   zickler  s   veloso  m   van der aalst  w     mcmillen  c          analyzing multiagent activity logs using process mining techniques  proceedings  th international
symposium distributed autonomous robotic systems  dars     tsukuba  japan 
satoh  s   nakamura  y     kanade  t          name it  naming detecting faces video
integration image natural language processing  proceedings fifteenth
international joint conference artificial intelligence  ijcai     
shawe taylor  j     cristianini  n          kernel methods pattern analysis  cambridge university press 
shieber  s  m          uniform architecture parsing generation  proceedings
  th international conference computational linguistics  coling      pp         budapest  hungary 
siskind  j  m          computational study cross situational techniques learning word tomeaning mappings  cognition              
   

fit raining ultilingual portscaster

snyder  b     barzilay  r          database text alignment via structured multilabel classification  proceedings twentieth international joint conference artificial intelligence
 ijcai       
srihari  r  k     burhans  d  t          visual semantics  extracting visual information
text accompanying pictures  proceedings twelfth national conference artificial
intelligence  aaai     
stolcke  a          efficient probabilistic context free parsing algorithm computes prefix
probabilities  computational linguistics                
whorf  b  l          language  thought  reality  selected writings  mit press 
wong  y     mooney  r  j          learning semantic parsing statistical machine translation  proceedings human language technology conference   north american chapter
association computational linguistics annual meeting  hlt naacl      pp     
    new york city  ny 
wong  y     mooney  r  j          generation inverting semantic parser uses statistical
machine translation  proceedings human language technologies  conference
north american chapter association computational linguistics  naacl hlt     pp         rochester  ny 
yamada  k     knight  k          syntax based statistical translation model  proceedings
  th annual meeting association computational linguistics  acl        pp 
       toulouse  france 
yu  c     ballard  d  h          integration grounding language learning objects 
proceedings nineteenth national conference artificial intelligence  aaai      pp 
       
zaragoza  h     li  c  h          learning talk descriptive games  proceedings
human language technology conference conference empirical methods
natural language processing  hlt emnlp      pp         vancouver  canada 
zelenko  d   aone  c     richardella  a          kernel methods relation extraction  journal
machine learning research              
zettlemoyer  l  s     collins  m          learning map sentences logical form  structured
classification probabilistic categorial grammars  proceedings   st conference
uncertainty artificial intelligence  uai       edinburgh  scotland 
zettlemoyer  l  s     collins  m          online learning relaxed ccg grammars parsing
logical form  proceedings      joint conference empirical methods natural
language processing computational natural language learning  emnlp conll     
pp         prague  czech republic 

   


