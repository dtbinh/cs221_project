journal of artificial intelligence research                  

submitted        published     

an investigation into mathematical programming
for finite horizon decentralized pomdps
raghav aras

raghav aras gmail com

ims  suplec metz
  rue edouard belin  metz technopole
      metz   france

alain dutech

alain dutech loria fr

maia   loria inria
campus scientifique   bp    
      vandoeuvre les nancy   france

abstract
decentralized planning in uncertain environments is a complex task generally dealt with
by using a decision theoretic approach  mainly through the framework of decentralized partially observable markov decision processes  dec pomdps   although dec pomdps
are a general and powerful modeling tool  solving them is a task with an overwhelming
complexity that can be doubly exponential  in this paper  we study an alternate formulation of dec pomdps relying on a sequence form representation of policies  from this
formulation  we show how to derive mixed integer linear programming  milp  problems
that  once solved  give exact optimal solutions to the dec pomdps  we show that these
milps can be derived either by using some combinatorial characteristics of the optimal
solutions of the dec pomdps or by using concepts borrowed from game theory  through
an experimental validation on classical test problems from the dec pomdp literature  we
compare our approach to existing algorithms  results show that mathematical programming outperforms dynamic programming but is less efficient than forward search  except
for some particular problems 
the main contributions of this work are the use of mathematical programming for decpomdps and a better understanding of dec pomdps and of their solutions  besides 
we argue that our alternate representation of dec pomdps could be helpful for designing
novel algorithms looking for approximate solutions to dec pomdps 

   introduction
the framework of decentralized partially observable markov decision processes  decpomdps  can be used to model the problem of designing a system made of autonomous
agents that need to coordinate in order to achieve a joint goal  solving dec pomdps is an
untractable task as they belong to the class of nexp complete problems  see section      
in this paper  dec pomdps are reformulated into sequence form dec pomdps so as
to derive mixed integer linear programs that can be solved using very efficient solvers
in order to design exact optimal solutions to finite horizon dec pomdps  our main
motivation is to investigate the benefits and limits of this novel approach and to get a
better understanding of dec pomdps  see section       on a practical level  we provide
new algorithms and heuristics for solving dec pomdps and evaluate them on classical
problems  see section      
c
    
ai access foundation  all rights reserved 

fiaras   dutech

    context
one of the main goals of artificial intelligence is to build artificial agents that exhibit
intelligent behavior  an agent is an entity situated in an environment which it can perceive
through sensors and act upon using actuators  the concept of planning  i e   to select
a sequence of actions in order to reach a goal  has been central to the field of artificial
intelligence for years  while the notion of intelligent behavior is difficult to assess and to
measure  we prefer to refer to the concept of rational behavior as formulated by russell
and norvig         as a consequence  the work presented here uses a decision theoretic
approach in order to build agents that take optimal actions in an uncertain and partially
unknown environment 
we are more particularly interested in cooperative multi agent systems where multiple
independent agents with limited perception of their environment must interact and coordinate in order to achieve a joint task  no central process with a full knowledge of the state
of the system is there to control the agents  on the contrary  each agent is an autonomous
entity that must execute its actions by itself  this setting is both a blessing  as each agent
should ideally deal with a small part of the problem  and a curse  as coordination and
cooperation are harder to develop and to enforce 
the decision theoretic approach to rational behavior relies mostly on the framework of
markov decision processes  mdp   puterman         a system is seen as a sequence of
discrete states with stochastic dynamics  some particular states giving a positive or negative
reward  the process is divided into discrete decision periods  the number of such periods
is called the horizon of the mdp  at each of these periods  an action is chosen which
will influence the transition of the process to its next state  by using the right actions to
influence the transition probabilities between states  the objective of the controller of the
system is to maximize its long term return  which is often an additive function of the reward
earned for the given horizon  if the controller knows the dynamics of the system  which is
made of a transition function and of a reward function  algorithms derived from the field
of dynamic programming  see bellman        allow the controller to compute an optimal
deterministic policy  i e   a decision function which associates an optimal action to every
state so that the expected long term return is optimal  this process is called planning in
the mdp community 
in fact  using the mdp framework  it is quite straightforward to model a problem with
one agent which has a full and complete knowledge of the state of the system  but agents 
and especially in a multi agent setting  are generally not able to determine the complete
and exact state of the system because of noisy  faulty or limited sensors or because of the
nature of the problem itself  as a consequence  different states of the system are observed
as similar by the agent which is a problem when different optimal actions should be taken in
these states  one speaks then of perceptual aliasing  an extension of mdps called partially
observable markov decisions processes  pomdps  deals explicitly with this phenomenon
and allows a single agent to compute plans in such a setting provided it knows the conditional
probabilities of observations given the state of the environment  cassandra  kaelbling   
littman        
as pointed out by boutilier         multi agent problems could be solved as mdps
if considered from a centralized point of view for planning and control  here  although
   

fimathematical programming for dec pomdps

planning is a centralized process  we are interested in decentralized settings where every
agent executes its own policy  even if the agents could instantly communicate their observation  we consider problems where the joint observation resulting from such communications would still not be enough to identify the state of the system  the framework of
decentralized partially observable markov decision processes  dec pomdp  proposed
by bernstein  givan  immerman  and zilberstein        takes into account decentralization
of control and partial observability  in a dec pomdp  we are looking for optimal joint
policies which are composed of one policy for each agent  these individual policies being
computed in a centralized way but then independently executed by the agents 
the main limitation of dec pomdps is that they are provably untractable as they
belong to the class of nexp complete problems  bernstein et al          concretely  this
complexity result implies that  in the worst case  finding an optimal joint policy of a finite
horizon dec pomdp requires time that is exponential in the horizon if one always make
good choices  because of this complexity  there are very few algorithms for finding exact
optimal solutions for dec pomdps  they all have a doubly exponential complexity  and
only a few more that look for approximate solutions  as discussed and detailed in the
work of oliehoek  spaan  and vlassis         these algorithms follow either a dynamic
programming approach or a forward search approach by adapting concepts and algorithms
that were designed for pomdps 
yet  the concept of decentralized planning has been the focus of quite a large body of
previous work in other fields of research  for example  the team decision problem  radner 
       later formulated as a markov system in the field of control theory by anderson and
moore         led to the markov team decision problem  pynadath   tambe         in
the field of mathematics  the abundant literature on game theory brings a new way for
looking at multi agent planning  in particular  a dec pomdp with finite horizon can
be thought as a game in extensive form with imperfect information and identical interests
 osborne   rubinstein        
taking inspiration from the field of game theory and mathematical programming to design exact algorithms for solving dec pomdps is precisely the subject of our contribution
to the field of decentralized multi agent planning 
    motivations
the main objective of our work is to investigate the use of mathematical programming 
more especially mixed integer linear programs  milp   diwekar         for solving decpomdps  our motivation relies on the fact that the field of linear programming is quite
mature and of great interest to the industry  as a consequence  there exist many efficient
solvers for mixed integer linear programs and we want to see how these efficient solvers
perform in the framework of dec pomdps 
therefore  we have to reformulate a dec pomdp to solve it as a mixed integer linear
program  as shown in this article  two paths lead to such mathematical programs  one
grounded on the work from koller  megiddo  and von stengel         koller and megiddo
       and von stengel         and another one grounded on combinatorial considerations 
both methods rely on a special reformulation of dec pomdps in what we have called
   

fiaras   dutech

sequence form dec pomdps where a policy is defined by the histories  i e   sequences of
observations and actions  it can generate when applied to the dec pomdp 
the basic idea of our work is to select  among all the histories of the dec pomdp 
the histories that will be part of the optimal policy  to that end  an optimal solution to
the milp presented in this article will assign a positive weight to each history of the decpomdp and every history with a non negative weight will be part of the optimal policy
to the dec pomdp  as the number of possible histories is exponential in the horizon of
the problem  the complexity of a naive search for the optimal set of histories is doubly
exponential  therefore  our idea appears untractable and useless 
nevertheless  we will show that combining the efficiency of milp solvers with some quite
simple heuristics leads to exact algorithms that compare quite well to some existing exact
algorithms  in fact  sequence form dec pomdps only need a memory space exponential
in the size of the problem  even if solving milps can also be exponential in the size of the
milp and thus leads to doubly exponential complexity for sequence form based algorithms 
we argue that sequence form milps compare quite well to dynamic programming thanks
to optimized industrial milp solvers like cplex 
still  our investigations and experiments with mathematical programming for decpomdps do not solely aim at finding exact solutions to dec pomdps  our main motivation is to have a better understanding of dec pomdps and of the limits and benefits of
the mathematical programming approach  we hope that this knowledge will help deciding
to what extent mathematical programming and sequence form dec pomdps can be used
to design novel algorithms that look for approximate solutions to dec pomdps 
    contributions
in this paper we develop new algorithms in order to find exact optimal joint policies for
dec pomdps  our main inspiration comes from the work of koller  von stegel and
megiddo that shows how to solve games in extensive form with imperfect information and
identical interests  that is how to find a nash equilibrium for this kind of game  koller et al  
      koller   megiddo        von stengel         their algorithms caused a breakthrough
as the memory space requirement of their approach is linear in the size of the game whereas
more canonical algorithms required space that is exponential in the size of the game  this
breakthrough is mostly due to the use of a new formulation of a policy in what they call a
sequence form 
our main contribution  as detailed in section      is then to adapt the sequence form
introduced by koller  von stegel and megiddo to the framework of dec pomdps  koller
et al         koller   megiddo        von stengel         as a result  it is possible to
formulate the resolution of a dec pomdp as a special kind of mathematical program
that can still be solved quite efficiently  a mixed linear program where some variables
are required to be either   or    the adaptation and the resulting mixed integer linear
program is not straightforward  in fact  koller  von stegel and megiddo could only find
one nash equilibrium in a   agent game  what is needed for dec pomdps is to find
the set of policies  called a joint policy  that corresponds to the nash equilibrium with
the highest value  finding only one nash equilibrium  already a complex task  is not
enough  besides  whereas koller  von stegel and megiddo algorithms could only be applied
   

fimathematical programming for dec pomdps

to   agent games  we extend the approach so as to solve dec pomdps with an arbitrary
number of agents  which constitutes an important contribution 
in order to formulate dec pomdps as milps  we analyze in detail the structure of
an optimal joint policy for a dec pomdp  a joint policy in sequence form is expressed as
a set of individual policies that are themselves described as a set of possible trajectories for
each of the agents of the dec pomdp  combinatorial considerations on these individual
histories  as well as constraints that ensure these histories do define a valid joint policy are
at the heart of the formulation of a dec pomdp as a mixed linear program  as developped
in sections   and    thus  another contribution of our work is a better understanding of
the properties of optimal solutions to dec pomdps  a knowledge that might lead to the
formulation of new approximate algorithms for dec pomdps 
another important contribution of this work is that we introduce heuristics for boosting the performance of the mathematical programs we propose  see section     these
heuristics take advantage of the succinctness of the dec pomdp model and of the knowledge acquired regarding the structure of optimal policies  consequently  we are able to
reduce the size of the mathematical programs  resulting also in reducing the time taken to
solve them   these heuristics constitute an important pre processing step in solving the
programs  we present two types of heuristics  the elimination of extraneous histories which
reduces the size of the mixed integer linear programs and the introduction of cuts in the
mixed integer linear programs which reduces the time taken to solve a program 
on a more practical level  this article presents three different mixed integer linear
programs  two are more directly derived from the work of koller  von stegel and megiddo
 see table   and    and a third one is based solely on combinatorial considerations on the
individual policies and histories  see table     the theoretical validity of these formulations is backed by several theorems  we also conducted experimental evaluations of our
algorithms and of our heuristics on several classical dec pomdp problems  we were thus
able to confirm that our algorithms are quite comparable to dynamic programming exact
algorithms but outperformed by forward search algorithms like gmaa   oliehoek et al  
       on some problems  though  milps are indeed faster by one order of magnitude or
two than gmaa  
    overview of this article
the remainder of this article is organized as follows  section   introduces the formalism of
dec pomdp and some background on the classical algorithms  usually based on dynamic
programing  then we expose our reformulation of the dec pomdp in sequence form in
section   where we also define various notions needed by the sequence form  in section   
we show how to use combinatorial properties of the sequence form policies to derive a first
mixed integer linear program  milp  in table    for solving dec pomdp  by using game
theoretic concepts like nash equilibrium  we take inspiration from previous work on games
in extensive form to design two other milps for solving dec pomdp  tables        these
milps are smaller in size and their detailed derivation is presented in section    our
contributed heuristics to speed up the practical resolutions of the various milps make
up the core of section    section   presents experimental validations of our milp based
algorithms on classical benchmarks of the dec pomdp literature as well as on randomly
   

fiaras   dutech

built problems  finally  section   analyzes and discusses our work and we conclude this
paper with section   

   dec pomdp
this section gives a formal definition of decentralized partially observed markov decision
processes as introduced by bernstein et al          as described  a solution of a decpomdp is a policy defined on the space of information sets that has an optimal value 
this sections ends with a quick overview of the classical methods that have been developed
to solve dec pomdps 
    formal definition
a dec pomdp is defined as a tuple d   h i  s   ai    p   oi    g  r  t    i where 
 i               n  is a set of agents 
 s is a finite set of states  the set of probability distributions over s shall be denoted
by  s   members of  s  shall be called belief states 
 for each agent i  i  ai is a set of actions  a   ii ai denotes the set of joint
actions 
 p   s  a  s         is a state transition function  for each s  s  s and for each
a  a  p s  a  s   is the probability that the state of the problem in a period t is s if 
in period t     its state was s and the agents performed the joint action a  thus  for
any time period t     for each pair of states s  s  s and for each joint action a  a 
there holds 
p s  a  s     pr st   s  st    s  at   a  
thus   s  a  p  defines a discrete state  discrete time controlled markov process 
 for each agent i  i  oi is a set of observations  o   ii oi denotes the set of joint
observations 
 g   a  s  o         is a joint observation function  for each a  a  for each
o  o and for each s  s  g a  s  o  is the probability that the agents receive the
joint observation o  that is  each agent i receives the observation oi   if the state of
the problem in that period is s and if in the previous period the agents took the joint
action a  thus  for any time period t     for each joint action a  a  for each state
s  s and for each joint observation o  o  there holds 
g a  s  o    pr ot   o st   s  at    a  
 r   s  a  r is a reward function  for each s  s and for each a  a  r s  a   r
is the reward obtained by the agents if they take the joint action a when the state of
the process is s 
   

fimathematical programming for dec pomdps

 t is the horizon of the problem  the agents are allowed t joint actions before the
process halts 
    s  is the initial state of the dec pomdp  for each s  s   s  denotes the
probability that the state of the problem in the first period is s 
as said  s  a and p define a controlled markov process where the next state depends
only on the previous state and on the joint action chosen by the agents  but the agents
do not have access to the state of the process and can only rely on observations  generally
partial and noisy  of this state  as specified by the observation function g  from time to
time  agents receive a non zero reward according to the reward function r 

a n 

o n 

a   

o   

s 

s 

o nt

a n 

o  t

a   

s 

a nt

a  t

st

figure    dec pomdp  at every period t of the process  the environment is in state
st   every agent i receives observations oti and decides of its action ati   the joint
action hat    at         atn i alters the state of the process 
more specifically  as illustrated in figure    the control of a dec pomdp by the n
agents unfolds over discrete time periods  t             t as follows  in each period t  the
process is in a state denoted by st from s  in the first period t      the state s  is chosen
according to  and the agents take actions a i   in each period t     afterward  each agent
i  i takes an action denoted by ati from ai according to the agents policy  when the
agents take the joint action at   hat    at         atn i  the following events occur 
   the agents all obtain the same reward r st   at   
   the state st   is determined according to the function p with arguments st and at  
   each agent i  i receives an observation ot  
from oi   the joint observation ot    
i
t  
t  
t   and at  
hot  
    o         on i is determined by the function g with arguments s
   the period changes from t to t     
in this paper  the dec pomdp we are interested in have the following properties 
   

fiaras   dutech

 the horizon t is finite and known by the agents 
 agents cannot infer the exact state of the system from their joint observations  this is
the more general setting of dec pomdps  
 agents do not observe actions and observations of the other agents  they are only
aware of their own observations and reward 
 agents have a perfect memory of their past  they can base their choice of action on
the sequence of past actions and observations  we speak of perfect recall setting 
 transition and observation functions are stationary  meaning that they do not depend
on the period t 
solving a dec pomdp means finding the agents policies  i e   their decision functions 
to optimize a given criterion based on the rewards received  the criterion we will work with
is called the cumulative reward and defined by 
  t
 
x
t
t
t
t
e
r s   ha    a            an i 
   
t  

where e is the mathematical expectation 
    example of dec pomdp
the problem known as the decentralized tiger problem  hereby denoted ma tiger  
introduced by nair  tambe  yokoo  pynadath  and marsella         has been widely used
to test dec pomdps algorithms  it is a variation of a problem previously introduced
for pomdps  i e   dec pomdps with one agent  by kaelbling  littman  and cassandra
       
in this problem  we are given two agents confronted with two closed doors  behind one
door is a tiger  behind the other an escape route  the agents do not know which door
leads to what  each agent  independently of the other  can open one of the two doors or
listen carefully in order to detect the tiger  if either of them opens the wrong door  the
lives of both will be imperiled  if they both open the escape door  they will be free  the
agents have a limited time in which to decide which door to open  they can use this time
to gather information about the precise location of the tiger by listening carefully to detect
the location of the tiger  this problem can be formalized as a dec pomdp with 
 two states as the tiger is either behind the left door  sl   or the right door  sr   
 two agents  that must decide and act 
 three actions for each agent  open the left door  al    open the right door  ar   and
listen  ao   
 two observations  as the only thing the agent can observe is that they hear the tiger
on the left  ol   or on the right  or   
   

fimathematical programming for dec pomdps

the initial state is chosen according to a uniform distribution over s  as long as the door
remains closed  the state does not change but  when one door is opened  the state is reset
to either sl or sr with equal probability  the observations are noisy  reflecting the difficulty
of detecting the tiger  for example  when the tiger is on the left  the action ao produces
an observation ol only     of the time  so if both agents perform ao   the joint observation
 ol  ol   occurs with a probability of                    the reward function encourages the
agents to coordinate their actions as  for example  the reward when both open the escape
door       is bigger than when one listens while the other opens the good door       the
full state transition function  joint observation function and reward function are described
in the work of nair et al         
    information sets and histories
an information set  of agent i is a sequence  a   o   a   o      ot   of even length in which
the elements in odd positions are actions of the agent  members of ai   and those in even
positions are observations of the agent  members of oi    an information set of length  
shall be called the null information set  denoted by   an information set of length t   
shall be called a terminal information set  the set of information sets of lengths less
than or equal to t    shall be denoted by i  
we define a history of agent i  i to be a sequence  a   o    a    o      ot  at   of odd
length in which the elements in odd positions are actions of the agent  members of ai   and
those in even positions are observations of the agent  members of oi    we define the length
of a history to be the number of actions in the history  t in our example   a history of
length t shall be called a terminal history  histories of lengths less than t shall be called
non terminal histories  the history of null length shall be denoted   the information
set associated to an history h  denoted  h   is the information set composed by removing
from h its last action  if h is a history and o an observation  then h o is an information set 
we shall denote by hit the set of all possible histories of length t of agent i  thus  hi  is
just the set of actions ai   we shall denote by hi the set of histories of agent i of lengths
less than or equal to t   the size ni of hi is thus 
ni    hi    

pt

t
t 
t    ai    oi  

   ai  

  ai   oi   t   
 
 ai   oi     

   

the set hit of terminal histories of agent i shall be denoted by ei   the set hi  hit of
non terminal histories of agent i shall be denoted by ni  
a tuple hh    h            hn i made of one history for each agent is called a joint history  the
tuple obtained by removing the history hi from the joint history h is noted hi and called
an i reduced joint history 
example coming back to the ma tiger example  a set of valid histories could be     ao   
 ao  ol  ao     ao  or  ao     ao  ol  ao  ol  ao     ao  ol  ao  or  ar     ao  or  ao  ol  ao   and  ao  or  ao  or  ar   
incidently  this set of histories corresponds to the support of the policy  i e   the histories
generated by using this policy  of the figure    as explained in the next section 
   

fiaras   dutech

    policies
at each period of time  a policy must tell an agent what action to choose  this choice
can be based on whatever past and present knowledge the agent has about the process at
time t  one possibility is to define an individual policy i of agent i as a mapping from
information sets to actions  more formally 
i   i   ai  

   

among the set  of policies  three families are usually distinguished 
 pure policies  a pure or deterministic policy maps a given information set to one
unique action  the set of pure policies for the agent i is denoted   pure policies
could also be defined using trajectories of past observations only since actions  which
are chosen deterministically  can be reconstructed from the observations 
 mixed policies  a mixed policy is a probability distribution over the set of pure
policies  thus  an agent using a mixed policy will control the dec pomdp by using
a pure policy randomly chosen from a set of pure policies 
 stochastic policies  a stochastic policy is the more general formulation as it associates
a probability distribution over actions to each history 
if we come back to the ma tiger problem  section       figure   gives a possible policy
for a horizon    as shown  a policy is classically represented by an action observation tree 
in that kind of tree  each branch is labelled by an observation  for a given sequence of past
observations  one starts from the root node and follows the branches down to an action
node  this node contains the action to be executed by the agent when it has seen this
sequence of observations 
observation sequence
chosen action

ol
ao


ao

or
ao

ol  ol
al

ol  or
ao

or  ol
ao

or  or
ar

ao
ol

or

ao

ao

ol

or

ol

or

al

ao

ao

ar

figure    pure policy for ma tiger  a pure policy maps sequences of observations to
actions  this can be represented by an action observation tree 

a joint policy    h             n i is an n tuple where each i is a policy for agent i 
each of the individual policies must have the same horizon  for an agent i  we also define
the notion of an i reduced joint policy i   h         i    i          n i composed of
the policies of all the other agents  we thus have that    hi   i i 
   

fimathematical programming for dec pomdps

    value function
when executed by the agents  every t  horizon joint policy generates a probability distribution over the possible sequences of reward from which one can compute the value of the
policy according to equation    thus the value of the joint policy  is formally defined as 
v        e

  t
x

r st   at     

t  

 

   

given that the state in the first period is chosen according to  and that actions are chosen
according to  
there is a recursive definition of the value function of a policy  that is also a way to
compute it when the horizon t is finite  this definition requires some concepts that we
shall now introduce 
given a belief state    s   a joint action a  a and a joint observation o  o 
let t  o   a  denote the probability that the agents receive joint observation o if they take
joint action a in a period t in which the state is chosen according to   this probability is
defined as
x
x
t  o   a   
 s 
p s  a  s  g a  s   o 
   
s s

ss

given a belief state    s   a joint action a  a and a joint observation o  o   the
updated belief state  ao   s  of  with respect to a and o is defined as  for each
s  s  
 ao  s    

p
g a s  o   ss  s p s a s   
t  o  a 

if t  o   a     

   

 ao  s    

 

if t  o   a     

   

p
given a belief state    s  and a joint action a  a  r   a  denotes ss  s r s  a  
using the above definitions and notations  the value v      of  is defined as follows 
v        v       

   

where v        is defined by recursion using equations           and       given below 
these equations are a straight reformulation of the classical bellman equations for finite
horizon problems 
 for histories of null length
v          r        

x

t  o      v    o     o 

   

oo

   denotes the joint action h                 n   i and   o denotes the
updated state of  given    and the joint observation o 
   

fiaras   dutech

 for non terminal histories  for any    s   for each t of             t      for each
  t   o
  t
  t
tuple of sequences of t observations o  t   ho  t
t i
    o         on i where oi
is a sequence of t observations of agent i  i 
v       o  t     r     o  t     

x

  t  o

t  o     o  t   v   o

    o  t  o      

oo
  t

 o  o is the updated state of  given the joint action  o  t   and joint observation
o   ho    o         on i and o  t  o is the tuple of sequences of  t      observations ho  t
   o   
  t  o i 
o  t
 o
 



 
o
n
 
n
 
 for terminal histories  for any    s   for each tuple of sequences of  t     
 
 
  i 
observations o  t     ho  t
  o  t
       o  t
n
 
 
v       o  t       r    o  t       

x

  s r s   o  t     

    

ss

an optimal policy   is a policy with the best possible value  verifying 
v         v     

   

    

an important fact about dec pomdps  based on the following theorem  is that we can
restrict ourselves to the set of pure policies when looking for a solution to a dec pomdp 
theorem      a dec pomdp has at least one optimal pure joint policy 
proof  see proof in the work of nair et al         



    overview of dec pomdps solutions and limitations
as detailed in the work of oliehoek et al          existing methods for solving decpomdps with finite horizon belong to several broad families  brute force  alternating
maximization  search algorithms and dynamic programming 
brute force the simplest approach for solving a dec pomdp is to enumerate all
possible joint policies and to evaluate them in order to find the optimal one  however  such
a method becomes quickly untractable as the number of joint policies is doubly exponential
in the horizon of the problem 
alternating maximization following chades  scherrer  and charpillet        and nair
et al          one possible way to solve dec pomdps is for each agent  or each small group
of agents  to alternatively search for a better policy while all the other agents freeze their
own policy  called alternating maximization by oliehoek and alternated co evolution by
chades this method guarantees only to find a nash equilibria  that is a locally optimal joint
policy 
   

fimathematical programming for dec pomdps

heuristic search algorithms the concept was introduced by szer  charpillet  and
zilberstein        and relies on heuristic search for looking for an optimal joint policy 
using an admissible approximation of the value of the optimal joint policy  as the search
progresses  joint policies that will provably by worse that the current admissible solution are
pruned  szer et al  used underlying mdps or pomdps to compute the admissible heuristic 
oliehoek et al         introduced a better heuristic based on the resolution of a bayesian
game with a carefully crafted cost function  currently  oliehoeks method called gmaa 
 for generic multi agent a   is the quickest exact method on a large set of benchmarks 
but  as every exact method  it is limited to quite simple problems 
dynamic programming the work from hansen  bernstein  and zilberstein       
adapts solutions designed for pomdps to the domain of dec pomdps  the general
idea is to start with policies for   time step that are used to build   time step policies
and so on  but the process is clearly less efficient that the heuristic search approach as an
exponential number of policies must be constructed and evaluated at each iteration of the
algorithm  some of these policies can be pruned but  once again  pruning is less efficient 
as exposed in more details in the paper by oliehoek et al          several others approaches have been developed for subclasses of dec pomdps  for example  special settings where agents are allowed to communicate and exchange informations or settings where
the transition function can be split into independant transition functions for each agent have
been studied and found easier to solve than generic dec pomdps 

   sequence form of dec pomdps
this section introduces the fundamental concept of policies in sequence form  a new
formulation of a dec pomdp is thus possible and this leads to a non linear program
 nlp  the solution of which defines an optimal solution to the dec pomdp 
    policies in sequence form
a history function p of an agent i is a mapping from the set of histories to the interval
        the value p h  is the weight of the history h for the history function p  a policy i
defines a probability function over the set of histories of the agent i by saying that  for each
history hi of hi   p hi   is the conditional probability of hi given an observation sequence
 o i  o i       oti   and i  
if every policy defines a policy function  not every policy function can be associated to
a valid policy  some constraints must be met  in fact  a history function p is a sequenceform policy for agent i when the following constraints are met 
x

p a      

    

aai

p h   

x

p h o a      

h  ni   o  oi  

    

aai

where h o a denotes the history obtained on concatenating o and a to h  this definition
appears in a slightly different form as lemma     in the work of koller et al         
   

fiaras   dutech

variables  x h   h  hi  
x

x a     

    

aai

x h   

x

x h o a      

h  ni   o  oi

    

h  hi

    

aai

x h     

table    policy constraints  this set of linear inequalities  once solved  provide a valid
sequence form policy for the agent i  that is  from the weights x h   it is possible
to define a policy for the agent i 

a sequence form policy can be stochastic as the probability of choosing action a in the
information set h o is p h o a  p h   the support s p  of a sequence form policy is made
of the set of histories that have a non negative weight  i e  s p     h  hi   p h        as
a sequence form policy p defines a unique policy  for an agent  a sequence form policy will
be called a policy in the rest of this paper when no ambiguity is present 
the set of policies in the sequence form of agent i shall be denoted by xi   the set of
pure policies in the sequence form shall be denoted by xi  xi  
in a way similar to the definitions of section      we define a sequence form joint
policy as a tuple of sequence form policies  one for each agent  the weight q
of the joint
history h   hhi i of a sequence form joint policy hp    p         pn i is the product ii pi  hi   
the set of joint policies in the sequence form ii xi shall be denoted by x and the set of
i reduced sequence form joint policy is called xi  
    policy constraints
a policy of agent i in the sequence form can be found by solving a set of linear inequalities
 li  found in table    these li merely implement the definition of a policy in the sequenceform  the li contains one variable x h  for each history h  hi to represent the weight of
h in the policy  a solution x to these li constitutes a policy in the sequence form 
example in the section e   of the appendices  the policy constraints for the decentralized
tiger problem are given for   agents and a horizon of   
notice that in the policy constraints of an agent  each variable is only constrained to be
non negative whereas by the definition of a policy in sequence form  the weight of a history
must be in the interval         does it mean that a variable in a solution to the policy
constraints can assume a value higher than    actually  the policy constraints are such
that they prevent any variable from assuming a value higher than   as the following lemma
shows 
lemma      in every solution x to            for each h  hi   x  h  belongs to the       
interval 
   

fimathematical programming for dec pomdps

proof  this can be shown by forward induction 
every x h  being non negative  see eq         it is also the case for every action a of
ai   then  no x a  can be greater than   otherwise constraint      would be violated  so 
h  hi     i e  a  ai    we have x h  belong to        
if every h of hit is such that x h           the previous reasoning applied using constraint
     leads evidently to the fact that x h          for every h of hit    
thereby  by induction this holds for all t 

later in this article  in order to simplify the task of looking for joint policies  the policy
constraints li will be used to find pure policies  looking for pure policies is not a limitation
as finite horizon dec pomdps admit deterministic policies when the policies are defined
on information set  in fact  pure policies are needed in two of the three milps we build
in order to solve dec pomdps  otherwise their derivation would not be possible  see
sections   and      
looking for pure policies  an obvious solution would be to impose that every variable
x h  belongs to the set         but  when solving a mixed integer linear program  it is
generally a good idea to limit the number of integer variables as each integer variable is
a possible node for the branch and bound method used to assign integer values to the
variables  a more efficient implementation of a mixed integer linear program is to take
advantage of the following lemma to impose that only the weights of the terminal histories
take   or   as possible values 
lemma      if in                 is replaced by 
x h     

h  ni

x h          

h  ei

    
    

then in every solution x to the resulting li  for each h  hi   x  h      or    we will
speak of a     li 
proof  we can prove this by backward induction  let h be a history of length t     
due to       for each o  oi   there holds 
x
x  h   
x  h o a  
    
aai

since h is a history of length t      each history h o a is a terminal history  due to lemma
     x  h           therefore  the sum on the right hand side of the above equation is also
in         but due to       each x  h o a           hence the sum on the right hand side
is either   or    and not any value in between  ergo  x  h          and not any value in
between  by this same reasoning  we can show that x  h          for every non terminal
history h of length t      t             

to formulate the linear inequalities of table   in memory  we require
p space that is only
exponential in the horizon  for each agent i  i  the size of hi is tt    ai  t  oi  t    it is
then exponential in t and the number of variables
lp is also exponential in t   the
pt  in the
t
number of constraints in the li of table   is t    ai    oi  t   meaning that the number of
constraints of the li is also exponential in t  
   

fiaras   dutech

    sequence form of a dec pomdp
we are now able to give a formulation of a dec pomdp based on the use of sequence form
policies  we want to stress that this is only a re formulation  but as such will provide us
with new ways of solving dec pomdps with mathematical programming 
given a classical formulation of a dec pomdp  see section       the equivalent
sequence form dec pomdp is a tuple hi   hi      ri where 
 i               n  is a set of agents 
 for each agent i  i  hi is the set of histories of length less than or equal to t for
the agent i  as defined in the previous section  each set hi is derived using the sets
ai and oi  
  is the joint history conditional probability function  for each joint history j  h 
   j  is the probability of j occurring conditional on the agents taking joint actions
according to it and given that the initial state of the dec pomdp is   this function
is derived using the set of states s  the state transition function p and the joint
observation function g 
 r is the joint history value  for each joint history j  h  r   j  is the value of
the expected reward the agents obtain if the joint history j occurs  this function is
derived using the set of states s  the state transition function p  the joint observation
function g and the reward function r  alternatively  r can be described as a function
of  and r 
this formulation folds s  p and g into  and r by relying on the set of histories  we
will now give more details about the computation of  and r 
   j  is the conditional probability that the sequence of joint observations received
by the agents till period t is  o   j  o   j       ot   j   if the sequence of joint actions
taken by them till period t     is  a   j   a   j        at   j   and the initial state of the
dec pomdp is   that is 
   j    prob  o   j  o   j       ot   j    a   j  a   j       at   j  

    

this probability is the product of the probabilities of seeing observation ok  j  given the
appropriate belief state and action chosen at time k  that is 
   j   

t 
y

t  ok  j  jk    ak  j  

    

k  

where jk is the probability distribution on s given that the agents have followed the joint
history j up to time k  that is 
jk  s    prob  s o   j  a   j       ok  j   
   

    

fimathematical programming for dec pomdps

variables  xi  h   i  i  h  hi
maximize

x

r   j 

je

y

xi  ji  

    

ii

subject to
x

xi  a      

i  i

    

i  i  h  ni   o  oi

    

i  i  h  hi

    

aai

xi  h   

x

xi  h o a      

aai

xi  h     

table    nlp  this non linear program expresses the constraints for finding a sequenceform joint policy that is an optimal solution to a dec pomdp 

regarding the value of a joint history  it is defined by 
r   j    r   j    j 

    

where
r   j   

t x
x

jk   s r s  ak  j   

    

k   ss

thus  v   p   the value of a sequence form joint policy p  is the weighted sum of
the value of the histories in its support 
x
v   p   
p j r   j 
    
jh

with p j   

q

ii

pi  ji   

    non linear program for solving dec pomdps 
by using the sequence form formulation of a dec pomdp  we are able to express joint
policies as sets of linear constraints and to assess the value of every policy  solving a decpomdp amounts to finding the policy with the maximal value  which can be done with the
non linear program  nlp  of table   where  once again  the xi variables are the weights of
the histories for the agent i 
example an example of the formulation of such an nlp can be found in the appendices 
in section e    it is given for the decentralized tiger problem with   agents and an horizon
of   
the constraints of the program form a convex set  but the objective function is not
concave  as explained in appendix a   in the general case  solving non linear program is
   

fiaras   dutech

very difficult and there are no generalized method that guarantee finding a global maximum
point  however  this particular nlp is in fact a multilinear mathematical program  see
drenick        and this kind of programs are still very difficult to solve  when only two
agents are considered  one speaks of bilinear programs  that can be solved more easily
 petrik   zilberstein        horst   tuy        
an evident  but inefficient  method to find a global maximum point is to evaluate all the
extreme points of the set of feasible solutions of the program since it is known that every
global as well as local maximum point of a non concave function lies at an extreme point of
such a set  fletcher         this is an inefficient method because there is no test that tells
if an extreme point is a local maximum point or a global maximum point  hence  unless
all extreme points are evaluated  we cannot be sure of having obtained a global maximum
point  the set of feasible solutions to the nlp is x  the set of t  step joint policies  the
set of extreme points of this set is x  the set of pure t  step joint policies  whose number is
doubly exponential in t and exponential in n  so enumerating the extreme points for this
nlp is untractable 
our approach  developed in the next sections  is to linearize the objective function of
this nlp in order to deal only with linear programs  we will describe two ways for doing
this  one is based on combinatorial consideration  section    and the other is based on
game theory concepts  section     in both cases  this shall mean adding more variables and
constraints to the nlp  but upon doing so  we shall derive mixed integer linear programs
for which it is possible to find a global maximum point and hence an optimal joint policy
of the dec pomdp 

   from combinatorial considerations to mathematical programming
this section explains how it is possible to use combinatorial properties of dec pomdps
to transform the previous nlp into a mixed integer linear program  as shown  this mathematical program belongs to the family of     mixed integer linear programs  meaning that
some variables of this linear program must take integer values in the set        
    linearization of the objective function
borrowing ideas from the field of quadratic assignment problems  papadimitriou   steiglitz         we turn the non linear objective function of the previous nlp into a linear
objective function and linear constraints involving new variables z that must take integer
values  the variable z j  represents the product of the xi  ji   variables 
thus  the objective function that was 
x
y
maximize
r   j 
xi  ji  
    
je

ii

can now be rewritten as
maximize

x

r   j z j 

je

where j   hj    j         jn i 
   

    

fimathematical programming for dec pomdps

we must ensure that there is a two way mapping between the value of the new variables
z and the x variables for any solution of the mathematical program  that is 
y
z   j   
xi  ji   
    
ii

for this  we will restrict ourself to pure policies where the x variables can only be   or   
in that case  the previous constraint      becomes 
z   j       xi  ji       

i  i

    

there  we take advantage on the fact that the support of a pure policy for an agent i is
composed of  oi  t   terminal histories to express these new constraints  on the one hand 
to guarantee that z j  is equal to   only when enough x variables are also equal to    we
write 
n
x

xi  ji    nz j     

j  e 

    

i  

on the other hand  to limit the number of z j  variables that can take a value of    we will
enumerate the number of joint terminal histories to end up with 
y
x
 oi  t    
    
z j   
ii

je

the constraints      would weight heavily on any mathematical program as there would
be one constraint for each terminal joint history  a number which is exponential in n and
t   our idea to reduce this number of constraints is not to reason about joint histories
but with individual histories  an history h of agent i is part of the support of the solution
ofpthe problem  i e   xq
i  h       if and only if the number of joint histories
q it belongs to
  j  ei z hh  j  i   is ki  i   ok  t     then  we suggest to replace the  ei   constraints
    
n
x

xi  ji    nz j     

j  e 

    

i  

by the

p

 ei   constraints
x



 ok  t  
xi  h 
 oi  t  
y
  xi  h 
 ok  t    

z hh  j i   

j  ei

q

ki

i  i  h  ei  

    

ki  i 

    fewer integer variables
the linearization of the objective function rely on the fact that we are dealing with pure
policies  meaning that every x and z variable is supposed to value either   or    as solving
linear programs with integer variables is usually based on the branch and bound technique
   

fiaras   dutech

variables 
xi  h   i  i  h  hi
z j   j  e
x

maximize

r   j z j 

    

je

subject to 
x

xi  a      

i  i

    

i  i  h  ni   o  oi

    

aai

xi  h   

x

xi  h o a      

aai

x

y

z hh  j  i    xi  h 

t
j  hi

 ok  t    

i  i  h  ei

    

ki  i 

x

z j   

je

y

 oi  t  

    

ii

xi  h     

i  i  h  ni

xi  h          

    

i  i  h  ei

z j          

    

j  e

    

table    milp  this     mixed integer linear program finds a sequence form joint policy
that is an optimal solution to a dec pomdp 

 fletcher         for efficiency reasons  it is important to reduce the number of integer
variables in our mathematical programs 
as done in section      we can relax most x variables and allow them to take non negative
values provided that the x values for terminal histories are constrained to integer values 
furthermore  as proved by the following lemma  these constraints on x also guarantee that
z variables only take their value in        
we eventually end up with the following linear program with real and integer variables 
thus called an     mixed integer linear program  milp   the milp is shown in table   
example in section e   of the appendices  an example if such milp is given for the
problem of the decentralized tiger for   agents and an horizon of   
lemma      in every solution  x   z    to the milp of table    for each j  e  z   j  is
either   or   
proof  let  x   z    be a solution of milp  let 
s z     j  e z   j      
si  xi      h 


ei  xi  h 

     




    
i  i

si  z  j      j  e ji   j   z  j       
   

    
i  i 

j 

 ei

    

fimathematical programming for dec pomdps

q
q
t     by showing that  s z   
t    
now  due to      and        s z  
i 
ii  oi  
q iit o
 
we shall establish that  s z     ii  oi  
  then due to the upper bound of   on each z

variable  the implication will be that z  j  is   or   for each terminal joint history j thus
proving the statement of the lemma 
note that by lemma        for each agent i  xi is a pure policy  therefore  we have that
 si  x      oi  t     this means that in the set of constraints       an i reduced terminal
joint history j   ei will appear on the right hand side not more than  oi  t   times when
in the left hand side  we have xi  h       thus  j   ei  
 si  z  j       oi  t    

    

 h  is either   or   since
now  we know that for each agent i and for each history h  hi   xi q
xi is a pure policy  so  given an i reduced terminal joint history j    ki  i  xk  jk   is either
  or    secondly  due to       the following implication clearly holds for each terminal joint
history j 
z   j       xi  ji       

i  i 

    

therefore  we obtain
 si  z  j       oi  t  

    
y

t  

   oi  

xk  jk   

    

ki  i 

as a consequence 
x

x

 si  z  j     

j  ei

y

 oi  t  

j  ei

xk  jk  

    

xk  jk  

    

xk  h  

    

ki  i 

   oi  t  

x

y

j  ei ki  i 

   oi  t  

y

x

ki  i  h ek

   oi  t  

y

 ok  t  

    

ki  i 

 

y

t  

 oj  

 

    

ji

since

s

j  ei

si  z  j      s z   there holds that

p

y

 s z   

j  ei

 si  z  j        s z    hence 

 oj  t    

    

ji

thus the statement of the lemma is proved 
   



fiaras   dutech

    summary
by using combinatorial considerations  it is possible to design a     milp for solving a given
dec pomdp  as proved by theorem      the solution of this milp defines an optimal joint
policy
nevertheless  this milp is quite large  with o kt   constraints
pfor the dec pomdp 
q
nt
and i  hi     i  ei     o k   variables  o kt   of these variables must take integer values 
the next section details another method for the linearization of nlp which leads to a
smaller mathematical program for the   agent case 
theorem      given a solution  x   z    to milp  x   hx    x         xn i is a pure
t  period optimal joint policy in sequence form 
proof  due to the policy constraints and the domain constraints of each agent  each xi

is a pure sequence form policy
q of agent i  due to the constraints            each z values  
if and only if the product ii xi  ji   values    then  by maximizing the objective function
we are effectively maximizing the value of the sequence form policy hx    x         xn i  thus 

hx    x         xn i is an optimal joint policy of the original dec pomdp 

   from game theoretical considerations to mathematical
programming
this section borrows concepts like nash equilibrium and regret from game theory in
order to design yet another     mixed integer linear program for solving dec pomdps 
in fact  two milps are designed  one that can only be applied for   agents and the other
one for any number of agents  the main objective of this part is to derive a smaller
mathematical program for the   agent case  indeed  milp   agents  see table    has
slightly less variables and constraints than milp  see table    and thus might prove easier
to solve  on the other hand  when more than   agents are considered  the new derivation
leads to a milp which is only given for completeness as it is bigger than milp 
links between the fields of multiagent systems and game theory are numerous in the
literature  see  for example  sandholm        parsons   wooldridge         we will elaborate on the fact that the optimal policy of a dec pomdp is a nash equilibrium  it is in
fact the nash equilibrium with the highest utility as the agents all share the same reward 
for the   agent case  the derivation we make in order to build the milp is similar to
the first derivation of sandholm  gilpin  and conitzer         we give more details of this
derivation and adapt it to dec pomdp by adding an objective function to it  for more
than   agents  our derivation can still be use to find nash equilibriae with pure strategies 
for the rest of this article  we will make no distinction between a policy  a sequence form
policy or a strategy of an agent as  in our context  these concepts are equivalent  borrowing
from game theory  a joint policy will be denoted p or q  an individual policy pi or qi and a
i reduced policy pi or qi  
    nash equilibrium
a nash equilibrium is a joint policy in which each policy is a best response to the reduced
joint policy formed by the other policies of the joint policy  in the context of a sequence form
   

fimathematical programming for dec pomdps

dec pomdp  a policy pi  xi of agent i is said to be a best response to an i reduced
joint policy qi  xi if there holds that
v   hpi   qi i   v   hpi   qi i     

pi  xi  

    

a joint policy p  x is a nash equilibrium if there holds that
v   p   v   hpi   pi i     
that is 
x x

hei j  ei

r   hh  j  i 

y

ki  i 

i  i  pi  xi  


 
   
pk  jk   pi  h   pi  h 

i  i  pi  xi  

    

    

the derivation of the necessary conditions for a nash equilibrium consists of deriving
the necessary conditions for a policy to be a best response to a reduced joint policy  the
following program finds a policy for an agent i that is a best response to an i reduced joint
policy qi  xi   constraints           ensure that the policy defines a valid joint policy
 see section      and the objective function is a traduction of the concept of best response 
variables  xi  h   i  i  h  hi



x x
y
maximize
r   hh  j  i 
qk  jk   xi  h 


hei

j ei

    

ki  i 

subject to 

x

xi  a     

    

aai

xi  h   

x

xi  h o a      

h  ni   o  oi

    

h  hi  

    

aai

xi  h     

this linear program  lp  must still be refined so that its solution is not only a best
response for agent i but a global best response  i e   the policy of each agent is a best
response to all the other agents  this will mean introducing new variables  a set of variable
for each agent   the main point will be to adapt the objective function as the current
objective function  when applied to find global best response  would lead to a non linear
objective function where product of weights of policies would appear  to do this  we will
make use of the dual of the program  lp  
the linear program  lp  has one variable xi  h  for each history h  hi representing
the weight of h  it has one constraint per information set of agent i  in other words  each
constraint of the linear program  lp  is uniquely labeled by an information set  for instance 
the constraint      is labeled by the null information set   and for each nonterminal
history h and for each observation o  the corresponding constraint in      is labeled by the
information set h o  thus   lp  has ni variables and mi constraints 
as described in the appendix  see appendix b   the dual of  lp  is expressed as 
   

fiaras   dutech

variables  yi       i
minimize

yi   

    

subject to 
yi   h   

x

yi  h o     

h  ni

    

qk  jk      

h  ei

    

ooi

yi   h   

x

r   hh  j  i 

j  ei

y

ki  i 

yi           

  i

    

where  h  denotes the information set to which h belongs  the dual has one free variable
yi    for every information set of agent i  this is why the function  h   defined in section      appears as a mapping from histories to information sets    the dual program has
one constraint per history of the agent  thus  the dual has mi variables and ni constraints 
note that the objective of the dual is to minimize only yi    because in the primal  lp  
the right hand side of all the constraints  except the very first one  is a   
the theorem of duality  see the appendix b   applied to the primal  lp            and the
transformed dual            says that their solutions have the same value  mathematically 
that means that 



x x
y
r   hh  j  i 
qk  jk   xi  h    yi    
    


hei

j ei

ki  i 

thus  the value of the joint policy hxi   qi i can be expressed either as
x x
y
 
v   hxi   qi i   
r   hh  j  i 
qk  jk   xi  h 
hei

j  ei

    

ki  i 

or as

v   hxi   qi i    yi    

    

due to the constraints      and      of the primal lp  there holds that
x

 
x    x x 
xi  a   
yi  h o   xi  h   
xi  h o a 
yi      yi   
aai

hni ooi

    

aai

as constraint      guarantees that the first term in the braces is   and constraints     
guarantee that each of the remaining terms inside the braces is    the right hand side of
     can be rewritten as
x
x
x

 

 
p
xi  a  yi   
 ooi yi  a o 
 
xi  h  yi   h   
yi  h o 
aai

ooi

hni  ai

 

x

xi  h yi   h  

hei

 

p


hni xi  h 



yi   h  

x

ooi

  x 
yi  h o   
xi  h yi   h  
hei

   as h o is an information set  yi  h o  is a shortcut in writing for yi   h o   

   

    

fimathematical programming for dec pomdps

so  combining equations      and       we get
x
x
 
 
xi  h 
yi   h   
yi  h o 
ooi

hni

 

x

xi  h 



hei

yi   h  

r   hh  j  i 

x



j  ei

y

ki  i 

 
qk  jk      

    

it is time to introduce supplementary variables w for each information set  these variables  usually called slack variables  are defined as 
x
yi   h   
yi  h o    wi  h   h  ni
    
ooi

yi   h   

x



r   hh  j i 

j  ei

y

qk  jk     wi  h  

h  ei  

    

ki  i 

as shown is section c of the appendix  these slack variables correspond to the concept of
regret as defined in game theory  the regret of an history expresses the loss in accumulated
reward the agent incurs when he acts according to this history rather than according to a
history which would belong to the optimal joint policy 
thanks to the slack variables  we can furthermore rewrite      as simply
x
x
xi  h wi  h   
xi  h wi  h     
    
hni

hei

x

xi  h wi  h      

    

hhi

now       is a sum of ni products  ni being the size of hi   each product in this sum is
necessarily   because both xi  h  and wi  h  are constrained to be nonnegative in the primal
and the dual respectively  this property is strongly linked to the complementary slackness
optimality criterion in linear programs  see  for example  vanderbei         hence       is
equivalent to
xi  h wi  h      

h  hi  

    

back to the framework of dec pomdps  these constraints are written 
pi  h i  hh  qi i      

h  hi  

    

to sum up  solving the following mathematical program would give an optimal joint
policy for the dec pomdp  but constraints      are non linear and thus prevent us from
solving this program directly  the linearization of these constraints  called complementarity
constraints  is the subject of the next section 
variables 
xi  h   wi  h  i  i and h  hi
yi    i  i and   i
maximize

y    

   

    

fiaras   dutech

subject to 
x

xi  a     

    

aai

xi  h   

x

i  i  h  ni   o  oi

    

yi  h o    wi  h  

i  i  h  ni

    

xk  jk     wi  h  

i  i  h  ei

    

xi  h o a      

aai

yi   h   

x

ooi

yi   h   

x

j  ei

r   hh  j  i 

y

ki  i 

xi  h wi  h      

i  i  h  hi

    

xi  h     

i  i  h  hi

    

wi  h     

i  i  h  hi

    

yi           

i  i    i

    

    dealing with complementarity constraints
this section explains how the non linear constraints xi  h wi  h      in the previous mathematical program can be turned into sets of linear constraints and thus lead to a mixed
integer linear programming formulation of the solution of a dec pomdp 
consider a complementarity constraint ab     in variables a and b  assume that the
lower bound on the values of a and b is    let the upper bounds on the values of a and b be
respectively ua and ub   now let c be a     variable  then  the complementarity constraint
ab     can be separated into the following equivalent pair of linear constraints 
a  ua c

    

b  ub     c  

    

in other words  if this pair of constraints is satisfied  then it is surely the case that ab     
this is easily verified  c can either be   or    if c      then a will be set to   because a is
constrained to be no more than ua c  and not less than     if c      then b will be set to  
since b is constrained to be not more than ub     c   and not less than     in either case 
ab     
now consider each complementarity constraint xi  h wi  h      from the non linear program           above  we wish to separate each constraint into a pair of linear constraints 
we recall that xi  h  represents the weight of h and wi  h  represents the regret of h  the
first requirement to convert this constraint to a pair of linear constraints is that the lower
bound on the values of the two terms be    this is indeed the case since xi  h  and wi  h 
are both constrained to be non negative in the nlp  next  we require upper bounds on the
weights of histories and regrets of histories  we have shown in lemma     that the upper
bound on the value of xi  h  for each h is    for the upper bounds on the regrets of histories 
we require some calculus 
   

fimathematical programming for dec pomdps

in any policy pi of agent i there holds that
x
pi  h     oi  t    

    

hei

therefore  in every i reduced joint policy hq    q         qn i  xi   there holds
y
x y
 ok  t  
qk  jk    
j  ei ki  i 

    

ki  i 

since the regret of a terminal history h of agent i given hq    q         qn i is defined as
x y

 
    
i  h  q    max
qk  jk   r   hh   j  i   r   hh  j  i   
h  h 

j  ei ki  i 

we can conclude that an upper bound ui  h  on the regret of a terminal history h  ei
of agent i is 


y
t  
 

ui  h   
 ok  
max max
r   hh   j i   min r   hh  j i        

ki  i 

h  h  j ei

j ei

now let us consider the upper bounds on the regrets of non terminal histories  let  be
an information set of length t of agent i  let ei     ei denote the set of terminal histories
of agent i such the first  t elements of each history in the set are identical to   let h be a
history of length t  t of agent i  let ei  h   ei denote the set of terminal histories such
that the first  t     elements of each history in the set are identical to h  since in any policy
pi of agent i  there holds
x
pi  h     oi  t t
    
h ei  h 

we can conclude that an upper bound ui  h  on the regret of a nonterminal history
h  ni of length t agent i is


 

max
max
r  
hh
 
j
i 

min
min
r  
hg 
j
i 
    
ui  h    li


h ei   h   j ei

gei  h  j ei

where
li    oi  t t

y

 ok  t    

    

ki  i 

notice that if t   t  that is  h is terminal       reduces to      
so  the complementarity constraint xi  h wi  h      can be separated into a pair of linear
constraints by using a     variable bi  h  as follows 
xi  h      bi  h 
wi  ui  h bi  h 
bi  h         
   

     
     
     

fiaras   dutech

variables 
xi  h   wi  h  and bi  h  for i         and h  hi
yi    for i         and   i
maximize

y    

     

subject to 
x

xi  a     

     

aai

xi  h   

x

i         h  ni   o  oi

     

i         h  ni

     

r   hh  h i x   h     w   h  

h  e 

     

r   hh   hi x   h     w   h  

h  e 

     

xi  h o a      

aai

yi   h   

x

yi  h o    wi  h  

ooi

y    h   

x

h e 

y    h   

x

h e 

xi  h      bi  h  
wi  h   ui  h bi  h  

i         h  hi
i         h  hi

     
     

xi  h     

i         h  hi

     

wi  h     

i         h  hi

     

bi  h          

i         h  hi

yi           

     

i           i      

table    milp   agents  this     mixed integer linear program  derived from game
theoretic considerations  finds optimal stochastic joint policies for dec pomdps
with   agents 

    program for   agents
when we combine the policy constraints  section       the constraints we have just seen
for a policy to be a best response  sections           and a maximization of the value of the
joint policy  we can derive a     mixed integer linear program the solution of which is an
optimal joint policy for a dec pomdp for   agents  table   details this program that we
will call milp   agents 
example the formulation of the decentralized tiger problem for   agents and for an
horizon of   can be found in the appendices  in section e  
the variables of the program are the vectors xi   wi   bi and yi for each agent i  note that
for each agent i  i and for each history h of agent i  ui  h  denotes the upper bound on
the regret of history h 
   

fimathematical programming for dec pomdps

a solution  x   y    w   b   to milp   agents consists of the following quantities   i 
an optimal joint policy x   hx    x  i which may be stochastic   ii  for each agent i     
   for each history h  hi   wi  h   the regret of h given the policy xi of the other agent 
 iii  for each agent i         for each information set   i   yi     the value of  given
the policy xi of the other agent   iv  for each agent i         the vector bi simply tells us
which histories are not in the support of xi   each history h of agent i such that bi  h      is
not in the support of xi   note that we can replace y     by y     in the objective function
without affecting the program  we have the following result 
theorem      given a solution  x   w   y    b   to milp   agents  x   hx    x  i is an
optimal joint policy in sequence form 
proof  due to the policy constraints of each agent  each xi is a sequence form policy of
agent i  due to the constraints              yi contains the values of the information sets
of agent i given xi   due to the complementarity constraints              each xi is a best
response to xi   thus hx    x  i is a nash equilibrium  finally  by maximizing the value
of the null information set of agent    we are effectively maximizing the value of hx    x  i 
thus hx    x  i is an optimal joint policy 

in comparison with the milp presented before in table    milp   agents should
constitutes a particularly effective program in term of computation time for finding a  agent optimal t  period joint policy because it is a much smaller program  while the number
of variables required by milp is exponential in t and in n  the number of variables required
by milp   agents is exponential only in t   this represents a major reduction in size
that should lead to an improvement in term of computation time 
    program for   or more agents
when the number of agents is more than    the constraint      of the non linear program
          is no longer a complementarity constraint
between   variables that could be linq
earized as before  in particular  the term ki  i  xk  jk   of the constraint      involves as
many variables as there are different agents  to linearize this term  we will restrict ourselves once again to pure joint policies and exploit some combinatorial facts on the number
of histories involved  this leads to the     mixed linear program called milp n agents
and depicted in table   
the variables of the program milp n agents are the vectors xi   wi   bi and yi for each
agent i and the vector z  we have the following result 
theorem      given a solution  x   w   y    b   z    to milp n agents  x   hx    x   
     xn i is a pure t  period optimal joint policy in sequence form 
proof  due to the policy constraints and the domain constraints of each agent  each
is a pure sequence form policy of agent i  due to the constraints              each yi
contains the values of the information sets of agent i given xi   due to the complementarity
constraints              each xi is a best response to xi   thus x is a nash equilibrium 
finally  by maximizing the value of the null information set of agent    we are effectively
maximizing the value of x   thus x is an optimal joint policy 

xi

   

fiaras   dutech

variables 
xi  h   wi  h  and bi  h  i  i and h  hi
yi    i  i    i
z j  j  e
maximize

y    

     

xi  a     

     

xi  h o a      

i  i  h  ni   o  oi      

subject to 
x

aai

xi  h   

x

aai

yi   h   

x

yi  h o    wi  h  

i  i  h  ni

     

ooi

yi   h   

x
 
r   hh  ji i z j    wi  h   i  i  h  ei
t
 
 oi  
je
x
y
z hh  j  i    xi  h 
 ok  t    
j  ei

     

ki  i 

i  i  h  ei
x
y
z j   
 oi  t  
je

     
     

ii

xi  h      bi  h  

i  i  h  hi      

wi  h   ui  h bi  h  

i  i  h  hi      

xi  h     

i  i  h  ni

xi  h         
wi  h     

i  i  h  ei

i  i  h  hi

bi  h          

h  hi

yi           
z j          

j  e

     
     
     
     

i  i    i     
     

table    milp n agents  this     mixed integer linear program  derived from game
theoretic considerations  finds pure optimal joint policies for dec pomdps with
  or more agents 

   

fimathematical programming for dec pomdps

compared to the milp of table    milp n agents has roughly the same size but
with more real valued variables and more     variables  to be precise  p
milp has a    
variable for every terminal history of every agent  that is approximatively ii  ai  t  oi  t  
integer variables  while milp n agents has two     variables
for every terminal as well
p
as nonterminal history of each agent  approximatively   ii   ai   oi   t integer variables  

    summary

the formulation of the solution of a dec pomdp and the application of the duality
theorem for linear programs allow us to formulate the solution of a dec pomdp as the
solution of a new kind of     milp  for   agents  this milp has only o kt   variables
and constraints and is thus smaller than milp of the previous section  still  all these
milps are quite large and the next section investigates heuristic ways to speed up their
resolution 

   heuristics for speeding up the mathematical programs
this section focusses on ways to speed up the resolution of the various milps presented so
far  two ideas are exploited  first  we show how to prune the set of sequence form policies
by removing histories that will provably not be part of the optimal joint policy  these
histories are called locally extraneous  then  we give some lower and uppers bounds to
the objective function of the milps  these bounds can sometimes be used in the branch and
bound method often used by milp solvers to finalize the values of the integer variables 
    locally extraneous histories
a locally extraneous history is a history that is not required to find an optimal joint policy
when the initial state of the dec pomdp is  because it could be replaced by a co history
without affecting the value of the joint policy  a co history of a history h of an agent is
defined to be a history of that agent that is identical to h in all aspects except for its last
action  if ai    b  c   the only co history of c u b v b is the history c u b v c  the set of
co histories of a history h shall be denoted by c h  
formally  a history h  hit of length t of agent i is said to be locally extraneous if 
t of i reduced joint histories of length t 
for every probability distribution  over the set hi
there exists a history h  c h  such that
x

 
 j    r   hh   j  i   r   hh  j  i 
  
     
j  hti

where  j    denotes the probability of j  in  
an alternative definition is as follows  a history h  hit of length t of agent i is said to be
locally extraneous if there exists a probability distribution  over the set of co histories
of h such that for each i reduced joint history j  of length t  there holds
x
 h  r   hh   j  i   r   hh  j  i 
     
h c h 

   

fiaras   dutech

where  h   denotes the probability of the co history h in  
the following theorem justifies our incremental pruning of locally extraneous histories
so that the search for optimal joint policies is faster because it is performed on a smaller
set of possible support histories 
theorem      for every optimal t  period joint policy p such that for some agent i  i
and for a terminal history h of agent i that is locally extraneous at   pi  h       there exists
another t  period joint policy p that is optimal at  and that is identical to p in all respects
except that pi  h      
proof  let p be a t  period joint policy that is optimal at   assume that for some
agent i  i and for a terminal history h of agent i that is locally extraneous at   pi  h      
by        there exists at least one co history h of h such that 
x

 
pi  j    r   hh   j  i   r   hh  j  i 
   
     
j  ht
i

let q be a t  period policy of agent i that is identical to pi in all respects except that q h  
  pi  h    pi  h   and q h       we shall show that q is also optimal at   there holds 
x

j  ht
i

v   hq  pi i   v   hpi   pi i   

 
pi  j    r   hh   j  i q h    r   hh   j  i pi  h    r   hh  j  i pi  h 
 
x

j  ht
i


 
pi  j    r   hh   j  i  q h    pi  h     r   hh  j  i pi  h 
 
x

j  ht
i


 
pi  j    r   hh   j  i pi  h   r   hh  j  i pi  h 

since q h     pi  h    pi  h    therefore 
x

j  ht
i

v   hq  pi i   v   hpi   pi i   

 
pi  j    r   hh   j  i   r   hh  j  i 
    due to        

hence  p   hq  pi i is also an optimal t  period joint policy at  



one could also wonder if the order with which extraneous histories are pruned is important
or not  to answer this question  the following theorem shows that if many co histories are
extraneous  they can be pruned in any order as 
 either they all have the same value  so any one of them can be pruned  
 or pruning one of them does not change the fact that the others are still extraneous 
theorem      if two co histories h  and h  are both locally extraneous  either their values
t are equal or h is also locally extraneous
r   hh    j  i  and r   hh    j  i for all j   hi
 
relatively to c h     h    
   

fimathematical programming for dec pomdps

proof  let c   denotes the union c h     c h     we have immediately that c h     
c      h    and c h      c      h     h   resp  h    being locally extraneous means that there
exists a probability distribution   on c h     resp    on c h     such that  for all j  of
t  
hi
x
   h  r   hh   j  i   r   hh    j  i 
     
h c     h   

x

   h  r   hh   j  i   r   hh    j  i 

     

h c     h   

     
eq        can be expanded in 
x

   h   r   hh    j  i   

h c     h

   h  r   hh   j  i   r   hh    j  i  
   h   

using       in       gives
x
   h   
   h  r   hh   j  i   
h c     h   

leading to
x

     

x

   h  r   hh   j  i   r   hh    j  i 

h c     h   h   

     

    h      h        h   r   hh   j  i          h      h    r   hh    j  i       

h c     h   h   

so  two cases are possible 
    h         h         in that case  as r   hh    j  i   r   hh    j  i  and r   hh    j  i  
t  
r   hh    j  i   we have that r   hh    j  i    r   hh    j  i  for all j  of hi
    h      h         in that case we have 
x

h c     h   h   

   h      h        h  
r   hh   j  i   r   hh    j  i 
      h      h   

     

meaning that even without using h    h  is still locally extraneous because
   h      h      h  
is a probability distribution over c      h    h   
    h      h   
x

h c     h

   h   

   h          h              h    
   h      h        h  
 
      h      h   
      h      h   
      h      h   
      h      h   
    
 

     
     
     


   

fiaras   dutech

in order to prune locally extraneous histories  one must be able to identify these histories 
there are indeed two complementary ways for doing this 
the first method relies on the definition of the value of a history  see section       that
is
r   hh  j  i       hh  j  i r   hh  j  i  

     

therefore  if
   hh  j  i      

t
j   hi

     

is true for a history h  then that means that every joint history of length t occurring from
 of which the given history is a part of has an a priori probability of    thus  h is clearly
extraneous  besides  every co history of h will also be locally extraneous as they share the
same probabilities 
a second test is needed because some locally extraneous histories do not verify       
once again  we turn to linear programing and in particular to the following linear program
t
variables  y j   j  hi
minimize



     

subject to 
x

j  hti


 
y j    r   hh   j  i   r   hh  j  i 
  
x

h  c h 

y j       

     
     

j  hti

because of the following lemma 

y j       

t
j   hi

     

lemma      if  it exists a solution     y    to the linear program             where     
then h is locally extraneous 
proof   let     y    be a solution to the lp              y  is a probability distribution
t due to constraints              if      since we are minimizing   due to
over hi
t    and for every co history h of h
constraints        we have that for every y   hi
x

 
y j    r   hh   j  i   r   hh  j  i 
   
     
j  hti

therefore  by definition  h is locally extraneous 



the following procedure identifies all locally extraneous terminal histories of all the agents
and proceed to their iterative pruning  this is mainly motivated by theorems     and    
for effectively removing extraneous histories  the procedure is similar to the procedure of
iterated elimination of dominated strategies in a game  osborne   rubinstein         the
concept is also quite similar to the process of policy elimination in the backward step of the
dynamic programming for partially observable stochastic games  hansen et al         
   

fimathematical programming for dec pomdps

 step    for each agent i  i  set hit to ei   let h t denote the set ii hit   for
each joint history j  h t   compute and store the value r   j  of j and the joint
observation sequence probability    j  of j 
 step    for each agent i  i  for each history h  hit   if for each i reduced joint
t      hh  j  i       remove h from h t  
history j   hi
i
 step    for each agent i  i  for each history h  hit do as follows  if c h   hit
is non empty  check whether h is locally extraneous or not by setting up and solving
t by the set h t and the set c h 
lp              when setting the lp  replace hi
i
by the set c h   hit   if upon solving the lp  h is found to be locally extraneous at
  remove h from hit  
 step    if in step   a history  of any agent  is found to be locally extraneous  go to
step    otherwise  terminate the procedure 
the procedure builds the set hit for each agent i  this set contains every terminal
history of agent i that is required for finding an optimal joint policy at   that is every
terminal history that is not locally extraneous at   for each agent i  every history that
is in hit but not in hit is locally extraneous  the reason for reiterating step   is that if
a history h of some agent i is found to be locally extraneous and consequently removed
from hit   it is possible that a history of some other agent that was previously not locally
extraneous now becomes so  due to the removal of h from hit   hence  in order to verify if
this is the case for any history or not  we reiterate step   
besides  step   of the procedure below also prunes histories that are impossible given
the model of the dec pomdp because their observation sequence can not be observed 
a last pruning step can be taken in order to remove non terminal histories that can only
lead to extraneous terminal histories  this last step is recursive  starting from histories of
horizon t     we remove histories hi that have no non extraneous terminal histories  that
is  histories hi such that all h o a are extraneous for a  ai and o  oi  
complexity the algorithm for pruning locally extraneous histories has an exponential
complexity  each joint history must be examined to compute its value and its occurence
probability  then  in the worst case  a linear program can be run for every local history in
order to check it is extraneous or not  experimentations are needed to see if the prunning
is really interesting 
    cutting planes
previous heuristics were aimed at reducing the search space of the linear programs  which
incidentally has a good impact on the time needed to solve these programs  another option
which directly aims at reducing the computation time is to use cutting planes  cornuejols 
       a cut  dantzig        is a special constraint that identifies a portion of the set
of feasible solutions in which the optimal solution provably does not lie  cuts are used in
conjunction with various branch and bounds mechanism to reduce the number of possibles
combination of integer variables that are examined by a solver 
we will present two kinds of cuts 
   

fiaras   dutech

variables  y j   j  h
maximize

x

r   j y j 

     

je

subject to 
x

y a     

     

aa

y j   

x

y j o a      

j  n   o  o

     

j  h

     

aa

y j     

table    pomdp  this linear program finds an optimal policy for a pomdp 
      upper bound for the objective function
the first cut we propose is the upper bound pomdp cut  the value of an optimal
t  period joint policy at  for a given dec pomdp is bounded from above by the value
vp of an optimal t  period policy at  for the pomdp derived from the dec pomdp 
this derived pomdp is the dec pomdp but assuming a centralized controller  i e  with
only one agent using joint actions  
a sequence form representation of the pomdp is quite straightforward  calling h the
set tt   ht of joint histories of lengths less than or equal to t and n the set h e of nonterminal joint histories  a policy for pomdp with horizon t in sequence form is a function
q from h to        such that 
x
q a     
     
aa

q j   

x

q j o a      

j  n   o  o

     

aa

the value vp    q  of a sequence form policy q is then given by 
x
vp    q   
r   j q j 

     

je

thereby  the solution y  of the linear program of table   is p
an optimal policy for the
pomdp of horizon t and the optimal value of the pomdp is je r   j y   j   so  the
value v   p   of the optimal joint policy p   hp    p         pn i of the dec pomdp is
bounded by above by the value vp    q    of the associated pomdp 
complexity the complexity of finding an upper bound is linked to the complexity of
solving a pomdp which  as showed by papadimitriou and tsitsiklis         can be pspace
 i e  require a memory that is polynomial in the size of the problem  leading to a possible
exponential complexity in time   once again  only experimentation can help us decide in
which cases the upper bound cut is efficient 
   

fimathematical programming for dec pomdps

      lower bound for the objective function
in the case of dec pomdps with non negative reward  it is trivial to show that the value
of a t  period optimal policy is bounded from below by the value of the t   horizon optimal
value  so  in the general case  we have to take into account the lowest reward possible to
compute this lower bound and we can say that 
x
r   j z j   v t        min min r s  a 
     
aa ss

je

where v t   is the value of the optimal policy with horizon t     the reasoning leads to
an iterated computation of dec pomdps of longer and longer horizon  reminiscent of the
maa  algorithm  szer et al          experiments will tell if it is worthwhile to solve bigger
and bigger dec pomdps to take advantage of a lower bound or if it is better to directly
tackle the t horizon problem without using any lower bound 
complexity to compute the lower bound  one is required to solve a dec pomdp whith
an horizon that is one step shorter than the current horizon  the complexity is clearly at
least exponential  in our experiments  the value of a dec pomdp has been used for the
same dec pomdp with a bigger horizon  in such case  the computation time has been
augmented by the best time to solve the smaller dec pomdp 
    summary
pruning locally extraneous histories and using the bounds of the objective function can be
of practical use for software solving the milps presented in this paper  pruning histories
means that the space of policies used by the milp is reduced and  because the formulation
of the milp depends on combinatorial characteristics of the dec pomdp  these milp
must be altered as show in appendix d 
validity as far as cuts are concerned  they do not alter the solution found by the milps 
so a solution to these milps is still an optimal solution to the dec pomdp  when extraneous histories are pruned  at least one valid policy is left as a solution because  in step
  of the algorithm  an history is pruned only if it has other co histories left  besides  this
reduced set of histories can still be used to build an optimal policy because of theroem     
as a consequence  the milp build on this reduced set of histories admit a solution and this
solution is one optimal joint policy 
in the next section  experimental results will allow us to understand in which cases the
heuristics introduced can be useful 

   experiments
the mathematical programs and the heuristics designed in this paper are tested on four
classical problems found in the literature  for these problems  involving two agents  we have
mainly compared the computation time required to solve a dec pomdp using mixed
integer linear programming methods to computation time reported for methods found
in the literature  then we have tested our programs on three agent problems randomly
designed 
   

fiaras   dutech

problem
mabc
ma tiger
fire fighting
grid meeting
random pbs

 ai  
 
 
 
 
 

 oi  
 
 
 
 
 

 s 
 
 
  
  
  

n
 
 
 
 
 

table    complexity of the various problems used as test beds 

milp and milp   are solved using the ilog cplex    solver  a commercial set of
java packages  that relies on a combination of the simplex and branch and bounds
methods  fletcher         the software is run on an intel p  at     ghz with  gb of
ram using default configuration parameters  for the mathematical programs  different
combination of heuristics have been evaluated  pruning of locally extraneous histories  using
a lower bound cut and using an upper bound cut  respectively denoted loc  low and
up in the result tables to come 
the non linear program  nlp  of section     has been evaluated by using various solvers from the neos website  http   www neos mcs anl gov    even thought this
method does not guarantee an optimal solution to the dec pomdp  three solvers have
been used  lancelot  abbreviated as lanc    loqo and snopt 
the result tables also report results found in the literature for the following algorithms 
dp stands for dynamic programming from hansen et al          dp lpc is an improved
version of dynamic programming where policies are compressed in order to fit more of them
in memory and speed up their evaluation as proposed by boularias and chaib draa        
pbdp is an extension of dynamic programming where pruning is guided by the knowledge
of reachable belief states as detailed in the work of szer and charpillet         maa  is
a heuristically guided forward search proposed by szer et al         and a generalized and
improved version of this algorithm called gmaa  developed by oliehoek et al         
the problems selected to evaluate the algorithms are detailed in the coming subsections 
they have been widely used to evaluate dec pomdps algorithms in the literature and
their complexity  in term of space size  is summarized in table   
    multi access broadcast channel problem
several versions of the multi access broadcast channel  mabc  problem can be found in
the literature  we will use the description given by hansen et al         that allows this
problem to be formalized as a dec pomdp 
in the mabc  we are given two nodes  computers  which are required to send messages
to each other over a common channel for a given duration of time  time is imagined to
be split into discrete periods  each node has a buffer with a capacity of one message  a
buffer that is empty in a period is refilled with a certain probability in the next period  in
a period  only one node can send a message  if both nodes send a message in the same
period  a collision of the messages occurs and neither message is transmitted  in case of
a collision  each node is intimated about it through a collision signal  but the collision
   

fimathematical programming for dec pomdps

signaling mechanism is faulty  in case of a collision  with a certain probability  it does not
send a signal to either one or both nodes 
we are interested in pre allocating the channel amongst the two nodes for a given number
of periods  the pre allocation consists of giving the channel to one or both nodes in a period
as a function of the nodes information in that period  a nodes information in a period
consists only of the sequence of collision signals it has received till that period 
in modeling this problem as a dec pomdp  we obtain a   agent    state    actionsper agent    observations per agent dec pomdp whose components are as follows 
 each node is an agent 
 the state of the problem is described by the states of the buffers of the two nodes 
the state of a buffer is either empty or full  hence  the problem has four states 
 empty  empty    empty  full    full  empty  and  full  full  
 each node has two possible actions  use channel and dont use channel 
 in a period  a node may either receive a collision signal or it may not  so each node
has two possible observations  collision and no collision 
the initial state of the problem  is  full  full   the state transition function p  the
joint observation function g and the reward function r have been taken from hansen et al 
        if both agents have full buffers in a period  and both use the channel in that period 
the state of the problem is unchanged in the next period  both agents have full buffers in
the next period  if an agent has a full buffer in a period and only he uses the channel in
that period  then his buffer is refilled with a certain probability in the next period  for
agent    this probability is     and for agent    this probability is      if both agents have
empty buffers in a period  irrespective of the actions they take in that period  their buffers
get refilled with probabilities      for agent    and      for agent    
the observation function g is as follows  if the state in a period is  full  full  and
the joint action taken by the agents in the previous period is  use channel  use channel  
the probability that both receive a collision signal is       the probability that only one of
them receives a collision signal is      and the probability that neither of them receives a
collision signal is       for any other state the problem may be in a period and for any other
joint action the agents may have taken in the previous period  the agents do not receive a
collision signal 
the reward function r is quite simple  if the state in a period is  full  empty  and
the joint action taken is  use channel  dont use channel  or if the state in a period is
 empty  full  and the joint action taken is  dont use channel  use channel   the reward
is    for any other combination of state and joint action  the reward is   
we have evaluated the various algorithms on this problem for three different horizons    
  and    and the respective optimal policies have a value of            and       results are
detailed in table   where  for each horizon and algorithm  the value and the computation
time for the best policy found are given 
the results show that the milp compares favorably to more classical algorithms except
for gmaa  that is always far better for horizon   and  for horizon    roughly within the
   

fiaras   dutech

resolution method
program
solver
heuristics
milp
cplex
milp
cplex
low
milp
cplex
up
milp
cplex
loc
milp
cplex
loc  low
milp
cplex
loc  up
milp  
cplex
nlp
snopt
nlp
lanc 
nlp
loqo
algorithm
family
dp
dyn  prog 
dp lpc
dyn  prog 
pbdp
dyn  prog 
maa 
fw  search
gmaa 
fw  search

horizon  
value
time
    
    
                
                
                
                
                
    
    
    
    
    
    
    
    
value
time
    
 
    
    
    
   s
    
   s
 
 

horizon  
value
time
    
   
    
          
    
          
    
         
    
          
                
    
    
    
    
    
    
    
    
value
time
    
     
    
    
    
 
    
    
    
    

horizon  
value
time
 m
       m
        m
        t
        t
    
         
 m
    
    
    
  
    
    
value
time
 m
 m
    
   
 t
    
    

table    mabc problem  value and computation time  in seconds  for the solution of
the problem as computed by several methods  best results are highlighted  when
appropriate  time shows first the time used to run the heuristics then the global
time  in the format heuristic total time   t means a timeout of       s 
 m indicates that the problem does not fit into memory and   indicates that
the algorithm was not tested on that problem 

same order of magnitude as milp with the more pertinent heuristics  as expected  apart
for the simplest setting  horizon of     nlp based resolution can not find the optimal policy
of the dec pomdp  but the computation time is lower than the other methods  among
milp methods  milp   is better than milp even with the best heuristics for horizon  
and    when the size of the problem increases  heuristics are the only way for milps to
be able to cope with the size of the problem  the table also shows that  for the mabc
problem  pruning extraneous histories using the loc heuristic is always a good method and
further investigation revealed that     of the heuristics proved to be locally extraneous  as
far are cutting bounds are concerned  they dont seem to be very useful at first  for horizon
  and    but are necessary for milp to find a solution for horizon    for this problem 
one must also have in mind that there is only one optimal policy for each horizon 
    multi agent tiger problem
as explained in section      the multi agent tiger problem  ma tiger  has been introduced
in the paper from nair et al          from the general description of the problem  we ob   

fimathematical programming for dec pomdps

joint action
 listen  listen 
 listen  listen 
 listen  listen 
 listen  listen 
 listen  listen 
 listen  listen 
 listen  listen 
 listen  listen 
      

state
left
left
left
left
right
right
right
right
 

joint observation
 noise left  noise left 
 noise left  noise right 
 noise right  noise left 
 noise right  noise right 
 noise left  noise left 
 noise left  noise right 
 noise right  noise left 
 noise right  noise right 
      

probability
      
      
      
      
      
      
      
      
    

table    joint observation function g for the ma tiger problem 
tain a   agent    state    actions per agent    observations per agent dec pomdp whose
elements are as follows 
 each person is an agent  so  we have a   agent dec pomdp 
 the state of the problem is described by the location of the tiger  thus  s consists
of two states left  tiger is behind the left door  and right  tiger is behind the right
door  
 each agents set of actions consists of three actions  open left  open the left door  
open right  open the right door  and listen  listen  
 each agents set of observations consists of two observations  noise left  noise coming
from the left door  and noise right  noise coming from the right door  
the initial state is an equi probability distribution over s  the state transition function p 
joint observation function g and the reward function r are taken from the paper by nair
et al          p is quite simple  if one or both agents opens a door in a period  the state of
the problem in the next period is set back to   if both agents listen in a period  the state
of the process in unchanged in the next period  g  given in table      is also quite simple 
nair et al         describes two reward functions called a and b for this problem  here
we report only results for reward function a  given in table     as the behavior of the
algorithm are similar for both reward functions  the optimal value of this problem for
horizons   and   are respectively      and      
for horizon    dynamic programming or forward search methods are generally better
than mathematical programs  but this is the contrary for horizon   were the computation time of milp with the low heuristic is significatively better than any other  even
gmaa   unlike mabc  the pruning of extraneous histories does not improve methods
based on milp  this is quite understandable as deeper investigations showed that there are
no extraneous histories  using lower cutting bounds proves to be very efficient and can be
seen as a kind of heuristic search for the best policy   not directly in the set of policies  like
   

fiaras   dutech

joint action
 open right  open right 
 open left  open left 
 open right  open left 
 open left  open right 
 listen  listen 
 listen  open right 
 open right  listen 
 listen  open left 
 open left  listen 

left
  
   
    
    
  
 
 
    
    

right
   
  
    
    
  
    
    
 
 

table     reward function a for the ma tiger problem 

gmaa   but in the set of combination of histories  which may explain the good behavior
of milp low 
it must also be noted that for this problem  approximate methods like nlp but also
other algorithms not depicted here like the memory bound dynamic programming of
seuken and zilberstein        are able to find the optimal solution  and  once again 
methods based on a nlp are quite fast and sometimes very accurate 
    fire fighters problem
the problem of the fire fighters  ff  has been introduced as a new benchmark by oliehoek
et al          it models a team of n fire fighters that have to extinguish fires in a row of nh
houses 
the state of each house is given by an integer parameter  called the fire level f   that
takes discrete value between    no fire  and nf  fire of maximum severity   at every time
step  each agent can move to any one house  if two agents are at the same house  they
extinguish any existing fire in that house  if an agent is alone  the fire level is lowered with
a     probability if a neighbor house is also burning or with a   probability otherwise  a
burning house with no fireman present will increase its fire level f by one point with a    
probability if a neighbor house is also burning or with a probability of     otherwise  an
unattended non burning house can catch fire with a probability of     if a neighbor house
is burning  after an action  the agents receive a reward of f for each house that is still
burning  each agent can only observe if there are flames at its location with a probability
that depends on the fire level      if f          if f     and     otherwise  at start 
the agents are outside any of the houses and the fire level of the houses is sampled from a
uniform distribution 
the model has the following characteristics 
 na agents  each with nh actions and nf possible informations 

h  
states as there are nnf h possible states for the burning houses
 there are nnf h   na  n
n
a

h  
different ways to distribute the na fire fighters in the houses  for
and na  n
na
example    agents with   houses and   levels of fire lead to           states  but  it
   

fimathematical programming for dec pomdps

resolution method
program
solver
heuristics
milp
cplex
milp
cplex
low
milp
cplex
up
milp
cplex
loc
milp
cplex
loc  low
milp
cplex
loc  up
milp  
cplex
nlp
snopt
nlp
lanc 
nlp
loqo
algorithm
family
dp
dyn  prog 
dp lpc
dyn  prog 
pbdp
dyn  prog 
maa 
fw  search
gmaa 
fw  search

horizon  
value
time
    
    
               
               
               
               
               
    
     
   
    
    
    
    
    
value
time
    
    
    
    
 
 
    
    
    
    

horizon  
value
time
 t
    
        
        t
        t
               
         t
 t
     
    
    
   
    
  
value
time
 m
    
   
 
 
    
    
    
    

table     ma tiger problem  value and computation time  in seconds  for the solution
of the problem as computed by several methods  best results are highlighted 
when appropriate  time shows first the time used to run the heuristics then
the global time  in the format heuristic total time  t means a timeout of
      s   m indicates that the problem does not fit into memory and   indicates that the algorithm was not tested on that problem 

   

fiaras   dutech

resolution method
program
solver
heuristics
milp
cplex
milp  
cplex
nlp
snopt
nlp
lanc 
nlp
loqo
algorithm
family
maa 
fw  search
gmaa 
fw  search

horizon  
value
time
 t
     
  
     
    
     
    
     
    
value
time
            
       
    

horizon  
value
time
 t
 t
     
    
     
    
     
  
value
time
            
            

table     fire fighting problem  value and computation time  in seconds  for the solution of the problem as computed by several methods  best results are highlighted 
 t means a timeout of       s  for maa  and gmaa   value in parenthesis
are taken from the work of oliehoek et al         and should be optimal but are
different from our optimal values 

is possible to use the information from the joint action to reduce the number of state
needed in the transition function to simply nnf h   meaning only    states for   agents
with   houses and   levels of fire 
 transition  observation and reward functions are easily derived from the above description 
for this problem  dynamic programming based methods are not tested as the problem
formulation is quite new  for horizon    the value of the optimal policy given by oliehoek
et al                differs from the value found by the milp algorithms        whereas
both methods are supposed to be exact  this might come from slight differences in our
respective formulation of the problems  for horizon    oliehoek et al         report an
optimal value of        
for this problem  milp methods are clearly outperformed by maa  and gmaa  
only nlp methods  which give an optimal solution for horizon    are better in term of
computation time  it might be that nlp are also able to find optimal policies for horizon  
but as our setting differs from the work of oliehoek et al          we are not able to check
if the policy found is really the optimal  the main reason for the superiority of forward
search method lies in the fact that this problem admits many many optimal policies with
the same value  in fact  for horizon    milp based methods find an optimal policy quite
quickly  around   s for milp    but then  using branch and bound  must evaluate all the
other potential policies before knowing that it indeed found an optimal policy  forward
search methods stop nearly as soon as they hit one optimal solution 
heuristics are not reported as  not only do they not improve the performance of milp
but they take away some computation time and thus the results are worse 
   

fimathematical programming for dec pomdps

    meeting on a grid
the problem called meeting on a grid deals with two agents that want to meet and stay
together in a grid world  it has been introduced in the work of bernstein  hansen  and
zilberstein        
in this problem  we have two robots navigating on a two by two grid world with no
obstacles  each robot can only sense whether there are walls to its left or right  and the
goal is for the robots to spend as much time as possible on the same square  the actions
are to move up  down  left or right  or to stay on the same square  when a robot attempts
to move to an open square  it only goes in the intended direction with probability     
otherwise it randomly either goes in another direction or stays in the same square  any
move into a wall results in staying in the same square  the robots do not interfere with
each other and cannot sense each other  the reward is   when the agents share a square 
and   otherwise  the initial state distribution is deterministic  placing both robots in the
upper left corner of the grid 
the problem is modelled as a dec pomdp where 
 there are   agents  each one with   actions and observations  wall on left  wall on
right  
 there are    states  since each robot can be in any of   squares at any time 
 transition  observation and reward functions are easily derived from the above description 
for this problem  dynamic programming based methods are not tested as the problem
formulation is quite new  this problem is intrinsically more complex that ff and as such
is only solved for horizon   and    again  optimal value found by our method differ from
the value reported by oliehoek et al          whereas we found that the optimal values are
     and      for horizon   and    they report optimal values of      and      
results for this problem have roughly the same pattern that the results for the ff
problem  maa  and gmaa  are quicker than milp  but this time milp is able to find
an optimal solution for horizon    nlp methods give quite good results but they are slower
than gmaa   as for the ff  there are numerous optimal policies and milp methods are
not able to detect that the policy found quickly is indeed optimal 
again  heuristics are not reported as  not only do they not improve the performance of
milp but they take away some computation time and thus the results are worse 
    random   agent problems
to test our approach on problems with   agents  we have used randomly generated decpomdps where the state transition function  the joint observation function and the reward
functions are randomly generated  the dec pomdps have   actions and   observations
per agent and    states  rewards are randomly generated integers in the range   to   
the complexity of this family of problem is quite similar to the complexity of the mabc
problem  see section      
   

fiaras   dutech

resolution method
program
solver
heuristics
milp
cplex
milp  
cplex
nlp
snopt
nlp
lanc 
nlp
loqo
algorithm
family
maa 
fw  search
gmaa 
fw  search

horizon  
value time
    
    
    
    
    
    
    
    
    
    
value time
      
 s
      
 s

horizon  
value time
    
    
 t
    
    
    
   
    
  
value time
      
    
           

table     meeting on a grid problem  value and computation time  in seconds  for
the solution of the problem as computed by several methods  best results are
highlighted   t means a timeout of       s  for maa  and gmaa   value
in parenthesis are taken from the work of oliehoek et al         and should be
optimal but are different from our optimal values   

program
milp
milp  

least time  secs 
    
    

most time  secs 
   
   

average
     
     

std  deviation
      
      

table     times taken by milp and milp   on the   agent random problem for horizon   

in order to assess the real complexity of this random problem  we have first tested a
two agent version of the problem for a horizon of    results averaged over    runs of the
programs are given in table     when compared to the mabc problem which seemed of
comparable complexity  the random problem proves easier to solve     s vs    s   for this
problem  the number of     variable is relatively small  as such it does not weight too much
on the resolution time of milp   which is thus faster 
results for a three agent problem with horizon   are given in table     once again
averaged over    runs  even though the size of the search space is smaller in that case
 for   agents and a horizon of    there are         policies whereas the problem with  
agents and horizon    there are           possible policies   the   agent problems seems
more difficult to solve  demonstrating that one of the big issue is policy coordination  here 
heuristics bring a significative improvement on the resolution time of milp  as predicted 
milp n is not very efficient and is only given for completeness 
   

fimathematical programming for dec pomdps

program
milp
milp low
milp n

least time  secs 
  
  
   

most time  secs 
   
  
    

average
    
    
    

std  deviation
     
    
   

table     times taken by milp and milp n on the   agent random problem for horizon   

   discussion
we have organized the discussion in two parts  in the first part  we analyze our results and
offer explanations on the behavior of our algorithms and the usefulness of heuristics  then 
in a second part  we explicitely address some important questions 
    analysis of the results
from the results  it appears that milp methods are a better alternative to dynamic programming methods for solving dec pomdps but are globally and generally clearly outperformed by forward search methods  the structure and thus the characteristics of the
problem have a big influence on the efficiency of the milp methods  whereas it seems
that the behavior of gmaa  in terms of computation time is quite correlated with the
complexity of the problem  size of the action and observation spaces   milp methods seem
sometimes less correlated to this complexity  it is the case for the mabc problem  many
extraneous histories can be pruned  and the ma tiger problem  special structure  where
they outperform gmaa   on the contrary  when many optimal policies exists  forward
search methods like gmaa  are clearly a better choice  finally  non linear programs 
even though they can not guarantee an optimal solution  are generally a good alternative
as they are sometimes able to find a very good solution and their computation time is often
better than gmaa   this might prove useful for approximate heuristic driven forward
searches 
the computational record of the two   agent programs shows that milp   agents is
slower than milp when the horizon grows  there are two reasons to which the sluggishness
of milp   agents may be attributed  the time taken by the branch and bound  bb 
method to solve a     milp is inversely proportional to the number of     variables in
the milp  milp   agents has many more     variables than milp event hough the
total number of variables in it is exponentially less than in milp  this is the first reason 
secondly  milp   agents is a more complicated program than milp  it has many more
constraints than milp  milp is a simple program  concerned only with finding a subset
of a given set  in addition to finding weights of histories  milp also finds weights of
terminal joint histories  this is the only extra or superfluous quantity it is forced to find 
on the other hand  milp   agents takes a much more circuitous route  finding many more
superfluous quantities than milp  in addition to weights of histories  milp   agents
also finds supports of policies  regrets of histories and values of information sets  thus  the
   

fiaras   dutech

problem

heuristic

mabc

loc
low
up
loc
low
up
loc

ma tiger

meeting

horizon  
time  pruned

    

    

    

      

horizon  
time  pruned
    
     
    
    
    
     
    
    
               

horizon  
time  pruned
     
      
    
    
    
     
   
    

horizon  
time  pruned
    
       
   
    

table     computation time of heuristics  for the loc heuristics  we give the computation time in seconds and the number of locally extraneous histories pruned
over the total number of histories  for an agent   a   denotes cases where one
additional history is prunned for the second agent  for the low and up heuristic 
only computation time is given 

relaxation of milp   agents takes longer to solve than the relaxation of milp  this is
the second reason for the slowness with which the bb method solves milp   agents 
for bigger problems  namely fire fighters and meeting on a grid  when the horizon
stays small  milp   agents can compete with milp because of its slightly lower size  its
complexity grows like o   ai   oi   t   whereas it grows like o   ai   oi    t   for milp  but
that small difference does not hold long as the number of integer variables quickly lessens
the efficiency of milp   agents 
as far as heuristic are concerned  they proved to be invaluable for some problems  mabc
and ma tiger  and useless for others  in the case of mabc  heuristics are very helpful to
prune a large number of extraneous heuristics but ultimately  it is the combination with
the upper bound cut that it the more efficient when the horizon grows  in the case of
ma tiger  although no extraneous histories are found  using the lower bound cut heuristic
with milp leads to the quickest algorithm for solving the problem with a horizon of   
for other problems  heuristics are more of a burden as they are too greedy in computation
time to speed up the resolution  for example  for the grid meeting problem  the time
taken to prune extraneous histories is bigger than the time saved for solving the problem 
as a result  the added value of using heuristics depends on the nature of the problem
 as depicted in table     but  right now  we are not able to predict their usefulness without
trying them 
we also emphasize that the results given here lie at the limit of what is possible to solve
in an exact manner given the memory of the computer used for the resolution  especially
in terms of the horizon  furthermore  as the number of agent increases  the length of the
horizon must be decreased for the problems to still be solvable 
   

fimathematical programming for dec pomdps

    questions
the mathematical programing approach presented in this paper raises different questions 
we have explicitly addressed some of the questions that appears important to us 
q   why is the sequence form approach not entirely doomed by its exponential
complexity 
as the number of sequence form joint policies grows doubly exponentially with the horizon and the number of agents  the sequence form approach seems doomed  even compared
to dynamic programming which is doubly exponential in the worst cases only  but  indeed 
some arguments must be taken into consideration 
only an exponential number of individual histories need to be evaluated  the joint
part of the sequence form is left to the milp solver  and every computation done on a
particular history  like computing its value or checking if it is extraneous  has a greater
reusability than computations done on entire policies  an history is shared by many
more joint policies than an individual policy  in some way  sequence form allows us to work
on reusable part of policies without having to work directly in the world of distributions on
the set of joint policies 
then  the milps derived from the sequence form dec pomdps need a memory size
which grows only exponentially with the horizon and the number of agents  obviously 
such a complexity is quickly overwhelming but it is also the case of every other exact method
so far  as shown by the experiments  the milp approach derived from the sequence form
compares quite well with dynamic programming  even if outperformed by forward methods
like gmaa  
q   why does milp sometimes take so little time to find an optimal joint
policy when compared to existing algorithms 
despite the complexity of our milp approach  three factors contribute to the relative
efficiency of milp 
   first  the efficiency of linear programming tools themselves  in solving milp  the
bb method solves a sequence of linear programs using the simplex algorithm  each of
these lps is a relaxation of milp  in theory  the simplex algorithm requires in the
worst case an exponential number of steps  in the size of the lp  in solving a lp   
but it is well known that  in practice  it usually solves a lp in a polynomial number
of steps  in the size of the lp   since the size of a relaxation of milp is exponential
in the horizon  this means that  roughly speaking  the time taken to solve a relaxation
of milp is only exponential in the horizon whereas it can be doubly exponential
for other methods 
   the second factor is the sparsity of the matrix of coefficients of the constraints of
milp  the sparsity of the matrix formed by the coefficients of the constraints of
   this statement must be qualified  this worst case time requirement has not been demonstrated for all
variants of the simplex algorithm  it has been demonstrated only for the basic version of the simplex
algorithm 

   

fiaras   dutech

an lp determines in practice the rate with which a pivoting algorithm such as the
simplex solves the lp  this also applies to lemkes algorithm in the context of an
lcp   the sparser this matrix  the lesser the time required to perform elementary
pivoting  row  operations involved in the simplex algorithm and the lesser the space
required to model the lp 
   the third factor is the fact that we supplement milp with cuts  the computational
experience clearly shows how this speeds up the computations  while the first two
factors were related to solving a relaxation of milp  i e   an lp   this third factor
has an impact on the bb method itself  the upper bound cut identifies an additional
terminating condition for the bb method  thus enabling it to terminate earlier than
in the absence of this condition  the lower bound cut attempts to shorten the list
of active subproblems  lps  which the bb method solves sequentially  due to this
cut  the bb method has potentially a lesser number of lps to solve  note that in
inserting the lower bound cut  we are emulating the forward search properties of the
a  algorithm 
q   how do we know that the milp solver  ilogs cplex in our experiments 
is not the only reason for the speedup 
clearly  our approach would be slower  even sometime slower than a classical dynamic
programming approach if we had used another program for solving our milps as we experimented also our milps with solvers from the neos website that were indeed very very
slow  it is true that cplex  the solver we have used in our experiments  is quite optimized 
nevertheless  it is exactly one of the points we wanted to experiment with in this paper 
one of the advantages of formulating a dec pomdp as a milp is the possibility to use
the fact that  as mixed integer linear programs are very important for the industrial world 
optimized solvers do exist 
then  we had to formulate a dec pomdp as a milp and this is mostly what this paper
is about 
q   what is the main contribution of this paper 
as stated earlier in the paper  current algorithms for dec pomdps were largely inspired by pomdps algorithms  our main contribution was to pursue an entirely different
approach  i e   mixed integer linear programming  as such  we have learned a lot about
dec pomdps and about the pro   con of this mathematical programming approach  this
has lead to the formulation of new algorithms 
in designing these algorithms  we have  first of all  drawn attention to a new representation of a policy  namely the sequence form of a policy  introduced by koller  megiddo
and von stengel  the sequence form of a policy is not a compact representation of the
policy of an agent  but it does afford a compact representation of the set of policies of the
agent 
the algorithms we have proposed for finite horizon dec pomdps are mathematical
programming algorithms  to be precise  they are     milps  in the mdp domain 
   

fimathematical programming for dec pomdps

mathematical programming has been long used for solving the infinite horizon case  for
instance  an infinite horizon mdp can be solved by a linear program  depenoux        
more recently  mathematical programming has been directed at infinite horizon pomdps
and dec pomdps  thus  an infinite horizon dec mdp  with state transition independence  can be solved by a     milp  petrik   zilberstein        and an infinite horizon
pomdp or dec pomdp can be solved  for local optima  by a nonlinear program  amato  bernstein    zilberstein      b      a   the finite horizon case  much different in
character than the infinite horizon case  has been dealt with using dynamic programming 
as stated earlier  whereas dynamic programming has been quite successful for finite horizon
mdps and pomdps  it has been less so for finite horizon dec pomdps 
in contrast  in game theory  mathematical programming has been successfully directed
at games of finite horizon  lemkes algorithm        for two player normal form games  the
govindan wilson algorithm        for n player normal form games and the koller  megiddo
and von stengel approach  which internally uses lemkes algorithm  for two player extensive
form games are all for finite horizon games 
what remained then was to a find way to appropriate mathematical programming for
solving the finite horizon case of the pomdp dec pomdp domain  our work has done
precisely this  incidently  we now have an algorithm for solving some kind of n player normal form games   throughout the paper  we have shown how mathematical programming
 in particular      integer programming  can be applied for solving finite horizon decpomdps  it is easy to see that the approach we have presented yields a linear program
for solving a finite horizon pomdp   additionally  the computational experience of our
approach indicates that for finite horizon dec pomdps  mathematical programming may
be better  faster  than dynamic programming  we have also shown how the well entrenched
dynamic programming heuristic of the pruning of redundant or extraneous objects  in our
case  histories  can be integrated into this mathematical programming approach 
hence  the main contribution of this paper is that it presents  for the first time  an alternative approach for solving finite horizon pomdps dec pomdps based on milps 
q   is the mathematical programming approach presented in this paper something of a dead end 
this question is bit controversial and a very short answer to this question could be
a small yes  but this is true for every approach that looks for exact optimal solutions
to dec pomdps  whether it is grounded on dynamic programming or forward search or
mathematical programming  because of the complexity of the problem  an exact solution
will always be untractable but our algorithms can still be improved 
a longer answer is more mitigated  especially in the light of the recent advances made
for dynamic programming and forward search algorithms  one crucial point in sequenceform dec pomdps is the pruning of extraneous histories  a recent work from oliehoek 
whiteson  and spaan        has shown how to clusters histories that are equivalent in a
way that could also reduce the nomber of constraints in milps  the approach of amato 
dibangoye  and zilberstein        that improves and speed up the dynamic programming
operator could help in finding extraneous histories  so  at the very least  some work is
   

fiaras   dutech

still required before stating that every aspect of sequence form dec pomdps have been
studied 
we now turn to an even longer answer  consider the long horizon case  given that exact
algorithms  including the ones presented in this paper  can only tackle horizons less than   
by long horizon  we mean anything upwards of   time periods  for the long horizon case 
we are required to conceive a possibly sub optimal joint policy for the given horizon and
determine an upper bound on the loss of value incurred by using the joint policy instead of
using an optimal joint policy 
the current trend for the long horizon case is a memory bounded approach  the memory
bounded dynamic programming  mbdp  algorithm  seuken   zilberstein        is the
main exponent of this approach  this algorithm is based on the backward induction dp
algorithm  hansen et al          the algorithm attempts to run in a limited amount of
space  in order to do so  unlike the dp algorithm  it prunes even non extraneous  i e   nondominated  policy trees at each iteration  thus  at each iteration  the algorithm retains
a pre determined number of trees  this algorithm and its variants have been used to find
a joint policy for the mabc  the ma tiger and the box pushing problems for very long
horizons  of the order of thousands of time periods  
mbdp does not provide an upper bound on the loss of value  the bounded dp  bdp 
algorithm presented in the paper by amato  carlin  and zilberstein      c  does give an
upper bound  however  on more interesting dec pomdp problems  such as ma tiger  
mbdp finds a much better joint policy than bdp 
a meaningful way to introduce the notion of memory boundedness into our approach is
to fix an a priori upper bound on the size of the concerned mathematical program  this
presents all sorts of difficulties but the main difficulty seems to be the need to represent
a policy for a long horizon in limited space  the mbdp algorithm solves this problem
by using what may be termed as a recursive representation  the recursive representation
causes the mbdp algorithm to take a long time to evaluate a joint policy  but it does allow
the algorithm to represent a long horizon joint policy in limited space  in the context of
our mathematical programming approach  we would have to change the policy constraints
in some way so that a long horizon policy is represented by a system consisting of a limited
number of linear equations and linear inequalities  besides the policy constraints  other
constraints of the presented programs would also have to be accordingly transfigured  it is
not evident  to us  if such a transfiguration of the constraints is possible 
on the other hand  the infinite horizon case seems to be a promising candidate to adapt
our approach to  mathematical programming has already been applied  with some success 
to solving infinite horizon dec pomdps  amato et al       a   the computational experience of this mathematical programming approach shows that it is better  finds higher
quality solutions in lesser time  than a dynamic programming approach  bernstein et al  
      szer   charpillet        
nevertheless  this approach has two inter related shortcomings  first  the approach
finds a joint controller  i e   an infinite horizon joint policy  of a fixed size and not of the
optimal size  second  much graver than the first  for the fixed size  it finds a locally optimal
joint controller  the approach does not guarantee finding an optimal joint controller  this
is because the program presented in the work of amato et al       a  is a  non convex 
   

fimathematical programming for dec pomdps

nonlinear program  nlp   the nlp finds a fixed size joint controller in the canonical form
 i e   in the form of a finite state machine   we believe that both these shortcomings can
be removed by conceiving a mathematical program  specifically  a     mixed integer linear
program  that finds a joint controller in the sequence form  as stated earlier  the main
challenge in this regard is therefore an identification of the sequence form of an infinite
horizon policy  in fact  it may be that if such sequence form characterization of an infinite
horizon policy is obtained  it could be used in conceiving a program for the long horizon
 undiscounted reward  case as well 
q   how does this help achieve designing artificial autonomous agents  
at first sight  our work does not have any direct and immediate applied benefits for the
purpose of building artificial intelligent agents or understanding how intelligence works 
even in the limited field of multi agent planning  our contributions are more on a theoretical
level than on a practical one 
real artificial multi agent systems can indeed be modeled as dec pomdps  even if they
make use of communication  of common knowledge  of common social law  then  such real
systems would likely be made of a large number of states  actions or observations and require
solutions over a large horizon  our mathematical programming approach is practically
useless in that setting as limited to dec pomdps of very small size  other models that
are simpler  but far from trivial  to solve because they explicitly take into account some
characteristics of the real systems do exist  some works take advantage of communications
 xuan  lesser    zilberstein        ghavamzadeh   mahadevan         some of the existing
independencies in the system  wu   durfee        becker  zilberstein  lesser    goldman 
       some do focus on interaction between agents  thomas  bourjot    chevrier        
some  as said while answering the previous questions  rely on approximate solutions  etc   
it is our intention to facilitate the re use and the adaptation to these other models of the
concepts used in our work and of the knowledge about the structure of an optimal solution of
a dec pomdp  to that end  we decided not only to describe the milp programs but also 
and most importantly  how we derived these programs by making use of some properties of
optimal dec pomdp solutions 
truly autonomous agents will also require to adapt to new and unforeseen situations 
our work being dedicated to planning  it seems easy to argue that it does not contribute
very much to that end either  on the other hand  learning in dec pomdps has never
really been addressed except for some fringe work in particular settings  scherrer   charpillet        ghavamzadeh   mahadevan        buffet  dutech    charpillet         in fact 
even for simple pomdps  learning is a very difficult task  singh  jaakkola    jordan 
       currently  the more promising research deals with learning the predictive state
representation  psr  of a pomdp  singh  littman  jong  pardoe    stone        james
  singh        mccracken   bowling         making due allowance to the fundamental
differences between the functional role of psr and histories  we notice that psr and histories are quite similar in structure  while it is too early to say  it might be that trying to
learn the useful histories of a dec pomdp could take some inspiration from the way the
right psrs are learned for pomdps 

   

fiaras   dutech

   conclusion
we designed and investigated new exact algorithms for solving decentralized partially observable markov decision processes with finite horizon  dec pomdps   the main contribution of our paper is the use of sequence form policies  based on a sets of histories  in
order to reformulate a dec pomdp as a non linear programming problem  nlp   we
have then presented two different approaches to linearize the nlp in order to find global
and optimal solutions to dec pomdps  the first approach is based on the combinatorial
properties of the optimal policies of dec pomdps and the second one relies on concepts
borrowed from the field of game theory  both lead to formulating dec pomdps as    
mixed integer linear programming problems  milps   several heuristics for speeding up
the resolution of these milps make another important contribution of our work 
experimental validation of the mathematical programming problems designed in this
work was conducted on classical dec pomdp problems found in the literature  these
experiments show that  as expected  our milp methods outperform classical dynamic
programming algorithms  but  in general  they are less efficient and more costly than
forward search methods like gmaa   especially in the case where the dec pomdp admits
many optimal policies  nevertheless  according to the nature of the problem  milp methods
can sometimes greatly outperform gmaa   as in the ma tiger problem  
while it is clear that exact resolution of dec pomdps can not scale up with the size
of the problems or the length of the horizon  designing exact methods is useful in order
to develop or improve approximate methods  we see at least three research directions
where our work can contribute  one direction could be to take advantage of the large
literature on algorithms for finding approximate solutions to milps and to adapt them to
the milps formulated for dec pomdps  another direction would be to use the knowledge
gained from our work to derive improved heuristics for guiding existing approximate existing
methods for dec pomdps  for example  the work of seuken and zilberstein         in
order to limit the memory resources used by the resolution algorithm  prune the space of
policies to only consider some of them  our work could help using a better estimation of
the policies that are important to be kept in the search space  then  the one direction we
are currently investigating is to adapt our approach to dec pomdps of infinite length by
looking for yet another representation that would allow such problems to be seen as milps 
more importantly  our work participates to a better understanding of dec pomdps 
we analyzed and understood key characteristics of the nature of optimal policies in order
to design the milps presented in this paper  this knowledge can be useful for other work
dealing with dec pomdps and even pomdps  the experimentations have also given
some interesting insights on the nature of the various problems tested  in term of existence
of extraneous histories or on the number of optimal policies  these insights might be a first
step toward a taxonomy of dec pomdps 

appendix a  non convex non linear program
using the simplest example  this section aims at showing that the non linear program
 nlp  expressed in table   can be non convex 
   

fimathematical programming for dec pomdps

let us consider an example with two agents  each one with   possible actions  a and b 
that want to solve a horizon   decision problem  the set of possible joint histories is then 
ha  ai  ha  bi  hb  ai and hb  bi  then the nlp to solve is 
variables  x   a   x   b   x   a   x   a 
maximize

r   ha  ai x   a x   a    r   ha  bi x   a x   b 

     

 r   hb  ai x   b x   a    r   hb  bi x   b x   b 
subject to
x   a    x   b     
x   a    x   b     
x   a     

x   b    

x   a     

x   b    

a matrix formulation of the objective
x of the following kind 

    c
     e
c 
 c e  
d f  

function of eq        would be xt  c x with c and

d
f 

  
 




x   a 
 x   b  

x 
 x   a    
x   b 

     

if  is the eigen value of vector v    v  v  v  v   t then it is straightforward to show that
 is also an eigen value   v   v  v  v   t   c  v  v   v   v   t   as a result  the
matrix c  hessian of the objective function  is not positive definite and thus the objective
function is not convex 

appendix b  linear program duality
every linear program  lp  has a converse linear program called its dual  the first lp is
called the primal to distinguish it from its dual  if the primal maximizes a quantity  the
dual minimizes the quantity  if there are n variables and m constraints in the primal  there
are m variables and n constraints in the dual  consider the following  primal  lp 
variables  x i   i              n 
maximize

n
x

c i x i 

i  

subject to 
n
x

a i  j x i    b j  

j              m

i  

x i     

i              n

   

fiaras   dutech

this primal lp has one variable x i  for each i     to n  the data of the lp consists
of numbers c i  for each i     to n  the numbers b j  for each j     to m and the numbers
a i  j  for each i     to n and for each j     to m  the lp thus has n variables and m
constraints  the dual of this lp is the following lp 
variables  y j   j              m  


minimize

m
x

b j y j 

j  

subject to 


m
x

a i  j y j   c i  

i              n

j  

y j         

j              m

the dual lp has one variable y j  for each j     to m  each y j  variable is a free
variable  that is  it is allowed to take any value in r  the dual lp has m variables and n
constraints 
the theorem of linear programming duality is as follows 
theorem b     luenberger        if either a primal lp or its dual lp has a finite optimal
solution  then so does the other  and the corresponding values of the objective functions are
equal 
applying this theorem to the primal dual pair given above  there holds 
n
x

c i x  i   

m
x

b j y   j 

j  

i  

where x denotes an optimal solution to the primal and y  denotes an optimal solution to
the dual 
the theorem of complementary slackness is as follows 
theorem b     vanderbei        suppose that x is feasible for a primal linear program and
y is feasible for its dual  let  w       wm   denote the corresponding primal slack variables 
and let  z       zn   denote the corresponding dual slack variables  then x and y are optimal
for their respective problems if and only if
xj zj    

for j           n 

wi yi    

for i           m 
   

fimathematical programming for dec pomdps

appendix c  regret for dec pomdps
the value of an information set   ii of an agent i for a i reduced joint policy q 
denoted i    q   is defined by 
x
i    q    max
r   hh  j  i q j   
     
h

j  ei

for any terminal information set and  if  is non terminal  by 
x
i  h o  q 
i    q    max
h

     

ooi

then  the regret of a history h for an agent i and for a i reduced joint policy q 
denoted i  h  q   it is defined by 
x
i  h  q    i   h   q  
r   hh  j  i q j   
     
t
j  hi

if h is terminal and  if h is non terminal  by 
i  h  q    i   h   q  

x

i  h o  q 

     

ooi

the concept of regret of the agent i  which is independant of the policy of the agent i  is
very useful when looking for optimal policy because its optimal value is known  it is    it
is thus easier to manipulate than the optimal value of a policy 

appendix d  program changes due to optimizations
pruning locally or globally extraneous histories reduces the size of the search space of the
mathematical programs  now  some constraints of the programs depend on the size of the
search space  we must then alter some of these constraints 
let denote by a  superscript the sets actually used in our program  for example  ei
will be the actual set of terminal histories of agent i  be it pruned of extraneous histories
or not 
programs milp  table    and milp n agents  table    rely on the fact that the
number of histories of a given length t in the support of a pure policy of each agent is fixed
and equal to  oi  t    as it may not be the case with pruned sets  the following changes
have to be made 
 the constraint      of milp or       milp n agents  that is
x
y
z j   
 oi  t  
je

ii

must be replaced by
x

z j  

y
ii

je

   

 oi  t    

     

fiaras   dutech

 the set of constraints      of milp or       of milp n agents  that is
x

z hh  j  i   

j  ei

y

 ok  t   xi  h  

i  i  h  ei

y

 ok  t   xi  h  

i  i  h  ei  

ki  i 

must be replaced by
x

z hh  j  i  

     

ki  i 

j  ei

 the set of constraints       of milp n agents  that is
yi   h   

x
 
r   hh  ji i z j    wi  h  
 oi  t  

h  ei

je

must be replaced by
yi   h   

x
 
r   hh  ji i z j    wi  h  
 oi  t  

h  ei  

     

je

appendix e  example using ma tiger
all these example are derived using the decentralized tiger problem  ma tiger  described
in section      we have two agents  with   actions  al   ar   ao   and   observations  ol   or   
we will only consider problem with an horizon of   
there are            terminal histories for an agent  ao  ol  ao   ao  ol  al   ao  ol  ar   ao  or  ao  
ao  or  al   ao  or  ar   al  ol  ao   al  ol  al   al  ol  ar   al  or  ao   al  or  al   al  or  ar   ar  ol  ao   ar  ol  al  
ar  ol  ar   ar  or  ao   ar  or  al   ar  or  ar  
and thus                     joint histories for the agents  hao  ol  ao  ao  ol  ao i hao  ol  ao  ao  ol  al i 
hao  ol  ao  ao  ol  ar i       har  or  ar  ar  or  ar i 
e   policy constraints
the policy constraints with horizon   for one agent in the ma tiger problem would be 
variables  x for every history
x ao     x al     x ar      
x ao     x ao  ol  ao     x ao  ol  al     x ao  ol  ar      
x ao     x ao  or  ao     x ao  or  al     x ao  or  ar      
x al     x al  ol  ao     x al  ol  al     x al  ol  ar      
x al     x al  or  ao     x al  or  al     x al  or  ar      
x ar     x ar  ol  ao     x ar  ol  al     x ar  ol  ar      
x ar     x ar  or  ao     x ar  or  al     x ar  or  ar      
   

fimathematical programming for dec pomdps

x ao     

x al     

x ar     

x ao  ol  ao      x ao  ol  al      x ao  ol  ar     
x ao  or  ao      x ao  or  al      x ao  or  ar     
x al  ol  ao     

x al  ol  al     

x al  ol  ar     

x al  or  ao      x al  or  al      x al  or  ar     
x ar  ol  ao      x ar  ol  al      x ar  ol  ar     
x ar  or  ao      x ar  or  al      x ar  or  ar     
e   non linear program for ma tiger
the non linear program for finding an optimal sequence form policy for the ma tiger
with horizon   would be 
variables  xi for every history for each agent

maximize

r   hao  ol  ao   ao  ol  ao i x   ao  ol  ao  x   ao  ol  ao  
  r   hao  ol  ao   ao  ol  al i x   ao  ol  ao  x   ao  ol  al  
  r   hao  ol  ao   ao  ol  ar i x   ao  ol  ao  x   ao  ol  ar  
  

subject to 
x   ao     x   al     x   ar      
x   ao     x   ao  ol  ao     x   ao  ol  al     x   ao  ol  ar      
x   ao     x   ao  or  ao     x   ao  or  al     x   ao  or  ar      
x   al     x   al  ol  ao     x   al  ol  al     x   al  ol  ar      
x   al     x   al  or  ao     x   al  or  al     x   al  or  ar      
x   ar     x   ar  ol  ao     x   ar  ol  al     x   ar  ol  ar      
x   ar     x   ar  or  ao     x   ar  or  al     x   ar  or  ar      

x   ao     x   al     x   ar      
x   ao     x   ao  ol  ao     x   ao  ol  al     x   ao  ol  ar      
x   ao     x   ao  or  ao     x   ao  or  al     x   ao  or  ar      
x   al     x   al  ol  ao     x   al  ol  al     x   al  ol  ar      
x   al     x   al  or  ao     x   al  or  al     x   al  or  ar      
x   ar     x   ar  ol  ao     x   ar  ol  al     x   ar  ol  ar      
x   ar     x   ar  or  ao     x   ar  or  al     x   ar  or  ar      
   

fiaras   dutech

x   ao     

x   al     

x   ar     

x   ao  ol  ao      x   ao  ol  al      x   ao  ol  ar     
x   ao  or  ao      x   ao  or  al      x   ao  or  ar     
x   al  ol  ao     

x   al  ol  al     

x   al  ol  ar     

x   al  or  ao      x   al  or  al      x   al  or  ar     
x   ar  ol  ao      x   ar  ol  al      x   ar  ol  ar     
x   ar  or  ao      x   ar  or  al      x   ar  or  ar     

x   ao     

x   al     

x   ar     

x   ao  ol  ao      x   ao  ol  al      x   ao  ol  ar     
x   ao  or  ao      x   ao  or  al      x   ao  or  ar     
x   al  ol  ao     

x   al  ol  al     

x   al  ol  ar     

x   al  or  ao      x   al  or  al      x   al  or  ar     
x   ar  ol  ao      x   ar  ol  al      x   ar  ol  ar     
x   ar  or  ao      x   ar  or  al      x   ar  or  ar     

e   milp for ma tiger
the milp with horizon   for the agents in the ma tiger problem would be 
variables 
xi  h  for every history of agent i
z j  for every terminal joint history

maximize

r   hao  ol  ao   ao  ol  ao i z hao  ol  ao   ao  ol  ao i 
  r   hao  ol  ao   ao  ol  al i z hao  ol  ao   ao  ol  al i 
  r   hao  ol  ao   ao  ol  ar i z hao  ol  ao   ao  ol  ar i 
  
   

fimathematical programming for dec pomdps

subject to 
x   ao     x   al     x   ar      
x   ao     x   ao  ol  ao     x   ao  ol  al     x   ao  ol  ar      
x   ao     x   ao  or  ao     x   ao  or  al     x   ao  or  ar      

x   ao     x   al     x   ar      
x   ao     x   ao  ol  ao     x   ao  ol  al     x   ao  ol  ar      
x   ao     x   ao  or  ao     x   ao  or  al     x   ao  or  ar      

z hao  ol  ao   ao  ol  ao i    z hao  ol  ao   ao  ol  al i    z hao  ol  ao   ao  ol  ar i       x   ao  ol  ao  
z hao  ol  ao   ao  ol  ao i    z hao  ol  al   ao  ol  ao i    z hao  ol  ar   ao  ol  ao i       x   ao  ol  ao  
z hao  ol  al   ao  ol  ao i    z hao  ol  al   ao  ol  al i    z hao  ol  al   ao  ol  ar i       x   ao  ol  al  
z hao  ol  ao   ao  ol  al i    z hao  ol  al   ao  ol  al i    z hao  ol  ar   ao  ol  al i       x   ao  ol  al  


x   ao     

x   al     

x   ar     

x   ao  ol  ao          

x   ao  ol  al          

x   ao  ol  ar          

x   ao  or  ao          

x   ao  or  al          

x   ao  or  ar          


x   ao     

x   al     

x   ar     

x   ao  ol  ao          

x   ao  ol  al          

x   ao  ol  ar          

x   ao  or  ao          

x   ao  or  al          

x   ao  or  ar          


z hao  ol  ao   ao  ol  ao i          z hao  ol  ao   ao  ol  al i          z hao  ol  ao   ao  ol  ar i         
z hao  ol  al   ao  ol  ao i          z hao  ol  al   ao  ol  al i          z hao  ol  al   ao  ol  ar i         

e   milp   agents for ma tiger
the milp   agents with horizon   for the agents in the ma tiger problem would be 
variables 
xi  h   wi  h  and bi  h  for every history of agent i
yi     for each agent and for every information set
maximize

y    

   

fiaras   dutech

subject to 

x   ao     x   al     x   ar      
x   ao     x   ao  ol  ao     x   ao  ol  al     x   ao  ol  ar      
x   ao     x   ao  or  ao     x   ao  or  al     x   ao  or  ar      

x   ao     x   al     x   ar      
x   ao     x   ao  ol  ao     x   ao  ol  al     x   ao  ol  ar      
x   ao     x   ao  or  ao     x   ao  or  al     x   ao  or  ar      

y      y   ao  ol    y   ao  or     w   ao  
y      y   al  ol    y   al  or     w   al  
y      y   ar  ol    y   ar  or     w   ar  
y      y   ao  ol    y   ao  or     w   ao  
y      y   al  ol    y   al  or     w   al  
y      y   ar  ol    y   ar  or     w   ar  

y   ao  ol    r   hao  ol  ao   ao  ol  ao i x   ao  ol  ao  
r   hao  ol  ao   ao  ol  al i x   ao  ol  al  
r   hao  ol  ao   ao  ol  ar i x   ao  ol  ar  
r   hao  ol  ao   al  ol  ao i x   al  ol  ao  
r   hao  ol  ao   al  ol  al i x   al  ol  al  
r   hao  ol  ao   al  ol  ar i x   al  ol  ar  
     w   ao  ol  ao  

y   ao  ol    r   hao  ol  al   ao  ol  ao i x   ao  ol  ao  
r   hao  ol  al   ao  ol  al i x   ao  ol  al  
r   hao  ol  al   ao  ol  ar i x   ao  ol  ar  
r   hao  ol  al   al  ol  ao i x   al  ol  ao  
r   hao  ol  al   al  ol  al i x   al  ol  al  
r   hao  ol  al   al  ol  ar i x   al  ol  ar  
     w   ao  ol  al  
   

fimathematical programming for dec pomdps


y   ar  or    r   har  or  ar   ao  ol  ao i x   ao  ol  ao  
r   har  or  ar   ao  ol  al i x   ao  ol  al  
r   har  or  ar   ao  ol  ar i x   ao  ol  ar  
r   har  or  ar   al  ol  ao i x   al  ol  ao  
r   har  or  ar   al  ol  al i x   al  ol  al  
r   har  or  ar   al  ol  ar i x   al  ol  ar  
     w   ar  or  ar  
y   ao  ol    r   hao  ol  ao   ao  ol  ao i x   ao  ol  ao  
r   hao  ol  al   ao  ol  ao i x   ao  ol  al  
r   hao  ol  ar   ao  ol  ao i x   ao  ol  ar  
r   hal  ol  ao   ao  ol  ao i x   al  ol  ao  
r   hal  ol  al   ao  ol  ao i x   al  ol  al  
r   hal  ol  ar   ao  ol  ao i x   al  ol  ar  
     w   ao  ol  ao  
y   ao  ol    r   hao  ol  ao   ao  ol  al i x   ao  ol  ao  
r   hao  ol  al   ao  ol  al i x   ao  ol  al  
r   hao  ol  ar   ao  ol  al i x   ao  ol  ar  
r   hal  ol  ao   ao  ol  al i x   al  ol  ao  
r   hal  ol  al   ao  ol  al i x   al  ol  al  
r   hal  ol  ar   ao  ol  al i x   al  ol  ar  
     w   ao  ol  al  


x   ao       b   ao  

x   al       b   al  

x   ar       b   ar  

x   ao  ol  ao       b   ao  ol  ao  

x   ao  ol  al       b   ao  ol  al  

x   ao  ol  ar       b   ao  ol  ar  



w   ao    u   ao  b   ao  

w   al    u   al  b   al  

w   ar    u   ar  b   ar  

w   ao  ol  ao    u   ao  ol  ao  b   ao  ol  ao  

w   ao  ol  al    u   ao  ol  al  b   ao  ol  al  

w   ao  ol  ar    u   ao  ol  ar  b   ao  ol  ar  


   

fiaras   dutech

x   ao     
x   ao  ol  ao     

x   al     
x   ao  ol  al     

x   ar     
x   ao  ol  ar     


w   ao     
w   ao  ol  ao     

w   al     
w   ao  ol  al     

w   ar     
w   ao  ol  ar     


b   ao          
b   ao  ol  ao          

b   al          
b   ao  ol  al          


y           
y   ao  ol          y   ao  or         

    and the same for agent  

   

b   ar          
b   ao  ol  ar          

fimathematical programming for dec pomdps

references
amato  c   bernstein  d  s     zilberstein  s       a   optimizing memory bounded controllers for decentralized pomdps  in proc  of the twenty third conf  on uncertainty
in artificial intelligence  uai     
amato  c   bernstein  d  s     zilberstein  s       b   solving pomdps using quadratically
constrained linear programs  in proc  of the twentieth int  joint conf  on artificial
intelligence  ijcai    
amato  c   carlin  a     zilberstein  s       c   bounded dynamic programming for
decentralized pomdps  in proc  of the workshop on multi agent sequential decision
making in uncertain domains  msdm  in aamas   
amato  c   dibangoye  j     zilberstein  s          incremental policy generation for finitehorizon dec pomdps  in proc  of the nineteenth int  conf  on automated planning
and scheduling  icaps     
anderson  b     moore  j          time varying feedback laws for decentralized control 
nineteenth ieee conference on decision and control including the symposium on
adaptive processes                
becker  r   zilberstein  s   lesser  v     goldman  c          solving transition independent
decentralized markov decision processes  journal of artificial intelligence research 
           
bellman  r          dynamic programming  princeton university press  princeton  newjersey 
bernstein  d   givan  r   immerman  n     zilberstein  s          the complexity of decentralized control of markov decision processes  mathematics of operations research 
               
bernstein  d  s   hansen  e  a     zilberstein  s          bounded policy iteration for
decentralized pomdps  in proc  of the nineteenth int  joint conf  on artificial
intelligence  ijcai   pp           
boularias  a     chaib draa  b          exact dynamic programming for decentralized
pomdps with lossless policy compression  in proc  of the int  conf  on automated
planning and scheduling  icaps    
boutilier  c          planning  learning and coordination in multiagent decision processes 
in proceedings of the  th conference on theoretical aspects of rationality and knowledge  tark      de zeeuwse stromen  nederlands 
buffet  o   dutech  a     charpillet  f          shaping multi agent systems with gradient reinforcement learning  autonomous agent and multi agent system journal
 aamasj                  
   

fiaras   dutech

cassandra  a   kaelbling  l     littman  m          acting optimally in partially observable
stochastic domains  in proc  of the   th nat  conf  on artificial intelligence  aaai  
chades  i   scherrer  b     charpillet  f          a heuristic approach for solving
decentralized pomdp  assessment on the pursuit problem  in proc  of the      acm
symposium on applied computing  pp       
cornuejols  g          valid inequalities for mixed integer linear programs  mathematical
programming b           
dantzig  g  b          on the significance of solving linear programming problems with
some integer variables  econometrica              
depenoux  f          a probabilistic production and inventory problem  management
science               
diwekar  u          introduction to applied optimization    edition   springer 
drenick  r          multilinear programming  duality theories  journal of optimization
theory and applications                
fletcher  r          practical methods of optimization  john wiley   sons  new york 
ghavamzadeh  m     mahadevan  s          learning to communicate and act in cooperative multiagent systems using hierarchical reinforcement learning  in proc  of the  rd
int  joint conf  on autonomous agents and multi agent systems  aamas    
govindan  s     wilson  r          a global newton method to compute nash equilibria 
journal of economic theory            
hansen  e   bernstein  d     zilberstein  s          dynamic programming for partially
observable stochastic games  in proc  of the nineteenth national conference on artificial intelligence  aaai     
horst  r     tuy  h          global optimization  deterministic approaches   rd edition  
springer 
james  m     singh  s          learning and discovery of predictive state representations
in dynamical systems with reset  in proc  of the twenty first int  conf  of machine
learning  icml    
kaelbling  l   littman  m     cassandra  a          planning and acting in partially
observable stochastic domains  artificial intelligence             
koller  d   megiddo  n     von stengel  b          fast algorithms for finding randomized
strategies in game trees  in proceedings of the   th acm symposium on theory of
computing  stoc      pp         
koller  d     megiddo  n          finding mixed strategies with small supports in extensive
form games  international journal of game theory              
   

fimathematical programming for dec pomdps

lemke  c          bimatrix equilibrium points and mathematical programming  management science                
luenberger  d          linear and nonlinear programming  addison wesley publishing
company  reading  massachussetts 
mccracken  p     bowling  m  h          online discovery and learning of predictive state
representations  in advances in neural information processing systems     nips    
nair  r   tambe  m   yokoo  m   pynadath  d     marsella  s          taming decentralized
pomdps  towards efficient policy computation for multiagent setting  in proc  of
int  joint conference on artificial intelligence  ijcai   
oliehoek  f   spaan  m     vlassis  n          optimal and approximate q value functions
for decentralized pomdps  journal of artificial intelligence research  jair      
       
oliehoek  f   whiteson  s     spaan  m          lossless clustering of histories in decentralized pomdps  in proc  of the international joint conference on autonomous
agents and multi agent systems  pp         
osborne  m  j     rubinstein  a          a course in game theory  the mit press 
cambridge  mass 
papadimitriou  c  h     steiglitz  k          combinatorial optimization  algorithms and
complexity  dover publications 
papadimitriou  c  h     tsitsiklis  j          the complexity of markov decision processes  mathematics of operations research                   
parsons  s     wooldridge  m          game theory and decision theory in multi agent
systems  autonomous agents and multi agent systems  jaamas                
petrik  m     zilberstein  s          average reward decentralized markov decision processes  in proc  of the twentieth int  joint conf  on artificial intelligence  ijcai
      
petrik  m     zilberstein  s          a bilinear programming approach for multiagent
planning  journal of artificial intelligence research  jair              
puterman  m          markov decision processes  discrete stochastic dynamic programming  john wiley   sons  inc  new york  ny 
pynadath  d     tambe  m          the communicative multiagent team decision problem  analyzing teamwork theories and models  journal of artificial intelligence
research             
radner  r          the application of linear programming to team decision problems 
management science            
russell  s     norvig  p          artificial intelligence  a modern approach  prentice hall 
   

fiaras   dutech

sandholm  t          multiagent systems  chap  distributed rational decision making  pp 
        the mit press  ed  by g  weiss 
sandholm  t   gilpin  a     conitzer  v          mixed integer programming methods for
finding nash equilibria  in proc  of the national conference on artificial intelligence
 aaai  
scherrer  b     charpillet  f          cooperative co learning  a model based approach
for solving multi agent reinforcement problems  in proc  of the ieee int  conf  on
tools with artificial intelligence  ictai    
seuken  s     zilberstein  s          memory bounded dynamic programming for decpomdps  in proc  of the twentieth int  joint conf  on artificial intelligence  ijcai    
singh  s   jaakkola  t     jordan  m          learning without state estimation in partially
observable markovian decision processes   in proceedings of the eleventh international
conference on machine learning 
singh  s   littman  m   jong  n   pardoe  d     stone  p          learning predictive state
representations  in proc  of the twentieth int  conf  of machine learning  icml    
szer  d     charpillet  f          point based dynamic programming for dec pomdps 
in proc  of the twenty first national conf  on artificial intelligence  aaai       
szer  d   charpillet  f     zilberstein  s          maa   a heuristic search algorithm for
solving decentralized pomdps  in proc  of the twenty first conf  on uncertainty
in artificial intelligence  uai     pp          
thomas  v   bourjot  c     chevrier  v          interac dec mdp   towards the use of
interactions in dec mdp  in proc  of the third int  joint conf  on autonomous
agents and multi agent systems  aamas     new york  usa  pp           
vanderbei  r  j          linear programming  foundations and extensions   rd edition  
springer 
von stengel  b          handbook of game theory  vol     chap     computing equilibria
for two person games  pp            north holland  amsterdam 
wu  j     durfee  e  h          mixed integer linear programming for transitionindependent decentralized mdps  in proc  of the fifth int  joint conf  on autonomous
agents and multiagent systems  aamas     pp           new york  ny  usa 
acm 
xuan  p   lesser  v     zilberstein  s          communication in multi agent markov
decision processes  in proc  of icmas workshop on game theoretic and decision
theoretics agents boston  ma 

   

fi