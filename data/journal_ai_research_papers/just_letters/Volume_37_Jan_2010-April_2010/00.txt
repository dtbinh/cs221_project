journal of artificial intelligence research               

submitted        published      

text relatedness based on a word thesaurus
george tsatsaronis

gbt   idi   ntnu   no

department of computer and information science
norwegian university of science and technology  norway

iraklis varlamis

varlamis   hua   gr

department of informatics and telematics
harokopio university  greece

michalis vazirgiannis

mvazirg   aueb   gr

department of informatics
athens university of economics and business  greece

abstract
the computation of relatedness between two fragments of text in an automated manner requires
taking into account a wide range of factors pertaining to the meaning the two fragments convey 
and the pairwise relations between their words  without doubt  a measure of relatedness between
text segments must take into account both the lexical and the semantic relatedness between words 
such a measure that captures well both aspects of text relatedness may help in many tasks  such as
text retrieval  classification and clustering  in this paper we present a new approach for measuring
the semantic relatedness between words based on their implicit semantic links  the approach exploits only a word thesaurus in order to devise implicit semantic links between words  based on
this approach  we introduce omiotis  a new measure of semantic relatedness between texts which
capitalizes on the word to word semantic relatedness measure  sr  and extends it to measure the
relatedness between texts  we gradually validate our method  we first evaluate the performance
of the semantic relatedness measure between individual words  covering word to word similarity and relatedness  synonym identification and word analogy  then  we proceed with evaluating
the performance of our method in measuring text to text semantic relatedness in two tasks  namely
sentence to sentence similarity and paraphrase recognition  experimental evaluation shows that the
proposed method outperforms every lexicon based method of semantic relatedness in the selected
tasks and the used data sets  and competes well against corpus based and hybrid approaches 

   introduction
relatedness between texts can be perceived in several different ways  primarily  one can think of
lexical relatedness or similarity between texts  which can be easily captured by a vectorial representation of texts  van rijsbergen        and a standard similarity measure  like cosine  dice  salton
  mcgill         and jaccard         such models have had high impact in information retrieval
over the past decades  several improvements have been proposed for such techniques towards inventing more sophisticated weighting schemes for the text words  like for example tf idf and its
variations  aizawa         other directions explore the need to capture the latent semantic relations between dimensions  words  in the created vector space model  by using techniques of latent
semantic analysis  landauer  foltz    laham         another aspect of text relatedness  probably
of equal importance  is the semantic relatedness between two text segments  for example  the sentences the shares of the company dropped    cents  and the business institutions stock slumped
   cents have an obvious semantic relatedness  which traditional measures of text similarity fail
c
    
ai access foundation  all rights reserved 

fit satsaronis   varlamis     vazirgiannis

to recognize  the motivation of this work is to show that a measure of relatedness between texts 
which takes into account both the lexical and the semantic relatedness of word elements  performs
better than the traditional lexical matching models  and can handle cases like the one above 
in this paper we propose omiotis    a new measure of semantic relatedness between texts  which
extends sr  a measure of semantic relatedness between words  the word to word relatedness measure  in its turn  is based on the construction of semantic links between individual words  according
to a word thesaurus  which in our case is wordnet  fellbaum         each pair of words is potentially connected via one or more semantic paths  each one comprising one or more semantic
relations  edges  that connect intermediate thesaurus concepts  nodes   for weighting the semantic
path we consider three key factors   a  the semantic path length   b  the intermediate nodes specificity denoted by the node depth in the thesaurus hierarchy  and  c  the types of the semantic edges
that compose the path  this triptych allows our measure to perform well in complex linguistic tasks 
that require more than simple similarity  such as the sat analogy test  that is demonstrated in the
experiments  to the best of our knowledge  omiotis is the first measure of semantic relatedness
between texts that considers in tandem all three factors for measuring the pairwise word to word
semantic relatedness scores  omiotis integrates the semantic relatedness in word level with words
statistical information in the text level to provide the final semantic relatedness score between texts 
the contributions of this work can be summarized in the following     a new measure for computing semantic relatedness between words  namely sr  which exploits all of the semantic information a thesaurus can offer  including semantic relations crossing parts of speech  pos   while taking
into account the relation weights and the depth of the thesaurus nodes     a new measure for computing semantic relatedness between texts  namely omiotis  that does not require the use of external
corpora or learning methods  supervised or unsupervised     thorough experimental evaluation on
benchmark data sets for measuring the performance on word to word similarity and relatedness
tasks  as well as on word analogy  in addition  experimental evaluation on two text related tasks
 sentence to sentence similarity and paraphrase recognition  for measuring the performance of our
text to text relatedness measure  additional contributions of this work are  a  the use of all semantic relations offered by wordnet  which increases the chances of finding a semantic path between
any two words  b  the availability of pre computed semantic relatedness scores between every pair
of wordnet senses  which accelerates computation of semantic relatedness between texts and facilitates the incorporation of semantic relatedness in several applications  tsatsaronis  varlamis 
nrvag    vazirgiannis        tsatsaronis   panagiotopoulou        
the rest of the paper is organized as follows  section   discusses preliminary concepts regarding
word thesauri  semantic network construction  and semantic relatedness or similarity measures  and
summarizes related work on these fields  section   presents the key contributions of our work 
section   provides the experimental evaluation and the analysis of the results  finally  section  
presents our conclusions and the next steps of our work 

   preliminaries and related work
our approach capitalizes on a word thesaurus in order to define a measure of semantic relatedness
between words  and expands this measure to compute text relatedness using both semantic and
   omiotis is the greek word for relatedness or similarity 
   http   www aclweb org aclwiki index php title sat analogy questions

 

fit ext r elatedness based on a w ord t hesaurus

lexical information  in order to facilitate the understanding of our methodology we elaborate on
preliminary concepts in this section and present related research approaches 
    word thesauri and their use in text applications
word thesauri  like wordnet  fellbaum        or rogets international thesaurus  morris   hirst 
       constitute the knowledge base for several text related research tasks  wordnet has been
used successfully as a knowledge base in the construction of generalized vector space models
 gvsm  and semantic kernels for document similarity with application to text classification  such
as the works of mavroeidis  tsatsaronis  vazirgiannis  theobald and weikum         and basili 
cammisa and moschitti         and text retrieval  such as the works of voorhees         stokoe 
oakes and tait         and our previous work regarding the definition of a new gvsm that uses
word to word semantic relatedness  tsatsaronis   panagiotopoulou         furthermore  the idea
of using a thesaurus as a knowledge base in text retrieval has also been proven successful in the case
of cross language information retrieval  like for example in the case of the clir system introduced
by clough and stevenson         finally  the exploitation of word thesauri in linguistic tasks 
such as word sense disambiguation  wsd   ide   veronis        has yielded interesting results
 mihalcea   moldovan        tsatsaronis  vazirgiannis    androutsopoulos        tsatsaronis 
varlamis    vazirgiannis        
the application of a text relatedness measure to text classification and retrieval tasks should
first consider the impact of lexical ambiguity and wsd in the overall performance in these tasks 
sanderson              concludes that ambiguity in words can take many forms  but new test collections are needed to realize the true importance of resolving ambiguity and embedding semantic
relatedness and sense disambiguation in the text retrieval task  in the analysis of barzilay and elhadad         and barzilay  elhadad and mckeown        the impact of wsd in the performance
of text summarization tasks is addressed by considering all possible interpretations of the lexical
chains created from text  similar to this methodology  we tackle word ambiguity by taking into account every possible type of semantic information that the thesaurus can offer  for any given sense
of a text word 
from the aforementioned approaches  it is clear that the use of a word thesaurus can offer much
potential in the design of models that capture the semantic relatedness between texts  and consequently  it may improve the performance of existing retrieval and classification models under certain
circumstances that are discussed in the respective research works  mavroeidis et al         basili
et al         stokoe et al         clough   stevenson         the word thesaurus employed in the
development of omiotis is wordnet  its lexical database contains english nouns  verbs  adjectives
and adverbs  organized in sets of synonym senses  synsets   hereafter  the terms senses  synsets and
concepts are used interchangeably  synsets are connected with various links that represent semantic
relations between them  i e   hypernymy   hyponymy  meronymy   holonymy  synonymy   antonymy 
entailment   causality  troponymy  domain   domain terms  derivationally related forms  coordinate
terms  attributes  stem adjectives  etc    several relations cross parts of speech  like the domain
terms relation  which connects senses pertaining to the same domain  e g   light  as a noun meaning electromagnetic radiation producing a visual sensation  belongs to the domain of physics   to
the best of our knowledge  the proposed approach is the first that utilizes all of the aforementioned
semantic relations that exist in wordnet for the construction of a semantic relatedness measure 
 

fit satsaronis   varlamis     vazirgiannis

    creating semantic networks from word thesauri
omiotis is based on the creation of semantic paths between words in a text using the thesaurus
concepts and relations  early approaches in this field  used gloss words from the respective word
definitions in order to build semantic networks from text  veronis   ide         the idea of representing text as a semantic network was initially introduced by quilian         the expansion
of wordnet with semantic relations that cross parts of speech has added more possibilities of semantic network construction from text  more recent approaches to semantic network construction
from word thesauri  by mihalcea  tarau and figa        and navigli         utilize a wide range
of wordnet semantic relations instead of the gloss words  these methods outperformed previous
methods that used semantic networks in the all words wsd tasks of senseval   and   for the english language  palmer  fellbaum    cotton        snyder   palmer         in this work we adopt
the semantic network construction method that we introduced in the past  tsatsaronis et al         
the method utilizes all of the available semantic relations in wordnet  in the wsd task  the respective method outperformed or matched previous methods that used semantic networks in the all
words wsd tasks of senseval   and   for the english language  and this was largely due to the rich
representation that the semantic networks offered  section     introduces our semantic relatedness
measure 
    measures of semantic relatedness
semantic relatedness between words or concepts has been exploited  in the past  in text summarization  barzilay et al          text retrieval  stokoe et al         smeaton  kelledy    odonnell 
      richardson   smeaton        and wsd  patwardhan  banerjee    pedersen        tasks 
semantic relatedness measures can be widely classified to dictionary based    corpus based and hybrid 
among dictionary based measures  the measure of agirre and rigau        was one of the first
measures developed to compute semantic relatedness between two or more concepts  i e   for a set
of concepts   their measure was based on the density and depth of concepts in the set and on the
length of the shortest path that connects them  however  they assume that all edges in the path are
equally important 
the measure proposed by leacock  miller and chodorow        for computing the semantic
similarity between a pair of concepts takes into account the length of the shortest path connecting
them  measured as the number of nodes participating in the path  and the maximum depth of the
taxonomy  the measure for two concepts s  and s  can be computed as follows 
sim s    s      log

length
 d

   

where length is the length of the shortest path connecting s  and s  and d is the maximum depth of
the taxonomy used 
regarding hybrid measures  resniks              measure for pairs of concepts is based on the
information content  ic  of the deepest concept that can subsume both  least common subsumer  
and can be considered as a hybrid measure  since it combines both the hierarchy of the used thesaurus  and statistical information for concepts measured in large corpora  more specifically  the
   also found in the bibliography as knowledge based  thesaurus based  or lexicon based 

 

fit ext r elatedness based on a w ord t hesaurus

semantic similarity for a given pair of concepts s  and s    which have s  as their least common
subsumer  i e   least common ancestor   is defined in the following equation 
sim s    s      ic s   

   

where the information content  ic  of a concept  i e   s    is defined as 
ic s      logp  s   

   

and p  s    is the probability of occurrence of the concept s  in a large corpus 
the measure proposed by jiang and conrath         is also based on the concept of ic  given
two concepts s  and s    and their least common subsumer s    their semantic similarity is defined as
follows 
 
sim s    s     
   
ic s      ic s        ic s   
the measure of lin        is also based on ic  given  again  s    s    and s    as before  the
similarity between s  and s  is defined as follows 
sim s    s     

   ic s   
ic s      ic s   

   

hirst and st onge        reexamine the idea of constructing lexical chains between words 
based on their synsets and the respective semantic edges that connect them in wordnet  the initial
idea of lexical chains was first introduced by morris and hirst         who defined the lexical
cohesion of a passage  based on the cohesion of the lexical chains between the passages elements 
which acted as an indicator for the continuity of the passages lexical meaning 
we encourage the reader to consult the analysis of budanitsky and hirst        for a detailed
discussion on most of the aforementioned measures  as well as for more measures proposed prior to
the aforementioned  while all these measures use only the noun hierarchy  except from the measure
of hirst and st onge   the implementation of several of those measures provided by patwardhan 
banerjee and pedersen        in the publicly available wordnet  similarity package can also utilize
the verb hierarchy  still  the relations that cross parts of speech are not considered  as well as other
factors discussed in detail in section    in contrast  our measure defines the semantic relatedness
between any two concepts  independently of their part of speech  pos   utilizing all of the available
semantic links offered by wordnet 
more recent works of interest on semantic relatedness  include  the measures of jarmasz and
szpakowicz         who use rogets thesaurus to compute semantic similarity  by replicating a
number of wordnet based approaches  the lsa based measure of finkelstein et al          who
perform latent semantic analysis  landauer et al         to capture text relatedness and can be
considered as a corpus based method  the measure of patwardhan and pedersen         who utilize the gloss words found from the words definitions to create wordnet based context vectors 
the methods of strube and ponzetto            a   gabrilovich and markovitch         and milne
and witten        who use wikipedia to compute semantic relatedness and can also be considered
as corpus based approaches  and the method of mihalcea  corley and strappavara         which
is a hybrid method that combines knowledge based and corpus based measures of text relatedness 
other recent hybrid measures of semantic similarity are  the measure proposed by li et al         
who use information from wordnet and corpus statistics collected from the brown corpus  kucera 
 

fit satsaronis   varlamis     vazirgiannis

francis    caroll        to compute similarity between very short texts  and the measure for text distance proposed by tsang         that uses both distributional similarity and ontological knowledge
information to compute the distance between text fragments  distributional similarity is also used
in a supervised combination with wordnet based approaches  agirre  alfonseca  hall  kravalova 
pasca    soroa         to produce a supervised measure of semantic relatedness  li et al        
have created a new data set for their experimental evaluation  which we also use in section   to
evaluate our omiotis measure and compare against their approach 
in the following section we formally define omiotis and provide its details  from the creation of
the semantic links to the computation of relatedness between words and texts  we give evidence on
the measures complexity and justify our design choices  finally  we discuss potential applications
of the measure on text related tasks 

   measuring word to word and text to text semantic relatedness
this section presents the details of omiotis  our measure of text semantic relatedness  the measure
capitalizes on the idea of semantic relatedness between wordnet senses  extends it to compute
relatedness between words and finally between texts  since the definition of semantic relatedness
ranges from pairs of keyword senses to pairs of texts  omiotis is defined in a way that captures
relatedness in every granularity  as a result  it can be applied in a wide range of linguistic and
text related tasks such as wsd  word similarity and word analogy  text similarity  and keyword
ranking  the key points of the proposed measure are   a  it constructs semantic links between
all word senses in wordnet and pre computes a relatedness score between every pair of wordnet
senses   b  it computes the semantic relatedness for a pair of words by taking into account the
relatedness of their corresponding wordnet senses  and  c  it computes a semantic relatedness score
for any two given text segments by extending word to word relatedness  depending on the task  the
computation of semantic relatedness can be modified to take into account all or some of the senses
of each word  all or some of the words in each text  or to apply additional weights depending on
the word importance or sense importance in context  this allows omiotis to be adapted in various
text related tasks  without modifying the main process of computing relatedness  in section    
that follows  we formally define our semantic relatedness measure and in section     we provide a
detailed justification of our design decisions 
    construct semantic links between words
the first step in measuring the semantic relatedness between two text fragments  is to find the
implicit semantic links between the words of the two fragments  thus  we present a definition of
semantic relatedness for a pair of thesaurus concepts  which takes into account the semantic path
connecting the concepts  and expands it to measure the relatedness between words  in order to solve
the problem of constructing semantic paths between words  we base our approach on our previous
method on how to construct semantic networks between words  tsatsaronis et al         
      s emantic n etwork c onstruction from w ord t hesauri
figure   gives an example of the construction of a semantic network for two words ti and tj   for
simplicity reasons  we assume the construction of a semantic path between senses s i   and s j  
only  initial phase   though we could do the same for every possible combination of the two words
 

fit ext r elatedness based on a w ord t hesaurus

ti

s i  

s j  

s i  

s j  

   

   

s i  

s j  

synonym

   

holonym

meronym
s i  

tj

   

s j  

hypernym
antonym

   
hyponym

initial phase
index 

  word node

network expansion
  sense node

  semantic link

figure    constructing semantic networks from word thesauri 
senses  initially  the two sense nodes are expanded using all the semantic links offered by wordnet 
the semantic links of the senses  as found in the thesaurus  become the edges and the pointed senses
the nodes of the network  network expansion   the expansion process is repeated recursively until
the shortest   path between s i   and s j   is found  when no path is found from s i   to s j   then
the senses and consequently the words are not semantically related 
      s emantic r elatedness

between a

pair of c oncepts

the semantic relatedness for a pair of concepts is measured over the constructed semantic network 
it considers the path length  captured by compactness  and the path depth  captured by semantic
path elaboration  which are defined in the following  a measure for wsd based on the idea of
compactness was initially proposed by mavroeidis et al          the original measure used only
nouns and the hypernym relation  and is extended in the current work to support all of wordnets
relations and the noun  verb and adjective parts of speech  here we define a new compactness
measure  definition    as the core of the omiotis measure 
definition   given a word thesaurus o  a weighting scheme for the edges that assigns a weight
w         for each edge  a pair of senses s    s    s     and a path p of length l connecting
qthe two
senses  the semantic compactness of s  scm  s  o  p    is defined as  scm  s  o  p     li   wi  
where w    w         wl are the paths edges weights  if s    s  then scm  s  o  p        if there is
no path between s  and s  then scm  s  o  p       
note that compactness takes the path length into account and is bound in         higher compactness
between senses implies higher semantic relatedness  the intuition behind edge types weighting is
that certain types provide stronger semantic connections than others  considering that the lexicographers of wordnet tend to use some relation types more often than others  we assume that the most
used relation types are stronger than the types less used   a straightforward solution is to define edge
types weights in proportion to their frequency of occurrence in wordnet      the weights assigned
to each type using this solution are shown in table   and are in accordance to those found by song
et al          the table shows the probability of occurrence in wordnet     for every possible edge
type in the thesaurus  in descending order of probability values  a detailed analysis of the choices
we made in definition   and in the definitions that follow is performed in section     
the depth of nodes that belong to the path also affects term relatedness  a standard means of
measuring depth in a word thesaurus is the hypernym hyponym hierarchical relation for the noun
and adjective pos and hypernym troponym for the verb pos  for the adverb pos the related stem
   the details are presented in algorithm   

 

fit satsaronis   varlamis     vazirgiannis

wordnet     edge type

probability of occurrence

hypernym hyponym
nominalization
category domain
part meronym holonym
region domain
similar
usage domain
member meronym holonym
antonym
verb group
also see
attribute
entailment
cause
substance meronym holonym
derived
participle of

    
     
     
      
      
    
     
     
      
    
      
       
       
       
       
      
   e    

table    probability of occurrence for every edge type in wordnet     
adjective sense can be used to measure its depth  a path with shallow sense nodes is more general
compared to a path with deep nodes  this parameter of semantic relatedness between terms is
captured by the measure of semantic path elaboration introduced in the following definition 
definition   given a word thesaurus o   a pair of senses s    s    s     where s   s   o and
s     s   and a path p    p    p         pl   of length l  where either s    p  and s    pl or
s    pl and s    p    the semantic path elaboration of the path  sp e s  o  p    is defined as 
q
 
i di  
sp e s  o  p     li   d di  d
 dmax
  where di is the depth of sense pi according to o  and dmax
i  
the maximum depth of o  if s    s    then d    d    d and sp e s  o  p    
path from s  to s  then sp e s  o  p       

d
dmax  

if there is no

it is obvious in definition   that a path of length l comprises l   nodes  thus when i   l  di   is
the last node in the path  essentially  spe is the harmonic mean of the two depths normalized to the
maximum thesaurus depth  the harmonic mean is preferred over the average of depths  since it offers a lower upper bound and gives a more realistic estimation of the paths depth  compactness and
semantic path elaboration measures capture the two most important parameters of measuring semantic relatedness between terms  budanitsky   hirst         namely path length and senses depth
in the used thesaurus  we combine these two measures following the definition of the semantic
relatedness between two terms 
definition   given a word thesaurus o  and a pair of senses s    s    s    the semantic relatedness
of s  sr s  o   is defined as maxp  scm  s  o  p    sp e s  o  p    
 

fit ext r elatedness based on a w ord t hesaurus

algorithm   maximum semantic relatedness g  u  v  w 
  
  

  
  
  
  
  
  
  
   
   

   
   
   
   
   
   
   
   
   
   
   

input  a directed weighted graph g  two nodes u  v and a weighting scheme w   e         
output  the path from u to v with the maximum product of the edges weights 
initialize single source g  u 
for all vertices v  vg do
d v    
 v    n u ll
end for
d u     
relax u  v  w 
if d v    d u   w u  v  then
d v    d u   w u  v 
 v    u
end if
maximum relatedness g  u  v  w 
initialize single source g  u 
s 
q   vg
while v  q do
s   extract from q the vertex with the maximum d
s  ss
for all vertices k  adjacency list of s do
relax s  k  w 
end for
end while
return the path following all the ancestors  of v back to u

given a word thesaurus  there can be more than one semantic path connecting two senses  the
senses compactness can take different values for all the different paths  in these cases  we use the
path that maximizes the semantic relatedness  for its computation we introduce algorithm    which
is a modification of dijkstras algorithm  cormen  leiserson    rivest        for finding the shortest path between two nodes in a weighted directed graph  in the algorithm  g is the representation
of the directed weighted graph given as input  e g   using adjacency lists   and vg is the set of all
the vertices of g  also  two more sets are used  s  which contains all the vertices for which the
maximum semantic relatedness has been computed from the source vertex  i e   from u   and q 
which contains all the vertices for which the algorithm has not computed yet the maximum relatedness from the source vertex  furthermore  three tables are used  d  which  for any vertex v stores the
maximum semantic relatedness found at any given time of the algorithm execution from the source
vertex  i e   u in d v     which for any vertex v stores its predecessor in  v   and w  which stores
the edge weights of the graph  e g   w k  m  stores the edge weight of the edge that starts from k
and goes to m  
 

fit satsaronis   varlamis     vazirgiannis

the algorithm comprises three functions   a  initialize single source g  u   which initializes
tables d and   for every vertex v of the graph  more precisely  it sets d v      since the semantic relatedness from the source is unknown at the beginning  and because the algorithm seeks
for the maximum semantic relatedness this is initially set to the minimum value  i e      it
also sets  v    n u ll  since at the beginning of the algorithm execution we are not aware
yet of the predecessor of any vertex v following the path from the source vertex u to v that results to the maximum semantic relatedness   b  relax u  v  w   which given two vertices  u and v
that are directly connected with an edge of weight w u  v   it updates the value d v   in case that
if we follow the edge  u  v  this results to a higher semantic relatedness for vertex v from the
source  compared to the value we have computed up to that time of the algorithm execution  and
 c  maximum relatedness g  u  v  w   which uses the aforementioned functions and executes the
dijkstras algorithm  the proof of the algorithms correctness follows in the next theorem 
theorem   given a word thesaurus o  an edges weighting function w   e          where a
higher value declares a stronger edge  and a pair of senses s ss   sf   declaring source  ss   and destination  sf   vertices  then the scm  s  o  p    sp e s  o  p   is maximized for the path returned
 di dj
  w 

by algorithm    by using the weighting scheme wij
ij dmax  di  dj     where wij is the new weight
of the edge connecting senses si and sj  
proof   we will show that for each vertex sf  vg   d sf   is the maximum product of edges weight
through the selected path  starting from ss   at the time when sf is inserted into s  from now on 
the notation  ss   sf   will represent this product  path p connects a vertex in s  namely ss   to a
vertex in vg  s  namely sf   consider the first vertex sy along p such that sy  vg  s and let
sx be ys predecessor  now  path p can be decomposed as ss  sx  sy  sf   we claim that
d sy      ss   sy   when sf is inserted into s  observe that sx  s  then  because sf is chosen
as the first vertex for which d sf       ss   sf   when it is inserted into s  we had d sx      ss   sx  
when sx was inserted into s 
because sy occurs before sf on the path from ss to sf and all edge weights are nonnegative
and in        we have  ss   sy     ss   sf    and thus d sy      ss   sy     ss   sf    d sf    but
both sy and sf were in v  s when sf was chosen  so we have d sf    d sy    thus  d sy    
 ss   sy      ss   sf     d sf    consequently  d sf      ss   sf   which contradicts our choice of sf  
we conclude that at the time each vertex sf is inserted into s  d sf      ss   sf   
next  to prove that the returned maximum product is the scm  s  o  p    sp e s  o  p    let
the path between ss and sf with the maximum edge weight product have k edges  then  algorithm  
q
 dk df
 d  d 
 ds d 

returns the maximum ki   wi i   
  ws   dmax
 ds  d    w    dmax  d   d       wkf  dmax  dk  df    
qk  di di  
qk
 
i   di  di    dmax   scm  s  o  p    sp e s  o  p   
i   wi i    
      s emantic r elatedness

for a

pair of t erms

based on definition    which measures the semantic relatedness between a pair of senses s  we can
define the semantic relatedness between a pair of terms t  t    t    as follows 
definition   let a word thesaurus o  let t    t    t    be a pair of terms for which there are entries
in o  let x  be the set of senses of t  and x  be the set of senses of t  in o  let s    s         s x    x   
be the set of pairs of senses  sk    si   sj    with si  x  and sj  x    now the semantic relatedness
of t  sr t  s  o   is defined as 
  

fit ext r elatedness based on a w ord t hesaurus

maxsk  maxp  scm  sk   o  p    sp e sk   o  p       maxsk  sr sk   o  
for all k       x      x     semantic relatedness between two terms t    t  where t   t   t and
t
  o is defined as    semantic relatedness between t    t  when t   o and t  
  o  or vice versa 
is considered   
for the remaining of the paper  the sr t  s  o  for a pair of terms will be denoted as sr t    to
ease readability 
    analysis of the sr measure
in this section we present the rationale behind the definitions       and    by providing theoretical
and or experimental evidence for the decisions made on the design of the measure  we illustrate
the advantages and disadvantages of the different alternatives using simple examples and argue for
our decisions  finally  we discuss on the advantages of sr against previous measures of semantic
relatedness 
the list of decisions made for the design of our semantic relatedness measure comprises  a 
use of senses in all pos  instead of noun senses only  b  use of all semantic edge types found in
wordnet  instead of the is a relation only  c  use of edge weights  and d  use of senses depth as
a scaling factor  it is important to mention that measures of semantic relatedness differ from the
measures of semantic similarity  which traditionally use hierarchical relations only and ignore all
other type of semantic relations  in addition  both concepts differentiate from semantic distance  in
the sense that the latter is a metric 
      u se all pos i nformation
firstly  we shall argue on the fact that the use of all pos in designing a semantic relatedness measure is important  and can increase the coverage of such a measure  the rationale supporting this
decision is fairly simple  current data sets for evaluating semantic relatedness or even semantic similarity measures are restricted to nouns  like for example the rubenstein and goodenough    word
pairs         the miller and charles    word pairs         and the word similarity     collection
 finkelstein et al          thus  the experimental evaluation in those data sets cannot pinpoint the
caveat of omitting the remaining parts of speech  however  text similarity tasks and their benchmark
data sets comprise more than nouns  throughout the following analysis  the reader must consider
that the resulting measure of semantic relatedness among words is destined to be embedded in a
text to text semantic relatedness  as shown in the next section 
the following two sentences are a paraphrase example taken from the microsoft paraphrase
corpus  dolan  quirk    brockett        and show the importance of using other pos as well 
such as verbs 
the charges of espionage and aiding the enemy can carry the death penalty 
if convicted of the spying charges he could face the death penalty 

words that appear in wordnet are written in bold and stopwords have been omitted for simplicity   
the two sentences have many nouns in common  charges  death  penalty   but there are also pairs
of words between these two sentences that can contribute the evidence that these two sentences are
   the stopwords list that we used is available at http   www db net aueb gr gbt resources stopwords txt

  

fit satsaronis   varlamis     vazirgiannis

a paraphrase  for example espionage and spying have an obvious semantic relatedness  as well
as enemy and spying  also  convicted and charges  as well as convicted and penalty  this type
of evidence would have been disregarded by any measure of semantic relatedness or similarity that
uses only the noun pos hierarchy of wordnet  examples of such measures are  the measure of
sussna         wu and palmer         jiang and conrath         resnik               and the
wordnet based component of the measure proposed by finkelstein et al          from this point
of view  the decision to use all pos information expands the potential matches found by the measure and allows the use of the measure in more complicated tasks  like paraphrase recognition  text
retrieval  and text classification 
      u se e very t ype of s emantic r elations
the decision to use all parts of speech in the construction of the semantic graphs  as it was introduced in our previous work  tsatsaronis et al          imposes the involvement of all semantic
relations instead of merely taxonomic  is a  ones  moreover  this decision was based on evidence
from related literature  the work of smeaton et al         provides experimental evidence that measuring semantic similarity by incorporating non hierarchical link types  i e  part meronym holonym 
member meronym holonym  substance meronym holonym  improves much the performance of such
a measure  the experimental evaluation was conducted by adopting a small variation of the resniks
measure        
hirst and st onge        reported that they have discovered several limitations and missing
connections in the set of wordnet relations during the construction of lexical chains from sentences
for the detection and correction of malapropisms  they provided the following example using the
pair of words in bold to report this caveat 
school administrators say these same taxpayers expect the schools to provide child care and
school lunches  to integrate immigrants into the community  to offer special classes for adult
students  

the intrinsic connection between the nouns child care and school  which both exist in wordnet 
cannot be discovered by considering only hierarchical edge types  this connection is depicted in
figure    which shows the path in wordnet  our rich semantic representation is able to detect such
connections and address problems of the aforementioned type 
      u se w eights

on

e dges

the work of resnik        reports that simple edge counting  which implicitly assumes that links
in the taxonomy represent uniform distances  is problematic and is not the best semantic distance
measure for wordnet  in a similar direction lie the findings of sussna         who has performed
thorough experimental evaluation by varying edge weights in order to measure semantic distance
between concepts  sussnas findings  revealed that weights on semantic edges are a non negligible
factor in the application of his measure for wsd  and that the best results were reported when an
edge weighting scheme was used  instead of assigning each edge the same weight  for all these
reasons  we decided to assign a weight on every edge type  and we chose the simple probability
of occurrence for each edge type in wordnet  as our edge weighting scheme  see table     this
very important factor is absent in several similarity measures proposed in the past  such as in the
measures of leacock et al          jarmasz and szpakowicz        and banerjee and pedersen
        which are outperformed in experimental evaluation by our measure 
  

fit ext r elatedness based on a w ord t hesaurus

activity
 noun 
hyponym
hypernym

education
 noun 

aid
 noun 

nominalization

educate
 verb 

hyponym
hypernym

service
 noun 

school
 verb 

nominalization

school
 noun 

hypernym

child care
 noun 

figure    semantic path from child care to school following wordnet edges 
instrumentality
 noun 

conveyance
 noun 

hyponym
hypernym

hypernym
implement
 noun 
container
 noun 

vehicle
 noun 

hyponym

public
transport
 noun 

hyponym
hypernym
bar
 noun 

hypernym

hyponym

wheeled
vehicle
 noun 

autobus
 noun 

hyponym
wheeled
vehicle
 noun 

hypernym
lever
 noun 

self propelled
vehicle
 noun 
category domain

hyponym
hypernym

car
 noun 

hypernym

pedal
 noun 

motor vehicle
 noun 
passenger
 noun 

part meronym
hyponym
hypernym

accelerator
 noun 

car
 noun 

category domain

nwpl path
pr path

figure    product relatedness  pr  and normalized weighted path length  nwpl  paths for pairs 
car and accelerator  left   car and autobus  right  

      u se d epth s caling factor
our decision to incorporate the depth scaling factor  spe in definition    in the edge weighting
mechanism has been inspired by the thorough experimental evaluation conducted by sussna        
  

fit satsaronis   varlamis     vazirgiannis

which has provided evidence on the importance of the edge weighting factor in semantic network
based measures  according to our experiments on the miller and charles data set the spearman
correlation with human judgements was much lower    percentage points  when omitting the depth
scaling factor than when adopting the spe factor  see definition    
      j ustification of sr d efinitions
according to definition    the semantic compactness for a pair of concepts is the product of depthscaled weights of the edges connecting the two concepts  the use of product instead of sum or
normalized sum of edges weights is explained in the following 
since there might be several paths connecting the two concepts  definition   clearly selects
the path that maximizes the product of semantic compactness  sc  and semantic path elaboration  spe   for simplicity  we ignore the effect of the depth scaling
factor  spe in definition   
q
and consequently  our aim is to find the path that maximizes li   ei   where e    e         el are the
 non depth scaled  weights of edges in the path connecting two given concepts  let us name this
less elaborate version of our semantic relatedness measure after product relatedness  pr   where
p r s  o    maxp  scm  s  o  p     an alternative would have beenpto define semantic coml

e

i
pactness as the normalized sum of the weights in the path  which is  i  
  in this case  the
l
semantic relatedness would be measured on the path that maximizes the latter formula  since by
nature  semantic relatedness always seeks to find the path that maximizes the connectivity between
two concepts  let us name this alternative after normalized weighted path length  nwpl  
in the example of figure    we show how pr and nwpl compute the semantic relatedness for
the term pair car and accelerator  left  and car and autobus  right   the path that maximizes the
respective formulas of pr and nwpl using algorithm   and edge weights in table    is illustrated in
figure   using black and white arrows respectively  for the pair car and accelerator the sum based
formula  normalized against the path length  selects a very large path in this example  with a final
computed relatedness of       which is the weight of the hypernym hyponym edges  pr finds that
the path maximizing the product is the immediate part meronym relation from car to accelerator 
with a computed relatedness of         which is the weight of the part meronym edges  the main
problem arising with nwpl is the fact that it cannot distinguish among the relatedness between
any pair of concepts in the hypernym hyponym hierarchy of wordnet  in this example  nwpl
computes the same relatedness        between every possible concept pair shown in the top figure 
in contrast  pr is able to distinguish most of these pairs in terms of relatedness  more precisely  this
behavior of pr is due to the fact that it embeds the notion of the path length  since the computed
relatedness decays by a factor in the range        for every hop made following any type of semantic
relation  another example  that also shows the importance of considering all wordnet relations  is
the one shown on the right part of figure    where nwpl and pr paths have been computed for the
term pair car and autobus  again  nwpl selects a very large path  and does not incline from the
hypernym hyponym tree 
clearly  nwpl would rather traverse through a huge path of hypernym hyponym edges  than
following any other less important edge type  which would decrease its average path importance 
this behavior creates serious drawbacks   a  lack of ability to distinguish relatedness among any
pair of concepts in the same hierarchy  and  b  large increase of the actual computational cost of
algorithm    due to the fact that it will tend not to incline from the hypernym hyponym hierarchy 
even if there is a direct semantic edge  other than hypernym hyponym  connecting the two concepts 

  

fit ext r elatedness based on a w ord t hesaurus

like shown in figure    furthermore  by conducting experiments with nwpl in the    word pairs
of miller and charles  we discovered that in almost     of the cases  nwpl produces the same
value of semantic relatedness  equal to       being unable to distinguish them and creating many
ties  thus  pr is a better option to use in our measure  as the semantic compactness factor 
last  but not least  regarding the overall design of sr  we should mention that the proposed measure is solely based on the use of wordnet  in contrast to measures of semantic relatedness that use
large corpora  such as wikipedia  although  such measures  like the ones proposed by gabrilovich
and markovitch         and ponzetto and strube      a   provide a larger coverage regarding concepts that do not reside in wordnet  they require the processing of a very large corpora  wikipedia  
which also changes very fast and very frequently  experimental evaluation in section   shows that
our measure competes well against the aforementioned word to word relatedness measures in the
used data sets  in the following section  we introduce omiotis  the extension of sr for measuring
text to text relatedness 
    omiotis
to quantify the degree to which two text segments semantically relate to each other  we build upon
the sr measure  which we significantly extend in order to account not only for the terms semantic
relatedness but also for their lexical similarity  this is because texts may contain overly specialized
terms  e g   an algorithms name  that are not represented in wordnet  therefore  relying entirely on
the term semantics for identifying the degree to which texts relate to each other would hamper the
performance of our approach  on the other hand  semantics serve as complement to our relevance
estimations given that different text terms might refer to  nearly   identical concepts 
to quantify the lexical similarity between two texts  e g   text a and b  we begin with the estimation of the terms importance weights as these are determined by the standard tf idf weighting
scheme  salton  buckley    yu        
thereafter  we estimate the lexical relevance  denoted as a b between terms a  a and b  b
based on the harmonic mean of the respective terms tf idf values  given by 
a b  

   t f idf  a  a   t f idf  b  b 
t f idf  a  a    t f idf  b  b 

   

harmonic mean is preferred instead of average  since it provides a more tight upper bound  li 
       this decision is based on the fact that t f idf  a  a  and t f idf  b  b  are two different
quantities measuring the qualitative strength of a and b in the respective texts 
having computed the lexical relevance between text terms a and b  we estimate their semantic
relatedness  i e  sr a  b  as described previously  based on the estimated lexical relevance and
semantic relatedness between pairs of text terms  our next step is to find for every word a in text a
the corresponding word b in text b that maximizes the product of semantic relatedness and lexical
similarity values as given by equation   
b   arg max a b  sr a  b  

   

bb

where b corresponds to that term in text b  which entails the maximum lexical similarity and
semantic relatedness with term a from text a   in a similar manner  we define a   which corresponds
   the function argmax selects the case from the examined ones  that maximizes the input formula of the function 

  

fit satsaronis   varlamis     vazirgiannis

to that term in text a  which entails the maximum lexical similarity and semantic relatedness with
term b from text b 
a   arg max a b  sr a  b  
   
aa

consequently  we aggregate the lexical and semantic relevance scores for all terms in text a 
with reference to their best match in text b denoted as shown in equation   

 
x
 
 a  b   
   
a b  sr a  b  
 a 
aa

we do the same for the opposite direction  i e  from the words of b to the words of a  to cover
the cases where the two texts do not have an equal number of terms 
finally  we derive the degree of relevance between texts a and b by combining the values
estimated for their terms that entail the maximum lexical and semantic relevance to one another 
given by 
  a  b     b  a  
    
 
algorithm   summarizes the computation of omiotis  its computation entails a series of steps 
the complexity of which is discussed in section     
omiotis a  b   

    applications of semantic relatedness
in this section we describe the methodology of incorporating semantic relatedness between pairs of
words or pairs of text segments  into several applications 
      w ord   to  w ord s imilarity
rubenstein and goodenough        obtained synonymy judgements from    human subjects on   
pairs of words  in an effort to investigate the relationship between similarity of context and similarity of meaning  synonymy   since then  the idea of evaluating computational measures of semantic
relatedness by comparing against human judgments on a given set of word pairs  has been widely
used  and even more data sets were developed  the proposed measure of semantic relatedness
between words  sr   introduced in definition    can be used directly in such a task  in order to
evaluate the basis of omiotis measure  which is the measurement of word to word semantic relatedness  the application is straightforward  let n be all pairs of words in the used word similarity data
set  then  the semantic relatedness for every pair is computed  using sr t  s  o  as defined in   
the computed values are sorted in a descending order  and the produced ranking of similarities is
compared against the gold standard ranking of humans  using spearman correlation  the scores
can be used to compute pearsons product moment correlation  additional measures of semantic
relatedness can be compared against each other by examining the respective correlation values with
human judgements 
      sat a nalogy t ests
the problem of identifying similarities in word analogies among pairs of words is a difficult problem
and it has been standardized as a test for assessing the human ability for language understanding 
  

fit ext r elatedness based on a w ord t hesaurus

algorithm   omiotis a b  sem  lex  
   input  two texts a and b  comprising m and n terms each  a and b are terms from a and b
respectively  
a semantic relatedness measure sem   sr a  b          
a weighting scheme of term importance in a text lex   t f idf  a  a         
   output  find the pair of terms that maximizes the product of sem and lex values 

  
  
  
  
  
  
  
   
   
   
   
   
   
   

compute zeta a b 
sum a      
for all terms a  a do
b    n u ll
t empzeta     
for all terms b  b do
a b    lex a a lex b b 
lex a a  lex b b 
if t empzeta   a b  sem a  b  then
t empzeta   i j  sem a  b 
b   b
end if
end for
sum a     sum a    t empzeta
end for
zeta a  b     sum a   a 
compute omiotis a b 

   

omiotis a  b    

zeta a b  zeta b a 
 

under the scope of the well known sat analogy tests  scholastic aptitude tests   sat tests are
used as admission tests by universities and colleges in the united states  the participants aim is to
locate out of the five given word pairs the one that presents the most similar analogy to the target
pair 
although it is difficult for machines to model the human cognition of word analogy  several
approaches exist in the bibliography that attempt to tackle this problem  previous approaches can be
widely categorized into  corpus based  lexicon based and hybrid  some examples of corpus based
are the approaches of turney      b  and bicici and yuret         examples of lexicon based
approaches  are those of veale        and the application of the lexicon based measure by hirst
and st onge        in sat  that can be found in the work of turney         hybrid approaches are
applied in sat  through the application of the measures of resnik        and lin        that can
also be found in the work of turney        
in order for the reader to understand the difficulty of answering sat questions  we must point
out that the average us college applicant scores      turney   littman         while the top
corpus based approach scores        turney         the top lexicon based scores      veale 
      and the top hybrid scores        resnik        
  

fit satsaronis   varlamis     vazirgiannis

another way of categorizing the approaches that measure semantic similarity in analogy tasks
is to distinguish among attributional and relational similarity measures  gentner          representative approaches of the first category are lexicon based approaches  while paradigms of relational
similarity measures can be found in approaches based on latent relational analysis  lra   turney 
       it is of great interest to point out that lra based approaches  like the lrme algorithm proposed recently by turney      a   are superior to attributional similarity approaches in discovering
word analogies  this fact is also supported by the experimental findings of turney         without
doubt  relational similarity approaches may perform better in the sat analogy task  but still  as
shown later in the experiments we conducted in other applications  like paraphrase recognition  the
lexicon based measures can outperform lra based approaches in such tasks 
semantic relatedness  sr  between words  as applied in omiotis  can be exploited to solve the
word analogy task  the aim of word analogy is  given a pair of words w  and w    to identify the
series of semantic relations that lead from w  to w   semantic path   in the sat test  the target pair
 w   w    and candidate word pairs  w k  w k    with k usually being from   to    are processed in order
to find each pairs analogy  the aim is to locate the pair k  which exposes maximum similarity to w 
and w    a straightforward method to choose among the   candidate pairs is to employ two criteria 
at first  the k analogies to the analogy of the target pair can be compared  and then the candidate that
shows by far the most similar analogy can be selected  however  when the most similar analogy is
not obvious  all the   pairs may be examined together in order for the slightest differences that lead
to the correct answer to be discovered  we attempt to model human cognition of this task using sr
in a two fold manner   a  we measure sr to capture the horizontal analogy between the given pair
and the possible candidate pairs  and  b  we measure sr to capture the vertical analogy between
the given pair and the possible candidate pairs  these two aspects are covered by the following
equations    to     to capture the horizontal analogy between a pair of words and a candidate pair 
we measure the difference of the sr score between the two words respectively as shown 
s   w k   w k         sr w    w     sr w k   w k   

    

essentially  s  expresses the horizontal analogy of the candidate pair  w k   w k   with the given
pair  w    w     similarly  we capture the notion of the vertical analogy between the two pairs by
computing the difference of the sr scores among the two pairs words  as follows 
s   w k   w k         sr w    w k    sr w    w k   

    

finally  we rank candidates depending on the combined vertical and horizontal analogy they have
with the given pair  according to the following equation 
s w k   w k    

s   w k   w k     s   w k   w k  
 

    

eventually  we select the candidate pair with the maximum combined score  taking into account
both aspects  horizontal and vertical  of analogy between the given and the candidate pairs 
the intuition behind the selection of the these two scores for handling the sat test  is the
following  the order of the words in the pairs  both target and candidates  is not random  usually 
given a pair  w    w     and the candidate pairs  w k   w k   the test is solved if one can successfully
   two objects  x and y  are attributionally similar when the attributes of x are similar to the attributes of y  two pairs  a b and c d 
are relationally similar when the relations between a and b are similar to the relations between c and d 

  

fit ext r elatedness based on a w ord t hesaurus

stem  wallet   money

choices 

 a 

safe   lock

 b 

suitcase   clothing

s         
s       e   
 c 

camera   film

s         
s        
 d 

setting   jewel

s         
s       e   
 e 

car   engine

s         
s       e   

winner based on s   horizontal analogy   b
winner based on s   vertical analogy   b
winner based on combined s  b
correct answer  b

s         
s         

figure    example of computing the semantic relatedness measure  sr  in a given scholastic aptitude test  sat  question 

find the analogy  w k is to w k what w  is to w    from this perspective  s  and s  try to find the
candidate pair that best aligns with the target pair  figure   illustrates these two types of analogies
 horizontal and vertical  for an example sat question 
in order to motivate more our selection of s  and s  for answering sat questions  we will
discuss in more detail how these two quantities pertain to the concepts of strength and type of
the relations between a pair of sat words  turney        describes a method for comparing the
relations between candidate word pairs and the stem word pair  in which he utilizes the type of
the relation connecting the words in each pair and finally selects the pair that best matches the
type of the relation connecting the words in the stem pair  though we do not explicitly examine
the label of the edges connecting the words in each pair  implicitly we do so by computing sr
between them  since our weighting of the wordnet edges is fine grained  and distinguishes every
type of semantic relation in wordnet  instead of labels  we are using edge weights  sr definition
can provide a fine grained distinguishment between two pairs of words  depending on the types of
the edges connecting the words respectively  which is expressed by their weights  and also taking
into account other factors  like the depth of the nodes comprising their connecting path inside the
thesaurus  besides s    which attempts to capture the aforementioned properties between word pairs 
s  attempts the same between the words of the same order among two word pairs  i e   the first word
from the first pair  with the second word from the second pair   this forms an attempt to capture
how aligned are two word pairs  according to their sr values between their words 
      paraphrase r ecognition

and

s entence   to  s entence s imilarity

performance of applications relying on natural language processing may suffer from the fact that
the processed documents might contain lexically different  yet semantically related  text segments 
the task of recognizing synonym text segments  which is better known as paraphrase recognition 
or detection  is challenging and difficult to solve  as shown in the work of pasca         the task
itself is important for many text related applications  like summarization  hirao  fukusima  oku  

fit satsaronis   varlamis     vazirgiannis

mura  nobata    nanba         information extraction  shinyama   sekine        and question
answering  pasca         we experimentally evaluate the application of omiotis in the paraphrasing
detection task  section       using the microsoft research paraphrase corpus  dolan et al         
the application of omiotis in paraphrase detection is straightforward  given a pair of text segments 
we compute the omiotis score between them  using equation    and algorithm    higher values
of omiotis for a given pair denote stronger semantic relation between the examined text segments 
the task is now reduced to define a threshold  above which an omiotis value can be considered as
a determining sign of a paraphrasing pair  in the experimental evaluation of omiotis  we explain in
detail how we have selected this threshold for the paraphrase recognition task 
in a similar manner  by using equation    and algorithm    the semantic relatedness scores for
pairs of sentences can be computed  for this task  we are using the data set of li et al         to
evaluate omiotis  comprising    sentence pairs  for which human scores are provided  in section  
we describe in detail the experimental set up 
    complexity and implementation issues
the computation of omiotis entails a series of steps  the complexity of which is strongly related to its
base measure of semantic relatedness  sr   primarily  given two words  w  and w  the construction
time of the semantic network used to compute sr according to algorithm    has been proven to
be o    k l      tsatsaronis et al          where k is the maximum branching factor of the used
thesaurus nodes and l is the maximum semantic path length in the thesaurus  once the semantic
network is constructed  the complexity of algorithm   is reduced to the standard time complexity
cost of dijkstras algorithm  using fibonacci heaps  it is possible to alleviate the computational
burden of dijkstra and further improve time complexity  in the semantic network  dijkstra takes
o nl   md   ne   where n is the number of nodes in the network  m the number of edges  l is
the time for insert  d the time for decrease key and e the time for extract min  if fibonacci heaps
are used then l   d   o    and the cost of extract min is o logn   thus significantly reducing
the cost of execution  this whole procedure is repeated    n   n  times for the computation of
omiotis between two documents d  and d  having in total n  and n  distinct words respectively 
from the aforementioned  it is obvious that the computation of omiotis is not cheap in general 
for this purpose  and in order to improve the systems scalability  we have pre computed and stored
all sr values between every possible pair of synsets in a rdbms  this is a one time computation
cost  which dramatically decreases the computational complexity of omiotis  the database schema
has three entities  namely node  edge and paths  node contains all wordnet synsets  edge indexes
all edges of the wordnet graph adding weight information for each edge computed using the sr
measure  finally  paths contains all pairs of wordnet synsets that are directly or indirectly connected in the wordnet graph and the computed relatedness  these pairs were found by running a
breadth first search  bfs  starting from all wordnet roots for all pos  table   provides statistical
information for the rdbms which exceeds     gbytes in size  column   indicates the number of
distinct synsets examined  column   shows the total number of the edges  and column   depicts the
number of the connected synsets  by at least one path following the offered wordnet edges   the
current implementation takes advantage of the database structures  indices  stored procedures etc 
in order to decrease the computational complexity of omiotis  the following example is indicative
of the complexity of sr computation  the average number of senses per term is between   and  
 
 depending on the pos   for a pair of terms of known pos  we perform n   n     combinations
  

fit ext r elatedness based on a w ord t hesaurus

distinct synsets
       

total edges
       

connected synset pairs
             

table    statistics of the wordnet graph in the implemented database 

and for each pair of synsets we compute the similarity as presented in definition    when these
similarities are pre computed  the time required for processing     pairs of terms is    sec  which
makes the computation of omiotis feasible and scalable  as a proof of concept  we have developed
an on line version of the sr and the omiotis measures    where the user can test the term to term
and sentence to sentence semantic relatedness measures  tsatsaronis et al         

   experimental evaluation
the experimental evaluation of omiotis is two fold  first  we test the performance of the wordto word semantic relatedness measure  sr   in which omiotis is based  in three types of tasks   a 
word to word similarity and relatedness   b  synonym identification  and  c  scholastic aptitude
test  sat   second  we evaluate the performance of omiotis in two tasks   a  sentence to sentence
similarity  and  b  the paraphrase recognition task 
    evaluation of the semantic relatedness  sr  measure
for the evaluation of the proposed semantic relatedness measure between two terms we experimented on three different categories of tests  the first category comprises data sets that contain
word pairs  for which human subjects have provided similarity scores or relatedness scores  the
provided scores create a ranking of the word pairs  from the most similar to the most irrelevant  we
evaluate the performance of measures  by computing the correlation between the list of the human
rankings and the list produced by the measures  in this task  we evaluate the performance of sr in
three benchmark data sets  namely the rubenstein and goodenough    word pairs         r g  
and the miller and charles    word pairs         m c   for which humans have provided similarity scores  and  also  in the word similarity     collection  finkelstein et al              c   which
comprises     word pairs  for which humans have provided relatedness scores 
the second category of experiments comprises synonym identification tests  in these tests  given
an initial word  the most appropriate synonym word must be identified among the given options 
in this task we evaluate the performance of sr in the toefl data set  comprising    multiple
choice synonym questions  and the esl data set  comprising    multiple choice synonym questions
questions  
the third category of experiments is based on the scholastic aptitude test  sat  questions  in
sat  given a pair of words  the most relevant pair among five other given pairs must be selected  this
task is based on word analogy identification  the evaluation data set comprises     test questions 
   publicly available at http   omiotis hua gr
   http   www aclweb org aclwiki index php title toefl synonym questions
http   www aclweb org aclwiki index php title esl synonym questions  state of the art 

  

fit satsaronis   varlamis     vazirgiannis

category
lexicon based

corpus based

hybrid

method
hs
lc
js
gm
wlm
sp
is a sp
jc
l
r
hr
sr

r g
spearmans  pearsons r
     
     

     
     
n a
     

     
n a

    
n a
n a
    
n a
    

     
     

    
     
      
     
     
n a
      
     

m c
spearmans  pearsons r
     
     

     
     
n a
     
     
n a

    
n a
n a
    
n a
    
     
    
     
     
     
     
     
n a
     
     

table    spearmans and pearsons correlations for the rubenstein and goodenough  r g  and
miller and charles  m c  data sets  confidence levels                        

      e valuation

on

s emantic s imilarity

and

r elatedness

for the first category of experiments  we compared our measure against ten known measures of
semantic relatedness  hirst and st onge         hs   jiang and conrath         jc   leacock
et al          lc   lin         l   resnik               r   jarmasz and szpakowicz         js  
gabrilovich and markovitch               gm   milne and witten         wlm   finkelstein et al 
        lsa   hughes and ramage         hr   and strube and ponzetto            a   sp   for
the measure of strube and ponzetto we have also included the results of a version of the measure that
is only based on is a relations  ponzetto   strube      b   is a sp   for each measure  including
our own measure  sr   we have computed both the spearman rank order correlation coefficient
   and the pearson product moment correlation coefficient  r   with  being derived from r  since
for the computation of  the relatedness scores are transformed into rankings  both correlation
coefficients are computed based on the relatedness scores and rankings provided by humans in
all three data sets  the relatedness scores create a ranking of the pairs of words  based on their
similarity   for the measures hs  jc  lc  l and r  the rankings and the relatedness scores of the
word pairs for the r g and the m c data sets  are given in the work of budanitsky and hirst
        for the js measure  the r value is given in the work of jarmasz and szpakowicz       
for the r g and the m c data sets  and the  value is given in the work of gabrilovich and
markovitch         for the gm measure the  values are given in the work of gabrilovich and
markovitch         for the wlm measure the  values are given in the work of milne and witten
        for the lsa method the  value is given in the work of gabrilovich and markovitch        
only for the     c data set  for the hr measure the  values are given in the work of hughes and
ramage         finally  for the sp measure the r values are given in the work of ponzetto and
strube      a   and for the is a sp are given in the work of ponzetto and strube      b  
  

fit ext r elatedness based on a w ord t hesaurus

in table   we show the values of  and r for the r g and the m c data sets and for sr
and the compared measures  the human scores for all pairs of words for the two data sets can
be found in the analysis of budanitsky and hirst         note that the m c data set is a subset
of the r g data set  in some cases  the computation of  or r was not feasible  due to missing
information regarding the detailed rankings or relatedness scores for the respective measures  in
these cases the table has the entry n a  also the lsa measure is omitted in this table because 
and r were not reported in the literature for these two data sets  we have also conducted a statistical
significance test on the difference between sr correlations and the respective correlations of the
compared measures  using fishers z transformation  fisher         for each reported number  the
symbol  indicates that the difference between the correlation produced by sr and the respective
measure is statistically significant at the      confidence level  p          the symbol  indicates
the same at the      confidence level  p         and  finally  the symbol  indicates statistical
significance of the correlations difference at the      confidence level  p          in cases when
the difference is not statistically significant in any of those confidence levels  there is no indicating
symbol 
in table   we show the values of  and r for the     c data set  the reason we present the results
of the experiments in the     c data set in another table than the respective results of the r b and
m c data sets is that this collection focuses on the concept of semantic relatedness  rather than on
the concept of semantic similarity  gabrilovich   markovitch         relatedness is more general
concept than similarity  as argued in the analysis of budanitsky and hirst         thus  it can be
argued that the humans in the     c thought differently when scoring  compared to the case of the
r b and m c data sets  the detailed human scores for the     c data set are made available with
the collection     the measures l  jc and hs are omitted  because no information was available for
computing  or r values  as a further remark regarding the     c collection  we need to add the fact
that there are cases where the inter judge correlations may fall below      while r b and m c
data sets have inter judge correlations between      and       again  statistical significance tests
have been conducted using the fishers z transformation  regarding the difference of sr correlations
and the correlations of the compared measures  the used symbols that indicate the level of the
statistical significance are the same as previously  with regards to the reported correlations for the
r g and m c data sets  it is shown that sr performs very well  since in the majority of the cases
sr has higher correlation compared to the other measures of semantic relatedness or similarity
of any category  knowledge based  corpus based or hybrid   in the r g data set sr reports the
highest  and r correlations  in the m c data set sr has the second highest  correlation  the
hr measure has the highest  correlation  but in the r g and     c sr outperforms hr  the
differences between sr and hr are not statistically significant in any of the two examined data sets 
also  in the m c data set sr has the second r correlation with the js reporting the highest  but
js is outperformed by sr in the r g and     c data sets  in the case of the m c data set  the
difference between sr and js is not statistically significant  but sr outperforms js in the r g and
the     c data sets  with statistically significant difference in the reported correlations  another
important conclusion from the results  is the fact that the is a sp measure performs better than the
sp measure  this is mainly due to the fact that for the computation of the similarity values in such
data sets  the inclusion of only is a relations is much more reasonable  ponzetto   strube      b  
the differences in their results  sp and is a sp  motivate even more our sr measure  since we
    http   www cs technion ac il gabr resources data wordsim    

  

fit satsaronis   varlamis     vazirgiannis

category
lexicon based

corpus based

hybrid

method
lc
js
gm
wlm
lsa
sp
r
hr
sr

    c
spearmans  pearsons r
n a
    

    
n a

    
n a
    
n a

    
n a
n a
    
n a
    

     
n a
    
     

table    spearmans and pearsons correlations for the     word pairs      c  data set  confidence
levels                        

take the best of both worlds  i e   we weigh is a relations high  and fall back to other relations if
necessary 
regarding the     c data set  the results in table   show that sr again performs well  with
the top performers being the wikipedia based approaches  gabrilovich   markovitch        milne
  witten         the difference between them is statistically significant  but we should note that
sr outperforms both gm and wlm in the r g and m c data sets  with statistically significant
difference as well  partly  this difference in the performance of sr compared to gm and wlm can
be explained as follows  the gm measure considers words in context  gabrilovich   markovitch 
       and thus inherently performs word sense disambiguation  in contrast  sr takes as input a pair
of words  lacks context  and is based only on the information existing in wordnet  which  especially
for several of the cases in the     c data set  creates a disadvantage  e g   in the word pair arafat
and jackson  there are    different entries for the second word in wordnet   the same holds for
the wlm measure  another reason for this difference in performance is the coverage of wordnet 
in several cases  one or both of the two words in the     c data set comprising a pair  do not exist
in wordnet  e g   the football player maradona   however  as expected  and also shown in the
experimental analysis of omiotis that follows  when context is considered  the proposed semantic
relatedness measure performs better  the reader may wish to consult table    where for a subset of
the r g data set that contains the full definitions of the words  the correlations of omiotis with the
human judgements are the top found among the compared approaches  
to visualize the performance of our measure in a more comprehensible manner  we also present
in figure   the relatedness values given by humans for all pairs in the r g and m c data sets 
in increasing order of value  left side  and the respective values for these pairs produced using sr
 right side   note that the x axis in both charts begins from the least related pair of terms  according
to humans  and continues up to the most related pair of terms  the y axis in the left chart is the
respective humans rating for each pair of terms  the right figure shows sr for each pair  a closer
look on figure   reveals that the values produced by sr  right figure  follow a pattern similar to that
of the human ratings  left figure  
  

fit ext r elatedness based on a w ord t hesaurus

human ratings against human rankings   r g data set

semantic relatedness against human rankings   r g data set

 
   
semantic relatedness

human rating

   
 
   
 
   
 
   

  

  

  

  

   
   
   
   
   
   

correlation of human pairs ranking and human ratings

correlation of human pairs ranking and semantic relatedness

   

 
  

   

  

  

  

  

  

  

  

  

pair number

pair number

human ratings against human rankings   m c data set

semantic relatedness against human rankings   m c data set

  

 
semantic relatedness

human rating

   
 
   
 
   
 
   

correlation of human pairs ranking and human ratings

   
   
   
   
   
   
   
   
   

correlation of human pairs ranking and semantic relatedness

 
 

  

  

  

  

  

 

pair number

  

  

  

  

  

pair number

figure    correlation between human ratings and semantic relatedness measure  sr  in the rubenstein and goodenough  r g  and miller and charles  m c  data sets 

      e valuation

on

s ynonym i dentification

for the synonym identification task we are using the toefl    questions data set and the esl   
questions data set  for the toefl data set we compare with several other methods  more specifically  we examine  the lexicon based measures of leacock et al          lc   hirst and st onge
        hs   and jarmasz and szpakowicz         js   the corpus based measures of landauer and
dumais         ld   pado and lapata         pl   turney      b   t   terra and clarke       
 tc   and matveeva et al          m   the hybrid measures of resnik         r   lin         l  
jiang and conrath         jc   and turney et al          pr   and a web based method by ruizcasado et al          rc   we also report the results of random guessing  rg  and the performance
of the average college applicant  h   table   shows the results on the    toefl questions  the
table reports the number of the correct and the respective percentage given by all measures  in order
to test the statistical significance of the differences in the measures performance  we conducted
fishers exact test  agresti         as in the previous tables  the symbol  indicates statistically
significant difference at the      confidence level   at the      confidence level  and  at the     
confidence level  the results of table   show that sr ranks second among all reported methods 
with the best method being the hybrid pr  turney et al          with regards to its comparison with
the lexicon based methods  sr reports better results  statistically significant at the confidence levels
indicated 
in a similar manner  we have conducted experiments in the esl    questions data set  and
compare our results with  the lexicon based measures of leacock et al          lc   hirst and stonge         hs   and jarmasz and szpakowicz         js   the corpus based measures of turney
        pmi ir   and terra and clarke         tc   and the hybrid measures of resnik         r  
  

fit satsaronis   varlamis     vazirgiannis

category
lexicon based

corpus based

hybrid
web based
other

method
lc
hs
js
ld
pl
t
tc
m
r
l
jc
pr
rc
rg
h
sr

 correct answers
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

percentage of correct answers
     
     
     
    
     
     
     
     
   
     
    
     
     
    
    
     

table    number and percentage of correct answers in the toefl    questions test  confidence
levels                        

category
lexicon based
corpus based
hybrid
other

method
lc
hs
js
pmi ir
tc
r
l
jc
rg
sr

 correct answers
  
  
  
  
  
  
  
  
  
  

percentage of correct answers
    
    
    
    
   
    
    
    
    
    

table    number and percentage of correct answers in the esl    questions test  confidence levels 
              

lin         l   and jiang and conrath         jc   we report the results  together with random
guessing  in table    the results of table   show that sr ranks first  having the same performance
with js in this data set  both outperforming all of the compared corpus based methods  these
  

fit ext r elatedness based on a w ord t hesaurus

category
lexicon based
corpus based
hybrid
web based
other

method
lc
hs
v
lra
pmi ir
r
l
jc
b
rg
s 
s 
s
nb
ub

 correct answers
   
   
   
   
   
   
   
   
   
  
   
   
   
   
   

percentage of correct answers
     
     
    
     
    
     
     
     
   
   
     
     
     
     
     

table    number and percentage of correct answers in the     scholastic aptitude test  sat 
questions  confidence levels                        

results are very interesting  since they indicate that lexicon based methods are very promising in the
synonym identification tasks 
      e valuation

on

sat a nalogy q uestions

the approach that we choose to evaluate sr in the analogy task is to use the typical benchmark test
set employed in the related bibliography  namely the scholastic aptitude test  sat     it comprises
of     words pairs and for each target pair   supplementary pairs of words  the average us college
applicant answered correctly only the    percent of the questions  and no machine based approach
has yet surpassed the performance of the average college applicant 
in table    we present the number of correct answers and the respective percentage  recall  on
the     sat questions  of the following methods  random guessing  rg   jiang and conrath       
 jc   lin         l   leacock et al          lc   hirst and st onge         hs   resnik       
 r   bollegala et al          b   veale         v   pmi ir  turney        and lra  turney        
furthermore  we present the results of s   equation      s   equation     and s  equation      we
also present  as before  the statistical significance of the differences in performance  conducting
fishers exact test 
towards the direction of combining the answers of s  and s  in a different manner than the
naive average  we also report the upper bound performance of such an attempt  this is computed
by simply finding the union of the correct answers that s  and s  may provide  this is reported
in the table as  ub   in an effort to design a learning mechanism that would learn when to select
    many thanks to peter turney  for providing us with a standard set for experimentation  comprising of     sat questions 

  

fit satsaronis   varlamis     vazirgiannis

s  or s  answers for each sat question  with the goal to reach our upper bound  we designed
and implemented a simple representation of the sat questions as training instances  for each
sat question  we created a training instance that has   features  the minimum s  value found for
this question  among the five computed values for all the possible pairs   the maximum s  value 
and their difference  we also added the same features regarding s    we then trained and tested a
naive bayes classifier using ten fold cross validation in the     sat questions  the results of this
experiment are shown in the table as nb  naive bayes   finally  we also present the top results ever
reported in the literature for the specific data set  which is the lra method by turney         this
is reported in the table as  lra  
the results presented in table   show that s ranks second among the compared lexicon based
measures with the first being the measure of veale         v   the method of bollegala et al        
 b  achieves higher score than sr  but needs training in sat questions  at this point we have to
note that the lra method needs almost   days to process the     sat questions  turney        
 b  needs around   hours  bollegala et al          while s needs less than   minutes 
furthermore  the fact that combining s  and s  can reach       shows that s can produce very
promising results  if a classifier learns successfully how to combine them  the n b results  which
are a simple attempt to construct such a learner with few features  shows an important boost in
performance of       a proper feature engineering in the task  and more training sat questions
can potentially yield more promising results  as the gap between       and the upper bound of
      is still large  in all  these results prove that our lexicon based relatedness measure has a
comparable performance to the state of the art measures for the sat task  while it has smaller
execution time than the majority of the methods which outperform it in recall 
    evaluation of the omiotis measure
in order to evaluate the performance of the omiotis measure  we performed two experiments which
test the ability of the measure to capture the similarity between sentences  the first experiment is
based on the data set produced by li et al          the second experiment is based on the paraphrase
recognition task  using the microsoft research paraphrase corpus  dolan et al         
      e valuation

on

s entence s imilarity

the data set produced by li et al         comprises    sentence pairs  each pair consists of two
sentences that are the respective dictionary word definitions of the r g    word pairs data set  
the used dictionary was the collins cobuild dictionary  sinclair         for each sentence pair 
similarity scores have been provided by    human participants  ranging from      the sentences are
unrelated in meaning   to      the sentences are identical in meaning      
from the    sentence pairs  li et al         decided to keep a subset of    sentence pairs 
similarly to the process applied by miller and charles         in order to retain the sentences whose
human ratings create a more even distribution across the similarity range  thus  we apply omiotis
in this same subset of the    sentence pairs  described by li et al          in this data set  we
compare omiotis against the stasis measure of semantic similarity  proposed by li et al         
an lsa based approach described by oshea et al          and the sts measure proposed by islam
and inkpen         to the best of our knowledge  this data set has only been used by these three
    the data set is publicly available at http   www docm mmu ac uk staff j oshea 

  

fit ext r elatedness based on a w ord t hesaurus

previous works  in table   we present the sentence pairs used  and the respective scores by humans 
stasis  lsa  sts  and omiotis 
sentence pair
  cord smile
  autograph shore
  asylum fruit
   boy rooster
   coast forest
   boy sage
   forest graveyard
   bird woodland
   hill woodland
   magician oracle
   oracle sage
   furnace stove
   magician wizard
   hill mound
   cord string
   glass tumbler
   grin smile
   serf slave
   journey voyage
   autograph signature
   coast shore
   forest woodland
   implement tool
   cock rooster
   boy lad
   cushion pillow
   cemetery graveyard
   automobile car
   midday noon
   gem  jewel

human
    
     
     
     
     
     
     
     
     
    
     
     
     
     
    
     
     
     
    
     
     
     
    
     
    
     
     
     
     
     

stasis
     
     
     
    
     
     
     
     
    
     
     
     
     
     
     
     
     
     
     
    
     
   
     
 
     
     
     
     
     
     

lsa
    
    
     
     
     
    
     
     
    
    
     
     
     
    
     
     
     
    
    
   
    
    
    
     
    
    
    
    
 
    

sts
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

omiotis
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      

table    human  stasis  lsa  sts and omiotis scores for the    sentence pairs 
in table   we present the results of the comparison  comprising the reporting of the spearmans
rank order correlation coefficient  and the pearsons product moment correlation coefficient r for
stasis  lsa  sts  and omiotis  we have also included in the results  a version of omiotis that
does not take into account the inter pos relations  i e   relations that cross parts of speech   this
version of omiotis is indicated in the table as simpleomiotis  the objective of this experiment was
to measure the contribution of the relations that cross parts of speech in the computation of text to  

fit satsaronis   varlamis     vazirgiannis

stasis
lsa
sts
simple omiotis
omiotis
average participant
worst participant
best participant

spearmans 
      
      
     
      
      
n a
n a
n a

pearsons r
      
      
     
      
     
     
     
     

table    spearmans and pearsons correlations with human similarity ratings  confidence levels 
              

text semantic relatedness values  though these types of relations have been reported in the previous
bibliography as advantageous  jarmasz        jarmasz   szpakowicz         but their individual
contribution had never been measured 
we also show the r correlation between the average participant  mean of individuals with group 
n       leave one out resampling and standard deviation         the worst participant  worst participant with group  n       leave one out resampling  and the best participant  best participant
with group  n       leave one out resampling   taken from the work of oshea et al          in
addition  we have also conducted a z test regarding the difference between omiotis correlations and
the compared measures correlations  the symbols used in the previous tables indicate the confidence level of the statistical significance  note  also  that the reported correlations  stasis  lsa 
sts  and omiotis  individually constitute statistically significant positive correlations with the human scores  r  and rankings     as the results indicate  omiotis has the best correlation  according
to  and r values  compared to stasis  lsa  and sts  furthermore  the contribution of the semantic relations that cross parts of speech is obvious  since the difference between the simple version of
omiotis that omits them and the defined omiotis measure is large and statistically significant at the
     confidence level  overall  the results indicate that omiotis can be applied successfully to the
computation of similarities between small text segments  like sentences 
      e valuation

on

paraphrase r ecognition

in order to further evaluate the performance of omiotis in measuring the semantic relatedness between small text segments  we conducted additional experiments on the paraphrase recognition task
using the test pairs of the microsoft research paraphrase corpus  dolan et al          from the
original data set  containing both training and test pairs  we run experiments only on the      test
pairs of text segments  which have been collected from news sources on the web over a period of
   months  for each pair  human subjects have determined whether any of the two texts in the pair
consists a paraphrase of the other  direction is not an issue   the reported inter judge agreement
between annotators is      the paraphrase recognition task has been widely studied in the past 
since it is very important in many natural language applications  like question answering  harabagiu
  

fit ext r elatedness based on a w ord t hesaurus

category
baselines
corpus based

lexicon based

machine learning based

method
random
vsm and cosine
pmi ir
lsa
sts
jc
lc
lesk
l
wp
r
comb 
wan et al 
zhang and patrick
qiu et al 
finch et al 
omiotis

accuracy
    
    
    
    
    
    
    
    
    
  
  
    
  
    
  
     
     

precision
    
    
    
    
    
    
    
    
    
    
  
    
  
    
    
     
     

recall
  
    
    
    
    
    
  
    
    
    
    
    
  
    
    
    
    

f measure
    
    
  
    
    
  
  
    
    
  
    
    
  
    
    
     
     

table     omiotis and competitive methods performance on the microsoft research paraphrase
corpus  msr  

  hickl         and text summarization  madnani  zajic  dorr  fazil ayan    lin         for this
task we computed omiotis between the sentences of every pair and marked as paraphrases only
those pairs with omiotis value greater than a threshold  the threshold was set to      after tuning in
the training set  we used a simple approach for the tuning  namely forward hill climbing and beam
search  guyon  gunn  nikravesh    zadeh        
we compare the performance of omiotis against several other methods of various categories 
more precisely  against   a  two baseline methods  a random selection method that marks randomly
each pair as being paraphrase of not  random   and a vector based similarity measure  using the
cosine similarity measure and tf idf weighting for the features  vsm and cosine        b  corpusbased methods  the pmi ir proposed by turney         an lsa based approach introduced by
mihalcea et al          and the sts measure proposed by islam and inkpen          c  lexiconbased methods  jiang and conrath         jc   leacock et al          lc   lin         l   resnik
              r   lesk         lesk   wu and palmer         wp   and a metric that combines the
measures of this category  proposed by mihalcea et al          comb    and  d  machine learning
based techniques  which also constitute the state of the art in paraphrase recognition  like the method
of wan et al          which trains a classifier with lexical and dependency similarity measures 
the method of zhang and patrick         who also build a feature vector with lexical similarities
between the sentence pairs  e g   edit distance  number of common words   the method of qiu et al 
    the features are all words of the used data set 

  

fit satsaronis   varlamis     vazirgiannis

        who use an svm classifier  vapnik        to decide whether or not a set of features for each
sentence that has been created by parsing and semantic role labelling matches or not the respective
set of the second sentence in the pair  and with what importance  and  finally  the method of finch
et al          who also train an svm classifier based on machine translation evaluation metrics 
the results of the evaluation are shown in table     the results indicate that omiotis surpasses
all the lexicon based methods  and matches the combined method of mihalcea et al          at
this point we must mention that we also tuned omiotis with a goal to maximize f measure in the
test set  at the cost of dropping precision in favor of recall  this type of tuning reported an fmeasure of       which is larger than the f measures of the lexicon based  the corpus based and
two of the machine learning based approaches  even though the reported results used a different
and simpler tuning explained previously  still the results indicate that omiotis manages very well
in the paraphrase recognition task and produces comparable results with the state of the art  we
believe that it can be used as part of a machine learning based method  since it is one of the best
choices in lexicon based methods for paraphrase recognition  and this also constitutes part of our
plan for future work in this application 

   conclusions and future work
in this paper we presented a new measure of text semantic relatedness  the major strength of this
measure lies in the formulation of the semantic relatedness between words  experimental evaluation  proved that our measure approximates human understanding of semantic relatedness between
words better than previous related measures  the combination of path length  nodes depth and
edges type in a single formula allowed us to apply our semantic relatedness measure to different
text based tasks with very good performance  more specifically  the sr measure outperformed overall in the used data sets all state of the art measures in word to word tasks and the omiotis measure
performed very well in the sentence similarity and the paraphrase recognition tasks  although  the
results in the word analogy task are satisfactory  since no special tuning has been performed  we
are confident that there is still place for improvement  the extensive evaluation of sr and omiotis
in several applications shows the capabilities of our measures and proves that both can be applied
to several text related tasks  it is on our next plans to apply our relatedness measures to more applications  such as text classification and clustering  keyword and sentence extraction  and query
expansion  and compare with state of the art techniques in each field  finally  we are improving our
supporting infrastructure in order to facilitate large scale tasks such as document clustering and text
retrieval 

acknowledgments
part of this work was done while george tsatsaronis was at the department of informatics of athens
university of economics and business  we would like to thank kjetil nrvag for his constructive
comments  and ion androutsopoulos for his feedback on the early stage of this work  we would
also like to thank the anonymous reviewers for their detailed feedback 
  

fit ext r elatedness based on a w ord t hesaurus

references
agirre  e   alfonseca  e   hall  k   kravalova  j   pasca  m     soroa  a          a study on
similarity and relatedness using distributional and wordnet based approaches   in proceedings of human language technologies  the      annual conference of the north american
chapter of the association for computational linguistics  naacl   pp       
agirre  e     rigau  g          a proposal for word sense disambiguation using conceptual distance  in proceedings of the international conference on recent advances in natural language processing  ranlp  
agresti  a          categorical data analysis  wiley  hoboken  nj 
aizawa  a          an information theoretic perspective of tf idf measures  information processing and management              
banerjee  s     pedersen  t          extended gloss overlaps as a measure of semantic relatedness 
in proceedings of the eighteenth international joint conference on artificial intelligence
 ijcai   pp         
barzilay  r     elhadad  m          using lexical chains for text summarization  in proceedings of
the acl    eacl    workshop on intelligent scalable text summarization  pp       
barzilay  r   elhadad  m     mckeown  k          inferring strategies for sentence ordering in
multidocument news summarization  journal of artificial intelligence research           
basili  r   cammisa  m     moschitti  a          a semantic kernel to exploit linguistic knowledge  in proceedings of advances in artificial intelligence  ninth congress of the italian
association for artificial intelligence  ai ia   pp         
bicici  e     yuret  d          clustering word pairs to answer analogy questions  in proceedings
of the fifteenth turkish symposium on artificial intelligence and neural networks 
bollegala  d   matsuo  y     ishizuka  m          www sits the sat  measuring relational similarity from the web  in proceedings of the eighteenth european conference on artificial
intelligence  ecai   pp         
budanitsky  a     hirst  g          evaluating wordnet based measures of lexical semantic relatedness  computational linguistics              
clough  p     stevenson  m          cross language information retrieval using eurowordnet and
word sense disambiguation  in proceedings of the twenty sixth european conference on
information retrieval  ecir   pp         
cormen  t   leiserson  c     rivest  r          introduction to algorithms  the mit press 
dolan  w   quirk  c     brockett  c          unsupervised construction of large paraphrase corpora 
exploiting massively parallel news sources  in proceedings of the twentieth international
conference on computational linguistics  coling  
fellbaum  c          wordnet  an electronic lexical database  mit press 
  

fit satsaronis   varlamis     vazirgiannis

finch  a   hwang  y     sumita  e          using machine translation evaluation techniques to determine sentence level semantic equivalence  in proceedings of the  rd international workshop on paraphrasing  
finkelstein  l   gabrilovich  e   matias  y   rivlin  e   solan  z   wolfman  g     ruppin  e         
placing search in context  the concept revisited  acm transactions on information systems 
              
fisher  r          frequency distribution of the values of the correlation coefficient in samples of
an indefinitely large population  biometrika             
gabrilovich  e     markovitch  r          computing semantic relatedness using wikipedia based
explicit semantic analysis  in proceedings of the twentieth international joint conference on
artificial intelligence  ijcai   pp           
gabrilovich  e     markovitch  r          wikipedia based semantic interpretation for natural
language processing  journal of artificial intelligence research             
gentner  d          structure mapping  a theoretical framework for analogy  cognitive science 
             
guyon  i   gunn  s   nikravesh  m     zadeh  l          feature extraction  foundations and
applications  springer 
harabagiu  s     hickl  a          methods for using textual entailment in open domain question
answering   in proceedings of the joint conference of the international committee on computational linguistics and the association for computational linguistics  coling acl  
pp         
hirao  t   fukusima  t   okumura  m   nobata  c     nanba  h          corpus and evaluation
measures for multiple document summarization with multiple sources  in proceedings of the
twentieth international conference on computational linguistics  coling   pp         
hirst  g     st onge  d          lexical chains as representations of context for the detection and
correction of malapropisms  in wordnet  an electronic lexical database  chapter     pp 
       cambridge  the mit press 
hughes  t     ramage  d          lexical semantic relatedness with random graph walks  in proceedings of the conference on empirical methods in natural language processing  emnlp  
pp         
ide  n     veronis  j          word sense disambiguation  the state of the art  computational
linguistics             
islam  a     inkpen  d          semantic text similarity using corpus based word similarity and
string similarity  acm transactions on knowledge discovery from data            
jaccard  p          etude comparative de la distribution florale dans une portion des alpes et des
jura   bulletin del la societe vaudoise des sciences naturelles             
  

fit ext r elatedness based on a w ord t hesaurus

jarmasz  m          rogets thesaurus and semantic similarity  masters thesis  university of
ottawa 
jarmasz  m     szpakowicz  s          rogets thesaurus and semantic similarity  in proceedings of
the international conference on recent advances in natural language processing  ranlp  
pp         
jiang  j     conrath  d          semantic similarity based on corpus statistics and lexical taxonomy  in proceedings of the international conference research on computational linguistics
 rocling x   pp       
kucera  h   francis  w     caroll  j          computational analysis of present day american
english  brown university press 
landauer  t     dumais  s          a solution to platos problem  the latent semantic analysis
theory of the acquisition  induction  and representation of knowledge  psychological review 
               
landauer  t   foltz  p     laham  d          introduction to latent semantc analysis  discourse
processes             
leacock  c   miller  g     chodorow  m          using corpus statistics and wordnet relations for
sense identification  computational linguistics                
lesk  m          automated sense disambiguation using machine readable dictionaries  how to
tell a pine cone from an ice cream cone  in proceedings of the fifth annual international
conference on systems documentation  sigdoc   pp       
li  p          estimators and tail bounds for dimension reduction in l           using stable
random projections  in proceedings of the nineteenth annual acm siam symposium on
discrete algorithms  soda   pp       
li  y   mclean  d   bandar  z   oshea  j     crockett  k          sentence similarity based on
semantic nets and corpus statistics  ieee transactions on knowledge and data engineering 
                
lin  d          an information theoretic definition of similarity  in proceedings of the fifteenth
international conference on machine learning  icml   pp         
madnani  n   zajic  d   dorr  b   fazil ayan  n     lin  j          multiple alternative sentence
compressions for automatic text summarization  in proceedings of the hlt naacl document understanding conference  duc  
matveeva  i   levow  g   farahat  a     royer  c          generalized latent semantic analysis for
term representation  in proceedings of the international conference on recent advances in
natural language processing  ranlp  
mavroeidis  d   tsatsaronis  g   vazirgiannis  m   theobald  m     weikum  g          word sense
disambiguation for exploiting hierarchical thesauri in text classification  in proceedings of the
ninth european conference on principles and practice of knowledge discovery in databases
 pkdd   pp         
  

fit satsaronis   varlamis     vazirgiannis

mihalcea  r   corley  c     strapparava  c          corpus based and knowledge based measures
of text semantic similarity  in proceedings of the twenty first conference on artificial intelligence  aaai   pp         
mihalcea  r     moldovan  d          a method for word sense disambiguation of unrestricted text 
in proceedings of the   th annual meeting of the association for computational linguistics
 acl   pp         
mihalcea  r   tarau  p     figa  e          pagerank on semantic networks with application to
word sense disambiguation  in proceedings of the twentieth international conference on
computational linguistics  coling  
miller  g     charles  w          contextual correlates of semantic similarity  language and
cognitive processes            
milne  d     witten  i          an effective  low cost measure of semantic relatedness obtained
from wikipedia links  in proceedings of the first aaai workshop on wikipedia and artificial
intelligence  wikiai  
morris  j     hirst  g          lexical cohesion computed by thesaural relations as an indicator of
the structure of text  computational linguistics           
navigli  r          a structural approach to the automatic adjudication of word sense disagreements  natural language engineering                
oshea  j   bandar  z   crocket  k     mclean  d          a comparative study of two short
text semantic similarity measures  in proceedings of the agent and multi agent systems 
technologies and applications  second kes international symposium  kes amsta   pp 
       
pado  s     lapata  m          dependency based construction of semantic space models  computational linguistics                
palmer  m   fellbaum  c     cotton  s          english tasks  all words and verb lexical sample 
in proceedings of senseval    pp       
pasca  m          open domain question answering from large text collections  in csli studies
in computational linguistics  csli publications  distributed by the university of chicago
press 
pasca  m          mining paraphrases from self anchored web sentence fragments  in proceedings
of the ninth european conference on principles and practice of knowledge discovery in
databases  pkdd   pp         
patwardhan  s   banerjee  s     pedersen  t          using measures of semantic relatedness for
word sense disambiguation  in proceedings of the fourth international conference on intelligent text processing and computational linguistics  cicling   pp         
patwardhan  s     pedersen  t          using wordnet based context vectors to estimate the semantic relatedness of concepts  in proceedings of the eacl      workshop making sense of
sense   bringing computational linguistics and psycholinguistics together  pp     
  

fit ext r elatedness based on a w ord t hesaurus

ponzetto  s     strube  m       a   knowledge derived from wikipedia for computing semantic
relatedness  journal of artificial intelligence research             
ponzetto  s     strube  m       b   deriving a large scale taxonomy from wikipedia  in proceedings of the twenty second conference on artificial intelligence  aaai   pp           
qiu  l   kan  m     chua  t          paraphrase recognition via dissimilarity significance classification  in proceedings of the conference on empirical methods in natural language
processing  emnlp   pp       
quilian  r          the teachable language comprehender  a simulation program and theory of
language  communications of acm                
resnik  p          using information content to evaluate semantic similarity  in proceedings of the
fourteenth international joint conference on artificial intelligence  ijcai   pp         
resnik  p          semantic similarity in a taxonomy  an information based measure and its application to problems of ambiguity in natural language  journal of artificial intelligence
research            
richardson  r     smeaton  a          using wordnet in a knowledge based approach to information retrieval  in proceedings of the bcs irsg colloquium 
rubenstein  h     goodenough  j          contextual correlates of synonymy  communications of
the acm                
ruiz casado  m   alfonseca  e     castells  p          using context window overlapping in synonym discovery and ontology extension  in proceedings of the international conference on
recent advances in natural language processing  ranlp  
salton  g   buckley  c     yu  c          an evaluation of term dependence models in information retrieval  in proceedings of the fifth annual international acm sigir conference on
research and development in information retrieval  pp         
salton  g     mcgill  m          introduction to modern information retrieval  mcgraw hill 
sanderson  m          word sense disambiguation and information retrieval  in proceedings of the
seventeenth annual international acm sigir conference on research and development in
information retrieval  pp         
sanderson  m          ambiguous queries  test collections need more sense  in proceedings of
the thirty first annual international acm sigir conference on research and development
in information retrieval  pp         
shinyama  y     sekine  s          paraphrase acquisition for information extraction  in proceedings of the acl  nd workshop on paraphrasing  paraphrase acquisition and applications 
pp       
sinclair  j          collins cobuild english dictionary for advanced learners   rd edn  harper
collins  new york 
  

fit satsaronis   varlamis     vazirgiannis

smeaton  a   kelledy  f     odonnell  r          trec   experiments at dublin city university 
thresholding posting lists  query expansion with wordnet and pos tagging of spanish  in
proceedings of the fourth text retrieval conference  trec  
snyder  b     palmer  m          the english all words task  in proceedings of senseval    pp 
     
song  y   han  k     rim  h          a term weighting method based on lexical chain for automatic summarization  in proceedings of the fifth international conference on intelligent text
processing and computational linguistics  cicling   pp         
stokoe  c   oakes  m     tait  j          word sense disambiguation in information retrieval revisited  in proceedings of the twenty sixth annual international acm sigir conference on
research and development in information retrieval  pp         
strube  m     ponzetto  s          wikirelate  computing semantic relatedness using wikipedia 
in proceedings of the twenty first conference on artificial intelligence  aaai   pp      
     
sussna  m          word sense disambiguation for free text indexing using a massive semantic network  in proceedings of the second international conference on information and knowledge
management  cikm   pp       
terra  e     clarke  c          frequency estimates for statistical word similarity measures  in
proceedings of the north american chapter of the association for computational linguistics
  human language technologies conference  hlt naacl    pp         
tsang  v          a graph approach to measuring text distance  phd thesis  university of
toronto 
tsatsaronis  g     panagiotopoulou  v          a generalized vector space model for text retrieval
based on semantic relatedness  in proceedings of the the   th conference of the european
chapter of the association for computational linguistics  eacl   student research workshop   pp       
tsatsaronis  g   varlamis  i   nrvag  k     vazirgiannis  m          omiotis  a thesaurus based
measure of text relatedness  in proceedings of the european conference on machine learning
and principles and practice of knowledge discovery in databases  ecml pkdd   pp     
    
tsatsaronis  g   varlamis  i     vazirgiannis  m          word sense disambiguation with semantic
networks  in proceedings of the   th international conference on text  speech and dialogue
 tsd   pp         
tsatsaronis  g   vazirgiannis  m     androutsopoulos  i          word sense disambiguation with
spreading activation networks generated from thesauri  in proceedings of the twentieth international joint conference on artificial intelligence  ijcai   pp           
turney  p          mining the web for synonyms  pmi ir versus lsa on toefl  in proceedings
of the twelfth european conference on machine learning  ecml   pp         
  

fit ext r elatedness based on a w ord t hesaurus

turney  p          similarity of semantic relations  computational linguistics                
turney  p       a   the latent relation mapping engine  algorithm and experiments  journal of
artificial intelligence research             
turney  p       b   a uniform approach to analogies  synonyms  antonyms  and associations  in
proceedings of the twenty second international conference on computational linguistics
 coling   pp         
turney  p     littman  m          corpus based learning of analogies and semantic relations 
machine learning                  
turney  p   littman  m   bigham  j     shnayder  v          combining independent modules to
solve multiple choice synonym and analogy problems  in proceedings of the international
conference on recent advances in natural language processing  ranlp   pp         
van rijsbergen  c          information retrieval  butterworth 
vapnik  v          the nature of statistical learning theory  springer 
veale  t          wordnet sits the sat  a knowledge based approach to lexical analogy  in proceedings of the sixteenth european conference on artificial intelligence  ecai   pp     
    
veronis  j     ide  n          word sense disambiguation with very large neural networks extracted
from machine readable dictionaries  in proceedings of the thirteenth international conference on computational linguistics  coling   pp         
voorhees  e          using wordnet to disambiguate word sense for text retrieval  in proceedings
of the sixteenth annual international acm sigir conference on research and development
in information retrieval  pp         
wan  s   dras  m   dale  r     paris  c          using dependency based features to take the parafarce out of paraphrase  in proceedings of the australasian language technology workshop 
pp         
wu  z     palmer  m          verb semantics and lexical selection  in proceedings of the thirty
second annual meeting of the association for computational linguistics  acl   pp     
    
zhang  y     patrick  j          paraphrase identification by text canonicalization  in proceedings
of the australasian language technology workshop  pp         

  

fi