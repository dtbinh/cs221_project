journal artificial intelligence research                  

submitted        published      

frequency meaning 
vector space models semantics
peter d  turney

peter turney nrc cnrc gc ca

national research council canada
ottawa  ontario  canada  k a  r 

patrick pantel

me patrickpantel com

yahoo  labs
sunnyvale  ca         usa

abstract
computers understand little meaning human language  profoundly
limits ability give instructions computers  ability computers explain
actions us  ability computers analyse process text  vector space
models  vsms  semantics beginning address limits  paper surveys
use vsms semantic processing text  organize literature vsms according
structure matrix vsm  currently three broad classes vsms 
based termdocument  wordcontext  pairpattern matrices  yielding three classes
applications  survey broad range applications three categories
take detailed look specific open source project category  goal
survey show breadth applications vsms semantics  provide new
perspective vsms already familiar area  provide
pointers literature less familiar field 

   introduction
one biggest obstacles making full use power computers
currently understand little meaning human language  recent progress
search engine technology scratching surface human language  yet
impact society economy already immense  hints transformative
impact deeper semantic technologies have  vector space models  vsms   surveyed
paper  likely part new semantic technologies 
paper  use term semantics general sense  meaning word 
phrase  sentence  text human language  study meaning 
concerned narrower senses semantics  semantic web approaches
semantics based formal logic  present survey vsms relation
distributional hypothesis approach representing aspects natural language
semantics 
vsm developed smart information retrieval system  salton       
gerard salton colleagues  salton  wong    yang         smart pioneered
many concepts used modern search engines  manning  raghavan   
schutze         idea vsm represent document collection
point space  a vector vector space   points close together space
semantically similar points far apart semantically distant  users
c
    
ai access foundation national research council canada  reprinted permission 

fiturney   pantel

query represented point space documents  the query pseudodocument   documents sorted order increasing distance  decreasing semantic
similarity  query presented user 
success vsm information retrieval inspired researchers extend
vsm semantic tasks natural language processing  impressive results 
instance  rapp        used vector based representation word meaning achieve
score       multiple choice synonym questions test english foreign
language  toefl   whereas average human score         turney        used
vector based representation semantic relations attain score     multiple choice
analogy questions sat college entrance test  compared average human score
     
survey  organized past work vsms according type matrix
involved  termdocument  wordcontext  pairpattern  believe choice
particular matrix type fundamental choices  particular
linguistic processing mathematical processing  although three matrix types cover
work  reason believe three types exhaust possibilities 
expect future work introduce new types matrices higher order tensors  
    motivation vector space models semantics
vsms several attractive properties  vsms extract knowledge automatically
given corpus  thus require much less labour approaches semantics 
hand coded knowledge bases ontologies  example  main resource used
rapps        vsm system measuring word similarity british national corpus
 bnc    whereas main resource used non vsm systems measuring word similarity
 hirst   st onge        leacock   chodrow        jarmasz   szpakowicz       
lexicon  wordnet  rogets thesaurus  gathering corpus new language
generally much easier building lexicon  building lexicon often involves
gathering corpus  semcor wordnet  miller  leacock  tengi    bunker        
vsms perform well tasks involve measuring similarity meaning
words  phrases  documents  search engines use vsms measure similarity
query document  manning et al          leading algorithms measuring semantic relatedness use vsms  pantel   lin      a  rapp        turney  littman 
bigham    shnayder         leading algorithms measuring similarity semantic relations use vsms  lin   pantel        turney        nakov   hearst        
 section     discusses differences types similarity  
find vsms especially interesting due relation distributional hypothesis related hypotheses  see section       distributional hypothesis
   regarding average score       toefl questions  landauer dumais        note
that  although know performance would compare  example  u s  school
children particular age  told average score adequate admission many
universities 
   average score highschool students senior year  applying us universities 
discussion score  see section     turneys        paper 
   vector first order tensor matrix second order tensor  see section     
   see http   www natcorp ox ac uk  
   see http   wordnet princeton edu  

   

fifrom frequency meaning

words occur similar contexts tend similar meanings  wittgenstein       
harris        weaver        firth        deerwester  dumais  landauer  furnas    harshman         efforts apply abstract hypothesis concrete algorithms measuring
similarity meaning often lead vectors  matrices  higher order tensors 
intimate connection distributional hypothesis vsms strong motivation
taking close look vsms 
uses vectors matrices count vector space models  purposes
survey  take defining property vsms values elements
vsm must derived event frequencies  number times given word
appears given context  see section       example  often lexicon knowledge
base may viewed graph  graph may represented using adjacency matrix 
imply lexicon vsm  because  general  values
elements adjacency matrix derived event frequencies  emphasis
event frequencies brings unity variety vsms explicitly connects
distributional hypothesis  furthermore  avoids triviality excluding many possible
matrix representations 
    vectors ai cognitive science
vectors common ai cognitive science  common vsm
introduced salton et al          novelty vsm use frequencies
corpus text clue discovering semantic information 
machine learning  typical problem learn classify cluster set items
 i e   examples  cases  individuals  entities  represented feature vectors  mitchell       
witten   frank         general  features derived event frequencies 
although possible  see section       example  machine learning algorithm
applied classifying clustering documents  sebastiani        
collaborative filtering recommender systems use vectors  resnick  iacovou 
suchak  bergstrom    riedl        breese  heckerman    kadie        linden  smith   
york         typical recommender system  person item matrix 
rows correspond people  customers  consumers   columns correspond items
 products  purchases   value element rating  poor  fair  excellent 
person given item  many mathematical techniques work well
termdocument matrices  see section    work well person item matrices 
ratings derived event frequencies 
cognitive science  prototype theory often makes use vectors  basic idea
prototype theory members category central others  rosch
  lloyd        lakoff         example  robin central  prototypical  member
category bird  whereas penguin peripheral  concepts varying degrees
membership categories  graded categorization   natural way formalize
represent concepts vectors categories sets vectors  nosofsky        smith  osherson  rips    keane         however  vectors usually based numerical scores
elicited questioning human subjects  based event frequencies 
another area psychology makes extensive use vectors psychometrics 
studies measurement psychological abilities traits  usual instrument
   

fiturney   pantel

measurement test questionnaire  personality test  results test
typically represented subject item matrix  rows represent subjects
 people  experiment columns represent items  questions  test
 questionnaire   value element matrix answer corresponding
subject gave corresponding item  many techniques vector analysis  factor
analysis  spearman         pioneered psychometrics 
cognitive science  latent semantic analysis  lsa   deerwester et al         landauer   dumais         hyperspace analogue language  hal   lund  burgess   
atchley        lund   burgess         related research  landauer  mcnamara  dennis    kintsch        entirely within scope vsms  defined above  since
research uses vector space models values elements derived
event frequencies  number times given word appears given context  cognitive scientists argued empirical theoretical reasons
believing vsms  lsa hal  plausible models aspects human cognition  landauer et al          ai  computational linguistics  information
retrieval  plausibility essential  may seen sign vsms
promising area research 
    motivation survey
paper survey vector space models semantics  currently comprehensive  up to date survey field  show survey  vector space models
highly successful approach semantics  wide range potential actual
applications  much recent growth research area 
paper interest ai researchers work natural language 
especially researchers interested semantics  survey serve general
introduction area provide framework unified perspective
organizing diverse literature topic  encourage new research area 
pointing open problems areas exploration 
survey makes following contributions 
new framework  provide new framework organizing literature  term
document  wordcontext  pairpattern matrices  see section     framework shows
importance structure matrix  the choice rows columns  determining potential applications may inspire researchers explore new structures
 different kinds rows columns  higher order tensors instead matrices  
new developments  draw attention pairpattern matrices  use pair
pattern matrices relatively new deserves study  matrices address
criticisms directed wordcontext matrices  regarding lack sensitivity
word order 
breadth approaches applications  existing survey shows
breadth potential actual applications vsms semantics  existing summaries
omit pairpattern matrices  landauer et al         
focus nlp cl  focus survey systems perform practical
tasks natural language processing computational linguistics  existing overviews focus
cognitive psychology  landauer et al         
   

fifrom frequency meaning

success stories  draw attention fact vsms arguably
successful approach semantics  far 
    intended readership
goal writing paper survey state art vector space models
semantics  introduce topic new area  give new
perspective already familiar area 
assume reader basic understanding vectors  matrices  linear algebra 
one might acquire introductory undergraduate course linear algebra 
text book  golub   van loan         basic concepts vectors matrices
important mathematical details  widdows        gives gentle
introduction vectors perspective semantics 
assume reader familiarity computational linguistics information retrieval  manning et al         provide good introduction information retrieval 
computational linguistics  recommend manning schutzes        text 
reader familiar linear algebra computational linguistics  survey
present barriers understanding  beyond background  necessary
familiar vsms used information retrieval  natural language processing  computational linguistics  however  reader would
background reading  recommend landauer et al s        collection 
    highlights outline
article structured follows  section   explains framework organizing
literature vsms according type matrix involved  termdocument  wordcontext 
pairpattern  section  present overview vsms  without getting
details matrix generated corpus raw text 
high level framework place  sections     examine steps involved
generating matrix  section   discusses linguistic processing section   reviews
mathematical processing  order corpus would processed
vsm systems  first linguistic processing  mathematical processing  
vsms used semantics  input model usually plain text 
vsms work directly raw text  first apply linguistic processing
text  stemming  part of speech tagging  word sense tagging  parsing  section  
looks linguistic tools semantic vsms 
simple vsm  simple termdocument vsm  value element
document vector number times corresponding word occurs given
document  vsms apply mathematical processing raw frequency values 
section   presents main mathematical operations  weighting elements  smoothing
matrix  comparing vectors  section describes optimization strategies
comparing vectors  distributed sparse matrix multiplication randomized
techniques 
end section    reader general view concepts involved
vector space models semantics  take detailed look three vsm systems
section    representative termdocument vsms  present lucene information
   

fiturney   pantel

retrieval library   wordcontext vsms  explore semantic vectors package 
builds lucene   representative pairpattern vsms  review latent
relational analysis module s space package  builds lucene  
source code three systems available open source licensing 
turn broad survey applications semantic vsms section    section serves short historical view research semantic vsms  beginning
information retrieval section      purpose give reader idea
breadth applications vsms provide pointers literature 
reader wishes examine applications detail 
termdocument matrix  rows correspond terms columns correspond documents  section       document provides context understanding term 
generalize idea documents chunks text arbitrary size  phrases  sentences 
paragraphs  chapters  books  collections   result wordcontext matrix  includes termdocument matrix special case  section     discusses applications
wordcontext matrices  section     considers pairpattern matrices  rows correspond pairs terms columns correspond patterns pairs
occur 
section    discuss alternatives vsms semantics  section   considers
future vsms  raising questions power limitations  conclude
section   

   vector space models semantics
theme unites various forms vsms discuss paper
stated statistical semantics hypothesis  statistical patterns human word usage
used figure people mean   general hypothesis underlies several
specific hypotheses  bag words hypothesis  distributional hypothesis 
extended distributional hypothesis  latent relation hypothesis  discussed below 
    similarity documents  termdocument matrix
paper  use following notational conventions  matrices denoted bold
capital letters  a  vectors denoted bold lowercase letters  b  scalars represented
lowercase italic letters  c 
large collection documents  hence large number document
vectors  convenient organize vectors matrix  row vectors matrix
correspond terms  usually terms words  discuss possibilities 
  
  
  
  

see http   lucene apache org java docs  
see http   code google com p semanticvectors  
see http   code google com p airhead research wiki latentrelationalanalysis 
phrase taken faculty profile george furnas university michigan 
http   www si umich edu people faculty detail htm sid     full quote is  statistical semantics
studies statistical patterns human word usage used figure people
mean  least level sufficient information access  term statistical semantics appeared
work furnas  landauer  gomez  dumais         defined there 

   

fifrom frequency meaning

column vectors correspond documents  web pages  example   kind
matrix called termdocument matrix 
mathematics  bag  also called multiset  set  except duplicates
allowed  example   a  a  b  c  c  c  bag containing a  b  c  order matter
bags sets  bags  a  a  b  c  c  c   c  a  c  b  a  c  equivalent  represent
bag  a  a  b  c  c  c  vector x   h       i  stipulating first element
x frequency bag  second element frequency b bag 
third element frequency c  set bags represented matrix x 
column x j corresponds bag  row xi  corresponds unique member 
element xij frequency i th member j th bag 
termdocument matrix  document vector represents corresponding document
bag words  information retrieval  bag words hypothesis
estimate relevance documents query representing documents
query bags words  is  frequencies words document tend indicate
relevance document query  bag words hypothesis basis
applying vsm information retrieval  salton et al          hypothesis expresses
belief column vector termdocument matrix captures  to degree 
aspect meaning corresponding document  document about 
let x termdocument matrix  suppose document collection contains n documents unique terms  matrix x rows  one row unique
term vocabulary  n columns  one column document   let wi i th
term vocabulary let dj j th document collection  i th row
x row vector xi  j th column x column vector x j   row vector
xi  contains n elements  one element document  column vector x j contains
elements  one element term  suppose x simple matrix frequencies 
element xij x frequency i th term wi j th document dj  
general  value elements x zero  the matrix sparse  
since documents use small fraction whole vocabulary  randomly
choose term wi document dj   likely wi occur anywhere dj  
therefore xij equals   
pattern numbers xi  kind signature i th term wi   likewise 
pattern numbers x j signature j th document dj   is  pattern
numbers tells us  degree  term document about 
vector x j may seem rather crude representation document dj   tells
us frequently words appear document  sequential order words
lost  vector attempt capture structure phrases  sentences 
paragraphs  chapters document  however  spite crudeness  search
engines work surprisingly well  vectors seem capture important aspect semantics 
vsm salton et al         arguably first practical  useful algorithm
extracting semantic information word usage  intuitive justification term
document matrix topic document probabilistically influence authors
choice words writing document    two documents similar topics 
two corresponding column vectors tend similar patterns numbers 
    newer generative models  latent dirichlet allocation  lda   blei  ng    jordan         directly
model intuition  see sections       

   

fiturney   pantel

    similarity words  wordcontext matrix
salton et al         focused measuring document similarity  treating query search
engine pseudo document  relevance document query given
similarity vectors  deerwester et al         observed shift focus
measuring word similarity  instead document similarity  looking row vectors
termdocument matrix  instead column vectors 
deerwester et al         inspired termdocument matrix salton et al         
document necessarily optimal length text measuring word similarity 
general  may wordcontext matrix  context given words 
phrases  sentences  paragraphs  chapters  documents  exotic possibilities 
sequences characters patterns 
distributional hypothesis linguistics words occur similar contexts
tend similar meanings  harris         hypothesis justification applying vsm measuring word similarity  word may represented vector
elements derived occurrences word various contexts 
windows words  lund   burgess         grammatical dependencies  lin       
pado   lapata         richer contexts consisting dependency links selectional
preferences argument positions  erk   pado         see sahlgrens        thesis
comprehensive study various contexts  similar row vectors wordcontext matrix
indicate similar word meanings 
idea word usage reveal semantics implicit things
wittgenstein        said language games family resemblance  wittgenstein
primarily interested physical activities form context word usage  e g  
word brick  spoken context physical activity building house   main
context word often words   
weaver        argued word sense disambiguation machine translation
based co occurrence frequency context words near given target word  the
word want disambiguate   firth        p      said  shall know word
company keeps  deerwester et al         showed intuitions wittgenstein         harris         weaver  firth could used practical algorithm 
    similarity relations  pairpattern matrix
pairpattern matrix  row vectors correspond pairs words  mason   stone
carpenter   wood  column vectors correspond patterns pairs cooccur  x cuts x works   lin pantel        introduced
pairpattern matrix purpose measuring semantic similarity patterns 
is  similarity column vectors  given pattern x solves   algorithm
able find similar patterns  solved x  resolved x 
x resolves  
lin pantel        proposed extended distributional hypothesis  patterns
co occur similar pairs tend similar meanings  patterns x solves
    wittgensteins intuition might better captured matrix combines words modalities 
images  monay   gatica perez         values elements derived event
frequencies  would include vsm approach semantics 

   

fifrom frequency meaning

solved x tend co occur similar x   pairs  suggests
patterns similar meanings  pattern similarity used infer one sentence
paraphrase another  lin   pantel        
turney et al         introduced use pairpattern matrix measuring
semantic similarity relations word pairs  is  similarity row vectors 
example  pairs mason   stone  carpenter   wood  potter   clay  glassblower   glass
share semantic relation artisan   material  case  first member pair
artisan makes artifacts material second member pair 
pairs tend co occur similar patterns  x used x
shaped into 
latent relation hypothesis pairs words co occur similar patterns
tend similar semantic relations  turney      a   word pairs similar row
vectors pairpattern matrix tend similar semantic relations  inverse
extended distributional hypothesis  patterns similar column vectors
pairpattern matrix tend similar meanings 
    similarities
pairpattern matrices suited measuring similarity semantic relations
pairs words  is  relational similarity  contrast  wordcontext matrices suited
measuring attributional similarity  distinction attributional relational
similarity explored depth gentner        
attributional similarity two words b  sima  a  b     depends
degree correspondence properties b  correspondence
is  greater attributional similarity  relational similarity two pairs
words   b c   d  simr  a   b  c   d     depends degree correspondence
relations   b c   d  correspondence is  greater relational
similarity  example  dog wolf relatively high degree attributional similarity  whereas dog   bark cat   meow relatively high degree relational similarity
 turney        
tempting suppose relational similarity reduced attributional
similarity  example  mason carpenter similar words stone wood
similar words  therefore  perhaps follows mason   stone carpenter   wood
similar relations  perhaps simr  a   b  c   d  reduced sima  a  c    sima  b  d   however 
mason  carpenter  potter  glassblower similar words  they artisans  
wood  clay  stone  glass  they materials used artisans   cannot infer
mason   glass carpenter   clay similar relations  turney            a 
presented experimental evidence relational similarity reduce attributional
similarity 
term semantic relatedness computational linguistics  budanitsky   hirst       
corresponds attributional similarity cognitive science  gentner         two words
semantically related kind semantic relation  budanitsky   hirst 
       semantically related degree share attributes  turney        
examples synonyms  bank trust company   meronyms  car wheel   antonyms
 hot cold   words functionally related frequently associated  pencil
   

fiturney   pantel

paper   might usually think antonyms similar  antonyms high
degree attributional similarity  hot cold kinds temperature  black white
kinds colour  loud quiet kinds sound   prefer term attributional
similarity term semantic relatedness  attributional similarity emphasizes
contrast relational similarity  whereas semantic relatedness could confused
relational similarity 
computational linguistics  term semantic similarity applied words share
hypernym  car bicycle semantically similar  share hypernym
vehicle   resnik         semantic similarity specific type attributional similarity 
prefer term taxonomical similarity term semantic similarity  term
semantic similarity misleading  intuitively  attributional relational similarity
involve meaning  deserve called semantic similarity 
words semantically associated tend co occur frequently  e g   bee
honey   chiarello  burgess  richards    pollock         words may taxonomically similar semantically associated  doctor nurse   taxonomically similar semantically associated  horse platypus   semantically associated taxonomically similar
 cradle baby   neither semantically associated taxonomically similar  calculus
candy  
schutze pedersen        defined two ways words distributed corpus text  two words tend neighbours other  syntagmatic
associates  two words similar neighbours  paradigmatic parallels  syntagmatic associates often different parts speech  whereas paradigmatic parallels
usually part speech  syntagmatic associates tend semantically associated  bee honey often neighbours   paradigmatic parallels tend taxonomically
similar  doctor nurse similar neighbours  
    semantic vsms
possibilities exhausted termdocument  wordcontext  pairpattern
matrices  might want consider triplepattern matrices  measuring semantic
similarity word triples  whereas pairpattern matrix might row mason  
stone column x works   triplepattern matrix could row mason  
stone   masonry column x uses build z  however  n tuples words grow
increasingly rare n increases  example  phrases contain mason  stone 
masonry together less frequent phrases contain mason stone together 
triplepattern matrix much sparse pairpattern matrix  ceteris paribus  
quantity text need  order enough numbers make matrices
useful  grows rapidly n increases  may better break n tuples pairs 
example    b   c could decomposed   b    c  b   c  turney      a   similarity
two triples    b   c   e   f   could estimated similarity corresponding
pairs  relatively dense pairpattern matrix could serve surrogate relatively
sparse triplepattern matrix 
may go beyond matrices  generalization matrix tensor  kolda
  bader        acar   yener         scalar  a single number  zeroth order tensor 
vector first order tensor  matrix second order tensor  tensor order three
   

fifrom frequency meaning

higher called higher order tensor  chew  bader  kolda  abdelali        use term
documentlanguage third order tensor multilingual information retrieval  turney       
uses wordwordpattern tensor measure similarity words  van de cruys        uses
verbsubjectobject tensor learn selectional preferences verbs 
turneys        tensor  example  rows correspond words toefl
multiple choice synonym questions  columns correspond words basic english  ogden           tubes correspond patterns join rows columns  hence
wordwordpattern third order tensor   given word toefl questions represented corresponding wordpattern matrix slice tensor  elements
slice correspond patterns relate given toefl word word
basic english  similarity two toefl words calculated comparing two
corresponding matrix slices  algorithm achieves       toefl questions 
    types tokens
token single instance symbol  whereas type general class tokens  manning
et al          consider following example  from samuel beckett  

ever tried  ever failed 
matter  try again 
fail again  fail better 

two tokens type ever  two tokens type again  two tokens
type fail  lets say line example document  three
documents two sentences each  represent example tokendocument
matrix typedocument matrix  tokendocument matrix twelve rows  one
token  three columns  one line  figure     typedocument matrix
nine rows  one type  three columns  figure    
row vector token binary values  element   given token appears
given document   otherwise  row vector type integer values  element
frequency given type given document  vectors related 
type vector sum corresponding token vectors  example  row vector
type ever sum two token vectors two tokens ever 
applications dealing polysemy  one approach uses vectors represent word
tokens  schutze        agirre   edmonds        another uses vectors represent
word types  pantel   lin      a   typical word sense disambiguation  wsd  algorithms
deal word tokens  instances words specific contexts  rather word types 
mention approaches polysemy section    due similarity close
relationship  although defining characteristic vsm concerned
frequencies  see section       frequency property types  tokens 
    basic english highly reduced subset english  designed easy people learn  words
basic english listed http   ogden basic english org  

   

fiturney   pantel

ever tried 
ever failed 
ever
tried
ever
failed

matter
try

fail

fail
better

matter 
try again 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

fail again 
fail better 
 
 
 
 
 
 
 
 
 
 
 
 

figure    tokendocument matrix  rows tokens columns documents 

ever tried 
ever failed 
ever
tried
failed

matter
try

fail
better

matter 
try again 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

fail again 
fail better 
 
 
 
 
 
 
 
 
 

figure    typedocument matrix  rows types columns documents 

   

fifrom frequency meaning

    hypotheses
mentioned five hypotheses section  repeat hypotheses
interpret terms vectors  hypothesis  cite work explicitly
states something hypothesis implicitly assumes something hypothesis 
statistical semantics hypothesis  statistical patterns human word usage
used figure people mean  weaver        furnas et al          units text
similar vectors text frequency matrix    tend similar meanings 
 we take general hypothesis subsumes four specific hypotheses
follow  
bag words hypothesis  frequencies words document tend indicate
relevance document query  salton et al          documents pseudodocuments  queries  similar column vectors termdocument matrix 
tend similar meanings 
distributional hypothesis  words occur similar contexts tend similar
meanings  harris        firth        deerwester et al          words similar row
vectors wordcontext matrix  tend similar meanings 
extended distributional hypothesis  patterns co occur similar pairs tend
similar meanings  lin   pantel         patterns similar column vectors
pairpattern matrix  tend express similar semantic relations 
latent relation hypothesis  pairs words co occur similar patterns tend
similar semantic relations  turney et al          word pairs similar row
vectors pairpattern matrix  tend similar semantic relations 
yet explained means say vectors similar  discuss
section     

   linguistic processing vector space models
assume raw data large corpus natural language text 
generate termdocument  wordcontext  pairpattern matrix  useful apply
linguistic processing raw text  types processing used
grouped three classes  first  need tokenize raw text  is  need decide
constitutes term extract terms raw text  second  may want
normalize raw text  convert superficially different strings characters
form  e g   car  car  cars  cars could normalized car   third  may want
annotate raw text  mark identical strings characters different  e g   fly
verb could annotated fly vb fly noun could annotated fly nn  
grefenstette        presents good study linguistic processing wordcontext
vsms  uses similar three step decomposition linguistic processing  tokenization 
surface syntactic analysis  syntactic attribute extraction 
    text frequency matrix  mean matrix higher order tensor values elements
derived frequencies pieces text context pieces text collection
text  text frequency matrix intended general structure  includes termdocument 
wordcontext  pairpattern matrices special cases 

   

fiturney   pantel

    tokenization
tokenization english seems simple first glance  words separated spaces 
assumption approximately true english  may work sufficiently well basic
vsm  advanced vsm requires sophisticated approach tokenization 
accurate english tokenizer must know handle punctuation  e g   dont  janes 
and or   hyphenation  e g   state of the art versus state art   recognize multi word
terms  e g   barack obama ice hockey   manning et al          may wish
ignore stop words  high frequency words relatively low information content 
function words  e g   of  the  and  pronouns  e g   them  who  that   popular list
stop words set     common words included source code smart
system  salton          
languages  e g   chinese   words separated spaces  basic vsm
break text character unigrams bigrams  sophisticated approach
match input text entries lexicon  matching often
determine unique tokenization  sproat   emerson         furthermore  native speakers
often disagree correct segmentation  highly accurate tokenization challenging
task human languages 
    normalization
motivation normalization observation many different strings characters often convey essentially identical meanings  given want get meaning
underlies words  seems reasonable normalize superficial variations converting form  common types normalization case folding
 converting words lower case  stemming  reducing inflected words stem
root form  
case folding easy english  problematic languages  french 
accents optional uppercase  may difficult restore missing accents
converting words lowercase  words cannot distinguished without accents 
example  peche could either peche  meaning fishing peach  peche  meaning sin  
even english  case folding cause problems  case sometimes semantic
significance  example  smart information retrieval system  whereas smart
common adjective  bush may surname  whereas bush kind plant 
morphology study internal structure words  often word composed
stem  root  added affixes  inflections   plural forms past tenses  e g  
trapped composed stem trap affix  ed   stemming  kind morphological
analysis  process reducing inflected words stems  english  affixes
simpler regular many languages  stemming algorithms based
heuristics  rules thumb  work relatively well  lovins        porter        minnen 
carroll    pearce         agglutinative language  e g   inuktitut   many concepts
combined single word  using various prefixes  infixes  suffixes  morphological
analysis complicated  single word agglutinative language may correspond
sentence half dozen words english  johnson   martin        
    source code available ftp   ftp cs cornell edu pub smart  

   

fifrom frequency meaning

performance information retrieval system often measured precision
recall  manning et al          precision system estimate conditional
probability document truly relevant query  system says relevant 
recall system estimate conditional probability system say
document relevant query  truly relevant 
general  normalization increases recall reduces precision  kraaij   pohlmann 
       natural  given nature normalization  remove superficial
variations believe irrelevant meaning  make easier recognize similarities  find similar things  recall increases  sometimes superficial
variations semantic significance  ignoring variations causes errors  precision
decreases  normalization positive effect precision cases variant
tokens infrequent smoothing variations gives reliable statistics 
small corpus  may able afford overly selective  may
best aggressively normalize text  increase recall  large corpus 
precision may important  might want normalization  hull       
gives good analysis normalization information retrieval 
    annotation
annotation inverse normalization  different strings characters may
meaning  happens identical strings characters may different
meanings  depending context  common forms annotation include part of speech
tagging  marking words according parts speech   word sense tagging  marking
ambiguous words according intended meanings   parsing  analyzing grammatical structure sentences marking words sentences according
grammatical roles   manning   schutze        
since annotation inverse normalization  expect decrease recall
increase precision  example  tagging program noun verb  may
able selectively search documents act computer programming
 verb  instead documents discuss particular computer programs  noun   hence
increase precision  however  document computer programs  noun  may
something useful say act computer programming  verb   even document
never uses verb form program  hence may decrease recall 
large gains ir performance recently reported result query annotation syntactic semantic information  syntactic annotation includes query
segmentation  tan   peng        part speech tagging  barr  jones    regelson 
       examples semantic annotation disambiguating abbreviations queries  wei 
peng    dumoulin        finding query keyword associations  lavrenko   croft       
cao  nie    bai        
annotation useful measuring semantic similarity words concepts
 wordcontext matrices   example  pantel lin      a  presented algorithm
discover word senses clustering row vectors wordcontext matrix  using
contextual information derived parsing 
   

fiturney   pantel

   mathematical processing vector space models
text tokenized  optionally  normalized annotated  first step
generate matrix frequencies  second  may want adjust weights
elements matrix  common words high frequencies  yet less
informative rare words  third  may want smooth matrix  reduce
amount random noise fill zero elements sparse matrix  fourth 
many different ways measure similarity two vectors 
lowe        gives good summary mathematical processing wordcontext vsms 
decomposes vsm construction similar four step process  calculate frequencies 
transform raw frequency counts  smooth space  dimensionality reduction  
calculate similarities 
    building frequency matrix
element frequency matrix corresponds event  certain item  term  word 
word pair  occurred certain situation  document  context  pattern  certain number
times  frequency   abstract level  building frequency matrix simple matter
counting events  practice  complicated corpus large 
typical approach building frequency matrix involves two steps  first  scan sequentially corpus  recording events frequencies hash table 
database  search engine index  second  use resulting data structure generate
frequency matrix  sparse matrix representation  gilbert  moler    schreiber        
    weighting elements
idea weighting give weight surprising events less weight expected
events  hypothesis surprising events  shared two vectors  discriminative similarity vectors less surprising events  example 
measuring semantic similarity words mouse rat  contexts dissect
exterminate discriminative similarity contexts like 
information theory  surprising event higher information content expected
event  shannon         popular way formalize idea termdocument
matrices tf idf  term frequency inverse document frequency  family weighting
functions  sparck jones         element gets high weight corresponding term
frequent corresponding document  i e   tf high   term rare
documents corpus  i e   df low  thus idf high   salton buckley       
defined large family tf idf weighting functions evaluated information retrieval tasks  demonstrating tf idf weighting yield significant improvements
raw frequency 
another kind weighting  often combined tf idf weighting  length normalization
 singhal  salton  mitra    buckley         information retrieval  document length
ignored  search engines tend bias favour longer documents  length
normalization corrects bias 
term weighting may used correct correlated terms  example 
terms hostage hostages tend correlated  yet may want normalize
   

fifrom frequency meaning

term  as section       slightly different meanings 
alternative normalizing them  may reduce weights co occur
document  church        
feature selection may viewed form weighting  terms get
weight zero hence removed matrix  forman        provides good
study feature selection methods text classification 
alternative tf idf pointwise mutual information  pmi   church   hanks       
turney         works well wordcontext matrices  pantel   lin      a 
termdocument matrices  pantel   lin      b   variation pmi positive pmi
 ppmi   pmi values less zero replaced zero  niwa  
nitta         bullinaria levy        demonstrated ppmi performs better
wide variety weighting approaches measuring semantic similarity word
context matrices  turney      a  applied ppmi pairpattern matrices  give
formal definition ppmi here  example effective weighting function 
let f wordcontext frequency matrix nr rows nc columns  i th row
f row vector fi  j th column f column vector f j   row fi 
corresponds word wi column f j corresponds context cj   value
element fij number times wi occurs context cj   let x matrix
results ppmi applied f  new matrix x number rows
columns raw frequency matrix f  value element xij x defined
follows 
fij
pij   pnr pnc

j   fij

i  

   

pnc

j   fij
pi   pnr pnc

   

pnr
f
pncij
  pnr i  

   

i  

pj

i  

j   fij
j   fij



pij
pmiij   log
pi pj

pmiij pmiij    
xij  
  otherwise

   
   

definition  pij estimated probability word wi occurs context
cj   pi estimated probability word wi   pj estimated probability
context cj   wi cj statistically independent  pi pj   pij  by definition
independence   thus pmiij zero  since log          product pi pj
would expect pij wi occurs cj pure random chance  hand 
interesting semantic relation wi cj   expect pij larger
would wi cj indepedent  hence find pij   pi pj  
thus pmiij positive  follows distributional hypothesis  see section    
word wi unrelated context cj   may find pmiij negative  ppmi
designed give high value xij interesting semantic relation
   

fiturney   pantel

wi cj   otherwise  xij value zero  indicating occurrence wi
cj uninformative 
well known problem pmi biased towards infrequent events  consider
case wi cj statistically dependent  i e   maximum association  
pij   pi   pj   hence     becomes log    pi   pmi increases probability
word wi decreases  several discounting factors proposed alleviate problem 
example follows  pantel   lin      a  
p c
p r
fik  
fkj   nk  
min   nk  
fij
pnc
pnr
ij  

fij     min   k   fkj   k   fik      
newpmiij   ij pmiij

   
   

another way deal infrequent events laplace smoothing probability
estimates  pij   pi   pj  turney   littman         constant positive value added
raw frequencies calculating probabilities  fij replaced fij   k 
k      larger constant  greater smoothing effect  laplace smoothing
pushes pmiij values towards zero  magnitude push  the difference
pmiij without laplace smoothing  depends raw frequency fij  
frequency large  push small  frequency small  push large  thus
laplace smoothing reduces bias pmi towards infrequent events 
    smoothing matrix
simplest way improve information retrieval performance limit number
vector components  keeping components representing frequently occurring
content words way  however  common words  have  carry little
semantic discrimination power  simple component smoothing heuristics  based properties weighting schemes presented section      shown maintain
semantic discrimination power improve performance similarity computations 
computing similarity pairs vectors  described section     
computationally intensive task  however  vectors share non zero coordinate
must compared  i e   two vectors share coordinate dissimilar  
frequent context words  word the  unfortunately result vectors matching
non zero coordinate  words precisely contexts little semantic
discrimination power  consider pointwise mutual information weighting described
section      highly weighted dimensions co occur frequently words
definition highly discriminating contexts  i e   high association
words co occur   keeping context word dimensions
pmi conservative threshold setting others zero  lin        showed
number comparisons needed compare vectors greatly decreases losing
little precision similarity score top     similar words every word 
smoothing matrix  one computes reverse index non zero coordinates 
then  compare similarity words context vector words context
vectors  vectors found match non zero component reverse index must
compared  section     proposes optimizations along lines 
   

fifrom frequency meaning

deerwester et al         found elegant way improve similarity measurements
mathematical operation termdocument matrix  x  based linear algebra  operation truncated singular value decomposition  svd   called thin svd  deerwester
et al  briefly mentioned truncated svd applied document similarity
word similarity  focus document similarity  landauer dumais       
applied truncated svd word similarity  achieving human level scores multiple choice
synonym questions test english foreign language  toefl   truncated
svd applied document similarity called latent semantic indexing  lsi  
called latent semantic analysis  lsa  applied word similarity 
several ways thinking truncated svd works  first
present math behind truncated svd describe four ways looking it 
latent meaning  noise reduction  high order co occurrence  sparsity reduction 
svd decomposes x product three matrices uvt   u v
column orthonormal form  i e   columns orthogonal unit length  ut u  
vt v   i  diagonal matrix singular values  golub   van loan         x
rank r  rank r  let k   k   r  diagonal matrix formed
top k singular values  let uk vk matrices produced selecting
corresponding columns u v  matrix uk k vkt matrix rank k
best approximates original matrix x  sense minimizes approximation
errors  is  x   uk k vkt minimizes kx xkf matrices x rank k 
k       kf denotes frobenius norm  golub   van loan        
latent meaning  deerwester et al         landauer dumais        describe
truncated svd method discovering latent meaning  suppose word
context matrix x  truncated svd  x   uk k vkt   creates low dimensional linear
mapping row space  words  column space  contexts   low dimensional
mapping captures latent  hidden  meaning words contexts  limiting
number latent dimensions  k   r  forces greater correspondence words
contexts  forced correspondence words contexts improves similarity
measurement 
noise reduction  rapp        describes truncated svd noise reduction technique 
may think matrix x   uk k vkt smoothed version original matrix x 
matrix uk maps row space  the space spanned rows x  smaller
k dimensional space matrix vk maps column space  the space spanned
columns x  k dimensional space  diagonal matrix k specifies
weights reduced k dimensional space  singular values ranked
descending order amount variation x fit  think matrix
x composed mixture signal noise  signal noise 
uk k vkt mostly captures variation x due signal  whereas remaining
vectors uvt mostly fitting variation x due noise 
high order co occurrence  landauer dumais        describe truncated
svd method discovering high order co occurrence  direct co occurrence  firstorder co occurrence  two words appear identical contexts  indirect co occurrence
 high order co occurrence  two words appear similar contexts  similarity
contexts may defined recursively terms lower order co occurrence  lemaire
denhiere        demonstrate truncated svd discover high order co occurrence 
   

fiturney   pantel

sparsity reduction  general  matrix x sparse  mostly zeroes  
truncated svd  x   uk k vkt   dense  sparsity may viewed problem insufficient
data  text  matrix x would fewer zeroes  vsm would perform
better chosen task  perspective  truncated svd way simulating
missing text  compensating lack data  vozalis   margaritis        
different ways viewing truncated svd compatible other 
possible perspectives correct  future work likely provide
views svd perhaps unifying view 
good c implementation svd large sparse matrices rohdes svdlibc   
another approach brands        incremental truncated svd algorithm    yet another
approach gorrells        hebbian algorithm incremental truncated svd  brands
gorrells algorithms introduce interesting new ways handling missing values 
instead treating zero values 
higher order tensors  operations analogous truncated svd 
parallel factor analysis  parafac   harshman         canonical decomposition
 candecomp   carroll   chang         equivalent parafac discovered independently   tucker decomposition  tucker         overview tensor decompositions  see surveys kolda bader        acar yener         turney       
gives empirical evaluation well four different tucker decomposition algorithms
scale large sparse third order tensors  low ram algorithm  multislice projection 
large sparse tensors presented evaluated   
since work deerwester et al          subsequent research discovered many
alternative matrix smoothing processes  nonnegative matrix factorization  nmf 
 lee   seung         probabilistic latent semantic indexing  plsi   hofmann         iterative scaling  is   ando         kernel principal components analysis  kpca   scholkopf 
smola    muller         latent dirichlet allocation  lda   blei et al          discrete
component analysis  dca   buntine   jakulin        
four perspectives truncated svd  presented above  apply equally well
recent matrix smoothing algorithms  newer smoothing algorithms tend
computationally intensive truncated svd  attempt model
word frequencies better svd  truncated svd implicitly assumes elements
x gaussian distribution  minimizing frobenius norm kx xkf
minimize noise  noise gaussian distribution  however  known
word frequencies gaussian distributions  recent algorithms based
realistic models distribution word frequencies   
    comparing vectors
popular way measure similarity two frequency vectors  raw weighted 
take cosine  let x two vectors  n elements 
   
   
   
   

svdlibc available http   tedlab mit edu dr svdlibc  
matlab source code available http   web mit edu wingated www resources html 
matlab source code available http   www apperceptual com multislice  
experience  pmiij appears approximately gaussian  may explain pmi works well
truncated svd  ppmi puzzling  less gaussian pmi  yet apparently
yields better semantic models pmi 

   

fifrom frequency meaning

x   hx    x            xn

   

  hy    y            yn

   

cosine angle x calculated follows 
pn
cos x  y    qp
n

yi
pn

i   xi

 
 
i   xi
i   yi
xy
 

xx yy
x

 

kxk kyk

    
    
    

words  cosine angle two vectors inner product
vectors  normalized unit length  x frequency vectors
words  frequent word long vector rare word short vector 
yet words might synonyms  cosine captures idea length vectors
irrelevant  important thing angle vectors 
cosine ranges   vectors point opposite directions      
degrees     point direction     degrees   vectors
orthogonal      degrees   cosine zero  raw frequency vectors 
necessarily cannot negative elements  cosine cannot negative  weighting
smoothing often introduce negative elements  ppmi weighting yield negative
elements  truncated svd generate negative elements  even input matrix
negative values 
measure distance vectors easily converted measure similarity
inversion      subtraction      

sim x  y      dist x  y 

    

sim x  y      dist x  y 

    

many similarity measures proposed ir  jones   furnas       
lexical semantics circles  lin        dagan  lee    pereira        lee        weeds  weir 
  mccarthy         commonly said ir that  properly normalized  difference
retrieval performance using different measures insignificant  van rijsbergen        
often vectors normalized way  e g   unit length unit probability 
applying similarity measure 
popular geometric measures vector distance include euclidean distance manhattan distance  distance measures information theory include hellinger  bhattacharya 
kullback leibler  bullinaria levy        compared five distance measures
cosine similarity measure four different tasks involving word similarity  overall 
best measure cosine  popular measures dice jaccard coefficients
 manning et al         
   

fiturney   pantel

lee        proposed that  finding word similarities  measures focused
overlapping coordinates less importance negative features  i e   coordinates
one word nonzero value zero value  appear perform
better  lees experiments  jaccard  jensen shannon  l  measures seemed
perform best  weeds et al         studied linguistic statistical properties
similar words returned various similarity measures found measures
grouped three classes 
   high frequency sensitive measures  cosine  jensen shannon   skew  recall  
   low frequency sensitive measures  precision  
   similar frequency sensitive methods  jaccard  jaccard mi  lin  harmonic mean  
given word w    use high frequency sensitive measure score words wi
according similarity w    higher frequency words tend get higher scores
lower frequency words  use low frequency sensitive measure 
bias towards lower frequency words  similar frequency sensitive methods prefer word wi
approximately frequency w    one experiment determining
compositionality collocations  high frequency sensitive measures outperformed
classes  weeds et al          believe determining appropriate similarity
measure inherently dependent similarity task  sparsity statistics 
frequency distribution elements compared  smoothing method applied
matrix 
    efficient comparisons
computing similarity rows  or columns  large matrix non trivial
problem  worst case cubic running time o n r nc    nr number rows
nc number columns  i e   dimensionality feature space   optimizations
parallelization often necessary 
      sparse matrix multiplication
one optimization strategy generalized sparse matrix multiplication approach  sarawagi
  kirpal         based observation scalar product two vectors
depends coordinates vectors nonzero values  further 
observe commonly used similarity measures vectors x y  cosine 
overlap  dice  decomposed three values  one depending nonzero
values x  another depending nonzero values y  third depending
nonzero coordinates shared x y  formally  commonly used similarity
scores  sim x  y   expressed follows 
sim x  y    f   

pn

i   f   xi   yi    f   x   f   y  

    

example  cosine measure  cos x  y   defined       expressed model
follows 
   

fifrom frequency meaning

p
cos x  y    f    ni   f   xi   yi    f   x   f   y  

f   a  b  c   
bc
f   a  b    b
qp
n
 
f   a    f   a   
i   ai

    
    
    
    

let x matrix want compute pairwise similarity  sim x  y  
rows columns x y  efficient computation similarity matrix
achieved leveraging fact sim x  y  determined solely nonzero
coordinates shared x  i e   f      xi     f   xi          xi  
vectors sparse  case  calculating f   xi   yi   required
vectors shared nonzero coordinate  significantly reducing cost computation 
determining vectors share nonzero coodinate easily achieved first building
inverted index coordinates  indexing  precompute f   x 
f   y  without changing algorithm complexity  then  vector x retrieve
constant time  index  vector shares nonzero coordinate x
wep
apply f   xi   yi   shared coordinates i  computational cost algorithm
ni  ni number vectors nonzero i th coordinate  worst
case time complexity o ncv  n number vectors compared  c
maximum number nonzero coordinates vector  v number vectors
nonzero i th coordinate coordinate nonzero
vectors  words  algorithm efficient density coordinates
low  experiments computing semantic similarity pairs
words large web crawl  observed near linear average running time complexity n 
computational cost reduced leveraging element weighting
techniques described section      setting zero coordinates low
ppmi  pmi tf idf score  coordinate density dramatically reduced cost
losing little discriminative power  vein  bayardo  ma  srikant        described
strategy omits coordinates highest number nonzero values 
algorithm gives significant advantage interested finding solely
similarity highly similar vectors 
      distributed implementation using mapreduce
algorithm described section       assumes matrix x fit memory 
large x may impossible  also  element x processed independently  running parallel processes non intersecting subsets x makes processing
faster  elsayed  lin  oard        proposed mapreduce implementation deployed using hadoop  open source software package implementing mapreduce framework
distributed file system    hadoop shown scale several thousands machines 
allowing users write simple code  seamlessly manage sophisticated parallel execution code  dean ghemawat        provide good primer mapreduce
programming 
    hadoop available download http   lucene apache org hadoop  

   

fiturney   pantel

mapreduce models map step used start n map tasks parallel 
caching one m th part x inverted index streaming one n th part x
it  actual inputs read tasks directly hdfs  hadoop distributed
file system   value determined amount memory dedicated
inverted index  n determined trading fact that  n increases 
parallelism obtained increased cost building inverted index
n times 
similarity algorithm section       runs task map step
mapreduce job  reduce step groups output rows  or columns  x 
      randomized algorithms
optimization strategies use randomized techniques approximate various similarity measures  aim randomized algorithms improve computational efficiency
 memory time  projecting high dimensional vectors low dimensional subspace 
truncated svd performs projection  svd computationally intensive   
insight randomized techniques high dimensional vectors randomly projected low dimensional subspace relatively little impact final similarity
scores  significant reductions computational cost reported little average error computing true similarity scores  especially applications word
similarity interested top k similar vectors vector
 ravichandran  pantel    hovy        gorman   curran        
random indexing  approximation technique based sparse distributed memory
 kanerva         computes pairwise similarity rows  or vectors  matrix
complexity o nr nc        fixed constant representing length index
vectors assigned column  value   controls tradeoff accuracy versus
efficiency  elements index vector mostly zeros small number
randomly assigned   s  s  cosine measure two rows r  r 
approximated computing cosine two fingerprint vectors  fingerprint r   
fingerprint r     fingerprint r  computed summing index vectors
non unique coordinate r  random indexing shown perform well lsa
word synonym selection task  karlgren   sahlgren        
locality sensitive hashing  lsh   broder        another technique approximates
similarity matrix complexity o n r        constant number random
projections  controls accuracy versus efficiency tradeoff    lsh general class
techniques defining functions map vectors  rows columns  short signatures
fingerprints  two similar vectors likely similar fingerprints  definitions
lsh functions include min wise independent function  preserves jaccard
similarity vectors  broder         functions preserve cosine similarity
vectors  charikar         word similarity task  ravichandran et al        
showed that  average      top    similar words random words found
top    results using charikars functions  average cosine error      
    however  efficient forms svd  brand        gorrell        
    lsh stems work rabin         proposed use hash functions random irreducible
polynomials create short fingerprints collections documents  techniques useful many
tasks  removing duplicate documents  deduping  web crawl 

   

fifrom frequency meaning

 using            random projections   gorman curran        provide detailed
comparison random indexing lsh distributional similarity task  bnc
corpus  lsh outperformed random indexing  however  larger corpora combining bnc 
reuters corpus  english news holdings ldc       random
indexing outperformed lsh efficiency accuracy 
    machine learning
intended application vsm clustering classification  similarity measure
cosine  section      may used  classification  nearest neighbour algorithm
use cosine measure nearness  dasarathy         clustering  similaritybased clustering algorithm use cosine measure similarity  jain  murty    flynn 
       however  many machine learning algorithms work directly
vectors vsm  without requiring external similarity measure  cosine 
effect  machine learning algorithms implicitly use internal approaches
measuring similarity 
machine learning algorithm works real valued vectors use vectors
vsm  witten   frank         linguistic processing  section    mathematical
processing  section    may still necessary  machine learning algorithm handle
vector comparison  sections          
addition unsupervised  clustering  supervised  classification  machine learning  vectors vsm may used semi supervised learning  ando   zhang 
      collobert   weston         general  nothing unique vsms would
compel choice one machine learning algorithm another  aside algorithms
performance given task  therefore refer readers machine learning
literature  witten   frank         since advice specific vsms 

   three open source vsm systems
illustrate three types vsms discussed section    section presents three open
source systems  one vsm type  chosen present open source systems
interested readers obtain source code find systems
apply systems projects  three systems written java
designed portability ease use 
    termdocument matrix  lucene
lucene   open source full featured text search engine library supported apache
software foundation  arguably ubiquitous implementation termdocument
matrix  powering many search engines cnet  sourceforge  wikipedia  disney 
aol comcast  lucene offers efficient storage  indexing  well retrieval ranking
functionalities  although primarily used termdocument matrix  generalizes
vsms 
    apache lucene available download http   lucene apache org  

   

fiturney   pantel

content  webpages  pdf documents  images  video  programmatically
decomposed fields stored database  database implements term
document matrix  content corresponds documents fields correspond terms 
fields stored database indices computed field values  lucene
uses fields generalization content terms  allowing string literal index
documents  example  webpage could indexed terms contains 
anchor texts pointing it  host name  semantic classes
classified  e g   spam  product review  news  etc    webpage retrieved
search terms matching fields 
columns termdocument matrix consist fields particular instance
content  e g   webpage   rows consist instances content index 
various statistics frequency tf idf stored matrix  developer
defines fields schema identifies indexed lucene  developer
optionally defines content ranking function indexed field 
index built  lucene offers functionalities retrieving content  users
issue many query types phrase queries  wildcard queries  proximity queries  range
queries  e g   date range queries   field restricted queries  results sorted
field index updates occur simultaneously searching  lucenes index
directly loaded tomcat webserver offers apis common programming languages  solr    separate apache software foundation project  open source enterprise
webserver searching lucene index presenting search results  full featured
webserver providing functionalities xml http json apis  hit highlighting 
faceted search  caching  replication 
simple recipe creating web search service  using nutch  lucene solr  consists
crawling set urls  using nutch   creating termdocument matrix index  using
lucene   serving search results  using solr   nutch    apache software foundation
open source web search software  offers functionality crawling web seed set
urls  building link graph web crawl  parsing web documents
html pages  good set seed urls nutch downloaded freely
open directory project    crawled pages html parsed  indexed
lucene  resulting indexed collection queried served solr
installation tomcat 
information lucene  recommend gospodnetic hatchers       
book  konchady        explains integrate lucene lingpipe gate
sophisticated semantic processing   

   
   
   
   

apache solr available download http   lucene apache org solr  
apache nutch available download http   lucene apache org nutch  
see http   www dmoz org  
information lingpipe available http   alias i com lingpipe   gate  general architecture text engineering  home page http   gate ac uk  

   

fifrom frequency meaning

    wordcontext matrix  semantic vectors
semantic vectors   open source project implementing random projection approach
measuring word similarity  see section         package uses lucene create term
document matrix  creates vectors lucenes termdocument matrix  using
random projection dimensionality reduction  random projection vectors
used  example  measure semantic similarity two words find words
similar given word 
idea random projection take high dimensional vectors randomly project
relatively low dimensional space  sahlgren         viewed
kind smoothing operation  section       developers semantic vectors
package emphasize simplicity efficiency random projection  section       rather
smoothing ability  argue matrix smoothing algorithms might
smooth better  none perform well random indexing  terms
computational complexity building smooth matrix incrementally updating
matrix new data arrives  widdows   ferraro         aim encourage
research development semantic vectors creating simple efficient open
source package 
semantic vectors package designed convenient use  portable  easy
extend modify  design software incorporates lessons learned
earlier stanford infomap project    although default generate random projection
vectors  system modular design allows kinds wordcontext matrices
used instead random projection matrices 
package supports two basic functions  building wordcontext matrix searching
vectors matrix  addition generating word vectors  building
operation generate document vectors calculating weighted sums word vectors
words document  searching operation used search similar
words search documents similar query  query single word
several words combined  using various mathematical operations corresponding
vectors  mathematical operations include vector negation disjunction  based
quantum logic  widdows         widdows ferraro        provide good summary
semantic vectors software 
    pairpattern matrix  latent relational analysis s space
latent relational analysis    lra  open source project implementing pairpattern
matrix  component s space package  library tools building
comparing different semantic spaces 
lra takes input textual corpus set word pairs  pairpattern matrix
built deriving lexical patterns link together word pairs corpus  example  consider word pair hkorea  japani following retrieved matching sentences 
    semantic vectors software package measuring word similarity  available simplified bsd
license http   code google com p semanticvectors  
    see http   infomap nlp sourceforge net  
    latent relational analysis part s space package distributed gnu general
public license version    available http   code google com p airhead research   time
writing  lra module development 

   

fiturney   pantel

korea looks new japan prime ministers effect korea japan relations 
channel korea vs  japan football game 
two sentences  lra extracts two patterns  x looks new x vs   
patterns become two columns pairpattern matrix  word pair hkorea 
japani becomes row  pattern frequencies counted smoothed using svd  see
section      
order mitigate sparseness occurrences word pairs  thesaurus
wordnet used expand seed word pairs alternatives  example pair
hkorea  japani may expanded include hsouth korea  japani  hrepublic korea 
japani  hkorea  nipponi  hsouth korea  nipponi  hrepublic korea  nipponi 
lra uses lucene  see section      backend store matrix  index it  serve
contents  detailed description lra algorithm  suggest turneys       
paper 

   applications
section  survey semantic applications vsms  aim
breadth  rather depth  readers want depth consult references 
goal give reader impression scope flexibility vsms semantics 
following applications grouped according type matrix involved  term
document  wordcontext  pairpattern  note section exhaustive 
many references applications space list here 
    termdocument matrices
termdocument matrices suited measuring semantic similarity documents
queries  see section       usual measure similarity cosine column vectors
weighted termdocument matrix  variety applications measures
document similarity 
document retrieval  termdocument matrix first developed document
retrieval  salton et al          large body literature vsm
document retrieval  manning et al          including several journals conferences
devoted topic  core idea is  given query  rank documents order
decreasing cosine angles query vector document vectors  salton
et al          one variation theme cross lingual document retrieval 
query one language used retrieve document another language  landauer  
littman         important technical advance discovery smoothing
termdocument matrix truncated svd improve precision recall  deerwester
et al          although commercial systems use smoothing  due computational
expense document collection large dynamic  random indexing  sahlgren 
      incremental svd  brand        may help address scaling issues  another
important development document retrieval addition collaborative filtering 
form pagerank  brin   page        
document clustering  given measure document similarity  cluster
documents groups  similarity tends high within group  low across
   

fifrom frequency meaning

groups  manning et al          clusters may partitional  flat   cutting  karger 
pedersen    tukey        pantel   lin      b  may hierarchical structure
 groups groups   zhao   karypis         may non overlapping  hard   croft 
      overlapping  soft   zamir   etzioni         clustering algorithms differ
clusters compared abstracted  single link clustering  similarity
two clusters maximum similarities members  complete link
clustering uses minimum similarities average link clustering uses average
similarities  manning et al         
document classification  given training set documents class labels
testing set unlabeled documents  task document classification learn
training set assign labels testing set  manning et al          labels may
topics documents  sebastiani         sentiment documents  e g  
positive versus negative product reviews   pang  lee    vaithyanathan        kim  pantel 
chklovski    pennacchiotti         spam versus non spam  sahami  dumais  heckerman   
horvitz        pantel   lin         labels might inferred words
documents  classify documents  implying documents
class similar way  thus document classification implies notion document
similarity  machine learning approaches document classification involve term
document matrix  sebastiani         measure document similarity  cosine 
directly applied document classification using nearest neighbour algorithm  yang 
      
essay grading  student essays may automatically graded comparing
one high quality reference essays given essay topic  wolfe  schreiner  rehder 
laham  foltz  kintsch    landauer        foltz  laham    landauer         student
essays reference essays compared cosines termdocument matrix 
grade assigned student essay proportional similarity one
reference essays  student essay highly similar reference essay gets high grade 
document segmentation  task document segmentation partition document sections  section focuses different subtopic document
 hearst        choi         may treat document series blocks  block
sentence paragraph  problem detect topic shift one block
next  hearst        choi        use cosine columns wordblock
frequency matrix measure semantic similarity blocks  topic shift signaled
drop cosine consecutive blocks  wordblock matrix viewed
small termdocument matrix  corpus single document documents
blocks 
question answering  given simple question  task question answering  qa 
find short answer question searching large corpus  typical question is  many calories big mac  algorithms qa four
components  question analysis  document retrieval  passage retrieval  answer extraction
 tellex  katz  lin  fern    marton        dang  lin    kelly         vector based similarity measurements often used document retrieval passage retrieval  tellex
et al         
call routing  chu carroll carpenter        present vector based system
automatically routing telephone calls  based callers spoken answer question 
   

fiturney   pantel

may direct call  callers answer ambiguous  system automatically
generates question caller  derived vsm  prompts caller
information 
    wordcontext matrices
wordcontext matrices suited measuring semantic similarity words  see
section       example  measure similarity two words cosine
angle corresponding row vectors wordcontext matrix  many
applications measures word similarity 
word similarity  deerwester et al         discovered measure word similarity comparing row vectors termdocument matrix  landauer dumais       
evaluated approach    multiple choice synonym questions test english foreign language  toefl   achieving human level performance        correct
wordcontext matrix       average non english us college applicant  
documents used landauer dumais average length     words 
seems short document  long context word  researchers soon
switched much shorter lengths  prefer call wordcontext matrices  instead termdocument matrices  lund burgess        used context window
ten words  schutze        used fifty word window     words  centered target
word   rapp        achieved       correct    toefl questions  using four word
context window    words  centered target word  removing stop words  
toefl results suggest performance improves context window shrinks  seems
immediate context word much important distant context
determining meaning word 
word clustering  pereira  tishby  lee        applied soft hierarchical clustering
row vectors wordcontext matrix  one experiment  words nouns
contexts verbs given nouns direct objects  another experiment 
words verbs contexts nouns direct objects given
verbs  schutzes        seminal word sense discrimination model used hard flat clustering
row vectors wordcontext matrix  context given window   
words  centered target word  pantel lin      a  applied soft flat clustering
wordcontext matrix  context based parsed text  algorithms
able discover different senses polysemous words  generating different clusters
sense  effect  different clusters correspond different concepts underlie
words 
word classification  turney littman        used wordcontext matrix classify words positive  honest  intrepid  negative  disturbing  superfluous   used
general inquirer  gi  lexicon  stone  dunphy  smith    ogilvie        evaluate
algorithms  gi lexicon includes        words  labeled     categories related
opinion  affect  attitude    turney littman hypothesize     categories
discriminated wordcontext matrix 
automatic thesaurus generation  wordnet popular tool research natural
language processing  fellbaum         creating maintaing lexical resources
    gi lexicon available http   www wjh harvard edu inquirer spreadsheet guide htm 

   

fifrom frequency meaning

labour intensive  natural wonder whether process automated
degree    task seen instance word clustering  when thesaurus
generated scratch  classification  when existing thesaurus automatically
extended   worthwhile consider task automatic thesaurus generation
separately clustering classification  due specific requirements thesaurus 
particular kind similarity appropriate thesaurus  see section      
several researchers used wordcontext matrices specifically task assisting
automating thesaurus generation  crouch        grefenstette        ruge        pantel  
lin      a  curran   moens        
word sense disambiguation  typical word sense disambiguation  wsd  system
 agirre   edmonds        pedersen        uses feature vector representation
vector corresponds token word  type  see section       however  leacock 
towell  voorhees        used wordcontext frequency matrix wsd 
vector corresponds type annotated sense tag  yuret yatbaz        applied
wordcontext frequency matrix unsupervised wsd  achieving results comparable
performance supervised wsd systems 
context sensitive spelling correction  people frequently confuse certain sets
words  there  theyre  their  confusions cannot detected simple dictionary based spelling checker  require context sensitive spelling correction 
wordcontext frequency matrix may used correct kinds spelling errors  jones
  martin        
semantic role labeling  task semantic role labeling label parts sentence according roles play sentence  usually terms connection
main verb sentence  erk        presented system wordcontext
frequency matrix used improve performance semantic role labeling  pennacchiotti  cao  basili  croce  roth        show wordcontext matrices reliably
predict semantic frame unknown lexical unit refers  good levels
accuracy  lexical unit induction important semantic role labeling  narrow
candidate set roles observed lexical unit 
query expansion  queries submitted search engines google yahoo 
often directly match terms relevant documents  alleviate
problem  process query expansion used generating new search terms
consistent intent original query  vsms form basis query semantics
models  cao  jiang  pei  he  liao  chen    li         methods represent queries
using session contexts  query cooccurrences user sessions  huang  chien   
oyang        jones  rey  madani    greiner         others use click contexts 
urls clicked result query  wen  nie    zhang        
textual advertising  pay per click advertising models  prevalent search engines
google yahoo   users pay keywords  called bidterms  used
display ads relevant queries issued users  scarcity data makes ad
matching difficult and  response  several techniques bidterm expansion using vsms
proposed  wordcontext matrix consists rows bidterms columns
    wordnet available http   wordnet princeton edu  

   

fiturney   pantel

 contexts  consist advertiser identifiers  gleich   zhukov        co bidded bidterms
 second order co occurrences   chang  pantel  popescu    gabrilovich        
information extraction  field information extraction  ie  includes named
entity recognition  ner  recognizing chunk text name entity 
person place   relation extraction  event extraction  fact extraction  pasca et
al         demonstrate wordcontext frequency matrix facilitate fact extraction 
vyas pantel        propose semi supervised model using wordcontext matrix
building iteratively refining arbitrary classes named entities 
    pairpattern matrices
pairpattern matrices suited measuring semantic similarity word pairs
patterns  see section       example  measure similarity two word
pairs cosine angle corresponding row vectors pairpattern
matrix  many applications measures relational similarity 
relational similarity  measure attributional similarity cosine
angle row vectors wordcontext matrix  measure relational
similarity cosine angle rows pairpattern matrix  approach
measuring relational similarity introduced turney et al         examined
detail turney littman         turney        evaluated approach
relational similarity     multiple choice analogy questions sat college
entrance test  achieving human level performance      correct pairpattern matrix
    correct average us college applicant   highest performance
far algorithm  best algorithm based attributional similarity accuracy
     turney         best non vsm algorithm achieves      veale        
pattern similarity  instead measuring similarity row vectors pair
pattern matrix  measure similarity columns  is  measure
pattern similarity  lin pantel        constructed pairpattern matrix
patterns derived parsed text  pattern similarity used infer one
phrase paraphrase another phrase  useful natural language generation 
text summarization  information retrieval  question answering 
relational clustering  bicici yuret        clustered word pairs representing
row vectors pairpattern matrix  davidov rappoport        first clustered
contexts  patterns  identified representative pairs context cluster 
used representative pairs automatically generate multiple choice analogy questions 
style sat analogy questions 
relational classification  chklovski pantel        used pairpattern matrix
classify pairs verbs semantic classes  example  taint   poison classified
strength  poisoning stronger tainting  assess   review classified enablement
 assessing enabled reviewing   turney        used pairpattern matrix classify
noun compounds semantic classes  example  flu virus classified cause  the
virus causes flu   home town classified location  the home located town  
weather report classified topic  the topic report weather  
relational search  cafarella  banko  etzioni        described relational search
task searching entities satisfy given semantic relations  example
   

fifrom frequency meaning

query relational search engine list x x causes cancer 
example  relation  cause  one terms relation  cancer  given
user  task search engine find terms satisfy users query 
organizers task   semeval       girju  nakov  nastase  szpakowicz  turney    yuret 
      envisioned two step approach relational search  first conventional search engine
would look candidate answers  relational classification system would filter
incorrect answers  first step manually simulated task   organizers
goal task   design systems second step  task attracted    teams
submitted    systems  nakov hearst        achieved good results using pairpattern
matrix 
automatic thesaurus generation  discussed automatic thesaurus generation
section      wordcontext matrices  arguably relational similarity relevant
attributional similarity thesaurus generation  example  information wordnet relations words rather words individually 
snow  jurafsky  ng        used pairpattern matrix build hypernym hyponym
taxonomy  whereas pennacchiotti pantel        built meronymy causation taxonomy  turney      b  showed pairpattern matrix distinguish synonyms
antonyms  synonyms non synonyms  taxonomically similar words  hair fur 
words merely semantically associated  cradle baby  
analogical mapping  proportional analogies form   b    c   d  means
b c d  example  mason   stone    carpenter   wood means mason stone
carpenter wood      multiple choice analogy questions sat college
entrance test  mentioned above  involve proportional analogies  pairpattern
matrix  solve proportional analogies selecting choice maximizes relational
similarity  e g   simr  mason   stone  carpenter   wood  high value   however  often
encounter analogies involve four terms  well known analogy
solar system rutherford bohr model atom contains least fourteen
terms  solar system  planet  attracts  revolves  sun  gravity  solar system 
mass  atom  revolves  atom  attracts  electromagnetism  nucleus 
charge  electron  turney      a  demonstrated handle complex 
systematic analogies decomposing sets proportional analogies 

   alternative approaches semantics
applications list section   necessarily require vsm approach 
application  many possible approaches  section  briefly
consider main alternatives 
underlying applications termdocument matrices  section      task
measuring semantic similarity documents queries  main alternatives
vsms task probabilistic models  traditional probabilistic retrieval
models information retrieval  van rijsbergen        baeza yates   ribeiro neto       
recent statistical language models inspired information theory  liu  
croft         idea statistical language models information retrieval measure
similarity query document creating probabilistic language model
   

fiturney   pantel

given document measuring probability given query according
language model 
progress information retrieval  distinction vsm approach
probabilistic approach becoming blurred  approach borrows ideas
other  language models typically involve multiplying probabilities  view
adding logs probabilities  makes language models look similar vsms 
applications wordcontext matrices  section      share task measuring
semantic similarity words  main alternatives vsms measuring word similarity
approaches use lexicons  wordnet  resnik        jiang   conrath       
hirst   st onge        leacock   chodrow        budanitsky   hirst         idea
view lexicon graph  nodes correspond word senses edges represent
relations words  hypernymy hyponymy  similarity two
words proportional length path graph joins two words 
several approaches measuring semantic similarity words combine vsm
lexicon  turney et al         pantel        patwardhan   pedersen        mohammad  
hirst         humans use dictionary definitions observations word usage 
natural expect best performance algorithms use distributional
lexical information 
pairpattern matrices  section      common task measuring semantic
similarity relations  wordcontext matrices  main alternatives approaches
use lexicons  rosario   hearst        rosario  hearst    fillmore        nastase
  szpakowicz        veale               idea reduce relational similarity
attributional similarity  simr  a   b  c   d  sima  a  c    sima  b  d   use lexicon
measure attributional similarity  discuss section      reduction
work general  however  reduction often good approximation 
evidence hybrid approach  combining vsm lexicon  beneficial  turney
et al         nastase  sayyad shirabad  sokolova    szpakowicz        

   future vector space models semantics
several authors criticized vsms  french   labiouse        pado   lapata       
morris   hirst        budanitsky   hirst         criticism stems
fact termdocument wordcontext matrices typically ignore word order  lsa 
instance  phrase commonly represented sum vectors individual
words phrase  hence phrases house boat boat house represented
vector  although different meanings  english  word order expresses
relational information  house boat boat house tool purpose relation 
house boat means tool purpose boat  house   a boat serves house   whereas boat
house means tool purpose house  boat   a house sheltering storing boats  
landauer        estimates     meaning english text comes word
choice remaining     comes word order  however  vsms inherently
limited     meaning text  mitchell lapata        propose composition
models sensitive word order  example  make simple additive model become
syntax aware  allow different weightings contributions vector components  constituents important composition therefore participate
   

fifrom frequency meaning

actively  clark pulman        assigned distributional meaning sentences using hilbert space tensor product  widdows ferraro         inspired quantum
mechanics  explores several operators modeling composition meaning  pairpattern
matrices sensitive order words pair  turney         thus
several ways handle word order vsms 
raises question  limits vsms semantics  semantics
represented vsms  much yet know represent
vsms  example  widdows        van rijsbergen        show disjunction 
conjunction  negation represented vectors  yet know
represent arbitrary statements first order predicate calculus  however  seems possible
future work may discover answers limitations 
survey  assumed vsms composed elements values
derived event frequencies  ties vsms form distributional hypothesis
 see sections           therefore limits vsms depend limits family
distributional hypotheses  statistical patterns word usage sufficient figure
people mean  arguably major open question vsms  answer
determine future vsms  strong argument one way other 
believe continuing progress vsms suggests far reaching
limits 

   conclusions
want information help person  use words make request
describe problem  person replies words  unfortunately  computers
understand human language  forced use artificial languages unnatural user
interfaces  science fiction  dream computers understand human language 
listen us talk us  achieve full potential computers  must enable
understand semantics natural language  vsms likely part
solution problem computing semantics 
many researchers struggled problem semantics come
conclusion meaning words closely connected statistics word usage
 section       try make intuition precise  soon find working
vectors values derived event frequencies  is  dealing vsms 
survey  organized past work vsms according structure
matrix  termdocument  wordcontext  pairpattern   believe structure
matrix important factor determining types applications
possible  linguistic processing  section    mathematical processing  section   
play smaller  but important  roles 
goal survey show breadth power vsms  introduce
vsms less familiar them  provide new perspective vsms
already familiar them  hope emphasis structure
matrix inspire new research  reason believe three matrix
types present exhaust possibilities  expect new matrix types new tensors
open applications vsms  seems possible us semantics
human language might one day captured kind vsm 
   

fiturney   pantel

acknowledgments
thanks annie zaenen prompting paper  thanks saif mohammad mariana
soffer comments  thanks arkady borkovsky eric crestan developing
distributed sparse matrix multiplication algorithm  marco pennacchiotti
invaluable comments  thanks anonymous reviewers jair helpful
comments suggestions 

references
acar  e     yener  b          unsupervised multiway data analysis  literature survey 
ieee transactions knowledge data engineering              
agirre  e     edmonds  p  g          word sense disambiguation  algorithms applications  springer 
ando  r  k          latent semantic space  iterative scaling improves precision interdocument similarity measurement  proceedings   rd annual acm sigir
conference research development information retrieval  sigir        pp 
       
ando  r  k     zhang  t          framework learning predictive structures
multiple tasks unlabeled data  journal machine learning research         
     
baeza yates  r     ribeiro neto  b          modern information retrieval  addison wesley 
barr  c   jones  r     regelson  m          linguistic structure english websearch queries  conference empirical methods natural language processing
 emnlp  
bayardo  r  j   ma  y     srikant  r          scaling pairs similarity search 
proceedings   th international conference world wide web  www     
pp          new york  ny  acm 
bicici  e     yuret  d          clustering word pairs answer analogy questions 
proceedings fifteenth turkish symposium artificial intelligence neural
networks  tainn        akyaka  mugla  turkey 
blei  d  m   ng  a  y     jordan  m  i          latent dirichlet allocation  journal
machine learning research             
brand  m          fast low rank modifications thin singular value decomposition 
linear algebra applications                
breese  j   heckerman  d     kadie  c          empirical analysis predictive algorithms
collaborative filtering  proceedings   th conference uncertainty
artificial intelligence  pp        morgan kaufmann 
brin  s     page  l          anatomy large scale hypertextual web search engine 
proceedings seventh world wide web conference  www    pp         
broder  a          resemblance containment documents  compression
complexity sequences  sequences     pp        ieee computer society 
   

fifrom frequency meaning

budanitsky  a     hirst  g          semantic distance wordnet  experimental 
application oriented evaluation five measures  proceedings workshop
wordnet lexical resources  second meeting north american
chapter association computational linguistics  naacl        pp       
pittsburgh  pa 
budanitsky  a     hirst  g          evaluating wordnet based measures semantic distance  computational linguistics               
bullinaria  j     levy  j          extracting semantic representations word cooccurrence statistics  computational study  behavior research methods         
       
buntine  w     jakulin  a          discrete component analysis  subspace  latent
structure feature selection  statistical optimization perspectives workshop
slsfs       pp       bohinj  slovenia  springer 
cafarella  m  j   banko  m     etzioni  o          relational web search  tech  rep   university washington  department computer science engineering  technical
report            
cao  g   nie  j  y     bai  j          integrating word relationships language models 
proceedings   th annual international acm sigir conference research
development information retrieval  sigir      pp          new york  ny 
acm 
cao  h   jiang  d   pei  j   he  q   liao  z   chen  e     li  h          context aware query
suggestion mining click through session data  proceeding   th acm
sigkdd international conference knowledge discovery data mining  kdd
     pp          acm 
carroll  j  d     chang  j  j          analysis individual differences multidimensional
scaling via n way generalization eckart young decomposition  psychometrika 
               
chang  w   pantel  p   popescu  a  m     gabrilovich  e          towards intent driven
bidterm suggestion  proceedings www     short paper   madrid  spain 
charikar  m  s          similarity estimation techniques rounding algorithms  proceedings thiry fourth annual acm symposium theory computing  stoc
     pp          acm 
chew  p   bader  b   kolda  t     abdelali  a          cross language information retrieval using parafac   proceedings   th acm sigkdd international
conference knowledge discovery data mining  kdd     pp          acm
press 
chiarello  c   burgess  c   richards  l     pollock  a          semantic associative
priming cerebral hemispheres  words do  words dont       sometimes 
places  brain language            
chklovski  t     pantel  p          verbocean  mining web fine grained semantic
verb relations  proceedings experimental methods natural language processing       emnlp      pp        barcelona  spain 
   

fiturney   pantel

choi  f  y  y          advances domain independent linear text segmentation 
proceedings  st meeting north american chapter association
computational linguistics  pp       
chu carroll  j     carpenter  b          vector based natural language call routing  computational linguistics                 
church  k          one term two   proceedings   th annual international
acm sigir conference research development information retrieval  pp 
       
church  k     hanks  p          word association norms  mutual information  lexicography  proceedings   th annual conference association computational linguistics  pp        vancouver  british columbia 
clark  s     pulman  s          combining symbolic distributional models meaning 
proceedings aaai spring symposium quantum interaction  pp       
collobert  r     weston  j          unified architecture natural language processing 
deep neural networks multitask learning  proceedings   th international
conference machine learning  icml      pp         
croft  w  b          clustering large files documents using single link method 
journal american society information science                 
crouch  c  j          cluster based approach thesaurus construction  proceedings
  th annual international acm sigir conference  pp          grenoble 
france 
curran  j  r     moens  m          improvements automatic thesaurus extraction 
unsupervised lexical acquisition  proceedings workshop acl special
interest group lexicon  siglex   pp        philadelphia  pa 
cutting  d  r   karger  d  r   pedersen  j  o     tukey  j  w          scatter gather 
cluster based approach browsing large document collections  proceedings
  th annual international acm sigir conference  pp         
dagan  i   lee  l     pereira  f  c  n          similarity based models word cooccurrence
probabilities  machine learning                
dang  h  t   lin  j     kelly  d          overview trec      question answering
track  proceedings fifteenth text retrieval conference  trec       
dasarathy  b          nearest neighbor  nn  norms  nn pattern classification techniques 
ieee computer society press 
davidov  d     rappoport  a          unsupervised discovery generic relationships using
pattern clusters evaluation automatically generated sat analogy questions 
proceedings   th annual meeting acl hlt  acl hlt      pp 
        columbus  ohio 
dean  j     ghemawat  s          mapreduce  simplified data processing large clusters 
communications acm                 
   

fifrom frequency meaning

deerwester  s  c   dumais  s  t   landauer  t  k   furnas  g  w     harshman  r  a 
        indexing latent semantic analysis  journal american society
information science  jasis                  
elsayed  t   lin  j     oard  d          pairwise document similarity large collections
mapreduce  proceedings association computational linguistics
human language technology conference       acl     hlt   short papers  pp 
        columbus  ohio  association computational linguistics 
erk  k          simple  similarity based model selectional preferences  proceedings
  th annual meeting association computational linguistics  pp     
      prague  czech republic 
erk  k     pado  s          structured vector space model word meaning context 
proceedings      conference empirical methods natural language
processing  emnlp      pp          honolulu  hi 
fellbaum  c   ed            wordnet  electronic lexical database  mit press 
firth  j  r          synopsis linguistic theory           studies linguistic
analysis  pp       blackwell  oxford 
foltz  p  w   laham  d     landauer  t  k          intelligent essay assessor  applications educational technology  interactive multimedia electronic journal
computer enhanced learning        
forman  g          extensive empirical study feature selection metrics text classification  journal machine learning research              
french  r  m     labiouse  c          four problems extracting human semantics
large text corpora  proceedings   th annual conference cognitive
science society 
furnas  g  w   landauer  t  k   gomez  l  m     dumais  s  t          statistical semantics  analysis potential performance keyword information systems  bell
system technical journal                   
gentner  d          structure mapping  theoretical framework analogy  cognitive
science                
gilbert  j  r   moler  c     schreiber  r          sparse matrices matlab  design
implementation  siam journal matrix analysis applications                 
girju  r   nakov  p   nastase  v   szpakowicz  s   turney  p     yuret  d          semeval     task     classification semantic relations nominals  proceedings
fourth international workshop semantic evaluations  semeval        pp 
      prague  czech republic 
gleich  d     zhukov  l          svd based term suggestion ranking system 
proceedings fourth ieee international conference data mining  icdm
     pp          ieee computer society 
golub  g  h     van loan  c  f          matrix computations  third edition   johns
hopkins university press  baltimore  md 
   

fiturney   pantel

gorman  j     curran  j  r          scaling distributional similarity large corpora 
proceedings   st international conference computational linguistics
  th annual meeting association computational linguistics  acl       
pp          association computational linguistics 
gorrell  g          generalized hebbian algorithm incremental singular value decomposition natural language processing  proceedings   th conference
european chapter association computational linguistics  eacl      pp 
      
gospodnetic  o     hatcher  e          lucene action  manning publications 
grefenstette  g          explorations automatic thesaurus discovery  kluwer 
harris  z          distributional structure  word                  
harshman  r          foundations parafac procedure  models conditions
explanatory multi modal factor analysis  ucla working papers phonetics     
hearst  m          texttiling  segmenting text multi paragraph subtopic passages 
computational linguistics               
hirst  g     st onge  d          lexical chains representations context detection
correction malapropisms  fellbaum  c   ed    wordnet  electronic
lexical database  pp          mit press 
hofmann  t          probabilistic latent semantic indexing  proceedings   nd
annual acm conference research development information retrieval  sigir      pp        berkeley  california 
huang  c  k   chien  l  f     oyang  y  j          relevant term suggestion interactive
web search based contextual information query session logs  journal
american society information science technology                 
hull  d          stemming algorithms  case study detailed evaluation  journal
american society information science               
jain  a   murty  n     flynn  p          data clustering  review  acm computing
surveys                 
jarmasz  m     szpakowicz  s          rogets thesaurus semantic similarity 
proceedings international conference recent advances natural language
processing  ranlp      pp          borovets  bulgaria 
jiang  j  j     conrath  d  w          semantic similarity based corpus statistics
lexical taxonomy  proceedings international conference research
computational linguistics  rocling x   pp        tapei  taiwan 
johnson  h     martin  j          unsupervised learning morphology english
inuktitut  proceedings hlt naacl       pp       
jones  m  p     martin  j  h          contextual spelling correction using latent semantic analysis  proceedings fifth conference applied natural language
processing  pp          washington  dc 
   

fifrom frequency meaning

jones  r   rey  b   madani  o     greiner  w          generating query substitutions 
proceedings   th international conference world wide web  www     
pp          new york  ny  acm 
jones  w  p     furnas  g  w          pictures relevance  geometric analysis
similarity measures  journal american society information science         
       
kanerva  p          sparse distributed memory related models  hassoun  m  h 
 ed    associative neural memories  pp        oxford university press  new york 
ny 
karlgren  j     sahlgren  m          words understanding  uesaka  y   kanerva 
p     asoh  h   eds    foundations real world intelligence  pp          csli
publications 
kim  s  m   pantel  p   chklovski  t     pennacchiotti  m          automatically assessing
review helpfulness  proceedings      conference empirical methods
natural language processing  pp         
kolda  t     bader  b          tensor decompositions applications  siam review 
               
konchady  m          building search applications  lucene  lingpipe  gate  mustru
publishing 
kraaij  w     pohlmann  r          viewing stemming recall enhancement  proceedings   th annual international acm sigir conference  pp       
lakoff  g          women  fire  dangerous things  university chicago press 
chicago  il 
landauer  t  k          computational basis learning cognition  arguments
lsa  ross  b  h   ed    psychology learning motivation  advances
research theory  vol      pp        academic press 
landauer  t  k     dumais  s  t          solution platos problem  latent semantic analysis theory acquisition  induction  representation knowledge 
psychological review                  
landauer  t  k     littman  m  l          fully automatic cross language document
retrieval using latent semantic indexing  proceedings sixth annual conference
uw centre new oxford english dictionary text research  pp    
    waterloo  ontario 
landauer  t  k   mcnamara  d  s   dennis  s     kintsch  w          handbook latent
semantic analysis  lawrence erlbaum  mahwah  nj 
lavrenko  v     croft  w  b          relevance based language models  proceedings
  th annual international acm sigir conference research development
information retrieval  sigir      pp          new york  ny  acm 
leacock  c     chodrow  m          combining local context wordnet similarity
word sense identification  fellbaum  c   ed    wordnet  electronic lexical
database  mit press 
   

fiturney   pantel

leacock  c   towell  g     voorhees  e          corpus based statistical sense resolution 
proceedings arpa workshop human language technology  pp         
lee  d  d     seung  h  s          learning parts objects nonnegative matrix
factorization  nature              
lee  l          measures distributional similarity  proceedings   th annual
meeting association computational linguistics  pp       
lemaire  b     denhiere  g          effects high order co occurrences word semantic
similarity  current psychology letters  behaviour  brain   cognition         
lin  d          automatic retrieval clustering similar words  roceedings
  th international conference computational linguistics  pp          association
computational linguistics 
lin  d     pantel  p          dirt discovery inference rules text  proceedings
acm sigkdd conference knowledge discovery data mining       pp 
       
linden  g   smith  b     york  j          amazon com recommendations  item to item
collaborative filtering  ieee internet computing       
liu  x     croft  w  b          statistical language modeling information retrieval 
annual review information science technology          
lovins  j  b          development stemming algorithm  mechanical translation
computational linguistics           
lowe  w          towards theory semantic space  proceedings twenty first
annual conference cognitive science society  pp         
lund  k     burgess  c          producing high dimensional semantic spaces lexical
co occurrence  behavior research methods  instruments  computers             
    
lund  k   burgess  c     atchley  r  a          semantic associative priming highdimensional semantic space  proceedings   th annual conference
cognitive science society  pp         
manning  c     schutze  h          foundations statistical natural language processing 
mit press  cambridge  ma 
manning  c  d   raghavan  p     schutze  h          introduction information retrieval 
cambridge university press  cambridge  uk 
miller  g   leacock  c   tengi  r     bunker  r          semantic concordance 
proceedings  rd darpa workshop human language technology  pp     
    
minnen  g   carroll  j     pearce  d          applied morphological processing english 
natural language engineering                
mitchell  j     lapata  m          vector based models semantic composition  proceedings acl     hlt  pp          columbus  ohio  association computational
linguistics 
   

fifrom frequency meaning

mitchell  t          machine learning  mcgraw hill  columbus  oh 
mohammad  s     hirst  g          distributional measures concept distance  taskoriented evaluation  proceedings conference empirical methods natural
language processing  emnlp        pp       
monay  f     gatica perez  d          image auto annotation latent space models 
proceedings eleventh acm international conference multimedia  pp 
       
morris  j     hirst  g          non classical lexical semantic relations  workshop
computational lexical semantics  hlt naacl     boston  ma 
nakov  p     hearst  m          ucb  system description semeval task    proceedings fourth international workshop semantic evaluations  semeval       
pp          prague  czech republic 
nakov  p     hearst  m          solving relational similarity problems using theweb
corpus  proceedings acl     hlt  pp          columbus  ohio 
nastase  v   sayyad shirabad  j   sokolova  m     szpakowicz  s          learning nounmodifier semantic relations corpus based wordnet based features  proceedings   st national conference artificial intelligence  aaai      pp 
       
nastase  v     szpakowicz  s          exploring noun modifier semantic relations 
fifth international workshop computational semantics  iwcs     pp         
tilburg  netherlands 
niwa  y     nitta  y          co occurrence vectors corpora vs  distance vectors
dictionaries  proceedings   th international conference computational
linguistics  pp          kyoto  japan 
nosofsky  r          attention  similarity  identification categorization relationship 
journal experimental psychology  general                
ogden  c  k          basic english  general introduction rules grammar 
kegan paul  trench  trubner co 
pado  s     lapata  m          constructing semantic space models parsed corpora 
proceedings   st annual meeting association computational linguistics  pp          sapporo  japan 
pado  s     lapata  m          dependency based construction semantic space models 
computational linguistics                 
pang  b   lee  l     vaithyanathan  s          thumbs up  sentiment classification using
machine learning techniques  proceedings conference empirical methods
natural language processing  emnlp   pp        philadelphia  pa 
pantel  p          inducing ontological co occurrence vectors  proceedings association
computational linguistics  acl      pp         
pantel  p     lin  d          spamcop  spam classification organization program 
learning text categorization  papers aaai      workshop  pp       
   

fiturney   pantel

pantel  p     lin  d       a   discovering word senses text  proceedings
eighth acm sigkdd international conference knowledge discovery data
mining  pp          edmonton  canada 
pantel  p     lin  d       b   document clustering committees  proceedings
  th annual international acm sigir conference  pp         
pasca  m   lin  d   bigham  j   lifchits  a     jain  a          names similarities
web  fact extraction fast lane  proceedings   st international
conference computational linguistics   th annual meeting acl  pp 
        sydney  australia 
patwardhan  s     pedersen  t          using wordnet based context vectors estimate
semantic relatedness concepts  proceedings workshop making
sense sense   th conference european chapter association
computational linguistics  eacl        pp     
pedersen  t          unsupervised corpus based methods wsd  word sense disambiguation  algorithms applications  pp          springer 
pennacchiotti  m   cao  d  d   basili  r   croce  d     roth  m          automatic induction
framenet lexical units  proceedings      conference empirical methods
natural language processing  emnlp      pp          honolulu  hawaii 
pennacchiotti  m     pantel  p          ontologizing semantic relations  proceedings
  st international conference computational linguistics   th annual
meeting association computational linguistics  pp          association
computational linguistics 
pereira  f   tishby  n     lee  l          distributional clustering english words 
proceedings   st annual meeting association computational linguistics 
pp         
porter  m          algorithm suffix stripping  program                 
rabin  m  o          fingerprinting random polynomials  tech  rep   center research
computing technology  harvard university  technical report tr       
rapp  r          word sense discovery based sense descriptor dissimilarity  proceedings ninth machine translation summit  pp         
ravichandran  d   pantel  p     hovy  e          randomized algorithms nlp  using
locality sensitive hash function high speed noun clustering  proceedings
  rd annual meeting association computational linguistics  acl      pp 
        morristown  nj  association computational linguistics 
resnick  p   iacovou  n   suchak  m   bergstrom  p     riedl  j          grouplens  open
architecture collaborative filtering netnews  proceedings acm     
conference computer supported cooperative work  pp          acm press 
resnik  p          using information content evaluate semantic similarity taxonomy 
proceedings   th international joint conference artificial intelligence
 ijcai      pp          san mateo  ca  morgan kaufmann 
   

fifrom frequency meaning

rosario  b     hearst  m          classifying semantic relations noun compounds
via domain specific lexical hierarchy  proceedings      conference
empirical methods natural language processing  emnlp      pp       
rosario  b   hearst  m     fillmore  c          descent hierarchy  selection
relational semantics  proceedings   th annual meeting association
computational linguistics  acl      pp         
rosch  e     lloyd  b          cognition categorization  lawrence erlbaum  hillsdale 
nj 
ruge  g          automatic detection thesaurus relations information retrieval applications  freksa  c   jantzen  m     valk  r   eds    foundations computer
science  pp          springer 
sahami  m   dumais  s   heckerman  d     horvitz  e          bayesian approach
filtering junk e mail  proceedings aaai    workshop learning text
categorization 
sahlgren  m          introduction random indexing  proceedings methods
applications semantic indexing workshop  th international conference
terminology knowledge engineering  tke   copenhagen  denmark 
sahlgren  m          word space model  using distributional analysis represent syntagmatic paradigmatic relations words high dimensional vector spaces 
ph d  thesis  department linguistics  stockholm university 
salton  g          smart retrieval system  experiments automatic document processing  prentice hall  upper saddle river  nj 
salton  g     buckley  c          term weighting approaches automatic text retrieval 
information processing management                 
salton  g   wong  a     yang  c  s          vector space model automatic indexing 
communications acm                  
sarawagi  s     kirpal  a          efficient set joins similarity predicates  proceedings      acm sigmod international conference management data
 sigmod      pp          new york  ny  acm 
scholkopf  b   smola  a  j     muller  k  r          kernel principal component analysis 
proceedings international conference artificial neural networks  icann       pp          berlin 
schutze  h          automatic word sense discrimination  computational linguistics         
      
schutze  h     pedersen  j          vector model syntagmatic paradigmatic
relatedness  making sense words  proceedings conference  pp         
oxford  england 
sebastiani  f          machine learning automated text categorization  acm computing
surveys  csur               
shannon  c          mathematical theory communication  bell system technical
journal                     
   

fiturney   pantel

singhal  a   salton  g   mitra  m     buckley  c          document length normalization 
information processing management                 
smith  e   osherson  d   rips  l     keane  m          combining prototypes  selective
modification model  cognitive science                 
snow  r   jurafsky  d     ng  a  y          semantic taxonomy induction heterogenous evidence  proceedings   st international conference computational
linguistics   th annual meeting acl  pp         
sparck jones  k          statistical interpretation term specificity application
retrieval  journal documentation               
spearman  c          general intelligence  objectively determined measured  american journal psychology             
sproat  r     emerson  t          first international chinese word segmentation bakeoff  proceedings second sighan workshop chinese language processing 
pp          sapporo  japan 
stone  p  j   dunphy  d  c   smith  m  s     ogilvie  d  m          general inquirer 
computer approach content analysis  mit press  cambridge  ma 
tan  b     peng  f          unsupervised query segmentation using generative language
models wikipedia  proceeding   th international conference world
wide web  www      pp          new york  ny  acm 
tellex  s   katz  b   lin  j   fern  a     marton  g          quantitative evaluation
passage retrieval algorithms question answering  proceedings   th annual
international acm sigir conference research development information
retrieval  sigir   pp       
tucker  l  r          mathematical notes three mode factor analysis  psychometrika                 
turney  p  d          mining web synonyms  pmi ir versus lsa toefl 
proceedings twelfth european conference machine learning  ecml     
pp          freiburg  germany 
turney  p  d          measuring semantic similarity latent relational analysis  proceedings nineteenth international joint conference artificial intelligence
 ijcai      pp            edinburgh  scotland 
turney  p  d          similarity semantic relations  computational linguistics         
       
turney  p  d          empirical evaluation four tensor decomposition algorithms  tech 
rep   institute information technology  national research council canada 
technical report erb      
turney  p  d       a   latent relation mapping engine  algorithm experiments 
journal artificial intelligence research             
turney  p  d       b   uniform approach analogies  synonyms  antonyms  associations  proceedings   nd international conference computational
linguistics  coling        pp          manchester  uk 
   

fifrom frequency meaning

turney  p  d     littman  m  l          measuring praise criticism  inference
semantic orientation association  acm transactions information systems 
               
turney  p  d     littman  m  l          corpus based learning analogies semantic
relations  machine learning                  
turney  p  d   littman  m  l   bigham  j     shnayder  v          combining independent
modules solve multiple choice synonym analogy problems  proceedings
international conference recent advances natural language processing
 ranlp      pp          borovets  bulgaria 
van de cruys  t          non negative tensor factorization model selectional preference
induction  proceedings workshop geometric models natural language
semantics  gems      pp        athens  greece 
van rijsbergen  c  j          geometry information retrieval  cambridge university
press  cambridge  uk 
van rijsbergen  c  j          information retrieval  butterworths 
veale  t          analogical thesaurus  proceedings   th innovative applications artificial intelligence conference  iaai        pp          acapulco 
mexico 
veale  t          wordnet sits sat  knowledge based approach lexical analogy 
proceedings   th european conference artificial intelligence  ecai       
pp          valencia  spain 
vozalis  e     margaritis  k          analysis recommender systems algorithms 
proceedings  th hellenic european conference computer mathematics
applications  hercma        athens  greece 
vyas  v     pantel  p          semi automatic entity set refinement  proceedings
naacl     boulder  co 
weaver  w          translation  locke  w     booth  d   eds    machine translation
languages  fourteen essays  mit press  cambridge  ma 
weeds  j   weir  d     mccarthy  d          characterising measures lexical distributional similarity  proceedings   th international conference computational linguistics  coling      pp            association computational
linguistics 
wei  x   peng  f     dumoulin  b          analyzing web text association disambiguate
abbreviation queries  proceedings   st annual international acm sigir
conference research development information retrieval  sigir      pp 
        new york  ny  acm 
wen  j  r   nie  j  y     zhang  h  j          clustering user queries search engine 
proceedings   th international conference world wide web  www     
pp          new york  ny  acm 
widdows  d          geometry meaning  center study language
information  stanford  ca 
   

fiturney   pantel

widdows  d     ferraro  k          semantic vectors  scalable open source package
online technology management application  proceedings sixth international
conference language resources evaluation  lrec        pp           
witten  i  h     frank  e          data mining  practical machine learning tools
techniques java implementations  morgan kaufmann  san francisco 
wittgenstein  l          philosophical investigations  blackwell  translated g e m 
anscombe 
wolfe  m  b  w   schreiner  m  e   rehder  b   laham  d   foltz  p  w   kintsch  w    
landauer  t  k          learning text  matching readers texts latent
semantic analysis  discourse processes             
yang  y          evaluation statistical approaches text categorization  information
retrieval              
yuret  d     yatbaz  m  a          noisy channel model unsupervised word sense
disambiguation  computational linguistics  review 
zamir  o     etzioni  o          grouper  dynamic clustering interface web search
results  computer networks  international journal computer telecommunications networking                    
zhao  y     karypis  g          evaluation hierarchical clustering algorithms document datasets  proceedings eleventh international conference information knowledge management  pp          mclean  virginia 

   


