journal artificial intelligence research                  

submitted        published     

investigation mathematical programming
finite horizon decentralized pomdps
raghav aras

raghav aras gmail com

ims  suplec metz
  rue edouard belin  metz technopole
      metz   france

alain dutech

alain dutech loria fr

maia   loria inria
campus scientifique   bp    
      vandoeuvre les nancy   france

abstract
decentralized planning uncertain environments complex task generally dealt
using decision theoretic approach  mainly framework decentralized partially observable markov decision processes  dec pomdps   although dec pomdps
general powerful modeling tool  solving task overwhelming
complexity doubly exponential  paper  study alternate formulation dec pomdps relying sequence form representation policies 
formulation  show derive mixed integer linear programming  milp  problems
that  solved  give exact optimal solutions dec pomdps  show
milps derived either using combinatorial characteristics optimal
solutions dec pomdps using concepts borrowed game theory 
experimental validation classical test problems dec pomdp literature 
compare approach existing algorithms  results show mathematical programming outperforms dynamic programming less efficient forward search  except
particular problems 
main contributions work use mathematical programming decpomdps better understanding dec pomdps solutions  besides 
argue alternate representation dec pomdps could helpful designing
novel algorithms looking approximate solutions dec pomdps 

   introduction
framework decentralized partially observable markov decision processes  decpomdps  used model problem designing system made autonomous
agents need coordinate order achieve joint goal  solving dec pomdps
untractable task belong class nexp complete problems  see section      
paper  dec pomdps reformulated sequence form dec pomdps
derive mixed integer linear programs solved using efficient solvers
order design exact optimal solutions finite horizon dec pomdps  main
motivation investigate benefits limits novel approach get
better understanding dec pomdps  see section       practical level  provide
new algorithms heuristics solving dec pomdps evaluate classical
problems  see section      
c
    
ai access foundation  rights reserved 

fiaras   dutech

    context
one main goals artificial intelligence build artificial agents exhibit
intelligent behavior  agent entity situated environment perceive
sensors act upon using actuators  concept planning  i e   select
sequence actions order reach goal  central field artificial
intelligence years  notion intelligent behavior difficult assess
measure  prefer refer concept rational behavior formulated russell
norvig         consequence  work presented uses decision theoretic
approach order build agents take optimal actions uncertain partially
unknown environment 
particularly interested cooperative multi agent systems multiple
independent agents limited perception environment must interact coordinate order achieve joint task  central process full knowledge state
system control agents  contrary  agent autonomous
entity must execute actions itself  setting blessing  agent
ideally deal small part problem  curse  coordination
cooperation harder develop enforce 
decision theoretic approach rational behavior relies mostly framework
markov decision processes  mdp   puterman         system seen sequence
discrete states stochastic dynamics  particular states giving positive negative
reward  process divided discrete decision periods  number periods
called horizon mdp  periods  action chosen
influence transition process next state  using right actions
influence transition probabilities states  objective controller
system maximize long term return  often additive function reward
earned given horizon  controller knows dynamics system 
made transition function reward function  algorithms derived field
dynamic programming  see bellman        allow controller compute optimal
deterministic policy  i e   decision function associates optimal action every
state expected long term return optimal  process called planning
mdp community 
fact  using mdp framework  quite straightforward model problem
one agent full complete knowledge state system  agents 
especially multi agent setting  generally able determine complete
exact state system noisy  faulty limited sensors
nature problem itself  consequence  different states system observed
similar agent problem different optimal actions taken
states  one speaks perceptual aliasing  extension mdps called partially
observable markov decisions processes  pomdps  deals explicitly phenomenon
allows single agent compute plans setting provided knows conditional
probabilities observations given state environment  cassandra  kaelbling   
littman        
pointed boutilier         multi agent problems could solved mdps
considered centralized point view planning control  here  although
   

fimathematical programming dec pomdps

planning centralized process  interested decentralized settings every
agent executes policy  even agents could instantly communicate observation  consider problems joint observation resulting communications would still enough identify state system  framework
decentralized partially observable markov decision processes  dec pomdp  proposed
bernstein  givan  immerman  zilberstein        takes account decentralization
control partial observability  dec pomdp  looking optimal joint
policies composed one policy agent  individual policies
computed centralized way independently executed agents 
main limitation dec pomdps provably untractable
belong class nexp complete problems  bernstein et al          concretely 
complexity result implies that  worst case  finding optimal joint policy finite
horizon dec pomdp requires time exponential horizon one always make
good choices  complexity  algorithms finding exact
optimal solutions dec pomdps  they doubly exponential complexity 
look approximate solutions  discussed detailed
work oliehoek  spaan  vlassis         algorithms follow either dynamic
programming approach forward search approach adapting concepts algorithms
designed pomdps 
yet  concept decentralized planning focus quite large body
previous work fields research  example  team decision problem  radner 
       later formulated markov system field control theory anderson
moore         led markov team decision problem  pynadath   tambe        
field mathematics  abundant literature game theory brings new way
looking multi agent planning  particular  dec pomdp finite horizon
thought game extensive form imperfect information identical interests
 osborne   rubinstein        
taking inspiration field game theory mathematical programming design exact algorithms solving dec pomdps precisely subject contribution
field decentralized multi agent planning 
    motivations
main objective work investigate use mathematical programming 
especially mixed integer linear programs  milp   diwekar         solving decpomdps  motivation relies fact field linear programming quite
mature great interest industry  consequence  exist many efficient
solvers mixed integer linear programs want see efficient solvers
perform framework dec pomdps 
therefore  reformulate dec pomdp solve mixed integer linear
program  shown article  two paths lead mathematical programs  one
grounded work koller  megiddo  von stengel         koller megiddo
       von stengel         another one grounded combinatorial considerations 
methods rely special reformulation dec pomdps called
   

fiaras   dutech

sequence form dec pomdps policy defined histories  i e   sequences
observations actions  generate applied dec pomdp 
basic idea work select  among histories dec pomdp 
histories part optimal policy  end  optimal solution
milp presented article assign positive weight history decpomdp every history non negative weight part optimal policy
dec pomdp  number possible histories exponential horizon
problem  complexity naive search optimal set histories doubly
exponential  therefore  idea appears untractable useless 
nevertheless  show combining efficiency milp solvers quite
simple heuristics leads exact algorithms compare quite well existing exact
algorithms  fact  sequence form dec pomdps need memory space exponential
size problem  even solving milps exponential size
milp thus leads doubly exponential complexity sequence form based algorithms 
argue sequence form milps compare quite well dynamic programming thanks
optimized industrial milp solvers cplex 
still  investigations experiments mathematical programming decpomdps solely aim finding exact solutions dec pomdps  main motivation better understanding dec pomdps limits benefits
mathematical programming approach  hope knowledge help deciding
extent mathematical programming sequence form dec pomdps used
design novel algorithms look approximate solutions dec pomdps 
    contributions
paper develop new algorithms order find exact optimal joint policies
dec pomdps  main inspiration comes work koller  von stegel
megiddo shows solve games extensive form imperfect information
identical interests  find nash equilibrium kind game  koller et al  
      koller   megiddo        von stengel         algorithms caused breakthrough
memory space requirement approach linear size game whereas
canonical algorithms required space exponential size game 
breakthrough mostly due use new formulation policy call
sequence form 
main contribution  detailed section      adapt sequence form
introduced koller  von stegel megiddo framework dec pomdps  koller
et al         koller   megiddo        von stengel         result  possible
formulate resolution dec pomdp special kind mathematical program
still solved quite efficiently  mixed linear program variables
required either      adaptation resulting mixed integer linear
program straightforward  fact  koller  von stegel megiddo could find
one nash equilibrium   agent game  needed dec pomdps find
set policies  called joint policy  corresponds nash equilibrium
highest value  finding one nash equilibrium already complex task
enough  besides  whereas koller  von stegel megiddo algorithms could applied
   

fimathematical programming dec pomdps

  agent games  extend approach solve dec pomdps arbitrary
number agents  constitutes important contribution 
order formulate dec pomdps milps  analyze detail structure
optimal joint policy dec pomdp  joint policy sequence form expressed
set individual policies described set possible trajectories
agents dec pomdp  combinatorial considerations individual
histories  well constraints ensure histories define valid joint policy
heart formulation dec pomdp mixed linear program  developped
sections      thus  another contribution work better understanding
properties optimal solutions dec pomdps  knowledge might lead
formulation new approximate algorithms dec pomdps 
another important contribution work introduce heuristics boosting performance mathematical programs propose  see section    
heuristics take advantage succinctness dec pomdp model knowledge acquired regarding structure optimal policies  consequently  able
reduce size mathematical programs  resulting reducing time taken
solve them   heuristics constitute important pre processing step solving
programs  present two types heuristics  elimination extraneous histories
reduces size mixed integer linear programs introduction cuts
mixed integer linear programs reduces time taken solve program 
practical level  article presents three different mixed integer linear
programs  two directly derived work koller  von stegel megiddo
 see table      third one based solely combinatorial considerations
individual policies histories  see table     theoretical validity formulations backed several theorems  conducted experimental evaluations
algorithms heuristics several classical dec pomdp problems  thus
able confirm algorithms quite comparable dynamic programming exact
algorithms outperformed forward search algorithms gmaa   oliehoek et al  
       problems  though  milps indeed faster one order magnitude
two gmaa  
    overview article
remainder article organized follows  section   introduces formalism
dec pomdp background classical algorithms  usually based dynamic
programing  expose reformulation dec pomdp sequence form
section   define various notions needed sequence form  section   
show use combinatorial properties sequence form policies derive first
mixed integer linear program  milp  table    solving dec pomdp  using game
theoretic concepts nash equilibrium  take inspiration previous work games
extensive form design two milps solving dec pomdp  tables       
milps smaller size detailed derivation presented section   
contributed heuristics speed practical resolutions various milps make
core section    section   presents experimental validations milp based
algorithms classical benchmarks dec pomdp literature well randomly
   

fiaras   dutech

built problems  finally  section   analyzes discusses work conclude
paper section   

   dec pomdp
section gives formal definition decentralized partially observed markov decision
processes introduced bernstein et al          described  solution decpomdp policy defined space information sets optimal value 
sections ends quick overview classical methods developed
solve dec pomdps 
    formal definition
dec pomdp defined tuple   h i  s   ai    p   oi    g  r    where 
           n  set agents 
finite set states  set probability distributions shall denoted
 s   members  s  shall called belief states 
agent i  ai set actions    ii ai denotes set joint
actions 
p          state transition function  s 
a  p s  a    probability state problem period if 
period    state agents performed joint action a  thus 
time period    pair states s  joint action a 
holds 
p s  a      pr st    st    s    a  
thus   s  a  p  defines discrete state  discrete time controlled markov process 
agent i  oi set observations    ii oi denotes set joint
observations 
g          joint observation function  a 
s  g a  s  o  probability agents receive
joint observation  that is  agent receives observation oi   state
problem period previous period agents took joint
action a  thus  time period    joint action a  state
joint observation o  holds 
g a  s  o    pr ot   o st   s  at    a  
r   r reward function  a  r s  a  r
reward obtained agents take joint action state
process s 
   

fimathematical programming dec pomdps

horizon problem  agents allowed joint actions
process halts 
 s  initial state dec pomdp  s   s  denotes
probability state problem first period s 
said  s  p define controlled markov process next state depends
previous state joint action chosen agents  agents
access state process rely observations  generally
partial noisy  state  specified observation function g  time
time  agents receive non zero reward according reward function r 

n 

n 

  

  

s 

s 

nt

n 

 t

  

s 

nt

 t

st

figure    dec pomdp  every period process  environment state
st   every agent receives observations oti decides action ati   joint
action hat    at      atn alters state process 
specifically  illustrated figure    control dec pomdp n
agents unfolds discrete time periods           t follows  period t 
process state denoted st s  first period      state s  chosen
according agents take actions a i   period     afterward  agent
takes action denoted ati ai according agents policy 
agents take joint action   hat    at      atn i  following events occur 
   agents obtain reward r st     
   state st   determined according function p arguments st  
   agent receives observation ot  
oi   joint observation ot    

t  
t  
t    
hot  
    o      determined function g arguments
   period changes     
paper  dec pomdp interested following properties 
   

fiaras   dutech

horizon finite known agents 
agents cannot infer exact state system joint observations  this
general setting dec pomdps  
agents observe actions observations agents 
aware observations reward 
agents perfect memory past  base choice action
sequence past actions observations  speak perfect recall setting 
transition observation functions stationary  meaning depend
period t 
solving dec pomdp means finding agents policies  i e   decision functions 
optimize given criterion based rewards received  criterion work
called cumulative reward defined by 
 
 
x




e
r s   ha    a            i 
   
t  

e mathematical expectation 
    example dec pomdp
problem known decentralized tiger problem  hereby denoted ma tiger  
introduced nair  tambe  yokoo  pynadath  marsella         widely used
test dec pomdps algorithms  variation problem previously introduced
pomdps  i e   dec pomdps one agent  kaelbling  littman  cassandra
       
problem  given two agents confronted two closed doors  behind one
door tiger  behind escape route  agents know door
leads what  agent  independently other  open one two doors
listen carefully order detect tiger  either opens wrong door 
lives imperiled  open escape door  free 
agents limited time decide door open  use time
gather information precise location tiger listening carefully detect
location tiger  problem formalized dec pomdp with 
two states tiger either behind left door  sl   right door  sr   
two agents  must decide act 
three actions agent  open left door  al    open right door  ar  
listen  ao   
two observations  thing agent observe hear tiger
left  ol   right  or   
   

fimathematical programming dec pomdps

initial state chosen according uniform distribution s  long door
remains closed  state change but  one door opened  state reset
either sl sr equal probability  observations noisy  reflecting difficulty
detecting tiger  example  tiger left  action ao produces
observation ol     time  agents perform ao   joint observation
 ol  ol   occurs probability                   reward function encourages
agents coordinate actions as  example  reward open escape
door       bigger one listens opens good door      
full state transition function  joint observation function reward function described
work nair et al         
    information sets histories
information set agent sequence  a   o   a   o   ot   even length
elements odd positions actions agent  members ai   even
positions observations agent  members oi    information set length  
shall called null information set  denoted   information set length  
shall called terminal information set  set information sets lengths less
equal   shall denoted  
define history agent sequence  a   o    a    o   ot  at   odd
length elements odd positions actions agent  members ai  
even positions observations agent  members oi    define length
history number actions history  t example   history
length shall called terminal history  histories lengths less shall called
non terminal histories  history null length shall denoted   information
set associated history h  denoted  h   information set composed removing
h last action  h history observation  h o information set 
shall denote hit set possible histories length agent i  thus  hi 
set actions ai   shall denote hi set histories agent lengths
less equal   size ni hi thus 
ni    hi    

pt


t 
t    ai    oi  

   ai  

  ai   oi   t  
 
 ai   oi    

   

set hit terminal histories agent shall denoted ei   set hi  hit
non terminal histories agent shall denoted ni  
tuple hh    h            hn made one history agent called joint history 
tuple obtained removing history hi joint history h noted hi called
i reduced joint history 
example coming back ma tiger example  set valid histories could be     ao   
 ao  ol  ao     ao  or  ao     ao  ol  ao  ol  ao     ao  ol  ao  or  ar     ao  or  ao  ol  ao    ao  or  ao  or  ar   
incidently  set histories corresponds support policy  i e   histories
generated using policy  figure    explained next section 
   

fiaras   dutech

    policies
period time  policy must tell agent action choose  choice
based whatever past present knowledge agent process
time t  one possibility define individual policy agent mapping
information sets actions  formally 
   ai  

   

among set policies  three families usually distinguished 
pure policies  pure deterministic policy maps given information set one
unique action  set pure policies agent denoted   pure policies
could defined using trajectories past observations since actions 
chosen deterministically  reconstructed observations 
mixed policies  mixed policy probability distribution set pure
policies  thus  agent using mixed policy control dec pomdp using
pure policy randomly chosen set pure policies 
stochastic policies  stochastic policy general formulation associates
probability distribution actions history 
come back ma tiger problem  section       figure   gives possible policy
horizon    shown  policy classically represented action observation tree 
kind tree  branch labelled observation  given sequence past
observations  one starts root node follows branches action
node  node contains action executed agent seen
sequence observations 
observation sequence
chosen action

ol
ao


ao


ao

ol  ol
al

ol  or
ao

 ol
ao

 or
ar

ao
ol



ao

ao

ol



ol



al

ao

ao

ar

figure    pure policy ma tiger  pure policy maps sequences observations
actions  represented action observation tree 

joint policy   h          n n tuple policy agent i 
individual policies must horizon  agent i  define
notion i reduced joint policy   h      i    i       n composed
policies agents  thus   hi   i 
   

fimathematical programming dec pomdps

    value function
executed agents  every  horizon joint policy generates probability distribution possible sequences reward one compute value
policy according equation    thus value joint policy formally defined as 
v        e

 
x

r st      

t  

 

   

given state first period chosen according actions chosen
according  
recursive definition value function policy way
compute horizon finite  definition requires concepts
shall introduce 
given belief state  s   joint action joint observation o 
let  o   a  denote probability agents receive joint observation take
joint action period state chosen according   probability
defined
x
x
 o   a   
 s 
p s  a   g a    o 
   


ss

given belief state  s   joint action joint observation  
updated belief state ao  s  respect defined  for
s  
ao  s    

p
g a s  o   ss  s p s a s   
 o  a 

 o   a     

   

ao  s    

 

 o   a     

   

p
given belief state  s  joint action a  r   a  denotes ss  s r s  a  
using definitions notations  value v      defined follows 
v        v       

   

v        defined recursion using equations                 given below 
equations straight reformulation classical bellman equations finite
horizon problems 
histories null length
v          r        

x

 o      v    o     o 

   

oo

   denotes joint action h              n   i   o denotes
updated state given    joint observation o 
   

fiaras   dutech

non terminal histories   s                  
  t
  t
  t
tuple sequences observations o  t   ho  t

    o      oi
sequence observations agent i 
v       o  t     r     o  t     

x

  t  o

 o     o  t   v   o

    o  t  o      

oo
  t

 o  o updated state given joint action  o  t   joint observation
  ho    o      o  t  o tuple sequences  t      observations ho  t
   o   
  t  o i 
o  t
 o
 



 

n
 
n
 
terminal histories   s   tuple sequences  t     
 
 
  i 
observations o  t     ho  t
  o  t
    o  t
n
 
 
v       o  t       r    o  t       

x

 s r s   o  t     

    

ss

optimal policy policy best possible value  verifying 
v      v     

 

    

important fact dec pomdps  based following theorem 
restrict set pure policies looking solution dec pomdp 
theorem      dec pomdp least one optimal pure joint policy 
proof  see proof work nair et al         



    overview dec pomdps solutions limitations
detailed work oliehoek et al          existing methods solving decpomdps finite horizon belong several broad families  brute force  alternating
maximization  search algorithms dynamic programming 
brute force simplest approach solving dec pomdp enumerate
possible joint policies evaluate order find optimal one  however 
method becomes quickly untractable number joint policies doubly exponential
horizon problem 
alternating maximization following chades  scherrer  charpillet        nair
et al          one possible way solve dec pomdps agent  or small group
agents  alternatively search better policy agents freeze
policy  called alternating maximization oliehoek alternated co evolution
chades method guarantees find nash equilibria  locally optimal joint
policy 
   

fimathematical programming dec pomdps

heuristic search algorithms concept introduced szer  charpillet 
zilberstein        relies heuristic search looking optimal joint policy 
using admissible approximation value optimal joint policy  search
progresses  joint policies provably worse current admissible solution
pruned  szer et al  used underlying mdps pomdps compute admissible heuristic 
oliehoek et al         introduced better heuristic based resolution bayesian
game carefully crafted cost function  currently  oliehoeks method called gmaa 
 for generic multi agent a   quickest exact method large set benchmarks 
but  every exact method  limited quite simple problems 
dynamic programming work hansen  bernstein  zilberstein       
adapts solutions designed pomdps domain dec pomdps  general
idea start policies   time step used build   time step policies
on  process clearly less efficient heuristic search approach
exponential number policies must constructed evaluated iteration
algorithm  policies pruned but  again  pruning less efficient 
exposed details paper oliehoek et al          several others approaches developed subclasses dec pomdps  example  special settings agents allowed communicate exchange informations settings
transition function split independant transition functions agent
studied found easier solve generic dec pomdps 

   sequence form dec pomdps
section introduces fundamental concept policies sequence form  new
formulation dec pomdp thus possible leads non linear program
 nlp  solution defines optimal solution dec pomdp 
    policies sequence form
history function p agent mapping set histories interval
        value p h  weight history h history function p  policy
defines probability function set histories agent saying that 
history hi hi   p hi   conditional probability hi given observation sequence
 o i  o i    oti    
every policy defines policy function  every policy function associated
valid policy  constraints must met  fact  history function p sequenceform policy agent following constraints met 
x

p a      

    

aai

p h   

x

p h o a      

h ni   oi  

    

aai

h o a denotes history obtained concatenating h  definition
appears slightly different form lemma     work koller et al         
   

fiaras   dutech

variables  x h   h hi  
x

x a     

    

aai

x h   

x

x h o a      

h ni   oi

    

h hi

    

aai

x h    

table    policy constraints  set linear inequalities  solved  provide valid
sequence form policy agent i  is  weights x h   possible
define policy agent i 

sequence form policy stochastic probability choosing action
information set h o p h o a  p h   support s p  sequence form policy made
set histories non negative weight  i e  s p     h hi   p h       
sequence form policy p defines unique policy agent  sequence form policy
called policy rest paper ambiguity present 
set policies sequence form agent shall denoted xi   set
pure policies sequence form shall denoted xi xi  
way similar definitions section      define sequence form joint
policy tuple sequence form policies  one agent  weight q
joint
history h   hhi sequence form joint policy hp    p      pn product ii pi  hi   
set joint policies sequence form ii xi shall denoted x set
i reduced sequence form joint policy called xi  
    policy constraints
policy agent sequence form found solving set linear inequalities
 li  found table    li merely implement definition policy sequenceform  li contains one variable x h  history h hi represent weight
h policy  solution x li constitutes policy sequence form 
example section e   appendices  policy constraints decentralized
tiger problem given   agents horizon   
notice policy constraints agent  variable constrained
non negative whereas definition policy sequence form  weight history
must interval         mean variable solution policy
constraints assume value higher    actually  policy constraints
prevent variable assuming value higher   following lemma
shows 
lemma      every solution x            h hi   x  h  belongs       
interval 
   

fimathematical programming dec pomdps

proof  shown forward induction 
every x h  non negative  see eq         case every action
ai   then  x a  greater   otherwise constraint      would violated  so 
h hi     i e  ai    x h  belong        
every h hit x h          previous reasoning applied using constraint
     leads evidently fact x h         every h hit    
thereby  induction holds t 

later article  order simplify task looking joint policies  policy
constraints li used find pure policies  looking pure policies limitation
finite horizon dec pomdps admit deterministic policies policies defined
information set  fact  pure policies needed two three milps build
order solve dec pomdps  otherwise derivation would possible  see
sections        
looking pure policies  obvious solution would impose every variable
x h  belongs set         but  solving mixed integer linear program 
generally good idea limit number integer variables integer variable
possible node branch bound method used assign integer values
variables  efficient implementation mixed integer linear program take
advantage following lemma impose weights terminal histories
take     possible values 
lemma                      replaced by 
x h    

h ni

x h         

h ei

    
    

every solution x resulting li  h hi   x  h        
speak     li 
proof  prove backward induction  let h history length     
due       oi   holds 
x
x  h   
x  h o a  
    
aai

since h history length      history h o a terminal history  due lemma
     x  h          therefore  sum right hand side equation
        due       x  h o a          hence sum right hand side
either      value between  ergo  x  h         value
between  reasoning  show x  h         every non terminal
history h length               

formulate linear inequalities table   memory  require
p space
exponential horizon  agent i  size hi tt    ai  t  oi  t   
exponential number variables
lp exponential  
pt  in

number constraints li table   t    ai    oi  t   meaning number
constraints li exponential  
   

fiaras   dutech

    sequence form dec pomdp
able give formulation dec pomdp based use sequence form
policies  want stress re formulation  provide us
new ways solving dec pomdps mathematical programming 
given classical formulation dec pomdp  see section       equivalent
sequence form dec pomdp tuple hi   hi      ri where 
           n  set agents 
agent i  hi set histories length less equal
agent i  defined previous section  set hi derived using sets
ai oi  
joint history conditional probability function  joint history j h 
   j  probability j occurring conditional agents taking joint actions
according given initial state dec pomdp   function
derived using set states s  state transition function p joint
observation function g 
r joint history value  joint history j h  r   j  value
expected reward agents obtain joint history j occurs  function
derived using set states s  state transition function p  joint observation
function g reward function r  alternatively  r described function
r 
formulation folds s  p g r relying set histories 
give details computation r 
   j  conditional probability sequence joint observations received
agents till period  o   j  o   j     ot   j   sequence joint actions
taken till period      a   j   a   j     at   j   initial state
dec pomdp   is 
   j    prob  o   j  o   j    ot   j    a   j  a   j    at   j  

    

probability product probabilities seeing observation ok  j  given
appropriate belief state action chosen time k  is 
   j   

t 


 ok  j  jk    ak  j  

    

k  

jk probability distribution given agents followed joint
history j time k  is 
jk  s    prob  s o   j  a   j    ok  j   
   

    

fimathematical programming dec pomdps

variables  xi  h   i  h hi
maximize

x

r   j 

je



xi  ji  

    

ii

subject
x

xi  a      



    

i  h ni   oi

    

i  h hi

    

aai

xi  h   

x

xi  h o a      

aai

xi  h    

table    nlp  non linear program expresses constraints finding sequenceform joint policy optimal solution dec pomdp 

regarding value joint history  defined by 
r   j    r   j    j 

    


r   j   

x
x

jk   s r s  ak  j   

    

k   ss

thus  v   p   value sequence form joint policy p  weighted sum
value histories support 
x
v   p   
p j r   j 
    
jh

p j   

q

ii

pi  ji   

    non linear program solving dec pomdps 
using sequence form formulation dec pomdp  able express joint
policies sets linear constraints assess value every policy  solving decpomdp amounts finding policy maximal value  done
non linear program  nlp  table   where  again  xi variables weights
histories agent i 
example example formulation nlp found appendices 
section e    given decentralized tiger problem   agents horizon
  
constraints program form convex set  objective function
concave  as explained appendix a   general case  solving non linear program
   

fiaras   dutech

difficult generalized method guarantee finding global maximum
point  however  particular nlp fact multilinear mathematical program  see
drenick        kind programs still difficult solve  two
agents considered  one speaks bilinear programs  solved easily
 petrik   zilberstein        horst   tuy        
evident  inefficient  method find global maximum point evaluate
extreme points set feasible solutions program since known every
global well local maximum point non concave function lies extreme point
set  fletcher         inefficient method test tells
extreme point local maximum point global maximum point  hence  unless
extreme points evaluated  cannot sure obtained global maximum
point  set feasible solutions nlp x  set  step joint policies 
set extreme points set x  set pure  step joint policies  whose number
doubly exponential exponential n  enumerating extreme points
nlp untractable 
approach  developed next sections  linearize objective function
nlp order deal linear programs  describe two ways
this  one based combinatorial consideration  section    based
game theory concepts  section     cases  shall mean adding variables
constraints nlp  upon so  shall derive mixed integer linear programs
possible find global maximum point hence optimal joint policy
dec pomdp 

   combinatorial considerations mathematical programming
section explains possible use combinatorial properties dec pomdps
transform previous nlp mixed integer linear program  shown  mathematical program belongs family     mixed integer linear programs  meaning
variables linear program must take integer values set        
    linearization objective function
borrowing ideas field quadratic assignment problems  papadimitriou   steiglitz         turn non linear objective function previous nlp linear
objective function linear constraints involving new variables z must take integer
values  variable z j  represents product xi  ji   variables 
thus  objective function was 
x

maximize
r   j 
xi  ji  
    
je

ii

rewritten
maximize

x

r   j z j 

je

j   hj    j      jn i 
   

    

fimathematical programming dec pomdps

must ensure two way mapping value new variables
z x variables solution mathematical program  is 

z  j   
xi  ji   
    
ii

this  restrict ourself pure policies x variables     
case  previous constraint      becomes 
z  j      xi  ji       



    

there  take advantage fact support pure policy agent
composed  oi  t   terminal histories express new constraints  one hand 
guarantee z j  equal   enough x variables equal   
write 
n
x

xi  ji   nz j    

j e 

    

i  

hand  limit number z j  variables take value   
enumerate number joint terminal histories end with 

x
 oi  t    
    
z j   
ii

je

constraints      would weight heavily mathematical program would
one constraint terminal joint history  number exponential n
  idea reduce number constraints reason joint histories
individual histories  history h agent part support solution
ofpthe problem  i e   xq
 h       number joint histories
q belongs
  j ei z hh  j i   ki  i   ok  t     then  suggest replace  ei   constraints
    
n
x

xi  ji   nz j    

j e 

    

i  



p

 ei   constraints
x



 ok  t  
xi  h 
 oi  t  

  xi  h 
 ok  t    

z hh  j i   

j ei

q

ki

i  h ei  

    

ki  i 

    fewer integer variables
linearization objective function rely fact dealing pure
policies  meaning every x z variable supposed value either      solving
linear programs integer variables usually based branch bound technique
   

fiaras   dutech

variables 
xi  h   i  h hi
z j   j e
x

maximize

r   j z j 

    

je

subject to 
x

xi  a      



    

i  h ni   oi

    

aai

xi  h   

x

xi  h o a      

aai

x



z hh  j i    xi  h 


j hi

 ok  t    

i  h ei

    

ki  i 

x

z j   

je



 oi  t  

    

ii

xi  h    

i  h ni

xi  h         

    

i  h ei

z j         

    

j e

    

table    milp      mixed integer linear program finds sequence form joint policy
optimal solution dec pomdp 

 fletcher         efficiency reasons  important reduce number integer
variables mathematical programs 
done section      relax x variables allow take non negative
values provided x values terminal histories constrained integer values 
furthermore  proved following lemma  constraints x guarantee
z variables take value        
eventually end following linear program real integer variables 
thus called     mixed integer linear program  milp   milp shown table   
example section e   appendices  example milp given
problem decentralized tiger   agents horizon   
lemma      every solution  x   z   milp table    j e  z  j 
either     
proof  let  x   z   solution milp  let 
s z     j e z  j      
si  xi      h


ei  xi  h 

     




    


si  z  j      j e ji   j   z  j       
   

    
i 

j

ei

    

fimathematical programming dec pomdps

q
q
    showing  s z  
   
now  due             s z  
i 
ii  oi  
q iit o
 
shall establish  s z     ii  oi  
  due upper bound   z

variable  implication z  j      terminal joint history j thus
proving statement lemma 
note lemma        agent i  xi pure policy  therefore 
 si  x      oi  t     means set constraints       i reduced terminal
joint history j ei appear right hand side  oi  t   times
left hand side  xi  h       thus  j ei  
 si  z  j     oi  t    

    

 h  either     since
now  know agent history h hi   xi q
xi pure policy  so  given i reduced terminal joint history j   ki  i  xk  jk   either
     secondly  due       following implication clearly holds terminal joint
history j 
z  j      xi  ji       

i 

    

therefore  obtain
 si  z  j     oi  t  

    


 

   oi  

xk  jk   

    

ki  i 

consequence 
x

x

 si  z  j   

j ei



 oi  t  

j ei

xk  jk  

    

xk  jk  

    

xk  h  

    

ki  i 

   oi  t  

x



j ei ki  i 

   oi  t  



x

ki  i  h ek

   oi  t  



 ok  t  

    

ki  i 

 



 

 oj  

 

    

ji

since



j ei

si  z  j     s z   holds

p



 s z  

j ei

 si  z  j       s z    hence 

 oj  t    

    

ji

thus statement lemma proved 
   



fiaras   dutech

    summary
using combinatorial considerations  possible design     milp solving given
dec pomdp  proved theorem      solution milp defines optimal joint
policy
nevertheless  milp quite large  o kt   constraints
pfor dec pomdp 
q
nt
 hi      ei     o k   variables  o kt   variables must take integer values 
next section details another method linearization nlp leads
smaller mathematical program   agent case 
theorem      given solution  x   z   milp  x   hx    x      xn pure
 period optimal joint policy sequence form 
proof  due policy constraints domain constraints agent  xi

pure sequence form policy
q agent i  due constraints            z values  
product ii xi  ji   values    then  maximizing objective function
effectively maximizing value sequence form policy hx    x      xn i  thus 

hx    x      xn optimal joint policy original dec pomdp 

   game theoretical considerations mathematical
programming
section borrows concepts nash equilibrium regret game theory
order design yet another     mixed integer linear program solving dec pomdps 
fact  two milps designed  one applied   agents
one number agents  main objective part derive smaller
mathematical program   agent case  indeed  milp   agents  see table   
slightly less variables constraints milp  see table    thus might prove easier
solve  hand    agents considered  new derivation
leads milp given completeness bigger milp 
links fields multiagent systems game theory numerous
literature  see  example  sandholm        parsons   wooldridge         elaborate fact optimal policy dec pomdp nash equilibrium 
fact nash equilibrium highest utility agents share reward 
  agent case  derivation make order build milp similar
first derivation sandholm  gilpin  conitzer         give details
derivation adapt dec pomdp adding objective function it 
  agents  derivation still use find nash equilibriae pure strategies 
rest article  make distinction policy  sequence form
policy strategy agent as  context  concepts equivalent  borrowing
game theory  joint policy denoted p q  individual policy pi qi
i reduced policy pi qi  
    nash equilibrium
nash equilibrium joint policy policy best response reduced
joint policy formed policies joint policy  context sequence form
   

fimathematical programming dec pomdps

dec pomdp  policy pi xi agent said best response i reduced
joint policy qi xi holds
v   hpi   qi i  v   hpi   qi i    

pi xi  

    

joint policy p x nash equilibrium holds
v   p  v   hpi   pi i    
is 
x x

hei j ei

r   hh  j i 



ki  i 

i  pi xi  



  
pk  jk   pi  h  pi  h 

i  pi xi  

    

    

derivation necessary conditions nash equilibrium consists deriving
necessary conditions policy best response reduced joint policy 
following program finds policy agent best response i reduced joint
policy qi xi   constraints           ensure policy defines valid joint policy
 see section      objective function traduction concept best response 
variables  xi  h   i  h hi



x x

maximize
r   hh  j i 
qk  jk   xi  h 


hei

j ei

    

ki  i 

subject to 

x

xi  a     

    

aai

xi  h   

x

xi  h o a      

h ni   oi

    

h hi  

    

aai

xi  h    

linear program  lp  must still refined solution best
response agent global best response  i e   policy agent best
response agents  mean introducing new variables  a set variable
agent   main point adapt objective function current
objective function  applied find global best response  would lead non linear
objective function product weights policies would appear  this 
make use dual program  lp  
linear program  lp  one variable xi  h  history h hi representing
weight h  one constraint per information set agent i  words 
constraint linear program  lp  uniquely labeled information set  instance 
constraint      labeled null information set   nonterminal
history h observation o  corresponding constraint      labeled
information set h o  thus   lp  ni variables mi constraints 
described appendix  see appendix b   dual  lp  expressed as 
   

fiaras   dutech

variables  yi    
minimize

yi   

    

subject to 
yi   h  

x

yi  h o    

h ni

    

qk  jk     

h ei

    

ooi

yi   h  

x

r   hh  j i 

j ei



ki  i 

yi          



    

 h  denotes information set h belongs  dual one free variable
yi    every information set agent i  function  h   defined section      appears mapping histories information sets    dual program
one constraint per history agent  thus  dual mi variables ni constraints 
note objective dual minimize yi    primal  lp  
right hand side constraints  except first one    
theorem duality  see appendix b   applied primal  lp           
transformed dual            says solutions value  mathematically 
means that 



x x

r   hh  j i 
qk  jk   xi  h    yi    
    


hei

j ei

ki  i 

thus  value joint policy hxi   qi expressed either
x x


v   hxi   qi i   
r   hh  j i 
qk  jk   xi  h 
hei

j ei

    

ki  i 



v   hxi   qi i    yi    

    

due constraints           primal lp  holds
x


x x x
xi  a   
yi  h o  xi  h   
xi  h o a 
yi      yi   
aai

hni ooi

    

aai

constraint      guarantees first term braces   constraints     
guarantee remaining terms inside braces    right hand side
     rewritten
x
x
x




p
xi  a  yi   
ooi yi  a o 
 
xi  h  yi   h  
yi  h o 
aai

ooi

hni  ai

 

x

xi  h yi   h  

hei

 

p


hni xi  h 



yi   h  

x

ooi

x
yi  h o   
xi  h yi   h  
hei

   h o information set  yi  h o  shortcut writing yi   h o   

   

    

fimathematical programming dec pomdps

so  combining equations            get
x
x


xi  h 
yi   h  
yi  h o 
ooi

hni

 

x

xi  h 



hei

yi   h  

r   hh  j i 

x



j ei



ki  i 


qk  jk      

    

time introduce supplementary variables w information set  variables  usually called slack variables  defined as 
x
yi   h  
yi  h o    wi  h   h ni
    
ooi

yi   h  

x



r   hh  j i 

j ei



qk  jk     wi  h  

h ei  

    

ki  i 

shown section c appendix  slack variables correspond concept
regret defined game theory  regret history expresses loss accumulated
reward agent incurs acts according history rather according
history would belong optimal joint policy 
thanks slack variables  furthermore rewrite      simply
x
x
xi  h wi  h   
xi  h wi  h     
    
hni

hei

x

xi  h wi  h      

    

hhi

now       sum ni products  ni size hi   product sum
necessarily   xi  h  wi  h  constrained nonnegative primal
dual respectively  property strongly linked complementary slackness
optimality criterion linear programs  see  example  vanderbei         hence      
equivalent
xi  h wi  h      

h hi  

    

back framework dec pomdps  constraints written 
pi  h i  hh  qi i      

h hi  

    

sum up  solving following mathematical program would give optimal joint
policy dec pomdp  constraints      non linear thus prevent us
solving program directly  linearization constraints  called complementarity
constraints  subject next section 
variables 
xi  h   wi  h  h hi
yi   
maximize

y    

   

    

fiaras   dutech

subject to 
x

xi  a     

    

aai

xi  h   

x

i  h ni   oi

    

yi  h o    wi  h  

i  h ni

    

xk  jk     wi  h  

i  h ei

    

xi  h o a      

aai

yi   h  

x

ooi

yi   h  

x

j ei

r   hh  j i 



ki  i 

xi  h wi  h      

i  h hi

    

xi  h    

i  h hi

    

wi  h    

i  h hi

    

yi          

i 

    

    dealing complementarity constraints
section explains non linear constraints xi  h wi  h      previous mathematical program turned sets linear constraints thus lead mixed
integer linear programming formulation solution dec pomdp 
consider complementarity constraint ab     variables b  assume
lower bound values b    let upper bounds values b
respectively ua ub   let c     variable  then  complementarity constraint
ab     separated following equivalent pair linear constraints 
ua c

    

b ub    c  

    

words  pair constraints satisfied  surely case ab     
easily verified  c either      c      set  
constrained ua c  and less     c      b set  
since b constrained ub    c   and less     either case 
ab     
consider complementarity constraint xi  h wi  h      non linear program           above  wish separate constraint pair linear constraints 
recall xi  h  represents weight h wi  h  represents regret h 
first requirement convert constraint pair linear constraints lower
bound values two terms    indeed case since xi  h  wi  h 
constrained non negative nlp  next  require upper bounds
weights histories regrets histories  shown lemma     upper
bound value xi  h  h    upper bounds regrets histories 
require calculus 
   

fimathematical programming dec pomdps

policy pi agent holds
x
pi  h     oi  t    

    

hei

therefore  every i reduced joint policy hq    q      qn xi   holds

x
 ok  t  
qk  jk    
j ei ki  i 

    

ki  i 

since regret terminal history h agent given hq    q      qn defined
x


    
 h  q    max
qk  jk   r   hh   j i  r   hh  j i   
h  h 

j ei ki  i 

conclude upper bound ui  h  regret terminal history h ei
agent is 



 


ui  h   
 ok  
max max
r   hh   j i  min r   hh  j i        

ki  i 

h  h  j ei

j ei

let us consider upper bounds regrets non terminal histories  let
information set length agent i  let ei    ei denote set terminal histories
agent first  t elements history set identical   let h
history length agent i  let ei  h  ei denote set terminal histories
first  t     elements history set identical h  since policy
pi agent i  holds
x
pi  h    oi  t
    
h ei  h 

conclude upper bound ui  h  regret nonterminal history
h ni length agent




max
max
r  
hh
 
j
i 

min
min
r  
hg 
j
i 
    
ui  h    li


h ei   h   j ei

gei  h  j ei


li    oi  t



 ok  t    

    

ki  i 

notice    that is  h terminal       reduces      
so  complementarity constraint xi  h wi  h      separated pair linear
constraints using     variable bi  h  follows 
xi  h    bi  h 
wi ui  h bi  h 
bi  h        
   

     
     
     

fiaras   dutech

variables 
xi  h   wi  h  bi  h         h hi
yi          
maximize

y    

     

subject to 
x

xi  a     

     

aai

xi  h   

x

        h ni   oi

     

        h ni

     

r   hh  h i x   h     w   h  

h e 

     

r   hh   hi x   h     w   h  

h e 

     

xi  h o a      

aai

yi   h  

x

yi  h o    wi  h  

ooi

y    h  

x

h e 

y    h  

x

h e 

xi  h    bi  h  
wi  h  ui  h bi  h  

        h hi
        h hi

     
     

xi  h    

        h hi

     

wi  h    

        h hi

     

bi  h         

        h hi

yi          

     

             

table    milp   agents      mixed integer linear program  derived game
theoretic considerations  finds optimal stochastic joint policies dec pomdps
  agents 

    program   agents
combine policy constraints  section       constraints seen
policy best response  sections           maximization value
joint policy  derive     mixed integer linear program solution
optimal joint policy dec pomdp   agents  table   details program
call milp   agents 
example formulation decentralized tiger problem   agents
horizon   found appendices  section e  
variables program vectors xi   wi   bi yi agent i  note
agent history h agent i  ui  h  denotes upper bound
regret history h 
   

fimathematical programming dec pomdps

solution  x     w   b   milp   agents consists following quantities   i 
optimal joint policy x   hx    x  may stochastic   ii  agent     
   history h hi   wi  h   regret h given policy xi agent 
 iii  agent         information set   yi     value given
policy xi agent   iv  agent         vector bi simply tells us
histories support xi   history h agent bi  h     
support xi   note replace y     y     objective function
without affecting program  following result 
theorem      given solution  x   w     b   milp   agents  x   hx    x 
optimal joint policy sequence form 
proof  due policy constraints agent  xi sequence form policy
agent i  due constraints              yi contains values information sets
agent given xi   due complementarity constraints              xi best
response xi   thus hx    x  nash equilibrium  finally  maximizing value
null information set agent    effectively maximizing value hx    x  i 
thus hx    x  optimal joint policy 

comparison milp presented table    milp   agents
constitutes particularly effective program term computation time finding  agent optimal  period joint policy much smaller program  number
variables required milp exponential n  number variables required
milp   agents exponential   represents major reduction size
lead improvement term computation time 
    program   agents
number agents    constraint      non linear program
          longer complementarity constraint
  variables could linq
earized before  particular  term ki  i  xk  jk   constraint      involves
many variables different agents  linearize term  restrict pure joint policies exploit combinatorial facts number
histories involved  leads     mixed linear program called milp n agents
depicted table   
variables program milp n agents vectors xi   wi   bi yi
agent vector z  following result 
theorem      given solution  x   w     b   z   milp n agents  x   hx    x   
  xn pure  period optimal joint policy sequence form 
proof  due policy constraints domain constraints agent 
pure sequence form policy agent i  due constraints              yi
contains values information sets agent given xi   due complementarity
constraints              xi best response xi   thus x nash equilibrium 
finally  maximizing value null information set agent    effectively
maximizing value x   thus x optimal joint policy 

xi

   

fiaras   dutech

variables 
xi  h   wi  h  bi  h  h hi
yi    i 
z j  j e
maximize

y    

     

xi  a     

     

xi  h o a      

i  h ni   oi      

subject to 
x

aai

xi  h   

x

aai

yi   h  

x

yi  h o    wi  h  

i  h ni

     

ooi

yi   h  

x
 
r   hh  ji i z j    wi  h   i  h ei

 
 oi  
je
x

z hh  j i    xi  h 
 ok  t    
j ei

     

ki  i 

i  h ei
x

z j   
 oi  t  
je

     
     

ii

xi  h    bi  h  

i  h hi      

wi  h  ui  h bi  h  

i  h hi      

xi  h    

i  h ni

xi  h        
wi  h    

i  h ei

i  h hi

bi  h         

h hi

yi          
z j         

j e

     
     
     
     

i  i     
     

table    milp n agents      mixed integer linear program  derived game
theoretic considerations  finds pure optimal joint policies dec pomdps
  agents 

   

fimathematical programming dec pomdps

compared milp table    milp n agents roughly size
real valued variables     variables  precise  p
milp    
variable every terminal history every agent  that approximatively ii  ai  t  oi  t  
integer variables  milp n agents two     variables
every terminal well
p
nonterminal history agent  approximatively   ii   ai   oi   t integer variables  

    summary

formulation solution dec pomdp application duality
theorem linear programs allow us formulate solution dec pomdp
solution new kind     milp    agents  milp o kt   variables
constraints thus smaller milp previous section  still 
milps quite large next section investigates heuristic ways speed
resolution 

   heuristics speeding mathematical programs
section focusses ways speed resolution various milps presented
far  two ideas exploited  first  show prune set sequence form policies
removing histories provably part optimal joint policy 
histories called locally extraneous  then  give lower uppers bounds
objective function milps  bounds sometimes used branch
bound method often used milp solvers finalize values integer variables 
    locally extraneous histories
locally extraneous history history required find optimal joint policy
initial state dec pomdp could replaced co history
without affecting value joint policy  co history history h agent
defined history agent identical h aspects except last
action  ai    b  c   co history c u b v b history c u b v c  set
co histories history h shall denoted c h  
formally  history h hit length agent said locally extraneous if 
i reduced joint histories length t 
every probability distribution set hi
exists history h c h 
x


 j   r   hh   j i  r   hh  j i 
 
     
j hti

 j   denotes probability j  
alternative definition follows  history h hit length agent said
locally extraneous exists probability distribution set co histories
h i reduced joint history j length t  holds
x
 h  r   hh   j i  r   hh  j i 
     
h c h 

   

fiaras   dutech

 h   denotes probability co history h  
following theorem justifies incremental pruning locally extraneous histories
search optimal joint policies faster performed smaller
set possible support histories 
theorem      every optimal  period joint policy p agent
terminal history h agent locally extraneous   pi  h       exists
another  period joint policy p optimal identical p respects
except pi  h      
proof  let p  period joint policy optimal   assume
agent terminal history h agent locally extraneous   pi  h      
       exists least one co history h h that 
x


pi  j   r   hh   j i  r   hh  j i 
  
     
j ht


let q  period policy agent identical pi respects except q h  
  pi  h    pi  h   q h       shall show q optimal   holds 
x

j ht


v   hq  pi i  v   hpi   pi i   


pi  j   r   hh   j i q h   r   hh   j i pi  h   r   hh  j i pi  h 
 
x

j ht




pi  j   r   hh   j i  q h   pi  h    r   hh  j i pi  h 
 
x

j ht




pi  j   r   hh   j i pi  h  r   hh  j i pi  h 

since q h     pi  h    pi  h    therefore 
x

j ht


v   hq  pi i  v   hpi   pi i   


pi  j   r   hh   j i  r   hh  j i 
   due        

hence  p   hq  pi optimal  period joint policy  



one could wonder order extraneous histories pruned important
not  answer question  following theorem shows many co histories
extraneous  pruned order as 
either value  one pruned  
pruning one change fact others still extraneous 
theorem      two co histories h  h  locally extraneous  either values
equal h locally extraneous
r   hh    j i  r   hh    j i for j hi
 
relatively c h     h    
   

fimathematical programming dec pomdps

proof  let c   denotes union c h    c h     immediately c h     
c      h    c h      c      h     h   resp  h    locally extraneous means
exists probability distribution   c h     resp    c h     that  j
 
hi
x
   h  r   hh   j i  r   hh    j i 
     
h c     h   

x

   h  r   hh   j i  r   hh    j i 

     

h c     h   

     
eq        expanded in 
x

   h   r   hh    j i   

h c     h

   h  r   hh   j i  r   hh    j i  
   h   

using             gives
x
   h   
   h  r   hh   j i   
h c     h   

leading
x

     

x

   h  r   hh   j i  r   hh    j i 

h c     h   h   

     

    h      h        h   r   hh   j i        h      h    r   hh    j i       

h c     h   h   

so  two cases possible 
   h         h         case  r   hh    j i  r   hh    j i  r   hh    j i 
 
r   hh    j i   r   hh    j i    r   hh    j i  j hi
   h      h         case have 
x

h c     h   h   

   h      h        h  
r   hh   j i  r   hh    j i 
     h      h   

     

meaning even without using h    h  still locally extraneous
   h      h      h  
probability distribution c      h    h   
    h      h   
x

h c     h

   h   

   h         h             h    
   h      h        h  
 
     h      h   
     h      h   
     h      h   
     h      h   
    
 

     
     
     


   

fiaras   dutech

order prune locally extraneous histories  one must able identify histories 
indeed two complementary ways this 
first method relies definition value history  see section      

r   hh  j i       hh  j i r   hh  j i  

     

therefore 
   hh  j i      


j hi

     

true history h  means every joint history length occurring
given history part priori probability    thus  h clearly
extraneous  besides  every co history h locally extraneous share
probabilities 
second test needed locally extraneous histories verify       
again  turn linear programing particular following linear program

variables  y j   j hi
minimize



     

subject to 
x

j hti



y j   r   hh   j i  r   hh  j i 
 
x

h c h 

y j      

     
     

j hti

following lemma 

y j     


j hi

     

lemma      if  exists solution       linear program               
h locally extraneous 
proof   let       solution lp              probability distribution
due constraints                 since minimizing   due
hi
   every co history h h
constraints        every  hi
x


y j   r   hh   j i  r   hh  j i 
 
     
j hti

therefore  definition  h locally extraneous 



following procedure identifies locally extraneous terminal histories agents
proceed iterative pruning  mainly motivated theorems        
effectively removing extraneous histories  procedure similar procedure
iterated elimination dominated strategies game  osborne   rubinstein        
concept quite similar process policy elimination backward step
dynamic programming partially observable stochastic games  hansen et al         
   

fimathematical programming dec pomdps

step    agent i  set hit ei   let h denote set ii hit  
joint history j h   compute store value r   j  j joint
observation sequence probability    j  j 
step    agent i  history h hit   i reduced joint
     hh  j i       remove h h  
history j hi

step    agent i  history h hit follows  c h  hit
non empty  check whether h locally extraneous setting solving
set h set c h 
lp              setting lp  replace hi

set c h  hit   upon solving lp  h found locally extraneous
  remove h hit  
step    step   history  of agent  found locally extraneous  go
step    otherwise  terminate procedure 
procedure builds set hit agent i  set contains every terminal
history agent required finding optimal joint policy   every
terminal history locally extraneous   agent i  every history
hit hit locally extraneous  reason reiterating step  
history h agent found locally extraneous consequently removed
hit   possible history agent previously locally
extraneous becomes so  due removal h hit   hence  order verify
case history not  reiterate step   
besides  step   procedure prunes histories impossible given
model dec pomdp observation sequence observed 
last pruning step taken order remove non terminal histories
lead extraneous terminal histories  last step recursive  starting histories
horizon    remove histories hi non extraneous terminal histories 
is  histories hi h o a extraneous ai oi  
complexity algorithm pruning locally extraneous histories exponential
complexity  joint history must examined compute value occurence
probability  then  worst case  linear program run every local history
order check extraneous not  experimentations needed see prunning
really interesting 
    cutting planes
previous heuristics aimed reducing search space linear programs 
incidentally good impact time needed solve programs  another option
directly aims reducing computation time use cutting planes  cornuejols 
       cut  dantzig        special constraint identifies portion set
feasible solutions optimal solution provably lie  cuts used
conjunction various branch bounds mechanism reduce number possibles
combination integer variables examined solver 
present two kinds cuts 
   

fiaras   dutech

variables  y j   j h
maximize

x

r   j y j 

     

je

subject to 
x

y a     

     

aa

y j   

x

y j o a      

j n  

     

j h

     

aa

y j    

table    pomdp  linear program finds optimal policy pomdp 
      upper bound objective function
first cut propose upper bound pomdp cut  value optimal
 period joint policy given dec pomdp bounded value
vp optimal  period policy pomdp derived dec pomdp 
derived pomdp dec pomdp assuming centralized controller  i e 
one agent using joint actions  
sequence form representation pomdp quite straightforward  calling h
set tt   ht joint histories lengths less equal n set h e nonterminal joint histories  policy pomdp horizon sequence form function
q h        that 
x
q a     
     
aa

q j   

x

q j o a      

j n  

     

aa

value vp    q  sequence form policy q given by 
x
vp    q   
r   j q j 

     

je

thereby  solution linear program table   p
optimal policy
pomdp horizon optimal value pomdp je r   j y  j   so 
value v   p   optimal joint policy p   hp    p      pn dec pomdp
bounded value vp    q   associated pomdp 
complexity complexity finding upper bound linked complexity
solving pomdp which  showed papadimitriou tsitsiklis         pspace
 i e  require memory polynomial size problem  leading possible
exponential complexity time   again  experimentation help us decide
cases upper bound cut efficient 
   

fimathematical programming dec pomdps

      lower bound objective function
case dec pomdps non negative reward  trivial show value
 period optimal policy bounded value   horizon optimal
value  so  general case  take account lowest reward possible
compute lower bound say that 
x
r   j z j  v        min min r s  a 
     
aa ss

je

v   value optimal policy horizon    reasoning leads
iterated computation dec pomdps longer longer horizon  reminiscent
maa  algorithm  szer et al          experiments tell worthwhile solve bigger
bigger dec pomdps take advantage lower bound better directly
tackle horizon problem without using lower bound 
complexity compute lower bound  one required solve dec pomdp whith
horizon one step shorter current horizon  complexity clearly
least exponential  experiments  value dec pomdp used
dec pomdp bigger horizon  case  computation time
augmented best time solve smaller dec pomdp 
    summary
pruning locally extraneous histories using bounds objective function
practical use software solving milps presented paper  pruning histories
means space policies used milp reduced and  formulation
milp depends combinatorial characteristics dec pomdp  milp
must altered show appendix d 
validity far cuts concerned  alter solution found milps 
solution milps still optimal solution dec pomdp  extraneous histories pruned  least one valid policy left solution because  step
  algorithm  history pruned co histories left  besides 
reduced set histories still used build optimal policy theroem     
consequence  milp build reduced set histories admit solution
solution one optimal joint policy 
next section  experimental results allow us understand cases
heuristics introduced useful 

   experiments
mathematical programs heuristics designed paper tested four
classical problems found literature  problems  involving two agents 
mainly compared computation time required solve dec pomdp using mixed
integer linear programming methods computation time reported methods found
literature  tested programs three agent problems randomly
designed 
   

fiaras   dutech

problem
mabc
ma tiger
fire fighting
grid meeting
random pbs

 ai  
 
 
 
 
 

 oi  
 
 
 
 
 

 s 
 
 
  
  
  

n
 
 
 
 
 

table    complexity various problems used test beds 

milp milp   solved using ilog cplex    solver commercial set
java packages relies combination simplex branch bounds
methods  fletcher         software run intel p      ghz  gb
ram using default configuration parameters  mathematical programs  different
combination heuristics evaluated  pruning locally extraneous histories  using
lower bound cut using upper bound cut  respectively denoted loc  low
result tables come 
non linear program  nlp  section     evaluated using various solvers neos website  http   www neos mcs anl gov    even thought
method guarantee optimal solution dec pomdp  three solvers
used  lancelot  abbreviated lanc    loqo snopt 
result tables report results found literature following algorithms 
dp stands dynamic programming hansen et al          dp lpc improved
version dynamic programming policies compressed order fit
memory speed evaluation proposed boularias chaib draa        
pbdp extension dynamic programming pruning guided knowledge
reachable belief states detailed work szer charpillet         maa 
heuristically guided forward search proposed szer et al         generalized
improved version algorithm called gmaa  developed oliehoek et al         
problems selected evaluate algorithms detailed coming subsections 
widely used evaluate dec pomdps algorithms literature
complexity  term space size  summarized table   
    multi access broadcast channel problem
several versions multi access broadcast channel  mabc  problem found
literature  use description given hansen et al         allows
problem formalized dec pomdp 
mabc  given two nodes  computers  required send messages
common channel given duration time  time imagined
split discrete periods  node buffer capacity one message 
buffer empty period refilled certain probability next period 
period  one node send message  nodes send message
period  collision messages occurs neither message transmitted  case
collision  node intimated collision signal  collision
   

fimathematical programming dec pomdps

signaling mechanism faulty  case collision  certain probability 
send signal either one nodes 
interested pre allocating channel amongst two nodes given number
periods  pre allocation consists giving channel one nodes period
function nodes information period  nodes information period
consists sequence collision signals received till period 
modeling problem dec pomdp  obtain   agent    state    actionsper agent    observations per agent dec pomdp whose components follows 
node agent 
state problem described states buffers two nodes 
state buffer either empty full  hence  problem four states 
 empty  empty    empty  full    full  empty   full  full  
node two possible actions  use channel dont use channel 
period  node may either receive collision signal may not  node
two possible observations  collision collision 
initial state problem  full  full   state transition function p 
joint observation function g reward function r taken hansen et al 
        agents full buffers period  use channel period 
state problem unchanged next period  agents full buffers
next period  agent full buffer period uses channel
period  buffer refilled certain probability next period 
agent    probability     agent    probability      agents
empty buffers period  irrespective actions take period  buffers
get refilled probabilities      for agent         for agent    
observation function g follows  state period  full  full 
joint action taken agents previous period  use channel  use channel  
probability receive collision signal       probability one
receives collision signal      probability neither receives
collision signal       state problem may period
joint action agents may taken previous period  agents receive
collision signal 
reward function r quite simple  state period  full  empty 
joint action taken  use channel  dont use channel  state period
 empty  full  joint action taken  dont use channel  use channel   reward
   combination state joint action  reward   
evaluated various algorithms problem three different horizons    
     respective optimal policies value                  results
detailed table   where  horizon algorithm  value computation
time best policy found given 
results show milp compares favorably classical algorithms except
gmaa  always far better horizon   and  horizon    roughly within
   

fiaras   dutech

resolution method
program
solver
heuristics
milp
cplex
milp
cplex
low
milp
cplex

milp
cplex
loc
milp
cplex
loc  low
milp
cplex
loc 
milp  
cplex
nlp
snopt
nlp
lanc 
nlp
loqo
algorithm
family
dp
dyn  prog 
dp lpc
dyn  prog 
pbdp
dyn  prog 
maa 
fw  search
gmaa 
fw  search

horizon  
value
time
    
    
                
                
                
                
                
    
    
    
    
    
    
    
    
value
time
    
 
    
    
    
   s
    
   s
 
 

horizon  
value
time
    
   
    
          
    
          
    
         
    
          
                
    
    
    
    
    
    
    
    
value
time
    
     
    
    
    
 
    
    
    
    

horizon  
value
time
 m
       m
        m
        t
        t
    
         
 m
    
    
    
  
    
    
value
time
 m
 m
    
   
 t
    
    

table    mabc problem  value computation time  in seconds  solution
problem computed several methods  best results highlighted 
appropriate  time shows first time used run heuristics global
time  format heuristic total time   t means timeout       s 
 m indicates problem fit memory   indicates
algorithm tested problem 

order magnitude milp pertinent heuristics  expected  apart
simplest setting  horizon     nlp based resolution find optimal policy
dec pomdp  computation time lower methods  among
milp methods  milp   better milp even best heuristics horizon  
   size problem increases  heuristics way milps
able cope size problem  table shows that  mabc
problem  pruning extraneous histories using loc heuristic always good method
investigation revealed     heuristics proved locally extraneous 
far cutting bounds concerned  dont seem useful first  for horizon
     necessary milp find solution horizon    problem 
one must mind one optimal policy horizon 
    multi agent tiger problem
explained section      multi agent tiger problem  ma tiger  introduced
paper nair et al          general description problem  ob   

fimathematical programming dec pomdps

joint action
 listen  listen 
 listen  listen 
 listen  listen 
 listen  listen 
 listen  listen 
 listen  listen 
 listen  listen 
 listen  listen 
      

state
left
left
left
left
right
right
right
right
 

joint observation
 noise left  noise left 
 noise left  noise right 
 noise right  noise left 
 noise right  noise right 
 noise left  noise left 
 noise left  noise right 
 noise right  noise left 
 noise right  noise right 
      

probability
      
      
      
      
      
      
      
      
    

table    joint observation function g ma tiger problem 
tain   agent    state    actions per agent    observations per agent dec pomdp whose
elements follows 
person agent  so    agent dec pomdp 
state problem described location tiger  thus  consists
two states left  tiger behind left door  right  tiger behind right
door  
agents set actions consists three actions  open left  open left door  
open right  open right door  listen  listen  
agents set observations consists two observations  noise left  noise coming
left door  noise right  noise coming right door  
initial state equi probability distribution s  state transition function p 
joint observation function g reward function r taken paper nair
et al          p quite simple  one agents opens door period  state
problem next period set back   agents listen period  state
process unchanged next period  g  given table      quite simple 
nair et al         describes two reward functions called b problem 
report results reward function a  given table     behavior
algorithm similar reward functions  optimal value problem
horizons     respectively           
horizon    dynamic programming forward search methods generally better
mathematical programs  contrary horizon   computation time milp low heuristic significatively better other  even
gmaa   unlike mabc  pruning extraneous histories improve methods
based milp  quite understandable deeper investigations showed
extraneous histories  using lower cutting bounds proves efficient
seen kind heuristic search best policy   directly set policies  like
   

fiaras   dutech

joint action
 open right  open right 
 open left  open left 
 open right  open left 
 open left  open right 
 listen  listen 
 listen  open right 
 open right  listen 
 listen  open left 
 open left  listen 

left
  
   
    
    
  
 
 
    
    

right
   
  
    
    
  
    
    
 
 

table     reward function ma tiger problem 

gmaa   set combination histories  may explain good behavior
milp low 
must noted problem  approximate methods nlp
algorithms depicted memory bound dynamic programming
seuken zilberstein        able find optimal solution  and  again 
methods based nlp quite fast sometimes accurate 
    fire fighters problem
problem fire fighters  ff  introduced new benchmark oliehoek
et al          models team n fire fighters extinguish fires row nh
houses 
state house given integer parameter  called fire level f  
takes discrete value    no fire  nf  fire maximum severity   every time
step  agent move one house  two agents house 
extinguish existing fire house  agent alone  fire level lowered
    probability neighbor house burning   probability otherwise 
burning house fireman present increase fire level f one point    
probability neighbor house burning probability     otherwise 
unattended non burning house catch fire probability     neighbor house
burning  action  agents receive reward f house still
burning  agent observe flames location probability
depends fire level      f          f         otherwise  start 
agents outside houses fire level houses sampled
uniform distribution 
model following characteristics 
na agents  nh actions nf possible informations 

h  
states nnf h possible states burning houses
nnf h   na  n
n


h  
different ways distribute na fire fighters houses 
na  n
na
example    agents   houses   levels fire lead          states  but 
   

fimathematical programming dec pomdps

resolution method
program
solver
heuristics
milp
cplex
milp
cplex
low
milp
cplex

milp
cplex
loc
milp
cplex
loc  low
milp
cplex
loc 
milp  
cplex
nlp
snopt
nlp
lanc 
nlp
loqo
algorithm
family
dp
dyn  prog 
dp lpc
dyn  prog 
pbdp
dyn  prog 
maa 
fw  search
gmaa 
fw  search

horizon  
value
time
    
    
               
               
               
               
               
    
     
   
    
    
    
    
    
value
time
    
    
    
    
 
 
    
    
    
    

horizon  
value
time
 t
    
        
        t
        t
               
         t
 t
     
    
    
   
    
  
value
time
 m
    
   
 
 
    
    
    
    

table     ma tiger problem  value computation time  in seconds  solution
problem computed several methods  best results highlighted 
appropriate  time shows first time used run heuristics
global time  format heuristic total time  t means timeout
      s   m indicates problem fit memory   indicates algorithm tested problem 

   

fiaras   dutech

resolution method
program
solver
heuristics
milp
cplex
milp  
cplex
nlp
snopt
nlp
lanc 
nlp
loqo
algorithm
family
maa 
fw  search
gmaa 
fw  search

horizon  
value
time
 t
     
  
     
    
     
    
     
    
value
time
            
       
    

horizon  
value
time
 t
 t
     
    
     
    
     
  
value
time
            
            

table     fire fighting problem  value computation time  in seconds  solution problem computed several methods  best results highlighted 
 t means timeout       s  maa  gmaa   value parenthesis
taken work oliehoek et al         optimal
different optimal values 

possible use information joint action reduce number state
needed transition function simply nnf h   meaning    states   agents
  houses   levels fire 
transition  observation reward functions easily derived description 
problem  dynamic programming based methods tested problem
formulation quite new  horizon    value optimal policy given oliehoek
et al                differs value found milp algorithms        whereas
methods supposed exact  might come slight differences
respective formulation problems  horizon    oliehoek et al         report
optimal value        
problem  milp methods clearly outperformed maa  gmaa  
nlp methods  give optimal solution horizon    better term
computation time  might nlp able find optimal policies horizon  
setting differs work oliehoek et al          able check
policy found really optimal  main reason superiority forward
search method lies fact problem admits many many optimal policies
value  fact  horizon    milp based methods find optimal policy quite
quickly  around   s milp    then  using branch and bound  must evaluate
potential policies knowing indeed found optimal policy  forward
search methods stop nearly soon hit one optimal solution 
heuristics reported as  improve performance milp
take away computation time thus results worse 
   

fimathematical programming dec pomdps

    meeting grid
problem called meeting grid deals two agents want meet stay
together grid world  introduced work bernstein  hansen 
zilberstein        
problem  two robots navigating two by two grid world
obstacles  robot sense whether walls left right 
goal robots spend much time possible square  actions
move up  down  left right  stay square  robot attempts
move open square  goes intended direction probability     
otherwise randomly either goes another direction stays square 
move wall results staying square  robots interfere
cannot sense other  reward   agents share square 
  otherwise  initial state distribution deterministic  placing robots
upper left corner grid 
problem modelled dec pomdp where 
  agents  one   actions observations  wall left  wall
right  
   states  since robot   squares time 
transition  observation reward functions easily derived description 
problem  dynamic programming based methods tested problem
formulation quite new  problem intrinsically complex
solved horizon      again  optimal value found method differ
value reported oliehoek et al          whereas found optimal values
          horizon      report optimal values           
results problem roughly pattern results
problem  maa  gmaa  quicker milp  time milp able find
optimal solution horizon    nlp methods give quite good results slower
gmaa   ff  numerous optimal policies milp methods
able detect policy found quickly indeed optimal 
again  heuristics reported as  improve performance
milp take away computation time thus results worse 
    random   agent problems
test approach problems   agents  used randomly generated decpomdps state transition function  joint observation function reward
functions randomly generated  dec pomdps   actions   observations
per agent    states  rewards randomly generated integers range     
complexity family problem quite similar complexity mabc
problem  see section      
   

fiaras   dutech

resolution method
program
solver
heuristics
milp
cplex
milp  
cplex
nlp
snopt
nlp
lanc 
nlp
loqo
algorithm
family
maa 
fw  search
gmaa 
fw  search

horizon  
value time
    
    
    
    
    
    
    
    
    
    
value time
      
 s
      
 s

horizon  
value time
    
    
 t
    
    
    
   
    
  
value time
      
    
           

table     meeting grid problem  value computation time  in seconds 
solution problem computed several methods  best results
highlighted   t means timeout       s  maa  gmaa   value
parenthesis taken work oliehoek et al        
optimal different optimal values   

program
milp
milp  

least time  secs 
    
    

time  secs 
   
   

average
     
     

std  deviation
      
      

table     times taken milp milp     agent random problem horizon   

order assess real complexity random problem  first tested
two agent version problem horizon    results averaged    runs
programs given table     compared mabc problem seemed
comparable complexity  random problem proves easier solve     s vs    s  
problem  number     variable relatively small  weight much
resolution time milp   thus faster 
results three agent problem horizon   given table    
averaged    runs  even though size search space smaller case
 for   agents horizon           policies whereas problem  
agents horizon             possible policies     agent problems seems
difficult solve  demonstrating one big issue policy coordination  here 
heuristics bring significative improvement resolution time milp  predicted 
milp n efficient given completeness 
   

fimathematical programming dec pomdps

program
milp
milp low
milp n

least time  secs 
  
  
   

time  secs 
   
  
    

average
    
    
    

std  deviation
     
    
   

table     times taken milp milp n   agent random problem horizon   

   discussion
organized discussion two parts  first part  analyze results
offer explanations behavior algorithms usefulness heuristics  then 
second part  explicitely address important questions 
    analysis results
results  appears milp methods better alternative dynamic programming methods solving dec pomdps globally generally clearly outperformed forward search methods  structure thus characteristics
problem big influence efficiency milp methods  whereas seems
behavior gmaa  terms computation time quite correlated
complexity problem  size action observation spaces   milp methods seem
sometimes less correlated complexity  case mabc problem  many
extraneous histories pruned  ma tiger problem  special structure 
outperform gmaa   contrary  many optimal policies exists  forward
search methods gmaa  clearly better choice  finally  non linear programs 
even though guarantee optimal solution  generally good alternative
sometimes able find good solution computation time often
better gmaa   might prove useful approximate heuristic driven forward
searches 
computational record two   agent programs shows milp   agents
slower milp horizon grows  two reasons sluggishness
milp   agents may attributed  time taken branch bound  bb 
method solve     milp inversely proportional number     variables
milp  milp   agents many     variables milp event hough
total number variables exponentially less milp  first reason 
secondly  milp   agents complicated program milp  many
constraints milp  milp simple program  concerned finding subset
given set  addition finding weights histories  milp finds weights
terminal joint histories  extra superfluous quantity forced find 
hand  milp   agents takes much circuitous route  finding many
superfluous quantities milp  addition weights histories  milp   agents
finds supports policies  regrets histories values information sets  thus 
   

fiaras   dutech

problem

heuristic

mabc

loc
low

loc
low

loc

ma tiger

meeting

horizon  
time  pruned

    

    

    

      

horizon  
time  pruned
    
     
    
    
    
     
    
    
               

horizon  
time  pruned
     
      
    
    
    
     
   
    

horizon  
time  pruned
    
       
   
    

table     computation time heuristics  loc heuristics  give computation time seconds number locally extraneous histories pruned
total number histories  for agent     denotes cases one
additional history prunned second agent  low heuristic 
computation time given 

relaxation milp   agents takes longer solve relaxation milp 
second reason slowness bb method solves milp   agents 
bigger problems  namely fire fighters meeting grid  horizon
stays small  milp   agents compete milp slightly lower size 
complexity grows o   ai   oi   t   whereas grows o   ai   oi    t   milp 
small difference hold long number integer variables quickly lessens
efficiency milp   agents 
far heuristic concerned  proved invaluable problems  mabc
ma tiger  useless others  case mabc  heuristics helpful
prune large number extraneous heuristics ultimately  combination
upper bound cut efficient horizon grows  case
ma tiger  although extraneous histories found  using lower bound cut heuristic
milp leads quickest algorithm solving problem horizon   
problems  heuristics burden greedy computation
time speed resolution  example  grid meeting problem  time
taken prune extraneous histories bigger time saved solving problem 
result  added value using heuristics depends nature problem
 as depicted table     but  right now  able predict usefulness without
trying them 
emphasize results given lie limit possible solve
exact manner given memory computer used resolution  especially
terms horizon  furthermore  number agent increases  length
horizon must decreased problems still solvable 
   

fimathematical programming dec pomdps

    questions
mathematical programing approach presented paper raises different questions 
explicitly addressed questions appears important us 
q   sequence form approach entirely doomed exponential
complexity 
number sequence form joint policies grows doubly exponentially horizon number agents  sequence form approach seems doomed  even compared
dynamic programming doubly exponential worst cases only  but  indeed 
arguments must taken consideration 
exponential number individual histories need evaluated  joint
part sequence form left milp solver  every computation done
particular history  computing value checking extraneous  greater
reusability computations done entire policies  history shared many
joint policies individual policy  way  sequence form allows us work
reusable part policies without work directly world distributions
set joint policies 
then  milps derived sequence form dec pomdps need memory size
grows exponentially horizon number agents  obviously 
complexity quickly overwhelming case every exact method
far  shown experiments  milp approach derived sequence form
compares quite well dynamic programming  even outperformed forward methods
gmaa  
q   milp sometimes take little time find optimal joint
policy compared existing algorithms 
despite complexity milp approach  three factors contribute relative
efficiency milp 
   first  efficiency linear programming tools themselves  solving milp 
bb method solves sequence linear programs using simplex algorithm 
lps relaxation milp  theory  simplex algorithm requires
worst case exponential number steps  in size lp  solving lp   
well known that  practice  usually solves lp polynomial number
steps  in size lp   since size relaxation milp exponential
horizon  means that  roughly speaking  time taken solve relaxation
milp exponential horizon whereas doubly exponential
methods 
   second factor sparsity matrix coefficients constraints
milp  sparsity matrix formed coefficients constraints
   statement must qualified  worst case time requirement demonstrated
variants simplex algorithm  demonstrated basic version simplex
algorithm 

   

fiaras   dutech

lp determines practice rate pivoting algorithm
simplex solves lp  this applies lemkes algorithm context
lcp   sparser matrix  lesser time required perform elementary
pivoting  row  operations involved simplex algorithm lesser space
required model lp 
   third factor fact supplement milp cuts  computational
experience clearly shows speeds computations  first two
factors related solving relaxation milp  i e   lp   third factor
impact bb method itself  upper bound cut identifies additional
terminating condition bb method  thus enabling terminate earlier
absence condition  lower bound cut attempts shorten list
active subproblems  lps  bb method solves sequentially  due
cut  bb method potentially lesser number lps solve  note
inserting lower bound cut  emulating forward search properties
a  algorithm 
q   know milp solver  ilogs cplex experiments 
reason speedup 
clearly  approach would slower  even sometime slower classical dynamic
programming approach used another program solving milps experimented milps solvers neos website indeed
slow  true cplex  solver used experiments  quite optimized 
nevertheless  exactly one points wanted experiment paper 
one advantages formulating dec pomdp milp possibility use
fact that  mixed integer linear programs important industrial world 
optimized solvers exist 
then  formulate dec pomdp milp mostly paper
about 
q   main contribution paper 
stated earlier paper  current algorithms dec pomdps largely inspired pomdps algorithms  main contribution pursue entirely different
approach  i e   mixed integer linear programming  such  learned lot
dec pomdps pro   con mathematical programming approach 
lead formulation new algorithms 
designing algorithms  have  first all  drawn attention new representation policy  namely sequence form policy  introduced koller  megiddo
von stengel  sequence form policy compact representation
policy agent  afford compact representation set policies
agent 
algorithms proposed finite horizon dec pomdps mathematical
programming algorithms  precise      milps  mdp domain 
   

fimathematical programming dec pomdps

mathematical programming long used solving infinite horizon case 
instance  infinite horizon mdp solved linear program  depenoux        
recently  mathematical programming directed infinite horizon pomdps
dec pomdps  thus  infinite horizon dec mdp  with state transition independence  solved     milp  petrik   zilberstein        infinite horizon
pomdp dec pomdp solved  for local optima  nonlinear program  amato  bernstein    zilberstein      b      a   finite horizon case much different
character infinite horizon case dealt using dynamic programming 
stated earlier  whereas dynamic programming quite successful finite horizon
mdps pomdps  less finite horizon dec pomdps 
contrast  game theory  mathematical programming successfully directed
games finite horizon  lemkes algorithm        two player normal form games 
govindan wilson algorithm        n player normal form games koller  megiddo
von stengel approach  which internally uses lemkes algorithm  two player extensive
form games finite horizon games 
remained find way appropriate mathematical programming
solving finite horizon case pomdp dec pomdp domain  work done
precisely  incidently  algorithm solving kind n player normal form games   throughout paper  shown mathematical programming
 in particular      integer programming  applied solving finite horizon decpomdps  it easy see approach presented yields linear program
solving finite horizon pomdp   additionally  computational experience
approach indicates finite horizon dec pomdps  mathematical programming may
better  faster  dynamic programming  shown well entrenched
dynamic programming heuristic pruning redundant extraneous objects  in
case  histories  integrated mathematical programming approach 
hence  main contribution paper presents  first time  alternative approach solving finite horizon pomdps dec pomdps based milps 
q   mathematical programming approach presented paper something dead end 
question bit controversial short answer question could
small yes  true every approach looks exact optimal solutions
dec pomdps  whether grounded dynamic programming forward search
mathematical programming  complexity problem  exact solution
always untractable algorithms still improved 
longer answer mitigated  especially light recent advances made
dynamic programming forward search algorithms  one crucial point sequenceform dec pomdps pruning extraneous histories  recent work oliehoek 
whiteson  spaan        shown clusters histories equivalent
way could reduce nomber constraints milps  approach amato 
dibangoye  zilberstein        improves speed dynamic programming
operator could help finding extraneous histories  so  least  work
   

fiaras   dutech

still required stating every aspect sequence form dec pomdps
studied 
turn even longer answer  consider long horizon case  given exact
algorithms  including ones presented paper  tackle horizons less   
long horizon  mean anything upwards   time periods  long horizon case 
required conceive possibly sub optimal joint policy given horizon
determine upper bound loss value incurred using joint policy instead
using optimal joint policy 
current trend long horizon case memory bounded approach  memory
bounded dynamic programming  mbdp  algorithm  seuken   zilberstein       
main exponent approach  algorithm based backward induction dp
algorithm  hansen et al          algorithm attempts run limited amount
space  order so  unlike dp algorithm  prunes even non extraneous  i e   nondominated  policy trees iteration  thus  iteration  algorithm retains
pre determined number trees  algorithm variants used find
joint policy mabc  ma tiger box pushing problems long
horizons  of order thousands time periods  
mbdp provide upper bound loss value  bounded dp  bdp 
algorithm presented paper amato  carlin  zilberstein      c  give
upper bound  however  interesting dec pomdp problems  such ma tiger  
mbdp finds much better joint policy bdp 
meaningful way introduce notion memory boundedness approach
fix priori upper bound size concerned mathematical program 
presents sorts difficulties main difficulty seems need represent
policy long horizon limited space  mbdp algorithm solves problem
using may termed recursive representation  recursive representation
causes mbdp algorithm take long time evaluate joint policy  allow
algorithm represent long horizon joint policy limited space  context
mathematical programming approach  would change policy constraints
way long horizon policy represented system consisting limited
number linear equations linear inequalities  besides policy constraints 
constraints presented programs would accordingly transfigured 
evident  to us  transfiguration constraints possible 
hand  infinite horizon case seems promising candidate adapt
approach to  mathematical programming already applied  success 
solving infinite horizon dec pomdps  amato et al       a   computational experience mathematical programming approach shows better  finds higher
quality solutions lesser time  dynamic programming approach  bernstein et al  
      szer   charpillet        
nevertheless  approach two inter related shortcomings  first  approach
finds joint controller  i e   infinite horizon joint policy  fixed size
optimal size  second  much graver first  fixed size  finds locally optimal
joint controller  approach guarantee finding optimal joint controller 
program presented work amato et al       a   non convex 
   

fimathematical programming dec pomdps

nonlinear program  nlp   nlp finds fixed size joint controller canonical form
 i e   form finite state machine   believe shortcomings
removed conceiving mathematical program  specifically      mixed integer linear
program  finds joint controller sequence form  stated earlier  main
challenge regard therefore identification sequence form infinite
horizon policy  fact  may sequence form characterization infinite
horizon policy obtained  could used conceiving program long horizon
 undiscounted reward  case well 
q   help achieve designing artificial autonomous agents  
first sight  work direct immediate applied benefits
purpose building artificial intelligent agents understanding intelligence works 
even limited field multi agent planning  contributions theoretical
level practical one 
real artificial multi agent systems indeed modeled dec pomdps  even
make use communication  common knowledge  common social law  then  real
systems would likely made large number states  actions observations require
solutions large horizon  mathematical programming approach practically
useless setting limited dec pomdps small size  models
simpler far trivial solve explicitly take account
characteristics real systems exist  works take advantage communications
 xuan  lesser    zilberstein        ghavamzadeh   mahadevan         existing
independencies system  wu   durfee        becker  zilberstein  lesser    goldman 
       focus interaction agents  thomas  bourjot    chevrier        
some  said answering previous questions  rely approximate solutions  etc   
intention facilitate re use adaptation models
concepts used work knowledge structure optimal solution
dec pomdp  end  decided describe milp programs also 
importantly  derived programs making use properties
optimal dec pomdp solutions 
truly autonomous agents require adapt new unforeseen situations 
work dedicated planning  seems easy argue contribute
much end either  hand  learning dec pomdps never
really addressed except fringe work particular settings  scherrer   charpillet        ghavamzadeh   mahadevan        buffet  dutech    charpillet         fact 
even simple pomdps  learning difficult task  singh  jaakkola    jordan 
       currently  promising research deals learning predictive state
representation  psr  pomdp  singh  littman  jong  pardoe    stone        james
  singh        mccracken   bowling         making due allowance fundamental
differences functional role psr histories  notice psr histories quite similar structure  early say  might trying
learn useful histories dec pomdp could take inspiration way
right psrs learned pomdps 

   

fiaras   dutech

   conclusion
designed investigated new exact algorithms solving decentralized partially observable markov decision processes finite horizon  dec pomdps   main contribution paper use sequence form policies  based sets histories 
order reformulate dec pomdp non linear programming problem  nlp  
presented two different approaches linearize nlp order find global
optimal solutions dec pomdps  first approach based combinatorial
properties optimal policies dec pomdps second one relies concepts
borrowed field game theory  lead formulating dec pomdps    
mixed integer linear programming problems  milps   several heuristics speeding
resolution milps make another important contribution work 
experimental validation mathematical programming problems designed
work conducted classical dec pomdp problems found literature 
experiments show that  expected  milp methods outperform classical dynamic
programming algorithms  but  general  less efficient costly
forward search methods gmaa   especially case dec pomdp admits
many optimal policies  nevertheless  according nature problem  milp methods
sometimes greatly outperform gmaa   as ma tiger problem  
clear exact resolution dec pomdps scale size
problems length horizon  designing exact methods useful order
develop improve approximate methods  see least three research directions
work contribute  one direction could take advantage large
literature algorithms finding approximate solutions milps adapt
milps formulated dec pomdps  another direction would use knowledge
gained work derive improved heuristics guiding existing approximate existing
methods dec pomdps  example  work seuken zilberstein        
order limit memory resources used resolution algorithm  prune space
policies consider them  work could help using better estimation
policies important kept search space  then  one direction
currently investigating adapt approach dec pomdps infinite length
looking yet another representation would allow problems seen milps 
importantly  work participates better understanding dec pomdps 
analyzed understood key characteristics nature optimal policies order
design milps presented paper  knowledge useful work
dealing dec pomdps even pomdps  experimentations given
interesting insights nature various problems tested  term existence
extraneous histories number optimal policies  insights might first
step toward taxonomy dec pomdps 

appendix a  non convex non linear program
using simplest example  section aims showing non linear program
 nlp  expressed table   non convex 
   

fimathematical programming dec pomdps

let us consider example two agents  one   possible actions  a b 
want solve horizon   decision problem  set possible joint histories then 
ha  ai  ha  bi  hb  ai hb  bi  nlp solve is 
variables  x   a   x   b   x   a   x   a 
maximize

r   ha  ai x   a x   a    r   ha  bi x   a x   b 

     

 r   hb  ai x   b x   a    r   hb  bi x   b x   b 
subject
x   a    x   b     
x   a    x   b     
x   a    

x   b   

x   a    

x   b   

matrix formulation objective
x following kind 

    c
    e
c 
c e  
f  

function eq        would xt  c x c


f

 
 




x   a 
x   b 

x 
x   a   
x   b 

     

eigen value vector v    v  v  v  v   t straightforward show
eigen value   v  v  v  v   t   c  v  v  v  v   t   result 
matrix c  hessian objective function  positive definite thus objective
function convex 

appendix b  linear program duality
every linear program  lp  converse linear program called dual  first lp
called primal distinguish dual  primal maximizes quantity 
dual minimizes quantity  n variables constraints primal 
variables n constraints dual  consider following  primal  lp 
variables  x i            n 
maximize

n
x

c i x i 

i  

subject to 
n
x

a i  j x i    b j  

j          

i  

x i    

          n

   

fiaras   dutech

primal lp one variable x i      n  data lp consists
numbers c i      n  numbers b j  j     numbers
a i  j      n j     m  lp thus n variables
constraints  dual lp following lp 
variables  y j   j           


minimize


x

b j y j 

j  

subject to 



x

a i  j y j  c i  

          n

j  

y j        

j          

dual lp one variable y j  j     m  y j  variable free
variable  is  allowed take value r  dual lp variables n
constraints 
theorem linear programming duality follows 
theorem b     luenberger        either primal lp dual lp finite optimal
solution  other  corresponding values objective functions
equal 
applying theorem primal dual pair given above  holds 
n
x

c i x  i   


x

b j y  j 

j  

i  

x denotes optimal solution primal denotes optimal solution
dual 
theorem complementary slackness follows 
theorem b     vanderbei        suppose x feasible primal linear program
feasible dual  let  w     wm   denote corresponding primal slack variables 
let  z     zn   denote corresponding dual slack variables  x optimal
respective problems
xj zj    

j        n 

wi yi    

       m 
   

fimathematical programming dec pomdps

appendix c  regret dec pomdps
value information set ii agent i reduced joint policy q 
denoted    q   defined by 
x
   q    max
r   hh  j i q j  
     
h

j ei

terminal information set and  non terminal  by 
x
 h o  q 
   q    max
h

     

ooi

then  regret history h agent i reduced joint policy q 
denoted  h  q   defined by 
x
 h  q      h   q 
r   hh  j i q j  
     

j hi

h terminal and  h non terminal  by 
 h  q      h   q 

x

 h o  q 

     

ooi

concept regret agent i  independant policy agent i 
useful looking optimal policy optimal value known    
thus easier manipulate optimal value policy 

appendix d  program changes due optimizations
pruning locally globally extraneous histories reduces size search space
mathematical programs  now  constraints programs depend size
search space  must alter constraints 
let denote superscript sets actually used program  example  ei
actual set terminal histories agent i  pruned extraneous histories
not 
programs milp  table    milp n agents  table    rely fact
number histories given length support pure policy agent fixed
equal  oi  t    may case pruned sets  following changes
made 
constraint      milp       milp n agents 
x

z j   
 oi  t  
je

ii

must replaced
x

z j 


ii

je

   

 oi  t    

     

fiaras   dutech

set constraints      milp       milp n agents 
x

z hh  j i   

j ei



 ok  t   xi  h  

i  h ei



 ok  t   xi  h  

i  h ei  

ki  i 

must replaced
x

z hh  j i 

     

ki  i 

j ei

set constraints       milp n agents 
yi   h  

x
 
r   hh  ji i z j    wi  h  
 oi  t  

h ei

je

must replaced
yi   h  

x
 
r   hh  ji i z j    wi  h  
 oi  t  

h ei  

     

je

appendix e  example using ma tiger
example derived using decentralized tiger problem  ma tiger  described
section      two agents    actions  al   ar   ao     observations  ol     
consider problem horizon   
          terminal histories agent  ao  ol  ao   ao  ol  al   ao  ol  ar   ao  or  ao  
ao  or  al   ao  or  ar   al  ol  ao   al  ol  al   al  ol  ar   al  or  ao   al  or  al   al  or  ar   ar  ol  ao   ar  ol  al  
ar  ol  ar   ar  or  ao   ar  or  al   ar  or  ar  
thus                     joint histories agents  hao  ol  ao  ao  ol  ao i hao  ol  ao  ao  ol  al i 
hao  ol  ao  ao  ol  ar i    har  or  ar  ar  or  ar i 
e   policy constraints
policy constraints horizon   one agent ma tiger problem would be 
variables  x every history
x ao     x al     x ar      
x ao     x ao  ol  ao     x ao  ol  al     x ao  ol  ar      
x ao     x ao  or  ao     x ao  or  al     x ao  or  ar      
x al     x al  ol  ao     x al  ol  al     x al  ol  ar      
x al     x al  or  ao     x al  or  al     x al  or  ar      
x ar     x ar  ol  ao     x ar  ol  al     x ar  ol  ar      
x ar     x ar  or  ao     x ar  or  al     x ar  or  ar      
   

fimathematical programming dec pomdps

x ao    

x al    

x ar    

x ao  ol  ao     x ao  ol  al     x ao  ol  ar    
x ao  or  ao     x ao  or  al     x ao  or  ar    
x al  ol  ao    

x al  ol  al    

x al  ol  ar    

x al  or  ao     x al  or  al     x al  or  ar    
x ar  ol  ao     x ar  ol  al     x ar  ol  ar    
x ar  or  ao     x ar  or  al     x ar  or  ar    
e   non linear program ma tiger
non linear program finding optimal sequence form policy ma tiger
horizon   would be 
variables  xi every history agent

maximize

r   hao  ol  ao   ao  ol  ao i x   ao  ol  ao  x   ao  ol  ao  
  r   hao  ol  ao   ao  ol  al i x   ao  ol  ao  x   ao  ol  al  
  r   hao  ol  ao   ao  ol  ar i x   ao  ol  ao  x   ao  ol  ar  
 

subject to 
x   ao     x   al     x   ar      
x   ao     x   ao  ol  ao     x   ao  ol  al     x   ao  ol  ar      
x   ao     x   ao  or  ao     x   ao  or  al     x   ao  or  ar      
x   al     x   al  ol  ao     x   al  ol  al     x   al  ol  ar      
x   al     x   al  or  ao     x   al  or  al     x   al  or  ar      
x   ar     x   ar  ol  ao     x   ar  ol  al     x   ar  ol  ar      
x   ar     x   ar  or  ao     x   ar  or  al     x   ar  or  ar      

x   ao     x   al     x   ar      
x   ao     x   ao  ol  ao     x   ao  ol  al     x   ao  ol  ar      
x   ao     x   ao  or  ao     x   ao  or  al     x   ao  or  ar      
x   al     x   al  ol  ao     x   al  ol  al     x   al  ol  ar      
x   al     x   al  or  ao     x   al  or  al     x   al  or  ar      
x   ar     x   ar  ol  ao     x   ar  ol  al     x   ar  ol  ar      
x   ar     x   ar  or  ao     x   ar  or  al     x   ar  or  ar      
   

fiaras   dutech

x   ao    

x   al    

x   ar    

x   ao  ol  ao     x   ao  ol  al     x   ao  ol  ar    
x   ao  or  ao     x   ao  or  al     x   ao  or  ar    
x   al  ol  ao    

x   al  ol  al    

x   al  ol  ar    

x   al  or  ao     x   al  or  al     x   al  or  ar    
x   ar  ol  ao     x   ar  ol  al     x   ar  ol  ar    
x   ar  or  ao     x   ar  or  al     x   ar  or  ar    

x   ao    

x   al    

x   ar    

x   ao  ol  ao     x   ao  ol  al     x   ao  ol  ar    
x   ao  or  ao     x   ao  or  al     x   ao  or  ar    
x   al  ol  ao    

x   al  ol  al    

x   al  ol  ar    

x   al  or  ao     x   al  or  al     x   al  or  ar    
x   ar  ol  ao     x   ar  ol  al     x   ar  ol  ar    
x   ar  or  ao     x   ar  or  al     x   ar  or  ar    

e   milp ma tiger
milp horizon   agents ma tiger problem would be 
variables 
xi  h  every history agent
z j  every terminal joint history

maximize

r   hao  ol  ao   ao  ol  ao i z hao  ol  ao   ao  ol  ao i 
  r   hao  ol  ao   ao  ol  al i z hao  ol  ao   ao  ol  al i 
  r   hao  ol  ao   ao  ol  ar i z hao  ol  ao   ao  ol  ar i 
 
   

fimathematical programming dec pomdps

subject to 
x   ao     x   al     x   ar      
x   ao     x   ao  ol  ao     x   ao  ol  al     x   ao  ol  ar      
x   ao     x   ao  or  ao     x   ao  or  al     x   ao  or  ar      

x   ao     x   al     x   ar      
x   ao     x   ao  ol  ao     x   ao  ol  al     x   ao  ol  ar      
x   ao     x   ao  or  ao     x   ao  or  al     x   ao  or  ar      

z hao  ol  ao   ao  ol  ao i    z hao  ol  ao   ao  ol  al i    z hao  ol  ao   ao  ol  ar i      x   ao  ol  ao  
z hao  ol  ao   ao  ol  ao i    z hao  ol  al   ao  ol  ao i    z hao  ol  ar   ao  ol  ao i      x   ao  ol  ao  
z hao  ol  al   ao  ol  ao i    z hao  ol  al   ao  ol  al i    z hao  ol  al   ao  ol  ar i      x   ao  ol  al  
z hao  ol  ao   ao  ol  al i    z hao  ol  al   ao  ol  al i    z hao  ol  ar   ao  ol  al i      x   ao  ol  al  


x   ao    

x   al    

x   ar    

x   ao  ol  ao         

x   ao  ol  al         

x   ao  ol  ar         

x   ao  or  ao         

x   ao  or  al         

x   ao  or  ar         


x   ao    

x   al    

x   ar    

x   ao  ol  ao         

x   ao  ol  al         

x   ao  ol  ar         

x   ao  or  ao         

x   ao  or  al         

x   ao  or  ar         


z hao  ol  ao   ao  ol  ao i         z hao  ol  ao   ao  ol  al i         z hao  ol  ao   ao  ol  ar i        
z hao  ol  al   ao  ol  ao i         z hao  ol  al   ao  ol  al i         z hao  ol  al   ao  ol  ar i        

e   milp   agents ma tiger
milp   agents horizon   agents ma tiger problem would be 
variables 
xi  h   wi  h  bi  h  every history agent
yi     agent every information set
maximize

y    

   

fiaras   dutech

subject to 

x   ao     x   al     x   ar      
x   ao     x   ao  ol  ao     x   ao  ol  al     x   ao  ol  ar      
x   ao     x   ao  or  ao     x   ao  or  al     x   ao  or  ar      

x   ao     x   al     x   ar      
x   ao     x   ao  ol  ao     x   ao  ol  al     x   ao  ol  ar      
x   ao     x   ao  or  ao     x   ao  or  al     x   ao  or  ar      

y     y   ao  ol   y   ao  or     w   ao  
y     y   al  ol   y   al  or     w   al  
y     y   ar  ol   y   ar  or     w   ar  
y     y   ao  ol   y   ao  or     w   ao  
y     y   al  ol   y   al  or     w   al  
y     y   ar  ol   y   ar  or     w   ar  

y   ao  ol   r   hao  ol  ao   ao  ol  ao i x   ao  ol  ao  
r   hao  ol  ao   ao  ol  al i x   ao  ol  al  
r   hao  ol  ao   ao  ol  ar i x   ao  ol  ar  
r   hao  ol  ao   al  ol  ao i x   al  ol  ao  
r   hao  ol  ao   al  ol  al i x   al  ol  al  
r   hao  ol  ao   al  ol  ar i x   al  ol  ar  
  w   ao  ol  ao  

y   ao  ol   r   hao  ol  al   ao  ol  ao i x   ao  ol  ao  
r   hao  ol  al   ao  ol  al i x   ao  ol  al  
r   hao  ol  al   ao  ol  ar i x   ao  ol  ar  
r   hao  ol  al   al  ol  ao i x   al  ol  ao  
r   hao  ol  al   al  ol  al i x   al  ol  al  
r   hao  ol  al   al  ol  ar i x   al  ol  ar  
  w   ao  ol  al  
   

fimathematical programming dec pomdps


y   ar  or   r   har  or  ar   ao  ol  ao i x   ao  ol  ao  
r   har  or  ar   ao  ol  al i x   ao  ol  al  
r   har  or  ar   ao  ol  ar i x   ao  ol  ar  
r   har  or  ar   al  ol  ao i x   al  ol  ao  
r   har  or  ar   al  ol  al i x   al  ol  al  
r   har  or  ar   al  ol  ar i x   al  ol  ar  
  w   ar  or  ar  
y   ao  ol   r   hao  ol  ao   ao  ol  ao i x   ao  ol  ao  
r   hao  ol  al   ao  ol  ao i x   ao  ol  al  
r   hao  ol  ar   ao  ol  ao i x   ao  ol  ar  
r   hal  ol  ao   ao  ol  ao i x   al  ol  ao  
r   hal  ol  al   ao  ol  ao i x   al  ol  al  
r   hal  ol  ar   ao  ol  ao i x   al  ol  ar  
  w   ao  ol  ao  
y   ao  ol   r   hao  ol  ao   ao  ol  al i x   ao  ol  ao  
r   hao  ol  al   ao  ol  al i x   ao  ol  al  
r   hao  ol  ar   ao  ol  al i x   ao  ol  ar  
r   hal  ol  ao   ao  ol  al i x   al  ol  ao  
r   hal  ol  al   ao  ol  al i x   al  ol  al  
r   hal  ol  ar   ao  ol  al i x   al  ol  ar  
  w   ao  ol  al  


x   ao     b   ao  

x   al     b   al  

x   ar     b   ar  

x   ao  ol  ao     b   ao  ol  ao  

x   ao  ol  al     b   ao  ol  al  

x   ao  ol  ar     b   ao  ol  ar  



w   ao   u   ao  b   ao  

w   al   u   al  b   al  

w   ar   u   ar  b   ar  

w   ao  ol  ao   u   ao  ol  ao  b   ao  ol  ao  

w   ao  ol  al   u   ao  ol  al  b   ao  ol  al  

w   ao  ol  ar   u   ao  ol  ar  b   ao  ol  ar  


   

fiaras   dutech

x   ao    
x   ao  ol  ao    

x   al    
x   ao  ol  al    

x   ar    
x   ao  ol  ar    


w   ao    
w   ao  ol  ao    

w   al    
w   ao  ol  al    

w   ar    
w   ao  ol  ar    


b   ao         
b   ao  ol  ao         

b   al         
b   ao  ol  al         


y          
y   ao  ol         y   ao  or        

    agent  

   

b   ar         
b   ao  ol  ar         

fimathematical programming dec pomdps

references
amato  c   bernstein  d  s     zilberstein  s       a   optimizing memory bounded controllers decentralized pomdps  proc  twenty third conf  uncertainty
artificial intelligence  uai     
amato  c   bernstein  d  s     zilberstein  s       b   solving pomdps using quadratically
constrained linear programs  proc  twentieth int  joint conf  artificial
intelligence  ijcai    
amato  c   carlin  a     zilberstein  s       c   bounded dynamic programming
decentralized pomdps  proc  workshop multi agent sequential decision
making uncertain domains  msdm  aamas   
amato  c   dibangoye  j     zilberstein  s          incremental policy generation finitehorizon dec pomdps  proc  nineteenth int  conf  automated planning
scheduling  icaps     
anderson  b     moore  j          time varying feedback laws decentralized control 
nineteenth ieee conference decision control including symposium
adaptive processes                
becker  r   zilberstein  s   lesser  v     goldman  c          solving transition independent
decentralized markov decision processes  journal artificial intelligence research 
           
bellman  r          dynamic programming  princeton university press  princeton  newjersey 
bernstein  d   givan  r   immerman  n     zilberstein  s          complexity decentralized control markov decision processes  mathematics operations research 
               
bernstein  d  s   hansen  e  a     zilberstein  s          bounded policy iteration
decentralized pomdps  proc  nineteenth int  joint conf  artificial
intelligence  ijcai   pp           
boularias  a     chaib draa  b          exact dynamic programming decentralized
pomdps lossless policy compression  proc  int  conf  automated
planning scheduling  icaps    
boutilier  c          planning  learning coordination multiagent decision processes 
proceedings  th conference theoretical aspects rationality knowledge  tark      de zeeuwse stromen  nederlands 
buffet  o   dutech  a     charpillet  f          shaping multi agent systems gradient reinforcement learning  autonomous agent multi agent system journal
 aamasj                  
   

fiaras   dutech

cassandra  a   kaelbling  l     littman  m          acting optimally partially observable
stochastic domains  proc    th nat  conf  artificial intelligence  aaai  
chades  i   scherrer  b     charpillet  f          heuristic approach solving
decentralized pomdp  assessment pursuit problem  proc       acm
symposium applied computing  pp       
cornuejols  g          valid inequalities mixed integer linear programs  mathematical
programming b           
dantzig  g  b          significance solving linear programming problems
integer variables  econometrica              
depenoux  f          probabilistic production inventory problem  management
science               
diwekar  u          introduction applied optimization    edition   springer 
drenick  r          multilinear programming  duality theories  journal optimization
theory applications                
fletcher  r          practical methods optimization  john wiley   sons  new york 
ghavamzadeh  m     mahadevan  s          learning communicate act cooperative multiagent systems using hierarchical reinforcement learning  proc   rd
int  joint conf  autonomous agents multi agent systems  aamas    
govindan  s     wilson  r          global newton method compute nash equilibria 
journal economic theory            
hansen  e   bernstein  d     zilberstein  s          dynamic programming partially
observable stochastic games  proc  nineteenth national conference artificial intelligence  aaai     
horst  r     tuy  h          global optimization  deterministic approaches   rd edition  
springer 
james  m     singh  s          learning discovery predictive state representations
dynamical systems reset  proc  twenty first int  conf  machine
learning  icml    
kaelbling  l   littman  m     cassandra  a          planning acting partially
observable stochastic domains  artificial intelligence             
koller  d   megiddo  n     von stengel  b          fast algorithms finding randomized
strategies game trees  proceedings   th acm symposium theory
computing  stoc      pp         
koller  d     megiddo  n          finding mixed strategies small supports extensive
form games  international journal game theory              
   

fimathematical programming dec pomdps

lemke  c          bimatrix equilibrium points mathematical programming  management science                
luenberger  d          linear nonlinear programming  addison wesley publishing
company  reading  massachussetts 
mccracken  p     bowling  m  h          online discovery learning predictive state
representations  advances neural information processing systems     nips    
nair  r   tambe  m   yokoo  m   pynadath  d     marsella  s          taming decentralized
pomdps  towards efficient policy computation multiagent setting  proc 
int  joint conference artificial intelligence  ijcai   
oliehoek  f   spaan  m     vlassis  n          optimal approximate q value functions
decentralized pomdps  journal artificial intelligence research  jair      
       
oliehoek  f   whiteson  s     spaan  m          lossless clustering histories decentralized pomdps  proc  international joint conference autonomous
agents multi agent systems  pp         
osborne  m  j     rubinstein  a          course game theory  mit press 
cambridge  mass 
papadimitriou  c  h     steiglitz  k          combinatorial optimization  algorithms
complexity  dover publications 
papadimitriou  c  h     tsitsiklis  j          complexity markov decision processes  mathematics operations research                  
parsons  s     wooldridge  m          game theory decision theory multi agent
systems  autonomous agents multi agent systems  jaamas                
petrik  m     zilberstein  s          average reward decentralized markov decision processes  proc  twentieth int  joint conf  artificial intelligence  ijcai
      
petrik  m     zilberstein  s          bilinear programming approach multiagent
planning  journal artificial intelligence research  jair              
puterman  m          markov decision processes  discrete stochastic dynamic programming  john wiley   sons  inc  new york  ny 
pynadath  d     tambe  m          communicative multiagent team decision problem  analyzing teamwork theories models  journal artificial intelligence
research             
radner  r          application linear programming team decision problems 
management science            
russell  s     norvig  p          artificial intelligence  modern approach  prentice hall 
   

fiaras   dutech

sandholm  t          multiagent systems  chap  distributed rational decision making  pp 
        mit press  ed  g  weiss 
sandholm  t   gilpin  a     conitzer  v          mixed integer programming methods
finding nash equilibria  proc  national conference artificial intelligence
 aaai  
scherrer  b     charpillet  f          cooperative co learning  model based approach
solving multi agent reinforcement problems  proc  ieee int  conf 
tools artificial intelligence  ictai    
seuken  s     zilberstein  s          memory bounded dynamic programming decpomdps  proc  twentieth int  joint conf  artificial intelligence  ijcai    
singh  s   jaakkola  t     jordan  m          learning without state estimation partially
observable markovian decision processes   proceedings eleventh international
conference machine learning 
singh  s   littman  m   jong  n   pardoe  d     stone  p          learning predictive state
representations  proc  twentieth int  conf  machine learning  icml    
szer  d     charpillet  f          point based dynamic programming dec pomdps 
proc  twenty first national conf  artificial intelligence  aaai       
szer  d   charpillet  f     zilberstein  s          maa   heuristic search algorithm
solving decentralized pomdps  proc  twenty first conf  uncertainty
artificial intelligence  uai     pp          
thomas  v   bourjot  c     chevrier  v          interac dec mdp   towards use
interactions dec mdp  proc  third int  joint conf  autonomous
agents multi agent systems  aamas     new york  usa  pp           
vanderbei  r  j          linear programming  foundations extensions   rd edition  
springer 
von stengel  b          handbook game theory  vol     chap     computing equilibria
two person games  pp            north holland  amsterdam 
wu  j     durfee  e  h          mixed integer linear programming transitionindependent decentralized mdps  proc  fifth int  joint conf  autonomous
agents multiagent systems  aamas     pp           new york  ny  usa 
acm 
xuan  p   lesser  v     zilberstein  s          communication multi agent markov
decision processes  proc  icmas workshop game theoretic decision
theoretics agents boston  ma 

   


