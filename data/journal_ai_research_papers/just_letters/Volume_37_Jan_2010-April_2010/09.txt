journal of artificial intelligence research                  

submitted        published      

training a multilingual sportscaster 
using perceptual context to learn language
david l  chen
joohyun kim
raymond j  mooney

dlcc   cs   utexas   edu
scimitar   cs   utexas   edu
mooney   cs   utexas   edu

department of computer science
the university of texas at austin
  university station c      austin tx        usa

abstract
we present a novel framework for learning to interpret and generate language using only perceptual context as supervision  we demonstrate its capabilities by developing a system that learns to
sportscast simulated robot soccer games in both english and korean without any language specific
prior knowledge  training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace  the system
simultaneously establishes correspondences between individual comments and the events that they
describe while building a translation model that supports both parsing and generation  we also
present a novel algorithm for learning which events are worth describing  human evaluations of
the generated commentaries indicate they are of reasonable quality and in some cases even on par
with those produced by humans for our limited domain 

   introduction
most current natural language processing  nlp  systems are built using statistical learning algorithms trained on large annotated corpora  however  annotating sentences with the requisite parse
trees  marcus  santorini    marcinkiewicz         word senses  ide   jeronis        and semantic
roles  kingsbury  palmer    marcus        is a difficult and expensive undertaking  by contrast 
children acquire language through exposure to linguistic input in the context of a rich  relevant 
perceptual environment  also  by connecting words and phrases to objects and events in the world 
the semantics of language is grounded in perceptual experience  harnad         ideally  a machine
learning system would be able to acquire language in a similar manner without explicit human supervision  as a step in this direction  we present a system that can describe events in a simulated
soccer game by learning only from sample language commentaries paired with traces of simulated
activity without any language specific prior knowledge  a screenshot of our system with generated
commentary is shown in figure   
while there has been a fair amount of research on grounded language learning  roy       
bailey  feldman  narayanan    lakoff        barnard  duygulu  forsyth  de freitas  blei    jordan        yu   ballard        gold   scassellati         most of the focus has been on dealing
with raw perceptual data rather than language issues  many of these systems aimed to learn meanings of words and phrases rather than interpreting entire sentences  some more recent work has dealt
with fairly complex language data  liang  jordan    klein        branavan  chen  zettlemoyer   
c
    
ai access foundation  all rights reserved 

fic hen   k im     m ooney

figure    screenshot of our commentator system
barzilay        but do not address all three problems of alignment  semantic parsing  and natural
language generation  in contrast  our work investigates how to build a complete language learning
system using parallel data from the perceptual context  we study the problem in a simulated environment that retains many of the important properties of a dynamic world with multiple agents and
actions while avoiding many of the complexities of robotics and computer vision  specifically  we
use the robocup simulator  chen  foroughi  heintz  kapetanakis  kostiadis  kummeneje  noda 
obst  riley  steffens  wang    yin        which provides a fairly detailed physical simulation
of robot soccer  while several groups have constructed robocup commentator systems  andre 
binsted  tanaka ishii  luke  herzog    rist        that provide a textual natural language  nl 
transcript of the simulated game  their systems use manually developed templates and are not based
on learning 
our commentator system learns to semantically interpret and generate language in the robocup
soccer domain by observing an on going commentary of the game paired with the evolving simulator state  by exploiting existing techniques for abstracting a symbolic description of the activity on
the field from the detailed states of the physical simulator  andre et al          we obtain a pairing
of natural language with a symbolic description of the perceptual context in which it was uttered 
however  such training data is highly ambiguous because each comment usually co occurs with several events in the game  we integrate and enhance existing methods for learning semantic parsers
and nl generators  kate   mooney        wong   mooney        in order to learn to understand
and generate language from such ambiguous training data  we also develop a system that  from the
same ambiguous training data  learns which events are worth describing  so that it can also perform
strategic generation  that is  deciding what to say as well as how to say it  tactical generation    
   for conciseness  we use this terminology from early work in generation  e g   mckeown         strategic and tactical
generation are now also commonly referred to as content selection and surface realization  respectively

   

fit raining a m ultilingual s portscaster

we evaluate our system and demonstrate its language independence by training it to generate
commentaries in both english and korean  experiments on test data  annotated for evaluation purposes only  demonstrate that the system learns to accurately semantically parse sentences  generate
sentences  and decide which events to describe  finally  subjective human evaluation of commentated game clips demonstrate that in our limited domain  the system generates sportscasts that are
in some cases similar in quality to those produced by humans 
there are three main contributions we make in this paper  first  we explore the possibility of
learning grounded language models from the perceptual context in the form of ambiguous parallel
data  second  we investigate several different methods of disambiguating this data and determined
that using a combined score that includes both tactical and strategic generation scores performed the
best overall  finally  we built a complete system that learns how to sportscast in multiple languages 
we carefully verified through automatic and human evaluations that the system is able to perform
several tasks including disambiguating the training data  semantic parsing  tactical and strategic
generation  while the language involved in this work is restricted compared to handcrafted commercial sportscasting systems  our goal was to demonstrate the feasibility of learning a grounded
language system with no language specific prior knowledge 
the remainder of the paper is structured as follows  section   provides background on previous work that we utilize and extend to build our system  section   describes the sportscasting
data we collected to train and test our approach  section   and section   present the details of our
basic methods for learning tactical and strategic generation  respectively  and some initial experimental results  section   discusses extensions to the basic system that incorporate information from
strategic generation into the process of disambiguating the training data  section   presents experimental results on initializing our system with data disambiguated by a recent method for aligning
language with facts to which it may refer  section   discusses additions that try to detect superfluous sentences that do not refer to any extracted event  section   presents human evaluation of our
automatically generated sportscasts  section    reviews related work  section    discusses future
work  and section    presents our conclusions 

   background
systems for learning semantic parsers induce a function that maps natural language  nl  sentences
to meaning representations  mrs  in some formal logical language  existing work has focused on
learning from a supervised corpus in which each sentence is manually annotated with its correct mr
 mooney        zettlemoyer   collins        lu  ng  lee    zettlemoyer        jurcicek  gasic 
keizer  mairesse  thomson    young         such human annotated corpora are expensive and
difficult to produce  limiting the utility of this approach  kate and mooney        introduced an
extension to one such system  k risp  kate   mooney         so that it can learn from ambiguous
training data that requires little or no human annotation effort  however  their system was unable to
generate language which is required for our sportscasting task  thus  we enhanced another system
called wasp  wong   mooney        that is capable of language generation as well as semantic
parsing in a similar manner to allow it to learn from ambiguous supervision  we briefly describe
the previous systems below  all of these systems assume they have access to a formal deterministic
context free grammar  cfg  that defines the formal meaning representation language  mrl   since
mrls are formal computer interpretable languages  such a grammar is usually easily available 
   

fic hen   k im     m ooney

    krisp and krisper
k risp  kernel based robust interpretation for semantic parsing   kate   mooney        uses
support vector machines  svms  with string kernels to build semantic parsers  svms are state ofthe art machine learning methods that learn maximum margin separators to prevent over fitting in
very high dimensional data such as natural language text  joachims         they can be extended to
non linear separators and non vector data by exploiting kernels that implicitly create an even higher
dimensional space in which complex data is  nearly  linearly separable  shawe taylor   cristianini 
       recently  kernels over strings and trees have been effectively applied to a variety of problems
in text learning and nlp  lodhi  saunders  shawe taylor  cristianini    watkins        zelenko 
aone    richardella        collins        bunescu   mooney         in particular  k risp uses
the string kernel introduced by lodhi et al         to classify substrings in an nl sentence 
first  k risp learns classifiers that recognize when a word or phrase in an nl sentence indicates
that a particular concept in the mrl should be introduced into its mr  it uses production rules
in the mrl grammar to represent semantic concepts  and it learns classifiers for each production
that classify nl substrings as indicative of that production or not  when semantically parsing a
sentence  each classifier estimates the probability of each production covering different substrings
of the sentence  this information is then used to compositionally build a complete mr for the
sentence  given the partial matching provided by string kernels and the over fitting prevention
provided by svms  k risp has been experimentally shown to be particularly robust to noisy training
data  kate   mooney        
k risper  kate   mooney        is an extension to k risp that handles ambiguous training
data  in which each sentence is annotated with a set of potential mrs  only one of which is correct 
psuedocode for the method is shown in algorithm    it employs an iterative approach analogous to
expectation maximization  em   dempster  laird    rubin        that improves upon the selection
of the correct nlmr pairs in each iteration  in the first iteration  lines       it assumes that all of
the mrs paired with a sentence are correct and trains k risp with the resulting noisy supervision 
in subsequent iterations  lines         k risper uses the currently trained parser to score each
potential nlmr pair  selects the most likely mr for each sentence  and retrains the parser on the
resulting disambiguated supervised data  in this manner  k risper is able to learn from the type of
weak supervision expected for a grounded language learner exposed only to sentences in ambiguous
contexts  however  the system has previously only been tested on artificially corrupted or generated
data 
    wasp and wasp 
wasp  word alignment based semantic parsing   wong   mooney        uses state of the art
statistical machine translation  smt  techniques  brown  cocke  della pietra  della pietra  jelinek 
lafferty  mercer    roossin        yamada   knight        chiang        to learn semantic
parsers  smt methods learn effective machine translators by training on parallel corpora consisting
of human translations of documents into one or more alternative natural languages  the resulting
translators are typically significantly more effective than manually developed systems and smt has
become the dominant approach to machine translation  wong and mooney        adapted such
methods to learn to translate from nl to mrl rather than from one nl to another 
first  an smt word alignment system  giza    och   ney        brown  della pietra  della
pietra    mercer         is used to acquire a bilingual lexicon consisting of nl substrings coupled
   

fit raining a m ultilingual s portscaster

algorithm   k risper
input sentences s and their associated sets of meaning representations m r s 
output bestexamplesset  a set of nl mr pairs 
semanticmodel   a k risp semantic parser
  
  
  
  
  
  
  
  
  

main
  initial training loop
for sentence si  s do
for meaning representation mj  m r si   do
add  si   mj   to initialtrainingset
end for
end for
semanticmodel   train initialtrainingset 

   
   
   
   
   
   
   
   
   
   
   
   

  iterative retraining
repeat
for sentence si  s do
for meaning representation mj  m r si   do
mj  score   evaluate si   mj   semanticmodel  
end for
end for
bestexampleset
 the set of consistent examples t     s  m  s  s  m  mr s  
p
such that t m score is maximized
semanticmodel   t rain bestexamplesset 
until convergence or max iter reached
end main

   

function train trainingexamples 
train k risp on the unambiguous trainingexamples
   
return the trained k risp semantic parser
    end function
   

   

   

function evaluate s  m  semanticmodel  
use the k risp semantic parser semanticmodel to find a derivation of meaning representation m from sentence s
   
return the parsing score
    end function

   

   

   

fic hen   k im     m ooney

with their translations in the target mrl  as formal languages  mrls frequently contain many
purely syntactic tokens such as parentheses or brackets  which are difficult to align with words in
nl  consequently  we found it was much more effective to align words in the nl with productions
of the mrl grammar used in the parse of the corresponding mr  therefore  giza   is used
to produce an n to   alignment between the words in the nl sentence and a sequence of mrl
productions corresponding to a top down left most derivation of the corresponding mr 
complete mrs are then formed by combining these nl substrings and their translations using a
grammatical framework called synchronous cfg  scfg   aho   ullman         which forms the
basis of most existing syntax based smt  yamada   knight        chiang         in an scfg  the
right hand side of each production rule contains two strings  in our case one in nl and the other in
mrl  derivations of the scfg simultaneously produce nl sentences and their corresponding mrs 
the bilingual lexicon acquired from word alignments over the training data is used to construct a set
of scfg production rules  a probabilistic parser is then produced by training a maximum entropy
model using em to learn parameters for each of these scfg productions  similar to the methods
used by riezler  prescher  kuhn  and johnson         and zettlemoyer and collins         to
translate a novel nl sentence into its mr  a probabilistic chart parser  stolcke        is used to find
the most probable synchronous derivation that generates the given nl  and the corresponding mr
generated by this derivation is returned 
since scfgs are symmetric  they can be used to generate nl from mr as well as parse nl into
mr  wong   mooney         this allows the same learned grammar to be used for both parsing
and generation  an elegant property that has important advantages  shieber         the generation
system  wasp    uses a noisy channel model  brown et al         
arg max pr e f     arg max pr e  pr f  e 
e

   

e

where e refers to the nl string generated for a given input mr  f   pr e  is the language model 
and pr f  e  is the parsing model provided by wasps learned scfg  the generation task is to find
a sentence e such that     e is a good sentence a priori  and     its meaning is the same as the input
mr  for the language model  we use a standard n gram model  which is useful in ranking candidate
generated sentences  knight   hatzivassiloglou        

   sportscasting data
to train and test our system  we assembled human commentated soccer games from the robocup
simulation league  www robocup org   since our focus is language learning and not computer vision  we chose to use simulated games instead of real game video to simplify the extraction of
perceptual information  based on the rocco robocup commentators incremental event recognition module  andre et al         we manually developed symbolic representations of game events
and a rule based system to automatically extract them from the simulator traces  the extracted
events mainly involve actions with the ball  such as kicking and passing  but also include other
game information such as whether the current playmode is kickoff  offside  or corner kick  the
events are represented as atomic formulas in predicate logic with timestamps  these logical facts
constitute the requisite mrs  and we manually developed a simple cfg for this formal semantic
language  details of the events detected and the complete grammar can be found in appendix a 
for the nl portion of the data  we had humans commentate games while watching them on the
simulator  we collected commentaries in both english and korean  the english commentaries were
   

fit raining a m ultilingual s portscaster

total   of comments
total   of words
vocabulary size
avg  words per comment

english dataset
    
     
   
    

korean dataset
    
    
   
    

table    word statistics for the english and korean datasets

number of events

total

     final
     final
     final
     final

    
    
    
    

   
   
   
   

     final
     final
     final
     final

    
    
    
    

   
   
   
   

number of comments
have mrs have correct mr
english dataset
   
   
   
   
   
   
   
   
korean dataset
   
   
   
   
   
   
   
   

events per comment
max average std  dev 
 
  
  
 

     
     
     
     

     
     
     
     

  
  
  
 

     
     
     
     

     
     
     
     

table    alignment statistics for the english and korean datasets  some comments do not have correct meaning representations associated with them and are essentially noise in the training
data      of the english dataset and    of the korean dataset   moreover  on average
there are more than   possible events linked to each comment so over half of these links
are incorrect 

produced by two different people while the korean commentaries were produced by a single person 
the commentators typed their comments into a text box  which were recorded with a timestamp 
to construct the final ambiguous training data  we paired each comment with all of the events that
occurred five seconds or less before the comment was made  examples of the ambiguous training
data are shown in figure    the edges connect sentences to events to which they might refer  english
translations of the korean commentaries have been included in the figure for the readers benefit and
are not part of the actual data  note that the use of english words for predicates and constants in the
mrs is for human readability only  the system treats these as arbitrary conceptual tokens and must
learn their connection to english or korean words 
we annotated a total of four games  namely  the finals for the robocup simulation league for
each year from      to       word statistics about the data are shown in table    while the
sentences are fairly short due to the nature of sportscasts  this data provides challenges in the form
of synonyms  e g  pink   pinkg and pink goalie all refer to the same player  and polysemes
 e g  kick in kicks toward the goal refers to a kick event whereas kicks to pink  refers to a
pass event   alignment statistics for the datasets are shown in table    the      final has almost
twice the number of events as the other games because it went into double overtime 
   

fic hen   k im     m ooney

natural language commentary

meaning representation
badpass   purpleplayer   
pinkplayer   
turnover   purpleplayer   
pinkplayer   
kick   pinkplayer   
pass   pinkplayer    pinkplayer    
kick   pinkplayer    

purple goalie turns the ball over to
pink 
purple team is very sloppy today
pink  passes to pink  
pink   looks around for a teammate

kick   pinkplayer    
ballstopped
kick   pinkplayer    
pass   pinkplayer     pinkplayer   
kick   pinkplayer   
pass   pinkplayer    pinkplayer    

pink   makes a long pass to pink 

pink  passes back to pink  

 a  sample trace of ambiguous english training data

natural language commentary

meaning representation

       
 purple   passes to purple    

kick   purpleplayer    

        
 purple   passes again to purple    

kick   purpleplayer    

        
 pink  steals the ball from purple    

steal   pinkplayer   

     
 pink  passes to pink goalie 

kick   pinkplayer   

pass   purpleplayer     purpleplayer    

pass   purpleplayer     purpleplayer    

turnover   purpleplayer     pinkplayer   

playmode   free kick r  

 b  sample trace of ambiguous korean training data

figure    examples of our training data  each of the outgoing edges from the comments indicate
a possibly associated meaning representations considered by our system  the bold links
indicate correct matches between the comments and the meaning representations 

   

fit raining a m ultilingual s portscaster

for evaluation purposes only  a gold standard matching was produced by examining each comment manually and selecting the correct mr if it exists  the matching is only approximate because
sometimes the comments contain more information than present in the mrs  for example  a comment might describe the location and the length of a pass while the mr captures only the participants
of a pass  the bold lines in figure   indicate the annotated correct matches in our sample data  notice some sentences do not have correct matches  about one fifth of the english data and one tenth
of the korean data   for example  the sentence purple team is very sloppy today in figure   a 
cannot be represented in our mrl and consequently does not have a corresponding correct mr 
for another example  the korean sentence with the translation pink  passes to pink goalie in figure   b  can be represented in our mrl  but does not have a correct match due to the incomplete
event detection  a free kick was called while pink  was passing to the pink goalie so the pass event
was not retrieved  finally  in the case of the sentence pink   makes a long pass to pink  in figure   a   the correct mr falls outside of the   second window  for each game  table   shows the
total number of nl sentences  the number of these that have at least one recent extracted event to
which it could refer  and the number of these that actually do refer to one of these recent extracted
events  the maximum  average  and standard deviation for the number of recent events paired with
each comment is also given 

   learning tactical generation from ambiguous supervision
while existing systems are capable of solving parts of the sportscasting problem  none of them are
able to perform the whole task  we need a system that can both deal with ambiguous supervision
like k risper and generate language like wasp  we introduce three systems below that can do
both  an overview of the differences between the existing systems and the new systems we present
are shown in table   
all three systems introduced here are based on extensions to wasp  our underlying language
learner  the main problem we need to solve is to disambiguate the training data so that we can
train wasp as before to create a language generator  each of the new system uses a different
disambiguation criteria to determine the best matching between the nl sentences and the mrs 

    wasper
the first system is an extension of wasp in a manner similar to how k risp was extended to create
k risper  it uses an em like retraining to handle ambiguously annotated data  resulting in a system
we call wasper  in general  any system that learns semantic parsers can be extended to handle
ambiguous data as long as it can produce confidence levels for given nlmr pairs  given a set of
sentences s  s and the set of mrs associated with each sentence m r s   we disambiguate the
data by finding pairs  s  m   s  s and m  m r s  such that m   arg maxm p r m s   although
probability is used here  a ranking of the relative potential parses would suffice  the pseudocode
for wasper is shown in algorithm    the only difference compared to the k risper pseudocode
is that we now use a wasp semantic parser instead of a k risp parser  also  we produce a wasp
language generator as well which is the desired final output for our task 
   

fic hen   k im     m ooney

algorithm

underlying learner

k risp
k risper
wasp

svm
k risp
giza to align words 
and mr tokens  then
learn probalistic scfg
wasp
first disambiguate
with k risper 
then train wasp
wasp

wasper
k risper  wasp

wasper  g en

generate 

disambiguation criteria

no
no
yes

ambiguous
data 
no
yes
no

yes
yes

yes
yes

wasps parsing score
k risps parsing score

yes

yes

nist score of
best nl given mr

n a
k risps parsing score
n a

table    overview of the various learning systems presented  the first three algorithms are existing
systems  we introduce the last three systems that are able to both learn from ambiguous
training data and acquire a language generator  they differ in how they disambiguate the
training data 

algorithm   wasper
input sentences s and their associated sets of meaning representations m r s 
output bestexamplesset  a set nl mr pairs 
semanticmodel   a wasp semantic parser language generator
   main
  
same as algorithm  
   end main
  

function train trainingexamples 
train wasp on the unambiguous trainingexamples
  
return the trained wasp semantic parser language generator
   end function

  

  

  

function evaluate s  m  semanticmodel  
   
use the wasp semantic parser in semanticmodel to find a derivation of meaning representation m from sentence s
   
return the parsing score
    end function
   

   

fit raining a m ultilingual s portscaster

    krisper wasp
k risp has been shown to be quite robust at handling noisy training data  kate   mooney        
this is important when training on the very noisy training data used to initialize the parser in
k rispers first iteration  however  k risper cannot learn a language generator  which is necessary for our sportscasting task  as a result  we create a new system called k risper wasp that
is both good at disambiguating the training data and capable of generation  we first use k risper
to train on the ambiguous data and produce a disambiguated training set by using its prediction for
the most likely mr for each sentence  this unambiguous training set is then used to train wasp to
produce both a parser and a generator 
    wasper gen
in both k risper and wasper  the criterion for selecting the best nlmr pairs during retraining is based on maximizing the probability of parsing a sentence into a particular mr  however 
since wasper is capable of both parsing and generation  we could alternatively select the best
nlmr pairs by evaluating how likely it is to generate the sentence from a particular mr  thus 
we built another version of wasper called wasper g en that disambiguates the training data in
order to maximize the performance of generation rather than parsing  the pseudocode is shown in
algorithm    the algorithm is the same as wasper except for the evaluation function  it uses a
generation based score rather than a parsing based score to select the best nlmr pairs 
specifically  an nlmr pair  s  m  is scored by computing the nist score  a machine translation  mt  metric  between the sentence s and the best generated sentence for m  lines        
formally  given a set of sentences s  s and the set of mrs associated with each sentence
m r s   we disambiguate the data by finding pairs  s  m   s  s and m  m r s  such that m  
arg maxm n ist  s  argmaxs p r s  m   
nist measures the precision of a translation in terms of the proportion of n grams it shares with
a human translation  doddington         it has also been used to evaluate nl generation  another
popular mt metric is bleu score  papineni  roukos  ward    zhu         but it is inadequate
for our purpose since we are comparing one short sentence to another instead of comparing whole
documents  bleu score computes the geometric mean of the n gram precision for each value of n 
which means the score is   if a matching n gram is not found for every value of n  in the common
setting in which the maximum n is    any two sentences that do not have a matching   gram would
receive a bleu score of    consequently  bleu score is unable to distinguish the quality of most
of our generated sentences since they are fairly short  in contrast  nist uses an additive score and
avoids this problem 
    experimental evaluation
this section presents experimental results on the robocup data for four systems  k risper  wasper 
k risper wasp  and wasper g en  since we are not aware of any existing systems that could
learn how to semantically parse and generate language using ambiguous supervision based on perceptual context  we constructed our own lower and upper baselines using unmodified wasp  since
   a natural way to use a generation based score would be to use the probability of an nl given a mr  p r s m   
however  initial experiments using this metric did not produce very good results  we also tried changing wasp to
maximize the joint probability instead of just the parsing probability  however  this also did not improve the results 

   

fic hen   k im     m ooney

algorithm   wasper  g en
input sentences s and their associated sets of meaning representations m r s 
output bestexamplesset  a set of nl mr pairs 
semanticmodel   a wasp semantic parser language generator
   main
  
same as algorithm  
   end main
  

function train trainingexamples 
  
same as algorithm  
   end function

  

  

function evaluate s  m  semanticmodel  
generatedsentence  use the wasp language generator in semanticmodel to produce a
sentence from the meaning representation m
   
return the nist score between generatedsentence and s
    end function
  

   

wasp requires unambiguous training data  we randomly pick a meaning for each sentence from its
set of potential mrs to serve as our lower baseline  we use wasp trained on the gold matching
which consists of the correct nlmr pairs annotated by a human as the upper baseline  this represents an upper bound on what our systems could achieve if they disambiguated the training data
perfectly 
we evaluate each system on three tasks  matching  parsing  and generation  the matching task
measures how well the systems can disambiguate the training data  the parsing and generation tasks
measure how well the systems can translate from nl to mr  and from mr to nl  respectively 
since there are four games in total  we trained using all possible combinations of one to three
games  for matching  we measured the performance on the training data since our goal is to disambiguate this data  for parsing and generation  we tested on the games not used for training 
results were averaged over all train test combinations  we evaluated matching and parsing using
f measure  the harmonic mean of recall and precision  precision is the fraction of the systems
annotations that are correct  recall is the fraction of the annotations from the gold standard that
the system correctly produces  generation is evaluated using bleu scores which roughly estimates how well the produced sentences match with the target sentences  we treat each game as a
whole document to avoid the problem of using bleu score for sentence level comparisons mentioned earlier  also  we increase the number of reference sentences for each mr by using all of
the sentences in the test data corresponding to equivalent mrs  e g  if pass pinkplayer  
pinkplayer   occurs multiple times in the test data  all of the sentences matched to this mr in
the gold matchings are used as reference sentences for this mr 
      m atching nl

and

mr

since handling ambiguous training data is an important aspect of grounded language learning  we
first evaluate how well the various systems pick the correct nlmr pairs  figure   shows the fmeasure for identifying the correct set of pairs for the various systems  all of the learning systems
   

fi   

   

    

    

   

   

    

    
f measure

f measure

t raining a m ultilingual s portscaster

   
    

   
    

   

   

    

    

wasper
wasper gen
krisper
random matching

   

wasper
wasper gen
krisper
random matching

   

    

    
 

 
number of training games

 

 

 a  english

 
number of training games

 

 b  korean

   

   

   

   

   

   

f measure

f measure

figure    matching results for our basic systems  wasper  g en performs the best  outperforming
the existing system k risper on both datasets 

   

   
wasp with gold matching
krisper
krisper wasp
wasper
wasper gen
wasp with random matching

   

   

   

   
wasp with gold matching
krisper
krisper wasp
wasper
wasper gen
wasp with random matching

   

   

   

   
 

 
number of training games

 

 

 a  english

 
number of training games

 

 b  korean

figure    semantic parsing results for our basic systems  the results largely mirrors that of the
matching results with wasper  g en performing the best overall 

perform significantly better than random which has a f measure below      in both the english and
korean data  wasper  g en is the best system  wasper also equals or outperforms the previous
system k risper as well 
      s emantic parsing
next  we present results on the accuracy of the learned semantic parsers  each trained system is
used to parse and produce an mr for each sentence in the test set that has a correct mr in the
gold standard matching  a parse is considered correct if and only if it matches the gold standard
exactly  parsing is a fairly difficult task because there is usually more than one way to describe the
same event  for example  player  passes to player  can refer to the same event as player  kicks
the ball to player   thus  accurate parsing requires learning all the different ways people describe
   

fi   

   

    

    

   

   

    

    
bleu

bleu

c hen   k im     m ooney

   
    

   
    

wasp with gold matching
krisper wasp
wasper
wasper gen
wasp with random matching

   
    

wasp with gold matching
krisper wasp
wasper
wasper gen
wasp with random matching

   
    

   

   
 

 
number of training games

 

 

 a  english

 
number of training games

 

 b  korean

figure    tactical generation results for our basic systems  while the relative performances of the
various systems change  wasper  g en is still the best system 

an event  synonymy is not limited to verbs  in our data  pink   pinkg and pink goalie all
refer to player  on the pink team  since we are not providing the systems with any prior knowledge 
they have to learn all these different ways of referring to the same entity 
the parsing results shown in figure   generally correlate well with the matching results  systems that did better at disambiguating the training data also did better on parsing because their
supervised training data is less noisy  wasper g en again does the best overall on both the english and korean data  it is interesting to note that k risper did relatively well on the english data
compared to its matching performance  this is because k risp is more robust to noise than wasp
 kate   mooney        so even though it is trained on a noisier set of data than wasper  g en it
still produced a comparable parser 
      g eneration
the third evaluation task is generation  all of the wasp based systems are given each mr in the test
set that has a gold standard matching nl sentence and asked to generate an nl description  the
quality of the generated sentence is measured by comparing it to the gold standard using bleu
scoring 
this task is more tolerant to noise in the training data than parsing because the system only
needs to learn one way to accurately describe an event  this property is reflected in the results 
shown in figure    where even the baseline system  wasp with random matching  does fairly well 
outperforming k risper wasp on both datasets and wasper on the korean data  as the number
of event types is fairly small  only a relatively small number of correct matchings is required to
perform this task well as long as each event type is associated with some correct sentence pattern
more often than any other sentence pattern 
as with the other two tasks  wasper  g en is the best system on this task  one possible explanation of wasper g ens superior performance stems from its disambiguation objective function 
systems like wasper and k risper wasp that use parsing scores attempt to learn a good translation model for each sentence pattern  on the other hand  wasper g en only tries to learn a good
   

fit raining a m ultilingual s portscaster

translation model for each mr pattern  thus  wasper g en is more likely to converge on a good
model as there are fewer mr patterns than sentence patterns  however  it can be argued that learning
good translation models for each sentence pattern will help in producing more varied commentaries 
a quality that is not captured by the bleu score  another possible advantage for wasper  g en is
that it uses a softer scoring function  while the probabilities of parsing from a particular sentence
to a mr can be sensitive to noise in the training data  wasper  g en only looks at the top generated
sentences for each mr  even with noise in the data  this top generated sentence remains relatively
constant  moreover  minor variations of this sentence do not change the results dramatically since
the nist score allows for partial matching 

   learning for strategic generation
a language generator alone is not enough to produce a sportscast  in addition to tactical generation
which is deciding how to to say something  a sportscaster must also preform strategic generation
which is choosing what to say  mckeown        
we developed a novel method for learning which events to describe  for each event type  i e 
for each predicate like pass  or goal   the system uses the training data to estimate the probability
that it is mentioned by the sportscaster  given the gold standard nlmr matches  this probability
is easy to estimate  however  the learner does not know the correct matching  instead  the system
must estimate the probabilities from the ambiguous training data  we compare two basic methods
for estimating these probabilities 
the first method uses the inferred nlmr matching produced by the language learning system 
the probability of commenting on each event type  ei   is estimated as the percentage of events of
type ei that have been matched to some nl sentence 
the second method  which we call iterative generation strategy learning  igsl   uses a variant of em  treating the matching assignments as hidden variables  initializing each match with a
prior probability  and iterating to improve the probability estimates of commenting on each event
type  unlike the first method  igsl uses information about mrs not explicitly associated with any
sentence in training  algorithm   shows the pseudocode  the main loop alternates between two
steps 
   calculating the expected probability of each nlmr matching given the current model of
how likely an event is commented on  line   
   update the prior probability that an event type is mentioned by a human commentator based
on the matchings  line    
in the first iteration  each nlmr match is assigned
a probability inversely proportional to the
p
amount of ambiguity associated with the sentence   eevent s  pr  e     event s     for example 
a sentence associated with five possible mrs will assign each match a probability of      the prior
probability of mentioning an event type is then estimated as the average probability assigned to
instances of this event type  notice this process does not always guarantee a proper probability since
a mr can be associated with multiple sentences  thus  we limit the probability to be at most one  in
the subsequent iterations  the probabilities of the nlmr matchings are updated according to these
new priors  we assign each match the prior probability of its event type normalized across all the
associated mrs of the nl sentence  we then update the priors for each event type as before using
   

fic hen   k im     m ooney

algorithm   iterative generation strategy learning
input event types e    e         en    the number of occurrences of each event type t otalcount ei  
in the entire game trace  sentences s and the event types of their associated meaning representations event s 
output probabilities of commenting on each event type p r ei  
   initialize all pr  ei      
   repeat
  
for event type ei  e do
  
matchcount    
  
for sentence s  s do
p
pr  e 
i
p
  
probofmatch   eevent s e e
pr  e 
eevent s 

  
  
  
   
   

matchcount   matchcount   probofmatch
end for
matchcount
pr  ei     min  totalcount e
     ensure proper probabilities 
i 
end for
until convergence or max iter reached

       

                                             
             

            

             

           a 

       b c    de   f g 

   d  

    a 

 c      b c    de   f g 

      

  h h 

 c      b c    de   f g 

      
      

ij                                
k                           
    j                                                                 
                      

figure    an example of how our strategic generation component works  at every timestep  we
stochastically select an event from all the events occurring at that moment  we then
decide whether to verbalize the selected event based on igsls estimated probability of
it being commented upon 

the new estimated probabilities for the matchings  this process is repeated until the probabilities
converge or a pre specified number of iterations has occurred 
to generate a sportscast  we use the learned probabilities to determine which events to describe 
for each time step  we first determine all the events that are occurring at the time  we then select one
randomly based on their normalized probabilities  to avoid being overly verbose  we do not want to
make a comment every time something is happening  especially if its an event rarely commented on 
thus  we stochastically decide whether to comment on this selected event based on its probability 
an example of this process is shown in figure   
   

fi   

   

   

   

   

   

f measure

f measure

t raining a m ultilingual s portscaster

   
inferred from gold matching
igsl
inferred from krisper
inferred from wasper
inferred from wasper gen
inferred from random matching

   

   
inferred from gold matching
igsl
inferred from krisper
inferred from wasper
inferred from wasper gen
inferred from random matching

   

   

   
 

 
number of training games

 

 

 a  english

 
number of training games

 

 b  korean

figure    strategic generation results for our various systems  our novel algorithm igsl performs
the best  almost on par with the upper bound which uses gold annotated matchings 

event
ballstopped
kick
pass
turnover
badpass

  occurrences
    
    
    
   
   

  commented
         
      
     
     
     

igsl
         
     
     
     
     

inferred from wasper  g en
     
     
     
     
     

table    top   most frequent events  the   of times they were commented on  and the probabilities
learned by the top algorithms for the english data

    experimental evaluation
the different methods for learning strategic generation were evaluated based on how often the events
they describe in the test data coincide with those the human decided to describe  for the first
approach  results using the inferred matchings produced by k risper  wasper  and wasper g en
as well as the gold and random matching for establishing baselines are all presented in figure   
from the graph  it is clear that igsl outperforms learning from the inferred matchings and actually
performs at a level close to using the gold matching  however  it is important to note that we are
limiting the potential of learning from the gold matching by using only the predicates to decide
whether to talk about an event 
for the english data  the probabilities learned by igsl and by inferred matchings from wasper g en for the five most frequently occurring events are shown in table    while wasper  g en learns
fairly good probabilities in general  it does not do as well as igsl for the most frequent events  this
is because igsl uses occurrences of events that are not associated with any possible comments in
its training iterations  rarely commented events such as ballstopped and kick often occur without
any comments being uttered  consequently  igsl assigns low prior probabilities to them which
lowers their chances of being matched to any sentences  on the other hand  wasper  g en does
not use these priors and sometimes incorrectly matches comments to them  thus  using the inferred
   

fic hen   k im     m ooney

matches from wasper  g en results in learning higher probabilities of commenting on these rarely
commented events 
while all our methods only use the predicates of the mrs to decide whether to comment or not 
they perform quite well on the data we collected  in particular  igsl performs the best  so we use
it for strategic generation in the rest of the paper 

   using strategic generation to improve matching
in this section  we explore how knowledge learned about strategic generation can be used to improve
the accuracy of matching sentences to mrs  in the previous section  we described several ways to
learn strategic generation  including igsl which learns directly from the ambiguous training data 
knowing what events people tend to talk about should also help resolve ambiguities in the training
data  events that are more likely to be discussed should also be more likely to be matched to an
nl sentence when disambiguating the training data  therefore  this section describes methods that
integrate strategic generation scores  such as those in table    into the scoring of nlmr pairs used
in the matching process 
    wasper gen igsl
wasper  g en  igsl is an extension of wasper  g en that also uses strategic generation scores
from igsl  wasper  g en uses nist score to pick the best mr for a sentence by finding the mr
that generates the sentence closet to the actual nl sentence  wasper  g en  igsl combines tactical
 nist  and strategic  igsl  generation scores to pick the best nlmr pairs  it simply multiplies
the nist score and the igsl score together to form a composite score  this new score biases
the selection of matching pairs to include events that igsl determines are  a priori  more likely
to be discussed  this is very helpful  especially in the beginning when wasp does not produce
a particularly good language generator  in many instances  the generated sentences for all of the
possible mrs are equally bad and do not overlap with the target sentence  even if generation
produces a perfectly good sentence  the generation score can be unreliable because it is comparing
a single sentence to a single reference that is often very short as well  consequently  it is often
difficult for wasper g en to distinguish among several mrs with equal scores  on the other hand 
if their event types have very different strategic generation scores  then we can default to choosing
the mr with the higher prior probability of being mentioned  algorithm   shows the pseudocode
of wasper  g en  igsl 
    variant of wasper gen systems
although wasper  g en uses nist score to estimate the goodness of nlmr pairs  it could easily
use any mt evaluation metric  we have already discussed the unsuitability of bleu for comparing short individual sentences since it assigns zero to many pairs  however  nist score also has
limitations  for example  it is not normalized  which may affect the performance of wasper  g en igsl when it is combined with the igsl score  another limitation comes from using higher order
n grams  commentaries in our domain are often short  so there are frequently no higher order
n gram matches between generated sentences and target nl sentences 
the meteor metric  banerjee   lavie        was designed to resolve various weaknesses of
the bleu and nist metrics  and it is more focused on word to word matches between the reference
   

fit raining a m ultilingual s portscaster

algorithm   wasper  g en  igsl
input sentences s and their associated sets of meaning representations m r s 
output bestexamplesset  a set of nl mr pairs 
semanticmodel   a wasp semantic parser language generator
   main
  
same as algorithm  
   end main
  

function train trainingexamples 
  
same as algorithm  
   end function

  

  
  
   
   
   
   
   
   

function evaluate s  m  semanticmodel  
call algorithm   to collect igsl scores
generatedsentence  use the wasp language generator in semanticmodel to produce a
sentence from the meaning representation m
tacticalgenerationscore  the nist score between generatedsentence and s
strategicgenerationscore  pr  event type of m  from the result of algorithm  
return tacticalgenerationscore  strategicgenerationscore
end function

sentence and the test sentence  meteor first evaluates uni gram matches between the reference
and the test sentence and also determines how well the words are ordered  meteor seems more
appropriate for our domain because some good generated sentences have missing adjectives or adverbs that are not critical to the meaning of the sentence but prevent higher order n gram matches 
in addition  meteor is normalized to always be between   and    so it may combine more effectively with igsl scores  which are also in the range     
    experimental evaluation
we evaluated the new systems  wasper g en i gsl with both nist and meteor scoring using
the methodology from section      the matching results are shown in figure    including results
for wasper  g en  the best system from the previous section  both wasper g en igsl with
either nist or meteor scoring clearly outperforms wasper g en  this indicates that strategicgeneration information can help disambiguate the data  using different mt metrics produces a
less noticeable effect  there is no clear winner on the english data  however  meteor seems to
improve performance on the korean data 
parsing results are shown in figure    as previously noted  parsing results generally mirror the
matching results  both new systems again outperform wasper  g en  the previously best system 
and again  the english data does not show a clear advantage of using either nist or meteor 
while the korean data gives a slight edge to using the meteor metric 
results for tactical generation are shown in figure     for both the english and the korean
data  the new systems come close to the performance of wasper g en but do not beat it  however 
the new systems do outperform k risper  wasp and wasper which are not shown in the figure 
   

fi   

   

    

    

   

   

    

    
f measure

f measure

c hen   k im     m ooney

   
    
   

   
    
   

    

    
wasper gen
wasper gen igsl
wasper gen igsl using meteor

   

wasper gen
wasper gen igsl
wasper gen igsl using meteor

   

    

    
 

 
number of training games

 

 

 a  english

 
number of training games

 

 b  korean

   

   

    

    

   

   

    

    
f measure

f measure

figure    matching results  integrating strategic information improves the results over the previously best system wasper  g en  the choice of the mt metric used  however  makes
less of an impact 

   
    

   
    

   

   
wasp with gold matching
wasper gen
wasper gen igsl
wasper gen igsl using meteor

    

wasp with gold matching
wasper gen
wasper gen igsl
wasper gen igsl using meteor

    

   

   
 

 
number of training games

 

 

 a  english

 
number of training games

 

 b  korean

figure    semantic parsing results  the results are similar to the matching results in that integrating
strategic generation information improves the performance 

   

fi   

   

    

    

   

   

    

    
bleu

bleu

t raining a m ultilingual s portscaster

   
    

   
    

   

   
wasp with gold matching
wasper gen
wasper gen igsl
wasper gen igsl using meteor

    

wasp with gold matching
wasper gen
wasper gen igsl
wasper gen igsl using meteor

    

   

   
 

 
number of training games

 

 

 a  english

 
number of training games

 

 b  korean

figure     tactical generation results  while the two new systems come close to the performance
of wasper  g en  they do not beat it  however  they do outperform other systems
presented earlier which are not shown in this figure 

overall  as expected  using strategic information improves performance on the matching and
semantic parsing tasks  for both the english and the korean datasets  wasper g en igsl and
its variant using the meteor metric clearly outperform wasper g en which does not utilize
strategic information  however  strategic information does not improve tactical generation  this
could be due to the ceiling effect in which wasper  g en already performs at a level near the upper
baseline  while the matching performance has improved  the generation performance has little room
to grow 

   using a generative alignment model
recently  liang et al         developed a generative model that can be used to match naturallanguage sentences to facts in a corresponding database to which they may refer  as one of their
evaluation domains  they used our english robocup sportscasting data  their method solves the
matching  alignment  problem for our data  but does not address the tasks of semantic parsing
or language generation  however  their generative model elegantly integrates simple strategic and
tactical language generation models in order to find the overall most probable alignment of sentences
and events  they demonstrated improved matching performance on our english data  generating
more accurate nlmr pairs than our best system  thus  we were curious if their results could be
used to improve our own systems  which also perform semantic parsing and generation  we also
ran their code on our new korean data but that resulted in much worse matching results compared
to our best system as can be seen in table   
the simplest way of utilizing their results is to use the nlmr pairs produced by their method
as supervised data for wasp  as expected  the improved nlmr pairs for the english data resulted
in improved semantic parsers as can be seen in the results in table    even for the korean dataset 
training on matchings produced by their system ended up doing fairly well even though the matching performance was poor  for tactical generation  using their matching only produced marginal
improvement on the english dataset and a surprisingly large improvement on the korean data as
   

fic hen   k im     m ooney

algorithm
liang et al        
wasper
wasper g en
wasper g en i gsl
wasper g en i gsl  m eteor

english dataset
no initialization initialized
    
    
    
    
    
    
    
    
    

korean dataset
no initialization initialized
    
    
    
    
    
    
    
    
    

table    matching results  f  scores  on   fold cross validation for both the english and the korean
datasets  systems run with initialization are initialized with the matchings produced by
liang et al s        system 

algorithm
wasp
wasper
wasper g en
wasper g en i gsl
wasper g en i gsl  m eteor

english dataset
no initialization initialized
n a
    
     
     
     
     
     
     
     
     

korean dataset
no initialization initialized
n a
     
     
     
     
     
     
     
     
     

table    semantic parsing results  f  scores  on   fold cross validation for both the english and
the korean datasets  systems run with initialization are initialized with the matchings
produced by liang et al s        system 

algorithm
wasp
wasper
wasper g en
wasper g en i gsl
wasper g en i gsl  m eteor

english dataset
no initialization initialized
n a
      
      
      
      
      
      
      
      
      

korean dataset
no initialization initialized
n a
      
      
      
      
      
      
      
      
      

table    tactical generation results  bleu score  on   fold cross validation for both the english
and the korean datasets  systems run with initialization are initialized with the matchings
produced by liang et al s        system 

shown in table    overall  using the alignments produced by liang et al s system resulted in good
semantic parsers and tactical generators 
in addition to training wasp with their alignment  we can also utilize their output as a better
starting point for our own systems  instead of initializing our iterative alignment methods with a
model trained on all of the ambiguous nlmr pairs  they can be initialized with the disambiguated
nlmr pairs produced by liang et al s system 
   

fit raining a m ultilingual s portscaster

initializing the systems in this manner almost always improved the performance on all three
tasks  tables       and     moreover  the results from the best systems exceed that of simply training wasp with the alignment in all cases except for semantic parsing on the english data  thus 
combining liang et al s alignment with our disambiguation techniques seems to produce the best
overall results  for the english data  wasper with initialization performs the best on both matching and generation  it does slightly worse on the semantic parsing task compared to wasp trained
on liang et al s alignment  for the korean data  all the systems do better than just training wasp
on the alignment  wasper  g en  i gsl  m eteor with initialization performs the best on matching
and semantic parsing while wasper  g en with initialization performs the best on generation 
overall  initializing our systems with the alignment output of liang et al s generative model
improved performance as expected  starting with a cleaner set of data led to better initial semantic
parsers and language generators which led to better end results  furthermore  by incorporating a
semantic parser and a tactical generator  we were able to improve on the liang et al s alignments
and achieve even better results in most cases 

   removing superfluous comments
so far  we have only discussed how to handle ambiguity in which there are multiple possible mrs
for each nl sentence  during training  all our methods assume that each nl sentence matches
exactly one of the potential mrs  however  some comments are superfluous  in the sense that they
do not refer to any currently extracted event represented in the set of potential mrs  as previously
shown in tables    about one fifth of the english sentences and one tenth of the korean sentences
are superfluous in this sense 
there are many reasons for superfluous sentences  they occur naturally in language because
people do not always talk about the current environment  in our domain  sportscasters often mention
past events or more general information about particular teams or players  moreover  depending on
the application  the chosen mrl may not represent all of the things people talk about  for example 
our robocup mrl cannot represent information about players who are not actively engaged with
the ball  finally  even if a sentence can be represented in the chosen mrl  errors in the perceptual
system or an incorrect estimation of when an event occurred can also lead to superfluous sentences 
such perceptual errors can be alleviated to some degree by increasing the size of the window used
to capture potential mrs  the previous   seconds in our experiments   however  this comes at the
cost of increased ambiguity because it associates more mrs with each sentence 
to deal with the problem of superfluous sentences  we can eliminate the lowest scoring nlmr
pairs  e g  lowest parsing scores for wasper or lowest nist scores for wasper g en   however 
in order to set the pruning threshold  we need to automatically estimate the amount of superfluous
commentary in the absence of supervised data  notice that while this problem looks similar to the
strategic generation problem  estimating how likely an mr participates in a correct matching as
opposed to how likely an nl sentence participates in a correct matching   the approaches used there
cannot be applied  first  we cannot use the matches inferred by the existing systems to estimate the
fraction of superfluous comments since the current systems match every sentence to some mr  it is
also difficult to develop an algorithm similar to igsl due to the imbalance between nl sentences
and mrs  since there are many more mrs  there are more examples of events occurring without
commentaries than vice versa 
   

fic hen   k im     m ooney

    estimating the superfluous rate using internal cross validation
we propose using a form of internal  i e  within the training set  cross validation to estimate the rate
of superfluous comments  while this algorithm can be used in conjunction with any of our systems 
we chose to implement it for k risper which trains much faster than our other systems  this makes
it more tractable to train many different semantic parsers and choose the best one  the basic idea is
to use part of the ambiguous training data to estimate the accuracy of a semantic parser even though
we do not know the correct matchings  assuming a reasonable superfluous sentence rate  we know
that most of the time the correct mr is contained in the set of mrs associated with an nl sentence 
thus  we assume that a semantic parser that parses an nl sentence into one of the mrs associated
with it is better than one that parses it into an mr not in the set  with this approach to estimating
accuracy  we can evaluate semantic parsers learned using various pruning thresholds and pick the
best one  the algorithm is briefly summarized in the following steps 
   split the training set into an internal training set and an internal validation set 
   train k risper n times on the internal training set using n different threshold values  eliminating the lowest scoring nlmr pairs below the threshold in each retraining iteration in
algorithm    
   test the n semantic parsers on the internal validation set and determine which parser is able
to parse the largest number of sentences into one of their potential mrs 
   use the threshold value that produced the best parser in the previous step to train a final parser
on the complete original training set 
    experiments
we evaluated the effect of removing superfluous sentences on all three tasks  matching  parsing 
and generation  we present results for both k risper and k risper  wasp  for matching  we
only show results for k risper because it is responsible for disambiguating the training data for
both systems  so k risper  wasps results are the same   for generation  we only show results for
k risper wasp  since k risper cannot perform generation 
the matching results shown in figure    demonstrate that removing superfluous sentences does
improve the performance for both english and korean  although the difference is small in absolute
terms  the parsing results shown in figure    indicate that removing superfluous sentences usually
improves the accuracy of both k risper and k risper  wasp marginally  as we have observed
many times  the parsing results are consistent with the matching results  finally  the tactical generation results shown in figure    suggest that removing superfluous comments actually decreases
performance somewhat  once again  a potential explanation is that generation is less sensitive to
noisy training data  while removing superfluous comments improves the purity of the training data 
it also removes potentially useful examples  consequently  the system does not learn how to generate sentences that were removed from the data  overall  for generation  the advantage of having
cleaner disambiguated training data is apparently outweighed by the loss of data 
   

fi   

   

    

    

   

   

    

    
f measure

f measure

t raining a m ultilingual s portscaster

   
    

   
    

   

   

    

    

   

   

krisper
krisper with superfluous comment removal

    

krisper
krisper with superfluous comment removal

    
 

 
number of training games

 

 

 a  english

 
number of training games

 

 b  korean

   

   

    

    

   

   

    

    
f measure

f measure

figure     matching results comparing the effects of removing superfluous comments 

   
    

   
    

   

   
krisper
krisper with superfluous comment removal
krisper wasp
krisper wasp with superfluous comment removal

    

krisper
krisper with superfluous comment removal
krisper wasp
krisper wasp with superfluous comment removal

    

   

   
 

 
number of training games

 

 

 a  english

 
number of training games

 

 b  korean

   

   

    

    

   

   

    

    
bleu

bleu

figure     semantic parsing results are improved marginally after superfluous comment removal 

   

   

    

    

   

   

    

    

krisper wasp
krisper wasp with superfluous comment removal

   

krisper wasp
krisper wasp with superfluous comment removal

   
 

 
number of training games

 

 

 a  english

 
number of training games

 

 b  korean

figure     tactical generation performance decreases after removing superfluous comments 

   

fic hen   k im     m ooney

   human subjective evaluation
at best  automatic evaluation of generation is an imperfect approximation of human assessment 
moreover  automatically evaluating the quality of an entire generated sportscast is even more difficult  consequently  we used amazons mechanical turk to collect human judgements of the
produced sportscasts  each human judge was shown three clips of simulated game video in one sitting  there were   video clips total  the   clips use   game segments of   minutes each  one from
each of the four games            robocup finals   each of the   game segments is commentated
once by a human and once by our system  we use igsl to determine the events to comment on and
wasper  g en  our best performing system for tactical generation  to produce the commentaries 
to make the commentaries more varied  we took the top   outputs from wasper  g en and chose
one stochastically weighted by their scores  the system was always trained on three games  leaving out the game from which the test segment was extracted  the video clips were accompanied
by commentaries that both appear as subtitles on the screen as well as audio produced by a automated text to speech system   the videos are shown in random counter balanced order to ensure
no consistent bias toward segments being shown earlier or later  we asked the judges to score the
commentaries using the following metrics 

score
 
 
 
 
 

fluency
flawless
good
non native
disfluent
gibberish

semantic
correctness
always
usually
sometimes
rarely
never

sportscasting
ability
excellent
good
average
bad
terrible

fluency and semantic correctness  or adequacy  are standard metrics in human evaluations of nl
translations and generations  fluency measures how well the commentaries are structured  including
syntax and grammar  semantic correctness indicates whether the commentaries accurately describe
what is happening in the game  finally  sportscasting ability measures the overall quality of the
sportscast  this includes whether the sportscasts are interesting and flow well  in addition to these
metrics  we also asked them whether they thought the sportscast was composed by a human or a
computer  human   
since mechanical turk recruits judges over the internet  we had to make sure that the judges
were not assigning ratings randomly  thus  in addition to asking them to rate each video  we also
asked them to count the number of goals in each video  incorrect responses to this question caused
their ratings to be discarded  this is to ensure that the judges faithfully watched the entire clip before
assigning ratings  after such pruning  there was on average    ratings  from    original ratings 
for each of the   videos for the english data  since it was more difficult to recruit korean judges
over the internet  we recruited them in person and collected   ratings on average for each video
in the korean data  table   and   show the results for the english and korean data  respectively 
statistically significant results are shown in boldface 
results are surprisingly good for the english data across all categories with the machine actually
scoring higher than the human on average  however  the differences are not statistically significant
   sample video clips with sound are available on the web at http   www cs utexas edu users ml 
clamp sportscasting  

   

fit raining a m ultilingual s portscaster

     final
     final
     final
     final
average

commentator
human
machine
human
machine
human
machine
human
machine
human
machine

fluency
    
    
    
    
    
    
    
    
    
    

semantic
correctness
    
    
    
    
    
    
    
    
    
    

sportscasting
ability
    
    
    
    
    
    
    
    
    
    

human 
      
      
      
      
      
      
      
      
      
      

table    human evaluation of overall sportscasts for english data  bold numbers indicate statistical
significance 

     final
     final
     final
     final
average

commentator
human
machine
human
machine
human
machine
human
machine
human
machine

fluency
    
    
    
    
    
    
    
    
    
    

semantic
correctness
    
    
    
    
    
    
    
    
    
    

sportscasting
ability
    
    
    
    
    
    
    
    
    
    

human 
      
      
      
      
      
      
      
      
      
      

table    human evaluation of overall sportscasts for korean data  bold numbers indicate statistical
significance 

   

fic hen   k im     m ooney

based on an unpaired t test  p          nevertheless  it is encouraging to see the machine being
rated so highly  there is some variance in the humans performance since there were two different
commentators  most notably  compared to the machine  the humans performance on the      final
is quite good because his commentary included many details such as the position of the players 
the types of passes  and comments about the overall flow of the game  on the other hand  the
humans performance on the      final is quite bad because the human commentator was very
mechanical and used the same sentence pattern repeatedly  the machine performance was more
even throughout although sometimes it gets lucky  for example  the machine serendipitously said
this is the beginning of an exciting match  near the start of the      final clip simply because this
statement was incorrectly learned to correspond to an extracted mr that is actually unrelated 
the results for korean are not as impressive  the human beats the machine on average for
all categories  however  the largest difference between the scores in any category is only     
moreover  the absolute scores indicate that the generated korean sportscast is at least of acceptable
quality  the judges even mistakenly thought they were produced by humans one third of the time 
part of the reason for the worse performance compared to the english data is that the korean commentaries were fairly detailed and included events that were not extracted by our limited perceptual
system  thus  the machine simply had no way of competing because it is limited to only expressing
information that is present in the extracted mrs 
we also elicited comments from the human judges to get a more qualitative evaluation  overall 
the judges thought the generated commentaries were good and accurately described the actions on
the field  picking from the top   generated sentences also added variability to the machine generated
sportscasts that improved the results compared with earlier experiments presented by chen and
mooney         however  the machine still sometimes misses significant plays such as scoring
or corner kicks  this is because these plays happen much less frequently and often coincide with
many other events  e g  shooting the ball and kickoffs co occur with scoring   thus  the machine
has a harder time learning about these infrequent events  another issue concerns our representation 
many people complain about long gaps in the sportscasts or lack of details  our event detector only
concentrates on ball possession and not on positions or elapsed time  thus  a player holding onto a
ball or dribbling for a long time does not produce any events detected by our simulated perceptual
system  also  a short pass in the backfield is treated exactly the same as a long pass across the
field to near the goal  finally  people desired more colorful commentary  background information 
statistics  or analysis of the game  to fill in the voids  this is a somewhat orthogonal issue since our
goal was to build a play by play commentator that described events that were currently happening 

    related work
in this section we review some of the related work in semantic parsing  natural language generation
as well as grounded language learning 
     semantic parsing
as mentioned in section    existing work on semantic parser learners has focused on supervised
learning where each sentence is annotated with its semantic meaning  some semantic parser learners additionally require either syntactic annotations  ge   mooney        or prior syntactic knowledge of the target language  ge   mooney        zettlemoyer   collins               since the
world never provides any direct feedback on syntactic structure  language learning methods that
   

fit raining a m ultilingual s portscaster

require syntactic annotation are not directly applicable to grounded language learning  therefore 
methods that learn from only semantic annotation are critical to learning language from perceptual
context 
while we use logic formulas as our mrs  the particular mrl we use contains only atomic
formulas and can be equivalently represented as frames and slots  there are systems that use
transformation based learning  jurcicek et al          or markov logic  meza ruiz  riedel   
lemon        to learn semantic parsers using frames and slots  in principle  our framework can
be used with any semantic parser learner as long as it provides confidence scores for its parse results 
     natural language generation
there are several existing systems that sportscast robocup games  andre et al          given
game states provided by the robocup simulator  they extract game events and generate real time
commentaries  they consider many practical issues such as timeliness  coherence  variability  and
emotion that are needed to produce good sportscasts  however  these systems are hand built and
generate language using pre determined templates and rules  in contrast  we concentrate on the
learning problem and induce the generation components from ambiguous training data  nevertheless  augmenting our system with some of the other components in these systems could improve the
final sportscasts produced 
there is also prior work on learning a lexicon of elementary semantic expressions and their corresponding natural language realizations  barzilay   lee         this work uses multiple sequence
alignment on datasets that supply several verbalizations of the corresponding semantics to extract a
dictionary 
duboue and mckeown        were the first to propose an algorithm for learning strategic generation automatically from data  using semantics and associated texts  their system learns a classifier
that determines whether a particular piece of information should be included for presentation or not 
there has been some recent work on learning strategic generation using reinforcement learning
 zaragoza   li         this work involves a game setting where the speaker must aid the listener
in reaching a given destination while avoiding obstacles  the game is played repeatedly to find
an optimal strategy that conveys the most pertinent information while minimizing the number of
messages  we consider a different problem setting where such reinforcements are not available to
our strategic generation learner 
in addition  there has also been work on performing strategic generation as a collective task
 barzilay   lapata         by considering all strategic generation decisions jointly  it captures
dependencies between utterances  this creates more consistent overall output and is more consistent
with how humans perform this task  such an approach could potentially help our system produce
better overall sportscasts 
     grounded language learning
one of the most ambitious end to end visually grounded scene description system is vitra  herzog   wazinski        which comments on traffic scenes and soccer matches  the system first
transforms raw visual data into geometrical representations  next  a set of rules extract spatial relations and interesting motion events from those representations  presumed intentions  plans  and plan
   

fic hen   k im     m ooney

interactions between the agents are also extracted based on domain specific knowledge  however 
since their system is hand coded it cannot be adapted easily to new domains 
srihari and burhans        used captions accompanying photos to help identify people and
objects  they introduced the idea of visual semantics  a theory of extracting visual information and
constraints from accompanying text  for example  by using caption information  the system can
determine the spatial relationship between the entities mentioned  the likely size and shape of the
object of interest  and whether the entity is natural or artificial  however  their system is also based
on hand coded knowledge 
siskind        performed some of the earliest work on learning grounded word meanings  his
learning algorithm addresses the problem of ambiguous training or referential uncertainty for
semantic lexical acquisition  but does not address the larger problems of learning complete semantic
parsers and language generators 
several robotics and computer vision researchers have worked on inferring grounded meanings
of individual words or short referring expressions from visual perceptual context  e g   roy       
bailey et al         barnard et al         yu   ballard         however  the complexity of the
natural language used in this existing work is very restrictive  many of the systems use pre coded
knowledge of the language  and almost all use static images to learn language describing objects and
their relations  and cannot learn language describing actions  the most sophisticated grammatical
formalism used to learn syntax in this work is a finite state hidden markov model  by contrast  our
work exploits the latest techniques in statistical context free grammars and syntax based statistical
machine translation that handle more of the complexities of natural language 
more recently  gold and scassellati        built a system called twig that uses existing language knowledge to help it learn the meaning of new words  the robot uses partial parses to focus
its attention on possible meanings of new words  by playing a game of catch  the robot was able to
learn the meaning of you and me as well as am and are as identity relations 
there has also been a variety of work on learning from captions that accompany pictures or
videos  satoh  nakamura    kanade        berg  berg  edwards    forsyth         this area is of
particular interest given the large amount of captioned images and video available on the web and
television  satoh et al         built a system to detect faces in newscasts  however  they use fairly
simple manually written rules to determine the entity in the picture to which the language refers 
berg et al         used a more elaborate learning method to cluster faces with names  using the
data  they estimate the likelihood of an entity appearing in a picture given its context 
some recent work on video retrieval has focused on learning to recognize events in sports videos
and connecting them to english words appearing in accompanying closed captions  fleischman
  roy        gupta   mooney         however  this work only learns the connection between
individual words and video events and does not learn to describe events using full grammatical
sentences  to avoid difficult problems in computer vision  our work uses a simulated world where
perception of complex events and their participants is much simpler 
in addition to observing events passively  there has also been work on grounded language learning in more interactive environments such as in computer video games  gorniak   roy         in
this work  players cooperate and communicate with each in order to accomplish a certain task  the
system learns to map spoken instructions to specific actions  however  it relies on existing statistical
parsers and does not learn the syntax and semantics of the language from the perceptual environment
alone  kerr  cohen  and chang        developed a system that learns grounded word meanings for
nouns  adjectives  and spatial prepositions while a human is instructing it to perform tasks in a vir   

fit raining a m ultilingual s portscaster

tual world  however  the system assumes an existing syntactic parser and prior knowledge of verb
semantics and is unable to learn these from experience 
recently  there has been some interest in learning how to interpret english instructions describing how to use a particular website or perform other computer tasks  branavan et al         lau 
drews    nichols         these systems learn to predict the correct computer action  pressing a
button  choosing a menu item  typing into a text field  etc   corresponding to each step in the instructions  instead of using parallel training data from the perceptual context  these systems utilize
direct matches between words in the natural language instructions and english words explicitly occurring in the menu items and computer instructions in order to establish a connection between the
language and the environment 
one of the core subproblems our work addresses is matching sentences to facts in the world to
which they refer  some recent projects attempt to align text from english summaries of american
football games with database records that contain statistics and events about the game  snyder
  barzilay        liang et al          however  snyder and barzilay        use a supervised
approach that requires annotating the correct correspondences between the text and the semantic
representations  on the other hand  liang et al         have developed an unsupervised approach
using a generative model to solve the alignment problem  they also demonstrated improved results
on matching sentences and events on our robocup english sportscasting data  however  their work
does not address semantic parsing or language generation  section   presents results showing how
our methods can improve the nlmr matches produced by this approach as well as use them to
learn parsers and generators 

    future work
as previously discussed  some of the limitations of the current system are due to inadequacies in
the perception of events extracted from the robocup simulator  some of the language commentary 
particularly in the korean data  refers to information about events that is not currently represented in
the extracted mrs  for example  a player dribbling the ball is not captured by our perceptual system 
the event extractor could be extended to include such information in the output representations 
commentaries are not always about the immediate actions happening on the field  they can also
refer to statistics about the game  background information  or analysis of the game  while some of
these are difficult to obtain  it would be simple to augment the potential mrs to include events such
as the current score or the number of turnovers  etc  while these may be difficult to learn correctly 
they potentially would make the commentaries much more natural and engaging 
some statements in the commentaries specifically refer to a pattern of activity across several
recent events rather than to a single event  for example  in one of the english commentaries  the
statement purple team is very sloppy today  appears after a series of turn overs to the other team 
the simulated perception could be extended to extract patterns of activity such as sloppiness 
however this assumes that such concepts are predefined  and extracting many such higher level
predicates would greatly increase ambiguity in the training data  the current system assumes it
already has concepts for the words it needs to learn and can perceive these concepts and represent
them in mrs  however  it would be interesting to include a more whorfian style of language
learning  whorf        in which an unknown word such as sloppiness could actually cause the
creation of a new concept  for content words that do not seem to consistently correlate with any
perceived event  the system could collect examples of recent activity where the word is used and try
   

fic hen   k im     m ooney

to learn a new higher level concept that captures a regularity in these situations  for example  given
examples of situations referred to as sloppy  an inductive logic programming system  lavrac  
dzeroski        should be able to detect the pattern of several recent turnovers 
another shortcoming of the current system is that each mr is treated independently  this fails
to exploit the fact that many mrs are related to each other  for example  a pass is preceded by a kick 
and a bad pass is followed by a turnover  a more natural way is to use a graphical representation to
represent not only the entities and events but also the relationships between them 
currently tactical and strategic generation in our system are only loosely coupled  however 
conceptually they are much more closely related  and solving one problem should help solve the
other  initializing our system with the output from liang et al          which uses a generative model
that includes both strategic and tactical components  produced somewhat better results  however 
the interaction between all these components is very loose and a tighter integration of the different
pieces could yield stronger results in all the tasks 
an obvious extension to the current work is to apply it to real robocup games rather than
simulated ones  recent work by rozinat  zickler  veloso  van der aalst  and mcmillen       
analyzes games in the robocup small size league using video from the overhead camera  by
using the symbolic event trace extracted by such a real perceptual system  our methods could be
applied to real world games  using speech recognition to accept spoken language input is another
obvious extension 
we are currently exploring extending our approach to learn to interpret and generate nl instructions for navigating in a virtual environment  the system will observe one person giving english
navigation instructions  e g  go down the hall and turn left after you pass the chair   to another person who follows these directions to get to a chosen destination  by collecting examples of sentences
paired with the actions that were executed together with information about the local environment 
the system will construct an ambiguous supervised dataset for language learning  such an approach
could eventually lead to virtual agents in games and educational simulations that can automatically
learn to interpret and generate natural language instructions 

    conclusion
we have presented an end to end system that learns to generate natural language sportscasts for
simulated robocup soccer games by training on sample human commentaries paired with automatically extracted game events  by learning to semantically interpret and generate language without
explicitly annotated training data  we have demonstrated that a system can learn language by simply
observing linguistic descriptions of ongoing events  we also demonstrated the systems language
independence by successfully training it to produce sportscasts in both english and korean 
dealing with the ambiguous supervision inherent in the training environment is a critical issue in
learning language from perceptual context  we have evaluated various methods for disambiguating
the training data in order to learn semantic parsers and language generators  using a generation
evaluation metric as the criterion for selecting the best nlmr pairs produced better results than
using semantic parsing scores when the initial training data is very noisy  our system also learns
a model for strategic generation from the ambiguous training data by estimating the probability
that each event type evokes human commentary  moreover  using strategic generation information
to help disambiguate the training data was shown to improve the results  we also demonstrated
that our system can be initialized with alignments produced by a different system to achieve better
   

fit raining a m ultilingual s portscaster

results than either system alone  finally  experimental evaluation verified that the overall system
learns to accurately parse and generate comments and to generate sportscasts that are competitive
with those produced by humans 

acknowledgments
we thank adam bossy for his work on simulating perception for the robocup games  we also
thank percy liang for sharing his software and experimental results with us  finally  we thank
the anonymous reviewers of jair and the editor  lillian lee  for their insightful comments which
helped improve the final presentation of this paper  this work was funded by the nsf grant iis
       x  most of the experiments were run on the mastodon cluster  provided by nsf grant
eia         

appendix a  details of the meaning representation language
table    shows brief explanations of the different events we detect with our simulated perception 
event
playmode
ballstopped
turnover
kick
pass
badpass
defense
steal
block

description
signifies the current play mode as defined by the game
the ball speed is below a minimum threshold
the current possessor of the ball and the last possessor are on different teams
a player having possession of the ball in one time interval and not in the next
a player gains possession of the ball from a different player on the same team
a pass in which the player gaining possession of the ball is on a different team
a transfer from one player to an opposing player in their penalty area
a player having possession of the ball in one time interval and another player on
a different team having it in the next time interval
transfer from one player to the opposing goalie 
table     description of the different events detected

below we include the context free grammar we developed for our meaning representation language  all derivations start at the root symbol  s 

 s
 s
 s
 s
 s
 s
 s
 s
 s

  
  
  
  
  
  
  
  
  

playmode    playmode  
ballstopped
turnover    player    player  
kick    player  
pass    player    player  
badpass    player    player  
defense    player    player  
steal    player  
block    player  

   

fic hen   k im     m ooney

 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 playmode
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player
 player

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

kick off l
kick off r
kick in l
kick in r
play on
offside l
offside r
free kick l
free kick r
corner kick l
corner kick r
goal kick l
goal kick r
goal l
goal r

pink 
pink 
pink 
pink 
pink 
pink 
pink 
pink 
pink 
pink  
pink  
purple 
purple 
purple 
purple 
purple 
purple 
purple 
purple 
purple 
purple  
purple  

   

fit raining a m ultilingual s portscaster

references
aho  a  v     ullman  j  d          the theory of parsing  translation  and compiling  prentice
hall  englewood cliffs  nj 
andre  e   binsted  k   tanaka ishii  k   luke  s   herzog  g     rist  t          three robocup
simulation league commentator systems  ai magazine              
bailey  d   feldman  j   narayanan  s     lakoff  g          modeling embodied lexical development  in proceedings of the nineteenth annual conference of the cognitive science society 
banerjee  s     lavie  a          meteor  an automatic metric for mt evaluation with improved
correlation with human judgments  in proceedings of the acl workshop on intrinsic and
extrinsic evaluation measures for machine translation and or summarization  pp      
ann arbor  michigan  association for computational linguistics 
barnard  k   duygulu  p   forsyth  d   de freitas  n   blei  d  m     jordan  m  i          matching
words and pictures  journal of machine learning research              
barzilay  r     lapata  m          collective content selection for concept to text generation  in
proceedings of the human language technology conference and conference on empirical
methods in natural language processing  hlt emnlp     
barzilay  r     lee  l          bootstrapping lexical choice via multiple sequence alignment  in
proceedings of the      conference on empirical methods in natural language processing
 emnlp     
berg  t  l   berg  a  c   edwards  j     forsyth  d  a          whos in the picture  in advances
in neural information processing systems     nips       
branavan  s   chen  h   zettlemoyer  l  s     barzilay  r          reinforcement learning for
mapping instructions to actions  in proceedings of the joint conference of the   th annual
meeting of the association for computational linguistics and the  th international joint
conference on natural language processing of the asian federation of natural language
processin  acl ijcnlp       
brown  p  f   cocke  j   della pietra  s  a   della pietra  v  j   jelinek  f   lafferty  j  d   mercer 
r  l     roossin  p  s          a statistical approach to machine translation  computational
linguistics              
brown  p  f   della pietra  v  j   della pietra  s  a     mercer  r  l          the mathematics
of statistical machine translation  parameter estimation  computational linguistics        
       
bunescu  r  c     mooney  r  j          subsequence kernels for relation extraction  in weiss 
y   scholkopf  b     platt  j   eds    advances in neural information processing systems   
 nips       vancouver  bc 
chen  d  l     mooney  r  j          learning to sportscast  a test of grounded language acquisition  in proceedings of   th international conference on machine learning  icml      
helsinki  finland 
   

fic hen   k im     m ooney

chen  m   foroughi  e   heintz  f   kapetanakis  s   kostiadis  k   kummeneje  j   noda  i   obst 
o   riley  p   steffens  t   wang  y     yin  x          users manual  robocup soccer server
manual for soccer server version      and later   available at http   sourceforge 
net projects sserver  
chiang  d          a hierarchical phrase based model for statistical machine translation  in proceedings of the   nd annual meeting of the association for computational linguistics  acl     pp         ann arbor  mi 
collins  m          new ranking algorithms for parsing and tagging  kernels over discrete structures  and the voted perceptron  in proceedings of the   th annual meeting of the association
for computational linguistics  acl        pp         philadelphia  pa 
dempster  a  p   laird  n  m     rubin  d  b          maximum likelihood from incomplete data
via the em algorithm  journal of the royal statistical society b          
doddington  g          automatic evaluation of machine translation quality using n gram cooccurrence statistics  in proceedings of arpa workshop on human language technology 
pp         san diego  ca 
duboue  p  a     mckeown  k  r          statistical acquisition of content selection rules for
natural language generation  in proceedings of the      conference on empirical methods
in natural language processing  emnlp      pp         
fleischman  m     roy  d          situated models of meaning for sports video retrieval  in proceedings of human language technologies  the conference of the north american chapter
of the association for computational linguistics  naacl hlt     rochester  ny 
ge  r     mooney  r  j          a statistical semantic parser that integrates syntax and semantics  in
proceedings of the ninth conference on computational natural language learning  conll       pp      ann arbor  mi 
ge  r     mooney  r  j          learning a compositional semantic parser using an existing syntactic parser  in proceedings of the joint conference of the   th annual meeting of the association for computational linguistics and the  th international joint conference on natural
language processing of the asian federation of natural language processin  acl ijcnlp
      
gold  k     scassellati  b          a robot that uses existing vocabulary to infer non visual word
meanings from observation  in proceedings of the twenty second conference on artificial
intelligence  aaai     
gorniak  p     roy  d          speaking with your sidekick  understanding situated speech in
computer role playing games  in proceedings of the  th conference on artificial intelligence
and interactive digital entertainment stanford  ca 
gupta  s     mooney  r          using closed captions to train activity recognizers that improve
video retrieval  in proceedings of the cvpr    workshop on visual and contextual learning
from annotated images and videos  vcl  miami  fl 
   

fit raining a m ultilingual s portscaster

harnad  s          the symbol grounding problem  physica d             
herzog  g     wazinski  p          visual translator  linking perceptions and natural language
descriptions  artificial intelligence review                 
ide  n  a     jeronis  j          introduction to the special issue on word sense disambiguation 
the state of the art  computational linguistics             
joachims  t          text categorization with support vector machines  learning with many relevant
features  in proceedings of the tenth european conference on machine learning  ecml     pp         berlin  springer verlag 
jurcicek  j   gasic  m   keizer  s   mairesse  f   thomson  b     young  s          transformationbased learning for semantic parsing  in interspeech brighton  uk 
kate  r  j     mooney  r  j          using string kernels for learning semantic parsers  in proceedings of the   st international conference on computational linguistics and   th annual
meeting of the association for computational linguistics  coling acl      pp        
sydney  australia 
kate  r  j     mooney  r  j          learning language semantics from ambiguous supervision 
in proceedings of the twenty second conference on artificial intelligence  aaai      pp 
       vancouver  canada 
kerr  w   cohen  p  r     chang  y  h          learning and playing in wubble world  in proceedings of the fourth artificial intelligence for interactive digital entertainment conference
 aiide  palo alto  ca 
kingsbury  p   palmer  m     marcus  m          adding semantic annotation to the penn treebank 
in proceedings of the human language technology conference san diego  ca 
knight  k     hatzivassiloglou  v          two level  many paths generation  in proceedings
of the   rd annual meeting of the association for computational linguistics  acl      pp 
       cambridge  ma 
lau  t   drews  c     nichols  j          interpreting written how to instructions  in proceedings
of the twenty first international joint conference on artificial intelligence  ijcai       
lavrac  n     dzeroski  s          inductive logic programming  techniques and applications 
ellis horwood 
liang  p   jordan  m  i     klein  d          learning semantic correspondences with less supervision  in proceedings of the joint conference of the   th annual meeting of the association for
computational linguistics and the  th international joint conference on natural language
processing of the asian federation of natural language processin  acl ijcnlp       
lodhi  h   saunders  c   shawe taylor  j   cristianini  n     watkins  c          text classification
using string kernels  journal of machine learning research            
   

fic hen   k im     m ooney

lu  w   ng  h  t   lee  w  s     zettlemoyer  l  s          a generative model for parsing natural
language to meaning representations  in proceedings of the      conference on empirical
methods in natural language processing  emnlp     honolulu  hi 
marcus  m   santorini  b     marcinkiewicz  m  a          building a large annotated corpus of
english  the penn treebank  computational linguistics                
mckeown  k  r          discourse strategies for generating natural language text  artificial intelligence             
meza ruiz  i  v   riedel  s     lemon  o          spoken language understanding in dialogue
systems  using a   layer markov logic network  improving semantic accuracy  in proceedings
of londial 
mooney  r  j          learning for semantic parsing  in gelbukh  a   ed    computational linguistics and intelligent text processing  proceedings of the  th international conference 
cicling       mexico city  pp          springer verlag  berlin 
och  f  j     ney  h          a systematic comparison of various statistical alignment models 
computational linguistics              
papineni  k   roukos  s   ward  t     zhu  w  j          bleu  a method for automatic evaluation
of machine translation  in proceedings of the   th annual meeting of the association for
computational linguistics  acl        pp         philadelphia  pa 
riezler  s   prescher  d   kuhn  j     johnson  m          lexicalized stochastic modeling of
constraint based grammars using log linear measures and em training  in proceedings of
the   th annual meeting of the association for computational linguistics  acl        pp 
       hong kong 
roy  d          learning visually grounded words and syntax for a scene description task  computer speech and language                
rozinat  a   zickler  s   veloso  m   van der aalst  w     mcmillen  c          analyzing multiagent activity logs using process mining techniques  in proceedings of the  th international
symposium on distributed autonomous robotic systems  dars     tsukuba  japan 
satoh  s   nakamura  y     kanade  t          name it  naming and detecting faces in video by
the integration of image and natural language processing  in proceedings of the fifteenth
international joint conference on artificial intelligence  ijcai     
shawe taylor  j     cristianini  n          kernel methods for pattern analysis  cambridge university press 
shieber  s  m          a uniform architecture for parsing and generation  in proceedings of the
  th international conference on computational linguistics  coling      pp         budapest  hungary 
siskind  j  m          a computational study of cross situational techniques for learning word tomeaning mappings  cognition              
   

fit raining a m ultilingual s portscaster

snyder  b     barzilay  r          database text alignment via structured multilabel classification  in proceedings of the twentieth international joint conference on artificial intelligence
 ijcai       
srihari  r  k     burhans  d  t          visual semantics  extracting visual information from
text accompanying pictures  in proceedings of the twelfth national conference on artificial
intelligence  aaai     
stolcke  a          an efficient probabilistic context free parsing algorithm that computes prefix
probabilities  computational linguistics                
whorf  b  l          language  thought  and reality  selected writings  mit press 
wong  y     mooney  r  j          learning for semantic parsing with statistical machine translation  in proceedings of human language technology conference   north american chapter
of the association for computational linguistics annual meeting  hlt naacl      pp     
    new york city  ny 
wong  y     mooney  r  j          generation by inverting a semantic parser that uses statistical
machine translation  in proceedings of human language technologies  the conference of
the north american chapter of the association for computational linguistics  naacl hlt     pp         rochester  ny 
yamada  k     knight  k          a syntax based statistical translation model  in proceedings of
the   th annual meeting of the association for computational linguistics  acl        pp 
       toulouse  france 
yu  c     ballard  d  h          on the integration of grounding language and learning objects  in
proceedings of the nineteenth national conference on artificial intelligence  aaai      pp 
       
zaragoza  h     li  c  h          learning what to talk about in descriptive games  in proceedings
of the human language technology conference and conference on empirical methods in
natural language processing  hlt emnlp      pp         vancouver  canada 
zelenko  d   aone  c     richardella  a          kernel methods for relation extraction  journal
of machine learning research              
zettlemoyer  l  s     collins  m          learning to map sentences to logical form  structured
classification with probabilistic categorial grammars  in proceedings of   st conference on
uncertainty in artificial intelligence  uai       edinburgh  scotland 
zettlemoyer  l  s     collins  m          online learning of relaxed ccg grammars for parsing to
logical form  in proceedings of the      joint conference on empirical methods in natural
language processing and computational natural language learning  emnlp conll     
pp         prague  czech republic 

   

fi