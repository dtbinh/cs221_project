journal of artificial intelligence research                  

submitted        published      

from frequency to meaning 
vector space models of semantics
peter d  turney

peter turney nrc cnrc gc ca

national research council canada
ottawa  ontario  canada  k a  r 

patrick pantel

me patrickpantel com

yahoo  labs
sunnyvale  ca         usa

abstract
computers understand very little of the meaning of human language  this profoundly
limits our ability to give instructions to computers  the ability of computers to explain
their actions to us  and the ability of computers to analyse and process text  vector space
models  vsms  of semantics are beginning to address these limits  this paper surveys the
use of vsms for semantic processing of text  we organize the literature on vsms according
to the structure of the matrix in a vsm  there are currently three broad classes of vsms 
based on termdocument  wordcontext  and pairpattern matrices  yielding three classes
of applications  we survey a broad range of applications in these three categories and we
take a detailed look at a specific open source project in each category  our goal in this
survey is to show the breadth of applications of vsms for semantics  to provide a new
perspective on vsms for those who are already familiar with the area  and to provide
pointers into the literature for those who are less familiar with the field 

   introduction
one of the biggest obstacles to making full use of the power of computers is that they
currently understand very little of the meaning of human language  recent progress in
search engine technology is only scratching the surface of human language  and yet the
impact on society and the economy is already immense  this hints at the transformative
impact that deeper semantic technologies will have  vector space models  vsms   surveyed
in this paper  are likely to be a part of these new semantic technologies 
in this paper  we use the term semantics in a general sense  as the meaning of a word  a
phrase  a sentence  or any text in human language  and the study of such meaning  we are
not concerned with narrower senses of semantics  such as the semantic web or approaches to
semantics based on formal logic  we present a survey of vsms and their relation with the
distributional hypothesis as an approach to representing some aspects of natural language
semantics 
the vsm was developed for the smart information retrieval system  salton       
by gerard salton and his colleagues  salton  wong    yang         smart pioneered
many of the concepts that are used in modern search engines  manning  raghavan   
schutze         the idea of the vsm is to represent each document in a collection as a
point in a space  a vector in a vector space   points that are close together in this space
are semantically similar and points that are far apart are semantically distant  the users
c
    
ai access foundation and national research council canada  reprinted with permission 

fiturney   pantel

query is represented as a point in the same space as the documents  the query is a pseudodocument   the documents are sorted in order of increasing distance  decreasing semantic
similarity  from the query and then presented to the user 
the success of the vsm for information retrieval has inspired researchers to extend the
vsm to other semantic tasks in natural language processing  with impressive results  for
instance  rapp        used a vector based representation of word meaning to achieve a
score of       on multiple choice synonym questions from the test of english as a foreign
language  toefl   whereas the average human score was         turney        used a
vector based representation of semantic relations to attain a score of     on multiple choice
analogy questions from the sat college entrance test  compared to an average human score
of      
in this survey  we have organized past work with vsms according to the type of matrix
involved  termdocument  wordcontext  and pairpattern  we believe that the choice of
a particular matrix type is more fundamental than other choices  such as the particular
linguistic processing or mathematical processing  although these three matrix types cover
most of the work  there is no reason to believe that these three types exhaust the possibilities 
we expect future work will introduce new types of matrices and higher order tensors  
    motivation for vector space models of semantics
vsms have several attractive properties  vsms extract knowledge automatically from a
given corpus  thus they require much less labour than other approaches to semantics  such
as hand coded knowledge bases and ontologies  for example  the main resource used in
rapps        vsm system for measuring word similarity is the british national corpus
 bnc    whereas the main resource used in non vsm systems for measuring word similarity
 hirst   st onge        leacock   chodrow        jarmasz   szpakowicz        is a
lexicon  such as wordnet  or rogets thesaurus  gathering a corpus for a new language
is generally much easier than building a lexicon  and building a lexicon often involves also
gathering a corpus  such as semcor for wordnet  miller  leacock  tengi    bunker        
vsms perform well on tasks that involve measuring the similarity of meaning between
words  phrases  and documents  most search engines use vsms to measure the similarity
between a query and a document  manning et al          the leading algorithms for measuring semantic relatedness use vsms  pantel   lin      a  rapp        turney  littman 
bigham    shnayder         the leading algorithms for measuring the similarity of semantic relations also use vsms  lin   pantel        turney        nakov   hearst        
 section     discusses the differences between these types of similarity  
we find vsms especially interesting due to their relation with the distributional hypothesis and related hypotheses  see section       the distributional hypothesis is that
   regarding the average score of       on the toefl questions  landauer and dumais        note
that  although we do not know how such a performance would compare  for example  with u s  school
children of a particular age  we have been told that the average score is adequate for admission to many
universities 
   this is the average score for highschool students in their senior year  applying to us universities  for
more discussion of this score  see section     in turneys        paper 
   a vector is a first order tensor and a matrix is a second order tensor  see section     
   see http   www natcorp ox ac uk  
   see http   wordnet princeton edu  

   

fifrom frequency to meaning

words that occur in similar contexts tend to have similar meanings  wittgenstein       
harris        weaver        firth        deerwester  dumais  landauer  furnas    harshman         efforts to apply this abstract hypothesis to concrete algorithms for measuring
the similarity of meaning often lead to vectors  matrices  and higher order tensors  this
intimate connection between the distributional hypothesis and vsms is a strong motivation
for taking a close look at vsms 
not all uses of vectors and matrices count as vector space models  for the purposes of
this survey  we take it as a defining property of vsms that the values of the elements in a
vsm must be derived from event frequencies  such as the number of times that a given word
appears in a given context  see section       for example  often a lexicon or a knowledge
base may be viewed as a graph  and a graph may be represented using an adjacency matrix 
but this does not imply that a lexicon is a vsm  because  in general  the values of the
elements in an adjacency matrix are not derived from event frequencies  this emphasis
on event frequencies brings unity to the variety of vsms and explicitly connects them to
the distributional hypothesis  furthermore  it avoids triviality by excluding many possible
matrix representations 
    vectors in ai and cognitive science
vectors are common in ai and cognitive science  they were common before the vsm was
introduced by salton et al          the novelty of the vsm was to use frequencies in a
corpus of text as a clue for discovering semantic information 
in machine learning  a typical problem is to learn to classify or cluster a set of items
 i e   examples  cases  individuals  entities  represented as feature vectors  mitchell       
witten   frank         in general  the features are not derived from event frequencies 
although this is possible  see section       for example  a machine learning algorithm can
be applied to classifying or clustering documents  sebastiani        
collaborative filtering and recommender systems also use vectors  resnick  iacovou 
suchak  bergstrom    riedl        breese  heckerman    kadie        linden  smith   
york         in a typical recommender system  we have a person item matrix  in which
the rows correspond to people  customers  consumers   the columns correspond to items
 products  purchases   and the value of an element is the rating  poor  fair  excellent  that
the person has given to the item  many of the mathematical techniques that work well
with termdocument matrices  see section    also work well with person item matrices  but
ratings are not derived from event frequencies 
in cognitive science  prototype theory often makes use of vectors  the basic idea of
prototype theory is that some members of a category are more central than others  rosch
  lloyd        lakoff         for example  robin is a central  prototypical  member of
the category bird  whereas penguin is more peripheral  concepts have varying degrees of
membership in categories  graded categorization   a natural way to formalize this is to
represent concepts as vectors and categories as sets of vectors  nosofsky        smith  osherson  rips    keane         however  these vectors are usually based on numerical scores
that are elicited by questioning human subjects  they are not based on event frequencies 
another area of psychology that makes extensive use of vectors is psychometrics  which
studies the measurement of psychological abilities and traits  the usual instrument for
   

fiturney   pantel

measurement is a test or questionnaire  such as a personality test  the results of a test
are typically represented as a subject item matrix  in which the rows represent the subjects
 people  in an experiment and the columns represent the items  questions  in the test
 questionnaire   the value of an element in the matrix is the answer that the corresponding
subject gave for the corresponding item  many techniques for vector analysis  such as factor
analysis  spearman         were pioneered in psychometrics 
in cognitive science  latent semantic analysis  lsa   deerwester et al         landauer   dumais         hyperspace analogue to language  hal   lund  burgess   
atchley        lund   burgess         and related research  landauer  mcnamara  dennis    kintsch        is entirely within the scope of vsms  as defined above  since this
research uses vector space models in which the values of the elements are derived from
event frequencies  such as the number of times that a given word appears in a given context  cognitive scientists have argued that there are empirical and theoretical reasons for
believing that vsms  such as lsa and hal  are plausible models of some aspects of human cognition  landauer et al          in ai  computational linguistics  and information
retrieval  such plausibility is not essential  but it may be seen as a sign that vsms are a
promising area for further research 
    motivation for this survey
this paper is a survey of vector space models of semantics  there is currently no comprehensive  up to date survey of this field  as we show in the survey  vector space models
are a highly successful approach to semantics  with a wide range of potential and actual
applications  there has been much recent growth in research in this area 
this paper should be of interest to all ai researchers who work with natural language 
especially researchers who are interested in semantics  the survey will serve as a general
introduction to this area and it will provide a framework  a unified perspective  for
organizing the diverse literature on the topic  it should encourage new research in the area 
by pointing out open problems and areas for further exploration 
this survey makes the following contributions 
new framework  we provide a new framework for organizing the literature  term
document  wordcontext  and pairpattern matrices  see section     this framework shows
the importance of the structure of the matrix  the choice of rows and columns  in determining the potential applications and may inspire researchers to explore new structures
 different kinds of rows and columns  or higher order tensors instead of matrices  
new developments  we draw attention to pairpattern matrices  the use of pair
pattern matrices is relatively new and deserves more study  these matrices address some
criticisms that have been directed at wordcontext matrices  regarding lack of sensitivity
to word order 
breadth of approaches and applications  there is no existing survey that shows
the breadth of potential and actual applications of vsms for semantics  existing summaries
omit pairpattern matrices  landauer et al         
focus on nlp and cl  our focus in this survey is on systems that perform practical
tasks in natural language processing and computational linguistics  existing overviews focus
on cognitive psychology  landauer et al         
   

fifrom frequency to meaning

success stories  we draw attention to the fact that vsms are arguably the most
successful approach to semantics  so far 
    intended readership
our goal in writing this paper has been to survey the state of the art in vector space models
of semantics  to introduce the topic to those who are new to the area  and to give a new
perspective to those who are already familiar with the area 
we assume our reader has a basic understanding of vectors  matrices  and linear algebra 
such as one might acquire from an introductory undergraduate course in linear algebra  or
from a text book  golub   van loan         the basic concepts of vectors and matrices
are more important here than the mathematical details  widdows        gives a gentle
introduction to vectors from the perspective of semantics 
we also assume our reader has some familiarity with computational linguistics or information retrieval  manning et al         provide a good introduction to information retrieval 
for computational linguistics  we recommend manning and schutzes        text 
if our reader is familiar with linear algebra and computational linguistics  this survey
should present no barriers to understanding  beyond this background  it is not necessary
to be familiar with vsms as they are used in information retrieval  natural language processing  and computational linguistics  however  if the reader would like to do some further
background reading  we recommend landauer et al s        collection 
    highlights and outline
this article is structured as follows  section   explains our framework for organizing the
literature on vsms according to the type of matrix involved  termdocument  wordcontext 
and pairpattern  in this section  we present an overview of vsms  without getting into
the details of how a matrix can be generated from a corpus of raw text 
after the high level framework is in place  sections   and   examine the steps involved
in generating a matrix  section   discusses linguistic processing and section   reviews
mathematical processing  this is the order in which a corpus would be processed in most
vsm systems  first linguistic processing  then mathematical processing  
when vsms are used for semantics  the input to the model is usually plain text  some
vsms work directly with the raw text  but most first apply some linguistic processing to the
text  such as stemming  part of speech tagging  word sense tagging  or parsing  section  
looks at some of these linguistic tools for semantic vsms 
in a simple vsm  such as a simple termdocument vsm  the value of an element in a
document vector is the number of times that the corresponding word occurs in the given
document  but most vsms apply some mathematical processing to the raw frequency values 
section   presents the main mathematical operations  weighting the elements  smoothing
the matrix  and comparing the vectors  this section also describes optimization strategies
for comparing the vectors  such as distributed sparse matrix multiplication and randomized
techniques 
by the end of section    the reader will have a general view of the concepts involved in
vector space models of semantics  we then take a detailed look at three vsm systems in
section    as a representative of termdocument vsms  we present the lucene information
   

fiturney   pantel

retrieval library   for wordcontext vsms  we explore the semantic vectors package  which
builds on lucene   as the representative of pairpattern vsms  we review the latent
relational analysis module in the s space package  which also builds on lucene   the
source code for all three of these systems is available under open source licensing 
we turn to a broad survey of applications for semantic vsms in section    this section also serves as a short historical view of research with semantic vsms  beginning with
information retrieval in section      our purpose here is to give the reader an idea of the
breadth of applications for vsms and also to provide pointers into the literature  if the
reader wishes to examine any of these applications in detail 
in a termdocument matrix  rows correspond to terms and columns correspond to documents  section       a document provides a context for understanding the term  if we
generalize the idea of documents to chunks of text of arbitrary size  phrases  sentences 
paragraphs  chapters  books  collections   the result is the wordcontext matrix  which includes the termdocument matrix as a special case  section     discusses applications for
wordcontext matrices  section     considers pairpattern matrices  in which the rows correspond to pairs of terms and the columns correspond to the patterns in which the pairs
occur 
in section    we discuss alternatives to vsms for semantics  section   considers the
future of vsms  raising some questions about their power and their limitations  we conclude
in section   

   vector space models of semantics
the theme that unites the various forms of vsms that we discuss in this paper can be
stated as the statistical semantics hypothesis  statistical patterns of human word usage can
be used to figure out what people mean   this general hypothesis underlies several more
specific hypotheses  such as the bag of words hypothesis  the distributional hypothesis  the
extended distributional hypothesis  and the latent relation hypothesis  discussed below 
    similarity of documents  the termdocument matrix
in this paper  we use the following notational conventions  matrices are denoted by bold
capital letters  a  vectors are denoted by bold lowercase letters  b  scalars are represented
by lowercase italic letters  c 
if we have a large collection of documents  and hence a large number of document
vectors  it is convenient to organize the vectors into a matrix  the row vectors of the matrix
correspond to terms  usually terms are words  but we will discuss some other possibilities 
  
  
  
  

see http   lucene apache org java docs  
see http   code google com p semanticvectors  
see http   code google com p airhead research wiki latentrelationalanalysis 
this phrase was taken from the faculty profile of george furnas at the university of michigan 
http   www si umich edu people faculty detail htm sid     the full quote is  statistical semantics
 studies of how the statistical patterns of human word usage can be used to figure out what people
mean  at least to a level sufficient for information access  the term statistical semantics appeared in
the work of furnas  landauer  gomez  and dumais         but it was not defined there 

   

fifrom frequency to meaning

and the column vectors correspond to documents  web pages  for example   this kind of
matrix is called a termdocument matrix 
in mathematics  a bag  also called a multiset  is like a set  except that duplicates are
allowed  for example   a  a  b  c  c  c  is a bag containing a  b  and c  order does not matter
in bags and sets  the bags  a  a  b  c  c  c  and  c  a  c  b  a  c  are equivalent  we can represent
the bag  a  a  b  c  c  c  with the vector x   h       i  by stipulating that the first element of
x is the frequency of a in the bag  the second element is the frequency of b in the bag  and
the third element is the frequency of c  a set of bags can be represented as a matrix x  in
which each column x j corresponds to a bag  each row xi  corresponds to a unique member 
and an element xij is the frequency of the i th member in the j th bag 
in a termdocument matrix  a document vector represents the corresponding document
as a bag of words  in information retrieval  the bag of words hypothesis is that we can
estimate the relevance of documents to a query by representing the documents and the
query as bags of words  that is  the frequencies of words in a document tend to indicate
the relevance of the document to a query  the bag of words hypothesis is the basis for
applying the vsm to information retrieval  salton et al          the hypothesis expresses
the belief that a column vector in a termdocument matrix captures  to some degree  an
aspect of the meaning of the corresponding document  what the document is about 
let x be a termdocument matrix  suppose our document collection contains n documents and m unique terms  the matrix x will then have m rows  one row for each unique
term in the vocabulary  and n columns  one column for each document   let wi be the i th
term in the vocabulary and let dj be the j th document in the collection  the i th row in
x is the row vector xi  and the j th column in x is the column vector x j   the row vector
xi  contains n elements  one element for each document  and the column vector x j contains
m elements  one element for each term  suppose x is a simple matrix of frequencies  the
element xij in x is the frequency of the i th term wi in the j th document dj  
in general  the value of most of the elements in x will be zero  the matrix is sparse  
since most documents will use only a small fraction of the whole vocabulary  if we randomly
choose a term wi and a document dj   its likely that wi does not occur anywhere in dj   and
therefore xij equals   
the pattern of numbers in xi  is a kind of signature of the i th term wi   likewise  the
pattern of numbers in x j is a signature of the j th document dj   that is  the pattern of
numbers tells us  to some degree  what the term or document is about 
the vector x j may seem to be a rather crude representation of the document dj   it tells
us how frequently the words appear in the document  but the sequential order of the words
is lost  the vector does not attempt to capture the structure in the phrases  sentences 
paragraphs  and chapters of the document  however  in spite of this crudeness  search
engines work surprisingly well  vectors seem to capture an important aspect of semantics 
the vsm of salton et al         was arguably the first practical  useful algorithm for
extracting semantic information from word usage  an intuitive justification for the term
document matrix is that the topic of a document will probabilistically influence the authors
choice of words when writing the document    if two documents have similar topics  then
the two corresponding column vectors will tend to have similar patterns of numbers 
    newer generative models  such as latent dirichlet allocation  lda   blei  ng    jordan         directly
model this intuition  see sections     and   

   

fiturney   pantel

    similarity of words  the wordcontext matrix
salton et al         focused on measuring document similarity  treating a query to a search
engine as a pseudo document  the relevance of a document to a query is given by the
similarity of their vectors  deerwester et al         observed that we can shift the focus to
measuring word similarity  instead of document similarity  by looking at row vectors in the
termdocument matrix  instead of column vectors 
deerwester et al         were inspired by the termdocument matrix of salton et al         
but a document is not necessarily the optimal length of text for measuring word similarity 
in general  we may have a wordcontext matrix  in which the context is given by words 
phrases  sentences  paragraphs  chapters  documents  or more exotic possibilities  such as
sequences of characters or patterns 
the distributional hypothesis in linguistics is that words that occur in similar contexts
tend to have similar meanings  harris         this hypothesis is the justification for applying the vsm to measuring word similarity  a word may be represented by a vector
in which the elements are derived from the occurrences of the word in various contexts 
such as windows of words  lund   burgess         grammatical dependencies  lin       
pado   lapata         and richer contexts consisting of dependency links and selectional
preferences on the argument positions  erk   pado         see sahlgrens        thesis for
a comprehensive study of various contexts  similar row vectors in the wordcontext matrix
indicate similar word meanings 
the idea that word usage can reveal semantics was implicit in some of the things that
wittgenstein        said about language games and family resemblance  wittgenstein was
primarily interested in the physical activities that form the context of word usage  e g   the
word brick  spoken in the context of the physical activity of building a house   but the main
context for a word is often other words   
weaver        argued that word sense disambiguation for machine translation should
be based on the co occurrence frequency of the context words near a given target word  the
word that we want to disambiguate   firth        p      said  you shall know a word
by the company it keeps  deerwester et al         showed how the intuitions of wittgenstein         harris         weaver  and firth could be used in a practical algorithm 
    similarity of relations  the pairpattern matrix
in a pairpattern matrix  row vectors correspond to pairs of words  such as mason   stone
and carpenter   wood  and column vectors correspond to the patterns in which the pairs cooccur  such as x cuts y  and x works with y   lin and pantel        introduced the
pairpattern matrix for the purpose of measuring the semantic similarity of patterns  that
is  the similarity of column vectors  given a pattern such as x solves y   their algorithm
was able to find similar patterns  such as y is solved by x  y is resolved in x  and
x resolves y  
lin and pantel        proposed the extended distributional hypothesis  that patterns
that co occur with similar pairs tend to have similar meanings  the patterns x solves y 
    wittgensteins intuition might be better captured by a matrix that combines words with other modalities 
such as images  monay   gatica perez         if the values of the elements are derived from event
frequencies  we would include this as a vsm approach to semantics 

   

fifrom frequency to meaning

and y is solved by x tend to co occur with similar x   y pairs  which suggests that these
patterns have similar meanings  pattern similarity can be used to infer that one sentence
is a paraphrase of another  lin   pantel        
turney et al         introduced the use of the pairpattern matrix for measuring the
semantic similarity of relations between word pairs  that is  the similarity of row vectors 
for example  the pairs mason   stone  carpenter   wood  potter   clay  and glassblower   glass
share the semantic relation artisan   material  in each case  the first member of the pair is
an artisan who makes artifacts from the material that is the second member of the pair 
the pairs tend to co occur in similar patterns  such as the x used the y to and the x
shaped the y into 
the latent relation hypothesis is that pairs of words that co occur in similar patterns
tend to have similar semantic relations  turney      a   word pairs with similar row
vectors in a pairpattern matrix tend to have similar semantic relations  this is the inverse
of the extended distributional hypothesis  that patterns with similar column vectors in the
pairpattern matrix tend to have similar meanings 
    similarities
pairpattern matrices are suited to measuring the similarity of semantic relations between
pairs of words  that is  relational similarity  in contrast  wordcontext matrices are suited
to measuring attributional similarity  the distinction between attributional and relational
similarity has been explored in depth by gentner        
the attributional similarity between two words a and b  sima  a  b      depends on the
degree of correspondence between the properties of a and b  the more correspondence there
is  the greater their attributional similarity  the relational similarity between two pairs of
words a   b and c   d  simr  a   b  c   d      depends on the degree of correspondence between
the relations of a   b and c   d  the more correspondence there is  the greater their relational
similarity  for example  dog and wolf have a relatively high degree of attributional similarity  whereas dog   bark and cat   meow have a relatively high degree of relational similarity
 turney        
it is tempting to suppose that relational similarity can be reduced to attributional
similarity  for example  mason and carpenter are similar words and stone and wood are
similar words  therefore  perhaps it follows that mason   stone and carpenter   wood have
similar relations  perhaps simr  a   b  c   d  can be reduced to sima  a  c    sima  b  d   however 
mason  carpenter  potter  and glassblower are similar words  they are all artisans   as are
wood  clay  stone  and glass  they are all materials used by artisans   but we cannot infer
from this that mason   glass and carpenter   clay have similar relations  turney            a 
presented experimental evidence that relational similarity does not reduce to attributional
similarity 
the term semantic relatedness in computational linguistics  budanitsky   hirst       
corresponds to attributional similarity in cognitive science  gentner         two words
are semantically related if they have any kind of semantic relation  budanitsky   hirst 
       they are semantically related to the degree that they share attributes  turney        
examples are synonyms  bank and trust company   meronyms  car and wheel   antonyms
 hot and cold   and words that are functionally related or frequently associated  pencil and
   

fiturney   pantel

paper   we might not usually think that antonyms are similar  but antonyms have a high
degree of attributional similarity  hot and cold are kinds of temperature  black and white
are kinds of colour  loud and quiet are kinds of sound   we prefer the term attributional
similarity to the term semantic relatedness  because attributional similarity emphasizes the
contrast with relational similarity  whereas semantic relatedness could be confused with
relational similarity 
in computational linguistics  the term semantic similarity is applied to words that share
a hypernym  car and bicycle are semantically similar  because they share the hypernym
vehicle   resnik         semantic similarity is a specific type of attributional similarity  we
prefer the term taxonomical similarity to the term semantic similarity  because the term
semantic similarity is misleading  intuitively  both attributional and relational similarity
involve meaning  so both deserve to be called semantic similarity 
words are semantically associated if they tend to co occur frequently  e g   bee and
honey   chiarello  burgess  richards    pollock         words may be taxonomically similar and semantically associated  doctor and nurse   taxonomically similar but not semantically associated  horse and platypus   semantically associated but not taxonomically similar
 cradle and baby   or neither semantically associated nor taxonomically similar  calculus and
candy  
schutze and pedersen        defined two ways that words can be distributed in a corpus of text  if two words tend to be neighbours of each other  then they are syntagmatic
associates  if two words have similar neighbours  then they are paradigmatic parallels  syntagmatic associates are often different parts of speech  whereas paradigmatic parallels are
usually the same part of speech  syntagmatic associates tend to be semantically associated  bee and honey are often neighbours   paradigmatic parallels tend to be taxonomically
similar  doctor and nurse have similar neighbours  
    other semantic vsms
the possibilities are not exhausted by termdocument  wordcontext  and pairpattern
matrices  we might want to consider triplepattern matrices  for measuring the semantic
similarity between word triples  whereas a pairpattern matrix might have a row mason  
stone and a column x works with y   a triplepattern matrix could have a row mason  
stone   masonry and a column x uses y to build z  however  n tuples of words grow
increasingly rare as n increases  for example  phrases that contain mason  stone  and
masonry together are less frequent than phrases that contain mason and stone together  a
triplepattern matrix will be much more sparse than a pairpattern matrix  ceteris paribus  
the quantity of text that we need  in order to have enough numbers to make our matrices
useful  grows rapidly as n increases  it may be better to break n tuples into pairs  for
example  a   b   c could be decomposed into a   b  a   c  and b   c  turney      a   the similarity
of two triples  a   b   c and d   e   f   could be estimated by the similarity of their corresponding
pairs  a relatively dense pairpattern matrix could serve as a surrogate for a relatively
sparse triplepattern matrix 
we may also go beyond matrices  the generalization of a matrix is a tensor  kolda
  bader        acar   yener         a scalar  a single number  is zeroth order tensor  a
vector is first order tensor  and a matrix is a second order tensor  a tensor of order three or
   

fifrom frequency to meaning

higher is called a higher order tensor  chew  bader  kolda  and abdelali        use a term
documentlanguage third order tensor for multilingual information retrieval  turney       
uses a wordwordpattern tensor to measure similarity of words  van de cruys        uses
a verbsubjectobject tensor to learn selectional preferences of verbs 
in turneys        tensor  for example  rows correspond to words from the toefl
multiple choice synonym questions  columns correspond to words from basic english  ogden           and tubes correspond to patterns that join rows and columns  hence we have
a wordwordpattern third order tensor   a given word from the toefl questions is represented by the corresponding wordpattern matrix slice in the tensor  the elements in
this slice correspond to all the patterns that relate the given toefl word to any word
in basic english  the similarity of two toefl words is calculated by comparing the two
corresponding matrix slices  the algorithm achieves       on the toefl questions 
    types and tokens
a token is a single instance of a symbol  whereas a type is a general class of tokens  manning
et al          consider the following example  from samuel beckett  

ever tried  ever failed 
no matter  try again 
fail again  fail better 

there are two tokens of the type ever  two tokens of the type again  and two tokens of
the type fail  lets say that each line in this example is a document  so we have three
documents of two sentences each  we can represent this example with a tokendocument
matrix or a typedocument matrix  the tokendocument matrix has twelve rows  one for
each token  and three columns  one for each line  figure     the typedocument matrix
has nine rows  one for each type  and three columns  figure    
a row vector for a token has binary values  an element is   if the given token appears in
the given document and   otherwise  a row vector for a type has integer values  an element
is the frequency of the given type in the given document  these vectors are related  in that
a type vector is the sum of the corresponding token vectors  for example  the row vector
for the type ever is the sum of the two token vectors for the two tokens of ever 
in applications dealing with polysemy  one approach uses vectors that represent word
tokens  schutze        agirre   edmonds        and another uses vectors that represent
word types  pantel   lin      a   typical word sense disambiguation  wsd  algorithms
deal with word tokens  instances of words in specific contexts  rather than word types 
we mention both approaches to polysemy in section    due to their similarity and close
relationship  although a defining characteristic of the vsm is that it is concerned with
frequencies  see section       and frequency is a property of types  not tokens 
    basic english is a highly reduced subset of english  designed to be easy for people to learn  the words
of basic english are listed at http   ogden basic english org  

   

fiturney   pantel

ever tried 
ever failed 
ever
tried
ever
failed
no
matter
try
again
fail
again
fail
better

no matter 
try again 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

fail again 
fail better 
 
 
 
 
 
 
 
 
 
 
 
 

figure    the tokendocument matrix  rows are tokens and columns are documents 

ever tried 
ever failed 
ever
tried
failed
no
matter
try
again
fail
better

no matter 
try again 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

fail again 
fail better 
 
 
 
 
 
 
 
 
 

figure    the typedocument matrix  rows are types and columns are documents 

   

fifrom frequency to meaning

    hypotheses
we have mentioned five hypotheses in this section  here we repeat these hypotheses and
then interpret them in terms of vectors  for each hypothesis  we cite work that explicitly
states something like the hypothesis or implicitly assumes something like the hypothesis 
statistical semantics hypothesis  statistical patterns of human word usage can be
used to figure out what people mean  weaver        furnas et al           if units of text
have similar vectors in a text frequency matrix    then they tend to have similar meanings 
 we take this to be a general hypothesis that subsumes the four more specific hypotheses
that follow  
bag of words hypothesis  the frequencies of words in a document tend to indicate
the relevance of the document to a query  salton et al           if documents and pseudodocuments  queries  have similar column vectors in a termdocument matrix  then they
tend to have similar meanings 
distributional hypothesis  words that occur in similar contexts tend to have similar
meanings  harris        firth        deerwester et al           if words have similar row
vectors in a wordcontext matrix  then they tend to have similar meanings 
extended distributional hypothesis  patterns that co occur with similar pairs tend
to have similar meanings  lin   pantel          if patterns have similar column vectors
in a pairpattern matrix  then they tend to express similar semantic relations 
latent relation hypothesis  pairs of words that co occur in similar patterns tend
to have similar semantic relations  turney et al           if word pairs have similar row
vectors in a pairpattern matrix  then they tend to have similar semantic relations 
we have not yet explained what it means to say that vectors are similar  we discuss
this in section     

   linguistic processing for vector space models
we will assume that our raw data is a large corpus of natural language text  before we
generate a termdocument  wordcontext  or pairpattern matrix  it can be useful to apply
some linguistic processing to the raw text  the types of processing that are used can be
grouped into three classes  first  we need to tokenize the raw text  that is  we need to decide
what constitutes a term and how to extract terms from raw text  second  we may want to
normalize the raw text  to convert superficially different strings of characters to the same
form  e g   car  car  cars  and cars could all be normalized to car   third  we may want
to annotate the raw text  to mark identical strings of characters as being different  e g   fly
as a verb could be annotated as fly vb and fly as a noun could be annotated as fly nn  
grefenstette        presents a good study of linguistic processing for wordcontext
vsms  he uses a similar three step decomposition of linguistic processing  tokenization 
surface syntactic analysis  and syntactic attribute extraction 
    by text frequency matrix  we mean any matrix or higher order tensor in which the values of the elements
are derived from the frequencies of pieces of text in the context of other pieces of text in some collection
of text  a text frequency matrix is intended to be a general structure  which includes termdocument 
wordcontext  and pairpattern matrices as special cases 

   

fiturney   pantel

    tokenization
tokenization of english seems simple at first glance  words are separated by spaces  this
assumption is approximately true for english  and it may work sufficiently well for a basic
vsm  but a more advanced vsm requires a more sophisticated approach to tokenization 
an accurate english tokenizer must know how to handle punctuation  e g   dont  janes 
and or   hyphenation  e g   state of the art versus state of the art   and recognize multi word
terms  e g   barack obama and ice hockey   manning et al          we may also wish to
ignore stop words  high frequency words with relatively low information content  such as
function words  e g   of  the  and  and pronouns  e g   them  who  that   a popular list of
stop words is the set of     common words included in the source code for the smart
system  salton          
in some languages  e g   chinese   words are not separated by spaces  a basic vsm
can break the text into character unigrams or bigrams  a more sophisticated approach
is to match the input text against entries in a lexicon  but the matching often does not
determine a unique tokenization  sproat   emerson         furthermore  native speakers
often disagree about the correct segmentation  highly accurate tokenization is a challenging
task for most human languages 
    normalization
the motivation for normalization is the observation that many different strings of characters often convey essentially identical meanings  given that we want to get at the meaning
that underlies the words  it seems reasonable to normalize superficial variations by converting them to the same form  the most common types of normalization are case folding
 converting all words to lower case  and stemming  reducing inflected words to their stem
or root form  
case folding is easy in english  but can be problematic in some languages  in french 
accents are optional for uppercase  and it may be difficult to restore missing accents when
converting the words to lowercase  some words cannot be distinguished without accents  for
example  peche could be either peche  meaning fishing or peach  or peche  meaning sin  
even in english  case folding can cause problems  because case sometimes has semantic
significance  for example  smart is an information retrieval system  whereas smart is a
common adjective  bush may be a surname  whereas bush is a kind of plant 
morphology is the study of the internal structure of words  often a word is composed
of a stem  root  with added affixes  inflections   such as plural forms and past tenses  e g  
trapped is composed of the stem trap and the affix  ed   stemming  a kind of morphological
analysis  is the process of reducing inflected words to their stems  in english  affixes are
simpler and more regular than in many other languages  and stemming algorithms based
on heuristics  rules of thumb  work relatively well  lovins        porter        minnen 
carroll    pearce         in an agglutinative language  e g   inuktitut   many concepts are
combined into a single word  using various prefixes  infixes  and suffixes  and morphological
analysis is complicated  a single word in an agglutinative language may correspond to a
sentence of half a dozen words in english  johnson   martin        
    the source code is available at ftp   ftp cs cornell edu pub smart  

   

fifrom frequency to meaning

the performance of an information retrieval system is often measured by precision and
recall  manning et al          the precision of a system is an estimate of the conditional
probability that a document is truly relevant to a query  if the system says it is relevant 
the recall of a system is an estimate of the conditional probability that the system will say
that a document is relevant to a query  if it truly is relevant 
in general  normalization increases recall and reduces precision  kraaij   pohlmann 
       this is natural  given the nature of normalization  when we remove superficial
variations that we believe are irrelevant to meaning  we make it easier to recognize similarities  we find more similar things  and so recall increases  but sometimes these superficial
variations have semantic significance  ignoring the variations causes errors  and so precision
decreases  normalization can also have a positive effect on precision in cases where variant
tokens are infrequent and smoothing the variations gives more reliable statistics 
if we have a small corpus  we may not be able to afford to be overly selective  and it may
be best to aggressively normalize the text  to increase recall  if we have a very large corpus 
precision may be more important  and we might not want any normalization  hull       
gives a good analysis of normalization for information retrieval 
    annotation
annotation is the inverse of normalization  just as different strings of characters may have
the same meaning  it also happens that identical strings of characters may have different
meanings  depending on the context  common forms of annotation include part of speech
tagging  marking words according to their parts of speech   word sense tagging  marking
ambiguous words according to their intended meanings   and parsing  analyzing the grammatical structure of sentences and marking the words in the sentences according to their
grammatical roles   manning   schutze        
since annotation is the inverse of normalization  we expect it to decrease recall and
increase precision  for example  by tagging program as a noun or a verb  we may be
able to selectively search for documents that are about the act of computer programming
 verb  instead of documents that discuss particular computer programs  noun   hence we
can increase precision  however  a document about computer programs  noun  may have
something useful to say about the act of computer programming  verb   even if the document
never uses the verb form of program  hence we may decrease recall 
large gains in ir performance have recently been reported as a result of query annotation with syntactic and semantic information  syntactic annotation includes query
segmentation  tan   peng        and part of speech tagging  barr  jones    regelson 
       examples of semantic annotation are disambiguating abbreviations in queries  wei 
peng    dumoulin        and finding query keyword associations  lavrenko   croft       
cao  nie    bai        
annotation is also useful for measuring the semantic similarity of words and concepts
 wordcontext matrices   for example  pantel and lin      a  presented an algorithm
that can discover word senses by clustering row vectors in a wordcontext matrix  using
contextual information derived from parsing 
   

fiturney   pantel

   mathematical processing for vector space models
after the text has been tokenized and  optionally  normalized and annotated  the first step
is to generate a matrix of frequencies  second  we may want to adjust the weights of the
elements in the matrix  because common words will have high frequencies  yet they are less
informative than rare words  third  we may want to smooth the matrix  to reduce the
amount of random noise and to fill in some of the zero elements in a sparse matrix  fourth 
there are many different ways to measure the similarity of two vectors 
lowe        gives a good summary of mathematical processing for wordcontext vsms 
he decomposes vsm construction into a similar four step process  calculate the frequencies 
transform the raw frequency counts  smooth the space  dimensionality reduction   then
calculate the similarities 
    building the frequency matrix
an element in a frequency matrix corresponds to an event  a certain item  term  word 
word pair  occurred in a certain situation  document  context  pattern  a certain number
of times  frequency   at an abstract level  building a frequency matrix is a simple matter
of counting events  in practice  it can be complicated when the corpus is large 
a typical approach to building a frequency matrix involves two steps  first  scan sequentially through the corpus  recording events and their frequencies in a hash table  a
database  or a search engine index  second  use the resulting data structure to generate the
frequency matrix  with a sparse matrix representation  gilbert  moler    schreiber        
    weighting the elements
the idea of weighting is to give more weight to surprising events and less weight to expected
events  the hypothesis is that surprising events  if shared by two vectors  are more discriminative of the similarity between the vectors than less surprising events  for example 
in measuring the semantic similarity between the words mouse and rat  the contexts dissect
and exterminate are more discriminative of their similarity than the contexts have and like 
in information theory  a surprising event has higher information content than an expected
event  shannon         the most popular way to formalize this idea for termdocument
matrices is the tf idf  term frequency  inverse document frequency  family of weighting
functions  sparck jones         an element gets a high weight when the corresponding term
is frequent in the corresponding document  i e   tf is high   but the term is rare in other
documents in the corpus  i e   df is low  and thus idf is high   salton and buckley       
defined a large family of tf idf weighting functions and evaluated them on information retrieval tasks  demonstrating that tf idf weighting can yield significant improvements over
raw frequency 
another kind of weighting  often combined with tf idf weighting  is length normalization
 singhal  salton  mitra    buckley         in information retrieval  if document length
is ignored  search engines tend to have a bias in favour of longer documents  length
normalization corrects for this bias 
term weighting may also be used to correct for correlated terms  for example  the
terms hostage and hostages tend to be correlated  yet we may not want to normalize them
   

fifrom frequency to meaning

to the same term  as in section       because they have slightly different meanings  as
an alternative to normalizing them  we may reduce their weights when they co occur in a
document  church        
feature selection may be viewed as a form of weighting  in which some terms get a
weight of zero and hence can be removed from the matrix  forman        provides a good
study of feature selection methods for text classification 
an alternative to tf idf is pointwise mutual information  pmi   church   hanks       
turney         which works well for both wordcontext matrices  pantel   lin      a 
and termdocument matrices  pantel   lin      b   a variation of pmi is positive pmi
 ppmi   in which all pmi values that are less than zero are replaced with zero  niwa  
nitta         bullinaria and levy        demonstrated that ppmi performs better than a
wide variety of other weighting approaches when measuring semantic similarity with word
context matrices  turney      a  applied ppmi to pairpattern matrices  we will give the
formal definition of ppmi here  as an example of an effective weighting function 
let f be a wordcontext frequency matrix with nr rows and nc columns  the i th row
in f is the row vector fi  and the j th column in f is the column vector f j   the row fi 
corresponds to a word wi and the column f j corresponds to a context cj   the value of the
element fij is the number of times that wi occurs in the context cj   let x be the matrix
that results when ppmi is applied to f  the new matrix x has the same number of rows
and columns as the raw frequency matrix f  the value of an element xij in x is defined
as follows 
fij
pij   pnr pnc

j   fij

i  

   

pnc

j   fij
pi   pnr pnc

   

pnr
f
pncij
  pnr i  

   

i  

pj

i  

j   fij
j   fij



pij
pmiij   log
pi pj

pmiij if pmiij    
xij  
  otherwise

   
   

in this definition  pij is the estimated probability that the word wi occurs in the context
cj   pi is the estimated probability of the word wi   and pj is the estimated probability of
the context cj   if wi and cj are statistically independent  then pi pj   pij  by the definition
of independence   and thus pmiij is zero  since log          the product pi pj is what we
would expect for pij if wi occurs in cj by pure random chance  on the other hand  if there
is an interesting semantic relation between wi and cj   then we should expect pij to be larger
than it would be if wi and cj were indepedent  hence we should find that pij   pi pj   and
thus pmiij is positive  this follows from the distributional hypothesis  see section     if
the word wi is unrelated to the context cj   we may find that pmiij is negative  ppmi is
designed to give a high value to xij when there is an interesting semantic relation between
   

fiturney   pantel

wi and cj   otherwise  xij should have a value of zero  indicating that the occurrence of wi
in cj is uninformative 
a well known problem of pmi is that it is biased towards infrequent events  consider
the case where wi and cj are statistically dependent  i e   they have maximum association  
then pij   pi   pj   hence     becomes log    pi   and pmi increases as the probability of
word wi decreases  several discounting factors have been proposed to alleviate this problem 
an example follows  pantel   lin      a  
p c
p r
fik  
fkj   nk  
min   nk  
fij
pnc
pnr
ij  

fij     min   k   fkj   k   fik      
newpmiij   ij  pmiij

   
   

another way to deal with infrequent events is laplace smoothing of the probability
estimates  pij   pi   and pj  turney   littman         a constant positive value is added to
the raw frequencies before calculating the probabilities  each fij is replaced with fij   k  for
some k      the larger the constant  the greater the smoothing effect  laplace smoothing
pushes the pmiij values towards zero  the magnitude of the push  the difference between
pmiij with and without laplace smoothing  depends on the raw frequency fij   if the
frequency is large  the push is small  if the frequency is small  the push is large  thus
laplace smoothing reduces the bias of pmi towards infrequent events 
    smoothing the matrix
the simplest way to improve information retrieval performance is to limit the number of
vector components  keeping only components representing the most frequently occurring
content words is such a way  however  common words  such as the and have  carry little
semantic discrimination power  simple component smoothing heuristics  based on the properties of the weighting schemes presented in section      have been shown to both maintain
semantic discrimination power and improve the performance of similarity computations 
computing the similarity between all pairs of vectors  described in section      is a
computationally intensive task  however  only vectors that share a non zero coordinate
must be compared  i e   two vectors that do not share a coordinate are dissimilar   very
frequent context words  such as the word the  unfortunately result in most vectors matching
a non zero coordinate  such words are precisely the contexts that have little semantic
discrimination power  consider the pointwise mutual information weighting described in
section      highly weighted dimensions co occur frequently with only very few words and
are by definition highly discriminating contexts  i e   they have very high association with
the words with which they co occur   by keeping only the context word dimensions with
a pmi above a conservative threshold and setting the others to zero  lin        showed
that the number of comparisons needed to compare vectors greatly decreases while losing
little precision in the similarity score between the top     most similar words of every word 
while smoothing the matrix  one computes a reverse index on the non zero coordinates 
then  to compare the similarity between a words context vector and all other words context
vectors  only those vectors found to match a non zero component in the reverse index must
be compared  section     proposes further optimizations along these lines 
   

fifrom frequency to meaning

deerwester et al         found an elegant way to improve similarity measurements with a
mathematical operation on the termdocument matrix  x  based on linear algebra  the operation is truncated singular value decomposition  svd   also called thin svd  deerwester
et al  briefly mentioned that truncated svd can be applied to both document similarity
and word similarity  but their focus was document similarity  landauer and dumais       
applied truncated svd to word similarity  achieving human level scores on multiple choice
synonym questions from the test of english as a foreign language  toefl   truncated
svd applied to document similarity is called latent semantic indexing  lsi   but it is
called latent semantic analysis  lsa  when applied to word similarity 
there are several ways of thinking about how truncated svd works  we will first
present the math behind truncated svd and then describe four ways of looking at it 
latent meaning  noise reduction  high order co occurrence  and sparsity reduction 
svd decomposes x into the product of three matrices uvt   where u and v are in
column orthonormal form  i e   the columns are orthogonal and have unit length  ut u  
vt v   i  and  is a diagonal matrix of singular values  golub   van loan         if x
is of rank r  then  is also of rank r  let k   where k   r  be the diagonal matrix formed
from the top k singular values  and let uk and vk be the matrices produced by selecting the
corresponding columns from u and v  the matrix uk k vkt is the matrix of rank k that
best approximates the original matrix x  in the sense that it minimizes the approximation
errors  that is  x   uk k vkt minimizes kx  xkf over all matrices x of rank k  where
k       kf denotes the frobenius norm  golub   van loan        
latent meaning  deerwester et al         and landauer and dumais        describe
truncated svd as a method for discovering latent meaning  suppose we have a word
context matrix x  the truncated svd  x   uk k vkt   creates a low dimensional linear
mapping between row space  words  and column space  contexts   this low dimensional
mapping captures the latent  hidden  meaning in the words and the contexts  limiting the
number of latent dimensions  k   r  forces a greater correspondence between words and
contexts  this forced correspondence between words and contexts improves the similarity
measurement 
noise reduction  rapp        describes truncated svd as a noise reduction technique 
we may think of the matrix x   uk k vkt as a smoothed version of the original matrix x 
the matrix uk maps the row space  the space spanned by the rows of x  into a smaller
k dimensional space and the matrix vk maps the column space  the space spanned by
the columns of x  into the same k dimensional space  the diagonal matrix k specifies
the weights in this reduced k dimensional space  the singular values in  are ranked in
descending order of the amount of variation in x that they fit  if we think of the matrix
x as being composed of a mixture of signal and noise  with more signal than noise  then
uk k vkt mostly captures the variation in x that is due to the signal  whereas the remaining
vectors in uvt are mostly fitting the variation in x that is due to the noise 
high order co occurrence  landauer and dumais        also describe truncated
svd as a method for discovering high order co occurrence  direct co occurrence  firstorder co occurrence  is when two words appear in identical contexts  indirect co occurrence
 high order co occurrence  is when two words appear in similar contexts  similarity of
contexts may be defined recursively in terms of lower order co occurrence  lemaire and
denhiere        demonstrate that truncated svd can discover high order co occurrence 
   

fiturney   pantel

sparsity reduction  in general  the matrix x is very sparse  mostly zeroes   but the
truncated svd  x   uk k vkt   is dense  sparsity may be viewed as a problem of insufficient
data  with more text  the matrix x would have fewer zeroes  and the vsm would perform
better on the chosen task  from this perspective  truncated svd is a way of simulating the
missing text  compensating for the lack of data  vozalis   margaritis        
these different ways of viewing truncated svd are compatible with each other  it is
possible for all of these perspectives to be correct  future work is likely to provide more
views of svd and perhaps a unifying view 
a good c implementation of svd for large sparse matrices is rohdes svdlibc   
another approach is brands        incremental truncated svd algorithm    yet another
approach is gorrells        hebbian algorithm for incremental truncated svd  brands
and gorrells algorithms both introduce interesting new ways of handling missing values 
instead of treating them as zero values 
for higher order tensors  there are operations that are analogous to truncated svd 
such as parallel factor analysis  parafac   harshman         canonical decomposition
 candecomp   carroll   chang         equivalent to parafac but discovered independently   and tucker decomposition  tucker         for an overview of tensor decompositions  see the surveys of kolda and bader        or acar and yener         turney       
gives an empirical evaluation of how well four different tucker decomposition algorithms
scale up for large sparse third order tensors  a low ram algorithm  multislice projection 
for large sparse tensors is presented and evaluated   
since the work of deerwester et al          subsequent research has discovered many
alternative matrix smoothing processes  such as nonnegative matrix factorization  nmf 
 lee   seung         probabilistic latent semantic indexing  plsi   hofmann         iterative scaling  is   ando         kernel principal components analysis  kpca   scholkopf 
smola    muller         latent dirichlet allocation  lda   blei et al          and discrete
component analysis  dca   buntine   jakulin        
the four perspectives on truncated svd  presented above  apply equally well to all of
these more recent matrix smoothing algorithms  these newer smoothing algorithms tend
to be more computationally intensive than truncated svd  but they attempt to model
word frequencies better than svd  truncated svd implicitly assumes that the elements
in x have a gaussian distribution  minimizing the the frobenius norm kx  xkf will
minimize the noise  if the noise has a gaussian distribution  however  it is known that
word frequencies do not have gaussian distributions  more recent algorithms are based on
more realistic models of the distribution for word frequencies   
    comparing the vectors
the most popular way to measure the similarity of two frequency vectors  raw or weighted 
is to take their cosine  let x and y be two vectors  each with n elements 
   
   
   
   

svdlibc is available at http   tedlab mit edu dr svdlibc  
matlab source code is available at http   web mit edu wingated www resources html 
matlab source code is available at http   www apperceptual com multislice  
in our experience  pmiij appears to be approximately gaussian  which may explain why pmi works well
with truncated svd  but then ppmi is puzzling  because it is less gaussian than pmi  yet it apparently
yields better semantic models than pmi 

   

fifrom frequency to meaning

x   hx    x            xn i

   

y   hy    y            yn i

   

the cosine of the angle  between x and y can be calculated as follows 
pn
cos x  y    qp
n

 yi
pn

i   xi

 
 
i   xi 
i   yi
xy
 

xx yy
x
y
 

kxk kyk

    
    
    

in other words  the cosine of the angle between two vectors is the inner product of the
vectors  after they have been normalized to unit length  if x and y are frequency vectors
for words  a frequent word will have a long vector and a rare word will have a short vector 
yet the words might be synonyms  cosine captures the idea that the length of the vectors
is irrelevant  the important thing is the angle between the vectors 
the cosine ranges from   when the vectors point in opposite directions   is    
degrees  to    when they point in the same direction   is   degrees   when the vectors
are orthogonal   is    degrees   the cosine is zero  with raw frequency vectors  which
necessarily cannot have negative elements  the cosine cannot be negative  but weighting
and smoothing often introduce negative elements  ppmi weighting does not yield negative
elements  but truncated svd can generate negative elements  even when the input matrix
has no negative values 
a measure of distance between vectors can easily be converted to a measure of similarity
by inversion      or subtraction      

sim x  y      dist x  y 

    

sim x  y       dist x  y 

    

many similarity measures have been proposed in both ir  jones   furnas        and
lexical semantics circles  lin        dagan  lee    pereira        lee        weeds  weir 
  mccarthy         it is commonly said in ir that  properly normalized  the difference
in retrieval performance using different measures is insignificant  van rijsbergen        
often the vectors are normalized in some way  e g   unit length or unit probability  before
applying any similarity measure 
popular geometric measures of vector distance include euclidean distance and manhattan distance  distance measures from information theory include hellinger  bhattacharya 
and kullback leibler  bullinaria and levy        compared these five distance measures
and the cosine similarity measure on four different tasks involving word similarity  overall 
the best measure was cosine  other popular measures are the dice and jaccard coefficients
 manning et al         
   

fiturney   pantel

lee        proposed that  for finding word similarities  measures that focused more on
overlapping coordinates and less on the importance of negative features  i e   coordinates
where one word has a nonzero value and the other has a zero value  appear to perform
better  in lees experiments  the jaccard  jensen shannon  and l  measures seemed to
perform best  weeds et al         studied the linguistic and statistical properties of the
similar words returned by various similarity measures and found that the measures can be
grouped into three classes 
   high frequency sensitive measures  cosine  jensen shannon   skew  recall  
   low frequency sensitive measures  precision   and
   similar frequency sensitive methods  jaccard  jaccard mi  lin  harmonic mean  
given a word w    if we use a high frequency sensitive measure to score other words wi
according to their similarity with w    higher frequency words will tend to get higher scores
than lower frequency words  if we use a low frequency sensitive measure  there will be a
bias towards lower frequency words  similar frequency sensitive methods prefer a word wi
that has approximately the same frequency as w    in one experiment on determining the
compositionality of collocations  high frequency sensitive measures outperformed the other
classes  weeds et al          we believe that determining the most appropriate similarity
measure is inherently dependent on the similarity task  the sparsity of the statistics  the
frequency distribution of the elements being compared  and the smoothing method applied
to the matrix 
    efficient comparisons
computing the similarity between all rows  or columns  in a large matrix is a non trivial
problem  with a worst case cubic running time o n r nc    where nr is the number of rows and
nc is the number of columns  i e   the dimensionality of the feature space   optimizations
and parallelization are often necessary 
      sparse matrix multiplication
one optimization strategy is a generalized sparse matrix multiplication approach  sarawagi
  kirpal         which is based on the observation that a scalar product of two vectors
depends only on the coordinates for which both vectors have nonzero values  further  we
observe that most commonly used similarity measures for vectors x and y  such as cosine 
overlap  and dice  can be decomposed into three values  one depending only on the nonzero
values of x  another depending only on the nonzero values of y  and the third depending on
the nonzero coordinates shared both by x and y  more formally  commonly used similarity
scores  sim x  y   can be expressed as follows 
sim x  y    f   

pn

i   f   xi   yi    f   x   f   y  

    

for example  the cosine measure  cos x  y   defined in       can be expressed in this model
as follows 
   

fifrom frequency to meaning

p
cos x  y    f    ni   f   xi   yi    f   x   f   y  
a
f   a  b  c   
bc
f   a  b    a  b
qp
n
 
f   a    f   a   
i   ai

    
    
    
    

let x be a matrix for which we want to compute the pairwise similarity  sim x  y  
between all rows or all columns x and y  efficient computation of the similarity matrix s
can be achieved by leveraging the fact that sim x  y  is determined solely by the nonzero
coordinates shared by x and y  i e   f      xi     f   xi          for any xi   and that most of
the vectors are very sparse  in this case  calculating f   xi   yi   is only required when both
vectors have a shared nonzero coordinate  significantly reducing the cost of computation 
determining which vectors share a nonzero coodinate can easily be achieved by first building
an inverted index for the coordinates  during indexing  we can also precompute f   x  and
f   y  without changing the algorithm complexity  then  for each vector x we retrieve in
constant time  from the index  each vector y that shares a nonzero coordinate with x and
wep
apply f   xi   yi   on the shared coordinates i  the computational cost of this algorithm
is i ni  where ni is the number of vectors that have a nonzero i th coordinate  its worst
case time complexity is o ncv  where n is the number of vectors to be compared  c is the
maximum number of nonzero coordinates of any vector  and v is the number of vectors
that have a nonzero i th coordinate where i is the coordinate which is nonzero for the most
vectors  in other words  the algorithm is efficient only when the density of the coordinates
is low  in our own experiments of computing the semantic similarity between all pairs of
words in a large web crawl  we observed near linear average running time complexity in n 
the computational cost can be reduced further by leveraging the element weighting
techniques described in section      by setting to zero all coordinates that have a low
ppmi  pmi or tf idf score  the coordinate density is dramatically reduced at the cost of
losing little discriminative power  in this vein  bayardo  ma  and srikant        described
a strategy that omits the coordinates with the highest number of nonzero values  their
algorithm gives a significant advantage only when we are interested in finding solely the
similarity between highly similar vectors 
      distributed implementation using mapreduce
the algorithm described in section       assumes that the matrix x can fit into memory 
which for large x may be impossible  also  as each element of x is processed independently  running parallel processes for non intersecting subsets of x makes the processing
faster  elsayed  lin  and oard        proposed a mapreduce implementation deployed using hadoop  an open source software package implementing the mapreduce framework and
distributed file system    hadoop has been shown to scale to several thousands of machines 
allowing users to write simple code  and to seamlessly manage the sophisticated parallel execution of the code  dean and ghemawat        provide a good primer on mapreduce
programming 
    hadoop is available for download at http   lucene apache org hadoop  

   

fiturney   pantel

the mapreduce models map step is used to start m  n map tasks in parallel  each
caching one m th part of x as an inverted index and streaming one n th part of x through
it  the actual inputs are read by the tasks directly from hdfs  hadoop distributed
file system   the value of m is determined by the amount of memory dedicated for the
inverted index  and n should be determined by trading off the fact that  as n increases 
more parallelism can be obtained at the increased cost of building the same inverted index
n times 
the similarity algorithm from section       runs in each task of the map step of a
mapreduce job  the reduce step groups the output by the rows  or columns  of x 
      randomized algorithms
other optimization strategies use randomized techniques to approximate various similarity measures  the aim of randomized algorithms is to improve computational efficiency
 memory and time  by projecting high dimensional vectors into a low dimensional subspace 
truncated svd performs such a projection  but svd can be computationally intensive   
the insight of randomized techniques is that high dimensional vectors can be randomly projected into a low dimensional subspace with relatively little impact on the final similarity
scores  significant reductions in computational cost have been reported with little average error to computing the true similarity scores  especially in applications such as word
similarity where we are interested in only the top k most similar vectors to each vector
 ravichandran  pantel    hovy        gorman   curran        
random indexing  an approximation technique based on sparse distributed memory
 kanerva         computes the pairwise similarity between all rows  or vectors  of a matrix
with complexity o nr nc      where   is a fixed constant representing the length of the index
vectors assigned to each column  the value of   controls the tradeoff of accuracy versus
efficiency  the elements of each index vector are mostly zeros with a small number of
randomly assigned   s and  s  the cosine measure between two rows r  and r  is then
approximated by computing the cosine between two fingerprint vectors  fingerprint r   
and fingerprint r     where fingerprint r  is computed by summing the index vectors of
each non unique coordinate of r  random indexing was shown to perform as well as lsa
on a word synonym selection task  karlgren   sahlgren        
locality sensitive hashing  lsh   broder        is another technique that approximates
the similarity matrix with complexity o n r      where   is a constant number of random
projections  which controls the accuracy versus efficiency tradeoff    lsh is a general class of
techniques for defining functions that map vectors  rows or columns  into short signatures or
fingerprints  such that two similar vectors are likely to have similar fingerprints  definitions
of lsh functions include the min wise independent function  which preserves the jaccard
similarity between vectors  broder         and functions that preserve the cosine similarity
between vectors  charikar         on a word similarity task  ravichandran et al        
showed that  on average  over     of the top    similar words of random words are found
in the top    results using charikars functions  and that the average cosine error is      
    however  there are efficient forms of svd  brand        gorrell        
    lsh stems from work by rabin         who proposed the use of hash functions from random irreducible
polynomials to create short fingerprints of collections of documents  such techniques are useful for many
tasks  such as removing duplicate documents  deduping  in a web crawl 

   

fifrom frequency to meaning

 using            random projections   gorman and curran        provide a detailed
comparison of random indexing and lsh on a distributional similarity task  on the bnc
corpus  lsh outperformed random indexing  however  on a larger corpora combining bnc 
the reuters corpus  and most of the english news holdings of the ldc in       random
indexing outperformed lsh in both efficiency and accuracy 
    machine learning
if the intended application for a vsm is clustering or classification  a similarity measure
such as cosine  section      may be used  for classification  a nearest neighbour algorithm
can use cosine as a measure of nearness  dasarathy         for clustering  a similaritybased clustering algorithm can use cosine as a measure of similarity  jain  murty    flynn 
       however  there are many machine learning algorithms that can work directly with
the vectors in a vsm  without requiring an external similarity measure  such as cosine 
in effect  such machine learning algorithms implicitly use their own internal approaches to
measuring similarity 
any machine learning algorithm that works with real valued vectors can use vectors
from a vsm  witten   frank         linguistic processing  section    and mathematical
processing  section    may still be necessary  but the machine learning algorithm can handle
vector comparison  sections     and      
in addition to unsupervised  clustering  and supervised  classification  machine learning  vectors from a vsm may also be used in semi supervised learning  ando   zhang 
      collobert   weston         in general  there is nothing unique to vsms that would
compel a choice of one machine learning algorithm over another  aside from the algorithms
performance on the given task  therefore we refer our readers to the machine learning
literature  witten   frank         since we have no advice that is specific to vsms 

   three open source vsm systems
to illustrate the three types of vsms discussed in section    this section presents three open
source systems  one for each vsm type  we have chosen to present open source systems so
that interested readers can obtain the source code to find out more about the systems and
to apply the systems in their own projects  all three systems are written in java and are
designed for portability and ease of use 
    the termdocument matrix  lucene
lucene   is an open source full featured text search engine library supported by the apache
software foundation  it is arguably the most ubiquitous implementation of a termdocument
matrix  powering many search engines such as at cnet  sourceforge  wikipedia  disney 
aol and comcast  lucene offers efficient storage  indexing  as well as retrieval and ranking
functionalities  although it is primarily used as a termdocument matrix  it generalizes to
other vsms 
    apache lucene is available for download at http   lucene apache org  

   

fiturney   pantel

content  such as webpages  pdf documents  images  and video  are programmatically
decomposed into fields and stored in a database  the database implements the term
document matrix  where content corresponds to documents and fields correspond to terms 
fields are stored in the database and indices are computed on the field values  lucene
uses fields as a generalization of content terms  allowing any other string or literal to index
documents  for example  a webpage could be indexed by all the terms it contains  and also
by the anchor texts pointing to it  its host name  and the semantic classes in which it is
classified  e g   spam  product review  news  etc    the webpage can then be retrieved by
search terms matching any of these fields 
columns in the termdocument matrix consist of all the fields of a particular instance
of content  e g   a webpage   the rows consist of all instances of content in the index 
various statistics such as frequency and tf idf are stored in the matrix  the developer
defines the fields in a schema and identifies those to be indexed by lucene  the developer
also optionally defines a content ranking function for each indexed field 
once the index is built  lucene offers functionalities for retrieving content  users can
issue many query types such as phrase queries  wildcard queries  proximity queries  range
queries  e g   date range queries   and field restricted queries  results can be sorted by any
field and index updates can occur simultaneously during searching  lucenes index can be
directly loaded into a tomcat webserver and it offers apis for common programming languages  solr    a separate apache software foundation project  is an open source enterprise
webserver for searching a lucene index and presenting search results  it is a full featured
webserver providing functionalities such as xml http and json apis  hit highlighting 
faceted search  caching  and replication 
a simple recipe for creating a web search service  using nutch  lucene and solr  consists
of crawling a set of urls  using nutch   creating a termdocument matrix index  using
lucene   and serving search results  using solr   nutch    the apache software foundation
open source web search software  offers functionality for crawling the web from a seed set
of urls  for building a link graph of the web crawl  and for parsing web documents such
as html pages  a good set of seed urls for nutch can be downloaded freely from the
open directory project    crawled pages are html parsed  and they are then indexed
by lucene  the resulting indexed collection is then queried and served through a solr
installation with tomcat 
for more information on lucene  we recommend gospodnetic and hatchers       
book  konchady        explains how to integrate lucene with lingpipe and gate for
sophisticated semantic processing   

   
   
   
   

apache solr is available for download at http   lucene apache org solr  
apache nutch is available for download at http   lucene apache org nutch  
see http   www dmoz org  
information about lingpipe is available at http   alias i com lingpipe   the gate  general architecture for text engineering  home page is at http   gate ac uk  

   

fifrom frequency to meaning

    the wordcontext matrix  semantic vectors
semantic vectors   is an open source project implementing the random projection approach
to measuring word similarity  see section         the package uses lucene to create a term
document matrix  and it then creates vectors from lucenes termdocument matrix  using
random projection for dimensionality reduction  the random projection vectors can be
used  for example  to measure the semantic similarity of two words or to find the words
that are most similar to a given word 
the idea of random projection is to take high dimensional vectors and randomly project
them into a relatively low dimensional space  sahlgren         this can be viewed as a
kind of smoothing operation  section       but the developers of the semantic vectors
package emphasize the simplicity and efficiency of random projection  section       rather
than its smoothing ability  they argue that other matrix smoothing algorithms might
smooth better  but none of them perform as well as random indexing  in terms of the
computational complexity of building a smooth matrix and incrementally updating the
matrix when new data arrives  widdows   ferraro         their aim is to encourage
research and development with semantic vectors by creating a simple and efficient open
source package 
the semantic vectors package is designed to be convenient to use  portable  and easy
to extend and modify  the design of the software incorporates lessons learned from the
earlier stanford infomap project    although the default is to generate random projection
vectors  the system has a modular design that allows other kinds of wordcontext matrices
to be used instead of random projection matrices 
the package supports two basic functions  building a wordcontext matrix and searching
through the vectors in the matrix  in addition to generating word vectors  the building
operation can generate document vectors by calculating weighted sums of the word vectors
for the words in each document  the searching operation can be used to search for similar
words or to search for documents that are similar to a query  a query can be a single word or
several words can be combined  using various mathematical operations on the corresponding
vectors  the mathematical operations include vector negation and disjunction  based on
quantum logic  widdows         widdows and ferraro        provide a good summary of
the semantic vectors software 
    the pairpattern matrix  latent relational analysis in s space
latent relational analysis    lra  is an open source project implementing the pairpattern
matrix  it is a component of the s space package  a library of tools for building and
comparing different semantic spaces 
lra takes as input a textual corpus and a set of word pairs  a pairpattern matrix is
built by deriving lexical patterns that link together the word pairs in the corpus  for example  consider the word pair hkorea  japani and the following retrieved matching sentences 
    semantic vectors is a software package for measuring word similarity  available under the simplified bsd
license at http   code google com p semanticvectors  
    see http   infomap nlp sourceforge net  
    latent relational analysis is part of the s space package and is distributed under the gnu general
public license version    it is available at http   code google com p airhead research   at the time of
writing  the lra module was under development 

   

fiturney   pantel

 korea looks to new japan prime ministers effect on korea japan relations 
 what channel is the korea vs  japan football game 
from these two sentences  lra extracts two patterns  x looks to new y  and x vs  y  
these patterns become two columns in the pairpattern matrix  and the word pair hkorea 
japani becomes a row  pattern frequencies are counted and then smoothed using svd  see
section      
in order to mitigate the sparseness of occurrences of word pairs  a thesaurus such as
wordnet is used to expand the seed word pairs to alternatives  for example the pair
hkorea  japani may be expanded to include hsouth korea  japani  hrepublic of korea 
japani  hkorea  nipponi  hsouth korea  nipponi  and hrepublic of korea  nipponi 
lra uses lucene  see section      as its backend to store the matrix  index it  and serve
its contents  for a detailed description of the lra algorithm  we suggest turneys       
paper 

   applications
in this section  we will survey some of the semantic applications for vsms  we will aim for
breadth  rather than depth  readers who want more depth should consult the references  our
goal is to give the reader an impression of the scope and flexibility of vsms for semantics 
the following applications are grouped according to the type of matrix involved  term
document  wordcontext  or pairpattern  note that this section is not exhaustive  there
are many more references and applications than we have space to list here 
    termdocument matrices
termdocument matrices are most suited to measuring the semantic similarity of documents
and queries  see section       the usual measure of similarity is the cosine of column vectors
in a weighted termdocument matrix  there are a variety of applications for measures of
document similarity 
document retrieval  the termdocument matrix was first developed for document
retrieval  salton et al          and there is now a large body of literature on the vsm
for document retrieval  manning et al          including several journals and conferences
devoted to the topic  the core idea is  given a query  rank the documents in order of
decreasing cosine of the angles between the query vector and the document vectors  salton
et al          one variation on the theme is cross lingual document retrieval  where a
query in one language is used to retrieve a document in another language  landauer  
littman         an important technical advance was the discovery that smoothing the
termdocument matrix by truncated svd can improve precision and recall  deerwester
et al          although few commercial systems use smoothing  due to the computational
expense when the document collection is large and dynamic  random indexing  sahlgren 
      or incremental svd  brand        may help to address these scaling issues  another
important development in document retrieval has been the addition of collaborative filtering 
in the form of pagerank  brin   page        
document clustering  given a measure of document similarity  we can cluster the
documents into groups  such that similarity tends to be high within a group  but low across
   

fifrom frequency to meaning

groups  manning et al          the clusters may be partitional  flat   cutting  karger 
pedersen    tukey        pantel   lin      b  or they may have a hierarchical structure
 groups of groups   zhao   karypis         they may be non overlapping  hard   croft 
      or overlapping  soft   zamir   etzioni         clustering algorithms also differ in how
clusters are compared and abstracted  with single link clustering  the similarity between
two clusters is the maximum of the similarities between their members  complete link
clustering uses the minimum of the similarities and average link clustering uses the average
of the similarities  manning et al         
document classification  given a training set of documents with class labels and a
testing set of unlabeled documents  the task of document classification is to learn from the
training set how to assign labels to the testing set  manning et al          the labels may
be the topics of the documents  sebastiani         the sentiment of the documents  e g  
positive versus negative product reviews   pang  lee    vaithyanathan        kim  pantel 
chklovski    pennacchiotti         spam versus non spam  sahami  dumais  heckerman   
horvitz        pantel   lin         or any other labels that might be inferred from the words
in the documents  when we classify documents  we are implying that the documents in a
class are similar in some way  thus document classification implies some notion of document
similarity  and most machine learning approaches to document classification involve a term
document matrix  sebastiani         a measure of document similarity  such as cosine  can
be directly applied to document classification by using a nearest neighbour algorithm  yang 
      
essay grading  student essays may be automatically graded by comparing them to
one or more high quality reference essays on the given essay topic  wolfe  schreiner  rehder 
laham  foltz  kintsch    landauer        foltz  laham    landauer         the student
essays and the reference essays can be compared by their cosines in a termdocument matrix 
the grade that is assigned to a student essay is proportional to its similarity to one of the
reference essays  a student essay that is highly similar to a reference essay gets a high grade 
document segmentation  the task of document segmentation is to partition a document into sections  where each section focuses on a different subtopic of the document
 hearst        choi         we may treat the document as a series of blocks  where a block
is a sentence or a paragraph  the problem is to detect a topic shift from one block to the
next  hearst        and choi        both use the cosine between columns in a wordblock
frequency matrix to measure the semantic similarity of blocks  a topic shift is signaled by
a drop in the cosine between consecutive blocks  the wordblock matrix can be viewed as
a small termdocument matrix  where the corpus is a single document and the documents
are blocks 
question answering  given a simple question  the task in question answering  qa 
is to find a short answer to the question by searching in a large corpus  a typical question is  how many calories are there in a big mac  most algorithms for qa have four
components  question analysis  document retrieval  passage retrieval  and answer extraction
 tellex  katz  lin  fern    marton        dang  lin    kelly         vector based similarity measurements are often used for both document retrieval and passage retrieval  tellex
et al         
call routing  chu carroll and carpenter        present a vector based system for
automatically routing telephone calls  based on the callers spoken answer to the question 
   

fiturney   pantel

how may i direct your call  if the callers answer is ambiguous  the system automatically
generates a question for the caller  derived from the vsm  that prompts the caller for further
information 
    wordcontext matrices
wordcontext matrices are most suited to measuring the semantic similarity of words  see
section       for example  we can measure the similarity of two words by the cosine of the
angle between their corresponding row vectors in a wordcontext matrix  there are many
applications for measures of word similarity 
word similarity  deerwester et al         discovered that we can measure word similarity by comparing row vectors in a termdocument matrix  landauer and dumais       
evaluated this approach with    multiple choice synonym questions from the test of english as a foreign language  toefl   achieving human level performance        correct
for the wordcontext matrix and       for the average non english us college applicant  
the documents used by landauer and dumais had an average length of     words  which
seems short for a document  but long for the context of a word  other researchers soon
switched to much shorter lengths  which is why we prefer to call these wordcontext matrices  instead of termdocument matrices  lund and burgess        used a context window
of ten words  schutze        used a fifty word window     words  centered on the target
word   rapp        achieved       correct on the    toefl questions  using a four word
context window    words  centered on the target word  after removing stop words   the
toefl results suggest that performance improves as the context window shrinks  it seems
that the immediate context of a word is much more important than the distant context for
determining the meaning of the word 
word clustering  pereira  tishby  and lee        applied soft hierarchical clustering
to row vectors in a wordcontext matrix  in one experiment  the words were nouns and the
contexts were verbs for which the given nouns were direct objects  in another experiment 
the words were verbs and the contexts were nouns that were direct objects of the given
verbs  schutzes        seminal word sense discrimination model used hard flat clustering
for row vectors in a wordcontext matrix  where the context was given by a window of   
words  centered on the target word  pantel and lin      a  applied soft flat clustering to
a wordcontext matrix  where the context was based on parsed text  these algorithms are
able to discover different senses of polysemous words  generating different clusters for each
sense  in effect  the different clusters correspond to the different concepts that underlie the
words 
word classification  turney and littman        used a wordcontext matrix to classify words as positive  honest  intrepid  or negative  disturbing  superfluous   they used the
general inquirer  gi  lexicon  stone  dunphy  smith    ogilvie        to evaluate their
algorithms  the gi lexicon includes        words  labeled with     categories related to
opinion  affect  and attitude    turney and littman hypothesize that all     categories can
be discriminated with a wordcontext matrix 
automatic thesaurus generation  wordnet is a popular tool for research in natural
language processing  fellbaum         but creating and maintaing such lexical resources
    the gi lexicon is available at http   www wjh harvard edu inquirer spreadsheet guide htm 

   

fifrom frequency to meaning

is labour intensive  so it is natural to wonder whether the process can be automated to
some degree    this task can seen as an instance of word clustering  when the thesaurus
is generated from scratch  or classification  when an existing thesaurus is automatically
extended   but it is worthwhile to consider the task of automatic thesaurus generation
separately from clustering and classification  due to the specific requirements of a thesaurus 
such as the particular kind of similarity that is appropriate for a thesaurus  see section      
several researchers have used wordcontext matrices specifically for the task of assisting or
automating thesaurus generation  crouch        grefenstette        ruge        pantel  
lin      a  curran   moens        
word sense disambiguation  a typical word sense disambiguation  wsd  system
 agirre   edmonds        pedersen        uses a feature vector representation in which
each vector corresponds to a token of a word  not a type  see section       however  leacock 
towell  and voorhees        used a wordcontext frequency matrix for wsd  in which each
vector corresponds to a type annotated with a sense tag  yuret and yatbaz        applied
a wordcontext frequency matrix to unsupervised wsd  achieving results comparable to
the performance of supervised wsd systems 
context sensitive spelling correction  people frequently confuse certain sets of
words  such as there  theyre  and their  these confusions cannot be detected by a simple dictionary based spelling checker  they require context sensitive spelling correction  a
wordcontext frequency matrix may be used to correct these kinds of spelling errors  jones
  martin        
semantic role labeling  the task of semantic role labeling is to label parts of a sentence according to the roles they play in the sentence  usually in terms of their connection
to the main verb of the sentence  erk        presented a system in which a wordcontext
frequency matrix was used to improve the performance of semantic role labeling  pennacchiotti  cao  basili  croce  and roth        show that wordcontext matrices can reliably
predict the semantic frame to which an unknown lexical unit refers  with good levels of
accuracy  such lexical unit induction is important in semantic role labeling  to narrow the
candidate set of roles of any observed lexical unit 
query expansion  queries submitted to search engines such as google and yahoo 
often do not directly match the terms in the most relevant documents  to alleviate this
problem  the process of query expansion is used for generating new search terms that are
consistent with the intent of the original query  vsms form the basis of query semantics
models  cao  jiang  pei  he  liao  chen    li         some methods represent queries
by using session contexts  such as query cooccurrences in user sessions  huang  chien   
oyang        jones  rey  madani    greiner         and others use click contexts  such as
the urls that were clicked on as a result of a query  wen  nie    zhang        
textual advertising  in pay per click advertising models  prevalent in search engines
such as google and yahoo   users pay for keywords  called bidterms  which are then used to
display their ads when relevant queries are issued by users  the scarcity of data makes ad
matching difficult and  in response  several techniques for bidterm expansion using vsms
have been proposed  the wordcontext matrix consists of rows of bidterms and the columns
    wordnet is available at http   wordnet princeton edu  

   

fiturney   pantel

 contexts  consist of advertiser identifiers  gleich   zhukov        or co bidded bidterms
 second order co occurrences   chang  pantel  popescu    gabrilovich        
information extraction  the field of information extraction  ie  includes named
entity recognition  ner  recognizing that a chunk of text is the name of an entity  such as
a person or a place   relation extraction  event extraction  and fact extraction  pasca et
al         demonstrate that a wordcontext frequency matrix can facilitate fact extraction 
vyas and pantel        propose a semi supervised model using a wordcontext matrix for
building and iteratively refining arbitrary classes of named entities 
    pairpattern matrices
pairpattern matrices are most suited to measuring the semantic similarity of word pairs
and patterns  see section       for example  we can measure the similarity of two word
pairs by the cosine of the angle between their corresponding row vectors in a pairpattern
matrix  there are many applications for measures of relational similarity 
relational similarity  just as we can measure attributional similarity by the cosine
of the angle between row vectors in a wordcontext matrix  we can measure relational
similarity by the cosine of the angle between rows in a pairpattern matrix  this approach
to measuring relational similarity was introduced by turney et al         and examined
in more detail by turney and littman         turney        evaluated this approach
to relational similarity with     multiple choice analogy questions from the sat college
entrance test  achieving human level performance      correct for the pairpattern matrix
and     correct for the average us college applicant   this is the highest performance so
far for an algorithm  the best algorithm based on attributional similarity has an accuracy
of only      turney         the best non vsm algorithm achieves      veale        
pattern similarity  instead of measuring the similarity between row vectors in a pair
pattern matrix  we can measure the similarity between columns  that is  we can measure
pattern similarity  lin and pantel        constructed a pairpattern matrix in which the
patterns were derived from parsed text  pattern similarity can be used to infer that one
phrase is a paraphrase of another phrase  which is useful for natural language generation 
text summarization  information retrieval  and question answering 
relational clustering  bicici and yuret        clustered word pairs by representing
them as row vectors in a pairpattern matrix  davidov and rappoport        first clustered
contexts  patterns  and then identified representative pairs for each context cluster  they
used the representative pairs to automatically generate multiple choice analogy questions 
in the style of sat analogy questions 
relational classification  chklovski and pantel        used a pairpattern matrix
to classify pairs of verbs into semantic classes  for example  taint   poison is classified as
strength  poisoning is stronger than tainting  and assess   review is classified as enablement
 assessing is enabled by reviewing   turney        used a pairpattern matrix to classify
noun compounds into semantic classes  for example  flu virus is classified as cause  the
virus causes the flu   home town is classified as location  the home is located in the town  
and weather report is classified as topic  the topic of the report is the weather  
relational search  cafarella  banko  and etzioni        described relational search
as the task of searching for entities that satisfy given semantic relations  an example of
   

fifrom frequency to meaning

a query for a relational search engine is list all x such that x causes cancer  in this
example  the relation  cause  and one of the terms in the relation  cancer  are given by the
user  and the task of the search engine is to find terms that satisfy the users query  the
organizers of task   in semeval       girju  nakov  nastase  szpakowicz  turney    yuret 
      envisioned a two step approach to relational search  first a conventional search engine
would look for candidate answers  then a relational classification system would filter out
incorrect answers  the first step was manually simulated by the task   organizers and the
goal of task   was to design systems for the second step  this task attracted    teams who
submitted    systems  nakov and hearst        achieved good results using a pairpattern
matrix 
automatic thesaurus generation  we discussed automatic thesaurus generation in
section      with wordcontext matrices  but arguably relational similarity is more relevant
than attributional similarity for thesaurus generation  for example  most of the information in wordnet is in the relations between the words rather than in the words individually 
snow  jurafsky  and ng        used a pairpattern matrix to build a hypernym hyponym
taxonomy  whereas pennacchiotti and pantel        built a meronymy and causation taxonomy  turney      b  showed how a pairpattern matrix can distinguish synonyms from
antonyms  synonyms from non synonyms  and taxonomically similar words  hair and fur 
from words that are merely semantically associated  cradle and baby  
analogical mapping  proportional analogies have the form a   b    c   d  which means a
is to b as c is to d  for example  mason   stone    carpenter   wood means mason is to stone
as carpenter is to wood  the     multiple choice analogy questions from the sat college
entrance test  mentioned above  all involve proportional analogies  with a pairpattern
matrix  we can solve proportional analogies by selecting the choice that maximizes relational
similarity  e g   simr  mason   stone  carpenter   wood  has a high value   however  we often
encounter analogies that involve more than four terms  the well known analogy between
the solar system and the rutherford bohr model of the atom contains at least fourteen
terms  for the solar system  we have planet  attracts  revolves  sun  gravity  solar system 
and mass  for the atom  we have revolves  atom  attracts  electromagnetism  nucleus 
charge  and electron  turney      a  demonstrated that we can handle these more complex 
systematic analogies by decomposing them into sets of proportional analogies 

   alternative approaches to semantics
the applications that we list in section   do not necessarily require a vsm approach  for
each application  there are many other possible approaches  in this section  we briefly
consider a few of the main alternatives 
underlying the applications for termdocument matrices  section      is the task of
measuring the semantic similarity of documents and queries  the main alternatives to
vsms for this task are probabilistic models  such as the traditional probabilistic retrieval
models in information retrieval  van rijsbergen        baeza yates   ribeiro neto       
and the more recent statistical language models inspired by information theory  liu  
croft         the idea of statistical language models for information retrieval is to measure
the similarity between a query and a document by creating a probabilistic language model
   

fiturney   pantel

of the given document and then measuring the probability of the given query according to
the language model 
with progress in information retrieval  the distinction between the vsm approach and
the probabilistic approach is becoming blurred  as each approach borrows ideas from the
other  language models typically involve multiplying probabilities  but we can view this as
adding logs of probabilities  which makes some language models look similar to vsms 
the applications for wordcontext matrices  section      share the task of measuring the
semantic similarity of words  the main alternatives to vsms for measuring word similarity
are approaches that use lexicons  such as wordnet  resnik        jiang   conrath       
hirst   st onge        leacock   chodrow        budanitsky   hirst         the idea is
to view the lexicon as a graph  in which nodes correspond to word senses and edges represent
relations between words  such as hypernymy and hyponymy  the similarity between two
words is then proportional to the length of the path in the graph that joins the two words 
several approaches to measuring the semantic similarity of words combine a vsm with
a lexicon  turney et al         pantel        patwardhan   pedersen        mohammad  
hirst         humans use both dictionary definitions and observations of word usage  so it
is natural to expect the best performance from algorithms that use both distributional and
lexical information 
pairpattern matrices  section      have in common the task of measuring the semantic
similarity of relations  as with wordcontext matrices  the main alternatives are approaches
that use lexicons  rosario   hearst        rosario  hearst    fillmore        nastase
  szpakowicz        veale               the idea is to reduce relational similarity to
attributional similarity  simr  a   b  c   d   sima  a  c    sima  b  d   and then use a lexicon
to measure attributional similarity  as we discuss in section      this reduction does not
work in general  however  the reduction is often a good approximation  and there is some
evidence that a hybrid approach  combining a vsm with a lexicon  can be beneficial  turney
et al         nastase  sayyad shirabad  sokolova    szpakowicz        

   the future of vector space models of semantics
several authors have criticized vsms  french   labiouse        pado   lapata       
morris   hirst        budanitsky   hirst         most of the criticism stems from the
fact that termdocument and wordcontext matrices typically ignore word order  in lsa 
for instance  a phrase is commonly represented by the sum of the vectors for the individual
words in the phrase  hence the phrases house boat and boat house will be represented by
the same vector  although they have different meanings  in english  word order expresses
relational information  both house boat and boat house have a tool purpose relation  but
house boat means tool purpose boat  house   a boat that serves as a house   whereas boat
house means tool purpose house  boat   a house for sheltering and storing boats  
landauer        estimates that     of the meaning of english text comes from word
choice and the remaining     comes from word order  however  vsms are not inherently
limited to     of the meaning of text  mitchell and lapata        propose composition
models sensitive to word order  for example  to make a simple additive model become
syntax aware  they allow for different weightings of the contributions of the vector components  constituents that are more important to the composition therefore can participate
   

fifrom frequency to meaning

more actively  clark and pulman        assigned distributional meaning to sentences using the hilbert space tensor product  widdows and ferraro         inspired by quantum
mechanics  explores several operators for modeling composition of meaning  pairpattern
matrices are sensitive to the order of the words in a pair  turney         thus there are
several ways to handle word order in vsms 
this raises the question  what are the limits of vsms for semantics  can all semantics
be represented with vsms  there is much that we do not yet know how to represent with
vsms  for example  widdows        and van rijsbergen        show how disjunction 
conjunction  and negation can be represented with vectors  but we do not yet know how to
represent arbitrary statements in first order predicate calculus  however  it seems possible
that future work may discover answers to these limitations 
in this survey  we have assumed that vsms are composed of elements with values that
are derived from event frequencies  this ties vsms to some form of distributional hypothesis
 see sections     and       therefore the limits of vsms depend on the limits of the family
of distributional hypotheses  are statistical patterns of word usage sufficient to figure out
what people mean  this is arguably the major open question of vsms  and the answer will
determine the future of vsms  we do not have a strong argument one way or the other 
but we believe that the continuing progress with vsms suggests we are far from reaching
their limits 

   conclusions
when we want information or help from a person  we use words to make a request or
describe a problem  and the person replies with words  unfortunately  computers do not
understand human language  so we are forced to use artificial languages and unnatural user
interfaces  in science fiction  we dream of computers that understand human language  that
can listen to us and talk with us  to achieve the full potential of computers  we must enable
them to understand the semantics of natural language  vsms are likely to be part of the
solution to the problem of computing semantics 
many researchers who have struggled with the problem of semantics have come to the
conclusion that the meaning of words is closely connected to the statistics of word usage
 section       when we try to make this intuition precise  we soon find we are working with
vectors of values derived from event frequencies  that is  we are dealing with vsms 
in this survey  we have organized past work with vsms according to the structure of
the matrix  termdocument  wordcontext  or pairpattern   we believe that the structure
of the matrix is the most important factor in determining the types of applications that
are possible  the linguistic processing  section    and mathematical processing  section   
play smaller  but important  roles 
our goal in this survey has been to show the breadth and power of vsms  to introduce
vsms to those who less familiar with them  and to provide a new perspective on vsms to
those who are already familiar with them  we hope that our emphasis on the structure of
the matrix will inspire new research  there is no reason to believe that the three matrix
types we present here exhaust the possibilities  we expect new matrix types and new tensors
will open up more applications for vsms  it seems possible to us that all of the semantics
of human language might one day be captured in some kind of vsm 
   

fiturney   pantel

acknowledgments
thanks to annie zaenen for prompting this paper  thanks to saif mohammad and mariana
soffer for their comments  thanks to arkady borkovsky and eric crestan for developing
the distributed sparse matrix multiplication algorithm  and to marco pennacchiotti for his
invaluable comments  thanks to the anonymous reviewers of jair for their very helpful
comments and suggestions 

references
acar  e     yener  b          unsupervised multiway data analysis  a literature survey 
ieee transactions on knowledge and data engineering              
agirre  e     edmonds  p  g          word sense disambiguation  algorithms and applications  springer 
ando  r  k          latent semantic space  iterative scaling improves precision of interdocument similarity measurement  in proceedings of the   rd annual acm sigir
conference on research and development in information retrieval  sigir        pp 
       
ando  r  k     zhang  t          a framework for learning predictive structures from
multiple tasks and unlabeled data  journal of machine learning research         
     
baeza yates  r     ribeiro neto  b          modern information retrieval  addison wesley 
barr  c   jones  r     regelson  m          the linguistic structure of english websearch queries  in conference on empirical methods in natural language processing
 emnlp  
bayardo  r  j   ma  y     srikant  r          scaling up all pairs similarity search  in
proceedings of the   th international conference on world wide web  www     
pp          new york  ny  acm 
bicici  e     yuret  d          clustering word pairs to answer analogy questions  in
proceedings of the fifteenth turkish symposium on artificial intelligence and neural
networks  tainn        akyaka  mugla  turkey 
blei  d  m   ng  a  y     jordan  m  i          latent dirichlet allocation  journal of
machine learning research             
brand  m          fast low rank modifications of the thin singular value decomposition 
linear algebra and its applications                
breese  j   heckerman  d     kadie  c          empirical analysis of predictive algorithms
for collaborative filtering  in proceedings of the   th conference on uncertainty in
artificial intelligence  pp        morgan kaufmann 
brin  s     page  l          the anatomy of a large scale hypertextual web search engine 
in proceedings of the seventh world wide web conference  www    pp         
broder  a          on the resemblance and containment of documents  in in compression
and complexity of sequences  sequences     pp        ieee computer society 
   

fifrom frequency to meaning

budanitsky  a     hirst  g          semantic distance in wordnet  an experimental 
application oriented evaluation of five measures  in proceedings of the workshop
on wordnet and other lexical resources  second meeting of the north american
chapter of the association for computational linguistics  naacl        pp       
pittsburgh  pa 
budanitsky  a     hirst  g          evaluating wordnet based measures of semantic distance  computational linguistics               
bullinaria  j     levy  j          extracting semantic representations from word cooccurrence statistics  a computational study  behavior research methods         
       
buntine  w     jakulin  a          discrete component analysis  in subspace  latent
structure and feature selection  statistical and optimization perspectives workshop
at slsfs       pp       bohinj  slovenia  springer 
cafarella  m  j   banko  m     etzioni  o          relational web search  tech  rep   university of washington  department of computer science and engineering  technical
report            
cao  g   nie  j  y     bai  j          integrating word relationships into language models 
in proceedings of the   th annual international acm sigir conference on research
and development in information retrieval  sigir      pp          new york  ny 
acm 
cao  h   jiang  d   pei  j   he  q   liao  z   chen  e     li  h          context aware query
suggestion by mining click through and session data  in proceeding of the   th acm
sigkdd international conference on knowledge discovery and data mining  kdd
     pp          acm 
carroll  j  d     chang  j  j          analysis of individual differences in multidimensional
scaling via an n way generalization of eckart young decomposition  psychometrika 
               
chang  w   pantel  p   popescu  a  m     gabrilovich  e          towards intent driven
bidterm suggestion  in proceedings of www     short paper   madrid  spain 
charikar  m  s          similarity estimation techniques from rounding algorithms  in proceedings of the thiry fourth annual acm symposium on theory of computing  stoc
     pp          acm 
chew  p   bader  b   kolda  t     abdelali  a          cross language information retrieval using parafac   in proceedings of the   th acm sigkdd international
conference on knowledge discovery and data mining  kdd     pp          acm
press 
chiarello  c   burgess  c   richards  l     pollock  a          semantic and associative
priming in the cerebral hemispheres  some words do  some words dont       sometimes 
some places  brain and language            
chklovski  t     pantel  p          verbocean  mining the web for fine grained semantic
verb relations  in proceedings of experimental methods in natural language processing       emnlp      pp        barcelona  spain 
   

fiturney   pantel

choi  f  y  y          advances in domain independent linear text segmentation  in
proceedings of the  st meeting of the north american chapter of the association for
computational linguistics  pp       
chu carroll  j     carpenter  b          vector based natural language call routing  computational linguistics                 
church  k          one term or two   in proceedings of the   th annual international
acm sigir conference on research and development in information retrieval  pp 
       
church  k     hanks  p          word association norms  mutual information  and lexicography  in proceedings of the   th annual conference of the association of computational linguistics  pp        vancouver  british columbia 
clark  s     pulman  s          combining symbolic and distributional models of meaning 
in proceedings of aaai spring symposium on quantum interaction  pp       
collobert  r     weston  j          a unified architecture for natural language processing 
deep neural networks with multitask learning  in proceedings of the   th international
conference on machine learning  icml      pp         
croft  w  b          clustering large files of documents using the single link method 
journal of the american society for information science                 
crouch  c  j          a cluster based approach to thesaurus construction  in proceedings
of the   th annual international acm sigir conference  pp          grenoble 
france 
curran  j  r     moens  m          improvements in automatic thesaurus extraction  in
unsupervised lexical acquisition  proceedings of the workshop of the acl special
interest group on the lexicon  siglex   pp        philadelphia  pa 
cutting  d  r   karger  d  r   pedersen  j  o     tukey  j  w          scatter gather  a
cluster based approach to browsing large document collections  in proceedings of the
  th annual international acm sigir conference  pp         
dagan  i   lee  l     pereira  f  c  n          similarity based models of word cooccurrence
probabilities  machine learning                
dang  h  t   lin  j     kelly  d          overview of the trec      question answering
track  in proceedings of the fifteenth text retrieval conference  trec       
dasarathy  b          nearest neighbor  nn  norms  nn pattern classification techniques 
ieee computer society press 
davidov  d     rappoport  a          unsupervised discovery of generic relationships using
pattern clusters and its evaluation by automatically generated sat analogy questions 
in proceedings of the   th annual meeting of the acl and hlt  acl hlt      pp 
        columbus  ohio 
dean  j     ghemawat  s          mapreduce  simplified data processing on large clusters 
communications of the acm                 
   

fifrom frequency to meaning

deerwester  s  c   dumais  s  t   landauer  t  k   furnas  g  w     harshman  r  a 
        indexing by latent semantic analysis  journal of the american society for
information science  jasis                  
elsayed  t   lin  j     oard  d          pairwise document similarity in large collections
with mapreduce  in proceedings of association for computational linguistics and
human language technology conference       acl     hlt   short papers  pp 
        columbus  ohio  association for computational linguistics 
erk  k          a simple  similarity based model for selectional preferences  in proceedings
of the   th annual meeting of the association of computational linguistics  pp     
      prague  czech republic 
erk  k     pado  s          a structured vector space model for word meaning in context 
in proceedings of the      conference on empirical methods in natural language
processing  emnlp      pp          honolulu  hi 
fellbaum  c   ed            wordnet  an electronic lexical database  mit press 
firth  j  r          a synopsis of linguistic theory           in studies in linguistic
analysis  pp       blackwell  oxford 
foltz  p  w   laham  d     landauer  t  k          the intelligent essay assessor  applications to educational technology  interactive multimedia electronic journal of
computer enhanced learning        
forman  g          an extensive empirical study of feature selection metrics for text classification  journal of machine learning research              
french  r  m     labiouse  c          four problems with extracting human semantics from
large text corpora  in proceedings of the   th annual conference of the cognitive
science society 
furnas  g  w   landauer  t  k   gomez  l  m     dumais  s  t          statistical semantics  analysis of the potential performance of keyword information systems  bell
system technical journal                   
gentner  d          structure mapping  a theoretical framework for analogy  cognitive
science                
gilbert  j  r   moler  c     schreiber  r          sparse matrices in matlab  design and
implementation  siam journal on matrix analysis and applications                 
girju  r   nakov  p   nastase  v   szpakowicz  s   turney  p     yuret  d          semeval     task     classification of semantic relations between nominals  in proceedings
of the fourth international workshop on semantic evaluations  semeval        pp 
      prague  czech republic 
gleich  d     zhukov  l          svd based term suggestion and ranking system  in
proceedings of the fourth ieee international conference on data mining  icdm
     pp          ieee computer society 
golub  g  h     van loan  c  f          matrix computations  third edition   johns
hopkins university press  baltimore  md 
   

fiturney   pantel

gorman  j     curran  j  r          scaling distributional similarity to large corpora  in
proceedings of the   st international conference on computational linguistics and
the   th annual meeting of the association for computational linguistics  acl       
pp          association for computational linguistics 
gorrell  g          generalized hebbian algorithm for incremental singular value decomposition in natural language processing  in proceedings of the   th conference of the
european chapter of the association for computational linguistics  eacl      pp 
      
gospodnetic  o     hatcher  e          lucene in action  manning publications 
grefenstette  g          explorations in automatic thesaurus discovery  kluwer 
harris  z          distributional structure  word                  
harshman  r          foundations of the parafac procedure  models and conditions for an
explanatory multi modal factor analysis  ucla working papers in phonetics     
hearst  m          texttiling  segmenting text into multi paragraph subtopic passages 
computational linguistics               
hirst  g     st onge  d          lexical chains as representations of context for the detection
and correction of malapropisms  in fellbaum  c   ed    wordnet  an electronic
lexical database  pp          mit press 
hofmann  t          probabilistic latent semantic indexing  in proceedings of the   nd
annual acm conference on research and development in information retrieval  sigir      pp        berkeley  california 
huang  c  k   chien  l  f     oyang  y  j          relevant term suggestion in interactive
web search based on contextual information in query session logs  journal of the
american society for information science and technology                 
hull  d          stemming algorithms  a case study for detailed evaluation  journal of the
american society for information science               
jain  a   murty  n     flynn  p          data clustering  a review  acm computing
surveys                 
jarmasz  m     szpakowicz  s          rogets thesaurus and semantic similarity  in
proceedings of the international conference on recent advances in natural language
processing  ranlp      pp          borovets  bulgaria 
jiang  j  j     conrath  d  w          semantic similarity based on corpus statistics
and lexical taxonomy  in proceedings of the international conference on research in
computational linguistics  rocling x   pp        tapei  taiwan 
johnson  h     martin  j          unsupervised learning of morphology for english and
inuktitut  in proceedings of hlt naacl       pp       
jones  m  p     martin  j  h          contextual spelling correction using latent semantic analysis  in proceedings of the fifth conference on applied natural language
processing  pp          washington  dc 
   

fifrom frequency to meaning

jones  r   rey  b   madani  o     greiner  w          generating query substitutions  in
proceedings of the   th international conference on world wide web  www     
pp          new york  ny  acm 
jones  w  p     furnas  g  w          pictures of relevance  a geometric analysis of
similarity measures  journal of the american society for information science         
       
kanerva  p          sparse distributed memory and related models  in hassoun  m  h 
 ed    associative neural memories  pp        oxford university press  new york 
ny 
karlgren  j     sahlgren  m          from words to understanding  in uesaka  y   kanerva 
p     asoh  h   eds    foundations of real world intelligence  pp          csli
publications 
kim  s  m   pantel  p   chklovski  t     pennacchiotti  m          automatically assessing
review helpfulness  in proceedings of the      conference on empirical methods in
natural language processing  pp         
kolda  t     bader  b          tensor decompositions and applications  siam review 
               
konchady  m          building search applications  lucene  lingpipe  and gate  mustru
publishing 
kraaij  w     pohlmann  r          viewing stemming as recall enhancement  in proceedings of the   th annual international acm sigir conference  pp       
lakoff  g          women  fire  and dangerous things  university of chicago press 
chicago  il 
landauer  t  k          on the computational basis of learning and cognition  arguments
from lsa  in ross  b  h   ed    the psychology of learning and motivation  advances
in research and theory  vol      pp        academic press 
landauer  t  k     dumais  s  t          a solution to platos problem  the latent semantic analysis theory of the acquisition  induction  and representation of knowledge 
psychological review                  
landauer  t  k     littman  m  l          fully automatic cross language document
retrieval using latent semantic indexing  in proceedings of the sixth annual conference
of the uw centre for the new oxford english dictionary and text research  pp    
    waterloo  ontario 
landauer  t  k   mcnamara  d  s   dennis  s     kintsch  w          handbook of latent
semantic analysis  lawrence erlbaum  mahwah  nj 
lavrenko  v     croft  w  b          relevance based language models  in proceedings of
the   th annual international acm sigir conference on research and development
in information retrieval  sigir      pp          new york  ny  acm 
leacock  c     chodrow  m          combining local context and wordnet similarity for
word sense identification  in fellbaum  c   ed    wordnet  an electronic lexical
database  mit press 
   

fiturney   pantel

leacock  c   towell  g     voorhees  e          corpus based statistical sense resolution 
in proceedings of the arpa workshop on human language technology  pp         
lee  d  d     seung  h  s          learning the parts of objects by nonnegative matrix
factorization  nature              
lee  l          measures of distributional similarity  in proceedings of the   th annual
meeting of the association for computational linguistics  pp       
lemaire  b     denhiere  g          effects of high order co occurrences on word semantic
similarity  current psychology letters  behaviour  brain   cognition         
lin  d          automatic retrieval and clustering of similar words  in roceedings of the
  th international conference on computational linguistics  pp          association
for computational linguistics 
lin  d     pantel  p          dirt  discovery of inference rules from text  in proceedings
of acm sigkdd conference on knowledge discovery and data mining       pp 
       
linden  g   smith  b     york  j          amazon com recommendations  item to item
collaborative filtering  ieee internet computing       
liu  x     croft  w  b          statistical language modeling for information retrieval 
annual review of information science and technology          
lovins  j  b          development of a stemming algorithm  mechanical translation and
computational linguistics           
lowe  w          towards a theory of semantic space  in proceedings of the twenty first
annual conference of the cognitive science society  pp         
lund  k     burgess  c          producing high dimensional semantic spaces from lexical
co occurrence  behavior research methods  instruments  and computers             
    
lund  k   burgess  c     atchley  r  a          semantic and associative priming in highdimensional semantic space  in proceedings of the   th annual conference of the
cognitive science society  pp         
manning  c     schutze  h          foundations of statistical natural language processing 
mit press  cambridge  ma 
manning  c  d   raghavan  p     schutze  h          introduction to information retrieval 
cambridge university press  cambridge  uk 
miller  g   leacock  c   tengi  r     bunker  r          a semantic concordance  in
proceedings of the  rd darpa workshop on human language technology  pp     
    
minnen  g   carroll  j     pearce  d          applied morphological processing of english 
natural language engineering                
mitchell  j     lapata  m          vector based models of semantic composition  in proceedings of acl     hlt  pp          columbus  ohio  association for computational
linguistics 
   

fifrom frequency to meaning

mitchell  t          machine learning  mcgraw hill  columbus  oh 
mohammad  s     hirst  g          distributional measures of concept distance  a taskoriented evaluation  in proceedings of the conference on empirical methods in natural
language processing  emnlp        pp       
monay  f     gatica perez  d          on image auto annotation with latent space models 
in proceedings of the eleventh acm international conference on multimedia  pp 
       
morris  j     hirst  g          non classical lexical semantic relations  in workshop on
computational lexical semantics  hlt naacl     boston  ma 
nakov  p     hearst  m          ucb  system description for semeval task    in proceedings of the fourth international workshop on semantic evaluations  semeval       
pp          prague  czech republic 
nakov  p     hearst  m          solving relational similarity problems using theweb as a
corpus  in proceedings of acl     hlt  pp          columbus  ohio 
nastase  v   sayyad shirabad  j   sokolova  m     szpakowicz  s          learning nounmodifier semantic relations with corpus based and wordnet based features  in proceedings of the   st national conference on artificial intelligence  aaai      pp 
       
nastase  v     szpakowicz  s          exploring noun modifier semantic relations  in
fifth international workshop on computational semantics  iwcs     pp         
tilburg  the netherlands 
niwa  y     nitta  y          co occurrence vectors from corpora vs  distance vectors from
dictionaries  in proceedings of the   th international conference on computational
linguistics  pp          kyoto  japan 
nosofsky  r          attention  similarity  and the identification categorization relationship 
journal of experimental psychology  general                
ogden  c  k          basic english  a general introduction with rules and grammar 
kegan paul  trench  trubner and co 
pado  s     lapata  m          constructing semantic space models from parsed corpora 
in proceedings of the   st annual meeting of the association for computational linguistics  pp          sapporo  japan 
pado  s     lapata  m          dependency based construction of semantic space models 
computational linguistics                 
pang  b   lee  l     vaithyanathan  s          thumbs up  sentiment classification using
machine learning techniques  in proceedings of the conference on empirical methods
in natural language processing  emnlp   pp        philadelphia  pa 
pantel  p          inducing ontological co occurrence vectors  in proceedings of association
for computational linguistics  acl      pp         
pantel  p     lin  d          spamcop  a spam classification and organization program  in
learning for text categorization  papers from the aaai      workshop  pp       
   

fiturney   pantel

pantel  p     lin  d       a   discovering word senses from text  in proceedings of the
eighth acm sigkdd international conference on knowledge discovery and data
mining  pp          edmonton  canada 
pantel  p     lin  d       b   document clustering with committees  in proceedings of the
  th annual international acm sigir conference  pp         
pasca  m   lin  d   bigham  j   lifchits  a     jain  a          names and similarities on
the web  fact extraction in the fast lane  in proceedings of the   st international
conference on computational linguistics and   th annual meeting of the acl  pp 
        sydney  australia 
patwardhan  s     pedersen  t          using wordnet based context vectors to estimate
the semantic relatedness of concepts  in proceedings of the workshop on making
sense of sense at the   th conference of the european chapter of the association for
computational linguistics  eacl        pp     
pedersen  t          unsupervised corpus based methods for wsd  in word sense disambiguation  algorithms and applications  pp          springer 
pennacchiotti  m   cao  d  d   basili  r   croce  d     roth  m          automatic induction
of framenet lexical units  in proceedings of the      conference on empirical methods
in natural language processing  emnlp      pp          honolulu  hawaii 
pennacchiotti  m     pantel  p          ontologizing semantic relations  in proceedings of
the   st international conference on computational linguistics and the   th annual
meeting of the association for computational linguistics  pp          association
for computational linguistics 
pereira  f   tishby  n     lee  l          distributional clustering of english words  in
proceedings of the   st annual meeting on association for computational linguistics 
pp         
porter  m          an algorithm for suffix stripping  program                 
rabin  m  o          fingerprinting by random polynomials  tech  rep   center for research
in computing technology  harvard university  technical report tr       
rapp  r          word sense discovery based on sense descriptor dissimilarity  in proceedings of the ninth machine translation summit  pp         
ravichandran  d   pantel  p     hovy  e          randomized algorithms and nlp  using
locality sensitive hash function for high speed noun clustering  in proceedings of the
  rd annual meeting of the association for computational linguistics  acl      pp 
        morristown  nj  association for computational linguistics 
resnick  p   iacovou  n   suchak  m   bergstrom  p     riedl  j          grouplens  an open
architecture for collaborative filtering of netnews  in proceedings of the acm     
conference on computer supported cooperative work  pp          acm press 
resnik  p          using information content to evaluate semantic similarity in a taxonomy 
in proceedings of the   th international joint conference on artificial intelligence
 ijcai      pp          san mateo  ca  morgan kaufmann 
   

fifrom frequency to meaning

rosario  b     hearst  m          classifying the semantic relations in noun compounds
via a domain specific lexical hierarchy  in proceedings of the      conference on
empirical methods in natural language processing  emnlp      pp       
rosario  b   hearst  m     fillmore  c          the descent of hierarchy  and selection in
relational semantics  in proceedings of the   th annual meeting of the association
for computational linguistics  acl      pp         
rosch  e     lloyd  b          cognition and categorization  lawrence erlbaum  hillsdale 
nj 
ruge  g          automatic detection of thesaurus relations for information retrieval applications  in freksa  c   jantzen  m     valk  r   eds    foundations of computer
science  pp          springer 
sahami  m   dumais  s   heckerman  d     horvitz  e          a bayesian approach to
filtering junk e mail  in proceedings of the aaai    workshop on learning for text
categorization 
sahlgren  m          an introduction to random indexing  in proceedings of the methods
and applications of semantic indexing workshop at the  th international conference
on terminology and knowledge engineering  tke   copenhagen  denmark 
sahlgren  m          the word space model  using distributional analysis to represent syntagmatic and paradigmatic relations between words in high dimensional vector spaces 
ph d  thesis  department of linguistics  stockholm university 
salton  g          the smart retrieval system  experiments in automatic document processing  prentice hall  upper saddle river  nj 
salton  g     buckley  c          term weighting approaches in automatic text retrieval 
information processing and management                 
salton  g   wong  a     yang  c  s          a vector space model for automatic indexing 
communications of the acm                  
sarawagi  s     kirpal  a          efficient set joins on similarity predicates  in proceedings of the      acm sigmod international conference on management of data
 sigmod      pp          new york  ny  acm 
scholkopf  b   smola  a  j     muller  k  r          kernel principal component analysis  in
proceedings of the international conference on artificial neural networks  icann       pp          berlin 
schutze  h          automatic word sense discrimination  computational linguistics         
      
schutze  h     pedersen  j          a vector model for syntagmatic and paradigmatic
relatedness  in making sense of words  proceedings of the conference  pp         
oxford  england 
sebastiani  f          machine learning in automated text categorization  acm computing
surveys  csur               
shannon  c          a mathematical theory of communication  bell system technical
journal                     
   

fiturney   pantel

singhal  a   salton  g   mitra  m     buckley  c          document length normalization 
information processing and management                 
smith  e   osherson  d   rips  l     keane  m          combining prototypes  a selective
modification model  cognitive science                 
snow  r   jurafsky  d     ng  a  y          semantic taxonomy induction from heterogenous evidence  in proceedings of the   st international conference on computational
linguistics and the   th annual meeting of the acl  pp         
sparck jones  k          a statistical interpretation of term specificity and its application
in retrieval  journal of documentation               
spearman  c          general intelligence  objectively determined and measured  american journal of psychology             
sproat  r     emerson  t          the first international chinese word segmentation bakeoff  in proceedings of the second sighan workshop on chinese language processing 
pp          sapporo  japan 
stone  p  j   dunphy  d  c   smith  m  s     ogilvie  d  m          the general inquirer 
a computer approach to content analysis  mit press  cambridge  ma 
tan  b     peng  f          unsupervised query segmentation using generative language
models and wikipedia  in proceeding of the   th international conference on world
wide web  www      pp          new york  ny  acm 
tellex  s   katz  b   lin  j   fern  a     marton  g          quantitative evaluation of
passage retrieval algorithms for question answering  in proceedings of the   th annual
international acm sigir conference on research and development in information
retrieval  sigir   pp       
tucker  l  r          some mathematical notes on three mode factor analysis  psychometrika                 
turney  p  d          mining the web for synonyms  pmi ir versus lsa on toefl  in
proceedings of the twelfth european conference on machine learning  ecml     
pp          freiburg  germany 
turney  p  d          measuring semantic similarity by latent relational analysis  in proceedings of the nineteenth international joint conference on artificial intelligence
 ijcai      pp            edinburgh  scotland 
turney  p  d          similarity of semantic relations  computational linguistics         
       
turney  p  d          empirical evaluation of four tensor decomposition algorithms  tech 
rep   institute for information technology  national research council of canada 
technical report erb      
turney  p  d       a   the latent relation mapping engine  algorithm and experiments 
journal of artificial intelligence research             
turney  p  d       b   a uniform approach to analogies  synonyms  antonyms  and associations  in proceedings of the   nd international conference on computational
linguistics  coling        pp          manchester  uk 
   

fifrom frequency to meaning

turney  p  d     littman  m  l          measuring praise and criticism  inference of
semantic orientation from association  acm transactions on information systems 
               
turney  p  d     littman  m  l          corpus based learning of analogies and semantic
relations  machine learning                  
turney  p  d   littman  m  l   bigham  j     shnayder  v          combining independent
modules to solve multiple choice synonym and analogy problems  in proceedings of
the international conference on recent advances in natural language processing
 ranlp      pp          borovets  bulgaria 
van de cruys  t          a non negative tensor factorization model for selectional preference
induction  in proceedings of the workshop on geometric models for natural language
semantics  gems      pp        athens  greece 
van rijsbergen  c  j          the geometry of information retrieval  cambridge university
press  cambridge  uk 
van rijsbergen  c  j          information retrieval  butterworths 
veale  t          the analogical thesaurus  in proceedings of the   th innovative applications of artificial intelligence conference  iaai        pp          acapulco 
mexico 
veale  t          wordnet sits the sat  a knowledge based approach to lexical analogy  in
proceedings of the   th european conference on artificial intelligence  ecai       
pp          valencia  spain 
vozalis  e     margaritis  k          analysis of recommender systems algorithms  in
proceedings of the  th hellenic european conference on computer mathematics and
its applications  hercma        athens  greece 
vyas  v     pantel  p          semi automatic entity set refinement  in proceedings of
naacl     boulder  co 
weaver  w          translation  in locke  w     booth  d   eds    machine translation
of languages  fourteen essays  mit press  cambridge  ma 
weeds  j   weir  d     mccarthy  d          characterising measures of lexical distributional similarity  in proceedings of the   th international conference on computational linguistics  coling      pp            association for computational
linguistics 
wei  x   peng  f     dumoulin  b          analyzing web text association to disambiguate
abbreviation in queries  in proceedings of the   st annual international acm sigir
conference on research and development in information retrieval  sigir      pp 
        new york  ny  acm 
wen  j  r   nie  j  y     zhang  h  j          clustering user queries of a search engine  in
proceedings of the   th international conference on world wide web  www     
pp          new york  ny  acm 
widdows  d          geometry and meaning  center for the study of language and
information  stanford  ca 
   

fiturney   pantel

widdows  d     ferraro  k          semantic vectors  a scalable open source package and
online technology management application  in proceedings of the sixth international
conference on language resources and evaluation  lrec        pp           
witten  i  h     frank  e          data mining  practical machine learning tools and
techniques with java implementations  morgan kaufmann  san francisco 
wittgenstein  l          philosophical investigations  blackwell  translated by g e m 
anscombe 
wolfe  m  b  w   schreiner  m  e   rehder  b   laham  d   foltz  p  w   kintsch  w    
landauer  t  k          learning from text  matching readers and texts by latent
semantic analysis  discourse processes             
yang  y          an evaluation of statistical approaches to text categorization  information
retrieval              
yuret  d     yatbaz  m  a          the noisy channel model for unsupervised word sense
disambiguation  computational linguistics  under review 
zamir  o     etzioni  o          grouper  a dynamic clustering interface to web search
results  computer networks  the international journal of computer and telecommunications networking                    
zhao  y     karypis  g          evaluation of hierarchical clustering algorithms for document datasets  in proceedings of the eleventh international conference on information and knowledge management  pp          mclean  virginia 

   

fi