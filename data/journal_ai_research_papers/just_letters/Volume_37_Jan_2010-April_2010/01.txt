journal of artificial intelligence research                

submitted        published      

predicting the performance of ida  using
conditional distributions
uzi zahavi

zahaviu cs biu ac il

computer science department
bar ilan university  israel

ariel felner

felner bgu ac il

department of information systems engineering
deutsche telekom labs
ben gurion university  israel

neil burch

burch cs ualberta ca

computing science department
university of alberta  canada

robert c  holte

holte cs ualberta ca

computing science department
university of alberta  canada

abstract
korf  reid  and edelkamp introduced a formula to predict the number of nodes ida 
will expand on a single iteration for a given consistent heuristic  and experimentally demonstrated that it could make very accurate predictions  in this paper we show that  in addition to requiring the heuristic to be consistent  their formulas predictions are accurate
only at levels of the brute force search tree where the heuristic values obey the unconditional distribution that they defined and then used in their formula  we then propose a
new formula that works well without these requirements  i e   it can make accurate predictions of ida s performance for inconsistent heuristics and if the heuristic values in any
level do not obey the unconditional distribution  in order to achieve this we introduce the
conditional distribution of heuristic values which is a generalization of their unconditional
heuristic distribution  we also provide extensions of our formula that handle individual
start states and the augmentation of ida  with bidirectional pathmax  bpmx   a technique for propagating heuristic values when inconsistent heuristics are used  experimental
results demonstrate the accuracy of our new method and all its variations 

   introduction and overview
heuristic search algorithms such as a   hart  nilsson    raphael        and ida   korf 
      are guided by the cost function f  n    g n    h n   where g n  is the actual distance
from the start state to state n and h n  is a heuristic function estimating the cost from n to
the nearest goal state  a heuristic h is admissible if h n   dist n  goal  for every state n
and goal state goal  where dist n  m  is the cost of a least cost path from n to m  if h n  is
admissible  i e  always returns a lower bound estimate of the optimal cost  these algorithms
are guaranteed to find an optimal path from the start state to a goal state if one exists 
c
    
ai access foundation  all rights reserved 

fizahavi  felner  burch    holte

an important question to ask is how many nodes will be expanded by these algorithms
to solve a given problem  a major advance in answering this question was the work done by
korf  reid  and edelkamp which introduced a formula to predict the number of nodes ida 
will expand  korf   reid        korf  reid    edelkamp         these papers  the formula
they present  and the predictions it makes  will all be referred to as kre in this paper  prior
to kre  the standard method for comparing two heuristic functions was to compare their
average values  with preference being given to the heuristic with the larger average  korf 
      korf   felner        felner  korf  meshulam    holte         kre made a substantial
improvement on this by characterizing the quality of a heuristic function by the distribution
of its values  they then developed the kre formula based on the heuristic distribution to
predict the number of nodes expanded by ida  when it is searching with a specific heuristic
and cost threshold  finally  they compared the predictions of their formula to the actual
number of nodes expanded by ida  for different thresholds on several benchmark search
spaces and showed that it gave virtually perfect predictions  this was a major advance in
the analysis of search algorithms and heuristics 
despite its impressive results  the kre formula has two main shortcomings  the first is
that kre assumes that in addition to being admissible the given heuristic is also consistent 
a heuristic h is consistent if for every pair of states  m and n  h m   h n   dist m  n   
when the heuristic is consistent  the heuristic values of a nodes children are thus constrained to be similar to the heuristic value of the node  a heuristic is inconsistent if it is not
consistent  i e  if for some pair of nodes m and n  h m   h n    dist m  n   inconsistency
allows a nodes children to have heuristic values that are arbitrarily larger or smaller than
the nodes own heuristic value  while the term inconsistency has a negative connotation as
something to be avoided  recent studies have shown that inconsistent heuristics are easy to
define in many search applications and can produce substantial performance improvements
 felner  zahavi  schaeffer    holte        zahavi  felner  schaeffer    sturtevant       
zahavi  felner  holte    schaeffer         for this reason  it is important to extend the
kre formula to accurately predict ida s performance on inconsistent heuristics  as such
heuristics are likely to become increasingly important in future applications 
the second shortcoming of the kre formula is that it works well only at levels of the
search tree where the heuristic distribution follows the equilibrium distribution  defined
below in section         this always holds at sufficiently deep levels of the search tree 
where the heuristic values converge to the equilibrium distribution  in addition  it will hold
at all levels when the heuristic values of the set of start states is distributed according to
the equilibrium distribution  however  as will be shown below  in section        the kre
formula can be very inaccurate at depths of practical interest on single start states and
on large sets of start states whose values are not distributed according to the equilibrium
distribution  in such cases  the heuristic values at the levels of the search tree that are
actually examined by ida  will not obey the equilibrium distribution and applying kre to
these cases will result in inaccurate predictions 
the main objective of this paper is to develop a formula to accurately predict the number
of nodes ida  will expand  for a given cost threshold  for any given heuristic and set of start
states  including those not currently covered by kre  to do this we first extend kres idea
   this is a general definition for any graph  in the case of undirected graphs we can write the consistency
definition as  h m   h n    dist m  n  

  

fipredicting the performance of ida  using conditional distributions

of a heuristic distribution  which is unconditional  to a conditional distribution  in which
the probability of a specific heuristic value is not constant  as in kre  but is conditioned
on certain local properties of the search space  our conditional distribution provides more
insights about the behavior of the heuristic values during search because it is informed
about when  in what context in the search tree  a specific heuristic value will be produced 
this allows for a better study of heuristic behavior 
based on the conditional distribution we develop a new formula  cdp  conditional distribution prediction   that predicts ida s performance on any set of start states  regardless of
how their heuristic values are distributed  and for any desired depth  not necessarily large 
whether the heuristic is consistent or not  cdp has a recursive structure and information
about the number of nodes is propagated from the root to the leaves of the search tree  in
all of our experiments cdps predictions are at least as accurate as kres  and cdp is much
more accurate for inconsistent heuristics or sets of start states that have non equilibrium
heuristic distributions  in its basic form  cdp is not particularly accurate on single start
states  we describe a simple extension that improves its accuracy in this setting  finally 
we adapt cdp to make predictions when ida  is augmented with the bidirectional pathmax
method  bpmx   felner et al          when inconsistent heuristics are being used  bpmx
is a useful addition to ida   it prunes many subtrees that would otherwise be explored 
thereby substantially reducing the number of nodes ida  expands 
throughout the paper we provide experimental results demonstrating the accuracy of
cdp in all of the above scenarios using the same two benchmark domains used in kre  the
sliding tile puzzle and rubiks cube 
for simplicity of discussion  we assume in this paper that all edges cost    this is
true for many problem domains  the generalization of the ideas to the case of variable edge
costs is straightforward  although their practical implementation introduces some additional
challenges  briefly described in section       
the paper is organized as follows  section   presents background material  section  
derives the kre formula from first principles and discusses its limitations  in section    our
notion of the conditional distribution of heuristic values is presented  our new formula  cdp 
is presented in section      section   discusses a subtle but important way in which our
experiments differ from kres  experimental results are presented in sections   and    the
extension of the cdp formula to better handle single start states is presented in section   
section   proposes a technique  based on cdp  for estimating upper and lower bounds on
the number of nodes ida  can expand for a given unconditional distribution  section   
presents an extension of cdp for predicting the performance of ida  when bpmx is applied 
related work is discussed in section     and conclusions and suggestions for future work are
given in section     a preliminary version of this paper appeared  zahavi  felner  burch 
  holte        

   background
two application domains were used by kre to demonstrate the accuracy of their formula 
in our experiments we use exactly the same domains  in this section we describe them
as well as the search algorithm and the different heuristic functions that are used in our
experiments 
  

fizahavi  felner  burch    holte

    problem domains
two of the classic examples in the ai literature of a single agent pathfinding problems are
rubiks cube and the sliding tile puzzle 
      rubiks cube

figure            rubiks cube
rubiks cube was invented in      by erno rubik of hungary  the standard version
consists of a         cube  figure     with different colored stickers on each of the exposed
squares of the sub cubes  or cubies  there are    movable cubies and   stable cubies in
the center of each face  the movable cubies can be divided into eight corner cubies  with
three faces each  and twelve edge cubies  with two faces each  corner cubies can only move
among corner positions  and edge cubies can only move among edge positions 
each one of the   faces of the cube can be rotated          or     degrees relative to the
rest of the cube  this results in    possible moves for each state  since twisting the same
face twice in a row is redundant  the branching factor after the first move can be reduced
to     in addition  movements of opposite faces are independent  for example  twisting the
left face and then the right face leads to the same state as performing the same moves in
the opposite order  pruning redundant moves results in a search tree with an asymptotic
branching factor of about           korf        
in the goal state  all the squares on each side of the cube are the same color  the puzzle
is scrambled by making a number of random moves  and the task is to restore the cube to
its original unscrambled state  there are about         different reachable states 
      the sliding tile puzzles
the sliding tile puzzle consists of a square frame containing a set of numbered square tiles 
and an empty position called the blank  the legal operators are to slide any tile that is
horizontally or vertically adjacent to the blank into the blank position  the problem is to
rearrange the tiles from some random initial configuration into a particular desired goal
configuration  the state space grows exponentially in size as the number of tiles increases 
and it has been shown that finding optimal solutions to the sliding tile problem is npcomplete  ratner   warmuth         the two most common versions of the sliding tile
puzzle are the        puzzle  and the         puzzle  the   puzzle contains               
  

fipredicting the performance of ida  using conditional distributions

 

 

 
 

 

 

 

 

 

 

 

 

 

 

     

 

 

 

           

figure    the   puzzle and    puzzle goal states
reachable states  and the    puzzle contains about      reachable states  the goal states of
these puzzles are shown in figure   
the classic heuristic function for the sliding tile puzzles is called the manhattan distance  it is computed by counting the number of grid units that each tile is displaced from
its goal position  and summing these values over all tiles  excluding the blank  since each
tile must move at least its manhattan distance to its goal position  and each move changes
the location of one tile by just one grid unit  the manhattan distance is a lower bound of
the minimum number of moves needed to solve a problem instance 
    iterative deepening a 
iterative deepening a   ida    korf        performs a series of depth first searches  increasing a cost threshold d each time  in the depth first search  all nodes n with f  n   d
are expanded  threshold d is initially set to h s   where s is the start node  if a goal is
found using the current threshold  the search ends successfully  otherwise  ida  proceeds
to the next iteration by increasing d to the minimum f value that exceeded d in the previous
iteration 
    pattern databases  pdbs 
a powerful approach for obtaining admissible heuristics is to create a simplified version  or
abstraction  of the given state space and then to use exact distances in the abstract space
as estimates of the distances in the original state space  the type of abstractions we use
in this paper for the sliding tile puzzles are illustrated in figure    the left side of the
figure shows a    puzzle state s and the goal state  the right side shows the corresponding
abstract states  which are defined by erasing the numbers on all the tiles except for        
and    to estimate the distance from s to the goal state in the    puzzle  we calculate the
exact distance from the abstract state corresponding to s to the abstract goal state 
a pattern database  pdb  is a lookup table that stores the distance to the abstract goal
of every abstract state  or pattern   culberson   schaeffer               a pdb is built
by running a breadth first search  backwards from the abstract goal until the whole abstract
space is spanned  to compute h s  for a state s in the original space  s is mapped to the
corresponding abstract state p and the distance to goal for p is looked up in the pdb 
   this description assumes all operators have the same cost  this technique can be easily extended to
cases where operators have different costs 

  

fizahavi  felner  burch    holte

the pdb lookup

state s
  

 

 

  

 

     

 

 

     

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

     

  
 

 

           

goal state

goal pattern

 a 

 b 

figure    example of regular lookups

for example  a pdb for the    puzzle based on tiles          and   would contain an entry
for every possible way of placing those four tiles and the blank in the    puzzle positions 
such a pdb could be implemented as a   dimensional array  p db  with the array indexes
being the locations of the blank and tiles          and   respectively  the lookup for state
s shown in figure   would then be p db                    the blank is in position    tile  
is in position    tile   is in position     etc    in this paper  accessing the pdb for a state
s as just described will be referred to as a regular lookup  and the heuristic value returned
by a regular lookup will be referred to as a regular heuristic value 
pattern databases have proven very useful for finding lower bounds for combinatorial
puzzles  korf        culberson   schaeffer        korf   felner        felner  korf   
hanan        felner et al          furthermore  they have also proven to be useful for other
search problems  e g   multiple sequence alignment  mcnaughton  lu  schaeffer    szafron 
      zhou   hansen        and planning  edelkamp      a  
    geometric symmetries
it is common practice to exploit special properties of a state space to enable additional
heuristic evaluations  in particular  additional pdb lookups can be performed given a
single pdb  for example  consider rubiks cube and suppose we had a pdb based on
the positions of the cubies that have a yellow face  the positions of the other cubies dont
matter   reflecting and rotating the puzzle will enable similar lookups for cubies with a
different color  e g   green  red  etc   since the puzzle is perfectly symmetric with respect
to color  thus  there are    symmetric lookups for such a pdb and different heuristic
values are obtained for each of these lookups in the same pdb  all these heuristic values
are admissible for any given state of the puzzle 
  

fipredicting the performance of ida  using conditional distributions

as another example  consider the sliding tile puzzle  a line of symmetry is the main
diagonal  assuming the goal location of the blank is in the upper left corner   any configuration of tiles can be reflected about the main diagonal and the reflected configuration
shares the same attributes as the original one  such reflections are usually used when using
pdbs for the sliding tile puzzle  culberson   schaeffer        korf   felner        felner
et al               and can be looked up from the same pdb 
    methods for creating inconsistent heuristics
with consistent heuristics  the difference between the heuristic value of neighboring nodes
is constrained to be less than or equal to the cost of the connecting edge  for inconsistent
heuristics  there is no constraint on the difference between heuristic values of neighboring
nodes and it can be much larger than the cost of the edge connecting them 
the kre formula is designed to work with consistent heuristics and therefore the kre
papers report on experiments done with consistent heuristics only  by contrast  our new
formula  cdp  works for all types of heuristics including inconsistent heuristics  therefore 
in this paper  in addition to the usual consistent heuristics such as regular pdb lookups or
manhattan distance we also experiment with inconsistent heuristics  we have previously
described several methods for producing inconsistent heuristics  zahavi et al          two
inconsistent heuristics that are used in the experiments below are the random selection of
heuristics and dual evaluations 
 random selection of heuristics  a well known method for overcoming the pitfalls
of a given heuristic is to employ several heuristics and use their maximum value  holte 
felner  newton  meshulam    furcy         for example  multiple heuristics can be
based on domain specific geometric symmetries such as the ones described above 
when using geometric symmetries there are no additional storage costs associated
with these extra evaluations  even when these evaluations are based on pdbs 
although using multiple heuristics results in an improved heuristic value  and therefore
is likely to reduce the number of nodes expanded in finding a solution  it also increases
the time required to calculate the heuristic values of the nodes  which might increase
the overall running time of the search  instead of using all the available heuristics
for every heuristic calculation  one could instead choose to consult only one of them 
with the selection being made either randomly or systematically  because only one
heuristic is consulted at each node  the time per node is virtually the same as if only
one heuristic was available  even if each of the individual heuristics is consistent  the
heuristic values that are actually used are inconsistent because different heuristics are
consulted at different nodes  we showed  zahavi et al         that this inconsistency
generally reduces the number of expanded nodes compared to using the same heuristic
for all the nodes and it is almost as low as if the maximum over all the heuristics had
been computed at every node  for rubiks cube  we randomly chose one of the   
different lookups of the same pdb that arise from the    lines of symmetry of this
cube 
 dual evaluation  in permutation state spaces such as rubiks cube  for each state
s there exists a dual state sd located the same distance from the goal as s  felner
  

fizahavi  felner  burch    holte

et al         zahavi  felner  holte    schaeffer        zahavi et al          therefore 
any admissible heuristic applied to sd is also admissible for s  the puzzles studied
in this paper are permutation state spaces  and the dual of a state in these puzzles
is calculated by reversing the role of locations and objects  the regular state uses
a set of objects indexed by their current location  while the dual state has a set
of locations indexed by the objects they contain  when using pdbs  a dual lookup
is to look up sd in the pdb  performing only regular pdb lookups for the states
generated during the search produces consistent values  however  the values produced
by performing the dual lookup can be inconsistent because the identity of the objects
being queried can change dramatically between two consecutive lookups  due to its
diversity  the dual heuristic was shown to be preferable to a regular heuristic  zahavi
et al          an exact definition and explanations about the dual lookup is provided
in the original papers  felner et al         zahavi et al               
it is important to note that all three pdb lookups  regular  dual  and random  consult
the same pdb  thus  they need the same amount of memory and share the same overall
distribution of heuristic values  zahavi et al         

   the kre formula and its limitations
this section begins with a short derivation of the kre formula for state spaces in which all
state transitions have a cost of    kre describe how this can be generalized to account for
variable edge costs  korf et al         
    the kre formula
for a given state s and ida  threshold d  kre aims to predict n  s  d   the number of nodes
that ida  will expand if it uses s as its start state and does a complete search with an
ida  threshold of d  i e   searches to depth d and does not terminate its search if the goal
is encountered   this can be written as
n  s  d   

d
x

ni  s  d 

   

i  

where ni  s  d  is the number of nodes expanded by ida  at level i when its threshold is d 
one way to decompose ni  s  d  is as the product of two terms
ni  s  d    ni  s   pex  s  d  i 

   

where ni  s  is the number of nodes in level i of bf ssd   the brute force search tree  i e  
the tree created by breadth first search without any heuristic pruning  of depth d rooted
at start state s  and pex  s  d  i  is the percentage of the nodes in level i of bf ssd that are
expanded by ida  when its threshold is d 
in kre  ni  s  is written as ni   i e   without the dependence on the start state s  this is
perfectly correct for state spaces with a uniform branching factor b  because ni  s  in such
cases is simply bi   for state spaces with a non uniform but regular branching structure 
  

fipredicting the performance of ida  using conditional distributions

kre showed how ni could be computed exactly using recurrence equations that are independent of s  however  the base cases of the recurrences in kre do depend on s so their
using ni instead of ni  s  is reasonable but not strictly correct 
      conditions for node expansion in ida 
to understand how pex  s  d  i  is treated in kre  it is necessary to reflect on the conditions
required for node expansion  a node n in level i of bf ssd will be expanded by ida  if it
satisfies two conditions 
   f  n    g n    h n  must be less than or equal to d  when all edges have a unit cost 
g n    i and this condition is equivalent to h n   d  i  we call nodes that satisfy
this condition potential nodes because they have the potential to be expanded 
   n must be generated by ida   i e   its parent  at level i     must be expanded by
ida  
kre restricted its analysis to heuristics that are consistent and proved that in this case
the second condition is implied by the first condition  in other words  when the given
heuristic is consistent  the nodes expanded by ida  at level i of bf ssd for threshold d are
exactly the set of potential nodes at level i   this observation allows equation   to be
rewritten as
ni  s  d    ni  s   pp ot en t ial  s  i  d  i 

   

where pp ot en t ial  s  i  v  is defined as the percentage of nodes at level i of bf ssd whose
heuristic value is less than or equal to v 
note that although pp ot en t ial s  i  di    pex  s  d  i  when the given heuristic is consistent  pp ot en t ial  s  i  d  i  overestimates pex  s  d  i  when the heuristic is inconsistent 
sometimes by a very large amount  see section        
      approximating pp ot en t ial  s  i  v 
kre use three different approximations to pp ot en t ial s  i  v   in kres theoretical analysis
pp ot en t ial s  i  v  is approximated by the equilibrium distribution  which we denote
as peq  v   it is defined to be the probability that a node chosen randomly and uniformly
among all nodes at a given depth of the brute force search tree has heuristic value less than
or equal to v  in the limit of large depth  korf et al        p        kre proved that  in
the limit of large d 
d
x
ni  s   peq  d  i 
i  

would converge to n  s  d  if the given heuristic is consistent  their final formula  the kre
formula  is therefore 
   see section       below for more discussion of the kre formula with consistent heuristics 

  

fizahavi  felner  burch    holte

n  s  d   

d
x

ni  s   peq  d  i 

   

i  

kre contrasted the equilibrium distribution with the overall distribution  which is
defined as the probability that a state chosen randomly and uniformly from all states in
the problem has heuristic value less than or equal to v  p        unlike the equilibrium
distribution  which is defined over a search tree  the overall distribution is a property of the
state space  the overall distribution can be directly computed from a pattern database  if
just one pattern database is used and each of its entries corresponds to the same number
of states in the original state space  or can be approximated  in more complex settings 
by computing the heuristic values of a large random sample of states  kre argued that in
rubiks cube the overall distribution for a heuristic defined by a single pattern database
is the same as the equilibrium distribution  but that for the sliding tile puzzles  the two
distributions are different 
the heuristic used in kres experiments with rubiks cube was defined as the maximum
over three pattern databases  for each individual pattern database  the overall distribution
was computed exactly  in kres experiments these distributions were combined to approximate pp ot en t ial s  i  v  by assuming that the values from the three pattern databases
were independent 
for the experiments with the sliding tile puzzles  kre defined three types of states based
on the whether the blank was located in a corner position  an edge position  or an interior
position  and approximated pp ot en t ial s  i  v  by a weighted combination of the overall distributions of the states of each type  the weights used at level i were the exact
percentages of states of the different types at that level 
in our experiments we followed kre precisely and use the overall distribution for individual rubiks cube pattern databases and the weighted overall distribution just described
for the sliding tile puzzles  for simplicity  in the reminder of this paper we use the phrase
unconditional heuristic distribution  and the notation p  v  to refer to the probability that
a node has a heuristic less than or equal to v  we let the exact context determine which
distribution p  v  actually denotes  whether it is the equilibrium distribution  the overall
distribution  or any other approximation of pp ot en t ial and pex   likewise we will use
p v   lower case p  to denote p  v   p  v      with p      p       p v  is the probability
that a state will have a heuristic value of exactly v according to the distribution p  
    limitations of the kre formula
the kre formula  equation    has two main shortcomings      its predictions are not
accurate if the given heuristic is inconsistent  and     even with consistent heuristics its
predictions can be inaccurate for individual start states or sets of start states whose heuristic
values are not distributed according to the unconditional heuristic distribution  p  v   we
now turn to examine each of these in detail 
  

fipredicting the performance of ida  using conditional distributions

consistent
 
 
 

   

expanded
generated
not generated

 

inconsistent
  r
 

 

   

 

  m
 

    n

figure    consistent versus inconsistent heuristics
      inconsistent heuristics
as specifically mentioned in the kre papers one property required for the kre analysis
is that the heuristic be consistent  this is necessary because the kre formula aims to
count the number of potential nodes at each level in bf ssd   with consistent heuristics  the
heuristic value of neighboring states never changes by more than the change in the g value 
as illustrated in the left side of figure    where the number inside a node is its heuristic
value   this implies that the f  value of a nodes ancestor is always less than or equal to
the f  value of the node  i e   f is monotone non decreasing along a path in the search tree  
therefore  it is easy to prove that with consistent heuristics all the ancestors of a potential
node are also potential nodes  korf et al          consequently ida  will expand all and
only the potential nodes in bf ssd   hence  a formula such as kre that aims to count the
number of potential nodes in bf ssd can be used to predict the number of nodes ida  will
expand when given a consistent heuristic 
for inconsistent heuristics this reasoning does not apply  the heuristic values of neighboring states can differ by much more than the cost of the edge that connects them  and
thus the f  values along a path in the search tree are not guaranteed to be monotonically
non decreasing  therefore  the ancestors of a potential node are not guaranteed to be
potential nodes themselves  with the consequence that a potential node might never be
generated  for example  consider the search tree in the right side of figure    the numbers
inside each node show the nodes heuristic value  assume that the start node is r and
that the ida  threshold is    a node is a potential node if its f  value is less than or equal
to     there are   potential nodes at depth    all with heuristic value     consider the
potential node n  the path to it is through node m but node m is not a potential node
 f  m                    so it will be generated but not expanded  therefore  node n
will never be generated  preventing ida  from expanding it  since the kre formula counts
the number of potential nodes  it will count node n and thus overestimate the number of
expanded nodes when an inconsistent heuristic is used 
the amount by which kre overestimates the number of nodes expanded by ida  with
an inconsistent heuristic can be very large  to illustrate this  consider the state space for
rubiks cube and a pdb heuristic defined by the locations of    out of     of the edge
cubies  the regular method for looking up a heuristic value in a pdb produces a consistent
heuristic  as discussed in section     two alternative pdb lookups that produce inconsistent
   unconditional to distinguish it from the conditional distribution we introduce in section    

  

fizahavi  felner  burch    holte

d
 
 
  
  
  
  

kre
   
     
      
       
         
           

regular
   
     
      
       
         
           

dual
  
   
     
      
         
          

random symmetry
  
   
     
      
       
          

table    rubiks cube   number of nodes expanded by ida  using regular  dual  and
random symmetry pdb lookups for different ida  threshold d and the corresponding kre predictions 

heuristics are the dual evaluation and the random selection from multiple heuristics  in
rubiks cube there are    symmetries and each can be applied to any state to create a new
way to perform a pdb lookup for it  thus  there are    heuristics for rubiks cube based
on the same pdb and the random symmetry lookup chooses one of them randomly 
because all three lookups  regular  dual  and random symmetry  consult the same pdb
they have the same distribution of heuristic values  p  v   and therefore kre will predict
that ida  will expand the same number of nodes regardless of whether a regular  dual 
or random symmetry lookup is done  the experimental results in table   show that a
substantially different number of nodes are actually expanded in practice for each of these
methods 
each row of table   presents results for a specific ida  threshold  d   each result is an
average over        random initial states  each of which is generated by making     random
moves from the goal state  the kre column shows the kre prediction based on the
unconditional heuristic distribution  the last three columns of table   show the number of
nodes ida  expands when it performs either a regular  dual  or random symmetry lookup
in the pdb  the kre prediction is within    of the actual number of nodes expanded
when ida  uses the regular  consistent  pdb lookup  third column  but it substantially
overestimates the number of nodes expanded when ida  uses the dual or random symmetry
inconsistent lookups in the same pdb  fourth and fifth columns  
      sets of start states whose heuristics values do not obey the
unconditional heuristic distribution
as explained above  kre used the unconditional heuristic distribution p  v  and  in their
theoretical analysis  proved that its use in the kre formula would give accurate predictions in the limit of large depth  in fact  accurate predictions will occur as soon as the
heuristic distribution at the depth of interest d closely approximates p  v   this happens
at large depths by definition but this can happen even at very shallow levels under certain
circumstances  the reason that kre was able to produce extremely accurate predictions
in its experiments using the unconditional heuristic distribution p  v  for all depths and
all start states is that its experiments report average predictions and performances over
  

fipredicting the performance of ida  using conditional distributions

a large number of randomly drawn start states  in the spaces used in kres experiments 
the heuristic distribution of a large random set of start states very closely approximated
the p  v  distribution they used  this caused heuristic distributions at all levels to closely
approximate p  v  
however  if the set of start states does not have its heuristic values distributed according
to p  v   as is the case for most non random sets of start states or a single start state 
kre should not be expected to make good predictions for small depths  in other words 
in such cases the unconditional heuristic distribution p  v  is not expected to be a good
approximation of pex  s  d  i  
consider the case of a single start state and a consistent heuristic  the distribution of
heuristic values in the search tree close to the start state will be highly correlated with the
heuristic value of the start state  and therefore will not be the same in search trees with
start states having different heuristic values  for example  a great deal of pruning is likely
to occur near the top of the search tree for a start state with a large heuristic value  resulting
in fewer nodes expanded than for a start state with a small heuristic value  applying kre
to these two states will produce the same prediction  and therefore be inaccurate for at
least one of them  because it uses the same unconditional heuristic distribution p  v  in
both cases 
h
 
 
 
 
 

ida 
          
          
          
         
         

kre
         
         
         
         
         

table    results for a set of       start states all with the h value shown in the first column
 regular pdb lookup  ida  threshold of d      

table   demonstrates this phenomenon on rubiks cube with one regular   edge pdb
lookup for ida  threshold d       the ida  column shows the average number of nodes
expanded for        start states  all with the same heuristic value h for any given row  kre
ignores the heuristic values of the start states and predicts that           nodes will be
expanded by ida  for every start state  the row for d      in table   shows that this is
an accurate prediction when performance is averaged over a large random sample of start
states  but in table   we see that it is too low for start states with small heuristic values
and too high for ones with large heuristic values 
      convergence of the heuristic distributions at large depths
as described above  kre will make accurate predictions for level i if the nodes at that level
actually obey the unconditional heuristic distribution p  v   as i increases  the distribution
of heuristic values will start to converge to p  v   the rate of convergence depends upon
the state space  it is believed to be fairly slow for the sliding tile puzzles  but faster for
  

fizahavi  felner  burch    holte

rubiks cube  if the convergence occurs before the ida  threshold is reached kre will
provide accurate predictions for any set of start states  including single start states  
in order to experimentally test this we repeated the kre rubiks cube experiment but 
in addition to using a large set of random start states  we also looked at the individual
performance of two start states  s    which has a low heuristic value      and s     which has
the maximum value for the heuristic used in this experiment       as in kre we used the
      heuristic which takes the maximum of   different pdbs  one based on all   corner
cubies and two based on   edge cubies each   this heuristic is admissible and consistent 
over the billion random states we sampled to estimate p  v  the maximum value was   
and the average value was       

kre
d
  
  
  
  
  
  
  
  

     
      
       
         
          
           
             
               

multiple start states
ida 
ratio
     
    
      
    
       
    
         
    
          
    
           
    
             
    
               
    

s 
      
       
         
          
           
             
              
               

single start state
ratio
s  
    
    
     
    
       
    
         
    
          
    
           
    
             
                    

ratio
    
    
    
    
    
    
    

table    rubiks cube   max of         pdbs
table   presents the results  the kre column presents the kre prediction and the
multiple start states columns presents the actual number of states generated  averaged over
a set of random start states  for each ida  threshold  both columns are copied from the kre
journal paper  korf et al          the ratio columns of table   shows the value predicted
by the kre formula divided by the actual number of nodes generated  this ratio was found
to be very close     for multiple start states  indicating that kres predictions were very
accurate 
the results for the two individual start states we tested are shown in the single start
state part of the table  note that both states are optimally solved at depth     but  as in
kre  the search at that depth was run to completion  in both cases the kre formula was not
accurate for small thresholds but the accuracy of the prediction increased as the threshold
increased  at threshold d      the kre prediction was roughly a factor of   too small for s 
and about     too large for s     this is a large improvement over the smaller thresholds 
these predictions will become even more accurate as depth continues to increase 
the reason the predictions improve for larger values of d is that at deeper depths the
heuristic distribution within a single level converges to the unconditional heuristic distribution  using dashed and dotted lines of various types  figure   a  shows the distribution of
heuristic values seen in states that are         and   moves away from s    the solid line in
figure   a  is the unconditional heuristic distribution  the x axis corresponds to different
heuristic values and the y axis shows the percentage of states at the specified depth with
heuristic values less than or equal to each x value  for example for depth    which includes
  

fipredicting the performance of ida  using conditional distributions

   

   
unconditional heuristic distribution
depth    
depth    
depth    
depth    

  

   
cumulative percentage

cumulative percentage

   

unconditional heuristic distribution
depth    
depth    
depth    
depth    

  
  
  

  
  
  
  

 

 
 

 

 

 

 

  

  

 

 

heuristic value

 

 

 

  

heuristic value

 a  heuristic distributions for s 

 b  heuristic distributions for s  

figure    convergence of heuristic distributions

the start state only  only a heuristic value of   was seen  leftmost curve   for depth   
heuristic values of      and   were seen  second curve from the left   and so on  the figure
shows that the heuristic distribution at successive depths converges to the unconditional
heuristic distribution  rightmost curve in figure   a    at depth     not shown   the heuristic distribution is probably quite close to the unconditional heuristic distribution  making
the kre prediction quite accurate even for this single start state 
figure   b  shows the heuristic distributions for nodes that are          and   moves
away from s     in this case the unconditional heuristic distribution is to the left of the
heuristic distributions for the shallow depths  with the heuristic distribution for depth  
being the rightmost curve in this figure  comparing parts  a  and  b  of figure   we see
that the convergence to the unconditional heuristic distribution is faster for s   than for s   
which explains why the kre prediction in table   is more accurate for s    

   conditional distribution and the cdp formula
we now present our new formula cdp  conditional distribution prediction   which overcomes the two shortcomings of kre described in the previous section  an important feature
of cdp is that it extends the unconditional heuristic distribution of heuristic values p  v 
used in kre to be a conditional distribution 
    conditional distribution of heuristic values
the conditional distribution of heuristic values is denoted p  v context   where the context
represents local properties of the search tree in the neighborhood of a node that influence the
distribution of heuristic values in the nodes children  specifically  if pn  v  is the percentage
of node ns children that have a heuristic value less than or equal to v  then we define
p  v context  to be the average of pn  v  over all nodes n that satisfy the conditions defined
  

  

fizahavi  felner  burch    holte

by the context  p  v context  can be interpreted as the probability that a node with heuristic
value less than or equal to v will be produced when a node satisfying the conditions specified
by context is expanded  when the context is empty it is denoted p  v  as in section    we
use p v context   lower case p  to denote the probability that a node with heuristic value
equal to v will be produced when a node
p satisfying the conditions specified by context is
expanded  obviously  p  v context    vi   p i context  
      the basic   step model
the conditioning context can be any combination of local properties of the search tree 
including properties of the node itself  e g  its heuristic value   the operator that was applied
to generate the node  properties of the nodes ancestors in the search tree  etc  the simplest
conditional distribution is p v vp    the probability of a node with a heuristic value equal to v
being produced when a node with value vp is expanded  we call this a   step model because
each value is conditioned by nodes that are one step away only  in special circumstances 
p v vp   can be determined exactly by analysis of the state space and the heuristic  but in
general it must be approximated empirically by sampling the state space 
in our sampling method p v vp   is represented by the entry m  v  vp   in a two dimensional
matrix m     hmax      hmax    where hmax is the maximum possible heuristic value  to build
the matrix we first set all values in the matrix to    we then randomly generate a state
and calculate its heuristic value vp   after that  we generate each child of this state one at
a time  calculate the childs heuristic value  v   and increment m  v  vp    we repeat this
process a large number of times in order to generate a large sample  finally  we divide the
value of each cell of the matrix by the sum of the column the cell belongs to  so that entry
m  v  vp   represents the percentage of children generated with value v when a state with
value vp is expanded 

vp

vp
 

v

 

 

 

  

 

 

 

 

  

 

                        

 

                        

                        

 

                        

 

                        

 

                        

  

                        

  

                        

 

                        

 

                        

 

v

 a  consistent heuristic

 b  inconsistent heuristic

figure    a portion of the conditional distribution matrix for rubiks cube for consistent
and for inconsistent heuristics

  

fipredicting the performance of ida  using conditional distributions

figure   shows the bottom right corner of two such matrices for the   edge pdb of
rubiks cube  the left matrix  a  shows p v vp   for the regular  consistent  lookup in
this pdb and the right matrix  b  shows p v vp   for the inconsistent heuristic created by
the dual lookup in this pdb  the matrix in  a  is tridiagonal because neighboring values
cannot differ by more than    for example  states with a heuristic value of   can only have
children with heuristics of      and    these occur with probabilities of            and     
respectively  see column     by contrast  the matrix in  b  is not tridiagonal  in column   
for example  we see that    of the time states with heuristic value of   have children with
heuristic values of   
      richer models
when ida  expands a node  it eliminates some children because of operator pruning  for
example  in state spaces with undirected operators  such as we are using in our studies  the
parent of a node would be generated among the nodes children but ida  would immediately
prune it away  distribution p v vp   does not take this into account  in order to take this into
consideration it is necessary to extend the context of the conditional probability to include
the heuristic value of the parent of the node being expanded  we refer to the parent node
as gp   we denote this by p v vp  vgp   and call this a   step model because it conditions
on information from ancestors up to two steps away  p v vp  vgp   gives the probability of a
node with a heuristic value equal to v being generated when the node being expanded has
a heuristic value of vp and the parent of the node being expanded has a heuristic value of
vgp   it is estimated by sampling in the same way as was done to estimate p v vp    except
that each sample generates a random state  gp  then all its neighbors  and then all of
their neighbors except those eliminated by operator pruning  naturally  the results of the
sampling for this   step model are stored in a three dimensional array 
the context of the conditional distribution can be extended in other ways as well  for
the sliding tile puzzles  kre conditions the overall distribution on the type of the state
being expanded  where the type indicates if the blank is in a corner  edge  or interior
location  in our experiments with the sliding tile puzzle below  we extend p v vp  vgp   with
this type information  p v  t vp   tp  vgp   tgp   gives the probability of a node of type t with
heuristic value equal to v being generated when the node being expanded has heuristic
value vp and type tp and the expanded nodes parent has heuristic value vgp and type tgp  
    a new prediction formula  cdp  conditional distribution prediction 
in this section we use the conditional distributions just described to develop cdp  an alternative to the kre formula for predicting the number of nodes ida  will expand for a
given heuristic  ida  threshold  and set of start states  as will be shown below experimentally  the new formula cdp overcomes the limitations of kre and works well for inconsistent
heuristics and for any set of start states with arbitrary ida  threshold 
our overall approach is as follows  define ni  s  d  v  to be the number of nodes that
ida  will generate at level i with a heuristic value equal to v when s is the start state and
d is the ida  threshold 
pdi given ni  s  d  v   the number of nodes ida  will expand at level
i for threshold d is v   ni  s  d  v   and  n  s  d   the total number of nodes expanded in a
complete iteration of ida  with threshold d over all levels  the quantity we are ultimately
  

fizahavi  felner  burch    holte

p p
interested in  is di   di
v   ni  s  d  v   in these summations v only runs up to d  i because
only nodes with heuristic values in the range          d  i  will be expanded at level i 
if ni  s  d  v  could be calculated exactly  this formula would calculate n  s  d  exactly
whether the given heuristic is consistent or not  however  there is no general method for efficiently calculating ni  s  d  v  exactly  instead  ni  s  d  v  will be estimated recursively from
ni   s  d  v  and the conditional distribution  the exact details depend on the conditional
model being used and are given in the subsections that follow  we will use ni  s  d  v  to
denote an approximation of ni  s  d  v   in section       we will describe conditions in which
this calculation is  in fact  exact  and therefore produces perfect predictions of n  s  d  
but in the general case these predictions may not be perfect and are only estimates  at
the present time we have no analytical tools for estimating their accuracy but as we show
experimentally  these estimates are often very accurate 
    prediction using the basic   step model
if the basic   step conditional distribution p v vp   is being used  ni  s  d  v  can be estimated
recursively as follows 
d i  

ni  s  d  v   ni  s  d  v   

x

ni   s  d  vp    bvp  p v vp  

   

vp   

where bvp is the average branching factor of nodes with heuristic value vp   which is estimated
during the sampling process that estimates the conditional distribution   the reasoning
behind this equation is that ni   s  d  vp  bvp is the total number of children ida  generates
via the nodes it expands at level i    with heuristic value equal to vp   this is multiplied by
p v vp   to get the expected number of these children that have heuristic value v  nodes at
level i  are expanded if and only if their heuristic value is less than or equal to d   i     
hence the summation only includes vp values in the range of          d   i       by restricting
vp to be less than or equal to d   i     in every recursive application of this formula 
we ensure  even for inconsistent heuristics  that a node is only counted at level i if all
its ancestors are expanded by ida   the base case of this recursion  n   s  d  v   is   for
v   h s  and   for all other values of v 
based on this  the number of nodes expanded by ida  given start state s  threshold d 
and a particular heuristic can be predicted as follows 
cdp   s  d   

d x
di
x

ni  s  d  v 

   

i   v  

if a set  s  of start states is given instead of just one start state  the calculation is
identical except that the base case of the recursion is defined using all the start states in
s  that is  we define n   s  d  v  to be equal to k if there are k states in s with a heuristic
value of v  the rest of the formula remains the same  with s substituted for s everywhere  
   in the general case of our equation the branching factor depends on the context that defines the conditional distribution  since in the   step model  the context is just the heuristic value v  we formally
allow the branching factor to depend on it  in practice  the branching factor is usually the same for all
heuristic values 

  

fipredicting the performance of ida  using conditional distributions

    prediction using richer models
if the   step conditional distribution p v vp   vgp   is being used  we define ni  s  d  v  vp   to
be the number of nodes that ida  will generate at level i with a heuristic value equal to v
from nodes at level i    with heuristic value vp when s is the start state and d is the ida 
threshold  ni  s  d  v  vp   can be estimated recursively as follows 
d i  

ni  s  d  v  vp    ni  s  d  v  vp    

x

ni   s  d  vp   vgp    bvp  vgp  p v vp   vgp  

   

vgp   

where bvp  vgp is the average branching factor of nodes with heuristic value vp and a parent
with heuristic value vgp   the base case for this   step model is at level    not level   
n   s  d  v  vp   is   for vp    h s   and is the number of children of the start state s with
heuristic value v for vp   h s   based on this   step model the number of nodes expanded
by ida  given start state s  threshold d  and a particular heuristic can be predicted as
follows 
cdp   s  d   

d x
di d i  
x
x
i   v  

ni  s  d  v  vp  

   

vp   

if there is a set s of start states instead of just one  the base case is n   s  d  v  vp    the
number of children with heuristic value v of the states in s with heuristic value vp  
analogous definitions of ni and cdp can be used with any definition of context  for
example  if using a   step model with a set t of state types  one would define ni  s  d  v  t 
as the number of nodes of type t that ida  will generate at level i with a heuristic value
equal to v  and estimate it recursively as follows 
d i  

ni  s  d  v  t   ni  s  d  v  t   

x

x

vp   

tp t

ni   s  d  vp   tp    bvp  tp  p v  t vp   tp  

   

based on this model the number of nodes expanded by ida  given start state s  threshold
d  and a particular heuristic can be predicted as follows 
cdp s  d   

d x
di x
x

ni  s  d  v  t 

    

i   v   tt

    prediction accuracy
the accuracy of our predictions can be arbitrarily good or arbitrarily bad depending on
the accuracy of the conditional model being used  in the following subsections we examine
each of these extreme cases 
in principle  extending the context should never decrease the accuracy of the predictions
because additional information is taken into account  however  when the conditional model
is being estimated by sampling  an extended context can result in poorer predictions because
there are fewer samples in each context  this is our explanation of why the   step model
is more accurate than the   step model in rows h     and h     in table   in section    
below 
  

fizahavi  felner  burch    holte

      perfect predictions
consider any definition of context that includes the heuristic value of the node being expanded  vp in the contexts defined above  and contains sufficient information to allow
operator pruning to be correctly accounted for  we will use the notation  v  x  to refer to a
specific instance of such a context  where v is the heuristic value of the node being expanded
and x is an instantiation of the other information in the the context  e g   the state type
information in the last model above   the general form of our predictive model with such
a context is
cdp s  d   

d x
di
x
i   v  

x

ni  s  d  v  x 

    

all x
such that  v  x 
is an instance
of the context

with
d i  

ni  s  d  v  x   

x

x

vp   

all xp
such that  vp   xp  
is an instance
of the context

ni   s  d  vp   xp    bvp  xp  p v  x vp   xp  

    

where bvp  xp is the average branching factor  after operator pruning  of all nodes satisfying
the conditions of context  vp   xp    and p v  x vp   xp   is the average over all nodes n satisfying
the conditions of context  vp   xp   of pn  v  x   the percentage of ns children  after operator
pruning  that satisfy the conditions of context  v  x  
if  for every context  vp   xp    all nodes n satisfying the conditions defined by  vp   xp  
have exactly the same branching factor bvp  xp and exactly the same value of pn  v  x  for all
contexts  v  x   a simple proof by induction starting from the correctness of the base cases 
n   s  d  v  x   shows that ni  s  d  v  x    ni  s  d  v  x  for all i  i e   that our prediction
method correctly calculates exactly how many nodes will satisfy the conditions of each
context at every level of the search tree  from this it follows that cdp s  d  will be exactly
the number of nodes ida  will expand given start state s and ida  threshold d 
a practical setting in which the predictions of our   step model are guaranteed to be
perfect by this reasoning is when the following conditions hold 
   the heuristic is defined to be the exact distance to the goal in an abstract state space 
as is the case when a single pattern database is used 
   any two states  s    s    that map to the same abstract state x have the same set of
operators  op         opk   that apply to them  and
   if states s  and s  map to abstract state x  then for all operators op   op         opk  
that apply to s  and s    s  s child op s    and s  s child op s    map to the same abstract
state  op x  
  

fipredicting the performance of ida  using conditional distributions

define the context of a node to be its heuristic value and the abstract state to which it maps 
condition     guarantees that for every context  v  x   all nodes satisfying the conditions
of  v  x  have exactly the same branching factor bv x   this is true because if nodes n 
and n  satisfy the conditions of context  v  x   they both map to the same abstract state
x  and condition     then requires that exactly the same set of operators apply to them
both  conditions     and     together guarantee that for every context  vp   xp    all nodes
satisfying the conditions of  vp   xp   have exactly the same value of pn  v  x  for all v and x 
this is true because if nodes n  and n  satisfy the conditions of context  vp   xp    they both
map to the abstract state xp   the same set of operators applies to both  and each operator
op creates a child that  in both cases  maps to a specific abstract state  op xp    therefore
the percentage of children that map to any particular abstract state is the same for both
n  and n   
a straightforward implementation of the prediction method in this setting associates a
counter with each abstract state  which is initialized to the number of start states that map
to the abstract state  the counter for abstract state x is updated once for each value of i
    i  d  by adding to it  for each operator op  the current value of the counter of each
abstract state y such that op y    x  this algorithm has a computational complexity that
is o d   a        where  a  is the number of abstract states and  is the effective branching
factor in the abstract space  because the complexity depends only linearly on d  in contrast
to the typically exponential dependency on d for the number of nodes ida  will expand 
for sufficiently large d the prediction will be arbitrarily faster to compute than the search
itself  for example  for a pdb for the    puzzle based on the positions of   tiles and the
blank  roughly   billion abstract states   the prediction for      start states with d     
takes only    of the time required to execute the search 
an exact prediction for this setting has two potential uses  the first is to determine if
searching with a single pdb is feasible or not  for example  the calculation might show
that even the first iteration of ida   with a threshold of h start   will take more than a
year to complete  the second is to use the prediction to compare the actual performance
of an alternative method executed on a set of start states  e g  taking the maximum over
a set of pdbs  to the performance using a single pdb without actually having to execute
the ida  search with the single pdb 
      very poor predictions
the predictions made by a conditional model will be extremely inaccurate if the distribution
of heuristic values is independent of the information supplied by the context  we illustrate
this with an example based on the  x  sliding tile puzzle and two heuristics  a pdb based
on the locations of tiles     and the blank  and the heuristic that returns   for every state 
if the given state has the blank in its goal position  or in a position that is an even number
of moves from the goal position  the heuristic value for that state is taken from the pdb 
the other states have a heuristic value of    in the search tree  the heuristic used at level i
will therefore be the opposite of the one used at level i    
a   step model in this situation will clearly be hopeless for predicting the heuristic
distribution in levels where the pdb is being used until i is sufficiently large that the
distribution at level i converges to the unconditional distribution 
  

fizahavi  felner  burch    holte

there is some hope that a   step model could make reasonably accurate predictions
because the pdb  considered by itself  defines a consistent heuristic and therefore the distribution of heuristic values of a nodes children are somewhat correlated with the heuristic
value of the nodes parent 
we tested this using the  x  sliding tile puzzle  which is small enough that we could
build our   step model using all the states in the state space so that no error is introduced
through a sampling process  to test the prediction accuracy of the model we generated
       solvable states at random and  as will be explained in detail in the next section  used
state s as a start state in combination with ida  threshold d if ida  would actually have
executed an iteration with threshold d when given state s as a start state  this means that
a different number of start states might be used for each value of d  the num column in
table   indicates how many start states were used for each value of d  first column  and we
have only included in this table results for which more than       start states were used 
the ida  column shows the average number of nodes expanded by ida  on the start
states used for each d and the prediction column shows the number predicted by our
  step model  the ratio column is prediction divided by ida   one can clearly see
the improvement of the predictions as d increases  but even at the deepest depth at which
our sample provided more than       start states  the prediction is a factor of   smaller than
the true value  of course  using the constant heuristic value of   in alternate levels is not
something one would do in practice  but we obtained similar results  for essentially the same
reason  with the    puzzle when switching  from one level to the next  between a pattern
database based on tiles     and a pattern database based on tiles       see section      

d
  
  
  
  
  
  
  
  
  
  
  
  
  

ida 
     
     
     
     
     
     
      
      
      
      
      
      
       

cdp 
prediction ratio
  
    
  
    
  
    
   
    
   
    
   
    
   
    
     
    
     
    
     
    
     
    
      
    
      
    

num
     
     
     
      
      
      
      
      
      
      
      
     
     

table     x  sliding tile puzzle  alternating between a good heuristic and   

   experimental setup
the next two sections describe the experimental results that we obtained by running ida 
and comparing the number of nodes it expanded to the number predicted by kre and by
  

fipredicting the performance of ida  using conditional distributions

cdp  we experimented on the same two application domains used by kre  namely  rubiks
cube  section    and the sliding tile puzzle  section     in each domain we evaluated the
accuracy of the two formulas  for both consistent and inconsistent heuristics  on a set of
solvable start states that were generated at random 
in all the experiments reported here  the start states used for a given ida  threshold
d were subject to a special condition  state s is only used as a start state in combination
with threshold d if ida  actually performs an iteration with threshold d when s is the start
state  for example  we would not use s as a start state for d      if s is only distance   
from the goal or if h s        in addition  for the sliding tile puzzle  start state s would
not be used with ida  threshold d if h s  and d were of different parity  by contrast  the
experiments in the kre paper did not restrict the choice of start states in this way  the
same start states were used with every ida  threshold  
this difference in how start states are chosen can have a large impact on the number
of nodes ida  expands  table   illustrates this for the    puzzle using the manhattan
distance heuristic for ida  threshold d  first column  between    and     the nodes
column under unrestricted shows the number of nodes ida  expanded on average for
       randomly generated solvable start states  the values in this column are in close
agreement with the corresponding results of table   in the kre paper  korf et al         
the number column shows how many of these start states satisfy our additional condition 
if we remove the start states that violate our condition  ida  expands substantially fewer
nodes on average  as shown in the nodes column under restricted  and the difference
increases as d increases  at d      there is almost an order of magnitude difference between
the number of nodes expanded in the two settings  this difference needs to be kept in
mind when making comparisons with the experimental results reported here and in the kre
papers 

d
  
  
  
  
  
  
  
  

unrestricted
nodes
       
         
         
         
         
          
          
          

restricted
number
nodes
      
       
      
       
      
       
                
                
                
                
                

table       puzzle with manhattan distance  the effect on nodes expanded if start states
are randomly chosen or subject to our condition 

  

fizahavi  felner  burch    holte

   experimental results for rubiks cube
we begin with rubiks cube experiments  the heuristic used here is the   edge pdb
heuristic described above  section         we experimented with the  consistent  regular
lookup and the  inconsistent  random symmetry and dual lookups on this pdb  for the
cdp formula  two models were used  cdp  and cdp    which denote the   step and   step
models  respectively 
as outlined in section        the conditional distribution tables were built by generating one billion states  each is generated by applying     random moves to the goal state  
computing all their neighbors  and incorporating their heuristic information into the matrix representing the one step model  for the two step model we also generated all the
grandchildren and used their heuristic information 
in addition  in order to get reliable samples we added the following two procedures 
 while generating children and grandchildren for sampling we used the same pruning
techniques based on operator ordering that were used in the main search  see the
description in section         that is  we did not use a sequence of operators that
would not be generated by the main search  this is done by looking at the random
walk that led to the initial state and using the last operator in the random walk as
the basis for operator pruning 
 in order to get a reliable sample we need each entry in the table to be sufficiently
sampled  some entries in the table have very low frequency  for example  states
with heuristic value of   are very rare even in a sample of a billion states causing our
table for the   row to be generated by a very small sample  therefore  we enriched
such entries by artificially creating random states with heuristic value of    other
under sampled entries were sampled in a similar way  one technique  for example 
for creating  with high probability  a random state with a heuristic value of x  is to
perform a random walk of length x on a random state with heuristic value of   
    rubiks cube with consistent heuristics
table   compares kre to cdp  and to cdp    the accuracy of the three prediction methods
was compared while using regular lookups on the   edge pdb  results in each row are
averages over a set of      random states  each row presents the results of an ida  iteration

d
 
 
  
  
  
  

ida 
   
     
      
       
         
           

kre
prediction
   
     
      
       
         
           

ratio
    
    
    
    
    
    

cdp 
prediction ratio
   
    
     
    
      
    
       
    
         
    
          
    

cdp 
prediction ratio
   
    
     
    
      
    
       
    
         
    
           
    

table    rubiks cube with a consistent heuristic 
  

fipredicting the performance of ida  using conditional distributions

with different threshold  d   given in the first column  the second column  ida   presents
the actual number of nodes expanded for each ida  threshold  the next columns report
the predictions and the accuracy  ratio  of each prediction defined as the ratio between
the predicted number and the actual number of expanded nodes  as was reported in the
kre paper  the kre formula was found to be very accurate for a consistent heuristic when
averaged over a large set of random start states  the table shows that cdp  is reasonably
accurate but systematically underestimates because the one step model does not consider
that a nodes parent will not be included among its children  we elaborate on this below 
cdp  s predictions are very accurate  slightly more accurate than kres 
    rubiks cube with start states having specific heuristic values
table    presented above  section         and the related discussion  show that kre might
not make accurate predictions when start states are restricted to have a specific heuristic
value h  for the particular example shown  ida  threshold     kre will always predict
a value of              but the exact value depends on the specific set of start states used
because the ida  threshold of    is not sufficiently large for the number of nodes to be
independent of the start states  table   extends table   to include the predictions of cdp 
it shows that both versions of cdp substantially outperform kre on any particular set of
start states 

h
 
 
 
 
 

ida 
          
          
          
         
         

kre
prediction ratio
         
    
         
    
         
    
         
    
         
    

cdp 
prediction ratio
          
    
          
    
         
    
         
    
         
    

cdp 
prediction ratio
          
    
          
    
         
    
         
    
         
    

table    results for different start state heuristic values  h  for a regular pdb with an
ida  threshold of d      

    rubiks cube with inconsistent heuristics
the same experiments were repeated for inconsistent heuristics  the dual and randomsymmetry lookups were performed on the   edge pdb instead of the regular lookup  thereby
creating an inconsistent heuristic  as discussed in section        kre produces the same
prediction for all heuristics  consistent and inconsistent  derived from a single pdb and
overestimates for the inconsistent heuristics  table   shows that cdp  is extremely accurate 
its prediction is always within    of the actual number of nodes expanded 
the   step model used by cdp  systematically underestimates the actual number of
nodes expanded for regular and dual lookups  see the regular lookup in table   and the
dual lookup in table     to understand why  consider what happens when the node m in
the right side of figure   is expanded  it generates two children  node n and  assuming
  

fizahavi  felner  burch    holte

kre
prediction

d

ida 

 
 
  
  
  
  

  
   
     
      
         
          

 
 
  
  
  
  

  
   
     
      
       
          

cdp 
ratio prediction ratio
dual
   
    
  
    
     
    
   
    
      
    
     
    
       
    
      
    
         
    
       
    
           
               
    
random symmetry
   
    
  
    
     
    
   
    
      
    
     
    
       
    
      
    
         
    
       
    
           
               
    

cdp 
prediction ratio
  
   
     
      
         
          

    
    
    
    
    
    

  
   
     
      
       
          

    
    
    
    
    
    

table    rubiks cube with dual  and random symmetry  inconsistent  heuristics
operators have inverses as is the case in rubiks cube  a copy of its parent r  shown as ms
left child in figure     this child is   levels deeper than r and therefore has an f  value that
is   greater than rs  with an ida  threshold of    this child will not be a potential node
and the   step model will conclude that m will generate a potential child with a probability
of      whereas in fact all of the children that remain after operator pruning are potential
nodes 

  r
  m
 

  n

figure    the   step model may underestimate
the reason the   step model does not underestimate the number of nodes expanded
when random symmetry lookups are done is because the child copy of r is not constrained
to have the same heuristic value as r itself  different symmetries could be chosen for
different occurrences r  the childs f  value has no correlation with the f  value of r and
the above explanation of why cdp  underestimates does not apply 
in fact  if different copies of a state have uncorrelated h values the only effect of operator
pruning that needs to be taken into account is that it reduces the number of children  and
this can be done as well within a   step model when calculating the branching factor  there
may be other advantages of using the wider context of the   step model but the results for
the random symmetry heuristic show that they are minor in this case 
  

fipredicting the performance of ida  using conditional distributions

   experimental results   sliding tile puzzle
in the kre experiments on the sliding tile puzzle  three state types are used  based on
whether the blank is in a corner  edge  or interior location  we used the same state types in
our experiments and used exact recurrence equations for n  s  v  d  t  in the type dependent
version of the kre formula  the heuristic used was manhattan distance  md   we experimented with the   step cdp that includes the type system in the recurrence equations 
results for the   step cdp are not included here because it performed poorly in early versions of these experiments 
for the   puzzle the conditional distribution p  v  t vp   tp  vgp   tgp   needed by cdp  and
the typed unconditional distribution p  v  t  needed by the type dependent kre formula
were computed by enumerating all the states in the   puzzle reachable from the goal 
for the    puzzle  it was not possible to do exhaustive enumeration of the entire state
space so the conditional distributions were estimated by generating ten billion reachable
states at random  this uniform random sample was used to estimate p  v  t  for kre  and
each state in the sample was used as gp in the sampling method described in section       for
p  v  t vp   tp  vgp   tgp    for the latter  however  the basic sampling method had to be extended
because even after processing ten billion gp states some of the entries in the   dimensional
matrix were missing or were not sampled sufficiently  to correct this  after we generate
gp  its children  and its grandchildren and update the matrix accordingly  we check if the
matrix already contains data for gps great grandchildren  if it does not then we generate
gps great grandchildren and update the corresponding entries in the matrix  this continues
as long as we encounter contexts that have never been seen before  this introduces a small
statistical bias into the sample  but it guarantees that the sample contains the required
data 

h

 states

  
  
  
  
  

      
      
      
      
     

  
  
  
  
  
  

     
     
     
     
     
     

kre
ida 
prediction
ratio
  puzzle depth   
     
     
    
     
     
    
   
     
    
   
     
    
   
     
    
   puzzle depth   
                      
    
                      
     
                      
     
                     
     
                            
                          

cdp 
prediction ratio
     
     
   
   
  

    
    
    
    
    

           
          
          
         
         
       

    
    
    
    
    
    

table    sliding tile puzzles with a consistent heuristic  md  
prediction results for kre and cdp  for the    and    puzzles are shown in table   in the
same format as above  for the   puzzle the predictions were made for an ida  threshold
  

fizahavi  felner  burch    holte

of    and each row corresponds to the group of all   puzzle states with the same heuristic
value h  shown in the first column  for which ida  would actually have used threshold
    the second column gives the number of states in each group  clearly  as shown in
the ida  column  for states with higher initial heuristic values ida  expanded a smaller
number of nodes  this trend is not reflected in the kre predictions since kre does not take
h into account  for kre the only difference between the attributes of different rows is the
different type distribution for the given group  thus  the predicted number of expanded
nodes of kre is very similar for all rows  around         the cdp formula takes the heuristic
value of the start state into account and was able to predict the number of expanded nodes
much better than kre  the bottom part of table   show results for the    puzzle for an
ida  threshold of     similar tendencies are observed 
    inconsistent heuristics for the sliding tile puzzle
our next experiment is for an inconsistent heuristic on the   puzzle  we defined two pdbs 
one based on the location of the blank and tiles     the other based on the location of the
blank and tiles     to create an inconsistent heuristic  only one of the pdbs was consulted
by a regular lookup  the choice of pdb was made systematically  not randomly  based on
the position of the blank  different occurrences of the same state were guaranteed to do
the same lookup but neighboring states were guaranteed to consult different pdbs and this
causes inconsistency  the results are presented in table    for a variety of ida  thresholds 
for each threshold the num column indicates how many start states were used  the
results show that cdps predictions are reasonably accurate  and very much more accurate
than kres which overestimate by up to a factor of    

d
  
  
  
  
  
  
  
  
  
  
  
  

num
      
      
      
      
      
      
      
      
      
     
     
   

ida 
    
    
    
    
    
    
     
     
     
     
     
       

kre
prediction
    
     
     
     
     
       
       
       
       
        
        
        

ratio
    
    
    
     
     
     
     
     
     
     
     
     

cdp 
prediction ratio
    
    
    
    
    
    
    
    
    
    
    
    
     
    
     
    
     
    
     
    
     
    
       
    

table     inconsistent heuristic for the   puzzle 
similar experiments were conducted for the    puzzle  here  the first pdb was based
on the location of the blank and tiles     the other was based on the location of the
blank and tiles      table    shows the results for ida  thresholds from    to     recall
that the median solution length for this puzzle is      the numbers shown are averages
  

fipredicting the performance of ida  using conditional distributions

over        start states  the cdp predictions for the    puzzle are considerably worse
than for the   puzzle  but the kre predictions have degraded much more  the reason for
the inaccuracy of these predictions was discussed in section        much more accurate
predictions are produced if the context is extended to include the heuristic value of both
pattern databases  not just the one that the search algorithm actually consults 

d
  
  
  
  
  
  
  
  

ida 
         
         
         
           
           
           
           
           

kre
prediction
             
             
               
               
               
                
                
                

ratio
       
       
       
       
       
       
       
       

cdp 
prediction
        
         
         
         
           
           
           
            

ratio
     
     
     
     
     
     
     
     

table     inconsistent heuristic for the    puzzle 

   accurate predictions for single start states
we have seen that cdp works well when the base cases of the recursive calculation of
ni  s  d  v  is seeded by a large set of start states  no matter how their heuristic values
are distributed  however  the actual number of expanded nodes for a specific single start
state can deviate from the number predicted by cdp  the conditional distribution reflects
the expected values over all nodes that share the same context  and the single start state
of interest might behave differently than the average state that has the same context 
consider a rubiks cube state with a heuristic value of    cdp  predicts that ida  will
expand             for such a state with ida  threshold     table   shows that on the
average  over        start states with a heuristic value of                states are expanded 
examining the results for the individual start states showed that the actual number of
expanded nodes ranged between             to              nodes 
in order to predict the number of expanded nodes for a single start state we propose the
following enhancement to cdp  suppose that we want to predict the number of expanded
nodes for ida  threshold d and start state s  first  we perform a small initial search from
s to depth r  we then use all the states at depth r to seed the base cases of the cdp formula
and compute the formula with ida  threshold d  r  this will cause a larger set of nodes
to be used in calculating ni  s  d  v   thereby improving the accuracy of cdps predictions 
    rubiks cube    edge pdb heuristic
table    shows results for four specific rubiks cube states with a heuristic value of    of
the regular   edge pdb lookup  when the ida  threshold was set to     we chose the
states with the least and greatest number of expanded nodes and two states around the
median  the first column shows the actual number of nodes ida  expands for each state 
  

fizahavi  felner  burch    holte

the next columns show the number of expanded nodes predicted by our enhanced cdp 
formula where the initial search was performed to depths  r  of         and    clearly  these
initial searches give much better predictions than the original cdp   with r       which
predicts             for all these states  with an initial search to depth    the predictions
are very accurate 
h
 
 
 
 

ida 
         
         
         
          

cdp   r   
         
         
         
         

cdp   r   
         
         
         
         

cdp   r   
         
         
         
          

cdp   r   
         
         
         
          

table     single state  d       

    rubiks cube        heuristic
section       presented kre predictions for two start states  s    with a heuristic value of
   and s     with a heuristic value of     for rubiks cube with the       heuristic  here
we repeat these experiments with cdp    tables    and    show the results with an initial
search of depth  r    and    the tables show that cdp  was able to achieve substantially
better predictions than kre in most cases  and that an initial search to depth   usually
improved cdp  s predictions 
d
  
  
  
  
  
  
  
  

ida 
      
       
         
          
           
             
              
               

kre
     
      
       
         
          
           
             
               

ratio
    
    
    
    
    
    
    
    

cdp   r   
      
       
         
          
           
             
              
               

ratio
    
    
    
    
    
    
    
    

cdp   r   
      
       
         
          
           
             
              
               

ratio
    
    
    
    
    
    
    
    

table           pdb  single start state s 
    experiments on the   puzzle   single start states
we performed experiments with the enhanced cdp  formula on all the states of the   puzzle
with the  consistent  md heuristic  we use the term trial to refer to each pair of a single
start state and a given ida  threshold d  the trials included all possible values of d and for
each d all start states for which ida  would actually perform a search with ida  threshold
d  predictions were made for each trial separately  and the relative error  predicted actual 
for the trial was calculated  the results are shown in figure    there are four curves in
the figure  for kre  for cdp  and for the enhanced cdp with initial search depths  r  of  
  

fipredicting the performance of ida  using conditional distributions

d
  
  
  
  
  
  
  

ida 
     
       
         
          
           
             
               

kre
      
       
         
          
           
             
               

ratio
    
    
    
    
    
    
    

cdp   r   
     
       
         
          
           
             
               

ratio
    
    
    
    
    
    
    

cdp   r   
     
       
         
          
           
             
               

ratio
    
    
    
    
    
    
    

table           pdb  single start state s  

cumulative percentage

   
  
  
  
kre
cdp 
cdp   radius   
cdp   radius    

  
 
 

 

 

 

 

 

 

 

 

 

  

predicted   actual

figure    relative error for the   puzzle
and     the x axis is relative error  the y axis is the percentage of trials for which the
prediction had a relative error of x or less  for example  the y value of     for the kre
curve at x       means that kre underestimated by a factor of   or more on     of the
trials  the rightmost point of the kre plot  x       y        indicates that on    of the
trials kres prediction was more than    times the actual number of nodes expanded  by
contrast cdp has a much larger percentage of highly accurate predictions  with over     of
its predictions within a factor of two of the actual number of nodes expanded  the figure
clearly shows the advantage of using the enhanced cdp  with an initial search to a depth
of         of the trials had predictions within     of the correct number 

   performance range for a given unconditional distribution
the experiments in this paper that have used the   edge pdb for rubiks cube have illustrated the fact that the number of nodes ida  expands given a pdb can vary tremendously
depending on how the pdb is used  zahavi et al          to see this clearly  the middle
three columns of table    show data that has already been seen in tables   and    namely 
the number of nodes ida  expands when the   edge pdb is used in the regular manner 
  

fizahavi  felner  burch    holte

with dual lookups  and with random symmetry lookups  ida  expands ten times fewer
nodes when the   edge pdb is consulted with random symmetry lookups than when it is
consulted in the normal way 
this raises the intriguing question of what range of performance can be achieved by
varying the conditional distribution when the unconditional distribution is fixed 
d
 
 
  
  
  
  
correlation

cdp
   
     
      
       
         
           

regular
   
     
      
       
         
           
     

dual
  
   
     
      
         
          
     

random symmetry
  
   
     
      
       
          
     

cdp
  
   
     
      
       
         

table     range of ida  performace for the   edge rubiks cube pdb

    upper limit
the upper extreme  which results in the most nodes expanded  occurs when a consistent
heuristic is used  this is because ida  only expands potential nodes  so the maximum
number of nodes is expanded when the conditional distribution is such that the parent of
every potential node at level i is a potential node at level i     an exact calculation of
the number of potential nodes in the brute force tree is therefore a theoretical upper bound
on the number of nodes ida  will expand for a given unconditional distribution  as we
have already discussed  one way to estimate the number of potential nodes is to use the
kre formula  this estimate of the upper bound of the number of nodes that ida  could
expand is denoted as cdp in table    
alternatively  the number of potential nodes can be approximated with the cdp formula
given the conditional distribution  consider equation    in the summation we consider all
possible vp values in     d i    as only these nodes are potential nodes at level i   thus
only these nodes are expanded by ida  at level i    and only these nodes can generate
children at level i   now  lets substitute this with vp      hmax    we now consider all
the nodes at level i     even the ones that are not potential nodes  using this in the
summation will calculate the number of all nodes with heuristic v at level i even ones that
are not actually generated be ida   because their parents were not potential nodes  i e 
with vp   d   i      this is shown in equation    
ni  s  v   

hx
max

ni   s  vp    bvp  p v vp  

    

vp   

   note that if the heuristic is consistent then only vp values in  v     v v      need to be considered in
the summation because nodes with other values of vp  smaller than v    or larger than v      cannot
generate children with a heuristic value of v 

  

fipredicting the performance of ida  using conditional distributions

using this in the general prediction equation we get 

cdp  

d x
di
x

ni  s  v 

    

i   v  

this gives an alternative method to approximate the number of potential nodes  both
these methods approximate this upper bound  in practice  however  it is possible that the
number of expanded nodes will slightly exceed this approximate bound due to noise and
small errors in the sampling or the calculations 
    lower limit
with consistent heuristics values of neighboring states are highly correlated  at the other
extreme are cases where there is no correlation between heuristic values of neighboring
nodes  that is  the heuristic value of a child node is statistically independent of the heuristic
value of its parent  this means that regardless of the parents heuristic value vp   the heuristic
values of the children are distributed according to the unconditional heuristic distribution 
i e   p v vp     p v  
our motivation for using this as an estimated lower bound on the number of nodes ida 
could expand for a given unconditional distribution is the empirical observation that the
number of nodes ida  expands decreases as the correlation between a parents heuristic
value and its childrens heuristic values decreases 
this is illustrated in the last row of the three middle columns of table     which shows
the correlation between the heuristic values of neighboring states when different types of
lookups are done in the   edge pdb  it was calculated using pearsons correlation coefficient 
defined over n pairs of x  y values according to the following equation

correlationxy

pn
pn
xi yi  i   xi i   yi
p pn
  p pn
pn
pn
n i   x i    i   xi    n i   yi     i   yi   
n

pn

i  

    

in order to calculate the correlation          random pairs of  xi  yi   neighboring states
were generated  their heuristic values were computed and used in equation     the bottom
row of table    shows that the number of nodes expanded decreases as the correlation
between neighboring heuristic values decreases  this leads us to suggest that the number
of nodes expanded will reach a minimum when the correlation is zero  
this estimated lower bound can be calculated using the cdp formula with p v vp     p v  
we denote this by cdp  for the   step model this would be calculated using the following
equations 
   in theory  it is possible for a heuristic to have a negative correlation between the parents heuristic value
and its childrens heuristic values  i e   parents with low heuristic values could tend to have children with
large heuristic values and vice versa  we believe this is unlikely to occur in practice 

  

fizahavi  felner  burch    holte

d i  

ni  s  d  v   

x

ni   s  d  vp    bvp  p v 

    

vp   

cdp  

di
d x
x

ni  s  d  v 

    

i   v  

as can be seen by comparing the rightmost two columns in table     the randomsymmetry use of the   edge pdb is within a factor of two of our estimated minimum
possible number of nodes expanded with this pdb  which suggests that to substantially
improve upon its performance one would have to use a different pdb 
table    shows the estimated upper and lower bounds of ida s performance  for a range
of ida  thresholds  for three different pdbs for rubiks cube  the bounds are calculated
using        random start states  the table shows that  according to these estimates  inconsistent heuristics based on the   edge pdb can outperform consistent heuristics based on
the   edge pdb but probably cannot outperform consistent heuristics based on the   edge
pdb since the estimated lower bound of the   edge pdb is larger than the estimated upper
bound of the   edge pdb 

d
 
 
  
  
  
  

  edge pdb
cdp
cdp
     
   
      
     
       
      
         
       
          
         
                        

  edge pdb
cdp
cdp
   
  
     
   
      
     
       
      
         
       
                     

  edge pdb
cdp
cdp
  
  
   
  
     
   
      
     
       
      
                  

table     estimated bounds on performance for three rubiks cube pdbs 

    predicting the performance of ida  with bpmx
with an inconsistent heuristic  the heuristic value of a child can be much larger than that
of the parent  if this happens in a state space with undirected edges  the childs heuristic
value can be propagated back to the parent  if this causes the parents f  value to exceed
the ida  threshold the entire search subtree rooted at the parent can be pruned without
generating any of the remaining children  this propagation technique is called bidirectional
pathmax  bpmx   felner et al         zahavi et al          it was shown to be very effective
in reducing the search effort by pruning subtrees that would otherwise be explored  we
now show how to modify cdp to handle bpmx propagation  since bpmx only applies to
state spaces with undirected edges  the discussion in this section is limited to such spaces 
  

fipredicting the performance of ida  using conditional distributions

     bidirectional pathmax  bpmx 
traditional pathmax  mero        propagates heuristic values from a parent to its children 
and can be applied in any state space  admissibility is preserved by subtracting the cost
of the connecting edge from the heuristic value  the basic insight of bidirectional pathmax
 bpmx  is that when edges are undirected heuristic values can propagate to all neighbors 
which includes from a child node to its parent  this process can continue to any distance
in any direction  bpmx is illustrated in figure    the left side of the figure shows the
inconsistent heuristic values for a node and its two children  consider the left child with a
heuristic value of    since this value is admissible and all edges in this example have a cost
of one  all its immediate neighbors are at least   moves away from the goal  their neighbors
are at least   moves away  and so on  when the left child is generated  its heuristic value
 h      can propagate up to the parent and then down again to the right child  to preserve
admissibility  each propagation along a path reduces h by the cost of traversing the path 
this results in h     for the root and h     for the right child  when using ida   this
bidirectional propagation may cause many nodes to be pruned that would otherwise be
expanded  for example  suppose the current ida  threshold is    without the propagation
of h from the left child  both the root node  f   g   h              and the right child
 f   g   h              would be expanded  using the above propagation  the left child
will increase the parents h value to    resulting in search at this node being abandoned
without even generating the right child 

 

 
 

 

 

 

figure    propagation of values with inconsistent heuristics

     cdp overestimates when bpmx is applied
when an inconsistent heuristic is being used and bpmx is applied  cdp will overestimate
the number of expanded nodes because it will count the nodes in subtrees that bpmx
prunes  in section      we defined ni  s  d  v  to be the number of nodes that ida  will
generate at level i with a heuristic value exactly equal to v when s is the start state and d
is the ida  threshold  the formula given for estimating ni  s  d  v   equation    was 
d i  

ni  s  d  v   

x

ni   s  d  vp    bvp  p v vp  

vp   

in calculating ni  s  d  v  from ni   s  d  vp   this formula assumes that when a node is
expanded all its children are generated  this is why ni   s  d  vp   is multiplied by the
branching factor bvp   when bpmx is applied  a child may prune the parent before the
rest of the children are generated  if this happens  the assumption that all the children
of expanded nodes are generated would be wrong  for example  without bpmx  while
  

fizahavi  felner  burch    holte

expanding the root of the left tree in figure   both children are generated and the child on
the right is also expanded  indeed cdp will count two nodes in this case  when bpmx is
applied the root is expanded but the child on the right will not be generated  and therefore
not expanded   thus  cdp  which counts the two nodes  is overestimating the number of
nodes expanded  in the following section we modify our equation to correct this 
     new formula for estimating ni  s  d  v 
let n be the node that is currently being expanded  assume that n has b children and
consider the order in which they are generated  we call this order the generation order 
note that when bpmx is applied  the probability that a child will be generated decreases
as we move through the generation order  children that appear late in the order will have
a larger chance of not being generated since there are more previous children that might
cause a bpmx cutoff  let pbx  l  be the probability that the child in location l in the order
will be generated even if bpmx is applied  with this definition we can extend equation  
as follows 
d i   bvp

ni  s  d  v   

x x
 ni   s  d  vp    pbx  l   p v vp   
vp   

    

l  

ni  s  d  v  is being calculated in a similar way to equation    except for the way we count
the total number of children ida  generates via the nodes it expands at level i    with
heuristic value equal to vp   the idea here is to iterate over all the possible locations in the
generation order and calculate the probability that a node in location l will be generated 
in practice  however  the actual context for pbx has other variables besides the location l 
it also includes the ida  threshold  d   the depth of the parent  i     and the heuristic
value of the parent  vp    we thus get our final formula 
d i   bvp

ni  s  d  v   

x x
 ni   s  d  vp    pbx  l  d  i     vp    p v vp   
vp   

    

l  

this is exactly equal to equation   in the special case when pbx  l      for all l  which
happens when bpmx is not used or when it is used with a consistent heuristic 
     calculating pbx
for simplicity  our model assumes that a heuristic value can only be propagated by bpmx
one level up the tree  this means that a state can be pruned only from its immediate
children and not by descendants at deeper levels  we make this assumption for another
reason besides simplicity of description  our experiments with rubiks cube and other
domains showed that indeed almost all the pruning of bpmx was caused by a   level bpmx
propagation  a generalized formula with deeper bpmx propagations can be similarly
developed but it will include complicated recursive terms with very low practical value  at
least for the state spaces and heuristics we have studied 
assume that c is a child of n in location l of the generation order  child c will be
generated only if n is not pruned by any of its l    children that appear before c in the
  

fipredicting the performance of ida  using conditional distributions

generation order  assume that n is at level i and that the threshold is d  since n is
expanded  h n   d  i  with bpmx h n  can be increased  and cause bpmx pruning 
from a child k if h k    d  i      in this case  h k     is larger than d  i  so when it
is used instead of h n  ida  will decide not to expand n and no additional children will
be generated  therefore  in order for a child c in location l of the generation order to be
generated  all its l    predecessors in the generation order must have heuristics less than
or equal to d  i      assuming that the heuristic value of the parent is v the probability of
this will be
pbx  l  d  i  v     

di  
x

p h v  l 

    

h  

we sum up the probability of each relevant heuristic value and raise the sum to the
power of l    since l    children appear before c 
     experiments on rubiks cube with bpmx
we repeated the experiments on rubiks cube with the   edge pdb but with bpmx
activated  since bpmx affects only inconsistent heuristics  only the dual and random
symmetry heuristics were tested  each heuristic was tested for ida  thresholds   through
    the results  averaged over the same set of        random states  are presented in
table     the no bpmx columns are repeated from table    the additional columns
show our results with bpmx  the column ida    bpmx presents the actual number of
expanded nodes when using bpmx  bpmx reduces the number of nodes expanded by more
than     for the dual and by more than     reduction for the random symmetry  making
the unmodified cdp  s predictions high by about the same amount  the cdpbx
   column
shows that the modifications introduced in this section greatly improve the accuracy 

d

ida 

 
 
  
  
  
  

  
   
     
      
         
          

 
 
  
  
  
  

  
   
     
      
       
          

no bpmx
cdp 

with bpmx
ratio ida    bpmx
cdpbx
 
dual
  
    
  
  
   
    
   
   
     
    
     
     
      
    
      
      
         
    
       
       
          
    
                     
random symmetry
  
    
  
  
   
    
   
   
     
    
     
     
      
    
      
      
       
    
       
       
          
    
         
         

ratio
    
    
    
    
    
    
    
    
    
    
    
    

table     bpmx on rubiks cube   dual   random symmetry

  

fizahavi  felner  burch    holte

    related work
previous work on predicting a  or ida s performance from properties of a heuristic falls
into two main camps  the first bases its analysis on the accuracy of the heuristic  while
the second bases its analysis  as we have done  on the distribution of heuristic values  the
next two subsections survey these approaches 
     analysis based on a heuristics accuracy
one common approach is to characterize a heuristic by focusing on the error in the heuristic
value  deviation from the optimal cost   the first analysis in this line  focusing on the effect
of errors on the performance of search algorithms  was done by pohl         many other
papers in this line have appeared since  pohl        gaschnig        huyn  dechter   
pearl        karp   pearl        pearl        chenoweth   davis        mcdiarmid  
provan        sen  bagchi    zhang        dinh  russell    su        helmert   roger 
      
these works usually assume an abstract model space of a tree where every node has
exactly b children and aim to provide the asymptotic estimation for the number of expanded
nodes  they mainly differ by the model assumptions  e g  binary or non binary trees  and
for what case the results are derived  worst case or average case   worst case analysis
showed that there is a correlation between the
heuristic errors and the search complexity 
 h n h  n  
they found that if the relative error 
  is constant  the search complexity will
h  n 
be exponential  in the length of solution path  but if the absolute error   h n   h  n    is
bounded by a constant the search complexity is linear  pohl        gaschnig         three
main assumptions used by pohl        are that the branching factor is assumed to be
constant across inputs  that there is a single goal state and that there are no transpositions
in the search space  when these assumptions do not hold  as is the case for many standard
benchmark domains in planning  general search algorithms such as a  explore exponential
number of states even under the assumption of an almost perfect heuristic  i e   a heuristic
whose error is bounded by a small additive constant   helmert   roger        
since it is difficult to guarantee precise bounds on the magnitude of errors produced by
a given heuristic  a probabilistic characterization of these magnitudes was suggested  huyn
et al         pearl         heuristics are modeled as random variables  rvs   and the relative
errors are assumed to be independent and identically distributed  iid model   in this model 
attaining an average polynomial a  complexity was proved to be essentially equivalent to
requiring that values of h n  be clustered near h  n  where the allowed deviation is a
logarithmic function of h  n  itself 
additional research in this line was conducted by chenoweth and davis         instead
of using the iid model  they suggested using the nc model  which places no constraints
on the errors of h  with this model the heuristic is defined according to how the heuristic values grow with respect to the distance to the goal  and not according to the error 
they predicted that a  complexity will be polynomial whenever the values of h n  are
logarithmicaly clustered near h  n     h  n    where  is an arbitrary  non negative  and
non decreasing function  heuristics whose values grow slower than the distance to the
goal cause exponential complexity  studies with the nc model showed that replacing a
  

fipredicting the performance of ida  using conditional distributions

heuristic h with wh for some w    can often change a  complexity from exponential to
polynomial 
most of these works focused on tree searches  by contrast  sen et al         presented
a general technique for extending the analysis of the average case performance of a  from
search spaces that are trees to search spaces that are directed acyclic graphs  their analytical results show that the expected complexity can change from exponential to polynomial
as the heuristic estimates of nodes become more accurate and restrictions are placed on the
cost matrix  recent research in this line  analyzing the complexity of the a  algorithm was
presented by dinh et al          this research presented both worst and average case analysis for the performance of a  for approximately accurate heuristics  for search problems
with multiple solutions  bounds presented in that paper have been proved to be dependent
on the heuristic accuracy and distribution of solutions 
     analysis based on the heuristic distribution
as discussed at the outset of this paper  kre suggested an alternative approach for calculating the time complexity of ida  on multiple goal spaces  korf   reid        korf
et al          arguing that the heuristic accuracy is very difficult to obtain  they suggested
deriving the analysis from the unconditional distribution of heuristic values  which is easy
to determine at least approximately  they also came up with a method for deriving a
closed form formula for ni   the number of nodes at level i of the brute force search tree 
that method was later formalized  edelkamp      b   unlike the work described in the
previous subsection  which provides a big o complexity analysis  kres aim  and ours  is
to exactly predict the number of nodes ida  will expand 
kre correctly point out that  when operators do not all have the same cost  ni must be
defined as the number of nodes that can be reached by a path of cost i  as opposed to the
number of nodes that are i edges from the start state  the calculation of ni in this more
general setting has been studied in detail by ruml  in a slightly different context  ruml 
       his solution involves using a conditional distribution for edge costs that bears a
strong resemblance to our conditional distribution on heuristic values 
based on the work of kre and on the insight that for pdb heuristics there is a correlation
between the size of the pdb and its heuristic value distribution  a new analysis limited to
pdb heuristics has been done  korf        breyer   korf         the prediction is achieved
based on the branching factor of the problem and the size of the pdb without knowing
the actual heuristic distribution  in order to derive the heuristic distribution from the
size of the pdb it was assumed that the forward and backward branching factors of the
abstract space are equal and that the abstract space has a negligible number of cycles 
since the second assumption is usually not realistic this model underestimates the number
of expanded nodes 
the kre formula was developed to predict the performance of the ida  algorithm  the
general approach can also be applied to a  as long as appropriate modifications are made
to the computations of ni and p  v   korf et al         holte   hernadvolgyi        breyer
  korf         the challenge is accounting for the effect of a s pruning of the search
tree when it generates a state that it has previously reached by a path of smaller or equal
   a heuristic is an  approximation if      h  s   h s         h  s  for all states in the search space 

  

fizahavi  felner  burch    holte

cost  this is particularly challenging when the heuristic is inconsistent  because in that case
the first time a  generates a state it is not guaranteed to have reached it via a least cost
path  so the state will occur more than once in a s search tree  indeed  in the worst case 
for every state a  will enumerate all the paths to the state in decreasing order of cost 
thereby generating exactly the same search tree as ida   martelli         but in general 
a s pruning will reduce ni   especially for large i  in ways that may be hard to capture
in a small set of recurrence equations  the heuristic distribution over a s entire search
tree  taken to its maximum depth  is  for consistent heuristics  the overall distribution  korf
et al         since each state occurs exactly once in a s search tree  as just observed  this
is not true for inconsistent heuristics   this does not imply that the overall distribution can
be used to good effect on a level by level basis  but its use in the kre formula did result
in accurate predictions of a s performance on the    puzzle for two different consistent
heuristics when used together with an exact calculation of ni for a s search tree  breyer
  korf        

    conclusions and future work
historically  heuristics were characterized by their average  kre introduced the idea of
characterizing heuristics by their unconditional heuristic distribution and presented their
formula to predict the number of nodes expanded on one iteration of ida  based on the
unconditional heuristic distribution  the work we have presented in this paper takes another
step along this line  the conditional distribution we have introduced  and the prediction
formula cdp based on it  advance our understanding of how properties of a heuristic affect
the performance of ida  
our cdp method advances kre by improving its predictions at shallow depths  on a
wider range of sets of start states  and for inconsistent heuristics  we have also shown how
to use it to make an accurate prediction for a single start state and for an ida  search that
uses bpmx heuristic value propagation 
of course  with the more sophisticated methods  more preprocessing is needed and
special care must be taken when gathering the data in order to get a reliable sample  it
is much easier to calculate the average of the heuristic than to calculate a   dimensional
matrix  on the other hand  the latter approach better characterizes the heuristic and
enables generating accurate predictions for a larger variety of circumstances 
future work will address a number of issues  it is not yet clear what attributes make
the best context for prediction  and how this is influenced by the choice of the heuristic
and by the attributes of the specific domain  larger contexts  more parameters  will probably provide better prediction at a cost of more pre processing  this tradeoff needs to be
further studied  another direction will aim to extend this analysis approach to predict the
performance of other search algorithms such as a  

    acknowledgments
this research was supported by grant number        and        from the israeli science
foundation  isf  to ariel felner  robert holte and neil burch gratefully acknowledge the
ongoing support for this work from canadas natural sciences and engineering research
  

fipredicting the performance of ida  using conditional distributions

council  nserc  and albertas informatics circle of research excellence  icore   the
code for rubiks cube in this paper is based on the implementation of richard e  korf
used in his seminal work on this domain korf         we thank the anonymous reviewer
who encouraged us to widen our experimental results and to better explain the results of
kre and their relation to our results  his her comments clearly improved the strength of
this paper  thanks also to sandra zilles for her careful checking of the details in section   

references
breyer  t     korf  r          recent results in analyzing the performance of heuristic
search  in proceedings of the first international workshop on search in artificial
intelligence and robotics  held in conjunction with aaai   pp       
chenoweth  s  v     davis  h  w          high performance a  search using rapidly
growing heuristics  in proceedings of the twelfth international joint conference on
artificial intelligence  ijcai      pp         
culberson  j  c     schaeffer  j          efficiently searching the    puzzle  tech  rep 
       department of computer science  university of alberta 
culberson  j  c     schaeffer  j          pattern databases  computational intelligence 
               
dinh  h  t   russell  a     su  y          on the value of good advice  the complexity of
a  search with accurate heuristics  in proceedings of the twenty second conference
on artificial intelligence  aaai      pp           
edelkamp  s       a   planning with pattern databases  in proceedings of the  th european
conference on planning  ecp      pp       
edelkamp  s       b   prediction of regular search tree growth by spectral analysis 
in advances in artificial intelligence  joint german austrian conference on ai 
 ki ogai        pp         
felner  a   korf  r  e     hanan  s          additive pattern database heuristics  journal
of artificial intelligence research             
felner  a   korf  r  e   meshulam  r     holte  r  c          compressed pattern databases 
journal of artificial intelligence research             
felner  a   zahavi  u   schaeffer  j     holte  r  c          dual lookups in pattern
databases  in proceedings of the nineteenth international joint conference on artificial intelligence  ijcai      pp         
gaschnig  j          performance measurement and analysis of certain search algorithms 
ph d  thesis  carnegie mellon university 
hart  p  e   nilsson  n  j     raphael  b          a formal basis for the heuristic determination of minimum cost paths  ieee transactions on systems science and cybernetics 
scc              
helmert  m     roger  g          how good is almost perfect   in proceedings of the
twenty third conference on artificial intelligence  aaai      pp         
  

fizahavi  felner  burch    holte

holte  r  c   felner  a   newton  j   meshulam  r     furcy  d          maximizing over
multiple pattern databases speeds up heuristic search  artificial intelligence                       
holte  r  c     hernadvolgyi  i  t          steps towards the automatic creation of search
heuristics  tech  rep  tr       computing science department  university of alberta 
huyn  n   dechter  r     pearl  j          probabilistic analysis of the complexity of a  
artificial intelligence                 
karp  r  m     pearl  j          searching for an optimal path in a tree with random costs 
artificial intelligence                  
korf  r  e          depth first iterative deepening  an optimal admissible tree search 
artificial intelligence                
korf  r  e          finding optimal solutions to rubiks cube using pattern databases 
in proceedings of the fourteenth conference on artificial intelligence  aaai      pp 
       
korf  r  e          analyzing the performance of pattern database heuristics  in proceedings
of the twenty second conference on artificial intelligence  aaai      pp           
korf  r  e     felner  a          disjoint pattern database heuristics  artificial intelligence 
               
korf  r  e     reid  m          complexity analysis of admissible heuristic search  in
proceedings of the fifteenth conference on artificial intelligence  aaai      pp     
    
korf  r  e   reid  m     edelkamp  s          time complexity of iterative deepening a   
artificial intelligence                    
martelli  a          on the complexity of admissible search algorithms  artificial intelligence         
mcdiarmid  c  j  h     provan  g  m          an expected cost analysis of backtracking
and non backtracking algorithms  in proceedings of the twelfth international joint
conference on artificial intelligence  ijcai      pp         
mcnaughton  m   lu  p   schaeffer  j     szafron  d          memory efficient a  heuristics
for multiple sequence alignment  in proceedings of the eighteenth conference on
artificial intelligence  aaai      pp         
mero  l          a heuristic search algorithm with modifiable estimate  artificial intelligence               
pearl  j          heuristics  intelligent search strategies for computer problem solving 
addison   wesley 
pohl  i          heuristic search viewed as path finding in a graph  artificial intelligence 
              
pohl  i          practical and theoretical considerations in heuristic search algorithms 
machine intelligence          
  

fipredicting the performance of ida  using conditional distributions

ratner  d     warmuth  m  k          finding a shortest solution for the n  n extension
of the    puzzle is intractable  in proceedings of the fifth conference on artificial
intelligence  aaai      pp         
ruml  w          adaptive tree search  ph d  thesis  harvard university 
sen  a  k   bagchi  a     zhang  w          average case analysis of best first search in
two representative directed acyclic graphs  artificial intelligence                    
zahavi  u   felner  a   burch  n     holte  r  c          predicting the performance of
ida  with conditional distributions  in proceedings of the twenty third conference
on artificial intelligence  aaai      pp         
zahavi  u   felner  a   holte  r     schaeffer  j          dual search in permutation
state spaces  in proceedings of the twenty first conference on artificial intelligence
 aaai      pp           
zahavi  u   felner  a   holte  r  c     schaeffer  j          duality in permutation state
spaces and the dual search algorithm  artificial intelligence                    
zahavi  u   felner  a   schaeffer  j     sturtevant  n  r          inconsistent heuristics 
in proceedings of the twenty second conference on artificial intelligence  aaai     
pp           
zhou  r     hansen  e  a          space efficient memory based heuristics  in proceedings
of the nineteenth conference on artificial intelligence  aaai      pp         

  

fi