journal artificial intelligence research              

submitted       published     

induction first order decision lists 
results learning past tense english verbs

raymond j  mooney
mary elaine califf

department computer sciences  university texas
austin  tx           

mooney cs utexas edu
mecaliff cs utexas edu

abstract

paper presents method inducing logic programs examples learns
new class concepts called first order decision lists  defined ordered lists clauses
ending cut  method  called foidl  based foil  quinlan       
employs intensional background knowledge avoids need explicit negative examples  particularly useful problems involve rules specific exceptions 
learning past tense english verbs  task widely studied context
symbolic connectionist debate  foidl able learn concise  accurate programs
problem significantly fewer examples previous methods  both connectionist
symbolic  

   introduction

inductive logic programming  ilp  growing subtopic machine learning studies
induction prolog programs examples presence background knowledge
 muggleton        lavrac   dzeroski         due expressiveness first order logic 
ilp methods learn relational recursive concepts cannot represented
attribute value representations assumed machine learning algorithms  ilp methods successfully induced small programs sorting list manipulation  shapiro 
      sammut   banerji        muggleton   buntine        quinlan   cameron jones 
      well produced encouraging results important applications predicting protein secondary structure  muggleton  king    sternberg        automating
construction natural language parsers  zelle   mooney      b  
however  current ilp techniques make important assumptions restrict application  three common assumptions 
   background knowledge provided extensional form set ground literals 
   explicit negative examples target predicate available 
   target program expressed  pure  prolog clause order irrelevant
procedural operators cut     disallowed 
currently well known successful ilp systems  golem  muggleton   feng 
      foil  quinlan         make three assumptions  however 
assumptions brings significant limitations since 
   adequate extensional representation background knowledge frequently infinite
intractably large 

c      ai access foundation morgan kaufmann publishers  rights reserved 

fimooney   califf

   explicit negative examples frequently unavailable adequate set negative
examples computed using closed world assumption infinite intractably large 
   concise representation many concepts requires use clause ordering and or
cuts  bergadano  gunetti    trinchero        
paper presents new ilp method called foidl  first order induction decision lists  helps overcome limitations incorporating following
properties 
   background knowledge represented intensionally logic program 
   explicit negative examples need supplied constructed  assumption
output completeness used instead implicitly determine hypothesized
clause overly general and  so  quantify degree over generality simply
estimating number negative examples covered 
   learned program represented first order decision list  ordered set
clauses ending cut  representation useful problems
best represented general rules specific exceptions 
name implies  foidl closely related foil follows similar top down 
greedy specialization guided information gain heuristic  however  algorithm
substantially modified address three advantages listed above  use intensional
background knowledge fairly straightforward incorporated previous foil
derivatives  lavrac   dzeroski        pazzani   kibler        zelle   mooney      b  
development foidl motivated failure observed applying existing ilp methods particular problem  learning past tense english
verbs  problem studied fairly extensively using connectionist symbolic methods  rumelhart   mcclelland        macwhinney   leinbach        ling 
       however  previous efforts used specially designed feature based encodings impose fixed limit length words fail capture position independence
underlying transformation  believed representing problem constructing logic program predicate past x y  x words represented
lists letters  e g past  a c t    a c t e d    past  a c h e    a c h e d   
past  a r i s e    a r o s e    would produce much better results  however  due
limitations mentioned above  unable get reasonable results either foil
golem  however  overcoming limitations  foidl able learn highly accurate programs past tense problem many fewer examples required
previous methods 
remainder paper organized follows  section   provides important background material foil past tense learning problem  section   presents
foidl algorithm details incorporates three advantages discussed above  section   presents results learning past tense english verbs demonstrating
foidl out performs previous methods problem  section   reviews related work 
section   discusses limitations future directions  section   summarizes presents
conclusions 
 

fiinduction first order decision lists  learning english past tense

   background

since foidl based foil  section presents brief review important ilp system  quinlan         quinlan cameron jones         cameron jones quinlan
       provide complete description  section presents brief review
previous work english past tense problem 

    foil

foil learns function free  first order  horn clause definition target predicate terms
background predicates  input consists extensional definitions
predicates tuples constants specified types  example  input appropriate
learning definition list membership is 
member elt lst      a  a     a  a b     b  a b     a  a b c        
components lst elt lst       a  a        a b  a  b      a b c  a  b c       

elt type denoting possible elements includes a b c  d  lst
type defined consisting lists containing three elements 
components a b c  background predicate true iff list whose first element b whose rest list c  this must provided place function
list construction   foil requires negative examples target concept 
supplied directly computed using closed world assumption  example 
closed world assumption would produce pairs form  elt lst  explicitly provided positive examples  e g    b  a    
given input  foil learns program one clause time using greedy covering
algorithm summarized follows 
let positives to cover   positive examples 
positives to cover empty
find clause  c   covers preferably large subset positives to cover
covers negative examples 
add c developing definition 
remove examples covered c positives to cover 
example  clause might learned member one iteration loop is 
member a b     components b a c  

since covers positive examples element first one list
cover negatives  clause could learned cover remaining examples is 
member a b     components b c d   member a d  

together two clauses constitute correct program member 
 find clause  step implemented general to specific hill climbing search
adds antecedents developing clause one time  step  evaluates possible
literals might added selects one maximizes information gain heuristic 
algorithm maintains set tuples satisfy current clause includes bindings
new variables introduced body  following pseudocode summarizes
procedure 
 

fimooney   califf

initialize c r v   v        vk      r target predicate arity k 
initialize contain positive tuples positives to cover negative tuples 
contains negative tuples
find best literal l add clause 
form new training set containing tuple satisfies l 
tuples form b  t b concatenated  b set bindings
new variables introduced l literal satisfied
 i e   matches tuple extensional definition predicate  
replace  
foil considers adding literals possible variablizations predicate long
type restrictions satisfied least one arguments existing variable
bound head previous literal body  literals evaluated based
number positive negative tuples covered  preferring literals cover many positives
negatives  let t  denote number positive tuples set define 
 t       log   t   jt j  
   
chosen literal one maximizes 
gain l     i  t      t    
   
number tuples extensions  i e   number current
positive tuples covered l  
foil includes many additional features as  heuristics pruning space
literals searched  methods including equality  negation failure  useful literals
immediately provide gain  determinate literals   pre pruning post pruning
clauses prevent over fitting  methods ensuring induced programs
terminate  papers referenced consulted details
features 
 

 

 

 

    learning past tense english verbs

rumelhart mcclelland        first build computational model pasttense learning using classic perceptron algorithm special phonemic encoding
words employing so called wickelphones wickelfeatures  general goal show
connectionist models could account interesting language learning behavior
previously thought require explicit rules  model heavily criticized opponents
connectionist approach language acquisition relatively poor results achieved
heavily engineered representations training techniques employed  pinker  
prince        lachter   bever         macwhinney leinbach        attempted
address criticisms using standard multi layer backpropagation learning
algorithm simpler unibet encoding phonemes  in    phonemes
encoded single ascii character  
ling marinov        ling        criticize current connectionist models past tense acquisition heavily engineered representations poor experimental
methodology  present systematic results system called spa  symbolic pattern associator  uses slightly modified version c     quinlan        build
 

fiinduction first order decision lists  learning english past tense

forest decision trees maps fixed length input pattern fixed length output pattern  ling s        head to head results show spa generalizes significantly better
backpropagation number variations problem employing different phonemic
encodings  e g       vs      given     training examples  
however  previous work encodes problem fixed length pattern association fails capture generativity position independence true transformation  example  use    letter patterns like 
a c t                            a c t e d                    

unibet phonemic encoding 
  k t                              k t i d                    

separate decision tree output unit used predict character output
pattern input characters  therefore  learning general rules   add
 ed    must repeated position word end  words longer   
characters cannot handled  also  best results spa exploit highly engineered
feature template modified version c    s default leaf labeling strategy tailor
string transformation problems 
although ilp methods seem appropriate problem  initial attempts
apply foil golem past tense learning gave disappointing results  califf 
       below  discuss three problems listed introduction contribute
diculty applying current ilp methods problem 
principle  background predicate append sucient constructing accurate
past tense programs incorporated ability include constants arguments
or  equivalently  ability add literals bind variables specific constants  called
theory constants foil   however  background predicate allow appending
empty list appropriate  use predicate called split a  b  c 
splits list two non empty sublists b c  intensional definition split is 
split  x    z    x     y   z   
split  x   y    x   w   z     split y w z  

using split   add  ed   rule represented as 
past a b     split b a  e d   

which  foil  learned form 
past a b     split b a c   c    e d  

providing extensional definition split includes possible strings    fewer
characters  at least      strings  clearly intractable  however  providing partial definition includes possible splits strings actually appear training corpus
possible generally sucient  therefore  providing adequate extensional background
knowledge cumbersome requires careful engineering  however  major
problem 
supplying appropriate set negative examples problematic  using closedworld assumption produce pairs words training set second
past tense first feasible useful  case  clause 
 

fimooney   califf

past a b     split b a c  

likely learned since covers positives  if any 
negatives since unlikely word prefix another word past
tense  however  clause useless producing past tense novel verbs  and 
domain  accuracy must measured ability actually generate correct output
novel inputs  rather ability classify pre supplied tuples arguments positive
negative  obvious solution supplying strings    characters less
negative examples past tense word clearly intractable  providing specially
constructed  near miss  negative examples past  a c h e   a c h e e d   
helpful  requires careful engineering exploits detailed prior knowledge
problem 
order address problem negative examples  quinlan        applied
foil problem  employed different target predicate representing pasttense transformation   used three place predicate past x y z  true iff
input word x transformed past tense form removing current ending
substituting ending z  example  past  a c t        e d    past  a r i s e  
 i s e    o s e    simple preprocessor map data two place predicate
form  since sample     verb pairs contains       different end fragments 
results manageable number closed world negatives  approximately     
every positive example training set  using approach unibet phonemic
encodings  quinlan obtained slightly better results ling s best spa results exploited highly engineered feature template        vs            training examples 
significantly better spa s normal results          although three place target predicate incorporates knowledge desired transformation  arguably
requires less representation engineering previous methods 
however  quinlan        notes results still hampered foil s inability
exploit clause order  example  using normal alphabetic encoding  foil quickly
learns clause sucient regular verbs 
past a b c     b     c  e d  

however  since clause still covers fair number negative examples due many
irregular verbs  continues add literals  result  foil creates number specialized
versions clause together still fail capture generality underlying
default rule  problem compounded foil s inability add constraints
 does end  e    since foil separates addition literals containing variables
binding variables constants using literals form v   c  cannot learn clauses
like 
past a b c     b     c  e d   not split a d  e    

since word split several ways  clearly equivalent learnable
clause 
past a b c     b     c  e d   not split a d e    e     e  

   quinlan s work problem motivated early attempts use foil 

 

fiinduction first order decision lists  learning english past tense

consequently  must approximate true rule learning many clauses form 
past a b c     b     c  e d   split a d e   e    b  
past a b c     b     c  e d   split a d e   e    d  
   

result  foil generated overly complex programs containing    clauses
phonemic alphabetic versions problem 
however  experienced prolog programmer would exploit clause order cuts
write concise program first handles most specific exceptions falls
more general default rules exceptions fail apply  example  program 
past a b 
past a b 
past a b 
past a b 

     

split a c  e e p    split b c  e p t      
split a c  y    split b c  i e d      
split a c  e    split b a  d      
split b a  e d   

summarized as 
word ends  eep   replace  eep   ept   e g   sleep  slept  
else  word ends  y   replace  y   ied 
else  word ends  e   add  d 
else  add  ed  
foidl directly learn programs form  i e   ordered sets clauses ending
cut  call programs first order decision lists due similarity propositional
decision lists introduced rivest         foidl uses normal binary target predicate
requires explicit negative examples  therefore  believe requires significantly
less representation engineering previous work area 

   foidl induction algorithm

stated introduction  foidl adds three major features foil     intensional
specification background knowledge     output completeness substitute explicit
negative examples     support learning first order decision lists  following
subsections describe modifications made incorporate features 

    intensional background

described above  foil assumes background predicates provided extensional
definitions  however  burdensome frequently intractable  providing intensional definition form general prolog clauses generally preferable  example 
instead providing numerous tuples components predicates  easier give
intensional definition 
components  a   b   a  b  

intentional background definitions restricted function free pure prolog
exploit features language 
 

fimooney   califf

modifying foil use intensional background straightforward  instead matching
literal set tuples determine whether covers example 
prolog interpreter used attempt prove literal satisfied using
intensional definitions  unlike foil  expanded tuples maintained positive
negative examples target concept reproved alternative specialization
developing clause  therefore  pseudocode learning clause simply 
initialize c r v   v        vk      r target predicate arity k 
initialize contain examples positives to cover negative examples 
contains negative tuples
find best literal l add clause 
let subset examples still proved instances
target concept using specialized clause 
replace
since expanded tuples produced  information gain heuristic picking best
literal simply 
gain l    jt j  i  t      t    
   
 

 

 

 

    output completeness implicit negatives

order overcome need explicit negative examples  mode declaration
target concept must provided  i e   specification whether argument input    
output       assumption output completeness made  indicating
every unique input pattern training set  training set includes correct
output patterns  therefore  output program produces given input
assumed represent negative example  require positive
examples part training set  unique input pattern training
set  positive examples input pattern  if any  must training
set  assumption trivially met predicate represents function single
unique output input 
example  assumption output completeness mode declaration past     
indicates correct past tense forms included input word
training set  predicates representing functions  past  implies
output example unique outputs implicitly represent negative examples  however  output completeness applied non functional cases
append         indicating possible pairs lists appended
together produce list included training set  e g   append     a b   a b   
append  a   b   a b    append  a b      a b    
given output completeness assumption  determining clause overly general
straightforward  positive example  output query made determine
outputs given input  e g   past  a c t   x    outputs generated
positive examples  clause still covers negative examples requires
specialization  note intensional interpretation learned clauses required order
answer output queries 
addition  order compute gain alternative literals specialization 
negative coverage clause needs quantified  incorrect answer output
 

fiinduction first order decision lists  learning english past tense

query ground  i e   contains variables  clearly counts single negative example  e g   past  a c h e    a c h e e d     however  output queries frequently
produce answers universally quantified variables  example  given overly general
clause past a b     split a c d    query past  a c t   x  generates answer
past  a c t   y   implicitly represents coverage infinite number negative
examples  order quantify negative coverage  foidl uses parameter u represent
bound number possible terms  since set possible terms  the herbrand
universe background knowledge together examples  generally infinite  u
meant represent heuristic estimate finite number terms ever
actually occur practice  e g   number distinct words english   negative coverage represented non ground answer output query estimated uv   p 
v number variable arguments answer p number positive
examples answer unifies  uv term stands number unique
ground outputs represented answer  e g   answer append x y  a b   stands
 
u different ground outputs  p term stands number represent
positive examples  allows foidl quantify coverage large numbers implicit
negative examples without ever explicitly constructing them  generally sucient
estimate u fairly large constant  e g          empirically method
sensitive exact value long significantly greater number ground
outputs ever generated clause 
unfortunately  estimate sensitive enough  example  clauses
past a b     split a c d  
past a b     split b a c  

cover u implicit negative examples output query past  a c t   x  since first
produces answer past  a c t   y  second produces answer past  a c t  
 a c t   y    however  second clause clearly better since least requires output input sux added  since presumably words
words start  a c t   assuming total number words finite  
first clause considered cover negative examples  therefore  arguments
partially instantiated   a c t   y   counted fraction
variable calculating v   specifically  partially instantiated output argument scored
fraction subterms variables  e g    a c t   y  counts    
variable argument  therefore  first clause scored covering u implicit negatives second covering u     given reasonable values u number
positives covered clause  literal split b a c  preferred 
revised specialization algorithm incorporates implicit negatives is 
initialize c r v   v        vk      r target predicate arity k 
initialize contain examples positives to cover output queries
positive examples 
contains output queries
find best literal l add clause 
let subset positive examples still proved instances
target concept using specialized clause  plus output queries
 

 

fimooney   califf

still produce incorrect answers 
replace  
 

literals scored described previous section except jt j computed
number positive examples plus sum number implicit negatives covered
output query  

    first order decision lists

described above  first order decision lists ordered sets clauses ending
cut  answering output query  cuts simply eliminate first answer
produced trying clauses order  therefore  representation similar
propositional decision lists  rivest         ordered lists pairs  rules 
form  ti   ci  test ti conjunction features ci category label
example assigned category first pair whose test satisfies 
original algorithm rivest        cn   clark   niblett         rules
learned order appear final decision list  i e   new rules appended
end list learned   however  webb brkic        argue learning
decision lists reverse order since preference functions tend learn general
rules first  best positioned default cases towards end  introduce
algorithm  prepend  learns decision lists reverse order present results indicating
cases learns simpler decision lists superior predictive accuracy  foidl
seen generalizing prepend first order case target predicates representing
functions  learns ordered sequence clauses reverse order  resulting program
produces first output generated first satisfied clause 
basic operation algorithm best illustrated concrete example 
alphabetic past tense  current algorithm easily learns partial clause 
past a b     split b a c   c    e d  

however  discussed section      clause still covers negative examples due irregular verbs  however  produces correct ground output subset examples  i e  
regular verbs    indication best terminate clause handle
examples  add earlier clauses decision list handle remaining examples 
fact produces incorrect answers output queries safely ignored
decision list framework since handled earlier clauses  therefore 
examples correctly covered clause removed positives to cover new
clause begun  literals provide best gain are 
past a b     split b a c   c    d  

since many irregulars add  d   since end  e    clause
produces correct ground output subset examples  however 
complete since produces incorrect output examples correctly covered previously
learned clause  e g   past  a c t    a c t d     therefore  specialization continues
cases eliminated  results clause 
   note untrue literals added initially empty clause 

  

fiinduction first order decision lists  learning english past tense

past a b     split b a c   c    d   split a d e   e    e  

added front decision list examples covers removed
positives to cover  approach ensures every new clause produces correct outputs
new subset examples doesn t result incorrect output examples already
correctly covered previously learned clauses  process continues adding clauses
front decision list exceptions handled positives to cover
empty 
resulting clause specialization algorithm summarized follows 
initialize c r v   v        vk      r target predicate arity k 
initialize contain examples positives to cover output queries
positive examples 
contains output queries
find best literal l add clause 
let subset positive examples whose output query still produces
first answer unifies correct answer  plus output queries
either
   produce non ground first answer unifies correct answer 
   produce incorrect answer produce correct answer using
previously learned clause 
replace  
 

 

many cases  algorithm able learn accurate  compact  first order decision lists
past tense   expert  program shown section      however  due highly irregular verbs  algorithm encounter local minima unable find literals
provide positive gain still covering required minimum number examples  
originally handled terminating search memorizing remaining uncovered examples specific exceptions top decision list  e g   past  a r i s e  
 a r o s e           however  result premature termination prevents
algorithm finding low frequency regularities  example  alphabetic version  system get stuck trying learn complex rule double final
consonant  e g   grab   grabbed  fail learn rule changing  y   ied  since
actually less frequent 
current version  foil  tests learned clause meets minimum accuracy
threshold  however  unlike foil  counting errors incorrect outputs queries correctly answered previously learned clauses  meet threshold  clause
thrown positive examples covers memorized top decision
list  algorithm continues learn clauses remaining positive examples 
allows foidl memorize dicult irregularities  consonant doubling 
still continue learn rules changing  y   ied  
minimum accuracy threshold met  decision list property exploited
final attempt still learn completely accurate program  negatives covered
clause examples correctly covered previously learned clauses  foidl
   foil  foidl includes parameter minimum number examples clause must cover
 normally set    

  

fimooney   califf

treats  exceptions exception rule  returns positives tocover covered correctly subsequently learned clauses  example  foidl
frequently learns clause 
past a  b     split a  c   y    split b  c   i  e  d   

changing  y   ied   however  clause incorrectly covers examples
correctly covered previously learned  add  ed   rule  e g   bay   bayed  delay  
delayed   since exceptions  y   ied  rule small percentage words
end  y   system keeps rule returns examples add  ed 
positives to cover  subsequently  rules as 
past a  b     split b  a   e  d    split a  d   a  y   

learned recover examples  resulting program completely consistent
training data  setting minimum clause accuracy threshold      foidl
applies uncovering technique results covering examples
uncovers  thereby guaranteeing progress towards fitting training examples 

    algorithmic implementation details

section brie discusses additional details foidl algorithm implementation  includes discussion use modes  types  weak literals  theory
constants  current version foil includes features basically
form 
foidl makes use types modes limit space literals searched  argument predicate typed literals whose previously bound arguments
correct type tested specializing clause  example  split given
types split word prefix suffix   preventing system splitting prefixes
suxes exploring arbitrary substrings word regularities  predicate
given mode declaration  literals whose input arguments previouslybound variables tested  example  split given mode split         preventing
clause creating new strings appending together previously generated prefixes
suxes 
case literal provides positive information gain  foidl gives small bonus literals
introduce new variables  however  number weak literals added
row limited user parameter  normally set     example  allows
system split word possible prefixes suxes  even though may provide
gain substrings constrained subsequent literals 
theory constants provided type  literals tested binding
existing variable constant appropriate type  example  literal x  e d 
generated x type suffix  runs past tense  theory constants included
every prefix sux occurs least two words training data  helps
control training time limiting number literals searched  affect
literals actually chosen since minimum clause coverage test prevents foidl
choosing literals don t cover least two examples anyway 
  

fiinduction first order decision lists  learning english past tense

foidl currently implemented common lisp quintus prolog  unlike
current prolog version  common lisp version supports learning recursive clauses 
output completeness non functional target predicates  however  common lisp
version significantly slower since relies un optimized prolog interpreter
compiler written lisp  from norvig         consequently  presented results
prolog version running sun sparcstation    

   experimental results
test foidl s performance english past tense task  ran experiments using
data ling        made available appendix 

    experimental design
data used consist      english verb forms normal alphabetic form
unibet phoneme representation along label indicating verb form  base  past
tense  past participle  etc   label indicating whether form regular irregular 
francis kucera frequency verb  data include      distinct pairs base
past tense verb forms  ran three different experiments  one used phonetic
forms verbs  second used phonetic forms regular verbs only 
easiest form task problem
ling provides learning curves  finally  ran trials using alphabetic forms verbs 
training testing followed standard paradigm splitting data testing
training sets training progressively larger samples training set  results
averaged    trials  testing set trial contained     verbs 
order better separate contribution using implicit negatives contribution decision list representation  ran experiments ifoil  variant
system uses intensional background output completeness assumption 
build decision lists 
ran experiments foil  foidl  ifoil compared
results ling  foil experiments run using quinlan s representation described
section      quinlan         negative examples provided using randomlyselected     could generated using closed world assumption  
experiments foidl ifoil used standard default values various numeric
parameters  term universe size        minimum clause coverage     weak literal limit     
differences among foil  ifoil  foidl tested significance using twotailed paired t test 
   handling intensional interpretation recursive clauses target predicate requires additional
complexities discussed paper since relevant decision lists 
generally recursive 
   versions available anonymous ftp net cs utexas edu directory
pub mooney foidl 
   replicated quinlan s approach since memory limitations prevented us using      generated negatives larger training sets 

  

fimooney   califf

   

  

accuracy

  

  
foidl
ifoil
foil
spa
neural network

  

 
 

   

   

   
training examples

   

   

figure    accuracy phonetic past tense task using verbs

    results
results phonetic task using regular irregular verbs presented
figure    graph shows results foil  ifoil  foidl along
best results ling  provide learning curve task  expected 
foidl out performed systems task  surpassing ling s best results    
examples     examples  ifoil performed quite poorly  barely beating neural
network results despite effectively      negatives opposed foil s     
poor performance due least part overfitting training data  ifoil
lacks noise handling techniques foil   foil advantage three place
predicate  gives bias toward learning suxes  ifoil s poor performance
task shows implicit negatives sucient 
bias decision lists three place predicate noise handling needed 
differences foil foidl significant      level  foidl
ifoil significant       level  differences foil ifoil
significant     training examples less  significant       level
        examples 
figure   presents accuracy results phonetic task using regulars only  curves
spa neural net results reported ling  again  foidl outperformed systems  particular task demonstrated one problems
using closed world negatives  regular past tense task  second argument quinlan s   place predicate always same  empty list  therefore  constants
generated positive examples  foil never produce rules ground second argument  since cannot create negative examples constants second
argument  prevents system learning rule generate past tense  order
  

fiinduction first order decision lists  learning english past tense

   

  

accuracy

  

  
foidl
ifoil
foil
spa
neural network

  

 
 

  

   

   

   
   
   
training examples

   

   

   

   

figure    accuracy phonetic past tense task using regulars
obtain results reported here  introduced extra constants second argument
 specifically constants third argument   enabling closed world assumption
generate appropriate negatives  task  ifoil seem gain advantage
foil able effectively use negatives  regularity data
allows ifoil foil achieve     accuracy     examples  differences
foil foidl significant       level  ifoil
foidl  differences ifoil foil significant    examples 
significant      level     examples  significant       level
       training examples 
results alphabetic version appear figure    task
typically considered literature  interest concerned
incorporating morphology natural language understanding systems deal
text  dicult task  primarily consonant doubling 
results foidl  ifoil  foil  alphabetic task even
irregular full phonetic task  ifoil overfits data performs quite poorly 
differences foil foidl significant       level             
    examples      level     examples  differences
ifoil foidl significant       level  foil ifoil
significant    training examples significant      level   
training examples  significant       level     examples 
three tasks  foidl clearly outperforms systems  demonstrating
first order decision list bias good one learning task  sucient set
negatives necessary  five systems provide way  neural
network spa learn multiple class classification tasks  which phoneme belongs
position   foil uses three place predicate closed world negatives  ifoil
  

fimooney   califf

  

  

  

accuracy

  

  

  

  

foidl
ifoil
foil

  

  

 
 

  

   

   

   
   
   
training examples

   

   

   

   

figure    accuracy alphabetic past tense task
foidl  course  use output completeness assumption  primary importance
implicit negatives provide advantage propositional
neural network systems  enable first order systems perform task
all  without them  knowledge task required  foidl s decision lists give
significant added advantage  though advantage less apparent regular phonetic
task  exceptions 
clearly  foidl produces accurate rules systems  another consideration complexity rule sets  ilp systems  two good measures
complexity number rules number literals generated  figure   shows
number rules generated foil  ifoil  foidl phonetic task using verbs 
number literals generated appears figure    since interested generalization since foil attempt fit training data  results
include rules foidl ifoil add order memorize individual exceptions   although numbers comparable examples  increasing numbers
examples  programs foil ifoil generate grow much faster foidl s programs 
large number rules literals learned ifoil show tendency overfit data 
foidl generates comprehensible programs  following example program generated alphabetic version task using     examples  again excluding
memorized examples  
past a b     split a c  e p    split b c  p t     
past a b     split a c  y    split b c  i e d    split a d  r y     
past a b     split a c  y    split b c  i e d    split a d  l y     

   large number irregular pasts english  foidl memorizes average    verbs per
trial     examples 

  

fiinduction first order decision lists  learning english past tense

  

  

foidl
ifoil
foil

number rules

  

  

  

  

  

  

 
 

  

   

   

   
   
   
training examples

   

   

   

   

figure    number rules created phonetic past tense task

   

   

foidl
ifoil
foil

number literals

   

   

   

   

  

 
 

  

   

   

   
   
   
training examples

   

   

   

   

figure    number literals created phonetic past tense task
  

fimooney   califf

past a b 
past a b 
past a b 
past a b 

     

split b a  m e d    split a c  m    split a  s  d    
split b a  r e d    split a c  u r     
split b a  d    split a c  e     
split b a  e d     

training times various systems considered research dicult
compare  ling provide timing results  though probably assume based
research comparing symbolic neural learning algorithms  shavlik  mooney    towell 
      spa runs fairly quickly since based c    backpropagation took
considerably longer  tests foil foidl directly comparable
run different architectures  foil runs done sparc   
    examples  foil averaged    minutes phonetic task verbs  foidl
experiments ran sparc   averaged      minutes task  even allowing
differences speed two machines  about factor two   foidl quite
bit slower  probably due largely cost using intentional background part
implementation prolog opposed c 

   related work

    related work ilp

although three features mentioned introduction distinguishes foidl
work inductive logic programming  number related pieces research
mentioned  use intensional background knowledge least distinguishing feature
since number ilp systems incorporate aspect  focl  pazzani   kibler 
       mfoil  lavrac   dzeroski         grendel  cohen         forte  richards  
mooney         chillin  zelle   mooney      a  use intensional background
degree context foil like algorithm  ilp systems employ
intensional background include early ones shapiro        sammut banerji       
recent ones bergadano et al         stahl  tausend  wirth        
use implicit negatives significantly novel  described section     
approach considerably different explicit construction using closed world assumption  therefore employed explicit construction sucient negative examples intractable  bergadano et al         allows user supply intensional definition
negative examples covers large set ground instances  e g  past  a c t  x  
not equal x  a c t e d       however  equivalent output completeness  user
would explicitly provide separate intensional negative definition positive
example  non monotonic semantics used eliminate need negative examples
claudien  de raedt   bruynooghe        effect output completeness
assumption case arguments target relation outputs  however 
output completeness permits exibility allowing arguments specified
inputs counting negative examples extra outputs generated specific
inputs training set  flip  bergadano        provides method learning functional programs without negative examples making assumption equivalent output
completeness functional case  output completeness general permits learning non functional programs well  also  unlike foidl  none previous
  

fiinduction first order decision lists  learning english past tense

methods provide way quantifying implicit negative coverage context heuristic
top down specialization algorithm 
notion first order decision list unique foidl  ilp system
attempts learn programs exploit clause order cuts bergadano et al 
        paper discusses many problems learning arbitrary programs cuts 
brute force search used approach intractable realistic problems 
instead addressing general problem learning arbitrary programs cuts  foidl
tailored specific problem learning first order decision lists  use cuts
stylized manner particularly useful functional problems involve rules
exceptions  bain muggleton        bain        discuss technique uses
negation failure handle exceptions  however  using negation failure significantly
different decision lists since simply prevents clause covering exceptions rather
learning additional clause over rides existing clause specifies
correct output set exceptions 

    related work past tense learning
shortcomings previous work past tense learning reviewed section     
results section   clearly demonstrate generalization advantage foidl exhibits
problem  however  couple issues deserve additional discussion 
previous work problem concerned modelling various
psychological phenomenon  u shaped learning curve children exhibit
irregular verbs acquiring language  paper addressed issue psychological validity  rather focused performance accuracy exposure fixed
number training examples  therefore  make specific psychological claims based
current results 
however  humans obviously produce correct past tense arbitrarily long novel
words  foidl easily model fixed length feature based representations clearly
cannot  ling developed version spa eliminates position dependence fixed
word length  ling        using sliding window used nettalk  sejnowski
  rosenberg         large window used includes    letters either side
current position  padded blanks necessary  order always include entire
word examples corpus  results approach significantly better
normal spa still inferior foidl s results  also  approach still requires
fixed sized input window prevents handling arbitrary length irregular verbs 
recurrent neural networks could used avoid word length restrictions  cotrell  
plunkett         although appears one yet applied standard
present tense past tense mapping problem  however  believe diculty training
recurrent networks relatively poor ability maintain state information arbitrarily
long would limit performance task 
another issue comprehensibility transparency learned result 
foidl s programs past tense short  concise  readable  unlike complicated networks  decision forests  pure logic programs generated previous approaches 
ling marinov        discusses possibility transforming spa s decision forest
  

fimooney   califf

comprehensible first order rules  however  approach directly learning first order
rules data seems clearly preferable 

   future work
one obvious topic future research foidl s cognitive modelling abilities context
past tense task  incorporating over fitting avoidance methods may allow system
model u shaped learning curve manner analogous demonstrated ling
marinov         ability model human results generating past tense
novel psuedo verbs  e g   spling   splang  could examined compared spa
 ling   marinov        connectionist methods 
although first order decision lists represent fairly general class programs  currently
convincing experimental results past tense problem  many realistic
problems consist rules exceptions  experimental results additional applications needed support general utility representation 
despite advantages  use intensional background knowledge ilp incurs
significant performance cost  since examples must continually reproved testing
alternative literals specialization  computation accounts training
time foidl  one approach improving computational eciency would maintain
partial proofs examples incrementally update proofs additional literals
added clause  approach would foil s approach maintaining
tuples  would require using meta interpreter prolog  incurs significant
overhead  ecient use intensional knowledge ilp could greatly benefit work
rapid incremental compilation logic programs  i e   incrementally updating compiled code
account small changes definition predicate 
foidl could potentially benefit methods handling noisy data preventing
over fitting  pruning methods employed foil related systems  quinlan        lavrac
  dzeroski        could easily incorporated  decision list framework  alternative
simply ignoring incorrectly covered examples noise treat exceptions
handled subsequently learned clauses  as uncovering technique discussed
section      
theoretical results learnability restricted classes first order decision lists
another interesting area research  given results pac learnability propositional decision lists  rivest        restricted classes ilp problems  dzeroski  muggleton    russell        cohen         appropriately restricted class first order decision
lists pac learnable 

   conclusions
paper addressed two main issues  appropriateness first order learner
popular past tense problem  problems previous ilp systems handling
functional tasks whose best representation rules exceptions  results clearly
demonstrate ilp system outperforms decision tree neural network
systems previously applied past tense task  important since
results showing first order learner performs significantly better apply  

fiinduction first order decision lists  learning english past tense

ing propositional learners best feature based encoding problem  research
demonstrates ecient effective algorithm learning concise 
comprehensible symbolic programs small interesting subproblem language acquisition  finally  work shows possible eciently learn logic programs
involve cuts exploit clause order particular class problems  demonstrates usefulness intensional background implicit negatives  solutions many
practical problems seem require general default rules characterizable exceptions 
therefore may best learned using first order decision lists 

acknowledgements
basic research paper conducted first author leave
university sydney supported grant prof  j r  quinlan australian
research council  thanks ross quinlan providing enjoyable productive
opportunity ross mike cameron jones important discussions
pointers greatly aided development foidl  thanks ross aiding us
running foil experiments  discussions john zelle cindi thompson
university texas uenced work  partial support provided
grant iri         national science foundation mcd fellowship
university texas awarded second author 

references

bain  m          experiments non monotonic first order induction  muggleton  s 
 ed    inductive logic programming  pp           academic press  new york  ny 
bain  m     muggleton  s          non monotonic learning  muggleton  s   ed    inductive logic programming  pp           academic press  new york  ny 
bergadano  f          interactive system learn functional logic programs  proceedings thirteenth international joint conference artificial intelligence  pp 
          chambery  france 
bergadano  f   gunetti  d     trinchero  u          diculties learning logic programs cut  journal artificial intelligence research            
califf  m  e          learning past tense english verbs  inductive logic programming approach  unpublished project report 
cameron jones  r  m     quinlan  j  r          ecient top down induction logic
programs  sigart bulletin               
clark  p     niblett  t          cn  induction algorithm  machine learning    
        
cohen  w  w          pac learning nondeterminate clauses  proceedings twelfth
national conference artificial intelligence  pp          seattle  wa 
  

fimooney   califf

cohen  w          compiling prior knowledge explicit bias  proceedings
ninth international conference machine learning  pp          aberdeen 
scotland 
cotrell  g     plunkett  k          learning past tense recurrent network  acquiring mapping meaning sounds  proceedings thirteenth annual
conference cognitive science society  pp          chicago  il 
de raedt  l     bruynooghe  m          theory clausal discovery  proceedings
thirteenth international joint conference artificial intelligence  pp           
chambery  france 
dzeroski  s   muggleton  s     russell  s          pac learnability determinate logic
programs   proceedings      workshop computational learning theory
pittsburgh  pa 
lachter  j     bever  t          relation linguistic structure associative
theories language learning  constructive critique connectionist learning
models  pinker  s     mehler  j   eds    connections symbols  pp          
mit press  cambridge  ma 
lavrac  n     dzeroski  s   eds            inductive logic programming  techniques
applications  ellis horwood 
ling  c  x          learning past tense english verbs  symbolic pattern associator vs  connectionist models  journal artificial intelligence research             
ling  c  x          personal communication 
ling  c  x     marinov  m          answering connectionist challenge  symbolic
model learning past tense english verbs  cognition                  
macwhinney  b     leinbach  j          implementations conceptualizations  revising verb model  cognition              
muggleton  s     buntine  w          machine invention first order predicates inverting resolution  proceedings fifth international conference machine
learning  pp          ann arbor  mi 
muggleton  s     feng  c          ecient induction logic programs  proceedings
first conference algorithmic learning theory tokyo  japan  ohmsha 
muggleton  s   king  r     sternberg  m          protein secondary structure prediction
using logic based machine learning  protein engineering                 
muggleton  s  h   ed            inductive logic programming  academic press  new york 
ny 
norvig  p          paradigms artificial intelligence programming  case studies common lisp  morgan kaufmann  san mateo  ca 
  

fiinduction first order decision lists  learning english past tense

pazzani  m     kibler  d          utility background knowledge inductive learning 
machine learning           
pinker  s     prince  a          language connectionism  analysis parallel
distributed model language acquisition  pinker  s     mehler  j   eds    connections symbols  pp          mit press  cambridge  ma 
quinlan  j  r          c     programs machine learning  morgan kaufmann  san
mateo ca 
quinlan  j  r          past tenses verbs first order learning  zhang  c   debenham 
j     lukose  d   eds    proceedings seventh australian joint conference
artificial intelligence  pp        singapore  world scientific 
quinlan  j  r     cameron jones  r  m          foil  midterm report  proceedings
european conference machine learning  pp       vienna 
quinlan  j          learning logical definitions relations  machine learning        
        
richards  b  l     mooney  r  j          automated refinement first order horn clause
domain theories  machine learning  press 
rivest  r  l            learning decision lists  machine learning                 
rumelhart  d  e     mcclelland  j          learning past tense english verbs 
rumelhart  d  e     mcclelland  j  l   eds    parallel distributed processing  vol 
ii  pp           mit press  cambridge  ma 
sammut  c     banerji  r  b          learning concepts asking questions  michalski 
r  s   carbonell  j  g     mitchell  t  m   eds    machine learning  ai approach 
vol  ii  pp           morgan kaufman 
sejnowski  t  j     rosenberg  c          parallel networks learn pronounce english
text  complex systems             
shapiro  e          algorithmic program debugging  mit press  cambridge  ma 
shavlik  j  w   mooney  r  j     towell  g  g          symbolic neural learning
algorithms  experimental comparison  machine learning             
stahl  i   tausend  b     wirth  r          two methods improving inductive logic
programming systems  machine learning  ecml     pp        vienna 
webb  g  i     brkic  n          learning decision lists prepending inferred rules 
proceedings australian workshop machine learning hybrid systems 
pp       melbourne  australia 
zelle  j  m     mooney  r  j       a   combining top down bottom up methods
inductive logic programming  proceedings eleventh international conference
machine learning new brunswick  nj 
  

fimooney   califf

zelle  j  m     mooney  r  j       b   inducing deterministic prolog parsers treebanks 
machine learning approach  proceedings twelfth national conference
artificial intelligence  pp          seattle  wa 

  


