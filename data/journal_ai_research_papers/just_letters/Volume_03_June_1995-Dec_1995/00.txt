journal of artificial intelligence research              

submitted       published     

induction of first order decision lists 
results on learning the past tense of english verbs

raymond j  mooney
mary elaine califf

department of computer sciences  university of texas
austin  tx           

mooney cs utexas edu
mecaliff cs utexas edu

abstract

this paper presents a method for inducing logic programs from examples that learns
a new class of concepts called first order decision lists  defined as ordered lists of clauses
each ending in a cut  the method  called foidl  is based on foil  quinlan        but
employs intensional background knowledge and avoids the need for explicit negative examples  it is particularly useful for problems that involve rules with specific exceptions 
such as learning the past tense of english verbs  a task widely studied in the context of the
symbolic connectionist debate  foidl is able to learn concise  accurate programs for this
problem from significantly fewer examples than previous methods  both connectionist and
symbolic  

   introduction

inductive logic programming  ilp  is a growing subtopic of machine learning that studies
the induction of prolog programs from examples in the presence of background knowledge
 muggleton        lavrac   dzeroski         due to the expressiveness of first order logic 
ilp methods can learn relational and recursive concepts that cannot be represented in the
attribute value representations assumed by most machine learning algorithms  ilp methods have successfully induced small programs for sorting and list manipulation  shapiro 
      sammut   banerji        muggleton   buntine        quinlan   cameron jones 
      as well as produced encouraging results on important applications such as predicting protein secondary structure  muggleton  king    sternberg        and automating the
construction of natural language parsers  zelle   mooney      b  
however  current ilp techniques make important assumptions that restrict their application  below are three common assumptions 
   background knowledge is provided in extensional form as a set of ground literals 
   explicit negative examples of the target predicate are available 
   the target program is expressed in  pure  prolog where clause order is irrelevant and
procedural operators such as cut     are disallowed 
the currently most well known and successful ilp systems  golem  muggleton   feng 
      and foil  quinlan         both make all three of these assumptions  however  each
of these assumptions brings significant limitations since 
   an adequate extensional representation of background knowledge is frequently infinite
or intractably large 

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fimooney   califf

   explicit negative examples are frequently unavailable and an adequate set of negative
examples computed using a closed world assumption is infinite or intractably large 
   concise representation of many concepts requires the use of clause ordering and or
cuts  bergadano  gunetti    trinchero        
this paper presents a new ilp method called foidl  first order induction of decision lists  which helps overcome each of these limitations by incorporating the following
properties 
   background knowledge is represented intensionally as a logic program 
   no explicit negative examples need be supplied or constructed  an assumption of
output completeness can be used instead to implicitly determine if a hypothesized
clause is overly general and  if so  to quantify the degree of over generality by simply
estimating the number of negative examples covered 
   a learned program can be represented as a first order decision list  an ordered set of
clauses each ending with a cut  this representation is very useful for problems that
are best represented as general rules with specific exceptions 
as its name implies  foidl is closely related to foil and follows a similar top down 
greedy specialization guided by an information gain heuristic  however  the algorithm is
substantially modified to address the three advantages listed above  the use of intensional
background knowledge is fairly straightforward and has been incorporated in previous foil
derivatives  lavrac   dzeroski        pazzani   kibler        zelle   mooney      b  
the development of foidl was motivated by a failure we observed when applying existing ilp methods to a particular problem  that of learning the past tense of english
verbs  this problem has been studied fairly extensively using both connectionist and symbolic methods  rumelhart   mcclelland        macwhinney   leinbach        ling 
       however  previous efforts used specially designed feature based encodings that impose a fixed limit on the length of words and fail to capture the position independence of
the underlying transformation  we believed that representing the problem as constructing a logic program for the predicate past x y  where x and y are words represented
as lists of letters  e g past  a c t    a c t e d    past  a c h e    a c h e d   
past  a r i s e    a r o s e    would produce much better results  however  due to
the limitations mentioned above  we were unable to get reasonable results from either foil
or golem  however  by overcoming these limitations  foidl is able to learn highly accurate programs for the past tense problem from many fewer examples than required by
previous methods 
the remainder of the paper is organized as follows  section   provides important background material on foil and on the past tense learning problem  section   presents the
foidl algorithm and details how it incorporates the three advantages discussed above  section   presents our results on learning the past tense of english verbs demonstrating that
foidl out performs all previous methods on this problem  section   reviews related work 
section   discusses limitations and future directions  and section   summarizes and presents
our conclusions 
 

fiinduction of first order decision lists  learning english past tense

   background

since foidl is based on foil  this section presents a brief review of this important ilp system  quinlan         quinlan and cameron jones         and cameron jones and quinlan
       provide a more complete description  the section also presents a brief review of
previous work on the english past tense problem 

    foil

foil learns a function free  first order  horn clause definition of a target predicate in terms
of itself and other background predicates  the input consists of extensional definitions of
these predicates as tuples of constants of specified types  for example  input appropriate
for learning a definition of list membership is 
member elt lst      a  a     a  a b     b  a b     a  a b c        
components lst elt lst       a  a        a b  a  b      a b c  a  b c       

where elt is a type denoting possible elements which includes a b c  and d  lst is
a type defined as consisting of at lists containing up to three of these elements  and
components a b c  is a background predicate which is true iff a is a list whose first element is b and whose rest is the list c  this must be provided in place of a function for
list construction   foil also requires negative examples of the target concept  which can
be supplied directly or computed using a closed world assumption  for the example  the
closed world assumption would produce all pairs of the form  elt lst  that are not explicitly provided as positive examples  e g    b  a    
given this input  foil learns a program one clause at a time using a greedy covering
algorithm that can be summarized as follows 
let positives to cover   positive examples 
while positives to cover is not empty
find a clause  c   that covers a preferably large subset of positives to cover
but covers no negative examples 
add c to the developing definition 
remove examples covered by c from positives to cover 
for example  a clause that might be learned for member during one iteration of this loop is 
member a b     components b a c  

since it covers all positive examples where the element is the first one in the list but does
not cover any negatives  a clause that could be learned to cover the remaining examples is 
member a b     components b c d   member a d  

together these two clauses constitute a correct program for member 
the  find a clause  step is implemented by a general to specific hill climbing search that
adds antecedents to the developing clause one at a time  at each step  it evaluates possible
literals that might be added and selects one that maximizes an information gain heuristic 
the algorithm maintains a set of tuples that satisfy the current clause and includes bindings
for any new variables introduced in the body  the following pseudocode summarizes the
procedure 
 

fimooney   califf

initialize c to r v   v        vk      where r is the target predicate with arity k 
initialize t to contain the positive tuples in positives to cover and all the negative tuples 
while t contains negative tuples
find the best literal l to add to the clause 
form a new training set t containing for each tuple t in t that satisfies l 
all tuples of the form t  b  t and b concatenated  where b is a set of bindings
for the new variables introduced by l such that the literal is satisfied
 i e   matches a tuple in the extensional definition of its predicate  
replace t by t  
foil considers adding literals for all possible variablizations of each predicate as long
as type restrictions are satisfied and at least one of the arguments is an existing variable
bound by the head or a previous literal in the body  literals are evaluated based on the
number of positive and negative tuples covered  preferring literals that cover many positives
and few negatives  let t  denote the number of positive tuples in the set t and define 
i  t       log   t   jt j  
   
the chosen literal is then the one that maximizes 
gain l    s   i  t     i  t    
   
where s is the number of tuples in t that have extensions in t  i e   the number of current
positive tuples covered by l  
foil also includes many additional features such as  heuristics for pruning the space
of literals searched  methods for including equality  negation as failure  and useful literals
that do not immediately provide gain  determinate literals   pre pruning and post pruning
of clauses to prevent over fitting  and methods for ensuring that induced programs will
terminate  the papers referenced above should be consulted for details on these and other
features 
 

 

 

 

    learning the past tense of english verbs

rumelhart and mcclelland        were the first to build a computational model of pasttense learning using the classic perceptron algorithm and a special phonemic encoding of
words employing so called wickelphones and wickelfeatures  their general goal was to show
that connectionist models could account for interesting language learning behavior that was
previously thought to require explicit rules  this model was heavily criticized by opponents
of the connectionist approach to language acquisition for the relatively poor results achieved
and the heavily engineered representations and training techniques employed  pinker  
prince        lachter   bever         macwhinney and leinbach        attempted to
address some of these criticisms by using a standard multi layer backpropagation learning
algorithm and a simpler unibet encoding of phonemes  in which each of    phonemes is
encoded as a single ascii character  
ling and marinov        and ling        criticize all of the current connectionist models of past tense acquisition for heavily engineered representations and poor experimental
methodology  they present more systematic results on a system called spa  symbolic pattern associator  which uses a slightly modified version of c     quinlan        to build a
 

fiinduction of first order decision lists  learning english past tense

forest of decision trees that maps a fixed length input pattern to a fixed length output pattern  ling s        head to head results show that spa generalizes significantly better than
backpropagation on a number of variations of the problem employing different phonemic
encodings  e g       vs      given     training examples  
however  all of this previous work encodes the problem as fixed length pattern association and fails to capture the generativity and position independence of the true transformation  for example  they use    letter patterns like 
a c t                            a c t e d                    

or in unibet phonemic encoding 
  k t                              k t i d                    

where a separate decision tree or output unit is used to predict each character in the output
pattern from all of the input characters  therefore  learning general rules  such as  add
 ed    must be repeated at each position where a word can end  and words longer than   
characters cannot be handled  also  the best results with spa exploit a highly engineered
feature template and a modified version of c    s default leaf labeling strategy that tailor
it to string transformation problems 
although ilp methods seem more appropriate for this problem  our initial attempts
to apply foil and golem to past tense learning gave very disappointing results  califf 
       below  we discuss how the three problems listed in the introduction contribute to
the diculty of applying current ilp methods to this problem 
in principle  a background predicate for append is sucient for constructing accurate
past tense programs when incorporated with an ability to include constants as arguments
or  equivalently  an ability to add literals that bind variables to specific constants  called
theory constants in foil   however  a background predicate that does not allow appending
with the empty list is more appropriate  we use a predicate called split a  b  c  which
splits a list a into two non empty sublists b and c  an intensional definition for split is 
split  x  y   z    x     y   z   
split  x   y    x   w   z     split y w z  

using split  an  add  ed   rule can be represented as 
past a b     split b a  e d   

which  in foil  is learned in the form 
past a b     split b a c   c    e d  

providing an extensional definition of split that includes all possible strings of    or fewer
characters  at least      strings  is clearly intractable  however  providing a partial definition that includes all possible splits of strings that actually appear in the training corpus
is possible and generally sucient  therefore  providing adequate extensional background
knowledge is cumbersome and requires careful engineering  however  it is not the major
problem 
supplying an appropriate set of negative examples is more problematic  using a closedworld assumption to produce all pairs of words in the training set where the second is not
the past tense of the first is feasible but not very useful  in this case  the clause 
 

fimooney   califf

past a b     split b a c  

is very likely to be learned since it covers most of the positives but very few  if any 
negatives since it is unlikely that a word is a prefix of another word which is not its past
tense  however  this clause is useless for producing the past tense of novel verbs  and  in this
domain  accuracy must be measured by the ability to actually generate correct output for
novel inputs  rather than the ability to classify pre supplied tuples of arguments as positive
or negative  the obvious solution of supplying all other strings of    characters or less as
negative examples of the past tense of each word is clearly intractable  providing specially
constructed  near miss  negative examples such as past  a c h e   a c h e e d    is
very helpful  but requires careful engineering that exploits detailed prior knowledge of the
problem 
in order to address the problem of negative examples  when quinlan        applied
foil to this problem  he employed a different target predicate for representing the pasttense transformation   he used a three place predicate past x y z  which is true iff the
input word x is transformed into past tense form by removing its current ending y and
substituting the ending z  for example  past  a c t        e d    past  a r i s e  
 i s e    o s e    a simple preprocessor can map data for the two place predicate into
this form  since a sample of     verb pairs contains about       different end fragments 
this results in a more manageable number of closed world negatives  approximately     
for every positive example in the training set  using this approach on unibet phonemic
encodings  quinlan obtained slightly better results than ling s best spa results that exploited a highly engineered feature template        vs        with     training examples 
and significantly better than spa s normal results          although the three place target predicate incorporates some knowledge about the desired transformation  it arguably
requires less representation engineering than most previous methods 
however  quinlan        notes that his results are still hampered by foil s inability to
exploit clause order  for example  when using normal alphabetic encoding  foil quickly
learns a clause sucient for regular verbs 
past a b c     b     c  e d  

however  since this clause still covers a fair number of negative examples due to many
irregular verbs  it continues to add literals  as a result  foil creates a number of specialized
versions of this clause that together still fail to capture the generality of the underlying
default rule  this problem is compounded by foil s inability to add constraints such as
 does not end in  e    since foil separates the addition of literals containing variables and
the binding of variables to constants using literals of the form v   c  it cannot learn clauses
like 
past a b c     b     c  e d   not split a d  e    

since a word can be split in several ways  this is clearly not equivalent to the learnable
clause 
past a b c     b     c  e d   not split a d e    e     e  

   quinlan s work on this problem was motivated by our own early attempts to use foil 

 

fiinduction of first order decision lists  learning english past tense

consequently  it must approximate the true rule by learning many clauses of the form 
past a b c     b     c  e d   split a d e   e    b  
past a b c     b     c  e d   split a d e   e    d  
   

as a result  foil generated overly complex programs containing more than    clauses for
both the phonemic and alphabetic versions of the problem 
however  an experienced prolog programmer would exploit clause order and cuts to
write a concise program that first handles the most specific exceptions and falls through to
more general default rules if the exceptions fail to apply  for example  the program 
past a b 
past a b 
past a b 
past a b 

     

split a c  e e p    split b c  e p t      
split a c  y    split b c  i e d      
split a c  e    split b a  d      
split b a  e d   

can be summarized as 
if the word ends in  eep   then replace  eep  with  ept   e g   sleep  slept  
else  if the word ends in  y   then replace  y  with  ied 
else  if the word ends in  e   add  d 
else  add  ed  
foidl can directly learn programs of this form  i e   ordered sets of clauses each ending in a
cut  we call such programs first order decision lists due to the similarity to the propositional
decision lists introduced by rivest         foidl uses the normal binary target predicate
and requires no explicit negative examples  therefore  we believe it requires significantly
less representation engineering than all previous work in the area 

   foidl induction algorithm

as stated in the introduction  foidl adds three major features to foil     intensional
specification of background knowledge     output completeness as a substitute for explicit
negative examples  and    support for learning first order decision lists  the following
subsections describe the modifications made to incorporate these features 

    intensional background

as described above  foil assumes background predicates are provided with extensional
definitions  however  this is burdensome and frequently intractable  providing an intensional definition in the form of general prolog clauses is generally preferable  for example 
instead of providing numerous tuples for the components predicates  it is easier to give the
intensional definition 
components  a   b   a  b  

intentional background definitions are not restricted to function free pure prolog and can
exploit all features of the language 
 

fimooney   califf

modifying foil to use intensional background is straightforward  instead of matching
a literal against a set of tuples to determine whether or not it covers an example  the
prolog interpreter is used in an attempt to prove that the literal can be satisfied using the
intensional definitions  unlike foil  expanded tuples are not maintained and positive and
negative examples of the target concept are reproved for each alternative specialization of
the developing clause  therefore  the pseudocode for learning a clause is simply 
initialize c to r v   v        vk      where r is the target predicate with arity k 
initialize t to contain the examples in positives to cover and all the negative examples 
while t contains negative tuples
find the best literal l to add to the clause 
let t be the subset of examples in t that can still be proved as instances of the
target concept using the specialized clause 
replace t by t
since expanded tuples are not produced  the information gain heuristic for picking the best
literal is simply 
gain l    jt j   i  t     i  t    
   
 

 

 

 

    output completeness and implicit negatives

in order to overcome the need for explicit negative examples  a mode declaration for the
target concept must be provided  i e   a specification whether each argument is an input    
or an output       an assumption of output completeness can then be made  indicating that
for every unique input pattern in the training set  the training set includes all of the correct
output patterns  therefore  any other output which a program produces for a given input
can be assumed to represent a negative example  this does not require that all positive
examples be part of the training set  only that for each unique input pattern in the training
set  all other positive examples with that input pattern  if any  must also be in the training
set  this assumption is trivially met if the predicate represents a function with a single
unique output for each input 
for example  an assumption of output completeness for the mode declaration past     
indicates that all of the correct past tense forms are included for each input word in the
training set  for predicates representing functions  such as past  this implies that the
output for each example is unique and that all other outputs implicitly represent negative examples  however  output completeness can also be applied to non functional cases
such as append         indicating that all possible pairs of lists that can be appended
together to produce a list are included in the training set  e g   append     a b   a b   
append  a   b   a b    append  a b      a b    
given an output completeness assumption  determining if a clause is overly general
is straightforward  for each positive example  an output query is made to determine all
outputs for the given input  e g   past  a c t   x    if any outputs are generated that
are not positive examples  the clause still covers negative examples and requires further
specialization  note that intensional interpretation of learned clauses is required in order
to answer output queries 
in addition  in order to compute the gain of alternative literals during specialization  the
negative coverage of a clause needs to be quantified  each incorrect answer to an output
 

fiinduction of first order decision lists  learning english past tense

query which is ground  i e   contains no variables  clearly counts as a single negative example  e g   past  a c h e    a c h e e d     however  output queries will frequently
produce answers with universally quantified variables  for example  given the overly general
clause past a b     split a c d    the query past  a c t   x  generates the answer
past  a c t   y   this implicitly represents coverage of an infinite number of negative
examples  in order to quantify negative coverage  foidl uses a parameter u to represent a
bound on the number of possible terms  since the set of all possible terms  the herbrand
universe of the background knowledge together with the examples  is generally infinite  u
is meant to represent a heuristic estimate of the finite number of these terms that will ever
actually occur in practice  e g   the number of distinct words in english   the negative coverage represented by a non ground answer to an output query is then estimated as uv   p 
where v is the number of variable arguments in the answer and p is the number of positive
examples with which the answer unifies  the uv term stands for the number of unique
ground outputs represented by the answer  e g   the answer append x y  a b   stands for
 
u different ground outputs  and the p term stands for the number of these that represent
positive examples  this allows foidl to quantify coverage of large numbers of implicit
negative examples without ever explicitly constructing them  it is generally sucient to
estimate u as a fairly large constant  e g          and empirically the method is not very
sensitive to its exact value as long as it is significantly greater than the number of ground
outputs ever generated by a clause 
unfortunately  this estimate is not sensitive enough  for example  both clauses
past a b     split a c d  
past a b     split b a c  

cover u implicit negative examples for the output query past  a c t   x  since the first
produces the answer past  a c t   y  and the second produces the answer past  a c t  
 a c t   y    however  the second clause is clearly better since it at least requires the output to be the input with some sux added  since there are presumably more words than
there are words that start with  a c t   assuming the total number of words is finite   the
first clause should be considered to cover more negative examples  therefore  arguments
that are partially instantiated  such as  a c t   y   are counted as only a fraction of a
variable when calculating v   specifically  a partially instantiated output argument is scored
as the fraction of its subterms that are variables  e g    a c t   y  counts as only     of
a variable argument  therefore  the first clause above is scored as covering u implicit negatives and the second as covering only u     given reasonable values for u and the number
of positives covered by each clause  the literal split b a c  will be preferred 
the revised specialization algorithm that incorporates implicit negatives is 
initialize c to r v   v        vk      where r is the target predicate with arity k 
initialize t to contain the examples in positives to cover and output queries for all
positive examples 
while t contains output queries
find the best literal l to add to the clause 
let t be the subset of positive examples in t that can still be proved as instances
of the target concept using the specialized clause  plus the output queries in t
 

 

fimooney   califf

that still produce incorrect answers 
replace t by t  
 

literals are scored as described in the previous section except that jt j is computed as the
number of positive examples in t plus the sum of the number of implicit negatives covered
by each output query in t  

    first order decision lists

as described above  first order decision lists are ordered sets of clauses each ending in a
cut  when answering an output query  the cuts simply eliminate all but the first answer
produced when trying the clauses in order  therefore  this representation is similar to
propositional decision lists  rivest         which are ordered lists of pairs  rules  of the
form  ti   ci  where the test ti is a conjunction of features and ci is a category label and an
example is assigned to the category of the first pair whose test it satisfies 
in the original algorithm of rivest        and in cn   clark   niblett         rules are
learned in the order they appear in the final decision list  i e   new rules are appended to
the end of the list as they are learned   however  webb and brkic        argue for learning
decision lists in the reverse order since most preference functions tend to learn more general
rules first  and these are best positioned as default cases towards the end  they introduce an
algorithm  prepend  that learns decision lists in reverse order and present results indicating
that in most cases it learns simpler decision lists with superior predictive accuracy  foidl
can be seen as generalizing prepend to the first order case for target predicates representing
functions  it learns an ordered sequence of clauses in reverse order  resulting in a program
which produces only the first output generated by the first satisfied clause 
the basic operation of the algorithm is best illustrated by a concrete example  for
alphabetic past tense  the current algorithm easily learns the partial clause 
past a b     split b a c   c    e d  

however  as discussed in section      this clause still covers negative examples due to irregular verbs  however  it produces correct ground output for a subset of the examples  i e   the
regular verbs    this is an indication that it is best to terminate this clause to handle these
examples  and add earlier clauses in the decision list to handle the remaining examples 
the fact that it produces incorrect answers for other output queries can be safely ignored
in the decision list framework since these can be handled by earlier clauses  therefore  the
examples correctly covered by this clause are removed from positives to cover and a new
clause is begun  the literals that now provide the best gain are 
past a b     split b a c   c    d  

since many of the irregulars are those that just add  d   since they end in  e    this clause
also now produces correct ground output for a subset of the examples  however  it is not
complete since it produces incorrect output for examples correctly covered by a previously
learned clause  e g   past  a c t    a c t d     therefore  specialization continues until
all of these cases are also eliminated  this results in the clause 
   note that this is untrue until both of the literals are added to this initially empty clause 

  

fiinduction of first order decision lists  learning english past tense

past a b     split b a c   c    d   split a d e   e    e  

which is added to the front of the decision list and the examples it covers are removed from
positives to cover  this approach ensures that every new clause produces correct outputs for
some new subset of the examples but doesn t result in incorrect output for examples already
correctly covered by previously learned clauses  this process continues adding clauses to
the front of the decision list until all of the exceptions are handled and positives to cover is
empty 
the resulting clause specialization algorithm can now be summarized as follows 
initialize c to r v   v        vk      where r is the target predicate with arity k 
initialize t to contain the examples in positives to cover and output queries for all
positive examples 
while t contains output queries
find the best literal l to add to the clause 
let t be the subset of positive examples in t whose output query still produces
a first answer that unifies with the correct answer  plus the output queries in t
that either
   produce a non ground first answer that unifies with the correct answer  or
   produce an incorrect answer but produce a correct answer using a
previously learned clause 
replace t by t  
 

 

in many cases  this algorithm is able to learn accurate  compact  first order decision lists
for past tense  like the  expert  program shown in section      however  due to highly irregular verbs  the algorithm can encounter local minima in which it is unable to find any literals
that provide positive gain while still covering the required minimum number of examples  
this was originally handled by terminating search and memorizing any remaining uncovered examples as specific exceptions at the top of the decision list  e g   past  a r i s e  
 a r o s e           however  this can result in premature termination that prevents
the algorithm from finding low frequency regularities  for example  in the alphabetic version  the system can get stuck trying to learn the complex rule for when to double a final
consonant  e g   grab   grabbed  and fail to learn the rule for changing  y  to  ied  since
this is actually less frequent 
the current version  like foil  tests if the learned clause meets a minimum accuracy
threshold  however  unlike foil  only counting as errors incorrect outputs for queries correctly answered by previously learned clauses  if it does not meet the threshold  the clause
is thrown out and the positive examples it covers are memorized at the top of the decision
list  the algorithm then continues to learn clauses for any remaining positive examples 
this allows foidl to just memorize dicult irregularities  such as consonant doubling  and
still continue on to learn other rules such as changing  y  to  ied  
if the minimum accuracy threshold is met  the decision list property is exploited in a
final attempt to still learn a completely accurate program  if the negatives covered by the
clause are all examples that were correctly covered by previously learned clauses  foidl
   like foil  foidl includes a parameter for the minimum number of examples that a clause must cover
 normally set to    

  

fimooney   califf

treats them as  exceptions to the exception to the rule  and returns them to positives tocover to be covered correctly again by subsequently learned clauses  for example  foidl
frequently learns the clause 
past a  b     split a  c   y    split b  c   i  e  d   

for changing  y  to  ied   however  this clause incorrectly covers a few examples that are
correctly covered by the previously learned  add  ed   rule  e g   bay   bayed  delay  
delayed   since these exceptions to the  y  to  ied  rule are a small percentage of the words
that end in  y   the system keeps the rule and returns the examples that just add  ed  to
positives to cover  subsequently  rules such as 
past a  b     split b  a   e  d    split a  d   a  y   

are learned to recover these examples  resulting in a program that is completely consistent
with the training data  by setting the minimum clause accuracy threshold to      foidl
only applies this uncovering technique when it results in covering more examples than it
uncovers  thereby guaranteeing progress towards fitting all of the training examples 

    algorithmic and implementation details

this section briey discusses a few additional details of the foidl algorithm and its implementation  this includes a discussion of the use of modes  types  weak literals  and theory
constants  the current version of foil includes all of these features in basically the same
form 
foidl makes use of types and modes to limit the space of literals searched  the argument of each predicate is typed and only literals whose previously bound arguments are
of the correct type are tested when specializing a clause  for example  split is given the
types split word prefix suffix   preventing the system from further splitting prefixes
and suxes and exploring arbitrary substrings of a word for regularities  each predicate is
also given a mode declaration  and only literals whose input arguments are all previouslybound variables are tested  for example  split is given the mode split         preventing
a clause from creating new strings by appending together previously generated prefixes and
suxes 
in case no literal provides positive information gain  foidl gives a small bonus to literals
that introduce new variables  however  the number of such weak literals that can be added
in a row is limited by a user parameter  normally set to     for example  this allows the
system to split a word into possible prefixes and suxes  even though this may not provide
gain until these substrings are constrained by subsequent literals 
theory constants are provided for each type  and literals are tested for binding each
existing variable to each constant of the appropriate type  for example  the literal x  e d 
is generated if x is of type suffix  for our runs on past tense  theory constants are included
for every prefix and sux that occurs in at least two words in the training data  this helps
control training time by limiting the number of literals searched  but does not affect which
literals are actually chosen since the minimum clause coverage test prevents foidl from
choosing literals that don t cover at least two examples anyway 
  

fiinduction of first order decision lists  learning english past tense

foidl is currently implemented in both common lisp and quintus prolog  unlike
the current prolog version  the common lisp version supports learning recursive clauses 
and output completeness for non functional target predicates  however  the common lisp
version is significantly slower since it relies on an un optimized prolog interpreter and
compiler written in lisp  from norvig         consequently  all of the presented results
are from the prolog version running on a sun sparcstation    

   experimental results
to test foidl s performance on the english past tense task  we ran experiments using the
data which ling        made available in an appendix 

    experimental design
the data used consist of      english verb forms in both normal alphabetic form and
unibet phoneme representation along with a label indicating the verb form  base  past
tense  past participle  etc   a label indicating whether the form is regular or irregular  and
the francis kucera frequency of the verb  the data include      distinct pairs of base and
past tense verb forms  we ran three different experiments  in one we used the phonetic
forms of all verbs  in the second we used the phonetic forms of the regular verbs only 
because this is the easiest form of the task and because this is the only problem for which
ling provides learning curves  finally  we ran trials using the alphabetic forms of all verbs 
the training and testing followed the standard paradigm of splitting the data into testing
and training sets and training on progressively larger samples of the training set  all results
were averaged over    trials  and the testing set for each trial contained     verbs 
in order to better separate the contribution of using implicit negatives from the contribution of the decision list representation  we also ran experiments with ifoil  a variant
of the system which uses intensional background and the output completeness assumption 
but does not build decision lists 
we ran our own experiments with foil  foidl  and ifoil and compared those with the
results from ling  the foil experiments were run using quinlan s representation described
in section      as in quinlan         negative examples were provided by using a randomlyselected     of those which could be generated using the closed world assumption   all
experiments with foidl and ifoil used the standard default values for the various numeric
parameters  term universe size        minimum clause coverage     weak literal limit     
the differences among foil  ifoil  and foidl were tested for significance using a twotailed paired t test 
   handling intensional interpretation of recursive clauses for the target predicate requires some additional
complexities that have not been discussed in this paper since they are not relevant to decision lists  which
are generally not recursive 
   both versions are available by anonymous ftp from net cs utexas edu in the directory
pub mooney foidl 
   we replicated quinlan s approach since memory limitations prevented us from using      of the generated negatives with larger training sets 

  

fimooney   califf

   

  

accuracy

  

  
foidl
ifoil
foil
spa
neural network

  

 
 

   

   

   
training examples

   

   

figure    accuracy on phonetic past tense task using all verbs

    results
the results for the phonetic task using both regular and irregular verbs are presented
in figure    the graph shows our results with foil  ifoil  and foidl along with the
best results from ling  who did not provide a learning curve for this task  as expected 
foidl out performed the other systems on this task  surpassing ling s best results with    
examples with only     examples  ifoil performed quite poorly  barely beating the neural
network results despite effectively having      of the negatives as opposed to foil s     
this poor performance is due at least in part to overfitting the training data  because ifoil
lacks the noise handling techniques of foil   foil also has the advantage of the three place
predicate  which gives it a bias toward learning suxes  ifoil s poor performance on this
task shows that the implicit negatives by themselves are not sucient  and that some other
bias such as decision lists or the three place predicate and noise handling is needed  the
differences between foil and foidl are significant at the      level  those between foidl
and ifoil are significant at the       level  the differences between foil and ifoil are
not significant with     training examples or less  but are significant at the       level with
    and     examples 
figure   presents accuracy results on the phonetic task using regulars only  the curves
for spa and the neural net are the results reported by ling  here again  foidl outperformed the other systems  this particular task demonstrated one of the problems with
using closed world negatives  in the regular past tense task  the second argument of quinlan s   place predicate is always the same  an empty list  therefore  if the constants are
generated from the positive examples  foil will never produce rules which ground the second argument  since it cannot create negative examples with other constants in the second
argument  this prevents the system from learning a rule to generate the past tense  in order
  

fiinduction of first order decision lists  learning english past tense

   

  

accuracy

  

  
foidl
ifoil
foil
spa
neural network

  

 
 

  

   

   

   
   
   
training examples

   

   

   

   

figure    accuracy on phonetic past tense task using regulars only
to obtain the results reported here  we introduced extra constants for the second argument
 specifically the constants for the third argument   enabling the closed world assumption
to generate appropriate negatives  on this task  ifoil does seem to gain some advantage
over foil from being able to effectively use all of the negatives  the regularity of the data
allows both ifoil and foil to achieve over     accuracy at     examples  the differences
between foil and foidl are significant at the       level  as are those between ifoil and
foidl  the differences between ifoil and foil are not significant with    examples  and
are significant at the      level with     examples  but are significant at the       level with
       training examples 
results for the alphabetic version appear in figure    this is a task which has not
typically been considered in the literature  but it is of interest to those concerned with
incorporating morphology into natural language understanding systems which deal with
text  it is also the most dicult task  primarily because of consonant doubling  here we
have results only for foidl  ifoil  and foil  because the alphabetic task is even more
irregular that the full phonetic task  ifoil again overfits the data and performs quite poorly 
the differences between foil and foidl are significant at the       level with             
and     examples  but only at the     level with     examples  the differences between
ifoil and foidl are all significant at the       level  those between foil and ifoil are
not significant with    training examples and are significant only at the      level with   
training examples  but are significant at the       level with     or more examples 
for all three of these tasks  foidl clearly outperforms the other systems  demonstrating
that the first order decision list bias is a good one for this learning task  a sucient set of
negatives is necessary  and all five of these systems provide them in some way  the neural
network and spa both learn multiple class classification tasks  which phoneme belongs in
each position   foil uses the three place predicate with closed world negatives  and ifoil
  

fimooney   califf

  

  

  

accuracy

  

  

  

  

foidl
ifoil
foil

  

  

 
 

  

   

   

   
   
   
training examples

   

   

   

   

figure    accuracy on alphabetic past tense task
and foidl  of course  use the output completeness assumption  the primary importance
of the implicit negatives is not that they provide an advantage over propositional and
neural network systems  but that they enable first order systems to perform this task at
all  without them  some knowledge of the task is required  foidl s decision lists give it a
significant added advantage  though this advantage is less apparent in the regular phonetic
task  where there are no exceptions 
clearly  foidl produces more accurate rules than the other systems  but another consideration is the complexity of the rule sets  for the ilp systems  two good measures of
complexity are the number of rules and number of literals generated  figure   shows the
number of rules generated by foil  ifoil  and foidl for the phonetic task using all verbs 
the number of literals generated appears in figure    since we are interested in generalization and since foil does not attempt to fit all of the training data  these results do not
include the rules foidl and ifoil add in order to memorize individual exceptions   although the numbers are comparable with only a few examples  with increasing numbers of
examples  the programs foil and ifoil generate grow much faster than foidl s programs 
the large number of rules literals learned by ifoil show its tendency to overfit the data 
foidl also generates very comprehensible programs  the following is an example program generated for the alphabetic version of the task using     examples  again excluding
the memorized examples  
past a b     split a c  e p    split b c  p t     
past a b     split a c  y    split b c  i e d    split a d  r y     
past a b     split a c  y    split b c  i e d    split a d  l y     

   because of the large number of irregular pasts in english  foidl memorizes an average of    verbs per
trial with     examples 

  

fiinduction of first order decision lists  learning english past tense

  

  

foidl
ifoil
foil

number of rules

  

  

  

  

  

  

 
 

  

   

   

   
   
   
training examples

   

   

   

   

figure    number of rules created for phonetic past tense task

   

   

foidl
ifoil
foil

number of literals

   

   

   

   

  

 
 

  

   

   

   
   
   
training examples

   

   

   

   

figure    number of literals created for phonetic past tense task
  

fimooney   califf

past a b 
past a b 
past a b 
past a b 

     

split b a  m e d    split a c  m    split a  s  d    
split b a  r e d    split a c  u r     
split b a  d    split a c  e     
split b a  e d     

the training times for the various systems considered in this research are dicult to
compare  ling does not provide timing results  though we can probably assume based on
research comparing symbolic and neural learning algorithms  shavlik  mooney    towell 
      that spa runs fairly quickly since it is based on c    and that backpropagation took
considerably longer  our tests with foil and foidl are not directly comparable because
they were run on different architectures  the foil runs were done on a sparc    for
    examples  foil averaged    minutes on the phonetic task with all verbs  the foidl
experiments ran on a sparc   and averaged      minutes on the same task  even allowing
for the differences in speed of the two machines  about a factor of two   foidl is quite a
bit slower  probably due largely to the cost of using intentional background and in part to
its implementation in prolog as opposed to c 

   related work

    related work on ilp

although each of the three features mentioned in the introduction distinguishes foidl from
most work in inductive logic programming  a number of related pieces of research should be
mentioned  the use of intensional background knowledge is the least distinguishing feature
since a number of other ilp systems also incorporate this aspect  focl  pazzani   kibler 
       mfoil  lavrac   dzeroski         grendel  cohen         forte  richards  
mooney         and chillin  zelle   mooney      a  all use intensional background to
some degree in the context of a foil like algorithm  some other ilp systems which employ
intensional background include early ones by shapiro        and sammut and banerji       
and more recent ones by bergadano et al         and stahl  tausend  and wirth        
the use of implicit negatives is significantly more novel  as described in section      this
approach is considerably different from explicit construction using a closed world assumption  and therefore can be employed when explicit construction of sucient negative examples is intractable  bergadano et al         allows the user to supply an intensional definition
of negative examples that covers a large set of ground instances  e g  past  a c t  x  
not equal x  a c t e d       however  to be equivalent to output completeness  the user
would have to explicitly provide a separate intensional negative definition for each positive
example  the non monotonic semantics used to eliminate the need for negative examples in
claudien  de raedt   bruynooghe        has the same effect as an output completeness
assumption in the case where all arguments of the target relation are outputs  however 
output completeness permits more exibility by allowing some arguments to be specified as
inputs and only counting as negative examples those extra outputs generated for specific
inputs in the training set  flip  bergadano        provides a method for learning functional programs without negative examples by making an assumption equivalent to output
completeness for the functional case  output completeness is more general in that it permits learning non functional programs as well  also  unlike foidl  none of these previous
  

fiinduction of first order decision lists  learning english past tense

methods provide a way of quantifying implicit negative coverage in the context of a heuristic
top down specialization algorithm 
the notion of a first order decision list is unique to foidl  the only other ilp system
that attempts to learn programs that exploit clause order and cuts is that of bergadano et al 
        their paper discusses many problems with learning arbitrary programs with cuts 
and the brute force search used in their approach is intractable for most realistic problems 
instead of addressing the general problem of learning arbitrary programs with cuts  foidl
is tailored to the specific problem of learning first order decision lists  which use cuts in a
very stylized manner that is particularly useful for functional problems that involve rules
with exceptions  bain and muggleton        and bain        discuss a technique which uses
negation as failure to handle exceptions  however  using negation as failure is significantly
different from decision lists since it simply prevents a clause from covering exceptions rather
than learning an additional clause that both over rides an existing clause and specifies the
correct output for a set of exceptions 

    related work on past tense learning
the shortcomings of most previous work on past tense learning were reviewed in section     
and the results in section   clearly demonstrate the generalization advantage foidl exhibits
on this problem  however  a couple of issues deserve some additional discussion 
most of the previous work on this problem has concerned the modelling of various
psychological phenomenon  such as the u shaped learning curve that children exhibit for
irregular verbs when acquiring language  this paper has not addressed the issue of psychological validity  rather it has focused on performance accuracy after exposure to a fixed
number of training examples  therefore  we make no specific psychological claims based on
our current results 
however  humans can obviously produce the correct past tense of arbitrarily long novel
words  which foidl can easily model while fixed length feature based representations clearly
cannot  ling also developed a version of spa that eliminates position dependence and fixed
word length  ling        by using a sliding window like that used in nettalk  sejnowski
  rosenberg         a large window is used which includes    letters on either side of
the current position  padded with blanks if necessary  in order to always include the entire
word for all the examples in the corpus  the results on this approach are significantly better
than normal spa but still inferior to foidl s results  also  this approach still requires a
fixed sized input window which prevents it from handling arbitrary length irregular verbs 
recurrent neural networks could also be used to avoid word length restrictions  cotrell  
plunkett         although it appears that no one has yet applied them to the standard
present tense to past tense mapping problem  however  we believe the diculty of training
recurrent networks and their relatively poor ability to maintain state information arbitrarily
long would limit their performance on this task 
another issue is that of the comprehensibility and transparency of the learned result 
foidl s programs for past tense are short  concise  and very readable  unlike the complicated networks  decision forests  and pure logic programs generated by previous approaches 
ling and marinov        discusses the possibility of transforming spa s decision forest into
  

fimooney   califf

more comprehensible first order rules  however  the approach of directly learning first order
rules from the data seems clearly preferable 

   future work
one obvious topic for future research is foidl s cognitive modelling abilities in the context
of the past tense task  incorporating over fitting avoidance methods may allow the system
to model the u shaped learning curve in a manner analogous to that demonstrated by ling
and marinov         its ability to model human results on generating the past tense of
novel psuedo verbs  e g   spling   splang  could also be examined and compared to spa
 ling   marinov        and connectionist methods 
although first order decision lists represent a fairly general class of programs  currently
our only convincing experimental results are on the past tense problem  many realistic
problems consist of rules with exceptions  and experimental results on additional applications are needed to support the general utility of this representation 
despite its advantages  the use of intensional background knowledge in ilp incurs a
significant performance cost  since examples must be continually reproved when testing
alternative literals during specialization  this computation accounts for most of the training
time in foidl  one approach to improving computational eciency would be to maintain
partial proofs of all examples and incrementally update these proofs as additional literals
are added to the clause  this approach would be more like foil s approach of maintaining
tuples  but would require using a meta interpreter in prolog  which incurs its own significant
overhead  ecient use of intensional knowledge in ilp could greatly benefit from work on
rapid incremental compilation of logic programs  i e   incrementally updating compiled code
to account for small changes in the definition of a predicate 
foidl could potentially benefit from methods for handling noisy data and preventing
over fitting  pruning methods employed in foil and related systems  quinlan        lavrac
  dzeroski        could easily be incorporated  in the decision list framework  an alternative
to simply ignoring incorrectly covered examples as noise is to treat them as exceptions to
be handled by subsequently learned clauses  as in the uncovering technique discussed in
section      
theoretical results on the learnability of restricted classes of first order decision lists is
another interesting area for research  given the results on the pac learnability of propositional decision lists  rivest        and restricted classes of ilp problems  dzeroski  muggleton    russell        cohen         an appropriately restricted class of first order decision
lists should be pac learnable 

   conclusions
this paper has addressed two main issues  the appropriateness of a first order learner for
the popular past tense problem  and the problems of previous ilp systems in handling
functional tasks whose best representation is rules with exceptions  our results clearly
demonstrate that an ilp system outperforms both the decision tree and the neural network
systems previously applied to the past tense task  this is important since there have been
very few results showing that a first order learner performs significantly better than apply  

fiinduction of first order decision lists  learning english past tense

ing propositional learners to the best feature based encoding of a problem  this research
also demonstrates that there is an ecient and effective algorithm for learning concise 
comprehensible symbolic programs for a small but interesting subproblem in language acquisition  finally  our work also shows that it is possible to eciently learn logic programs
which involve cuts and exploit clause order for a particular class of problems  and it demonstrates the usefulness of intensional background and implicit negatives  solutions to many
practical problems seem to require general default rules with characterizable exceptions 
and therefore may be best learned using first order decision lists 

acknowledgements
most of the basic research for this paper was conducted while the first author was on leave at
the university of sydney supported by a grant to prof  j r  quinlan from the australian
research council  thanks to ross quinlan for providing this enjoyable and productive
opportunity and to both ross and mike cameron jones for very important discussions and
pointers that greatly aided the development of foidl  thanks also to ross for aiding us
in running the foil experiments  discussions with john zelle and cindi thompson at
the university of texas also inuenced this work  partial support was also provided by
grant iri         from the national science foundation and an mcd fellowship from the
university of texas awarded to the second author 

references

bain  m          experiments in non monotonic first order induction  in muggleton  s 
 ed    inductive logic programming  pp           academic press  new york  ny 
bain  m     muggleton  s          non monotonic learning  in muggleton  s   ed    inductive logic programming  pp           academic press  new york  ny 
bergadano  f          an interactive system to learn functional logic programs  in proceedings of the thirteenth international joint conference on artificial intelligence  pp 
          chambery  france 
bergadano  f   gunetti  d     trinchero  u          the diculties of learning logic programs with cut  journal of artificial intelligence research            
califf  m  e          learning the past tense of english verbs  an inductive logic programming approach  unpublished project report 
cameron jones  r  m     quinlan  j  r          ecient top down induction of logic
programs  sigart bulletin               
clark  p     niblett  t          the cn  induction algorithm  machine learning    
        
cohen  w  w          pac learning nondeterminate clauses  in proceedings of the twelfth
national conference on artificial intelligence  pp          seattle  wa 
  

fimooney   califf

cohen  w          compiling prior knowledge into an explicit bias  in proceedings of
the ninth international conference on machine learning  pp          aberdeen 
scotland 
cotrell  g     plunkett  k          learning the past tense in a recurrent network  acquiring the mapping from meaning to sounds  in proceedings of the thirteenth annual
conference of the cognitive science society  pp          chicago  il 
de raedt  l     bruynooghe  m          a theory of clausal discovery  in proceedings of
the thirteenth international joint conference on artificial intelligence  pp           
chambery  france 
dzeroski  s   muggleton  s     russell  s          pac learnability of determinate logic
programs   in proceedings of the      workshop on computational learning theory
pittsburgh  pa 
lachter  j     bever  t          the relation between linguistic structure and associative
theories of language learning  a constructive critique of some connectionist learning
models  in pinker  s     mehler  j   eds    connections and symbols  pp          
mit press  cambridge  ma 
lavrac  n     dzeroski  s   eds            inductive logic programming  techniques and
applications  ellis horwood 
ling  c  x          learning the past tense of english verbs  the symbolic pattern associator vs  connectionist models  journal of artificial intelligence research             
ling  c  x          personal communication 
ling  c  x     marinov  m          answering the connectionist challenge  a symbolic
model of learning the past tense of english verbs  cognition                  
macwhinney  b     leinbach  j          implementations are not conceptualizations  revising the verb model  cognition              
muggleton  s     buntine  w          machine invention of first order predicates by inverting resolution  in proceedings of the fifth international conference on machine
learning  pp          ann arbor  mi 
muggleton  s     feng  c          ecient induction of logic programs  in proceedings of
the first conference on algorithmic learning theory tokyo  japan  ohmsha 
muggleton  s   king  r     sternberg  m          protein secondary structure prediction
using logic based machine learning  protein engineering                 
muggleton  s  h   ed            inductive logic programming  academic press  new york 
ny 
norvig  p          paradigms of artificial intelligence programming  case studies in common lisp  morgan kaufmann  san mateo  ca 
  

fiinduction of first order decision lists  learning english past tense

pazzani  m     kibler  d          the utility of background knowledge in inductive learning 
machine learning           
pinker  s     prince  a          on language and connectionism  analysis of a parallel
distributed model of language acquisition  in pinker  s     mehler  j   eds    connections and symbols  pp          mit press  cambridge  ma 
quinlan  j  r          c     programs for machine learning  morgan kaufmann  san
mateo ca 
quinlan  j  r          past tenses of verbs and first order learning  in zhang  c   debenham 
j     lukose  d   eds    proceedings of the seventh australian joint conference on
artificial intelligence  pp        singapore  world scientific 
quinlan  j  r     cameron jones  r  m          foil  a midterm report  in proceedings
of the european conference on machine learning  pp       vienna 
quinlan  j          learning logical definitions from relations  machine learning        
        
richards  b  l     mooney  r  j          automated refinement of first order horn clause
domain theories  machine learning  in press 
rivest  r  l            learning decision lists  machine learning                 
rumelhart  d  e     mcclelland  j          on learning the past tense of english verbs  in
rumelhart  d  e     mcclelland  j  l   eds    parallel distributed processing  vol 
ii  pp           mit press  cambridge  ma 
sammut  c     banerji  r  b          learning concepts by asking questions  in michalski 
r  s   carbonell  j  g     mitchell  t  m   eds    machine learning  an ai approach 
vol  ii  pp           morgan kaufman 
sejnowski  t  j     rosenberg  c          parallel networks that learn to pronounce english
text  complex systems             
shapiro  e          algorithmic program debugging  mit press  cambridge  ma 
shavlik  j  w   mooney  r  j     towell  g  g          symbolic and neural learning
algorithms  an experimental comparison  machine learning             
stahl  i   tausend  b     wirth  r          two methods for improving inductive logic
programming systems  in machine learning  ecml     pp        vienna 
webb  g  i     brkic  n          learning decision lists by prepending inferred rules  in
proceedings of the australian workshop on machine learning and hybrid systems 
pp       melbourne  australia 
zelle  j  m     mooney  r  j       a   combining top down and bottom up methods in
inductive logic programming  in proceedings of the eleventh international conference
on machine learning new brunswick  nj 
  

fimooney   califf

zelle  j  m     mooney  r  j       b   inducing deterministic prolog parsers from treebanks 
a machine learning approach  in proceedings of the twelfth national conference on
artificial intelligence  pp          seattle  wa 

  

fi