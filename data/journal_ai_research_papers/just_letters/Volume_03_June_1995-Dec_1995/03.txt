journal of artificial intelligence research                 

submitted       published      

rule based machine learning methods for functional
prediction

sholom m  weiss

weiss cs rutgers edu

nitin indurkhya

nitin cs usyd edu au

department of computer science  rutgers university
new brunswick  new jersey        usa
department of computer science  university of sydney
sydney  nsw       australia

abstract

we describe a machine learning method for predicting the value of a real valued function  given the values of multiple input variables  the method induces solutions from
samples in the form of ordered disjunctive normal form  dnf  decision rules  a central
objective of the method and representation is the induction of compact  easily interpretable
solutions  this rule based decision model can be extended to search eciently for similar cases prior to approximating function values  experimental results on real world data
demonstrate that the new techniques are competitive with existing machine learning and
statistical methods and can sometimes yield superior regression performance 

   introduction
the problem of approximating the values of a continuous variable is described in the statistical literature as regression  given samples of output  response  variable y and input
 predictor  variables x   fx     xn g  the regression task is to find a mapping y   f x   relative to the space of possibilities  finite samples are far from complete  and a predefined
model is needed to concisely map x to y  accuracy of prediction  i e  generalization to new
cases  is of primary concern  regression differs from classification in that the output variable y in regression problems is continuous  whereas in classification y is strictly categorical 
from this perspective  classification can be thought of as a subcategory of regression  some
machine learning researchers have emphasized this connection by describing regression as
 learning how to classify among continuous classes   quinlan        
the traditional approach to the problem is classical linear least squares regression
 scheffe         developed and refined over many years  linear regression has proven quite
effective for many real world applications  clearly the elegant and computationally simple linear model has its limits  and more complex models may fit the data better  with
the increasing computational power of computers and with larger volumes of data  interest has grown in pursuing alternative nonlinear regression methods  nonlinear regression
models have been explored by the statistics research community and many new effective
methods have emerged  efron         including projection pursuit  friedman   stuetzle 
      and mars  friedman         methods for nonlinear regression have also been developed outside the mainstream statistics research community  a neural network trained
by back propagation  mcclelland   rumelhart        is one such model  other models
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fiweiss   indurkhya
can be found in numerical analysis  girosi   poggio         an overview of many different
regression models  with application to classification  is available in the literature  ripley 
       most of these methods produce solutions in terms of weighted models 
in the real world  classification problems are more commonly encountered than regression problems  this accounts for the greater attention paid to classification than to regression  but many important problems in the real world are of the regression type  for
instance  problems involving time series usually involve prediction of real values  besides
the fact that regression problems are important on their own  another reason for the need to
focus on regression is that regression methods can be used to solve classification problems 
for example  neural networks are often applied to classification problems 
the issue of interpretable solutions has been an important consideration leading to
development of  symbolic learning methods   a popular format for interpretable solutions
is the disjunctive normal form  dnf  model  weiss   indurkhya      a   decision trees and
rules are examples of dnf models  decision rules are similar in characteristics to decision
trees  but they also have some potential advantages   a  a stronger model  b  often better
explanatory capabilities  unlike trees  dnf rules need not be mutually exclusive  thus 
their solution space includes all tree solutions  these rules are potentially more compact
and predictive than trees  decision rules may also offer greater explanatory capabilities
than trees because as a tree grows in size  its interpretability diminishes 
among symbolic learning methods  decision tree induction  using recursive partitioning  is highly developed  many of these methods developed within the machine learning
community  such as id  decision tree induction  quinlan         have been applied exclusively to classification tasks  less widely known is that decision trees are also effective
in regression  the cart program  developed in the statistical research community  induces both classification and regression trees  breiman  friedman  olshen    stone        
these regression trees are strictly binary trees  a representation which naturally follows
from intensive modeling using continuous variables  
in terms of performance  regression trees often are competitive in performance to other
regression methods  breiman et al          regression trees are noted to be particularly
strong when there are many higher order dependencies among the input variables  friedman 
       the advantages of the regression tree model are similar to the advantages enjoyed by
classification trees over other models  two principal advantages can be cited   a  dynamic
feature selection and  b  explanatory capabilities  tree induction methods are extremely
effective in finding the key attributes in high dimensional applications  in most applications 
these key features are only a small subset of the original feature set  another characteristic
of decision trees that is often cited is its capability for explanation in terms acceptable
to people  on the negative side  decision trees cannot represent compactly many simple
functions  for example linear functions  a second weakness is that the regression tree
model is discrete  yet predicts a continuous variable  for function approximation  the
expectation is a smooth continuous function  but a decision tree provides discrete regions
that are discontinuous at the boundaries  all in all though  regression trees often produce
strong results  and for many applications their advantages strongly outweigh their potential
disadvantages 
   a comparative study  fayyad   irani        suggests that binary classification trees are somewhat more
predictive even for categorical variables 

   

firule based functional prediction
in this paper we describe a new method for inducing regression rules  the method
takes advantage of the close relationship between classification and regression and provides a
uniform and general model for dealing with both problems  additional gains can be obtained
by extending this method in a manner that preserves the strengths of the partitioning
schemes while compensating for their weaknesses  rules can be used to search for the most
relevant cases  and a subset of these cases can help determine the function value  thus 
some of the model s interpretability can be traded off for better performance  empirical
results suggest that these methods are effective and can induce solutions that are often
superior to decision trees 

   measuring performance

the objective of regression is to minimize the distance between the sample output values 
yi and the predicted values yi   two measures of distance are commonly used  the classical
regression measure is equation    the average squared distance between yi and yi   i e  the
variance  it leads to an elegant formulation for the linear least squares model  the mean
absolute distance  deviation  of equation   is used in least absolute deviation regression 
and is perhaps the more intuitive measure 
the mean absolute distance  deviation  of equation   is used in our studies  this is a
measure of the average error of prediction for each yi over n cases 
 

 

xn  yi   yi  
i  
n
x
mad     jyi   yi j

v ariance   n 

 

   

   
n i  
the regression problem is sometimes described as a signal and noise problem  the model
is extended to include a stochastic component  in equation    thus  the true function may
not produce a zero error distance  in contrast to classification where the labels are assumed
correct  for regression the predicted y values could be explained by a number of factors
including a random noise component    in the signal  y 
 

y   f  x        xn    
   
because prediction is the primary concern  estimates based on training cases alone
are inadequate  the principles of predicting performance on new cases are analogous to
classification  but here the mean absolute distance is used as the error rate  the best
estimate of true performance of a model is the error rate on a large set of independent test
cases  when large samples of data are unavailable  the process of train and test is simulated
by random resampling  in most of our experiments  we used     fold  cross validation to
estimate predictive performance 

   regression by tree induction

in this section  we contrast regression tree induction with classification tree induction  like
classification trees  regression trees are induced by recursive partitioning  the solution takes
   

fiweiss   indurkhya
the form of equation    where ri are disjoint regions  ki are constant values  and yji refers
to the y values of the training cases that fall within the region ri  

if x  ri then f  x    ki   medianfyji g
   
regression trees have the same representation as classification trees except for the terminal nodes  the decision at a terminal node is to assign a case a constant y value  the
single best constant value is the median of the training cases falling into that terminal node
because for a partition  the median is the minimizer of mean absolute distance  figure   is
an example of a binary regression tree  all cases reaching shaded terminal node    x   
are assigned a constant value of y    
 
x    

x   

 

 

y   

x    

x   

 

 

y  

y  

figure    example of regression tree
tree induction methods usually proceed by  a  finding a covering set for the training
cases and  b  pruning the tree to the best size  although classification trees have been
more widely studied  a similar approach can be applied to regression trees  we assume
the reader is familiar with classification trees  and we cite only the differences in binary
tree induction  breiman et al         quinlan        weiss   kulikowski         in many
respects  regression tree induction is more straightforward  for classification trees  the error
rate is a poor choice for node splitting  and alternative functions such as entropy or gini
are employed  for regression tree induction  the minimized function  i e  absolute distance 
is most satisfactory  at each node  the single best split that minimizes the mean absolute
distance is selected  splitting continues until fewer than a minimum number of cases are
covered by a node  or until all cases within the node have the identical value of y 
the goal is to find the tree that generalizes best to new cases  and this is often not a
full covering tree  particularly in presence of noise or weak features  the pruning strategies
employed for classification trees are equally valid for regression trees  like the covering
procedures  the only substantial difference is that the error rate is measured in terms of
mean absolute distance  one popular method is the weakest link pruning strategy  breiman
et al          for weakest link pruning  a tree is recursively pruned so that the ratio delta n
is minimized  where n is the number of pruned nodes and delta is the increase in error 
   

firule based functional prediction
x     y   
x     y  
otherwise y  
figure    example of regression rules
weakest link pruning has several desirable characteristics   a  it prunes by training cases
only  so that the remaining test cases are relatively independent  b  it is compatible with
resampling 

   regression by rule induction
both tree and rule induction models find solutions in disjunctive normal form  and the model
of equation   is applicable to both  each rule in a rule set represents a single partition or
region ri   however  unlike the tree regions  the regions for rules need not be disjoint  with
non disjoint regions  several rules may be satisfied for a single sample  some mechanism
is needed to resolve the conicts in ki   the constant values assigned  when multiple rules 
ri regions  are invoked  one standard model  weiss   indurkhya      a  is to order the
rules  such ordered rule sets have also been referred to as decision lists  the first rule that
is satisfied is selected  as in equation   

if i   j and x  both ri and rj then f  x    ki
   
figure   is an example of an ordered rule set corresponding to the tree of figure    all
cases satisfying rule    and not rules   and    are assigned a value of y   
given this model of regression rule sets  the problem is to find procedures that effectively
induce solutions  for rule based regression  a covering strategy analogous to the classification tree strategy could be specified  a rule could be induced by adding a single component
at a time  where each added component is the single best minimizer of distance  as usual 
the constant value ki is the median of the region formed by the current rule  as the rule is
extended  fewer cases are covered  when fewer than a minimal number of cases are covered 
rule extension terminates  the covered cases are removed and rule induction can continue
on the remaining cases  this is also the regression analogue of rule induction procedures
for classification  michalski  mozetic  hong    lavrac        clark   niblett        
however  instead of this approach  we propose a novel strategy of mapping the regression
covering problem into a classification problem 

    a reformulation of the regression problem

the motivation for mapping regression into classification is based on a number of factors
related to the extra information given in the regression problem  the natural ordering of yi
by magnitude  if i   j then yi   yj  
let fci g be a set consisting of an arbitrary number of classes  each class containing
approximately equal values of fyi g  to solve a classification problem  we expect that the
classes are different from each other  and that patterns can be found to distinguish these
   

fiweiss   indurkhya
   generate a set of pseudo classes using the p class algorithm  figure    
   generate a covering rule set for the transformed classification
problem using a rule induction method such as swap  
 weiss   indurkhya      a  
   initialize the current rule set to be the covering rule set and save it 
   if the current rule set can be pruned  iteratively do the following 
a  prune the current rule set 
b  optimize the pruned rule set  figure    and save it 
c  make this pruned rule set the new current rule set 
   use test cases or cross validation to pick the best of the saved rule sets 
figure    overview of method for learning regression rules
classes  should we expect classes formed by an ordering of fyi g to be a reasonable classification problem  there are a numbers of reasons why the answer is yes  particularly for a
rule induction procedure 
the most obvious situation is the classical linear relationship  in this instance  by
definition  some ordering of fx i       xni g corresponds to the ordering of yi   although classical
methods are very strong in compactly determining linear functions  most interest in modern
methods centers around their potential for finding nonlinear relationships  for nonlinear
functions  we know there is usually no such ordering of fx i       xni g corresponding to the
fyig  still  we expect that the true function is smooth  and in a local region the ordering
relationship will hold  in terms of classification  we know that a class cj with similar values
of y is quite different than class ck with much lower values of y  for a nonlinear function
within a class of similar values of y  some of these y have very similar values of fx i       xni g 
these correspond to some local region of the function  however  it is also true that some
identical values of y can have very different fx i       xni g so that multiple clusters can be
found within the class  because rule induction methods do not cover a class with a single
rule  the expectation is that multiple patterns will be found to cover these clusters 
once the cases have been assigned such  pseudo  classes  the classification problem can
be solved in the following stages   a  find a covering set and  b  prune the rule set to an
appropriate size  with improved results achieved when an additional technique is considered 
 c  refine or optimize a rule set  the overall method is outlined in figure   

    generating pseudo classes
in the previous section  we described the motivation for pseudo classes  the specification
of these classes does not use any information beyond the ordering of y  no assumptions
about the true nature of the underlying function are made  within this environment  the
goal is to make the y values within one class most similar and y values across classes most
dissimilar  we wish to assign the y values to classes such that the overall distance between
each yi and its class mean is minimum 
   

firule based functional prediction
input  fyi g a set of output values
initialize n    number of cases  k    number of classes
for each classi
classi    next n k cases from list of sorted y values
end for
compute errnew
repeat
errold   errnew
for each casej
when it is in classi
   if dist casej   mean classi        dist casej   mean classi   
move casej to classi  
   if dist casej   mean classi        dist casej   mean classi   
move casej to classi  
next casej
compute errnew
until errnew is not less than errold
figure    composing pseudo classes  p class 
figure   describes an algorithm  p class  for assigning the values fyi g to k classes  essentially the algorithm does the following   a  sorts the y values   b  assigns approximately
equal numbers of contiguous sorted yi to each class   c  moves a yi to a contiguous class
when that reduces the global distance err from each yi to the mean of its assigned class 
classes with identical means should be merged  p class is a variation of k means clustering  a statistical method that minimizes a distance measure  hartigan   wong        
alternative methods that do not depend on distance measures  lebowitz        may also
be used 
given a fixed number of k classes  this procedure will relatively quickly assign the yi to
classes such that the overall distances are minimized  because the underlying function is
unknown  it is not critical to have a global minimum assignment of the yi  this procedure
matches well to our stated goals for ordering the yi values  the obvious remaining question is
how do we determine k  the number of classes  unfortunately  there is no direct answer  and
some experimentation is necessary  however  as we shall see in section    there is empirical
evidence suggesting that results are quite similar within a local neighborhood of values of
k  moreover  relatively large values of k  which entail increased computational complexity
for rule induction  are typically necessary only for noise free functions that can be modeled
exactly  analogous to comparisons of neural nets with increasing numbers of hidden units 
the trends for increasing numbers of partitions become evident during experimentation 
one additional variation on the classification theme arises for rule induction schemes
that cover one class at a time  the classes must be ordered  and the last class typically
   

fiweiss   indurkhya
becomes a default class to cover situations when no rule for other classes is satisfied  for
regression  having one default partition for a class is unlikely to be the best covering solution 
and instead the remaining cases for the last class are repeatedly partitioned  by p class 
into   classes until fewer than m cases remain 
an interesting characteristic of this transformation of the regression problem is that
we now have a uniform and general model that once again relates both classification and
regression  if the yi values are discrete and categorical  p class merely restates the standard
classification problem  for example  if all values of yi are either   or    then the result of
p class will be be   non empty classes 

    a covering rule set
with this transformation  rule induction algorithms for classification can be applied  we
will consider those induction methods that fully cover a class before moving on to induce
rules for the next class  at each step of the covering algorithm  the problem is considered
a binary classification problem for the current class ci versus all cj where j   i  i e  the
current class versus the remaining classes  when a rule is induced  its corresponding cases
are removed and the remaining cases are considered  when a class has been covered  the
next class is considered  an example of such a covering algorithm is that used in swap  
 weiss   indurkhya      a   and this is the procedure used in this paper  the covering
method is identical for classification and regression  however  one distinction is that the
regression classes are transient labels that are replaced with the median of the y values for
the cases covered by each induced rule  because the rules are ordered and multiple rules
may be satisfied  the medians are derived only from those instances where the rule is the
first to be satisfied 
although this procedure may yield good  compact covering sets  additional procedures
are necessary for a complete solution 

    pruning the rule set
typical real world applications have noisy features that are not fully predictive  a covering
set  particularly one composed of many continuous variables  can be far too over specialized
to produce the best results  for classification  relatively few classes are specified in advance 
for regression  we expect many smaller groups because values of yi are likely to be quite
different 
we noted earlier that for regression trees the usual classification pruning techniques can
be applied with the substitution of mean absolute distance for the classification error rate 
as in weakest link tree pruning  the same ratio of delta n can be recursively minimized for
weakest link rule pruning  the intuitive rationale is to remove those parts of a rule set that
have the least impact on increasing the error  pruning rule sets is usually accomplished by
either deleting complete rules or single rule components  quinlan        weiss   indurkhya 
    a   in general  rule pruning  for both classification and regression  is less natural and
far more computationally expensive than tree pruning  tree pruning has a natural ow
from set to subset  thus a tree can be pruned from bottom up  typically considering the
effect of removing a subtree  non disjoint rules have no such natural pruning order  for
   

firule based functional prediction
example every component in a rule is a candidate for pruning and may affect all other rules
that follow it in the specified rule order 
there is a major difference in pruning regression rules vs  classification rules  for
classification  deleting a rule or a rule component has no effect on the class labels  for
regression  pruning will change the median values of y for the regions  even the deletion of
a rule will affect other region medians because the rules are ordered and multiple rules may
be satisfied  this characteristic of rule pruning for regression adds substantial complexity
to the task  however  by assuming that the median values of y remain unchanged during
the evaluation of candidate rules to prune  a pruning procedure can achieve reasonable
computational eciency at the expense of some loss in the accuracy of evaluation  once
the best rule or component for deletion is selected  the medians of all regions can then be
re evaluated 
even for classification rules  rule pruning has some inherent weaknesses  for example 
rule deletion will often create a gap in coverage  for classification rules though  it is quite
feasible to develop an additional procedure to refine and optimize a rule set  to a large
extent  this overcomes the cited weakness in pruned rules sets  a similar refinement and
optimization procedure can be developed for regression and is described next 

    rule refinement and optimization

given a rule set rsi   can it be improved  this question applies to any rule set  although
we are mostly motivated by trying to improve the pruned rules sets frso       rsi       rsng 
this is a combinatorial optimization problem  using error measure err rs   can we improve
rsi without changing its size  i e  the number of rules and components  figure   describes
an algorithm that minimizes err rs   the mad of the model prediction on sample cases 
by local swapping  i e  replacing a single rule component with the best alternative  it is a
variation of the techniques used in swap    weiss   indurkhya      a  
the central theme is to hold a model configuration constant and make a single local
improvement to that configuration  local modifications are made until no further improvements are possible  making local changes to a configuration is a widely used optimization
technique to approximate a global optimum and has been applied quite successfully  for
example to find near optimum solutions to traveling salesman problems  lin   kernighan 
       an analogous local optimization technique  called backfitting  has been used in the
context of nonlinear statistical regression  hastie   tibshirani        
variations on the selection of the next improvement move could include 
   first local improvement encountered  such as in backfitting 
   best local improvement  such as in swap   
in our experiments with rule induction methods  the results are consistently better for
         is more ecient  but the  pruned  rule induction environment is mostly stable with
relatively few local improvements prior to convergence  in a less stable environment  with
very large numbers of possible configuration changes      may not be feasible or even better 
in the pruned rule set environment  if the covering procedure is effective  then each pruned
solution should be relatively close to a local minimum solution  weakest link pruning
   

fiweiss   indurkhya
input  rs a rule set consisting of rules ri   and
s a set of training cases
d    true
while  d is true  do
rsnew    rs with the single best replacement for a
component of rs that most reduces err rs  on
cases in s using current median ri  
if no replacement is found then
d    false
else
rs    rsnew   recompute medians ri  
endwhile
return the rule set rs
figure    optimization by rule component swapping
results in a series of pruned rule sets rsi that number far fewer than sets which would
result from a single prune of a rule or rule component  each of the rsi are optimized prior
to continuing the pruning process  however  rule set optimization can usually be suspended
until substantial segments of the covering set have already been pruned 
if     is used  then either sequentially ordered evaluations  as in backfitting  or stochastic evaluations can be considered  empirical evidence in the optimization literature supports the superiority of stochastic evaluation  jacoby  kowalik    pizzo         further
improvements may be obtained by occasionally making random changes in configuration
 kirpatrick  gelatt    vecchi         these are general combinatorial optimization techniques that must be substantially reworked to fit a specific problem type  most are expected
to be applied throughout problem solving 
the result of pruning a covering rule set  rso   is a series of progressively smaller rule
sets frso       rsi       rsn g  the objective is to pick the best one  usually by some form
of error estimation  model complexity and future performance are highly related  both
too complex or too simple a model can yield poor results  the objective being to find just
the right size model  independent test cases or resampling by cross validation are effective
for estimating future performance  in the absence of these estimates  approximations  such
as gcv  craven   wahba        friedman         as described in equation    have been
used in the statistics literature to estimate performance   both measures of training error
and model complexity are used in the estimates  c m   is a measure of model complexity
expressed in terms of parameters estimated  such as the number of weights in a neural net 
or tests performed  where c m  is assumed to be less than n  the number of cases 
   gcv is an acronym for generalized cross validation  but only the apparent error on training cases is used
and not true cross validation by resampling 

   

firule based functional prediction

xn
gcv  m    

jy  y j
 

i

i

n
c m 
i       n

   

in our experiments we used cross validated estimates to guide the final model selection
process  but other measures such as gcv may also be used 

    potential problems with rule based regression

regression rules  like trees  are induced by recursive partitioning methods that approximate a function with constant value regions  they are relatively strong in dynamic feature
selection in high dimensional applications  sometimes using only a few highly predictive
features  an essential weakness of these methods is the approximation of a partition or
region by a constant value  for a continuous function and even a moderately sized sample 
this approximation can lead to increased error 
to deal with this limitation  instead of constant value functions  linear functions can
be substituted in a partition  quinlan         however  a linear function has the obvious
weakness that the true function may be far from linear even in the restricted context of
a single region  in general  use of such linearity compromises the highly non parametric
nature of the dnf model  a better strategy might be to examine alternative non linear
methods 

   an alternative to rules  k nearest neighbors
the k nearest neighbor method is one of the simplest regression methods  relying on table
lookup  to classify an unknown case x  the k cases that are closest to the new case are
found in a sample data base of stored cases  the predicted y x  of equation   is the mean
of the y values for the k nearest neighbors  the nearest neighbors are found by a distance
metric such as euclidean distance  usually with some feature normalization   the method
is non parametric and highly non linear in nature

yknn x    k 

xk yk for k nearest neighbours of x

k  

   

a major problem with this approach is how to limit the effect of irrelevant features 
while limited forms of feature selection are sometimes employed in a preprocessing stage 
the method itself cannot determine which features should be weighted more than others  as
a result  the procedure is very sensitive to the distance measure used  in a high dimensional
feature space  k nearest neighbor methods may perform very poorly  these limitations are
precisely those that the partitioning methods address  thus  in theory  the two methods
potentially complement one another 

   model combination

in practice  one learning model is not always superior to others  and a learning strategy
that examines the results of different models may do better  moreover  by combining
   

fiweiss   indurkhya
different models  enhanced results may be achieved  a general approach to combining
learning models is a scheme referred to as stacking  wolpert         additional studies have
been performed in applying the scheme to regression problems  breiman        leblanc  
tibshirani         using small training samples of simulated data  and linear combinations
of regression methods  improved results were reported  let mi be the i th model trained
on the same sample  and wi   the weight to be given to mi    if the new case vector is
x  the predictions of different models can be combined as in equation   to produce an
estimate of y  the models may use the same representation  such as k nearest neighbors with
variable size k  or perhaps variable size decision trees  the models could also be completely
different  such as combining decision trees with linear regression models  different models
are applied independently to find solutions  and later a weighted vote is taken to reach a
combined solution  this method of model combination is in contrast to the usual approach
to evaluation of different models  where the single best performing model is selected 

y 

xk wkmk x 

k  

   

while stacking has been shown to give improved results on simulated data  a major
drawback is that properties of the combined models are not retained  thus when interpretable models are combined  the result may not be interpretable at all  it is also not
possible to compensate for weaknesses in one model by introducing another model in a
controlled fashion 
as suggested earlier  partitioning regression methods and k nearest neighbor regression
methods are complementary  hence one might expect that by suitably combining the two
methods  one might obtain better performance  in one recent study  quinlan         model
trees  i e   regression trees with linear combinations at the leaf nodes  and nearest neighbor
methods were also combined  the combination method is described in equation    where
the n  x k is one of the k nearest neighbors of x  v x  is the y value of the stored instance
x  and t x  is the result of applying a model tree to x 

y   k 

xk v  n  x k     t  n  x k    t  x  

k  

   

the k nearest neighbors are found independently of the induced regression tree  results
were reported with k     in that sense  the approach is similar to the combination method
of equation    the k nearest neighbors are passed down the tree  and the results are used
to refine the nearest neighbor answer  thus  we have a combination model formed by
independently computing a global solution  and later combining results 
however  there are strong reasons for not determining the global nearest neighbor solution independently  while  at the limit  with large samples  the non parametric k nearest
neighbor methods will correctly fit the function  in practice though  their weaknesses can
be substantial  finding an effective global distance measure may not be easy  particularly
in the presence of many noisy features  hence a different technique for combining the two
methods is needed 
   these weights are obtained so as to minimize the least squared error under some constraints  breiman 
      

   

firule based functional prediction

    integrating rules with table lookup

consider the following strategy  to determine y value of a case x that falls in region ri  
instead of assigning a single constant value ki for region ri   where ki is determined by the
i  x   the mean of the k nearest
median y value of training cases in the region  assign yknn
 training set  instances of x in region ri   thus for regression trees  we now have equation
    for regression rules  we also have equation    
i  x 
if x  ri then f  x    yknn

    

i  x 
if i   j and x  both ri and rj then f  x    yknn

    

an interesting aspect of this strategy is that k nearest neighbor results need only be
considered for the cases covered by a particular partition  while this increases the interaction between the models and eliminates the independent computation of the two models 
the model rationale and  as we shall show  the empirical results  are supportive of this
approach 
we now have a representation which potentially alleviates the weakness of partitions
being assigned single constant values  moreover  some of the global distance measure difficulties of the k nn methods may also be relieved because the table lookup is reduced to
partitioned and related groupings 
this is the rationale for a hybrid partition and k nn scheme  note that unlike stacking 
our hybrid models are not independently determined  but interact very strongly with one
another  however  it must be demonstrated that these methods are in fact complementary 
preserving the strengths of the partitioning schemes while compensating for the weaknesses
that would be introduced if constant values were used for each region  with respect to model
combination  two principal questions need to be addressed by empirical experimentation 

 are results improved relative to using each model alone 
 are these methods competitive with alternative regression methods 

   results
experiments were conducted to assess the competitiveness of rule based regression compared
to other procedures  including less interpretable ones   as well as to evaluate the performance of the integrated partition and k nn regression method  experiments were performed
using seven datasets  six of which are described in previous studies  quinlan         in addition to these six datasets  new experiments were done on a very large telecommunications
application  which is labeled pole  in each of the seven datasets  there was one continuous
real valued response variable  experimental results are reported in terms of the mad  as
measured using    fold cross validation  for pole        cases were used for training and
       for independent testing  the features from the different datasets were a mixture of
continuous and categorical features  for pole  all    features were continuous  descriptions
   

fiweiss   indurkhya

dataset cases vars
price
servo
cpu
mpg
peptide
housing
pole

   
   
   
   
   
   
     

  
  
 
  
   
  
  

table    dataset characteristics
of the other datasets can be found in the literature  quinlan          table   summarizes
the key characteristics of the datasets used in this study 
table   summarizes the original results reported  quinlan         these include modeltrees  mt   which are regression trees with linear fits at the terminal nodes  neural nets
 nnet     nearest neighbors    nn   and the combined results of model trees and   nearest
neighbors  mt   nn   
table   summarizes the additional results that we obtained  these include the cart
regression tree  rt     nearest neighbors with euclidean distance    nn   rule regression
using swap    rule regression with   nn applied to the rule region  rule   nn   and mars 
  nn was used because the expectation is that the nearest neighbor method incrementally
improves a constant value region when the region has a moderately large sample of neighbors
to average 
for the rule based method  the parameter m  the number of pseudo classes  must be
determined  this can be found using cross validation or independent test cases  in our
experiments  cross validation was used   figure   represents a typical plot of the relative
error vs  the number of pseudo classes  weiss   indurkhya      b   as the number of
partitions increases  results improve until they reach a relative plateau and deteriorate
somewhat  similar complexity plots can be found for other models  for example neural nets
 weiss   kapouleas        
the mars procedure has several adjustable parameters   for the parameter mi  values
tried were    additive modeling           and number of inputs  for df  the default value of
    was tried as well the optimal value estimated by cross validation  the parameter nk was
varied from    to     in steps of     lastly  both piece wise linear as well as piece wise cubic
solutions were tried  for each of the above setting of the parameters  the cross validated
accuracy was monitored  and the value for the best mars model is reported 
for each method  besides the mad  the relative error is also reported  the relative
error is simply the estimated true mean absolute distance  measured by cross validation 
normalized by the initial mean absolute distance from the median  analogous to classifi   the peptide dataset is a slightly modified version of the one quinlan refers to as lhrh att in his paper 
in the version used in our experiments  cases with missing values were removed 
   because peptide was a slightly modified version of the lhrh att dataset  the result listed is one that was
provided by quinlan in a personal communication 
   the particular program used was mars     

   

firule based functional prediction

relative error
    

   

    

   

    

   

    

   
 

 

 

 
 
 
number of pseudo classes

 

 

  

figure    prototypical performance for varying pseudo classes

dataset mt nnet   nn mt   nn
price     
servo
   
cpu
    
mpg
    
peptide    
housing     

    
   
    
    
    

    
   
    
    
    

    
   
    
    
    

table    previous results
cation  where predictions must have fewer errors than simply predicting the largest class 
in regression too we must do better than the average distance from the median to have
meaningful results 
in comparing the performance of two methods for a dataset  the standard error for
each method was independently estimated  and the larger one was used in comparisons 
if the difference in performance was greater than   standard errors  the difference was
considered statistically significant  as with any significance test  one must also consider the
overall pattern of performance and the relative advantages of competing solutions  weiss
  indurkhya        
for each dataset  figure   plots the relative best error found by the ratio of the best
reported result to each model s result  a relative best error of   indicates that the result is
the best reported result for any regression model  the model results that are compared to
the best results are for regression rules    nn  and the mixed model  the graph indicates
   

fiweiss   indurkhya

dataset

rt

  nn

rule

rule   nn

mars

mad error mad error mad error mad error mad error
price
                                            
servo
                                            
cpu
                                               
mpg
                                            
peptide    
   
   
   
   
   
   
   
   
   
housing                                             
pole
                                            
table    performance of additional methods
relative best erate
   
  nn
rule
 

rule   nn

   

   

   

   

 
servo

house

mpg

cpu

price

peptide

pole

figure    relative best erates of   nn  rules  and rule   nn
trends across datasets and helps assess the overall pattern of performance  in this respect 
both rule and rule  nn exhibit excellent performance across many applications 
these empirical results allow us to consider several relevant questions regarding rulebased regression 
   how does rule based regression perform compared to tree based regression  comparing
the results for rule with rt  one can see that except for servo  rule does consistently
better than rt on all the remaining six datasets  the difference in performance also
   

firule based functional prediction
tests as significant  the results of the significance tests  and the general trend  which
can be seen visually in figure    leads us to conclude that rule based regression is
definitely competitive to trees and often yields superior performance 
   does integrating  nn with rules lead to improved performance relative to using each
model alone  a comparison of rule  nn with  nn shows that for all datasets  rule  nn
is significantly better  in comparing rule  nn with rule  the results indicate that for
three datasets  mpg  pole and housing   rule  nn was significantly better than rule 
and for the remaining three datasets both were about the same  the overall pattern
of performance also appears to favor rule  nn over rule  thus the empirical results
indicate that our method improved results relative to using each model alone  the
general trend can be seen in figure   
   are the new methods competitive with alternative regression methods  among the previous reported results  mt  nn is the best performer  other alternatives to consider
are  regression trees  rt  and mars  none of these three methods were significantly
better than rule  nn on any of the datasets under consideration except for rt doing
significantly better on servo  furthermore  rule  nn was significantly better than
mt  nn on three of five datasets  servo  cpu and mpg  on which comparison is possible  the overall trend also is in favor of rule  nn  comparing rt to rule  nn  we
find that except for servo  rule  nn is significantly better than rt on all the remaining datasets  comparing mars to rule  nn  we find that for three of the datasets
 price  peptide and pole   rule  nn is significantly better  hence the empirical results overwhelmingly suggest that our new method is competitive with alternative
regression methods  with hints of superiority over some methods 

   discussion

we have considered a new model for rule based regression and provided comparisons with
tree based regression  for many applications  strong explanatory capabilities and high dimensional feature selection can make a dnf model quite advantageous  this is particularly
true for knowledge based applications  for example equipment repair or medical diagnosis 
in contrast to pure pattern recognition applications such as speech recognition 
while rules are similar to trees  the rule representation is potentially more compact
because the rules are not mutually exclusive  this potential of finding a more compact
solution can be particularly important for problems where model interpretation is crucial 
note that the space of all rules includes the space of all trees  thus  if a tree solution is the
best  theoretically the rule induction procedure has the potential to find it 
in our experiments  the regression rules generally outperformed the regression trees 
fewer constant regions were required and the estimated error rates were generally lower 
finding the dnf regions was substantially more computationally expensive for the regression rules than the regression trees  for the regression rules  fairly complex optimization
techniques were necessary  in addition  experiments must be performed to find the appropriate number of pseudo classes  this is more a matter of scale  scale of the application
versus the scale of available computing  excluding the telecommunications application 
none of the cited applications takes more than    minutes of cpu time on a ss    for a sin   

fiweiss   indurkhya
gle pseudo classification problem and a full cross validation   as computing power increases
the timing distinction is less important  even a small percentage gain can be quite valuable for the appropriate application  apte  damerau    weiss        and computational
requirements are a secondary factor 
we have provided results on several real world datasets  mostly  these involve nonlinear relationships  one may wonder how the rule based method would perform on data
with obvious linear relationships  in our earlier experiments with data exhibiting linear
relationships  for example  the drug study data  efron          the rule based solutions did
slightly better than trees  however  the true test is real world data which  often involve
complex non linear relationships  comparisons with alternative models can help assess the
effectiveness of the new techniques 
looking at figure   and tables   and    we see that the pure rule based solutions are
competitive with other models  additional gains are made when rules are used not for
obtaining the function values directly  but instead used to find the relevant cases which are
then used to compute the function value  the results of these experiments support the
view that this strategy of combining different methods can improve predictive performance 
strategies similar to ours have been applied before for classification problems  ting       
widmer        and similar conclusions were drawn from those results  our results indicate
that the strategy is useful in the regression context too  our empirical results also support
the contention that for regression  partitioning methods and nearest neighbor methods are
complementary  a solution can be found by partitioning alone  and then the incremental
improvement can be observed when substituting the average y of the k nearest neighbors for
the median y of a partition  from the perspective of nearest neighbor regression methods 
the sample cases are compartmentalized  simplifying the table lookup for a new case 
while not conclusive  there are hints that our combination strategy is most effective for
small to moderate samples  it is likely that when the sample size grows large  increased
numbers of partitions  in terms of rules or terminal nodes  can compensate for having single
constant valued regions  this conjecture is supported by the large sample pole application 
where the incremental gain for the addition of k nn is small  
in our experiments we used k nn with k    depending on the application  a different
value of k might produce better results  the optimal value might be estimated by crossvalidation in a strategy that systematically varies k and picks the value that gives the best
results overall  however  it is unclear whether the increased computational effort will result
in any significant performance gain 
another practical issue with large samples is the storage requirement  all the cases must
be stored  this can be a serious drawback in real world applications with limited memory 
however  we tried experiments in which the cases associated with a partition are replaced
by a fewer number of  typical cases   this results in considerable savings in terms of storage
requirements  results are slightly weaker  though not significantly different  
it would appear that further gains might be obtained by restricting the k nn to consider
only those features that appear in the path to the leaf node under examination  this might
seem like a good idea because it attempts to ensure that only features that are relevant to
   a    fold cross validation requires solving a problem essentially    times  once on all training cases and
   times for each group of test cases 
   although small  this difference tests as significant because the sample is large 

   

firule based functional prediction
the cases in the node  are used in the distance calculations  however  we found results for
this to be weaker 
a number of regression techniques have been presented by others to demonstrate the
advantages of combined models  most of these combine methods that are independently
invoked  instead of a typical election where there is one winner  the alternative models
are combined and weighted  these combination techniques have the advantage that the
outputs of different models can be treated as independent variables  they can be combined
in a form of post processing  after all model outputs are available 
in no way do we contradict the value of these alternative combination techniques  both
approaches show improved results for various applications  we do conclude  however  that
there are advantages for more complex regression procedures that dynamically mix the
alternative models  these procedures may be particularly strong when there is a fundamental rationale for choice of methods such as partitioning methods  or when properties of
the combined models must be preserved 
we have presented the regression problem with one output variable  this is the classical form for linear models and regression trees  the issue of multiple outputs has not been
directly addressed although such extensions are feasible  this issue and further experimentation await future work  our model of regression can provide a basis for these efforts 
while leveraging current strong methods in classification rule induction 

references

apte  c   damerau  f     weiss  s          automated learning of decison rules for text
categorization  acm transactions on oce information systems                  
breiman  l          stacked regression  tech  rep   u  of ca  berkeley 
breiman  l   friedman  j   olshen  r     stone  c          classification and regression
tress  wadsworth  monterrey  ca 
clark  p     niblett  t          the cn  induction algorithm  machine learning    
        
craven  p     wahba  g          smoothing noisy data with spline functions  estimating
the correct degree of smoothing by the method of generalized cross validation  numer 
math               
efron  b          computer intensive methods in statistical regression  siam review 
                
fayyad  u     irani  k          the attribute selection problem in decision tree generation 
in proceedings of aaai     pp          san jose 
friedman  j          multivariate adaptive regression splines  annals of statistics         
      
friedman  j     stuetzle  w          projection pursuit regression  j  amer  stat  assoc  
            
   

fiweiss   indurkhya
girosi  f     poggio  t          networks and the best approximation property  biological
cybernetics              
hartigan  j     wong  m          a k means clustering algorithm  algorithm as     
applied statistics         
hastie  t     tibshirani  r          generalized additive models  chapman and hall 
jacoby  s   kowalik  j     pizzo  j          iterative methods for non linear optimization
problems  prentice hall  new jersey 
kirpatrick  s   gelatt  c     vecchi  m          optimization by simulated annealing 
science           
leblanc  m     tibshirani  r          combining estimates in regression and classification 
tech  rep   department of statistics  u  of toronto 
lebowitz  m          categorizing numeric information for generalization  cognitive science             
lin  s     kernighan  b          an ecient heuristic for the traveling salesman problem 
operations research                  
mcclelland  j     rumelhart  d          explorations in parallel distributed processing 
mit press  cambridge  ma 
michalski  r   mozetic  i   hong  j     lavrac  n          the multi purpose incremental learning system aq   and its testing application to three medical domains  in
proceedings of aaai     pp            philadelphia  pa 
quinlan  j          induction of decision trees  machine learning            
quinlan  j          simplifying decision trees  international journal of man machine
studies              
quinlan  j          combining instance based and model based learning  in international
conference on machine learning  pp          
ripley  b          statistical aspects of neural networks  in proceedings of seminair europeen de statistique london  chapman and hall 
scheffe  h          the analysis of variance  wiley  new york 
ting  k          the problem of small disjuncts  its remedy in decision trees  in proceedings
of the   th canadian conference on artificial intelligence  pp        
weiss  s     indurkhya  n       a   optimized rule induction  ieee expert               
weiss  s     indurkhya  n       b   rule based regression  in proceedings of the   th
international joint conference on artificial intelligence  pp            
   

firule based functional prediction
weiss  s     indurkhya  n          decision tree pruning  biased or optimal   in proceedings
of aaai     pp          
weiss  s     kapouleas  i          an empirical comparison of pattern recognition  neural
nets  and machine learning classification methods  in international joint conference
on artificial intelligence  pp          detroit  michigan 
weiss  s     kulikowski  c          computer systems that learn  classification and prediction methods from statistics  neural nets  machine learning  and expert systems 
morgan kaufmann 
widmer  g          combining knowledge based and instance based learning to exploit
qualitative knowledge  informatica              
wolpert  d          stacked generalization  neural networks             

   

fi