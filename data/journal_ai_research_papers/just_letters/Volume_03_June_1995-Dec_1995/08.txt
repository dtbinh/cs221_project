journal of artificial intelligence research                 

submitted       published      

flexibly instructable agents
scott b  huffman

price waterhouse technology centre     willow road
menlo park  ca       usa

john e  laird

artificial intelligence laboratory
the university of michigan       beal ave 
ann arbor  mi            usa

huffman tc pw com
laird eecs umich edu

abstract

this paper presents an approach to learning from situated  interactive tutorial instruction within an ongoing agent  tutorial instruction is a exible  and thus powerful 
paradigm for teaching tasks because it allows an instructor to communicate whatever types
of knowledge an agent might need in whatever situations might arise  to support this exibility  however  the agent must be able to learn multiple kinds of knowledge from a broad
range of instructional interactions  our approach  called situated explanation  achieves
such learning through a combination of analytic and inductive techniques  it combines a
form of explanation based learning that is situated for each instruction with a full suite of
contextually guided responses to incomplete explanations  the approach is implemented
in an agent called instructo soar that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions  instructo soar meets
three key requirements of exible instructability that distinguish it from previous systems 
    it can take known or unknown commands at any instruction point      it can handle
instructions that apply to either its current situation or to a hypothetical situation specified in language  as in  for instance  conditional instructions   and     it can learn  from
instructions  each class of knowledge it uses to perform tasks 

   introduction
the intelligent  autonomous agents of the future will be called upon to perform a wide
and varying range of tasks  under a wide range of circumstances  over the course of their
lifetimes  performing these tasks requires knowledge  if the number of possible tasks and
circumstances is large and variable over time  as it will be for a general agent   it becomes
nearly impossible to preprogram all of the knowledge required  thus  knowledge must
be added during the agent s lifetime  unfortunately  such knowledge cannot be added to
current intelligent systems while they perform  they must be shut down and programmed
for each new task 
this work examines an alternative  intelligent agents that can be taught to perform tasks
through tutorial instruction  as a part of their ongoing performance  tutorial instruction
is a highly interactive dialogue that focuses on the specific task s  being performed  while
working on tasks  a student may receive instruction as needed to complete tasks or to
understand aspects of the domain or of previous instructions  this situated  interactive
form of instruction produces very strong human learning  bloom         although it has

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fihuffman   laird

received little attention in ai  it has the potential to be a powerful knowledge source for
artificial agents as well 
much of tutorial instruction s power comes from its communicative exibility  the instructor can communicate whatever type of knowledge a student may need in whatever
situation it is needed  the challenge in designing a tutorable agent is to support the
breadth of interaction and learning abilities required by this exible communication 
in this paper  we present a theory of learning from tutorial instruction within an ongoing
agent  in developing the theory  we have given special attention to supporting communicative exibility for the instructor  the human user   we began by identifying the properties
of tutorial instruction from the instructor s perspective  from these properties  we have
derived a set of requirements that an instructable agent must meet to support exible instructability  these requirements drove the development of the theory and its evaluation 
finally  we have implemented the theory in an instructable agent called instructo soar
 huffman        huffman   laird               and evaluated its performance  
identifying requirements for exible instructability provides a target   a set of evaluation
criteria   for instructable agents  the requirements encompass the ways an agent interacts
with its instructor  comprehends instructions  and learns from them  the most general
requirements are common to all interactive learning systems  e g   the agent is expected
to learn general knowledge from instructions  to learn quickly  with a minimal number of
examples   to integrate what is learned with its previous knowledge  etc  other requirements
are specific to tutorial instruction 
our theory of learning from tutorial instruction specifies how analytic and inductive
learning techniques can be combined within an agent to meet the requirements  producing
general learning from a wide range of instructional interactions  we present a learning
framework called situated explanation that utilizes the situation an instruction applies to
and the larger instructional context  the instruction s type and place in the current dialogue 
to guide the learning process  situated explanation combines a form of explanation based
learning  dejong   mooney        mitchell  keller    kedar cabelli        that is situated for each individual instruction  with a full suite of contextually guided responses to
incomplete explanations  these responses include delaying explanation until more information is available  inducing knowledge to complete explanations  completing explanations
through further instruction  or abandoning explanation in favor of weaker learning methods 
previous explanation based learning systems have employed one or in some cases a static
sequence of these options  but have not chosen dynamically among all the options based on
the context of each example  such dynamic selection is required for exible instructability  the learning framework is cast within a computational model for general intelligent
behavior called the problem space computational model 
instructo soar is an implemented agent that embodies the theory  from interactive natural language instructions  instructo soar learns to perform new tasks  extends
known tasks to apply in new situations  and acquires a variety of other types of domain
knowledge  it allows more exible instruction than previous instructable systems  e g  
learning apprentice systems  mitchell  mahadevan    steinberg        by meeting three
   because our work is inspired by human students  we have also taken cues from psychological effects
where appropriate  the theory s potential as a cognitive model is discussed elsewhere  huffman       
huffman  miller    laird        

   

fiflexibly instructable agents

push the green button 
thats a new one  how do i do that 
move to the grey table 
ok  what next 
move above the green button 
how do i do that 
move the arm up 
oh  i see  what next 
move down 
ok  what next 
the operator is finished 

to turn on the light  push the button 
does this mean pushing the button
causes the light to come on 
yes 
   
never grasp red blocks 
why not 
red blocks are explosive 
if the light is off then turn it on 
   

figure    an example of tutorial instruction 
key requirements of tutorial instruction      it can take known or unknown commands at
any instruction point      it can handle instructions that apply to either its current situation or to a hypothetical situation specified in language  as in  for instance  conditional
instructions   and     it can learn  from instructions  each class of knowledge it uses to
perform tasks 
in what follows  we first discuss the properties and requirements of tutorial instruction 
then  we present our approach and its implementation in instructo soar  including a
series of examples illustrating the instructional capabilities that are supported  we conclude
with a discussion of limitations and areas for further research 

   properties of tutorial instruction
tutorial instruction is situated  interactive instruction given to an agent as it attempts to
perform tasks  it is situated in that it applies to particular task situations that arise in the
domain  it is interactive in that the agent may request instruction as needed  this type of
instruction is common in task oriented dialogues between experts and apprentices  grosz 
       an example of tutorial instruction given to instructo soar in a robotic domain
is shown in figure   
tutorial instruction has a number of properties that make it exible and easy for the
instructor to produce 
p   situation specificity  instructions are given for particular tasks in particular situations  to teach a task  the instructor need only provide suggestions for the specific
situation at hand  rather than producing a global procedure that includes general conditions for applicability of each step  that handles all possible contingencies  etc  the
situation can also help to disambiguate an otherwise ambiguous instruction  a number of authors have discussed the advantages of situation specific knowledge elicitation
 e g   davis        gruber        
p   situation specification as needed  although instructions typically apply to the
situation at hand  the instructor is free to specify other situations as needed  for
instance  specifying contingencies using conditional instructions 
p   incremental as needed elicitation  knowledge is elicited incrementally as a part
of the agent s ongoing performance  instructions are given when the agent is unable
   

fihuffman   laird

to perform a task  thus  they directly address points where the agent s knowledge is
lacking 
p   task structuring by instructor  the instructor can structure larger tasks into
smaller subtasks in any way desired  for instance  a task requiring ten primitive steps
may be taught as a simple sequence of the ten steps  or as two subtasks of five steps
each  etc  if the agent does not know what an instructed action or subtask is  or how
to perform it in the situation at hand  it will ask for further instruction 
p   knowledge level interaction  the instructor provides knowledge to the agent at
the knowledge level  newell         that is  the instructions refer to objects and
actions in the world  not to symbol level structures  e g   data structures  within the
agent  the interaction occurs in natural language  the language that the instructor
uses to talk about the task  rather than requiring artificial terminology and syntax to
specify the agent s internal data and processes 
tutorial instructions provide knowledge applicable to the agent s current situation or
a closely related one  thus  this type of instruction is most appropriate for tasks with
a local control structure  in which control decisions are made based on presently available
information  local control structure is characteristic of constructive synthesis tasks  in
which primitive steps are composed one after another to form a complete solution  our
work focuses on this type of task  

   requirements on an instructable agent
although easing the instructor s burden in providing knowledge  the properties of tutorial
instruction described above place severe requirements on an instructable agent  in general 
such an agent must solve three conceptually distinct problems  it must     comprehend individual instructions to produce behavior      support a exible dialogue with its instructor 
and     produce general learning from the interaction  the properties of tutorial instruction described in the previous section place requirements on the solutions to each of these
problems  in what follows  we identify key requirements for each problem in turn 

    comprehending instructions  the mapping problem

the mapping problem involves comprehending instructions that are given in natural language and transforming the information they contain into the agent s internal representation
   in contrast  problem solving methods like constraint satisfaction and heuristic classification involve global
control strategies  these strategies either follow a fixed global regime or require an aggregation of
information from multiple problem solving states to make control decisions  it is possible to produce a
global control strategy using a combination of local decisions  yost   newell         however  teaching
a global method by casting it purely as a sequence of local decisions may be dicult  other types of
instruction  beyond the scope of this work  are required to teach global methods in a natural way  to
acquire knowledge for tasks that involve a known global control strategy  it may be most ecient to use a
method based knowledge acquisition tool  e g   birmingham   klinker        birmingham   siewiorek 
      eshelman  ehret  mcdermott    tan        marcus   mcdermott        musen        with that
control strategy built in 

   

fiflexibly instructable agents

language  this is required for the agent to apply information communicated by instructions
at the knowledge level  property p   above  to its internal processing 
solving the mapping problem in general involves all of the complexities of natural language comprehension  as just and carpenter        point out  instructions can be linguistically complex and dicult to interpret independent of the diculty of the task being
instructed  even in linguistically simple instructions  actions and objects are often incompletely specified  requiring the use of context and domain knowledge to produce a complete
interpretation  chapman        dieugenio   webber        frederking        martin  
firby        
the general requirement for the mapping problem on a tutorable agent is straightforward 

m    a tutorable agent must be able to comprehend and map all aspects of each instruction
that fall within the scope of information it can possibly represent 

the agent cannot be expected to interpret aspects that fall outside its representation abilities  these abilities may be augmented through instruction  but this occurs by building up
from existing abilities   a more detailed analysis could break this general requirement into
a set of more specific ones 
this work has not focused on the mapping problem  rather  the agent we have implemented uses fairly standard natural language processing techniques to handle instructions
that express a sucient range of actions and situations to demonstrate its other capabilities 
we have concentrated our efforts on the interaction and transfer problems 

    supporting interactive dialogue  the interaction problem
the interaction problem is the problem of supporting exible dialogue with an instructor 
the properties of tutorial instruction indicate that this dialogue occurs during the agent s
ongoing performance to address its lacks of knowledge  property p    within the dialogue 
the agent must handle instructions that apply to different kinds of situations  properties
p  and p   and that structure tasks in different ways  property p   
an instructable agent moves toward solving the interaction problem to the degree that
it supports these properties  in this work  we concentrate on the instructor s utterances
within the dialogue  since exibility for the instructor is the goal  we have not considered
the potential complexity of the agent s utterances  e g   to give the instructor various kinds
of feedback  in much detail 
the properties of exible interaction can be specified in terms of individual instruction
events  where an instruction event is the utterance of a single instruction at a particular
point in the discourse  to support truly exible dialogue  an instructable agent must be
able to handle any instruction event that is coherent at the current discourse point  each
instruction event is initiated by either the student or the teacher  and carries knowledge
of some type to be applied to a particular task situation  thus  a exible tutorable agent
should support instruction events with 

i   flexible initiation  instruction events can be initiated by agent or instructor 
   

fihuffman   laird

i   flexibility of knowledge content  the knowledge carried by an instruction event
can be any piece of any of the types of knowledge the agent uses that is applicable in
some way within the ongoing task and discourse context 

i   situation exibility  an instruction event can apply either to the current task
situation or to some specified hypothetical situation 

the following sections discuss each of these requirements in more detail 
      flexible initiation

in human tutorial dialogues  initiation of instruction is mixed between student and teacher 
one study indicates that teacher initiation is more prevalent early in instruction  student
initiation increases as the student learns more  and then drops off again as the student
masters the task  emihovich   miller        
instructor initiated instruction is dicult to support because instruction events can
interrupt the agent s ongoing processing  upon interrupting the agent  an instruction event
may alter the agent s knowledge in a way that could change or invalidate the reasoning in
which the agent was previously engaged  because of these diculties  instructable systems
to date have not fully supported instructor initiated instruction   likewise  instructosoar does not handle instructor initiated instruction 
agent initiated instruction can be directed in  at least  two possible ways  by verification
or by impasses  some learning apprentice systems  such as leap  mitchell et al         and
disciple  kodratoff   tecuci      b  ask the instructor to verify or alter each reasoning
step  the advantage of this approach is that each step is examined by the instructor  the
disadvantage  of course  is that each step must be examined  an alternative approach is
to drive instruction requests by impasses in the agent s task performance  golding  rosenbloom    laird        laird  hucka  yager    tuck         this is the approach used by
instructo soar  an impasse indicates that the agent s knowledge is lacking and it needs
instruction  the advantage of this approach is that as the agent learns  it becomes more
autonomous  its need for instruction decreases over time  the disadvantage is that not all
lacks of knowledge can be recognized by reaching impasses  e g   no impasse will occur when
performance is correct but inecient 
      flexibility of knowledge content

a exible tutorable agent must handle instruction events involving any knowledge that is
applicable in some way within the ongoing task and discourse context  this requirement is
dicult to meet in general  because of the wide range of knowledge that may be relevant to
any particular situation  it requires a robust ability to relate each utterance to the ongoing
discourse and task situation  no instructable systems have met this requirement fully 
however  we can define a more constrained form of this requirement  limited to instructions that command actions  i e   imperatives   imperative commands are especially
prevalent in tutorial instruction of procedures  supporting exible knowledge content for
   some systems have learned purely by observing an expert  e g   dent  boticario  mcdermott  mitchell  
zabowski        redmond         observation is a type of instructor initiatedness  but the instruction
is not an interactive dialogue 

   

fiflexibly instructable agents

commands means allowing the instructor to give any relevant command at any point in the
dialogue for teaching a task  we call this ability command exibility 
for any command that is given  there are three possibilities      the commanded action
is known  and the agent performs it      the commanded action is known  but the agent
does not know how to perform it in the current situation  extra  unknown steps are needed  
or     the commanded action is unknown  thus  command exibility allows the instructor
teaching a procedure to skip steps     or to command a subtask that is unknown     at
any point  in such cases  the agent asks for further instruction  the interaction pattern
that results  in which procedures are commanded and then taught as needed  has been
observed in human instruction  wertsch        notes that     adults spontaneously follow a
communication strategy in which they use directives that children do not understand and
then guide the children through the behaviors necessary to carry out these directives  
command exibility gives the instructor great exibility in teaching a set of tasks because the instructions can hierarchically structure the tasks in whatever way the instructor
wishes  a mathematical analysis  huffman        revealed that the number of possible
sequences of instructions that can be used to teach a given procedure grows exponentially
with the number of actions in the procedure  for a procedure with   primitive actions 
there are over     possible instruction sequences  for    there are over     
      situation flexibility

a exible tutorable agent must handle instructions that apply to either the current task
situation or some hypothetical situation that the instructor specifies  instructors make
frequent use of both of these options  for instance  analysis of a protocol of a student being
taught to use a ight simulator revealed that     out of     instructions       involved
hypothetical situations  with the remainder applying to the current situation at the time
they were given 
instructions that apply to the current situation  such as imperative commands  e g  
 move to the yellow table    are called implicitly situated  huffman   laird         since
the instruction itself says nothing about the situation to which it should be applied  the
current situation  the task being performed and the current state  is implied 
in contrast  instructions that specify elements of the situation to which they are meant
to apply are explicitly situated  huffman   laird         the agent is not meant to carry
out these instructions immediately  as an implicitly situated instruction   but rather when a
situation arises that is like the one specified  examples include conditionals and instructions
with purpose clauses  dieugenio         such as the following  


when using chocolate chips  add them to coconut mixture just before pressing into
pie pan 



to restart this  you can hit r or shift r 



when you get to the interval that you want  you just center up the joystick again 

   these examples are taken from a protocol of tutorial instruction and a written source of instruction  a
cookbook  

   

fihuffman   laird

as a number of researchers have pointed out  ford   thompson        haiman       
johnson laird         conditional clauses introduce a shared reference between speaker and
hearer that forms an explicit background for interpreting or evaluating the consequent  
here  the clauses in italics indicate a hypothetical situation to which the command in the
remainder of the instruction is meant to apply  in most cases  the situation is only partially
specified  with the remainder drawn from the current situation  as in  when using chocolate
chips  and cooking this recipe  and at the current point in the process     
in general  a hypothetical situation may be created and referred to across multiple
utterances  the agent presented here handles both implicitly and single explicitly situated
instructions  but does not deal with hypothetical situations that exist through multiple
instructions 

    producing general learning  the transfer problem

the transfer problem is the problem of learning generally applicable knowledge from instructions  that will transfer to appropriate situations in the future  this general learning
is based on instructions that apply to specific situations  property p   above   many types
of knowledge may be learned  since instructions can provide any type of knowledge that the
agent is lacking  property p   
solving this problem involves more than simply memorizing instructions for future use 
rather  conditions for applying each instruction must be determined from the situation 
consider  for example  the following exchange between instructor and agent 
block open our oce door 
how do i do that 
pick up a red block 
now  drop it here  next to the door 
what are the proper conditions for performing the  pick up  action  simple memorization yields poor learning  e g   whenever blocking open an office door  pick up a
red block  however  the block s color  and even the fact that it is a block  are irrelevant in this case  rather  the fact that the block weighs  say  more than five pounds 
giving it enough friction with the oor to hold open the door  is crucial  thus  the proper
learning might be 
if trying to block open a door  and
there is an object obj that is can be picked up  and
obj weighs more than   pounds
then propose picking up obj 
here  the original instruction is both generalized  color red and isa block drop out  and
specialized  weight     is added  
the transfer problem places a number of demands on a tutorable agent 
t   general learning from specific cases  the agent is instructed in a particular
situation  but is expected to learn general knowledge that will apply in suciently
similar situations 
   some types of conditionals do not follow this pattern  akatsuka         but they are not relevant to
tutorial instruction 

   

fiflexibly instructable agents

t   fast learning  an instructable agent is expected to learn new procedures quickly 
t  

t  
t  

t  

typically  a task should only have to be taught once 
maximal use of prior knowledge  an agent must apply its prior knowledge in
learning from instruction  this is a maxim for machine learning systems in general  if
you have knowledge  use it   and is particularly relevant for learning from instruction
because learning is expected to happen quickly 
incremental learning  the agent must be able to continually increase in knowledge
through instruction  new knowledge must be smoothly integrated with the agent s
existing knowledge as a part of its ongoing performance 
knowledge type exibility  since any type of knowledge  e g   control knowledge 
causal knowledge  etc   might be communicated by instructions  a exible tutorable
agent must be able to learn each type of knowledge it uses  we make this a testable
criterion below by laying out the types of knowledge in an agent based on a particular
computational model 
dealing with incorrect knowledge  the agent s knowledge is clearly incomplete
 otherwise  it would not need instruction   it may also be incorrect  a general tutorable agent must be able to perform and learn effectively despite incorrect knowledge 

t   learning from instruction coexisting with learning from other sources 

in addition to instruction  a complete agent should be able to learn from other
sources of knowledge that are available  these might include learning from observation demonstrations  experimentation in the environment  analogy  etc 
the theory of learning from tutorial instruction presented here focuses on extending
incomplete knowledge through instruction   requirements t  through t  of this list  handling incorrect knowledge  t   and learning from other sources  t   are planned extensions
in progress 
table   summarizes the requirements that must be met by an instructable agent to support exible tutorial instruction  and indicates the requirements targeted by instructosoar  we have made two simplifications in using the requirements to evaluate instructosoar  first  we treat each requirement as binary  that is  as if either completely met or
unmet  in reality  some requirements could be broken into finer grained pieces to be evaluated separately  second  we treat each requirement independently  the table indicates
instructo soar s performance on each requirement alone  but does not account for potential interactions between them  these interactions can be complex  for instance  in
pursuing fast learning  t    an agent may sacrifice good general learning  t   because it
bases its generalizations on too few examples  we have not addressed such tradeoffs in our
evaluation of instructo soar 

   related work

although there has not been extensive research on agents that learn from tutorial instruction per se  learning from instruction like input has been a long time goal in ai  carbonell 
   

fihuffman   laird

problem requirement
mapping m  mapping of all representable
information
interaction i  flexible initiation of instruction
i  flexibility of instructional knowledge
content
i  situation exibility
 implicitly situated
 explicitly situated single utterance
 explicitly situated multiple utterance
transfer t  general learning from specific cases

t 
t 
t 
t 
t 
t 

instructo soar 

no

 as needed to show
other capabilities 
no
 only agent initiated 
partial  command exibility 

partial
yes
yes
no
yes
 via situated explanation 
fast learning
yes
 new procedures only
taught once 
maximal use of prior knowledge
yes
incremental learning
yes
knowledge type exibility
yes
 learns all pscm
knowledge types 
ability to deal with incorrect knowledge no
 only extending incomplete knowledge 
learning from instruction coexisting no
 not demonstrated 
with learning from other sources

table    the requirements on a exible tutorable agent  and instructo soar s performance on them 
michalski    mitchell        mccarthy        rychener         early non interactive systems learned declarative  ontological knowledge from language  haas   hendrix        lindsay         simple tasks from unsituated descriptions  lewis  newell    polk        simon 
      simon   hayes         and task heuristics from non operational advice  hayes roth 
klahr    mostow        mostow        
other work has concentrated on behaving based on interactive natural language instructions  shrdlu  winograd        performed natural language commands and did a small
amount of rote learning   e g   learning new goal specifications by directly transforming
sentences into state descriptions  more recent systems that act in response to language
 concentrating on the mapping problem  but do only minimal learning include sonja
 chapman         animnl  dieugenio   webber         and homer  vere   bickmore 
      
some recent work has focused more on learning from situated natural language instructions  martin and firby        discuss an approach to interpreting and learning from
elliptical instructions  e g    use the shovel   by matching the instruction to expectations
generated from a task execution failure  alterman et al  s flobn  alterman  zito wolf   
carpenter        carpenter   alterman        searches for instructions in its environment
   

fiflexibly instructable agents

in order to operate devices  flobn learns by relating a device s instructions to known
procedures for operating similar devices  these systems do not target learning from exible
interactive instructions or types of instructions other than imperatives  however 
the bulk of work on learning from instruction like input has been under the rubric
of learning apprentice systems  lass   and closely related programming by demonstration
 pbd  systems  cypher          as employed  for instance  in recent work on learning
within software agents  dent et al         maes        maes   kozierok        mitchell 
caruana  freitag  mcdermott    zabowski         these systems learn by interacting with
an expert  either observing the expert solving problems  cypher        donoho   wilkins 
      mitchell et al         redmond        segre        vanlehn        wilkins        
or attempting to solve problems and allowing the expert to guide and critique decisions
that are made  golding et al         gruber        kodratoff   tecuci      b  laird
et al         porter  bareiss    holte        porter   kibler         each las has learned
particular types of knowledge  e g   operator implementations  mitchell et al          goal
decomposition rules  kodratoff   tecuci      b   operational versions of functional goals
 segre         control knowledge and control features  gruber         procedure schemas  a
combination of goal decomposition and control knowledge   vanlehn         useful macrooperations  cypher         heuristic classification knowledge  porter et al         wilkins 
       etc 
tutorial instruction is a more exible type of instruction than that supported by past
lass  for three reasons  first  the instructor may command unknown tasks or tasks with
unachieved preconditions to the agent at any instruction point  command exibility   past
lass limit input to particular commands observations at particular times  e g   only commanding or observing directly executable actions  and typically do not allow unknown
commands at all  second  tutorial instruction allows the use of explicitly situated instructions  situation exibility   to teach about contingencies that may not be present in the
current situation  past lass do not  third  tutorial instruction requires that all types of
task knowledge can be learned  knowledge type exibility   past lass learn only a subset
of the types of knowledge they use to perform tasks 

   a theory of learning from tutorial instruction

our theory of learning from tutorial instruction consists of a learning framework  situated
explanation  placed within a computational model for general agenthood  the problem space
computational model  we first describe the computational model and then the learning
framework 

    the problem space computational model

a computational model  cm  is a  a set of operations on entities that can be interpreted
in computational terms   newell et al         p      a computational model for a general
instructable agent must meet two requirements 
   support of general computation agenthood 
   close correspondence to the knowledge level  because tutorial instructions
provide knowledge at the knowledge level  newell         the further the cm com   

fihuffman   laird

ponents are from the knowledge level  the more dicult mapping and learning from
instructions will be  in addition  a close correspondence to the knowledge level will
allow us to use the cm to identify the types of knowledge the agent uses 
many potential cms are ruled out by these requirements  standard programming languages  e g   lisp  and theoretical cms like turing machines and push down automata
support general computation  but their operations and constructs are at the symbol level 
without direct correspondence to the knowledge level  similarly  connectionist and neural
network models of computation  e g   rumelhart   mcclelland        employ  by design 
computational operations and entities at a level far below the knowledge level  thus  these
models are not appropriate as the top level cm for an instructable agent  however  because higher levels of description of a computational system are implemented by lower levels
 newell         these cms might be used as the implementation substrate for the higher
level cm of an instructable agent 
another alternative is logic  which has entities that are well matched to the knowledge
level  e g   propositions  well formed formulas   logics specify the set of legal operations
 e g   modus ponens   thus defining the space of what can possibly be computed  however 
logic provides no notion of what should be computed  that is  logics alone do not specify the
control of the logical operations  application  it is desirable that the cm of an instructable
agent include control knowledge  because control knowledge is a crucial type of knowledge
for general agenthood  that can be communicated by instructions 
since one of our goals is to identify an agent s knowledge types  it might appear that
selecting a theory of knowledge representation would be more appropriate than selecting
a computational model  such theories define the functions and structures used to represent knowledge  e g   kl one  brachman         some also define the possible content of
those structures  e g   conceptual dependency theory  schank        cyc  guha   lenat 
       however  computational structure must be added to these theories to produce working agents  thus  rather than an alternative to specifying a computational model  a theory
of knowledge representation is an addition  a content theory of knowledge representation would provide a more fine grained breakdown of the knowledge to be learned by an
instructable agent within each category of knowledge specified by its cm  we have not
employed a particular content theory in this work thus far  however 
the computational model adopted here is called the problem space computational model
 pscm   newell et al         yost         the pscm is a general formulation of computation in a knowledge level agent  and many applications have been built within it  rosenbloom  laird    newell      a   it specifies an agent in terms of computation within
problem spaces  without reference to the symbol level structures used for implementation 
because its components approximate the knowledge level  newell et al          the pscm
is an apt choice for identifying an agent s knowledge types  soar  laird  newell    rosenbloom        is a symbol level implementation of the pscm 
a schematic of a pscm agent is shown in figure    perception and motor modules
connect the agent to the external environment  a pscm agent reaches a goal by moving
through a sequence of states in a problem space  it progresses toward its goals by sequentially
applying operators to the current state  operators transform the state  and may produce
motor commands  in pscm  operators can be more powerful than simple strips operators
 fikes  hart    nilsson         because they can perform arbitrary computation  e g   they
   

fiflexibly instructable agents

external environment

perceptual modules

motor modules

figure    the processing of a pscm based agent  triangles represent problem spaces 
squares  states  arrows  operators  and ovals  impasses 
can include conditional effects  multiple substeps  reactivity to different situations  etc   
the pscm agent reaches an impasse when its immediately available knowledge is not
sucient either to select or fully apply an operator  when this occurs  another problem
space context   a subgoal   is created  with the goal of resolving the impasse  this second
context may impasse as well  causing a third context to arise  and so on 
the only computational entities in the pscm mediated by the agent s knowledge are
states and operators  there are a small set of basic pscm level operations on these entities
that the agent performs 
   state inference  simple monotonic inferences that are always to be applied can be
made without using a pscm operator  such inferences augment the agent s representation of the state it is in by inferring state properties based on other state properties
 including those delivered by perception   for instance  an agent might know that a
block is held if its gripper is closed and positioned directly above the block 
   operator selection  the agent must select an operator to apply  given the current
state  this process involves two types of knowledge 
     proposal knowledge  indicates operators deemed appropriate for the current situation  this knowledge is similar to  precondition  knowledge in simple strips
operators 
     control knowledge  orders proposed operators  e g    a is better than b    c is
best    d is rejected  
   

fihuffman   laird

   operator application  once selected  an operator may be applied directly  or
indirectly via a subgoal 
     operator effects  the operator is applied directly in the current problem space 
the agent has knowledge of the effects of the operator on the state and motor
commands produced  if any  
     sub operator selection  the operator is applied by reaching an impasse and selecting operators in a subgoal  here  knowledge to apply the operator is selection
knowledge     above  for the sub operators 
   operator termination  an operator must be terminated when its application has
been completed  the termination conditions  or goal concept  mitchell et al         
of an operator indicate the state conditions that the operator is meant to achieve  for
example  the termination conditions of pick up blk  might be that blk is held and
the arm is raised  
each of these functions is performed by the agent using knowledge  thus  they define the set
of knowledge types present within a pscm agent  the knowledge types  five types total 
are summarized in table    because soar is an implementation of the pscm  all knowledge
within soar agents is of these types 
in soar s implementation of the pscm  learning occurs whenever results are returned
from a subgoal to resolve impasses  the learning process  called chunking  creates rules
 called chunks  that summarize the processing in the subgoal leading to the creation of the
result  depending on the type of result  chunks may correspond to any of the five types of
pscm knowledge  when similar situations arise in the future  chunks allow the impasse
that caused the original subgoal to be avoided by producing their results directly  chunking
is a form of explanation based learning  rosenbloom   laird         although it is a
summarization mechanism  through taking both inductive and deductive steps in subgoals 
chunking can produce both inductive and deductive learning  miller        rosenbloom
  aasman         chunking occurs continuously  making learning a part of the ongoing
activity of a soar pscm agent 
the pscm clarifies the task of an instructable agent  it must be able to learn each of
the five types of pscm knowledge from instruction  the next section discusses the learning
process itself 

    learning from instructions through situated explanation

learning from instruction involves both analytic learning  learning based on prior knowledge  and inductive learning  going beyond prior knowledge   analytic learning is needed
because the agent must learn from instructions that combine known elements   e g   learning to pick up objects by combining known steps to pick up a particular object  the agent s
prior knowledge of these elements can be used to produce better and faster learning  inductive learning is needed because the agent must learn new task goals and domain knowledge
   pscm operators have explicit termination knowledge because they can have a string of conditional
effects that take place over time  they can respond to  or wait for  the external environment  etc 
strips operators  in contrast  do not need explicit termination knowledge  because they are defined by
a single list of effects  and are  terminated  by definition after applying those effects 

   

fiflexibly instructable agents

entity knowledge type example
state
inference
if gripper is closed   directly above obj   holding obj 
operator proposal
if goal is to pick up obj on table x  and not docked at tablex  then propose moving to table x 
operator control
if goal is to pick up small metal obj on table x  prefer moving to table x over fetching magnet 
operator effects
an effect of the operator move to table x is that the robot
becomes docked at table x 
operator termination
termination conditions of pick up obj are that the gripper
is raised   holding obj 
table    the five types of knowledge of pscm agents 
that are beyond the scope of its prior knowledge  the goal of this research is not to produce
more powerful analytic or inductive techniques  but rather to specify how these techniques
come together to produce a variety of learning in the variety of instructional situations faced
by an instructable agent  the resulting approach is called situated explanation 
instruction requirements t  through t  specify that general learning  t   must occur
from single  specific examples  t    by making maximal use of prior knowledge  t    these
requirements bode strongly for a learning approach based on explanation  the use of
explanation to produce general learning has been a common theme in machine learning  e g  
dejong   mooney        fikes et al         minton  carbonell  knoblock  kuokka  etzioni 
  gil        rosenbloom  laird    newell        schank   leake        many others  and
cognitive science  anderson        chi  bassok  lewis  reimann    glaser        lewis 
      rosenbloom   newell         forming explanations enables general learning from
specific cases  requirement t   because the explanation indicates which features of a case
are important and which can be generalized  learning by explaining typically requires only
a single example  requirement t   because the prior knowledge employed to construct the
explanation  requirement t   provides a strong bias that allows this fast learning 
thus  we use an explanation based method as the core of our learning from instruction approach  and fall back on inductive methods when explanation fails  in standard
explanation based learning  explaining a reasoning step involves forming a  proof   using
prior knowledge  that the step leads from the current state of reasoning toward the current
goal  the proof is a path of reasoning from the current state to the goal  through the step
being explained  as diagrammed in figure    general learning is produced by forming a
rule that includes only the causally required features of the state  goal  and step appearing
in the proof  features that do not appear are generalized away 
figure   indicates the three key elements of an explanation  the step being explained 
the endpoints of the explanation  a state s and goal g to be reached   and the other steps
required to complete the explanation  what form do these elements of an explanation take
for situated explanation of an instruction 


step to be explained  in situated explanation  the step to be explained is an individual
instruction given to the agent 
   

fihuffman   laird

reasoning step to be
explained
 indicated by instruction i 

other steps  from agents knowledge

   

s

g

 mk  

figure    caricature of an explanation of how a reasoning step applies to a situation starting
in a state s   with a goal g to be achieved 





alternatively  an entire instruction episode   e g   the full sequence of instructions for
a new procedure   could be explained at once  applying explanation to single steps
results in knowledge applicable at each step  as in golding et al         laird et al  
       explaining full sequences of reasoning steps results in learning schemas that
encode the whole reasoning episode  as in mooney        schank   leake        vanlehn         learning factored pieces of knowledge rather than monolithic schemas
allows more reactive behavior  since knowledge is accessed locally based on the current situation  drummond        laird   rosenbloom         this meshes with the
pscm s local control structure  explaining individual instructions is also supported
by psychological results on the self explanation effect  which have shown that subjects
who self explain instructional examples do so by re deriving individual lines of the example   students virtually never reect on the overall solution and try to recognize
a plan that spans all the lines   vanlehn and jones        p       
endpoints of explanation  the endpoints of the explanation   a state s and a goal g
to be achieved   correspond to the situation that the instruction applies to  situation
exibility  requirement i    stipulates that this situation may be either the current
state of the world and goal being pursued or some hypothetical situation that is specified explicitly in the instruction  an instruction that does not specify any situational
features is implicitly situated  and applies to the agent s current situation  alternatively  an instruction can specify features of s or g  making for two kinds of explicitly
situated instructions  for example   if the light is on  push the button  indicates a
hypothetical state with a light on   to turn on the machine  ip the switch  indicates
a hypothetical goal of turning on the machine  a situation  s  g  is produced for
each instruction  based on the current task situation and any situation features the
instruction specifies 
other required steps  to complete an explanation of an instruction  an agent must
bring its prior knowledge to bear to complete the path through the instruction to
achievement of the situation goal  a pscm agent s knowledge applies to its current
situation to select and apply operators and to make inferences  when explaining an
instruction i   this knowledge is applied internally to the situation  s  g  associated
with i   that is  explanation takes the form of forward internal projection within
that situation  as depicted in figure    the agent  imagines  itself in state s   and
then runs forward  applying the instructed step and any knowledge that it has about
subsequent states operators  this knowledge includes both what is normally used
   

fiflexibly instructable agents

in the external world and knowledge of operators  expected effects that is used to
produce those effects in the projected world  if g is reached within the projection 
then the projected path from s   through the step instructed by i   to g comprises
an explanation of i   by indicating the features of i   s   and g causally required for
success  the explanation allows the agent to learn general knowledge from i  as in
standard ebl  realized in our agent by soar s chunking mechanism  rosenbloom  
laird         however  the agent s prior knowledge may be insucient  causing an
incomplete explanation  as described further below 
combining these elements produces an approach to learning from tutorial instruction
that is conceptually quite simple  for each instruction i that is received  the agent first
determines what situation i is meant to apply to  and then attempts to explain why the
step indicated by i leads to goal achievement in that situation  or prohibits it  for negative
instructions   if an explanation can be made  it produces general learning of some knowledge
ik by indicating the key features of the situation and instruction that cause success 
if an explanation cannot be completed  it indicates that the agent is missing one or
more pieces of prior knowledge mk  of any pscm type  needed to explain the instruction 
missing knowledge  in figure    missing arrows  causes an incomplete explanation by precluding achievement of g in the projection  for instance  the agent may not know a key
effect of an operator  or a crucial state inference  needed to reach g  more radically  the
action commanded by i may be completely unknown and thus inexplicable 
as shown in figure    there are four general options a learning agent might follow when
it cannot complete an explanation   o   it could delay the explanation until later  in the
hope that the missing knowledge  mk   will be learned in the meantime  alternatively 
 o  o   it could try to complete the explanation now by somehow learning the missing
knowledge  the missing knowledge could be learned  o   inductively  e g   by inducing
over the  gap  in the explanation  as described by vanlehn  jones   chi        and many
others   or   o   in an instructable agent s case  through further instruction  finally   o   it
could abandon the explanation altogether and try to learn the desired knowledge in another
way instead 
given only an incomplete explanation  it would be dicult to choose which option to
follow  identifying the missing knowledge mk in the general case is a dicult credit assignment problem  with no algorithmic solution   and there is nothing in the incomplete
explanation itself that predicts whether mk will be learned later if the explanation is delayed  thus  past machine learning systems have responded to incomplete explanations
either in only a single way  or in multiple ways  but that are tried in a fixed sequence 
many authors  bergadano   giordana        hall        vanlehn        vanlehn  jones 
  chi        widmer         for instance  describe systems that make inductions to complete incomplete explanations  option o    because of the diculty of determining missing
knowledge  these systems either base their induction on multiple examples  and or bias the
induction with an underlying theory or a teacher s help  sierra  vanlehn         for
example  induces over multiple partially explained examples  and constrains the induction
by requiring that each of the examples is unexplainable because of the same piece of missing knowledge  the same disjunct  in sierra s terminology   swale  schank   leake 
      uses an underlying theory of  anomalies  in explanations to complete incomplete explanations of events  occam  pazzani      b  uses options o  and o  in a static order 
   

fihuffman   laird

delay explanation until mk is learned

o 

after delay 
m

instruction
context
s

i

m

s

g

incomplete explanation
 missing knowledge mk 

i

m

k

g

learn mk inductively to complete explanation

o 
k

k

 

induce

i

s

g
m

k

learn mk from instruction to complete explanation

o 

s

m

i

s

k

g

further i

g

abandon explanation  learn another way 

o 

s

i

m

k

g

figure    options when faced with an incomplete explanation because of missing knowledge
mk  
it first attempts to fill in the gaps in an incomplete explanation inductively  biased by a
naive theory  if that fails  it abandons explanation and falls back on correlational learning
methods  pet  porter   kibler        is an example of a system that delays explanation
of a reasoning step until it learns further knowledge  option o   
however  as indicated in figure    an instructable agent has additional information
available to it besides the incomplete explanation itself  namely  the instructional context
 that is  the type of instruction and its place within the dialogue  often indicates which
option is most appropriate for a given incomplete explanation  thus  situated explanation includes all four of the options and dynamically selects between them based on the
instructional context  for a situated explanation of an instruction i in a situation  s  g  
where missing knowledge mk precludes completing the explanation to learn knowledge ik  
options o  o  take the following form 

o   delay the explanation until later  the instructional context can indicate a likelihood that the missing knowledge mk will be learned later  for instance  an instruction
i given in teaching a new procedure cannot be immediately explained because the re 

maining steps of the procedure are unknown  but they will be known later  assuming
the instructor completes teaching the procedure   in such cases  the agent discards its
current  incomplete explanation and simply memorizes i  s use in  s  g   rote learning   later  after mk is learned  i is recalled and explained in  s  g   causing ik to
be learned 
   

fiflexibly instructable agents

given instruction i from which knowledge ik can be learned 
 determine the situation  s  g   current or hypothetical  to which i applies
i
 explain i in  s  g  by forward projecting from s         g
  success  g met   learn ik from the complete explanation   ebl  
  failure  missing knowledge mk   options 
o   delay explanation until later 
o   induce mk   completing the explanation 
o   take instruction to learn mk   completing the explanation 
o   abandon explanation  instead  learn ik inductively 
table    situated explanation 

o   induce mk   completing the explanation  in some cases  the instructional context
localizes the missing knowledge mk to be part of a particular operator  for instance 

a purpose clause instruction   to do x  do y   suggests that the single operator y
should cause x to occur  because this localization tightly constrains the  gap  in
the incomplete explanation  the agent can use heuristics to induce a strong guess at
the mk needed to span the gap  inducing mk allows the explanation of i to be
completed and ik to be learned 
o   take instruction to learn mk   completing the explanation  the default response of the agent  when the other options are not deemed appropriate  is to ask
the instructor to explain i further  the further instruction can teach the agent mk  
again  learning mk allows the explanation of i to be completed and ik to be learned 
o   abandon the explanation and learn ik in another way  the instructional
context can indicate that the missing knowledge mk would be very dicult to learn 
this occurs when either the instructor refuses to give further information when asked
to  or when the agent has projected multiple operators that may be missing pieces
of knowledge  multiple potential mk s   since it is unknown whether mk will ever
be acquired  the agent abandons its explanation of i altogether  instead  it attempts
to learn ik directly  using inductive heuristics   without an explanation to base the
learning on 
these options will be made clearer through examples presented in the following sections 
situated explanation is summarized in table    unlike some knowledge acquisition approaches  it does not include an explicit check for consistency when newly learned knowledge
is added to the agent s knowledge base  as kodratoff and tecuci      a  point out  techniques like situated explanation are biased toward consistency because they only acquire
new knowledge when current knowledge is insucient  and they use current knowledge when
deriving new knowledge  however  in some domains  explicit consistency checks  such as
those used by wilkins         odysseus  may be required 
situated explanation meets the requirement that learning be incremental  t   because
it occurs during the ongoing processing of the agent and adds new pieces of knowledge to
   

fihuffman   laird

t 
t 
t 
t 
t 

i 
i 

general learning from specific cases
fast learning  each task instructed only once 
maximal use of prior knowledge
incremental learning
knowledge type exibility
a  state inference
b  operator proposal
c  operator control
d  operator effects
e  operator termination
command exibility
a  known command
b  skipped steps
c  unknown command
situation exibility
a  implicitly situated
b  explicitly situated  hypothetical state
hypothetical goal

table    expanded requirements of tutorial instruction met by instructo soar 
the agent s memory in a modular way  the local control structure of the pscm allows new
knowledge to be added independent of current knowledge  if there is a conict between
pieces of knowledge  for example  proposing two different operators in the same situation  
an impasse will arise that can be reasoned about or resolved with further instruction 

  

instructo soar

instructo soar is an instructable agent built within soar   and thus  the pscm   that
uses situated explanation to learn from tutorial instruction   instructo soar engages

in an interactive dialogue with its instructor  receiving natural language instructions and
learning to perform tasks and extend its knowledge of the domain  this section and the
next describe how instructo soar meets the targeted requirements of tutorial instruction 
which are shown in expanded form in table    this section describes the system s basic
performance when learning new procedures  and extending procedures to new situations 
from imperative commands  implicitly situated instructions   the next describes learning
other types of knowledge and handling explicitly situated instructions 

   for an overview of soar  and other systems built within it  see  rosenbloom  laird    newell      b  

   

fiflexibly instructable agents

figure    the robotic domain to which instructo soar has been applied 

    the domain and the agent s initial knowledge
the primary domain to which instructo soar has been applied is the simulated robotic
world shown in figure     the agent is a simulated hero robot  in a room with tables 
buttons  blocks of different sizes and materials  an electromagnet  and a light  the magnet
is toggled by closing the gripper around it  a red button toggles the light on or off  a green
button toggles it dim or bright  when it is on 
instructo soar consists of a set of problem spaces within soar that contain three
main categories of knowledge  natural language processing knowledge  originally developed
for nl soar  lewis         knowledge about obtaining and using instruction  and knowledge of the task domain itself  this task knowledge is extended through learning from
instruction  instructo soar does not expand its natural language capabilities per se as
it takes instruction  although it does learn how sentences map onto new operators that it
learns  it has complete  noiseless perception of its world  and can recognize a set of basic
object properties  e g   type  color  size  and relationships  e g   robot docked at table 
   the techniques have also been applied in a limited way to a ight domain  pearson  huffman  willis 
laird    jones         in which soar controls a ight simulator and instructions are given for taking off 

   

fihuffman   laird

pick up the red block 
move to the yellow table 
move the arm above the red block 
move up 
move down 
close the hand 
move up 
the operator is finished 

figure    instructions given to instructo soar to teach it to pick up a block 
gripper holding object  objects above  directly above  left of  right of one another  
the set of properties and relations can be extended through instruction  as described below 
the agent begins with knowledge of a set of primitive operators to which it can map
natural language sentences  and can execute  these include moving to tables  opening and
closing the hand  and moving the arm up  down  and above  left of  or right of things 
the agent can also internally project these operators  however  some of their effects under
various conditions are unknown  for instance  the agent does not know which operators
affect the light or magnet  or that the magnet will attract metal objects  also  the agent
begins with no knowledge of complex operators  that involve combinations of primitive
operators   such as picking up or arranging objects  pushing buttons  etc 

    learning new procedures through delayed explanation
instructo soar learns new procedures  pscm operators  from instructions like those

shown in figure    for picking up a block  since  pick up  is not a known procedure
initially  when told to  pick up the red block   the agent realizes that it must learn a new
operator 
to perform a pscm operator  the operator must be selected  implemented  and terminated  to select the operator in the future based on a command requires knowledge of
the operator s argument structure  a template   and how natural language maps to this
structure  thus  to learn a new operator  the agent must learn four things 
   template  knowledge of the operator s arguments and how they can be instantiated 
for picking up blocks  the agent acquires a new operator with a single argument  the
object to be picked up 
   mapping from natural language  a mapping from natural language semantic
structures to an instantiation of the new operator  so that the operator can be selected
when commanded in the future  for picking up blocks  the agent learns to map the
semantic object of  pick up      to the single argument of its new operator template 
   implementation  how to perform the operator  new operators are performed by
executing a sequence of smaller operators  the implementation takes the form of
selection knowledge for these sub operators  e g   move to the proper table  move the
arm  etc  
   

fiflexibly instructable agents

   termination conditions  knowledge to recognize when the new operator is achieved
  the goal concept of the new operator  for  pick up   the termination conditions
include holding the desired block  with the arm raised 
requirement t    fast learning   stipulates that after the first execution of a new procedure  the agent must be able to perform at least the same task without being re instructed 
thus  the agent must learn  in some form  each of the four parts of a new operator during
its first execution 
a general implementation of a new operator can be learned through situated explanation
of each of its steps  during the first execution of a new operator  though  the instructions
for performing it cannot be explained  because the agent does not yet know the goal of the
operator  e g   the agent does not know the termination conditions of  pick up   or the
steps following the current one to reach that goal  however  in this instructional context
  explaining instructed steps of a procedure being learned   it is clear that the missing
knowledge of the remaining steps and the procedure s goal will be acquired later  because
the instructor is expected to teach the procedure to completion  thus  the agent delays
explanation  option o   and for now memorizes each implementation instruction in a rote 
episodic form  at the end of the first execution of a new procedure  the agent induces the
procedure s goal   its termination conditions   using a set of simple inductive heuristics 
on later executions of the procedure  the original instructions are recalled and explained to
learn a general implementation 
we describe the details of this process using the  pick up  example 
      first execution

the example  shown in figure    begins with the instruction  pick up the red block   the
agent comprehends this instruction  producing a semantic structure and resolving  the red
block  to a block in its environment  however  the semantic structure does not correspond
to any known operator  indicating that the agent must learn a new operator  which it
calls  say  new op     to learn a template for the new operator  the agent simply assumes
that the argument structure of the command used to request the operator is the required
argument structure of the operator itself  in this case  a template for the new operator is
generated with an argument structure that directly corresponds to the semantic arguments
of the  pick up  command  here  one argument  object   the agent learns a mapping from
the semantic structure to the new operator s template  to be used when presented with
similar requests in the future  this simple approach to learning templates and mappings is
sucient for imperative sentences with direct arguments  but will fail for commands with
complex arguments  such as path constraints   move the dynamite into the other room 
keeping it as far from the heater as possible   
next  the new operator is selected for execution  since its implementation is unknown 
the agent immediately reaches an impasse and asks for further instructions  each instruction
in figure   is given  comprehended and executed in turn  these instructions provide the
implementation for the new operator  they are implicitly situated   each applies to the
current situation in which the agent finds itself 
at any point  the agent may be given another command that cannot be directly completed   one that requests either another unknown procedure or a known procedure that
   

fihuffman   laird

instruction being
explained

other steps

   

s

 
 
 

g 

figure    instructions teaching a new operator cannot be explained before the termination
conditions of the new operator are learned 
the agent does not know how to perform in the current situation due to skipped steps  this
is command exibility  requirement i    for example  within the instructions for  pick up  
the command  move above the red block  cannot be completed because of a skipped step
 the arm must be raised to move above something   an impasse arises where the instructor
indicates the needed step   move up    and then continues instructing  pick up  
ultimately  the implementation of a new operator can be learned at the proper level of
generality by explaining each instructed step  however  as illustrated in figure    during
its initial execution forming this explanation is impossible  because the goal of the new
operator and the other steps  further instructions  needed to reach it are not yet known 
since these missing pieces of the explanation are expected to be available later  the agent
delays explanation and resorts to rote learning of each instructed step 
in instructo soar  rote learning occurs as a side effect of language comprehension 
while reading each sentence  the agent learns a set of rules that encode the sentence s
semantic features  the rules allow nl soar to resolve referents in later sentences  implementing a simple version of grosz s focus space mechanism  grosz         the rules record
each instruction  indexed by the goal to which it applies and its place in the instruction
sequence  the result is essentially an episodic case that records the specific  lock step sequence of the instructions given to perform the new operator  for instance  it is recorded
that  to pick up  that is  new op    the red block  rb   i was first told to move to the yellow table  yt    of course  the information contained within the case could be generalized 
but at this point any generalization would be purely heuristic  because the agent cannot
explain the steps of the episode  thus  instructo soar takes the conservative approach
of leaving the case in rote form 
finally  the agent is told  the operator is finished   indicating that the goal of the
new operator has been achieved  this instruction triggers the agent to learn termination
conditions for the new operator  learning termination conditions is an inductive concept
formation problem  the agent must induce which features of those that hold in the current
state imply a positive instance of the new operator s goal being achieved  standard concept
learning approaches may be used here  as long as they produce a strong hypothesis within a
small number of examples  due to the  fast learning  requirement  t    instructo soar
uses a simple heuristic to strongly bias its induction  it hypothesizes that everything that
has changed between the initial state when the new operator was requested and the current
state is part of the new operator s termination conditions  in this case  the changes are that
the robot is docked at a table  holding a block  and the block and gripper are both up in
the air 
   

fiflexibly instructable agents

this heuristic gives a reasonable guess  but is clearly too simple  conditions that
changed may not matter  e g   perhaps it doesn t matter to picking up blocks that the
robot ends up at a table  unchanged conditions may matter  e g   if learning to build
a  stoplight   block colors are important although they do not change  thus  the agent
presents the induced set of termination conditions to the instructor for possible alteration
and verification  the instructor can add or remove conditions  for example  in the  pick
up  case the instructor might say  the robot need not be docked at the yellow table  to
remove a condition deemed unnecessary  before verifying the termination conditions 
instructo soar performs induction by ebl  chunking  over an overgeneral theory
that can make inductive leaps  similar to  e g   miller        rosenbloom   aasman       
vanlehn  ball    kowalski         this type of inductive learning has the advantage that
the agent can alter the bias to reect other available knowledge  in this case  the agent
uses further instruction  the instructor s indications of features to add or remove  to alter
the induction  other knowledge sources that could be employed  but are not in the current
implementation  include analogy to other known operators  e g   pick up actions in other
domains   domain specific heuristics  etc 
through the first execution of a new operator  then  the agent 
 carries out a sequence of instructions achieving a new operator 
 learns an operator template for the new operator 
 learns the mapping from natural language to the new operator 
 learns a rote execution sequence for the new operator 
 learns the termination conditions of the new operator 
since the agent has learned all of the necessary parts of an operator  it will be able to
perform the same task again without instruction  however  since the implementation of the
operator is rote  it can only perform the exact same task  it has not learned generally how
to pick up things yet 
      generalizing the new operator s implementation

the agent now knows the goal concept and full  though rote  implementation sequence for
the new operator  thus  it has the information that it needs to explain how each instruction
in the implementation sequence leads to goal achievement  provided its underlying domain
knowledge is sucient 
each instruction is explained by recalling it from episodic memory and internally projecting its effects and the rest of the path to achievement of the termination conditions
of the new operator  the projection is a  proof  that the instructed operator will lead
to goal achievement in the situation  soar s chunking mechanism essentially computes the
weakest preconditions of the situation and the instruction required for success  similar to
standard ebl  to form a general rule proposing the instructed operator  the rule learned
from the instruction  move to the yellow table  is shown in figure    the rule generalizes
the original instruction by dropping the table s color  and specializes it by adding the facts
that the table has the object sitting on it and that the object is small  only small objects
   

fihuffman   laird

if

the goal is new op     obj   and
 obj is on table  t  and small  obj   and
the robot is not docked at  t  and
the gripper has status open  
then propose operator move to table  t  
figure    the general operator proposal rule learned from the instruction  move to the
yellow table   new op    is the newly learned  pick up  operator  
can be grasped by the gripper   the rule also tests that the gripper is open  because this
condition was important for grasping the block in the instructed case  
after learning general proposal rules for each step in the instruction sequence  the agent
can perform the task without reference to the rote case  for instance  if asked to  pick
up the green block   the agent selects new op    instantiated with the green block  then 
general sub operator proposal rules like the one in figure   fire one by one  as they match the
current situation  to implement the operator  after performing all of the implementation
steps  the agent recognizes that the termination conditions are met  the gripper is raised
and holding the green block   and new op   is terminated 
since the general proposal rules for implementing the task are directly conditional on
the state  the agent can perform the task starting from any state along the implementation
path and can react to unexpected conditions  e g   another robot stealing the block   in
contrast  the rote implementation that was initially learned applied only when starting from
the original starting state  and was not reactive because its steps were not conditional on
the current state 

    recall strategies

we have described how our agent recalls and explains each step of a new operator s implementation sequence  after the operator s termination conditions are induced  there are
still two open issues   a  at what point after learning the termination conditions should
the agent perform the recall projection   and  b  how many steps should be recalled and
projected in sequence at a time 
to investigate these issues  we have implemented two different recall project strategies 
   immediate complete recall  the agent recalls and attempts to explain the full
sequence of instructions for the new operator immediately after learning the new
operator s termination conditions 
   lazy single step recall  the agent recalls and attempts to explain single instructions in the sequence when asked to perform the operator again starting from the
same initial state  that is  at each point in the execution of the operator  the agent
   more technical details of how soar s chunking mechanism forms such rules can be found in  huffman 
      laird  congdon  altmann    doorenbos        

   

fiflexibly instructable agents

recalls the next instruction  and attempts to explain it by forward projecting it  however  if this projection does not result in a path to goal achievement without any
further instructions being recalled  then rather than recalling the next instruction in
the sequence to continue the forward projection  the agent gives up on explaining this
instruction and simply executes it in the external world 
these strategies represent the extremes of a continuum of strategies    the strategy to
use is a parameter of the agent  it does not dynamically select between strategies while it
is running  a possible extension would be to reason about the time pressure in different
situations to select the appropriate strategy  next  we briey describe the implications of
each recall strategy 
      immediate complete recall strategy

immediate complete recall and explanation involves internally projecting multiple operators
 the full instruction sequence  immediately after the first execution of the new operator  the
projection begins at the state the agent was in when the new operator was first suggested 
if the projection successfully achieves the termination conditions of the new operator  the
agent learns general implementation rules for every step  the advantage of this strategy is
that the agent learns a general implementation for the new operator immediately after its
first execution  e g   the agent can pick up other objects right away  
the strategy has three important disadvantages  first  it requires that the agent reconstruct the initial state in which it was commanded to perform the new operator  this
reconstruction may be dicult if the amount of information in the state is large  although
it is not in the small robotic domain being used here  
second  recall and projection of the entire sequence of instructed steps is time consuming 
requiring time proportional to the length of the instruction sequence  during the process 
the agent s performance of tasks at hand is suspended  this suspension could be awkward
if the agent is under pressure to act quickly 
third  as illustrated in figure    multiple step projections are susceptible to compounding of errors in underlying domain knowledge  the projection of each successive operator
begins from a state that reects the agent s knowledge of the effects of prior operators in
the sequence  if this knowledge is incomplete or incorrect  the state will move further and
further from reecting the actual effects of prior operators  minor domain knowledge problems in the knowledge of individual operators  that alone would not produce an error in
a single step explanation  may combine within the projection to cause an error  this can
lead to incomplete explanations or  more rarely  to spuriously successful explanations  e g  
reaching success too early in the instruction sequence  
      lazy single step recall strategy

in the lazy single step recall strategy  the agent waits to recall and explain instructions until
asked to perform the new operator a second time from the same initial state  in addition 
the agent only recalls a single instruction to internally project at a time  after the recalled
    we also implemented a lazy complete recall strategy  which will not be described here  see huffman 
      for details  

   

fihuffman   laird

opx
op 

s 

op 

op 

s 
s 

op 

s 

   

g tc

opx

internally projected path
 reflecting agents incorrect
knowledge of operator effects 

sx

states reflecting
actual operator effects

sx

states reflecting
projected operator effects

g tc

state that meets the
termination conditions
for the current goal

op 

s 
op 

    sn    g  
tc

correct path  reflecting
actual operator effects 

figure    multiple step projections can result in incomplete explanations due to compounding of errors in domain knowledge 
operator is projected  the agent applies whatever general knowledge it has about the rest
of the implementation of the new operator  this general knowledge  however  does not
include rote memories of other past instructions  that is  if the agent does not know the
rest of the path to complete the new operator using general knowledge  it does not recall any
further instructions in the sequence from its rote memories  rather  the internal projection
is terminated and the single recalled operator is applied in the external world 
this strategy addresses the three disadvantages of the immediate complete strategy 
first  it does not require reconstruction of the original instruction state  rather  it waits for
a similar state to occur again 
second  recalling and projecting a single instruction at a time does not require a timeconsuming introspection that suspends the agent s ongoing activity  for  pick up   for
instance  table   shows the longest time that the agent s external action  movements or
instruction requests  is suspended using each strategy  as measured in soar decision cycles 
which last about    milliseconds each for instructo soar on an sgi r     indigo  
the immediate complete strategy does no external actions for     decision cycles  about
   seconds on our indigo  immediately following the first execution  in order to recall
and explain the complete instruction sequence  using the lazy single step strategy  only
one instruction is ever recalled explained at a time before action is taken in the world 
thus  the longest time without action is only    decision cycles  about   seconds   the
total recall explanation time is proportional to the length of the instruction sequence in
both cases      vs      decision cycles   but in the lazy single step strategy  that time
is interleaved with the execution of the instructions rather than fully taken after the first
execution 
third  the lazy single step strategy overcomes the problem of compounding of domain
theory errors by beginning the projection of each instruction from the current state of the
world after external execution of the previous instructions  thus  the beginning state of
each projection correctly reects the effects of the previous operators in the implementation
sequence 
the major disadvantage of this strategy is that it requires a number of executions of
the new operator equal to the length of the instruction sequence in order to learn the whole
   

fiflexibly instructable agents

immediate complete lazy single step
largest time without external action    
  
largest total recall explanation time      end of  st exec n       during  nd exec n  
during an execution

decision cycles  log scale 

table    timing comparison  in soar decision cycles  for learning  pick up  using the immediate complete and lazy single step recall strategies 
    

 r        
 r        

    

   

   

  
 

  

execution number  log scale    pick up 
 a 

 

  

execution number  log scale    move left of 
 b 

figure     decision cycles versus execution number to learn to  a  pick up and  b  move
objects left of one another  using the lazy single step strategy 
general implementation  this is because limiting recall to a single step allows only a single
sub operator per execution to be generalized  this disadvantage  however  leads to two
interesting learning characteristics 
 back to front generalization  generalized learning starts at the end of the implementation sequence and moves towards the beginning  on the second execution of the
new operator  a path to the goal is known only for the last instruction in the sequence
 it leads directly to goal completion   so a general proposal for that instruction is
learned  on the third execution  after the second to last instruction is projected  the
proposal learned previously for the last operator applies  leading to goal achievement
and allowing a general proposal for the second to last instruction to be learned  this
pattern continues back through the entire sequence until the full implementation is
learned generally  as figure    shows  the resulting learning curves closely approximate the power law of practice  rosenbloom   newell         r        for both  a 
and  b   
 effectiveness of hierarchical instruction  due to the back to front effect  the
agent learns a new procedure more quickly when its steps are taught using a hierarchical organization than when they are taught as a at sequence  figure    depicts a
at  nine step instruction sequence for teaching instructo soar to move one block
   

fihuffman   laird

 
move left of block  block  

 

 

 

move arm down

move to table  table 
 

graphical

  

moveleftof arm block  

 

 

move above  block 

figure     a

 

move arm up

close gripper

view

move arm down

move to table  table 

of

a

move left of block block   

open gripper

 

at

instruction

sequence

for

 
move left of block  block  

 

  

 
moveleftof arm block  

pick up  block 

put down  block 

  
move to table  table 
 

 
move to table  table 

 

 

 
move above  block 

figure     a

  
move arm up

grasp  block 

graphical

move arm down

  
open gripper

 
move arm down

view

close gripper

of

a hierarchical instruction sequence
move left of block block    new operators are shown in bold 

for

left of another  figure    depicts a hierarchical instruction sequence for the same procedure  that contains    instructed steps  but a maximum of   in any subsequence  by
breaking the instruction sequence into shorter subsequences  a hierarchical organization allows multiple subtrees of the hierarchy to be generalized during each execution 
general learning for an n step operator takes n executions using
a at instruction
hpn subtasks in each
sequence  taught hierarchically
as
an
h
 level
hierarchy
with
p
subsequence  only h  h n executions are required for full generalization  the hierarchy in figure    has an irregular structure  but results in apspeedup because the
length of every subsequence is small  in this case  smaller than n    empirically  the
at sequence of figure    takes nine  n   executions to generalize  whereas the hierarchical sequence takes only six  hierarchical organization has the additional advantage
that more operators are learned that can be used in future instructions 
   

fiflexibly instructable agents

    supporting command flexibility

command exibility  requirement i    stipulates that the instructor may request either an
unknown procedure  or a known procedure that the agent does not know how to perform in
the current state  skipping steps   at any point  this can lead to multiple levels of embedded
instruction  as we have seen  instructo soar learns completely new procedures from
instructions for unknown commands  in addition  when the agent is asked to perform a
known procedure in an unfamiliar situation   one from which the agent does not know what
step to take   it learns to extend its knowledge of the procedure to that situation 
an example is contained in the instructions for  pick up the red block   when the agent
is asked to  move above the red block   the agent knows how to perform the operator
when its arm is raised  however  in this case the arm is lowered  and so the agent reaches an
impasse and asks for further instruction    when told to  move up   the agent internally
projects raising its arm  which allows it to achieve moving above the red block  from this
projection it learns the general rule  move the arm up when trying to move above an object
that is on the table the agent is docked at  this rule extends the  move above  procedure
to cover this situation 
any operator   even one previously learned from instruction   may require extension to
apply to a new situation  this is because when the agent learns the general implementation
for a new operator  it does not reason about all possible situations in which the operator
might be performed  but limits its explanations to the series of situations that arises during
the actual execution of the new operator while it is being learned 
newly learned operators may be included in the instructions for later operators  leading
to learning of operator hierarchies  one hierarchy of operators learned by instructo soar
is shown in figure     learning procedural hierarchies has been identified as a fundamental
component of children s skill acquisition from tutorial instruction  wood  bruner    ross 
       in learning the hierarchy of figure     instructo soar learned four new operators  an extension of a known operator  move above   and an extension of a new operator
 extending  pick up  to work if the robot already is holding a block   because of command
exibility  this same hierarchy can be taught in exponentially many different ways  huffman         for instance  new operators that appear as sub operators  e g   grasp  can be
taught either before or during teaching of higher operators  e g   pick up  

    abandoning explanation when domain knowledge is incomplete

all of the general operator implementation learning described thus far depends on explaining
instructions using prior domain knowledge  as opposed to the learning of operator termination conditions  which is inductive   what if the domain knowledge is incomplete  making
explanation impossible  for sequences of multiple operators  pinpointing what knowledge
is missing is an extremely dicult credit assignment problem  sequences known to contain
only one operator  however  are a more constrained case  as described in the next section  
    another option would be to search  i e   to apply a weak method such as means ends analysis  in this
example  the search would be easy  in other cases  it could be costly  in any event  since the goal of
instructo soar is to investigate the use of instruction  our agent always asks for instructions when
it reaches an impasse in task performance  nothing in instructo soar precludes the use of search or
knowledge from other sources  however 

   

fihuffman   laird

lineup block  block  block  

move left of block  block  

pick up  block 

move arm down

moveleftof arm block  

move to table  table 

grasp  block 

put down  blockx 

move left of block  block  

put down  block 

move arm up

 lg  metal 

open gripper
grasp  magnet 

move above  block 

move arm down

 small 

move to table  table 

move above  blk mag 

move arm down

close gripper

move arm up

figure     a hierarchy of operators learned by instructo soar  primitive operators are
in light print  learned operators are in bold 
in general  an explanation failure that is detected at the end of the projection of an instruction sequence could be caused by missing knowledge about any operator in the sequence 
thus  when faced with an incomplete explanation of a sequence of multiple instructions 
instructo soar abandons the explanation and instead tries to induce knowledge directly
from the instructions  option o   
as an example  consider a case in which all of instructo soar s knowledge of secondary operator effects  frame axiom type knowledge  is removed before teaching it a procedure  for example  although the agent knows that closing the hand causes it to have
status closed  it no longer knows that closing the hand around a block causes the block
to be held  now  the agent is taught a new procedure  such as to pick up the red block 
after the first execution  the agent attempts to recall and explain the instructions as usual 
but fails because of the missing knowledge  that is  the block is not picked up during the
projection of the instructions  since the agent s knowledge does not indicate that it is held 
the agent records the fact that this procedure s instructions cannot be explained 
later  the agent is again asked to perform the procedure  and again recalls the instructions  however  it also recalls that explaining the instructions failed in the past  thus  it
abandons explanation and instead attempts to induce a general proposal rule directly from
each instruction   
    since an incomplete explanation for a procedure may indicate that some effect s  of an operator in the
instruction sequence is unknown  another alternative  not yet implemented in instructo soar  would
be for the agent to observe the effects of each operator in the sequence as it is performed  comparing
the observations to the effects predicted by domain knowledge  any differences would allow the agent

   

fiflexibly instructable agents

g  newop     pick up  
object   redblock 

 redblock  on  yellowtable 

op  movetotable
destination   yellowtable 

figure     the use of the op to g path heuristic  with op  move to the yellow table  
and g  pick up the red block  
in the  pick up  example  the agent first recalls the command to move to the yellow
table  to learn a proposal rule for this operator  call it op    the agent must induce a set of
conditions of the state under which performing op will contribute to achieving the  pick
up  goal  call it g   instructo soar uses two simple heuristics to induce these state
conditions 
 op to g path  for each object obj   filling a slot of op   and each object obj  
attached to g  include the shortest existing path  heuristically of length less than
three  of relationships between obj   and obj   in the set of induced conditions 
this heuristic captures the intuition that if an operator involves some object  its
relationship to the objects relevant to the goal is probably important  figure   
shows its operation for  move to the yellow table   as the figure indicates  there is a
path between g s object  the red block  and the destination of op   the yellow table 
through the relationship that the block is on the table 
 op features unachieved  each termination condition  essentially  each primary
effect  of op that is not achieved in the state before op is performed is considered
an important condition 
this heuristic captures the intuition that all of the primary effects of op are probably
important  therefore  it matters that they are not achieved when op is selected  in
our example  op  s primary effect is that the robot ends up docked at the table  thus 
the fact that the robot is not initially docked at the table is added to the inferred set
of conditions for proposing op  
these heuristics are implemented as soar operators that compute the appropriate conditions  once a set of conditions is induced  it is presented to the instructor  who can add or
remove conditions before verifying them  upon verification  a rule is learned proposing op
 e g   move to table  t   when the induced conditions hold  e g   goal is pick up  b  
 b isa block  on  b  t    this rule is similar to the rule learned from explanation  figure     but only applies to picking up a block  overspecific   and does not stipulate that the
to learn new operator effects that could complete the explanation of the procedure  learning effects
of operators from observation has been explored by a number of researchers  carbonell   gil       
pazzani      b  shen        sutton   pinette        thrun   mitchell        

   

fihuffman   laird

object must be small  overgeneral   a similar induction occurs for each step of  pick up  
so that the agent learns a general implementation for the full  pick up  operator  however 
unless corrections are made by the instructor  this induced implementation is not as correct
as one learned from explanation  for instance  it applies  wrongly  to any block instead of
to any small object  in a more complex domain  inferring implementation rules would be
even less successful  not surprisingly  psychological research shows that human subjects 
learning from procedural instructions also degrades if they lack domain knowledge  kieras
  bovair        
returning to the targeted instruction requirements in table    instructo soar s learning of procedures illustrates  t   general learning from specific instructions   t   fast learning  because each procedure need only be instructed once  by  t   using prior domain
knowledge to construct explanations  and  t   incremental learning during the agent s ongoing performance  two types of pscm knowledge are learned   t  b   operator proposals
for sub operators of the procedure  and  t  e   the procedure s termination conditions 
the learning involves either delayed explanation  or when domain knowledge is inadequate 
abandoning explanation in favor of simple induction  the instructions are each  i   a   implicitly situated imperative commands  for either  i  a   known procedures   i  b   known
procedures where steps have been skipped  or  i  c   unknown procedures 

   beyond imperative commands

next  we turn to learning the remaining types of pscm knowledge  t  a c d   from various
kinds of explicitly situated instructions  i  b    from an explicitly situated instruction 
instructo soar constructs a hypothetical situation  goal and state  that includes the
objects  properties  and relationships mentioned explicitly in the instruction as well as
any features of the current situation that are needed to carry out the instruction    this
hypothetical situation is used as the context for a situated explanation of the instruction 

    hypothetical goals and learning effects of operators

a goal is explicitly specified in an instruction by a purpose clause  dieugenio          to
do x  do y   the basic knowledge to be learned from such an instruction is an operator
proposal rule for doing y when the goal is to achieve x 
consider this example from instructo soar s domain 
  to turn on the light  push the red button 

the agent has been taught how to push buttons  but does not know the red button s
effect on the light  from a purpose clause instruction like this example  the agent creates a
hypothetical situation with the goal stated in the purpose clause  here   turn on the light   
and a state like the current state  but with that goal not achieved  here  with the light off  
within this situation  the agent attempts to explain the instruction by forward projecting
the action of pushing the red button 
if the agent knew that pushing the red button toggles the light  then in the projection 
the light would come on  thus  the explanation would succeed  and a general operator
    see  huffman        for details of how these features are determined 

   

fiflexibly instructable agents

proposal rule would be learned  that proposed pushing the red button when the light is off
and the goal is to turn it on 
however  since in actuality the agent is missing the knowledge  mk   that pushing the
button affects the light  the light does not come on within the projection  the explanation
is incomplete 
when instructo soar s explanation of a sequence of operators fails  the agent does
not try to induce the missing knowledge needed to complete the explanation  because it
could be associated with any of the multiple operators  rather  the explanation is simply
abandoned  as described in section      however  in this case  the unexplainable sequence
contains only one operator  in addition  the form of the instruction gives the agent a
strong expectation about that operator s intended effect  based on the purpose clause  the
agent expects that the specified action  pushing the button  will cause achievement of the
specified goal  turning on the light   dieugenio        found empirically that this type of
expectation holds for     of naturally occurring purpose clauses 
the expectation constrains the  gap  in the incomplete explanation  the state after
pushing the button should be a state with the light on  and only one action was performed to
produce this effect  based on this constrained gap  the agent attempts to induce the missing
knowledge mk in order to complete the explanation  option o    the most straightforward
inference of mk is simply that an unknown effect of the single action is to produce the
expected goal conditions   e g   pushing the button should cause the light to come on  the
instructor is asked to verify this inference   
once it is verified  instructo soar heuristically guesses at the state conditions under
which the effect will occur  it uses the op to g path heuristic as a very naive causality
theory  pazzani      a  to guess at the causes of the inferred operator effect  here  op tog path notices that the light and the red button are both on the same table  in addition 
the agent includes the fact that the inferred effect did not hold  the light was off  before
the operator caused it  the result is presented to the instructor 
i think that doing push the button causes 
the light to be on
under the following conditions 
the light is not currently on  the light is on the table  the button is on the table
are those the right conditions 
here  the heuristics have not recognized that it matters which button is pushed  the red
one   the instructor can add this condition by saying   the button must be red   
once the instructor verifies the conditions  the agent adds the new piece of operator effect
knowledge to its memory 
if

projecting push button  b   and
 l isa light with status off  on table  t  and
 b isa button with color red  on table  t 
then light  l now has status on 

    if the inference is rejected  the agent abandons the explanation and directly induces a proposal rule for
pushing the button from the instruction  as described in section     

   

fihuffman   laird

immediately after being learned  this rule applies to the light in the forward projection for
the current instruction  the light comes on  completing the instruction s explanation by
achieving its goal  from this explanation  the agent learns the proposal rule that proposes
pushing the red button when the goal is to turn on the light  thus  the agent has acquired
new knowledge at multiple levels  inferring an unknown effect of an operator supported
learning a proposal for that operator 
this example illustrates  i   b   the use of hypothetical goal instructions and the use
of option o  for dealing with incomplete explanations   inferring missing knowledge   to
learn new operator effects  t  d    thus extending domain knowledge 

    hypothetical states to learn about contingencies

instructors use instructions with hypothetical states  e g   conditionals   if  state conditions   do       either to teach general policies   if the lights are on when you leave the
room  turn them off    or to teach contingencies when performing a task  instructosoar handles both of these  here  we will describe the latter 
a contingency instruction indicates a course of action to be followed when the current
task is performed in a future situation different from the current situation  instructors
often use contingency instructions to teach about situations that differ from the current
one in some crucial way that should alter the agent s behavior  contingency instructions
are very common in human instruction  ford and thompson        found that     of the
conditional statements in an instruction manual communicated contingency options to the
student 
consider this interaction 
  grasp the blue block 

that s a new one for me  how do i do that 

  if the blue block is metal  then pick up the magnet 

the blue block is not made of metal  but the instructor is communicating that if it were  a
different course of action would be required 
from the conditional instruction  if the blue block is metal  then pick up the magnet  
the agent needs to learn an operator proposal rule for picking up the magnet under appropriate conditions  the agent begins by constructing the hypothetical situation to which
 pick up the magnet  applies   if the blue block is metal  indicates a hypothetical state
that is a variant of the current state with the blue block having material metal  the
current goal   grasp the blue block   is also the goal in the hypothetical situation 
within this situation  the agent projects picking up the magnet to explain how it will
allow the block to be grasped  however  the agent is missing much of the knowledge needed
to complete this explanation  it does not know the goal concept of  grasp  yet  or the rest
of the instructions to reach that goal 
since the instruction being explained is for a contingency  the rest of the instructions
that the agent is given to  grasp the blue block  may not  and in this case  do not  apply
to the contingent situation  where the block is metal  in the normal grasp sequence  for
instance  the agent learns to close its hand around the grasped object  but when grasping
a metal object  the hand is closed around the magnet  since knowledge of how to complete
   

fiflexibly instructable agents

grasping a metal object is needed to explain the contingency instruction  and the agent
does not know when it might learn this missing knowledge  it abandons the explanation
 option o    instead  it uses the heuristics described in section     to directly induce an
operator proposal rule for  grasp the magnet   in addition to the conditions generated by
the heuristics  the conditions indicated in the antecedent of the instruction are included 
the result is presented to the instructor for alteration and verification 
so i m guessing the conditions for doing  pick up the magnet 
when your goal is  grasp the block  are 
the block is metal
is that right 
  right 

from this interaction the agent learns a rule that proposes picking up the magnet when the
goal is to grasp a metal block  after this learning is completed  since the agent has not yet
finished grasping the blue block  it continues to receive instruction for that task  further
contingencies can be indicated at any point  learning contingencies illustrates  i  b   the
handling of hypothetical state instructions 

    learning to reject operators

our final examples illustrate learning to reject an operator   a type of operator control
knowledge in the pscm  the examples also detail the remaining option for dealing with
incomplete explanations   o   completing an explanation through further instruction 
consider these instructions 
  never grasp green blocks 

why 
 a    trust me 
 b    green blocks are explosive 
a negative imperative prohibits a step from applying to a hypothetical situation in which
it might apply  thus  instructo soar creates a hypothetical situation in which the
prohibited action might be executed  in this case  a state with a graspable green block 
since no goal is specified by the instruction  and there is no other current goal  a default
goal of  maintaining happiness   which is always considered one of the agent s current goals 
is used  from this hypothetical situation  the agent internally projects the  grasp  action 
expecting an  unhappy  result  however  the resulting state  in which the agent is grasping
a green block  is acceptable according to the agent s knowledge  thus  the projection does
not explain why the action is prohibited 
the agent deals with the incomplete explanation by asking for further instruction  in an
attempt to learn mk and complete the explanation  however  the instructor can decline
to give further information by saying  a  trust me  although the instructor will not provide mk   because the prohibition of a single operator  grasping the green block  is being
explained  the agent can induce a plausible mk that will complete the explanation  option
o    since the agent knows that the final state after the prohibited operator is meant to be
 unhappy   it simply induces that this state is to be avoided  this is the converse of learning to recognize when a desired goal has been reached  learning an operator s termination
   

fihuffman   laird

conditions   the agent conservatively guesses that all of the features of the hypothetical
state  here  that there is a green block that is held   taken together  make it a state to
be avoided  because this inference is so conservative  in the current implementation the
instructor is not even asked to verify it  the state inference rule that results is as follows 
if

goal is   happiness    and
 b isa block with color green  and
holding gripper  b  
then this state fails to achieve   happiness   

this rule applies to the final state in the projection of  never grasp     the state s failure
to achieve happiness completes the agent s explanation of why it should  never grasp     
and it learns a rule that rejects any proposed operator for grasping a green block 
alternatively  the instructor could provide further instruction  as in  b  green blocks
are explosive  such instruction can provide the missing knowledge mk needed to complete an incomplete explanation  option o    from  b   the agent learns a state inference
rule  blocks with color green have explosiveness high  instructo soar learns state
inferences from simple statements like  b   and from conditionals  e g    if the magnet is
powered and directly above a metal block  then the magnet is stuck to the block   by essentially translating the utterance directly into a rule    such state inference instructions
can be used to introduce new features that extend the agent s representation vocabulary
 e g  stuck to  
the rule learned from  green blocks are explosive  adds explosiveness high to the
block that the agent had simulated grasping in the hypothetical situation  the agent knows
that touching an explosive object may cause an explosion   a negative result  this negative
result completes the explanation of  never grasp      and from it the agent learns to avoid
grasping objects with explosiveness high 
completing an explanation through further instruction  as in  b   can produce more
general learning than heuristically inferring missing knowledge  as in  a    in  b   if the
agent is later told blue blocks are explosive  it will avoid grasping them as well  in
general  multiple levels of instruction can lead to higher quality learning than a single level
because learning is based on an explanation composed from strong lower level knowledge
 mk   rather than inductive heuristics alone  mk  here  the state inference rule  is also
available for future use 
because the agent has learned not only to reject the  grasp  operator but to recognize
the bad state that performing it would lead to  the agent can recognize the bad state if it
is reached from another path  for instance  the agent can be led through the individual
steps of grasping an explosive block without the instructor ever mentioning  grasp   when
the agent is finally asked to  close the gripper  around the explosive object  it does so 
but then immediately recognizes the undesirable state it has arrived in and reverses the
close gripper action  in the process  it learns to reject close gripper if the hand is
around an explosive object  so that in the future it will not reach the undesirable state
through this path 
    this translation occurs by chunking  but in an uninteresting way  instructo soar does not use explanation to learn state inferences  an extension would be to try to explain why an inference holds using
a deeper causal theory 

   

fiflexibly instructable agents

notice here the effect of the situated nature of instructo soar s learning  the agent
learns to avoid operators that lead to a bad state only when they arise in the agent s
performance  its initial learning about the bad state is recognitional rather than predictive 
alternatively  when the agent first learns about a bad state  it could do extensive reasoning
to determine every possible operator that could lead to that state  from every possible
previous state  to learn to reject those operators at the appropriate times  this unsituated
reasoning would be very expensive  the agent would have to reason through a huge number
of possible situations  in addition  whenever new operators were learned  the agent would
have to reason about all the possible situations in which they could arise  to learn if they
could ever led to a bad state  rather than this costly reasoning  instructo soar simply
learns what it can from its situations as they arise 
another alternative for completely avoiding bad states would be to think through the
effects of every action before taking it  to see if a bad state will result  this highly cautious
execution strategy would be appropriate in dangerous situations  but is not appropriate
in safer situations where the agent is under time pressure   moving between more or less
cautious execution strategies is not currently implemented in instructo soar  
the  never grasp     examples have illustrated the agent s learning of one type of
operator control knowledge  namely operator rejection  t  c    learning of state inferences
 t  a    and the use of further instruction to complete incomplete explanations  option o   
the final category of learning we will discuss is a second type of operator control knowledge 

    learning operator comparison knowledge
another type of control knowledge besides operator rejection rules is operator comparison
rules  which compare two operators and express a preference for one over the other in a given
situation  instructo soar learns operator comparison rules by asking for the instructor s
feedback when multiple operators are proposed at the same point to achieve a particular
goal  multiple operators can be proposed  for instance  when the agent has been taught two
different methods for achieving the same goal  e g   to pick up a metal block either using
the magnet or directly with the gripper   the instructor is asked to either select one of
the proposed operators or to indicate that some other action is appropriate  selecting one
of the proposed choices causes the agent to learn a rule that prefers the selected operator
over the other proposed operators in situations like the current situation  alternatively 
if the instructor indicates some other operator outside of the set of proposed operators 
instructo soar attempts to explain that operator in the usual way  to learn a general
rule proposing it  in addition  the agent learns rules preferring the instructed operator to
each of the other currently proposed operators 
there are two weaknesses to instructo soar s learning of operator comparison rules 
first  the instructor can be required to indicate a preference for each step needed to complete a procedure  rather than simply choosing between overall methods  that is  the
instructor cannot say  use the method where you grab the block with your gripper  instead
of using the magnet   but must indicate a preference for each individual step of the method
employing the gripper  this is because in the pscm  knowledge about steps in a procedure
is accessed independently  as separate proposal rules  rather than as an aggregate method 
independent access improves exibility and reactivity   the agent can combine steps from
   

fihuffman   laird

different methods as needed based on the current situation   but a higher level grouping of
steps would simplify instruction for selecting between complete methods 
the second weakness is that although the agent uses situated explanation to explain the
selection the instructor makes  it does not explain why that selection is better than the other
possibilities  preferences between viable operators are often based on global considerations 
e g    prefer actions that lead to overall faster cheaper goal achievement   learning based
on this type of global preference  which in turn may be learned through instruction  is a
point for further research 

   discussion of results
we have shown how instructo soar learns from various kinds of instructions  although
the domain used to demonstrate this behavior is simple  it has enough complexity to exhibit
a variety of the different types of instructional interactions that occur in tutorial instruction 
of the    requirements that tutorial instruction places on an instructable agent  listed in
table     instructo soar meets    listed in expanded form in table    either fully or partially  three of these in particular distinguish instructo soar from previous instructable
systems 






command exibility  the instructor can give a command for any task at each
instruction point  whether or not the agent knows the task or how to perform it in
the current situation 

situation exibility  the agent can learn from both implicitly situated instructions
and explicitly situated instructions specifying either hypothetical goals or states 

knowledge type exibility  the agent is able to learn each of the types of knowledge it uses in task performance  the five pscm types  from instruction 

earlier  we claimed that handling tutorial instruction s exibility requires a breadth of
learning and interaction capabilities  combining command  situation  and knowledge type
exibility  instructo soar displays    distinct instructional capabilities  as listed in table    this variety of instructional behavior does not require    different learning techniques 
but arises as one general technique  situated explanation in a pscm based agent  is applied
in a range of instructional situations 
our series of examples has illustrated how situated explanation uses an instruction s
situation and context during the learning process  first  the situation to which an instruction applies provides the endpoints for attempting to explain the instruction  second  the
instructional context can indicate which option to follow when an explanation cannot be
completed  the context of learning a new procedure indicates that delaying explanation
 option o   is best  since the full procedure will eventually be taught  if a step cannot be
explained in a previously taught procedure  missing knowledge could be anywhere in the
procedure  so it is best to abandon explanation  option o   and learn another way  instructions that provide an explicit context  such as through a purpose clause  localize missing
knowledge by giving strong expectations about a single operator that should achieve a single
goal  this localization makes it plausible to induce missing knowledge and complete the
   

fiflexibly instructable agents

  
  
  
  
  
  
  

instructional capability
learning completely new procedures
extending a procedure to apply in a new situation
hierarchical instruction  handling instructions for a
procedure embedded in instruction for others
altering induced knowledge based on further
instruction
learning procedures inductively when domain
knowledge is incomplete
learning to avoid prohibited actions
more general learning due to further instruction

   learning to avoid indirect achievement of a bad
state
   inferences from simple specific statements
    inferences from simple generic statements
    inferences from conditionals
    learning an operator to perform for a hypothetical
goal
    learning an operator to perform in a hypothetical
state  general policy  active at all times 
    learning an operator to perform in a hypothetical
state  contingency within a particular procedure
    learning operator effects
    learning non perceivable operator effects and associated inferences to recognize them
    learning control knowledge  learning which of a set
of operators to prefer
    learning control knowledge  learning operators are
indifferent

example
pick up

move up to move above
teaching pick up within line

removing docked at from pick
up s termination conditions
learning with secondary operator
effects knowledge removed
 never grasp red blocks  
avoid grasping because  red
blocks are explosive  
closing hand around explosive
block
 the grey block is metal  
 white magnets are powered  
 if condition  and condition  
then concluded state feature 
 to turn on the light  push the
red button  
 if the light is bright  then dim
the light  
 if the block is metal  then grasp
the magnet  to pick up
pushing the red button turns on
the light
the magnet becomes stuck to a
metal block when moved above it
two ways to grasp a small metal
block
two ways to grasp a small metal
block

table    instructional capabilities demonstrated by instructo soar 

   

up

fihuffman   laird

explanation  option o    in other cases  the default is to ask for instruction about missing
knowledge to complete the explanation  option o   

    empirical evaluation

most empirical evaluations of machine learning systems take one of four forms  each appropriate for addressing different evaluation questions 
a  comparison to other systems  this technique is useful for evaluating how overall
performance compares to the state of the art  it can be used when there are other
systems available that do the same learning task 
b  comparison to an altered version of the same system  this technique evaluates the
impact of some component of the system on its overall performance  typically  the
system is compared to a version of itself without the key component  sometimes called
a  lesion study   
c  measuring performance on a systematically generated series of problems  this technique evaluates how the method is affected by different dimensions of the input  e g  
noise in training data  
d  measuring performance on known hard problems  known hard problems provide an
evaluation of overall performance under extreme conditions  for instance  concept
learners  performance is often measured on standard  dicult datasets 
these evaluation techniques have been applied in limited ways to instructo soar 
they are dicult to apply in great depth for two reasons  first  whereas most machine
learning efforts concentrate on depth of a single type of learning from a single type of input 
tutorial instruction requires a breadth of learning from a range of instructional interactions  whereas depth can be measured by quantitative performance  breadth is measured
by  possibly qualitative  coverage   here  our coverage of   out of    instructability requirements  second  tutorial instruction has not been extensively studied in machine learning  so
there is not a battery of standard systems and problems available  nonetheless  evaluation
techniques  b    c   and  d  have been applied to instructo soar to address specific
evaluation questions 
b  comparison to altered version  we removed frame axiom knowledge to illustrate the
effect of prior knowledge on the agent s performance  as described in section     
without prior knowledge  the agent is unable to explain instructions and must resort
to inductive methods  thus  removing frame axiom knowledge increased the amount
of instruction required and reduced learning quality  we also compared versions of
the agent that use different instruction recall strategies  section      
c  performance on systematically varied input  we examined the effects of varying three
dimensions of the instructions given to the agent  first  we compared learning curves
for instruction sequences of different lengths  section       as the graphs in figure   
show  instructo soar s execution time for an instructed procedure varies with the
number of instructions in sequence used to teach it  total execution time drops each
   

fiflexibly instructable agents

time the procedure is executed  according to a power law function  until the procedure has been learned in general form  second  we compared teaching a procedure
through hierarchical subtasks versus using a at instruction sequence  based on the
power law result  we predicted that hierarchical instruction would allow faster general
learning than at instruction  this prediction was confirmed empirically  third  we
examined the number of instruction orderings that can be used to teach a given procedure to instructo soar in order to measure the value of supporting command
exibility  rather than an experimental measurement  we performed a mathematical
analysis  the analysis showed that due to command exibility  the number of instruction sequences that can be used to teach a given procedure is very large  growing
exponentially with the number of primitive steps in the procedure  huffman        
d  performance on a known hard problem  since learning from tutorial instruction has
not been extensively studied in machine learning  there are no standard  dicult
problems  we created a comprehensive instruction scenario by crossing the command
exibility  situation exibility  and knowledge type exibility requirements  the scenario  described in detail in  huffman         contains     instructions and demonstrates    of instructo soar s    instructional capabilities from table    it does
not include learning indifference in selecting between two operators   the agent learns
about       chunks during the scenario  including examples of each type of pscm
knowledge  that extend the agent s domain knowledge significantly 

   limitations and further research
this work s limitations fall into three major categories  limitations to tutorial instruction as
a teaching technique  limitations of the agent s general capabilities  and limitations because
of incomplete solutions to the mapping  interaction  and transfer problems  we discuss each
of these in turn 

    limitations of tutorial instruction

tutorial instruction is both highly interactive and situated  however  much of human
instruction is either non interactive or unsituated  or both   and these have not been considered in this work  in non interactive instruction  the content and ow of information to
the student is controlled primarily by the information source  examples include classroom
lectures  instruction manuals  and textbooks  one issue in using this type of instruction is
locating and extracting the information that is needed for particular problems  carpenter
  alterman         non interactive instruction can contain both situated information  e g  
worked out example problems  chi et al         vanlehn        and unsituated information
 e g   general expository text  
unsituated instruction conveys general or abstract knowledge that can be applied in
a large number of different situations  such general purpose knowledge is often described
as  declarative   singley   anderson         for example  in physics class  students are
taught that f   m  a  this general equation applies in specific ways to a great variety of
situations  the advantage of unsituated instruction is precisely this ability to compactly
communicate abstract knowledge that is broadly applicable  sandberg   wielinga        
   

fihuffman   laird

however  to use such abstract knowledge  students must learn how it applies to specific
situations  singley   anderson        

    limitations of the agent

an agent s inherent limitations constrain what it can be taught  we have developed our
theory of learning from tutorial instruction within a particular computational model of
agents  the pscm   and within this computational model  we implemented an agent with
a particular set of capabilities to demonstrate the theory  thus  both the weaknesses of the
computational model and the specific implemented agent must be examined 
      computational model

the problem space computational model is well suited for situated instruction because
of its elements  close correspondence to the knowledge level  facilitating mapping from
instructions to those elements   and its inherently local control structure  however  the
pscm s local application of knowledge makes it dicult to learn global control regimes
through instruction  because they must be translated into a series of local decisions that
will each result in local learning 
a second weakness of the pscm is that it provides a theory of the functional types of
knowledge used by an intelligent agent  but gives no indication of the possible content of
that knowledge  a content theory of knowledge would allow a finer grained analysis of an
agent s instructability  within the larger grained knowledge types analysis provided by the
pscm 
      implemented agent s capabilities

producing a definitive agent has not been the goal of this work  rather  the instructosoar agent s capabilities have been developed only as needed to demonstrate its instructional learning capabilities  thus  it is limited in a number of ways    for instance  it
performs simple actions serially in a static world  this would not be sucient for a dynamic
domain such as ying an airplane  where multiple goals at multiple levels of granularity 
involving both achievement and or maintenance of conditions in the environment  may be
active at once  pearson et al          instructo soar s procedures are implemented by a
series of locally decided steps  precluding instruction containing procedure wide  i e   nonlocal  path constraints  e g    go to the other room  but don t walk on the carpeting    
there is only a single agent in the world  precluding instructions that involve cooperation
with other agents  e g   two robots carrying a couch  and instructions that require reasoning
about other agents  potential actions  e g    don t go down the alley  because your enemy
may block you in   
the agent has complete perception  clearly unrealistic in real physical domains   so it
never has to be told where to look  or asked to notice a feature that it overlooked  in contrast  our instruction protocols show that human students are often told where to attend or
what features to notice  instructo soar s world is noise free  so the agent does not need
    these limitations are of the particular agent implemented here  not of soar  which has been used to
build more powerful agents  e g   jones et al         pearson et al         

   

fiflexibly instructable agents

to reason or receive instruction about failed actions  because it has complete perception
and a noise free environment  the agent does not explicitly reason about uncertainty in its
perceptions or actions  and we have not demonstrated handling instructions that explicitly
describe uncertain or probabalistic outcomes    the agent also does not reason about time
 as  e g   vere and bickmore s        homer does   so it cannot be taught to perform tasks
in a time dependent way  it does not keep track of states it has seen or actions it performs
 other than its episodic instruction memory   so it cannot be asked to  do what you did
before   similarly  it cannot learn procedures that are defined by a particular sequence of
actions  rather than a set of state conditions to achieve  for example  it cannot be taught
how to dance  because dancing does not result in a net change to the external world  finally  whenever the agent does not know what to do next  it asks for more instruction  it
never tries to determine a solution through search and weak methods such as means ends
analysis  adding this capability would decrease its need for instruction 
in addition to the agent s capabilities  instructo soar is limited because its solutions
to the mapping  interaction  and transfer problems are incomplete in various ways  these
limitations are discussed next 

    mapping problem
instructo soar employs a straightforward approach to mapping instructions into the

agent s internal language  and leaves all of the problems of mapping dicult natural language constructions unaddressed  some of the relevant problems include reference resolution  incompleteness  and the use of domain knowledge in comprehension  mapping can
even require further instruction  as in this interaction to resolve a referent 
  grab the explosive block 

which one is that 
  the red one 

this type of interaction is not supported by instructo soar 
in addition to these general linguistic problems  instructo soar makes only limited
use of semantic information when learning new operators  for example  when it first reads
 move the red block left of the yellow block   it creates a new operator  but does not make
use of the semantic information communicated by  move   to the left of   a more complete
agent would try to glean any information it could from the semantics of an unfamiliar
command 

    interaction problem
the agent s shortcomings on the interaction problem center around its three requirements 
 i   exible initiation of instruction   i   full exibility of knowledge content  and  i  
situation exibility   i     in instructo soar  instruction is initiated only by the agent 
    in the instruction protocols we analyzed  most instructions were incomplete  missing conditions like those
instructo soar learns   but rarely described uncertainty explicitly 

   

fihuffman   laird

this limits the instructor s ability to drive the interaction or to interrupt the agent s actions
with instruction   no  don t push that button    
 i    instructo soar provides exibility for commands  but not for instructions that
communicate other kinds of information  similar to the notion of discourse coherence  mann
  thompson         a fully exible tutorable agent needs to support any instruction event
with knowledge coherence  that is  any instruction event delivering knowledge that makes
sense in the current context  the great variety of knowledge that could be relevant at any
point makes this requirement dicult 
 i    instructo soar provides situation exibility by handling both implicitly and
explicitly situated instructions  but hypothetical situations can only be referred to within a
single instruction  human tutors often refer to one hypothetical situation over the course
of multiple instructions 

    transfer problem

this work has focused primarily on the transfer problem   producing general learning from
tutorial instruction   and most of its requirements have been met  however  the inductive
heuristics that instructo soar uses are not very powerful 
in addition  two transfer problem requirements have not been achieved  first   t  
instructo soar has not yet demonstrated instructional learning in coexistence with learning from other knowledge sources  nothing in instructo soar s theory precludes this coexistence  however  learning from other knowledge sources could be invoked and possibly
enhanced through instruction  for instance  an instructor might invoke learning from observation by pointing to a set of objects and saying  this is a tower   similarly  an instruction
containing a metaphor could invoke analogical learning  one application where instruction
could potentially enhance other learning mechanisms is within  personal assistant  software
agents that learn by observing their users  e g   maes        mitchell et al          adding
the ability to learn from verbal instructions in addition to observations would allow users
to explicitly train these agents in situations where learning from observation alone may be
dicult or slow 
second   t   instructo soar cannot recover from incorrect knowledge that leads to
either invalid explanations or incorrect external performance  such incorrect knowledge
may be a part of the agent s initial domain theory  or may be learned through faulty
instruction  inability to recover from incorrect knowledge precludes instruction by general
case and exceptions  for instance   never grasp red blocks   and then later   it s ok to
grasp the ones with safety signs on them   in order to avoid learning anything incorrect 
whenever instructo soar attempts to induce new knowledge  it asks for the instructor s
verification before adding the knowledge to its long term memory  human students do not
ask for so much verification  they appear to jump to conclusions  and alter them later if
they prove to be incorrect based on further information 
rather than always verifying knowledge being learned  our next generation of instructable
agents will learn from reasonable inferences without verification  although they may ask
for verifications in extreme cases   we have recently produced such an agent  pearson  
    we have recently added a simple interruptability capability to a new version of instructo soar that
incorporates recovery from incorrect knowledge  pearson   huffman        

   

fiflexibly instructable agents

huffman        that incorporates current research on incremental recovery from incorrect
knowledge  pearson   laird         this agent learns to correct overgeneral knowledge that
it infers when completing explanations of instructions  the correction process is triggered
when using the overgeneral knowledge results in incorrect performance  e g   an action that
the agent expects to succeed does not   in the long run  we believe this work could push
research on incremental theory revision and error recovery  because instructable agents can
be taught many types of knowledge that may need revision 

    conclusion

although much work in machine learning aims for depth at a particular kind of learning 

instructo soar demonstrates breadth   of interaction with an instructor to learn a variety

of types of knowledge   but all arising from one underlying technique  this kind of breadth is
crucial in building an instructable agent because of the great variety of instructions and the
variety of knowledge that they can communicate  because instructable agents begin with
some basic knowledge of their domain  instructo soar uses an analytic  explanationbased approach to learn from instructions  which makes use of that knowledge  because
instructions may be either implicitly or explicitly situated  instructo soar situates its
explanations of each instruction within the situation indicated by the instruction  finally 
because the agent s knowledge is often deficient for explaining instructions  instructosoar employs four different options for dealing with incomplete explanations  and selects
between these options dynamically depending on the instructional context 
because of its availability and effectiveness  tutorial instruction is potentially a powerful knowledge source for intelligent agents  instructo soar illustrates this in a simple
domain  realizing instruction s potential in fielded applications will require more linguistically able agents that incorporate robust techniques for not only acquiring knowledge from
instruction  but also refining that knowledge as needed based on performance and further
instruction 

acknowledgements
this work was performed while the first author was a graduate student at the university of
michigan  it was sponsored by nasa onr under contract ncc        and by a university
of michigan predoctoral fellowship  thanks to paul rosenbloom  randy jones  and our
anonymous reviewers for helpful comments on earlier drafts 

references

akatsuka  n          conditionals are discourse bound  in traugott  e  c   ed    on
conditionals  pp          cambridge univ  press  cambridge 
alterman  r   zito wolf  r     carpenter  t          interaction  comprehension  and
instruction usage  journal of the learning sciences                   
anderson  j  r          the architecture of cognition  harvard university press  cambridge 
ma 
   

fihuffman   laird

bergadano  f     giordana  a          a knowledge intensive approach to concept induction  in proceedings of the international conference on machine learning  pp 
        
birmingham  w     klinker  g          knowledge acquisition tools with explicit problemsolving methods  the knowledge engineering review        
birmingham  w     siewiorek  d          automated knowledge acquisition for a computer
hardware synthesis system  knowledge acquisition             
bloom  b  s          the   sigma problem  the search for methods of group instruction as
effective as one to one tutoring  educational researcher               
brachman  r  j          an introduction to kl one  in brachman  r  j   ed    research
in natural language understanding  pp         bolt  beranek and newman inc  
cambridge  ma 
carbonell  j  g     gil  y          learning by experimentation  in proceedings of the
international workshop on machine learning  pp          
carbonell  j  g   michalski  r  s     mitchell  t  m          an overview of machine
learning  in michalski  r  s   carbonell  j  g     mitchell  t  m   eds    machine
learning  an artificial intelligence approach  morgan kaufmann 
carpenter  t     alterman  r          a reading agent  in proceedings of the twelfth
national conference on artificial intelligence seattle  wa 
chapman  d          vision  instruction  and action  ph d  thesis  massachusetts institute
of technology  artificial intelligence laboratory 
chi  m  t  h   bassok  m   lewis  m  w   reimann  p     glaser  r          selfexplanations  how students study and use examples in learning to solve problems 
cognitive science              
cypher  a   ed            watch what i do  programming by demonstration  mit press 
cambridge  mass 
davis  r          interactive transfer of expertise  acquisition of new inference rules 
artificial intelligence                  
dejong  g  f     mooney  r  j          explanation based learning  an alternative view 
machine learning                 
dent  l   boticario  j   mcdermott  j   mitchell  t     zabowski  d          a personal
learning apprentice  in proceedings of the international joint conference on artificial
intelligence 
dieugenio  b          understanding natural language instructions  a computational approach to purpose clauses  ph d  thesis  university of pennsylvania  ircs report
      
   

fiflexibly instructable agents

dieugenio  b     webber  b          plan recognition in understanding instructions  in
hendler  j   ed    proceedings of the first international conference on artificial intelligence planning systems  pp        college park  md 
donoho  s  k     wilkins  d  c          exploiting the ordering of observed problem solving
steps for knowledge ase refinement  an apprenticeship approach  in proceedings of
the   th national conference on artifical intelligence seattle  wa 
drummond  m          situated control rules  in proceedings of the first international conference on principles of knowledge representation toronto  canada  morgan kaufmann 
emihovich  c     miller  g  e          talking to the turtle  a discourse analysis of logo
instruction  discourse processes              
eshelman  l   ehret  d   mcdermott  j     tan  m          mole  a tenacious knowledgeacquisition tool  international journal of man machine studies                
fikes  r  e   hart  p  e     nilsson  n  j          learning and executing generalized robot
plans  artificial intelligence             
ford  c  a     thompson  s  a          conditionals in discourse  a text based study
from english  in traugott  e  c   ed    on conditionals  pp          cambridge
univ  press  cambridge 
frederking  r  e          integrated natural language dialogue  a computational model 
kluwer academic press  boston 
golding  a   rosenbloom  p  s     laird  j  e          learning search control from outside
guidance  in proceedings of the tenth international joint conference on artificial
intelligence  pp          
grosz  b  j          the representation and use of focus in dialogue understanding  ph d 
thesis  university of california  berkeley 
gruber  t          automated knowledge acquisition for strategic knowledge  machine
learning                   
guha  r  v     lenat  d  b          cyc  a mid term report  ai magazine                
haas  n     hendrix  g  g          learning by being told  acquiring knowledge for
information management  in michalski  r  s   carbonell  j  g     mitchell  t  m 
 eds    machine learning  an artificial intelligence approach  morgan kaufmann 
haiman  j          conditionals are topics  language             
hall  r  j          learning by failing to explain  machine learning               
hayes roth  f   klahr  p     mostow  d  j          advice taking and knowledge refinement 
an iterative view of skill acquisition  in anderson  j  r   ed    cognitive skills and
their acquisition  pp           lawrence erlbaum associates  hillsdale  nj 
   

fihuffman   laird

huffman  s  b          instructable autonomous agents  ph d  thesis  university of michigan  dept  of electrical engineering and computer science 
huffman  s  b     laird  j  e          dimensions of complexity in learning from interactive
instruction  in erickson  j   ed    proceedings of cooperative intelligent robotics in
space iii  spie volume      
huffman  s  b     laird  j  e          learning procedures from interactive natural language instructions  in utgoff  p   ed    machine learning  proceedings of the tenth
international conference 
huffman  s  b     laird  j  e          learning from highly exible tutorial instruction 
in proceedings of the   th national conference on artificial intelligence  aaai    
seattle  wa 
huffman  s  b   miller  c  s     laird  j  e          learning from instruction  a knowledgelevel capability within a unified theory of cognition  in proceedings of the fifteenth
annual conference of the cognitive science society  pp          
johnson laird  p  n          conditionals and mental models  in traugott  e  c   ed   
on conditionals  cambridge univ  press  cambridge 
jones  r  m   tambe  m   laird  j  e     rosenbloom  p  s          intelligent automated agents for ight training simulators  in proceedings of the third conference
on computer generated forces  pp        orlando  fl 
just  m  a     carpenter  p  a          verbal comprehension in instructional situations  in
klahr  d   ed    cognition and instruction  lawrence erlbaum associates  hillsdale 
nj 
kieras  d  e     bovair  s          the role of a mental model in learning to operate a
device  cognitive science             
kodratoff  y     tecuci  g       a   disciple    interactive apprentice system in weak
theory fields  in proceedings of the tenth international joint conference on artificial
intelligence  pp          
kodratoff  y     tecuci  g       b   techniques of design and disciple learning apprentice  international journal of expert systems               
laird  j  e   congdon  c  b   altmann  e     doorenbos  r          soar user s manual 
version    
laird  j  e   hucka  m   yager  e  s     tuck  c  m          correcting and extending
domain knowledge using outside guidance  in proceedings of the seventh international
conference on machine learning 
laird  j  e   newell  a     rosenbloom  p  s          soar  an architecture for general
intelligence  artificial intelligence               
   

fiflexibly instructable agents

laird  j  e     rosenbloom  p  s          integrating execution  planning  and learning in
soar for external environments  in proceedings of the eighth national conference on
artificial intelligence  pp             aaai press 
lewis  c          why and how to learn why  analysis based generalization of procedures 
cognitive science              
lewis  r  l          an architecturally based theory of human sentence comprehension 
ph d  thesis  carnegie mellon university  school of computer science 
lewis  r  l   newell  a     polk  t  a          toward a soar theory of taking instructions for immediate reasoning tasks  in proceedings of the annual conference of the
cognitive science society 
lindsay  r  k          inferential memory as the basis of machines which understand natural
language  in feigenbaum  e  a     feldman  j   eds    computers and thought  pp 
         r  oldenbourg kg 
maes  p          agents that reduce work and information overload  communications of
the acm         
maes  p     kozierok  r          learning interface agents  in proceedings of the national
conference on artificial intelligence  pp          
mann  w  c     thompson  s  a          rhetorical structure theory  toward a functional
theory of text organization  text                 
marcus  s     mcdermott  j          salt  a knowledge acquisition language for proposeand revise systems  artificial intelligence               
martin  c  e     firby  r  j          generating natural language expectations from a
reactive execution system  in proceedings of the thirteenth annual conference of the
cognitive science society  pp          
mccarthy  j          the advice taker  in minsky  m   ed    semantic information processing  pp           mit press  cambridge  mass 
miller  c  m          a model of concept acquisition in the context of a unified theory of
cognition  ph d  thesis  the university of michigan  dept  of computer science and
electrical engineering 
minton  s   carbonell  j  g   knoblock  c  a   kuokka  d  r   etzioni  o     gil  y         
explanation based learning  a problem solving perspective  artificial intelligence     
       
mitchell  t   caruana  r   freitag  d   mcdermott  j     zabowski  d          experience
with a learning personal assistant  communications of the acm         
mitchell  t  m   keller  r  m     kedar cabelli  s  t          explanation based generalization  a unifying view  machine learning    
   

fihuffman   laird

mitchell  t  m   mahadevan  s     steinberg  l  i          leap  a learning apprentice
system for vlsi design  in kodratoff  y     michalski  r  s   eds    machine learning 
an artificial intelligence approach  vol  iii  morgan kaufmann 
mooney  r  j          learning plan schemata from observation  explanation based learning for plan recognition  cognitive science              
mostow  d  j          learning by being told  machine transformation of advice into a
heuristic search procedure  in michalski  r  s   carbonell  j  g     mitchell  t  m 
 eds    machine learning  an artificial intelligence approach  morgan kaufmann 
musen  m  a          automated support for building and extending expert models  machine learning                   
newell  a          the knowledge level  ai magazine              
newell  a          unified theories of cognition  harvard university press  cambridge 
massachusetts 
newell  a   yost  g   laird  j  e   rosenbloom  p  s     altmann  e          formulating
the problem space computational model  in proceedings of the   th anniversary
symposium  school of computer science  carnegie mellon university 
pazzani  m       a   a computational theory of learning causal relationships  cognitive
science              
pazzani  m       b   learning to predict and explain  an integration of similarity based 
theory driven  and explanation based learning  journal of the learning sciences        
        
pearson  d  j     huffman  s  b          combining learning from instruction with recovery
from incorrect knowledge  in gordon  d     shavlik  j   eds    proceedings of the     
machine learning workshop on agents that learn from other agents 
pearson  d  j   huffman  s  b   willis  m  b   laird  j  e     jones  r  m          a
symbolic solution to intelligent real time control  ieee robotics and autonomous
systems              
pearson  d  j     laird  j  e          toward incremental knowledge correction for agents in
complex environments  in muggleton  s   michie  d     furukawa  k   eds    machine
intelligence  vol      oxford university press 
porter  b  w   bareiss  r     holte  r  c          concept learning and heuristic classification in weak theory domains  artificial intelligence                  
porter  b  w     kibler  d  f          experimental goal regression  a method for learning
problem solving heuristics  machine learning             
redmond  m  a          learning by observing and understanding expert problem solving 
ph d  thesis  georgia institute of technology 
   

fiflexibly instructable agents

rosenbloom  p  s     aasman  j          knowledge level and inductive uses of chunking
 ebl   in proceedings of the national conference on artificial intelligence 
rosenbloom  p  s     laird  j  e          mapping explanation based generalization onto
soar  in proceedings of the national conference on artificial intelligence  pp          
rosenbloom  p  s   laird  j  e     newell  a          the chunking of skill and knowledge 
in bouma  h     elsendoorn  a  g   eds    working models of human perception 
pp           academic press  london  england 
rosenbloom  p  s   laird  j  e     newell  a   eds         a   the soar papers  research
on integrated intelligence  mit press  cambridge  mass 
rosenbloom  p  s   laird  j  e     newell  a   eds         b   the soar papers  research
on integrated intelligence  mit press  cambridge  mass 
rosenbloom  p  s     newell  a          the chunking of goal hierarchies  a generalized
model of practice  in michalski  r  s   carbonell  j  g     mitchell  t  m   eds   
machine learning  an artificial intelligence approach  volume ii  morgan kaufmann 
rumelhart  d  e     mcclelland  j  l   eds            parallel distributed processing  explorations in the microstructure of cognition  mit press  cambridge  ma 
rychener  m  d          the instructible production system  a retrospective analysis  in
michalski  r  s   carbonell  j  g     mitchell  t  m   eds    machine learning  an
artificial intelligence approach  pp           morgan kaufmann 
sandberg  j     wielinga  b          how situated is cognition   in proceedings of the
international joint conference on artificial intelligence  pp          
schank  r  c          conceptual information processing  american elsevier  new york 
schank  r  c     leake  d  b          creativity and learning in a case based explainer 
artificial intelligence              
segre  a  m          a learning apprentice system for mechanical assembly  in third ieee
conference on artificial intelligence for applications  pp          
shen  w          discovery as autonomous learning from the environment  machine learning              
simon  h  a          artificial intelligence systems that understand  in proceedings of the
fifth international joint conference on artificial intelligence  pp            
simon  h  a     hayes  j  r          understanding complex task instructions  in klahr 
d   ed    cognition and instruction  lawrence erlbaum associates  hillsdale  nj 
singley  m  k     anderson  j  r          the transfer of cognitive skill  harvard university
press 
   

fihuffman   laird

sutton  r  s     pinette  b          the learning of world models by connectionist networks 
in proceedings of the seventh annual conference of the cognitive science society  pp 
      
thrun  s  b     mitchell  t  m          integrating inductive neural network learning and
explanation based learning  in proceedings of the international joint conference on
artificial intelligence  pp          
vanlehn  k          learning one subprocedure per lesson  artificial intelligence         
     
vanlehn  k   ball  w     kowalski  b          explanation based learning of correctness 
towards a model of the self explanation effect  in proceedings of the   th annual
conference of the cognitive science society  pp          
vanlehn  k     jones  r          learning physics via explanation based learning of correctness and analogical search control  in proceedings of the international machine
learning workshop 
vanlehn  k   jones  r  m     chi  m  t  h          a model of the self explanation effect 
journal of the learning sciences              
vere  s     bickmore  t          a basic agent  computational intelligence           
wertsch  j  v          from social interaction to higher psychological processes  a clarification and application of vygotsky s theory  human development           
widmer  g          a tight integration of deductive and inductive learning  in proceedings
of the international workshop on machine learning  pp        
wilkins  d  c          knowledge base refinement as improving an incomplete and incorrect
domain theory  in kodratoff  y     michalski  r  s   eds    machine learning  an
artificial intelligence approach  volume iii  pp           morgan kaufmann 
winograd  t          understanding natural language  academic press  new york 
wood  d   bruner  j  s     ross  g          the role of tutoring in problem solving  journal
of child psychology and psychiatry             
yost  g  r          acquiring knowledge in soar  ieee expert               
yost  g  r     newell  a          a problem space approach to expert system specification 
in proceedings of the international joint conference on artificial intelligence  pp 
      

   

fi