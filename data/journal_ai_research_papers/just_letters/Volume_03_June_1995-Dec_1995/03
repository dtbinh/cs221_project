journal artificial intelligence research                 

submitted       published      

rule based machine learning methods functional
prediction

sholom m  weiss

weiss cs rutgers edu

nitin indurkhya

nitin cs usyd edu au

department computer science  rutgers university
new brunswick  new jersey        usa
department computer science  university sydney
sydney  nsw       australia

abstract

describe machine learning method predicting value real valued function  given values multiple input variables  method induces solutions
samples form ordered disjunctive normal form  dnf  decision rules  central
objective method representation induction compact  easily interpretable
solutions  rule based decision model extended search eciently similar cases prior approximating function values  experimental results real world data
demonstrate new techniques competitive existing machine learning
statistical methods sometimes yield superior regression performance 

   introduction
problem approximating values continuous variable described statistical literature regression  given samples output  response  variable input
 predictor  variables x   fx     xn g  regression task find mapping   f x   relative space possibilities  finite samples far complete  predefined
model needed concisely map x y  accuracy prediction  i e  generalization new
cases  primary concern  regression differs classification output variable regression problems continuous  whereas classification strictly categorical 
perspective  classification thought subcategory regression 
machine learning researchers emphasized connection describing regression
 learning classify among continuous classes   quinlan        
traditional approach problem classical linear least squares regression
 scheffe         developed refined many years  linear regression proven quite
effective many real world applications  clearly elegant computationally simple linear model limits  complex models may fit data better 
increasing computational power computers larger volumes data  interest grown pursuing alternative nonlinear regression methods  nonlinear regression
models explored statistics research community many new effective
methods emerged  efron         including projection pursuit  friedman   stuetzle 
      mars  friedman         methods nonlinear regression developed outside mainstream statistics research community  neural network trained
back propagation  mcclelland   rumelhart        one model  models
c      ai access foundation morgan kaufmann publishers  rights reserved 

fiweiss   indurkhya
found numerical analysis  girosi   poggio         overview many different
regression models  application classification  available literature  ripley 
       methods produce solutions terms weighted models 
real world  classification problems commonly encountered regression problems  accounts greater attention paid classification regression  many important problems real world regression type 
instance  problems involving time series usually involve prediction real values  besides
fact regression problems important own  another reason need
focus regression regression methods used solve classification problems 
example  neural networks often applied classification problems 
issue interpretable solutions important consideration leading
development  symbolic learning methods   popular format interpretable solutions
disjunctive normal form  dnf  model  weiss   indurkhya      a   decision trees
rules examples dnf models  decision rules similar characteristics decision
trees  potential advantages   a  stronger model  b  often better
explanatory capabilities  unlike trees  dnf rules need mutually exclusive  thus 
solution space includes tree solutions  rules potentially compact
predictive trees  decision rules may offer greater explanatory capabilities
trees tree grows size  interpretability diminishes 
among symbolic learning methods  decision tree induction  using recursive partitioning  highly developed  many methods developed within machine learning
community  id  decision tree induction  quinlan         applied exclusively classification tasks  less widely known decision trees effective
regression  cart program  developed statistical research community  induces classification regression trees  breiman  friedman  olshen    stone        
regression trees strictly binary trees  representation naturally follows
intensive modeling using continuous variables  
terms performance  regression trees often competitive performance
regression methods  breiman et al          regression trees noted particularly
strong many higher order dependencies among input variables  friedman 
       advantages regression tree model similar advantages enjoyed
classification trees models  two principal advantages cited   a  dynamic
feature selection  b  explanatory capabilities  tree induction methods extremely
effective finding key attributes high dimensional applications  applications 
key features small subset original feature set  another characteristic
decision trees often cited capability explanation terms acceptable
people  negative side  decision trees cannot represent compactly many simple
functions  example linear functions  second weakness regression tree
model discrete  yet predicts continuous variable  function approximation 
expectation smooth continuous function  decision tree provides discrete regions
discontinuous boundaries  though  regression trees often produce
strong results  many applications advantages strongly outweigh potential
disadvantages 
   comparative study  fayyad   irani        suggests binary classification trees somewhat
predictive even categorical variables 

   

firule based functional prediction
paper describe new method inducing regression rules  method
takes advantage close relationship classification regression provides
uniform general model dealing problems  additional gains obtained
extending method manner preserves strengths partitioning
schemes compensating weaknesses  rules used search
relevant cases  subset cases help determine function value  thus 
model s interpretability traded better performance  empirical
results suggest methods effective induce solutions often
superior decision trees 

   measuring performance

objective regression minimize distance sample output values 
yi predicted values yi   two measures distance commonly used  classical
regression measure equation    average squared distance yi yi   i e 
variance  leads elegant formulation linear least squares model  mean
absolute distance  deviation  equation   used least absolute deviation regression 
perhaps intuitive measure 
mean absolute distance  deviation  equation   used studies 
measure average error prediction yi n cases 
 

 

xn  yi   yi  
i  
n
x
mad     jyi   yi j

v ariance   n 

 

   

   
n i  
regression problem sometimes described signal noise problem  model
extended include stochastic component equation    thus  true function may
produce zero error distance  contrast classification labels assumed
correct  regression predicted values could explained number factors
including random noise component    signal  y 
 

  f  x        xn   
   
prediction primary concern  estimates based training cases alone
inadequate  principles predicting performance new cases analogous
classification  mean absolute distance used error rate  best
estimate true performance model error rate large set independent test
cases  large samples data unavailable  process train test simulated
random resampling  experiments  used     fold  cross validation
estimate predictive performance 

   regression tree induction

section  contrast regression tree induction classification tree induction 
classification trees  regression trees induced recursive partitioning  solution takes
   

fiweiss   indurkhya
form equation    ri disjoint regions  ki constant values  yji refers
y values training cases fall within region ri  

x ri f  x    ki   medianfyji g
   
regression trees representation classification trees except terminal nodes  decision terminal node assign case constant value 
single best constant value median training cases falling terminal node
partition  median minimizer mean absolute distance  figure  
example binary regression tree  cases reaching shaded terminal node    x   
assigned constant value y    
 
x    

x   

 

 

y   

x    

x   

 

 

y  

y  

figure    example regression tree
tree induction methods usually proceed  a  finding covering set training
cases  b  pruning tree best size  although classification trees
widely studied  similar approach applied regression trees  assume
reader familiar classification trees  cite differences binary
tree induction  breiman et al         quinlan        weiss   kulikowski         many
respects  regression tree induction straightforward  classification trees  error
rate poor choice node splitting  alternative functions entropy gini
employed  regression tree induction  minimized function  i e  absolute distance 
satisfactory  node  single best split minimizes mean absolute
distance selected  splitting continues fewer minimum number cases
covered node  cases within node identical value y 
goal find tree generalizes best new cases  often
full covering tree  particularly presence noise weak features  pruning strategies
employed classification trees equally valid regression trees  covering
procedures  substantial difference error rate measured terms
mean absolute distance  one popular method weakest link pruning strategy  breiman
et al          weakest link pruning  tree recursively pruned ratio delta n
minimized  n number pruned nodes delta increase error 
   

firule based functional prediction
x     y   
x     y  
otherwise y  
figure    example regression rules
weakest link pruning several desirable characteristics   a  prunes training cases
only  remaining test cases relatively independent  b  compatible
resampling 

   regression rule induction
tree rule induction models find solutions disjunctive normal form  model
equation   applicable both  rule rule set represents single partition
region ri   however  unlike tree regions  regions rules need disjoint 
non disjoint regions  several rules may satisfied single sample  mechanism
needed resolve con icts ki   constant values assigned  multiple rules 
ri regions  invoked  one standard model  weiss   indurkhya      a  order
rules  ordered rule sets referred decision lists  first rule
satisfied selected  equation   

  j x ri rj f  x    ki
   
figure   example ordered rule set corresponding tree figure   
cases satisfying rule    rules      assigned value y   
given model regression rule sets  problem find procedures effectively
induce solutions  rule based regression  covering strategy analogous classification tree strategy could specified  rule could induced adding single component
time  added component single best minimizer distance  usual 
constant value ki median region formed current rule  rule
extended  fewer cases covered  fewer minimal number cases covered 
rule extension terminates  covered cases removed rule induction continue
remaining cases  regression analogue rule induction procedures
classification  michalski  mozetic  hong    lavrac        clark   niblett        
however  instead approach  propose novel strategy mapping regression
covering problem classification problem 

    reformulation regression problem

motivation mapping regression classification based number factors
related extra information given regression problem  natural ordering yi
magnitude    j yi   yj  
let fci g set consisting arbitrary number classes  class containing
approximately equal values fyi g  solve classification problem  expect
classes different other  patterns found distinguish
   

fiweiss   indurkhya
   generate set pseudo classes using p class algorithm  figure    
   generate covering rule set transformed classification
problem using rule induction method swap  
 weiss   indurkhya      a  
   initialize current rule set covering rule set save it 
   current rule set pruned  iteratively following 
a  prune current rule set 
b  optimize pruned rule set  figure    save it 
c  make pruned rule set new current rule set 
   use test cases cross validation pick best saved rule sets 
figure    overview method learning regression rules
classes  expect classes formed ordering fyi g reasonable classification problem  numbers reasons answer yes  particularly
rule induction procedure 
obvious situation classical linear relationship  instance 
definition  ordering fx i       xni g corresponds ordering yi   although classical
methods strong compactly determining linear functions  interest modern
methods centers around potential finding nonlinear relationships  nonlinear
functions  know usually ordering fx i       xni g corresponding
fyig  still  expect true function smooth  local region ordering
relationship hold  terms classification  know class cj similar values
quite different class ck much lower values y  nonlinear function
within class similar values y  similar values fx i       xni g 
correspond local region function  however  true
identical values different fx i       xni g multiple clusters
found within class  rule induction methods cover class single
rule  expectation multiple patterns found cover clusters 
cases assigned  pseudo  classes  classification problem
solved following stages   a  find covering set  b  prune rule set
appropriate size  improved results achieved additional technique considered 
 c  refine optimize rule set  overall method outlined figure   

    generating pseudo classes
previous section  described motivation pseudo classes  specification
classes use information beyond ordering y  assumptions
true nature underlying function made  within environment 
goal make values within one class similar values across classes
dissimilar  wish assign values classes overall distance
yi class mean minimum 
   

firule based functional prediction
input  fyi g set output values
initialize n    number cases  k    number classes
classi
classi    next n k cases list sorted values
end for
compute errnew
repeat
errold   errnew
casej
classi
   dist casej   mean classi        dist casej   mean classi   
move casej classi  
   dist casej   mean classi        dist casej   mean classi   
move casej classi  
next casej
compute errnew
errnew less errold
figure    composing pseudo classes  p class 
figure   describes algorithm  p class  assigning values fyi g k classes  essentially algorithm following   a  sorts values   b  assigns approximately
equal numbers contiguous sorted yi class   c  moves yi contiguous class
reduces global distance err yi mean assigned class 
classes identical means merged  p class variation k means clustering  statistical method minimizes distance measure  hartigan   wong        
alternative methods depend distance measures  lebowitz        may
used 
given fixed number k classes  procedure relatively quickly assign yi
classes overall distances minimized  underlying function
unknown  critical global minimum assignment yi  procedure
matches well stated goals ordering yi values  obvious remaining question
determine k  number classes  unfortunately  direct answer 
experimentation necessary  however  shall see section    empirical
evidence suggesting results quite similar within local neighborhood values
k  moreover  relatively large values k  entail increased computational complexity
rule induction  typically necessary noise free functions modeled
exactly  analogous comparisons neural nets increasing numbers hidden units 
trends increasing numbers partitions become evident experimentation 
one additional variation classification theme arises rule induction schemes
cover one class time  classes must ordered  last class typically
   

fiweiss   indurkhya
becomes default class cover situations rule classes satisfied 
regression  one default partition class unlikely best covering solution 
instead remaining cases last class repeatedly partitioned  by p class 
  classes fewer cases remain 
interesting characteristic transformation regression problem
uniform general model relates classification
regression  yi values discrete categorical  p class merely restates standard
classification problem  example  values yi either      result
p class   non empty classes 

    covering rule set
transformation  rule induction algorithms classification applied 
consider induction methods fully cover class moving induce
rules next class  step covering algorithm  problem considered
binary classification problem current class ci versus cj j   i  i e 
current class versus remaining classes  rule induced  corresponding cases
removed remaining cases considered  class covered 
next class considered  example covering algorithm used swap  
 weiss   indurkhya      a   procedure used paper  covering
method identical classification regression  however  one distinction
regression classes transient labels replaced median values
cases covered induced rule  rules ordered multiple rules
may satisfied  medians derived instances rule
first satisfied 
although procedure may yield good  compact covering sets  additional procedures
necessary complete solution 

    pruning rule set
typical real world applications noisy features fully predictive  covering
set  particularly one composed many continuous variables  far over specialized
produce best results  classification  relatively classes specified advance 
regression  expect many smaller groups values yi likely quite
different 
noted earlier regression trees usual classification pruning techniques
applied substitution mean absolute distance classification error rate 
weakest link tree pruning  ratio delta n recursively minimized
weakest link rule pruning  intuitive rationale remove parts rule set
least impact increasing error  pruning rule sets usually accomplished
either deleting complete rules single rule components  quinlan        weiss   indurkhya 
    a   general  rule pruning  for classification regression  less natural
far computationally expensive tree pruning  tree pruning natural ow
set subset  thus tree pruned bottom up  typically considering
effect removing subtree  non disjoint rules natural pruning order 
   

firule based functional prediction
example every component rule candidate pruning may affect rules
follow specified rule order 
major difference pruning regression rules vs  classification rules 
classification  deleting rule rule component effect class labels 
regression  pruning change median values regions  even deletion
rule affect region medians rules ordered multiple rules may
satisfied  characteristic rule pruning regression adds substantial complexity
task  however  assuming median values remain unchanged
evaluation candidate rules prune  pruning procedure achieve reasonable
computational eciency expense loss accuracy evaluation 
best rule component deletion selected  medians regions
re evaluated 
even classification rules  rule pruning inherent weaknesses  example 
rule deletion often create gap coverage  classification rules though  quite
feasible develop additional procedure refine optimize rule set  large
extent  overcomes cited weakness pruned rules sets  similar refinement
optimization procedure developed regression described next 

    rule refinement optimization

given rule set rsi   improved  question applies rule set  although
mostly motivated trying improve pruned rules sets frso       rsi       rsng 
combinatorial optimization problem  using error measure err rs   improve
rsi without changing size  i e  number rules components  figure   describes
algorithm minimizes err rs   mad model prediction sample cases 
local swapping  i e  replacing single rule component best alternative 
variation techniques used swap    weiss   indurkhya      a  
central theme hold model configuration constant make single local
improvement configuration  local modifications made improvements possible  making local changes configuration widely used optimization
technique approximate global optimum applied quite successfully 
example find near optimum solutions traveling salesman problems  lin   kernighan 
       analogous local optimization technique  called backfitting  used
context nonlinear statistical regression  hastie   tibshirani        
variations selection next improvement move could include 
   first local improvement encountered  such backfitting 
   best local improvement  such swap   
experiments rule induction methods  results consistently better
         ecient   pruned  rule induction environment mostly stable
relatively local improvements prior convergence  less stable environment 
large numbers possible configuration changes      may feasible even better 
pruned rule set environment  covering procedure effective  pruned
solution relatively close local minimum solution  weakest link pruning
   

fiweiss   indurkhya
input  rs rule set consisting rules ri  
set training cases
   true
 d true 
rsnew    rs single best replacement
component rs reduces err rs 
cases using current median ri  
replacement found
   false
else
rs    rsnew   recompute medians ri  
endwhile
return rule set rs
figure    optimization rule component swapping
results series pruned rule sets rsi number far fewer sets would
result single prune rule rule component  rsi optimized prior
continuing pruning process  however  rule set optimization usually suspended
substantial segments covering set already pruned 
    used  either sequentially ordered evaluations  as backfitting  stochastic evaluations considered  empirical evidence optimization literature supports superiority stochastic evaluation  jacoby  kowalik    pizzo        
improvements may obtained occasionally making random changes configuration
 kirpatrick  gelatt    vecchi         general combinatorial optimization techniques must substantially reworked fit specific problem type  expected
applied throughout problem solving 
result pruning covering rule set  rso   series progressively smaller rule
sets frso       rsi       rsn g  objective pick best one  usually form
error estimation  model complexity future performance highly related 
complex simple model yield poor results  objective find
right size model  independent test cases resampling cross validation effective
estimating future performance  absence estimates  approximations 
gcv  craven   wahba        friedman         described equation   
used statistics literature estimate performance   measures training error
model complexity used estimates  c m   measure model complexity
expressed terms parameters estimated  such number weights neural net 
tests performed  c m  assumed less n  number cases 
   gcv acronym generalized cross validation  apparent error training cases used
true cross validation resampling 

   

firule based functional prediction

xn
gcv  m    

jy  y j
 





n
c m 
i       n

   

experiments used cross validated estimates guide final model selection
process  measures gcv may used 

    potential problems rule based regression

regression rules  trees  induced recursive partitioning methods approximate function constant value regions  relatively strong dynamic feature
selection high dimensional applications  sometimes using highly predictive
features  essential weakness methods approximation partition
region constant value  continuous function even moderately sized sample 
approximation lead increased error 
deal limitation  instead constant value functions  linear functions
substituted partition  quinlan         however  linear function obvious
weakness true function may far linear even restricted context
single region  general  use linearity compromises highly non parametric
nature dnf model  better strategy might examine alternative non linear
methods 

   alternative rules  k nearest neighbors
k nearest neighbor method one simplest regression methods  relying table
lookup  classify unknown case x  k cases closest new case
found sample data base stored cases  predicted y x  equation   mean
values k nearest neighbors  nearest neighbors found distance
metric euclidean distance  usually feature normalization   method
non parametric highly non linear nature

yknn x    k 

xk yk k nearest neighbours x

k  

   

major problem approach limit effect irrelevant features 
limited forms feature selection sometimes employed preprocessing stage 
method cannot determine features weighted others 
result  procedure sensitive distance measure used  high dimensional
feature space  k nearest neighbor methods may perform poorly  limitations
precisely partitioning methods address  thus  theory  two methods
potentially complement one another 

   model combination

practice  one learning model always superior others  learning strategy
examines results different models may better  moreover  combining
   

fiweiss   indurkhya
different models  enhanced results may achieved  general approach combining
learning models scheme referred stacking  wolpert         additional studies
performed applying scheme regression problems  breiman        leblanc  
tibshirani         using small training samples simulated data  linear combinations
regression methods  improved results reported  let mi i th model trained
sample  wi   weight given mi    new case vector
x  predictions different models combined equation   produce
estimate y  models may use representation  k nearest neighbors
variable size k  perhaps variable size decision trees  models could completely
different  combining decision trees linear regression models  different models
applied independently find solutions  later weighted vote taken reach
combined solution  method model combination contrast usual approach
evaluation different models  single best performing model selected 

y 

xk wkmk x 

k  

   

stacking shown give improved results simulated data  major
drawback properties combined models retained  thus interpretable models combined  result may interpretable all 
possible compensate weaknesses one model introducing another model
controlled fashion 
suggested earlier  partitioning regression methods k nearest neighbor regression
methods complementary  hence one might expect suitably combining two
methods  one might obtain better performance  one recent study  quinlan         model
trees  i e   regression trees linear combinations leaf nodes  nearest neighbor
methods combined  combination method described equation   
n  x k one k nearest neighbors x  v x  y value stored instance
x  t x  result applying model tree x 

  k 

xk v  n  x k     t  n  x k     x  

k  

   

k nearest neighbors found independently induced regression tree  results
reported k     sense  approach similar combination method
equation    k nearest neighbors passed tree  results used
refine nearest neighbor answer  thus  combination model formed
independently computing global solution  later combining results 
however  strong reasons determining global nearest neighbor solution independently  while  limit  large samples  non parametric k nearest
neighbor methods correctly fit function  practice though  weaknesses
substantial  finding effective global distance measure may easy  particularly
presence many noisy features  hence different technique combining two
methods needed 
   weights obtained minimize least squared error constraints  breiman 
      

   

firule based functional prediction

    integrating rules table lookup

consider following strategy  determine y value case x falls region ri  
instead assigning single constant value ki region ri   ki determined
 x   mean k nearest
median value training cases region  assign yknn
 training set  instances x region ri   thus regression trees  equation
    regression rules  equation    
 x 
x ri f  x    yknn

    

 x 
  j x ri rj f  x    yknn

    

interesting aspect strategy k nearest neighbor results need
considered cases covered particular partition  increases interaction models eliminates independent computation two models 
model rationale and  shall show  empirical results  supportive
approach 
representation potentially alleviates weakness partitions
assigned single constant values  moreover  global distance measure difficulties k nn methods may relieved table lookup reduced
partitioned related groupings 
rationale hybrid partition k nn scheme  note unlike stacking 
hybrid models independently determined  interact strongly one
another  however  must demonstrated methods fact complementary 
preserving strengths partitioning schemes compensating weaknesses
would introduced constant values used region  respect model
combination  two principal questions need addressed empirical experimentation 

results improved relative using model alone 
methods competitive alternative regression methods 

   results
experiments conducted assess competitiveness rule based regression compared
procedures  including less interpretable ones   well evaluate performance integrated partition k nn regression method  experiments performed
using seven datasets  six described previous studies  quinlan         addition six datasets  new experiments done large telecommunications
application  labeled pole  seven datasets  one continuous
real valued response variable  experimental results reported terms mad 
measured using    fold cross validation  pole        cases used training
       independent testing  features different datasets mixture
continuous categorical features  pole     features continuous  descriptions
   

fiweiss   indurkhya

dataset cases vars
price
servo
cpu
mpg
peptide
housing
pole

   
   
   
   
   
   
     

  
  
 
  
   
  
  

table    dataset characteristics
datasets found literature  quinlan          table   summarizes
key characteristics datasets used study 
table   summarizes original results reported  quinlan         include modeltrees  mt   regression trees linear fits terminal nodes  neural nets
 nnet     nearest neighbors    nn   combined results model trees   nearest
neighbors  mt   nn   
table   summarizes additional results obtained  include cart
regression tree  rt     nearest neighbors euclidean distance    nn   rule regression
using swap    rule regression   nn applied rule region  rule   nn   mars 
  nn used expectation nearest neighbor method incrementally
improves constant value region region moderately large sample neighbors
average 
rule based method  parameter m  number pseudo classes  must
determined  found using cross validation independent test cases  in
experiments  cross validation used   figure   represents typical plot relative
error vs  number pseudo classes  weiss   indurkhya      b   number
partitions increases  results improve reach relative plateau deteriorate
somewhat  similar complexity plots found models  example neural nets
 weiss   kapouleas        
mars procedure several adjustable parameters   parameter mi  values
tried    additive modeling           number inputs  df  default value
    tried well optimal value estimated cross validation  parameter nk
varied        steps     lastly  piece wise linear well piece wise cubic
solutions tried  setting parameters  cross validated
accuracy monitored  value best mars model reported 
method  besides mad  relative error reported  relative
error simply estimated true mean absolute distance  measured cross validation 
normalized initial mean absolute distance median  analogous classifi   peptide dataset slightly modified version one quinlan refers lhrh att paper 
version used experiments  cases missing values removed 
   peptide slightly modified version lhrh att dataset  result listed one
provided quinlan personal communication 
   particular program used mars     

   

firule based functional prediction

relative error
    

   

    

   

    

   

    

   
 

 

 

 
 
 
number pseudo classes

 

 

  

figure    prototypical performance varying pseudo classes

dataset mt nnet   nn mt   nn
price     
servo
   
cpu
    
mpg
    
peptide    
housing     

    
   
    
    
    

    
   
    
    
    

    
   
    
    
    

table    previous results
cation  predictions must fewer errors simply predicting largest class 
regression must better average distance median
meaningful results 
comparing performance two methods dataset  standard error
method independently estimated  larger one used comparisons 
difference performance greater   standard errors  difference
considered statistically significant  significance test  one must consider
overall pattern performance relative advantages competing solutions  weiss
  indurkhya        
dataset  figure   plots relative best error found ratio best
reported result model s result  relative best error   indicates result
best reported result regression model  model results compared
best results regression rules    nn  mixed model  graph indicates
   

fiweiss   indurkhya

dataset

rt

  nn

rule

rule   nn

mars

mad error mad error mad error mad error mad error
price
                                            
servo
                                            
cpu
                                               
mpg
                                            
peptide    
   
   
   
   
   
   
   
   
   
housing                                             
pole
                                            
table    performance additional methods
relative best erate
   
  nn
rule
 

rule   nn

   

   

   

   

 
servo

house

mpg

cpu

price

peptide

pole

figure    relative best erates   nn  rules  rule   nn
trends across datasets helps assess overall pattern performance  respect 
rule rule  nn exhibit excellent performance across many applications 
empirical results allow us consider several relevant questions regarding rulebased regression 
   rule based regression perform compared tree based regression  comparing
results rule rt  one see except servo  rule consistently
better rt remaining six datasets  difference performance
   

firule based functional prediction
tests significant  results significance tests  general trend  which
seen visually figure    leads us conclude rule based regression
definitely competitive trees often yields superior performance 
   integrating  nn rules lead improved performance relative using
model alone  comparison rule  nn  nn shows datasets  rule  nn
significantly better  comparing rule  nn rule  results indicate
three datasets  mpg  pole housing   rule  nn significantly better rule 
remaining three datasets same  overall pattern
performance appears favor rule  nn rule  thus empirical results
indicate method improved results relative using model alone 
general trend seen figure   
   new methods competitive alternative regression methods  among previous reported results  mt  nn best performer  alternatives consider
are  regression trees  rt  mars  none three methods significantly
better rule  nn datasets consideration except rt
significantly better servo  furthermore  rule  nn significantly better
mt  nn three five datasets  servo  cpu mpg  comparison possible  overall trend favor rule  nn  comparing rt rule  nn 
find except servo  rule  nn significantly better rt remaining datasets  comparing mars rule  nn  find three datasets
 price  peptide pole   rule  nn significantly better  hence empirical results overwhelmingly suggest new method competitive alternative
regression methods  hints superiority methods 

   discussion

considered new model rule based regression provided comparisons
tree based regression  many applications  strong explanatory capabilities high dimensional feature selection make dnf model quite advantageous  particularly
true knowledge based applications  example equipment repair medical diagnosis 
contrast pure pattern recognition applications speech recognition 
rules similar trees  rule representation potentially compact
rules mutually exclusive  potential finding compact
solution particularly important problems model interpretation crucial 
note space rules includes space trees  thus  tree solution
best  theoretically rule induction procedure potential find it 
experiments  regression rules generally outperformed regression trees 
fewer constant regions required estimated error rates generally lower 
finding dnf regions substantially computationally expensive regression rules regression trees  regression rules  fairly complex optimization
techniques necessary  addition  experiments must performed find appropriate number pseudo classes  matter scale  scale application
versus scale available computing  excluding telecommunications application 
none cited applications takes    minutes cpu time ss    sin   

fiweiss   indurkhya
gle pseudo classification problem full cross validation   computing power increases
timing distinction less important  even small percentage gain quite valuable appropriate application  apte  damerau    weiss        computational
requirements secondary factor 
provided results several real world datasets  mostly  involve nonlinear relationships  one may wonder rule based method would perform data
obvious linear relationships  earlier experiments data exhibiting linear
relationships  for example  drug study data  efron          rule based solutions
slightly better trees  however  true test real world data which  often involve
complex non linear relationships  comparisons alternative models help assess
effectiveness new techniques 
looking figure   tables      see pure rule based solutions
competitive models  additional gains made rules used
obtaining function values directly  instead used find relevant cases
used compute function value  results experiments support
view strategy combining different methods improve predictive performance 
strategies similar applied classification problems  ting       
widmer        similar conclusions drawn results  results indicate
strategy useful regression context too  empirical results support
contention regression  partitioning methods nearest neighbor methods
complementary  solution found partitioning alone  incremental
improvement observed substituting average k nearest neighbors
median partition  perspective nearest neighbor regression methods 
sample cases compartmentalized  simplifying table lookup new case 
conclusive  hints combination strategy effective
small moderate samples  likely sample size grows large  increased
numbers partitions  terms rules terminal nodes  compensate single
constant valued regions  conjecture supported large sample pole application 
incremental gain addition k nn small  
experiments used k nn k    depending application  different
value k might produce better results  optimal value might estimated crossvalidation strategy systematically varies k picks value gives best
results overall  however  unclear whether increased computational effort result
significant performance gain 
another practical issue large samples storage requirement  cases must
stored  serious drawback real world applications limited memory 
however  tried experiments cases associated partition replaced
fewer number  typical cases   results considerable savings terms storage
requirements  results slightly weaker  though significantly different  
would appear gains might obtained restricting k nn consider
features appear path leaf node examination  might
seem good idea attempts ensure features relevant
      fold cross validation requires solving problem essentially    times  training cases
   times group test cases 
   although small  difference tests significant sample large 

   

firule based functional prediction
cases node  used distance calculations  however  found results
weaker 
number regression techniques presented others demonstrate
advantages combined models  combine methods independently
invoked  instead typical election one winner  alternative models
combined weighted  combination techniques advantage
outputs different models treated independent variables  combined
form post processing  model outputs available 
way contradict value alternative combination techniques 
approaches show improved results various applications  conclude  however 
advantages complex regression procedures dynamically mix
alternative models  procedures may particularly strong fundamental rationale choice methods partitioning methods  properties
combined models must preserved 
presented regression problem one output variable  classical form linear models regression trees  issue multiple outputs
directly addressed although extensions feasible  issue experimentation await future work  model regression provide basis efforts 
leveraging current strong methods classification rule induction 

references

apte  c   damerau  f     weiss  s          automated learning decison rules text
categorization  acm transactions oce information systems                  
breiman  l          stacked regression  tech  rep   u  ca  berkeley 
breiman  l   friedman  j   olshen  r     stone  c          classification regression
tress  wadsworth  monterrey  ca 
clark  p     niblett  t          cn  induction algorithm  machine learning    
        
craven  p     wahba  g          smoothing noisy data spline functions  estimating
correct degree smoothing method generalized cross validation  numer 
math               
efron  b          computer intensive methods statistical regression  siam review 
                
fayyad  u     irani  k          attribute selection problem decision tree generation 
proceedings aaai     pp          san jose 
friedman  j          multivariate adaptive regression splines  annals statistics         
      
friedman  j     stuetzle  w          projection pursuit regression  j  amer  stat  assoc  
            
   

fiweiss   indurkhya
girosi  f     poggio  t          networks best approximation property  biological
cybernetics              
hartigan  j     wong  m          k means clustering algorithm  algorithm     
applied statistics         
hastie  t     tibshirani  r          generalized additive models  chapman hall 
jacoby  s   kowalik  j     pizzo  j          iterative methods non linear optimization
problems  prentice hall  new jersey 
kirpatrick  s   gelatt  c     vecchi  m          optimization simulated annealing 
science           
leblanc  m     tibshirani  r          combining estimates regression classification 
tech  rep   department statistics  u  toronto 
lebowitz  m          categorizing numeric information generalization  cognitive science             
lin  s     kernighan  b          ecient heuristic traveling salesman problem 
operations research                  
mcclelland  j     rumelhart  d          explorations parallel distributed processing 
mit press  cambridge  ma 
michalski  r   mozetic  i   hong  j     lavrac  n          multi purpose incremental learning system aq   testing application three medical domains 
proceedings aaai     pp            philadelphia  pa 
quinlan  j          induction decision trees  machine learning            
quinlan  j          simplifying decision trees  international journal man machine
studies              
quinlan  j          combining instance based model based learning  international
conference machine learning  pp          
ripley  b          statistical aspects neural networks  proceedings seminair europeen de statistique london  chapman hall 
scheffe  h          analysis variance  wiley  new york 
ting  k          problem small disjuncts  remedy decision trees  proceedings
  th canadian conference artificial intelligence  pp        
weiss  s     indurkhya  n       a   optimized rule induction  ieee expert               
weiss  s     indurkhya  n       b   rule based regression  proceedings   th
international joint conference artificial intelligence  pp            
   

firule based functional prediction
weiss  s     indurkhya  n          decision tree pruning  biased optimal   proceedings
aaai     pp          
weiss  s     kapouleas  i          empirical comparison pattern recognition  neural
nets  machine learning classification methods  international joint conference
artificial intelligence  pp          detroit  michigan 
weiss  s     kulikowski  c          computer systems learn  classification prediction methods statistics  neural nets  machine learning  expert systems 
morgan kaufmann 
widmer  g          combining knowledge based instance based learning exploit
qualitative knowledge  informatica              
wolpert  d          stacked generalization  neural networks             

   


