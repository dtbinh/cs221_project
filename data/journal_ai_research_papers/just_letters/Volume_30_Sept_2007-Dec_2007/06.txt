journal of artificial intelligence research                  

submitted       published      

probabilistic planning via heuristic forward search
and weighted model counting
carmel domshlak

dcarmel   ie   technion   ac   il

technion   israel institute of technology 
haifa  israel

jorg hoffmann

j oerg  h offmann   deri   at

university of innsbruck  deri 
innsbruck  austria

abstract
we present a new algorithm for probabilistic planning with no observability  our algorithm 
called probabilistic ff  extends the heuristic forward search machinery of conformant ff to problems with probabilistic uncertainty about both the initial state and action effects  specifically 
probabilistic ff combines conformant ffs techniques with a powerful machinery for weighted
model counting in  weighted  cnfs  serving to elegantly define both the search space and the
heuristic function  our evaluation of probabilistic ff shows its fine scalability in a range of probabilistic domains  constituting a several orders of magnitude improvement over previous results in
this area  we use a problematic case to point out the main open issue to be addressed by further
research 

   introduction
in this paper we address the problem of probabilistic planning with no observability  kushmerick 
hanks    weld         also known in the ai planning community as conditional  majercik  
littman        or conformant  hyafil   bacchus        probabilistic planning  in such problems
we are given an initial belief state in the form of a probability distribution over the world states w  
a set of actions  possibly  having probabilistic effects  and a set of alternative goal states wg  w  
a solution to such a problem is a single sequence of actions that transforms the system into one
of the goal states with probability exceeding a given threshold   the basic assumption of the
problem is that the system cannot be observed at the time of plan execution  such a setting is
useful in controlling systems with uncertain initial state and non deterministic actions  if sensing is
expensive or unreliable  non probabilistic conformant planning may fail due to non existence of a
plan that achieves the goals with      certainty  even if there is such a plan  that plan does not
necessarily contain information about what actions are most useful to achieve only the requested
threshold  
the state of the art performance of probabilistic planners has been advancing much more slowly
than that of deterministic planners  scaling from      step plans for problems with    world states
to       step plans for problems with     world states  kushmerick et al         majercik  
littman        hyafil   bacchus         since probabilistic planning is inherently harder than its
deterministic counterpart  littman  goldsmith    mundhenk         such a difference in evolution
rates is by itself not surprising  however  recent developments in the area  onder  whelan    li 
      bryce  kambhampati    smith        huang         and in particular our work here  show
that dramatic improvements in probabilistic planning can be obtained 
c
    
ai access foundation  all rights reserved 

fid omshlak   h offmann

in this paper we introduce probabilistic ff  a new probabilistic planner based on heuristic forward search in the space of implicitly represented probabilistic belief states  the planner is a natural
extension of the recent  non probabilistic  conformant planner conformant ff  hoffmann   brafman         the main trick is to replace conformant ffs sat based techniques with a recent
powerful technique for probabilistic reasoning by weighted model counting  wmc  in propositional cnfs  sang  beame    kautz         in more detail  conformant ff does a forward search
in a belief space in which each belief state corresponds to a set of world states considered to be
possible  the main trick of conformant ff is the use of cnf formulas for an implicit representation of belief states  implicit  in this context  means that formulas  a  encode the semantics of
executing action sequence a in the initial belief state  with propositional variables corresponding to
facts with time stamps  any actual knowledge about the belief states has to be  and can be  inferred
from these formulas  most particularly  a fact p is known to be true in a belief state if and only if
 a   p m   where m is the time endpoint of the formula  the only knowledge computed by
conformant ff about belief states are these known facts  as well as  symmetrically  the facts that
are known to be false  this suffices to do strips style planning  that is  to determine applicable
actions and goal belief states  in the heuristic function  ffs  hoffmann   nebel        relaxed
planning graph technique is enriched with approximate sat reasoning 
the basic ideas underlying probabilistic ff are 
 i  define time stamped bayesian networks  bns  describing probabilistic belief states 
 ii  extend conformant ffs belief state cnfs to model these bns 
 iii  in addition to the sat reasoning used by conformant ff  use weighted model counting to
determine whether the probability of the  unknown  goals in a belief state is high enough 
 iv  introduce approximate probabilistic reasoning into conformant ffs heuristic function 
note the synergetic effect  probabilistic ff re uses all of conformant ffs technology to recognize
facts that are true or false with probability    this fully serves to determine applicable actions  as
well as detect whether part of the goal is already known  in fact  it is as if conformant ffs cnfbased techniques were specifically made to suit the probabilistic setting  while without probabilities
one could imagine successfully replacing the cnfs with bdds  with probabilities this seems much
more problematic 
the algorithms we present cover probabilistic initial belief states given as bayesian networks 
deterministic and probabilistic actions  conditional effects  and standard action preconditions  our
experiments show that our approach is quite effective in a range of domains  in contrast to the
sat and csp based approaches mentioned above  majercik   littman        hyafil   bacchus 
       probabilistic ff can find     step plans for problem instances with billions of world states 
however  such a comparison is not entirely fair due to the different nature of the results provided  the
sat and csp based approaches provide guarantees on the length of the solution  the approach most
closely related to probabilistic ff is implemented in pond  bryce et al          this system  like
probabilistic ff  does conformant probabilistic planning for a threshold   using a non admissible 
planning graph based heuristic to guide the search  hence a comparison between probabilistic ff
and pond is fair  and in our experiments we perform a comparative evaluation of probabilistic ff
and pond  while the two approaches are related  there are significant differences in the search
   

fip robabilistic  ff

space representation  as well as in the definition and computation of the heuristic function   we run
the two approaches on a range of domains partly taken from the probabilistic planning literature 
partly obtained by enriching conformant benchmarks with probabilities  and partly obtained by
enriching classical benchmarks with probabilistic uncertainty  in almost all cases  conformant ff
outperforms pond by at least an order of magnitude  we make some interesting observations
regarding the behavior of the two planners  in particular we identify a domain  derived from the
classical logistics domain  where both approaches fail to scale  the apparent reason is that neither
approach is good enough at detecting how many times  at an early point in the plan  a probabilistic
action must be applied in order to sufficiently support a high goal threshold at the end of the plan 
devising methods that are better in this regard is the most pressing open issue in this line of work 
the paper is structured as follows  the next section provides the technical background  formally
defining the problem we address and illustrating it with our running example  section   details how
probabilistic belief states are represented as time stamped bayesian networks  how these bayesian
networks are encoded as weighted cnf formulas  and how the necessary reasoning is performed
on this representation  section   explains and illustrates our extension of conformant ffs heuristic function to the probabilistic settings  section   provides the empirical results  and section  
concludes  all proofs are moved into appendix a 

   background
the probabilistic planning framework we consider adds probabilistic uncertainty to a subset of
the classical adl language  namely  sequential  strips with conditional effects  such strips
planning tasks are described over a set of propositions p as triples  a  i  g   corresponding to the
action set  initial world state  and goals  i and g are sets of propositions  where i describes a
concrete initial state wi   while g describes the set of goal states w  g  actions a are pairs
 pre a   e a   of the precondition and the  conditional  effects  a conditional effect e is a triple
 con e   add e   del e   of  possibly empty  proposition sets  corresponding to the effects condition  add  and delete lists  respectively  the precondition pre a  is also a proposition set  and
an action a is applicable in a world state w if w  pre a   if a is not applicable in w  then
the result of applying a to w is undefined  if a is applicable in w  then all conditional effects
e  e a  with w  con e  occur  occurrence of a conditional effect e in w results in the world
state w  add e    del e  
if an action a is applied to w  and there is a proposition q such that q  add e   del e   for
 possibly the same  occurring e  e  e a   then the result of applying a in w is undefined  thus 
we require the actions to be not self contradictory  that is  for each a  a  and every e  e  e a  
if there exists a world state w  con e   con e    then add e   del e       finally  an action
sequence a is a plan if the world state that results from iterative execution of as actions  starting in
wi   leads to a goal state w  g 
    probabilistic planning
our probabilistic planning setting extends the above with  i  probabilistic uncertainty about the
initial state  and  ii  actions that can have probabilistic effects  in general  probabilistic planning
   pond does not use implicit belief states  and the probabilistic part of its heuristic function uses sampling techniques 
rather than the probabilistic reasoning techniques we employ 

   

fid omshlak   h offmann

tasks are quadruples  a  bi   g     corresponding to the action set  initial belief state  goals  and
acceptable goal satisfaction probability  as before  g is a set of propositions  the initial state is
no longer assumed to be known precisely  instead  we are given a probability distribution over the
world states  bi   where bi  w  describes the likelihood of w being the initial world state 
similarly to classical planning  actions a  a are pairs  pre a   e a    but the effect set e a 
for such a has richer structure and semantics  each e  e a  is a pair  con e    e   of a propositional condition and a set of probabilistic outcomes  each probabilistic outcome    e  is a
triplet  p r    add    del     where add and delete lists are as before  and p r   is the probability that outcome  occurs as a result of effect e  naturally 
p we require that probabilistic effects
define probability distributions over their outcomes  that is   e  p r        the special case of
deterministic effects e is modeled this way via  e       and p r        unconditional actions
are modeled as having a single effect e with con e      as before  if a is not applicable in w 
then the result of applying a to w is undefined  otherwise  if a is applicable in w  then there exists
exactly one effect e  e a  such that con e   w  and for each    e   applying a to w results
in w  add     del   with probability p r    the likelihood  b  a   w   of a world state w in the
belief state  b  a   resulting from applying a probabilistic action a in b  is given by
 b  a   w    

x

wpre a 

b w 

x

 e 


p r     w   w  s   s   s  add    s  del    

   

where e is the effect of a such that con e   w  and    is the kronecker step function that takes
the value   if the argument predicate evaluates to true  and   otherwise 
our formalism covers all the problem description features supported by the previously proposed
formalisms for conformant probabilistic planning  kushmerick et al         majercik   littman 
      hyafil   bacchus        onder et al         bryce et al         huang         and it corresponds to what is called unary nondeterminism   nd  normal form  rintanen         we note
that there are more succinct forms for specifying probabilistic planning problems  rintanen        
yet  nd normal form appears to be most intuitive from the perspective of knowledge engineering 
example   say we have a robot and a block that physically can be at one of two locations  this
information is captured by the propositions r    r  for the robot  and b    b  for the block  respectively  the robot can either move from one location to another  or do it while carrying the block 
if the robot moves without the block  then its move is guaranteed to succeed  this provides us
with a pair of symmetrically defined deterministic actions  move right  move lef t   the action move right has an empty precondition  and a single conditional effect e     r         with
p r        add      r     and del      r     if the robot tries to move while carrying the block 
then this move succeeds with probability      while with probability     the robot ends up moving
without the block  and with probability     this move of the robot fails completely  this provides us
with a pair of  again  symmetrically defined  probabilistic actions  move b right  move b lef t  
the action move b right has an empty precondition  and two conditional effects specified as in
table   
having specified the semantics and structure of all the components of  a  bi   g    but   we are
now ready to specify the actual task of probabilistic planning in our setting  recall that our actions
transform probabilistic belief states to belief states  for any action sequence a  a   and any belief
   

fip robabilistic  ff

e a 

con e 

e

r   b  

e

r   b 

 e 

p r  

add  

del  

 
 
 
 

   
   
   
   

 r    b   
 r   



 r    b   
 r   



table    possible effects and outcomes of the action move b right in example   
state b  the new belief state  b  a  resulting from applying a at b is given by


a   hi
b 
 b  a     b  a   
 
a   hai  a  a





  b  a    a     a   hai  a   a  a  a    

   

in such setting  achieving g with certainty is typically unrealistic  hence   specifies the required
lower bound on the probability of achieving g  a sequence of actions a is called a plan if we have
ba  g    for the belief state ba    bi   a  
    specifying the initial belief state
considering the initial belief state  practical considerations force us to limit our attention only to
compactly representable probability distributions bi   while there are numerous alternatives for
compact representation of structured probability distributions  bayes networks  bns   pearl       
to date is by far the most popular such representation model   therefore  in probabilistic ff we
assume that the initial belief state bi is described by a bn nbi over our set of propositions p 
as excellent introductions to bns abound  e g   see jensen         it suffices here to briefly
define our notation  a bn n    g  t   represents a probability distribution as a directed acyclic
graph g  where its set of nodes x stands for random variables  assumed discrete in this paper  
and t   a set of tables of conditional probabilities  cpts one table tx for each node x  x  
for each possible value x  dom x   where dom x  denotes the domain of x   the table tx
lists the probability of the event x   x given each possible value assignment to all of its immediate
ancestors  parents  p a x  in g  thus  the table size is exponential in the in degree of x  usually  it
is assumed either that this in degree is small  pearl         or that the probabilistic dependence of x
on p a x  induces a significant local structure allowing a compact representation of tx  shimony 
            boutilier  friedman  goldszmidt    koller          otherwise  representation of the
distribution as a bn would not be a good idea in the first place   the joint probability of a complete
assignment  to the variables x is given by the product of  x   terms taken from the respective
cpts  pearl        
y
y
p r   x     p a x     
tx   x     p a x     
p r    
xx

xx

where    stands for the partial assignment provided by  to the corresponding subset of x  
   while bns are our choice here  our framework can support other models as well  e g  stochastic decision trees 

   

fid omshlak   h offmann

in probabilistic ff we allow nbi to be described over the multi valued variables underlying
the planning problem  this significantly simplifies the process of specifying nbi since the strips
 
propositions p do
sknot correspond to the true random variables underlying problem specification 
specifically  let i   pi be a partition of p such that each proposition set pi uniquely corresponds
to the domain of a multi valued variable underlying our problem  that is  for every world state w
and every pi   if  pi        then there is exactly one proposition q  pi that holds in w  the variables
of the bn nbi describing our initial belief state bi are x    x            xk    where dom xi     pi
if  pi        and dom xi      q  q  if pi    q  
example   for an illustration of such nbi   consider our running example  and say the robot is
known to be initially at one of the two possible locations with probability p r r          and
p r r           suppose there is a correlation in our belief about the initial locations of the robot and
the block  we believe that  if the robot is at r    then p r b           and p r b            while if the
robot is at r    then p r b           and p r b            the initial belief state bn nbi is then defined
over two variables r  robot  and b  block  with dom r     r    r    and dom b     b    b    
respectively  and it is depicted in figure   
r 
   

r 
   

r

   b

r 
r 

b 
   
   

b 
   
   

figure    bayes network nbi for example   
it is not hard to see that our strips style actions a  a can be equivalently specified in terms
of the multi valued variables x   specifically  if  pi        then no action a can add a proposition
q  pi without deleting some other proposition q   pi   and thus  we can consider a as setting
xi   q  if  pi        then adding and deleting q  pi has the standard semantics of setting xi   q
and xi   q  respectively  for simplicity of presentation  we assume that our actions are not selfcontradictory at the level of x as wellif two conditional effects e  e  e a  can possibly occur in
some world state w  then the subsets of x affected by these two effects have to be disjoint  finally 
our goal g directly corresponds to a partial assignment to x  unless our g is self contradictory 
requiring q  q  for some q  q   pi   

   belief states
in this section  we explain our representation of  and reasoning about  belief states  we first explain
how probabilistic belief states are represented as time stamped bns  then we explain how those
bns are encoded and reasoned about in the form of weighted cnf formulas  this representation
of belief states by weighted cnfs is then illustrated on the belief state from our running example in
figure    we finally provide the details about how this works in probabilistic ff 
   specifying nbi directly over p would require identifying the multi valued variables anyway  followed by connecting
all the propositions corresponding to a multi valued variable by a complete dag  and then normalizing the cpts of
these propositions in a certain manner 

   

fip robabilistic  ff

    
r
     
r 

r  r 
       
r   



b   

ii
ii
ii
ii
ii
  
  
uu
uu
u
u
uu
uu

b  b 
r         
r         

y   

r 
 
 
 

r 
 
 
 

   r
n      
n
n
nnn
nnn
n
n
       
nn
nnn
r   b               
ppp
ppp
       
ppp othrw
ppp
ppp
p  
   b   

b  b 
 
   
b    
   
b     

r  r 
r     
r     
   r   

   b   

b  b 
b     
b     

figure    bayes network nba for our running example     and action sequence
a   hmove b right  move lef ti 

    bayesian networks
probabilistic ff performs a forward search in a space of belief states  the search states are belief
states  that is  probability distributions over the world states w   and the search is restricted to belief
states reachable from the initial belief state bi through some sequences of actions a  a key decision
one should make is the actual representation of the belief states  let bi be our initial belief state
captured by the bn nbi   and let ba be a belief state resulting from applying to bi a sequence of
actions a  one of the well known problems in the area of decision theoretic planning is that the
description of ba directly over the state variables x becomes less and less structured as the number
of  especially stochastic  actions in a increases  to overcome this limitation  we represent belief
states ba as a bn nba that explicitly captures the sequential application of a starting from bi   trading
the representation size for the cost of inference  compared to representing belief states directly as
distributions over world states  below we formally specify the structure of such a bn nba   assuming
that all the actions a are applicable in the corresponding belief states of their application  and later
showing that probabilistic ff makes sure this is indeed the case  we note that these belief state
bns are similar in spirit and structure to those proposed in the ai literature for verifying that a
probabilistic plan achieves its goals with a certain probability  dean   kanazawa        hanks  
mcdermott        kushmerick et al         
figure   illustrates the construction of nba for our running example with a   hmove b right 
move lef ti  in general  let a   ha            am i be a sequence of actions  numbered according to their
appearance on a  for    t  m  let x t  be a replica of our state variables x   with x t   x t 
   

fid omshlak   h offmann

corresponding to x  x   the variable set of nba is the union of x              x m    plus some
additional variables that we introduce for the actions in a 
first  for each x     x      we set the parents p a x      and conditional probability tables
tx    to simply copy these of the state variable x in nbi   now  consider an action at from a  and
let at   a  for each such action we introduce a discrete variable y t  that s
mediates between the
variable layers x t   and x t    the domain of y t  is set to dom y t      ee a   e   that is  to
the union of probabilistic outcomes of all possible effects of a  the parents of y t  in nba are set to
p a y t     

  

ee a 

 
x i     con e   dom x       

   

and  for each   dom p a y t      we set
ty  t   y i 

 
p r   
        
  

con  e     
 
otherwise

   

where e   denotes the effect e of a such that    e  
we refer to the set of all such variables y t  created for the actions of a as y  now  let ex  a  
e a  be the probabilistic effects of a that affect a variable x  x   if ex  a      then we set
p a x t       x t      and
 
   x   x  
 
   
tx t   x t    x   x t     x    
   otherwise
otherwise  if ex  a       let x  dom x  be the value provided to x by    e   e  ex  a  
recall that the outcomes of effects e a  are all mutually exclusive  hence  we set p a x t     
 x t     y t      and



tx i   x i    x   x i     x   y i  



  
        


  

e    ex  a   x   x  
e     ex  a   x   x    
otherwise

   

where e   denotes the effect responsible for the outcome  
it is not hard to verify that equations     capture the frame axioms and probabilistic semantics of oursactions  in principle  this accomplishes our construction of nba over the variables
xba   y m
t   x t    we note  however  that the mediating variable y t  are really needed only
for truly probabilistic actions  specifically  if at is a deterministic action a  let ex  a   e a  be
the conditional effects of a that add and or delete propositions associated with the domain of a variable x  x   if ex  a      then we set p a x t       x t      and tx t  according to equation   
otherwise  we set
o
 n

p a x t       x t    
x t  
  con e   dom x       
   
eex  a 

and specify tx t  as follows  let xe  dom x  be the value that  the only deterministic outcome
of  the effect e  ex  a  provides to x  for each   dom p a x t      if there exists e  ex  a 
   

fip robabilistic  ff

such that con e     then we set
tx t   x t 

 
  
  x      
  

x   xe  
otherwise

   

x    x t     
otherwise

   

otherwise  we set
tx t   x t 

 
  
  x      
  

due to the self consistency of the action  it is not hard to verify that equations     are consistent 
and  together with equation    capture the semantics of the conditional deterministic actions  this
special treatment of deterministic actions is illustrated in figure   by the direct dependencies of
x    on x     
proposition   let  a  nbi   g    be a probabilistic planning problem  and a be an m step sequence
of actions applicable in bi   let p r be the joint probability distribution induced by nba on its
variables xba   the belief state ba corresponds to the marginal distribution of p r on x m    that is 
ba  x     p r x m     and if g m  is a partial assignment provided by g to x m    then the probability
ba  g  that a achieves g starting from bi is equal to p r g m    
as we already mentioned  our belief state bns are constructed along the principles outlined
and used by dean and kanazawa         hanks and mcdermott         and kushmerick et al 
        and thus the correctness of proposition   is immediate from these previous results  at this
point  it is worth bringing attention to the fact that all the variables in x              x m  are completely
deterministic  moreover  the cpts of all the variables of nba are all compactly representable due to
either a low number of parents  or some local structure induced by a large amount of context specific
independence  or both  this compactness of the cpts in nba is implied by the compactness of the
strips style specification of the planning actions  by exploiting this compactness of the action
specification  the size of the nba s description can be kept linear in the size of the input and the
number of actions in a 
proposition   let  a  nbi   g    be a probabilistic planning problem described over k state variables  and a be an m step sequence of actions from a  then  we have  nba     o  nbi   m k    
where  is the largest description size of an action in a 
the proof of proposition    as well as the proofs of other formal claims in the paper  are relegated
to appendix a  pp      
    weighted cnfs
given the representation of belief states as bns  next we should select a mechanism for reasoning
about these bns  in general  computing the probability of a query in bns is known to be  pcomplete  roth         in addition  it is not hard to verify  using an analysis similar to the ones
of darwiche        and brafman and domshlak         that the networks arising in our work
will typically exhibit large tree width  while numerous exact algorithms for inference with bns
have been proposed in the literature  darwiche        dechter        zhang   poole         the
classical algorithms do not scale well on large networks exhibiting high tree width  on the positive
   

fid omshlak   h offmann

side  however  an observation that guides some recent advances in the area of probabilistic reasoning
is that real world domains typically exhibit a significant degree of deterministic dependencies and
context specific independencies between the problem variables  targeting this property of practical
bns already resulted in powerful inference techniques  chavira   darwiche        sang et al  
       the general principle underlying these techniques is to
 i  compile a bn n into a weighted propositional logic formula  n   in cnf  and
 ii  perform an efficient weighted model counting for  n   by reusing  and adapting  certain
techniques that appear powerful in enhancing backtracking dpll style search for sat 
one observation we had at the early stages of developing probabilistic ff is that the type of
networks and type of queries we have in our problems make this machinery for solving bns by
weighted cnf model counting very attractive for our needs  first  in section     we have already
shown that the bns representing our belief states exhibit a large amount of both deterministic nodes
and context specific independence  second  the queries of our interest correspond to computing
probability of the evidence g m  in nba   and this type of query has a clear interpretation in terms
of model counting  sang et al          hence  taking this route in probabilistic ff  we compile our
belief state bns to weighted cnfs following the encoding scheme proposed by sang et al         
and answer probabilistic queries using cachet  sang  bacchus  beame  kautz    pitassi         one
of the most powerful systems to date for exact weighted model counting in cnfs 
in general  the weighted cnfs and the weights of such formulas are specified as follows  let
v    v            vn   be a set of propositional variables with dom vi      vi   vi    and let   
s
   be a non negative  real valued weight function from the literals of v  for any
i dom vi    r
partial assignment  to v q
the weight    of this assignment is defined as the product of its literals
weights  that is       l  l   finally  a propositional logic formula  is called weighted if it
is defined over such a weighted set of propositional variables  for any weighted formula  over v 
the weight    is defined as the sum of the weights of all the complete assignments to v satisfying
  that is 
x
    
          
dom v 

where dom v    i dom vi    for instance  if for all variables vi we have  vi      vi       
then    simply stands for the number of complete assignments to v that satisfy  
given an initial belief state bn nbi   and a sequence of actions a   ha            am i applicable in
bi   here we describe how the weighted cnf encoding  nba    or  ba    for short  of the belief state
ba is built and used in probabilistic ff  first  we formally specify the generic scheme introduced
by sang et al         for encoding a bn n over variables x into a weighted cnf  n    the
encoding formula  n   contains two sets of variables  first  for each variable z  x and each
value z  dom z   the formula  n   contains a state proposition with literals  z  z   weighted
as  z     z       these state propositions act in  ba   as regular sat propositions  now 
for each variable z  xba   let dom z     z            zk   be an arbitrary fixed ordering of dom z  
recall that each row tz  i  in the cpt of z corresponds to an assignment i  or a set of such assignments  to p a z   thus  the number of rows in tz is upper bounded by the number of different
assignments to p a z   but  as it happens in our case  it can be significantly lower if the dependence of z on p a z  induces a substantial local structure  following the ordering of dom z  as
above  the entry tz  i  j  contains the conditional probability of p r zj   i    for every cpt entry
   

fip robabilistic  ff

procedure basic wmc  
if     return  
if  has an empty clause return  
select a variable v  
return basic wmc  v     v    basic wmc  v     v 
figure    basic dppl style weighted model counting 
tz  i  j  but the last one  i e   tz  i  k    the formula  n   contains a chance proposition with literals
 hzji i  hzji i   these chance variables aim at capturing the probabilistic information from the cpts
of nba   specifically  the weight of the literal hzji i is set to p r zj   i   z            zj     that is to
conditional probability that the entry is true  given that the row is true  and no prior entry in the row
is true 
tz  i  j 
pj 
   k   tz  i  k 


i
 hzj i       hzji i

 hzji i  

    

considering the clauses of  n    for each variable z  x   and each cpt entry tz  i  j   the
formula  n   contains a clause

i
i  hz i i      hzj 
i  hzji i  zj  
    

where i is a conjunction of the literals forming the assignment i  dom p a z    these clauses
ensure that the weights of the complete assignments to the variables of  n   are equal to the probability of the corresponding atomic events as postulated by the bn n   to illustrate the construction
in equations        let boolean variables a and b be the parents of a ternary variable c  with
dom c     c    c    c     in some bn  and let p r c   a  b         p r c   a  b         and
p r c   a  b         let the raw corresponding to the assignment a  b to p a c  be the i th
row of the cpt tc   in the encoding of this bn  the first two entries of this raw of tc are captured
by a pair of respective chance propositions
hc i i  and hc i i  according
to equation     the weights


   
i
i
of these propositions are set to  hc  i        and  hc  i               then  according to
equation     the encoding contains three clauses

a  b  hc i i  c 

a  b  hc i i  hc i i  c 

a  b  hc i i  hc i i  c 

finally  for each variable z  x   the formula  n   contains a standard set of clauses encoding
the exactly one relationship between the state propositions capturing the value of z  this accomplishes the encoding of n into  n    in the next section     we illustrate this encoding on the
belief state bn from our running example 
the weighted cnf encoding  ba   of the belief state bn nba provides the input to a weighted
model counting procedure  a simple recursive dppl style procedure basic wmc underlying cachet  sang et al         is depicted in figure    where the formula  v is obtained from  by setting
   

fid omshlak   h offmann

the literal v to true  theorem   by sang et al         shows that if  is a weighted cnf encoding
of a bn n   and p r q e  is a general query with respect to n   query q  and evidence e  then we
have 
basic wmc   q  e 
p r q e   
 
    
basic wmc   e 
where query q and evidence e can in fact be arbitrary formulas in propositional logic  note that 
in a special  and very relevant to us  case of empty evidence  equation    reduces to p r q   
basic wmc q   that is  a single call to the basic wmc procedure  corollary   is then immediate
from our proposition   and theorem   by sang et al         
corollary   let  a  bi   g    be a probabilistic planning task with a bn nbi describing bi   and a
be an m step sequence of actions applicable in bi   the probability ba  g  that a achieves g starting
from bi is given by 
ba  g    wmc   ba    g m    
    
where g m  is a conjunction of the goal literals time stamped with the time endpoint m of a 
    example  weighted cnf encoding of belief states
we now illustrate the generic bn to wcnf encoding scheme of sang et al         on the belief
state bn nba from our running example in figure   
for    i     we introduce time stamped state propositions r   i   r   i   b   i   b   i   likewise 
we introduce four state propositions                            corresponding to the values of the
variable y      the first set of clauses in  ba   ensure the exactly one relationship between the
state propositions capturing the value of a variable in nba  

                            

 i j   

 yi      yj       
 i   

    

 r   i   r   i      r   i   r   i  
 b   i   b   i      b   i   b   i  
now we proceed with encoding the cpts of nba   the root nodes have only one row in their
cpts so their chance propositions can be identified with the corresponding state variables  sang
et al          hence  for the root variable r    we need neither additional clauses nor special
chance propositions  but the state proposition r      of  ba   is treated as a chance proposition
with   r             
encoding of the variable b    is a bit more involved  the cpt tb    contains two  content wise
different  rows corresponding to the given r   and given r   cases  and both these cases induce
a non deterministic dependence of b    on r      to encode the content of tb    we introduce
two chance variables hb       i and hb       i with  hb       i        and  hb       i         the
positive literals of hb       i and hb       i capture the events b  given r   and b  given r    while
the negations hb       i and hb       i capture the complementary events b  given r   and b 
given r    respectively  now consider the given r   row in tb      to encode this row  we need
   

fip robabilistic  ff



 ba   to contain r       hb       i  b      and r       hb       i  b       similar encoding
is required for the row given r    and thus the encoding of tb   introduces four additional clauses 


r       hb       i  b        r       hb       i  b     


    
r       hb       i  b        r       hb       i  b     

having finished with the nbi part of nba   we proceed with encoding the variable y    corresponding to the probabilistic action move b right  to encode the first row of ty    we introduce three chance propositions h       i  h       i  and h       i  in general  no chance variables are needed for the last entries of the cpt rows  the weights of these chance propositions
   
          and
are set according to equation    to  h       i         h       i       

   
 
 h      i               using these chance propositions  we add to  ba   four clauses as in
equation     notably the first four clauses of equation    below 
proceeding the second row of ty      observe that the value of r    and b    in this case fully
determines the value of y      this deterministic dependence can be encoded without using any
chance propositions using the last two clauses in equation    

r       b       h       i         

r       b       h       i  h       i         

r       b       h       i  h       i  h       i         
    

r       b       h       i  h       i  h       i         


r               b            

using the state chance variables introduced for r    b     and y      we encode the cpts of r   
and b    as 
r              r                 r        

        r       r                r       r       

        r       r                r       r     

b              b        

    

        b       b        
        b       b      
since the cpts of both r    and b    are completely deterministic  their encoding as well is using
no chance propositions  finally  we encode the  deterministic  cpts of r    and b    as 
r       r      
b       b       b      

    

 b       b      
where the unary clause  r       is a reduction of  r       r       and  r       r        this accomplishes our encoding of  ba   
   

fid omshlak   h offmann

    from conformant ff to probabilistic ff
besides the fact that weighted model counting is attractive for the kinds of bns arising in our context  the weighted cnf representation of belief states works extremely well with the ideas underlying conformant ff  hoffmann   brafman         this was outlined in the introduction already 
here we give a few more details 
as stated  conformant ff does a forward search in a non probabilistic belief space in which
each belief state corresponds to a set of world states considered to be possible  the main trick of
conformant ff is the use of cnf formulas for an implicit representation of belief states  where
formulas  a  encode the semantics of executing action sequence a in the initial belief state  facts
known to be true or false are inferred from these formulas  this computation of only a partial
knowledge constitutes a lazy kind of belief state representation  in comparison to other approaches
that use explicit enumeration  bonet   geffner        or bdds  bertoli  cimatti  pistore  roveri 
  traverso        to fully represent belief states  the basic ideas underlying probabilistic ff are 
 i  define time stamped bayesian networks  bn  describing probabilistic belief states  section     above  
 ii  extend conformant ffs belief state cnfs to model these bn  section     above  
 iii  in addition to the sat reasoning used by conformant ff  use weighted model counting to
determine whether the probability of the  unknown  goals in a belief state is high enough
 directly below  
 iv  introduce approximate probabilistic reasoning into conformant ffs heuristic function  section   below  
in more detail  given a probabilistic planning task  a  bi   g     a belief state ba corresponding to
some applicable in bi m step action sequence a  and a proposition q  p  we say that q is known
in ba if ba  q       negatively known in ba if ba  q       and unknown in ba   otherwise  we begin
with determining whether each q is known  negatively known  or unknown at time m  re using the
conformant ff machinery  this classification requires up to two sat tests of  ba    q m  and
 ba    q m   respectively  the information provided by this classification is used threefold  first 
if a subgoal g  g is negatively known at time m  then we have ba  g       on the other extreme 
if all the subgoals of g are known at time m  then we have ba  g       finally  if some subgoals of
g are known and the rest are unknown at time m  then we accomplish evaluating the belief state ba
by testing whether
ba  g    wmc   ba    g m     

    

note also that having the sets of all  positively negatively  known propositions at all time steps up
to m allows us to significantly simplify the cnf formula  ba    g m  by inserting into it the
corresponding values of known propositions 
after evaluating the considered action sequence a  if we get ba  g     then we are done 
otherwise  the forward search continues  and the actions that are applicable in ba  and thus used to
generate the successor belief states  are actions whose preconditions are all known in ba  
   

fip robabilistic  ff

   heuristic function
the key component of any heuristic search procedure is the heuristic function  the quality  informedness  and computational cost of that function determine the performance of the search  the
heuristic function is usually obtained from solutions to a relaxation of the actual problem of interest  pearl        russell   norvig         in classical planning  a successful idea has been to
use a relaxation that ignores the delete effects of the actions  mcdermott        bonet   geffner 
      hoffmann   nebel         in particular  the heuristic of the ff planning system is based on
the notion of relaxed plan  which is a plan that achieves the goals while assuming that all delete
lists of actions are empty  the relaxed plan is computed using a graphplan style  blum   furst 
      technique combining a forward chaining graph construction phase with a backward chaining
plan extraction phase  the heuristic value h w  that ff provides to a world state w encountered
during the search is the length of the relaxed plan from w  in conformant ff  this methodology was
extended to the setting of conformant planning under initial state uncertainty  without uncertainty
about action effects   herein  we extend conformant ffs machinery to handle probabilistic initial
states and effects  section     provides background on the techniques used in ff and conformantff  then sections     and     detail our algorithms for the forward and backward chaining phases in
probabilistic ff  respectively  these algorithms for the two phases of the probabilistic ff heuristic
computation are illustrated on our running example in sections     and      respectively 
    ff and conformant ff
we specify how relaxed plans are computed in ff  we provide a coarse sketch of how they are
computed in conformant ff  the purpose of the latter is only to slowly prepare the reader for what
is to come  conformant ffs techniques are re used for probabilistic ff anyway  and hence will be
described in full detail as part of sections     and     
formally  relaxed plans in classical planning are computed as follows  starting from w  ff
builds a relaxed planning graph as a sequence of alternating proposition layers p  t  and action
layers a t   where p     is the same as w  a t  is the set of all actions whose preconditions are
contained in p  t   and p  t      is obtained from p  t  by including the add effects  with fulfilled
conditions  of the actions in a t   that is  p  t  always contains those facts that will be true if one
would execute  the relaxed versions of  all actions at the earlier layers up to a t      the relaxed
planning graph is constructed either until it reaches a propositional layer p  m  that contains all
the goals  or until the construction reaches a fixpoint p  t    p  t      without reaching the goals 
the latter case corresponds to  all  situations in which a relaxed plan does not exist  and because
existence of a relaxed plan is a necessary condition for the existence of a real plan  the state w is
excluded from the search space by setting h w      in the former case of g  p  m   a relaxed
plan is a subset of actions in a             a m  that suffices to achieve the goals  under ignoring the
delete lists   and it can be extracted by a simple backchaining loop  for each goal in p  m   select
an action in a             a m  that achieves this goal  and iterate the process by considering those
actions preconditions and the respective effect conditions as new subgoals  the heuristic estimate
h w  is then set to the length of the extracted relaxed plan  that is  to the number of actions selected
in this backchaining process 
aiming at extending the machinery of ff to conformant planning  in conformant ff  hoffmann and brafman        suggested to extend the relaxed planning graph with additional fact layers up  t  containing the facts unknown at time t  and then to reason about when such unknown
   

fid omshlak   h offmann

facts become known in the relaxed planning graph  as the complexity of this type of reasoning is
prohibitive  conformant ff further relaxes the planning task by ignoring not only the delete lists 
but also all but one of the unknown conditions of each action effect  that is  if action a appears
in layer a t   and for effect e of a we have con e   p  t   up  t  and con e   up  t      
then con e   up  t  is arbitrarily reduced to contain exactly one literal  and reasoning is done as if
con e  had this reduced form from the beginning 
v
this relaxation converts implications   ccon e up  t  c t    q t      that the action effects
induce between unknown propositions into their   projections that take the form of binary implications c t   q t       for arbitrary c  con e   up  t   due to the layered structure of the
planning graph  the set of all these binary implications c t   q t      can be seen as forming a
directed acyclic graph imp  under the given relaxations  this graph captures exactly all dependencies between the truth of propositions over time  hence  checking whether a proposition q becomes
known at time t can be done as follows  first  backchain over the implication edges of imp that end
in q t   and collect the set support q t   of leafs  that are reached  then  if  is the cnf formula
describing the possible initial states  test by a sat check whether
 

l
lsupport q t  

this test will succeed if and only if at least one of the leafs in support q t   is true in every possible
initial state  under the given relaxations  this is the case if and only if  when applying all actions in
the relaxed planning graph  q will always be true at time t  
the process of extracting a relaxed plan from the constructed conformant relaxed planning
graph is an extension of ffs respective process with machinery that selects actions responsible for
relevant paths in imp  the overall conformant ff heuristic machinery is sound and complete for
relaxed tasks  and yields a heuristic function that is highly informative across a range of challenging
domains  hoffmann   brafman        
in this work  we adopt conformant ffs relaxations  ignoring the delete lists of the action effects  as well as all but one of the propositions in the effects condition  accordingly  we adopt the
following notations from conformant ff  given a set of actions a  we denote by   
  any function
 
from a into the set of all possible actions  such that    maps each a  a to the action similar to a
but with empty delete lists and with all but one conditioning propositions of each effect removed 
 
 
for   
denote the action set obtained by applying   
   a   we write a 
    by a   we
  to all the actions
 
 
 
a
we
denote
by
a 
of a  that is  a  
 
a 
 
a

a
 
for
an
action
sequence
  the sequence of
 
 
 
actions obtained by applying    to every action along a  that is 
 
hi 
a   hi
a  
 
   
 
 

ha   i  a      a   hai  a
for a probabilistic planning task  a  bi   g     the task  a  
    bi   g    is called a relaxation of
 
 
 a  bi   g     finally  if a   is a plan for  a     bi   g     then a is called a relaxed plan for
 a  bi   g    
   following the conformant ff terminology  by leafs we refer to the nodes having zero in degree 
   note here that it would be possible to do a full sat check  without any   projection  without relying on imp   to see
whether q becomes known at t  however  as indicated above  doing such a full check for every unknown proposition
at every level of the relaxed planning graph for every search state would very likely be too expensive  computationally 

   

fip robabilistic  ff

in the next two sections we describe the machinery underlying the probabilistic ff heuristic
estimation  due to the similarity between the conceptual relaxations used in probabilistic ff and
conformant ff  probabilistic ff inherits almost all of conformant ffs machinery  of course 
the new contributions are those algorithms dealing with probabilistic belief states and probabilistic
actions 
    probabilistic relaxed planning graphs
like ff and conformant ff  probabilistic ff computes its heuristic function in two steps  the first
one chaining forward to build a relaxed planning graph  and the second step chaining backward to
extract a relaxed plan  in this section  we describe in detail probabilistic ffs forward chaining step 
building a probabilistic relaxed planning graph  or prpg  for short   in section      we then show
how one can extract a  probabilistic  relaxed plan from the prpg  we provide a detailed illustration
of the prpg construction process on the basis of our running example  since the illustration is
lengthy  it is moved to a separate section     
the algorithms building a prpg are quite involved  it is instructive to first consider  some
of  the key points before delving into the details  the main issue is  of course  that we need to
extend conformant ffs machinery with the ability to determine when the goal set is sufficiently
likely  rather than when it is known to be true for sure  to achieve that  we must introduce into
relaxed planning some effective reasoning about both the probabilistic initial state  and the effects
of probabilistic actions  it turns out that such a reasoning can be obtained by a certain weighted
extension of the implication graph  in a nutshell  if we want to determine how likely it is that a fact
q is true at a time t  then we propagate certain weights backwards through the implication graph 
starting in q t   the weight of q t  is set to    and the weight for any p t   gives an estimate of the
probability of achieving q at t given that p holds at t   computing this probability exactly would 
of course  be too expensive  our estimation is based on assuming independence of the various
probabilistic events involved  this is a choice that we made very carefully  we experimented widely
with various other options before deciding in favor of this technique 
any simplifying assumption in the weight propagation constitutes  of course  another relaxation 
on top of the relaxations we already inherited from conformant ff  the particularly problematic
aspect of assuming independence is that it is not an under estimating technique  the actual weight
of a node p t    the probability of achieving q at t given that p holds at t  may be lower than our
estimate  in effect  the prpg may decide wrongly that a relaxed plan exists  even if we execute
all relaxed actions contained in the successful prpg  the probability of achieving the goal by this
execution may be less than the required threshold  in other words  we lose the soundness  relative
to relaxed tasks  of the relaxed planning process 
we experimented with an alternative weight propagation method  based on an opposite assumption  that the relevant probabilistic events always co occur  and that hence the weights must be
propagated according to simple maximization operations  this propagation method yielded very
uninformative heuristic values  and hence inacceptable empirical behaviour of probabilistic ff 
even in very simple benchmarks  in our view  it seems unlikely that an under estimating yet informative and efficient weight computation exists  we further experimented with some alternative
non under estimating propagation schemes  in particular one based on assuming that the probabilistic events are completely disjoint  and hence weights should be added   these schemes gave better
   

fid omshlak   h offmann

performance than maximization  but lagged far behind the independence assumption in the more
challenging benchmarks 
let us now get into the actual algorithm building a prpg  a coarse outline of the algorithm is as
follows  the prpg is built in a layer wise fashion  in each iteration extending the prpg  reaching
up to time t  by another layer  reaching up to time t      the actions in the new step are those whose
preconditions are known to hold at t  effects conditioned on unknown facts  note here the reduction
of effect conditions to a single fact  constitute new edges in the implication graph  in difference to
conformant ff  we dont obtain a single edge from condition to add effect  instead  we obtain edges
from the condition to chance nodes  where each chance node represents a probabilistic outcome of
the effect  the chance nodes  in turn  are linked by edges to their respective add effects  the weights
of the chance nodes are set to the probabilities of the respective outcomes  the weights of all other
nodes are set to    these weights are static weights which are not dynamically modified by
weight propagation  rather  the static weights form an input to the propagation 
once all implication graph edges are inserted at a layer  the algorithm checks whether any new
facts become known  this check is done very much like the corresponding check in conformant ff 
by testing whether the disjunction of the support leafs for a proposition p at t     is implied by the
initial state formula  the two differences to conformant ff are      only leafs are relevant whose
dynamic weight is    otherwise  achieving a leaf is not guaranteed to accomplish p at t          
another reason for p to become known may be that all outcomes of an unconditional effect  or an
effect with known condition  result in achievement of p at time t      we elegantly formulate the
overall test by a single implication test over support leafs whose dynamic weight equals their own
weight 
like ffs and conformant ffs algorithms  the prpg process has two termination criteria  the
prpg terminates positively if the goal probability is high enough at time t  the prpg terminates
negatively if  from t to t      nothing has changed that may result in a higher goal propability at
some future t   the goal probability in a layer t is computed based on weighted model counting over
a formula derived from the support leafs of all goals not known to be true  the criteria for negative
termination check  whether any new facts have become known or unknown  not negatively known  
whether any possibly relevant new support leafs have appeared  and whether the goal probability
has increased  if neither is the case  then we can stop safelyif the prpg terminates unsuccessfully
then we have a guarantee that there is no relaxed plan  and that the corresponding belief is hence a
dead end 
let us get into the details  figure   depicts the main routine for building the prpg for a belief
state ba   as we already specified  the sets p  t   up  t   and a t  contain the propositions that are
known to hold at time t  hold at t with probability     the propositions that are unknown to hold at
time t  hold at t with probability less than   but greater than     and actions that are known to be
applicable at time t  respectively  the layers t    of prpg capture applying the relaxed actions
starting from ba   the layers m to   of prpg correspond to the m step action sequence a leading
from the initial belief state to the belief state in question ba   we inherit the latter technique from
conformant ff  in a sense  the prpg reasons about the past  this may look confusing at first
sight  but it has a simple reason  imagine the prpg starts at level   instead  then  to check whether
a proposition becomes known  we have to do sat tests regarding support leafs against the belief
state formula   ba    instead of the initial state formula  similarly for weighted model counting
to test whether the goal is likely enough   testing against  ba   is possible  but very expensive
   

fip robabilistic  ff

procedure build prpg a  a   nbi    g      
    
returns a bool saying if there is a relaxed plan for the belief state
given by a   ham           a  i  and
builds data structures from which a relaxed plan can be extracted
     nbi    imp    
p  m      p   p is known in    up  m      p   p is unknown in  
for t    m       do
a t      at   
     n oop s
build timestep t  a t  
endfor
t     
while get p t  g     do
a t      a  
    a  a  pre a   p  t    n oop s
build timestep t  a t  
if p  t        p  t  and
up  t        up  t  and
p  up  t        up  m   support p t         up  m   support p t   and
get p t      g    get p t  g  then
return false
endif
t    t    
endwhile
t    t  return true

figure    main routine for building a probabilistic relaxed planning graph  prpg  
computationally   the negative index layers chain the implication graph all the way back to the
initial state  and hence enable us to perform sat tests against the  typically much smaller  initial
state formula 
returning to figure    the prpg is initialized with an empty implication set imp  p  m 
and up  m  are assigned the propositions that are known and unknown in the initial belief state 
and a weighted cnf formula  is initialized with  nbi     is the formula against which implication weighted model checking tests are run when asking whether a proposition becomes known whether
the goal is likely enough  while the prpg is built   is incrementally extended with further clauses
to capture the behavior of different effect outcomes 
the for loop builds the sets p and up for the as time steps m       by iterative invocation
of the build timestep procedure that each time expands prpg by a single time level  at each
iteration m  t     the sets p  t      and up  t      are made to contain the propositions
that are known unknown after applying the relaxed version of the action at  a  remember that
a   ha            am i   to simplify the presentation  each action set a t  contains a set of dummy
actions n oop s that simply
transport all

  the propositions from time layer t to time layer t    more
formally  n oop s   noopp   p  p   where pre noopp       e noopp        p         and
          p       
   in conformant ff  this configuration is implemented as an option  it significantly slows down the search in most
domains  and brings advantages only in a few cases 

   

fid omshlak   h offmann

the subsequent while loop constructs the relaxed planning graph from layer   onwards by 
again  iterative invocation of the build timestep procedure  the actions in each layer t    are
relaxations of those actions whose preconditions are known to hold at time t with certainty  this
iterative construction is controlled by two termination tests  first  if the goal is estimated to hold at
layer t with probability higher than   then we know that a relaxed plan estimate can be extracted 
otherwise  if the graph reaches a fix point  then we know that no relaxed  and thus  no real  plan
from bi exists  we postpone the discussion of these two termination criteria  and now focus on the
time layer construction procedure build timestep 
procedure build timestep t  a  
builds p  t       up  t       and the implication edges from t to t     
as induced by the action set a
p  t         p  t   up  t         
for all effects e of an action a  a  con e   p  t   up  t  do
for all    e  do
up  t         up  t       add  
introduce new fact  t  with   t     p r  
imp    imp     t   p t         p  add   
endfor
if con e   up  t 
sthen
imp    imp   e    con e  t    t   
else
 v
       e   t      e    t     t  
endif
endfor
for all p  up  t      do
build w impleafs p t       imp 
support p t           l   l  leafs impp t       p t     l     l  
w
if   lsupport p t     l then p  t         p  t        p  endif
endfor
up  t         up  t        p  t     

figure    building a time step of the prpg 
the build timestep procedure is shown in figure    the first for loop of build timestep proceeds
over all outcomes of  relaxed  actions in the given set a that may occur at time t  for each such
probabilistic outcome we introduce a new chance proposition weighted by the conditional likelihood
of that outcome   having that  we extend imp with binary implications from this new chance
proposition to the add list of the outcome  if we are uncertain about the condition con e  of the
corresponding effect at time t  that is  we have con e   up  t   then we also add implications
from con e  to the chance propositions created for the outcomes of e  otherwise  if con e  is
known at time t  then there is no uncertainty about our ability to make the effect e to hold at time
t  in this case  we do not ground the chance propositions created for the outcomes of e into the
implication graph  but simply extend the running formula  with clauses capturing the exactly
one relationship between these chance propositions corresponding to the alternative outcomes of e
   of course  in our implementation we have a special case treatment for deterministic actions  using no chance nodes
 rather than a single chance node with static weight    

   

fip robabilistic  ff

at time t  this way  the probabilistic uncertainty about the outcome of e can be treated as if being a
property of the initial belief state bi   this is the only type of knowledge we add into the knowledge
base formula  after initializing it in build prpg to  nbi   
notation
impvu
impu
leafs imp  
e imp  

description
the graph containing exactly all the paths from node v to node u in imp 
the subgraph of imp formed by node u and all the ancestors of u in imp 
the set of all zero in degree nodes in the subgraph imp of imp 
the set of time stamped action effects responsible for the implication edges
of the subgraph imp of imp 
table    overview of notations around the implication graph 

the second for loop checks whether a proposition p  unknown at time t  becomes known at
time t      this part of the build timestep procedure is somewhat more involved  table   provides
an overview of the main notations used in the follows when discussing the various uses of the
implication graph imp 
first thing in the second for loop of build timestep  a call to build w impleafs procedure associates each node v t   in impp t    with an estimate p t     v t    on the probability of achieving
p at time t     by the effects e impv t  p t       given that v holds at time t   in other words  the
dynamic weight  according to p t       of the implication graph nodes is computed  note that v t  
can be either a time stamped proposition q t   for some q  p  or a chance proposition  t   for
some probabilistic outcome  
we will discuss the build w impleafs procedure in detail below  for proceeding to understand
the second for loop of build timestep  the main thing we need to know is the following lemma 
lemma   given a node v t    impp t      we have p t     v t        v t    if and only if 
given v at time t   the sequence of effects e impv t  p t      achieves p at t     with probability   
in words  v t   leads to p t      with certainty iff the dynamic weight of v t   equals its static
weight  this is a simple consequence of how the weight propagation is arranged  it should hold true
for any reasonable weight propagation scheme  do not mark a node as certain if it is not   a full
proof of the lemma appears in appendix a on pp      
re consider the second for loop of build timestep  what happens is the following  having
finished the build w impleafs weight propagation for p at time t      we
   collect all the leafs support p t       of impp t  that meet the criteria of lemma    and
   check  by a call to a sat solver  whether the knowledge base formula  implies the disjunction of these leafs 
if the implication holds  then the examined fact p at time t is added to the set of facts known at time
t  finally  the procedure removes from the set of facts that are known to possibly hold at time t    
all those facts that were just proven to hold at time t     with certainty 
to understand the above  consider the following  with lemma    support p t       contains
exactly the set of leafs achieving which will lead to p t      with certainty  hence we can basically
   

fid omshlak   h offmann

procedure build w impleafs  p t   imp 
top down propagation of weights p t  from p t  to all nodes in impp t 
p t   p t       
for decreasing time steps t     t            m  do
for all chance nodes  t    impp t 
 do

q
   p t   r t      
    radd   r t    imp
p t 
p t    t          t          
endfor
for all fact nodes q t    impp t  do
     

for all a  a t
 e a   con e    q do
i
h    ep
p t    t   
          e   t  imp
p t 
endfor
p t   q t          
endfor
endfor

figure    the build w impleafs procedure for weight back propagation over the implication graph 
use the same implication test as in conformant ff  note  however  that the word basically in the
previous sentence hides a subtle but important detail  in difference to the situation in conformantff  support p t       may contain two kinds of nodes      proposition nodes at the start layer of
the prpg  i e   at layer m corresponding to the initial belief      chance nodes at later layers of
the prpg  corresponding to outcomes of effects that have no unknown conditions  this is the point
where the discussed above updates onwthe formula  are neededthose keep track of alternative
effect outcomes  hence testing   lsupport p t     l is the same as testing whether either      p
is known at t     because it is always triggered with certainty by at least one proposition true in the
initial world  or     p is known at t     because it is triggered by all outcomes of an effect that will
appear with certainty  we get the following result 
lemma   let  a  nbi   g    be a probabilistic planning task  a be a sequence of actions applicable
in bi   and   
  be a relaxation function for a  for each time step t  m  and each proposition p 
p  if p  t  is constructed by build prpg a  a   nbi    g      
     then p at time t can be achieved
 
by a relaxed plan starting with a  
    with probability      that is  p is not negatively known at time t  if and only if p  up  t p  t  
and
    with probability    that is  p is known at time t  if and only if p  p  t  
this is a consequence of the arguments outlined above  the full proof of lemma   is given in
appendix a on pp      
let us now consider the weight propagating  procedure build w impleafs depicted in figure   
this procedure performs a layered  top down weight propagation from a given node  p t   imp
   the weight propagation scheme of the build w impleafs procedure is similar in nature to this used in the heuristics
module of the recent probabilistic temporal planner prottle of little  aberdeen  and thiebaux        
   note that the t here will be instantiated with t     when called from build timestep 

   

fip robabilistic  ff

down to the leafs of impp t    this order of traversal ensures that each node of impp t  is processed only after all its successors in impp t    for the chance nodes  t    the dynamic weight
p t    t    is set to
   the probability that the outcome  takes place at time t given that the corresponding action
effect e   does take place at t   times
   an estimate of the probability of achieving p at time t by the effects e imp t  p t    
the first quantity is given by the global  static weight    t    assigned to  t   in the first for
loop of build timestep  the second quantity is derived from the dynamic weights p t   r t      
for r  add    computed in the previous iteration of the outermost for loop of build w impleafs 
making a heuristic assumption that the effect sets e impr t    p t    for different r  add   are
all pairwise independent   is then set to the probability of failure to achieve p at t by the effects
e imp t  p t     this computation of  for  t   is decomposed over the artifacts of   and this
is where the weight propagation starts taking place  for the fact nodes q t    the dynamic weight
p t   q t    is set to the probability that some action effect conditioned on q at time t allows
 possibly indirectly  achieving the desired fact p at time t  making again the heuristic assumption
of independence between various such effects conditioned on q at t   computing p t   q t    is
decomposed over the outcomes of these effects 
procedure get p  t  g 
estimates the probability of achieving g at time p 
if g   p  t   up  t  then return   endif
if g  p  t  then return   endif
for g  g   p  t  do
for each l  leafs impg t     introduce a chance proposition hlg i with weight g t   l 
w
v
g      lleafs impg t    l   lleafs impg t   up  m   l  hlg i 
endfor
v
return wmc   gg p  t  g  

figure    estimating the goal likelihood at a given time step 
what remains to be explained of the build prpg procedure are the two termination criteria of
the while loop constructing the planning graph from the layer   onwards  the first test is made by
a call to the get p procedure  and it checks whether the prpg built to the time layer t contains
a relaxed plan for  a  nbi   g     the get p procedure is shown in figure    first  if one of the
subgoals is negatively known at time t  then  from lemma    the overall probability of achieving
the goal is    on the other extreme  if all the subgoals are known at time t  then the probability of
achieving the goal is    the correctness of the latter test is implied by lemma   and non interference
of relaxed actions  this leaves us with the main case in which we are uncertain about some of the
subgoals  this uncertainty is either due to dependence of these subgoals on the actual initial world
state  or due to achieving these subgoals using probabilistic actions  or due to both  the uncertainty
about the initial state is fully captured by our weighted cnf formula  nbi      likewise  the
outcomes chance propositions  t   introduced into the implication graph by the build timestep
procedure are chained up in imp to the propositions on the add lists of these outcomes  and
   

fid omshlak   h offmann

chained down in imp to the unknown  relaxed  conditions of these outcomes  if any  therefore 
if some action outcome  at time t   t is relevant to achieving a subgoal g  g at time t  then
the corresponding node  t   must appear in impg t    and its weight will be back propagated by
build w impleafs g t   imp  down to the leafs of impg t    the get p procedure then exploits
these back propagated estimates by  again  taking a heuristic assumption of independence between
achieving different subgoals  namely  the probability of achieving the unknown sub goals g   p  t 
is estimated by weighted model counting over the formula   conjoined with probabilistic theories
g of achieving each unknown goal g in isolation  to understand the formulas g   consider that  in
order to make g true at t  we must achieve at least one of the leafs l of impg t    hence the left part of
the conjunction  on the other hand  if we make l true  then this achieves g t  only with  estimated 
probability g t   l   hence the right part of the conjunction requires us to pay the price if we set
l to true   
as was explained at the start of this section  the positive prpg termination test may fire even if
the real goal probability is not high enough  that is  get p may return a value higher than the real
goal probability  due to the approximation  independence assumption  done in the weight propagation  of course  due to the same approximation  it may also happen that get p returns a value lower
than the real goal probability 
the second prppg termination test comes to check whether we have reached a point in the
construction of prpg that allows us to conclude that there is no relaxed plan for  a  nbi   g    that
starts with the given action sequence a  this termination criterion asks whether  from time step t
to time step t      any potentially relevant changes have occurred  a potentially relevant change
would be if the goal satisfaction probability estimate get p grows  or if the known and unknown
propositions grow  of if the support leafs of the latter propositions in imp that correspond to the
initial belief state grow    if none occurs  then the same would hold in all future iterations t   t 
implying that the required goal satisfaction probability  would never be reached  in other words 
the prpg construction is complete 
theorem   let  a  nbi   g    be a probabilistic planning task  a be a sequence of actions appli 
cable in bi   and   
  be a relaxation function for a  if build prpg a  a   nbi    g         returns
 
false  then there is no relaxed plan for  a  bi   g    that starts with a    
note that theorem   holds despite the approximation done during weight propagation  making
the assumption of probabilistic independence  for theorem   to hold  the only requirement on the
weight propagation is this  if the real weight still grows  then the estimated weight still grows  this
requirement is met under the independence assumption  it would not be met under the assumption of
co occurence  propagating weights by maximization operations  and thereby conservatively underestimating the weights  with that propagation  if the prpg fails then we cannot conclude that there
is no plan for the respective belief  this is another good argument  besides the bad quality heuristics
we observed empirically  against using the conservative estimation 
    if we do not introduce the extra chance propositions hlg i  and instead assign the weight g t   l  to l itself  then the
outcome is not correct  we have to pay also for setting l to false 
    to understand the latter  note that prpg can always be added with more and more replicas of probabilistic actions
irrelevant to achieving the goals  and having effects with known conditions  while these action effects  since they are
irrelevant  will not influence our estimate of goal satisfaction probability  the chance propositions corresponding to
the outcomes of these effects may become the support leafs of some unknown proposition p  in the latter case  the
set of support leafs support p t    will infinitely grow with t    while the projection of support p t    on the
initial belief state  that is  support p t    up  t   is guaranteed to reach a fix point 

   

fip robabilistic  ff

the full proof to theorem   is given in appendix a on pp       the theorem finalizes our
presentation and analysis of the process of constructing probabilistic relaxed planning graphs 
    example  prpg construction
to illustrate the construction of a prpg by the algorithm in figures      let us consider a simplification of our running examples     in which
 i  only the actions  move b right  move lef t  constitute the action set a 
 ii  the goal is g    r    b     and the required lower bound on the probability of success        
 iii  the initial belief state bi is given by the bn nbi as in example    and
 iv  the belief state ba evaluated by the heuristic function corresponds to the actions sequence
a   hmove b righti 
the effects outcomes of the actions a considered in the construction of prpg are described in
table    where embr is a re notation of the effect e in table    the effect e in table   is effectively
ignored due to the emptiness of its add effects 
a

e a 

con e 

con e   
 

 e 

p r  

add  

   
   
   
   

 r    b   
 r   

 r   

   
   
   
   

 r   
 r   
 b   
 b   

embr

 r    b   

 r   

aml  move lef t 

eml

 r   

 r   

mbr
 
mbr
 
mbr
 
ml

noopr 
noopr 
noopb 
noopb 

er 

 r   
 r   
 b   
 b   

 r   
 r   
 b   
 b   

r 
r 
b 
b 

ambr

 move b right 

er 
eb 
eb 

table    actions and their   
  relaxation for the prpg construction example 
the initialization phase of the build prpg procedure results in     nbi    imp     
p         and up        r    r    b    b     the content of up     is depicted in the first column
of nodes in figure    the first for loop of build prpg  constructing prpg for the past layers
corresponding to a  makes a single iteration  and calls the build timestep procedure with t    
and a        ambr    n oop s   in what follows  using the names of the actions we refer to their
mbr is empty  and thus it adds no
  
  relaxations as given in table     the add list of the outcome  
nodes to the implication graph  other than that  the chance nodes introduced to imp by this call to
build timestep appear in the second column of figure    the first outer for loop of build timestep
results in imp given by columns     of figure    up       up      and no extension of  
in the second outer for loop of build timestep  the weight propagating procedure build w impleafs
is called for each unknown fact p     up        r       r       b       b        generating the p   oriented weights as in table    for each p     up      the set of supporting leafs support p      
   

fid omshlak   h offmann


wv



   ml    

  






mbr     
mbr


   aa

 aa    
ml
ml



  
  












 mbr
  mbr    

     
 q         

 
   


  
 qqq
  rrrr
  


  
 qq
  
rrrr
  
qqq

 r
  
 r
     

      r     
      r     
    
          
r      


    
  
  

  
  

 
rs
hi  
hi   




   r     
   r      
   r     
   r     
r      





b      



   b      



b      



   b      



 

   b     



   b     



   b     



   b     



 



mbr
     

ml
  



mbr
     
   
  
  
  
  
  
  
hi   
 r

    
   r     
    

   b     



   b     



   b     



   b     



   b     
 

   b     

figure    the implication graph imp  the odd columns of nodes depict the sets of unknown propositions up  t   the even columns of nodes depict the change propositions introduced for
the probabilistic outcomes of the actions a t  

 p      none of them is implied by    nbi   and thus the set of known facts p     remains equal
to p         and up     equal to   up     

r     
r     
b     
b     

t    
t    
mbr
mbr
r
r
r  r  b  b   
 
      b  b 
 
 
 
       
 
 
 
     
 

r  r  b  b 
 
     
 
   
 

table    the columns in the table correspond to the nodes in the implication graph imp  and each
row provides the weights p    for some p     up      an entry in the row of p    is
empty if and only if the node associated with the corresponding column does not belong
to the implication subgraph impp     
having finished with the for loop  the build prpg procedure proceeds with the while loop that
builds the future layers of prpg  the test of goal  un satisficing get p    g     evaluates to
true as we get get p    g                and thus the loop proceeds with its first iteration 
to see the former  consider the implication graph imp constructed so far  columns     in fig   

fip robabilistic  ff

ure     for our goal g    r    b    we have leafs impr           r        and leafs impb         
 r       b        as  r       b        up     and     nbi    we have
get p    g    wmc   nbi    r   b     
where
r     hr  r  i    r   hr  r  i   
b     hr  b  i  hb  b  i    r       hr  b  i    b       hb  b  i   

    

and
  hr  r  i    r       r          
  hb  b  i    b       b          

 

    

  hr  b  i    b       r            
observe that the two models of  nbi   consistent with r  immediately falsify the sub formula
 nbi    r    hence  we have

get p    g    wmc  nbi    r   b   r        b         

wmc  nbi    r   b   r        b       

  bi  r    b       hr  r  i     hr  b  i    bi  r    b       hr  r  i     hr  b  i     hb  b  i 

                                
      
in the first iteration of the while loop  build prpg calls the build timestep procedure with
t     and a       ambr   aml    n oop s  the chance nodes introduced to imp by this call to
build timestep appear in the forth column of figure    the first outer for loop of build timestep
results in imp given by columns     of figure    up       up      and no extension of   as
before  in the second for loop of build timestep  the build w impleafs procedure is called for each
unknown fact p     up        r       r       b       b        generating the p    oriented weights 
the interesting case here is the case of weight propagation build w impleafs r       imp   resulting
in weights
r       r          

r       r           

ml

r                
r 

r                
r       r          
r       r          



r       r           
r       mbr
       
mbr
r              

     



r       r           
r       r           

     

for the nodes in impr        from that  the set of supporting leafs of r      is assigned to support r        
 r       r        and since     nbi   does implies r       r       the fact r  is concluded
to be known at time    and is added to p      for all other nodes p     up     we still have
support p        p      and thus they all remain unknown at time t     as well  putting
things together  this call to the build w impleafs procedure results with p        r        and
   

fid omshlak   h offmann

up        r       b       b        the while loop of the build prpg procedure proceeds with checking the fixpoint termination test  and this immediately fails due to p        p      hence  the while
loop proceeds with the next iteration corresponding to t     
the test of goal  un satisficing get p    g     still evaluates to true because we have
get p    g                 let us follow this evaluation of get p    g  in detail as well  considering the implication graph imp constructed so far up to time t      columns     in figure     and
having g  up        b        leafs impb           r       b        and  still      nbi    we
obtain
get p    g    wmc   nbi    b     
with
b     hr  b  i  hb  b  i    r       hr  b  i    b       hb  b  i   

    

while the structure of b  in equation    is identical to this in equation     the weights associated
with the auxiliary chance propositions are different  notably
  hb  b  i    b       b          

 

  hr  b  i    b       r             

    

the difference in   hr  b  i  between equation    and equation    stems from the fact that r     
supports b      not only via the effect embr at time   but also via the a different instance of the
same effect at time    now  the only model of  nbi   that falsify b  is the one that sets both r 
and b  to false  hence  we have
get p    g    bi  r    b       hr  b  i   
bi  r    b       hr  b  i     hb  b  i   
bi  r    b       hb  b  i 
                                      
       
having verified get p    g      the while loop proceeds with the construction for time t     
and calls the build timestep procedure with t     and a       ambr   aml    n oop s  the chance
nodes introduced to imp by this call to build timestep appear in the sixth column of figure    the
first outer for loop of build timestep results in imp given by columns     of figure    and


mbr
mbr
    nbi    mbr
   


   


   

 
 
 

 
 

mbr
mbr
mbr
mbr
mbr
 mbr
   


   


   


   


   


   
 
 
 
 
 
 

    

next  the build w impleafs procedure is called as usual for each unknown fact p     up      
 r       b       b        the information worth detailing here is that now we have leafs impb         
mbr
 b       r       mbr
        and support b          b               however  we still have  
w
lsupport p     l for no p     up      and thus the set of known facts p     remains equal to
p        r    
   

fip robabilistic  ff

returning from the call to the build w impleafs procedure  build prpg proceeds with checking
the fixpoint termination condition  this time  the first three equalities of the condition do hold  yet
the condition is not satisfied due to get p    g    get p t  g   to see the latter  notice that we have
get p    g    wmc    b     
where  is given by equation    


b    hr  b  i  hb  b  i  mbr
        r       hr  b  i    b       hb  b  i   

    

and

  hb  b  i    b       b          

 

  hr  b  i    b       r             
 mbr
      

 

b       mbr
      

    

     

it is not hard to verify that
get p    g    get p    g    bi  r    b      mbr
      
                   
       
note that now we do have get p    g     and therefore build prpg aborts the while loop by
passing the goal satisficing test  and sets t      this finalizes the construction of prpg  and thus 
our example 
    extracting a probabilistic relaxed plan
if the construction of the prpg succeeds in reaching the goals with the estimated probability of success get p t  g  exceeding   then we extract a relaxed plan consisting of a  a             a t 
    and use the size of a as the heuristic value of the evaluated belief state ba  
before we get into the technical details  consider that there are some key differences between
relaxed  no delete lists  probabilistic planning on the one hand  and both relaxed classical and relaxed qualitative conformant planning on the other hand  in relaxed probabilistic planning  it might
make sense to execute the same action numerous times in consecutive time steps  in fact  this
might be essential  just think of throwing a dice in a game until a   appears  in contrast  in the
relaxed classical and qualitatively uncertain settings this is not needed  once an effect has been
executed  it remains true forever  another complication in probabilistic planning is that the required
goal achievement probability is specified over a conjunction  or  possibly  some more complicated
logical combination  of different facts  while increasing the probability of achieving each individual sub goal g  g in relaxed planning will always increase the overall probability of achieving g 
choosing the right distribution of effort among the sub goals to pass the required threshold  for the
whole goal g is a non trivial problem 
a fundamental problem is the aforementioned lack of guarantees of the weight propagation 
on the one hand  the construction of prpg and lemma   imply that a  
  concatenated with an
r
arbitrary linearization a of a             a t     is executable in bi   on the other hand  due to
the independence assumption made in the build w impleafs procedure  get p t  g    does not
   

fid omshlak   h offmann

r
imply that the probability of achieving g by a  
  concatenated with a exceeds   a real relaxed
plan  in that sense  might not even exist in the constructed prpg 
our answer to the above difficulties is to extract relaxed plans that are correct relative to the
weight propagation  namely  we use an implication graph reduction algorithm that computes
a minimal subset of that graph which still  according to the weight propagation  sufficiently
supports the goal  the relaxed plan then corresponds to that subset  obviously  this solves the
difficulty with the lack of real relaxed plans  we just do the relaxed plan extraction according to
the independence assumption  besides ignoring deletes and removing all but one condition of each
effect   the mechanism also naturally takes care of the need to apply the same action several times 
this corresponds to several implication graph edges which are all needed in order to obtain sufficient
weight  the choice of how effort is distributed among sub goals is circumvented in the sense that
all sub goals are considered in conjunction  that is  the reduction is performed once and for all  of
course  there remains a choice in which parts of the implication graph should be removed  we have
found that it is a useful heuristic to make this choice based on which actions have already been
applied on the path to the belief  we will detail this below 
making another assumption on top of the previous relaxations can of course be bad for heuristic
quality  the relaxed plans we extract are not guaranteed to actually achieve the desired goal probability  since the relaxed plans are used only for search guidance  per se this theoretical weakness is
only of marginal importance  however  an over estimation of goal probability might result in a bad
heuristic because the relaxed plan does not include the right actions  or does not apply them often
enough  in section    we will discuss an example domain where probabilistic ff fails to scale for
precisely this reason 
figure   shows the main routine extract prplan for extracting a relaxed plan from a given
prpg  note that t is the index of the highest prpg layer  c f  figure     the sub routines of
extract prplan are shown in figures        at a high level  the extract prplan procedure consists
of two parts 

   reduction of the implication graph  aiming at identifying a set of time stamped action effects
that can be ignored without decreasing our estimate of goal achievement probability get p t  g 
below the desired threshold   and
   extraction of a valid relaxed plan ar such that  schematically  constructing prpg with ar instead
of the full set of a             a t   would still result in get p t  g    
the first part is accomplished by the reduce implication graph procedure  depicted in figure    
as of the first step in the algorithm  the procedure considers only the parts of the implication graph
that are relevant to achieving the unknown sub goals  next  reduce implication graph performs a
greedy iterative elimination of actions from the future layers            t   of prpg until the probability estimate get p t  g  over the reduced set of actions goes below   while  in principle  any action from a             a t    can be considered for elimination  in reduce implication graph we examine only repetitions of the actions that already appear in a  specifically  reduce implication graph
iterates over the actions a in a  
    and if a repeats somewhere in the future layers of prpg  then
one such repetition a t   is considered for removal  if removing this repetition of a is found safe
with respect to achieving     then it is effectively removed by eliminating all the edges in imp that
are induced by a t    then the procedure considers the next repetition of a  if removing another
    note here that the formula for wmc is constructed exactly as for the get p function  c f  figure   

   

fip robabilistic  ff

procedure extract prplan p rp g a  a   nbi    g      
     
selects actions from a             a t    
imp    reduce implication graph  
extract subplan imp  
sub goal g  p  t   
for decreasing time steps t    t            do
for all g  g t  do
if a  a t      e  e a   con e   p  t         e    g  add   then
add to relaxed plan one such a at time t
sub goal pre a   con e  
else
imp g t     construct support graph support g t   
extract subplan imp g t   
endif
endfor
endfor

figure    extracting a probabilistic relaxed plan 
copy of a is not safe anymore  then the procedure breaks the inner loop and considers the next
action 
procedure reduce implication graph  
operates on the prpg 
returns a sub graph of imp 
imp    gg p  t   impg t  
for all actions a  a  
  do
for all edges   t    p t        imp   induced by a t    a t    for some t    do
imp    imp
remove from imp all the edges induced by a  a t  
for all g  g   p  t  do
for each l  leafs imp g t      introduce a chance proposition hlg i with weight g t    l 
v
w
g      lleafs imp
  l   lleafs imp
 up  m   l  hlg i 
g t  

g t  

endfor
v
if wmc   gg p  t   g     then imp    imp else break endif
endfor
endfor
return imp

figure     the procedure reducing the implication graph 
to illustrate the intuition behind our focus on the repetitions of the actions from a  let us consider the following example of a simple logistics style planning problem with probabilistic actions 
suppose we have two locations a and b  a truck that is known to be initially in a  and a heavy and
uneasy to grab package that is known to be initially on the truck  the goal is to have the package
unloaded in b with a reasonably high probability  and there are two actions we can use  moving
the truck from a to b  am    and unloading the package  au    moving the truck does not necessarily
   

fid omshlak   h offmann

move the truck to b  but it does that with an extremely high probability  on the other hand  unloading the bothersome package succeeds with an extremely low probability  leaving the package on the
truck otherwise  given this data  consider the belief state ba corresponding to after trying to move
the truck once  that is  to the action sequence ham i  to achieve the desired probability of success 
the prpg will have to be expanded to a very large time horizon t   allowing the action au to be
applied sufficiently many times  however  the fact truck in b is not known in the belief state ba  
and thus the implication graph will also contain the same amount of applications of am   trimming
away most of these applications of am will still keep the probability sufficiently high 
the reader might ask at this point what we hope to achieve by trimming away most of the
applications of am   the point is  intuitively  that the implication graph reduction mechanism is
a means to understand what has been accomplished already  on the path to ba   without such an
understanding  the relaxed planning can be quite indiscriminative between search states  consider
the above example  and assume we have not one but two troubled packages  p   and p    on the
truck  with unload actions au  and au    the prpg for ba contains copies of au  and au  at layers up
to the large horizon t   now  say our search starts to unload p    in the resulting belief  the prpg
still has t steps because the situation has not changed for p    each step of the prpg still contains
copies of both au  and au   and hence the heuristic value remains the same as before  in other
words  without an implication graph reduction technique  relevant things that are accomplished
may remain hidden behind other things that have not yet been accomplished  in the above example 
this is not really critical because  as soon as we have tried an unload for each of p   and p    the
time horizon t decreases by one step  and the heuristic value is reduced  it is  however  often
the case that some sub task must be accomplished before some other sub task can be attacked  in
such situations  without implication graph reduction  the search staggers across a huge plateau until
the first task is completed  we observed this in a variety of benchmarks  and hence designed the
implication graph reduction to make the relaxed planning aware of what has already been done 
of course  since our weight propagation may over estimate true probabilities  and hence overestimate what was achieved in the past  the implication graph reduction may conclude prematurely
that a sub task has been completed  this leads us to the main open question in this research  we
will get back to this at the end of section    where we discuss this in the context of an example
where probabilistic ffs performance is bad 
let us get back to explaining the extract prplan procedure  after the implication graph reduction  the procedure proceeds with the relaxed plan extraction  the process makes use of proposition
sets g             g t    which are used to store time stamped sub goals arising at layers    t  t
during the relaxed plan extraction  the sub routine extract subplan  figure    
   adds to the constructed relaxed plan all the time stamped actions responsible for the edges of the
reduced implication graph imp   and
   subgoals everything outside the implication graph that condition the applicability of the effects
responsible for the edges of imp  
here and in the later phases of the process  the sub goals are added into the sets g             g t   by
the sub goal procedure that simply inserts each given proposition as a sub goal at the first layer of
its appearance in the prpg  having accomplished this extract and subgoal pass of extract subplan
over imp   we also subgoal all the goal conjuncts known at time t  
in the next phase of the process  the sub goals are considered layer by layer in decreasing order
of time steps t  t     for each sub goal g at time t  certain supporting actions are selected into
   

fip robabilistic  ff

procedure extract subplan imp  
actions that are helpful for achieving uncertain goals g  up  t   and
subgoals all the essential conditions of these actions
for each edge   t   p t        imp such that t    do
if action a and its effect e  e a  be responsible for  at time t time
add to relaxed plan a at time t
sub goal  pre a   con e    p  t  
endif endfor
procedure sub goal p  
inserts the propositions in p as sub goals
at the layers of their first appearance in the prpg
for all p  p do
t     argmint  p  p  t  
if t     then g t       g t      p  endif
endfor
procedure construct support graph support g t   
takes a subset support g t   of leafs impg t    weighted according to g t  
returns a sub graph imp of imp 

imp    
open    support g t  
while open     do
open    open    p t   
choose a  a t    e  e a   con e     p  such that
   e     p t     t     impg t   g t    t        t   
for each    e  do
choose q  add   such that g t   q t          
imp    imp    p t     t       t    q t       
open    open   q t      
endfor endwhile
return imp

figure     sub routines for extract prplan 
the relaxed plan  if there is an action a and some effect e  e a  that are known to be applicable
at time t     and guarantee to achieve g with certainty  then a is added to the constructed relaxed
plan at t     otherwise  we
   use the construct support graph procedure to extract a sub graph imp g t  consisting of a set of
implications that together ensure achieving g at time t  and
   use the already discussed procedure extract subplan to
 a  add to the constructed relaxed plan all the time stamped actions responsible for the edges
of imp g t    and
 b  subgoal everything outside this implication graph imp g t  that condition the applicability
of the effects responsible for the edges of imp g t   
   

fid omshlak   h offmann

processing this way all the sub goals down to g    finalizes the extraction of the relaxed plan
estimate  section     provides a detailed illustration of this process on the prpg constructed in
section      in any event  it is easy to verify that the relaxed plan we extract is sound relative to our
weight propagation  in the following sense 
proposition   let  a  nbi   g    be a probabilistic planning task  a be a sequence of actions ap 
plicable in bi   and   
  be a relaxation function for a such that build prpg a  a   nbi    g        
returns true  let a   s           a t    s be the actions selected from a             a t     by
extract prplan  when constructing a relaxed planning graph using only a   s           a t    s  
then get p t  g    
proof  by construction  reduce implication graph leaves enough edges in the graph so that the
weight propagation underlying get p still concludes that the goal probability is high enough 
    example  extracting a relaxed plan from prpg
we illustrate the process of the relaxed plan extraction on the prpg as in figure    constructed for
the belief state and problem specification as in example in section      in this example we have
t      g  up        b     and thus the implication graph imp gets immediately reduced to its
sub graph imp depicted in figure   a  as the plan a to the belief state in question consists of only a
single action ambr   the only action instances that are considered for elimination by the outer for loop
of reduce implication graph are ambr     and ambr      if ambr     is chosen to be examined  then the
implication sub graph imp   imp is further reduced by removing all the edges due to ambr     
and the resulting imp appears   in figure   b  the  and b  components of the evaluated formula
  b  are given by equation    and equation     respectively  and the weights associated with
the chance propositions in equation    over the reduced implication graph imp are
  hb  b  i    b       b          
  hr  b  i    b       r            

 

    

mbr
 mbr
         b                   

the weight model counting of   b  evaluates to           and thus imp does not replace imp  
the only alternative action removal is this of ambr      and it can be seen from the example in section     that this attempt for action elimination will also result in probability estimate lower than  
hence  the only effect of reduce implication graph on the prpg processed by the extract prplan
procedure is the reduction of the implication graph to only the edges relevant to achieving  b    at
time t      the reduced implication sub graph imp returned by the reduce implication graph
procedure is depicted in figure   a 
next  the extract subplan procedure iterates over the edges of imp and adds to the initially
empty relaxed plan applications of ambr at times   and    the action ambr has no preconditions 
 e ambr   is known at time    hence  extract subplan
and the condition r  of the effect mbr
 
invokes the sub goal procedure on  r      and the latter is added into the proposition set g     the
subsequent call sub goal g  p  t      sub goal  r     leads to no further extensions of g     g   
    the dashed edges in figure   b can be removed from imp either now or at a latter stage if imp is chosen to replace
imp  

   

fip robabilistic  ff



mbr
         
qq
  
qqq
q
q
qq


   r      
r      



b      



   b      



   r     



mbr
        
rrr
  
rrr
r
r

    
b     



   b     





mbr
     

    
b     

  



   b     



    
b     

 a 


mbr
         
qq
  
qqq
q
q
qq

 r
r                                  r     

b      



   b      



 

   b     



mbr
     



   b     



   b     



   b     



  

 

   b     

 b 


   ml
      

r      



   r      



r      



   r      



wv

 r
   

   r     
         





rs

   r     

   r     

 c 
figure     illustrations for various steps of the relaxed plan extraction from the prpg constructed
in section      and  in particular  from the implication graph of the latter  depicted in
figure   

as we already have r   g     hence  the outer for loop of extract prplan starts with g       
and g       r    
since g    is empty  the first sub goal considered by extract prplanis r  from g     for r 
at time    no action effect at time   passes the test of the if statementthe condition r  of ml
is not known at time    and the same is true   for r    hence  the subgoal r      is processed
by extracting a sub plan to support achieving it with certainty  first  the construct support graph
procedure is called with support r          r       r        see section       the extracted sub    in fact  it is easy to see from the construction of the sub goal procedure that if p belongs to g t   then the condition
of the noops effect p cannot be known at time t    

   

fid omshlak   h offmann

graph imp r      of the original implication graph imp is depicted in figure   c  and invoking the
procedure extract subplan on imp r      results in adding  i  application of aml at time    and  ii 
no new subgoals  hence  the proposition sets g     g    get emptied  and thus we end up with
extracting a relaxed plan hambr      aml      ambr    i 

   empirical evaluation
we have implemented probabilistic ff in c  starting from the conformant ff code  with        
probabilistic ff behaves exactly like conformant ff  except that conformant ff cannot handle
non deterministic effects   otherwise  probabilistic ff behaves as described in the previous sections  and uses cachet  sang et al         for the weighted model counting  to better home in on
strengths and weaknesses of our approach  the empirical evaluation of probabilistic ff has been
done in two steps  in section     we evaluate probabilistic ff on problems having non trivial uncertain initial states  but only deterministic actions  in section     we examine probabilistic ff
on problems with probabilistic action effects  and with both sources of uncertainty  we compare
probabilistic ffs performance to that of the probabilistic planner pond  bryce et al          the
reasons for choosing pond as the reference point are twofold  first  similarly to probabilistic ff 
pond constitutes a forward search planner guided by a non admissible heuristic function based
on  relaxed  planning graph computations  second  to our knowledge  pond clearly is the most
efficient probabilistic planner reported in the literature   
the experiments were run on a pc running at  ghz with  gb main memory and  mb cache
running linux  unless stated otherwise  each domain problem pair was tried at four levels of desired probability of success                           each run of a planner was time limited by
     seconds of user time  probabilistic ff was run in the default configuration inherited from ff 
performing one trial of enforced hill climbing and switching to best first search in case of failure  in
domains without probabilistic effects  we found that probabilistic ffs simpler relaxed plan extraction developed for that case  domshlak   hoffmann         performs better than the one described
in here  we hence switch to the simpler version in these domains   
unlike probabilistic ff  the heuristic computation in pond has an element of randomization 
namely  the probability of goal achievement is estimated via sending a set of random particles
through the relaxed planning graph  the number of particles is an input parameter   for each problem instance  we averaged the runtime performance of pond over    independent runs  in special
cases where pond timed out on some runs for a certain problem instance  yet not on all of the
   runs  the average we report for pond uses the lower bounding time threshold of     s to replace the missing time points  in some cases  ponds best case performance differs a lot from
its average performance  in these cases  the best case performance is also reported  we note that 
following the suggestion of dan bryce  pond was run in its default parameter setting  and  in par    in our experiments we have used a recent version     of pond that significantly enhances pond     bryce et al  
       the authors would like to thank dan bryce and rao kambhampati for providing us with a binary distribution
of pond    
    without probabilistic effects  relaxed plan extraction proceeds very much like in conformant ff  with an additional
straightforward backchaining selecting support for the unknown goals  the more complicated techniques developed
in here to deal with relaxed plan extraction under probabilistic effects appear to have a more unstable behavior than
the simpler techniques  if there are probabilistic effects  then the simple backchaining is not meaningful because it
has no information on how many times an action must be applied in order to sufficiently support the goal 

   

fip robabilistic  ff

       
t  s  l

      
t  s  l

       
t  s  l

      
t  s  l

         
         

           
        

          
          

          
          

           
          

cube uni   
cube cub   

         
         

           
         

           
          

           
           

            
            

bomb      
bomb      
bomb     
bomb     

               
             
             
            

        
        
        
        

          
           
           
           

          
           
           
            

          
           
            
            

log  
log  
log  

                
                 
                

           
           
           

           
           
           

           
           
           

           
            
            

grid  
grid  
grid  

                
                
                

          
             
             

           
            
              

            
              
              

            
              
               

rovers  
roversp  
roverspp  
roversppp  

                
                 
                 
                 

           
           
           
            

           
           
            
            

           
            
            
     unsat

           
            
             
     unsat

instance

 actions  facts  states

safe uni   
safe cub   

table    empirical results for problems with probabilistic initial states  times t in seconds  search
space size  s   number of calls to the heuristic function   plan length l 

ticular  this includes the number of random particles      selected for computing ponds heuristic
estimate  bryce et al         
    initial state uncertainty and deterministic actions
we now examine the performance of probabilistic ff and pond in a collection of domains with
probabilistic initial states  but with deterministic action effects  we will consider the domains one
by one  discussing for each a set of runtime plots  for some of the problem instances  table   shows
more details  providing features of the instance size as well as detailed results for probabilistic ff 
including the number of explored search states and the plan length 
our first three domains are probabilistic versions of traditional conformant benchmarks  safe 
cube  and bomb  in safe  out of n combinations one opens the safe  we are given a probability
distribution over which combination is the right one  the only type of action in safe is trying a
combination  and the objective is to open the safe with probability    we experimented with
two probability distributions over the n combinations  a uniform one  safe uni  and a distribution
that declines according to a cubic function  safe cub   table   shows that probabilistic ff can
solve this very efficiently even with n       figure    compares between probabilistic ff and
pond  plotting their time performance on an identical linear scale  where x axes show the number
of combinations 
from the graphs it is easy to see that probabilistic ff outperforms pond by at least an order of
magnitude on both safe uni and safe cub  but a more interesting observation here is not necessarily
the difference in time performance  but the relative performance of each planner on safe uni and
safe cub  note that safe cub is somewhat easier than safe uni in the sense that  in safe cub  fewer
combinations must be tried to guarantee a given probability  of opening the safe  this because the
   

fid omshlak   h offmann

pff

pond   

  

  
p     
p     
p     
p     

  

  

  

  

  

time  sec 

time  sec 

  

p     
p     
p     
p     

  

  

  

  

  

  

 

 
  

  

  

  

  

  

 combinations

  

  

 combinations

 a  uniform prior distribution over the combinations 
pff

pond   

  

  
p     
p     
p     
p     

  

  

  

  

  

time  sec 

time  sec 

  

p     
p     
p     
p     

  

  

  

  

  

  

 

 
  

  

  

  

  

 combinations

  

  

  

 combinations

 b  cubic decay prior distribution over the combinations 
figure     the safe domain  probabilistic ff  left  vs  pond  right  
dominant part of the probability mass lies on the combinations at the head of the cubic distribution
 the last combination has probability   to be the right combination  and thus it needs not be tried
even when          the question is now whether the heuristic functions of probabilistic ff and
pond exploit this difference between safe uni and safe cub  table   and figure    provide an
affirmative answer for this question for the heuristic function of probabilistic ff  the picture with
pond was less clear as the times spent by pond on  otherwise identical  instances of safe uni and
safe cub were roughly the same   
another interesting observation is that  for both probabilistic ff and pond  moving from   
    to         that is  from planning with qualitative uncertainty to truly probabilistic planning 
    on safe cub with n      and                pond undergoes an exponential blow up that is not shown in the
graphs since these data points would obscure the other data points  anyway  we believe that this blow up is due only
to some unfortunate troubles with numerics 

   

fip robabilistic  ff

pff

pond   

  

    
p     
p     
p     
p     

  

p     
p     
p     
p     

    
    
    
time  sec 

time  sec 

  

  

  

    
   
   
   

 
   
 

 
 

 

 
  
n for grid nxnxn

  

  

 

 

 
  
n for grid nxnxn

  

  

  

  

 a  uniform prior distribution over the initial position 
pff

pond   

  

    
p     
p     
p     
p     

  

p     
p     
p     
p     

    
    
    
time  sec 

time  sec 

  

  

  

    
   
   
   

 
   
 

 
 

 

 
  
n for grid nxnxn

  

  

 

 

 
  
n for grid nxnxn

 b  cubic decay prior distribution over the initial position 
figure     the cube domain  probabilistic ff  left  vs  pond  right  

typically did not result in a performance decline  we even get improved performance  except for
        in safe uni   the reason seems to be that the plans become shorter  this trend can be
observed also in most other domains  the trend is particularly remarkable for probabilistic ff  since
moving from        to        means to move from a case where no model counting is needed
to a case where it is needed   in other words  probabilistic ff automatically specializes itself for
the qualitative uncertainty  by not using the model counting  to our knowledge  the same is not true
of pond  which uses the same techniques in both cases  
in cube  the task is to move into a corner of a   dimensional grid  and the actions correspond
to moving from the current cube cell to one of the  up to    adjacent cube cells  again  we created
problem instances with uniform and cubic distributions  over the initial position in each dimension  
and again  probabilistic ff scales well  easily solving instances on a            cube  within
our time limit  pond was capable of solving cube problems with cube width      figure   
   

fid omshlak   h offmann

compares between probabilistic ff and pond in more detail  plotting their time performance on
different linear scales  with x axes capturing the width of the grid in each dimension   and showing
at least an order of magnitude advantage for probabilistic ff  note that 
 probabilistic ff generally becomes faster with decreasing   with decreasing hardness of
achieving the objective   while  does not seem to have a substantial effect on the performance
of pond 
 probabilistic ff exploits the relative easiness of cube cub  e g   see table     while the time
performance of pond on cube cub and cube uni is qualitatively identical 
we also tried a version of cube where the task is to move into the grid center  probabilistic ff is
bad at doing so  reaching its performance limit at n      this weakness in the cube center domain
is inherited from conformant ff  as detailed by hoffmann and brafman         the reason for the
weakness lies in the inaccuracy of the heuristic function in this domain  there are two sources of
this inaccuracy  first  to solve cube center in reality  one must start with moving into a corner in
order to establish her position  in the relaxation  without delete lists  this is not necessary  second 
the relaxed planning graph computation over approximates not only what can be achieved in future
steps  but also what has already been achieved on the path to the considered belief state  for even
moderately long paths of actions  the relaxed planning graph comes to the  wrong  conclusion that
the goal has already been achieved  so the relaxed plan becomes empty and there is no heuristic
information 
next we consider the famous bomb in the toilet domain  or bomb  for short   our version
of bomb contains n bombs and m toilets  where each bomb may be armed or not armed independently with probability   n  resulting in huge numbers of initially possible world states  dunking a
bomb into an unclogged toilet disarms the bomb  but clogs the toilet  a toilet can be unclogged by
flushing it  table   shows that probabilistic ff scales nicely to n       and becomes faster as m
increases  the latter is logical and desirable as having more toilets means having more disarming
devices  resulting in shorter plans needed  figures    and    compare between probabilistic ff
and pond  plotting the time performance of probabilistic ff on a linear scale  and that of pond
on a logarithmic scale  the four pairs of graphs correspond to four choices of number of toilets
m                  the x axes in all these graphs correspond to the number of potentially armed
bombs  where we checked problems with n                   figure    shows that this time
probabilistic ff is at least four orders of magnitude faster than pond  at the extremes  while the
hardest combination of n       m      and         took probabilistic ff less than   seconds 
pond timed out on most of the problem instances  in addition 
 in bomb as well  probabilistic ff exhibit the nice pattern of improved performance as we
move from non probabilistic          to probabilistic planning  specifically         for
        the initial state is good enough already  
 while the performance of probabilistic ff improves with the number of toilets  pond seems
to exhibit the inverse dependence  that is  being more sensitive to the number of states in the
problem  see table    rather to the optimal solution depth 
finally  we remark that  though length optimality is not explicitly required in probabilistic conformant planning  for all of safe  cube  and bomb  probabilistic ffs plans are optimal  the shortest
possible  
   

fip robabilistic  ff

pff

pond   

  
p     
p     
p     
p     

    

 
   

time  sec 

time  sec 

 

 

  

 

 

   
p     
p     
p     
p     

 

    
 

  

  
  bombs

  

 

  

  
  bombs

  

 a     toilets
pff

pond   

  
p     
p     
p     
p     

    

 
   

time  sec 

time  sec 

 

 

  

 

 

   
p     
p     
p     
p     

 

    
 

  

  
  bombs

  

 

  

  
  bombs

  

 b     toilets
figure     the bomb domain  probabilistic ff  left  vs  pond  right  

our next three domains are adaptations of benchmarks from deterministic planning  logistics 
grid  and rovers  we assume that the reader is familiar with these domains  each logistics x
instance contains    cities     airplanes  and    packages  where each city has x locations  the
packages are with chance      at the airport of their origin city  and uniformly at any of the other
locations in that city  the effects of all loading and unloading actions are conditional on the  right 
position of the package  note that higher values of x increase not only the space of world states  but
also the initial uncertainty  grid is the complex grid world run in the aips   planning competition  mcdermott         featuring locked positions that must be opened with matching keys  each
grid x here is a modification of instance nr     of    run at aips    with a      grid    locked
positions  and    keys of which   must be transported to a goal position  each lock has x possible  uniformly distributed shapes  and each of the   goal keys has x possible  uniformly distributed
initial positions  the effects of pickup key  putdown key  and open lock actions are conditional 
   

fid omshlak   h offmann

pff

pond   

  
p     
p     
p     
p     

    

 
   

time  sec 

time  sec 

 

 

  

 

 

   
p     
p     
p     
p     

 

    
 

  

  
  bombs

  

 

  

  
  bombs

  

 c    toilets
pff

pond   

  
p     
p     
p     
p     

    

 
   

time  sec 

time  sec 

 

 

  

 

 

   
p     
p     
p     
p     

 

    
 

  

  
  bombs

  

 

  

  
  bombs

  

 d    toilet
figure     the bomb domain  probabilistic ff  left  vs  pond  right  

finally  our last set of problems comes from three cascading modifications of instance nr     of
    of the rovers domain used at the aips   planning competition  this problem instance has  
waypoints    rovers    objectives  and   rock soil samples  from rovers to roversppp we modify
the instance domain as follows 
 rovers is the original aips   problem instance nr     and we use it hear mainly for comparison 
 in roversp  each sample is with chance     at its original waypoint  and with chance    
at each of the others two waypoints  each objective may be visible from   waypoints with
uniform distribution  this is a probabilistic adaptation of the domain suggested by bryce  
kambhampati        
   

fip robabilistic  ff

sandcastle

sandcastle

   
pff
pond

pff
pond  min 
pond  avg 
   

   

  
time  sec 

time  sec 

   

   

 

   

   

 

    
   

   

   

   

   


   

   

   

   

 a 

   

   

   

   


   

   

   

 b 

figure     probabilistic ff and pond on problems from  a  sand castle  and  b  slipperygripper 

 roverspp enhances roversp by conditional probabilities in the initial state  stating that whether
or not an objective is visible from a waypoint depends on whether or not a rock sample  intuition  a large piece of rock  is located at the waypoint  the probability of visibility is much
higher if the latter is not the case  specifically  the visibility of each objective depends on the
locations of two rock samples  and if a rock sample is present  then the visibility probability
drops to     
 roversppp extends roverspp by introducing the need to collect data about water existence 
each of the soil samples has a certain probability       to be wet  for communicated
sample data  an additional operator tests whether the sample was wet  if so  a fact knowthat water contained in the goal is set to true  the probability of being wet depends on the
location of the sample 
we show no runtime plots for logistics  grid  and rovers  since pond runs out of either time or
memory on all considered instances of these domains  table   shows that the scaling behavior of
probabilistic ff in these three domains is similar to that observed in the previous domains  the
goals in the roversppp problem cannot be achieved with probabilities                this is
proved by probabilistic ffs heuristic function  providing the correct answer in split seconds 
    probabilistic actions
our first two domains with probabilistic actions are the famous sand castle  majercik   littman 
      and slippery gripper  kushmerick et al         domains  the domains are simple  but they
posed the first challenges for probabilistic planners  our performance in these domains serves an
indicator of the progress relative to previous ideas for probabilistic planning 
in sand castle  the states are specified by two boolean variables moat and castle  and state
transitions are given by two actions dig moat and erect castle  the goal is to erect the castle 
   

fid omshlak   h offmann

 d walkgrid

 d walkgrid

pff
pond

    

   

   

  

  

time  sec 

time  sec 

    

 

 

   

   

    

    

pff
pond
 

 

 

 

 

  

 

grid width

 a 

 

 

 
 
grid width

 

 

  

 b 

figure     probabilistic ff and pond on problems from  a   d walkgrid with         and  b 
 d walkgrid with         

building a moat with dig moat might fail with probability      erecting a castle with erect castle
succeeds with probability      if the moat has already been built  and with probability       otherwise  if failed  erect castle also destroys the moat with probability      figure    a  shows that
both probabilistic ff and pond solve this problem in less than a second for arbitrary high values
of   with the performance of both planners being almost independent of the required probability of
success 
slippery gripper is already a bit more complicated domain  the states in slippery gripper
are specified by four boolean variables grip dry  grip dirty  block painted  and block held  and
there are four actions dry  clean  paint  and pickup  in the initial state  the block is neither painted
nor held  the gripper is clean  and the gripper is dry with probability      the goal is to have a
clean gripper holding a painted block  action dry dries the gripper with probability      action
clean cleans the gripper with probability       action paint paints the block with probability   
but makes the gripper dirty with probability   if the block was held  and with probability     if it
was not  action pickup picks up the block with probability      if the gripper is dry  and with
probability     if the gripper is wet 
figure    b  depicts  on a log scale  the relative performance of probabilistic ff and pond
on slippery gripper as a function of growing   the performance of probabilistic ff is nicely flat
around      seconds  this time  the comparison with pond was somewhat problematic  because 
for any fixed   pond on slippery gripper exhibited a huge variance in runtime  in figure    b 
we plot the best runtimes for pond  as well as its average runtimes  the best run times for pond
for different values of  vary around a couple of seconds  but the average runtimes are significantly
worse   for some high values of  pond timed out on some sample runs  and thus the plot provides
a lower bound on the average runtimes  
in the next two domains   d walkgrid and  d walkgrid  the robot has to pre plan a sequence of conditional movements taking it from a corner of the grid to the farthest  from the initial
   

fip robabilistic  ff

position  corner  hyafil   bacchus         in  d walkgrid the grid is one dimensional  while
in  d walkgrid the grid is two dimensional  figure    a  depicts  on a log scale  a snapshot of
the relative performance of probabilistic ff and pond on one dimensional grids of width n and
        the robot is initially at         should get to     n   and it can try moving in each of the two
possible directions  each of the two movement actions moves the robot in the right direction with
probability      and keeps it in place with probability      it is easy to see from figure    a  that the
difference between the two planners in this domain is substantialwhile runtime of probabilisticff grows only linearly with x  the same dependence for pond is seemingly exponential 
the  d walkgrid domain is already much more challenging for probabilistic planning  in all
 d walkgrid problems with n  n grids the robot is initially at         should get to  n  n   and it
can try moving in each of the four possible directions  each of the four movement actions advances
the robot in the right direction with probability      in the opposite direction with probability   
and in either of the other two directions with probability      figure    a  depicts  on a log scale 
a snapshot of the relative performance of probabilistic ff and pond on  d walkgrid with very
low required probability of success          and this as a function of the grids width n  the
plot shows that probabilistic ff still scales well with increasing n  though not linearly anymore  
while pond time outs for all grid widths n      for higher values of   however  probabilistic ff
does reach the time out limit on rather small grids  notably n     and n     for         and
        respectively  the reason for this is that probabilistic ffs heuristic function is not good
enough at estimating how many times  at an early point in the plan  a probabilistic action must be
applied in order to sufficiently support a high goal threshold at the end of the plan  we explain this
phenomenon in more detail at the end of this section  where we find that it also appears in a variant
of the well known logistics domain 
our last set of problems comes from the standard logistics domain  each problem instance
x y z contains x locations per city  y cities  and z packages  we will see that probabilistic ff
scales much worse  in logistics  in the presence of probabilistic effects than if there is only initial
state uncertainty  we will explain the reason for this at the end of this section   hence we use much
smaller instances than the ones used above in section      namely  to allow a direct comparison to
previous results in this domain  we closely follow the specification of hyafil and bacchus        
we use instances with configurations x y z                 and        and distinguish between two
levels of uncertainty 
 l x y z correspond to problems with uncertainty only in the outcome of the load and unload
actions  specifically  the probabilities of success for load are       for trucks and     for
airplanes  and for unload       and      respectively 
 ll x y z extends l x y z with independent uniform priors for each initial location of a
package within its start city 
figure    depicts  on a log scale  runtimes of probabilistic ff and pond on l        l       
and l        as a function of growing   on these problems  both planners appear to scale well 
with the runtime of probabilistic ff and the optimal runtime of pond being roughly the same 
and the average runtime of pond somewhat degrading from       to       to        this shows
that both planners are much more efficient in this domain than the previously known sat and csp
based techniques  however  moving to ll x y z changes the picture for both planners  the results
are as follows 
   

fid omshlak   h offmann

l      

l      

   

pff
pond  min 
pond  avg 

 

   

  

time  sec 

  

time  sec 

time  sec 

   
pff
pond  min 
pond  avg 

  

    
    

l      

   
pff
pond  min 
pond  avg 

 

   

    

   


 a 

    

    

    
    

 

   

    

   


 b 

    

    

    
    

    

   

    

    



 c 

figure     probabilistic ff and pond on problems from logistics  a  l         b  l        and
 c  l       

   on ll        the runtimes of probabilistic ff were identical to those on l        and the
optimal runtimes of pond only slightly degraded to    seconds  however  for all examined
values of   some runs of pond resulted in timeouts 
   on ll        the runtimes of probabilistic ff were identical to those on l       for  
                         yet probabilistic ff time outed on          the optimal runtimes
of pond degraded from those for l       only to       seconds  and again  for all values
of   some runs of pond resulted in timeouts 
   on ll        probabilistic ff experienced hard times  finishing in      seconds for   
      and time outing for all other examined values of   the optimal runtimes of pond
degraded from those for l       to          seconds  and here as well  for all values of  
some runs of pond resulted in timeouts 
we also tried a variant of ll x y z with non uniform priors over the initial locations of the packages  but this resulted in a qualitatively similar picture of absolute and relative performance 
the ll x y z domain remains challenging  and deserves close attention in the future developments for probabilistic planning  in this context  it is interesting to have a close look at what the
reasons for the failure of probabilistic ff is  it turns out that probabilistic ff is not good enough
at estimating how many times  at an early point in the plan  a probabilistic action must be applied
in order to sufficiently support a high goal threshold at the end of the plan  to make this concrete 
consider a logistics example with uncertain effects of all load and unload actions  consider a package p that must go from a city a to a city b  lets say that p is initially not at as airport  if the
goal threshold is high  this means that  to be able to succeed  the package has to be brought to as
airport with a high probability before loading it onto an airplane  this is exactly the point where
probabilistic ffs heuristic function fails  the relaxed plan contains too few actions unloading p
at as airport  the effect is that the search proceeds too quickly to loading p onto a plane and
bringing it to b  once the search gets to the point where b should be unloaded to its goal location  the goal threshold cannot be achieved no matter how many times one unloads p  at this point 
   

fip robabilistic  ff

probabilistic ffs enforced hill climbing enters a loop and eventually fails because the relaxed plan
 which over estimates the past achievements  becomes empty   
the challenge here is to devise methods that are better at recognizing how many times p has
to be unloaded at as airport in order to sufficiently support the goal threshold  the error made by
probabilistic ff lies in that our propagation of weights on the implication graph over estimates the
goal probability  note here that this is much more critical for actions that must be applied early on
in the plan  than for actions that are applied later  if an action a appears early on in a plan  then
the relaxed plan  when a is executed  will be long  recall that the weight propagation proceeds
backwards  from the goal towards the current state  at each single backwards step  the propagation
makes an approximation that might lose precision of the results  over several backwards steps 
these imprecisions accumulate  hence the quality of the approximation decreases quickly over the
number of backwards steps  the longer the distance between goal and current state is  the more
information is lost  we have observed this phenomenon in detailed experiments with different
weight propagation schemes  that is  with different underlying assumptions  of the propagation
schemes we tried  the independence assumption  as presented in this paper  was by far the most
accurate one  all other schemes failed to deliver good results even for much shorter distances
between the goal and the current state 
it is interesting to consider how this issue affects pond  which uses a very different method for
estimating the probability of goal achievement  instead of performing a backwards propagation and
aggregation of weight values  pond sends a set of random particles through the relaxed planning
graph in a forward fashion  and stops the graph building if enough particles end up in the goal  from
our empirical results  it seems that this method suffers from similar difficulties as probabilistic ff 
but not to such a large extent  ponds optimal runtimes for ll x y z are much higher than those
for l x y z  this indicates that it is always challenging for pond to recognize the need for
applying some action a many times early on in the plan  more interestingly  pond never times out
in l x y z  but it does often time out in ll x y z  this indicates that  to some extent  it is a matter
of chance whether or not ponds random particles recognize the need for applying an action a
many times early on in the plan  an intuitive explanation is that the good cases are those where
sufficiently many of the particles failed to reach the goal due to taking the wrong effect of a 
based on this intuition  one would expect that it helps to increase the number of random particles in
ponds heuristic function  we did so  running pond on ll x y z with an increased number of
particles      and     instead of the default value of     to our surprise  the qualitative behavior of
pond did not change  time outing in a similar number of cases  it is unclear to us what the reason
for this phenomenon is  certainly  it can be observed that the situation encoded in ll x y z is not
solved to satisfaction by either of probabilistic ffs weight propagation or ponds random particle
methods  in their current configurations 
at the time of writing  it is unclear to the authors how better methods could be devised  it seems
unlikely that a weight propagation  at least one that does not resort to expensive reasoning  exists
which manages long distances better than the independence assumption  an alternative way out
might be to simply define a weaker notion of plans that allows to repeat certain kinds of actions 
    this does not happen in the above l        l        and l       instances simply because they are too small and a
high goal probability can be achieved without thinking too much about the above problem  if one increases the size
of these instances  the problem appears  the problem appears earlier in the presence of initial state uncertainty  even
in small instances such as ll        ll        and ll        because with uncertainty about the start position of
the packages one needs to try unloading them at the start airports more often 

   

fid omshlak   h offmann

throwing a dice or unloading a package  arbitrarily many times  however  since our assumption is
that we do not have any observability during plan execution  when executing such a plan there would
still arise the question how often an action should be tried  since logistics is a fairly well solved
domain in simpler formalisms  by virtue of probabilistic ff  even in the probabilistic setting as
long as the effects are deterministic  we consider addressing this problem as a quite pressing open
question 

   conclusion
we developed a probabilistic extension of conformant ffs search space representation  using
a synergetic combination of conformant ffs sat based techniques with recent techniques for
weighted model counting  we further provided an extension of conformant relaxed planning with
approximate probabilistic reasoning  the resulting planner scales well on a range of benchmark domains  in particular it outperforms its only close relative  pond  by at least an order of magnitude
in almost all of the cases we tried 
while this point may be somewhat obvious  we would like to emphasize that our achievements
do not solve the  this particular  problem once and for all  probabilistic ff inherits strengths and
weaknesses from ff and conformant ff  like domains where ffs or conformant ffs heuristic
functions yield bad estimates  e g  the mentioned cube center variant   whats more  the probabilistic setting introduces several new potential impediments for ffs performance  for one thing 
weighted model counting is inherently harder than sat testing  though this did not happen in our
set of benchmarks  there are bound to be cases where the cost for exact model counting becomes
prohibitive even in small examples  a promising way to address this issue lies in recent methods
for approximate model counting  gomes  sabharwal    selman        gomes  hoffmann  sabharwal    selman         such methods are much more efficient than exact model counters  they
provide high confidence lower bounds on the number of models  the lower bounds can be used in
probabilistic ff in place of the exact counts  it has been shown that good lower bounds with very
high confidecne can be achieved quickly  the challenge here is to extend the methods  which are
currently designed for non weighted cnfs  to handle weighted model counting 
more importantly perhaps  in the presence of probabilistic effects there is a fundamental weakness in probabilistic ffs  and ponds  heuristic information  this becomes a pitfall for performance even in a straightforward adaptation of the logistics domain  which is otherwise very easy
for this kind of planners  as outlined  the key problem is that  to obtain a high enough confidence
of goal achievement  one may have to apply particular actions several times early on in the plan 
neither probabilistic ffs nor ponds heuristics are good enough at identifying how many times 
in our view  finding techniques that address this issue is currently the most important open topic in
this area 
apart from addressing the latter challenge  we intend to work towards applicability in real word
settings  particularly  we will look at the space application settings that our rovers domain hints at 
at medication type treatment planning domains  and at the power supply restoration domain  bertoli 
cimatti  slaney    thiebaux        
   

fip robabilistic  ff

acknowledgments
the authors would like to thank dan bryce and rao kambhampati for providing a binary distribution of pond     carmel domshlak was partially supported by the israel science foundations
grant          as well as by the c  wellner research fund  some major parts of this research have
been accomplished at the time that jorg hoffmann was employed at the intelligent information
systems institute  cornell university 

appendix a  proofs
proposition   let  a  nbi   g    be a probabilistic planning problem described over k state variables  and a be an m step sequence of actions from a  then  we have  nba     o  nbi   m k    
where  is the largest description size of an action in a 
proof  the proof is rather straightforward  and it exploits the local structure of nba s cpts  the
first nodes cpts layer x    of nba constitutes an exact copy of nbi   then  for each    t  m  the
t th layer of nba contains k     node  y t     x t   
first  let us consider the action node y t    while specifying the cpt ty  t  in a straightforward
manner as if prescribed by eq    might result in an exponential blow up  the same eq    suggests
that the original description of at is by itself a compact specification of ty  t    therefore  ty  t 
can be described in space o    and this description can be efficiently used for answering queries
ty  t   y i         as in eq     next  consider the cpt tx t  of a state variable node x t   x t   
this time  it is rather evident from eq    that tx t  can be described in space o   so that queries
tx t   x t    x   x t     x   could be efficiently answered  thus  summing up for all layers
   t  m  the description size of  nba     o  nbi     m k      
lemma   given a node v t    impp t    we have p t   v t        v t    if and only if  given v
at time t   the sequence of effects e impv t  p t    achieves p at t with probability   
proof  the proof of lemma   is by a backward induction on the time layers of impv t  p t    for
time t  the only node of impp t  time stamped with t is p t  itself  for this node we do have
p t   p t       p t        but  given p at time t  an empty plan corresponding to  empty 
e impp t p t    trivially re establishes p at t with certainty  assuming now that the claim holds
for all nodes of impp t  time stamped with t              t  we now show that it holds for the nodes
time stamped with t  
it is easy to see that  for any node v t    impp t    we get p t   v t        v t    only if
 goes down to zero  first  consider the chance nodes  t    impvp t    for such a node  lb is
set to zero if and only if we have p t   r t           for some r  add    however  by our
inductive assumption  in this and only in this case the effects e imp t  p t      achieve p at t with
probability    given the occurrence of  at time t  
now  consider the fact nodes q t    impvp t    for such a node   can get nullified only by
some effect e  e a   a  a t    con e    q  the latter happens if only if  for all possible outcomes of e   i  the node  t   belongs to impp t    and  ii  and the estimate p t    t        t    
in other words  by our inductive assumption  given any outcome    e  at time t   the effects e imp t  p t    achieve p at t with probability    thus  given q at time t   the effects
e impq t  p t    achieve p at t with probability   independently of the actual outcome of e  alternatively  if for q t   we have lb      then for each effect e conditioned on q t   there exists an
   

fid omshlak   h offmann

outcome  of e such that  according to what we just proved for the chance nodes time stamped with
t   the effects e imp t  p t      do not achieve p at t with probability    hence  the whole set of
effects e impq t  p t      does not achieve p at t with probability   
lemma   let  a  nbi   g    be a probabilistic planning task  a be a sequence of actions applicable
in bi   and   
  be a relaxation function for a  for each time step t  m  and each proposition p 
p  if p  t  is constructed by build prpg a  a   nbi    g      
     then p at time t can be achieved
by a relaxed plan starting with a  
 
    with probability      that is  p is not negatively known at time t  if and only if p  up  t p  t  
and
    with probability    that is  p is known at time t  if and only if p  p  t  
proof  the proof of the if direction is by a straightforward induction on t  for t   m the claim
is immediate by the direct initialization of up  m  and p  m   assume that  for m  t   t 
if p  up  t    p  t    then p is not negatively known at time t   and if p  p  t    then p is known
at time t  
first  consider some p t   up  t   p  t   and suppose that p is egatively know at time t  by
the inductive assumption  and the property of the prpg construction that up  t      p  t     
up  t   p  t   we have p   up  t      p  t      therefore  p has to be added into up  t   and
then  possibly  moved from there to p  t   in the first for loop of the build timestep procedure 
however  if so  then there exists an action a  a t      e  e a   and    e  such that  i 
con e   up  t      p  t      and  ii  p  add    again  by the assumption of the induction we
have that pre a  is known at time t     and con e  is not negatively known at time t     hence  the
non zero probability of  occurring at time t implies that p can be achieved at time t with probability
greater than    contradicting that p is negatively know at time t 
now  let us consider some p t   p  t   notice that  for t   m  we have p t   p  t  if and
only if
 
l  
    
 
lsupport p t  

thus  for each world state w consistent with bi   we have either q  w for some fact proposition
q m   support p t    or  for some effect e of an action a t    a t    t   t  we have con e  
p  t   and   t        e    support p t    in this first case  lemma   immediately implies that
the concatenation of a  
  with an arbitrary linearization of the  relaxed  actions a             a t    
achieves p at t with probability    and thus p is known at time t  in the second case  our inductive
assumption implies that con e  is known at time t  and together with lemma   this again implies that
the concatenation of a  
  with an arbitrary linearization of the  relaxed  actions a             a t    
achieves p at t with probability   
the proof of the only if direction is by induction on t as well  for t   m this claim is
again immediate by the direct initialization of p  m   assume that  for m  t   t  if p is not
negatively known at time t   then p  up  t    p  t    and if p is known at time t   then p  p  t   
first  suppose that p is not negatively known at time t  and yet we have p   up  t   p  t   from
our inductive assumption plus that a t     containing all the noop actions for propositions in
up  t      p  t      we know that p is negatively known at time t     if so  then p can become
not negatively known at time t only due to some    e   e  e a   such that pre a  is known
   

fip robabilistic  ff

at time t     and con e  is not negatively known at time t     by our inductive assumption  the
latter conditions imply con e   up  t      p  t      and pre a   p  t      but if so  then p
has to be added to up  t   p  t  by the first for loop of the build timestep procedure  contradicting
our assumption that p   up  t   p  t  
now  let us consider some p known at time t  by our inductive assumption  p  t     contains
all the facts known at time t     and thus a t     is the maximal subset of actions a  
  applicable
at time t     let us begin with an exhaustive classification of the effects e of the actions a t    
with respect to our p at time t 
 i     e    p  add    and con e   p  t    
 ii     e    p  add    and con e   up  t    
 iii     e    p   add   or con e    p  t      up  t    
if the set  i  is not empty  then  by the construction of build w impleafs p t   imp   we have
  t          e    support p t   
for each e   i   likewise  by the construction of build timestep  notably  by the update of    for
each e   i   we have
 
 
 t     
  t    e  

putting these two facts together  we have that eq     holds for p at time t  and thus we have p  p  t  
now  suppose that the set  i  is empty  it is not hard to verify that no subset of only effects  iii 
makes p known at time t  thus  the event at least one of the effects  ii  occurs must hold with
probability    first  by the construction of build w impleafs p t   imp   we have
 
support  p t   
support  con e  t     
e ii 

then  and   from lemma   we have that the event at least one of the effects  ii  occurs holds with
probability   if and only if
 
 
l
e ii 
lsupport con e  t   

putting these two facts together  we have that eq     holds for p at time t  and thus we have p  p  t  

theorem   let  a  nbi   g    be a probabilistic planning task  a be a sequence of actions appli 
cable in bi   and   
  be a relaxation function for a  if build prpg a  a   nbi    g         returns
 
false  then there is no relaxed plan for  a  bi   g    that starts with a    
proof  let t     be the last layer of the prpg upon the termination of build prpg  for every
m  t  t  by the construction of prpg and lemma    the sets p  t   and up  t   contain all
 and only all  propositions that are known  respectively unknown  after executing all the actions in
the action layers up to and including a t     
   

fid omshlak   h offmann

first  let us show that if build prpg returns false  then the corresponding termination criterion would hold in all future iterations  if p  t        p  t   then we have a t        a t  
subsequently  since p  t       up  t        p  t   up  t  and a t        a t   we have
p  t       up  t        p  t       up  t       given that  we now show that p  t        p  t     
and up  t        up  t      
assume to the contrary that there exists p t       p  t      such that p t        p  t       that
is p t       up  t       by the construction of the sets p  t      and p  t      in the build timestep
procedure  we have
 
 
l  
lsupport p t    

 

  

    

l

lsupport p t    

consider an exhaustive classification of the effects e of the actions a t      with respect to our p at
time t     
 i     e    p  add    and con e   p  t     
 ii     e    p  add    and con e   up  t     
 iii     e    p   add   or con e    p  t       up  t     
suppose that the set  i  is not empty  and let e   i   from p  t    p  t      we have that con e  
p  t   and thus   t 
w      e    support p t       
w by the update of  in build timestep we
then have     t   e    t   and thus   lsupport p t     l  contradicting eq     
alternatively  assume that the set  i  is empty  using the arguments similar to these in the proof
of lemma    p t       p  t      and p t        p  t      in this case imply that
 
l
 
e ii 
lsupport con e  t    

 

  

    

l

e ii 
lsupport con e  t  

however  a t        a t   up  t        up  t   and p  t        p  t  together imply that all
the action effects that can possibly take place at time t     are also feasible to take place at time
t  therefore  since for each e   ii  we have con e   up  t      by the definition of  ii   eq    
implies that
 
 
support  con e  t        up  m    
support  con e  t    up  m  
    
e ii 

e ii 

contradicting our termination condition  hence  we arrived into contradiction with our assumption
that p t        p  t      
having shown that p  t        p  t      and up  t        up  t       we now show that the
termination criteria implies that  for each q t       up  t       we have
up  m   support p t         up  m   support p t       
   

fip robabilistic  ff

let ep t    be the set of all effects of actions a t      such that con e   up  t       and  for each
outcome    e   we have p  add    given that  we have
up  m   support p t         up  m  

 

support con e  t      

 

support con e  t  

eep t   

  up  m  

 

    

eep t   

  up  m   support p t      
where the first and third equalities are by the definition of support sets via lemma    and the second
equation is by our termination condition 
the last things that remains to be shown is that our termination criteria implies get p t  
   g   get p t      g   considering the simple cases first  if g   p  t       up  t       from
p  t       up  t        p  t       up  t      we have get p t      g   get p t      g       otherwise  if g  p  t       from p  t        p  t      we have get p t      g   get p t      g      
this leaves us with the case of g  p  t       up  t      and g  up  t           from
p  t        p  t       up  t        up  t       and the termination condition  we have
g  up  t    g  up  t        g  up  t      
from get p t      g   get p t  g  we know that action effects that become feasible only in a t 
do not increase our estimate of probability of achieving any g  g  up  t      from time t to time
t      however  from p  t        p  t   up  t        up  t   and a t        a t   we have that
no action effect will become feasible at time t     if it is not already feasible at time t  and thus
get p t      g   get p t  g  will imply get p t      g   get p t      g  
to this point we have shown that if build prpg returns false  then the corresponding termination criterion would hold in all future iterations  now  assume to the contrary to the claim of
the theorem that build prpg returns false at some iteration t  yet there exists a relaxed plan for
 a  bi   g    that starts with a  
    first  if       then lemma   implies that there exists time t
such that g  p  t    if so  then the persistence of our negative termination condition implies
g  p  t   however  in this case we would have get p t  g       see the second if of the get p
procedure   and thus build prpg would return true before ever getting to check the negative
termination condition in iteration t  alternatively  if       then build prpg would have terminated
with returning true before the negative termination condition is checked even once 
this leaves us with the case of          and get p t  g       get p t  g    will
again contradict reaching the negative termination condition at iteration t   we can also assume that
g  p  t   up  t  because p  t   up  t  contains all the facts that are not negatively known at time
t  and thus persistence of the negative termination condition together with g   p  t   up  t  would
imply that there is no relaxed plan for any       let us consider the sub goals g  up  t      
    if for all subgoals g  g  up  t   the implications in impg t  are only due to deterministic
outcomes of the effects e impg t     then the uncertainty about achieving g  up  t  at time
t is only due to the uncertainty about the initial state  since the initial
v belief state is reasoned
about with no relaxation  in this case get p t  g    wmc   gg p  t  g   provides us
with an upper bound on the probability of achieving our goal g by a  
  concatenated with
   

fid omshlak   h offmann

an arbitrary linearization of an arbitrary subset of a             a t      the termination subcondition get p t      g   get p t  g  and the persistence of the action sets a t    t  t 
imply then that get p t  g  provides us with an upper bound on the probability of achieving g
by a  
  concatenated with an arbitrary linearization of an arbitrary subset of a             a t   
for all t  t  together with get p t  g      the latter conclusion contradicts our assumption
that a desired relaxed plan exists 
    if there exists a subgoal g  g  up  t  such that some implications in impg t  are due to truly
probabilistic outcomes of the effects e impg t 
actions a t  in
v    then repeating the  relaxed  v
a t      will necessarily result in wmc   gg p  t    g     wmc   gg p  t  g   
contradicting our termination sub condition condition get p t      g   get p t  g  
hence  we arrived into contradiction that our assumption that build prpg returns false at time t 
yet there exists a relaxed plan for  a  bi   g    that starts with a  
  

references
bertoli  p   cimatti  a   pistore  m   roveri  m     traverso  p          mbp  a model based planner 
in proc  ijcai   workshop on planning under uncertainty and incomplete information 
seattle  wa 
bertoli  p   cimatti  a   slaney  j     thiebaux  s          solving power supply restoration problems with planning via symbolic model checking  in proceedings of the   th european conference on artificial intelligence  ecai   pp          lion  france 
blum  a  l     furst  m  l          fast planning through planning graph analysis  artificial
intelligence                  
bonet  b     geffner  h          planning as heuristic search  artificial intelligence          
    
bonet  b     geffner  h          planning with incomplete information as heuristic search in belief
space  in proceedings of the  th international conference on artificial intelligence planning
and scheduling systems  aips   pp        breckenridge  co 
boutilier  c   friedman  n   goldszmidt  m     koller  d          context specific independence
in bayesian networks  in proceedings of the twelfth conference on uncertainty in artificial
intelligence  uai   pp          portland  or 
brafman  r  i     domshlak  c          factored planning  how  when  and when not  in proceedings of the   th national conference on artificial intelligence  aaai   pp          boston 
ma 
bryce  d     kambhampati  s          heuristic guidance measures for conformant planning  in
proceedings of the   th international conference on automated planning and scheduling
 icaps   pp          whistler  bc  canada 
bryce  d   kambhampati  s     smith  d          sequential monte carlo in probabilistic planning
reachability heuristics  in proceedings of the   th international conference on automated
planning and scheduling  icaps   pp          cumbria  uk 
   

fip robabilistic  ff

chavira  m     darwiche  a          compiling bayesian networks with local structure  in proceedings of the   th international joint conference on artificial intelligence  ijcai   pp 
          edinburgh  scotland 
darwiche  a          recursive conditioning  artificial intelligence                
darwiche  a          constant space reasoning in dynamic bayesian networks  international journal of approximate reasoning                
dean  t     kanazawa  k          a model for reasoning about persistence and causation  computational intelligence            
dechter  r          bucket elimination  a unified framework for reasoning  artificial intelligence 
          
domshlak  c     hoffmann  j          fast probabilistic planning through weighted model counting  in proceedings of the   th international conference on automated planning and
scheduling  icaps   pp          cumbria  uk 
gomes  c  p   hoffmann  j   sabharwal  a     selman  b          from sampling to model counting 
in proceedings of the   th international joint conference on artificial intelligence  ijcai     hyderabad  india 
gomes  c  p   sabharwal  a     selman  b          model counting  a new strategy for obtaining good bounds  in proceedings of the   th national conference on artificial intelligence
 aaai      pp        boston  ma 
hanks  s     mcdermott  d          modeling a dynamic and uncertain world i  symbolic and
probabilistic reasoning about change  artificial intelligence             
hoffmann  j     nebel  b          the ff planning system  fast plan generation through heuristic
search  journal of artificial intelligence research             
hoffmann  j     brafman  r          conformant planning via heuristic forward search  a new
approach  artificial intelligence                  
huang  j          combining knowledge compilation and search for efficient conformant probabilistic planning  in proceedings of the   th international conference on automated planning
and scheduling  icaps   pp          cumbria  uk 
hyafil  n     bacchus  f          utilizing structured representations and csps in conformant
probabilistic planning  in proceedings of the european conference on artificial intelligence
 ecai   pp            valencia  spain 
jensen  f          an introduction to bayesian networks  springer verlag  new york 
kushmerick  n   hanks  s     weld  d          an algorithm for probabilistic planning  artificial
intelligence                  
little  i   aberdeen  d     thiebaux  s          prottle  a probabilistic temporal planner  in proceedings of the   th national conference on artificial intelligence  aaai      pp      
      pittsburgh  pa 
littman  m  l   goldsmith  j     mundhenk  m          the computational complexity of probabilistic planning  journal of artificial intelligence research         
   

fid omshlak   h offmann

majercik  s  m     littman  m  l          maxplan  a new approach to probabilistic planning  in proceedings of the  th international conference on artificial intelligence planning
systems  aips   pp        pittsburgh  pa 
majercik  s  m     littman  m  l          contingent planning under uncertainty via stochastic
satisfiability  artificial intelligence                   
mcdermott  d          the      ai planning systems competition  ai magazine             
mcdermott  d  v          using regression match graphs to control search in planning  artificial
intelligence                   
onder  n   whelan  g  c     li  l          engineering a conformant probabilistic planner  journal
of artificial intelligence research          
pearl  j          heuristics   intelligent search strategies for computer problem solving  addisonwesley 
pearl  j          probabilistic reasoning in intelligent systems  networks of plausible inference 
morgan kaufmann  san mateo  ca 
rintanen  j          expressive equivalence of formalisms for planning with sensing  in proceedings of the   th international conference on automated planning and scheduling  icaps  
pp          trento  italy 
roth  d          on the hardness of approximate reasoning  artificial intelligence              
    
russell  s     norvig  p          artificial intelligence  a modern approach    edition   pearson 
sang  t   bacchus  f   beame  p   kautz  h     pitassi  t          combining component caching
and clause learning for effective model counting  in  online  proceedings of the  th international conference on theory and applications of satisfiability testing  sat   vancouver  bc 
canada 
sang  t   beame  p     kautz  h          solving bayes networks by weighted model counting  in
proceedings of the   th national conference on artificial intelligence  aaai   pp         
pittsburgh  pa 
shimony  s  e          the role of relevance in explanation i  irrelevance as statistical independence  international journal of approximate reasoning               
shimony  s  e          the role of relevance in explanation ii  disjunctive assignments and approximate independence  international journal of approximate reasoning              
zhang  n  l     poole  d          a simple approach to bayesian network computations  in
proceedings of the   th canadian conference on artificial intelligence  pp          banff 
alberta  canada 

   

fi