journal of articial intelligence research                  

submitted        published      

learning to play using low complexity rule based policies 
illustrations through ms  pac man
istvn szita
andrs lrincz

szityu eotvos elte hu
andras lorincz elte hu

dept  of information systems
etvs university  hungary  h     

abstract
in this article we propose a method that can deal with certain combinatorial reinforcement learning tasks  we demonstrate the approach in the popular ms  pac man game  we
dene a set of high level observation and action modules  from which rule based policies
are constructed automatically  in these policies  actions are temporally extended  and may
work concurrently  the policy of the agent is encoded by a compact decision list  the components of the list are selected from a large pool of rules  which can be either hand crafted
or generated automatically  a suitable selection of rules is learnt by the cross entropy
method  a recent global optimization algorithm that ts our framework smoothly  crossentropy optimized policies perform better than our hand crafted policy  and reach the score
of average human players  we argue that learning is successful mainly because  i  policies
may apply concurrent actions and thus the policy space is suciently rich   ii  the search is
biased towards low complexity policies and therefore  solutions with a compact description
can be found quickly if they exist 

   introduction
during the last two decades  reinforcement learning  rl  has reached a mature state  and
has been laid on solid foundations  we have a large variety of algorithms  including valuefunction based  direct policy search and hybrid methods  for reviews on these subjects 
see  e g   the books of bertsekas and tsitsiklis        and sutton and barto         the
basic properties of many such algorithms are relatively well understood  e g  conditions
for convergence  complexity  the eect of various parameters  although it is needless to say
that there are still lots of important open questions  there are also plenty of test problems
 like various maze navigation tasks  pole balancing  car on the hill etc   on which the
capabilities of rl algorithms have been demonstrated  and the number of large scale rl
applications is also growing steadily  however  current rl algorithms are far from being
out of the box methods  so there is still need for more demonstrations showing that rl can
be ecient in complex tasks 
we think that games  including the diverse set of classical board games  card games  modern computer games  etc   are ideal test environments for reinforcement learning  games
are intended to be interesting and challenging for human intelligence and therefore  they
are ideal means to explore what articial intelligence is still missing  furthermore  most
games t well into the rl paradigm  they are goal oriented sequential decision problems 
where each decision can have long term eects  in many cases  hidden information  random
events  unknown environment  known or unknown players account for  part of  the diculty
c      ai access foundation  all rights reserved 


fiszita   lrincz

of playing the game  such circumstances are in the focus of reinforcement learning  games
are also attractive for testing new methods  the decision space is huge in most cases  so
nding a good strategy is a challenging task 
there is another great advantage of using games as test problems  the rules of the games
are xed  so the danger of  tailoring the task to the algorithm   i e   to tweak the rules
and or the environment so that they meet the capabilities of the proposed rl algorithm 
is reduced  compared  e g   to various maze navigation tasks 
rl has been tried in many classical games  including checkers  samuel         backgammon  tesauro         and chess  baxter  tridgell    weaver         on the other hand 
modern computer games got into the spotlight only recently  and there are not very many
successful attempts to learn them with ai tools  notable exceptions are  for example  roleplaying game baldur s gate  spronck  sprinkhuizen kuyper    postma         real time
strategy game wargus  ponsen   spronck         and possibly  tetris  szita   lrincz 
       these games pose new challenges to rl  for example  many observations have to be
considered in parallel  and both the observation space and the action space can be huge 
in this spirit  we decided to investigate the arcade game ms  pac man  the game is
interesting on its own as it is largely unsolved  but also imposes several important questions
in rl  which we will overview in section    we will provide hand coded high level actions
and observations  and the task of rl is to learn how to combine them into a good policy  we
will apply rule based policies  because they are easy to interpret and enable one to include
human domain knowledge easily  for learning  we will apply the cross entropy method  a
recently developed general optimization algorithm  we will show that the hybrid approach
is more successful than either tabula rasa learning or a hand coded strategy alone 
in the next section we introduce the ms  pac man game briey and discuss how it can
be formalized as a reinforcement learning task  in sections   and    we shall shortly describe
the cross entropy optimization method and rule based policies  respectively  in section   
details of the learning experiments are provided  and in section   we present our results 
section   provides a review of related literature  and nally  in section   we summarize and
discuss our approach with an emphasis on the implications for other rl problems 

   pac man and reinforcement learning
the video game pac man was rst released in       and reached immense success  it is
considered to be one of the most popular video games to date  wikipedia        
the player maneuvers pac man in a maze  see fig      while pac man eats the dots in
the maze  in this particular maze there are     dots   each one is worth    points  a level
is nished when all the dots are eaten  to make things more dicult  there are also four
ghosts in the maze who try to catch pac man  and if they succeed  pac man loses a life 
initially  he has three lives  and gets an extra life after reaching        points 
there are four power up items in the corners of the maze  called power dots  worth
   points   after pac man eats a power dot  the ghosts turn blue for a short period    
seconds   they slow down and try to escape from pac man  during this time  pac man is
   the maze of the original pac man game is slightly dierent  this description applies to the opensource pac man implementation of courtillat         the two versions are about equivalent in terms of
complexity and entertainment value 

   

filearning to play ms  pac man

figure    a snapshot of the pac man game
able to eat them  which is worth               and      points  consecutively  the point
values are reset to     each time another power dot is eaten  so the player would want to
eat all four ghosts per power dot  if a ghost is eaten  his remains hurry back to the center
of the maze where the ghost is reborn  at certain intervals  a fruit appears near the center
of the maze and remains there for a while  eating this fruit is worth     points 
our investigations are restricted to learning an optimal policy for the rst level  so the
maximum achievable score is                                                       plus
    points for each time a fruit is eaten 
in the original version of pac man  ghosts move on a complex but deterministic route  so
it is possible to learn a deterministic action sequence that does not require any observations 
many such patterns were found by enthusiastic players  in most of pac man s sequels  most
notably in ms  pac man  randomness was added to the movement of the ghosts  this way 
there is no single optimal action sequence  observations are necessary for optimal decision
making  in other respects  game play was mostly unchanged 
in our implementation  ghosts moved randomly in     of the time and straight towards
pac man in the remaining      but ghosts may not turn back  following koza        chapter
     to emphasize the presence of randomness  we shall refer to our implementation as a
ms  pac man clone 

    ms  pac man as an rl task
ms  pac man meets all the criteria of a reinforcement learning task  the agent has to make
a sequence of decisions that depend on its observations  the environment is stochastic
 because the paths of ghosts are unpredictable   there is also a well dened reward function
 the score for eating things   and actions inuence the rewards to be collected in the future 
   

fiszita   lrincz

the full description of the state would include     whether the dots have been eaten  one
bit for each dot and one for each power dot       the position and direction of ms  pac man 
    the position and direction of the four ghosts      whether the ghosts are blue  one bit for
each ghost   and if so  for how long they remain blue  in the range of   to    seconds     
whether the fruit is present  and the time left until it appears disappears     the number
of lives left  the size of the resulting state space is astronomical  so some kind of function
approximation or feature extraction is necessary for rl 
the action space is much smaller  as there are only four basic actions  go north south east west  however  a typical game consists of multiple hundreds of steps  so the
number of possible combinations is still enormous  this indicates the need for temporally
extended actions 
we have a moderate amount of domain knowledge on ms  pac man  for one  it is quite
easy to dene high level observations and action modules that are potentially useful  on
the other hand  constructing a well performing policy seems much more dicult  therefore 
we provide mid level domain knowledge to the algorithm  we use domain knowledge to
preprocess the state information and to dene action modules  on the other hand  it will be
the role of the policy search reinforcement learning to combine the observations and modules
into rule based policies and nd their proper combination 

   the cross entropy method
our goal is to optimize rule based policies by performing policy search in the space of
all legal rule based policies  for this search we apply the cross entropy method  cem   a
recently published global optimization algorithm  rubinstein         it aims to nd the
 approximate  solution for global optimization tasks in the following form

x    arg max f  x  
x

where f is a general objective function  e g   we do not need to assume continuity or dierentiability   below we summarize the mechanism of this method briey  see also section    
for an overview of applications  

    an intuitive description
while most optimization algorithms maintain a single candidate solution x t  in each time
step  cem maintains a distribution over possible solutions  from this distribution  solution
candidates are drawn at random  this is essentially random guessing  but with a nice trick
it is turned into a highly eective optimization method 
      the power of random guessing

random guessing is an overly simple  optimization  method  we draw many samples from
a xed distribution g   then select the best sample as an estimation of the optimum  in
the limit case of innitely many samples  random guessing nds the global optimum  we
have two notes here   i  as it has been shown by wolpert and macready         for the
most general problems  uniform random guessing is not worse than any other method   ii 
nonetheless  for practical problems  uniform random guessing can be extremely inecient 
   

filearning to play ms  pac man

thus  random guessing is safe to start with  but as one proceeds with the collection of
experience  it should be limited as much as possible 
the eciency of random guessing depends greatly on the distribution g from which the
samples are drawn  for example  if g is sharply peaked around x   then very few samples
may be sucient to get a good estimate  the case is the opposite  if the distribution is
sharply peaked around x    x   a tremendous number of examples may be needed to get a
good estimate of the global optimum  naturally  nding a good distribution is at least as
hard as nding x  
      improving the efficiency of random guessing

after drawing moderately many samples from distribution g   we may not be able to give
an acceptable approximation of x   but we may still obtain a better sampling distribution 
the basic idea of cem is that it selects the best few samples  and modies g so that it
becomes more peaked around them  consider an example  where x is a     vector and g is
a bernoulli distribution for each coordinate  suppose that we have drawn      samples and
selected the    best  if we see that in the majority of the selected samples  the ith coordinate
is    then cem shifts the bernoulli distribution of the corresponding component towards   
afterwards  the next set of samples is drawn already from the modied distribution 
the idea seems plausible  if for the majority of the best scoring samples the ith coordinate
was    and there is a structure in the tness landscape  then we may hope that the ith
coordinate of x is also    in what follows  we describe the update rule of cem in a more
formal way and sketch its derivation 

    formal description of the cross entropy method
we will pick g from a family of parameterized distributions  denoted by g   and describe an
algorithm that iteratively improves the parameters of this distribution g  
let n be the number of samples to be drawn  and let the samples x              x n   be
drawn independently from distribution g   for each   r  the set of high valued samples 

l     x i    f  x i          i  n   
provides an approximation to the level set

l     x   f  x     
let u be the uniform distribution over the level set l   for large values of    this distribution will be peaked around x   so it would be suitable for random sampling  this raises two
potential problems   i  for large  values l will contain very few points  possibly none  
making accurate approximation impossible  and  ii  the level set l is usually not a member
of the parameterized distribution family 
the rst problem is easy to avoid by choosing lower values for    however  we have to
make a compromise  because setting  too low would inhibit large improvement steps  this
compromise is achieved as follows  cem chooses a ratio          and adjusts l to be the
set of the best   n samples  this corresponds to setting     f  x n      provided that the
samples are arranged in decreasing order of their values  the best   n samples are called
the elite samples  in practice   is typically chosen from the range             
   

fiszita   lrincz

the other problem is solved by changing the goal of the approximation  cem chooses
the distribution g from the distribution family g that approximates best the empirical distribution over l   the best g is found by minimizing the distance of g and the uniform
distribution over the elite samples  the measure of distance is the cross entropy distance
 often called kullback leibler divergence   the cross entropy distance of two distributions
g and h is dened as
z
g x 
dce  g  h    g x  log
dx
h x 
the general form of the cross entropy method is summarized in table    it is known that
under mild regularity conditions  the ce method converges with probability    margolin 
       furthermore  for a suciently large population  the global optimum is found with
high probability 
input  g
input  g   g
input  n
input  
input  t
for t from   to t    
for i from   to n  
draw x i  from distribution gt
compute fi    f  x i   
sort fi  values in descending order
t      fn
et       x i    f  x i     t    
gt      arg mingg dce  g  uniform et     
end loop

 
 
 
 
 
 

parameterized distrib  family
initial distribution
population size
selection ratio
number of iterations
cem iteration main loop

  draw n samples
  evaluate them
  level set threshold
  get elite samples
  get nearest distrib  from g

table    pseudo code of the general cross entropy method

    the cross entropy method for bernoulli distribution
for many parameterized distribution families  the parameters of the minimum cross entropy
member can be computed easily from simple statistics of the elite samples  we provide the
formulae for bernoulli distributions  as these will be needed for the policy learning procedure
detailed in the next section  derivations as well as a list of other discrete and continuous
distributions that have simple update rules can be found in the tutorial of de boer  kroese 
mannor  and rubinstein        
let the domain of optimization be d         m   and each component be drawn from
independent bernoulli distributions  i e   g   bernoullim   each distribution g  g is parameterized with an m dimensional vector p    p            pm    when using g for sampling 
   

filearning to play ms  pac man

component j of the sample x  d will be

   with probability pj  
xj  
   with probability    pj  
after drawing n samples x              x n   and xing a threshold value    let e denote the set
of elite samples  i e  
e     x i    f  x i      
with this notation  the distribution g   with minimum ce distance from the uniform distribution over the elite set has the following parameters 

p      p             p m    where
p
p
 i 
 i 
x i  e  xj     
x i  e  xj     
 
p
 
pj   
n
x i  e  

   

in other words  the parameters of g   are simply the component wise empirical probabilities
of   s in the elite set  for the derivation of this rule  see the tutorial of de boer et al         
changing the distribution parameters from p to p  can be too coarse  so in some cases 
applying a step size parameter  is preferable  the resulting algorithm is summarized in
table   
input  p     p              p  m  
input  n
input  
input  t
for t from   to t    
for i from   to n  
draw x i  from bernoullim  pt  
compute fi    f  x i   
sort fi  values in descending order
t      fn
et       x i    f  x i     t    
p

 i 
p j   
x i  e  xj          n  
pt   j      p j          pt j
end loop

 
 
 
 
 

initial distribution parameters
population size
selection ratio
number of iterations
cem iteration main loop

  draw n samples
  evaluate them
 
 
 
 

level set threshold
get elite samples
get parameters of nearest distrib 
update with step size 

table    pseudo code of the cross entropy method for bernoulli distributions
we will also need to optimize functions over d                  k m with k      in the
simplest case  distributions over this domain can be parameterized
by m  k parameters 
pk
p    p              p  k           pm             pm k   with    pj k    and k   pj k     for each j  this
is a special case of the multinomial distribution  
the update rule of the parameters is essentially the same as eq    for the bernoulli case 
p
p
 i 
 i 
 i  e  xj   k 
x i  e  xj   k 
x
 
p
 
 
   
pj k   
n
x i  e  
   

fiszita   lrincz

note that constraint

pk

 
k   pj k

    is satised automatically for each j  

   rule based policies
in a basic formulation  a rule is a sentence of the form  if  condition  holds  then do
 action   a rule based policy is a set of rules with some mechanism for breaking ties  i e  
to decide which rule is executed  if there are multiple rules with satised conditions 
rule based policies are human readable  it is easy to include domain knowledge  and they
are able to represent complex behaviors  for these reasons  they are often used in many
areas of articial intelligence  see section     for a short overview of related literature  
in order to apply rule based policies to ms  pac man  we need to specify four things 
    what are the possible actions     what are the possible conditions and how are they
constructed from observations      how to make rules from conditions and actions  and
    how to combine the rules into policies  the answers will be described in the following
sections 

    action modules
while dening the action modules for ms  pac man  we listed only modules that are easy
to implement but are considered potentially useful  see table     this way  we kept human work at a minimum  but still managed to formalize a part of our domain knowledge
about the problem  as a consequence  this list of action modules is by no means optimal  some actions could be more eective with a more appropriate denition  others
may be superuous  for example  there are four dierent modules for ghost avoidance 
fromghost escapes from the nearest ghost  without considering the position of the other
ghosts  tolowerghostdensity tries to take into account the inuence of multiple ghosts 
fromghostcenter moves out from the geometrical center of ghosts  thus  it is able to avoid
being surrounded and trapped  but  on the other hand  it can easily bump into a ghost
while doing so  and nally  toghostfreearea considers the whole board in search of a safe
location  so that the agent can avoid being shepherded by the ghosts  all of these modules
may have their own strengths and weaknesses  and possibly a combination of them is needed
for success  there can also be actions  which are potentially useful  but were not listed here
 for example  moving towards the fruit  
note also that the modules are not exclusive  for example  while escaping from the
ghosts  ms  pac man may prefer the route where more dots can be eaten  or she may want to
head towards a power dot  without the possibility of concurrent actions  the performance of
the ms  pac man agent may be reduced considerably  which is investigated in experimental
section      
we need a mechanism for conict resolution  because dierent action modules may suggest dierent directions  we do this by assigning priorities to the modules  when the agent
switches on an action module  she also decides about its priority  this is also a decision 
and learning this decision is part of the learning task  
   action priorities are learnt indirectly  each rule has a xed priority  and when an action is switched
on by a rule  it also inherits this priority  the same action can be switched on by dierent rules with
dierent priorities  the mechanism is described in detail in section     

   

filearning to play ms  pac man

table    list of action modules used for rule construction 
name

description

todot
topowerdot
frompowerdot

go towards the nearest dot 
go towards the nearest power dot 
go in direction opposite to the nearest power
dot 
go towards the nearest edible  blue  ghost 
go in direction opposite to the nearest ghost 
go towards the maximally safe junction  for all
four directions  the safety of the nearest junction is estimated in that direction  if ms  pacman is n steps away from the junction and the
nearest ghost is k steps away  then the safety
value of this junction is n  k   a negative value
means that ms  pac man possibly cannot reach
that junction 
go in a direction which maximizes the euclidean
distance from the geometrical center of ghosts 
go further in the current direction  or choose a
random available action  except turning back  if
that is impossible 
go in the direction where the cumulative ghost
density decreases fastest  each ghost denes a
density cloud  with radius      and linear decay   from which the cumulative ghost density is
calculated 
choose a location on the board where the minimum ghost distance is largest  and head towards
it on the shortest path 

toedghost
fromghost
tosafejunction

fromghostcenter
keepdirection
tolowerghostdensity

toghostfreearea

   

fiszita   lrincz

table    list of observations used for rule construction  distances denote the length of the
shortest path  unless noted otherwise  distance to a particular object type is  
if no such object exists at that moment 
name

description

constant
nearestdot
nearestpowerdot
nearestghost
nearestedghost
maxjunctionsafety

constant   value 
distance of nearest dot 
distance of nearest power dot 
distance of nearest ghost 
distance of nearest edible  blue  ghost 
for all four directions  the safety of the nearest
junction in that direction is estimated  as dened
in the description of action tosafejunction 
the observation returns the value of the maximally safe junction 
euclidean distance from the geometrical center
of ghosts 
euclidean distance from the geometrical center
of uneaten dots 
each ghost denes a density cloud  with radius
     and linear decay   returns the value of the
cumulative ghost density 
travelling salesman distance to ghosts  the
length of the shortest route that starts at
ms  pac man and reaches all four ghosts  not
considering their movement  

ghostcenterdist
dotcenterdist
ghostdensity
totaldisttoghosts

we implemented this with the following mechanism  a decision of the agent concerns
action modules  the agent can either switch on or  switch o an action module  that is  in
principle  the agent is able to use any subset of the action modules  instead of selecting a
single one at each time step  basically  the module with highest priority decides the direction
of ms  pac man  if there are more than one equally ranked directions  then lower priority
modules are checked  if the direction cannot be decided after checking switched on modules
in the order of decreasing priority  for example  no module is switched on  or two directions
are ranked equally by all switched on modules   then a random direction is chosen 
ms  pac man can make decisions each time she advances a whole grid cell  the above
mechanism ensures that she never stands still   according to    game ticks or approx     
seconds of simulated game time 

    observations  conditions and rules
similarly to actions  we can easily dene a list of observations which are potentially useful
for decision making  the observations and their descriptions are summarized in table   
   

filearning to play ms  pac man

modules could have been improved in many ways  for example  checking whether there is
enough time to intercept edible ghosts when calculating nearestedghost or taking into
consideration the movement of ghosts when calculating nearestghost  nearestedghost or
maxjunctionsafety  we kept the implementation of the modules as simple as possible 
we designed reasonable modules  but no eort was made to make the module denitions
optimal  complete or non redundant 
now we have the necessary tools for dening the conditions of a rule  a typical condition
is true if its observations are in a given range  we note that the status of each action module
is also important for proper decision making  for example  the agent may decide that if a
ghost is very close  then she switches o all modules except the escape module  therefore
we allow conditions that check whether an action module is  on  or  o  
for the sake of simplicity  conditions were restricted to have the form  observation 
   value    observation     value    action     action    or the conjunction of such
terms  for example 

 nearestdot    and  nearestghost    and  fromghost  
is a valid condition for our rules 
once we have conditions and actions  rules can be constructed easily  in our implementation  a rule has the form  if  condition   then  action   for example 

if  nearestdot    and  nearestghost    and  fromghost  
then fromghostcenter 
is a valid rule 

    constructing policies from rules
decision lists are standard forms of constructing policies from single rules  this is the
approach we pursue here  too  decision lists are simply lists of rules  together with a
mechanism that decides the order in which the rules are checked 
each rule has a priority assigned  when the agent has to make a decision  she checks her
rule list starting with the ones with highest priority  if the conditions of a rule are fullled 
then the corresponding action is executed  and the decision making process halts 
note that in principle  the priority of a rule can be dierent from the priority of action
modules  however  for the sake of simplicity  we make no distinction  if a rule with priority
k switches on an action module  then the priority of the action module is also taken as k  
intuitively  this makes sense  if an important rule is activated  then its eect should also be
important  if a rule with priority k switches o a module  then it is executed  regardless of
the priority of the module 
it may be worth noting that there are many possible alternatives for ordering rules and
actions 

 each rule could have a xed priority  as a part of the provided domain knowledge
 spronck  ponsen  sprinkhuizen kuyper    postma        
 the priority of a rule could be a free parameter that should be learned by the cem
method 
   

fiszita   lrincz

 instead of absolute priorities  the agent could also learn the relative ordering of rules
 timuri  spronck    van den herik        
 the order of rules could be determined by some heuristic decision mechanism  for
example  the generality of the rule  e g   rules with few many conditions and large small domains  could be taken into account  such heuristics have been used in linear
classier systems  see e g  the work of bull   kovacs       
in principle  one would like to nd interesting solutions using the computer with minimal
bias from  domain knowledge   in this regard  the eciency of our simple priority management method was satisfactory  so we did not experiment with other priority heuristics 

    an example
let us consider the example shown in table    this is a rule based policy for the ms  pacman agent 
table    a hand coded policy for playing ms  pac man  bracketed numbers denote
priorities      is the highest priority 
   
   
   
   
   
   
   
   

if
if
if
if
if
if
if
if

nearestghost   then fromghost 
nearestghost   and junctionsafety   then fromghostnearestedghost    then toedghostnearestedghost    then toedghost 
constant   then keepdirection 
frompowerdot  then topowerdot 
ghostdensity     and nearestpowerdot   then frompowerdot 
nearestpowerdot    then frompowerdot 

the rst two rules manage ghost avoidance  if a ghost is too close  then the agent should
ee  and she should do so until she gets to a safe distance  ghost avoidance has priority
over any other activities  the next two rules regulate that if there is an edible ghost on
the board  then the agent should chase it  the value of nearestedghost is innity        if
there are no edible ghosts  but it is     on our board  if there are   this activity has also
relatively high priority  because eating ghosts is worth lots of points  but it must be done
before the blueness of the ghosts disappears  so it must be done quickly  the fth rule says
that the agent should not turn back  if all directions are equally good  this rule prevents
unnecessary zigzagging  while no dots are being eaten   and it is surprisingly eective  the
remaining rules tweak the management of power dots  basically  the agent prefers to eat
a power dot  however  if there are blue ghosts on the board  then a power dot resets the
score counter to      so it is a bad move  furthermore  if ghost density is low around the
agent  then most probably it will be hard to collect all of the ghosts  so it is preferable to
wait with eating the power dot 
   

filearning to play ms  pac man

    the mechanism of decision making
the mechanism of decision making is depicted in fig    in short  the  hidden  state space
is the world of the ms  pac man and the ghosts  the dynamics of this  hidden  statespace determines the vector of observations  which can be checked by the conditions  if the
conditions of a rule are satised  the corresponding action module is switched on or o  as
a consequence  multiple actions may be in eect at once  for example  the decision depicted
in fig    sets two actions to work together 

figure    decision making mechanism of the ms  pac man agent  at time step t 
the agent receives the actual observations and the state of her action modules  she
checks the rules of her script in order  and executes the rst rule with satised
conditions 
initially  each action module is in switched o state  after a module has been switched
on  it remains so until it is either explicitly switched o or another module of the same
priority is switched on and replaces it 

    learning rule based policies by cem
we will apply cem for searching in the space of rule based policies  learning is composed of
three phases      the generation of random policies drawn according to the current parameter
set      evaluation of the policies  which consists of playing a game of ms  pac man to
measure the score  and     updating the parameter set using the cem update rules 
      drawing random scripts from a predefined rule base

suppose that we have a predened rule base containing k rules  for example  the one listed
in appendix a   a policy has m rule slots  each slot can be lled with any of the k rules 
   

fiszita   lrincz

or left empty  as a result  policies could contain up to m rules  but possibly much less  each
rule slot has a xed priority  too  from the set             the priority of a rule slot does not
change during learning  learning can  however  push an important rule to a high priority
slot from a low priority one  and vice versa 
for each    i  m  slot i was lled with a rule from the rule base with probability pi  
and left empty with probability    pi   if it was decided that a slot should
pkbe lled  then
a particular rule j     j  k   was selected with probability qi j   where j   qi j     for
each slot i              m   as a result  policies could contain up to m rules  but possibly much
less  both the pi values and the qi j values are learnt simultaneously with the cross entropy
method  table     using the update rules     and      respectively  this gives a total of
m   m  k parameters to optimize  although the eective number of parameters is much
less  because the qi j values of unused slots are irrelevant   initial probabilities are set to
pi       and qi j     k  
      drawing random rules without a predefined rule base

we studied situations with lessened domain knowledge  we did not use a predened rulebase  script generation was kept the same  but the rule base of k rules was generated
randomly  in this case we generated dierent rule bases for each of the m rule slots  the low
ratio of meaningful rules was counteracted by increased rule variety 
a random rule is a random pair of a randomly drawn condition set and a randomly
drawn action  random condition sets contained   conditions  a random action is constructed as follows  an action module is selected uniformly from the set of modules listed in
table    and switched on or o with probability      the construction of a random condition starts with the uniformly random selection of a module from either table   or table
   if the selected module is an action  then the condition will be  action   or  action  
with equal probability  if the selected module is an observation  then the condition will be
 observation   value  or  observation   value  with equal probability  where  value 
is selected uniformly from a ve element set  the values in this set were determined separately for each observation module as follows  we played     games using a xed policy and
recorded the histogram of values for each observation  subsequently  the ve element set
was determined so that it would split the histogram into regions of equal area  for example 
the value set for nearestghost was                  
the design of the random rule generation procedure contains arbitrary elements  e g 
the number of conditions in a rule  the number of values an observation can be compared
to   the intuition behind this procedure was to generate rules that are suciently versatile 
but the ratio of meaningless rules  e g  rules with unsatisable conditions  is not too large 
however  no optimization of any form was done at this point 

   description of experiments
according to our assumptions  the eectiveness of the above described architecture is based
on three pillars      the human domain knowledge provided by the modules and rules     
   according to our preliminary experiments  the quality of the learned policy did not improve by increasing
the priority set or the number of the slots 

   

filearning to play ms  pac man

the eectiveness of the optimization algorithm      the possibility of concurrent actions 
below  we describe a set of experiments that were designed to test these assumptions 

    the full architecture
in the rst experiment  random rules are used  in their construction  we use all the modules
dened in sections     and      in the second experiment  rules were not generated randomly 
but were hand coded  in this case  the role of learning is only to determine which rules should
be used 
      learning with random rule construction

in the rst experiment  the rule base was generated randomly  as described in section       
the number of rule slots was xed to m        priorities were distributed evenly   each one
containing k       randomly generated rules  the values for k and m were selected by
coarse search over parameter space 
the parameters of cem were as follows  population size n         selection ratio
         step size          these values for  and  are fairly standard for cem  and
we have not tried varying them  in each step  the probabilities of using a rule slot  that
is  the values pi   but not qi j   were slightly decreased  by using a decay rate of         
with larger decay rate  useful rules were also annulled too often  on the other hand  smaller
decay did not aect the performance  but many superuous rules were left in the policies 
the score of a given policy has huge variance due to the random factors in the game 
therefore  to obtain reliable tness estimations  the score of each policy was averaged over
  subsequent games  learning lasted for    episodes  which was sucient to tune each
probability close to either   or    we performed    parallel training runs  this experiment
type is denoted as ce randomrb 
      learning with hand coded rules

in the second experiment we constructed a rule base of k      hand coded rules  shown in
appendix a  that were thought to be potentially useful  these could be placed in one of
the m      rule slots   other parameters of the experiment were identical to the previous
one  this experiment type is denoted as ce fixedrb 

    the eect of the learning algorithm
in the following experiment  we compared the performance of cem to simple stochastic
gradient optimization  this single comparison is not sucient to measure the eciency
of cem  it serves to provide a point of reference  the comparison is relevant  because
these algorithms are similar in complexity and both of them move gradually towards the
best samples that were found so far  the dierence is that sg maintains a single solution
   note that  is the per episode learning rate  this would correspond to a per instance learning rate of
        n           for an on line learning algorithm 
   in contrast to the previous experiment  all of the rules are meaningful and potentially useful  therefore
there is no need for a large pool of rules  and a much lower m can be used  we found that the algorithm
is fairly insensitive to the choice of m  signicant changes in performance can be observed if parameter
m is modied by a factor of   

   

fiszita   lrincz

candidate at a time  whereas cem maintains a distribution over solutions  thus  cem
maintains a memory over solutions and becomes less fragile to occasional wrong parameter
changes 
the particular form of stochastic gradient search was the following  the initial policy was
drawn at random  consisting of   rules   after that  we generated     random mutation of
the current solution candidate at each step  and evaluated the obtained policies  the bestperforming mutation was chosen as the next solution candidate  mutations were generated
using the following procedure      in each rule  each condition was changed to a random new
condition with probability           in each rule  the action was changed to a random new
action with probability       the listed parameter values  number of rules in policy  number
of mutated policies  probabilities of mutation  were the results of coarse parameter space
optimization 
the number of episodes was set to      this way  we evaluated the same number of
dierent policies          as in the cem experiments  both the random rule base and the
xed rule base experiments were repeated using the stochastic gradient method  executing
   parallel training runs  the resulting policies are denoted as sg randomrb and sgfixedrb  respectively 

    the eect of parallel actions
according to our assumptions  the possibility of parallel actions plays a crucial role in the
success of our architecture  to conrm this assumption  we repeated previous experiments
with concurrent actions disabled  if the agent switches on an action module  all other
action modules are switched o automatically  these experiment types are denoted as
ce randomrb  action  ce fixedrb  action  sg randomrb  action and sgfixedrb  action 

    baseline experiments
in order to isolate and assess the contribution of learning  we performed two additional
experiments with dierent amounts of domain knowledge and no learning  furthermore  we
asked human subjects to play the game 
      random policies

in the rst non learning experiment  we used the rule base of    hand coded rules  identical
to the rule base of ce fixedrb   ten rules were selected at random  and random priorities
were assigned to them  we measured the performance of policies constructed this way 
      hand coded policy

in the second non learning experiment  we hand coded both the rules and the priorities  that
is  we hand coded the full policy  the policy is shown in table    and has been constructed
by some trial and error  naturally  the policy was constructed before knowing the results of
the learning experiments 
   

filearning to play ms  pac man

table    ms  pac man results  see text for details  abbreviations  ce  learning with
the cross entropy method  sg  learning with stochastic gradient  randomrb 
randomly generated rule base  fixedrb  xed  hand coded rule base   action 
only one action module can work at a time 
method

avg  score

         percentiles 

    
    
    

           
           
           
           

ce randomrb  action
ce fixedrb  action
sg randomrb  action
sg fixedrb  action

    
    
    
    

           
            
           
           

random policy
hand coded policy
human play

   
    
    

         
           
            

ce randomrb
ce fixedrb
sg randomrb
sg fixedrb

    

      human play

in the nal experiment  ve human subjects were asked to play the rst level of ms  pacman and we measured their performance  each of the subjects has played pac man and or
similar games before  but none of them was an experienced player 

   experimental results
human experiments were performed on the rst level of an open source pac man clone of
courtillat         for the other experiments we applied the delphi re implementation of
the code 
in all learning experiments     parallel learning runs were executed  each one for   
episodes  this training period was sucient to tune all probabilities close to either   or
   so the learned policy could be determined unambiguously in all cases  each obtained
policy was tested by playing    consecutive games  giving a total of     test games per
experiment  in the non learning experiments the agents played     test games  too  using
random policies and the hand coded policy  respectively  each human subject played   
games  giving a total of     test games  results are summarized in table    we provide
    and     percentile values instead of the variances  because the distribution of scores
is highly non gaussian 
   the fact that the average is smaller than the     percentile is caused by a highly skewed distribution
of scores  in most games  the agent reached a score in the range            except for a few games
with extremely low score  these few games did not aect the     percentile but lowered the average
signicantly 

   

fiszita   lrincz

   
   
   
   
   
   

if
if
if
if
if
if

nearestghost   then fromghost 
maxjunctionsafety   then fromghostnearestedghost    then topowerdot 
nearestedghost    then toedghost 
ghostdensity     and nearestpowerdot   then frompowerdot 
constant   then tocenterofdots 

figure    best policy learned by ce fixedrb  average score over    games       
   
   
   
   
   
   
   

if
if
if
if
if
if
if

maxjunctionsafety     and tolowerghostdensity  then fromghostnearestghost   and maxjunctionsafety   then fromghost 
nearestghost   and fromghostcenter  then toedghost 
toedghost  and centerofdots    then toedghost 
toedghost  and nearestedghost    then toedghost 
nearestdot   and ghostcenterdist   then keepdirection 
toghostfreearea  and todot  then topowerdot 

figure    best policy learned by ce randomrb  average score over    games       
note the presence of always true  and thus  superuous  conditions like
tolowerghostdensity   fromghostcenter   toghostfreearea  or todot  

fig    shows the best individual policy learned by ce fixedrb  reaching      points
on average  ghost avoidance is given highest priority  but is only turned on when a ghost
is very close  otherwise ms  pac man concentrates on eating power dots and subsequently
eating the blue ghosts  she also takes care not to eat any power dot while there are blue
ghosts on the board  because otherwise she would miss the opportunity to eat the      point
ghost  and possibly several others  too   with lowest priority setting  the agent looks for
ordinary dots  although this rule is in eect only when the previous rules can not decide on
a direction  for example  in the endgame when there are no power dots left and all ghosts
are in their original form  
policies learnt by ce randomrb behave similarly to the ones learnt by ce fixedrb 
although the behavior is somewhat obscured by superuous conditions and or rules  as
demonstrated clearly on the example policy shown in fig     because of the noise generated
by the random rules  the algorithm often fails to learn the correct priorities of various
activities 
the eect of enabling disabling concurrent actions is also signicant  it is instructive
to take a look at the best policy learned by ce fixedrb  action shown in fig     the
agent has to concentrate on eating ghosts  as it is the major source of reward  however  she
cannot use modules that are necessary for ghost avoidance and long term survival 
the results also show that cem performs signicantly better than stochastic gradient
learning  we believe  however  that this dierence could be lowered with a thorough search
over the parameter space  sg and many other global optimization methods like evolutionary
methods or simulated annealing could reach similar performances to cem  according to
de boer et al         and the applications cited in section      an advantage of cem is
   

filearning to play ms  pac man

    if nearestedghost    then topowerdot 
    if nearestedghost    then toedghost 

figure    best policy learned by ce fixedrb  action  average score over    games 
     

that it maintains a distribution of solutions and can reach robust performance with very
little eort  requiring little or no tuning of the parameters  there is a canonical set of
parameters                           population as large as possible  for which
the performance of the method is robust  this claim coincides with our experiences in the
parameter optimization process 
finally  it is interesting to analyze the dierences between the tactics of human and
computer players  one fundamental tactic of human players is that they try to lure the
ghosts close to ms  pac man such that all ghosts are very close to each other  this way 
all of them can be eaten fast when they turn blue  no such behavior evolved in any of our
experiments  besides  there are other tactics that cem has no chance to discover  because
it is lacking the appropriate sensors  for example  a human player can  and does  calculate
the time remaining from the blue period  the approximate future position of ghosts  and so
on 

   related literature
in this section  we review literature on learning the pac man game  and on various components of our learning architecture  the cross entropy method  rule based policies  and
concurrent actions 

    previous work on  ms   pac man
variants of pac man have been used previously in several studies  a direct comparison of
performances is possible only in a few cases  however  because simplied versions of the
game are used in most of the other studies 
koza        uses ms  pac man as an example application for genetic programming  it
uses dierent score value for the fruit  worth      points instead of the     points used
here   and the shape of the board  and consequently  the number of dots  is also dierent 
therefore scores cannot be directly compared  however  koza reports  on p       that the
pac man could have scored an additional      points if he had captured all four monsters
on each of the four occasions when they turned blue  this score  the only one reported 
translates to approximately      points in our scoring system 
lucas        also uses the full scale ms  pac man game as a test problem  he trains
a neural network position evaluator with hand crafted input features  for the purposes of
training  he uses an evolutionary strategy approach  the obtained controller was able to
reach           points  averaged over     games 
bonet and stauer        restrict observations to a        window centered at ms  pacman  and uses a neural network and temporal dierence learning to learn a reactive con   

fiszita   lrincz

troller  through a series of increasingly dicult learning tasks  they were able to teach basic
pellet collecting and ghost avoidance behaviors in greatly simplied versions of the game 
they used simple mazes containing no power dots and only one ghost 
gallagher and ryan        denes the behavior of the agent as a parameterized nite
state automata  the parameters are learnt by population based incremental learning  an
evolutionary method similar to cem  they run a simplied version of pac man  they had a
single ghost and had no power dots  which takes away most of the complexity of the game 
tiong        codes rule based policies for pac man by hand  but uses no learning to
improve them  his tests  similarly to ours  are based on the pac man implementation of
courtillat         but he limits the number of ghosts to    the best performing rule set
reaches      points on average out of the maximal       however  the results are not likely
to scale up well with increasing the number of ghosts  the ghost is eaten only     times on
average  out of the possible   times per game   

    the cross entropy method
the cross entropy method of rubinstein        is a general algorithm for global optimization
tasks  bearing close resemblance to estimation of distribution evolutionary methods  see e g 
the paper of muehlenbein         the areas of successful application range from combinatorial optimization problems like the optimal buer allocation problem  allon  kroese  raviv 
  rubinstein         dna sequence alignment  keith   kroese        to independent process analysis  szab  pczos    lrincz        
the cross entropy method has several successful reinforcement learning applications  too 
dambreville        uses cem for learning an input output hierarchical hmm that controls
a predator agent in a partially observable grid world  menache  mannor  and shimkin       
use radial basis function approximation of the value function in a continuous maze navigation task  and use cem to adapt the parameters of the basis functions  and nally  mannor 
rubinstein  and gat        apply cem to policy search in a simple grid world maze navigation problem  recently  the cross entropy method has also been applied successfully to
the game tetris by szita and lrincz        

    rule based policies
the representation of policies as rule sequences is a widespread technique for complex problems like computer games  as an example  many of the pac man related papers listed above
use rule based representation 
learning classier systems  holland        are genetic algorithm based methods to
evolve suitable rules for a given task  bull        gives an excellent general overview and
pointers to further references  the hayek machine of baum        is a similar architecture 
where agents  corresponding to simple rules  dene an economical system  they make bids
for executing tasks in the hope that they can obtain rewards  schaul        applies this
architecture for the sokoban game 
dynamic scripting  spronck et al         is another prominent example of using and
learning rule based policies  it uses a hand coded rule base and a reinforcement learning   results are cited from section     

   

filearning to play ms  pac man

like principle to determine the rules that should be included in a policy  dynamic scripting
has successful applications in state of the art computer games like the role playing game
neverwinter nights  spronck et al         and the real time strategy game wargus  ponsen
  spronck        

    concurrent actions
in traditional formalizations of rl tasks  the agent can select and execute a single action at
a time  the only work known to us that handles concurrent actions explicitly is that of rohanimanesh and mahadevan         they formalize rl tasks with concurrent actions in the
framework of semi markov decision processes and present simple grid world demonstrations 

   summary and closing remarks
in this article we have proposed a method that learns to play ms  pac man  we have dened
a set of high level observation and action modules with the following properties   i  actions
are temporally extended   ii  actions are not exclusive  but may work concurrently  our
method can uncover action combinations together with their priorities  thus  our agent can
pursue multiple goals in parallel 
the decision of the agent concerns whether an action module should be turned on  if
it is o  or o  if it is on   furthermore  decisions depend on the current observations and
may depend on the state of action modules  the policy of the agent is represented as a
list of if then rules with priorities  such policies are easy to interpret and analyze  it is
also easy to incorporate additional human knowledge  the cross entropy method is used for
learning policies that play well  learning is biased towards low complexity policies  which
is a consequence of both the policy representation and the applied cross entropy learning
method  for cem  higher complexity solutions are harder to discover and special means
should be used to counteract premature convergence  for solutions of higher complexities 
noise injection has been suggested in our previous work  szita   lrincz         learned
low complexity policies reached better score than a hand coded policy or the average human
players 
the applied architecture has the potentials to handle large  structured observation  and
action spaces  partial observability  temporally extended and concurrent actions  despite
its versatility  policy search can be eective  because it is biased towards low complexity
policies  these properties are attractive from the point of view of large scale applications 

    the role of domain knowledge
when demonstrating the abilities of an rl algorithm  it is desirable that learning starts from
scratch  so that the contribution of learning is clearly measurable  however  the choices of
test problems are often misleading  many  abstract  domains contain considerable amount
of domain knowledge implicitly  as an example  consider grid world navigation tasks  an
often used class of problems for tabula rasa learning 
in a simple version of the grid world navigation task  the state is an integer that uniquely
identies the position of the agent  and the atomic actions are moves to grid cells north south east west from the actual cell  more importantly  the unique identication of the
   

fiszita   lrincz

position means that the moves of the agent do not change the direction of the agent and
the task is in laboratory coordinate framework  sometimes called allocentric coordinates 
and not in egocentric coordinates  the concepts of north  south  etc  correspond to very
high level abstraction  they have a meaning to humans only  so they must be considered as
part of the domain knowledge  the domain knowledge provided by us is similar to the grid
world in the sense that we also provide high level observations in allocentric form  such as
 distance of nearest ghost is d  or  ms  pac man is at position           similarly  action  go
north  and action  go towards the nearest power dot  are essentially of the same level 
the implicit presence of high level concepts becomes even more apparent as we move
from abstract mdps to the  real world   consider a robotic implementation of the maze
task  the full state information  i e  its own state as well as the state of the environment is
not available for the robot  it sees only local features and it may not see all local features at
a time  to obtain the exact position  or to move one unit s length in the prescribed direction 
the robot has to integrate information from movement sensors  optical radar sensors etc 
such information fusion  although necessary  is not a topic of reinforcement learning  thus 
in this task  there is a great amount of domain knowledge that needs to be provided before
our ce based policy search method could be applied 
in our opinion  the role of human knowledge is that it selects the set of observations and
actions that suit the learning algorithm  such extra knowledge is typically necessary for most
applications  nonetheless  numerous  more or less successful  approaches exist for obtaining
such domain knowledge automatically  according to one approach  the set of observations
is chosen from a rich  and redundant  set of observations by some feature selection method 
the cross entropy method seems promising here  too  see the paper of szita        for an
application to feature selection from brain fmri data at the      pittsburgh brain activity
interpretation competition   according to a dierent approach  successful combinations
of lower level rules can be joined into higher level concepts rules  machine learning has
powerful tools here  e g  arithmetic coding for data compression  witten  neal    cleary 
       it is applied in many areas  including the writing tool dasher developed by ward and
mackay         such extensions are to be included into the framework of reinforcement
learning 

    low complexity policies
the space of legal policies is huge  potentially innite   so it is an interesting question how
search can be eective in such huge space  direct search is formidable  we think that an
implicit bias towards low complexity policies can be useful and this is what we studied here 
by low complexity policy  we mean the following  the policy may consist of very many
rules  but in most cases  only a few of them are applied concurrently  unused rules do not
get rewarded nor do they get punished unless they limit a useful rule  so the eective length
of policies is biased towards short policies  this implicit bias is strengthened by an explicit
one in our work  in the absence of explicit reinforcement  the probability of applying a rule
decays  so indierent rules get wiped out quickly  it seems promising to use frequent low
complexity rule combinations as building blocks in a continued search for more powerful but
still low complexity policies 
   

filearning to play ms  pac man

the bias towards short policies reduces the eective search space considerably  moreover  for many real life problems  low complexity solutions exist  for an excellent analysis of
possible reasons  see the paper of schmidhuber         therefore  search is concentrated on
a relevant part of the policy space  and pays less attention to more complex policies  which
are therefore less likely according to occam s razor arguments  

acknowledgments
please send correspondence to andrs lrincz  the authors would like to thank the anonymous reviewers for their detailed comments and suggestions for improving the presentation
of the paper  this material is based upon work supported partially by the european oce
of aerospace research and development  air force oce of scientic research  air force
research laboratory  under contract no  fa         this research has also been supported
by an ec fet grant  the  new ties project  under contract         any opinions  ndings
and conclusions or recommendations expressed in this material are those of the authors and
do not necessarily reect the views of the european oce of aerospace research and development  air force oce of scientic research  air force research laboratory  the ec 
or other members of the ec new ties project 

appendix a  the hand coded rule base
below is a list of rules of the hand coded rule base used in the experiments 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if

constant   then todot 
constant   then tocenterofdots 
nearestghost   then fromghost 
nearestghost   then fromghost 
nearestghost   then fromghost 
nearestghost   then fromghostnearestghost   then fromghostnearestghost   then fromghostconstant   then tosafejunction 
maxjunctionsafety   then tosafejunction 
maxjunctionsafety   then tosafejunction 
maxjunctionsafety   then tosafejunctionmaxjunctionsafety   then fromghostmaxjunctionsafety   then tosafejunctionmaxjunctionsafety   then fromghostconstant   then keepdirection 
constant   then toedghost 
nearestghost   then topowerdot 
nearestedghost    then topowerdotnearestedghost    and nearestpowerdot   then frompowerdot 
nearestedghost    then frompowerdot 
nearestedghost    then frompowerdotnearestedghost    then topowerdot 
ghostdensity   then tolowerghostdensity 
ghostdensity     then tolowerghostdensity   

fiszita   lrincz

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if

nearestpowerdot   and nearestghost   then topowerdot 
nearestghost   and maxjunctionsafety   then fromghostghostdensity     and nearestpowerdot   then frompowerdot 
nearestpowerdot    then frompowerdottotaldisttoghosts    then frompowerdot 
maxjunctionsafety   then fromghost 
maxjunctionsafety   then fromghost 
maxjunctionsafety   then fromghost 
maxjunctionsafety   then fromghost 
constant   then fromghostcenter 
nearestghost   then fromghost 
nearestghost   and maxjunctionsafety   then fromghostnearestedghost    then toedghostnearestedghost    then toedghost 
frompowerdot  then topowerdot 
ghostdensity     and nearestpowerdot   then frompowerdot 
nearestpowerdot    then frompowerdot 

references
allon  g   kroese  d  p   raviv  t     rubinstein  r  y          application of the crossentropy method to the buer allocation problem in a simulation based environment 
annals of operations research              
baum  e  b          toward a model of mind as a laissez faire economy of idiots  in
proceedings of the   rd international conference on machine learning  pp       
baxter  j   tridgell  a     weaver  l          machines that learn to play games  chap 
reinforcement learning and chess  pp         nova science publishers  inc 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientic 
bonet  j  s  d     stauer  c  p          learning to play pac man using incremental
reinforcement learning    online  accessed    october       
bull  l          applications of learning classier systems  chap  learning classier systems  a brief introduction  pp       springer 
bull  l     kovacs  t          foundations of learning classier systems  chap  foundations
of learning classier systems  an introduction  pp       springer 
courtillat  p          non sens pacman     with c sourcecode    online  accessed   
october       
dambreville  f          cross entropic learning of a machine for the decision in a partially
observable universe  journal of global optimization  to appear 
de boer  p  t   kroese  d  p   mannor  s     rubinstein  r  y          a tutorial on the
cross entropy method  annals of operations research            
   

filearning to play ms  pac man

gallagher  m     ryan  a          learning to play pac man  an evolutionary  rule based
approach  in et  al   r  s   ed    proc  congress on evolutionary computation  pp 
         
holland  j  h          escaping brittleness  the possibilities of general purpose learning
algorithms applied to parallel rule based systems  in mitchell  michalski    carbonell
 eds    machine learning  an articial intelligence approach  volume ii  chap     
pp          morgan kaufmann 
keith  j     kroese  d  p          sequence alignment by rare event simulation  in proceedings of the      winter simulation conference  pp         
koza  j          genetic programming  on the programming of computers by means of
natural selection  mit press 
lucas  s  m          evolving a neural network location evaluator to play ms  pac man  in
ieee symposium on computational intelligence and games  pp         
mannor  s   rubinstein  r  y     gat  y          the cross entropy method for fast policy
search  in   th international conference on machine learning 
margolin  l          on the convergence of the cross entropy method  annals of operations
research              
menache  i   mannor  s     shimkin  n          basis function adaptation in temporal
dierence reinforcement learning  annals of operations research                  
muehlenbein  h          the equation for response to selection and its use for prediction 
evolutionary computation            
ponsen  m     spronck  p          improving adaptive game ai with evolutionary learning 
in computer games  articial intelligence  design and education 
rohanimanesh  k     mahadevan  s          decision theoretic planning with concurrent
temporally extended actions  in proceedings of the   th conference on uncerainty in
articial intelligence  pp         
rubinstein  r  y          the cross entropy method for combinatorial and continuous
optimization  methodology and computing in applied probability            
samuel  a  l          some studies in machine learning using the game of checkers  ibm
journal of research and development            
schaul  t          evolving a compact concept based sokoban solver  master s thesis  cole
polytechnique fdrale de lausanne 
schmidhuber  j          a computer scientist s view of life  the universe  and everything 
in freksa  c   jantzen  m     valk  r   eds    foundations of computer science 
potential   theory   cognition  vol       of lecture notes in computer science  pp 
        springer  berlin 
   

fiszita   lrincz

spronck  p   ponsen  m   sprinkhuizen kuyper  i     postma  e          adaptive game ai
with dynamic scripting  machine learning                 
spronck  p   sprinkhuizen kuyper  i     postma  e          online adaptation of computer
game opponent ai  in proceedings of the   th belgium netherlands conference on
articial intelligence  pp         
sutton  r  s     barto  a  g          reinforcement learning  an introduction  mit press 
cambridge 
szab  z   pczos  b     lrincz  a          cross entropy optimization for independent
process analysis  in ica  pp         
szita  i          how to select the     voxels that are best for prediction  a simplistic
approach  tech  rep   etvs lornd university  hungary 
szita  i     lrincz  a          learning tetris using the noisy cross entropy method  neural
computation                    
tesauro  g          td gammon  a self teaching backgammon program  achieves masterlevel play  neural computation                
timuri  t   spronck  p     van den herik  j          automatic rule ordering for dynamic
scripting  in the third articial intelligence and interactive digital entertainment
conference  pp       
tiong  a  l  k          rule set representation and tness functions for an articial pac man
playing agent  bachelor s thesis  department of information technology and electrical
engineering 
ward  d  j     mackay  d  j  c          fast hands free writing by gaze direction  nature 
            
wikipedia         pac man  wikipedia  the free encyclopedia  wikipedia   online 
accessed    may       
witten  i  a   neal  r  m     cleary  j  g          arithmetic coding for data compression 
communications of the acm             
wolpert  d  h     macready  w  g          no free lunch theorems for optimization  ieee
transactions on evolutionary computation          

   

fi