journal of artificial intelligence research                  

submitted        published      

query time entity resolution
indrajit bhattacharya

indrajbh in ibm com

ibm india research laboratory
vasant kunj  new delhi          india

lise getoor

getoor cs umd edu

department of computer science
university of maryland  college park  md       usa

abstract
entity resolution is the problem of reconciling database references corresponding to
the same real world entities  given the abundance of publicly available databases that
have unresolved entities  we motivate the problem of query time entity resolution  quick
and accurate resolution for answering queries over such unclean databases at query time 
since collective entity resolution approaches  where related references are resolved jointly
 have been shown to be more accurate than independent attribute based resolution for
off line entity resolution  we focus on developing new algorithms for collective resolution
for answering entity resolution queries at query time  for this purpose  we first formally
show that  for collective resolution  precision and recall for individual entities follow a
geometric progression as neighbors at increasing distances are considered  unfolding this
progression leads naturally to a two stage expand and resolve query processing strategy 
in this strategy  we first extract the related records for a query using two novel expansion
operators  and then resolve the extracted records collectively  we then show how the
same strategy can be adapted for query time entity resolution by identifying and resolving
only those database references that are the most helpful for processing the query  we
validate our approach on two large real world publication databases where we show the
usefulness of collective resolution and at the same time demonstrate the need for adaptive
strategies for query processing  we then show how the same queries can be answered in
real time using our adaptive approach while preserving the gains of collective resolution  in
addition to experiments on real datasets  we use synthetically generated data to empirically
demonstrate the validity of the performance trends predicted by our analysis of collective
entity resolution over a wide range of structural characteristics in the data 

   introduction
with the growing abundance of publicly available data in digital form  there has been intense research on data integration  a critical component of the data integration process is
the entity resolution problem  where uncertain references in the data to real world entities
such as people  places  organizations  events  etc   need to be resolved according to their
underlying real world entities  entity resolution is needed in order to solve the deduplication problem  where the goal is to identify and consolidate pairs of records or references
within the same relational table that are duplicates of each other  it also comes up as the
fuzzy match problem  where tuples from two heterogeneous databases with different keys 
and possibly different schemas  need to be matched and consolidated  it goes by different
c
    
ai access foundation  all rights reserved 

fibhattacharya   getoor

names even within the data mining and database communities  including record linkage 
object consolidation  and reference reconciliation 
the problem has a long history  and recent years have seen significant and fruitful
research on this problem  however  in spite of the widespread research interest and the
practical nature of the problem  many publicly accessible databases remain unresolved  or
partially resolved  at best  the popular publication databases  citeseer and pubmed  are
representative examples  citeseer contains several records for the same paper or author 
while author names in pubmed are not resolved at all  this is due to a variety of reasons 
ranging from rapid and often uncontrolled growth of the databases and the computational
and other expenses involved in maintaining resolved entities 
yet  millions of users access and query such databases everyday  mostly seeking information that  implicitly or explicitly  requires knowledge of the resolved entities  for example 
we may query the citeseer database of computer science publications looking for books by
s russell  pasula  marthi  milch  russell    shpitser         this query would be easy
to answer if all author names in citeseer were correctly mapped to their entities  but 
unfortunately  this is not the case  according to citeseer records  stuart russell and peter
norvig have written more than     different books together  one of the main reasons behind databases containing unresolved entities is that entity resolution is generally perceived
as an expensive process for large databases  also  maintaining a clean database requires
significant effort to keep pace with incoming records  alternatively  we may be searching
different online social network communities for a person named jon doe  in this case 
each online community may individually have records that are clean  even then  query
results that return records from all of the sources aggregated together may have multiple
representations for the same jon doe entity  additionally  in both cases  it is not sufficient
to simply return records that match the query name  s  russell or jon doe exactly  in
order to retrieve all the references correctly  we may need to retrieve records with similar
names as well  such as stuart russel or john doe  and  most importantly  for the results
to be useful  we need to partition the records that are returned according to the real world
entities to which they correspond  such on the fly partitioning of returned results is also
necessary when accessing third party or external databases that do not provide full access
possibly due to privacy and other concerns  and can be accessed only via specific query
interfaces 
in this paper  we propose an alternative solution for answering entity resolution queries 
where we obviate the need for maintaining resolved entities in a database  instead  we
investigate entity resolution at query time  where the goal is to enable users to query an
unresolved or partially resolved database and resolve the relevant entities on the fly  a user
may access several databases everyday and she does not want to resolve all entities in every
database that she queries  she only needs to resolve those entities that are relevant for a
particular query  for instance  when looking for all books by stuart russell in citeseer 
it is not useful to resolve all of the authors in citeseer  since the resolution needs to be
performed at query time  the requirement is that the resolution process needs to be quick 
even if it is not entirely accurate 
though entity resolution queries have not been addressed in the literature  there has
been significant progress on the general entity resolution problem  recent research has focused on the use of additional relational information between database references to improve
   

fiquery time entity resolution

resolution accuracy  bhattacharya   getoor        singla   domingos        dong  halevy 
  madhavan        ananthakrishna  chaudhuri    ganti        kalashnikov  mehrotra   
chen         this improvement is made possible by resolving related references or records
jointly  rather than independently  intuitively  this corresponds to the notion that figuring
out that two records refer to the same underlying entity may in turn give us useful information for resolving other record pairs that are related  imagine that we are trying to decide if
two authors stuart russell and s russell are the same person  we can be more confident
about this decision if we have already decided that their co authors peter norvig and p 
norvig are the same person 
as others have done  in our earlier work  bhattacharya   getoor               we have
demonstrated using extensive experiments on multiple real and synthetic datasets that
collective resolution significantly improves entity resolution accuracy over attribute based
and naive relational baselines  however  its application for query time entity resolution is
not straight forward  and this is precisely the problem that we focus on in this paper  the
first difficulty is that collective resolution works for a database as a whole and not for a
specific query  secondly  the accuracy improvement comes at a considerable computation
cost arising from the dependencies between related resolutions  this added computational
expense makes its application in query time resolution challenging 
in this paper  which builds on and significantly extends the work presented in bhattacharya  licamele  and getoor         we investigate the application of collective resolution for queries  first  we formally analyze how accuracies of different decisions in collective
resolution depend on each other and on the structural characteristics of the data  the recursive nature of the dependency leads naturally to a recursive expand and resolve strategy
for processing queries  the relevant records necessary for answering the query are extracted
by a recursive expansion process and then collective resolution is performed only on the extracted records  using our analysis  we show that the recursive expansion process can be
terminated at reasonably small depths for accurately answering any query  the returns fall
off exponentially as neighbors that are further away are considered 
however  the problem is that this unconstrained expansion process can return too many
records even at small depths  and thus the query may still be impossible to resolve in
real time  we address this issue using an adaptive strategy that only considers the most
informative of the related records for answering any query  this significantly reduces the
number of records that need to be investigated at query time  but  most importantly  does
not compromise on the resolution accuracy for the query 
our specific contributions in this paper are as follows 
   first  we motivate and formulate the problem of query time entity resolution  our
entity resolution approach is based on a relational clustering algorithm  to the best
of our knowledge  clustering based on queries in the presence of relations has received
little attention in the literature 
   for collective resolution using relational clustering  we present an analysis of how the
accuracy of different resolution decisions depends on each other and on the structural
characteristics of the data  we introduce the notion of precision and recall for individual entities  and show how they follow a geometric progression as neighbors at
increasing distances are considered and resolved  our analysis shows that collective
   

fibhattacharya   getoor

use of relationships can sometimes hurt entity resolution accuracy  this has not been
previously reported in the literature  our analysis additionally demonstrates the convergent nature of resolution performance for the recursive query resolution strategy
that we propose 
   for resolving queries collectively  we propose a two phase expand and resolve algorithm  it first extracts the related records for a query using two novel expansion
operators  and then resolves the query by only considering the extracted records  we
then improve on this algorithm using an adaptive approach that selectively considers
only the most informative ones among the related records for a query  this enables
collective resolution at query time without compromising on resolution accuracy for
the query 
   we present experimental results on two large real world datasets where our strategy
enables collective resolution in seconds  we compare against multiple baselines to
show that the accuracy achieved using collective query resolution is significantly higher
than those achieved using traditional approaches 
   we also use synthetically generated data to demonstrate the gains of collective query
resolution over a wide range of attribute and relational characteristics  we additionally show that the empirical results are in agreement with the trends predicted by our
analysis of collective resolution 
the rest of the paper is organized as follows  in section    we formalize the relational
entity resolution problem and entity resolution queries  and also illustrate these with an
example  in section    we briefly review the relational clustering algorithm that we employ
for collective entity resolution and then  in section    investigate how resolution accuracy
for related entities depend on each other for collective resolution using this algorithm  in
section    we extend collective resolution for queries  and describe and analyze an unconstrained recursive strategy for collectively resolving a query  we then modify this approach
in section   and present our adaptive algorithm that extracts only the most informative
references for resolving a query  we present experimental results on real and synthetic data
in section    review related work in section   and finally conclude in section   

   entity resolution and queries  formulation
in this section  we formally introduce the entity resolution problem and also entity resolution
queries  and illustrate them using a realistic example  that of resolving authors in a
citation database such as citeseer or pubmed 
in the simplest formulation of the entity resolution problem  we have a collection of
references  r    ri    with attributes  r a            r ak    let e    ej   be the unobserved
domain entities  for any particular reference ri   we denote the entity to which it maps
as e ri    we will say that two references ri and rj are co referent if they correspond to
the same entity  e ri     e rj    note that in the case of an unresolved database  this
mapping e r  is not provided  further  the domain entities e and even the number of
such entities is not known  however  in many domains  we may have additional information
about relationships between the references  to model relationships in a generic way  we use
   

fiquery time entity resolution

h 

h 

a mouse immunity model

r 
w wang

r 
c chen

r 

r 

r 

a ansari

w wang

a ansari

h   measuring protienbound fluxetine
r 
r 
r 
l li

c chen

a better mouse immunity model

h   autoimmunity in biliary cirrhosis
r 
r   
w w wang

w wang

a ansari

figure    an example set of papers represented as references connected by hyper edges 
references are represented as ovals shaded according to their entities  each paper
is represented as a hyper edge  shown as a rectangle  spanning multiple references 

a set of hyper edges h    hi    each hyper edge connects multiple references  to capture
this  we associate a set of references hi  r with each hyper edge hi   note that each reference
may be associated with zero or more hyper edges 
let us now look at a sample domain to see how it can be represented in our framework 
consider a database of academic publications similar to dblp  citeseer or pubmed  each
publication in the database has a set of author names  for every author name  we have a
reference ri in r  for any reference ri   ri  n ame records the observed name of the author
in the publication  in addition  we can have attributes such as r email to record other
information for each author reference that may be available in the paper  now we come to
the relationships for this domain  all the author references in any publication are connected
to each other by a co author relationship  this can be represented using a hyper edge hi  h
for each publication and by having rj  hi  r for each reference rj in the publication  if
publications have additional information such as title  keywords  etc  they are represented
as attributes of h 
to illustrate  consider the following four papers  which we will use as a running example 
   w  wang  c  chen  a  ansari  a mouse immunity model
   w  wang  a  ansari  a better mouse immunity model
   l  li  c  chen  w  wang measuring protein bound fluxetine
   w  w  wang  a  ansari  autoimmunity in biliary cirrhosis
to represent them in our notation  we have    references  r            r     in r  one for each
author name  such that r   n ame   w wang  etc  we also have   hyper edges  h            h   
in h  one for each paper  the first hyper edge h  connects the three references r    r  and
r  corresponding to the names w  wang   c  chen and a  ansari  this is represented
pictorially in figure   
given this representation  the entity resolution task is defined as the partitioning
or clustering of the references according to the underlying entity reference mapping e r  
two references ri and rj should be assigned to the same cluster if and only if they are
   

fibhattacharya   getoor

coreferent  i e   e ri     e rj    to illustrate  assume that we have six underlying entities for
our example  this is illustrated in figure   using a different shading for each entity  for
example  the wangs of papers      and   are names of the same individual but the wang
from paper   is a reference to a different person  also  the chens from papers   and   are
different individuals  then  the correct entity resolution for our example database with   
references returns   entity clusters    r    r    r      r      r      r      r    r    r       r      the
first two clusters correspond to two different people named wang  the next two to two
different people named chen  the fifth to ansari and the last to li 
any query to a database of references is called an entity resolution query if answering
it requires knowledge of the underlying entity mapping e r   we consider two different
types of entity resolution queries  most commonly  queries are specified using a particular
value a for an attribute r a of the references that serves as a quasi identifier for the
underlying entities  then the answer to the query q r a   a  should partition or group
all references that have r a   a according to their underlying entities  for references to
people  the name often serves as a weak or noisy identifier  for our example bibliographic
domain  we consider queries specified using r n ame  to retrieve all papers written by
some person named w  wang  we issue a query using r n ame and w  wang  since
names are ambiguous  treating them as identifiers leads to undesirable results  in this case 
it would be incorrect to return the set  r    r    r    of all references with name w wang as
the answer to our query  this answer does not indicate that r  is not the same person as
the other two  additionally  the answer should include the reference r  for w w wang 
that maps to the same entity as the author of the first paper  therefore  the correct answer
to the entity resolution query on w wang should be the partition   r    r    r      r     
entity resolution queries may alternatively be specified using a specific reference  imagine a citeseer user looking at a paper that contains some author name  the user may be
interested in looking up other papers written by the same author  even though they may
not know who that author is precisely  then the correct answer to a query on the reference
r is the group of references that are coreferent to r  or  in other words  correspond to the
same underlying entity  in our example  consider a query specified using the reference r 
corresponding to the name w  wang in the first paper  then the correct answer to the
query is the set of references  r    r    r     to distinguish it from the first type of entity
resolution query  note that it does not include the cluster  r    corresponding to the other
entity that also has name w  wang  this second query type may be answered by first
reducing it to an instance of the first type as q r a   r   a   and then selecting the entity
corresponding to reference r    we denote this as e r  e r     q r a   r   a    in the rest
of this paper  we focus only on queries of the first type 

   collective entity resolution and relational clustering
although entity resolution for queries has not been studied in the literature  the general
entity resolution problem has received a lot of attention  we review related work in detail in
section    in this section  we briefly review the different categories of proposed approaches
before discussing how they may be adapted for query time entity resolution 
in most entity resolution applications  data labeled with the underlying entities is hard
to acquire  our focus is on unsupervised approaches for resolving entities  traditionally 
   

fiquery time entity resolution

attributes of individual references  such as names  affiliation  etc   for person references  are
used for comparing references  a similarity measure is generally employed over attributes 
and only those pairs of references that have attribute similarity above a certain threshold
are considered to be co referent  this attribute based entity resolution approach  a 
often runs into problems  in our example  it is hard to infer with just attributes that
references r  and r  are not co referent although they have the same name  while r  and r 
are co referent although their names are different 
when relations between references are available  they may also be taken into account
for computing similarities in the naive relational entity resolution approach  nr 
 ananthakrishna et al         bhattacharya   getoor         for computing similarities
between two references  this approach additionally considers the attributes of the related
references when comparing the attributes of their related references  in our example  this
approach returns a higher similarity between r   w  wang  and r   w  w  wang  than
the attribute based approach  since they have co authors r  and r   with very similar  identical  in this case  names  although this approach can improve performance in some cases 
it does not always work  for instance  the two w  wang references r  and r  are not
co referent  though they both have co authors with identical names c  chen 
instead of considering the attribute similarities of the related references  the collective
entity resolution approach  pasula et al         bhattacharya   getoor        singla
  domingos        mccallum   wellner        li  morie    roth        dong et al  
      kalashnikov et al         takes into account the resolution decisions for them  in our
previous example  the correct evidence to use for the pair of references r  and r  is that
their co author references do not map to the same entity  although they have similar names 
therefore  in order to resolve the w  wang references in the collective resolution approach 
it is necessary to resolve the c  chen references as well  instead of considering the similarity
of their attributes  the collective entity resolution approach has recently been shown to
improve entity resolution accuracy over the previous approaches but is computationally
more challenging  the references cannot be resolved independently  instead  any resolution
decision is affected by other resolutions through hyper edges 
in earlier work  bhattacharya   getoor                     we developed a relational
clustering algorithm  rc er  for collective entity resolution using relationships  the goal
of this approach is to cluster the references according to their entities taking the relationships
into account  we associate a cluster label r c with each reference to denote its current
cluster membership  starting from an initial set of clusters c    ci   of references  the
algorithm iteratively merges the pair of clusters that are the most similar  to capture the
collective nature of the cluster assignment  the similarity measure between pairs of clusters
considers the cluster labels of the related references  the similarity of two clusters ci and
cj is defined as a linear combination of their attribute similarity sima and their relational
similarity simr  
sim ci   cj            sima  ci   cj       simr  ci   cj  

   

where           is the combination weight  the interesting aspect of the collective
approach is the dynamic nature of the relational similarity  the similarity between two
references depends on the current cluster labels of their related references  and therefore
changes when related references change clusters  in our example  the similarity of the two
   

fibhattacharya   getoor

clusters containing references w  wang and w  w  wang increases once their co author
references named a  ansari are assigned to the same cluster  we now briefly review how
the two components of the similarity measure are defined 
attribute similarity  for each reference attribute  we use a similarity measure that
returns a value between   and   for two attribute values indicating the degree of similarity
between them  several sophisticated similarity measures have been developed for names 
and popular tf idf schemes may be used for other textual attributes such as keywords 
the measure that works best for each attribute may be chosen  finally  a weighted linear
combination of the similarities over the different attributes yields the combined attribute
similarity between two reference clusters 
relational similarity  relational similarity between two clusters considers the similarity of their cluster neighborhoods  the neighborhood of each cluster is defined by the
hyper edges associated with the references in that cluster  recall that each reference r is
associated with one or more hyper edges in h  therefore  the hyper edge set c h for a
cluster c of references is defined as
c h  

 

 h   h  h  r  h r 

   

rrr c c

this set defines the hyper edges that connect a cluster c to other clusters  and are the
ones that relational similarity needs to consider  to illustrate  when all the references in
our running example have been correctly clustered as in figure   b   the hyper edge set
for the larger wang cluster is  h    h    h     which are the hyper edges associated with the
references r    r  and r  in that cluster 
given the hyper edge set for any cluster c  the neighborhood n br c  of that cluster c is
the set of clusters labels of the references spanned by these hyper edges 
n br c   

 

 cj   cj   r c 

   

hc h rh

for our example wang cluster  its neighborhood consists of the ansari cluster and one
of the chen clusters  which are connected by its edge set  then  the relational similarity
measure between two clusters  considers the similarity of their cluster neighborhoods  the
neighborhoods are essentially sets  or multi sets  of cluster labels and there are many possible ways to define the similarity of two neighborhoods  bhattacharya   getoor        
the specific similarity measure that we use for our experiments in this paper is jaccard
similarity   
simr  ci   cj     jaccard n br ci    n br cj   
   
clustering algorithm  given the similarity measure for a pair of clusters  a greedy
relational clustering algorithm can be used for collective entity resolution  figure   shows
high level pseudo code for the complete algorithm  the algorithm first identifies the candidate set of potential duplicates using a blocking approach  hernandez   stolfo       
monge   elkan        mccallum  nigam    ungar         next  it initializes the clusters
   jaccard similarity for two sets a and b is defined as jaccard a  b   

   

 ab 
 ab 

fiquery time entity resolution

  
  

algorithm rc er  reference set r 
find similar references in r using blocking
initialize clusters using bootstrapping

  
  

for clusters ci   cj such that similar ci   cj  
insert hsim ci   cj    cj   cj i into priority queue

  
  
  
  
  
   
   
   
   
   

while priority queue not empty
extract hsim ci   cj    ci   cj i from queue
if sim ci   cj   less than threshold  then stop
merge ci and cj to new cluster cij
remove entries for ci and cj from queue
for each cluster ck such that similar cij   ck  
insert hsim cij   ck    cij   ck i into queue
for each cluster cn neighbor of cij
for ck such that similar ck   cn  
update sim ck   cn   in queue
figure    high level description of the relational clustering algorithm

of references  identifies the similar clusters  or potential merge candidates  for each
cluster  inserts all the merge candidates into a priority queue and then iterates over the
following steps  at each step  it identifies the current closest pair of clusters from the candidate set and merges them to create a new cluster  it identifies new candidate pairs and
updates the similarity measures for the related cluster pairs  this is the key step where
evidence flows from one resolution decision to other related ones and this distinguishes relational clustering from traditional clustering approaches  the algorithm terminates when
the similarity for the closest pair falls below a threshold or when the list of potential candidates is exhausted  the algorithm is efficiently implemented to run in o nk log n  time
for n references where each block of similar names is connected to k other blocks through
the hyper edges 
    issues with collective resolution for queries
in previous work  we  and others  have shown that collective resolution using relationships
improves entity resolution accuracy significantly for offline cleaning of databases  so  naturally  we would like to use the same approach for query time entity resolution as well 
however  while the attribute based and naive relational approaches discussed earlier can
be applied at query time in a straight forward fashion  that is not the case for collective
resolution  two issues come up when using collective resolution for queries  first  the
set of references that influence the resolution decisions for a query need to be identified 
when answering a resolution query for s  russell using the attribute based approach  it is
sufficient to consider all papers that have s  russell  or  similar names  as author name 
for collective resolution  in contrast  the co authors of these author names  such as p 
   

fibhattacharya   getoor

norvig and peter norvig  also need to be clustered according to their entities  this in
turn requires clustering their co authors and so on  so the first task is to analyze these
dependencies for collective resolution and identify the references in the database that are
relevant for answering a query  but this is not enough  the set of references influencing a
query may be extremely large  but the query still needs to be answered quickly even though
the answer may not be completely accurate  so the second issue is performing the resolution
task at query time  these are the two problems that we address in the next few sections 

   analysis of collective resolution using relational clustering
for collective entity resolution  we have seen that resolution performance for the query
becomes dependent on the resolution accuracy of the related entities  before we can analyze
which other references influence entity resolution for the query and to what extent  we need
to analyze the nature of this dependence for collective resolution in general  in this section 
we identify the structural properties of the data that affect collective entity resolution and
formally model the interdependent nature of the resolution performance  this analysis
also helps us to understand when collective resolution using relational clustering helps 
and  equally importantly  when it has an adverse effect as compared against traditional
attribute based resolution 
the goal of an entity resolution algorithm is to partition the set r    ri   of references
into a set of clusters c    ci   according to the underlying entities e    ei    the accuracy of the resolution depends on how closely the separation of the references into clusters
corresponds to the underlying entities  we consider two different measures of performance 
the first measure is recall for each entity  for any entity ei   recall counts how many pairs
of references corresponding to ei are correctly assigned to the same computed cluster  the
second measure is precision for each computed cluster  for any cluster ci   precision counts
how many pairs of references assigned to ci truly correspond to the same underlying entity 
 alternatively  imprecision measures how many pairs of references assigned to the cluster
do not correspond to the same entity   in the next two subsections  we analyze how these
two performance metrics are influenced  first  by the attribute values of the references  and
then  by the observed relationships between them 
    influence of attributes
first  consider an entity resolution algorithm that follows the traditional attribute based approach and the analysis of its performance  such an algorithm only considers the attributes
of individual references  it uses a similarity measure defined over the domain of attributes 
and considers pair wise attribute similarity between references for resolving them  let us
define two references to be  similar if their attribute similarity is at least   then  given
a resolution threshold   the attribute based approach assigns a pair of references to the
same cluster if and only if they are  similar  to illustrate using our example  using any
similarity measure defined over names and an appropriately determined similarity threshold
  the attribute based approach would assign the three w  wang references  r    r    r    to
one cluster c  and the w  w  wang reference  r    to a different cluster c    this resolution
of the wang references is not perfect in terms of precision or recall  since references r    r 
and r  map to one entity e  and r  maps to a second entity e    cluster c  has precision less
   

fiquery time entity resolution

than    since it incorrectly includes references for two different entities  and recall is less
than   for entity e    since its references are dispersed over two different clusters 
in order to analyze the performance of this attribute based resolution approach given
an arbitrary dataset  we now characterize a dataset in terms of the attribute values of
its references  intuitively  the attribute based approach works well when the references
corresponding to the same entity are similar in terms of their attributes  and when the
references corresponding to different entities are not  to capture this formally  we define
two probabilities that measure the attribute similarity of references that map to the same
entity  and the attribute similarity of those that map to different entities 
 attribute identification probability ai  e     the probability that a pair of references chosen randomly from those corresponding to entity e are  similar to each
other 
 attribute ambiguity probability aa  e    e       the probability that a pair of references chosen randomly such that one corresponds to entity e  and the other to entity
e  are  similar to each other 
to illustrate using the four wang references  r    r  and r  correspond to the same
entity e  and r  corresponds to a different entity e    also  assume that for some similarity
measure for names and an appropriate threshold   references r    r  and r  are  similar to
each other  then  of the   pairs of references corresponding to entity e    only one  r  and
r    is  similar  so that the attribute identification probability ai  e      for entity e  is      
on the other hand  of the three pairs of references such that one maps to e  and the other
to e    two  r  and r    r  and r    are  similar  this means that the attribute ambiguity
probability aa  e    e      between e  and e  is      
as can be seen from the above example  the performance of the attribute based clustering algorithm can be represented in terms of these two probabilities  for any specified
threshold   the pairs of references for any entity that are correctly recalled are the ones that
are  similar  which is exactly what ai  e    captures  therefore  the recall for any domain
entity e is r e      ai  e     on the other hand  consider the cluster assignment for all the
references that correspond to two entities e  and e    the pairs that are incorrectly clustered
together are those that correspond to two different entities  and yet are  similar  this is
what aa  e    e      captures  therefore the imprecision of the cluster assignment of reference
pairs corresponding to entities e  and e  is i e    e        aa  e    e       alternatively  the
precision is given by p  e    e          i e    e           aa  e    e      
    influence of relationships
now  we consider the collective entity resolution approach that additionally makes use of
relationships  and analyze its impact on entity resolution accuracy  recall that we have
a set h    hj   of observed co occurrence relationships between the references  such cooccurrences between references are useful for entity resolution when they result from strong
ties or relations between their underlying entities  specifically  we assume that references to
any entity ei co occur frequently with references to a small set of other entities  e i           eki   
which we call the entity neighbors  denoted n  ei    of entity ei  
   

fibhattacharya   getoor

w w  wang
w  wang

h 

a  ansari
a  ansari

w  wang
w  wang

h 

h 
h 

c  chen
c  chen

figure    illustration of  a  identifying relation and  b  ambiguous relation from running
example  dashed lines represent co occurrence relations 

assuming such a neighborhood relationship among the underlying entities allows us
to analyze the performance of the relational clustering approach  for those reference pairs
that are  similar in terms of attributes  the attribute evidence is enough for resolution  but
now  unlike attribute based clustering  any pair of references that are  similar in terms of
attributes  for some      are considered as candidates for being clustered together  not all
of them actually get assigned to the same cluster  for reference pairs that are in the ring of
uncertainty between  and   their relationships play a role in determining if they are similar
enough  and consequently  if they should be clustered together  specifically  if references ri
and rj co occur through hyper edge h and references ri and rj co occur through hyper edge
h   then the relational similarity of the pair  ri   ri   is more when  rj   rj   belong to the same
cluster  in general  multiple such relationships may be needed for tipping the balance  but
for simplicity  we assume for now that a single pair of related references is sufficient  in
other words  ri and ri get assigned to the same cluster if rj and rj are in the same cluster 
we now analyze the impact that this approach has on entity resolution performance 
without loss of generality  assume that the  rj   rj   pair get clustered together first by the
relational clustering algorithm  this results in the other pair  ri   ri   also getting clustered
at some later iteration by considering this relational evidence  to see if this is accurate  we
consider two situations  as we did with attribute evidence  the first is shown in figure   a  
where both pairs truly correspond to the same entity  then the collective resolution decision
is correct and we say that hyper edges h and h are identifying relationships for that entity 
formally 
irel h  h   e    ri   rj  h r  ri   rj  h  r 
e ri     e ri     e  e rj     e rj  

   

on the other hand  we may have a different scenario  in which both pairs of references correspond to two different entities  this second scenario is depicted in figure   b   then the
first decision to resolve  rj   rj   as co referent is incorrect  and relational evidence obtained
through hyper edges h and h consequently leads to the incorrect resolution of  ri   ri    in
this situation  collective resolution hurts accuracy  and we say that h and h form ambiguous
relationships for both pairs of entities  whose references may be incorrectly clustered as a
result of these relationships  formally 
iamb h  h   e  e     ri   rj  h r  ri   rj  h  r 
   

fiquery time entity resolution

e ri     e  e ri     e   e    e  
e rj      e rj  

   

in general  a reference ri can have a co occurrence relation h that includes more than
one other reference  we may think of this as multiple co occurrence pairs involving ri  
cluster labels of all these other references in the pairs influence resolution decisions for ri  
when resolving ri with another reference ri that participates in co occurrence relation h  
the fraction of common cluster labels between h and h determines whether or not ri and
ri will be clustered together  if they are assigned to the same cluster  h and h are labeled
identifying or ambiguous relationships based on whether ri and ri are actually co referent
or not 
formally  we define 
 identifying relationship probability ri  e     the probability that a randomly chosen pair of  similar references corresponding to entity e has identifying relationships
h and h with some other entity 
 ambiguous relationship probability ra  e    e       the probability that a pair of
 similar references  chosen randomly such that one corresponds to entity e  and the
other to entity e    has ambiguous relationships h and h with some other pair of
entities 
to illustrate these probabilities using our example  we have two wang entities  e 
that has references r    r  and r    and e  that has reference r    assume that the attribute
threshold  is such that all six pairs are considered potential matches  of the three pairs of
references corresponding to e    all of them have identifying relationships with the ansari
entity  so  ri  e           to measure the relational ambiguity between the two wang
entities  we consider the   possible pairs  r  and r    r  and r    r  and r     of these only
one  r  and r    pair has ambiguous relationships with two different chen entities  so 
ra  e    e             
given these two probabilities  we can analyze the performance of our relational clustering algorithm that combines attribute and relational evidence for collective entity resolution 
it is not hard to see that the recall for any entity depends recursively on the recall of its
neighbor entities  any pair of references for entity e is resolved correctly on the basis of
attributes alone with probability ai  e     the identifying attribute probability   furthermore  it may still be resolved correctly in the presence of identifying relationships with a
neighbor entity  if the related reference pair for the neighbor is resolved correctly  denoting
as r e      the recall for entity e and that for its neighbors as r n  e        we have 
r e        ai  e          ai  e      ri  e     r n  e      

   

on the other hand  consider a pair of entities e  and e    the cluster assignment for
a pair of references corresponding to e  and e  is imprecise on the basis of its attributes
alone with probability aa  e    e       even otherwise  the cluster assignment can go wrong
by considering relational evidence  this happens in the presence of ambiguous relationships
with references corresponding to another pair of entities  if those references are also clustered
   

fibhattacharya   getoor

together incorrectly  so the imprecision i e    e        of the cluster assignment of reference
pairs corresponding to entities e  and e  turns out to be 
i e    e          aa  e    e            aa  e    e        ra  e    e       i n  e     n  e            
in general  any entity e has multiple neighbors ei in its neighborhood n  e   to formalize the performance dependence on multiple neighbors  assume that if a co occurrence
involving references corresponding to e is chosen at random  the probability of selecting a
co occurrence with a reference corresponding to ei is pei   then recall is given as 
 n  e  

r e    ai  e        ai  e    ri  e  

x

pei r ei  

   

i  

note that we have dropped  and  for notational brevity  for defining imprecision  observe
that a reference corresponding to any neighbor ei  of e  may co occur with a reference for
any neighbor ej  of e  with probability pei   pej     then imprecision is given as 
 n  e      n  e    

i e    e      aa  e    e          aa  e    e      ra  e    e    

x

x

i  

j  

pei   pej   i ei    ej   

    

given similarity thresholds  and   relational clustering increases recall beyond that
achievable using attributes alone  this improvement is larger when the probability of identifying relationships is higher  on the flip side  imprecision also increases with relational
clustering  typically  a low attribute threshold  that corresponds to high precision is used 
and then recall is increased using relational evidence  when the probability of ambiguous
relations ra is small  the accompanying increase in imprecision is negligible  and performance is improved overall  however  the higher the ambiguous relationship probability
ra   the less effective is relational clustering  thus the balance between ambiguous and
identifying relations determines the overall benefit of collective resolution using relational
clustering  when ra is high compared to ri   imprecision increases faster than recall  and
overall performance is adversely affected compared to attribute based clustering  eq     
and eq       quantify this dependence of resolution performance for any entity on the nature
of its relationships with other entities  in the next section  we will use these equations to
design and analyze a relational clustering algorithm for answering entity resolution queries 

   collective resolution for queries
our analysis of collective resolution using relational clustering showed that the resolution accuracy for any underlying entity depends on the resolution accuracy for its related neighboring entities  for the problem of answering entity resolution queries  the goal
is not to resolve all the entities in the database  we need to resolve entities for only those
references that are retrieved for the query  we have seen that collective resolution leads to
potential performance improvements over attribute based resolution  we now investigate
how collective resolution can be applied for answering queries to get similar improvements 
the obvious hurdle is illustrated by the expressions for performance metrics in eq      and
eq        they show that in order to get performance benefits for resolving the query using
   

fiquery time entity resolution

relational clustering  we need to resolve the neighboring entities as well  furthermore  to
resolve the neighboring entities  we need to resolve their neighboring entities  and so on 
these other entities that need to be resolved can be very large in number  and resolving
them is expensive in terms of query processing time  also  none of them are actually going
to be retrieved as part of the answer to the query  so it is critical to identify and resolve
those entities that contribute the most for improving resolution accuracy for the query 
we propose a two stage query processing strategy  consisting of an extraction phase  for
identifying all the relevant references that need to be resolved for answering the query  and
a resolution phase  where the relevant references that have been extracted are collectively
resolved using relational clustering  unfolding eq      and eq       starting from the query
entities leads to a natural expansion process  in this section  we describe the extraction
process using two novel expansion operators and  in parallel  we analyze the improvement
in resolution accuracy that is obtained from considering co occurrences 
recall that an entity resolution query q r a   a  is specified using an attribute a
and a value a for it  the answer to the query consists of a partitioning of all references
r that have r a   a or some value  similar to a  the correct answer to the query  in
general  involves references from multiple entities  eq    we measure resolution accuracy for
the query using two metrics as before  for each of the query entities eq   we measure recall
r eq   and imprecision i eq   e   with respect to any other entity e   entity e may or may
not belong to  eq   
before going into the details of our algorithm for collective resolution of queries  we
briefly recall the accuracy of the attribute based strategy of resolving a query  this approach
considers all references r with r a  similar to a  and resolves them using their attributes
only  the recall that results from this approach is r eq       ai  eq      and the imprecision
is given by i eq   e       aa  eq   e     
we propose two expansion operators for constructing the relevant set for an entity
resolution query  we denote as level   references all references that are  similar to the
query attribute  these are the references that the user is interested in  and the goal is
to resolve these correctly  the first operator we introduce is the attribute expansion
operator xa   or a expansion for short  given an attribute a and a value a for that
attribute  xa  a    returns all references r whose attributes r a exactly match a or are similar to a  for a query q r a   a   the level   references can be retrieved by expanding
q as 
rel   q    xa  a   
the first step in figure   shows a expansion for the query q r n ame   w w ang  in our
example  it retrieves the four references  r   r   r   r    that have name w  wang or w  w 
wang 
to consider co occurrence relations  we construct the level   references by including
all references that co occur with level   references  for this  we use our second operator 
which we call hyper edge expansion xh   or h expansion  for any reference r  xh  r 
returns all references that share a hyper edge with r  and for a set r of references xh  r 
s
returns rr xh  r   collective entity resolution requires that we consider all co occurring
references for each reference  this is achieved by performing h expansion on the references
   

fibhattacharya   getoor

q
r name w wang

 

r    a ansari

rel  q 

 

rel  q 

r   w w wang

r    a ansari

r   w wang

r   a ansari

r    a ansari

r   a ansari

r    c chen
   

r   w wang

r   c chen

r   w wang

r   c chen
r   l li

rel   q 

   

r    c chen
r    l li
   
r    l li

figure    relevant set for query q r n ame   w w ang  using h expansion and aexpansion alternately

at level   to retrieve the level   references 
rel   q    xh  rel   q  
figure   illustrates this operation in our example  where xh  r    retrieves references c 
chen  r    and a  ansari  r     and so on 
to perform collective resolution for the query  we additionally need to resolve the references at level    one option for level   references is attribute based resolution using a
conservative  similarity to keep imprecision to a minimum  we can use our analysis technique from before to evaluate the performance for this approach  expanding from eq      
and substituting ai  eiq     for the recall of each neighboring entity eiq for eq   the recall for
any query entity is 
r eq         ai  eq           ai  eq       ri  eq     

k
x

e

pi q ai  eiq    

i  

similarly  on substituting aa  eiq   ej     in eq       for the imprecision of each neighboring
entity eiq   we get the following expression for imprecision 
i eq   e         aa  eq   e           aa  eq   e       ra  eq   e     

k x
l
x

e





pi q pej aa  eiq   e j    

i   j  

to appreciate more easily the implications of considering first order neighbors  we may
assume that the attribute identification probability and the attribute ambiguity probability
are the same for all the entities involved  i e   ai  e      ai    and aa  e  e       aa     then 
p
using ki   pei     for any entity e  the expression for recall simplifies to
r eq         ai          ai      ri     ai   
  ai            ai    ri    
   

fiquery time entity resolution

similarly  the expression for imprecision simplifies to
i eq   e         aa            aa    ra    
so we can see that attribute clustering of the first level neighbors potentially increases
recall for any query entity eq   but imprecision goes up as well  however  when the balance between ra and ri is favorable  the increase in imprecision is insignificant and much
smaller than the corresponding increase in recall  so that there is an overall performance
improvement 
can we do better than this  we can go a step further and consider co occurrence
relations for resolving the level   references as well  so  instead of considering attributebased resolution for references in level   as before  we perform collective resolution for them 
we consider all of their  similar references  which we call level   references  rel   q    using
a expansion 
rel   q    xa  rel   q  
note that we have overloaded the a expansion operator for a set r of references  xa  r   
rr xa  r a   the level   references are the second order neighbors that co occur with
level   references  they are retrieved using h expansion on the level   references 

s

rel   q    xh  rel   q  
finally  as with the level   references earlier  we resolve the level   references using similarity of their attributes alone 
in order to evaluate the impact on resolution accuracy for the query  we unfold the
recursions in eq      and eq       up to two levels  and now substitute ai  eiq     for recall
and aa  ei   ej     for imprecision for the second order neighbors  the trend in the expressions
becomes clearly visible if we assume  as before  that ai and aa is identical for all entities  and 
additionally  ri and ra are also the same  i e   ri  e    e        ri    and ra  e    e        ra    
then  we can work through a few algebraic steps to get the following expressions for recall
and precision for any query entity eq  
r eq     ai          ai  ri       ai    ri   


i eq   e     aa          aa  ra      

 
aa    ra
 

    
    

we can continue to unfold the recursion further and grow the relevant set for the query 
formally  the expansion process alternates between a expansion and h expansion 
reli  q    xa  q 
xh  reli   q  
xa  reli   q  

for i    
for odd i
for even i

as we proceed recursively and consider higher order co occurrences for the query  additional terms appear in the expressions for precision and recall  but this does not imply
that we need to continue this process to arbitrary levels to get optimum benefit  using
our simplifying assumptions about the attribute and relational probabilities  the expressions for both recall and imprecision for nth order co occurrences turns out to be geometric
   

fibhattacharya   getoor

progressions with n     terms  the common ratio for the two geometric progressions are
    ai    ri    and     aa    ra    respectively  typically  both of these ratios are significantly smaller than    and therefore converge very quickly with increasing co occurrence
level  so the improvement in resolution accuracy for the query q falls off quickly with
expansion depth  and we can terminate the expansion process at some cut off depth d
without compromising on accuracy 


rel q   

d
 

reli  q 

i  

of course  the assumptions about the attribute and relational probabilities being entityindependent do not hold in practice  so that the performance trends for increasing levels of
co occurrence cannot be exactly captured by geometric progressions with a common ratio
for successive terms  but the converging trends for both of them still hold in general  and
the rate of convergence is still determined by the four probabilities ai   aa   ri and ra for the
entities that are encountered during the expansion process  intuitively  smaller values for
ri and ra indicate less sensitivity to co occurrences  and the convergence is quicker  on
the other hand  higher values of ai and aa mean that more entities are resolved based on
attributes alone  correctly or incorrectly  and the impact of co occurrence relations is
smaller  therefore convergence is quicker for higher values of ai and aa  
apart from imposing a cutoff on the expansion depth  the size of the relevant set can
also be significantly reduced by restricting attribute expansion beyond level   to exact
e  r   this only considers references with exactly the same attribute as
a expansion xa
r and disregards other  similar references  interestingly  we can show that the restricted
strategy that alternates between exact a expansion and h expansion does not reduce recall
significantly 

   adaptive query expansion
the limited depth query expansion strategy proposed in the previous section is an effective
approach that is able to answer queries quickly and accurately for many domains  however 
for some domains  the size of the relevant set that is generated can be extremely large even
for small expansion depths  and as a result  the retrieved references cannot be resolved
at query time  in this section  we propose adaptive strategies based on estimating the
ambiguity of individual references that makes our algorithm even more efficient while
preserving accuracy 
the main reason behind this explosive growth of the relevant set with increasing levels
is that our query expansion strategy from the previous section is unconstrained  it treats
all co occurrences as equally important for resolving any entity  it blindly expands all
references in the current relevant set  and also includes all new references generated by an
expansion operation  given the limited time to process a query  this approach is infeasible
for domains that have dense relationships  our solution is to identify the references that are
likely to be the most helpful for resolving the query  and to focus on only those references 
to illustrate using our example from figure    observe that chen and li are significantly
more common or ambiguous names than ansari  even different w  wang entities are
likely to have collaborators named chen or li  therefore  when h expanding rel   rq   for
   

fiquery time entity resolution

w  wang  ansari is more informative than chen or li  similarly  when n expanding
rel   rq    we can choose not to expand the name a  ansari any further  since two a 
ansari references are very likely to be coreferent  but we need more evidence for the
chens and the lis 
to describe this formally  the ambiguity of a value a for an attribute a is the probability that any two references ri and rj in the database that have ri  a   rj  a   a are not
coreferent  amb a    p  e ri      e rj     ri  a   rj  a   a   the goal of adaptive expansion
is to add less ambiguous references to the relevant set and to expand the most ambiguous
references currently in the relevant set  we first define adaptive versions of our two expansion operators treating the ambiguity estimation process as a black box  and then look at
ways to estimate ambiguity of references 
    adaptive expansion operators
the goal of adaptive expansion is to selectively choose the references to expand from the
current relevant set  and also the new references that are included at every expansion step 
for adaptive hyper edge expansion  we set an upper bound hmax on the number of new
references that h expansion at a particular level can generate  formally  we want
 xh  reli  q     hmax  reli  q    the value of hmax may depend on depth i but should be
small enough to rule out full h expansion of the current relevant set  then  given hmax   our
strategy is to choose the least ambiguous references from xh  reli  q    since they provide
the most informative evidence for resolving the references in reli  q   to achieve this  we
sort the h expanded references in increasing order of ambiguity and select the first k from
them  where k   hmax  reli  q   
i 
i
reladapt
 q  hmax     leastamb k  xh  reladapt
 q   

    

the setting for adaptive attribute expansion is very similar  for some positive number amax   exact a expansion of reli  q  is allowed to include at most amax  reli  q   references  note that now the selection preference needs to be flipped  more ambiguous names
e  reli  q   in decreasing
need more evidence  so they are expanded first  so we can sort xa
order of ambiguity and select the first k from the sorted list  where k   amax  reli  q    but
this could potentially retrieve only references for the most ambiguous name  totally ignoring
references with any other name  to avoid this  we choose the top k ambiguous references
from reli  q  before expansion  and then expand the references so chosen 
i
e
i
reladapt
 q  nmax     xa
 m ostamb k  reladapt
 q   

    

though this cannot directly control the number of new references added  r  k is a reasonable estimate  where r is the average number of references per name 
    ambiguity estimation
the adaptive expansion scheme proposed in this section is crucially dependent on the estimates of name ambiguity  we now describe one possible scheme that worked quite well 
recall that we want to estimate the probability that two randomly picked references with
value a for attribute a correspond to different entities  for a reference attribute a    denoted
   

fibhattacharya   getoor

  
  

algorithm query time resolve  r name name 
rset   relevantfrontier name 
rc er rset 

  
  
  
  
  
  
  
  
  
   
   
   

algorithm findrelevantrefs r name name 
initialize rset to   
initialize depth to  
initialize frontierrefs to   
while depth   d 
if depth is even or  
r   xa  frontierrefs 
else
r   xh  frontierrefs 
frontierrefs   r
add frontierrefs to rset
increment depth
return rset
figure    high level description of the query time entity resolution algorithm

r a    a naive estimate for the ambiguity of a value of n for the attribute is 
amb r a     

 r a   r a   r  
 
 r 

where  r a   r a   r   denotes the number of references with value r a  for a    this estimate is clearly not good since the number of references with a certain attribute value does
not always match the number of different entity labels for that attribute  we can do much
better if we have an additional attribute a    given a    the ambiguity for value of a  can
be estimated as
  r a   r a   r a   r    
amb r a    r a     
 
 r 
where   r a   r a   r a   r     is the number of distinct values observed for a  in references with r a    r a    for example  we can estimate the ambiguity of a last name by
counting the number of different first names observed for it  this provides a better estimate
of the ambiguity of any value of an attribute a    when a  is not correlated with a    when
multiple such uncorrelated attributes ai are available for references  this approach can be
generalized to obtain better ambiguity estimates 
putting everything together  high level pseudo code for the query time entity resolution
algorithm is shown in figure    the algorithm works in two stages  first  it identifies the
relevant set of references given an entity name as a query  and then it performs relational
clustering on the extracted relevant references  the relevant references are extracted using
the recursive process that we have already seen  the relevant references at any depth i are
obtained by expanding the relevant references at depth i   the expansion being dependent
   

fiquery time entity resolution

of whether it is an odd step or an even step  the actual expansion operator that is used
may either be unconstrained or adaptive 

   empirical evaluation
for experimental evaluation of our query time resolution strategies  we used both realworld and synthetically generated datasets  first  we describe our real datasets and the
experiments performed on them and then we move on to our experiments on synthetic data 
    experiments on real data
for real world data  we used two citation datasets with very different characteristics  the
first dataset  arxiv  contains papers from high energy physics and was used in kdd cup
        it has        references to       authors  contained in        publications  the number of author references per publication ranges from   to    with an average of       our
second dataset is the elsevier biobase database  of publications from biology used in the
recent ibm kdd challenge competition  it includes all publications under immunology
and infectious diseases between years      and       this dataset contains         publications with         author references  the number of author references per publication
is significantly higher than arxiv and ranges from   to      average       all names in this
database only have initials for first and middle names  if available   unlike arxiv  which has
both initialed and complete names  the number of distinct names in biobase is         
with the number of references for any name ranging from   to      average       unlike
arxiv  biobase includes keywords  topic classification  language  country of correspondence
and affiliation of the corresponding author as attributes of each paper  all of which we use
as attributes for resolution in addition to author names  biobase is diverse in terms of
these attributes  covering    languages      countries        topic classifications and      
keywords 
for entity resolution queries in arxiv  we selected all ambiguous names that correspond
to more than one author entity  this gave us    queries  with the number of true entities
for the selected names varying from   to     average       for biobase  we selected as
queries the top     author names with the highest number of references  the average
number of references for each of these     names is      and the number of entities for the
selected names ranges from   to      average      thereby providing a wide variety of entity
resolution settings over the queries 
      relevant set size vs  resolution time
we begin by exploring the growth rate of the relevant set for a query over expansion depth
in the two datasets  figure   a  plots the size of the relevant set for a sample query on the
name t  lee for arxiv and m  yamashita for biobase  the growth rate for the arxiv
query is moderate  the number of references with name t  lee is    which is the number
of relevant references at depth    and the size grows to       by depth    in contrast  for
biobase the plots clearly demonstrate the exponential growth of the relevant references
   http   www cs cornell edu projects kddcup index html
   http   help sciencedirect com robo projects sdhelp about biobase htm

   

fibhattacharya   getoor

   

   

biobase  similar
biobase  exact
arxive  exact

   

   
time  secs 

  references
 in thousands 

   
   
   
   

   
   
   
   

   

   

   

   

 

 
 

 a 

biobase
arxiv

   

 

 

 

 

expansion depth

 

 

 

 
 b 

  

  
  
  
  
 references  in thousands 

  

  

figure     a  size of the relevant set for increasing expansion depth for sample queries
in arxiv and biobase  b  execution time of rc er with increasing number of
references

with depth for both name expansion strategies  there are    relevant references at depth
   when references are expanded using name similarity expansion  there are     relevant
references at depth           at depth   and more than         at depth    this is for a
very restricted similarity measure where two names are considered similar only if their first
initials match  and the last names have the same first character and differ overall by at most
  characters  a more liberal measure would result in a significantly faster growth  we also
observe that for exact expansion  the growth is slower but we still have        references at
depth            at depth   and         by depth    it is interesting to note that the growth
slows down beyond depth    but this is because most of the references in the entire dataset
are already covered at that depth  biobase has         references in total   the growth
rates for these two examples from arxiv and biobase are typical for all of our queries in
these two datasets 
next  in figure   b   we observe how the relational clustering algorithm rc er scales
with increasing number of references in the relevant set  all execution times are reported
on a dell precision     server with    ghz intel xeon processor and  gb of memory  the
plot shows that the algorithm scales well with increasing references  but the gradient is
different for the two datasets  this is mainly due to the difference in the average number of
references per hyper edge  this suggests that for arxiv  rc er is capable of handling the
relevant sets generated using unconstrained expansion  but for biobase  it would require
up to     secs for        references  and up to     secs for         so it is clearly not
possible to use rc er with unconstrained expansion for query time resolution in biobase
even for depth   
      entity resolution accuracy for queries
in our next experiment  we evaluate several algorithms for entity resolution queries  we
compare entity resolution accuracy of the pair wise co reference decisions using the f 
measure  which is the harmonic mean of precision and recall   for a fair comparison  we
consider the best f  for each of these algorithms over all possible thresholds for determining
   

fiquery time entity resolution

table    average entity resolution accuracy  f   for different algorithms over    arxiv
queries and     biobase queries

a
a 
nr
nr 
rc er depth  
rc er depth  

arxiv
     
     
     
     
     
     

biobase
     
     
     
     
     
     

duplicates  for the algorithms  we compare attribute based entity resolution  a   naive
relational entity resolution  nr  that uses attributes of related references  and our relational
clustering algorithm for collective entity resolution  rc er  using unconstrained expansion
up to depth    we also consider transitive closures over the pair wise decisions for the first
two approaches  a  and nr    for attribute similarity  we use the soft tf idf with
jaro winkler similarity for names  which has been shown to perform the best for namebased resolution  bilenko  mooney  cohen  ravikumar    fienberg         and tf idf
similarity for the other textual attributes 
the average f  scores over all queries are shown in table   for each algorithm in the
two datasets  it shows that rc er improves accuracy significantly over the baselines 
for example in biobase  the improvement is     over a and nr      over a  and    
over nr   this demonstrates the potential benefits of collective resolution for answering
queries  and validates recent results in the context of offline entity resolution  bhattacharya
  getoor              singla   domingos        dong et al         mccallum   wellner 
       in our earlier work  bhattacharya   getoor        we have demonstrated using
extensive experiments on real and synthetic datasets how our relational clustering algorithm
 rc er  improves entity resolution performance over traditional baselines in the context
of offline data cleaning  where the entire database is cleaned as a whole  the numbers
in table   confirm that similar improvements can be obtained for localized resolution as
well  as predicted by our analysis  most of the accuracy improvement comes from the
depth   relevant references  for    out of the     biobase queries  accuracy does not
improve beyond the depth   relevant references  for the remaining    queries  the average
improvement is     however  for   of the most ambiguous queries  accuracy improves by
more than     the biggest improvement being as high as      from      to      f    such
instances are fewer for arxiv  but the biggest improvement is        from       to      
on one hand  this shows that considering related records and resolving them collectively
leads to significant improvement in accuracy  on the other hand  it also demonstrates that
while there are potential benefits to considering higher order neighbors  they fall off quickly
beyond depth    this also serves to validate our analysis of collective query resolution in
section   
   

fibhattacharya   getoor

table    average query processing time with unconstrained expansion

a
a 
nr
nr 
rc er depth  
rc er depth  

 

arxiv
    
    
    
     
    
    

biobase
    
    
     
     
     
      

 

depth  
depth  

depth  
depth  

   

   
recall

precision

   
   

   
   

   

   
   
   

   
 

   

   

   

   

 

 

   

similarity threshold

 a 

   
   
similarity threshold

   

 

 b 
 

 

   
   
recall

precision

   

   

   
   
   

   

   

depth  
depth  

   
 

   

depth  
depth  

   
   

   

   

 

 

similarity threshold

 c 

   

   
   
similarity threshold

   

 

 d 

figure    average precision and recall at different similarity thresholds for  a b  biobase
and  c d  arxiv

the last two rows of table   show the converging nature of entity resolution performance
with increasing depth  we verify this explicitly for precision and recall in figure    the
top two plots show average precision and recall over biobase queries at different similarity
thresholds for rc er  the bottom two plots show the same for arxiv  we can see that
the precision curve at depth   coincides with or stays marginally above the precision curve
at depth   for both biobase and arxiv  the recall curves show the opposite trend  recall
   

fiquery time entity resolution

marginally improves for depth    this is in agreement with our derived expressions for
precision and recall for increasing depth in eq        the difference in recall between depths
  
  and   can be quantified as ai     ai    ri    and the difference in precision as aa     aa    ra
the explanation for the small difference between average precision and recall in these two
plots is that both of these factors  when averaged over all queries  are significantly smaller
than   for arxiv and biobase  we will investigate this converging nature of performance in
more detail by varying these structural properties in our experiments with synthetic data
in section     
      reducing time with adaptive expansion
the first set of experiments show the effectiveness of our two phase query processing strategy
in terms of entity resolution performance  the challenge  as we have described earlier  is
in obtaining these benefits in real time  so  next  we focus on the time that is required to
process these queries in the two datasets using unconstrained expansion up to depth    the
results are shown in table    for arxiv  the average processing time for depth   expansion
is      secs  with     relevant references on average  this shows that our two phase strategy
with unconstrained expansion is a practical processing strategy for entity resolution queries
 it resolves the query entities accurately  and extremely quickly as well  however  for
biobase  the average number of references reached by depth   is more that         and the
time taken to resolve them collectively is more than    minutes  this is unacceptable for
answering queries  and next we focus on how the processing time is improved using our
proposed adaptive strategies  note that the time taken for depth   expansion is around   
secs  which is close to that for the attribute based baseline  a  and less than the time for
the naive relational algorithm  nr  
since unconstrained expansion is effective for arxiv  we focus only on biobase for evaluating our adaptive strategies  for estimating ambiguity of references  we use last names
with first initial as the secondary attribute  this results in very good estimates of ambiguity  the ambiguity estimate for a name is strongly correlated  correlation coeff      
with the number of entities for that name  first  we evaluate adaptive h expansion  since
h expansion occurs first at depth    for each query  we construct the relevant set with cutoff
depth d      and use adaptive h expansion for depth    the expansion upper bound hmax
is set to    we compare three different adaptive h expansion strategies   a  choosing the
least ambiguous references   b  choosing the most ambiguous references and  c  random
selection  then  for each query  we evaluate entity resolution accuracy using rc er on
the relevant sets constructed using these three adaptive strategies  the average accuracies
for the three strategies over all     queries are shown in the first column of table    least
ambiguous selection  which is the strategy that we propose  clearly shows the biggest improvement and most ambiguous the smallest  while random selection is in between  notably 
even without many of the depth   references  all of them improve accuracy over nr  by
virtue of collective resolution 
we perform a similar set of experiments for evaluating adaptive attribute expansion 
recall that depth   is the lowest depth where adaptive attribute expansion is performed 
so for each query  we construct the relevant set with d     using adaptive a expansion
at depth   and unconstrained h expansion at depths   and    the expansion upper bound
   

fibhattacharya   getoor

table    avg  resolution accuracy in f  with different adaptive expansion strategies

least ambiguous
most ambiguous
random

h expansion
     
     
     

a expansion
     
     
     

amax is set to      so that on average   out of   names are expanded  again  we compare three
strategies   a  expanding the least ambiguous names   b  expanding the most ambiguous
names and  c  random expansion  the average accuracies for the three schemes over all
    queries are listed in the second column of table    the experiment with adaptive aexpansion does not bring out the difference between the three schemes as clearly as adaptive
h expansion  this is because we are comparing a expansion at depth   and  on average 
not much improvement can be obtained beyond depth   because of a ceiling effect  but
it shows that almost all the benefit up to depth   comes from our proposed strategy of
expanding the most ambiguous names 
the above two experiments demonstrate the effectiveness of the two adaptive expansion
schemes in isolation  now  we look at the results when we use them together  for each
of the     queries  we construct the relevant set rel rq   with d     using adaptive hexpansion and adaptive exact a expansion  since most of the improvement from collective
resolution comes from depth   references  we consider two different experiments  in the
first experiment  ax     we use adaptive expansion only at depths   and beyond  and
unconstrained h expansion at depth    in the second experiment  ax     we use adaptive
h expansion even at depth    with hmax      for both of them  we use adaptive expansion
at higher depths   and   with parameters hmax     at   and amax       at   
table    comparison between unconstrained and adaptive expansion for biobase

relevant set size
time  cpu secs 
accuracy  f  

unconstrained
        
      
     

ax  
        
     
     

ax  
        
     
     

in table    we compare the two adaptive schemes against unconstrained expansion with
    over all queries  clearly  accuracy remains almost unaffected for both schemes 
first  we note that ax   matches the accuracy of unconstrained expansion  and shows
almost the same improvement over depth    this accuracy is achieved even though it
uses adaptive expansion that expands a small fraction of rel   q   and thereby reduces the
average size of the relevant set from        to        more significantly  ax   also matches
this improvement even without including many depth   references  this reduction in the
size of the relevant set has an immense impact on the query processing time  the average
processing time drops from more than     secs for unconstrained expansion to    secs for
d

   

fi   
 
   
   
   
   
   
   
   
   

 

pr    
pr    
pr    

   
   
precision

recall

query time entity resolution

   
   
   

pra    
pra    
pra    

   
   
   

   
   
sim  threshold

   

   

    

   
    
   
sim  threshold

    

figure    effect of  a  identifying relations on recall and  b  ambiguous relations on precision for collective clustering  error bars show standard deviation 

ax    and further to just    secs for ax    thus making it possible to use collective entity
resolution for query time resolution 
      adaptive depth selection
as a further improvement  we investigate if processing time can be reduced by setting the
expansion depth d adaptively  depending on the ambiguity of the query name  as compared
to a fixed d for all queries  in a simple setup  we set d to   for queries where the number
of different first initials for a last name is less than     out of      and explore depth   only
for more ambiguous queries  this reduces expansion depth from   to   for    out of the    
queries  as a result  the average processing time for these queries is reduced by     to     
secs from      secs with no reduction in accuracy  for three of these queries  the original
processing time at depth   is greater than    secs  in these preliminary experiments  we only
evaluated our original set of     queries that are inherently ambiguous  in a more general
setting  where a bigger fraction of queries have lower ambiguity  the impact is expected to
be even more significant 
    experiments using synthetic data
in addition to experiments on real datasets  we performed experiments on synthetically
generated data  this enables us to reason beyond specific datasets  and also to empirically
verify our performance analysis for relational clustering in general  and more specifically for
entity resolution queries  we have designed a generator for synthetic data  bhattacharya
  getoor        that allows us to control different properties of the underlying entities and
the relations between them  and also of the observed co occurrence relationships between
the entity references  among other properties  we can control the number of entities 
the average number of neighbor entities per entity  and the number and average size of
observed co occurrences  additionally  we can control the ambiguity of entity attributes 
and the number of ambiguous relationships between entities  we present an overview of the
synthetic data generation process in appendix a 
   

fibhattacharya   getoor

 

   
    
precision

   
recall

    

t    
t    
t    

   
   
   
   

   
    
   

   

    

   

   

   

t    
t    
t    

    
 

 
 
expansion level

 

 

 
 
expansion level

 

figure    change in  a  precision and  b  recall for increasing expansion levels used for
collective clustering  error bars show standard deviation 

we have performed a number of different experiments on synthetic data  in the first set
of experiments  we investigate the influence of identifying relationships on collective resolution using relational clustering  we generate     co occurrence relations from the same
    entities and     entity entity relationships  using varying probability of co occurrences
pr                   in the data  the probability of ambiguous relationships is held fixed 
so that higher pr translates to higher probability of identifying co occurrences in the data 
figure   a  shows recall at different similarity thresholds for three different co occurrence
probabilities  the results confirm that recall increases progressively with more identifying
relationships at all thresholds  the curves for pr       and pr       flatten out only when
no further recall is achievable 
next  we observe the effect of ambiguous relations on the precision of collective resolution using relational clustering  we add     binary relationships between     entities in
three stages with increasing ambiguous relationship probability  pr
a                   then
we perform collective resolution on     co occurrence relations generated from each of these
three settings  in figure   b  we plot precision at different similarity threshold for three different values of pr
a   the plots confirm the progressive decrease in precision for all thresholds
with higher pr
 
for
both experiments  the results are averaged over     different runs 
a
next  we evaluate collective resolution for queries  recall that the last two rows in table   clearly demonstrate the converging nature of performance over increasing expansion
levels for queries on real datasets  we ran further experiments on synthetic data to verify
this trend  in each run  we generated       co occurrence relations from     entities having
an average of   neighbors per entity  then we performed localized collective clustering
in each case  using as query the most ambiguous attribute value  that corresponds to the
highest number of underlying entities   in figure   c  and  d   we show how recall and precision change with increasing expansion level for a query  recall improves with increasing
expansion level  while precision decreases overall  as is predicted by our analysis  importantly  recall increases at a significantly faster rate than that for the decrease in precision 
in general  the rate of increase decrease depends on the structural properties of the data 
as we have shown in our analysis  in other experiments  we have seen different rates of
   

fiquery time entity resolution

change  but the overall trend remains the same  our analysis also showed that precision
and recall converge quickly over increasing expansion levels  this too is confirmed by the
two plots where the curves flatten out by level   
    current limitations
finally  we discuss two of the current limitations of our collective entity resolution approach 
recall that the similarity measure in eqn    involves a weighting parameter  for combining
attribute and relational similarity  for all of our experiments  we report the best accuracy
over all values of  for each query  selecting the optimal value of  for each query is an
unresolved issue  however  our experiments reveal that even a fixed           for all
queries brings significant improvements over the baselines 
the second issue is the determination of the termination threshold for rc er  note
that this is an issue for all of the baselines as well  and here we report best accuracy over
all thresholds  this is an area of ongoing research  preliminary experiments have shown
that the best threshold is often query specific  setting the threshold depending on the
ambiguity of the query results in significantly better accuracy than a fixed threshold for all
queries  for an empirical evaluation  we cleaned the entire arxiv dataset offline by running
rc er on all its references together  and terminated at the threshold that maximizes
resolution accuracy over all references  this results in an overall accuracy  f   of      
however  the average accuracy measured over the    queries in our test set is only       in
comparison  the best obtainable accuracy when resolving the queries individually each with
a different threshold is       this suggests that there may be potential benefits to localized
cleaning over its global counterpart in the offline setting 

   related work
the entity resolution problem has been studied in many different areas under different
names  deduplication  record linkage  co reference resolution  reference reconciliation 
object consolidation  etc  much of the work has focused on traditional attribute based
entity resolution  extensive research has been done on defining approximate string similarity
measures  monge   elkan        navarro        bilenko et al         chaudhuri  ganjam 
ganti    motwani        that may be used for unsupervised entity resolution  the other
approach uses adaptive supervised algorithms that learn similarity measures from labeled
data  tejada  knoblock    minton        bilenko   mooney        
resolving entities optimally is known to be computationally hard even when only attributes are considered  cohen  kautz    mcallester         therefore  efficiency has
received a lot of attention in attribute based data cleaning  the goal essentially is to avoid
irrelevant and expensive attribute similarity computations using a blocking approach without affecting accuracy significantly  hernandez   stolfo        monge   elkan        mccallum et al          the merge purge problem was posed by hernandez and stolfo       
with efficient schemes to retrieve potential duplicates without resorting to quadratic complexity  they use a sorted neighborhood method where an appropriate key is chosen for
matching  records are then sorted or grouped according to that key and potential matches
are identified using a sliding window technique  however  some keys may be badly distorted
so that their matches cannot be spanned by the window and such cases will not be retrieved 
   

fibhattacharya   getoor

the solution they propose is a multi pass method over different keys and then merging the
results using transitive closure  monge and elkan        combine the union find algorithm
with a priority queue look up to find connected components in an undirected graph  mccallum et al         propose the use of canopies to first partition the data into overlapping
clusters using a cheap distance metric and then use a more accurate and expensive distance
metric for those data pairs that lie within the same canopy  chaudhuri et al         use
an error tolerant index for data warehousing applications for probabilistically looking up
a small set of candidate reference tuples for matching against an incoming tuple  this is
considered probabilistically safe since the closest tuples in the database will be retrieved
with high probability  this is also efficient since only a small number of matches needs to
be performed  swoosh  benjelloun  garcia molina  su    widom        has recently been
proposed as a generic entity resolution framework that considers resolving and merging
duplicates as a database operator and the goal is to minimize the number of record level
and feature level operations  an alternative approach is to reduce the complexity of individual similarity computations  gravano  ipeirotis  koudas  and srivastava        propose
a sampling approach to quickly compute cosine similarity between tuples for fast text joins
within an sql framework  all of these approaches enable efficient data cleaning when only
attributes of references are considered 
many recently proposed approaches take relations into account for data integration
 ananthakrishna et al         bhattacharya   getoor              kalashnikov et al        
dong et al          ananthakrishna et al         introduce relational deduplication in data
warehouse applications where there is a dimensional hierarchy over the relations  kalashnikov et al         enhance attribute similarity between an ambiguous reference and the
many entity choices for it with relationship analysis between the entities  like affiliation and
co authorship  in earlier work  we have proposed different measures for relational similarity and a relational clustering algorithm for collective entity resolution using relationships
 bhattacharya   getoor               dong et al         collectively resolve entities of multiple types by propagating relational evidences in a dependency graph  and demonstrate the
benefits of collective resolution in real datasets  long  zhang  wu  and yu        have proposed a model for general multi type relational clustering  though it has not been applied
specifically for entity resolution  they perform collective factorization over related matrices
using spectral methods to identify the cluster space that minimizes distortion over relationships and individual features at the same time  all of these approaches that make use of
relationships either for entity matching  where the domain entities are known  or entity
resolution  where the underlying entities also need to be discovered  have been shown to
increase performance significantly over the attribute based solutions for the same problems 
however  the price they pay is in terms of computational complexity that increases due
to a couple of different reasons  firstly  the number of potential matches increases when
relationships are considered and individual similarity computations also become more expensive  secondly  collective resolution using relationships necessitates iterative solutions
that make multiple passes over the data  while some of these approaches have still been
shown to be scalable in practice  they cannot be employed for query time cleaning in a
straight forward manner 
the idea of multi relational clustering also comes up in the inductive logic programming
 ilp  literature  emde and wettschereck        have used multi relational similarity for
   

fiquery time entity resolution

instance based classification of representations in first order logic  they define the similarity
of two objects  e g   of two people  as a combination of the similarity of their attribute
values  such as their age  weight  etc   and the similarity of the objects that they are
related to  such as the companies they work for  this is similar to the naive relational
similarity that we discussed earlier  except that the similarity of the connected objects is
also defined recursively in terms of their connected objects  kirsten and wrobel       
have used this recursive relational similarity measure for agglomerative clustering of first
order representations  while recursive comparison of neighbors is shown to be effective in
terms of accuracy of results  the computational challenge is again a major drawback 
probabilistic approaches that cast entity resolution as a classification problem have been
extensively studied  the groundwork was done by fellegi and sunter         others  winkler        ravikumar   cohen        have more recently built upon this work  adaptive
machine learning approaches have been proposed for data integration  sarawagi   bhamidipaty        tejada et al          where active learning requires the user to label informative
examples  probabilistic models that use relationships for collective entity resolution have
been applied to named entity recognition and citation matching  pasula et al         mccallum   wellner        li et al         singla   domingos         these probabilistic
approaches are superior to similarity based clustering algorithms in that they associate a
degree of confidence with every decision  and learned models provide valuable insight into
the domain  however  probabilistic inference for collective entity resolution is not known
to be scalable in practice  particularly when relationships are also considered  these approaches have mostly been shown to work for small datasets  and are significantly slower
than their clustering counterparts 
little work has been done in the literature for query centric cleaning or relational approaches for answering queries  where execution time is as important as accuracy of resolution  approaches have been proposed for localized evaluation of bayesian networks  draper
  hanks         but not for clustering problems  recently  chandel  nagesh  and sarawagi
       have addressed efficiency issues in computing top k entity matches against a dictionary in the context of entity extraction from unstructured documents  they process top k
searches in batches where speed up is achieved by sharing computation between different
searches  fuxman  fazli  and miller        motivate the problem of answering queries over
databases that violate integrity constraints and address scalability issues in resolving inconsistencies dynamically at query time  however  the relational aspect of the problem 
which is the major scalability issue that we address  does not come up in any of these settings  in our earlier work on relational clustering bhattacharya   getoor         we used
the idea of relevant references for experimental evaluation on the biobase dataset  as
we have also discussed here  this dataset has entity labels only for the     most frequent
names  therefore  instead of running collective resolution over the entire biobase dataset 
we evaluated the     names separately  using only the relevant references in each case 
the relevant references were the ones directly connected to references having the names of
interest  the concept of focused cleaning  the performance analysis of relational clustering  the expand resolve strategy and  most importantly  the idea of adaptive expansion for
query time resolution were not addressed in that paper 
one of the first papers to make use of relational features for classification problem was
by chakrabarti  dom  and indyk         they showed that for the problem of classifying
   

fibhattacharya   getoor

hyper linked documents  naive use of relationships can hurt performance  specifically  if
key terms from neighboring documents are thrown into the document whose topic is to be
classified  classification accuracy degrades instead of improving  the parallel in our scenario
of clustering using relationships is that the naive relational model  nr  may perform worse
than the attribute model  a  in the presence of highly ambiguous relationships  chakrabarti
et al         showed that relationships can however be used for improved classification
when the topic labels of the neighboring documents are used as evidence instead of naively
considering the terms that they contain  in our earlier work  bhattacharya   getoor       
       we have shown similar results for collective clustering using relationships  where the
cluster labels of neighboring labels lead to improved clustering performance compared to
naive relational and attribute based clustering  the interesting result that we have shown
in this paper both in theory and empirically is that even collective use of relationships
can hurt clustering accuracy compared to attribute based clustering  this happens when
relationships between references are dense and ambiguous  and errors that propagate over
relationships exceed the identifying evidence that they provide 

   conclusions
in this paper  we have motivated the problem of query time entity resolution for accessing
unresolved third party databases  for answering entity resolution queries  we have addressed the challenges of using collective approaches  which have recently shown significant
performance improvements over traditional baselines in the offline setting  the first hurdle
for collective resolution arises from the interdependent nature of its resolution decisions  we
first formally analyzed the recursive nature of this dependency  and showed that the precision and recall for individual entities grow in a geometric progression as increasing levels of
neighbors are considered and collectively resolved  we then proposed a two stage expand
and resolve strategy for answering queries based on this analysis  using two novel expansion
operators  we showed using our analysis that it is sufficient to consider neighbors up to small
expansion depths  since resolution accuracy for the query converges quickly with increasing
expansion level  the second challenge for answering queries is that the computation has to
be quick  to achieve this  we improved on our unconstrained expansion strategy to propose
an adaptive algorithm  which dramatically reduces the size of the relevant references 
and  as a result  the processing time  by identifying the most informative references for
any query  we demonstrated using experiments on two real datasets that our strategies
enable collective resolution at query time  without compromising on accuracy  we additionally performed various experiments on synthetically generated data over a wide range
of settings to verify the trends predicted by our analysis  in summary  we have addressed
and motivated a critical data integration and retrieval problem  proposed algorithms for
solving it accurately and efficiently  provided a theoretical analysis to validate our approach
and explain why it works  and  finally  shown experimental results on multiple real world
and synthetically generated datasets to demonstrate that it works extremely well in practice  while we have presented results for bibliographic data  the techniques are applicable
in other relational domains 
while we have shown the dramatic reduction in query processing time that comes with
adaptive expansion  more research is necessary to be able to answer entity resolution queries
   

fiquery time entity resolution

on the order of milli seconds  as may be demanded in many scenarios  interesting directions
of future research include exploring stronger coupling between the extraction and resolution
phases of query processing  where the expansion happens on demand only when the
resolution process finds the residual ambiguity to be high and requires additional evidence
for taking further decisions  this would directly address the problem of determining the
expansion depth  while we have reported some preliminary experiments in this paper  more
work needs to be done on adaptive depth determination depending on ambiguity  in the
same context  we may imagine soft thresholds for adaptive expansion  where the expansion
operator automatically determines the number of hyper edges or names to be expanded so
that the residual ambiguity falls below some specified level  other interesting extensions
include caching of intermediate resolutions  where the related resolutions performed for any
query are stored and retrieved as and when required for answering future queries 

acknowledgments
we wish to thank our anonymous reviewers for their constructive suggestions which greatly
improved this paper  this work was supported by the national science foundation  nsf
         and nsf           with additional support from the itic kdd program 

appendix a
synthetic data generator
we have designed a synthetic data generator that allows us to control different structural
and attribute based characteristics of the data bhattacharya   getoor         here we
present an overview of the generation algorithm 
the generation process has two stages  in the first stage  we create the collaboration
graph among the underlying entities and the entity attributes  in the second  we generate
observed co occurrence relations from this collaboration graph  a high level description
of the generative process in shown in figure     next  we describe the two stages of the
generation process in greater detail 
the graph creation stage  in turn  has two sub stages  first  we create the domain
entities and their attributes and then add relationships between them  for creating entities 
we control the number of entities and the ambiguity of their attributes  we create n entities
and their attributes one after another  for simplicity and without losing generality  each
entity e has a single floating point attribute e x  instead of a character string  a parameter
pa controls the ambiguity of the entity attributes  with probability pa the attribute of a new
entity is chosen from values that are already in use by existing entities  then m binary
relationships are added between the created entities  as with the attributes  there is a
parameter controlling the ambiguity of the relationships  as defined in section    for each
binary relationship  ei   ej    first ei is chosen randomly and then ej is sampled so that  ei   ej  
is an ambiguous relationship with probability pr
a 
before describing the process of generating co occurrence relationships from the graph 
let us consider in a little more detail the issue of attribute ambiguity  what finally needs
to be controlled is the ambiguity of the reference attributes  while these depend on the
entity attributes  they are not completely determined by entities  taking the example
   

fibhattacharya   getoor

  
  
  
  
  
  
  

creation stage
repeat n times
create random attribute x with ambiguity pa
create entity e with attribute x
repeat m times
choose entity ei randomly
choose entity ej with prob pr
a of an ambiguous relationship  ei   ej  
set ei   n br ej   and ej   n br ei  

  
  
   
   
   
   
   
   
   

generation stage
repeat r times
randomly choose entity e
generate reference r using n  e x    
initialize hyper edge h   hri
repeat with probability pc
randomly choose ej from n br e  without replacement
generate reference rj using n  ej  x    
add rj hyper edge h
output hyper edge h

figure     high level description of synthetic data generation algorithm

of names  two people who have names john michael smyth and james daniel smith
can still be ambiguous in terms of their observed names in the data depending on the
generation process of observed names  in other words  attribute ambiguity of the references
depends both on the separation between entity attributes and the dispersion created by the
generation process  we make the assumption that for an entity e with attribute e x  its
references are generated from a gaussian distribution with mean x and variance      so 
with very high probability  any reference attribute generated from e x will be in the range
 e x     e x       so this range in the attribute domain is considered to be occupied by
entity e  any entity has an ambiguous attribute if its occupied range intersects with that
of another entity 
now we come to the generation of co occurrence relationships from the entity collaboration graph  in this stage  r co occurrence relationships or hyper edges are generated  each
with its own references  for each hyper edge hri   ri            rik i  two aspects need to be controlled  how many references and which references should be included in this hyper edge 
this is done as follows  first  we sample an entity ei which serves the initiator entity for
this hyper edge  then other entities eij for this hyper edge are repeatedly sampled  without replacement  from the neighbors of the initiator entity ei   the size of the hyper edge is
determined using a parameter pc   the sampling step for a hyper edge is terminated with
probability pc after each selection eij   the process is also terminated when the neighbors
of the initiator entity are exhausted  finally  references rij need to be generated from each
of the selected entities eij   this is done for each entity e by sampling from its gaussian
distribution n  e x     
   

fiquery time entity resolution

references
ananthakrishna  r   chaudhuri  s     ganti  v          eliminating fuzzy duplicates in
data warehouses  in the international conference on very large databases  vldb  
hong kong  china 
benjelloun  o   garcia molina  h   su  q     widom  j          swoosh  a generic approach
to entity resolution  tech  rep   stanford university 
bhattacharya  i     getoor  l          iterative record linkage for cleaning and integration  in the sigmod workshop on research issues on data mining and knowledge
discovery  dmkd   paris  france 
bhattacharya  i     getoor  l          relational clustering for multi type entity resolution  in the acm sigkdd workshop on multi relational data mining  mrdm  
chicago  il  usa 
bhattacharya  i     getoor  l          mining graph data  l  holder and d  cook  eds   
chap  entity resolution in graphs  wiley 
bhattacharya  i     getoor  l          collective entity resolution in relational data  acm
transactions on knowledge discovery from data  tkdd         
bhattacharya  i   licamele  l     getoor  l          query time entity resolution  in the
acm international conference on knowledge discovery and data mining  sigkdd  
philadelphia  pa  usa 
bilenko  m     mooney  r          adaptive duplicate detection using learnable string
similarity measures  in the acm international conference on knowledge discovery
and data mining  sigkdd   washington dc  usa 
bilenko  m   mooney  r   cohen  w   ravikumar  p     fienberg  s          adaptive name
matching in information integration   ieee intelligent systems               
chakrabarti  s   dom  b     indyk  p          enhanced hypertext categorization using
hyperlinks  in proceedings of the acm international conference on management of
data  sigmod  
chandel  a   nagesh  p  c     sarawagi  s          efficient batch top k search for
dictionary based entity recognition  in the ieee international conference on data
engineering  icde   washington  dc  usa 
chaudhuri  s   ganjam  k   ganti  v     motwani  r          robust and efficient fuzzy
match for online data cleaning  in the acm international conference on management
of data  sigmod   san diego  ca  usa 
cohen  w   kautz  h     mcallester  d          hardening soft information sources  in the
acm international conference on knowledge discovery and data mining  sigkdd  
boston  ma  usa 
dong  x   halevy  a     madhavan  j          reference reconciliation in complex information spaces  in the acm international conference on management of data
 sigmod   baltimore  md  usa 
   

fibhattacharya   getoor

draper  d     hanks  s          localized partial evaluation of belief networks  in the
annual conference on uncertainty in artificial intelligence  uai   seattle  wa  usa 
emde  w     wettschereck  d          relational instance based learning  in proceedings
of the international conference on machine learning  icml  
fellegi  i     sunter  a          a theory for record linkage  journal of the american
statistical association               
fuxman  a   fazli  e     miller  r          conquer  efficient management of inconsistent
databases  in the acm international conference on management of data  sigmod   baltimore  md  usa 
gravano  l   ipeirotis  p   koudas  n     srivastava  d          text joins for data cleansing and integration in an rdbms  in the ieee international conference on data
engineering  icde   bangalore  india 
hernandez  m     stolfo  s          the merge purge problem for large databases  in the
acm international conference on management of data  sigmod   san jose  ca 
usa 
kalashnikov  d   mehrotra  s     chen  z          exploiting relationships for domainindependent data cleaning  in siam international conference on data mining  siam
sdm   newport beach  ca  usa 
kirsten  m     wrobel  s          relational distance based clustering  in proceedings of
the international workshop on inductive logic programming  ilp  
li  x   morie  p     roth  d          semantic integration in text  from ambiguous names
to identifiable entities  ai magazine  special issue on semantic integration         
long  b   zhang  z  m   wu  x     yu  p  s          spectral clustering for multi type relational data  in proceedings of the   rd international conference on machine learning
 icml  
mccallum  a   nigam  k     ungar  l          efficient clustering of high dimensional data
sets with application to reference matching  in the acm international conference
on knowledge discovery and data mining  sigkdd   boston  ma  usa 
mccallum  a     wellner  b          conditional models of identity uncertainty with application to noun coreference  in advances in neural information processing systems
 nips   vancouver  bc  canada 
monge  a     elkan  c          the field matching problem  algorithms and applications 
in the acm international conference on knowledge discovery and data mining
 sigkdd   portland  or  usa 
monge  a     elkan  c          an efficient domain independent algorithm for detecting
approximately duplicate database records  in the sigmod workshop on research
issues on data mining and knowledge discovery  dmkd   tuscon  az  usa 
navarro  g          a guided tour to approximate string matching  acm computing
surveys               
   

fiquery time entity resolution

pasula  h   marthi  b   milch  b   russell  s     shpitser  i          identity uncertainty and
citation matching  in advances in neural information processing systems  nips  
vancouver  bc  canada 
ravikumar  p     cohen  w          a hierarchical graphical model for record linkage 
in the conference on uncertainty in artificial intelligence  uai   banff  alberta 
canada 
sarawagi  s     bhamidipaty  a          interactive deduplication using active learning 
in proceedings of the eighth acm international conference on knowledge discovery
and data mining  sigkdd   edmonton  alberta  canada 
singla  p     domingos  p          multi relational record linkage  in the sigkdd workshop on multi relational data mining  mrdm   seattle  wa  usa 
tejada  s   knoblock  c     minton  s          learning object identification rules for
information integration  information systems journal                 
winkler  w          methods for record linkage and bayesian networks  tech  rep   statistical research division  u s  census bureau  washington  dc 

   

fi