journal articial intelligence research                  

submitted        published      

learning play using low complexity rule based policies 
illustrations ms  pac man
istvn szita
andrs lrincz

szityu eotvos elte hu
andras lorincz elte hu

dept  information systems
etvs university  hungary  h     

abstract
article propose method deal certain combinatorial reinforcement learning tasks  demonstrate approach popular ms  pac man game 
dene set high level observation action modules  rule based policies
constructed automatically  policies  actions temporally extended  may
work concurrently  policy agent encoded compact decision list  components list selected large pool rules  either hand crafted
generated automatically  suitable selection rules learnt cross entropy
method  recent global optimization algorithm ts framework smoothly  crossentropy optimized policies perform better hand crafted policy  reach score
average human players  argue learning successful mainly  i  policies
may apply concurrent actions thus policy space suciently rich   ii  search
biased towards low complexity policies therefore  solutions compact description
found quickly exist 

   introduction
last two decades  reinforcement learning  rl  reached mature state 
laid solid foundations  large variety algorithms  including valuefunction based  direct policy search hybrid methods  reviews subjects 
see  e g   books bertsekas tsitsiklis        sutton barto        
basic properties many algorithms relatively well understood  e g  conditions
convergence  complexity  eect various parameters  although needless say
still lots important open questions  plenty test problems
 like various maze navigation tasks  pole balancing  car hill etc  
capabilities rl algorithms demonstrated  number large scale rl
applications growing steadily  however  current rl algorithms far
out of the box methods  still need demonstrations showing rl
ecient complex tasks 
think games  including diverse set classical board games  card games  modern computer games  etc   ideal test environments reinforcement learning  games
intended interesting challenging human intelligence therefore 
ideal means explore articial intelligence still missing  furthermore 
games well rl paradigm  goal oriented sequential decision problems 
decision long term eects  many cases  hidden information  random
events  unknown environment  known unknown players account  part of  diculty
c      ai access foundation  rights reserved 


fiszita   lrincz

playing game  circumstances focus reinforcement learning  games
attractive testing new methods  decision space huge cases 
nding good strategy challenging task 
another great advantage using games test problems  rules games
xed  danger  tailoring task algorithm  i e   tweak rules
and or environment meet capabilities proposed rl algorithm
reduced  compared  e g   various maze navigation tasks 
rl tried many classical games  including checkers  samuel         backgammon  tesauro         chess  baxter  tridgell    weaver         hand 
modern computer games got spotlight recently  many
successful attempts learn ai tools  notable exceptions are  example  roleplaying game baldur s gate  spronck  sprinkhuizen kuyper    postma         real time
strategy game wargus  ponsen   spronck         possibly  tetris  szita   lrincz 
       games pose new challenges rl  example  many observations
considered parallel  observation space action space huge 
spirit  decided investigate arcade game ms  pac man  game
interesting largely unsolved  imposes several important questions
rl  overview section    provide hand coded high level actions
observations  task rl learn combine good policy 
apply rule based policies  easy interpret enable one include
human domain knowledge easily  learning  apply cross entropy method 
recently developed general optimization algorithm  show hybrid approach
successful either tabula rasa learning hand coded strategy alone 
next section introduce ms  pac man game briey discuss
formalized reinforcement learning task  sections      shall shortly describe
cross entropy optimization method rule based policies  respectively  section   
details learning experiments provided  section   present results 
section   provides review related literature  nally  section   summarize
discuss approach emphasis implications rl problems 

   pac man reinforcement learning
video game pac man rst released       reached immense success 
considered one popular video games date  wikipedia        
player maneuvers pac man maze  see fig      pac man eats dots
maze  particular maze     dots   one worth    points  level
nished dots eaten  make things dicult  four
ghosts maze try catch pac man  succeed  pac man loses life 
initially  three lives  gets extra life reaching        points 
four power up items corners maze  called power dots  worth
   points   pac man eats power dot  ghosts turn blue short period    
seconds   slow try escape pac man  time  pac man
   maze original pac man game slightly dierent  description applies opensource pac man implementation courtillat         two versions equivalent terms
complexity entertainment value 

   

filearning play ms  pac man

figure    snapshot pac man game
able eat them  worth                    points  consecutively  point
values reset     time another power dot eaten  player would want
eat four ghosts per power dot  ghost eaten  remains hurry back center
maze ghost reborn  certain intervals  fruit appears near center
maze remains while  eating fruit worth     points 
investigations restricted learning optimal policy rst level 
maximum achievable score                                                    plus
    points time fruit eaten 
original version pac man  ghosts move complex deterministic route 
possible learn deterministic action sequence require observations 
many patterns found enthusiastic players  pac man s sequels 
notably ms  pac man  randomness added movement ghosts  way 
single optimal action sequence  observations necessary optimal decision
making  respects  game play mostly unchanged 
implementation  ghosts moved randomly     time straight towards
pac man remaining      ghosts may turn back  following koza        chapter
     emphasize presence randomness  shall refer implementation
ms  pac man clone 

    ms  pac man rl task
ms  pac man meets criteria reinforcement learning task  agent make
sequence decisions depend observations  environment stochastic
 because paths ghosts unpredictable   well dened reward function
 the score eating things   actions inuence rewards collected future 
   

fiszita   lrincz

full description state would include     whether dots eaten  one
bit dot one power dot       position direction ms  pac man 
    position direction four ghosts      whether ghosts blue  one bit
ghost   so  long remain blue  in range      seconds     
whether fruit present  time left appears disappears     number
lives left  size resulting state space astronomical  kind function
approximation feature extraction necessary rl 
action space much smaller  four basic actions  go north south east west  however  typical game consists multiple hundreds steps 
number possible combinations still enormous  indicates need temporally
extended actions 
moderate amount domain knowledge ms  pac man  one  quite
easy dene high level observations action modules potentially useful 
hand  constructing well performing policy seems much dicult  therefore 
provide mid level domain knowledge algorithm  use domain knowledge
preprocess state information dene action modules  hand 
role policy search reinforcement learning combine observations modules
rule based policies nd proper combination 

   cross entropy method
goal optimize rule based policies performing policy search space
legal rule based policies  search apply cross entropy method  cem  
recently published global optimization algorithm  rubinstein         aims nd
 approximate  solution global optimization tasks following form

x    arg max f  x  
x

f general objective function  e g   need assume continuity dierentiability   summarize mechanism method briey  see section    
overview applications  

    intuitive description
optimization algorithms maintain single candidate solution x t  time
step  cem maintains distribution possible solutions  distribution  solution
candidates drawn random  essentially random guessing  nice trick
turned highly eective optimization method 
      power random guessing

random guessing overly simple  optimization  method  draw many samples
xed distribution g   select best sample estimation optimum 
limit case innitely many samples  random guessing nds global optimum 
two notes here   i  shown wolpert macready        
general problems  uniform random guessing worse method   ii 
nonetheless  practical problems  uniform random guessing extremely inecient 
   

filearning play ms  pac man

thus  random guessing safe start with  one proceeds collection
experience  limited much possible 
eciency random guessing depends greatly distribution g
samples drawn  example  g sharply peaked around x   samples
may sucient get good estimate  case opposite  distribution
sharply peaked around x    x   tremendous number examples may needed get
good estimate global optimum  naturally  nding good distribution least
hard nding x  
      improving efficiency random guessing

drawing moderately many samples distribution g   may able give
acceptable approximation x   may still obtain better sampling distribution 
basic idea cem selects best samples  modies g
becomes peaked around them  consider example  x     vector g
bernoulli distribution coordinate  suppose drawn      samples
selected    best  see majority selected samples  ith coordinate
   cem shifts bernoulli distribution corresponding component towards   
afterwards  next set samples drawn already modied distribution 
idea seems plausible  majority best scoring samples ith coordinate
   structure tness landscape  may hope ith
coordinate x    follows  describe update rule cem
formal way sketch derivation 

    formal description cross entropy method
pick g family parameterized distributions  denoted g   describe
algorithm iteratively improves parameters distribution g  
let n number samples drawn  let samples x              x n  
drawn independently distribution g   r  set high valued samples 

l     x i    f  x i        n   
provides approximation level set

l     x   f  x    
let u uniform distribution level set l   large values   distribution peaked around x   would suitable random sampling  raises two
potential problems   i  large values l contain points  possibly none  
making accurate approximation impossible   ii  level set l usually member
parameterized distribution family 
rst problem easy avoid choosing lower values   however 
make compromise  setting low would inhibit large improvement steps 
compromise achieved follows  cem chooses ratio        adjusts l
set best n samples  corresponds setting    f  x n      provided
samples arranged decreasing order values  best n samples called
elite samples  practice  typically chosen range             
   

fiszita   lrincz

problem solved changing goal approximation  cem chooses
distribution g distribution family g approximates best empirical distribution l   best g found minimizing distance g uniform
distribution elite samples  measure distance cross entropy distance
 often called kullback leibler divergence   cross entropy distance two distributions
g h dened
z
g x 
dce  g  h    g x  log
dx
h x 
general form cross entropy method summarized table    known
mild regularity conditions  ce method converges probability    margolin 
       furthermore  suciently large population  global optimum found
high probability 
input  g
input  g  g
input  n
input 
input 
    
  n  
draw x i  distribution gt
compute    f  x i   
sort  values descending order
t      fn
et       x i    f  x i    t    
gt      arg mingg dce  g  uniform et     
end loop

 
 
 
 
 
 

parameterized distrib  family
initial distribution
population size
selection ratio
number iterations
cem iteration main loop

  draw n samples
  evaluate
  level set threshold
  get elite samples
  get nearest distrib  g

table    pseudo code general cross entropy method

    cross entropy method bernoulli distribution
many parameterized distribution families  parameters minimum cross entropy
member computed easily simple statistics elite samples  provide
formulae bernoulli distributions  needed policy learning procedure
detailed next section  derivations well list discrete continuous
distributions simple update rules found tutorial de boer  kroese 
mannor  rubinstein        
let domain optimization         m   component drawn
independent bernoulli distributions  i e   g   bernoullim   distribution g g parameterized m dimensional vector p    p            pm    using g sampling 
   

filearning play ms  pac man

component j sample x

   probability pj  
xj  
   probability   pj  
drawing n samples x              x n   xing threshold value   let e denote set
elite samples  i e  
e     x i    f  x i     
notation  distribution g   minimum ce distance uniform distribution elite set following parameters 

p      p             p m   
p
p
 i 
 i 
x i  e  xj     
x i  e  xj     
 
p
 
pj   
n
x i  e  

   

words  parameters g   simply component wise empirical probabilities
  s elite set  derivation rule  see tutorial de boer et al         
changing distribution parameters p p  coarse  cases 
applying step size parameter preferable  resulting algorithm summarized
table   
input  p     p              p  m  
input  n
input 
input 
    
  n  
draw x i  bernoullim  pt  
compute    f  x i   
sort  values descending order
t      fn
et       x i    f  x i    t    
p

 i 
p j   
x i  e  xj         n  
pt   j    p j        pt j
end loop

 
 
 
 
 

initial distribution parameters
population size
selection ratio
number iterations
cem iteration main loop

  draw n samples
  evaluate
 
 
 
 

level set threshold
get elite samples
get parameters nearest distrib 
update step size

table    pseudo code cross entropy method bernoulli distributions
need optimize functions                  k m k     
simplest case  distributions domain parameterized
k parameters 
pk
p    p              p  k           pm             pm k     pj k   k   pj k     j  this
special case multinomial distribution  
update rule parameters essentially eq    bernoulli case 
p
p
 i 
 i 
 i  e  xj   k 
x i  e  xj   k 
x
 
p
 
 
   
pj k   
n
x i  e  
   

fiszita   lrincz

note constraint

pk

 
k   pj k

    satised automatically j  

   rule based policies
basic formulation  rule sentence form  condition  holds 
 action   rule based policy set rules mechanism breaking ties  i e  
decide rule executed  multiple rules satised conditions 
rule based policies human readable  easy include domain knowledge 
able represent complex behaviors  reasons  often used many
areas articial intelligence  see section     short overview related literature  
order apply rule based policies ms  pac man  need specify four things 
    possible actions     possible conditions
constructed observations      make rules conditions actions 
    combine rules policies  answers described following
sections 

    action modules
dening action modules ms  pac man  listed modules easy
implement considered potentially useful  see table     way  kept human work minimum  still managed formalize part domain knowledge
problem  consequence  list action modules means optimal  actions could eective appropriate denition  others
may superuous  example  four dierent modules ghost avoidance 
fromghost escapes nearest ghost  without considering position
ghosts  tolowerghostdensity tries take account inuence multiple ghosts 
fromghostcenter moves geometrical center ghosts  thus  able avoid
surrounded trapped  but  hand  easily bump ghost
so  nally  toghostfreearea considers whole board search safe
location  agent avoid shepherded ghosts  modules
may strengths weaknesses  possibly combination needed
success  actions  potentially useful  listed
 for example  moving towards fruit  
note modules exclusive  example  escaping
ghosts  ms  pac man may prefer route dots eaten  may want
head towards power dot  without possibility concurrent actions  performance
ms  pac man agent may reduced considerably  which investigated experimental
section      
need mechanism conict resolution  dierent action modules may suggest dierent directions  assigning priorities modules  agent
switches action module  decides priority  decision 
learning decision part learning task  
   action priorities learnt indirectly  rule xed priority  action switched
rule  inherits priority  action switched dierent rules
dierent priorities  mechanism described detail section     

   

filearning play ms  pac man

table    list action modules used rule construction 
name

description

todot
topowerdot
frompowerdot

go towards nearest dot 
go towards nearest power dot 
go direction opposite nearest power
dot 
go towards nearest edible  blue  ghost 
go direction opposite nearest ghost 
go towards maximally safe junction 
four directions  safety nearest junction estimated direction  ms  pacman n steps away junction
nearest ghost k steps away  safety
value junction n k   negative value
means ms  pac man possibly cannot reach
junction 
go direction maximizes euclidean
distance geometrical center ghosts 
go current direction  choose
random available action  except turning back 
impossible 
go direction cumulative ghost
density decreases fastest  ghost denes
density cloud  with radius      linear decay   cumulative ghost density
calculated 
choose location board minimum ghost distance largest  head towards
shortest path 

toedghost
fromghost
tosafejunction

fromghostcenter
keepdirection
tolowerghostdensity

toghostfreearea

   

fiszita   lrincz

table    list observations used rule construction  distances denote length
shortest path  unless noted otherwise  distance particular object type  
object exists moment 
name

description

constant
nearestdot
nearestpowerdot
nearestghost
nearestedghost
maxjunctionsafety

constant   value 
distance nearest dot 
distance nearest power dot 
distance nearest ghost 
distance nearest edible  blue  ghost 
four directions  safety nearest
junction direction estimated  dened
description action tosafejunction 
observation returns value maximally safe junction 
euclidean distance geometrical center
ghosts 
euclidean distance geometrical center
uneaten dots 
ghost denes density cloud  with radius
     linear decay   returns value
cumulative ghost density 
travelling salesman distance ghosts 
length shortest route starts
ms  pac man reaches four ghosts  not
considering movement  

ghostcenterdist
dotcenterdist
ghostdensity
totaldisttoghosts

implemented following mechanism  decision agent concerns
action modules  agent either switch or  switch action module  is 
principle  agent able use subset action modules  instead selecting
single one time step  basically  module highest priority decides direction
ms  pac man  one equally ranked directions  lower priority
modules checked  direction cannot decided checking switched on modules
order decreasing priority  for example  module switched on  two directions
ranked equally switched on modules   random direction chosen 
ms  pac man make decisions time advances whole grid cell  the
mechanism ensures never stands still   according    game ticks approx     
seconds simulated game time 

    observations  conditions rules
similarly actions  easily dene list observations potentially useful
decision making  observations descriptions summarized table   
   

filearning play ms  pac man

modules could improved many ways  example  checking whether
enough time intercept edible ghosts calculating nearestedghost taking
consideration movement ghosts calculating nearestghost  nearestedghost
maxjunctionsafety  kept implementation modules simple possible 
designed reasonable modules  eort made make module denitions
optimal  complete non redundant 
necessary tools dening conditions rule  typical condition
true observations given range  note status action module
important proper decision making  example  agent may decide
ghost close  switches modules except escape module  therefore
allow conditions check whether action module  on   o  
sake simplicity  conditions restricted form  observation 
   value    observation     value    action     action    conjunction
terms  example 

 nearestdot     nearestghost     fromghost  
valid condition rules 
conditions actions  rules constructed easily  implementation  rule form  condition    action   example 

 nearestdot     nearestghost     fromghost  
fromghostcenter 
valid rule 

    constructing policies rules
decision lists standard forms constructing policies single rules 
approach pursue here  too  decision lists simply lists rules  together
mechanism decides order rules checked 
rule priority assigned  agent make decision  checks
rule list starting ones highest priority  conditions rule fullled 
corresponding action executed  decision making process halts 
note principle  priority rule dierent priority action
modules  however  sake simplicity  make distinction  rule priority
k switches action module  priority action module taken k  
intuitively  makes sense  important rule activated  eect
important  rule priority k switches module  executed  regardless
priority module 
may worth noting many possible alternatives ordering rules
actions 

rule could xed priority  part provided domain knowledge
 spronck  ponsen  sprinkhuizen kuyper    postma        
priority rule could free parameter learned cem
method 
   

fiszita   lrincz

instead absolute priorities  agent could learn relative ordering rules
 timuri  spronck    van den herik        
order rules could determined heuristic decision mechanism 
example  generality rule e g   rules few many conditions large small domains could taken account  heuristics used linear
classier systems  see e g  work bull   kovacs       
principle  one would nd interesting solutions using computer minimal
bias  domain knowledge   regard  eciency simple priority management method satisfactory  experiment priority heuristics 

    example
let us consider example shown table    rule based policy ms  pacman agent 
table    hand coded policy playing ms  pac man  bracketed numbers denote
priorities      highest priority 
   
   
   
   
   
   
   
   










nearestghost   fromghost 
nearestghost   junctionsafety   fromghostnearestedghost    toedghostnearestedghost    toedghost 
constant   keepdirection 
frompowerdot  topowerdot 
ghostdensity     nearestpowerdot   frompowerdot 
nearestpowerdot    frompowerdot 

rst two rules manage ghost avoidance  ghost close  agent
ee  gets safe distance  ghost avoidance priority
activities  next two rules regulate edible ghost
board  agent chase  the value nearestedghost innity       
edible ghosts     board  are   activity
relatively high priority  eating ghosts worth lots points  must done
blueness ghosts disappears  must done quickly  fth rule says
agent turn back  directions equally good  rule prevents
unnecessary zigzagging  while dots eaten   surprisingly eective 
remaining rules tweak management power dots  basically  agent prefers eat
power dot  however  blue ghosts board  power dot resets
score counter      bad move  furthermore  ghost density low around
agent  probably hard collect ghosts  preferable
wait eating power dot 
   

filearning play ms  pac man

    mechanism decision making
mechanism decision making depicted fig    short   hidden  state space
world ms  pac man ghosts  dynamics  hidden  statespace determines vector observations  checked conditions 
conditions rule satised  corresponding action module switched o 
consequence  multiple actions may eect once  example  decision depicted
fig    sets two actions work together 

figure    decision making mechanism ms  pac man agent  time step t 
agent receives actual observations state action modules 
checks rules script order  executes rst rule satised
conditions 
initially  action module switched o state  module switched
on  remains either explicitly switched another module
priority switched replaces it 

    learning rule based policies cem
apply cem searching space rule based policies  learning composed
three phases      generation random policies drawn according current parameter
set      evaluation policies  consists playing game ms  pac man
measure score      updating parameter set using cem update rules 
      drawing random scripts predefined rule base

suppose predened rule base containing k rules  for example  one listed
appendix a   policy rule slots  slot lled k rules 
   

fiszita   lrincz

left empty  result  policies could contain rules  possibly much less 
rule slot xed priority  too  set             priority rule slot
change learning  learning can  however  push important rule high priority
slot low priority one  vice versa 
  m  slot lled rule rule base probability pi  
left empty probability   pi   decided slot
pkbe lled 
particular rule j    j k   selected probability qi j   j   qi j    
slot             m   result  policies could contain rules  possibly much
less  pi values qi j values learnt simultaneously cross entropy
method  table     using update rules          respectively  gives total
  k parameters optimize  although eective number parameters much
less  qi j values unused slots irrelevant   initial probabilities set
pi       qi j     k  
      drawing random rules without predefined rule base

studied situations lessened domain knowledge  use predened rulebase  script generation kept same  rule base k rules generated
randomly  case generated dierent rule bases rule slots  low
ratio meaningful rules counteracted increased rule variety 
random rule random pair randomly drawn condition set randomly
drawn action  random condition sets contained   conditions  random action constructed follows  action module selected uniformly set modules listed
table    switched probability      construction random condition starts uniformly random selection module either table   table
   selected module action  condition  action    action  
equal probability  selected module observation  condition
 observation   value   observation   value  equal probability   value 
selected uniformly ve element set  values set determined separately observation module follows  played     games using xed policy
recorded histogram values observation  subsequently  ve element set
determined would split histogram regions equal area  example 
value set nearestghost                  
design random rule generation procedure contains arbitrary elements  e g 
number conditions rule  number values observation compared
to   intuition behind procedure generate rules suciently versatile 
ratio meaningless rules  e g  rules unsatisable conditions  large 
however  optimization form done point 

   description experiments
according assumptions  eectiveness described architecture based
three pillars      human domain knowledge provided modules rules     
   according preliminary experiments  quality learned policy improve increasing
priority set number slots 

   

filearning play ms  pac man

eectiveness optimization algorithm      possibility concurrent actions 
below  describe set experiments designed test assumptions 

    full architecture
rst experiment  random rules used  construction  use modules
dened sections          second experiment  rules generated randomly 
hand coded  case  role learning determine rules
used 
      learning random rule construction

rst experiment  rule base generated randomly  described section       
number rule slots xed        priorities distributed evenly   one
containing k       randomly generated rules  values k selected
coarse search parameter space 
parameters cem follows  population size n         selection ratio
        step size         values fairly standard cem 
tried varying them  step  probabilities using rule slot  that
is  values pi   qi j   slightly decreased  using decay rate        
larger decay rate  useful rules annulled often  hand  smaller
decay aect performance  many superuous rules left policies 
score given policy huge variance due random factors game 
therefore  obtain reliable tness estimations  score policy averaged
  subsequent games  learning lasted    episodes  sucient tune
probability close either      performed    parallel training runs  experiment
type denoted ce randomrb 
      learning hand coded rules

second experiment constructed rule base k      hand coded rules  shown
appendix a  thought potentially useful  could placed one
     rule slots   parameters experiment identical previous
one  experiment type denoted ce fixedrb 

    eect learning algorithm
following experiment  compared performance cem simple stochastic
gradient optimization  single comparison sucient measure eciency
cem  serves provide point reference  comparison relevant 
algorithms similar complexity move gradually towards
best samples found far  dierence sg maintains single solution
   note per episode learning rate  would correspond per instance learning rate
       n           on line learning algorithm 
   contrast previous experiment  rules meaningful potentially useful  therefore
need large pool rules  much lower used  found algorithm
fairly insensitive choice m  signicant changes performance observed parameter
modied factor   

   

fiszita   lrincz

candidate time  whereas cem maintains distribution solutions  thus  cem
maintains memory solutions becomes less fragile occasional wrong parameter
changes 
particular form stochastic gradient search following  initial policy
drawn random  consisting   rules   that  generated     random mutation
current solution candidate step  evaluated obtained policies  bestperforming mutation chosen next solution candidate  mutations generated
using following procedure      rule  condition changed random new
condition probability           rule  action changed random new
action probability       listed parameter values  number rules policy  number
mutated policies  probabilities mutation  results coarse parameter space
optimization 
number episodes set      way  evaluated number
dierent policies          cem experiments  random rule base
xed rule base experiments repeated using stochastic gradient method  executing
   parallel training runs  resulting policies denoted sg randomrb sgfixedrb  respectively 

    eect parallel actions
according assumptions  possibility parallel actions plays crucial role
success architecture  conrm assumption  repeated previous experiments
concurrent actions disabled  agent switches action module 
action modules switched automatically  experiment types denoted
ce randomrb  action  ce fixedrb  action  sg randomrb  action sgfixedrb  action 

    baseline experiments
order isolate assess contribution learning  performed two additional
experiments dierent amounts domain knowledge learning  furthermore 
asked human subjects play game 
      random policies

rst non learning experiment  used rule base    hand coded rules  identical
rule base ce fixedrb   ten rules selected random  random priorities
assigned them  measured performance policies constructed way 
      hand coded policy

second non learning experiment  hand coded rules priorities 
is  hand coded full policy  policy shown table    constructed
trial and error  naturally  policy constructed knowing results
learning experiments 
   

filearning play ms  pac man

table    ms  pac man results  see text details  abbreviations  ce  learning
cross entropy method  sg  learning stochastic gradient  randomrb 
randomly generated rule base  fixedrb  xed  hand coded rule base   action 
one action module work time 
method

avg  score

         percentiles 

    
    
    

           
           
           
           

ce randomrb  action
ce fixedrb  action
sg randomrb  action
sg fixedrb  action

    
    
    
    

           
            
           
           

random policy
hand coded policy
human play

   
    
    

         
           
            

ce randomrb
ce fixedrb
sg randomrb
sg fixedrb

    

      human play

nal experiment  human subjects asked play rst level ms  pacman measured performance  subjects played pac man and or
similar games before  none experienced player 

   experimental results
human experiments performed rst level open source pac man clone
courtillat         experiments applied delphi re implementation
code 
learning experiments     parallel learning runs executed  one   
episodes  training period sucient tune probabilities close either  
   learned policy could determined unambiguously cases  obtained
policy tested playing    consecutive games  giving total     test games per
experiment  non learning experiments agents played     test games  too  using
random policies hand coded policy  respectively  human subject played   
games  giving total     test games  results summarized table    provide
        percentile values instead variances  distribution scores
highly non gaussian 
   fact average smaller     percentile caused highly skewed distribution
scores  games  agent reached score range           except games
extremely low score  games aect     percentile lowered average
signicantly 

   

fiszita   lrincz

   
   
   
   
   
   








nearestghost   fromghost 
maxjunctionsafety   fromghostnearestedghost    topowerdot 
nearestedghost    toedghost 
ghostdensity     nearestpowerdot   frompowerdot 
constant   tocenterofdots 

figure    best policy learned ce fixedrb  average score    games       
   
   
   
   
   
   
   









maxjunctionsafety     tolowerghostdensity  fromghostnearestghost   maxjunctionsafety   fromghost 
nearestghost   fromghostcenter  toedghost 
toedghost  centerofdots    toedghost 
toedghost  nearestedghost    toedghost 
nearestdot   ghostcenterdist   keepdirection 
toghostfreearea  todot  topowerdot 

figure    best policy learned ce randomrb  average score    games       
note presence always true  and thus  superuous  conditions
tolowerghostdensity   fromghostcenter   toghostfreearea  todot  

fig    shows best individual policy learned ce fixedrb  reaching      points
average  ghost avoidance given highest priority  turned ghost
close  otherwise ms  pac man concentrates eating power dots subsequently
eating blue ghosts  takes care eat power dot blue
ghosts board  otherwise would miss opportunity eat      point
ghost  and possibly several others  too   lowest priority setting  agent looks
ordinary dots  although rule eect previous rules decide
direction  for example  endgame power dots left ghosts
original form  
policies learnt ce randomrb behave similarly ones learnt ce fixedrb 
although behavior somewhat obscured superuous conditions and or rules 
demonstrated clearly example policy shown fig     noise generated
random rules  algorithm often fails learn correct priorities various
activities 
eect enabling disabling concurrent actions signicant  instructive
take look best policy learned ce fixedrb  action shown fig    
agent concentrate eating ghosts  major source reward  however 
cannot use modules necessary ghost avoidance long term survival 
results show cem performs signicantly better stochastic gradient
learning  believe  however  dierence could lowered thorough search
parameter space  sg many global optimization methods evolutionary
methods simulated annealing could reach similar performances cem  according
de boer et al         applications cited section      advantage cem
   

filearning play ms  pac man

    nearestedghost    topowerdot 
    nearestedghost    toedghost 

figure    best policy learned ce fixedrb  action  average score    games 
     

maintains distribution solutions reach robust performance
little eort  requiring little tuning parameters  canonical set
parameters                     population large possible 
performance method robust  claim coincides experiences
parameter optimization process 
finally  interesting analyze dierences tactics human
computer players  one fundamental tactic human players try lure
ghosts close ms  pac man ghosts close other  way 
eaten fast turn blue  behavior evolved
experiments  besides  tactics cem chance discover 
lacking appropriate sensors  example  human player  and does  calculate
time remaining blue period  approximate future position ghosts 
on 

   related literature
section  review literature learning pac man game  various components learning architecture  cross entropy method  rule based policies 
concurrent actions 

    previous work  ms   pac man
variants pac man used previously several studies  direct comparison
performances possible cases  however  simplied versions
game used studies 
koza        uses ms  pac man example application genetic programming 
uses dierent score value fruit  worth      points instead     points used
here   shape board  and consequently  number dots  dierent 
therefore scores cannot directly compared  however  koza reports  on p      
pac man could scored additional      points captured four monsters
four occasions turned blue  score  one reported 
translates approximately      points scoring system 
lucas        uses full scale ms  pac man game test problem  trains
neural network position evaluator hand crafted input features  purposes
training  uses evolutionary strategy approach  obtained controller able
reach          points  averaged     games 
bonet stauer        restrict observations       window centered ms  pacman  uses neural network temporal dierence learning learn reactive con   

fiszita   lrincz

troller  series increasingly dicult learning tasks  able teach basic
pellet collecting ghost avoidance behaviors greatly simplied versions game 
used simple mazes containing power dots one ghost 
gallagher ryan        denes behavior agent parameterized nite
state automata  parameters learnt population based incremental learning 
evolutionary method similar cem  run simplied version pac man 
single ghost power dots  takes away complexity game 
tiong        codes rule based policies pac man hand  uses learning
improve them  tests  similarly ours  based pac man implementation
courtillat         limits number ghosts    best performing rule set
reaches      points average maximal       however  results likely
scale well increasing number ghosts  ghost eaten     times
average  out possible   times per game   

    cross entropy method
cross entropy method rubinstein        general algorithm global optimization
tasks  bearing close resemblance estimation of distribution evolutionary methods  see e g 
paper muehlenbein         areas successful application range combinatorial optimization problems optimal buer allocation problem  allon  kroese  raviv 
  rubinstein         dna sequence alignment  keith   kroese        independent process analysis  szab  pczos    lrincz        
cross entropy method several successful reinforcement learning applications  too 
dambreville        uses cem learning input output hierarchical hmm controls
predator agent partially observable grid world  menache  mannor  shimkin       
use radial basis function approximation value function continuous maze navigation task  use cem adapt parameters basis functions  nally  mannor 
rubinstein  gat        apply cem policy search simple grid world maze navigation problem  recently  cross entropy method applied successfully
game tetris szita lrincz        

    rule based policies
representation policies rule sequences widespread technique complex problems computer games  example  many pac man related papers listed
use rule based representation 
learning classier systems  holland        genetic algorithm based methods
evolve suitable rules given task  bull        gives excellent general overview
pointers references  hayek machine baum        similar architecture 
agents  corresponding simple rules  dene economical system  make bids
executing tasks hope obtain rewards  schaul        applies
architecture sokoban game 
dynamic scripting  spronck et al         another prominent example using
learning rule based policies  uses hand coded rule base reinforcement learning   results cited section     

   

filearning play ms  pac man

principle determine rules included policy  dynamic scripting
successful applications state of the art computer games role playing game
neverwinter nights  spronck et al         real time strategy game wargus  ponsen
  spronck        

    concurrent actions
traditional formalizations rl tasks  agent select execute single action
time  work known us handles concurrent actions explicitly rohanimanesh mahadevan         formalize rl tasks concurrent actions
framework semi markov decision processes present simple grid world demonstrations 

   summary closing remarks
article proposed method learns play ms  pac man  dened
set high level observation action modules following properties   i  actions
temporally extended   ii  actions exclusive  may work concurrently 
method uncover action combinations together priorities  thus  agent
pursue multiple goals parallel 
decision agent concerns whether action module turned  if
o   if on   furthermore  decisions depend current observations
may depend state action modules  policy agent represented
list if then rules priorities  policies easy interpret analyze 
easy incorporate additional human knowledge  cross entropy method used
learning policies play well  learning biased towards low complexity policies 
consequence policy representation applied cross entropy learning
method  cem  higher complexity solutions harder discover special means
used counteract premature convergence  solutions higher complexities 
noise injection suggested previous work  szita   lrincz         learned
low complexity policies reached better score hand coded policy average human
players 
applied architecture potentials handle large  structured observation 
action spaces  partial observability  temporally extended concurrent actions  despite
versatility  policy search eective  biased towards low complexity
policies  properties attractive point view large scale applications 

    role domain knowledge
demonstrating abilities rl algorithm  desirable learning starts
scratch  contribution learning clearly measurable  however  choices
test problems often misleading  many  abstract  domains contain considerable amount
domain knowledge implicitly  example  consider grid world navigation tasks 
often used class problems tabula rasa learning 
simple version grid world navigation task  state integer uniquely
identies position agent  atomic actions moves grid cells north south east west actual cell  importantly  unique identication
   

fiszita   lrincz

position means moves agent change direction agent
task laboratory coordinate framework  sometimes called allocentric coordinates 
egocentric coordinates  concepts north  south  etc  correspond
high level abstraction  meaning humans only  must considered
part domain knowledge  domain knowledge provided us similar grid
world sense provide high level observations allocentric form 
 distance nearest ghost d   ms  pac man position           similarly  action  go
north  action  go towards nearest power dot  essentially level 
implicit presence high level concepts becomes even apparent move
abstract mdps  real world   consider robotic implementation maze
task  full state information  i e  state well state environment
available robot  sees local features may see local features
time  obtain exact position  move one unit s length prescribed direction 
robot integrate information movement sensors  optical radar sensors etc 
information fusion  although necessary  topic reinforcement learning  thus 
task  great amount domain knowledge needs provided
ce based policy search method could applied 
opinion  role human knowledge selects set observations
actions suit learning algorithm  extra knowledge typically necessary
applications  nonetheless  numerous  more or less successful  approaches exist obtaining
domain knowledge automatically  according one approach  set observations
chosen rich  and redundant  set observations feature selection method 
cross entropy method seems promising here   see paper szita       
application feature selection brain fmri data      pittsburgh brain activity
interpretation competition   according dierent approach  successful combinations
lower level rules joined higher level concepts rules  machine learning
powerful tools here  e g  arithmetic coding data compression  witten  neal    cleary 
       applied many areas  including writing tool dasher developed ward
mackay         extensions included framework reinforcement
learning 

    low complexity policies
space legal policies huge  potentially innite   interesting question
search eective huge space  direct search formidable  think
implicit bias towards low complexity policies useful studied here 
low complexity policy  mean following  policy may consist many
rules  cases  applied concurrently  unused rules
get rewarded get punished unless limit useful rule  eective length
policies biased towards short policies  implicit bias strengthened explicit
one work  absence explicit reinforcement  probability applying rule
decays  indierent rules get wiped quickly  seems promising use frequent low
complexity rule combinations building blocks continued search powerful
still low complexity policies 
   

filearning play ms  pac man

bias towards short policies reduces eective search space considerably  moreover  many real life problems  low complexity solutions exist  for excellent analysis
possible reasons  see paper schmidhuber         therefore  search concentrated
relevant part policy space  pays less attention complex policies  which
therefore less likely according occam s razor arguments  

acknowledgments
please send correspondence andrs lrincz  authors would thank anonymous reviewers detailed comments suggestions improving presentation
paper  material based upon work supported partially european oce
aerospace research development  air force oce scientic research  air force
research laboratory  contract no  fa         research supported
ec fet grant   new ties project  contract         opinions  ndings
conclusions recommendations expressed material authors
necessarily reect views european oce aerospace research development  air force oce scientic research  air force research laboratory  ec 
members ec new ties project 

appendix a  hand coded rule base
list rules hand coded rule base used experiments 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  



























constant   todot 
constant   tocenterofdots 
nearestghost   fromghost 
nearestghost   fromghost 
nearestghost   fromghost 
nearestghost   fromghostnearestghost   fromghostnearestghost   fromghostconstant   tosafejunction 
maxjunctionsafety   tosafejunction 
maxjunctionsafety   tosafejunction 
maxjunctionsafety   tosafejunctionmaxjunctionsafety   fromghostmaxjunctionsafety   tosafejunctionmaxjunctionsafety   fromghostconstant   keepdirection 
constant   toedghost 
nearestghost   topowerdot 
nearestedghost    topowerdotnearestedghost    nearestpowerdot   frompowerdot 
nearestedghost    frompowerdot 
nearestedghost    frompowerdotnearestedghost    topowerdot 
ghostdensity   tolowerghostdensity 
ghostdensity     tolowerghostdensity   

fiszita   lrincz

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  



















nearestpowerdot   nearestghost   topowerdot 
nearestghost   maxjunctionsafety   fromghostghostdensity     nearestpowerdot   frompowerdot 
nearestpowerdot    frompowerdottotaldisttoghosts    frompowerdot 
maxjunctionsafety   fromghost 
maxjunctionsafety   fromghost 
maxjunctionsafety   fromghost 
maxjunctionsafety   fromghost 
constant   fromghostcenter 
nearestghost   fromghost 
nearestghost   maxjunctionsafety   fromghostnearestedghost    toedghostnearestedghost    toedghost 
frompowerdot  topowerdot 
ghostdensity     nearestpowerdot   frompowerdot 
nearestpowerdot    frompowerdot 

references
allon  g   kroese  d  p   raviv  t     rubinstein  r  y          application crossentropy method buer allocation problem simulation based environment 
annals operations research              
baum  e  b          toward model mind laissez faire economy idiots 
proceedings   rd international conference machine learning  pp       
baxter  j   tridgell  a     weaver  l          machines learn play games  chap 
reinforcement learning chess  pp         nova science publishers  inc 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientic 
bonet  j  s  d     stauer  c  p          learning play pac man using incremental
reinforcement learning    online  accessed    october       
bull  l          applications learning classier systems  chap  learning classier systems  brief introduction  pp       springer 
bull  l     kovacs  t          foundations learning classier systems  chap  foundations
learning classier systems  introduction  pp       springer 
courtillat  p          non sens pacman     c sourcecode    online  accessed   
october       
dambreville  f          cross entropic learning machine decision partially
observable universe  journal global optimization  appear 
de boer  p  t   kroese  d  p   mannor  s     rubinstein  r  y          tutorial
cross entropy method  annals operations research            
   

filearning play ms  pac man

gallagher  m     ryan  a          learning play pac man  evolutionary  rule based
approach  et  al   r  s   ed    proc  congress evolutionary computation  pp 
         
holland  j  h          escaping brittleness  possibilities general purpose learning
algorithms applied parallel rule based systems  mitchell  michalski    carbonell
 eds    machine learning  articial intelligence approach  volume ii  chap     
pp          morgan kaufmann 
keith  j     kroese  d  p          sequence alignment rare event simulation  proceedings      winter simulation conference  pp         
koza  j          genetic programming  programming computers means
natural selection  mit press 
lucas  s  m          evolving neural network location evaluator play ms  pac man 
ieee symposium computational intelligence games  pp         
mannor  s   rubinstein  r  y     gat  y          cross entropy method fast policy
search    th international conference machine learning 
margolin  l          convergence cross entropy method  annals operations
research              
menache  i   mannor  s     shimkin  n          basis function adaptation temporal
dierence reinforcement learning  annals operations research                  
muehlenbein  h          equation response selection use prediction 
evolutionary computation            
ponsen  m     spronck  p          improving adaptive game ai evolutionary learning 
computer games  articial intelligence  design education 
rohanimanesh  k     mahadevan  s          decision theoretic planning concurrent
temporally extended actions  proceedings   th conference uncerainty
articial intelligence  pp         
rubinstein  r  y          cross entropy method combinatorial continuous
optimization  methodology computing applied probability            
samuel  a  l          studies machine learning using game checkers  ibm
journal research development            
schaul  t          evolving compact concept based sokoban solver  master s thesis  cole
polytechnique fdrale de lausanne 
schmidhuber  j          computer scientist s view life  universe  everything 
freksa  c   jantzen  m     valk  r   eds    foundations computer science 
potential   theory   cognition  vol       lecture notes computer science  pp 
        springer  berlin 
   

fiszita   lrincz

spronck  p   ponsen  m   sprinkhuizen kuyper  i     postma  e          adaptive game ai
dynamic scripting  machine learning                 
spronck  p   sprinkhuizen kuyper  i     postma  e          online adaptation computer
game opponent ai  proceedings   th belgium netherlands conference
articial intelligence  pp         
sutton  r  s     barto  a  g          reinforcement learning  introduction  mit press 
cambridge 
szab  z   pczos  b     lrincz  a          cross entropy optimization independent
process analysis  ica  pp         
szita  i          select     voxels best prediction simplistic
approach  tech  rep   etvs lornd university  hungary 
szita  i     lrincz  a          learning tetris using noisy cross entropy method  neural
computation                    
tesauro  g          td gammon  self teaching backgammon program  achieves masterlevel play  neural computation                
timuri  t   spronck  p     van den herik  j          automatic rule ordering dynamic
scripting  third articial intelligence interactive digital entertainment
conference  pp       
tiong  a  l  k          rule set representation tness functions articial pac man
playing agent  bachelor s thesis  department information technology electrical
engineering 
ward  d  j     mackay  d  j  c          fast hands free writing gaze direction  nature 
            
wikipedia         pac man wikipedia  free encyclopedia  wikipedia   online 
accessed    may       
witten  i  a   neal  r  m     cleary  j  g          arithmetic coding data compression 
communications acm             
wolpert  d  h     macready  w  g          free lunch theorems optimization  ieee
transactions evolutionary computation          

   


