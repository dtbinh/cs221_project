journal of artificial intelligence research               

submitted        published     

learning semantic definitions of online information sources
mark james carman
craig a  knoblock

mark bradipo net
knoblock isi edu

university of southern california
information sciences institute
     admiralty way
marina del rey  ca      

abstract
the internet contains a very large number of information sources providing many types
of data from weather forecasts to travel deals and financial information  these sources can
be accessed via web forms  web services  rss feeds and so on  in order to make automated
use of these sources  we need to model them semantically  but writing semantic descriptions
for web services is both tedious and error prone  in this paper we investigate the problem
of automatically generating such models  we introduce a framework for learning datalog
definitions of web sources  in order to learn these definitions  our system actively invokes
the sources and compares the data they produce with that of known sources of information 
it then performs an inductive logic search through the space of plausible source definitions
in order to learn the best possible semantic model for each new source  in this paper we
perform an empirical evaluation of the system using real world web sources  the evaluation
demonstrates the effectiveness of the approach  showing that we can automatically learn
complex models for real sources in reasonable time  we also compare our system with
a complex schema matching system  showing that our approach can handle the kinds of
problems tackled by the latter 

   introduction
recent years have seen an explosion in the quantity and variety of information available
online  one can find shopping data  prices and availability of goods   geospatial data
 weather forecasts  housing information   travel data  flight pricing and status   financial
data  exchange rates and stock quotes   and that just scratches the surface of what is
available  the aim of this work is to make use of that vast store of information 
as the amount of information has increased  so too has its reuse across web portals
and applications  developers have realised the importance of managing content separately
from presentation  leading to the development of xml as a self describing data format 
content in xml is far easier to manipulate than html  simplifying integration across
different sources  standards have also emerged for providing programmatic access to data
 like soap and rest  so that developers can easily build programs  called mash ups  that
combine content from different sites in real time  many portals now provide such access to
their data and some even provide syntactic definitions  in wsdl  of the input and output
these data sources expect  missing  however  are semantic descriptions of what each source
does  which is required in order to support automated data integration 
c
    
ai access foundation  all rights reserved 

ficarman   knoblock

    structured querying
given all the structured sources available  we would like to combine the data dynamically to
answer specific user requests  as opposed to statically in the case of mash ups   dynamic
data requests can be expressed as queries such as those shown below  such queries may
require access to multiple  publicly available  data sources and combine information in ways
that were not envisaged by its producers 
   tourism  get prices and availability for all three star hotels within     kilometers of
trento  italy that lie within   kilometer of a ski resort that has over   meter of snow 
   transportation  determine the time that i need to leave work in order to catch a bus
to the airport to pick up my brother who is arriving on qantas flight     
   disaster prevention  find phone numbers for all people living within   mile of the
coast and below     feet of elevation 
it is clear even from this small set of examples just how powerful the ability to combine
data from disparate sources can be  in order to give these queries to an automated system 
we must first express them formally in a query language such as sql or datalog  ullman 
       in datalog the first query might look as follows 
q hotel  price   accommodation hotel      address   available hotel  today  price  
distance address  htrento italyi  dist    dist       km 
skiresort resort  loc    distance address  loc   dist   
dist     km  snowcondiditions resort  today  height   height    m 
the expression states that hotel and price pairs are generated by looking up three star hotels
in a relational table called accommodation  then checking the price for tomorrow night in a
table called available  the address of each hotel is used to calculate the distance to trento 
which must be less than     kilometers  the query also checks that there is a skiresort
within   kilometer of the hotel  and that the snowconditions for today show more than  
meter of snow 
    mediators
a system capable of generating a plan to answer such a query is called an information
mediator  wiederhold         in order to generate a plan  mediators look for sources that
are relevant to the query  in this case  relevant sources might be 
   the italian tourism website  to find all hotels near trento  italy 
   a ski search engine  to find ski resorts near each hotel 
   a weather provider  to find out how much snow has fallen at each ski resort 
for a mediator to know which sources are relevant  it needs to know what information each
source provides  while xml defines the syntax  formatting  used by a source  the semantics
 intended meaning  of the information the source provides must be defined separately 
this can be done using local as view  lav  source definitions in datalog  levy        
essentially  source definitions describe queries that if given to a mediator  will return the
same data as the source provides  example definitions are shown below  the first states
 

filearning semantic definitions of information sources on the internet

that the source hotelsearch takes four values as input  inputs are prefixed by the   symbol  
and returns a list of hotels which lie within the given distance of the input location  for
each hotel it also returns the address as well as the price for a room on the given date 
 note that the source only provides information for hotels in italy  
hotelsearch  location   distance   rating   date  hotel  address  price   country location  italy   accommodation hotel  rating  address  
available hotel  date  price   distance address  location  dist   
dist    distance 
findskiresorts  address   distance  resort  location   skiresort resort  location   distance address  location  dist   
dist    distance 
getskiconditions  resort   date  height   snowcondiditions resort  date  height  
in order to generate a plan for answering the query  a mediator performs a process called
query reformulation  levy         whereby it transforms the query into a new query over
 in terms of  the relevant information sources    a source is relevant if it refers to the same
relations as the query   the resulting plan in this case is shown below 
q hotel  price   hotelsearch htrento italyi     km      today  hotel  address  price  
findskiresorts address   km  resort  location  
getskiconditions resort  today  height   height    m 
in this work  the questions of interest are  where do all the definitions for these information
sources come from and more precisely  what happens when we want to add new sources to
the system  is it possible to generate these source definitions automatically 
    discovering new sources
in the example above  the mediator knows of a set of relevant sources to use to successfully
answer the query  if instead  one of those sources is missing or doesnt have the desired
scope  e g  getskiconditions doesnt provide data for trento   then the mediator first needs
to discover a source providing that information  as the number and variety of information
sources increase  we will undoubtedly rely on automated methods for discovering them and
annotating them with semantic descriptions  in order to discover relevant sources  a system
might inspect a service registry   such as those defined in uddi   or perform keywordbased search over a web index  such as google or del icio us   the research community has
looked at the problem of discovering relevant services  developing techniques for classifying
services into different domains  such as weather and flights  using service metadata  he  
kushmerick        and clustering similar services together to improve keyword based search
 dong  halevy  madhavan  nemes    zhang         these techniques  although useful  are
not sufficient for automating service integration 
   the complexity of query reformulation is known to be exponential  although efficient algorithms for
performing it do exist  pottinger   halevy        
   note that technically  a service is different from a source  a service is an interface providing access to
multiple operations  each of which may provide information  if an operation does not affect the state
of the world  e g  by charging somebodys credit card   then we call it an information source  for this
paper  however  we use the term service to refer only to information sources 

 

ficarman   knoblock

    labeling service inputs and outputs
once a relevant service is discovered  the problem shifts to modeling it semantically  modeling sources by hand can be laborious  so automating the process makes sense  since different
services often provide similar or overlapping data  it should be possible to use knowledge of
previously modeled services to learn descriptions for new ones 
the first step in modeling a source is to determine what type of data it requires as
input and produces as output  this is done by assigning semantic types  like zipcode 
telephone number  temperature  and so on  to the attributes of a service  semantic types
restrict the possible values of an attribute to a subset of the corresponding primitive type 
the research community has investigated automating the assignment process by viewing it
as a classification problem  he   kushmerick         in their system  he and kushmerick
trained a support vector machine  svm  on metadata describing different sources  the
system  given a source such as the following 
getweather  zip  temp 
uses the labels getweather  zip and temp  and any other available metadata  to assign
types to the input and output attributes  e g   zip zipcode  temp temperature  note that
additional metadata is often useful for distinguishing between possible assignments   if  for
example  the name of the operation had been listemployees  then temp may have referred
to a temporary employee rather than a temperature  
in subsequent work  researchers developed a more comprehensive system that used both
metadata and output data to classify service attributes  lerman  plangprasopchok   
knoblock         in that system  a logistic regression based classifier first assigns semantic types to input parameters  examples of those input types are then used to invoke
the service  and the output is given to a pattern language based classifier  that assigns types
to the output parameters  the authors argue that classification based on both data and
metadata is far more accurate than that based on metadata alone  using an example  it is
easy to see why  consider the following tuples produced by our getweather source 
h          ci  h          ci  h          ci     
given the data  the classifier can be certain that temp really does refers to a temperature 
and indeed can even assign it to a more specific type  temperaturec  in celsius  
while the problem of determining the semantic types of a services attributes is very
interesting  and there is room for improvement on current techniques  we assume for the
purposes of this work that it has already been solved 
    generating a definition
once we know the parameter types  we can invoke the service  but we are still unable to
make use of the data it returns  to do that  we need also to know how the output attributes
relate to the input  i e  a definition for the source   for example  for the getweather service
we need to know whether the temperature being returned is the current temperature  the
predicted high temperature for tomorrow or the average temperature for this time of year 
such relationships can be described by the following definitions 
getweather  zip  temp     currenttemp zip  temp  
getweather  zip  temp     forecast zip  tomorrow  temp  
getweather  zip  temp     averagetemp zip  today  temp  
 

filearning semantic definitions of information sources on the internet

the relations used in these definitions would be defined in a domain ontology  or schema  
in this paper we describe a system capable of learning which if any of these definitions
is the correct  the system leverages what it knows about the domain  i e  the domain
ontology and a set of known information sources  to learn what it does not know  namely
the relationship between the attributes of a newly discovered source 
    outline
this paper presents a comprehensive treatment of methods for learning semantic descriptions of web information sources  it extends our previous work on the subject  carman  
knoblock        by presenting detailed descriptions of the methods for both enumerating
the search space and evaluating the individual candidate definitions  we provide additional
details regarding the evaluation methodology and the results generated 
the paper is structured as follows  we start with an example to motivate the source
induction problem and then formulate the problem concisely  we discuss our approach
to learning definitions for sources in terms of other known sources of information  section
    we give details of the search procedure for generating candidate definitions  section   
and of the evaluation procedure for scoring candidates during search  section     we then
describe extensions to the basic algorithm  section    before discussing the evaluation setup
and the experiments  section     which demonstrate the capabilities of our system  finally 
we contrast our approach with prior work 

   problem
we now describe in detail the problem of learning definitions for newly discovered services 
we start with a concrete example of what is meant by learning a source definition  in the
example there are four types of data  semantic types   namely  zipcodes  distances  latitudes
and longitudes  there are also three known sources of information  each of the sources
has a definition in datalog as shown below  the first service  aptly named source   takes
in a zipcode and returns the latitude and longitude coordinates of its centroid  the second
service calculates the great circle distance  the shortest distance over the earths surface 
between two pairs of coordinates  and the third converts a distance from kilometres into
miles by multiplying the input by the constant       
source   zip  lat  long     centroid zip  lat  long  
source   lat    long    lat    long   dist   greatcircledist lat   long   lat   long   dist  
source   dist   dist      multiply dist          dist   
the goal in this example is to learn a definition for a newly discovered service  called source  
this service takes two zipcodes as input and returns a distance value as output 
source   zip    zip   dist 
the system we will describe uses this type signature  input and output type information 
to search for an appropriate definition for the source  the definition discovered in this case
might be the following conjunction of calls to the individual sources 
source   zip    zip   dist  source   zip   lat   long    source   zip   lat   long   
source   lat    long    lat    long   dist    source   dist   dist  
 

ficarman   knoblock

the definition states that sources output distance can be calculated from the input zipcodes 
by giving those zipcodes to source   taking the resulting coordinates and calculating the
distance between them using source   and then converting that distance into miles using
source   to test whether this definition is correct  the system must invoke both the new
source and the definition to see if the values generated agree with each other  the following
table shows such a test 
 zip 
     
     
     

 zip 
     
     
     

dist  actual 
      
      
      

dist  predicted 
      
      
      

in the table  the input zipcodes have been selected randomly from a set of examples  and
the output from the source and the definition are shown side by side  since the output
values are quite similar  once the system has seen a sufficient number of examples  it can
be confident that it has found the correct semantic definition for the source 
the definition above is given in terms of the source relations  but could also have been
written in terms of the domain relations  the relations used to define sources   to     to
convert the definition into that form  one simply needs to replace each source relation by
its definition as follows 
source   zip    zip   dist  centroid zip   lat   long    centroid zip   lat   long   
greatcircledist lat   long   lat   long   dist    multiply dist          dist   
written in this way  the new semantic definition makes sense at an intuitive level  the source
is simply calculating the distance in miles between the centroids of the two zipcodes 
    problem formulation
having given an example of the source definition induction problem  we now describe the
problem more formally  but before doing so  we introduce some concepts and notation   we
note that our focus in this paper is on learning definitions for information providing  as
opposed to world altering services  
 the domain of a semantic data type t  denoted d t   is the  possibly infinite  set of
constant values  c    c          which constitute the set of values for variables of that
type  for example d zipcode                       
 an attribute is a pair hlabel  semantic data typei  e g  hzip   zipcodei  the type of
attribute a is denoted type a  and the corresponding domain d type a   is abbreviated
to d a  
 a scheme is an ordered  finite  set of attributes ha         an i with unique labels  where
n is referred to as the arity of the scheme  an example scheme might be hzip   
zipcode  zip    zipcode  dist   distancei  the domain of a scheme a  denoted
d a   is the cartesian product of the domains of the attributes in the scheme  d a   
     d an     ai  a 
 a tuple over a scheme a is an element from the set d a   a tuple can be represented
by a set of name value pairs  such as  zip           zip           dist        
 

filearning semantic definitions of information sources on the internet

 a relation is a named scheme  such as airdistance zip   zip   dist   multiple relations may share the same scheme 
 an extension of a relation r  denoted e r   is a subset of the tuples in d r   for example  e airdistance  might be a table containing the distance between all zipcodes in
california   note that the extension of a relation may only contain distinct tuples  
 a database instance over a set of relations r  denoted i r   is a set of extensions
 e r          e rn     one for each relation r  r 
 a query language l is a formal language for constructing queries over a set of relations 
we denote the set of all queries that can be written using the language l over the set
of relations r returning tuples conforming to a scheme a as lr a  
 the result set produced by the execution of a query q  lr a on a database instance
i r  is denoted ei  q  
 a source is a relation s  with a binding pattern s  s  which distinguishes input
attributes from output attributes   the output attributes of a source are denoted by
the complement of the binding pattern    sc  s s   
 a view definition for a source s is a query vs written in some query language lr s  
the source definition induction problem is defined as a tuple 
ht  r  l  s  v  s i
where t is a set of semantic data types  r is a set of relations  l is a query language  s is a
set of known sources  v is a set of view definitions  one for each known source   and s is
the new source  also referred to as the target  
each semantic type t  t must be provided with a set of examples values et  d t  
 we do not require the entire set d t   because the domain of many types may be partially
unknown or too large to be enumerated   in addition  a predicate eqt  t  t  is available for
checking equality between values of each semantic type to handle the case where multiple
serialisations of a variable represent the same value 
each relation r  r is referred to as a global relation or domain predicate and its
extension is virtual  meaning that the extension can only be generated by inspecting every
relevant data source  the set of relations r may include some interpreted predicates  such
as   whose extension is defined and not virtual 
the language l used for constructing queries could be any query language including
sql and xquery  the xml query language   in this paper we will use a form of datalog 
each source s  s has an extension e s  which is the complete set of tuples that can be
produced by the source  at a given moment in time   we require that the corresponding view
definition vs  v  written in lr s   is consistent with the source  such that  e s   ei  vs   
 where i r  is the current virtual database instance over the global relations   note that
we do not require equivalence  because some sources may provide incomplete data 
the view definition for the source to be modeled s is unknown  the solution to the
source definition induction problem is a view definition v   lr s for the source s such
that e s    ei  v     and there is no other view definition v    lr s that better describes
 provides a tighter definition for  the source s   i e  
v    lr s s t  e s    ei  v       ei  v         ei  v    
   the   symbol denotes set difference 

 

ficarman   knoblock

given limited available computation and bandwidth  we note that it may not be possible to
guarantee that this optimality condition holds for a particular solution  thus in this paper
we will simply strive to find the best solution possible 
    implicit assumptions
a number of assumptions are implicit in the problem formulation  the first is that there
exists a system capable of discovering new sources and more importantly classifying  to
good accuracy  the semantic types of their input and output  systems capable of doing this
were discussed in section     
the second assumption has to do with the representation of each source as a relational
view definition  most sources on the internet provide tree structured xml data  it may
not always be obvious how best to flatten that data into a set of relational tuples  while
preserving the intended meaning of the data  consider a travel booking site which returns
a set of flight options each with a ticket number  a price and a list of flight segments that
constitute the itinerary  one possibility for converting this data into a set of tuples would
be to break each ticket up into individual flight segment tuples  thereby obscuring the
relationship between price and the number of flight segments   another would be to create
one very long tuple for each ticket with room for a number of flight segments  thereby
creating tuples with many null values   in this case  it is not obvious which  if either  of
these options is to be preferred  most online data sources can  however  be modeled quite
naturally as relational sources  and by first tackling the relational problem  we can develop
techniques that can later be applied to the more difficult semi structured case 
a third assumption is that the set of domain relations suffices for describing the source to
be modeled  for instance  consider the case where the domain model only contains relations
describing financial data  and the new source provides weather forecasts  obviously  the
system would be unable to find an adequate description of the behavior of the source and
would not learn a model of it  from a practical perspective  this limitation is not a big
problem  since a user can only request data  write queries to a mediator  using the relations
in the domain model anyway   thus any source which cannot be described using those
relations would not be needed to answer user requests   in other words  the onus is on
the domain modeler to model sufficient relations so as to be able to describe the types of
queries a user should be able to pose to the system and consequently  the types of sources
that should be available  that said  an interesting avenue for future research would be to
investigate the problem of automating  at least in part  the process of expanding the scope
of the domain model  by adding attributes to relations  or inventing new ones   based on
the types of sources discovered 
    problem discussion
a number of questions arise from the problem formulation  the first being where the domain
model comes from  in principle  the set of semantic types and relations could come from
many places  it could be taken from standard data models for the different domains  or it
might just be the simplest model possible that aptly describes the set of known sources  the
domain model may then evolve over time as sources are discovered for which no appropriate
model can be found  somewhat related is the question of how specific the semantic types
 

filearning semantic definitions of information sources on the internet

ought to be  for example  is it sufficient to have one semantic type distance or should
one distinguish between distance in meters and distance in feet  generally speaking  a
semantic type should be created for each attribute that is syntactically dissimilar to all
other attributes  for example  a phone number and a zipcode have very different syntax 
thus operations which accept one of the types as input are unlikely to accept the other  in
practice  one might create a new semantic type whenever a trained classifier can recognise
the type based on its syntax alone  in general  the more semantic types there are  the
harder the job of the system classifying the attributes  and the easier the job of the system
tasked with learning a definition for the source 
another question to be considered is where the definitions for the known sources come
from  initially such definitions would need to be written by hand  as the system learns
definitions for new sources  they too would be added to the set of known sources  making
it possible to learn ever more complicated definitions 
in order for the system to learn a definition for a new source  it must be able to invoke
that source and thus needs examples of the input types  the more representative the set of
examples available  the more efficient and accurate the learning process will be  an initial
set of examples will need to be provided by the domain modeler  then  as the system learns
over time  it will generate a large number of examples of different semantic types  as output
from various sources   which can be retained for future use 
information integration research has reached a point where mediator technology  is
becoming mature and practical  the need to involve a human in the writing of source
definitions is  however  the achilles heel of such systems  the gains in flexibility that come
with the ability to dynamically reformulate user queries are often partially offset by the
time and skill required to write definitions when incorporating new sources  thus a system
capable of learning definitions automatically could greatly enhance the viability of mediator
technology  this motivation alone seems sufficient for pursuing the problem 

   approach
the approach we take to learning semantic models for information sources on the web is
twofold  firstly  we choose to model sources using the powerful language of conjunctive
queries  secondly  we leverage the set of known sources in order to learn a definition for the
new one  in this section we discuss these aspects in more detail 
    modeling language
the source definition language l is the hypothesis language in which new definitions will
need to be learnt  as is often the case in machine learning  we are faced with a trade off
with respect to the expressiveness of this language  if the hypothesis language is too simple 
then we may not be able to model real services using it  on the other hand  if the language
is overly complex  then the space of possible hypotheses will be so large that learning will
not be feasible  the language we choose is that of conjunctive queries in datalog  which is
   influential information integration systems include tsimmis  garcia molina  hammer  ireland  papakonstantinou  ullman    widom         sims  arens  knoblock    shen         infomaster  duschka 
       and ariadne  knoblock  minton  ambite  ashish  muslea  philpot    tejada        

 

ficarman   knoblock

a highly expressive relational query language  in this section we argue why a less expressive
language is not sufficient for our purposes 
researchers interested in the problem of assigning semantics to web services  he  
kushmerick        have investigated the problem of using machine learning techniques to
classify services  based on metadata characteristics  into different semantic domains  such
as weather and flights  and the operations they provide into different classes of operation 
such as weatherforecast and flightstatus  from a relational perspective  we can consider
the different classes of operations as relations  for instance  consider the definition below 
source  zip  temp     weatherforecast zip  tomorrow  temp  
the source provides weather data by selecting tuples from a relation called weatherforecast 
which has the desired zipcode and date equal to tomorrow  this query is referred to as a
select project query because its evaluation can be performed using the relational operators
selection and projection  so far so good  we have been able to use a simple classifier to learn
a simple definition for a source  the limitation imposed by this restricted  select project 
modeling language becomes obvious  however  when we consider slightly more complicated
sources  consider a source that provides the temperature in fahrenheit as well as celsius 
in order to model such a source using a select project query  we would require that the
weatherforecast relation be extended with a new attribute as follows 
source  zip  tempc  tempf    weatherforecast zip  tomorrow  tempc  tempf  
the more attributes that could conceivably be returned by a weather forecast operation
 such as dewpoint  humidity  temperature in kelvin  latitude  etc    the longer the relation
will need to be to cover them all  better  in this case  would be to introduce a second
relation convertctof that makes explicit the relationship between the temperature values 
if  in addition  the source limits its output to zipcodes in california  a reasonable definition
for the source might be 
source  zip  tempc  tempf  weatherforecast zip  tomorrow  tempc   convertctof tempc  tempf  
state zip  california  
this definition is no longer expressed in the language of select project queries  because it now
involves multiple relations and joins across them  thus from this simple example  we see that
modeling services using simple select project queries is not sufficient for our purposes  what
we need are select project join queries  also referred to as conjunctive queries   the reader
has already been introduced to examples of conjunctive queries throughout the previous
sections  conjunctive queries form a subset of the logical query language datalog and can
be described more formally as follows 
a conjunctive query over a set of relations r is an expression of the form 
q x       r   x     r   x          rl  xl   
where each ri  r is a relation and xi is an ordered set of variable names of size
arity ri     each conjunct ri  xi   is referred to as a literal  the set of variables
s
in the query  denoted vars q    li   xi   consists of distinguished variables
x   from the head of the query   and existential variables vars q  x     which
   evaluating a select project join query requires additional relational operators  natural join and rename 
   note that a conjunctive query can also be expressed in first order logicsas follows 
l
x  y s t  r   x     r   x          rl  xl    q x    where x   y   i   xi

  

filearning semantic definitions of information sources on the internet

only appear in the body   a conjunctive query is said to be safe if all the
s
distinguished variables appear in the body  i e  x   li   xi  
    more expressive languages
modeling sources using conjunctive queries implies that aggregate operators like min and
order cannot be used in source definitions  the functionality of most sources can be
described without such operators  some sources can only be described poorly  however 
consider a hotel search service that returns the    closest hotels to a given location 
hotelsearch  loc  hotel  dist   accommodation hotel  loc    distance loc  loc   dist  
according to the definition  the source should return all hotels regardless of distance  one
cannot express the fact that only the closest hotels will be returned  the reason for not
including aggregate operators in the hypothesis language is that the search space associated
with learning definitions is prohibitively large   thus we leave aggregate operators to future
work as discussed in section      
similarly  source definitions cannot contain disjunction  which rules out union and recursive queries  again  this simplifying assumption holds for most information sources
and greatly reduces the search space  it means however  that a weather service providing
forecasts only for cities in the us and canada would be modeled as 
s  city  temp     forecast city  country  tomorrow  temp  
since the definition does not restrict the domain of the country attribute  when confronted
with a request for the forecast in australia  a mediator would proceed to call the service 
oblivious to any restriction on that attribute 
we also do not allow negation in the queries because source definitions very rarely
require it  so including it would needlessly complicate the search  in those rare cases where
the negation of a particular predicate is useful for describing certain types of sources  the
negated predicate can be included  as a distinct predicate  in the search  for instance  we
might use  to describe a source  even though strictly speaking it is the negation of   
    leveraging known sources
our approach to the problem of discovering semantic definitions for new services is to
leverage the set of known sources when learning a new definition  broadly speaking  we do
this by invoking the known sources  in a methodical manner  to see if any combination of
the information they provide matches the information provided by the new source  from a
practical perspective  this means in order to model a newly discovered source semantically 
we require some overlap in the data being produced by the new source and the set of known
sources  one way to understand this is to consider a new source producing weather data 
if none of the known sources produce any weather information  then there is no way for
the system to learn whether the new source is producing historical weather data  weather
forecasts   or even that it is describing weather at all   in principle  one could try to guess
what the service is doing based on the type signature alone  but there would be no guarantee
that the definition was correct  making it of little use to a mediator   given this overlapping
data requirement  one might claim that there is little benefit in incorporating new sources 
we detail some of the reasons why this is not the case below 
  

ficarman   knoblock

the most obvious benefit of learning definitions for new sources is redundancy  if the
system is able to learn that one source provides exactly the same information as a currently
available source  then if the latter suddenly becomes unavailable  the former can be used
in its place  for example  if a mediator knows of one weather source providing current
conditions and learns that a second source provides the same or similar data  then if the
first goes down for whatever reason  perhaps because an access quota has been reached  
weather data can still be accessed from the second 
the second and perhaps more interesting reason for wanting to learn a definition for
a new source is that the new source may provide data which lies outside the scope of  or
simply was not present in  the data provided by the other sources  for example  consider a
weather service which provides temperature values for zipcodes in the united states  then
consider a second source that provides weather forecasts for cities worldwide  if the system
can use the first source to learn a definition for the second  the amount of information
available for querying increases greatly 
binding constraints on a service can make accessing certain types of information difficult
or inefficient  in this case  discovering a new source providing the same or similar data but
with a different binding pattern may improve performance  for example  consider a hotel
search service that accepts a zipcode and returns a set of hotels along with their star rating 
hotelsearch  zip  hotel  rating  street  city  state  accommodation hotel  rating  street  city  state  zip  
now consider a simple query for the names and addresses of all five star hotels in california 
q hotel  street  city  zip    accommodation hotel      street  city  california  zip  
answering this query would require thousands of calls to the known source  one for every
zipcode in california  and a mediator could only answer the query if there was another
source providing those zipcodes  in contrast  if the system had learnt a definition for a new
source which provides exactly the same data but with a different binding pattern  such as
the one below   then answering the query would require only one call to a source 
hotelsbystate  state   rating  hotel  street  city  zip  accommodation hotel  rating  street  city  state  zip  
often the functionality of a complex source can be described in terms of a composition of
the functionality provided by other simpler services  for instance  consider the motivating
example from section    in which the functionality provided by the new source was to calculate the distance in miles between two zipcodes  the same functionality could be achieved
by performing four different calls to the available sources  in that case  the definition learnt
by the system meant that any query regarding the distance between zipcodes could be handled more efficiently  in general  by learning definitions for more complicated sources in
terms of simpler ones  the system can benefit from computation  optimisation and caching
abilities of services providing complex functionality 
finally  the newly discovered service may be faster to access than the known sources
providing similar data  for instance  consider a geocoding service that takes in an address
and returns the latitude and longitude coordinates of the location  because of the variety
in algorithms used to calculate the coordinates  its not unreasonable for some geocoding
services to take a very long time  upwards of one second  to return a result  if the system
were able to discover a new source providing the same geocoding functionality  but using
  

filearning semantic definitions of information sources on the internet

a faster algorithm  then it could locate and display many more addresses on a map in the
same amount of time 

   inducing definitions
in this section we describe an algorithm for generating candidate definitions for a newly
discovered source  the algorithm forms the first phase in a generate and test methodology
for learning source definitions  we defer discussion of the testing phase to later in the paper 
we start by briefly discussing work on relational rule learning and then describe how our
algorithm builds upon these ideas 
    inductive logic programming
the language of conjunctive queries is a restricted form of first order logic  in the machine
learning community  systems capable of learning models using first order representations
are referred to as inductive logic programming  ilp  systems or relational rule learners 
because of the expressiveness of the modeling language  the complexity of learning is much
higher than for propositional rule learners  also called attribute value learners   which form
the bulk of machine learning algorithms  given our relational modeling of services  many
of the techniques developed in ilp should also apply to our problem 
the first order inductive learner  foil  is a well known ilp search algorithm  cameronjones   quinlan         it is capable of learning first order rules to describe a target predicate  which is represented by a set of positive examples  tuples over the target relation 
denoted e     and optionally also a set of negative examples  e     the search for a viable
definition in foil starts with an empty clause  and progressively adds literals to the body
 antecedent  of the rule  thereby making the rule more specific  this process continues until
the definition  denoted h  covers only positive examples and no negative examples 
e    ei  h     

and e   ei  h    

usually a set of rules is learnt in this manner by removing the positive examples covered
by the first rule and repeating the process   the set of rules is then interpreted as a union
query   search in foil is performed in a greedy best first manner  guided by an information
gain based heuristic  many extensions to the basic algorithm exist  most notably those that
combine declarative background knowledge in the search process such as focl  pazzani  
kibler         such systems are categorised as performing a top down search because they
start from an empty clause  the most general rule possible  and progressively specialize
the clause  bottom up approaches  on the other hand  such as golem  muggleton   feng 
       perform a specific to general search starting from the positive examples of the target 
    search
we now describe the actual search procedure we use to generate candidate definitions for
a new source  the procedure is based on the top down search strategy used in foil  the
algorithm takes as input a type signature for the new source and uses it to seed the search for
   we use the terms clause and query interchangeably to refer to a conjunctive query in datalog  an empty
clause is a query without any literals in the body  right side  of the clause 

  

ficarman   knoblock

input   a predicate signature s
output  the best scoring view definition vbest
invoke target with set of random inputs 
vbest  empty clause s  
  add vbest to empty queue 
  while queue      time     timeout  i     limit do
 
v   best definition from queue 
 
forall v   expand v    do
 
insert v  into queue 
 
while eval v        do
 
forall v   constrain v    do
  
insert v  into queue 
  
if eval v     eval v    then v   v   
  
end
  
end
  
if eval v     eval vbest   then vbest  v   
  
end
   end
   return vbest  
algorithm    best first search through the space of candidate source definitions 
 

 

candidate definitions   we will refer to the new source relation as the target predicate and
the set of known source relations as source predicates   the space of candidate definitions is
enumerated in a best first manner  with each candidate tested to see if the data it returns
is similar to the target  pseudo code describing the procedure is given in algorithm    
the first step in the algorithm is to invoke the new source with a representative set of
input tuples to generate examples of output tuples that characterise the functionality of the
source  this set of invocations must include positive examples  invocations for which output
tuples were produced  and if possible  also negative tuples  inputs for which no output was
returned   the algorithms ability to induce the correct definition for a source depends
greatly on the number of positive examples available  thus a minimum number of positive
invocations of the source is imposed  meaning that the algorithm may have to invoke the
source repeatedly using different inputs until sufficient positive invocations can be recorded 
selecting appropriate input values so as to successfully invoke a service is easier said than
done  we defer discussion of the issues and difficulties involved in successfully invoking the
new source to section      and assume for the moment that the induction system is able to
generate a table of values that represent its functionality 
the next step in the algorithm is to initialise the search by adding an empty clause to
the queue of definitions to expand  the rest of the algorithm is simply a best first search
procedure  at each iteration the highest scoring but not yet expanded definition  denoted
v    is removed from the queue and expanded by adding a new predicate to the end of the
   the implementation of this algorithm used in the experiments of section     is available at 
http   www isi edu publications licensed sw eidos index html

  

filearning semantic definitions of information sources on the internet

clause  see the next section for an example   each candidate generated  denoted v    is
added to the queue  the algorithm then progressively constrains the candidate by binding
variables of the newly added predicate   see section       the eval function  see section
     evaluates the quality of each candidate produced  the procedure stops constraining
the candidate when the change in the evaluation function  eval  drops to zero  it then
compares v  to the previous best candidate vbest and updates the latter accordingly 
in principle the algorithm should terminate when the perfect candidate definition is
discovered   one that produces exactly the same data as the target  in practice that never
occurs because the sources are incomplete  dont perfectly overlap with each other  and are
noisy  instead the algorithm terminates when either the queue becomes empty  a time limit
is reached or a maximum number of iterations has been performed 
    an example
we now run through an example of the process of generating candidate definitions  consider
a newly discovered source  which takes in a zipcode and a distance  and returns all the
zipcodes that lie within the given radius  along with their respective distances   the target
predicate representing this source is 
source   zip    dist   zip   dist  
now assume that there are two known sources  the first is the source for which the definition
was learnt in the example from section    namely 
source   zip    zip   dist  centroid zip   lat   long    centroid zip   lat   long   
greatcircledist lat   long   lat   long   dist    multiply dist          dist   
the second source isnt actually a source but an interpreted predicate 
  dist   dist   
the search for a definition for the new source might then proceed as follows  the first
definition to be generated is the empty clause 
source              
the null character     represents the fact that none of the inputs or outputs have any
restrictions placed on their values  prior to adding the first literal  source predicate   the
system will check whether any output attributes echo the input values  in this case  given
the semantic types  two possibilities need to be checked 
source   zip       zip     
source       dist     dist   
assuming neither of these possibilities is true  i e  improves the score   then literals will
be added one at a time to refine the definition  a literal is a source predicate with an
assignment of variable names to its attributes  a new definition must be created for every
possible literal that includes at least one variable already present in the clause   for the
moment we ignore the issue of binding constraints on the sources being added   thus many
candidate definitions would be generated  including the following 
source   zip    dist          source   zip       dist   
source   zip       zip    
   source   zip    zip     
source       dist     dist       dist   dist   
  

ficarman   knoblock

note that the semantic types in the type signature of the target predicate limit greatly the
number of candidate definitions that can be produced  the system then evaluates each of
these candidates in turn  selecting the best one for further expansion  assuming the first of
the three has the best score  it would be expanded by adding another literal  forming more
complicated candidates such as the following 
source   zip    dist     dist      source   zip       dist     dist   dist   
this process continues until the system discovers a definition which perfectly describes the
source  or is forced to backtrack because no literal improves the score 
    iterative expansion
the sources used in the previous example all had relatively low arity  on the internet this is
rarely the case  with many sources producing a large number of attributes of the same type 
this is a problem because it causes an exponential number of definitions to be possible at
each expansion step  consider for instance a stock price service  which provides the current 
high  low  market opening and market closing prices for a given ticker symbol  the type
signature for that service would be 
stockprice  ticker  price  price  price  price  price 
if the definition to which this predicate is to be added already contains k distinct price
variables  then the number of ways in which the price attributes of the new relation can be
p
assigned variable names is  i    i k i   which is prohibitively large even for moderate k   to
limit the search space in the case of such high arity predicates  we first generate candidates
with a minimal number of bound variables in the new literal and progressively constrain
the best performing of these definitions within each expansion   high arity predicates are
handled in a similar fashion in foil  quinlan and cameron jones         for example 
consider using the source above to learn a definition for a new source with signature 
source   ticker  price  price 
we start by adding literals to an empty definition as before  this time though  instead of
generating a literal for every possible assignment of variable names to the attributes of each
relation  we generate only the simplest assignments such that all of the binding constraints
are met   this is the expand procedure referred to in algorithm     in this example  the
ticker symbol input of the stockprice source would need to be bound  generating a single
definition 
source   tic         stockprice  tic            
this definition would be evaluated  and then more constrained definitions are generated by
equating a variable from the same literal to other variables from the clause   this is the
constrain procedure from algorithm     two such definitions are shown below 
source   tic  pri        stockprice  tic  pri           
source   tic  pri        stockprice  tic    pri         
the best of these definitions would then be selected and constrained further  generating
definitions such as 
i
   intuitively  one can assign variable
 names to i attributes using k labels in k different ways  one can
choose i of the   attributes in  i ways  and one can do that for i                see weber  tausend  and
stahl        for a detailed discussion of the size of the hypothesis space in ilp 

  

filearning semantic definitions of information sources on the internet

source   tic  pri   pri      stockprice  tic    pri   pri       
source   tic  pri   pri      stockprice  tic    pri     pri     
in this way  the best scoring literal can be found without the need to iterate over all of the
possible assignments of variables to attributes 
    domain predicates vs  source predicates
in the examples of sections     and      the decision to perform search over the source
predicates rather than the domain predicates was made in an arbitrary fashion    in this
section we justify that decision  if one were to perform the search over the domain predicates
rather than the source predicates  then testing each definition would require an additional
query reformulation step  for example  consider the following candidate definition for
source  containing the domain predicate centroid 
source   zip              centroid zip       
in order to evaluate this candidate  the system would need to first treat the definition as
a query and reformulate it into a set of rewritings  that together form a union query  over
the various sources as follows 
source   zip              source   zip         
source   zip              source       zip     
this union query can then be executed against the available sources  in this case just
source   to see what tuples the candidate definition returns  in practice however  if the
definitions for the known sources contain multiple literals  as they normally do  and the
domain relations are of high arity  as they often are   then the search over the space of
conjunctions of domain predicates is often much larger than the corresponding search over
the space of conjunctions of source predicates  this is because multiple conjunctions of
domain predicates  candidate definitions  end up reformulating to the same conjunction of
source predicates  union queries   for example  consider the following candidate definitions
written in terms of the domain predicates 
source   zip              centroid zip   lat     greatcircledist lat          
source   zip              centroid zip     lon   greatcircledist    lon        
source   zip              centroid zip   lat  lon   greatcircledist lat  lon        
all three candidates would reformulate to the same query over the sources  shown below  
and thus are indistinguishable given the sources available 
source   zip              source   zip         
in general  the number of candidate definitions that map to the same reformulation can be
exponential in the number of hidden variables present in the definitions of the known sources 
for this reason  we simplify the problem and search the space of conjunctions of source
predicates  in some sense  performing the search over the source predicates can be seen as
introducing a similarity heuristic which focuses the search toward definitions with similar
structure to the definitions of the available sources  we note that the definitions produced
can  and will  later be converted to queries over the global predicates by unfolding and
    a note on the difference between domain  source and interpreted predicates  domain predicates are
invented by a domain expert for use in modeling a particular information domain  they define a common
schema that can be used for describing information from different sources  source predicates represent
the sources available in the system  interpreted predicates   such as    are a special type of domain
predicate  that can be treated as source predicates  since their meaning is interpreted  understood  

  

ficarman   knoblock

possibly tightening them to remove redundancies  we will discuss the process of tightening
the unfoldings in section     
    limiting the search
the search space generated by this top down search algorithm may be very large even for
a small number of sources  the use of semantic types limits greatly the ways in which
variables within each definition can be equated  aka the join paths  and thus goes a long
way to reduce the size of the search space  despite this reduction  as the number of sources
available increases  the search space becomes so large that techniques for limiting it must
be used  we employ some standard  and other not so standard  ilp techniques for limiting
this space  such limitations are often referred to as inductive search bias or language bias
 nedellec  rouveirol  ade  bergadano    tausend        
an obvious way to limit the search is to restrict the number of source predicates that
can occur in a definition  whenever a definition reaches the maximum length  backtracking
can be performed  allowing the search to escape from local minima that may result from
greedy enumeration  the assumption here is that shorter definitions are more probable
than longer ones  which makes sense since service providers are likely to provide data in
the simplest form possible  moreover  the simpler the definition learnt  the more useful
it will be to a mediator  so we decide to trade completeness  the ability to express longer
definitions  for improved accuracy over shorter definitions 
the second restriction placed on candidate definitions is to limit the number of times
the same source predicate appears in a given candidate  this makes sense because the
definitions of real services tend not to contain many repeated predicates  intuitively  this
is because most services provide raw data without performing many calculations upon it 
repeated use of the same predicate in a definition is more useful for describing some form
of calculation than raw data itself   exceptions to this rule exist  for example predicates
representing unit conversion functionality such as fahrenheit to celsius  may necessarily
occur multiple times in the definition of a source  
the third restriction limits the complexity of the definitions generated by reducing the
number of literals that do not contain variables from the head of the clause  specifically 
it limits the level of existential quantification  sometimes also referred to as the depth 
muggleton and feng        of each variable in a clause  this level is defined to be zero
for all distinguished variables  those appearing in the head of the clause   for existential
variables it is defined recursively as one plus the lowest level of any variable appearing in the
same literal  for example  the candidate definition shown below has a maximum existential
quantification level of three because the shortest path from the last literal to the head literal
 via join variables  passes through two other literals 
source   zip              source   zip       d    source   d   d    source   d     
the effect of this bias is to concentrate the search around simpler but highly connected
definitions  where each literal is closely linked to the input and output of the source 
the fourth restriction placed on source definitions is that they are executable  more
specifically  it should be possible to execute them from left to right  meaning that the inputs
of each source appear either in the target predicate  head of the clause  or in one of the
literals to the left of that literal  for example  of the two candidate definitions shown below 
  

filearning semantic definitions of information sources on the internet

only the first is executable  the second definition is not  because zip  is used as input for
source  in the first literal  without first being bound to a value in the head of the clause 
source   zip       zip        source   zip    zip     
source   zip              source   zip    zip   dist    source   zip    zip   dist   
this restriction serves two purposes  firstly  like the other biases  it limits the size of the
search space  secondly  it makes it easier to evaluate the definitions produced  in theory 
one could still evaluate the second definition above by generating lots of input values for
zip   but that would require a lot of invocations for minimal gain 
the last restriction reduces the search space by limiting the number of times the same
variable can appear in any given literal in the body of the clause  definitions in which the
same variable appears multiple times in a given literal  such as in the following example
which returns the distance between a zipcode and itself  are not very common in practice 
source   zip         dist      source   zip    zip   dist   
explicitly preventing such definitions from being generated makes sense because sources
requiring them are so rare  that it is better to reduce the search space exponentially by
ignoring them  than to explicitly check for them each time 

   scoring definitions
we now proceed to the problem of evaluating the candidate definitions generated during
search  the basic idea is to compare the output produced by the source with the output
produced by the definition on the same input  the more similar the set of tuples produced 
the higher the score for the candidate  the score is then averaged over a set of different
input tuples to see how well the candidate definition describes the new source 
in the motivating example of section    the source for which a definition was being learnt
 the definition is repeated below  only produced one output tuple hdisti for every input
tuple hzip   zip i 
source   zip    zip   dist  centroid zip   lat   long    centroid zip   lat   long   
greatcircledist lat   long   lat   long   dist   
multiply dist           dist   
this fact made it simple to compare the output of the service with the output of the induced
definition  in general however  the source to be modeled  and the candidate definitions
modeling it  may produce multiple output tuples for each input tuple  take for example
source  from section      which produces the set of output tuples hzip   dist i containing
all the zipcodes which lie within a given radius of the input zipcode hzip   dist i  in
such cases  the system needs to compare a set of output tuples with the set produced by
the definition to see if any of the tuples are the same  since both the new source and the
existing sources may not be complete  the two sets may simply overlap  even if the candidate
definition correctly describes the new source  assuming that we can count the number of
tuples that are the same  we need a measure that tells us how well a candidate hypothesis
describes the data returned by a source  one such measure is the following 
score s  v  i   

  x  os  i   ov  i  
 i  ii  os  i   ov  i  
  

ficarman   knoblock

where s is the new source  v is a candidate source definition  and i  d s   is the set of
input tuples used to test the source  s is the set of input attributes of source s   os  i 
denotes the set of tuples returned by the new source when invoked with input tuple i  ov  i 
is the corresponding set returned by the candidate definition  using relational projection
   and selection    operators and the notation introduced in section      these sets can
be written as follows   note that sc represents the output attributes of s  
os  i   sc  s  i  e s   

and

ov  i   sc  s  i  ei  v   

if we view this hypothesis testing as an information retrieval task  we can consider recall
to be the number of common tuples  divided by the number of tuples produced by the
source  and precision to be the number of common tuples divided by the number of tuples
produced by the definition  the above measure takes both precision and recall into account
by calculating the average jaccard similarity between the sets  the table below gives an
example of how this score is calculated for each input tuple 
input tuple
ii
ha  bi
hc  di
he  f i
hg  hi
hi  ji

actual output
tuples os  i 
 hx  yi  hx  zi 
 hx  wi  hx  zi 
 hx  wi  hx  yi 



predicted output
tuples ov  i 
 hx  yi 
 hx  wi  hx  yi 
 hx  wi  hx  yi 
 hx  yi 


jaccard similarity
for tuple i
   
   
 
 
 undef 

the first two rows of the table show inputs for which the predicted and actual output
tuples overlap  in the third row  the definition produces exactly the same set of tuples
as the source being modeled and thus gets the maximum score  in the fourth row  the
definition produced a tuple  while the source didnt  so the definition was penalised  in the
last row  the definition correctly predicted that no tuples would be output by the source 
our score function is undefined at this point  from a certain perspective the definition
should score well here because it has correctly predicted that no tuples be returned for that
input  but giving a high score to a definition when it produces no tuples can be dangerous 
doing so may cause overly constrained definitions that can generate very few output tuples
to score well  at the same time  less constrained definitions that are better at predicting the
output tuples on average may score poorly  for example  consider a source which returns
weather forecasts for zipcodes in los angeles 
source  zip  temp     forecast zip  tomorrow  temp   uscity zip  los angeles  
now consider two candidate definitions for the source  the first returns the temperature
for a zipcode  while the second returns the temperature only if it is below   c 
v    zip  temp     forecast zip  tomorrow  temp  
v    zip  temp     forecast zip  tomorrow  temp   temp      c  
assume that the source and candidates are invoked using    different randomly selected
zipcodes  for most zipcodes  the source will not return any output  because the zipcode
will lie outside of los angeles  the first candidate will likely return output for all zipcodes 
while the second candidate would  like the source  only rarely produce any output  this is
because the temperature in most zipcodes will be greater than zero  and has nothing to do
  

filearning semantic definitions of information sources on the internet

with whether or not the zipcode is in los angeles  if we score definitions highly when they
correctly produce no output  the system would erroneously prefer the second candidate over
the first  because the latter often produces no output   to prevent that from happening 
we simply ignore inputs for which the definition correctly predicts zero tuples  this is the
same as setting the score to be the average of the other values 
returning our attention to the table  after ignoring the last row  the overall score for
this definition would be calculated as      
    partial definitions
as the search proceeds toward the correct definition for the service  many semi complete
 unsafe  definitions will be generated  these definitions will not produce values for all
attributes of the target tuple but only a subset of them  for example  the candidate 
source   zip    dist   zip        source   zip    zip   dist   
produces only one of the two output attributes produced by the source  this presents a
problem  because our score is only defined over sets of tuples containing all of the output
attributes of the new source  one solution might be to wait until the definitions become
sufficiently long as to produce all outputs  before comparing them to see which one best
describes the new source  there are  however  two reasons why this would not make sense 
 the space of safe definitions is too large to enumerate  and thus we need to compare
partial definitions to guide the search toward the correct definition 
 the best definition that the system can generate may well be a partial one  as the set
of known sources may not be sufficient to completely model the source 
the simplest way to compute a score for a partial definition is to compute the same function
as before  but instead of using the raw source tuples  projecting them over the subset of
attributes that are produced by the definition  this revised score is shown below   note
that the projection is over v s   which denotes the subset of output attributes of s which
are produced by the view definition v  note also that the projection is not distinct  i e 
multiple instances of the same tuple may be produced  
score   s  v  i   

  x  v s  os  i    ov  i  
 i  ii  v s  os  i    ov  i  

this revised score is not very useful however  as it gives an unfair advantage to definitions
that do not produce all of the output attributes of the source  this is because it is far easier
to correctly produce a subset of the output attributes than to produce all of them  consider
for example the two source definitions shown below  the two definitions are identical except
that the second returns the output distance value dist   while the first does not 
source   zip    dist   zip    
   source   zip    zip   dist     dist   dist   
source   zip    dist   zip   dist     source   zip    zip   dist     dist   dist   
since the two are identical  the projection over the subset will in this case return the same
number of tuples  this means that both definitions would get the same score although the
second definition is clearly better than the first since it produces all of the required outputs 
we need to be able to penalise partial definitions in some way for the attributes they
dont produce  one way to do this is to first calculate the size of the domain  d a   of each
  

ficarman   knoblock

of the missing attributes  in the example above  the missing attribute is the distance value 
since distance is a continuous value  calculating the size of its domain is not obvious  we
can approximate the size of its domain by 
 d distance   

max  min
accuracy

where accuracy is the error bound on distance values   we will discuss error bounds further
in section       note that the cardinality calculation may be specific to each semantic type 
armed with the domain size  we can penalise the score for the definition by dividing it
by the product of the size of the domains of all output attributes not generated by the
definition  in essence  we are saying that all possible values for these extra attributes have
been allowed by this definition  this technique is similar to techniques used for learning
without explicit negative examples  zelle  thompson  califf    mooney        
the set of missing output attributes is given by the expression sc  v  thus the penalty
for missing attributes is just the size of the domain of tuples of that scheme  i e  
penalty    d sc  v  
using this penalty value we can calculate a new score  which takes into account the missing
attributes  simply dividing the projected score by the penalty would not adhere to the
intended meaning of compensating for the missing attribute values  and thus may skew the
results  instead  we derive a new score by introducing the concept of typed dom predicates
as follows 
a dom predicate for a semantic data type t  denoted domt   is a single arity
relation whose extension is set to be the domain of the datatype  i e  e domt    
d t   similarly  a dom predicate for a scheme a  denoted doma   is a relation
over a whose extension is e doma     d a  
dom predicates were introduced by duschka to handle the problem of query reformulation
in the presence of sources with binding constraints  duschka          in that work the
predicates were not typed  although typing would have resulted in a more efficient algorithm   here we use them to convert a partial definition v into a safe  complete  definition
v     we can do this simply by adding a dom predicate to the end of the view definition that
generates values for the missing attributes  for the example above  v   would be 
source   zip    dist   zip   x   source   zip    zip   dist     dist   dist    domdistance  x  
where x is a new variable of type distance  the new view definition v   is safe  because
all the variables in the head of the clause also appear in the body  in general  we can
turn any unsafe view definition v into a safe definition v   by appending a dom predicate
domsc  v  x         xn    where each xi is a distinguished variable  from the head of the clause 
corresponding to an output attribute of v   that wasnt bound in v  now we can use this
complete definition to calculate the score as before 
score   s  v  i    score s  v     i   

  

  x  os  i   ov   i  
 i  ii  os  i   ov   i  

filearning semantic definitions of information sources on the internet

which can be rewritten  by expanding the denominator  as follows 
score   s  v  i   

 os  i   ov   i  
  x
 i  ii  os  i      ov   i     os  i   ov   i  

we can then remove the references to v   from this equation by considering 
ov   i    ov  i   e domsc  v     ov  i   d sc  v 
thus the size of the set is given by  ov   i      ov  i   d sc  v   and the size of the intersection
can be calculated by taking the projection over the output attributes produced by v 
 os  i   ov   i      v s  os  i   ov  i   d sc  v       v s  os  i    ov  i  
substituting these cardinalities into the score function given above  we arrive at the following
equation for the penalised score 
score   s  v  i   

 v s  os  i    ov  i  
  x
 i  ii  os  i      ov  i   d sc  v     v s  os  i    ov  i  

    binding constraints
some of the candidate definitions generated during the search may have different binding
constraints from the target predicate  for instance in the partial definition shown below 
the variable zip  is an output of the target source  but an input to source   
source   zip    dist   zip        source   zip    zip   dist   
from a logical perspective  in order to test this definition correctly  we need to invoke
source  with every possible value from the domain of zipcodes  doing this is not practical
for two reasons  firstly  the system may not have a complete list of zipcodes at its disposal 
secondly and far more importantly  invoking source  with thousands of different zipcodes
would take a very long time and would probably result in the system being blocked from
further use of the service  so instead of invoking the same source thousands of times 
we approximate the score for this definition by sampling from the domain of zipcodes
and invoking the source using the sampled values  we then compensate for this sampling
by scaling  certain components of  the score by the ratio of the sampled zipcodes to the
entire domain  considering the example above  if we randomly choose a sample  denoted
 zipcode   of say    values from the domain of zipcodes  then the set of tuples returned by
the definition will need to be scaled by a factor of  d zipcode      
a more general equation for computing the scaling factor  denoted sf   is shown below 
note that the sampling may need to be performed over a set of attributes   here v  s
denotes the input attributes of v which are outputs of s  
sf  

 d v  s   
  v  s   

we now calculate the effect of this scaling factor on the overall score as follows  we denote
the set of tuples returned by the definition given the sampled input as ov  i   this value
  

ficarman   knoblock

when scaled will approximate the set of tuples that would have been returned had the
definition been invoked with all the possible values for the additional input attributes 
 ov  i     ov  i    sf
assuming the sampling is performed randomly over the domain of possible values  the
intersection between the tuples produced by the source and the definition should scale in
the same way  thus the only factor not affected by the scaling in the score defined previously
is  os  i    if we divide throughout by the scaling factor we have a new scoring function 
score   s  v  i   

 v s  os  i    ov  i  
  x
 i  ii  os  i   sf    ov  i   d sc  v     v s  os  i    ov  i  

the problem with this approach is that often the sampled set of values is too small and as a
result it does not intersect with the set of values returned by the source  even though a larger
sample would have intersected in some way  thus our sampling introduces unfair distortions
into the score for certain definitions  causing them to perform poorly  for example  consider
again source  and assume that for scalability purposes  the service places a limit on the
maximum value for the input radius dist    this makes sense  as otherwise the user could
set the input radius to cover the entire us  and a tuple for every possible zipcode would
need to be returned   now consider the sampling performed above  if we randomly choose
only    zipcodes from the set of all possible zipcodes  the chance of the sample containing
a zipcode which lies within a     mile radius of a particular zipcode  in the middle of the
desert  is very low  moreover  even if one pair of zipcodes  out of     results in a successful
invocation  this will not be sufficient for learning a good definition for the service 
so to get around this problem we bias the sample such that  whenever possible  half of
the values are taken from positive examples of the target  those tuples returned by the new
source  and half are taken from negative examples  those tuples not returned by the source  
by sampling from both positive and negative tuples  we guarantee that the approximation
generated will be as accurate as possible given the limited sample size  we denote the set
of positive and negative samples as     v  s   and    v  s    and use these values to define
positive and total scaling factors as shown below   the numerator for the positive values is
different from before  as these values have been taken from the output of the new source  
sf    

 v  s  v s  os  i    
     v  s   

the total scaling factor is the same value as before  but calculated slightly differently 
sf  

 d v  s   
     v  s          v  s   

the score can then be approximated accordingly by taking into account these new scaling
factors  the intersection now needs to be scaled using the positive scaling factor 
 v s  os  i    ov  i     v s  os  i    ov  i    sf  
this new scaling results in a new function for evaluating the quality of a view definition 
score   s  v  i   

 v s  os  i    ov  i    sf  
  x
 i  ii  os  i      ov  i   d sc  v    sf   v s  os  i    ov  i    sf  
  

filearning semantic definitions of information sources on the internet

    favouring shorter definitions
now that we have derived a score for comparing the data that a source and candidate
produce  we can define the evaluation function eval used in algorithm    as mentioned
in section      shorter definitions for the target source should be preferred over longer and
possibly less accurate ones  in accordance with this principle  we scale the score by the
length of the definition  so as to favour shorter definitions as follows 
eval v     length v   score   s  v  i 
here length v  is the length of the clause and      is a weighting factor  setting the
weighting factor to be a little less than    such as       helps to remove logically redundant
definitions  which can sometimes be hard to detect  but often return almost exactly the same
score as their shorter equivalent  we will discuss the problem of generating non redundant
clauses in section     
    approximating equality
until now  we have ignored the problem of deciding whether two tuples produced by the
target source and the definition are the same  since different sources may serialize data in
different ways and at different levels of accuracy  we must allow for some flexibility in the
values that the tuples contain  for instance  in the example from section    the distance
values returned by the source and definition did not match exactly  but were sufficiently
similar to be accepted as the same value 
for numeric types like temperature or distance it makes sense to use an error bound  like
    c  or a percentage error  such as     to decide if two values can be considered the
same  this is because the sensing equipment  in the case of temperature  or the algorithm  in
the case of distance  will have some error bound associated with the values it produces  we
require that an error bound for each numeric type be provided in the problem specification 
 ideally  these bounds would be learnt automatically from examples  
for certain nominal types like company names  where values like hibm corporationi
and hinternational business machines corp i represent the same value  simplistic equality
checking using exact or substring matches is not sufficient for deciding whether two values
correspond to the same entity  in this case  string edit distances such as the jarowinkler
score do better at distinguishing strings representing the same entity from those representing
different ones  bilenko  mooney  cohen  ravikumar    fienberg         a machine learning
classifier could be trained on a set of such examples to learn which of the available string
edit distances best distinguishes values of that type and what threshold to set for accepting
a pair as a match  we require that this pair of similarity metric and threshold  or any
combinations of metrics  be provided in the problem specification 
in other cases  enumerated types like months of the year might be associated with a
simple equality checking procedure  so that values like hjanuaryi  hjani and h i can be
found equal  the actual equality procedure used will depend on the semantic type and we
assume in this work that such a procedure is given in the problem definition  we note that
the procedure need not be      accurate  but only provide a sufficient level of accuracy
to guide the system toward the correct source description  indeed  the equality rules could
also be generated offline by training a classifier 
  

ficarman   knoblock

complex types such as date present a bigger problem when one considers the range of
possible serializations  including values like h        i or hthu    may     i or h          i 
in such cases specialized functions are not only required to check equality between values
but also to break the complex types up into their constituent parts  in this case day  month
and year    the latter would form part of the domain model 
in some cases  deciding whether two values of the same type can be considered equal
depends not only on the type  but also on the relations they are used in  consider the
two relations shown below  the first provides the latitude and longitude coordinates of the
centroid for a zipcode  while the second returns the coordinates for a particular address 
centroid zipcode  latitude  longitude 
geocode number  street  zipcode  latitude  longitude 
given the different ways of calculating the centroid of a zipcode  including using the center
of mass or the center of population density  an error bound of     meters might make sense
for equating latitude and longitude coordinates  for a geocoding service  on the other hand 
an error bound of    meters may be more reasonable  in general  such error bounds should
be associated with the set of global relations  instead of just the semantic types  and could
be learnt accordingly  when the relations contain multiple attributes  then the problem
of deciding whether two tuples refer to the same entity is called record linkage  winkler 
       an entire field of research is devoted to tackling this problem  due to the complexity
of the problem and the variety of techniques that have been developed to handle it  we do
not investigate it further here 

   extensions
in this section we discuss extensions to the basic algorithm needed for handling real data
sources  as well as ways to reduce the size of the hypothesis space and improve the quality
of the definitions produced 
    generating inputs
the first step in the source induction algorithm is to generate a set of tuples which will
represent the target relation during the induction process  in other words  the system must
try to invoke the new source to gather some example data  doing this without biasing the
induction process is easier said than done  the simplest approach to generating input values
is to select constants at random from the set of examples given in the problem specification 
the problem with this approach is that in some cases the new source will not produce any
output for the selected inputs  instead the system may need to select values according to
some distribution over the domain of values in order for the source to invoke correctly  for
example  consider a source providing posts of used cars for sale in a certain area  the source
takes the make of the car as input  and returns car details 
usedcars  make  model  year  price  phone 
although there are over a hundred different car manufacturers in the world  only a few
of them produce the bulk of the cars  thus invoking the source with values like ferrari 
lotus and aston martin will be less likely to return any tuples  when compared with more
common brands such as ford and toyota  unless the source is only providing data for sports
cars of course   if a distribution over possible values is available  the system can first try
  

filearning semantic definitions of information sources on the internet

the more common values  or more generally  it can choose values from that set according to
the distribution  in this particular example  it might not be too difficult to query the source
with a complete set of car manufacturers until one of the invocations returns some data 
in general  the set of examples may be very large  such as the         zipcodes in the us 
and the number of interesting values in that set  the ones likely to return results  may
be very small  in which case taking advantage of prior knowledge about the distribution of
possible values makes sense  it should be noted also that during execution the system will
receive a lot of output data from the different sources it accesses  this data can be recorded
to generate distributions over possible values for the different types 
the problem of generating viable input data for a new source becomes yet more difficult
if the input required is not a single value but a tuple of values  in this case the system
can first try to invoke the source with random combinations of attribute values from the
examples of each type  invoking some sources  such as source    is easy because there is no
explicit restriction on the combination of input values 
source   zip   distance  zip  distance 
in other cases  such as a geocoding service the combination of possible input values is highly
restricted 
usgeocoder  number   street   zipcode  latitude  longitude 
randomly selecting input values independently of one another is unlikely to result in any
successful invocations   in order for the invocation to succeed  the randomly generated
tuple must correspond to an address that actually exists   in such cases  after failing to
invoke the source a number of times  the system can try to invoke other sources  such as the
hotel lookup service below   which produce tuples containing the required attribute types 
hotelsearch  city  hotel  number  street  zipcode 
in general  this process of invoking sources to generate input for other sources can be chained
until a set of viable inputs is generated 
we note that the problem of synthesizing viable input data is itself a difficult and
interesting research problem  our combined approach of utilizing value distributions and
invoking alternative services performs well in our experiments  see section       but an area
of future work is to develop a more general solution 
    dealing with sources
in order to minimise source accesses  which can be very expensive in terms of both time and
bandwidth  all requests to the individual sources are cached in a local relational database 
this implementation means that there is an implicit assumption in this work that the
output produced by the services is constant for the duration of the induction process  this
could be problematic if the service being modeled provides  near  real time data with an
update frequency of less than the time it takes to induce a definition  for a weather
prediction service  updated hourly  this may not present much of a problem  since the
difference between predicted temperatures may vary only slightly from one update to the
next  for a real time flight status service providing the coordinates of a given aircraft
every five minutes  the caching may be problematic as the location of the plane will vary
greatly if it takes  for example  one hour to induce a definition  in theory one could test
for such variation systematically by periodically invoking the same source with a previously
  

ficarman   knoblock

successful input tuple to see if the output has changed  and update the caching policy
accordingly 
    logical optimisations
evaluating definitions can be expensive both in terms of time  waiting for sources to return data  and computation  calculating joins over large tables   thus it makes sense to
check each candidate for redundancy before evaluating it  to decide which definitions are
redundant  we use the concept of query containment 
a query q   lr a is said to be contained in another query q   lr a if for any
database instance i  the set of tuples returned by the first query is a subset of
those returned by the second  i e  i ei  q     ei  q     we denote containment
by q  v q    two queries are considered logically equivalent if q  v q   q  v q   
for the conjunctive queries learnt in this paper  testing for query containment reduces to
the problem of finding a containment mapping  chandra   merlin           we can use
this test to discover logically equivalent definitions such as the following   which contain a
reordering of the same literals  
source  zip  temp    getcentroid  zip  lat  lon   getconditions  lat   lon  temp  
source  zip  temp    getconditions  lat   lon  temp   getcentroid  zip  lat  lon  
such equivalence checking can be performed efficiently if a canonical ordering of predicate
and variable names is chosen a priori  whenever logically equivalent definitions are discovered  the search can backtrack  thereby avoiding entire sub trees of equivalent clauses 
similarly  we can test for and skip logically redundant clauses such as the following  which
is equivalent to a shorter definition without the second literal  
source  zip      getcentroid  zip  lat  long   getcentroid  zip  lat    
again  such redundancy checking can be performed efficiently  levy  mendelzon  sagiv   
srivastava        resulting in little computational overhead during search 
    functional sources
more information may be known about the functionality of certain sources than is expressed
by their source definitions  for example  sources like multiply and concatenate  which are
implemented locally  will be known to be complete   a source is considered complete  if
it returns all of the tuples implied by its definition  i e  e s    ei  v    whenever such
information is available  the induction system can take advantage of it to improve search
efficiency  to explain how  we define a class of sources that we call functional sources 
which are complete and for any input tuple return exactly one output tuple  this is slightly
more restrictive than the standard ilp concept of determinate literals  cameron jones  
quinlan         which for every input tuple return at most one output tuple  multiply and
concatenate are both examples of functional sources  the system takes advantage of the fact
that functional sources place no restrictions on their input  whenever a functional source is
added to a candidate definition  the score for that definition doesnt change providing all the
sources inputs and none of its outputs are bound   the set of tuples returned by the new
    if the queries contain interpreted predicates  then containment testing is a little more involved  afrati 
li    mitra        

  

filearning semantic definitions of information sources on the internet

definition is the same as before  but with a few new attributes corresponding to the outputs
of the source   thus the new definition does not need to be evaluated  but can be added to
the queue  of definitions to expand  as is  which becomes particularly advantageous when
a sources input arity is high 
    constants
constants are often used in source descriptions to define the scope of a service  consider a
weather service that provides reports for zipcodes only in california 
calweather  zip   date  temp     forecast zip  date  temp   usstate zip  california  
if a mediator receives a query asking for the forecast for chicago  it will know that this
source is not relevant to the query since chicago is not in california  although constants
in source descriptions can be very useful  simply introducing them into the hypothesis
language could cause the search space to grow prohibitively   for states  the branching
factor would be     while for zipcodes it would be in excess of          obviously a generate
and test methodology does not make sense when the domain of the semantic type is large 
alternatively  one can explicitly check for repeated values in the tuples returned by the
new source  i e  for constants in the head of the clause  and in the join of the source
and definition relations  i e  for constants in the body of the clause   for example  in the
definition below the join of the source relation hzip  date  tempi with the definition relation
hzip  date  temp  statei would produce only tuples with state equal to california  so that
constant could be added to the definition 
source  zip   date  temp     forecast zip  date  temp   usstate zip  state  
more complicated detection procedures would be required for discovering constants in interpreted predicates  i e  range restrictions over numeric attributes  
    post processing
after a definition has been learnt for a new source  it may be possible to tighten that
definition by removing logical redundancies from its unfolding  consider the following
definition involving calls to two hotel sources  one to check availability and the other to
check its rating 
source  hotel  address  rating  hotelavailability  hotel  address  price   hotelrating  hotel  rating  address  
the unfolding of that definition  in terms of the definitions of the hotel sources  contains
two references to an accommodation relation 
source  hotel  address  rating  accommodation hotel    address   available hotel  today  price  
accommodation hotel  rating  address  
the first literal is redundant and can be removed from the unfolding  in general  the same
rules used to discover redundancy in candidate definitions can be used to remove redundant
literals from the unfolding  moreover  since this post processing step needs to be performed
only once  time can be spent searching for more complicated forms of redundancy 
  

ficarman   knoblock

   evaluation
in this section we describe our evaluation of the source induction algorithm  we first
describe the experimental setup used and then the experiments performed  finally  we
compare the induction algorithm with a particular complex schema matching system 
    experimental setup
the source induction algorithm defined in this paper was implemented in a system called
eidos  which stands for efficiently inducing definitions for online sources  eidos implements the techniques and optimisations discussed in sections   through     certain
extensions from section   were only partially implemented  the implementation currently
checks for constants only in the head of the clause and does not perform any tightening of
the definitions   all code was written in java and a mysql database was used for caching
the results of source invocations 
eidos was tested on    different problems involving real services from several domains
including hotels  financial data  weather and cars  the domain model used was the same
for each problem and included over    different semantic types  ranging from common
ones like zipcode to more specific types such as stock ticker symbols  the data model
also contained    relations  excluding interpreted predicates   which were used to model   
different services  all of the modeled services are publicly available information sources 
we note here that the decision to use the same set of known sources for each problem
 regardless of the domain  was important in order to make sure that the tests were realistic 
this decision made the problem more difficult than the standard schema matching mapping
scenario in which the source schema is chosen  because it provides data that is known a
priori to be relevant to the output schema 
in order to give a better sense of the problem setting and the complexity of the known
sources available  we list ten below  ordered by arity   due to space limitations we dont
show the complete list nor their definitions  only the input output types for each source 
note that all the sources share the semantic types latitude and longitude  which means that
the search space associated with these sources alone is very large 
 

weatherconditions  city state country latitude longitude time time timeoffset 
datetime temperaturef sky pressurein direction speedmph humidity temperaturef 
  weatherforecast  city state country latitude longitude timeoffset day date 
temperaturef temperaturef time time sky direction speedmph humidity 
  getdistance  latitude  longitude  latitude  longitude distancekm 
  usgeocoder  street  zipcode city state latitude longitude 
  convertdms  latitudedms  longitudedms latitude longitude 
  usgsearthquakes decimal timestamp latitude longitude 
  getairportcoords  iata airport latitude longitude 
  countrycode  latitude  longitude countryabbr 
  getcentroid  zipcode latitude longitude 
   altitude  latitude  longitude distancem 

in order to induce definitions for each problem  the source  and each candidate definition  was invoked at least    times using random inputs  whenever possible  the system
attempted to generate    positive examples of the source  invocations for which the source
returned some tuples  and    negative examples  inputs which produced no output   to
  

filearning semantic definitions of information sources on the internet

ensure that the search terminated  the number of iterations of the algorithm including backtracking steps was limited to     a search time limit of    minutes was also imposed  the
inductive search bias used during the experiments is shown below  and a weighting factor
 defined in section      of     was used to direct the search toward shorter definitions 
search bias
maximum clause length    
maximum predicate repetition    
maximum variable level    
executable candidates only
no variable repetition within a literal
in the experiments  different procedures were used to decide equality between values of the
same type as discussed in section      some of the equality procedures used for different
types are listed below  the accuracy bounds and thresholds used were chosen to maximize
overall performance of the learning algorithm   in practice  a meta learning algorithm
could be used to determine the best accuracy bounds for different attribute types   for all
semantic types not listed below  substring matching  checking if one string contained the
other  was used to test equality between values 
types
latitudes  longitudes
distances  speeds  temperatures  prices
humidity  pressure  degrees
decimals
companies  hotels  airports
dates

equality procedure
accuracy bound of      
accuracy bound of   
accuracy bound of    
accuracy bound of    
jarowinkler score      
specialised equality procedure

the experiments were run on a dual core     ghz pentium   with   gb of ram  although
memory was not a limiting factor in any of the tests   the system was running windows
     server  java runtime environment     and mysql     
    evaluation criteria
in order to evaluate the induction system  one would like to compare for each problem the
definition generated by the system with the ideal definition for that source  denoted vbest
and v  respectively   in other words  we would like an evaluation function  which rates
the quality of each definition produced with respect to a hand written definition for the
source  i e  quality   vbest  v            the problem with this is twofold  firstly  it is
not obvious how to define such a similarity function over conjunctive queries and many
different possibilities exist  see markov and marinchev        for a particular example  
secondly  working out the best definition by hand  while taking into account the limitations
of the domain model and the fact that the available sources are noisy  incomplete  possibly
less accurate  and even serialise data in different ways  may be extremely difficult  if even
possible  so in order to evaluate each of the discovered definitions  we instead count the
number of correctly generated attributes in each definition  an attribute is said to be
correctly generated  if 
  

ficarman   knoblock

 it is an input  and the definition correctly restricts the domain of possible values for
that attribute  or
 it is an output  and the definition correctly predicts its value for given input tuples 
consider the following definition that takes two input values and returns the difference and
its square root  providing the difference is positive  
source  a   b  c  d     sum b  c  a   product d  d  c   a  b 
and imagine that the induction system managed to learn only that the source returns the
difference between the input and the output  i e  
source  a   b  c       sum b  c  a  
we say that the first input attribute a is not correctly generated as it is an input and is not
constrained with respect to the input attribute b  the inequality is missing   the input b
is deemed correctly generated as it is present in the sum relation  only one input is penalised
for the missing inequality   the output c is deemed correctly generated with respect to
the inputs  and the missing attribute d isnt generated at all   note that if the ordering of
variables in the sum relation had been different  say sum a  b  c   then c would have been
generated  but not correctly generated  
given a definition for correctly generated attributes  one can define expressions for
precision and recall over the attributes contained in a source definition  we define precision
to be the ratio of correctly generated attributes to the total number of attributes generated
by a definition  i e  
precision  

  of correctly generated attributes
total   of generated attributes

we define recall to be the ratio of generated attributes to the total number of attributes
that would have been generated by the ideal definition  given the sources available   in
some cases no sources are available to generate values for an attribute in which case  that
attribute is not included in the count  
recall  

  of correctly generated attributes
total   of attributes that should have been generated

note that we defined precision and recall at the schema level in terms of the attributes
involved in a source definition  they could also be defined at the data level in terms of
the tuples being returned by the source and the definition  indeed  the jaccard similarity
used to score candidate definitions is a combination of data level precision and recall values 
the reason for choosing schema level metrics in our evaluation is that they better reflect
the semantic correctness of the learnt definition  in so far as they are independent of the
completeness  amount of overlap  between the known and target sources 
returning to our example above  the precision for the simple definition learnt would be
    and the recall would be      note that  if the product relation had not been available
in our domain model  in which case attribute d could never have been generated   recall
would have been higher at     
  

filearning semantic definitions of information sources on the internet

    experiments
the definitions learnt by the system are described below  overall the system performed
very well and was able to learn the intended definition  ignoring missing join variables and
superfluous literals  in    out of the    problems 
      geospatial sources
the first set of problems involved nine geospatial data sources providing a variety of location
based information  the definitions learnt by the system are listed below  they are reported
in terms of the source predicates rather than the domain relations  i e  the unfoldings 
because the corresponding definitions are much shorter  this makes it easier to understand
how well the search algorithm is performing 
 
 
 

 
 
 

 
 
 

getinfobyzip  zip  cit  sta    tim    gettimezone  sta  tim        getcitystate  zip  cit  sta   
getinfobystate  sta  cit  zip    tim    gettimezone  sta  tim        getcitystate  zip  cit  sta   
getdistancebetweenzipcodes  zip   zip  dis    getcentroid  zip  lat  lon    getcentroid  zip  lat  lon   
getdistance  lat   lon   lat   lon  dis     convertkm mi  dis   dis   
getzipcodeswithin     dis    dis      dis  dis   
yahoogeocoder  str   zip  cit  sta    lat  lon    usgeocoder  str   zip  cit  sta  lat  lon   
getcenter  zip  lat  lon  cit  sta    weatherconditions  cit  sta    lat  lon                         
getzipcode  cit   sta  zip   
earthquakes             lat  lon    dec      usgsearthquakes dec    lat  lon   
usgselevation  lat   lon  dis    convertft m  dis  dis    altitude  lat   lon  dis   
countryinfo  cou  cou  cit      cur            getcountryname  cou  cou    gocurrency cur  cou     
weatherconditions  cit    cou                             

the first two sources provide information about zipcodes  such as the name of the city  the
state and the timezone  they differ in their binding constraints  with the first taking a
zipcode as input  and the second taking a state  the second source returns many output
tuples per input value  making it harder to learn the definition  even though the two sources
provide logically the same information  the induced definitions are the best possible given
the known sources available   none of them provided the missing output attribute  a telephone area code   the third source calculates the distance in miles between two zipcodes 
 it is the same as source  from section     the correct definition was learnt for this source 
but for the next source  which returned zipcodes within a given radius  a reasonable definition could not be learnt within the time limit    ignoring binding constraints  the intended
    the recall for this problem is     because the input attribute dis  is determined to be the only correctly
generated attribute  it is constrained with respect to the output attribute dis     while all four attributes
should have been generated  the output attribute dis  is not generated by the   predicate   the precision
is     because dis  is the only generated attribute  and it is correctly generated 

  

ficarman   knoblock

definition was the same as the third  but with a restriction that the output distance be less
than the input distance  thus it would have been far easier for eidos to learn a definition
for the fourth source in terms of the third  indeed  when the new definition for the third
source was added to the set of known sources  the system was able to learn the following 
  getzipcodeswithin  zip   dis  zip  dis    getdistancebetweenzipcodes  zip   zip  dis      dis  dis   

the ability of the system to improve its learning ability over time as the set of known
sources increases is a key benefit of the approach 
source five is a geocoding service provided by yahoo   geocoding services map addresses to latitude and longitude coordinates   eidos learnt that the same functionality
was provided by a service called usgeocoder  source six is a simple service providing the
latitude longitude coordinates and the city and state for a given zipcode  interestingly 
the system learnt that the sources coordinates were better predicted by a weather conditions service  discussed in section         than by the getcentroid source from the third
definition  note that when the new source definition is unfolded it will contain extraneous predicates related to weather information    the additional predicates do not interfere
with the usefulness of the definition  however  as a query reformulation algorithm will still
use the source to answer the same queries regardless   thus precision and recall scores
are not affected   a post processing step to remove extraneous predicates is possible  but
would require additional information in the domain model    the seventh source provided
earthquake data within a bounding box which it took as input  in this case  the system
discovered that the source was indeed providing earthquake data   lat  and lon  are the coordinates of the earthquake and dec  is its magnitude   it didnt manage  however  to work
out how the input coordinates related to the output  the next source provided elevation
data in feet  which was found to be sufficiently similar to known altitude data in metres 
finally  the system learnt a definition for a source providing information about countries
such as the currency used  and the name of the capital city  since known sources were not
available to provide this information  the system ended up learning that weather reports
were available for the capital of each country 
problem
 
 
 
 
 
 
 
 
 

  candidates
  
  
   
 
  
  
  
  
   

  invocations
    
    
     
     
     
     
     
   
     

time  s 
  
   
   
  
   
   
  
  
   

log    score 
     
     
     
    
     
     
     
     
     

precision
   
   
   
   
   
   
   
   
   

recall
   
   
   
   
   
   
   
   
   

    the unfolding is shown below  the conditions predicate could be removed without affecting its meaning 
getcenter  zip   lat   lon   cit   sta     municipality cit   sta   zip   tim    country    cou     
northamerica cou    centroid zip   lat   lon    conditions lat   lon                         
timezone tim        municipality cit   sta   zip     
    in particular  universally quantified knowledge would be needed in the domain model  e g  
lat  lon x         x   s t  conditions lat  long  x         x    

  

filearning semantic definitions of information sources on the internet

the table shows some details regarding the search performed to learn each of the definitions listed above  for each problem  it shows the number of candidates generated prior
to the winning definition  along with the time and number of source invocations required
to learn the definition   the last two values should be interpreted with caution as they
are highly dependent on the delay in accessing sources  and on the caching of data in the
system   the scores shown in the fifth column are a normalised version of the scoring
function used to compare the definitions during search   normalisation involved removing
the penalty applied for missing outputs   the scores can be very small  so the logarithm
of the values is shown  hence the negative values     these scores can be interpreted as
the confidence the system has in the definitions produced  the closer the value is to zero 
the better the definitions ability to produce the same tuples as the source  we see that
the system was far more confident about the definitions one through five  than the latter
ones    the last two columns give the precision and recall value for each problem  the
average precision for these problems was        note that a high precision value is to be
expected  given that the induction algorithm relies on finding matching tuples between the
source and definition   the average recall for the geospatial problems was also very high at
    
      financial sources
two sources were tested that provided financial data  the definitions generated by eidos
for these sources are shown below 
   getquote  tic  pri  dat  tim  pri  pri  pri  pri  cou    pri       pri     com    yahoofinance  tic  pri  dat  tim  pri  pri  pri  pri  cou   
getcompanyname  tic  com        add  pri   pri   pri    add  pri   pri   pri   
   yahooexchangerate     cur  pri  dat    pri  pri    getcurrenttime     dat      gocurrency cur  cou  pri   
gocurrency   cou  pri    add  pri   pri  pri     add  pri   pri  pri    

the first financial service provided stock quote information  and the system learnt that the
source returned exactly the same information as a stock market service provided by yahoo 
it was also able to work out that the previous days close plus todays change was equal to
the current price  the second source provided the rate of exchange between the currencies
given as input  in this case  the system did not fare well  it was unable to learn the intended
result  which involved calculating the exchange rate by taking the ratio of the values for the
first and second currency 
problem
  
  

  candidates
    
   

  invocations
     
     

time  s 
   
   

log    score 
     
     

precision
     
   

recall
     
   

details regarding the search spaces for the two problems are shown above  the average
precision and recall for these problems were much lower at     and     respectively 
because the system was unable to learn the intended definition in the second problem 
    the positive value for problem   results from an approximation error 
    low scores and perfect precision and recall  problems      and    indicate very little overlap between
the target and the known sources  the fact that the system learns the correct definition in such cases is
testimony to the robustness of the approach 

  

ficarman   knoblock

      weather sources
on the internet  there are two types of weather information services  those that provide
forecasts for coming days  and those that provide details of current weather conditions  in
the experiments  a pair of such services provided by weather com were used to learn definitions for a number of other weather sources  the first set of definitions  which correspond
to sources that provide current weather conditions  are listed below 
   noaaweather  ica  air      sky  tem  hum  dir  spe    pre   tem         getairportinfo  ica    air  cit       
weatherforecast  cit                        sky  dir       
weatherconditions  cit                  tem  sky  pre     spe  hum  tem    
convertin mb  pre   pre    
   wunderground  sta   cit  tem      pre  pre  sky  dir  spe  spe     weatherconditions  cit  sta              dat  tem  sky  pre  dir  spe     tem   
weatherforecast  cit  sta              tem             spe      
convertin mb  pre  pre     tem  tem    converttime  dat             spe  spe    
   weatherbuglive    cit  sta  zip  tem      dir        weatherconditions  cit  sta                tem      dir         
getzipcode  cit   sta  zip   
   weatherfeed  cit     tem    sky  tem      pre  lat        weatherconditions  cit      lat              sky  pre  dir       tem   
weatherforecast  cit                  tem        dir        
   weatherbyicao  ica  air  cou  lat  lon    dis    sky            altitude  lat   lon  dis    getairportinfo  ica    air  cit    cou   
weatherforecast  cit    cou                    sky         
getcountryname  cou  cou   
   weatherbylatlon             lat  lon    dis                altitude  lat   lon  dis   

in the first problem  the system learnt that source    provided current conditions at airports 
by checking the weather report for the cities in which each airport was located  this
particular problem demonstrates some of the advantages of learning definitions for new
sources described in section      once the definition has been learnt  if a mediator receives
a request for the current conditions at an airport  it can generate an answer for that query
by executing a single call to the newly modeled source   without needing to find a nearby
city   the system performed well on the next three sources     to     learning definitions
which cover most of the attributes of each  on the last two problems  the system did not
perform as well  in the case of source     the system spent most of its time learning which
attributes of the airport were being returned  such as its country  coordinates  elevation 
etc    in the last case  the system was only able to learn that the source was returning some
coordinates along with their elevation  we note here that different sources may provide
data at different levels of accuracy  thus the fact that the system is unable to learn a
definition for a particular source could simply mean that the data being returned by that
source wasnt sufficiently accurate for the system to label it a match 
in addition to current weather feeds  the system was run on two problems involving
weather forecast feeds  it did very well on the first problem  matching all bar one of the
attributes  the country  and finding that the order of the high and low temperatures was
  

filearning semantic definitions of information sources on the internet

inverted  it did well also for the second problem  learning a definition for the source that
produced most of the output attributes 
   yahooweather  zip  cit  sta    lat  lon  day  dat  tem  tem  sky     weatherforecast  cit  sta    lat  lon    day  dat  tem  tem      sky          
getcitystate  zip  cit  sta   
   weatherbugforecast    cit  sta    day  sky  tem      weatherforecast  cit  sta        tim  day    tem    tim     sky         
weatherconditions  cit          tim     tim                   

details regarding the number of candidates generated in order to learn definitions for the
different weather sources are shown below  the average precision of the definitions produced
was      while the average recall was     
problem
  
  
  
  
  
  
  
  

  candidates
   
    
  
   
   
  
   
   

  invocations
   
   
    
   
   
    
     
     

time  s 
   
   
   
   
   
    
   
    

log    score 
     
     
      
     
      
      
     
      

precision
   
   
   
   
   
   
     
   

recall
    
    
   
    
   
    
     
   

      hotel sources
definitions were also learnt for sources providing hotel information from yahoo  google and
the us fire administration  these definitions are shown below 
   usfirehotelsbycity  cit      sta  zip  cou      hotelsbyzip  zip      cit  sta  cou   
   usfirehotelsbyzip  zip      cit  sta  cou      hotelsbyzip  zip      cit  sta  cou   
   yahoohotel  zip     hot  str  cit  sta              hotelsbyzip  zip  hot  str  cit  sta     
   googlebasehotels  zip    cit  sta      lat  lon      weatherconditions  cit  sta    lat  lon                         
getzipcode  cit   sta  zip   

the system performed well on three out of the four problems  it was unable in the time
allocated to discover a definition for the hotel attributes  name  street  latitude and longitude  returned by the google web service  the average precision for these problems was
    while the average recall was     
problem
  
  
  
  

  candidates
  
  
  
  

  invocations
   
    
    
    

time  s 
  
 
   
    
  

log    score 
     
     
     
     

precision
   
   
   
   

recall
   
   
   
   

ficarman   knoblock

      cars and traffic sources
the last problems on which the system was tested were a pair of traffic related web services 
the first service  provided by yahoo  reported live traffic data  such as accidents and
construction work  within a given radius of the input zipcode  no known sources were
available which provided such information  so not surprisingly  the system was unable to
learn a definition for the traffic related attributes of that source   instead  the system
discovered a relationship between the input zipcode and the output longitude that wasnt
correct  so precision for this problem was zero  
   yahootraffic  zip       lat  lon          getcentroid  zip    lon    countrycode  lat   lon     
   yahooautos  zip   mak  dat  yea  mod      pri      googlebasecars  zip   mak    mod  pri      yea   
converttime  dat    dat         getcurrenttime     dat      

the second problem involved a classified used car listing from yahoo that took a zipcode
and car manufacturer as input  eidos was able to learn a good definition for that source 
taking advantage of the fact that some of the same cars  defined by their make  model  year
and price  were also listed for sale on googles classified car listing 
problem
  
  

  candidates
  
  

  invocations
     
   

time  s 
    
   

log    score 
      
     

precision
   
   

recall
   
   

since the system failed on the first problem  it found some incorrect non general relationships between different attributes   but succeeded on the second problem to find the best
possible definition  the average precision and recall for these problems were both     
      overall results
across the    problems  eidos managed to generate definitions with high accuracy  average
precision was      and a large number of attributes  average recall was       these results
are promising  especially considering that all problems involved real data sources with in
some cases very small overlap between the data produced by the target and that provided
by the known sources  as evidenced by low logarithmic scores   in addition to minimal
overlap  many sources provided incomplete tuples  i e  tuples containing multiple null
or n a values  as well as erroneous or inaccurate data  making the problem all the more
difficult  the high average precision and recall lead us to believe that the jaccard measure
is doing a good job of distinguishing correct from incorrect definitions in the presence of
data sources that are both noisy  inconsistent  and incomplete  missing tuples and values  
comparing the different domains  one can see that the system performed better on
problems with fewer input and output attributes  such as the geospatial problems   which
was to be expected given that the resulting search space is much smaller 
  

filearning semantic definitions of information sources on the internet

    empirical comparison
having demonstrated the effectiveness of eidos in learning definitions for real information
services  we now show that the system is capable of handling the same problems as a
well known complex schema matching system 
the imap system  dhamanka  lee  doan  halevy    domingos         discussed in
section      is a schema matcher that can learn complex  many to one  mappings between
the concepts of a source and a target schema  it uses a set of special purpose searchers to
learn different types of mappings  the eidos system  on the other hand  uses a generic
search algorithm to solve a comparable problem  since the two systems can be made to
perform a similar task  we show that eidos is capable of running on one of the problem
domains used in the evaluation of imap  we chose the particular domain of online cricket
databases because it is the only one used in the evaluation that involved aligning data from
two independent data sources   all other problems involved generating synthetic data by
splitting a single database into a source and target schema  which would not have been as
interesting for eidos  
player statistics from two online cricket databases  cricketbase com and cricinfo com 
were used in the experiments  since neither of the sources provided programmatic access
to their data  the statistics data was extracted from html pages and inserted into a
relational database  the extraction process involved flattening the data into a relational
model and a small amount of data cleaning   the resulting tables are similar but not
necessarily exactly the same as those used in the imap experiments   the data from the
two websites was used to create three data sources representing each website  the three
sources representing cricinfo com were then used to learn definitions for the three sources
representing cricketbase com  other known sources were available to the system  including
functionality for splitting apart comma separated lists  adding and multiplying numbers 
and so on  the definitions learnt to describe the cricketbase services are shown below 
  cricbaseplayers  cou  nam    dat    unk  unk    cricinfoplayer  nam  dat        lis  nam  unk  unk    contains  lis  cou   
cricinfotest  nam                                             
  cricbasetest    nam  cou        cou  cou  dec  cou        cou   cou   dec     
dec   cou         cricinfotest  nam      cou  cou  cou   cou    dec  dec     cou        cou   
cou           dec    
  cricbaseodi    nam  cou        cou  cou  dec  cou  cou     cou   cou     dec   
dec   dec   cou   cou       cricinfoodi  nam      cou  cou    cou     dec    cou  cou   cou   cou      
cou   cou   dec     dec   dec    

the first source provided player profiles by country  the second and third sources provided
detailed player statistics for two different types of cricket  test and one day international
respectively   the system easily found the best definition for the first source  the definition
involved looking for the players country in a list of teams that he played for  eidos did
not perform quite as well on the second and third problems  there were two reasons for
this  firstly  the arity of these sources was much higher with many instances of the same
semantic type  count and decimal    making the space of possible alignments much larger 
  

ficarman   knoblock

 because of the large search space  a longer timeout of    minutes was used   secondly 
a high frequency of null values  the constant n a  in the data for some of the fields
confused the algorithm  and made it harder for it to discover overlapping tuples with all of
the desired attributes 
problem
 
 
 

  candidates
   
    
    

  invocations
    
    
    

time  s 
   
    
    

log    score 
     
     
     

precision
   
    
    

recall
   
    
    

details of the search performed to learn the definitions are shown above  the average
precision for these problems was     while the average recall was lower at      these
values are comparable to the quality of the matchings reported for imap    these results are
very good  considering that eidos searches in the space of many to many correspondences 
 trying to define the set of target attributes contemporaneously   while imap searches the
spaces of one to one and many to one correspondences  moreover  eidos first invokes the
target source to generate representative data  a task not performed by imap  and then
performs a generic search for reasonable definitions without relying on specialised search
algorithms for different types of attributes  as is done in imap  

   related work
in this section we describe how the work in this paper relates to research performed by
the machine learning  database and the semantic web communities  before doing that 
we describe some early work performed by the artificial intelligence community  we also
discuss how our algorithm differs from standard ilp techniques  and in particular why a
direct application of such techniques was not possible for our problem 
    an early approach
the first work concerned with learning models for describing operations available on the
internet was performed  in the pre xml era  on a problem called category translation
 perkowitz   etzioni        perkowitz  doorenbos  etzioni    weld         this problem
consisted of an incomplete internal world model and an external information source with
the goal being to characterize the information source in terms of the world model  the
world model consisted of a set of objects o  where each object o  o belonged to a certain
category  e g  people  and was associated with a set of attributes ha   o        an  o i  made up
of strings and other objects  a simple relational interpretation of this world model would
consider each category to be a relation  and each object to be a tuple  the information
source  meanwhile  was an operation that took in a single value as input and returned a
single tuple as output  the category translation problem can be viewed as a simplification
of the source definition induction problem  whereby 
    the actual values for precision and recall in the cricket domain are not quoted  but an accuracy range
of        for simple matches  one to one correspondences between source and target fields  and       
for complex matches  many to one correspondences  across synthetic and real problems was given 

  

filearning semantic definitions of information sources on the internet

 the extensions of the global relations are explicit   there is one source per global
relation  and it doesnt have binding constraints  i e  r   s  
 the information provided by the sources does not change over time 
 the new source takes a single value as input and returns a single tuple as output 
in order to find solutions to instances of the category translation problem  the authors employed a variant of relational path finding  richards   mooney         which is an extension
on the foil algorithm  to learn models of the external source  the technique described in
this paper for solving instances of the source induction problem is similar in that it too is
based on a foil like inductive search algorithm 
    direct application of ilp techniques
researchers became interested in the field of inductive logic programming in the early
nineties  resulting in a number of different ilp systems being developed including foil
 cameron jones   quinlan         progol  muggleton        and aleph     ideally  one
would like to apply such off the shelf ilp systems to the source definition induction
problem  a number of issues  however  limit the direct applicability of these systems  the
issues can be summarised as follows 





extensions of the global relations are virtual 
sources may be incomplete with respect to their definitions 
explicit negative examples of the target are not available 
sources may serialise constants in different ways 

the first issue has to do with the fact that all ilp systems assume that there is an extensional
definition of the target predicate and extensional  or in some cases intentional  definitions
of the  source  predicates that will be used in the definition for the target  in other words 
they assume that tables already exist in some relational database to represent both the
new source and the known sources  in our case  we need to generate such tables by first
invoking the services with relevant inputs  one could envisage invoking each of the sources
with every possible input and using the resulting tables to perform induction  such a direct
approach would not be feasible for two reasons  firstly  a complete set of possible input
values may not be known to the system  secondly  even if it is possible to generate a
complete set of viable inputs to a service  it may not be practical to query the source with
such a large set of tuples  consider source  from section    which calculates the distance in
miles between two zipcodes  given that there are over        zipcodes in the us  generating
an extensional representation of this source would require performing more than a billion
invocations  performing such a large number of invocations does not make sense when a
small number of example invocations would suffice for characterising the functionality of
the source  in this paper we have developed an efficient algorithm that only queries the
sources as needed in order to evaluate individual candidate definitions 
the second issue regarding the incompleteness of the sources causes a problem when a
candidate is to be evaluated  since the set of tuples returned by each known source may only
    see the aleph manual by ashwin srinivasan  which is available at 
http   web comlab ox ac uk oucl research areas machlearn aleph aleph html

  

ficarman   knoblock

be a subset of those implied by its own definition  so too will be the set of tuples returned
by the candidate hypothesis when executed against those sources  this means that when
the system tries to evaluate a hypothesis by comparing those tuples with the output of the
new source  it cannot be sure that a tuple which is produced by the new source but not by
the hypothesis is in fact not logically implied by it  this fact is taken into account in our
evaluation function for scoring candidate definitions  discussed in section   
the third issue regarding the lack of explicit negative examples for the target predicate
also affects the evaluation of candidate hypotheses  the classic approach to dealing with
this problem is to assume a closed world  in which all tuples  over the head relation  which
are not explicitly declared to be positive must be negative  since the new source may in
fact be incomplete with respect to the best possible definition for it  this assumption does
not necessarily hold  in other words  just because a particular tuple is produced when the
candidate definition is executed and that same tuple is not returned by the new source does
not necessarily mean that the candidate definition is incorrect 
the fourth issue has to do with the fact that the data provided by different sources may
need to be reconciled  in the sense that different serialisations of  strings representing  the
same value  such as monday and mon for instance  must be recognized  since ilp
systems have been designed to operate over a single database containing multiple tables 
the issue of heterogeneity in the data is not handled by current systems  in section     we
discussed how this heterogeneity is resolved in our system 
    machine learning approaches
since the advent of services on the internet  researchers have been investigating ways to
model them automatically  primarily  interest has centered on using machine learning
techniques to classify the input and output types of a service  so as to facilitate service
discovery  he   kushmerick proposed using a support vector machine to classify the
input and output attributes into different semantic types based on metadata in interface
descriptions  he   kushmerick               their notion of semantic types  such as
zipcode  as opposed to syntactic types  like integer   went some way toward defining the
functionality that a source provides  recently  other researchers  lerman et al        
proposed the use of logistic regression for assigning semantic types to input parameters
based on metadata  and a pattern language for assigning semantic types to the output
parameters based on the data the source produces  this work on classifying input and
output attributes of a service to semantic types forms a prerequisite for the work in this
article  for the purposes of this paper  we have assumed that this problem has been solved 
in addition to classifying the input output attributes of services  he   kushmerick
investigated the idea of classifying the services themselves into different service types  more
precisely  they used the same classification techniques to assign service interfaces to different semantic domains  such as weather and flights  and the operations that each interface
provides to different classes of operation  such as weatherforecast and flightstatus   the
resulting source description  hypothesis  language is limited to select project queries  which
are not sufficiently expressive to describe many of the sources available on the internet 
according to that approach  since every operation must be characterized by a particular operation class  operations that provide overlapping  non identical  functionality would
  

filearning semantic definitions of information sources on the internet

need to be assigned different classes as would operations which provide composed functionality  such as  for example  an operation that provides both weather and flight data   the
need for an exhaustive set of operation classes  and accompanying training data  is a major
limitation of that approach  not shared by the work described in this paper  which relies on
a more expressive language for describing service operations 
one way to eliminate the need for a predefined set of operation types is to use unsupervised clustering techniques to generate the  operation  classes automatically from examples
 wsdl documents   this idea was implemented in a system called woogle  dong et al  
       the system clustered service interfaces together using a similarity score based on the
co occurrence of metadata labels  it then took advantage of the clusters produced to improve keyword based search for web services  an advantage of this unsupervised approach
is that no labeled training data is required  which can be time consuming to generate  such
clustering approaches  however  while useful for service discovery  suffer the same limitations
as the previous approach when it comes to expressiveness 
    database approaches
the database community has long been interested in the problem of integrating data from
disparate sources  specifically  in the areas of data warehousing  widom        and information integration  wiederhold         researchers are interested in resolving semantic
heterogeneity which exists between different databases so that the data can be combined or
accessed via a single interface  the schema mapping problem is the problem of determining
a mapping between the relations contained in a source schema and a particular relation in
a target schema  a mapping defines a transformation which can be used to populate the
target relation with data from the source schema  mappings may be arbitrarily complex
procedures  but in general they will be declarative queries in sql or datalog  the complexity of these queries makes the schema mapping problem far more difficult than the highly
investigated schema matching problem  rahm   bernstein         which involves finding
  to   correspondences between fields of a source and target schema 
the source definition induction problem can be viewed as a type of schema mapping
problem  in which the known sources define the source schema and the unknown source
specifies the target relation  in order to solve a schema mapping problem  one typically
takes advantage of all available auxiliary information  including source and target data
instances  labels from the respective schemas  and so on   such problems are generally
simpler  however  because the data  the extensions of the relations  in the source and target
schema are usually explicitly available  in source induction  that data is hidden behind a
service interface  which has binding constraints  and the data itself can be extremely large
or even  in the case of sources providing mathematical functions  infinite  thus making the
problem considerably more difficult 
the schema integration system clio  yan  miller  haas    fagin        helps users
build sql queries that map data from a source to a target schema  in clio  foreign keys
and instance data are used to generate integration rules semi automatically  since clio
relies heavily on user involvement  it does not make sense to compare it directly with the
automated system developed in this paper 
  

ficarman   knoblock

another closely related problem is that of complex schema matching  the goal of which is
to discover complex  many to one  mappings between two relational tables or xml schemas 
this problem is far more complicated than basic  one to one  schema matching because 
 the space of possible correspondences between the relations is no longer the cartesian
product of the source and target relations  but the powerset of the source relation times
the target relation 
 many to one mappings require a mapping function  which can be simple like concatenate x y z   or an arbitrarily complex formula such as z   x    y 
the imap system  dhamanka et al         tries to learn such many to one mappings between the concepts of a set of source relations and a target relation  it uses a set of special
purpose searchers to learn different types of mappings  such as mathematical expressions 
unit conversions and time date manipulations   it then uses a meta heuristic to control
the search being performed by the different special purpose searchers  if one views both
the source schema and the functions available for use in the mappings  such as concatenate x y z   add x y z   etc   as the set of known sources in the source definition induction
problem  then the complex schema matching and source induction problems are somewhat
similar  the main differences between the problems are 
 the data associated with the source schema is explicit  and static  in complex schema
matching  while it is hidden  and dynamic  in source induction 
 in general  the set of known sources in a source induction problem will be much larger
 and the data they provide may be less consistent   than the set of mapping functions
and source relations in a complex schema matching problem 
in this paper we develop a general framework for handling the source induction problem 
since imap provides functionality which is similar to that of our system  we perform a
simple empirical comparison in section     
    semantic web approach
the stated goal of the semantic web  berners lee  hendler    lassila        is to enable
machine understanding of web resources  this is done by annotating those resources with
semantically meaningful metadata  thus the work described in this paper is very much
in line with the semantic web  in so far as we are attempting to discover semantically
meaningful definitions for online information sources  de facto standards for annotating
services with semantic markup have been around for a number of years  these standards
provide service owners with a metadata language for adding declarative statements to service
interface descriptions in an attempt to describe the semantics of each service in terms of
the functionality  e g  a book purchase operation  or data  e g  a weather forecast  that it
provides  work on these languages is related to this article from two perspectives 
 it can be viewed as an alternative approach to gaining knowledge as to the semantics
of a newly discovered source  providing it has semantic metadata associated with it  
 semantic web service annotation languages can be seen as a target language for the
semantic descriptions learnt in this paper 
  

filearning semantic definitions of information sources on the internet

if a web service is already semantically annotated  heterogeneity may still exist between
the ontology used by the service provider and that used by the consumer  in which case the
learning capabilities described in this paper may be required to reconcile those differences 
more importantly  we are interested in the vast number of sources for which semantic
markup is currently unavailable  the work in this article complements that of the semantic
web community by providing a way of automatically annotating sources with semantic
information  thereby relieving service providers of the burden of manually annotating their
services  once learnt  datalog source definitions can be converted to description logicbased representations such as is used in owl s  martin  paolucci  mcilraith  burstein 
mcdermott  mcguinness  parsia  payne  sabou  solanki  srinivasan    sycara        and
wsmo  roman  keller  lausen  de bruijn  lara  stollberg  polleres  feier  bussler   
fensel         the reason we use datalog in this paper  rather than description logics  is
that most mediator based integration systems rely on it as a representation language 

   discussion
in this paper we have presented a completely automatic approach to learning definitions for
online services  our approach exploits the definition of sources that have either been given
to the system or learned previously  the resulting framework is a significant advance over
prior approaches that have focused on learning only the inputs and outputs or the class of
a service  we have demonstrated empirically the viability of the approach 
the key contribution of this article is a procedure for learning semantic definitions for
online information services that is 
 fully automated   definitions are learnt in a completely automated manner without
the need for any user intervention 
 more expressive  the query language for defining sources is that of conjunctive
queries  which is far more expressive than previous attribute value approaches 
 sufficiently robust  the procedure is able to learn definitions in the presence of noisy
and incomplete data  and thus is sufficiently robust to handle real data sources 
 data access efficient  the procedure samples data from live sources  invoking them
sparingly and only as required  making it highly efficient in terms of source accesses 
 evolving  the procedures ability to learn definitions improves over time as each new
definition is learnt and added to the set of known sources 
    application scenarios
there are a number of different application scenarios for a system that is capable of learning
definitions for online sources  they generally involve providing semantic definitions to data
integration systems  which then exploit and integrate the available sources 
the most obvious application for our work would be a system  depicted on the left side
of figure    that crawls the web  searching for information sources  upon finding a source 
the system would use a classifier to assign semantic types to it  followed by the inductive
learner to generate a definition for it  the definition could then be used to annotate the
source for the semantic web  or by a mediator for answering queries  importantly  this
entire process could run with minimal user involvement 
  

ficarman   knoblock

figure    architecture diagrams for three different application scenarios 

a more challenging application scenario  shown in the center of figure    would involve
real time service discovery  consider the case where a mediator is unable to answer a
particular query because the desired information lies out of scope of the sources available 
a search is then performed based on the missing conjuncts  relation names and constants 
from the query using a specialised web service search engine  such as woogle  dong et al  
       the services returned would be annotated with semantic types and  if possible 
semantic definitions  after the definitions are provided to the mediator  it would complete
the query processing and return an answer to the user  this scenario may seem a little farfetched until one considers a specific example  imagine a user interacting with a geospatial
browser  an online atlas   if the user turns on a particular information layer  such as ski
resorts  but no source is available for the current field of view  of  for instance  italy   then
no results would be displayed  in the background a search could be performed and a new
source discovered  which provides ski resorts all over europe  the relevant data could then
be displayed  with the user unaware that a search has been performed 
perhaps the most likely application scenario  to the right of figure    for a source
induction system would be a mixed initiative one  in this case a human would annotate
the different operations of a service interface with semantic definitions  at the same time 
the system would attempt to induce definitions for the remaining operations  and prompt
the user with suggestions for them  in this scenario the classifier may not be needed 
since attributes of the same name in the different operations would likely have the same
semantic type  moreover  since the definitions learnt by the system may in some cases
contain erroneous or superfluous predicates  the user could also be involved in a process of
checking and improving the definitions discovered 
  

filearning semantic definitions of information sources on the internet

    opportunities for further research
a number of future directions for this work will allow these techniques to be applied more
broadly  we now discuss two such directions  improving the search algorithm and extending
the query language 
as the number of known sources grows  so too will the search space  and it will be necessary to develop additional heuristics to better direct the search toward the best definition 
many heuristic techniques have been developed in the ilp community and some may be
applicable to the source induction problem  more pressing perhaps is the need to develop a
robust termination condition for halting the search once a sufficiently good definition has
been discovered  as the number of available sources increases  the simple timeout used in
the experiments will be ineffective as certain  more complicated  definitions will necessarily
take longer to learn than others 
another way to increase the applicability of this work is to extend the query language so
that it better describes the sources available  often online sources do not return a complete
set of results but rather cut off the list at some maximum cardinality  for example the
yahoohotel source described in section       returns a maximum of    hotels near a given
location  and orders them according to distance  in this case  recognising the specific
ordering on the tuples produced would be very useful to a mediator  a second useful
extension to the query language would be the ability to describe sources using the procedural
construct if then else  this construct is needed to describe the behaviour of some sources
on certain inputs  for example  consider the yahoogeocoder from section        which takes
as input a tuple containing a street name  number  and zipcode  if the geocoder is unable
to locate the corresponding address in its database  because it doesnt exist   instead of
returning no tuples  it returns the centroid of the zipcode  describing such behavior is only
possible using procedural constructs 

acknowledgments
this research is based upon work supported in part by the defense advanced research
projects agency  darpa   through the department of the interior  nbc  acquisition services division  under contract no  nbchd        the u s  government is authorized
to reproduce and distribute reports for governmental purposes notwithstanding any copyright annotation thereon  the views and conclusions contained herein are those of the
authors and should not be interpreted as necessarily representing the official policies or
endorsements  either expressed or implied  of any of the above organizations or any person
connected with them 

references
afrati  f  n   li  c     mitra  p          on containment of conjunctive queries using arithmetic comparisions  in  th international conference on extending database technology  edbt       heraklion crete  greece 
  

ficarman   knoblock

arens  y   knoblock  c  a     shen  w  m          query reformulation for dynamic
information integration  journal of intelligent information systems   special issue on
intelligent information integration                 
berners lee  t   hendler  j     lassila  o          the semantic web  scientific american 
              
bilenko  m   mooney  r  j   cohen  w  w   ravikumar  p     fienberg  s  e         
adaptive name matching in information integration   ieee intelligent systems         
     
cameron jones  r  m     quinlan  j  r          efficient top down induction of logic
programs  sigart bulletin              
carman  m  j     knoblock  c  a          learning semantic descriptions of web information sources  in proceedings of the twentieth international joint conference on
artificial intelligence  ijcai     hyderabad  india 
chandra  a  k     merlin  p  m          optimal implementation of conjunctive queries
in relational data bases  in proceedings of the  th acm symposium on theory of
computing  stoc   pp       boulder  colorado 
dhamanka  r   lee  y   doan  a   halevy  a     domingos  p          imap  discovering
complex semantic matches between database schemas  in sigmod     proceedings
of the      acm sigmod international conference on management of data 
dong  x   halevy  a  y   madhavan  j   nemes  e     zhang  j          simlarity search
for web services  in proceedings of vldb 
duschka  o  m          query planning and optimization in information integration  ph d 
thesis  department of computer science  stanford university 
garcia molina  h   hammer  j   ireland  k   papakonstantinou  y   ullman  j     widom 
j          integrating and accessing heterogeneous information sources in tsimmis  in
proceedings of the aaai symposium on information gathering  pp        
he  a     kushmerick  n          learning to attach semantic metadata to web services 
in  nd international semantic web conference  iswc  
he  a     kushmerick  n          iterative ensemble classification for relational data 
a case study of semantic web services  in   th european conference on machine
learning  ecml      pisa  italy  springer 
knoblock  c  a   minton  s   ambite  j  l   ashish  n   muslea  i   philpot  a     tejada 
s          the ariadne approach to web based information integration  international
journal of cooperative information systems                   
lerman  k   plangprasopchok  a     knoblock  c  a          automatically labeling data
used by web services  in proceedings of the   st national conference on artificial
intelligence  aaai  
  

filearning semantic definitions of information sources on the internet

levy  a  y          logic based techniques in data integration  in minker  j   ed    logicbased artificial intelligence  kluwer publishers 
levy  a  y   mendelzon  a  o   sagiv  y     srivastava  d          answering queries using
views  in proceedings of the   th acm sigact sigmod sigart symposium on
principles of database systems  pp        san jose  calif 
markov  z     marinchev  i          metric based inductive learning using semantic height
functions  in proceedings of the   th european conference on machine learning
 ecml        springer 
martin  d   paolucci  m   mcilraith  s   burstein  m   mcdermott  d   mcguinness  d  
parsia  b   payne  t   sabou  m   solanki  m   srinivasan  n     sycara  k         
bringing semantics to web services  the owl s approach  in proceedings of the first
international workshop on semantic web services and web process composition
 swswpc       
muggleton  s     feng  c          efficient induction of logic programs  in proceedings of
the  st conference on algorithmic learning theory 
muggleton  s          inverse entailment and progol  new generation computing  special
issue on inductive logic programming                   
nedellec  c   rouveirol  c   ade  h   bergadano  f     tausend  b          declarative
bias in ilp  in de raedt  l   ed    advances in inductive logic programming  pp 
       ios press 
pazzani  m  j     kibler  d  f          the utility of knowledge in inductive learning 
machine learning          
perkowitz  m     etzioni  o          category translation  learning to understand information on the internet  in proceedings of the fourteenth international joint conference
on artificial intelligence  ijcai     
perkowitz  m   doorenbos  r  b   etzioni  o     weld  d  s          learning to understand information on the internet  an example based approach  journal of intelligent
information systems                
pottinger  r     halevy  a  y          minicon  a scalable algorithm for answering queries
using views  vldb journal           
quinlan  j  r     cameron jones  r  m          foil  a midterm report  in machine
learning  ecml     european conference on machine learning  proceedings  vol 
     pp       springer verlag 
rahm  e     bernstein  p          a survey of approaches to automatic schema matching 
vldb journal         
richards  b  l     mooney  r  j          learning relations by pathfinding  in national
conference on artificial intelligence  pp       
  

ficarman   knoblock

roman  d   keller  u   lausen  h   de bruijn  j   lara  r   stollberg  m   polleres  a  
feier  c   bussler  c     fensel  d          web service modeling ontology  applied
ontology               
ullman  j  d          principles of database and knowledge base systems  vol     computer science press  rockville  maryland 
weber  i   tausend  b     stahl  i          language series revisited  the complexity of
hypothesis spaces in ilp  in proceedings of the  th european conference on machine
learning  vol       pp          springer verlag 
widom  j          research problems in data warehousing  in cikm     proceedings of
the fourth international conference on information and knowledge management  pp 
      acm press 
wiederhold  g          mediators in the architecture of future information systems  computer               
wiederhold  g   ed            intelligent integration of information  kluwer academic
publishers  boston ma 
winkler  w          the state of record linkage and current research problems  tech  rep  
statistical research division  u s  bureau of the census  washington  dc 
yan  l  l   miller  r  j   haas  l  m     fagin  r          data driven understanding
and refinement of schema mappings  in sigmod     proceedings of the      acm
sigmod international conference on management of data 
zelle  j  m   thompson  c  a   califf  m  e     mooney  r  j          inducing logic
programs without explicit negative examples  in proceedings of the fifth international
workshop on inductive logic programming 

  

fi