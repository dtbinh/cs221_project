journal artificial intelligence research               

submitted        published      

topic based dissimilarity sensitivity models
translation rule selection
min zhang

minzhang   suda   edu   cn

provincial key laboratory computer information processing technology 
soochow university  suzhou  china

xinyan xiao

xiaoxinyan   ict  ac   cn

iip key lab  institute computing technology 
chinese academy sciences  china

deyi xiong

dyxiong   suda   edu   cn

provincial key laboratory computer information processing technology 
soochow university  suzhou  china

qun liu

liuqun   ict  ac   cn

cngl  school computing  dublin city university  ireland
iip key lab  institute computing technology 
chinese academy sciences  china

abstract
translation rule selection task selecting appropriate translation rules ambiguous
source language segment  translation ambiguities pervasive statistical machine translation  introduce two topic based models translation rule selection incorporates global
topic information translation disambiguation  associate synchronous translation rule
source  target side topic distributions with topic distributions  propose topic
dissimilarity model select desirable  less dissimilar  rules imposing penalties rules
large value dissimilarity topic distributions given documents  order encourage use non topic specific translation rules  present topic sensitivity model
balance translation rule selection generic rules topic specific rules  furthermore 
project target side topic distributions onto source side topic model space benefit
topic information source target language  integrate proposed topic dissimilarity sensitivity model hierarchical phrase based machine translation synchronous
translation rule selection  experiments show topic based translation rule selection model
substantially improve translation quality 

   introduction
translation rules bilingual segments  establish translation equivalences source
target language  widely used statistical machine translation  smt  various representations ranging word pairs bilingual phrases synchronous rules word   phraseand syntax based smt respectively  normally  large number translation rules learnt
bilingual training data single source segment occurs different contexts 
example  xiong  zhang  li        observe chinese verb translated
   segment defined string terminals and or nonterminals 

c
    
ai access foundation  rights reserved 

fiz hang   x iao   x iong     l iu

    different translation rules average  therefore select appropriate translation
rule ambiguous source segment crucial issue smt 
traditionally appropriateness translation rule measured multiple probabilities
estimated word aligned data  bidirectional translation probabilities  koehn  och   
marcu         probabilities fail capture local global contexts highly ambiguous
source segments  sufficient select correct translation rules segments  therefore various approaches proposed capture rich contexts sentence level help
select proper translation rules phrase   carpuat   wu      a  syntax based smt  chan  ng 
  chiang        he  liu    lin        liu  he  liu    lin         studies show local
features  surrounding words  syntactic information on  helpful translation rule
selection 
beyond contextual features sentence level  conjecture translation rules
related high level global information  topic  hofmann        blei  ng    jordan 
      information document level  order visualize relatedness translation
rules document topics  show four hierarchical phrase based translation rules topic
distributions figure    figure  observe
first  translation rules divided two categories terms topic distributions 
topic sensitive rules  i e   topic specific rules  topic insensitive rules  i e   non topic specific generic rules   former rules  e g   translation rule  a    b   d  figure
   much higher distribution probabilities specific topics topics 
latter rules  e g   translation rule  c  figure    even distribution topics 
second  topic information used disambiguate ambiguous source segments  figure
   translation rule  b   c  source segment  however topic distributions
quite different  rule  b  distributes topic international relations
highest probability  suggests rule  b  much related topic
topics  contrast  rule  c  even distribution topics  therefore document
international relations  rule  b  appropriate rule  c  source
x   
segment



two observations suggest different translation rules different topic distributions
document level topic information used benefit translation rule selection 
article  propose framework translation rule selection exactly capitalizes
document level topic information  proposed topic based translation rule selection framework
associates translation rule topic distribution  rule topic distribution  source
target side  source document annotated corresponding topic distribution
 document topic distribution   dissimilarity document topic distribution rule topic
distribution calculated used help select translation rules related documents
terms topics  particular 
given document translated  use topic dissimilarity model calculate dissimilarity translation rule document based topic distributions 
translation system penalize candidate translations high dissimilarities  
   section   explains system penalizes candidate translations high dissimilarities 

 

fit opic  based issimilarity



ensitivity odels

   

   

   

   

   

   

 

u operational capability

 

 a 

 

  

  

  

  

 
  

 

   

   

   

   

   

   

 
 

 

 c 

x
  

 

 
  

  

  

 

 b 

  

 

give x 

 d  x 

x
  

  

    x
 

  

  

  

  

  

grants x 

 

  

 

  

  

held talks x  x 

figure    four synchronous rules topic distributions  sub graph shows rule
topic distribution  x axis shows topic index y axis topic probability  notably  rule  b  rule  c  shares source chinese string 
different topic distributions due different english translations 

dissimilarity topic insensitive translation rule given source document
computed topic dissimilarity model often high documents normally
topic sensitive  dont want penalize generic topic insensitive rules  therefore
propose topic sensitivity model rewards topic insensitive rules
complement topic dissimilarity model 
associate translation rule rule topic distribution source target side  order calculate dissimilarity target side rule topic distributions
translation rules source side document topic distributions given documents
decoding  project target side rule topic distributions translation rules onto space
source side document topic model one to many mapping 
use hierarchical phrase based smt system  chiang        validate effectiveness
topic based models translation rule selection  experiments chinese english translation
tasks  section    show method outperforms baseline hierarchial phrase based system
     b leu points large scale training data 
use topic based dissimilarity sensitivity models improve smt first presented
previous paper  xiao  xiong  zhang  liu    lin         article  provide
detailed comparison related work formulations two models well integration

 

fiz hang   x iao   x iong     l iu

procedure  importantly  carry large scale experiments bilingual monolingual training data incorporate detailed analysis output topic based dissimilarity
sensitivity models document translation hypothesis level 
rest article organized follows  section   introduces related work  section  
provides background knowledge statistical machine translation topic modeling  section  
elaborates topic based translation rule selection framework  including topic dissimilarity
topic sensitivity model  section   discusses estimate rule topic document topic distributions project target side rule topic distributions onto source side topic space
one to many mapping fashion  section   presents integration topic based translation rule
selection models hierarchical phrase based smt  section   describes series experiments
verify effectiveness approach  section   provides detailed analysis output
models  section   gives suggestions bilingual topic modeling perspective
machine translation  finally  conclude section    future directions 

   related work
topic based dissimilarity sensitivity models translation rule selection related three
categories work smt  translation rule selection  topic models smt document level
translation  section  introduce related approaches three categories highlight
differences method previous work 
    translation rule selection
mentioned before  translation rule selection important task smt  several approaches proposed recently  carpuat wu explore word phrase sense
disambiguation  wsd psd  translation rule selection phrase based smt  carpuat   wu 
    a      b   wsd psd system integrate sentence level local collocation features  experiments show multi word psd improve phrase selection  following wsd line 
chan et al         integrate wsd system hierarchical phrase based smt lexical selection
selection short phrases length      wsd system adopts sentence level
features local collocations  surrounding words on 
different lexical phrasal selection using wsd psd  et al         propose maximum entropy  maxent  based model context dependent synchronous rule selection hierarchical phrase based smt  local context features phrase boundary words part of speech
information incorporated model  liu et al         extends selection method
et al  integrate similar maxent based rule selection model tree to string syntax based
smt system  liu  liu    lin         model uses syntactic information source parse
trees features 
significant difference topic based rule selection framework previous approaches translation rule selection use global topic information help select translation rules ambiguous source segments rather sentence level local context features 
    topic models smt
topic modeling  hofmann        blei et al         popular technique discovering underlying
topic structures documents  recent years witnessed topic models explored
 

fit opic  based issimilarity



ensitivity odels

smt  zhao xing              tam  lane  schultz        proposed topicspecific lexicon translation adaptation models improve translation quality  models focus
word level translations  first estimate word translation probabilities conditioned topics 
adapt lexical translation probabilities phrases topic conditioned probabilities  since
modern smt systems use synchronous rules bilingual phrases translate sentences  believe
reasonable incorporate topic models phrase synchronous rule selection
lexical selection 
gong  zhang  zhou        adopt topic model filter phrase pairs consistent source documents terms topics  assign topic document
translated  similarly  phrase pair assigned one topic  phrase pair
discarded topic mismatches document topic  differences work twofold 
first  calculate dissimilarities translation rules documents based topic distributions instead comparing best topics assigned translation rules documents 
second  integrate topic information smt soft constraint manner via topic based
models  explore topic information hard constraint fashion discarding translation rules
unmatched topics 
topic models used domain adaptation translation language models smt 
foster kuhn        describe mixture model approach smt adaptation  divide
training corpus different domains  used train domain specific translation
model  decoding  combine general domain translation model specific domain
translation model selected according various text distances calculated topic model 
tam et al         ruiz federico        use bilingual topic model project latent topic
distributions across languages  based bilingual topic model  apply source side topic
weights onto target side topic model adapt target side n gram language model 
    document level machine translation
since incorporate document topic information smt  work related documentlevel machine translation  tiedemann        integrates cache based language translation models built recently translated sentences smt  gong  zhang  zhou       
extend cache based approach introducing two additional caches  static cache stores
phrases extracted documents training data similar document question
topic cache target language topic words  xiao  zhu  yao  zhang        try solve
translation consistency issue document level translation introducing hard constraint
ambiguous source words required consistently translated frequent translation options  ture  oard  resnik        soften consistency constraint integrating three
counting features decoder  studies normally focus surface structure capture inter sentence dependencies document level machine translation explore topic
structure document document translation 

   preliminaries
establish section background knowledge statistical machine translation
topic modeling  although introduction short  sufficient understanding

 

fiz hang   x iao   x iong     l iu

sub models
pi
logp  ei  f  
p i
logp  f  ei  
p i
logplex  ei  f  
p i
logplex  f  ei  
p  e 
logp  ei  e     ei   
pi 
  log ei   f  
 e 


descriptions
direct translation probabilities
inverse translation probabilities
direct lexical translation probabilities
inverse lexical translation probabilities
language model
reordering model
word count
rule count

table    widely used sub models statistical machine translation  number
translation rules used generate target sentence e given source sentence
f   ei f target source side translation rule ri  

topic based dissimilarity sensitivity models try bridge gap topic modeling
statistical machine translation 
    statistical machine translation
given source sentence f   smt systems find best translation e among possible translations follows 
hp




exp

h
 f 
e 

 
hp

e   argmax p



e
e exp
  hm  f  e  
  
 
 
x
   
hm  f  e 
  argmax exp
e

  argmax
e

m  

 


x

 

hm  f  e 

m  

hm  f  e  feature function defined source sentence f corresponding
transla i
hp
p


tion e  weight feature function  since normalization e exp
  hm  f  e  

constant possible translations e   need calculate decoding 
weighted model equation     log linear model  feature functions hm  f  e 
referred sub models  components log linear model  table   
show widely used feature functions smt  easily factored
translation rules  facilitates application dynamic programming decoding 
show proposed topic based dissimilarity sensitivity models easily factorized
section   
   notation used want emphasize sub model component log linear model  otherwise
call models  language model  reordering model on 

 

fit opic  based issimilarity



ensitivity odels

log linear model smt  sub models trained separately combined
assumption independent other  associated weights tuned
using minimum error rate training  mert   och        margin infused relaxed algorithm
 mira   chiang  marton    resnik         note normalization factor equation    
calculated training algorithms  algorithms directly optimize
log linear model smt towards translation quality measure bleu  feature weights
optimized towards criteria maximum mutual information  mmi  necessarily
optimal respect translation quality  och        
integrate proposed two models log linear model hierarchical phrasebased smt system  section    order validate effectiveness two models  provide
details hierarchical phrase based smt  chiang        section  translation rules
hierarchial phrase based smt synchronous context free grammar rules  denoted
follows 
x h   
   
x undifferentiated nonterminal  strings terminals nonterminals 
source target side respectively  denotes one to one mapping nonterminals
nonterminals   rules automatically extracted word aligned bilingual
training data  addition rules  two special rules introduced hierarchical
phrase based smt 
hx    x 
hs  x    s  x 

   

two rules used serially concatenate nonterminal xs monotonic manner form
initial symbol s  start symbol grammar hierarchical phrase based smt 
log linear model hierarchical phrase based smt formulated follows 
 
x
log t r     lm logplm  e    wp  e    rp
   
w d    exp
rd

derivation defined set triples  r  i  j   denotes application
translation rule spans words j source side  number translation rules
d  probability translation rule r defined
t r    p      p      plex      plex     

   

lexical translation probabilities plex     plex     estimate probabilities
words translate words word by word fashion  koehn et al         
    topic modeling
topic modeling used discover topics occur collection documents  latent
dirichlet allocation  lda   blei et al         probabilistic latent semantic analysis  plsa 
   order simplify decoder implementation  two nonterminals allowed hierarchical translation
rules 

 

fiz hang   x iao   x iong     l iu

 hofmann        topic models  lda widely used topic model  exploit
mine topics translation rule selection 
lda views document mixture various topics  probability distribution words  particularly  lda works generative process follows 
document dj   sample document topic distribution  per document topic distribution  j dirichlet distribution dir    j dir   
word wj i nj words document dj  
sample topic assignment zj i multinomial j   
sample word wj i multinomial zj i   zj i per topic word distribution topic zj i drawn dir   
generally speaking  lda contains two groups parameters  first group parameters
characterizes document topic distributions  j    record distribution document
topics  second group parameters used topic word distributions  k    represent
topic distribution words 
given document collection observed words w    wj i    goal lda inference
compute values two sets parameters well latent topic assignments
z    zj i    inference complicated due latent topic assignments z  efficient inference
algorithm proposed address problem collapsed gibbs sampling  griffiths
  steyvers         two sets parameters integrated lda model 
latent topic assignments z sampled p  z w   obtain values z 
estimate recovering posterior distributions given z w  section   
use two sets estimated parameters topic assignments words calculate
parameters models 

   topic based dissimilarity sensitivity models
section  elaborate topic based models translation rule selection  including topic
dissimilarity model topic sensitivity model 
    topic dissimilarity model
sentences translated accordance topics  zhao   xing              tam
et al          take translation rule  b  figure   example  source side rule
 b  occurs document international relations  hope encourage application rule
 b  rather rule  c   achieved calculating dissimilarity probability
distributions translation rule document topics 
order calculate topic dissimilarity translation rule selection  associate
source target side translation rule rule topic distribution p  z  r   
placeholder source side f target side e  r source target side translation
rule r  z corresponding topic r   therefore translation rule two rule topic
distributions  p  zf  rf   source side p  ze  re   target side 
 

fit opic  based issimilarity



ensitivity odels

supposing k topics  two distributions represented k dimension vector  k th component p  z   k r   denotes probability topic k given r   sourceand target side rule topic distributions separately estimated training data  estimation
method described section    discuss reason estimate
separate manner 
analogously  represent topic information document translated documenttopic distribution p  z d   k dimension vector  k th dimension p  z   k d 
topic proportion topic k document d  different rule topic distribution 
document topic distribution directly inferred off the shelf lda tool 
based defined rule topic document topic distributions  measure dissimilarity translation rule document decide whether rule suitable document
translation  traditionally  similarity two probability distributions calculated information measurements jensen shannon divergence  lin        hellinger distance  blei  
lafferty        
adopt hellinger distance  hd  measure topic dissimilarity  symmetric widely used comparing two probability distributions  blei   lafferty         given
rule topic distribution p  z  r   document topic distribution p  z d   hd computed
follows 
k p
 
x
p
p  z   k d  p  z   k r  
   
hd p  z d   p  z  r     
k  

let derivation defined section      let p z r  represent corresponding rule topic
distributions rules d  topic dissimilarity model dsim p  z d   p z r   derivation
defined hd equation     follows
x
dsim p  z d   p z r    
hd p  z d   p  z  r   
   
rd

obviously  larger hellinger distance candidate translation yielded derivation
document  larger dissimilarity them  topic dissimilarity model
defined above  aim select translation rules similar document translated
terms topics 
    topic sensitivity model
introduce topic sensitivity model  lets revisit figure    easily find
probability rule  c  distributes evenly topics  indicates insensitive topics 
therefore applied topics  contrast  distributions three rules
peak topics  generally speaking  topic insensitive rule fairly flat distribution
topics  topic sensitive rule sharp distribution topics 
document typically focuses topics  sharp distribution topics 
words  documents normally topic sensitive  since distribution topic insensitive
rule fairly flat  dissimilarity topic insensitive rule topic sensitive document
low  therefore  system proposed topic dissimilarity model punish
topic insensitive rules 
 

fiz hang   x iao   x iong     l iu

however  topic insensitive rules may preferable topic sensitive rules neither
similar given documents  document topic love  rule  b   c 
figure   dissimilar document rule  b  relates international relations topic
rule  c  topic insensitive  nevertheless  since rule  c  occurs frequently across various
topics  prefer rule  c  rule  b  translate document love 
address issue topic dissimilarity model  propose topic sensitivity
model  model employs entropy based metric measure topic sensitivity rule
follows
k
x
p  z   k r   log p  z   k r   
   
h p  z  r     
k  

according equation  topic insensitive rule normally large entropy topicsensitive rule smaller entropy 
given derivation rule topic distributions p z r  rules d  topic sensitivity
model defined follows 
x
h p  z  r   
   
sen p z r    
rd

incorporating topic sensitivity model topic dissimilarity model  enable smt
system balance selection topic sensitive topic insensitive rules  given rules approximately equal values topic dissimilarity  prefer topic insensitive rules 

   estimation
unlike document topic distributions directly learned lda tools  need estimate
rule topic distributions translation rules  want exploit topic information
source target language  separately train two monolingual topic models source
target side  learn correspondences two topic models via word alignments
bilingual training data 
particularly  adopt two rule topic distributions translation rule     source side
rule topic distribution p  zf  rf      target side rule topic distribution p  ze  re   
defined section      two rule topic distributions estimated using trained
topic models way  section       notably  source language documents available
decoding  order compute dissimilarity target side rule topic distribution
translation rule source side document topic distribution given document need
project target side rule topic distribution translation rule onto space source side
topic model  section      
establish alternative approaches estimation rule topic distributions via
multilingual topic models  mimno  wallach  naradowsky  smith    mccallum        boyd graber
  blei        bilingual topic models infer word to word alignments document pairs
 zhao   xing               former multilingual topic models require documents
different languages comparable terms content similarity  contrast  latter bilingual
topic models require documents parallel  i e   translations other  capture
word alignments 



  

fit opic  based issimilarity

z

n



ensitivity odels

z

w

n



z

w
z

topic
correspondence

n 

z



k

k

target

source

 a 

w

nl

n



word
alignment

n



w




w

z

z

e

j

w
 

l

 

l

k



f

j





k

k

b



target

source

 a  

 b 

 c 

figure    graphical model representations  a  bilingual topic model   b  polylingual topic
model mimno et al           c  bilingual topic model zhao xing       
number parallel sentence pairs document  word alignment
source target sentence  simplicity  display hmm transitions
among word alignments a  subfigure  a   shows build topic correspondences source target language source target topics separately learned
shown  a  

biggest difference method multilingual bilingual topic models
use per tuple topic distribution documents tuple  define
tuple set documents different languages  per tuple topic distribution similar
per document topic distribution  difference per tuple topic
distribution shared documents tuple 
topic assignments words languages naturally connected since sampled
topic distribution  contrast  assume document source target
side sampled document specific distribution topics  topic correspondences source target document learned projection via word alignments  visualize
difference figure   
yet another difference models topic specific lexicon translation model
zhao xing        use bilingual topics improve smt word level
instead rule level  since synchronous rule rarely factorized individual words 
believe reasonable incorporate topic model directly rule level rather
word level  section        empirically compare model topic specific lexicon
translation model 
tam et al         construct two monolingual topic models parallel source target
documents  build topic correspondences source target documents enforcing one to one topic mapping constraint  project target side topics onto space
source side topic model one to many fashion  section        compare two different
methods building topic correspondences 

  

fiz hang   x iao   x iong     l iu

    rule topic distribution estimation
estimate rule topic distributions word aligned bilingual training corpus document
boundaries explicitly given  source  target side rule topic distributions estimated
way  therefore  simplicity  describe estimation source side rule topic
distribution p  zf  rf   translation rule section 
estimation rule topic distributions analogous traditional estimation rule translation probabilities  chiang         addition word aligned corpus  input rule topic
distribution estimation contains source side document topic distributions inferred lda tool 
first extract translation rules bilingual training data traditional way 
source side translation rule rf extracted source language document df documenttopic distribution p  zf  df    obtain instance  rf   p  zf  df       fraction count
instance described chiang         way  collect set instances
    rf   p  zf  df       different document topic distributions translation rule  using
instances  calculate probability p  zf   k rf   rf topic k follows 
p
p  zf   k df  
    
p  zf   k rf     pk ii
p

k   
ii p  zf   k  df  

based equation  obtain two rule topic distributions p  zf  rf   p  ze  re  
rule using source  target side document topic distributions p  zf  df   p  ze  de   respectively 
    target side rule topic distribution projection

described previous section  estimate target side rule topic distributions  however  directly use equation     calculate dissimilarity target side
rule topic distribution p  ze  re   translation rule source side document topic distribution
p  zf  df   source language document translated  order measure dissimilarity  need project target side topics onto source side topic space  projection takes
following two steps 
first  calculate correspondence probability p zf  ze   pair target side topic
ze source side topic zf   inferred two separately trained monolingual
topic models respectively 
second  project target side rule topic distribution translation rule onto sourceside topic space using correspondence probabilities learned first step 
first step  estimate topic to topic correspondence probabilities using co occurrence
counts topic assignments source target words word aligned corpus  topic assignments source target words inferred two monolingual topic models  topic
assignments  characterize sentence pair  f  e   zf   ze   a   zf ze two vectors
containing topic assignments words source target sentence f e respectively 
set word alignment links   i  j   source target sentence  particularly  link
 i  j  represents source side position aligns target side position j 
  

fit opic  based issimilarity



ensitivity odels

notations  calculate co occurrence count source side topic kf
target side topic ke follows 
x x
 zfi   kf    zej   ke  
    
 zf  ze  a   i j a

zfi zej topic assignments words ej respectively   x  y  kronecker
function    x     otherwise 
compute topic to topic correspondence probability p  zf   kf  ze   ke  
normalizing co occurrence count follows 
p
p
 zf  ze  a 
 i j a  zfi   kf    zej   ke  
p
p
    
p  zf   kf  ze   ke    
 zf  ze  a 
 i j a  zej   ke  

overall  first step  obtain topic to topic correspondence matrix mke kf  
item mi j represents probability p  zf   i ze   j  
second step  given correspondence matrix mke kf   project target side ruletopic distribution p  ze  re   source side topic space multiplication follows 
 p  ze  re      p  ze  re   mke kf

    

way  get second distribution translation rule source side topic space 
call projected target side topic distribution  p  ze  re    
word alignment noises may introduced equation       turn may flatten
sharpness projected topic distributions calculated equation       order decrease
flattening effects word alignment noises  take following action practice 
topic to topic correspondence probability p  zf   kf  ze   ke   calculated via word alignments
 
k predefined number topics  set   re normalize
less k
correspondence probabilities target side topic ke  
obviously  projection method allows one target side topic ze align multiple source side
topics  different one to one correspondence used tam et al          investigate correspondence matrix mke kf obtained training data  find topic
correspondence source target language necessarily one to one  typically 
correspondence probability p  zf   kf  ze   ke   target side topic mainly distributes two
three source side topics  table   shows example target side topic three mainly
aligned source side topics 

   integration
incorporate topic dissimilarity sensitivity model two new features hierarchical
phrase based system  chiang        log linear discriminative framework  och   ney 
       dissimilarity values positive hellinger distances positive  weight
dissimilarity feature tuned mert negative  therefore log linear model favor
candidate translations lower values dissimilarity feature  less dissimilar  
words  translation rules similar document translated terms topics
selected 
  

fiz hang   x iao   x iong     l iu

e topic
enterprises
rural
state
agricultural
market
reform
production
peasants
owned
enterprise
p  zf  ze  

 agricultural 
  rural 
 peasant 
u reform 
 finance 
 social 
 safety 
n adjust 
 policy 
  income 

f topic  

 enterprise 
  market 
ik state 
i company 
 k finance 
  bank 
 investment 
 n manage 
u reform 
e operation 

f topic  

u develop 
l economic 
e technology  
i china 
e technique 
 industry 
  structure 
m  innovation 
  accelerate 
u reform 

f topic  

    

    

    

table    example topic to topic correspondence  last line shows correspondence
probability  column shows topic represented top    topical words  first
column target side topic  remaining three columns source side topics 

one possible side effect integration dissimilarity feature system
favour translations generated fewer translation rules generated translation
rules translation rules result higher dissimilarity  see equation      
say  topic based dissimilarity feature acts translation rule count penalty derivations 
fortunately  however  use translation rule count feature  see last row table   
normally favours translations yielded derivation large number translation rules 
feature balance mentioned side effect topic based dissimilarity feature 
translation rule associated source side rule topic distribution projected
target side rule topic distribution decoding  add four features follows  
dsim p  zf  d   p zf  rf     or dsimsrc   topic dissimilarity feature source side rule topic
distributions 
dsim p  zf  d    p ze  re      or dsimtrg   topic dissimilarity feature projected targetside rule topic distributions 
sen p zf  rf     or sensrc   topic sensitivity feature source side rule topic distributions 
sen t  p ze  re     or sentrg   topic sensitivity feature projected target side rule topic
distributions 
source side projected target side rule topic distributions translation rules
calculated decoding described last section  decoding  first infer
topic distribution p  zf  d  given document source language  translation rule
adopted derivation  scores four features updated correspondingly according
equation          obviously  computational cost features rather small 
   since glue rule rules unknown words extracted training data  set values four
features rules zero 

  

fit opic  based issimilarity



ensitivity odels

topic specific lexicon translation models  zhao   xing        tam et al          first
calculate topic specific translation probabilities normalizing entire lexicon translation table
adapt lexical weights translation rules correspondingly decoding  makes
decoder run slower  therefore  comparing previous topic specific lexicon translation methods  method provides efficient way incorporating topic models smt 

   experiments
section  conducted two groups experiments validate effectiveness topicbased translation rule selection framework  first group experiments  use medium scale
bilingual data train smt system topic models  purpose group experiments
quickly answer following questions 
topic dissimilarity model able improve translation rule selection terms b leu 
furthermore  source side target side rule topic distributions complementary
other 
helpful introduce topic sensitivity model distinguish topic insensitive topicsensitive rules 
topic based method better previous topic specific lexicon translation method  zhao
  xing        terms b leu decoding speed 
confirm efficacy topic based dissimilarity sensitivity model mediumscale training data  conducted second group experiments large scale training data
investigate following questions 
one to many target side rule topic projection method better previous methods
proposed zhao xing        tam et al         
effects models various types rules  phrase rules rules
non terminals 
else achieve use monolingual data train topic models 
    setup
carried experiments nist chinese to english translation  used nist evaluation set       mt    development set  sets mt   mt   test sets 
numbers documents mt    mt    mt                respectively  case insensitive
nist b leu  papineni  roukos  ward    zhu        used measure translation performance 
used minimum error rate training  och        optimize feature weights 
medium scale experiments  used fbis corpus bilingual training data 
contains        documents     k sentence pairs    m chinese words     m english
words  large scale experiments  bilingual training data consists ldc    e    ldc    t    ldc    t    ldc    t   ldc    t    hong kong hansards laws news  

  

fiz hang   x iao   x iong     l iu

selected corpora contain         documents     m sentences  average  document      sentences 
obtained symmetric word alignments training data first running giza    och   ney 
      directions applying refinement rule grow diag final and  koehn et al  
       hierarchical phrase translation rules extracted word aligned training data 
used srilm toolkit  stolcke        train language models xinhua portion
gigaword corpus  contains    m english words  trained   gram language model
medium scale experiments   gram language model large scale experiments 
order train two monolingual topic models source target side bilingual
training data  used open source lda tool gibbslda     gibsslda   implementation
lda using gibbs sampling parameter estimation inference  source  targetside topic models separately estimated chinese english part bilingual
training data  set number topic k      source  target side topic models 
used default setting tool training inference   decoding  inferred
document topic distribution document dev test sets translation using
trained source side topic model  note topic inference dev test sets performed
parameters two topic models estimated training data 
case insensitive bleu   used evaluation metric  performed statistical
significance bleu differences using paired bootstrap re sampling  koehn         order
alleviate impact instability mert  ran tuning process three times
large scale experiments presented average bleu scores three runs following
suggestion clark  dyer  lavie  smith       
    medium scale experiments
section  conducted medium scale experiments investigate effectiveness two
topic based models translation rule selection 
      e ffect



opic issimilarity odel

quickly investigated effectiveness topic dissimilarity sensitivity model using
medium scale training data  results shown table    table  observe
use topic dissimilarity model source side projected target side ruletopic distributions  dsimsrc dsimtrg table  see descriptions section    
obtain absolute improvement           b leu points baseline 
combine two topic dissimilarity features together  achieve improvement      b leu points dsimsrc 
two observations show topic dissimilarity model able improve translation quality
terms b leu 
   http   gibbslda sourceforge net 
   determine k testing                        preliminary experiments  find k      produces
slightly better performance values  order improve stability topic estimation  run
tool multiple times use best model respect log likelihood 

  

fit opic  based issimilarity

system
baseline
topiclex
dsimsrc
dsimtrg
dsimsrc dsimtrg
dsim sen

mt  
     
     
     
     
     
     



ensitivity odels

mt  
     
     
     
     
     
     

avg
     
     
     
     
     
     

speed
    
   
    
    
    
    

table    results topic dissimilarity sensitivity model terms b leu speed  words
per second   comparing traditional hierarchical system  baseline  system topic specific lexicon translation model  topiclex   dsimsrc dsimtrg topic dissimilarity features source side projected target side ruletopic distributions respectively  dsim sen activates two dissimilarity features
two sensitivity features described section    avg denotes average b leu
scores two test sets  scores bold significantly better baseline  p         
speed denotes number words translated per second 
rule type
phrase
monotone
reordering


count
   m
    m
   m
    m

src sen   
    
    
    
    

trg sen   
    
    
    
    

table    percentages topic sensitive rules listed rule types according entropies
source side  src  target side  trg  rule topic distributions  phrase rules fully
lexicalized  monotone reordering rules contain nonterminals 

order gain insights topic dissimilarity model helpful translation rule
selection  investigate many rules topic sensitive  described section     
use entropy measure whether translation rule topic sensitive based rule topic distribution  entropy translation rule calculated equation     smaller certain
threshold  rule topic sensitive  since documents often focus topics  use average entropy document topic distributions training documents threshold  compare
entropies source side target side rule topic distributions threshold  findings
shown table          translation rules topic sensitive rules compare entropies
source side rule topic distributions threshold  compare entropies targetside rule topic distributions threshold  topic sensitive rules account     
strongly suggest rules occur documents specific topics topic information
used improve translation rule selection 
      e ffect



opic ensitivity odel

see table    still     translation rules generic  sensitive topics  rules widely used documents  mentioned before 
  

fiz hang   x iao   x iong     l iu

topic dissimilarity model always punishes rules documents normally topic specific 
therefore introduce topic sensitivity model complement topic dissimilarity model  experiment result model show last line table    obtain improvement
     b leu points incorporating topic sensitivity model  indicates necessary
distinguish topic insensitive topic sensitive rules 
      c omparison



opic  s pecific l exicon ranslation odel

compared topic models topic specific lexicon translation model proposed
zhao xing         introduce framework combine hidden markov model  hmm 
lda topic model smt  shown figure    framework  bilingual sentence
pair single topic assignment sampled document pair topic distribution  
words target language  e g   english  sampled given sentence pair topic assignment
monolingual per topic word distribution   that  word alignments words
source language sampled first order markov process topic specific translation
lexicon respectively 
zhao xing integrate topic specific word to word translation lexicons estimated
bilingual topic model described topic specific lexicon translation model 
formulated follows 
p  we  wf   df   p  wf  we   df  p  we  df  
x
 
p  wf  we   z   k p  we  z   k p  z   k df  

    

k

model  probability candidate translation source word wf source document df calculated marginalizing topics corresponding topic specific translation
lexicons  simplify estimation p wf  we   z   k  directly computing probabilities
word aligned corpus associated target side topic assignments inferred
target side topic model  despite simplification  improvement implementation comparable improvement obtained zhao xing         given new document  need
adapt lexical translation weights rules  adapted lexicon translation model integrated
new feature log linear discriminative framework 
show comparison results table    topic specific lexicon translation model
better baseline     b leu points  however  topic based method  the combination
topic dissimilarity sensitivity models  outperforms baseline      b leu points 
compare two methods terms decoding speed  words second   baseline translates      words per second  system topic specific lexicon translation
model translates     words one second  overhead topic specific lexicon translation model mainly comes adaptation lexical weights  takes       time
adaptation  contrast  method speed      words per second sentence
average  three times faster topic specific lexicon translation method 
    large scale experiments
section  investigated deeper models second group experiments
large scale training data 
  

fit opic  based issimilarity

      e ffect





ensitivity odels

ne    m p rojection

discussed section      need project target side topics onto source side topic space
calculate dissimilarity target side rule topic distribution source side
document topic distribution  propose one to many projection method issue  order
investigate effectiveness method  conducted experiments large scale training
data compare following   methods 
one to one mapping enforce one to one mapping source side target side
topics  similar method tam et al          achieve aligning target side
topic corresponding source side topic largest correspondence probability
calculated section     
marginalization word alignments following zhao xing         first obtain
topics target side using lda retrieve topics source language
marginalization word alignments follows 
x
p  wf  k   
p  wf  we  p  we  z   k 
    


combination source target language documents concatenate target document aligned source document one document  run lda tool
combined documents train one topic model mixed language words  decoding 
use trained topic model infer topics source documents 
order compare one to many projection method three methods described above 
add target side topic dissimilarity feature  dsimtrg  log linear translation model 
experiment results reported table    clearly  four methods achieve improvements
baseline  however  one to many projection method performs better three
methods  particular 
method outperforms one to one topic mapping method  indicates sourceside target side topics exactly match one to one correspondence manner 
reason marginalization method performs worse among four methods may
topic model trained target documents 
surprisingly  combination method performs quite well  shows lda model
find hidden topics even mixed language documents 

      e ffect
rules



opic  based rule election f ramework



various ypes



conducted experiments investigate effect topic based models various
types rules selection  particularly  divide translation rules hierarchical phrase based smt
three types     phrase rules  contain terminals bilingual phrase
pairs used phrase based system     monotone rules  contain non terminals produce
  

fiz hang   x iao   x iong     l iu

system
baseline
one to one
marginalization
combination
one to many

mt  
     
     
     
     
     

mt  
     
     
     
     
     

avg
     
     
     
     
     

table    effect one to many topic projection method methods  marginalization  marginalization word alignments  combination  combination source
target language documents 
system
baseline
phrase rule
monotone rule
reordering rule


mt  
     
     
     
     
     

mt  
     
     
     
     
     

avg
     
     
     
     
     

table    effect topic based rule selection models three types rules  phrase rules
fully lexicalized  monotone reordering rules contain nonterminals 

monotone translations  finally    reordering rules  contain non terminals change
order translations  define monotone reordering rules according chiang et al 
       
study impact topic based models translation rule type a  activate
four features described section   rules type a  topic dissimilarity
sensitivity features two types translation rules deactivated 
table   shows experiment results  table  observe
topic based models achieve highest improvement      b leu points baseline phrase rules among three types translation rules  reasonable phrase
rules consist topical words 
obtain improvements          b leu points baseline monotone
reordering rules respectively  shows models able help select
appropriate translation rules non terminals 
activate topic dissimilarity sensitivity models translation rules 
still achieve additional improvement      b leu points  total  models outperform
baseline absolute improvement     b leu points 
      e ffect



ore onolingual data

comparing table   table    find topic based dissimilarity sensitivity models
trained medium scale data  about   k documents  collectively achieve improvement     
  

fit opic  based issimilarity



ensitivity odels

system
baseline
dsimsrc   sensrc   dsimtrg   sentrg
dsimsrc   sensrc   dsimtrg   sentrg
dsimsrc   sensrc   dsimtrg   sentrg
dsimsrc   sensrc   dsimtrg   sentrg

mt  
     
     
     
     
     

mt  
     
     
     
     
     

avg
     
     
     
     
     

table    effect using monolingual data train topic models  features bold
topic based dissimilarity sensitivity model lda topic model trained using
combination source target part large scale bilingual data corresponding
monolingual corpus 

b leu points baseline two models trained large scale data  about    k documents  obtain improvement     b leu points  suggests performance gains
may obtained data  parallel bilingual data document boundaries provided easily accessible  try collect monolingual data source or and target language 
interest study whether gain improvements using monolingual data
train topic models 
used chinese monolingual corpus documents collected chinese sohu
weblog        collected chinese corpus contains    k documents      m chinese
words  used english monolingual corpus documents collected
english blog authorship corpus  schler  koppel  argamon    pennebaker         english
monolingual corpus consists    k documents   m english words  combined new
chinese corpus source part large scale bilingual data train source side lda topic
model st   english monolingual corpus combined target part large scale
bilingual data train target side lda topic model  
used two topic models st infer topics test sets  topic information source target part large scale bilingual training data inferred st
used estimate source side rule topic distributions projected target side rule topic distributions  way  obtain new topic based dissimilarity sensitivity model
source target side 
experiment results shown table    unfortunately  obtain improvements training topic models larger data  combination chinese monolingual
corpus source part bilingual training data  instead  performance drops      
      use topic model st build source side dissimilarity sensitivity features
      adopt topic model build target side dissimilarity sensitivity
features 
one reason lower performance larger topic model training data may
use    topics  using topics may improve models larger corpora  order
investigate this  conducted new experiments topics     trained sourceside topic model using combination source part large scale bilingual data sohu
weblog data  based topic model  built source side topic dissimilarity model
   http   blog sohu com  

  

fiz hang   x iao   x iong     l iu

system
baseline
k     
k     
k      
k      

mt  
     
     
     
     
     

mt  
     
     
     
     
     

avg
     
     
     
     
     

table    experiment results different number topics  k   source side topic dissimilarity model  dsimsrc  integrated smt system 
test set
mt  
mt  

monosrc
     
     

bisrc
     
     

monobisrc
     
     

table    hellinger distances mt      test sets chinese monolingual corpus
 monosrc  source part bilingual training data  bisrc  well combination  monobisrc  terms average document topic distributions 

integrated smt system  experiment results shown table    table 
find using topics able improve model corpora 
yet another reason may additional monolingual corpus similar test sets
terms topic distributions  order examine hypothesis  inferred document topic
distributions documents test sets  chinese monolingual corpus source part
bilingual corpus using topic model st   average document topic distributions
obtain four average document topic distributions mt    mt    chinese monolingual
corpus source part bilingual corpus respectively  average topic distributions approximated corpus topic distributions four corpora  calculate
hellinger distances corpus topic distributions test sets chinese
monolingual corpus source part bilingual training data  shown table   
table  clearly find additional monolingual corpus much less similar
test sets comparing bilingual training corpus  hellinger distance test
set mt   monobisrc corpus almost twice large bilingual training data
       vs          topic model trained enlarged corpus make topic based
models select translation rules similar documents test sets terms topic
distributions  suggests select additional monolingual data similar
test sets want obtain improvements 
conducted new group experiments empirically examine hypothesis
translating web domain test set similar additional weblog corpus terms
topics  used web portion nist mt   set new development set web
portion nist mt   new test set  results displayed table     show
additional monolingual data improve performance time  suggests
select monolingual corpus similar test sets learn topics topic based
dissimilarity sensitivity models 
  

fit opic  based issimilarity



ensitivity odels

system
baseline
dsimsrc   sensrc   dsimtrg   sentrg
dsimsrc   sensrc   dsimtrg   sentrg

mt   web
     
     
     

table     results translating web domain test set topic based models trained
data augmented monolingual weblog corpus  features bold topicbased dissimilarity sensitivity model lda topic model trained using
combination source target part large scale bilingual data corresponding
monolingual corpus  mt   web web portion nist mt   test set 

   analysis
section  study details topic based models translation rule selection
looking differences make target documents individual translation hypotheses 
differences help us gain insights presented models improve translation
quality  analysis  baseline system system enhanced proposed
topic based models  all four features section   activated  trained large scale bilingual
data described section      notational convenience  hereafter refer baseline
system base system enhanced topic based dissimilarity sensitivity models
topsel 
    differences target documents
order measure impact topic based models target documents  calculate
hellinger distances target documents generated base   topsel system reference documents generated human terms topics inferred target side lda topic
model according following   steps  target side lda topic model trained target
part large scale bilingual data described section     
use target side lda topic model infer document topic distribution document
reference translations  called reference distribution  
use target side lda topic model infer document topic distribution target document generated base system  called base distribution  
similarly  obtain
topsel system 

topsel

distribution target document generated

calculate dissimilarity base reference distribution well topsel reference distribution according equation      dissimilarities first averaged documents averaged four reference translations 
table    shows calculated dissimilarities  according equation      smaller
hellinger distance two items  similar are  average hellinger distance
topsel reference documents       distance base reference
  

fiz hang   x iao   x iong     l iu

system
base
topsel

mt  
     
     

mt  
     
     

avg
     
     

table     dissimilarities  measured hellinger distance  reference documents target
documents generated base topsel system terms topics according
equation     

similar    
less similar    
p 

mt  
  
  
    

table     number target documents generated
erence documents base 

mt  
  
  
    
topsel

more less similar ref 

documents        therefore  target documents generated topsel system
mt   mt   similar documents reference translations
baseline system  calculate number target documents generated topsel
more less similar reference documents base based average hellinger
distances  numbers shown table     according sign test using numbers 
topsel statistically significantly better baseline system terms similarity
translations generated two systems human generated translations 
    differences translation hypotheses
look deeper translation hypotheses understand models select translation
rules  table    shows three translation examples compare baseline system
enhanced topic based models  order conduct quantitative comparison  calculate
dissimilarity values  measured hellinger distance  underlined phrases table    using
topic based dissimilarity model  dissimilarity values computed projected
target side rule topic distributions underlined phrases source side document topic
distributions corresponding documents phrase used  values shown
table    
two tables  easily observe system topic based dissimilarity
model prefers target phrases smaller hellinger distances documents
occur terms topic distributions  contrast  baseline able use document level
topic information translation rule selection  figure   shows topic distributions
source side document  topsel phrase allow base phrase permit eg    
major topics source side document topic        topsel phrase allow mainly
distributes    different topics  including topic       base phrase permit
mainly    different topics include topic    
   distribution probability topics larger      

  

fit opic  based issimilarity

source

ensitivity odels

       

described northern limit line unlawful  
referred northern limit line legitimate  
reference pointed northern limit line legitimate  
source
base
would permit love others accepted people  
topsel
allow love love others accepted people
reference would someone allow person loves accept peoples
love time
source
base
present   internet entitled statutory right leave
topsel
present internet enjoy statutory right leave
reference present internet enjoy rights
base

eg   



topsel

 n gc   o 

eg   

  p k   n  

eg   

table     translation examples nist mt      test sets  comparing baseline
system enhanced topic based models  underlined words highlight
difference enhanced models baseline 
phrase
unlawful
legitimate
permit
allow
entitled
enjoy

hd
    
    
    
    
    
    

table     dissimilarity values  measured hellinger distance  underlined phrases table    projected target side rule topic distributions corresponding
source side document topic distributions documents calculated topic based dissimilarity model 

   discussion bilingual topic modeling
although topic models widely adopted monolingual text analysis  bilingual multilingual
topic models less explored  especially tailored multilingual tasks machine
translation  section try provide suggestions bilingual topic modeling
perspective statistical machine translation well practice integration topic models smt  suggestions listed follows  future
directions 
investigation topic divergences across different languages cross language divergences
pervasive become one big challenges machine translation  dorr        
language level divergences hint divergences topic concept level may exist
across languages  may explain one to many topic projection target side
  

fiz hang   x iao   x iong     l iu

figure    topic distributions source side document  a  
base phrase permit shown eg    table    

topsel

phrase allow  b 

source side better one to one mapping  although mimno et al        
studied topic divergences using wikipedia articles  believe deeper wider
investigation topic divergence needed shed new light build
better bilingual topic models 
adding linguistic assumptions topic modeling practices smt show integrating linguistic knowledge machine translation normally generates better translations
 chiang et al          believe adding linguistic assumptions beyond bag ofwords improve topic modeling  flexible topic modeling framework allows us
integrate rich linguistic knowledge form features definitely facilitate
application topic models natural language processing 
joint modeling topic induction synchronous grammar induction synchronous grammar induction machine translation task automatically learning translation rules
bilingual data  blunsom  cohn  dyer    osborne        xiao   xiong         bayesian
approaches successfully used topic modeling synchronous grammar induction  joint modeling interesting direction  benefit grammar
adaptation one domain another domain machine translation 

    conclusions
article presented topic based translation rule selection framework incorporates topic information source target language translation rule disambiguation  particularly  use topic dissimilarity model select appropriate translation rules
documents according similarities translation rules documents  adopt
  

fit opic  based issimilarity



ensitivity odels

topic sensitivity model complement topic dissimilarity model order balance translation
rule selection topic sensitive topic insensitive rules  order calculate dissimilarities source  target side topic distributions  project topic distributions target
side onto source side topic model space new efficient way 
integrated topic based rule selection models hierarchical phrase based smt
system  experiments medium large scale training data show
topic dissimilarity sensitivity model able substantially improve translation
quality terms b leu improve translation rule selection various types rules  i e  
phrase monotone reordering rules  
method better previous topic specific lexicon translation method translation quality decoding speed 
proposed one to many projection method outperforms various methods
one to one mapping  marginalization via word alignments on 
want use additional monolingual corpus train topic models  first investigate whether new monolingual corpus similar test data terms topic
distributions 
topic models provide global document level information machine translation 
future  would use topic models address document level machine translation issues 
coherence cohesion  barzilay   lee        hardmeier  nivre    tiedemann        
want integrate topic based models linguistically syntax based machine translation
syntactic translation rule selection  liu et al         

acknowledgments
work sponsored national natural science foundation china projects
                   qun lius work partially supported science foundation ireland
 grant no     ce i      part cngl dublin city university  would thank
three anonymous reviewers insightful comments  corresponding author article
deyi xiong 

references
barzilay  r     lee  l          catching drift  probabilistic content models  applications
generation summarization  susan dumais  d  m     roukos  s   eds    hlt naacl
      main proceedings  pp          boston  massachusetts  usa  association computational linguistics 
blei  d  m     lafferty  j  d          correlated topic model science  aas             
blei  d  m   ng  a     jordan  m          latent dirichlet allocation  jmlr             

  

fiz hang   x iao   x iong     l iu

blunsom  p   cohn  t   dyer  c     osborne  m          gibbs sampler phrasal synchronous
grammar induction  proceedings joint conference   th annual meeting
acl  th international joint conference natural language processing
afnlp  pp          suntec  singapore  association computational linguistics 
boyd graber  j     blei  d  m          multilingual topic models unaligned text  proceedings
twenty fifth conference uncertainty artificial intelligence  uai     pp       
arlington  virginia  united states  auai press 
carpuat  m     wu  d       a   phrase sense disambiguation outperforms word sense disambiguation statistical machine translation  proceedings   th conference
theoretical methodological issues machine translation  pp       
carpuat  m     wu  d       b   improving statistical machine translation using word sense disambiguation  proceedings      joint conference empirical methods natural
language processing computational natural language learning  emnlp conll   pp 
      prague  czech republic  association computational linguistics 
chan  y  s   ng  h  t     chiang  d          word sense disambiguation improves statistical machine translation  proceedings   th annual meeting association computational linguistics  pp        prague  czech republic  association computational
linguistics 
chiang  d          hierarchical phrase based model statistical machine translation  proc 
acl      
chiang  d          hierarchical phrase based translation  computational linguistics            
    
chiang  d   marton  y     resnik  p          online large margin training syntactic structural
translation features  proceedings      conference empirical methods natural language processing  pp          honolulu  hawaii  association computational
linguistics 
clark  j  h   dyer  c   lavie  a     smith  n  a          better hypothesis testing statistical
machine translation  controlling optimizer instability  proceedings   th annual
meeting association computational linguistics  human language technologies 
pp          portland  oregon  usa 
dorr  b  j          machine translation divergences  formal description proposed solution 
computational linguistics                
foster  g     kuhn  r          mixture model adaptation smt  proc  second workshop
statistical machine translation  pp          prague  czech republic 
gong  z   zhang  m     zhou  g          cache based document level statistical machine translation  proc  emnlp      
gong  z   zhang  y     zhou  g          statistical machine translation based lda  proc 
iucs       p          



griffiths  t  l     steyvers  m          finding scientific topics  proceedings national
academy sciences      suppl               

  

fit opic  based issimilarity



ensitivity odels

hardmeier  c   nivre  j     tiedemann  j          document wide decoding phrase based statistical machine translation  proceedings      joint conference empirical
methods natural language processing computational natural language learning 
pp            jeju island  korea  association computational linguistics 
he  z   liu  q     lin  s          improving statistical machine translation using lexicalized rule
selection  proceedings   nd international conference computational linguistics
 coling        pp          manchester  uk  coling      organizing committee 
hofmann  t          probabilistic latent semantic analysis  proc  uai       pp         
koehn  p          statistical significance tests machine translation evaluation  proceedings
emnlp       pp          barcelona  spain 
koehn  p   och  f  j     marcu  d          statistical phrase based translation  proc  hlt naacl
     
lin  j          divergence measures based shannon entropy  ieee trans  inf  theor         
       
liu  q   he  z   liu  y     lin  s          maximum entropy based rule selection model syntaxbased statistical machine translation  proceedings      conference empirical methods natural language processing  pp        honolulu  hawaii  association
computational linguistics 
liu  y   liu  q     lin  s          tree to string alignment template statistical machine translation  proc  acl      
mimno  d   wallach  h  m   naradowsky  j   smith  d  a     mccallum  a          polylingual
topic models  proc  emnlp      
och  f  j     ney  h          discriminative training maximum entropy models statistical
machine translation  proc  acl      
och  f  j          minimum error rate training statistical machine translation  proc  acl      
och  f  j     ney  h          systematic comparison various statistical alignment models 
computational linguistics              
papineni  k   roukos  s   ward  t     zhu  w  j          bleu  method automatic evaluation
machine translation  proc  acl      
ruiz  n     federico  m          topic adaptation lecture translation bilingual latent
semantic models  proceedings sixth workshop statistical machine translation 
schler  j   koppel  m   argamon  s     pennebaker  j  w          effects age gender
blogging  aaai spring symposium  computational approaches analyzing weblogs  pp 
       
stolcke  a          srilm extensible language modeling toolkit  proc  icslp      
tam  y  c   lane  i  r     schultz  t          bilingual lsa based adaptation statistical machine
translation  machine translation                
tiedemann  j          context adaptation statistical machine translation using models exponentially decaying cache  proceedings      workshop domain adaptation

  

fiz hang   x iao   x iong     l iu

natural language processing  pp       uppsala  sweden  association computational
linguistics 
ture  f   oard  d  w     resnik  p          encouraging consistent translation choices  proceedings      conference north american chapter association computational linguistics  human language technologies  pp          montreal  canada  association computational linguistics 
xiao  t   zhu  j   yao  s     zhang  h          document level consistency verification machine
translation  proceedings      mt summit xiii  pp          xiamen  china 
xiao  x     xiong  d          max margin synchronous grammar induction machine translation  proceedings      conference empirical methods natural language
processing  pp          seattle  washington  usa  association computational linguistics 
xiao  x   xiong  d   zhang  m   liu  q     lin  s          topic similarity model hierarchical phrase based translation  proceedings   th annual meeting association
computational linguistics  volume    long papers   pp          jeju island  korea 
association computational linguistics 
xiong  d   zhang  m     li  h          modeling translation predicate argument structure
smt  proceedings   th annual meeting association computational
linguistics  volume    long papers   pp          jeju island  korea  association computational linguistics 
zhao  b     xing  e  p          hm bitam  bilingual topic exploration  word alignment 
translation  proc  nips      
zhao  b     xing  e  p          bitam  bilingual topic admixture models word alignment 
proc  acl      

  


