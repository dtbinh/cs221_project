journal artificial intelligence research          

submitted        published      

decision theoretic model assistance
alan fern

afern   eecs   oregonstate   edu

school eecs  oregon state university  corvallis  usa

sriraam natarajan

natarasr   indiana   edu

soic  indiana university  bloomington  usa

kshitij judah

judahk   eecs   oregonstate   edu

school eecs  oregon state university  corvallis  usa

prasad tadepalli

tadepall   eecs   oregonstate   edu

school eecs  oregon state university  corvallis  usa

abstract
growing interest intelligent assistants variety applications sorting
email helping people disabilities daily chores  paper  formulate
problem intelligent assistance decision theoretic framework  present theoretical
empirical results  first introduce class pomdps called hidden goal mdps  hgmdps  
formalizes problem interactively assisting agent whose goal hidden whose
actions observable  spite restricted nature  show optimal action selection
hgmdps pspace complete even deterministic dynamics  introduce
restricted model called helper action mdps  hamdps   sufficient modeling many
real world problems  show classes hamdps efficient algorithms possible 
interestingly  general hamdps show simple myopic policy achieves near
optimal regret  compared oracle assistant knows agents goal  introduce
sophisticated versions policy general case hgmdps combine
novel approach quickly learning agent assisted  evaluate approach
two game like computer environments human subjects perform tasks  real world
domain providing assistance folder navigation computer desktop environment 
results show three domains framework results assistant substantially reduces
user effort modest computation 

   introduction
personalized ai systems interactively assist human users received significant attention recent years  yorke smith  saadati  myers    morley        lieberman        myers  berry 
blythe  conleyn  gervasio  mcguinness  morley  pfeffer  pollack    tambe         however 
overarching formal framework interactive assistance captures different systems
provides theoretical foundation largely missing  paper address lacuna introducing general framework decision theoretic assistance  analyzing problem complexity
different assumptons  proposing different heuristic solutions  evaluating effectiveness 
consider model assistant observes goal oriented agent must select assistive
actions order best help agent achieve goals  real applications  requires
assistant able handle uncertainty environment agent  reason varying
c
    
ai access foundation  rights reserved 

fif ern   natarajan   j udah     tadepalli

action costs  handle unforeseen situations  adapt agent time  consider
decision theoretic model  based partially observable markov decision processes  pomdps  
naturally handles features  providing formal basis designing intelligent assistants 
first contribution work formulate problem selecting assistive actions
class partially observable markov decision processes  pomdps  called hidden goal mdps
 hgmdps   jointly models application environment along agents policy
hidden goals  key feature approach explicitly reasons environment
agent  provides potential flexibility assisting ways unforeseen developer
new situations encountered  thus  developer need design hand coded assistive
policy preconceived application scenario  instead  using framework  burden
developer provide model application domain agent  alternatively
mechanism learning one models experience  framework uses
models attempt compute  situation  whether assistance could beneficial
assistive action select 
second contribution work analyze properties formulation  despite
restricted nature hgmdps  complexity determining hgmdp finite horizon
policy given value pspace complete even deterministic environments  motivates
restricted model called helper action mdp  hamdp   assistant executes helper
action step  agent obliged accept helper action helpful goal
receives reward bonus  or cost reduction  so  otherwise  agent continue
preferred action without reward penalty assistant  show classes
problem complete pspace np  show class hamdps
deterministic agents polynomial time algorithms minimizing expected worstcase regret relative oracle assistant knows goal agent  further  show
optimal worst case regret characterized graph theoretic property called tree rank
corresponding all goals policy tree computed linear time 
principle  given hgmdp  one could apply pomdp solver order arrive optimal
assistant policy  unfortunately  relatively poor scalability pomdp solvers often force us
utilize approximate heuristic solutions  particularly true assistant continually
learning updated models agent and or environment  results sequence
accurate hgmdps  needs solved  third contribution work set
myopic action selection mecahnisms approximate optimal policy  hamdps 
analyze myopic heuristic show regret upper bounded entropy
goal distribution hamdps  furthermore give variant policy able
achieve worst case expected regret logarithmic number goals without
prior knowledge goal distribution  describe two approaches based
combination explicit goal estimation  myopic heuristics  bounded search
generally applicable hgmdps 
order approach useful  hgmdp must incorporate reasonably accurate model agent assisted  fourth contribution work describe novel
model based bootstrapping mechanism quickly learning agent policy  important
usability assistant early lifetime  main idea assume agent
close rational decision theoretic sense  motivates defining prior agent policies
places higher probability policies closer optimal  prior combination

  

fia ecision  t heoretic odel ssistance

bayesian updates allows agent model learned quickly rationality assumption
approximately satisfied 
final contribution work evaluate framework three domains  first
consider two game like computer environments human subjects  results domains
show assistants resulting framework substantially reduce amount work
performed human subjects  consider realistic domain  folder navigator
 bao  herlocker    dietterich        task tracer project  domain  user navigates
directory structure searching particular location open save file  unknown
assistant  job assistant predict users destination folder take actions
provide short cuts reach it  results show generic assistant framework compares
favorably hand coded solution bao et al 
remainder paper organized follows  next section  introduce formal problem setup terms hgmdps  followed analysis computational complexity
guarantees myopic heuristics case hamdps  next  present approximate solution approach general hgmdps based goal estimation online action selection  finally
give empirical evaluation approach three domains conclude discussion
related future work 

   hidden goal markov decision processes
throughout paper refer entity attempting assist agent
assisting entity assistant  consider episodic problem setting beginning
episode agent begins world state selects goal finite set possible
goals  goal set  example  might contain possible dishes agent might interested
cooking  possible destination folders agent may possibly navigate to  importantly  assistant fully observe world state agents actions  cannot observe
goal agent  model interaction agent assistant sequential
agent assistant alternate turns  taking single action per turn  possibly noop    episode
ends either agents assistants action leads goal state  immediate reward
accumulated action episode  total reward episode equal
sum rewards obtained episode  note available actions agent
assistant need may varying rewards  since assistant agent
share objective  rewards viewed perspective agent  objective
assistant behave way maximizes expected total reward episode 
formally  model interaction via hidden goal markov decision processes
 hgmdps   hgmdp mdp goal user observed rest
environment completely observed  hgmdp tuple hs  g  a  a    t  r      ig
set states  g finite set possible agent goals  set agent actions 
a  set assistant actions  typically a  include noop action  allows assistant
decide provide assistance particular decision epoch  transition function
 s  g  a  s    probability transition state s  taking action a 
agents goal g  r reward function maps g  a a    real values 
   consider strictly alternating turn model simplicity  however  straightforward use model
capture interactions strictly alternating  e g  allowing assistant agent take multiple actions
row 

  

fif ern   natarajan   j udah     tadepalli

agents policy maps g distributions need optimal sense 
 ig   initial state  goal  distribution 
assistant policy hgmdp defines distribution actions given sequence
preceding observations  i e   sequence state action pairs  important assistant policy depends history rather current state since entire history potentially
provide evidence goal agent  may necessary selecting appropriate
action  must define objective function used evaluate value particular assistant policy  consider finite horizon episodic problem setting  hgmdp
episode begins drawing initial state goal g ig   process alternates
agent assistant executing actions  including noops  environment
horizon terminal state reached  agent assumed select actions according  
many domains  terminal goal state reached within horizon  though general  goals
arbitrary impact reward function  reward episode equal sum
rewards actions executed agent assistant episode  objective
assistant reason hgmdp observed state action history order select
actions maximize expected  or worst case  total reward episode 
proceeding worth reviewing assumptions formulation potential implications 
partial observability  definition hgmdp similar definition
partially observable markov decision process  pomdp   fact  hgmdp special
case pomdp unobserved part state space consists single component corresponds goal user  simplicity assumed
world state fully observable  choice fundamental framework one
imagine relatively straightforward extensions techniques model environment
partially observable mdp  pomdp  world states fully observable 
section    shed light hardness hgmdps describe specialized
heuristic solutions performance guarantees 
agent policy  assumed agent modeled memoryless reactive
policy gives distribution actions conditioned current world state
goal  assumption fundamental framework one extend
include complex models user  example  include hierarchical goal
structures  extension explored previously  natarajan  tadepalli    fern 
      
sequential interaction  assumed simplicity interaction model
assistant agent involves interleaved  sequential actions rather parallel actions 
this  example  precludes assistant taking actions parallel agent 
parallel assistive actions useful many cases  many domains sequential
actions norm  especially motivated domains intelligent desktop
assistants help store retrieve files  filter spam  sort email  etc   smart homes
open doors  switch appliances  on  many opportunities assistance
domains sequential variety  many cases  tasks appear require parallel
activity often formulated set threads thread sequential hence

  

fia ecision  t heoretic odel ssistance

formulated separate assistant  extending framework handle general parallel
assistance interesting future direction 
goal dependent transitions  dependence reward policy goal allows
model capture agents desires behavior goal  dependence
goal less intuitive many cases dependence used
model dynamics environment  however  allow goal dependence
generality modeling  example  convenient model basic communication
actions agent changing aspects state  result actions often
goal dependent 
two main obstacles solving problem intelligent assistance framework 
first  many scenarios  initially hgmdp directly disposal since lack
accurate information agent policy and or goal distribution ig   often due
fact assistant deployed variety initially unknown agents  rather  assistant find environment given possible set goals  described
section    approach difficulty learn approximate hgmdp estimating agent
policy goal distribution ig   furthermore  describe bootstrapping mechanism learning approximations quickly  second obstacle solving hgmdp
generally high computational complexity solving hgmdps  deal issue section  
considers various approximate techniques efficiently solving hgmdps 
mentioned possible provide assistance using simpler domain specific
engineered solutions  particular  domains consider later could solved using several less
expensive solutions require machinery hgmdps  fact  one domains 
compare model existing supervised learning method  goal work
provide domain independent framework potentially encapsulate several assistant
systems hope gives rise robust understanding methodology building
systems much less human effort future 

   theoretical analysis
section  analyze hardness solving hgmdps  show despite
special case pomdps  hard  motivates new model called helper action
mdp  hamdp  restricted amenable approximate solutions 
introduce myopic heuristic solve hamdps analyze performance 
analyze special cases hamdps permit efficient solutions 
    complexity results hidden goal mdps
given knowledge agents goal g hgmdp  assistants problem reduces solving
mdp assistant actions  mdp transition function captures state change due
assistant action ensuing state change due agent action selected according
given g  likewise reward function transition captures reward due assistant action
ensuing agent action conditioned g  optimal policy mdp corresponds
optimal assistant policy g  however  since real assistant often uncertainty
agents goal  unlikely optimal performance achieved 

  

fif ern   natarajan   j udah     tadepalli

view hgmdp collection  g  mdps share state space 
assistant placed one mdps beginning episode  cannot observe
one  mdp result fixing goal component hgmdp definition one
goals  collection easily modeled restricted type partially observable mdp
 pomdp  state space g  component completely observable  g component unobservable changes beginning episode  according ig  
remains constant throughout episode  furthermore  pomdp transition provides observations agent action  gives direct evidence unchanging g component 
perspective hgmdps appear significant restriction general pomdps  however 
first result shows despite restriction worst case complexity reduced even
deterministic dynamics 
given hgmdp   horizon   o  m     reward target r   short term reward
maximization problem asks whether exists history dependent assistant policy achieves
expected finite horizon reward least r   general pomdps problem pspacecomplete  papadimitriou   tsitsiklis        mundhenk         pomdps deterministic
dynamics  np complete  littman         however  following result 
theorem    short term reward maximization hgmdps deterministic dynamics pspacecomplete 
proof  membership pspace follows fact hgmdp polynomially encoded pomdp policy existence pspace  show pspace hardness 
reduce pspace complete problem tqbf  truth quantified boolean formula  problem
existence history dependent assistant policy expected reward r 
let quantified boolean formula form x  x  x        xn  c   x            xn        
cm  x            xn     ci disjunctive clause  us  goal gi clause 
agent chooses goal uniformly randomly set goals formed hides
assistant  states consist pairs form  v  i   v        current value
goal clause  next variable set  actions assistant set existentially
quantified variables  agent simulates setting universally quantified variables choosing
actions set        equal probability  episode terminates variables
set  assistant gets reward   value clause   end reward  
otherwise 
note assistant get useful informtion goal termination
episode  satisfiable  assistant policy leads reward  
goals choices agent actions  hence expected value   goal
distribution  not  least one goals satisfied setting universal
quantifiers  leading expected value      hence tqbf problem reduces deciding
hgmdp policy expected reward   
result shows pomdp encoded hgmdp deterministic dynamics  stochastic dynamics pomdp captured via stochastic agent policy
hgmdp  however  hgmdps resulting pspace hardness reduction quite pathological compared likely arise realistic assistant domains  importantly 
agents actions provide practically information agents goal end episode 
late exploit knowledge  suggests search restricted classes
hgmdps allow efficient solutions performance guarantees 
  

fia ecision  t heoretic odel ssistance

    helper action mdps
motivation helper action mdps  hamdps  place restrictions agent assistant avoid following three complexities arise general hgmdps     agent
behave arbitrarily poorly left unassisted agent actions may provide significant evidence goal     agent free effectively ignore assistants help
exploit results assistive action  even would beneficial     assistant
actions possibility negatively impacting agent compared assistant 
hamdps address first issue assuming agent competent  approximately 
maximizing reward without assistant  second third issues addressed assuming agent always detect exploit helpful actions assistant actions
never hurt agent even unhelpful 
informally  hamdp provides assistant helper action agents actions  whenever helper action h executed directly corresponding agent action a 
agent receives bonus reward    however  agent accept helper action h
 by taking a  hence receive bonus  action agent considers good
achieving goal without assistant  thus  primary objective assistant hamdp
maximize number helper actions get accepted agent  
simple  model captures much essence assistive domains assistant
actions cause minimal harm agent able detect accept good assistance
arises 
hamdp hgmdp hs  g  a  a    t  r      ig following constraints 
agent assistant actions sets    a              a     h            hn   
ai corresponding helper action hi  
state space   w  w a     w set world states  states w a 
encode current world state previous assistant action 
reward function r   assistant actions  agent actions reward zero
unless agent selects action ai state  s  hi   gives reward    is 
agent receives bonus   whenever takes action directly corresponding helper
action 
assistant always acts states w   taking hi deterministically
transitions  s  hi   
agent always acts states a    resulting states according transition function depend hi   i e    s  hi    g  ai   s         s  g  ai   s   
transition function    
finally  agent policy  let  s  g  function returns set actions p  s  g 
distribution actions  view  s  g  set actions agent
   note assumed bonus rewards   simplicity  results easily extended
non uniform positive bonus rewards  particular main result concerning bounding regret
myopic policy  analogous result simply includes constant factor equal maximum possible reward bonus 
exceptions theorems      currently analogous results non uniform reward
bonuses 

  

fif ern   natarajan   j udah     tadepalli

considers acceptable  or equally good  state goal g  agent policy always selects
ai helper action hi whenever ai acceptable  is    s  hi    g    ai whenever
ai  s  g   otherwise agent draws action p  s  g  
hamdp  primary impact assistant action influence reward following
agent action  notice hamdps rewards inherent underlying
environment  rather  rewards bonuses received whenever agent accepts helper
action  could defined model include environmental reward addition
helper bonuses  unnecessarily complicates model  as following hardness result shows  
instead  assume inherent environmental reward already captured agent policy
via  s  g   considered contain actions approximately optimize reward 
example  hamdp model captures doorman domain  desktop domain experiments  doorman domain  helper actions correspond opening doors
agent  reduces cost navigating one room another  desktop domain  helper actions correspond offering shortcuts users destination folders  importantly
opening incorrect door offering incorrect shortcut increase  physical  cost
agent assistant all  key property hamdps 
despite apparent simplification hamdps hgmdps  turns somewhat
surprisingly worst case computational complexity reduced 
theorem    short term reward maximization hamdps pspace complete 
proof  membership pspace follows easily since hamdps specialization hgmdps 
proof pspace hardness identical theorem   except here  instead
agents actions  stochastic environment models universal quantifiers  agent accepts
actions last one sets variable suggested assistant 
assistants actions  environment chooses value universally quantified variable equal
probability  last action accepted agent goal clause evaluates    otherwise not 
history dependent policy whose expected reward greater equal number
existential variables quantified boolean formula satisfiable 
unlike case hgmdps  stochastic dynamics essential pspace hardness
shown later section  despite negative result  following sections demonstrate utility hamdp restriction giving performance guarantees simple policies improved
complexity results  far  analogous results general hgmdps 
    myopic heuristic analysis
hamdps closely related sequential prediction framework littlestone        
framework  round learner shown new instance predicts binary label 
prediction incorrect  mistake made  learner given true label  realizable
setting  true labels determined hypothesis target class  optimal prediction
algorithm minimizes upper bound number mistakes possible hypotheses 
view helper action hamdp prediction user action  maximizing bonus
reward hamdp equivalent minimizing number mistakes sequential prediction 
unlike sequential prediction  predictions  actions  need binary  sequential
prediction sequence states arbitrarily chosen  assume generated
  

fia ecision  t heoretic odel ssistance

markov process  spite differences  results sequential prediction
adapted hamdps albeit using different terminology  however  derive results
first principles consistency 
given assistant policy     regret particular episode extra reward oracle
assistant knowledge goal would achieve     hamdps oracle assistant
always achieve reward equal finite horizon m  always select helper action
accepted agent  thus  regret execution   hamdp equal
number helper actions accepted agent  call mispredictions 
know optimizing regret pspace hard thus focus bounding
expected worst case regret assistant  introduce first myopic heuristic
show able achieve regret bounds logarithmic number goals 
coarsened posterior heuristic  intuitively  myopic heuristic select action
highest probability accepted respect coarsened version posterior
distribution goals  myopic policy state given history h based consistent goal
set c h   set goals non zero probability respect history h 
straightforward maintain c h  observation  observations include world state
agents actions   myopic policy defined as 
 s  h    arg max ig  c h  g s  a  


g s  a     g    s  g   set goals agent considers
acceptable action state s  expression ig  c h  g s  a   viewed probability
mass g s  a  coarsened goal posterior assigns goals outside c h  probability
zero otherwise weighs proportional prior 
theorem    hamdp expected regret coarsened posterior heuristic bounded
entropy goal distribution h ig   
proof  main idea proof show misprediction myopic policy  i e 
selected helper action accepted agent  uncertainty goal reduced
constant factor  allow us bound total number mispredictions trajectory 
consider misprediction step coarsened posterior heuristic selects helper action hi
state given history h  agent accept action instead selects    ai  
definition myopic policy know ig  c h  g s  ai    ig  c h  g s      since
otherwise assistant would chosen hi   fact argue ig  c h     
ig  c h     h   history misprediction  is  probability mass
ig consistent goal set misprediction less half consistent goal set
misprediction  show consider two cases     ig  c h  g s  ai     
ig  c h         ig  c h  g s  ai    ig  c h      first case  immediately get
ig  c h  g s       ig  c h      combining fact c h     c h 
g s    get desired result ig  c h      ig  c h      second case  note
c h     c h   g s    g s  ai   
c h   c h  g s  ai   
combining assumption second case immediately implies ig  c h     
ig  c h     
  

fif ern   natarajan   j udah     tadepalli

shows misprediction made histories h h   ig  c h     
ig  c h      implies episode  n mispredictions resulting history hn  
ig  c hn     n   consider arbitrary episode true goal g  know
ig  g  lower bound ig  c hn     implies ig  g   n equivalently n
log ig  g    thus episode goal g maximum number mistakes bounded
log ig  g    using fact get thepexpected number mispredictions episode
respect ig bounded g ig  g  log ig  g     h ig    completes
proof 
since h ig   log  g    result implies hamdps expected regret myopic
policy logarithmic number goals  furthermore  uncertainty
goal decreases  decreasing h ig    regret bound improves get regret   ig
puts mass single goal  turns logarithmic bound asymptotically tight
worst case 
theorem    exists hamdp assistant policy expected regret least
log  g     
proof  consider deterministic hamdp environment structured binary tree
depth log  g    leaf corresponds one  g  goals  considering uniform
goal distribution easy verify node tree equal chance true
goal left right sub tree episode  thus  policy     chance
committing misprediction step episode  since episode length log  g   
expected regret episode policy log  g     
resolving gap myopic policy bound regret lower bound open problem 
      pproximate g oal istributions  
  instead true underlying
suppose assistant uses approximate goal distribution ig
goal distribution ig computing myopic policy  is  assistant selects actions
   c h  g s  a    refer myopic policy relative    
maximize ig
g
  instead bounded terms kullbackleibler  kl  diverextra regret using ig
g
     zero  
gence  kullback   leibler        distributions kl ig k ig
g
equals ig  

theorem    hamdp goal distribution ig   expected regret myopic policy
  bounded h i     kl i k     
respect distribution ig
g
g
g
proof  proof similar theorem    except since myopic policy respect
  rather   derive that  episode  maximum number mispredictions n
ig
g

  

fia ecision  t heoretic odel ssistance

   g    using fact  average number mispredictions given by 
bounded log ig

p

g

p

g

p

g

 

 
  
ig  g  log   
ig  g 


 
ig  g  log   
    log ig  g   log ig  g    
ig  g 
x
ig  g 
ig  g  log ig  g  
ig  g  log   
 
ig  g 
g
 
h ig     kl ig k ig
  

note random variable x distribution p finite domain size n  
kl p k u     log n   h p    u uniform distribution  thus  consequence
theorem   myopic policy respect uniform goal distribution expected regret
bounded log  g   hamdp  showing logarithmic regret achieved without
knowledge ig   strengthened hold worst case regret 
theorem    hamdp  worst case hence expected regret myopic policy
respect uniform goal distribution bounded log  g   
proof  proof theorem   shows number mispredictions episode bounded
     case        g  shows worst case regret bound log  g   
log ig
g
immediately implies expected regret bound uniform myopic policy bounded
log  g   
    deterministic agent policies
consider several special cases hamdps  first  restrict agents policy
deterministic goal  i e   s  g  single action state goal pair  s  g  
theorem    myopic policy achieves optimal expected reward hamdps deterministic agent policies 
proof given appendix  sometimes desirable minimize worst possible regret
compared oracle assistant knows agents goal  show below  captured
graph theoretic notion tree rank generalizes rank decision trees  ehrenfeucht  
haussler        
definition    rank rooted tree rank root node  node leaf node
rank node       else node least two distinct children c  c  equal highest ranks
among children  rank node       rank c     otherwise rank node    rank highest
ranked child 
optimal trajectory tree  ott  hamdp deterministic environments tree
nodes represent states hamdp reached prefixes optimal action sequences
different goals starting initial state   node tree represents state set
   multiple initial states  build ott initial state  rank would maximum
ranks trees 

  

fif ern   natarajan   j udah     tadepalli

goals optimal path initial state  since agent policy deterministic 
one trajectory per goal tree  hence size optimal trajectory tree
bounded number goals times maximum length trajectory 
size state space deterministic domains  following lemma follows induction
depth optimal trajectory tree  proof appendix 
lemma    minimum worst case regret policy hamdp deterministic environments deterministic agent policies equal tree rank optimal trajectory tree 
leads following 
theorem    agent policy deterministic  problem minimizing maximum regret
hamdps deterministic environments p 
proof  first construct optimal trajectory tree  compute rank linear time
simultaneously computing optimal minimax policy using recursive definition tree rank 
result follows lemma   
    bounded branching factor policies
assumption deterministic agent policy may restrictive many domains 
consider agent policies may constant number possible actions  s  g 
state goal pair defined below 
definition    branching factor hamdp largest number possible actions  s  g 
agent state goal assistants action 
doorman domain section     branching factor   since two optimal
actions reach goal state 
theorem    minimizing worst case regret finite horizon hamdps deterministic environments constant branching factor k np complete 
proof appendix  show minimizing expected regret bounded
k np hard  conjecture problem np  question remains open 

   solving practical hgmdps
although hamdps offer theoretically elegant framework  requirements practical assistant
systems easily satisfed assumptions  section  consider general
problem solving hgmdps offer practical heuristic solutions inspired
theoretical analysis 
principle could use general purpose pomdp solver solve hgmdps 
pomdp solvers based point based methods search based methods become efficient years  still inefficient used interactive setting 
parameters pomdp continually updated  moreover  analysis previous
section suggests  simple myopic heuristics based knowledge goal distribution
optimal policies given goals appear promising yield respectable performance  reason 
adopt approach based bayesian goal estimation followed heuristic action selection 
evaluate three different domains  below  first give overview solution algorithm
describe components detail 
  

fia ecision  t heoretic odel ssistance

    overview
section  assume given hgmdp   delegating problem learning
section    let ot   o         ot observation sequence observed assistant
beginning current trajectory time t  observation tuple world state
previously selected action  by either assistant agent   given ot goal compute
assistant action whose value  close to  optimal 
motivate approach  useful consider special characteristics hgmdp 
importantly  belief state corresponds distribution agents goal  since agent
assumed goal directed  observed agent actions provide substantial evidence
goal might might be  fact  even assistant nothing  agents goals
often rapidly revealed analyzing relevance agents initial actions possible
goals  cases  suggests state goal estimation problem hgmdp may
solved quite effectively observing agents actions relate various possible goals 
rather requiring assistant select actions explicitly purpose information gathering
agents goals  words  cases  expect purely  or nearly  myopic
action selection strategies  avoid reasoning information gathering  effective 
reasoning information gathering one key complexities involved solving pomdps
compared mdps  leverage intuitive properties hgmdp gain tractability
limiting completely avoiding reasoning  course  shown pspace hardness
results  goals always rapidly revealed non myopic reasoning essential 
note cases  assistant pure information gathering actions
disposal  e g  asking agent question  consider actions experiments 
believe actions handled naturally framework incorporating
small amount look ahead search 
motivation  assistant architecture  depicted figure    alternates
goal estimation action selection follows 
   observing agents next action  update goal distribution based hgmdp
model 
   based updated distribution evaluate effectiveness assistant actions  including
noop  building sparse sampling look ahead tree bounded depth  perhaps depth
one   leaves evaluated via myopic heuristic 
key element architecture computation myopic heuristics  top
heuristic  optionally obtain non myopic behavior via search building look ahead sparsesampling tree  experiments show search improve performance small margin
significant computational cost  note idea utilizing myopic heuristics select
actions pomdps new  see example  cassandra        geffner   bonet        
similar methods used previously success applications computer bridge
 ginsberg         main contribution show approach particularly well
suited setting evaluate efficiently computable heuristics specifically designed
solving hgmdps  describe goal estimation action selection operations
detail 

  

fif ern   natarajan   j udah     tadepalli

goal estimation

p g 

action selection

assistant
ot



wt
environment

user
ut

figure    depiction assistant architecture  agent hidden goal selects actions
ut cause environment change world state wt   typically moving closer
goal  assistant  upper rectangle  able observe world state along
observations generated environment  setting contain user agent
actions along world state  assistant divided two components  first 
goal estimation component computes posterior agent goals p  g  given observations  second  action selection component uses goal distribution compute
best assistive action via combination bounded search myopic heuristic
computation  best action might noop cases none assistive
actions higher utility user 

  

fia ecision  t heoretic odel ssistance

    goal estimation
given hgmdp agent policy initial goal distribution ig   objective maintain
posterior goal distribution p  g ot    gives probability agent goal g
conditioned observation sequence ot   note since assumed assistant cannot
affect agents goal  observations related agents actions relevant posterior 
given agent policy   straightforward incrementally update posterior p  g ot   upon
agents actions 
beginning episode initialize goal distribution p  g o    ig   timestep
episode  ot involve agent action  leave distribution unchanged  otherwise  agent selects action state s  update posterior according p  g ot    
   z  p  g ot     a s  g   z normalizing constant  is  distribution adjusted place weight goals likely cause agent execute action s 
accuracy goal estimation relies well policy learned assistant reflects
true agent policy  described section      use model based bootstrapping approach
estimating update estimate end episode  provided agent close
optimal  experimental domains  approach lead rapid goal estimation  even early
lifetime assistant 
assumed simplicity actions agent directly observable 
domains  natural assume state world observable  rather
actual action identities  cases  observing agent transitioning s 
use mdp transition function marginalize possible agent actions yielding update 
p  g ot        z  p  g ot   

x

 a s  g t  s  a  s    

aa

    action selection
given hgmdp distribution goals p  g ot    address problem
selecting assistive action  mechanisms utilize combination bounded look ahead search
myopic heuristic computations  increasing amount look ahead search actions
returned closer optimal cost computation  fortunately  many hgmdps 
useful assistant actions computed relatively little search  first describe several
myopic heuristics used either greedy action selection combination search 
next  review utilize sparse sampling obtain non myopic action selection 
      action election h euristics
explain action selection procedure  introduce idea assistant mdp relative
goal g   denote  g   mdp  g  identical except
change initial goal distribution p  g    g       is  goal always fixed g
episode  since hidden component state space goal  fixing goal
 g  makes state fully observable  yielding mdp  episode  g  evolves drawing
initial world state selecting assistant actions goal g achieved  note
state transition assistant action a  state result successive state transitions  first
due assistant action due ensuing agent action  selected based
agent policy goal g  optimal policy  g  gives optimal assistive action assuming
  

fif ern   natarajan   j udah     tadepalli

agent acting achieve goal g  denote q function  g  qg  s  a  
expected cost executing action state following optimal policy 
consider second heuristic action selection  accounts non uniform rewards true goal posterior  unlike coarsened posterior heuristic introduced section     
simply expected q value action assistant mdps  called qmdp
method cassandra         heuristic value assistant action state given observations
ot
x
qg  s  a  p  g ot   
h s  a  ot    
g

intuitively h s  a  ot   measures utility taking action assumption
goal ambiguity resolved one step  thus  heuristic value information gathering
utility action  rather  heuristic favor assistant actions make progress toward goals
high posterior probability  goal posterior highly ambiguous often lead
assistant prefer noop  least hurt progress toward goal  note
heuristic  well others below  used evaluate utility state s  rather
state action pair  maximizing actions maxa h s  a  ot   
primary computational complexity computing h solve assistant mdps
goal order obtain q functions  technically  since transition functions assistant
mdps depend approximate agent policy   must re solve mdp updating
estimate end episode  see section     policy learning   however  using incremental dynamic programming methods prioritized sweeping  moore   atkeson       
alleviate much computational cost  particular  deploying assistant solve
mdp offline based default agent policy given boltzmann bootstrapping distribution describe section      deployment  prioritized sweeping used incrementally
update q values based learned refinements make  
practical exactly solve assistant mdps  may resort various approximations  consider two approximations experiments  one replace users policy
used computing assistant mdp fixed default user policy  eliminating need
compute assistant mdp every step  denote approximation hd   another approximation uses simulation technique policy rollout  bertsekas   tsitsiklis        approximate
qg  s  a  expression h  done first simulating effect taking action
state using estimate expected cost agent achieve g resulting
state  is  approximate qg  s  a  assuming assistant select single
initial action followed agent actions  formally  let cn    s  g  function simulates n trajectories achieving goal state averaging trajectory costs 

h s  a  ot   except replace qg  s  a  expectation
p heuristic h r identical
 
s   s  a    c     g   combine heuristics  using fixed default
user policy policy rollouts  denote hd r  
      parse ampling
heuristics somewhat myopic sense take account
potentially persistent ambiguity agents goal consider use information
gathering actions resolve ambiguity  cases beneficial consider nonmyopic reasoning  one combine heuristics shallow search belief space

  

fia ecision  t heoretic odel ssistance

assistant mdp  purpose utilize depth bounded sparse sampling trees  kearns 
mansour    ng        compute approximation q function given belief state
 st   ot    denoted qd  st   a  ot    given particular belief state  assistant select
action maximizes qd   note convenience represent belief state pair
current state st observation history ot   lossless representation belief state since
posterior goal distribution computed exactly ot goal hidden
portion pomdp state 
base case q   st   a  ot   equal one myopic heuristics described above 
increasing depth result looking ahead state transitions evaluating one
heuristics  looking ahead possible track potential changes belief state
taking certain actions determine whether changes belief would beneficial
respect providing better assistance  sparse sampling look ahead approximately
computing 
qd  s  a  o    e r s  g  a    v d   s    o    




v  s  o    max q  s  a  o 


   
   

g random variable distributed according goal posterior p  g o   s    o   
random variable represents belief state taking action belief state  s  o  
particular  s  world state arrived o  simply observation sequence extended
observation obtained state transition  first term expectation
represents immediate reward assistant action goal g 
sparse sampling approximates expectation averaging set b samples successor belief states  sparse sampling pseudo code presented table        given input belief
state  s  o   assistant action a  heuristic h  depth bound d  sampling width b algorithm returns  an approximation of  qd  s  a  o   first  depth bound equal zero heuristic
value returned  otherwise b samples observations resulting taking action belief state
 s  o  generated  observations form oi    s i   ai   si    s i state
resulting taking action state s  ai ensuing agent action selected s i based goal
drawn goal posterior  si result taking action ai state s i   observation oi
corresponds new belief state  si    o  oi     o  oi   simply concatenation oi o 
code recursively computes value belief states maximizing qd
actions averages results 
b become large  sparse sampling produce arbitrarily close approximation
true q function belief state mdp  computational complexity sparse sampling linear
b exponential d  thus depth must kept small real time operation 

   learning hgmdps
section  tackle problem learning hgmdp interacting environment assist agent  assume set goals g known agent  primary
role learning acquire agents policy goal distribution  assumption natural
situations assistant applied many times environment  possibly
different agents  example  desktop environment  environment mdp corresponds
description various desktop functionalities  remains fixed across users  one
  

fif ern   natarajan   j udah     tadepalli

given  heuristic function h  belief state  s  o   action a  depth bound d  sampling width b
return  approximation qd  s  a  o  value belief state  s  o 
       return h s  a  o 
   sample set b observations  o            ob   resulting taking action
belief state  s  o  follows 
 a  sample s i environment mdp transition function  s  a   
 b  sample goal gi p  gi  o 
 c  sample agent action ai agent policy   s i   gi  
 d  oi    s i   ai   si    si sample environment mdp transition
function  s i   ai    
   oi    s i   gi   ai   si   compute vi   maxa  qd   si   a     o  oi   
p
   return qd  s  a  o     b r s  gi   a    vi

table    pseudo code sparse sampling hgmdp
provided description mdp typically straightforward learn model
primary cost longer warming period assistant 
relaxing assumption provided set possible goals problematic
current framework  saw section    solution methods depend knowing set
goals clear learn observations  since goals  unlike states
actions  directly observable assistant  extending framework assistant
automatically infer set possible user goals  allow user define goals 
interesting future direction  note  however  often possible designer enumerate
set user goals deployment perhaps complete  allows useful assistance
provided 
    maximum likelihood estimates
straightforward estimate goal distribution g  agent policy simply observing
agents actions  possibly assisted  compute empirical estimates relevant
quantities  done storing goal achieved end episode along
set world state action pairs observed agent episode  estimate ig
based observed frequency goal  usually laplace correction avoid
extreme values probabilities   likewise  estimate  a s  g  simply frequency
action taken agent state goal g  limit
maximum likelihood estimates converge correct values true hgmdp  practice
convergence slow  slow convergence lead poor performance early stages
assistants lifetime  alleviate problem propose approach bootstrapping
learning agent policy  
  

fia ecision  t heoretic odel ssistance

    model based bootstrapping
leverage environment mdp model order bootstrap learning agent policy 
particular  assume agent near optimal sense that  particular goal
world state  likely select actions close optimal  unrealistic
many application domains might benefit intelligent assistants  particular 
many tasks  conceptually simple humans  yet quite tedious  e g   navigating
directory structure computer desktop  performing optimally tasks difficult
humans 
given near rationality assumption  initialize estimate agents policy
prior biased toward optimal agent actions  consider environment mdp
assistant actions removed solve q function q a  s  g  using mdp planning
techniques  q function gives expected cost executing agent action world state
acting optimally achieve goal g using agent actions  world without assistant 
rational agent would always select actions maximize q function state goal 
furthermore  close to rational agent would prefer actions achieve higher q values highly
suboptimal actions  first define boltzmann distribution  used define
prior 
 
 a s  g   
exp k q a  s  g  
   
z s  g 
z s  g  normalizing constant  k temperature constant  using larger values
k skews distribution heavily toward optimal actions  given definition  prior
distribution   w  g  taken dirichlet parameters               a      
   ai  s  g     parameter controls strength prior  intuitively  
thought number pseudo actions represented prior  representing
number pseudo actions involved agent action ai   since dirichlet conjugate
multinomial distribution  form   s  g   easy update posterior
  s  g  observation  one take mode mean posterior point
estimate agent policy used define hgmdp 
experiments  found prior provides good initial proxy actual agent
policy  allowing assistant immediately useful  updating posterior tunes
assistant better peculiarities given agent  example  many cases
multiple optimal actions posterior come reflect systematic bias among equally
good actions agent has  computationally main obstacle approach computing
q function  needs done given application domain since environment
mdp constant  using dynamic programming accomplished polynomial time
number states goals  practical  number alternatives exist including
use factored mdp algorithms  boutilier et al          approximate solution methods  boutilier
et al         guestrin et al          developing domain specific solutions 
finally  work  utilize uninformative prior goal distribution  interesting
future direction would bootstrap goal distribution estimate based observations
population agents 

  

fif ern   natarajan   j udah     tadepalli

   experimental results
section  present results conducting user studies simulations three domains 
two game like environments folder predictor domain intelligent desktop assistant 
user studies two game like domains  episode  users assistants actions
recorded  user studies performed using    human subjects  graduate students cs
department oregon state university  single session  ratio cost achieving
goal assistants help optimal cost without assistant calculated averaged
multiple trials user  present similar results simulations well  third
domain folder predictor domain  simulated user used one heuristics
generate top   recommended folders user  present number clicks required
average user reach desired folder  two three domains  namely  doorman
domain folder predictor domain  fall category hamdps since assistive
actions merely viewed helper actions agent ignore  kitchen domain
hand needs slightly general formulation since agent assistant
strictly alternate  assistants actions cannot ignored agent 
    doorman domain
doorman domain  agent set possible goals collect wood  food
gold  grid cells blocked  cell four doors agent open
door move next cell  see figure     door closes one time step time
one door open  goal assistant help user reach goal faster opening
correct doors 
state tuple hs  di  stands agents cell door open 
total number states         squares   possibilities door   actions agent
open door move   directions pickup whatever cell 
total   actions  assistant open doors perform noop    actions  agents
assistants actions strictly alternate domain  satisfying definition hamdps 
reward    or cost    user open door reward assistants
action  trial ends agent picks desired object  note included
noop action assistant  domain action never selected  since cost opening
wrong door noop same  potential benefit selecting noop 
experiment  evaluated two heuristics  one fixed user policy default
policy hgmdp creation  hd   avoiding need repeated computation hgmdp
every step second use policy rollout calculate q values  hr   
trial  system chooses goal one two heuristics random  user shown
goal tries achieve it  always starting center square  every users action 
assistant opens door nothing  user may pass door open different door 
user achieves goal  trial ends  new one begins  assistant uses
users trajectory update agents policy 
results user studies doorman domain presented tabe    first two
rows give cumulative results user study actions selected greedily according hr
hd respectively  rather reporting negtive rewards  table shows total number

  

fia ecision  t heoretic odel ssistance

figure    doorman domain  agents goal fetch resource  grid cells separated
doors must opened passing through 

actions trials across users without assistant n  total number actions
assistant u  average percentage savings     u n   trials users  
seen  methods reduce number actions      note
assistant selects among four doors random would reduce number actions
    comparison  omniscient assistant knows users goal reduces number
actions           first door always opened user 
experiments  count users first action  number actions reduces      
observed hr appears slight edge hd   one possible reason
using hd   re solve mdp updating user policy  hr always
using updated user policy  thus  rollout reasoning accurate model user 
heuristic
hr
hd
hr
     b    
     b    
     b    
     b    

total
actions
n
   
   
    
    
    
    
    

user
actions
u
   
   
   
   
   
   
   

fractional savings
   u n  
          
         
          
          
          
        
          

time
per
action  in secs
      
      
     
     
    
     
    

table    results experiments doorman domain  first two rows table present
results user studies rest table presents results simulation 

   gives pessimistic estimate usefulness assistant assuming optimal user measure utility
normalized optimal utility without aid assistant 
   note first action requirement easily aviodable  simply equivalent switch indicate
user ready move grid  replace requirement explicitly adding button
interface start new episode 

  

fif ern   natarajan   j udah     tadepalli

another interesting observation individual differences among users 
users always prefer fixed path goal regardless assistants actions  users
flexible  survey conducted end experiment  learned one
features users liked system tolerant choice suboptimal paths 
data reveals system able reduce costs approximately     even
users chose suboptimal trajectories 
conducted experiments using sparse sampling non zero depths  considered
depths         using sampling widths b     b      leaves sparse
sampling tree evaluated using hr simply applies rollout user policy  hence sparse
sampling     b      would correspond heuristic hr   experiments 
conduct user studies  due high cost effort required humans studies 
simulated human users choosing actions according policies learned observed
actions previous user study  results presented last   rows table    note
absolute numbers actions user studies simulations comparable
based different numbers trajectories  human users tested fewer trajectories
minimize fatigue  see sparse sampling increased average run time  last column 
order magnitude  able produce reduction average cost user  result
surprising hindsight  simulated experiments  sparse sampling able sample
exact user policy  i e  sampling learned policy  used
simulations   results suggest small amount non myopic reasoning
positive benefit substantial computation cost  note  however  bulk benefit
realized assistant obtained without reasoning  showing myopic heuristics
well suited domain 
    kitchen domain
kitchen domain  goals agent cook various dishes    shelves
  ingredients each  dish recipe  represented partially ordered plan  ingredients
fetched order  mixed heated  shelves doors
must opened fetching ingredients one door open time 
  different recipes  state consists location ingredients
 bowl shelf table   mixing state temperature state ingredient  if bowl 
door open  state includes action history preserve ordering
plans recipes  users actions are  open doors  fetch ingredients  pour
bowl  mix  heat bake contents bowl  replace ingredient back shelf 
assistant perform user actions except pouring ingredients replacing ingredient
back shelf  restricted assistant pouring ingredients irreversible action  reward non pour actions     experiments conducted    human subjects
computer science graduate students  unlike doorman domain  allowed
assistant take multiple consecutive actions  turn switches user assistant
executes noop action 
domain large state space hence possible update user policy
every trajectory  hence  two heuristics compare use default user policy 
second heuristic addition uses policy rollout compare actions  words  compare
hd hd r   results user studies shown top part table    doorman

  

fia ecision  t heoretic odel ssistance

figure    kitchen domain  user prepare dishes described recipes
right  assistants actions shown bottom frame 

domain  total number agent actions without assistant  percentage reduction due assistant presented  number user actions summed    users
cumulative results presented  observed hd r performs better hd  
observed experiments hd r technique aggressive choosing non noop
actions hd   would wait goal distribution highly skewed toward particular
goal 
heuristic
hd r
hd
hd r
     b    
     b    
     b    
     b    

total
actions
n
    
    
    
    
    
    
    

user
actions
u
    
    
    
    
    
    
    

fractional savings
   u n  
           
           
           
           
          
           
          

time
per
action  secs 
     
     
     
     
     
     
     

table    results experiments kitchen domain  first two rows table present
results user studies last   rows present results simulation 

compared use sparse sampling heuristic simulated user trajectories
domain well  see last   rows table     again  absolute numbers actions
user studies comparable simuations due different numbers trajectories
case  since sparse sampling considers larger number trajectories methods 
policies learned sometimes better learned heuristics  although took
time execute  however  significant difference solution quality
rollouts sparse sampling simulations  showing myopic heuristics performing
  

fif ern   natarajan   j udah     tadepalli

well sparse sampling much less computation  sparse sampling higher depths requires
order magnitude computation time compared rollout 
    folder predictor
section  present evaluation framework real world domain  part
task tracer project  dragunov  dietterich  johnsrude  mclaughlin  li    herlocker        
researchers developed file location system called folder predictor  bao et al          idea
behind folder predictor learning users file access patterns  assistant
help user file accesses predicting folder file accessed
saved 
setting  goal folder predictor minimize number clicks user 
predictor would choose top three folders would minimize cost append
ui  shown ovals figure     also  user taken first recommended folder 
users target folder first recommended folder  user would reach folder zero clicks
reach second third recommended folder one click  user either choose one
recommendations navigate windows folder hierarchy recommendations
relevant 

figure    folder predictor  bao et al         
bao et al  considered problem supervised learning problem implemented costsensitive algorithm predictions cost number clicks user  bao
et al          but  algorithm take account response user
predictions  instance  user chooses ignore recommended folders navigates
folder hierarchy  make re predictions  due fact model
one time prediction consider user responses  also  algorithm considers
restricted set previously accessed folders ancestors possible destinations 
precludes handling possibility user accessing new folder 
decision theoretic model naturally handles case re predictions changing recommendations response user actions  first step  used data collected
user interface used model make predictions  use users response predic  

fia ecision  t heoretic odel ssistance

tions make predictions  also  handle possibility new folder  consider
folders folder hierarchies prediction  used mixture density obtain
probability distribution folders 
p  f       p   f           pl  f  
p  probability according bao et als algorithm         pl uniform probability distribution set folders   ratio number times previously accessed
folder accessed total number folder accesses 
idea behind using density function early stages task  user
accessing new folders later stages user access folders particular task
hierarchy  hence number folder accesses increases value   increases would
eventually converge    hence resulting distribution would converge p    data set
consists collection requests open file  open  save file  saveas   ordered time 
request contains information as  type request  open saveas   current task 
destination folder  etc  data set consists total     open saveas requests  folder
hierarchy consists     folders 
state space consists   parts  current folder user accessing three
recommendations two unordered  would correspond state space size
           
    action user either choose recommended folder select
different folder  action assistant corresponds choosing top   folders
action space size        
    reward case negative number user
clicks  domain  assistant users actions strictly alternate assistant revises
predictions every user action  prior distribution initialized using rewards computed
model developed bao et al         
applied decision theoretic model data set  request  assistant would
make prediction using hd r heuristic  which uses default user policy rollout
method  user simulated  user would accept recommendation shortens
path goal  otherwise would act according optimal policy  user
considered close optimal  unrealistic real world  compare results 
used model developed bao et al  data set present results table   
restricted folder set
folders

one time prediction
      
     

repredictions
    
      

table    results experiments folder predictor domain  numbers indicate average number clicks required agent reach his her correct folder  entry
top left hand cell performance current task tracer  one
bottom right hand cell performance decision theoretic assistant 

table shows average cost folder navigation   different cases  bao et als original
algorithm  algorithm modified include mixture distributions model without
mixture distributions  seen model use mixture distributions
least user cost navigation hence effective  bao et  al shown
  

fif ern   natarajan   j udah     tadepalli

algorithm performs significantly better windows default prediction average
    clicks per folder navigation  improvement attributed two modifications
mentioned earlier  first  use re predictions model natural decisiontheoretic framework model makes one time prediction hence cannot make use
users response recommendations  secondly  considering folders hierarchy
prediction including possibility user accessing new folder found useful 
observed either modifications yields lower cost original algorithm 
combining two changes significantly effective 

   discussion related work
work inspired growing interest success building useful software assistants
 yorke smith et al         lieberman        myers et al          effort focused
building desktop assistants help tasks calendar scheduling  refanidis  alexiadis    yorke smith         email filtering  cohen  carvalho    mitchell         on line diagnostics  skaanning  jensen    kjaerulff         travel planning  ambite  barish  knoblock 
muslea  oh    minton         tasks typically requires designing software system
around specialized technologies algorithms  example  email filtering typically posed
supervised learning problem  cohen et al          travel planning combines information gathering
search constraint propagation  ambite et al          printer diagnostics formulated
bayesian network inference  skaanning et al          approaches focus socially assistive robots setting robot designed aid human agents achieving goals  johnson 
cuijpers  juol  torta  simonov  frisiello  bazzani  yan  weber  wermter  et al          unfortunately plethora systems approaches lacks overarching conceptual framework 
makes difficult build others work  paper  argue decision theoretic
approach provides common framework allows design systems respond
novel situations flexible manner reducing need pre programmed behaviors  formulate general version assistantship problem involves inferring users goals taking
actions minimize expected costs 
earlier work learning apprentice systems focused learning users observation  mahadevan  mitchell  mostow  steinberg    tadepalli        mitchell  caruana  freitag 
j mcdermott    zabowski         work closely related learning demonstration
programming demonstration  johnson        konidaris  kuindersma  grupen    barto       
atkeson   schaal        cypher        lau  wolfman  domingos    weld         emphasis
systems provide interface computer system unobtrusively observe
human user task learn itself  human acts user teacher 
performance system measured quickly system learns imitate user 
i e   supervised learning setting  note imitation assistance two different things
general  expect secretaries learn us  typically expected replace
us  setting  assistants goal reduce expected cost users problem solving 
user assistant capable exactly set actions  assistants actions cost
nothing compared users  makes sense assistant try completely replace
human  even case  assistantship framework different learning demonstration
still requires assistant infer users goal actions trying achieve
it  moreover  assistant might learn solve goal reasoning action set

  

fia ecision  t heoretic odel ssistance

rather shown examples user  general  however  action
set user assistant may different  supervised learning appropriate 
example  case folder predictor  system needs decide set folders
present user  user needs decide choose  awkward
impossible formulate problem supervised learning programming demonstration 
taking decision theoretic view helps us approach assistantship problem principled
manner taking account uncertainty users goals costs taking different
actions  assistant chooses action whose expected cost lowest  framework naturally
prevents assistant taking actions  other noop  assistive action
expected reduce overall cost user  rather learning user behave 
framework assistant learns users policy  similar secretary learns
habits boss  much imitate her  help effective way  work
assumed user mdp small enough solved exactly given users goals 
assumption may always valid  makes sense cases learn user
behave  natural treat case users actions provide exploratory
guidance system  clouse   utgoff        driessens         gives opportunity
system imitate user knows nothing better improve upon users policy
can 
personal assistant systems based pomdp models  however 
systems formulated domain specific pomdps solved offline  instance 
coach system helped people suffering dementia giving appropriate prompts
needed daily activities  boger  poupart  hoey  boutilier  fernie    mihailidis        
use plan graph keep track users progress estimate users responsiveness
determine best prompting strategy  distinct difference approach
single fixed goal washing hands  hidden variable user responsiveness
either low high  rather  formulation goal random variable hidden
assistant  since state action space significantly smaller             states
folder predictor domain   possible solve pomdp exactly  given need
re solve pomdp every user action  becomes prohibitively expensive  yet another
difference length trajectory goal small case hence plan graph
would suffice capture user policy  model  restrict plan graph instead
solve user mdp bootstrap policy  mentioned learning user policy
future direction  work  even though start initial estimate user policy 
update every goal achieved  considered online learning user policy
reasonably good prior  note combination two frameworks  one modeling
users responsiveness modeling users goal  would useful  assistant
infers agent goals relevant hidden properties user  responsiveness 
electric elves  assistant takes many mundane responsibilities human
agent including rescheduling meeting appear user likely miss it 
domain specific pomdp formulated solved offline using variety techniques  one
approach  since system monitors users short regular intervals  radical changes belief
states usually possible pruned search space  varakantham  maheswaran 
  tambe         neither exact approximate pomdp solvers feasible online setting 
pomdp changing learn user  must repeatedly solved 
either costly run  boger et al          complex implement baseline  e g   electric
  

fif ern   natarajan   j udah     tadepalli

elves  varakantham et al          experiments demonstrate simple methods onestep look ahead followed rollouts would work well many domains pomdps
solved online  distinct related work  doshi   gmytrasiewicz         authors introduce
setting interactive pomdps  agent models agents beliefs  clearly 
general complex ordinary pomdps  model simpler assumes
agent oblivious presence beliefs assistant  simplified model suffices
many domains  relaxing assumption without sacrificing tractability would interesting 
several dialogue systems proposed many based decisiontheoretic principles  walker        singh  litman  kearns    walker         instance 
njfun system designed mdp provide assistance user interacting user
providing answer users questions  uses automatic speech recognizer  asr 
interpret human dialogues uses dialogue policy choose best action  the response  
goals user could set standard queries locations restaurants  wineries 
shopping centers etc  state space would dialogue states  i e   current state
dialogue user assistant  such greeting  choice state etc   observations
interpretations dialogues human asr  njfun system usefully
modeled hgmdp  goal assistant infer users query given observations provide appropriate response  initial assistant policy learned training
data  manner similar dialogue policy njfun system 
work related on line plan recognition naturally extended include
hierarchies hierarchical versions hmms  bui  venkatesh    west        pcfgs
 pynadath   wellman         blaylock allen describe statistical approach goal recognition
uses maximum likelihood estimates goal schemas parameters  blaylock   allen        
approaches notion cost reward  incorporating plan recognition
decision theoretic context  obtain natural notion optimal assistance  namely maximizing
expected utility 
substantial research area user modeling  horvitz et al  took bayesian
approach model whether user needs assistance based actions attributes provided
assistance needed spreadsheet application  horvitz et al          hui boutilier used
similar idea assistance text editing  hui   boutilier         use dbns handcoded
parameters infer type user compute expected utility assisting user 
would interesting explore use ideas plan recognition  charniak   goldman       
gal  reddy  shieber  rubin    grosz        chu  song  kautz    levinson        system
take account users intentions attitudes computing optimal policy
assistant 
recently  methods proposed solving pomdps called point based methods  pineau  gordon    thrun        porta  vlassis  spaan    poupart        kurniawati  hsu 
  lee        shani  pineau    kaplow         example method point based value
iteration  pbvi   pineau et al         porta et al         takes set belief points b input
maintains set pomdp  vectors iteration  iteration produces new set vectors optimal belief point respect  vectors previous iteration 
approximation made pbvi compared value iteration guarantee
set  vectors optimal entire belief space  omitting  vectors  pbvi
maintains constant run time per iteration  application efficient point based methods

  

fia ecision  t heoretic odel ssistance

pbvi decision theoretic assistance problem evaluation performance compared
policy rollout sparse sampling methods remains promising research direction 

   summary future work
introduced decision theoretic framework assistant systems described hgmdp
appropriate model selecting assistive actions  computational complexity hgmdps
motivated definition simpler model called hamdp  allows efficient myopic heurstics
tractable special cases 
described approximate solution approach based iteratively estimating agents
goal selecting actions using myopic heuristics  evaluation using human subjects two
game like domains show approach significantly help user  demonstrated
real world folder predictor decision theoretic framework effective state
art techniques folder prediction 
one future direction consider complex domains assistant able series activities parallel agent  another possible direction assume hierarchical goal
structure user goal estimation context  recently  assistantship model
extended hierarchical relational settings  natarajan et al         including parameterized
task hierarchies conditional relational influences prior knowledge assistant  prior
knowledge would relax assumption user mdp solved tractably  knowledge
compiled underlying dynamic bayesian network  bayesian network inference algorithms used infer distribution users goals given sequence atomic actions 
parameters users policy estimated observing users actions 
framework naturally extended case environment partially observable agent and or assistant  requires recognizing actions taken gather
information  e g   opening fridge decide make based available  incorporating sophisticated user modeling includes users forgetting goals  paying attention
important detail  and or changing intentions would extremely important building
practical systems  assistive technology useful assistant quickly learn
new tasks expert users transfer knowledge novice users training 

acknowledgements
material based upon work supported defense advanced research projects agency
 darpa   department interior  nbc  acquisition services division  contract no  nbchd        opinions  findings  conclusions recommendations expressed
material authors necessarily reflect views darpa  alan
fern prasad tadepalli gratefully acknowledge following grants  nsf iis         onr
n                 sriraam natarajan thanks army research office grant number w   nf          young investigator program 

appendix a  proof theorem  
according theory pomdps  optimal action pomdp maximizes sum
immediate expected reward value resulting belief state  of assistant   kaelbling 

  

fif ern   natarajan   j udah     tadepalli

littman    cassandra         agent policy deterministic  initial goal distribution
ig history agent actions states h fully captures belief state agent  let
v  ig   h  represent value current belief state  value function belief state
given following bellman equation  h   stands history assistants
action hi agents action aj  
v  ig   h    max e r  s  hi    g  aj      v  ig   h    
hi

h 

   

since one agents action  s  g   agent action aj   subsequent state s 
value depend hi   hence best helper action h assistant given by 
h  ig   h    arg max e r  s  hi    g   s  g   
hi
x
  arg max
ig  g i ai  s  g  
hi

gc h 

  arg max ig  c h  g s  ai   
hi

c h  set goals consistent current history h  g s  ai   set goals
ai good state s  i ai  s  g   indicator function     ai  s  g  
note h exactly myopic policy   

appendix b  proof lemma  
worst case regret pair  s  g  hamdp given following bellman equation 
assuming g set possible goals agent current state s  regret s  g     
terminal state goals g satisfied s  otherwise  regret s  g    mini maxj  i
 regret si   gi        regret sj   gj      si   gi    sj   gj   children  s  g   
outer min due assistant picking helper action goal gi maximize
reward inner max due agent either accepting it  picking different goal
minimize reward  proof induction  node trajectory tree represents state
set goals g state optimal path 
basis   s  g  leaf node  either terminal state goals g satisfied s  hence
rank equals reward   
inductive step  suppose induction true children  s  g   consider two
cases 
case    unique child  s  g  representing  s    g    highest regret among
children  inductive hypothesis  rank  s    g       regret s    g     assistant chooses
helper action corresponds  s    g     agent choose actions yield lower
regret worst case  choosing helper action would increase regret  since
agent could choose a  add   regret  have  regret s  g   regret s    g     
rank  s    g       rank  s  g   
case    least two children  s    g     s    g     s  g  highest rank
among children  inductive hypothesis  rank  s    g       rank  s    g       regret s    g   
  regret s    g     agent increase regret   choosing goal g 
assistant chooses g  vice versa  hence  regret s  g     regret s    g        rank  s    g      
   

fia ecision  t heoretic odel ssistance

rank  s  g   
hence cases  shown regret s  g  rank  s  g     

appendix c  proof theorem  
first show problem np  build tree representation history dependent policy
initial state  every node tree represeted triple  s  i  g   state 
g set goals good path  index helper action chosen
policy node  root node corresponds possible initial state initial goal
set ig   children node tree represent possible successor nodes  sj   j  gj   reached
agents response hi   whether accepting hi executing ai executing actions 
children resulting ai called accepted  latter called rejected  note
multiple children result action dynamics function agents
goal 
guess policy tree check maximum regret  i e  maximum number
rejected children path root leaf  within bounds  verify optimal
policy tree polynomial size note number leaf nodes upper bounded  g 
maxg n  g   n  g  number leaf nodes generated goal g  estimate n  g  
start root navigate downwards  node contains g goal set 
accepted child contains g  child reached g  not 
misprediction k children reached  hence  number nodes reached g
grows geometrically number mispredictions  theorem    since
log  g  mispredictions path  n  g  k log   g    k logk  g  log  k    g log  k   hence
total number leaf nodes tree bounded  g   log k   total number nodes
tree bounded m g   log k   number steps horizon  since
polynomial problem parameters  problem np 
show np hardness  reduce   sat given problem  consider   literal clause
ci propositional formula possible goal  rest proof identical
theorem   except variables set assistant since universal quantifiers 
agent rejects setting last variable clause clause evaluates   
worst regret goal   iff   sat problem satisfying assignment   

references
ambite  j  l   barish  g   knoblock  c  a   muslea  m   oh  j     minton  s          getting
there  interactive planning agent execution optimizing travel  proceedings
fourteenth conference innovative applications artificial intelligence  pp     
    
atkeson  c  g     schaal  s          learning tasks single demonstration  proceedings
ieee international conference robotics automation  pp           
bao  x   herlocker  j  l     dietterich  t  g          fewer clicks less frustration  reducing
cost reaching right folder  proceedings eleventh international conference
intelligent user interfaces  pp         
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 

   

fif ern   natarajan   j udah     tadepalli

blaylock  n     allen  j  f          statistical goal parameter recognition  proceedings
fourteenth international conference automated planning scheduling  pp         
boger  j   poupart  p   hoey  j   boutilier  c   fernie  g     mihailidis  a          decisiontheoretic approach task assistance persons dementia  proceedings nineteenth international joint conference artificial intelligence  pp           
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions
computational leverage  journal artificial intelligence research          
bui  h   venkatesh  s     west  g          policy recognition abstract hidden markov models 
journal artificial intelligence research             
cassandra  a  r          exact approximate algorithms partially observable markov decision processes  ph d  thesis  brown university 
charniak  e     goldman  r          plan recognition stories life  corr  abs           
chu  y   song  y   kautz  h     levinson  r          start thing
do  interactive activity recognition prompting  proceedings twenty fifth aaai
conference workshop artificial intelligence smarter living  pp       
clouse  j  a     utgoff  p  e          teaching method reinforcement learning  proceedings
ninth international workshop machine learning  pp        
cohen  w  w   carvalho  v  r     mitchell  t  m          learning classify email speech acts 
proceedings conference empirical methods natural language processing  pp 
       
cypher  a          watch do  programming demonstration  mit press 
doshi  p     gmytrasiewicz  p          particle filtering algorithm interactive pomdps 
proceedings workshop modeling agents observations  pp       
dragunov  a  n   dietterich  t  g   johnsrude  k   mclaughlin  m   li  l     herlocker  j  l         
tasktracer  desktop environment support multi tasking knowledge workers  proceedings tenth international conference intelligent user interfaces  pp       
driessens  k          adding guidance relational reinforcement learning  third freiburgleuven workshop machine learning 
ehrenfeucht  a     haussler  d          learning decision trees random examples  information computation                
gal  y   reddy  s   shieber  s   rubin  a     grosz  b          plan recognition exploratory
domains  artificial intelligence                   
geffner  h     bonet  b          solving large pomdps using real time dynamic programming 
proceedings aaai fall symposium pompds 
ginsberg  m  l          gib  steps toward expert level bridge playing program  proceedings sixteenth international joint conference artificial intelligence  pp         
guestrin  c   koller  d   parr  r     venkataraman  s          efficient solution algorithms
factored mdps  journal artificial intelligence research             

   

fia ecision  t heoretic odel ssistance

horvitz  e   breese  j   heckerman  d   hovel  d     rommelse  k          lumiere project 
bayesian user modeling inferring goals needs software users  proceedings
fourteenth conference uncertainty artificial intelligence  pp         
hui  b     boutilier  c          whos asking help   bayesian approach intelligent assistance  proceedings eleventh international conference intelligent user interfaces 
pp         
johnson  d   cuijpers  r   juol  j   torta  e   simonov  m   frisiello  a   bazzani  m   yan  w  
weber  c   wermter  s   et al          socially assistive robots  comprehensive approach
extending independent living  international journal social robotics               
johnson  m          inverse optimal control deterministic continuous time nonlinear systems 
ph d  thesis  university illinois urbana champaign 
kaelbling  l   littman  m     cassandra  a          planning acting partially bservable
stochastic domains  artificial intelligence                  
kearns  m  j   mansour  y     ng  a  y          sparse sampling algorithm near optimal
planning large markov decision processes  proceedings sixteenth international
joint conference artificial intelligence  pp           
konidaris  g   kuindersma  s   grupen  r     barto  a          robot learning demonstration
constructing skill trees  international journal robotics research                
kullback  s     leibler  r          information sufficiency  annals mathematical
statistics              
kurniawati  h   hsu  d     lee  w          sarsop  efficient point based pomdp planning
approximating optimally reachable belief spaces  proceedings robotics  science
systems iv 
lau  t   wolfman  s   domingos  p     weld  d          programming demonstration using
version space algebra  machine learning                  
lieberman  h          user interface goals  ai opportunities  ai magazine              
littlestone  n          learning quickly irrelevant attributes abound  new linear threshold
algorithm  machine learning               
littman  m  l            algorithms sequential decision making  ph d  thesis  brown university 
mahadevan  s   mitchell  t  m   mostow  j   steinberg  l  i     tadepalli  p          apprenticebased approach knowledge acquisition   artificial intelligence             
mitchell  t  m   caruana  r   freitag  d   j mcdermott    zabowski  d          experience
learning personal assistant  communications acm              
moore  a  w     atkeson  c  g          prioritized sweeping  reinforcement learning less
data less time  machine learning             
mundhenk  m          complexity planning partially observable markov decision
processes  ph d  thesis  friedrich schiller universitdt 

   

fif ern   natarajan   j udah     tadepalli

myers  k   berry  p   blythe  j   conleyn  k   gervasio  m   mcguinness  d   morley  d   pfeffer 
a   pollack  m     tambe  m          intelligent personal assistant task time
management  ai magazine  vol      pp       
natarajan  s   tadepalli  p     fern  a          relational hierarchical model decision theoretic
assistance  proceedings seventeenth annual international conference inductive
logic programming  pp         
papadimitriou  c     tsitsiklis  j          complexity markov decision processes  mathematics operations research                
pineau  j   gordon  g     thrun  s          point based value iteration  anytime algorithm
pomdps  proceedings eighteenth international joint conference artificial
intelligence  pp            
porta  j   vlassis  n   spaan  m     poupart  p          point based value iteration continuous
pomdps  journal machine learning research              
pynadath  d  v     wellman  m  p          probabilistic state dependent grammars plan recognition  proceedings sixteenth conference uncertainty artificial intelligence 
pp         
refanidis  i   alexiadis  a     yorke smith  n          beyond calendar mashups  intelligent calendaring  proceedings twenty first international conference automated planning
scheduling system demonstrations 
shani  g   pineau  j     kaplow  r          survey point based pomdp solvers  autonomous
agents multi agent systems             
singh  s  p   litman  d  j   kearns  m  j     walker  m  a          optimizing dialogue management reinforcement learning  experiments njfun system   journal artificial
intelligence research             
skaanning  c   jensen  f  v     kjaerulff  u          printer troubleshooting using bayesian networks  proceedings thirteenth international conference industrial engineering applications artificial intelligence expert systems  pp         
varakantham  p   maheswaran  r  t     tambe  m          exploiting belief bounds  practical
pomdps personal assistant agents  proceedings fourth internation conference
autonomous agents multiagent systems  pp         
walker  m  a          application reinforcement learning dialogue strategy selection
spoken dialogue system email  journal artificial intelligence research             
yorke smith  n   saadati  s   myers  k     morley  d          design proactive personal
agent task management  international journal artificial intelligence tools           
    

   


