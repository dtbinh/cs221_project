journal artificial intelligence research                  

submitted        published     

hc search  learning framework search based
structured prediction
janardhan rao doppa

doppa eecs oregonstate edu

school eecs  oregon state university
corvallis              usa

alan fern

afern eecs oregonstate edu

school eecs  oregon state university
corvallis              usa

prasad tadepalli

tadepall eecs oregonstate edu

school eecs  oregon state university
corvallis              usa

abstract
structured prediction problem learning function maps structured inputs
structured outputs  prototypical examples structured prediction include part ofspeech tagging semantic segmentation images  inspired recent successes
search based structured prediction  introduce new framework structured prediction
called hc search  given structured input  framework uses search procedure guided
learned heuristic h uncover high quality candidate outputs employs
separate learned cost function c select final prediction among outputs 
overall loss prediction architecture decomposes loss due h leading
high quality outputs  loss due c selecting best among generated
outputs  guided decomposition  minimize overall loss greedy stage wise
manner first training h quickly uncover high quality outputs via imitation learning 
training c correctly rank outputs generated via h according true
losses  importantly  training procedure sensitive particular loss function
interest time bound allowed predictions  experiments several benchmark
domains show approach significantly outperforms several state of the art methods 

   introduction
consider problem structured prediction  predictor must produce
structured output given structured input  example  part of speech  pos  tagging  structured input sequence words structured output corresponds
pos tags words  image scene labeling another example  structured
input image structured output semantic labeling image regions 
structured prediction tasks arise several domains ranging natural
language processing  e g   named entity recognition  coreference resolution  semantic
parsing  computer vision  e g   multi object tracking activity recognition videos 
speech  e g   text to speech mapping speech recognition  compuational biology
 e g   protein secondary structure prediction gene prediction  
viewed traditional classification problem  set possible classes structured
prediction exponential size input  thus  problem producing
c
    
ai access foundation  rights reserved 

fidoppa  fern    tadepalli

output combinatorial nature  introduces non trivial choice selecting
computational framework producing outputs  importantly  framework needs
balance two conflicting criteria     must flexible enough allow complex
accurate structured predictors learned     must support inference outputs
within computational time constraints application  one core research
challenges structured prediction achieve balance criteria 
standard approach structured prediction learn cost function c x  y 
scoring potential structured output given structured input x  given cost
function new input x  output computation involves solving so called argmin
problem  find minimum cost output given input 
  arg minyy x  c x  y 

   

example  approaches conditional random fields  crfs   lafferty  mccallum 
  pereira         max margin markov networks  taskar  guestrin    koller       
structured svms  tsochantaridis  hofmann  joachims    altun        represent cost
function linear model template features x y  unfortunately  exactly
solving argmin problem often intractable  efficient solutions exist limited
cases dependency structure among features forms tree  cases  one
forced simplify features allow tractable inference  detrimental
prediction accuracy  alternatively  heuristic optimization method used
loopy belief propagation variational inference  methods shown
success practice  difficult characterize solutions predict
likely work well new problem 
inspired recent successes output space search approaches  doppa  fern 
  tadepalli        wick  rohanimanesh  bellare  culotta    mccallum         place
restrictions form cost function  methods learn use cost
function conduct search space complete outputs via search procedure
 e g   greedy search   return least cost output uncovered search
prediction  search procedure needs able efficiently evaluate cost
function specific input output pairs  generally straightforward even
corresponding argmin problem intractable  thus  methods free increase
complexity cost function without considering impact inference complexity 
approaches achieved state of the art performance number
benchmark problems  primary contribution paper highlight fundamental
deficiency share  particular  prior work uses single cost function serve
dual roles both     guiding search toward good outputs     scoring generated
outputs order select best one  serving dual roles often means cost
function needs make unclear tradeoffs  increasing difficulty learning  indeed 
traditional ai search literature  roles typically served different functions 
mainly heuristic function guiding search  cost evaluation function  often part
problem definition  selecting final output 
paper  study new framework structured prediction called hc search
closely follows traditional search literature  key idea learn distinct functions
roles     heuristic function h guide search generate set
high quality candidate outputs     cost function c score outputs generated
   

fihc search  learning framework search based structured prediction

heuristic h  given structured input  predictions made using h guide
search strategy  e g   greedy search beam search  time bound generate set
candidate outputs returning generated output least cost according c 
move hc search might appear relatively small  significant
implications terms theory practice  first  regret hc search
approach decomposed loss due h leading high quality outputs 
loss due c selecting best among generated outputs  decomposition
helps us target training minimize losses individually greedy stagewise manner  second  show  performance approaches single
function arbitrarily bad compared hc search worst case 
finally  show practice hc search performs significantly better single
cost function search state of the art approaches structured prediction 
effectiveness hc search approach particular problem depends critically
on     quality search space complete outputs used  quality
defined expected depth target outputs  zero loss outputs  located    
ability learn heuristic function effectively guiding search generate highquality candidate outputs     accuracy learned cost function selecting
best output among candidate outputs generated heuristic function  work 
assume availability efficient search space complete outputs provide
effective training regime learning heuristic function cost function within
hc search framework 

    summary contributions
main contributions work follows     introduce hc search framework  two different functions learned serve purposes search heuristic
cost function search literature     analyze representational power
computational complexity learning within hc search framework     identify
novel decomposition overall regret hc search approach terms generation
loss  loss due heuristic generating high quality candidate outputs  selection
loss  loss due cost function selecting best among generated outputs    
guided decomposition  propose stage wise approach learning heuristic
cost functions based imitation learning     empirically evaluate hc search
approach number benchmarks  comparing state of the art methods analyzing different dimensions framework 
remainder paper proceeds follows  section    introduce problem
setup  give high level overview framework  analyze complexity hc search
learning problem  describe approaches heuristic cost function learning
section    section   presents experimental results followed engineering methodology applying framework new problems section    finally  sections    
discuss related work future directions 
   

fidoppa  fern    tadepalli

   hc search framework
section  first state formal problem setup describe specifics
search spaces search strategies investigate work  next  give
high level overview hc search framework along learning objective 
    problem setup
structured prediction problem specifies space structured inputs x   space structured outputs y  non negative loss function l   x      l x       
loss associated labeling particular input x output   true output   provided training set input output pairs   x     drawn
unknown target distribution d  goal return function predictor structured inputs outputs whose predicted outputs low expected loss respect
distribution d  since algorithms learning heuristic cost functions
input output pairs  standard structured prediction  assume availability
feature function   x    n computes n dimensional feature vector
pair  importantly  employ two different feature functions h c heuristic
cost function noting serving two different roles  heuristic making
local decisions guide search towards high quality outputs cost function
making global decisions scoring candidate outputs generated heuristic
framework 
    search spaces search strategies
overview basic search concepts context search based framework below 
      search spaces
approach based search space complete outputs  assume
given  every state search space complete outputs consists input output
pair  x  y   representing possibility predicting output structured input
x  search space defined terms two functions     initial state function
i x  returns initial state input x     successor function
search state  x  y   s  x  y   returns set next states   x  y        x  yk   
share input x parent  example  sequence labeling problem 
part of speech tagging   x  y  sequence words corresponding part of speech
 pos  labels  successors  x  y  might correspond ways changing one
output labels y  so called flipbit space  figure   provides illustration
flipbit search space handwriting recognition task 
search space quality  effectiveness hc search framework depends
quality search space used  quality search space turn
understood terms expected amount search needed uncover correct output
  search procedures  time required find target output grow
function depth target  thus  one way quantify expected amount
search  independently specific search strategy  considering expected depth
target outputs   particular  given input output pair  x     target depth
   

fihc search  learning framework search based structured prediction

figure    example flipbit search space handwriting recognition problem 
search state consists complete input output pair complete output
every state differs parent exactly one label  highlighted
state corresponds one true output smallest depth 
equal number errors initial state 

   

fidoppa  fern    tadepalli

defined minimum depth find state corresponding target
output  d     example flipbit space shown figure     clearly according
definition  expected target depth flipbit space equal expected number
errors output corresponding initial state 
variety search spaces  flipbit space  limited discrepancy search
 lds  space  doppa et al          defined based hand designed proposal
distributions  wick et al         used past research  work applies
space  focus lds space experiment  shown
effectively uncover high quality outputs relatively shallow search depths  doppa et al  
      
lds space defined terms recurrent classifier h uses next input
token  e g  word  output tokens small preceding window  e g  pos labels 
predict next output token  initial state lds space consists input
x paired output recurrent classifier h x  one problem recurrent
classifiers recurrent classifier makes mistake  effects get propagated
down stream tokens  lds space designed prevent error propagation
immediately correcting mistakes made continuing recurrent classifier 
since know mistakes made correct them  possible
corrections  called discrepancies  considered  hence successors state  x  y 
lds space consist results running recurrent classifier changing exactly
one label  i e   introducing single new discrepancy  somewhere current output
sequence preserving previously introduced discrepancies  previous work 
lds space shown effective uncovering high quality outputs relatively
shallow search depths  one would expect good recurrent classifier  doppa et al  
       appendix contains details examples lds space employ
work 
      search strategies
recall hc search framework  role search procedure uncover highquality outputs  consider uninformed informed search strategies  however 
uninformed search procedures depth bounded breadth first search practical
high quality outputs exist small depths even feasible 
good choice dont use search time bound intelligent way
make predictions  structured prediction problems  informed search strategies
take heuristic functions account  greedy search best first search better
choice  noting effectiveness depends quality search heuristic h  prior
work  doppa et al         wick et al         shown greedy search  hill climbing
based heuristic value  works quite well number structured prediction tasks
used effective search space  thus  work  focus empirical work
hc search framework using greedy search  though approach applies widely 
    hc search approach
approach parameterized search space complete outputs  e g   lds
space   heuristic search strategy  e g   greedy search   learned heuristic function
   

fihc search  learning framework search based structured prediction

figure    high level overview hc search framework  given structured input x
search space definition   first instantiate search space complete
outputs  search node space consists complete input output pair 
next  run search procedure  e g   greedy search  guided heuristic
function h time bound   highlighted nodes correspond search
trajectory traversed search procedure  case greedy search 
scores nodes correspond cost values  different heuristic scores  not shown figure   return least cost output
uncovered search prediction input x 

h   x      learned cost function c   x      given input x
prediction time bound   hc search makes predictions follows  traverses search
space starting i x  using search procedure guided heuristic function h
time bound exceeded  cost function c applied find return least cost
output generated search prediction input x  figure   gives
high level overview hc search framework 
formally  let yh  x  set candidate outputs generated using heuristic h
given input x  output returned hc search least cost output
set according c  i e  
  arg minyyh  x  c x  y 
   

fidoppa  fern    tadepalli

figure    example illustrates c search suffer arbitrarily large loss compared hc search 

expected loss hc search approach e h  c  given heuristic h c
defined
e  h  c    e x y  d l  x  y   
   
goal learn heuristic function h corresponding cost function c minimize
expected loss respective spaces h c  i e  
 h   c     arg min h c hc e  h  c 

   

contrast framework  existing approaches output space search  doppa et al  
      wick et al         use single function  say c  serve dual purpose heuristic
cost function  raises question whether hc search  uses two different functions  strictly powerful terms achievable losses  following
proposition shows expected loss hc search arbitrarily smaller
restricting using single function c 
proposition    let h c functions function space 
learning problems  minc e c  c  min h c  e h  c   moreover exist learning problems
minc e c  c  arbitrarily larger  i e  worse  min h c  e h  c  
proof  first part proposition follows fact first minimization
subset choices considered second 
see second part  consider problem single training instance search
space shown figure    search procedure greedy search either guided
h hc search  c one function used  l n   n  represents
true loss feature vector node n respectively  cost heuristic functions
linear functions  n   node   corresponds lowest loss output greedy search
must follow trajectory highlighted nodes order reach output  first consider
hc search  highlighted path followed heuristic h needs satisfy
following constraints  h    h     h    h     weights wh             result
   

fihc search  learning framework search based structured prediction

heuristic satisfies constraints  given heuristic function  order return node
  final output  cost function must satisfy following constraints  c    c    
c    c     c    c     c    c     weights wc             solve problem 
thus see hc search achieve zero loss problem 
consider case single function c used heuristic cost
function  order generate loss zero  function c must satisfy combined
set constraints placed heuristic cost function  however 
verified set weights satisfies c    c    c    c    
hence  single function c space achieve loss zero 
scaling losses constant factors make loss suffered arbitrarily high 
thus  see potential representational advantages following hcsearch framework  follows  consider implications added expressiveness
terms worst case time complexity learning 
    learning complexity
consider feasibility efficient  optimal learning simplest setting greedy
search using linear heuristic cost functions represented weight vectors wh
wc respectively  particular  consider hc search consistency problem 
input training set structured examples  must decide whether
exists wh wc hc search using greedy search achieve zero loss
training set  first note  problem shown np hard appealing
results learning beam search  xu  fern    yoon      a   particular  results
imply trivial cases  simply determining whether linear
heuristic wh uncovers zero loss search node np hard  since hc search
return zero loss outputs heuristic able uncover them  see problem
hard 
prove stronger result provides insight hc search framework  particular  show even easy learn heuristic uncovers
zero loss outputs  consistency problem still hard  shows  worst case
hardness learning problem simply result hardness discovering
good outputs  rather problem additionally complicated potential interaction
h c  intuitively  learning h worst case ambiguity
many small loss outputs generate 
able find effective c return best one  formalized following
theorem  whose proof appendix 
theorem    hc search consistency problem greedy search linear heuristic
cost functions np hard even restrict problems possible
heuristic functions uncover zero loss output 

   learning approach
complexity result suggests that  general  learning optimal  h   c   pair
impractical due potential interdependence  section  develop greedy
   

fidoppa  fern    tadepalli

stage wise learning approach first learns h corresponding c  approach
motivated observing decomposition expected loss components due h
c  below  first describe decomposition staged learning approach
motivates  next describe approaches learning heuristic cost functions 
    loss decomposition staged learning
heuristic h cost function c  expected loss e  h  c  decomposed
two parts     generation loss h   due h generating high quality outputs 
   selection loss c h   additional loss  conditional h  due c selecting
best loss output
best loss output generated heuristic  formally  let yh
set yh  x   i e  

yh
  arg minyyh  x  l x  y   

express decomposition follows 


e  h  c    e x y  d l  x  yh
      e x y  d l  x  y    l  x  yh
  y 
 
 z
 
 
 z
 
h

   

c h

note given labeled data  straightforward estimate generation
selection loss  useful diagnosing hc search framework  example  one
observes system high generation loss  little payoff working
improve cost function  empirical evaluation illustrate
decomposition useful understanding results learning 
addition useful diagnosis  decomposition motivates learning approach targets minimizing errors separately  particular  optimize
overall error hc search approach greedy stage wise manner  first train
heuristic h order optimize generation loss component h train cost
function c optimize selection loss c h conditioned h 
h arg minhh h
c arg mincc c h
note approach greedy sense h learned without considering
proof theorem   hinges coupling 
implications learning c 
found practice  learning h independently c effective strategy 
follows  first describe generic approach heuristic function learning
applicable wide range search spaces search strategies  explain
cost function learning algorithm 
    heuristic function learning
generally  learning heuristic viewed reinforcement learning  rl  problem heuristic viewed policy guiding search actions rewards
   

fihc search  learning framework search based structured prediction

received uncovering high quality outputs  zhang   dietterich         fact 
approach explored structured prediction case greedy search  wick 
rohanimanesh  singh    mccallum        shown effective given carefully
designed reward function action space  viable approach  general purpose
rl quite sensitive algorithm parameters specific definition reward
function actions  make designing effective learner quite challenging  indeed  recent work  jiang  teichert  daume iii    eisner         shown generic rl
algorithms struggle structured prediction problems  even significant effort
put forth designer  hence  work  follow approach based imitation
learning  makes stronger assumptions  nevertheless effective
easy apply across variety problems 
algorithm   heuristic function learning via exact imitation
input    training examples   i  s    search space definition  l   loss function   
rank based search procedure  max   search time bound
output  h  heuristic function
   initialize set ranking examples r  
   training example  x   
  
s    i x     initial state search tree
  
m     s       set open nodes internal memory search procedure
  
search step     max
  
select state s  expand  nt  select a  l  mt   
  
expand every state nt using successor function s  ct  expand nt   s 
  
prune states update internal memory state search procedure 
mt  prune a  l  mt  ct   nt  
  
generate ranking examples rt imitate search step
   
add ranking examples rt r  r   r rt    aggregation training data
   
end
    end
    h  rank learner r     learn heuristic function ranking examples
    return learned heuristic function h
heuristic learning approach based observation many structured
prediction problems  quickly generate high quality outputs guiding
search procedure using true loss function l heuristic  obviously
done training data know   suggests formulating heuristic
learning problem framework imitation learning attempting learn heuristic
mimics search decisions made true loss function training examples 
learned heuristic need approximate true loss function uniformly output
space  need make distinctions important guiding search 
main assumptions made approach are     true loss function provide effective
heuristic guidance search procedure  worth imitating    
learn imitate search decisions sufficiently well 
imitation learning approach similar prior work learning single cost functions
output space search  doppa et al          however  key distinction learning
   

fidoppa  fern    tadepalli

focused making distinctions necessary uncovering good outputs  the purpose
heuristic  hence requires different formulation  prior work  order
avoid need approximate loss function arbitrarily closely  restrict
rank based search strategies  search strategy called rank based makes
search decisions comparing relative values search nodes  their ranks  assigned
heuristic  rather sensitive absolute values heuristic  common
search procedures greedy search  beam search  best first search fall
category 
      imitating search behavior
given search space complete outputs s  rank based search procedure a 
search time bound   learning procedure generates imitation training data
training example  x    follows  run search procedure time bound
input x using heuristic equal true loss function  i e  h x  y    l x  y    
search process observe pairwise ranking decisions made using
oracle heuristic record sufficient  see below  replicating search 
state  x  y    smaller loss  x  y     ranking example generated
form constraint h x  y    h x  y     ties broken using fixed arbitrator   
aggregate set ranking examples collected training examples given
learning algorithm learn weights heuristic function 
learn function h hypothesis space h consistent
ranking examples  learned heuristic guaranteed replicate oracle guided
search training data  further  given assumptions base learning algorithm
 e g  pac   generic imitation learning results used give generalization guarantees
performance search new examples  khardon        fern  yoon    givan       
syed   schapire        ross   bagnell         experiments show  simple
approach described above  performs extremely well problems 
algorithm   describes approach heuristic function learning via exact imitation
search guided loss function  applicable wide range search spaces  search
procedures loss functions  learning algorithm takes input         x     
set training examples structured prediction problem  e g   handwriting recognition  
      i  s   search space complete outputs  e g   lds space   initial
state function successor function     l  task loss function defined
complete outputs  e g   hamming loss      a  rank based search procedure  e g   greedy
search      max   search time bound  e g   number search steps  
algorithmic description algorithm   assumes search procedure
described terms three steps executed repeatedly open list search
nodes     selection     expansion    pruning  execution  search procedure
selects one open nodes internal memory expansion  step    based
heuristic value  expands selected nodes generate candidate set  step    
retains subset open nodes expansion internal memory
prunes away remaining ones  step    based heuristic value  example 
   lds space employed work  implemented arbitrator breaks ties
based position discrepancy  prefers earlier discrepancies  

   

fihc search  learning framework search based structured prediction

greedy search maintains best node  best first beam search retains best b
nodes fixed beam width b  pure best first search pruning 
algorithm   loops training example collects set ranking constraints  specifically  example  x     search procedure run time bound
max using true loss function l heuristic  steps        search step
set pairwise ranking examples generated sufficient allowing search step
imitated  step    described detail below  constraints
aggregated across search steps training examples  given rank learning
algorithm  e g   perceptron svm rank  learn weights heuristic function
 step     
important step heuristic function learning algorithm generation
ranking examples imitate step search procedure  step     follows 
give generic description sufficient pairwise decisions imitate search 
illustrate greedy search simple example 
      sufficient pairwise decisions
noted need collect learn imitate sufficient pairwise
decisions encountered search  say set constraints sufficient
structured training example  x     heuristic function consistent
constraints causes search follow trajectory open lists encountered
search  precise specification constraints depends actual search procedure
used  rank based search procedures  sufficient constraints
categorized two types 
   selection constraints  ensure search node s  internal memory
state expanded next search step  are  ranked better
nodes 
   pruning constraints  ensure internal memory state  set search nodes 
search procedure preserved every search step  specifically 
constraints involve ranking every search node internal memory state better
 lower h value  pruned 
below  illustrate constraints concretely greedy search noting similar
formulations rank based search procedures straightforward  see  doppa  fern 
  tadepalli      a  beam search formulation  
      constraints greedy search
basic rank based search procedure  given input x  traverses
search space selecting next state successor current state looks best
according heuristic function h  particular  si search state step i  greedy
search selects si     arg minss si   h s   s    i x   greedy search  internal
memory state search procedure step consists best open  unexpanded 
node si  
   

fidoppa  fern    tadepalli

figure    example search tree illustrates greedy search loss function 
node represents complete input output pair evaluated using loss
function  highlighted nodes correspond trajectory greedy search
guided loss function 

let  x  yi   correspond input output pair associated state si   since greedy
search maintains single open node si internal memory every search step i 
selection constraints  let ci   candidate set expanding state si  
i e   ci     s si    let si   best node candidate set ci   evaluated
loss function  i e   si     arg minsci   l s   greedy search prunes nodes
candidate set si     pruning constraints need ensure si   ranked better
nodes ci     therefore  include one ranking constraint every
node  x  y  ci      x  yi     h x  yi       h x  y  
illustrate ranking constraints example  figure   shows
example search tree depth two associated losses every search node 
highlighted nodes correspond trajectory greedy search loss function
learner imitate  first search step   h      h     h      h     pruning
constraints  similarly   h       h     h       h     form pruning constraints
second search step  therefore  aggregate set constraints needed imitate greedy
search behavior shown figure   are 
 h      h     h      h     h       h     h       h     
    cost function learning
given learned heuristic h  want learn cost function correctly ranks
potential outputs generated search procedure guided h  formally  let yh  x 
set candidate outputs generated search procedure guided heuristic h
given input x  lbest loss best output among outputs evaluated
true loss function l  i e   lbest   minyyh  x  l x  y     exact learning scenario 
goal find parameters cost function c every training example
   

fihc search  learning framework search based structured prediction

 x     loss minimum cost output equals lbest   i e   l x  y      lbest  
  arg minyyh  x  c x  y   practice  exact learning isnt possible  goal
find cost function average loss training data predicted output
using cost function minimized 
algorithm   cost function learning via cross validation
input    training examples    search space definition  l   loss function   
search procedure  max   search time bound
output  c  cost function
   divide training set k folds d    d      dk
      learn k different heuristics h      hk
       k
  
ti   j  i dj    training data heuristic hi
  
hi   learn heuristic ti     l  a  max      heuristic learning via algorithm  
   end
      generate ranking examples cost function training
   intialize set ranking examples r  
       k
   
training example  x    di
   
generate outputs running search procedure heuristic hi time
bound max   yhi  x    generate outputs x    a  hi   max  
   
compute set best loss outputs  ybest    y yhi  x  l x  y      lbest   
lbest   minyyh  x  l x  y   

   
pair outputs  ybest   y  ybest yhi  x    ybest
   
add ranking example c x  ybest     c x  y  r
   
end
   
end
    end
       train cost function ranking examples
    c   rank learner r 
    return learned cost function c
formulate cost function training problem instance rank learning problem
 agarwal   roth         specifically  want best loss outputs yh  x 
ranked better non best loss outputs according cost function 
bi partite ranking problem  let ybest set best loss outputs yh  x   i e  
ybest    y yh  x  l x  y      lbest    generate one ranking example every pair
outputs  ybest   y  ybest yh  x    ybest   requiring c x  ybest   c x  y   search
procedure able generate target output  i e   lbest       similar
standard learning crfs svm struct  results much simpler rank learning
problem  cost function needs rank correct output incorrect outputs
generated search   set best loss outputs ybest large  bi partite
ranking may result highly over constrained problem  cases  one could relax
problem attempting learn cost function ranks least one output ybest higher
non best loss outputs  easily implemented online learning
   

fidoppa  fern    tadepalli

framework follows  error  i e   best cost output according current
weights
  ybest    weights updated ensure best cost output ybest ybest
according current weights ranked better outputs yh  x    ybest  
important note theory practice  distribution outputs
generated learned heuristic h testing data may slightly different
one training data  thus  train c training examples used train h  c
necessarily optimized test distribution  mitigate effect  train
cost function via cross validation  see algorithm    training cost function
data  used train heuristic  training methodology commonly
used re ranking style algorithms  collins        among others 
algorithm   describes approach cost function training via cross validation 
four main steps algorithm  first  divide training data k folds 
second  learn k different heuristics  heuristic hi learned using data
folds excluding ith fold  steps       third  generate ranking examples
cost function learning described using heuristic hi data
trained  steps        finally  give aggregate set ranking examples r rank
learner  e g   perceptron  svm rank  learn cost function c  step     
    rank learner
section  describe specifics rank learner used learn
heuristic cost functions aggregate sets ranking examples produced
algorithms  use off the shelf rank learning algorithm  e g   perceptron 
svm rank  base learner train heuristic function set ranking
examples r  specific implementation employed online passive aggressive
 pa  algorithm  crammer  dekel  keshet  shalev shwartz    singer        base
learner  training conducted    iterations experiments 
pa online large margin algorithm  makes several passes training
examples r  updates weights whenever encounters ranking error  recall
ranking example form h x  y      h x  y    heuristic training c x  y     
c x  y    cost function training  x structured input target output  
y  y  potential outputs x l x  y        l x  y       let   
difference losses two outputs involved ranking example 
experimented pa variants use margin scaling  margin scaled   slack
scaling  errors weighted    tsochantaridis  joachims  hofmann    altun         since
margin scaling performed slightly better slack scaling  report results pa
variant employs margin scaling  give full details margin scaling
update 
let wt current weights linear ranking function 
ranking error
cycling training data  i e   wt  x  y    wt  x  y        new
weights wt   corrects error obtained using following equation 
wt     wt     x  y     x  y    
   

fihc search  learning framework search based structured prediction

learning rate given
wt  x  y    wt  x  y     
 
k x  y     x  y   k 





specific update previously used cost sensitive multiclass classification
 crammer et al          see equation     structured output problems  keshet 
shalev shwartz  singer    chazan         see equation    

   experiments results
section empirically investigate hc search approach compare
state of the art structured prediction 
    datasets
evaluate approach following four structured prediction problems including
three benchmark sequence labeling problems  d image labeling problem 
handwriting recognition  hw   input sequence binary segmented
handwritten letters output corresponding character sequence  a z    
dataset contains roughly      examples divided    folds  taskar et al  
       consider two different variants task work hal daume
iii  langford  marcu         hw small version  use one fold training
remaining   folds testing  vice versa hw large 
nettalk stress  text to speech mapping problem  task
assign one   stress labels letter word       training
words      test words standard dataset  use sliding window size
  observational features 
nettalk phoneme  similar nettalk stress except task
assign one    phoneme labels letter word 
scene labeling  data set contains     images outdoor scenes  vogel  
schiele         image divided patches placing regular grid size
     entire image  patch takes one   semantic labels  sky 
water  grass  trunks  foliage  field  rocks  flowers  sand    simple appearance features
including color  texture position used represent patch  training
performed     images  remaining     images used testing 
    experimental setup
hc search experiments  use limited discrepancy space  lds  exactly
described work doppa et al         search space structured outputs 
prior work hc search shown greedy search works quite well structured prediction tasks  particularly using lds space  doppa et al          hence 
consider greedy search experiments  would point experiments shown using beam search best first search produce similar results 
   

fidoppa  fern    tadepalli

training testing set search time bound    search steps domains
except scene labeling  much larger search space uses       
found using values larger produce noticeable improvement 
extremely small values   performance tends worse  increases quickly
made larger  show results full spectrum time bounds later 
domains  learn linear heuristic cost functions second order features unless otherwise noted  case  feature vector measures features neighboring label pairs
triples along features structured input  measure error hamming
loss unless otherwise noted 
    comparison state of the art
compare results hc search approach structured prediction algorithms including crfs  lafferty et al          svm struct  tsochantaridis et al         
searn  hal daume iii et al          cascades  weiss   taskar        c search 
identical hc search except uses single function output space search
 doppa et al          show performance recurrent  simple
recurrent classifier trained exactly work doppa et al          top section
table   shows error rates different algorithms  scene labeling
possible run crfs  svm struct  cascades due complicated grid structure
outputs  hence   table   report best published results crfs 
svm struct  searn  cascades trained using implementation  weiss        provided authors  used sequence labeling problems hamming loss 
would point results cascades differ appear
work doppa  fern  tadepalli        obtained using updated  version
cascades training code  across benchmarks  see results hc search comparable significantly better state of the art including c search  uses single
function heuristic function cost function  results scene labeling
domain significant improving error rate              results
show hc search state of the art approach across problems learning
separate heuristic cost functions significantly improve output space search 
    higher order features
one advantages approach compared many frameworks structured prediction ability use expressive feature spaces without paying huge computational
price  bottom part table   shows results using third order features  compared
second order above  hc search  c search cascades  note practical
run methods using third order features due substantial increase inference
time  overall error hc search higher order features slightly improved compared
using second order features across benchmarks still better error rates
c search cascades third order features  exception cascades
hw large  fact  hc search using second order features still outperforming
third order results methods three five domains 
   personal communication author

   

fihc search  learning framework search based structured prediction

algorithms
hw small

hw large

datasets
stress phoneme

scene labeling

hc search
c search
crf
svm struct
recurrent
searn
cascades

a  comparison state of the art
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

     
     
     
     
 

hc search
c search
cascades

b  results third order features
     
     
     
     
     
     
     
     
     
     
     
     

     
     
 

table    error rates different structured prediction algorithms 
    loss decomposition analysis
examine hc search c search terms loss decomposition  see equation    generation loss h selection loss c h   quantities
easily measured hc search c search keeping track best loss output
generated search  guided either heuristic cost function c search 
across testing examples  table   shows results  giving overall error hc
decomposition across benchmarks hc search c search 
first see generation loss h similar c search hc search across
benchmarks exception scene labeling  hc search generates slightly better
outputs  shows least lds search space difference performance
c search hc search cannot explained c search generating lower quality
outputs  rather  difference two methods reflected difference
selection loss c h   meaning c search effective ranking outputs
generated search compared hc search  result clearly shows advantage
separating roles c h understandable light training mechanism
c search  approach  cost function trained satisfy constraints related
generation loss selection loss  turns many generation
loss constraints  hypothesize biases c search toward low generation loss
expense selection loss 
results show methods selection loss c h contributes significantly overall error compared h   shows approaches
able uncover high quality outputs  unable correctly rank generated
outputs according losses  suggests first avenue improving results
hc search would improve cost function learning component  e g  using
non linear cost functions 
   

fidoppa  fern    tadepalli

    ablation study
futher demonstrate two separate functions  heuristic cost function 
hc search lead accurate predictions compared using single function
c search  perform ablation experiments  study  take learned
heuristic function h cost function c hc search framwork  use one
make predictions  example  hh search corresponds configuration
use function h heuristic cost function  similarly  cc search corresponds
configuration use function c heuristic cost function 
table  b shows results ablation experiments  make several interesting observations results  first  overall error hc search significantly
better hh search cc search  second  selection loss hh search
increases compared hc search  understandable h trained
score candidate outputs generated search  third  generation
loss cc search increases compared hc search behavior significant
 increases       compared       scene labeling task  results provide
evidence importance separating training heuristic cost
functions 
hw small
c h
h

stress
c h
h

hc

    
    

a  hc search vs  c search
                        
                        

    
    

    
    

    
    

    
    

    
    

    
    

    
    

b  results ablation study
        
              
                        

    
    

    
    

    
    

    
    

    
    

    
    

c  results heuristic function training via dagger
                                            
                                            

    
    

    
    

    
    

    
    

    

    

    

    

datasets
error

hc

hc search
c search

    
    

    
    

    
    

hh search
cc search

    
    

    
    

    
    

hc search
c search

    
    

    
    

hc

hw large
c h
h

hc

phoneme
c h
h

hc

scene
c h
h

d  results oracle heuristic
lc search
 oracle h 

    

    

    

    

    

    

    

    

    

    

    

table    hc search  error decomposition heuristic cost function 
    results heuristic training via dagger
heuristic learning approach follows simplest approach imitation learning  exact
imitation  learner attempts exactly imitate observed expert trajectories
 here imitate search oracle heuristic   experiments show exact
imitation performs quite well  known exact imitation certain deficiencies
general  particular  functions trained via exact imitation prone error propagation  kaariainen        ross   bagnell         errors made test time change
distribution decisions encountered future compared training distribution 
address problem  sophisticated imitation learning algorithms developed  state of the art approach dagger  ross  gordon    bagnell        
   

fihc search  learning framework search based structured prediction

consider whether dagger improve heuristic learning turn overall
accuarcy 
dagger iterative algorithm  iteration adds imitation data aggregated data set  first iteration follows exact imitation approach  data
collected observing expert trajectory  or number them   iteration
imitation function  here heuristic  learned current data  successive iterations
generate trajectories following mixture expert suggestions  in case ranking decisions  suggestions recently learned imitation function  decision point
along trajectory added aggregate data set labeling expert decision 
way  later iterations allow dagger learn states visited possibly erroneous learned functions correct mistakes using expert input  ross et al        
show iterations dagger using learned policy without mixing
expert policy performs well across diverse domains  therefore  use
approach dagger experiments  experiments run   iterations dagger 
noting noticable improvement observed   iterations 
table  c shows results hc search c search obtained training dagger  hc search  generation loss  h   improved slightly sequence labeling
problems little room improvement  dagger leads significant improvement generation loss challenging problem scene labeling 
see overall error hc search scene labeling reduces due improvement
generation loss showing cost function able leverage better outputs produced
heuristic  similarly  overall error c search improved dagger across
board see significant improvements handwiriting scene labeling
domains  interesting note unlike hc search  improvement c search
mostly due improvement selection loss  c h   except scene labeling task 
due improvement generation loss selection loss 
results show improving heuristic learning able improve overall
performance  clear whether improvement  perhaps due future
advances imitation learning  would yet lead overall improvement  is 
may possible improve generation loss  clear cost function
able exploit improvments  help evaluate ran experiment
gave hc search true loss function use heuristic  an oracle heuristic   i e  
h x  y    l x  y     training cost function testing  provides
assessment much better might able could improve heuristic
learning  results table    label lc search  oracle h  show
using oracle heuristic  h negligible might expect smaller observed
hc search  shows may possible improve heuristic learning
via better imitation 
see oracle results overall error hc better
hc search  hw small scene labeling tasks  selection error c h got slightly
worse   indicates cost function learner able leverage  varying degrees  better outputs produced oracle heuristic  suggests improving
heuristic learner order reduce generation loss could viable way
reducing overall loss hc search  even without altering current cost learner  however  saw much less room improve heuristic learner
   

fidoppa  fern    tadepalli

data sets hence potential gains less directly trying improve cost
learner 
    results training different time bounds

train

trained hc search different time bounds  i e   number greedy search steps 
see overall loss  generation loss selection loss vary increase training
time bound  general  time bound increases  generation loss monotonically
decrease  since strictly outputs encountered  hand difficulty
cost function learning increase time bound grows since must learn distinguish larger set candidate outputs  thus  degree overall
error decreases  or grows  time bound depends combination much
generation loss decreases whether cost function learner able accurately
distinguish improved outputs 
figure   shows performance hc search full spectrum time bounds 
qualitatively  see generation loss  due heuristic  decreases remarkably
fast benchmarks improves little initial decrease  see
cost function learner achieves relatively stable selection loss short time  though
increase bit time cases  combined effect see overall
error hc improves quickly increase time bound improvement tends
small beyond certain time bound  also  cases  e g   phoneme prediction
scene labeling  performance tends get slightly worse large time bounds 
happens increase selection loss counteracted decreased generation
loss 
loss function
hamming
vc

test
hamming
vc
    
    
    
    

table    results training non hamming loss functions 

    results training non hamming loss functions
one advantages hc search compared many approaches structured
prediction sensitive loss function used training  trained hcsearch different loss functions handwriting domain verify true
practice not  used hamming loss  uniform misclassification cost   characters 
vowel consonant  vc  loss  different misclassification costs vowels consonants 
experiment  vc loss  used misclassification costs     vowels
consonants respectively  training done   folds remaining   folds used
testing  table     shows results training testing two loss functions 
report cumulative loss testing examples  see  testing
loss function  training loss function gives slightly better performance
training using different loss function  shows hc search learning approach
   

fihc search  learning framework search based structured prediction

figure    hc search results training different time bounds  training time
bound  i e   no  greedy search steps  x axis error y axis 
three curves graph corresponding overall loss hc   generation loss h
selection loss c h  

   

fidoppa  fern    tadepalli

sensitive loss function  however  result may hold generally much
depends problem structure  loss function ability cost function
capture loss 
     discussion efficiency hc search approach
hc search framework  basic computational elements include generating candidate
states given state  computing heuristic function features via h cost function
features via c candidate states  computing heuristic cost scores via
learned heuristic cost function pair  h  c   computational time generating
candidate states depends employed search space    i  s   initial
state function successor function  example  generation candidates
efficient flipbit space compared lds space  involves running
recurrent classifier every action specified successor function s   therefore 
efficiency overall approach depends size candidate set
greatly improved generating fewer candidate states  e g   via pruning  parallelizing
computation  done preliminary work direction introducing sparse
versions lds flipbit search spaces pruning actions based recurrent
classifier scores  as specified prunining parameter k   simple pruning strategy
resulted    fold speedup little loss accuracy across several benchmark
problems  doppa et al       a   however  work needs done learning pruning
rules improve efficiency hc search approach 

   engineering methodology applying hc search
section  describe engineering methodology applying hc search framework new problems  high level  methodology involves selecting effective
time bounded search architecture  search space  search procedure  search time bound  
leveraging loss decomposition terms generation selection loss training
debugging heuristic cost functions  describe steps detail 
    selection time bounded search architecture
time bounded search architecture instantiated selecting search space  search
strategy  search time bound  mentioned before  effectiveness hc search
depends critically quality search space  i e   search depth target
outputs found  employed  fact  prior work empirically demonstrated performance gap search architectures flipbit space lds
space grows difference target depths increase  doppa et al       a  
therefore  important select design high quality search space problem
hand 
exists greedy predictor structured prediction problem  one could leverage define appropriate variant lds space  fortunately  greedy
predictors several problems natural language processing  computer vision  relational
networks  planning preferences  example  transition based parsers dependency parsing  nivre        goldberg   elhadad         greedy classifiers co reference
   

fihc search  learning framework search based structured prediction

resolution  chang  samdani    roth        stoyanov   eisner        event extraction
 li  ji    huang         sequential labelers boundary detection objects images
 payet   todorovic         iterative classifiers collective inference relational networks
 sen  namata  bilgic  getoor  gallagher    eliassi rad        doppa  yu  tadepalli   
getoor               classifier chains multi label prediction  read  pfahringer  holmes 
  frank         greedy planners planning preferences  xu  fern    yoon 
       general  designing high quality search spaces key research topic
work needs done direction  learning search operators  macro actions 
transformation rules transformation based learning  tbl   brill        optimize
search space one many possibilities  sometimes problem structure help
designing effective search spaces  example  multi label prediction problems 
outputs binary vectors small number active labels  highly sparse  
simple flipbit space initialized null vector effective  doppa  yu 
ma  fern    tadepalli      b  
picking search space  need select appropriate search procedure
search time bound  effectiveness search architecture measured performing
oracle search  true loss function used heuristic cost function  training
data  one could perform oracle search  ll search  different search procedures  e g  
greedy beam search  different time bounds select search procedure
effective  see benefit beam search problems considered 
expect change harder problems non hamming loss functions  e g  
b cubed score co reference resolution   search space redundant 
fix search time bound value performance search architecture
stagnates  otherwise  one allow slack search procedure recover
errors  experiments  found  size structured output 
reasonable value time bound  figure   provides justification choice  
    training debugging
training procedure involves learning heuristic h cost function c optimize
performance selected time bounded search architecture training data 
following staged learning approach  one could start learning heuristic via exact
imitation oracle search  that  learned heuristic h evaluated
measuring generation loss  hl search configuration   performance hlsearch configuration acceptable respect performance ll search 
move cost function learning part  otherwise  try improve heuristic either
employing sophisticated imitation learning algorithms  e g   dagger   enriching
feature function h   employing powerful rank learner  similarly  learning
cost function c conditioned learned heuristic  measure selection loss 
selection loss high  try improve cost function either adding
expressive features c employing powerful rank learner 

   comparison related work
described earlier  majority structured prediction work focused use
exact inference computing outputs tractable  approximate inference
   

fidoppa  fern    tadepalli

techniques  loopy belief propagation relaxation methods  not  learning focused tuning cost function parameters order optimize various
objective functions  differ among learning algorithms  lafferty et al         taskar
et al         tsochantaridis et al         mcallester  hazan    keshet        
approximate cost function learning approaches employ inference routine
training  example  piece wise training  sutton   mccallum         decomposed
learning  samdani   roth        special case pseudo max training  sontag  meshi 
jaakkola    globerson        fall category  training approaches
efficient  still need inference algorithm make predictions testing 
cases  one could employ constrained conditional models  ccm  framework
 chang  ratinov    roth        declarative  global  constraints make predictions using learned cost function  ccm framework relies integer linear
programming  ilp  inference method  roth   tau yih         recent work attempted integrate  approximate  inference cost function learning principled
manner  meshi  sontag  jaakkola    globerson        stoyanov  ropson    eisner       
hazan   urtasun        domke         researchers worked using higher order
features crfs context sequence labeling pattern sparsity assumption
 ye  lee  chieu    wu        qian  jiang  zhang  huang    wu         however 
approaches applicable graphical models sparsity assumption
hold 
alternative approach addressing inference complexity cascade training  felzenszwalb   mcallester        weiss   taskar        weiss  sapp    taskar        
efficient inference achieved performing multiple runs inference coarse level
fine level abstraction  approaches shown good success  place
restrictions form cost functions facilitate cascading  another potential drawback cascades approaches either ignore loss
function problem  e g  assuming hamming loss  require loss function
decomposable way supports loss augmented inference  approach sensitive
loss function makes minimal assumptions it  requiring
blackbox evaluate potential output 
classifier based structured prediction algorithms avoid directly solving argmin problem assuming structured outputs generated making series discrete
decisions  approach attempts learn recurrent classifier given input
x iteratively applied order generate series decisions producing target
output y  simple training methods  e g  dietterich  hild    bakiri        shown
good success positive theoretical guarantees  syed   schapire       
ross   bagnell         however  recurrent classifiers prone error propagation
 kaariainen        ross   bagnell         recent work  e g  searn  hal daume iii
et al          smile  ross   bagnell         dagger  ross et al          attempts
address issue using sophisticated training techniques shown state of theart structured prediction results  however  approaches use classifiers produce
structured outputs single sequence greedy decisions  unfortunately  many
problems  decisions difficult predict greedy classifier  crucial
good performance  contrast  approach leverages recurrent classifiers define good
   

fihc search  learning framework search based structured prediction

quality search spaces complete outputs  allows decision making comparing
multiple complete outputs choosing best 
non greedy methods learn scoring function search space
partial structured outputs  daumeiii   marcu        daume iii        xu  fern    yoon 
    b  huang  fayong    guo        yu  huang  mi    zhao         methods
perform online training  differ way search errors defined
weights updated errors occur  unfortunately  training scoring function
difficult hard evaluate states partial outputs theoretical
guarantees learned scoring function  e g   convergence generalization results 
rely strong assumptions  xu et al       b  
work closely related output space search approaches  doppa et al  
      wick et al          use single cost function serve search heuristic
score candidate outputs  serving dual roles often means cost
function needs make unclear tradeoffs  increasing difficulty learning  hcsearch approach overcomes deficiency learning two different functions  heuristic
function guide search generate high quality candidate outputs  cost function
rank candidate outputs  additionally  error decomposition hc search terms
heuristic error cost function error allows human designers learning system
diagnose failures take corrective measures 
approach related re ranking  collins         uses generative
model propose k best list outputs  ranked separate ranking
function  contrast  rather restricting generative model producing potential
outputs  approach leverages generic search efficient search spaces guided
learned heuristic function minimal representational restrictions  employs
learned cost function rank candidate outputs  recent work generating multiple
diverse solutions probabilistic framework considered another way producing
candidate outputs  representative set approaches line work diverse mbest  batra  yadollahpour  guzman rivera    shakhnarovich         m best modes  park
  ramanan        chen  kolmogorov  zhu  metaxas    lampert        determinantal
point processes  kulesza   taskar        
general area speedup learning studied planning search community
related work  fern         problems  cost function typically known
objective learn control knowledge  i e   heuristic function  directing search
algorithm low cost terminal node search space  example  stage  boyan  
moore        learns evaluation function states improve performance
search  value state corresponds performance local search algorithm
starting state   zhang   dietterich        use reinforcement learning  rl 
methods learn heuristics job shop scheduling goal minimizing duration
schedule  unlike problems planning combinatorial optimization 
cost function given structured prediction problems  therefore  hc search
approach learns cost function score structured outputs along heuristic
function guide search towards low cost outputs 
   

fidoppa  fern    tadepalli

   summary future work
introduced hc search framework structured prediction whose principal feature
separation cost function search heuristic  showed framework
yields significantly superior performance state of the art results  allows informative error analysis diagnostics 
investigation showed main source error existing output space approaches including approach  hc search  inability cost function correctly rank candidate outputs produced output generation process  analysis
suggests learning powerful cost functions  e g   regression trees  mohan  chen 
  weinberger         eye towards anytime performance  grubb   bagnell       
xu  weinberger    chapelle        would productive  results suggested
room improve overall performance better heuristic learning  thus  another
direction pursue heuristic function learning speed process generating
high quality outputs  fern        
future work includes applying framework challenging problems natural language processing  e g   co reference resolution  dependency parsing  semantic
parsing  computer vision  e g   object detection biological images lam  doppa  hu 
todorovic  dietterich  reft    daly        multi object tracking complex sports
videos chen  fern    todorovic         effectiveness hc search approach depends
quality search space  therefore  work needs done learning
optimize search spaces leveraging problem structure  similarly  studying pruning
techniques improve efficiency learning inference another useful
direction 
acknowledgements
authors would thank anonymous reviewers jason eisner  associate
editor  comments feedback  first author would thank tom
dietterich encouragement support throughout work  work supported part nsf grants iis          iis         part defense advanced
research projects agency  darpa  air force research laboratory  afrl 
contract no  fa                opinions  findings conclusions recommendations expressed material author s  necessarily reflect
views nsf  darpa  air force research laboratory  afrl  
us government  preliminary version article published aaai       doppa
et al        

appendix a  limited discrepancy search  lds  space
limited discrepancy search  lds  space  doppa et al             a  defined terms
learned recurrent classifier h  thus  start describing recurrent classifier
explain key idea behind lds space  simplicity  explain main ideas using
sequence labeling problem  handwriting recognition task  noting generalize
non sequence labeling problems  for full details see doppa et al             a  
   

fihc search  learning framework search based structured prediction

figure    illustration recurrent classifier handwriting recognition problem  classifier predicts labels left to right order  makes labeling decision
position greedily based character image predicted label
previous position  shown dotted box   particular example 
classifier makes mistake first position error propagates
positions leading bad output 

a   recurrent classifier
sequence labeling problem  recurrent classifier produces label position
sequence  based input position predicted labels previous positions
 dietterich et al          learned classifier accurate  number incorrect
labeling decisions relatively small  however  even small number errors
propagate cause poor outputs 
figure   illustrates recurrent classifier handwriting recognition example 
classifier predicts labels left to right order  makes labeling decision
position greedily based character image predicted label previous
position  shown dotted box   particular example  classifier makes
mistake first position error propagates leading bad output   
errors  
a   limited discrepancy search  lds 
lds originally introduced context problem solving using heuristic search
 harvey   ginsberg         key idea behind lds realize classifier
prediction corrected small number critical errors  much better output
   

fidoppa  fern    tadepalli

 a 

 b 

figure    illustration limited discrepancy search  lds  handwriting recognition
problem  given discrepancy set d  generate unique output
running recurrent classifier changes d   a  lds one
discrepancy  introduce discrepancy first position label  shown
red  run classifier  able correct two subsequent labels   b 
lds two discrepancies  introduce additional discrepancy fifth
position label c  shown red  run classifier  recover target
output struct 

would produced  lds conducts  shallow  search space possible corrections
hope finding output better original 
given classifier h sequence length   discrepancy pair  i  l 
              index sequence position l label  generally different
prediction classifier position i  set discrepancies d 
generate unique output h d  x  running classifier changes d 
discrepancies viewed overriding prediction h particular positions 
possibly correcting errors  introducing new errors  one extreme  empty 
get original output produced greedy classifier  see figure    
extreme  specifies label position  output influenced h
completely specified discrepancy set  figure   illustrates lds
handwriting example  introduce discrepancy first position label
 shown red  run classifier  able correct two subsequent labels  see
figure   a    introduce additional discrepancy fifth position label c  shown
red  run classifier  recover target output struct  see figure   b   
   

fihc search  learning framework search based structured prediction

figure    example limited discrepancy search  lds  space handwriting recognition
problem  highlighted state corresponds one true output
smallest depth 

practice  h reasonably accurate  primarily interested small
discrepancy sets relative length sequence  problem know
corrections made thus lds conducts search discrepancy
sets  usually small large sets 

a   lds space
given recurrent classifier h  define corresponding limited discrepancy search space
complete outputs follows  state search space represented  x  d 
x input sequence discrepancy set  view state  x  d  equivalent
input output state  x  h d  x    initial state function simply returns  x   
corresponds original output recurrent classifier  successor function
state  x  d  returns set states form  x  d     d  d 
additional discrepancy  way  path lds search space starts
output generated recurrent classifier traverses sequence outputs
differ original number discrepancies  given reasonably accurate h 
expect high quality outputs generated relatively shallow depths
search space hence generated quickly 
   

fidoppa  fern    tadepalli

figure   illustrates  limited discrepancy search space  state consists
input x  discrepancy set output produced running classifier
specified discrepancy set  i e   h d  x   root node empty discrepancy set  nodes
level one contain discrepancy sets size one  highlighted state corresponds
smallest depth state containing target output 

appendix b  hardness proof hc search consistency problem
theorem    hc search consistency problem greedy search linear heuristic
cost functions np hard even restrict problems possible
heuristic functions uncover zero loss output 
proof  reduce minimum disagreement problem linear binary classifiers 
proven np complete work hoffgen  simon  horn        
one statement problem given input set n   p dimensional vectors
   x            xn   positive integer k  problem decide whether
p dimensional real valued weight vector w w xi     k
vectors 
first sketch high level idea proof  given instance minimum disagreement  construct hc search consistency problem single structured
training example  search space corresponding training example designed
single node n loss zero nodes loss
   linear heuristic functions greedy search paths terminate n  
generating set nodes outputs path there  search space designed
possible path initial node n corresponds selecting k fewer
vectors   denote   traversing path  set nodes
generated  and hence must scored c   say n   includes feature vectors corresponding
along negation feature vectors   define
n assigned zero vector  cost node   weight vector 
order achieve zero loss given path consideration  must weight
vector wc wc x   x n   construction equivalent
wc x     x   possible found solution minimum
disagreement problem since  t   k  remaining details show construct
space setting heuristic weights generate paths corresponding
possible way paths end n   completeness describe
construction below 
search node space n tuple  i  m  t    n  
  k  one   node types set  d  s      x    x   
viewed indexing example xi effectively codes many instances
selected mistakes hence put   finally  encodes type
search node following meanings become clear
construction   decision   s   positive selection    negative selection   x   positive
instance   x  negative instance   search space constructed example xi
   may clear example  allow over riding discrepancies provide opportunity recover search errors 

   

fihc search  learning framework search based structured prediction

figure    example search space    x    x    x    k      greedy paths
terminate zero loss node n path selects one instance
include mistake set  

considered order choice made whether count mistake  put
  not  choice made decision nodes  form  i  m  d  
indicating decision made example already
examples selected   decision node   k two children  i  m   
 i  m  s     respectively correspond selecting xi mistake set not 
later show features assigned nodes allow heuristic make
selection desired 
selection node single node child  particular  positive selection node
 i  m  s    positive instance node  i  m  x    child  negative selection nodes
 i  m    negative instance node  i  m  x   child  instance node
effectively implements process putting xi become clear
feature vectors described below  arriving either positive negative instance
node  consideration xi complete must move decision next
example xi     thus  positive instance node  i  m  x    single child decision node
   

fidoppa  fern    tadepalli

 i      m  d   negative instance node single child decision node  i           d  
noting number mistakes incremented negative nodes 
final details search space structure ensure k mistakes
allowed force search paths terminate n   particular  decision
node  i  m  d    k  know mistakes allowed hence
decisions allowed  thus  node form path n
goes positive instance nodes  i  m  x              n  m  x     reflects none
 xi           xn     figure   shows example search space construction 
given search space  polynomial size  since k n    one verify
set k fewer instances path root n goes
negative instance nodes instances positive instance nodes
instances   further  possible path goes either positive negative
instance node instance k negative nodes  thus direct
correspondence paths mistake sets  
describe assign features node way allows
heuristic function select path effectively construct set   node
u feature vector  u     x  s  b   component x p dimensional feature vector
correspond one xi   component n  dimensional vector
si        implement selection instances  finally b binary value
equal   non instance nodes   positive negative instance nodes 
mapping nodes feature vectors follows  decision node  i  m  d  
zeros  except b      positive selection node  i  m  s    zeros except si    
b      negative selection nodes similar except si      positive instance
node  i  m  x    feature vector  xi         negative instance nodes  i  m  x  
feature vector  xi          finally feature vector n zeros 
key idea note heuristic function effectively select positive
negative selection node setting weight si positive negative respectively 
particular  set negative selection nodes visited  and hence negative instance nodes 
correspond first k fewer negative weight values component feature
vector  thus  heuristic select set negative nodes wants go through 
k  path three types nodes encountered
cost function must rank  first  control nodes  decision selection nodes 
b      next positive instance nodes feature
vector  xi         k negative instance nodes feature vectors  xi         
cost function easily rank n higher control nodes setting weight
b negative  find heuristic weights x component allows n
ranked highest solution original minimum disagreement problem 
solution disagreement problem easy see
solution hc search consistency problem selecting heuristic spans
proper set  

references
agarwal  s     roth  d          learnability bipartite ranking functions  proceedings
international conference learning theory  colt   pp       
   

fihc search  learning framework search based structured prediction

batra  d   yadollahpour  p   guzman rivera  a     shakhnarovich  g          diverse mbest solutions markov random fields  proceedings european conference
computer vision  eccv   pp      
boyan  j  a     moore  a  w          learning evaluation functions improve optimization local search  journal machine learning research  jmlr            
brill  e          transformation based error driven learning natural language processing  case study part of speech tagging  computational linguistics         
       
chang  k  w   samdani  r     roth  d          constrained latent variable model
coreference resolution  proceedings conference empirical methods
natural language processing  emnlp   pp         
chang  m  w   ratinov  l  a     roth  d          structured learning constrained
conditional models  machine learning journal  mlj                  
chen  c   kolmogorov  v   zhu  y   metaxas  d     lampert  c  h          computing
probable modes graphical model  proceedings international
conference artificial intelligence statistics  aistats  
chen  s   fern  a     todorovic  s          multi object tracking via constrained sequential labeling  appear proceedings ieee conference computer vision
pattern recognition  cvpr  
collins  m          discriminative reranking natural language parsing  proceedings
international conference machine learning  icml   pp         
collins  m          ranking algorithms named entity extraction  boosting
voted perceptron  acl 
crammer  k   dekel  o   keshet  j   shalev shwartz  s     singer  y          online passiveaggressive algorithms  journal machine learning research  jmlr             
daume iii  h          practical structured learning techniques natural language
processing  ph d  thesis  university southern california  los angeles  ca 
daumeiii  h     marcu  d          learning search optimization  approximate large
margin methods structured prediction  icml 
dietterich  t  g   hild  h     bakiri  g          comparison id  backpropagation
english text to speech mapping  machine learning journal  mlj                
domke  j          structured learning via logistic regression  proceedings advances
neural information processing systems  nips   pp         
doppa  j  r   fern  a     tadepalli  p          output space search structured prediction  proceedings international conference machine learning  icml  
doppa  j  r   fern  a     tadepalli  p          hc search  learning heuristics cost
functions structured prediction  proceedings aaai conference artificial
intelligence  aaai  
doppa  j  r   fern  a     tadepalli  p       a   structured prediction via output space
search  journal machine learning research  jmlr                
   

fidoppa  fern    tadepalli

doppa  j  r   yu  j   ma  c   fern  a     tadepalli  p       b   hc search multi label
prediction  empirical study  appear proceedings aaai conference
artificial intelligence  aaai  
doppa  j  r   yu  j   tadepalli  p     getoor  l          chance constrained programs
link prediction  proceedings nips workshop analyzing networks
learning graphs 
doppa  j  r   yu  j   tadepalli  p     getoor  l          learning algorithms link
prediction based chance constraints  proceedings european conference
machine learning  ecml   pp         
felzenszwalb  p  f     mcallester  d  a          generalized a  architecture  journal
artificial intelligence research  jair              
fern  a          speedup learning  encyclopedia machine learning  pp         
fern  a   yoon  s  w     givan  r          approximate policy iteration policy
language bias  solving relational markov decision processes  journal artificial
intelligence research  jair             
goldberg  y     elhadad  m          efficient algorithm easy first non directional
dependency parsing  proceedings human language technologies  conference
north american chapter association computational linguistic  hltnaacl   pp         
grubb  a     bagnell  d          speedboost  anytime prediction uniform nearoptimality  journal machine learning research   proceedings track             
hal daume iii  langford  j     marcu  d          search based structured prediction 
machine learning journal  mlj                  
harvey  w  d     ginsberg  m  l          limited discrepancy search  proceedings
international joint conference artificial intelligence  ijcai   pp         
hazan  t     urtasun  r          efficient learning structured predictors general
graphical models  corr  abs           
hoffgen  k  u   simon  h  u     horn  k  s  v          robust trainability single
neurons  journal computer system sciences                 
huang  l   fayong  s     guo  y          structured perceptron inexact search 
proceedings human language technology conference north american
chapter association computational linguistics  hlt naacl   pp     
    
jiang  j   teichert  a   daume iii  h     eisner  j          learned prioritization
trading accuracy speed  proceedings advances neural information
processing  nips  
kaariainen  m          lower bounds reductions  atomic learning workshop 
keshet  j   shalev shwartz  s   singer  y     chazan  d          phoneme alignment based
discriminative learning  proceedings annual conference international
speech communication association  interspeech   pp           
   

fihc search  learning framework search based structured prediction

khardon  r          learning take actions  machine learning journal  mlj          
     
kulesza  a     taskar  b          determinantal point processes machine learning 
foundations trends machine learning                  
lafferty  j   mccallum  a     pereira  f          conditional random fields  probabilistic
models segmenting labeling sequence data  proceedings international
conference machine learning  icml   pp         
lam  m   doppa  j  r   hu  x   todorovic  s   dietterich  t   reft  a     daly  m         
learning detect basal tubules nematocysts sem images  iccv workshop
computer vision accelerated biosciences  cvab   ieee 
li  q   ji  h     huang  l          joint event extraction via structured prediction
global features  proceedings   st annual meeting association
computational linguistics  acl   pp       
mcallester  d  a   hazan  t     keshet  j          direct loss minimization structured
prediction  proceedings advances neural information processing systems
 nips   pp           
meshi  o   sontag  d   jaakkola  t     globerson  a          learning efficiently
approximate inference via dual losses  proceedings international conference
machine learning  icml   pp         
mohan  a   chen  z     weinberger  k  q          web search ranking initialized
gradient boosted regression trees  journal machine learning research   proceedings track           
nivre  j          algorithms deterministic incremental dependency parsing  computational linguistics                 
park  d     ramanan  d          n best maximal decoders part models  proccedings
ieee international conference computer vision  iccv   pp           
payet  n     todorovic  s          sledge  sequential labeling image edges
boundary detection  international journal computer vision  ijcv              
   
qian  x   jiang  x   zhang  q   huang  x     wu  l          sparse higher order conditional random fields improved sequence labeling  proceedings international
conference machine learning  icml  
read  j   pfahringer  b   holmes  g     frank  e          classifier chains multi label
classification  machine learning                 
ross  s     bagnell  d          efficient reductions imitation learning  journal
machine learning research   proceedings track            
ross  s   gordon  g  j     bagnell  d          reduction imitation learning
structured prediction no regret online learning  journal machine learning
research   proceedings track             
   

fidoppa  fern    tadepalli

roth  d     tau yih  w          integer linear programming inference conditional
random fields  proceedings international conference machine learning
 icml   pp         
samdani  r     roth  d          efficient decomposed learning structured prediction 
proceedings international conference machine learning  icml  
sen  p   namata  g   bilgic  m   getoor  l   gallagher  b     eliassi rad  t          collective classification network data  ai magazine                
sontag  d   meshi  o   jaakkola  t     globerson  a          data means less inference 
pseudo max approach structured learning  proceedings advances neural
information processing systems  nips   pp           
stoyanov  v     eisner  j          easy first coreference resolution  proceedings
international conference computational linguistics  coling   pp           
stoyanov  v   ropson  a     eisner  j          empirical risk minimization graphical
model parameters given approximate inference  decoding  model structure 
proceedings international conference artificial intelligence statistics
 aistats   pp         
sutton  c  a     mccallum  a          piecewise training structured prediction 
machine learning journal  mlj                    
syed  u     schapire  r          reduction apprenticeship learning classification  proceedings advances neural information processing systems  nips  
pp           
taskar  b   guestrin  c     koller  d          max margin markov networks  proceedings
advances neural information processing systems  nips  
tsochantaridis  i   hofmann  t   joachims  t     altun  y          support vector machine learning interdependent structured output spaces  proceedings
international conference machine learning  icml  
tsochantaridis  i   joachims  t   hofmann  t     altun  y          large margin methods
structured interdependent output variables  journal machine learning
research  jmlr               
vogel  j     schiele  b          semantic modeling natural scenes content based
image retrieval  international journal computer vision  ijcv                  
weiss  d          structured prediction cascades code  http   code google com p 
structured cascades  
weiss  d   sapp  b     taskar  b          sidestepping intractable inference structured
ensemble cascades  proceedings advances neural information processing
systems  nips   pp           
weiss  d     taskar  b          structured prediction cascades  journal machine
learning research   proceedings track            
wick  m  l   rohanimanesh  k   bellare  k   culotta  a     mccallum  a          samplerank  training factor graphs atomic gradients  proceedings international
conference machine learning  icml  
   

fihc search  learning framework search based structured prediction

wick  m  l   rohanimanesh  k   singh  s     mccallum  a          training factor graphs
reinforcement learning efficient map inference  proceedings advances
neural information processing systems  nips   pp           
xu  y   fern  a     yoon  s       a   learning linear ranking functions beam search
application planning  journal machine learning research          
     
xu  y   fern  a     yoon  s  w       b   learning linear ranking functions beam
search application planning  journal machine learning research  jmlr  
             
xu  y   fern  a     yoon  s  w          iterative learning weighted rule sets
greedy search  proceedings international conference automated planning
systems  icaps   pp         
xu  z   weinberger  k     chapelle  o          greedy miser  learning test time
budgets  proceedings international conference machine learning  icml  
ye  n   lee  w  s   chieu  h  l     wu  d          conditional random fields
high order features sequence labeling  proceedings advances neural
information processing systems  nips   pp           
yu  h   huang  l   mi  h     zhao  k          max violation perceptron forced
decoding scalable mt training  proceedings empirical methods natural
language processing  emnlp   pp           
zhang  w     dietterich  t  g          reinforcement learning approach job shop
scheduling  proceedings international joint conference artificial intelligence
 ijcai   pp           

   


